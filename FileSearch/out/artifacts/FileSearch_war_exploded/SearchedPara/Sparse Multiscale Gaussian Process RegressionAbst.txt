 Christian Walder christian.walder@tuebingen.mpg.de Kwang In Kim kimki@tuebingen.mpg.de Bernhard Sch  X olkopf berhard.schoelkopf@tuebingen.mpg.de Max Planck Institute for Biological Cybernetics Spemannstr. 38, 72076 Tuebingen, Germany The Gaussian process (g.p.) is a popular non-parametric model for supervised learning problems. Although g.p. X  X  have been shown to perform well on a wide range of tasks, their usefulness is severely lim-ited by the O ( n 3 ) time and O ( n 2 ) storage require-ments where n is the number of data points. A large amount of work has been done to alleviate this prob-lem, either by approximating the posterior distribu-tion, or constructing degenerate covariance functions for which the exact posterior is less expensive to eval-uate (Smola &amp; Bartlett, 2000; Csat  X o &amp; Opper, 2002; Lawrence et al., 2002; Seeger et al., 2003; Snelson &amp; Ghahramani, 2006)  X  for a unifying overview see (Qui  X nonero-Candela &amp; Rasmussen, 2005). The major-ity of such methods achieve an O ( m 2 n ) time complex-ity for training where m  X  n is the number of points on which the computations are based.
 The g.p. can be interpreted as a linear (in the pa-rameters) model which, due to its non-parametric na-ture, has potentially as many parameters to estimate as there are training points. An exception is the case where the covariance function has finite rank, such as the linear covariance function on R d  X  R d given by k ( x , x  X  ) = x  X  x  X  , which has rank d . In this case the g.p. collapses to a parametric method and it is possible to derive algorithms with O ( d 2 n ) time complexity by basing the computations on d basis functions. For non-degenerate covariance functions, most existing sparse g.p. algorithms all have in common that they base their computations on m basis functions of the form k ( v i , ). Typically the set V = { v 1 , v 2 , . . . , v is taken to be a subset of the training set (Smola &amp; Bartlett, 2000; Csat  X o &amp; Opper, 2002; Seeger et al., 2003). For example Seeger et al. (Seeger et al., 2003) employ a highly efficient approximate information gain criteria to incrementally select points from the training set in a greedy manner.
 More recently Snelson and Ghahramani (2006) have shown that further improvements in the quality of the model for a given m can be made  X  especially for small m  X  by removing the restriction that V be a subset of the training set. For this they introduced a new sparse g.p. model which has the advantage of being closer to the full g.p., and also of being more amenable to gradi-ent based optimisation of the marginal likelihood with respect to the set V . A further advantage of their con-tinuous optimisation of V is that the hyper-parameters of the model can be optimised at the same time  X  this is more difficult when V is taken to be a subset of the training set, since choosing such a subset is a hard combinatoric problem.
 In this paper we take a logical step forward in the de-velopment of sparse g.p. algorithms. We also base our computations on a finite set of basis functions, but remove the restriction that the basis functions be of the form k ( v i , ) where k is the covariance of the g.p. This will require computing integrals involving the ba-sis and covariance functions, and so cannot always be done in closed form. Fortunately however, closed form expressions can be obtained for arguably the most use-ful scenario, namely that of Gaussian covariance func-tion (with arbitrary diagonal covariance matrix) along with Gaussian basis functions (again each with their own arbitrary diagonal covariance matrix).
 The central idea is that, under some mild restrictions, we can compute the prior probability density  X  under the g.p. model with Gaussian covariance  X  of arbi-trary Gaussian mixtures. Our analysis is new, but there is a precedent for it in the literature. In par-ticular, Walder et al. (2006) employ a similar idea, but from an reproducing kernel Hilbert space (r.k.h.s.) rather than a g.p. perspective, and for a different ba-sis and covariance function. Also related is (Gehler &amp; Franz, 2006), which analyses from a g.p. perspective with arbitrary basis and covariance function, but with the difference that they do not take infinite limits. Our idea has a direct r.k.h.s. analogy. Indeed the main idea is applicable to any kernel machine, but in this pa-per we focus on the g.p. framework. The main reason for this is that it allows us to build on the sparse g.p. model of Snelson and Ghahramani (2006), which has been shown to be amenable to gradient based optimi-sation of the marginal likelihood. Nonetheless we do provide some experimental results for the kernel ridge regression (k.r.r.) case, as well as an animated toy ex-ample of the support vector machine (s.v.m.), in the accompanying video.
 The paper is structured as follows. Section 2 provides an introduction to g.p. regression. In Section 3 we derive the likelihood of arbitrary Gaussian mixtures under the g.p. model with Gaussian covariance, and clarify the link to r.k.h.s. X  X . In Section 4 we discuss and motivate the precise probabilistic model which we use to make practical use of our theoretical results. Experimental results and conclusions are presented in Sections 5 and 6, respectively. We assume that we are given an independent and iden-tically distributed (i.i.d.) sample drawn from an unknown distribution, and the goal is to estimate p ( y | x ). We introduce a latent variable u  X  R , and make the assumption that p ( y | u, x ) = p ( y | u ). Hence we can think of y as a noisy realisation of u , which we model by p ( y | u ) = N ( y | u,  X  2 n ) where  X  hyper-parameter. 1 The relationship x  X  u is a random process u ( ), namely a zero mean g.p. with covariance function k : R d  X  R d  X  R . Typically k will be defined in terms of further hyper-parameters. We shall denote such a g.p. as G ( k ), which is defined by the fact that its joint evaluation at a finite number of input points is a zero mean Gaussian random variable with covariance One can show that given the hyper-parameters, the posterior p ( u |S ), where 2 [ u ] i = u ( x i ), is where [ K xx ] ij = k ( x i , x j ). Like many authors we neglect to notate the conditioning upon the hyper-parameters, both in the above expression and for the remainder of the paper. Now, it can also be shown that the latent function u  X  = u ( x  X  ) at an arbitrary test point x  X  is distributed according to p ( u  X  | x  X  , S ) = R p ( u  X  | x  X  , S , u ) p ( u | x  X  , S ) d u = N ( u  X  |  X  and we have defined [ k  X  ] i = k ( x  X  , x i ). In a Bayesian setting, one places priors over the hyper-parameters and computes the hyper-posterior, but this usually involves costly numerical integration techniques. Alternatively one may fix the hyper-parameters to those obtained by maximising some cri-teria such as the marginal likelihood conditioned upon them, p ( y |X ) = N ( y | 0 , K y ), where X = ( x 1 , . . . , x and K yy = K xx +  X  2 n I is the covariance matrix for y . This can be computed using the result that where c is a term independent of the hyper-parameters. Even when one neglects the cost of choosing the hyper-parameters however, it typically costs O ( n ) and O ( n 2 time to evaluate the posterior mean and variance re-spectively, after an initial setup cost of O ( n 3 ). In this section we  X  loosely speaking  X  derive the likeli-hood of a mixture of Gaussians with arbitrary diagonal covariance matrices, under a g.p. prior with a covari-ance function that is also a Gaussian with arbitrary di-agonal covariance matrix. Let u be drawn from G ( k ). As we mentioned previously, this means that the vec-tor of joint evaluations at an arbitrary ordered set of points X = ( x 1 , . . . , x n ) is a random variable, call it u
X , distributed according to Hence by definition where || denotes the matrix determinant. Note that this is simply the probability density function (p.d.f.) of u X where we have set the argument to be P m i =1 c i u for some c i  X  R . We have done this because later we will wish to determine the likelihood of a function expressed as a summation of fixed basis functions. To this end we now consider an infinite limit of the above case. Taking the limit n  X  X  X  of uniformly distributed points 3 x i leads to the following p.d.f. for G ( k ), where  X  k ( u i , u j ) , that in the previous case of finite n , if we let u = K xx and assume that K xx is invertible, then  X  = K  X  1 xx u . Following this finite analogy, by k  X  1 we now intend a sloppy notation for the function which, for u = R Hence if we define then k  X  1 is by definition the Green X  X  function (Roach, 1970) of M k , as it satisfies Let us now consider the covariance function given by k ( x , y ) = cg ( x , y ,  X  ), where c &gt; 0,  X  &gt; 0  X  R g is a normalised Gaussian on R d  X  R d with diagonal covariance matrix, that is 4 g ( x , y ,  X  ) , | 2  X  diag (  X  ) |  X  1 2 exp  X  If we assume furthermore that our function is an arbi-trary mixture of such Gaussians, so that then the well known integral (for the convolution of two Gaussians)
Z leads to 1 c As the covariance function and the basis functions are all Gaussian we can obtain in closed form
 X  For clarity we have noted above each equals sign the number of the equation which implies the correspond-ing logical step. The following expression summarises the main idea of the present section  X  exp  X  We give only an unnormalised form by neglecting the to the inverse of the integral of the right hand side of the above expression with respect to all functions P i =1 c i g ( , v i ,  X  i ). We need not concern ourselves with choosing a measure with respect to which this integral is finite, due to the fact that, since we will be working only with ratios of the above likelihood ( i.e. for maximum a posteriori (m.a.p.) estimation and marginal likelihood maximisation), we need only the unnormalised form. Note that this peculiarity is not particular to our proposed sparse approximation to the g.p., but is a property of g.p. X  X  in general. Interpretation We now make two remarks regard-ing the expression (13). i) If  X  1 =  X  2 = =  X  n =  X  and we reparameterise c i = cc  X  i then it simplifies to (5). ii) Let c = 1 and h ( x ) = exp  X  1 2 x  X  diag (  X  1 )  X  1 an unnormalised Gaussian. Using (9) and (13) we can derive the log-likelihood of h under the g.p. prior, Simple analysis of this expression shows that the most likely such function h is that with  X  1 =  X  . From this extremal point, as any component of  X  1 increases, the log likelihood of h decreases without bound. Similarly decreasing any component of  X  1 also decreases the log likelihood, and as any component of  X  1 approaches half the value of the corresponding component of  X  , then the log likelihood decreases without bound. To be more precise, we have for all j = 1 , 2 , . . . , d that An interesting consequence of the second remark is that, roughly speaking, it is not possible to recover a Gaussian function using a g.p. with Gaussian covari-ance, if the covariance function is more than twice as broad as the function to be recovered. Although this may at first appear to contradict proven consistency results for the Gaussian covariance function (for ex-ample (Steinwart, 2002)), this is not the case. On the contrary, such results hold only for compact domains, and our analysis is for R d .
 An r.k.h.s. Analogy We note that (13) has a direct analogy in the theory of r.k.h.s. X  X , as made clear by the following lemma. The lemma follows from (13) and the well understood relationship between every g.p. and the corresponding r.k.h.s. of functions.
 Lemma 3.1. Let H be the r.k.h.s. with reproducing kernel g ( , ,  X  ) . If the conditions  X  i &gt; 1 2  X  and  X   X  are satisfied component-wise, then If either condition is not satisfied, then the correspond-ing function on the left hand side is not in H . Naturally this can also be proven directly, but doing so for the general case is more involved and we omit the details due to space limitations. 5 However, by assuming that the conditions  X  i &gt;  X  and  X  j &gt;  X  are satisfied component-wise, then it is straightfor-ward to obtain the main result. The basic idea is as follows. Using (11) we substitute g ( , v p ,  X  p ) = R g ( , x p ,  X  ) g ( x p , v p ,  X  p  X   X  ) d x p for p = i, j into the l.h.s. of (15). By linearity we can write the two integrals outside the inner product. Next we use the r.k.h.s. reproducing property  X  the fact that uate the inner product. Using (11) we integrate to obtain the r.h.s. of (15). 4.1. A Simple Approach In the previous section we derived the g.p. likelihood over a certain restricted function space. This likeli-hood defines a distribution over functions of the form P i =1 c i g ( , v i ,  X  i ) where g as given previously is deter-ministic and the c i are, by inspection of (13), normally distributed according to where [ U  X  ] i,j =  X  k ( u i , u j ). Let us write U = { u 1 , . . . , u m } (which we refer to as the basis) and re-fer to the random process thus defined as G U ( k ). This new random process is equivalent to a full g.p. with covariance function of rank at most m given by E where [ u vx ] i = g ( x , v i ,  X  i ) and [ u vz ] i = g ( z , v As an aside, note that if we choose as the ba-sis U = { g ( , x ,  X  ) , g ( , z ,  X  ) } , then it is easy to verify using (17) that E f  X  X  E f  X  X  ( k ) [ f ( x ) f ( z )]. This is analogous to a special case of the representer theorem from the theory of r.k.h.s. X  X , and agrees with the interpretation that (16) is such that G U ( k ) approximates G ( k ) well in some sense, for the given basis U .
 Returning to the main thread, the new posterior can be derived as it was at the end of Section 2 for the exact g.p., but using the new covariance function (17). Hence after some algebra we have from (2) and (3) that, conditioned again upon the hyper-parameters, the latent function u  X  = u ( x  X  ) at an arbitrary test point is distributed according to p u  X  X  N ( u  X  |  X   X  ,  X  2  X  ), where that these expressions can be evaluated in O ( m ) and O ( m 2 ) time respectively, after an initial setup or train-ing cost of O ( m 2 n ). This is the usual improvement over the full g.p. obtained by such sparse approxima-tion schemes. It turns out however that by employ-ing an idea introduced by Snelson and Ghahramani (2006), we can retain these computational advantages while switching to a different model that is closer to the full g.p. 4.2. Inference with Improved Variance A fair criticism of the previous model is that the pre-dictive variance approaches zero far away from the ba-sis function centres v i , as can be seen from (19). It turns out that this is particularly problematic to gra-dient based methods for choosing the basis (the v i and  X  ) by maximising the marginal likelihood (Snelson &amp; Ghahramani, 2006). An effective but still computa-tionally attractive way of healing the model is to switch to a different g.p.  X  which we denote  X  G U ( k )  X  whose covariance function satisfies where  X  a , b is the Kronecker delta function and  X  a , b 1  X   X  a , b . Note that if x = z then the covariance is that of the original g.p. G ( k ), otherwise it is that of G
U ( k ). Unlike (17), the prior variance in this case is the same as that of the full g.p., even though in general the covariance is not. Once again the posterior can be found as before by replacing the covariance function in (2) and (3) with the right hand side of (20). In this case we obtain after some algebra the expression p  X  = diag (  X  ), and To compute the marginal likelihood we can use the ex-pression (4). Note that it can be computed efficiently using Cholesky decompositions. In order to optimize the marginal likelihood, we also need its gradients with respect to the various parameters. Our derivation of the gradients (which closely follows (Seeger et al., 2003)) is long and tedious, and has been omitted due to space limitations. Note that by factorising appro-priately, all of the required gradients can be obtained in O ( m 2 n + mnd ). 4.3. A Unifying View We now briefly outline how the method of the previ-ous section fits into the unifying framework of sparse g.p. X  X  provided by Qui  X nonero-Candela and Rasmussen (2005). Using Bayes rule and marginalising out the training set latent variables u , we obtain the posterior Here we have neglected to notate conditioning on x  X  and x 1 , . . . , x n , and have written p instead of the more precise p u  X  X  ( k ) . Our algorithm can be interpreted as employing two separate approximations. The first is conditional independence of u and u  X  given a , i.e. where a (which is marginalised out) is taken to be the vector of inner products between the basis func-tions u i and the latent function u , in the r.k.h.s. H associated with k ( , ). The second approximation is where diag  X  ( A ) is a diagonal matrix matching A on the diagonal, and [ K xv ] i,j = k ( x i , v j ), etc. Note that the first line can be shown with some algebra, whereas the second is an approximation. One can show that this leads to the result of Section 4.2, but we omit the details for brevity. Of the algorithms considered in (Qui  X nonero-Candela &amp; Rasmussen, 2005), ours is clos-est to that of Snelson and Ghahramani (2006), however there the basis functions take the form u i = k ( v i , ), which has two implications. Firstly, a simplifies to the vector of the values of u at v 1 , . . . , v m . Secondly, U
 X  and U xv simplify to K vv and K xv , respectively. Our main goal is to demonstrate the value of being able to vary the  X  i individually. Note that the chief advantage of our method is in producing highly sparse solutions, and the results represent the state of the art in this respect. As such, and since the prediction cost is O ( md ), we analyse the predictive performance of the model as a function of the number of basis functions m . Note that neither our method nor the most closely related method of Snelson and Ghahra-mani (2006) are particularly competitive in terms of training time. Nonetheless, there is a demand for algo-rithms which sacrifice training speed for testing speed, such as real-time vision and control systems, and web services in which the number of queries is large. Let us clarify the terminology we use to refer to the various algorithms under comparison. Our new method is the variable sigma Gaussian process (v.s.g.p.). The vsgp-full variant consists of optimising the marginal likelihood with respect to the m basis centers v i  X  R d and length scales  X  i  X  R d of our ba-sis functions u i = g ( , v i ,  X  i ) where g is defined in (9). Also optimised are the following hyper parameters  X  the noise variance  X  n  X  R of (1), and the parameters cg ( , ,  X  ). The vsgp-basis variant is identical to vsgp-full except that  X  n , c and  X  are determined by opti-mising the marginal likelihood of a full g.p. trained on a subset of the training data, and then held fixed while the  X  i and v i are optimised as before. Both v.s.g.p. variants use the  X  G U ( k ) probabilistic model of Section 4.2, where k = cg ( , ,  X  ). For the optimisation of the sparse pseudo-input Gaussian process (s.p.g.p.) and v.s.g.p. methods we used a standard conjugate gradient type optimiser. 6 spgp-full and spgp-basis correspond to the work of Snelson and Ghahramani (2006), and are identical to their v.s.g.p. counterparts except that  X  as with all sparse g.p. methods prior to the present work  X  they are forced to satisfy the constraints  X  i =  X  , i = 1 . . . m . To initialise the marginal likelihood optimisa-tion we take the v i to be a k -means clustering of the training data. The other parameters are always ini-tialised to the same sensible starting values, which is reasonable due to the preprocessing we employ (which is identical to that of (Seeger et al., 2003)) in order to standardise the data sets.
 Figure 1 demonstrates the basic idea on a one dimen-sional toy problem. Using m = 4 basis functions is not enough for spgp-full to infer a posterior similar to that of the full g.p. trained on the depicted n = 200 train-ing points. The v.s.g.p. achieves a posterior closer to that of the full g.p. by employing  X  in comparison to the full g.p.  X  larger  X  i  X  X  and a smaller  X  . This leads to an effective covariance function  X  that of  X  G U ( k ) as given by (17)  X  which better matches that of the full g.p. depicted in Figure 1 (c). In addition to merely ob-serving the similarity between Figures 1 (b) and (c), we verified this last statement directly by visualising E we omit the plot due to space limitations.
 Figure 2 shows our experiments which, as in (Seeger et al., 2003) and (Snelson &amp; Ghahramani, 2006), were performed on the pumadyn-32nm and kin-40k data sets. 7 Optimising the v.s.g.p. methods from a random initialisation tended to lead to inferior local optima, so we used the s.p.g.p. to find a starting point for the optimisation. This is possible because both methods optimise the same criteria, while the s.p.g.p. merely searches a subset of the space permitted by the v.s.g.p. framework. To ensure a fair comparison, we optimised the s.p.g.p. for 4000 iterations, whereas for the v.s.g.p. we optimised first the s.p.g.p. for 2000 iterations ( i.e. fixing  X  i =  X  , i = 1 . . . m ), took the result as a starting point, and optimised the v.s.g.p. for a further 2000 iterations (with the  X  i unconstrained).
 We have also reproduced with kind permission the re-sults of Seeger et al. (Seeger et al., 2003), and hence have used exactly the experimental methodology de-scribed therein. The results we reproduce are from the info-gain and smo-bart methods. info-gain is their own method which is extremely cheap to train for a given set of hyper parameters. The method uses greedy subset selection based on a criteria which can be evaluated efficiently. smo-bart is similar but is based on a criteria which is more expensive to compute (Smola &amp; Bartlett, 2000). We also show the result of training a full g.p. on a subset of the data of size 2000 and 1024 for kin-40k and pumadyn-32nm , respectively. Neither info-gain nor smo-bart estimate the hyper-parameters, but rather fix them to the values deter-mined by optimising the marginal likelihood of the full g.p. Hence they are most directly comparable to spgp-basis and vsgp-basis . However, spgp-full and vsgp-full correspond to the more difficult task of estimating the hyper parameters at the same time as the basis. For pumadyn-32nm we do not plot spgp-basis and vsgp-basis as the results are practically identical to spgp-full and vsgp-full . This differs from (Snelson &amp; Ghahramani, 2006), where local minima problems with spgp-full on the pumadyn-32nm data set are re-ported. It is unclear why our experiments did not suf-fer in this way  X  possible explanations are the choice of initial starting point, as well as the choice of op-timisation algorithm. The results of the s.p.g.p. and v.s.g.p. methods on the pumadyn-32nm data set very similar, but both outperform the info-gain and smo-bart approaches.
 The kin-40k results are rather different. While the  X  i deviated little from  X  on the pumadyn-32nm data set, this was not the case for kin-40k , particularly for small m , as seen in Figure 2 (c) where we plot ment with those of (Snelson &amp; Ghahramani, 2006)  X  our vsgp-full outperforms spgp-full for small m , which in turn outperforms both info-gain and smo-bart . However for large m both spgp-full and vsgp-full tend to over-fit. This is to be expected due to the use of marginal likelihood optimisation, as the choice of basis U is equivalent to the choice of the order of md hyper parameters for the covariance function of  X  G
U ( k ). Happily, and somewhat surprisingly, the vsgp-full method tends not to over-fit more than the spgp-full , in spite of its having roughly twice as many basis parameters. Neither vsgp-basis nor spgp-basis suffered from over-fitting however, and while they both out-perform info-gain and smo-bart , our vsgp-basis clearly demonstrates the advantage of our new s.p.g.p. frame-work by consistently outperforming spgp-basis . Finally, to emphasise the applicability of our idea to other kernel algorithms, we provide an accompanying video which visualises the optimisation of an s.v.m. using multiscale gaussian basis functions. Sparse g.p. regression is an important topic which has received a lot of attention in recent years. Previous methods have based their computations on subsets of the data or pseudo input points. To relate this to our method, this is analogous to basing the computations on a set of basis functions of the form k ( v i , ) where k is the covariance function and the v i are for example the pseudo input points. We have generalised this for the case of Gaussian covariance function, by basing our computations on a set of Gaussian basis functions whose bandwidth parameters may vary independently. This provides a new avenue for approximations, appli-cable to all kernel based algorithms, including g.p. X  X  and the s.v.m., for example. To demonstrate the util-ity of this new degree of freedom, we have constructed sparse g.p. and k.r.r. algorithms which outperform pre-vious methods, particularly for very sparse solutions. As such, our approach yields state of the art perfor-mance as a function of prediction time.
 Csat  X o, L., &amp; Opper, M. (2002). Sparse on-line gaussian processes. Neural Comp. , 14 , 641 X 668.
 Gehler, P., &amp; Franz, M. (2006). Implicit wiener se-ries, part ii: Regularised estimation (Technical Re-port 148). Max Planck Institute for Biological Cy-bernetics.
 Lawrence, N., Seeger, M., &amp; Herbrich, R. (2002). Fast sparse gaussian process methods: The informative vector machine. Advances in Neural Information Processing Systems 15 (pp. 609 X 616).
 Qui  X nonero-Candela, J., &amp; Rasmussen, C. E. (2005). A unifying view of sparse approximate gaussian pro-cess regression. Journal of Machine Learning Re-search , 6 , 1935 X 1959.
 Roach, G. F. (1970). Green X  X  functions . Cambridge, UK: Cambridge University Press.
 Seeger, M., Williams, C., &amp; Lawrence, N. D. (2003).
Fast forward selection to speed up sparse gaussian process regression. In C. M. Bishop and B. J. Frey (Eds.), Workshop on ai and statistics 9 . Society for Artificial Intelligence and Statistics.
 Smola, A. J., &amp; Bartlett, P. L. (2000). Sparse greedy gaussian process regression. In T. K. Leen, T. G.
Dietterich and V. Tresp (Eds.), Advances in neural information processing systems 13 , 619 X 625. Cam-bridge, MA: MIT Press.
 Snelson, E., &amp; Ghahramani, Z. (2006). Sparse gaus-sian processes using pseudo-inputs. In Y. Weiss,
B. Sch  X olkopf and J. Platt (Eds.), Advances in neu-ral information processing systems 18 , 1257 X 1264. Cambridge, MA: MIT Press.
 Steinwart, I. (2002). On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research , 2 , 67 X 93.
 Walder, C., Sch  X olkopf, B., &amp; Chapelle, O. (2006). Im-plicit surface modelling with a globally regularised basis of compact support. Proc. EUROGRAPHICS ,
