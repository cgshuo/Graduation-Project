 FULL PAPER A. Rusu  X  A. Thomas  X  V. Govindaraju Abstract Automated recognition of unconstrained hand-writing continues to be a challenging research task. In contrast to the traditional role of handwriting recognition in applications such as postal automation and bank check reading, in this paper, we explore the use of handwriting recognition in designing CAPTCHAs for cyber security. CAPTCHAs ( Completely Automatic Public Turing tests to tell Computers and Humans Apart ) are automatic reverse Turing tests designed so that virtually all humans can pass the test, but state-of-the-art computer programs will fail. Machine-printed, text-based CAPTCHAs are now com-monly used to defend against bot attacks. Our focus is on exploring the generation and use of handwritten CAPT-CHAs. We have used a large repository of handwritten word images that current handwriting recognizers cannot read (even when provided with a lexicon) for this purpose and also used synthetic handwritten samples. We take advantage of both our knowledge of the common source of errors in automated handwriting recognition systems as well as the salient aspects of human reading. The simultaneous interplay of several Gestalt laws of perception and the geon theory of pattern recognition (that implies object recognition occurs by components) allows us to explore the parameters that truly separate human and machine abilities.
 Keywords Handwriting recognition  X  CAPTCHA  X  HIP 1 Introduction Interpreting handwritten text is a task humans usually perform easily and reliably. However, automating the pro-cess is difficult because it involves simultaneously recogniz-ing the symbols and comprehending the message conveyed. Although progress in optical character recognition (OCR) accuracy has been considerable, it is still inferior to that of a first-grade child [ 23 ].

A review of the handwriting recognition literature [ 3 , 10 , 12 , 15  X  17 , 19 , 22 , 26 , 28 ] shows that while some of the com-puter algorithms demonstrate human-like fluency, they fail when the images are degenerated, poorly written, or without a context. Thus, there is currently a gap between human and machine abilities in reading handwriting under noisy condi-tions which can be explored through controllable parameters that capture aspects of handwriting such as legibility, over-lapping of words, broken strokes, and the extent of overrun characters.

CAPTCHA is part of the set of protocols known as HIPs, which allows a person to authenticate as belonging to a select group, for example, human as opposed to machine, adult as opposed to child, etc. HIPs operate over a network, without theburdenofpasswords,biometrics,specialmechanicalaids, or special training [ 1 ]. Since CAPTCHAs exploit the areas where computers are not as good as humans (yet), handwrit-ten word image challenges are a strong candidate for these tests.

To the best of our knowledge, this is the first research effort in the design of Handwritten CAPTCHAs (Fig. 1 ). We believe it is important to explore this avenue of research because several machine-printed, text-based CAPTCHAs (Ez-Gimpy and Gimpy-R developed by researchers at Car-negie Melon University) have already been broken. Mori and Malik (University of California at Berkeley) report automated programs that can solve Ez-Gimpy with accu-racy of about 83%. The Cambridge vision group has reported 93% correct recognition rate on Ez-Gimpy, and a group from Aret X  Associates have reported 78% accuracy on Gimpy-R[ 6 ]. Handwritten text presents additional challenges that are rarely encountered in machine-printed text. Further, most problems faced in reading machine-printed text (for exam-ple character recognition, word segmentation, or letter seg-mentation) are exacerbated in handwritten text. Results of our experiments support our hypothesis that handwritten text images are well suited for use as CAPTCHAs. This is the basis for the research presented in this paper.

Handwriting recognition has been successfully used in several applications, such as postal address interpretation [ 29 ], bank check reading [ 11 ], and forms reading [ 18 ]. These applications are all characterized by small or fixed lexi-cons accompanied by contextual knowledge. Recognition of unconstrained handwriting is difficult because of diversity in writing styles, inconsistent spacing between words and lines, and uncertainty of the number of lines on a page as well as the number of words in a line [ 27 ]. In addition, current handwrit-ten word recognition approaches depend on the availability of a lexicon of words for matching, making the recognition accuracy dependent upon the size of the lexicon.

It must be noted that without the context of a lexicon, unconstrained cursive handwriting recognition (offline) is extremely difficult. Furthermore, the recognition accuracy drops dramatically with an increase in the lexicon size. The results in Fig. 2 [ 31 ] are based on fairly well-written clean images extracted from US mail piece images. They show the execution speed and recognition accuracy of the sys-tem. Thus, generating handwritten word images that are challenging for computers programs but relatively effortless for humans is a worthwhile avenue to pursue. One obvious approach to increasing the difficulty for computer programs would be to increase the lexicon size. However, this may not be always practical, because it would require presenting humanuserswithaverylargelexiconinachallenge-response test. This would take up large area on the computer screen making it onerous on genuine human users. In this paper, we have explored alterative ways of increasing the difficulty for automated programs. 2 Design of handwritten CAPTCHAs We have generated Handwritten CAPTCHA challenges and tested them with three state-of-the-art handwriting recog-nizers: Word Model Recognizer (WMR), Character Model Recognizer (CMR), and Accuscript (HMM) [ 9 , 13 , 31 ]. Our methodology is motivated by the Gestalt laws of perception and the geon theory. 2.1 Gestalt laws of perception  X  X estalt X  in German means  X  X hape X  which in psychology implies the idea of perception in context. It is based on the observation that we often experience things that are not part of our simple sensations [ 14 ]. What we see is believed to be an effect of the whole event, which is more than the sum of the parts (Fig. 3 ). The main idea is one of  X  X rouping X , and the concept is similar to the holistic word recognition approaches that focus on recognizing the entire word as a group [ 17 ] without breaking it into character units. The Gestalt laws of organization that relate to our work in designing robust CAPTCHAs include a subset of the following: 1. Proximity: refers to how things tend to be grouped 2. Similarity: refers to how elements that are similar tend 3. Symmetry: refers to how things are grouped into figures 4. Continuity: refers to grouping by flow of lines or by 5. Closure: refers to how elements are grouped together if 6. Familiarity: refers to how elements are more likely to 7. Figure-ground distinction: perception involves not only
For example, in Fig. 3 e, a set of dots outlining the shape of a  X  X  X  is likely to be perceived as a  X  X  X , and not as a set of dots. It is also more natural for us to see the o X  X  as a line within a field of x X  X  (Fig. 3 a). In Fig. 3 b, we are likely to see three collections of two vertical lines each as well as grouping the dots in three sets based on their proximity. Despite the law of proximity prompting us to group the brackets nearest to each other together, in Fig. 3 d, symmetry overwhelms our percep-tion and makes us see them as pairs of symmetrical brackets. We can see a line, for example, as continuing through another line, rather than stopping and starting as two angles (Fig. 3 c). The elements in an image are grouped together if we are used to seeing them together. For example, we are used to see rect-angles and squares rather than other odd shapes in Fig. 3 g. We also seem to have a tendency to perceive one aspect of an event as the figure or foreground and the other as the back-ground (Fig. 3 g). We can see two different things but not both at the same time. Also, internal metric relations play a role as part of an outside iconic memory and, therefore, we easily decode the text in Fig. 3 h X  X  X EADING UPSIDE-DOWN X . 2.2 Geon theory We have also explored the geon theory [ 4 ] of pattern rec-ognition (or recognition by components) as it provides cues on what is desirable to be preserved in image reconstruction, namely, edges and intersections .InFig. 4 , we can see the importance of junctions, crossing strokes, concavities, and convexities in recognition accuracy.

The geon theory explains how moderately occluded or degraded images and new instances of objects are success-fully recognized by the human visual system. Humans are also capable of recognizing objects from various view points, even views that have never been seen before [ 5 ] (Fig. 5 a). Objects can be recognized despite variations in size because it does not change the structure of an object (the geons and their spatial organization) (Fig. 5 b). Also, changes in position do not disrupt recognition accuracy (e.g., the book is on the desk, or letter a is next to letter b).

Geon theory researchers [ 21 ] explain word perception to be based on the following rules: 1. Words contain specific visual elements (e.g., connected 2. Visual elements are presented in orderly fashion (e.g., 3. Combination of letters follows specific rules (e.g., rules 4. Words convey meaning 2.3 Transforms based on Gestalt and geon theories SeveralexamplesofhandwrittenwordimagesonwhichOCR systems fail (but humans can recognize easily) are shown here. We have successfully designed transforms to deform these handwritten word images by using the Gestalt and geon theories. This ensures that the challenges are human readable but beyond the abilities of current handwriting recognizers (Sect. 5 ). The transforms are as follows: 1. Image transformation method: Create horizontal or ver-2. Image transformation method: Add occlusions by cir-3. Image transformation method: Add occlusions by waves 4. Image transformation method: Add occlusions using the 5. Image transformation method: Use empty letters, bro-6. Image transformation method: Split the image in parts 7. Image transformation method: Split the image in parts 8. Image transformation method: Add extra strokes 9. Image transformation method: Change word orienta-3 Generation Algorithm Here, we describe our methodology for automatic genera-tion of random and  X  X nfinitely many X  distinct handwritten CAPTCHAs.

We have used handwritten US city name images avail-able from postal applications (CEDAR CDROM, Fig. 7 ). Transformations are applied to randomly chosen handwrit-ten images from this set.

Wehavealsoconstructedhandwrittenwordimages(actual and unreal) by gluing together characters randomly chosen from a set of 20,000 handwritten character images of isolated upper and lower case alphabet letters (Fig. 8 ). Character size, height, stroke width, slope, etc., require prior normalization before concatenating to assure consistent aspect ratio for the entire word.

We have also used the handwriting distorter described in [ 25 ] for generating (potentially) an infinite number of differ-ent synthetic samples from handwritten words. Our design exploits the knowledge of the common source of errors in automated handwriting recognition systems and also takes advantageofthecognitiveaspectsofhumanreadingbyincor-porating the Gestalt laws of perception and geon theory. Fol-lowing is the algorithm: We have demonstrated our method of generating the CAPTCHAs with the described transformations cannot be easily  X  X everse engineered X  (Sect. 3.1 ) by hackers.
We have examined the sources of errors of computer recognition algorithms. Segmentation errors (over-and under-segmentation),recognitionerrors(confusionsbetween lexicon entries), and image quality are the most common.We have considered all the normalization operations that word recognizers use prior to recognition and have introduced the related distortions deliberately. Also, given our knowledge of the extent of the distortions a word recognizer can tol-erate, we are able to generate images that cannot be easily de-noised by such preprocessing algorithms.

Following transformations have been applied because cur-rent recognizers account for them in the preprocessing step. 1. Noise: Add lines, grids, arcs, circles, and background 2. Segmentation: Segmentation errors have been exploited 3. Lexicon: Use lexicons with similar entries, large lexi-4. Normalization: Createimageswithvariablestrokewidth,
It is intuitively understood that word recognition with larger lexicons is more difficult [ 9 , 13 ]. Another way to cat-egorize the difficulty of a word recognizer task is by the similarity between lexicon entries, defined as the distance between handwritten words, ( Lexicon Density ,[ 30 , 32 ]). Although the idea of generating random lexicons with higher density is expected to provide additional handwritten CAPT-CHAs, this direction of research was not pursued, since pre-liminary results raised human usability issues (Fig. 13 )
Consider the results in Fig. 2 that are based on clean, US mail piece images. We ensured that all the lexicon entries are real words, and the true word is always present in order to make a fair comparison with human ability, which relies heavily on context.
 Controlling distortions We remove features or add nontextual strokes or noise to a handwritten image in a systematic fashion based on Gestalt segmentation and grouping principles in order to pose diffi-culties for machine recognition, while preserving the overall letter legibility for human reading. 1. Create horizontal or vertical overlaps: use smaller dis-2. Add occlusions by circles, rectangles, lines, and random 3. Add occlusions by waves from left to right on the entire 4. Add occlusion using the same pixels as the foreground 5. Use empty letters, broken letters, edgy contour, and frag-6. Split the image in two parts on horizontal and displace 7. Split the images in parts, either by a vertical/horizon-8. Add occlusion using the same pixels as the foreground 9. Change word orientation even for just a few letters. Use
Figure 15 shows a set of images that have been success-fully recognized by humans but on which state-of-the-art handwriting recognizers failed (more examples have been included in Sect. 2.3 ,Fig. 6 ). We note that all the methods described here would work for machine-printed text images as well. However, the advantage of using handwriting is that most handwritten text challenges are more difficult. 3.1 Automated reversing of distortions We have developed several methods to attack the proposed handwritten CAPTCHAs and used preprocessing techniques to reverse the earlier mentioned transformations as follows. This was done by a different group of programers who were given the list of transformations applied. This is in keeping with the idea of HIPs where the algorithm for the creation of the CAPTCHAs is open public knowledge.  X  Gaps  X  Mosaic effect  X  Waves  X  Overlapping  X  Arcs / Jaws  X  Fragmentation  X  Background noise
We have run the state-of-the-art recognizers on the reverted images. However, it is practically impossible to have aprogramwhichaccountsforalltransforms.Althoughinpar-ticular instances certain preprocessing methods may seem to work, it is nearly impossible to generally and success-fully apply them on images that have been deformed at ran-dom with one (or more than one) transformation proposed in this paper. Moreover, combining more than one deformation complicates the revert process.
 4 Image complexity In addition to the errors caused by image quality, image fea-tures, segmentation, and recognition, we have also explored the influence of image complexity on handwriting recogni-tion (or how hard is it to read handwriting) and compared humans X  versus machines X  recognition rates. In [ 24 ]wehave investigated the influence of handwritten image complexity and Gestalt laws of perception on this gap.

However, in our attempt to quantify the strength of human reading abilities, we have obtained inconclusive results. In our experiments, neither image density nor perimetric com-plexity has shown to predict the efficiency of humans in handwritten word recognition ([ 24 ]). On the other hand, Gestalt and geon components have been shown to play an important role in the relative identification of characters and at the same time pose problems to machine recogni-tion.

Unlike the results reported by [ 20 ] for letter identifica-tion, in our experiments perimetric complexity does not cor-relate well with human recognition accuracy as seen on 1058 distinct handwritten sample images tested on human subjects (Fig. 19 ). However, these results support the impor-tance of other factors involved in human handwriting rec-ognition such as the role of Gestalt principles, as well as preservation of geon components such as intersections and edges, and having prior knowledge of the context. Similar noncorrelations between the perimetric complex-ity and human recognition efficiency have been reported in [ 2 ] for machine-printed word images. The researchers in [ 2 ] try to predict legibility of ScatterType challenges using features that can be automatically extracted from the images such as perimetric image complexity. However, the metric that worked well on another machine-printed CAPTCHA [ 8 ] failed to predict legibility in that case. We have found similar noncorrelations for handwritten CAPT-CHAs. 5 Handwriting-based HIP system Our HIP system has three main components: (i) the actual CAPTCHA challenge (handwritten word image) that is pre-sented to the user, (ii) user response or the answer to the challenge, and (iii) a method to validate the user response and report success or failure. The three components have been implemented and tested online (Fig. 20 ). The modules can be used independently to secure any online application. The process is entirely automatic so that it is easily deploy-able, and there is no risk of image repetition, which ensures higher security.
 We have used several sets of image files in TIFF and HIPS formats. We have generated handwritten CAPTCHAs and performed test legibility on human volunteers and the state-of-the-art handwriting recognizers available at CEDAR (WMR, CMR, and Accuscript) [ 9 , 13 , 31 ].

For each image, we have produced a deformed version by applying successive transformations according to the hand-written CAPTCHA generation algorithm in Sect. 3 .We assume that a valid lexicon is provided and that for every image, the corresponding truth word is always present. We ran tests on lexicons of size 4,000 and 40,000 (the entire list of US city names).

We have conducted several experiments on human sub-jects and machines. The handwritten CAPTCHA tests are graded pass or fail, where pass is granted when all the char-acters of the word are correctly recognized, and fail other-wise. Each set of tests was conducted at a different point in time with a different set of human subjects, so overall, we have worked with about a couple of dozen human sub-jects. Although we used about the same category of subjects in terms of their technical background and level of edu-cation, we have been able to achieve enough diversity in gender, ethnicity, and age, and also include native foreign language speakers due to a very diverse student population, even though that was not an intentional part of the study and therefore not documented.

The protocol of study involving human participants was reviewed and approved by the Social and Behavioral Sciences Institutional Review Board. A part of the partici-pants had a small monetary incentive. However, we do not have conclusive information collected to discuss on how this might have affected their motivation or not for the tests and consider that their motivation would be similar to the real one. All the tests were self-administered and were taken at a testing Web site that mimics a real HIP system using Hand-written CAPTCHA. Therefore, the look and feel is very much similar to a real situation when an user is provided with a challenge online when trying to use particular web ser-vices. Participants were given only very basic information on the concept of CAPTCHA, and no prior knowledge of the field was assumed. Users were asked to solve the handwrit-ten CAPTCHA presented and also to rate each image on a scale of 1 X 5, with 1 for the lowest difficulty. In generating the handwritten CAPTCHA, the program either selects a human-written or a synthetic generated sample and randomly apply transformations from a list of potential deformations. In all cases, the user must scan through and interpret all characters in the handwritten sample to correctly answer the question regarding which characters or word(s) they see in the image. To complete the generation of the test, the handwritten image and it truth word(s) (correct answer) are passed to the verifier. Upon challenge submittal, the user response is verified, and the application determines whether the user passes or fails. If the user passes by interpreting correctly all the letters in the handwritten image, they would then be informed that they passed the challenge and in real world scenario given access to the web service in question, otherwise they would be informed about failure to recognize the image and given another different challenge. Either way they advance to a new CAPTCHA instance until they are done with the number of samples proposed for consideration. 5.1 Various transformations on real words X  X S city names Method Procedure. The first experiment involves a database of 4,127 city name images. They are all handwritten city-words (cur-sive and hand-printed, with unconstrained writing styles) manually extracted from mail pieces. Each image contains one or two words that correspond to a U.S. city name. We have implemented an automated version of the deformation algorithm, and a number of transformations (up to three) are applied to each image. The transformations considered are adding lines, grids, arcs, background noise, applying convo-lution masks and special filters, using variable stroke width, slope, rotations, stretching, and compressing. We performed tests by running WMR and Accuscript recognizers on the same images.

We have also administered tests on 12 voluntaries (grad-uate students) in our department. The same set of 10 hand-written word images was tested on all subjects. The images were chosen randomly from images that are not recognized by our recognizers.

Results and Discussion. The corresponding accuracy rates for recognizers are shown in Table 1 .

By examining the set of recognized images, we have found that the majority of them are deformed by only one trans-formation, such as blur, spread, or wave, which makes these transformations alone inefficient. In the recognized set, there were just very few images with background noise, such as salt-and-pepper noise, and in all cases, the character image pixels were more prominent than the noisy pixels, so that they have been distinguished easier.

Generally, we observe that adding background noise is the most powerful transformation because it is easily repro-ducible, and the accuracy of the system drops significantly on noisy images. On the other hand, the extra components such as arcs, lines,and grids produce incorrect segmentation and recognition errors, thus significantly reducing the per-formance of the recognizers. The other transformations that we have considered (blur, spread, wave, median filter, etc) are efficient when applied in groups. As expected, the set of city names did not pose any problem for humans given the context (Table 2 ).

In order to utilize the lexicon level challenge, we also con-sideredafewimagesthatweresuccessfullyrecognizedbythe two word recognizers in the previous test. In that instance, a lexicon of size 10 was chosen randomly. In Fig. 11 ,weshow what happens when the lexicon (of size 10) is simulated to increase the confusion (density). In order to show the effect of this method without using image transformation, the images were not deformed. Even in this situation, the recognizers did not produce the correct results as top choice. 5.2 Various transformations on nonsense words Method Procedure. 3,000 random nonsense word images were gener-ated randomly by combination of characters, with one word per image and a random word length between 5 and 10. The characters were chosen randomly from a database of over 20,000 characters, which were previously extracted from city name images (Fig. 6 ). We have run WMR and Accuscript recognizers on these images. We have also used a subset of 100 images that recognizers cannot read correctly and tested them on human subjects.

Results and Discussion. A majority of these synthetic handwritten word images are readable by humans. However, human subjects confused the following characters:  X  X  X  vs.  X  X  X ,  X  X  X  vs.  X  X  X , and  X  X  X  vs.  X  X  X . Perhaps using real word images can help eliminate some of the errors humans have done in the case of ambiguous characters. The overall error rate of 20% for humans versus 100% for all recognizers shows the superiority of human abilities when reading hand-written text images even without the aid of context. Recog-nizers X  accuracy is presented in Table 3 . 5.3 Transformations related to Gestalt and geon principles Another set of experiments deals with the methods described in association with the Gestalt laws of perception. Method Procedure. Several new sets of 4,127 transformed images each were used, one set for each deformation method pre-viously described. We randomly chose some parameter val-ues for our transformations and successively applied them to handwritten word images.

We have run the three recognizers on the sets of images previously described and tested using lexicons with size 4,000 and 40,000. Both human and machine accuracy were computed as percentages of the entirely recognized images.
We have also used random sets of about 90 images each to be recognized by 9 voluntary students. The test consists of 10 handwritten word images for each of the 9 types of trans-formations previously described in relationship with Gestalt laws. The images were chosen at random from the set of deformed images for each transformation. The human sub-jects were relatively familiar with the words in the images since they are city names in the U.S.

Results and Discussion. The accuracy achieved by machine recognizers is presented in Table 4 for WMR, Table 5 for Accuscript, and Table 6 for CMR.

We tried various displacements of overlap in the vertical and horizontal direction. We noticed that by increasing the displacement in the horizontal direction, the error rate for machines increases, but it also poses problems for humans since visual segmentation becomes difficult.

The flip-flop transform is not relevant due to the nature of our recognizers. The accuracy for these cases is very small with our test recognizers, but we do not count these results yet since our current recognizers are not trained on these types of images. Some efficient methods based on our results are duplicate the word along the vertical axis (vertical over-laps) or add black occlusions such as waves, lines, arcs, or any stroke that can be confused with parts of a character. While computers have major difficulties in recognizing them, humans have little difficulty for such images. The Gestalt law of differentiating between background and foreground holds in this case, and humans easily connect the characters that are overlapped and are able to eliminate the background noise.
For methods that hide parts of images, we experimented with several ways of placing the occlusions (middle of image or determine the part of the image based on where the major-ity of black pixels are present) and also varying the size of occlusions (wave amplitude, wavelength, circle radius, or number of circles per image). In our tests, we were concerned with the overall results for each kind of transformation, get-ting a sense of which ones work based on the Gestalt assump-tions and humans results, and further varying the parameters for each transformation. We have considered image com-plexity (i.e., perimetric complexity and image density) as a factor that can be manipulated through the transformation parameters to achieve the maximum gap between human and machine accuracy.

Based on our results, the most efficient transformations are letter fragmentations (i.e., small and high fragmentation in Tables 4 , 5 and 6 ). The Gestalt laws of closure and continu-ity hold strongly in this case, and humans easily fill the gaps or continue the characters that are broken apart. One might expect low accuracy for handwriting recognizers when jag-ged strokes are added to the original images. Jagged strokes and arcs, as well as regularly spaced and sized graphics, or short drawings, can be misclassified as text and lead to seg-mentationfailure.Forthesplittingtransforms,weusedcutsin the middle, in the lower part and upper part of the word. Gen-erally, they have similar effect on both human and machine recognition.

Due to the randomness of some parameters in our trans-formations, we may end up with images with just small areas affected by occlusions and mostly covering parts of the back-ground. Most of the images correctly recognized by the rec-ognizers fall in this category. Through a better process of parameter selection, we can avoid most of these situations. On the other hand, the recognizers have difficulty with fairly clean images with well chosen parameters for transforma-tions (Fig. 6 ).

The tests on human subjects suggest that human perfor-mance depends on context, and prior knowledge of the word provides the greatest advantage to human readers. There-fore, memory and word familiarity (Gestalt principles) have proven to be useful cues for humans. In general, if the original handwritten sample is clean, after deformation, it does not createproblemsforhumans,butdoesformachines.However, if the original sample contains noise or is poorly written, then even the original image causes problems to both human and computer, even before deformation. We have noticed that most of the human errors come from nonsensical original images rather than difficulties with the deformations applied to those images. For occlusion by circles, we can explain the lower accuracy results using the fact that some of the occlu-sionsperhapscoveredalargepartofaletterortheentireletter. For larger space between the words that overlap in the hori-zontal direction, we can explain it by saying that it might have caused confusing words. The human results are presented in Table 7 . The gap in the ability in recognizing handwritten text between humans and computers is illustrated in Fig. 21 . 5.4 Various transformations on synthetic words Method Procedure. We have also automatically generated 300 syn-thetic handwriting samples corresponding to US city names, US states, and world wide countries, using the method described in [ 25 ], and applied various transformations to make them unreadable by automatic computer programs. We have applied noise, extra strokes, lines, grids, arcs, circles, background, occlusions, deleting ligatures, using empty let-ters, broken letters, and fragmentation, using the methods previously described. Several examples of synthetic word images, deformed or not, are presented in Fig. 22 .Wehave tested our recognizers on the set of 300 transformed syn-thetic images. We have also administered tests on human subjects and compared the human abilities in recognition of a set of handwritten US city name images available from postal applications to the set that contains 79 synthetic US city name, state, or country name images automatically gen-erated by our programs.

Results and Discussion. Similar high human accuracies in recognition for both sets have been observed which guaran-tees that synthetic handwritten images do not pose problems to the user when used online. The accuracies achieved by the state-of-the-art handwriting recognizers for the synthetic word images was as low as for the real handwritten samples, and for a set of 300 automatically generated synthetic images shown in Table 8 . 6 Conclusions We have presented a handwritten CAPTCHA-based HIP sys-tem as a security protocol for Web services and evaluated the performance of our challenge generation algorithm. The norms of CAPTCHA generation dictate that the method of generating these images must be public knowledge giving those who want to break the CAPTCHAs a fair shot. Evaluat-ing our handwritten image challenges reveals that they satisfy all the requirements to be a CAPTCHA: i) there is little risk of image repetition since the image generation is completely automated, the words, images and distortions are chosen at random; ii) the transformed images cannot be easily normal-ized or rendered noise-free by present computer programs (i.e., handwriting recognizers, OCRs), although the origi-nal handwritten images are open public knowledge; iii) the deformed images do not pose problems to humans, whereas the handwritten CAPTCHA images remain unbroken by state-of-the-art recognizers throughout our tests. Experimen-tal results on three handwritten word recognizers have shown the gap in the ability between humans and computers in handwriting recognition. We also conducted user studies and human surveys on handwritten CAPTCHAs, since human users are an important part of building a practical security system, and the analysis of the results correlates strongly with our hypothesis.

We have also administered experiments to determine how robust is our algorithm for image transformation and degradation, or how easily an image deformation can be reversed and the original image retrieved. Although the testing handwriting recognizers use general image process-ing techniques in the preprocessing phase, we have con-sidered developing more sophisticated methods to attack Handwritten CAPTCHA, using preprocessing techniques for de-noising, eliminating small degradations and gaps, line removal, etc. Experimental studies on the effect of Gestalt laws-based transformation on words have been conducted on both humans and computers. The experiments show signifi-cant benefits in using handwritten CAPTCHA, as opposed to less-efficient machine-printed CAPTCHAs, and impractical CAPTCHAs based on facial features or images of objects.
Experimental results support the importance of cognitive factors involved in human visual recognition, so we explain the results by identifying the role of Gestalt principles, geon theory and prior knowledge of the context. Humans have an innate ability to recognize writing in any form, whether it is machine-printed text or handwritten, also to distinguish between text and graphics. However, this cannot be said about machine recognition. There are many reasons why machine recognition of handwriting is more difficult than machine-printed text, for instance, segmentation problems, character confusion, unconstrained writing styles, and incon-sistent space between letters and words, etc. Early testing results show that humans are easily able to solve our hand-written CAPTCHAs and regard them as a viable alternative to machine-printed text CAPTCHAs. Given the success of our empirical study conducted on humans and machines rec-ognition and comparing with other CAPTCHA approaches, in particular machine-printed CAPTCHA, we can conclude that the handwritten HIP system that we propose is arguably more efficient than the currently used HIPs for cyber security applications. We believe that additional user feedback and image recognition tools studies will offer even more insight and possible enhancements. There are various extensions to our approach that might be used to create new CAPTCHAs useful in preventing the abuse of web services in cyberspace by automated programs. Combining handwritten text images with images of objects is another possible extension for the CAPTCHAs we have described here. For all these reasons, we believe that our handwritten CAPTCHAs are a valuable contribution both as secure and usable CAPTCHAs to pro-tect cyberspace and for the insights they provide in areas such as Image Analysis and Recognition, Artificial Intelligence, Cognitive Science, Web Security and others.
 References
