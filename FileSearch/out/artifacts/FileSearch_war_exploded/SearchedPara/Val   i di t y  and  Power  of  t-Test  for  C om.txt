 We examine the validity and power of the t-test, Wilcoxon test, and sign test in determining whether or not the dif-ference in performance between two IR systems is signifi-cant. Empirical tests conducted on subsets of the TREC 2004 Robust Retrieval collection indicate that the p -values computed by these tests for the difference in mean average precision (MAP) between two systems are very accurate for a wide range of sample sizes and significance estimates. Sim-ilarly, these tests have good power, with the t-test proving superior overall. The t-test is also valid for comparing geo-metric mean average precision (GMAP), exhibiting slightly superior accuracy and slightly inferior power than for MAP comparison.
 H.3.4 [ Information Search and Retrieval ]: Systems and Software  X  performance evaluation Experimentation, Measurement significance test, validity, statistical power
The most commonly reported measure for TREC experi-ments is Mean Average Precision (MAP), the mean of Av-erage Precision (AP) scores achieved by a particular system for a set of different information needs (topics). The aptness with which MAP characterizes the intended purpose of IR systems is debatable; however, in our experiments we shall assume that MAP (or a similar measure, GMAP [1]) aptly reflects differences in system effectiveness, and consider only the validity and power of statistical tests for the significance of the difference in MAP (or GMAP) between two systems.
Although they are based on questionable assumptions [3], the commonest tests used in comparing MAP are the paired t-test, Wilcoxon signed-rank test, and the sign test. The choice of test is typically based on an uncalibrated tradeoff between validity and power; the assumption being that the t-test is the least valid but the most powerful, the sign test derived by splitting the set of topics used in a larger ex-periment  X  the TREC 2004 robust retrieval track. We use each statistical test to predict d , the probability of a dis-cordant result between the split samples. Note that d&gt;p , as d accounts for the sampling error from both splits; p for only one. Over many predictions, the expected number of discordant results is simply the sum of the d values, and, if the test is valid the observed number should be close to this value, invariant when stratified by factors such as the value of p or the magnitude of M A  X  M B (contrary to the apparent results of some previous tests [2]).

It is common practice to deem significant an experimen-tal result with p&lt; X  for some fixed threshold  X  (typically  X  =0 . 05). The power of an experimental design is the prob-ability that it will compute a true result with p&lt; X  .Fora valid test, power may be estimated empirically by simulat-ing several experiments and measuring the proportion that yield a correct significant result.

The validity of p should be independent of sample size, magnitude of the difference between the results being com-pared, and so on. Power, on the other hand, depends di-rectly on both. A larger sample will in general result in lower p -values, and hence increase power. Experimental de-sign must optimize the tradeoff between power and the cost of conducting larger experiments.
The TREC 2004 Robust Track evaluated 110 systems on 249 topics. For each pair of systems we constructed several random equal splits with 124 topics per split, and applied three statistical tests  X  paired t-test, Wilcoxon signed-rank test, and sign test  X  to one of the splits. Using the test we computed both p and d . We summed the values of d , stratified by p , and also counted the number of discordant results between the two splits. The results for the t-test are presented in figure 1 and table 1. Of the 47080 t-tests, 32275 (68.6%) yielded p&lt;. 01. Of these tests, predicted and actual discordances totalled 166 and 193, a difference of 14%. Other strata of p contained fewer tests and resulted in smaller errors. The RMS error over all strata was 8.5%. We take this low value to validate the t-test. Power depends on
