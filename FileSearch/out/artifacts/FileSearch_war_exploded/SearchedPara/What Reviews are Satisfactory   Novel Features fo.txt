 This paper focuses on exploring the features of product reviews that satisfy users, by which to improve the automatic helpfulness voting for the reviews on commercial websites. Compared to the previous work, which single-minde dly adopts the textual features to assess the review helpfulness, we propose that user preferences are more explicit clues to infer the opinions of users on the review helpfulness. By using the user-preference based features, we firstly implement a binary helpfulness based review classification system to divide helpful reviews and useless, and on the basis, we secondly build a Ranking SVM based automatic helpfulness voting system (AHV) which ra nk reviews based on their helpfulness. Experiments used a large scale dataset containing over 34,266 reviews on 1289 products to test the systems, which achieves promising performances with accuracy of up to 0.72 and NDCG@10 of 0.25, and at least 9% accuracy improvement compared to the textual-feature based helpfulness assessment. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  Linguistic processing .
 Automatic helpfulness voting, user preference, helpfulness classification. Nowadays, product reviews have become new attention-getting parts of most commercial sites [1] . In contrast to gorgeous picture or brilliant tagline in advertisement, users prefer to accept the opinions of other users, especial ly the experienced ones. And the user-supplied review is exactly the best medium for this interaction. However, the free network environment permits anyone to submit any review, even meaningless reply (e.g. yup, haw-haw, etc.) or disparaging remarks about competitors. Rather than enhance user experience, these reviews actually make so many noises and misapprehensions. Thus, it is crucial to have a mechanism capable of assessing the quality of reviews and shielding users from useless reviews. Some websites already offer the mechanism for users to evaluate reviews. For example, on Amazon.com, an interface allows customers to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes. From an Amazon annotation of helpfulness:  X  67 of 68 people found the following review helpful  X  (Each Amazon review has this type of helpfulness annotation at the beginni ng part.). It specifies that 68 users voted and 67 of them thought the review is helpful. Such a voting rate (e.g. 67/68) gives a qua ntitative evaluation of review helpfulness, which can directly help other users select and scan high-quality reviews. Unfortunately, the helpfulness is normally estimated by having users manually assess it. Thus it is hard to properly annotate helpfulness of newly submitted reviews and reviews with few votes as lack of manual information for the assessment. For example, for all Book products on Amazon.com, 26% reviews receive three or fewer helpfulness votes. Worse still, most websites never provide interactiv e function to acquire user experiences of reviews. Therefore, it is necessary to have a mechanism of automatic helpfulness voting. Although few, there are still some pilot studies on the automatic helpfulness voting (abbr. AHV) for commercial reviews. Most of these works regard the AHV as an issue of textua l-feature based classification, and verify the contributions of variety of textual features on distinguishing the helpful reviews from the useless. For example, the length of review is believed to be a simple but effective feature because a long review may offer wealth of product information. However, AHV should be a more complex problem, which not only considers textual features (e.g. text length, language fluency and clearness), but also user preferences. As we have observed, whether a review gives the product attributes that users prefer to know, whether it wins the trust of users, and whether it has the consistent sentiments with users, all are important factors that influence the helpfulness voting. Consider the following reviews on Sony camera (from Amazon.com): (1) I prefer to buy the old version or Nikon . ( 3/54 ) (2) It costs more than Nikon . ( 89/129 ) (3) Amazing. This version offers 20x optical zoom. I even can see the sweat pores of my friends in photos . ( 125/158 ) Obviously, the review (1) cannot offer any valuable product information. This causes a very low voting rate 3/54 (6%). On the contrary, the reviews (2) and (3) show the price comparison and * Corresponding author: Guodong Zhou unique function, which are normally uppermost in most minds. This should be the reason why they achieve very high voting rates 89/129 (69%) and 125/158 (79%). Especially, the review (3) implies that the reviewer might have bought the product, which makes the words more persuasive and helpful. The examples show the textual features might be not always effective in assessing review helpfulness, e.g. the length of review (1) versus (2), but user preference learning seems to give a supplementary or even alternative method to solve the problem. In this paper, we focus on learning three kinds of user preferences:  X  Users prefer the reviews that meet their information needs  X  Users prefer the credible reviews  X  Users prefer the reviews that have the mainstreaming opinion On the basis, we attempt to capture features that to some extent represent the preferences without any human intervention, and well use the features to improve the helpfulness-based review classification. Empirically, the textual features, e.g. the grammatical correctness, language fluency and clearness, etc., are more commonly used to detect whether the writings of reviews can make comfortable reading for users. Compared to this, our features are used to detect whether reviews can provide valuable, credible and authoritative produ ct information for users. The rest of the paper is organized as follows: Section 2 introduces the related work. In Section 3, we define the task of automatic helpfulness voting (AHV). In Sec tion 4, we present the user preferences on product review and o ffer the quantitative evidences to support the preference-based AHV. In Section 5, we present the method of feature extraction and corresponding measurement. Section 6 gives the experimental setup. The main results are reported in Section 7. At last, we draw the conclusion in Section 8. A large body of research shows the availability of product reviews in extracting product aspects [2][3] and features [4] opinions [2][4] and summarizing corresponding sentiments However, there is so far a little research focusing on whether and how a product review should be determined to be helpful or useless (viz., reviews helpfulness assessment, abbr., RHA). This section specially introduces the related work on RHA. Kim et al [1] exploited the multitude of user-rated reviews on Amazon.com, and trained an SVM regression system to learn a helpfulness function and then applied it to rank unlabeled reviews. Especially this work performed a detailed analysis of different features to study the importance of several feature classes in capturing helpfulness. Within the features, the length of reviews, unigrams, and product ratings were the most useful features, but structural features (other than length) and syntactic features had no significant impact. Liu and Cao et al [5] studied the problem of detecting low-quality product reviews. This work firstly discovered three types of biases in the ground-truth used extensively in the previous work, and proposed a specification on the quality of product reviews. The three biases are imbalance vote bias , winner circle bias, and early bird bias. Secondly, rooting on the new ground-truth (conforming to the proposed specification), this work gave a classification-based approach to low-quality product review detection, which yields better performance of opinion summarization. Jindal et al [6] regarded the issue as spam filtering, and gave three general types of spam reviews: 1) untruthful reviews, such as fake reviews 2) reviews on brands on ly which do not comment on the products but only the brands, the manufacturers or the sellers of the products and 3) non-reviews (e .g., questions, answers, and random texts). On the basis, this work detected reviews of type 2 and type 3 based on traditional classification learning using manually labeled spam and non-spa m reviews, and detected type 1 by verifying whether reviews involve many opinions opposing to most other reviewers. Cristian et al [7] confirmed that a review  X  X  perceived helpfulness depends not just on its content, bu t also the relation of its score to other scores. This dependence on the score contrasts with a number of theories from sociology and social psychology, but is consistent with a simple and natu ral model of individual bias in the presence of a mixture of opinion distributions. Tsur et al [8] present a RevRank algorithm to rank book reviews according to review helpfulness. The RevRank is more like an adaptive Rocchio [9] algorithm when setting and modifying the  X  X irtual core X  of a set of revi ews on a product. On the basis, reviews are ranked according to their distance from the  X  X ore X . Although RevRank is proved to outperform a baseline imitating the Amazon user vote review ranki ng system, few evidences have be showed to illustrate the inevitability of the uselessness of those reviews which deviate from the  X  X ore X . Liu and Huang et al [10] et al show that the helpfulness of a review depends on three important factors: the reviewer X  X  expertise, the writing style of the review, and the timeliness of the review. Although not clearly mentioned by Liu and Huang et al, the factors indeed reflect that the exhilarating reviews normally cater to users X  tastes in the review quality (e.g. a new and well-written review from an expert is welcomed). This motivates us to explore the user-preference based review helpfulness prediction, and further search the features effectively describing user preferences. Jo et al [11] propose an aspect and sen timent unification model to automatically discover what aspects are evaluated in reviews and how sentiments for different aspects are expressed. This work motivates us to consider whether sentiment similarity between users and reviewers affects the user preferences on reviews, and further is an effective feature to infer the determination of users for review helpfulness. In this Section, we firstly define the task of automatic helpfulness voting (AHV), and secondly we introduce the data format of Amazon.com reviews. Th e data have been successfully used in previous AHV tests, so we in later Sections conduct all our analysis, illustrations and expe riments on this type of data. The task of AHV firstly aims to automatically assess the review helpfulness, and secondly rank all reviews of a specific product based on their helpfulness scores . It should be performed only depending on the existing Web in formation without any manual intervention. For example, AHV should determine an Amazon review as helpful and highly rank it if it has the manual voting rate 1 of  X  197 of 199  X , and AHV should perform this under the condition of being blind to the rate. On the contrary, an Amazon review which has only  X  16 of 199  X  voting rate should be determined as useless and ranked lower in the ranking list of reviews. The voting rate cannot be used in automatic helpfulness voting (AHV) because it involves the manual intervention. Therefore, given an input of a random sequence of reviews, the AHV system should output a helpfu lness-based ranking list of reviews. Within this, the most key issue is to automatically assess the review helpfulness, so in this paper we firstly simplify the task of AHV, and regard it as a helpfulness-based binary classification of reviews, by which to explore the valuable features for the helpfulness assessment. On the basis, by using a Ranking SVM model [12] , we finally implement a simple ranking system for product reviews. To evaluate the system, we need two basic evaluation metrics: accuracy and NDCG@n. The accuracy is used to evaluate the performance of the helpfulness-based review classification and inspect whether every review (including both helpful and useless ones) obtain the correct decision [13] . To do this, we succeed the optimal helpfulness boundary proposed by Kim et al [1] who report the voting rate 0.6 can be used to approximately divide the helpful reviews from the useless. So we regard the boundary as the criterion to label the training and test samples for the binary classifier. The metric NDCG@n is used to evaluate the ranking system, and inspect whether its output meets the manually labeled helpfulness ranks. 
Review &lt;serial number&gt; &lt;Product&gt; Nikon COOLPIX P300 &lt;/Product&gt; ---Available &lt;Title&gt; Low Resolution &lt;/Title&gt; ---Available &lt;Star&gt; one-star &lt;/Star&gt; ---NA &lt;Time&gt; 2011-3-4 &lt;/Time&gt; ---Available &lt;Reviewer&gt; Unicorn John &lt;/Reviewer&gt; ---Available &lt;Content&gt; The photos are not clear. &lt;/Content&gt; ---Available &lt;Vote&gt; 56/78 &lt;/Vote&gt; ---NA &lt;Buyer?&gt; Yes &lt;/Buyer?&gt; ---NA &lt;Er&gt; The COOLPIX P300 is ... &lt;/Er&gt; ---NA &lt;Pt&gt; 9 X 7 X 6 inches ... &lt;/Pt&gt; ---NA &lt;Ps&gt; From the manufacturer ... &lt;/Ps&gt; ---NA &lt;Url&gt; http://www.amazon ... &lt;/Url&gt; ---Available In this paper, we adopt reviews on Amazon.com as our corpus to evaluate the AHV system. Amazon.com 2 provides 1,692,256 reviews on 13 types (91 subtypes) of products, such as that on Books, Kindle, Electronics, etc. An Amazon.com review normally includes following components: title, star, release time, reviewer, main content and vote. But for most reviews, especially the most recent ones, the item of &lt;vote&gt; is empty for lack of voters, which is the problem that should be solved by the AHV system. We standardized the format of Amazon reviews and restricted the availability of the components as the example in Table 1. denotes the editorial review , product detail and product description. The item &lt;E r&gt; is written by domain experts. The item &lt;Pt&gt; normally gives the specificat ion, bestsellers rank, etc. And the item &lt;Ps&gt; often introduces the product functions, business analysis, and manufacturer. Bu t not all Amazon merchandisers provide the contents of the three items, and particularly most commercial websites never involve th at in reviews. Therefore, to meet the needs of the general AHV system, we restricted the use and the &lt;Buyer?&gt; records whether the reviewer is a buyer of the http://www.amazon.com/ product. Because the two items need human intervention, we also restrict the use of them in AHV. Actually the  X  X vailable X  items in Table 1 may be all effective features to assess the helpfulness, but some of them cannot support our exploration on user-preference based AHV. For instance, the &lt;Reviewer&gt; provides a link to the historical reviews of a reviewer, and the corresponding voting rates can provide important reference for assessing current reviews (motivated by the hypothesis that an experienced reviewer may always give helpful reviews). But this doesn X  X  belong to the application of user preferences in AHV, so we i gnored the items in our system. In this paper, we only regard the items &lt;Product&gt; and &lt;Content&gt; as the available local resources for AHV. The &lt;Product&gt; gives the product name, and the &lt;Content&gt; r ecords the main contents of reviews. Other items, as discusse d in the next Section, are only used to generate the quantitative evidences for our user-preference based AHV. In this Section, we present the user preferences on product reviews along with the quantitative evidences for their availability in assessing review helpfulness. The hypothesis is that the basic user intention is to acquire the product information from reviews, by which to support the purchase decisions. If the hypothesis is true, a helpful review should have the prerequisite that it can meet the information needs. But the question is what the information needs are? By analyzing the interactive model in commercial websites, it is not hard to find the user behaviors of searching product attributes and functions before purchase. Therefore, we in this paper regard the product attribute and function as two basic information needs, and verify the possibility of using the needs fulfillment to improve review helpfulness assessment. P1. Unlimited Instant Videos P2. MP3s &amp; Cloud Player P3. Amazon Cloud Drive P4. Kindle P5. Appstore for Android P6. Digital Games &amp; Software P7. Audible Audiobooks P8. Books To prove the feasibility of the needs fulfillment in AHV, we use to calculate the capture rate of the words of product attributes and functions in the &lt;Content&gt; of re views, and generate the curve diagram of the average rates on 16 types of Amazon products (see the types in Table 2) with up to 20,000 corresponding reviews, by which to compare the distributions of needs fulfillment between the helpful and useless reviews. Th ereinto, the item &lt;Pt&gt; provides product functions. Besides, because the item &lt;Er&gt; gives the review from the domain expert, we re gard the capture rate in &lt;Er&gt; as the best measure of needs fulf illment. We show the curves and the equation of capture rate in Figure 1. the contrary, the curve of useless reviews is on the bottom of the helpful reviews, just like the specialist reviews, normally have the capacity to meet the user needs for product information (here only the product attributes and functi ons). In other words, the needs fulfillment is an important factor for helpfulness assessment. And the capture rate, as the measurement of the needs fulfillment, is a useful feature to divide helpful reviews from useless. But when using the feature in our AHV system, we wi ll experience the problem of extracting the feature without any help from the Section 5.1. High (uncertainty) Medium Low May, Maybe Can, Could Will, Would Should, Ought to Need, Need to The hypothesis is that users prefer to vote for the reliable reviews because the reliability of information normally affects the correctness of purchase decision. In this Section, we focus on introducing two evidences of reliability based helpfulness voting: Volitive and Tense.  X  Volitive auxiliary based reliability determination At first, we think the volitiv e auxiliaries (along with the corresponding collocations) can give the evidence to prove the hypothesis. This is because the au xiliaries potentially represent the uncertainty (see Table 3) which can, to some extent, reflect the unreliability of reviews. C onsider the reviews below: (1) I prefer to buy Sony, for it may have higher resolution . ( 9/68 ) (2) It costs more than Nikon . ( 89/129 ) Compared to the review (2), the review (1) looks unreliable because without any definite information. Correspondingly, the review (1) receives a low voting rate. This shows the possibility of using the volitive auxiliary to prove the hypothesis (viz., reliability based helpfulness assessment). Therefore, we regard any sentence, which involves at leas t one volitive auxiliary, as an uncertainty, and use the number of uncertainties in a review to calculate the reliability score (see the equation in Figure 2). By calculating the average score for each type of Amazon product, we generated the curve diagram of average reliability scores on the 16 types of Amazon products with the corresponding 20,000 reviews, by which to compare the distribution of reliability scores between helpful and useless reviews. We show the curves in sub-figure (1) of Figure 2. Unfortunately, there is no clear discrimination between the curves . This is because the volitive auxiliaries normally have multiple pragmatic functions and senses (see the examples below). (1) If asked, the salespersons have ( volitive auxiliary ) said that they had ( notional word ) the detailed specification . (Progmatics) (2) They can (means  X  may  X ) come here to see this can (means  X  container  X ) of moldy bacon ? It is impossible . (Sense) Therefore it is not a proper meas ure to use the number of volitive auxiliaries to calculate the reliability score without considering pragmatics identification and word sense disambiguation. By simply using the syntactic structure (Stanford Parser [14] of-speech [15] to filter the notional words in the count of uncertainties, we improve the di scrimination between the curves (see sub-figure 2 of Figure 2). Although a little weak, the volitive auxiliary indeed gives the evidence for the availability of information reliability in helpfulness assessment. But when using the feature in our AHV system, we still need to consider the influences of different levels of uncertainty in the measure of reliability. We will discuss this in Section 5.2.  X  Tense based reliability determination Secondly, we think the tenses (onl y past and perfect tenses here) can also give the evidence to prove the hypothesis of reliability based helpfulness voting. This is motivated by the psychology of users normally trusting the experienced reviewers and the habit of the reviewers often using past and perfect tenses in their writing. Actually the habit is reasonable because an experienced reviewer should firstly become an owner of product (normally the buyer in commercial websites) and so the corresponding opinions in reviews were mostly generated from the past experience. The direct evidence is the frequent occurrence of high voting rate in the reviews of buyers. By using th e item &lt;Buyer?&gt; (see Table 1), we calculated the proportion of buyers in the reviewers. According to the calculation on 20,000 reviews, approximately 71.29% helpful reviews are written by the reviewers who claimed they were buyers (the item &lt;B uyer?&gt; ==  X  X es X ), and only 39.05% useless reviews by visitors ( &lt;Buyer?&gt; ==  X  X o X  ). Figure 3. Tense Rate (only past and perfect tenses here) in On the basis, we calculated the rate of past and perfect tenses occurring in verbs for each review. And by calculating the average rate for each type of Amazon product, we generated the curve diagram of the average rates on the 16 Amazon products. The diagram shows that the curve of helpful reviews is easily distinguished from the useless (see Figure 3). Therefore, the tenses also give the evidence for the availability of information reliability in helpfulness assessment. As discussed in Section 3.2, we restrict the use of the item &lt;Buyer?&gt; to shield our AHV system from human intervention. So only the tenses, which can be obtained from the item &lt;content&gt;, will be used to measure the information reliability. The hypothesis is that users prefer to vote for the reviews which are compatible with the mainstreaming opinions. In part, this is because mainstreaming opinion is normally authoritative and so more convincing. The direct evidence to suppor t the hypothesis is the low divergence between the star of helpful review and mainstreaming star (based on the data on Amazon.com). Here, the star (see Table 1) is a quantitative measure of product value, e.g. a five-star means  X  X reat value X  but a one-s tar means  X  X orthless X . And the mainstreaming star is the star which occurs the most frequently in the reviews of specific product. On Amazon.com, a star is given by a reviewer through the interactio n interface. Therefore, the star directly represents the opinion of reviewer on product value, and mainstreaming opinion. However, the use of the item &lt;Star&gt; is also restricted in our AHV system (see Table 1). The only way is to use the available item &lt;content&gt; to automatically dete ct the mainstreaming opinion and measure the divergence. The alternative for this is to adopt sentiment analysis to obtain a two-dimension opinion (positive polarity and negative polarity), a nd calculate the divergence based on the distribution of sentiment wo rds in &lt;content&gt; of reviews. We will discuss this in Section 3.3. In this paper, we build an automatic helpfulness voting system for the reviews in common commercial websites. The system involves two functions. One is the helpfulness based review classification, and the other is review ranking. Thereinto, the classifier determines whether a review is helpful or useless, and the ranking system generates a ra nking list of reviews based on their helpfulness scores. Because we directly use a current Ranking SVM model [12] to implement the review ranking, the remaining problem is only how to ex tract effective features for the classification. In th is Section, we focus on introducing the extraction methods of our user-preference based features. As discussed in Section 4.1, the needs fulfillment is an important factor for helpfulness assessment, and the capture rate of product attribute and function is an effective measure of needs fulfillment. Therefore we use the rate as a feature in the classification, by which to support our AHV system. Table 4: A type example of ps eudo-feedback of Search Boss The key issue of calculating the capture rate is to know what attributes and functions a product has. However, because the items &lt;Content&gt; can be used), it is difficult to obtain the attributes and functions locally. An altern ative way is to mine global information about product by using search engine. In this paper, we used the Yahoo! X  X  open search web services platform, named Search Boss 3 , to mine the information. The Yahoo! Search Boss normally provides the keywords of pseudo-relevant feedbacks to a query, and if when the query is a product name, the keywords often contain plenty of product attributes and functions (see Table 4). Thus, according to the item &lt;Product&gt;, which denotes the product name, we can use the Search Boss to obtain the attributes and functions easily. For each product, we use N fbk top-ranked feedbacks as the global resources for the feature extraction. To obtain the optimal N randomly selected 20,000 Amazon products, and use their names as query to retrieve corresponding keywords (viz., the product attributes and functions) on the Yahoo Search Boss. And then we calculated the average overlapping rate of the keywords in editorial review (offered by the &lt;Er&gt; item) at every possible N (see Figure 4) and the average number of keywords increasing along with N fbk (see Figure 5). Under the hypothesis that the editorial reviews (from domain experts) have enough product attributes and functions, by analyzing the trend curves of overlapping rate and keyword X  X  number, we set the optimal number of N fbk as 20 which just occurs at the first degression of the overlapping trend (see Figure 4) and that of the number of keywords. This illustrates that if we use more than 20 feedbacks in the keywords. http://developer.yahoo.com/search/boss/ Figure 4. Overlapping trend Figure 5. Increasing trend Besides, the capture rate, as calculated in Section 4.1 (see the equation in Figure 1), neglects the distributions of product attributes and functions in global information resource. This will cause that some common attributes and functions generalize the capture-rate based measure of needs fulfillment. Therefore, according the distributions, we improve the calculation of capture rate by involving a combination weight. For an attribute or function, the weight is calculated as: where,  X  is a discount factor, IG means the mutual information of the attribute or function in training corpus, w weight in keywords. The w glb is calculated as: where, N glb denotes the number of attributes or functions in global resource (viz., the top N fbk pseudo-feedback of specific product), df denotes the number of pseudo -feedback that involving the attribute or function. On the basis, for a review, we calculate the improved capture rate as follows: where L denotes the length of review (viz., the number of words in review), R cap denotes the original capture rate (see the equation in Figure 1). In our AHV system, we use the improved capture rate as the measure of needs fulfillment. Besides, we expanded the words of product attribute and functions by using WordNet 4 , by which to enhance the possibility of attribute and function word matching. And we also used the OpenNLP parser tool 5 to extract phrases from the item &lt;content&gt; of review. As discussed in Section 4.2, the information reliability is also an effective feature for helpfulne ss assessment. And both volition based uncertainty factor and tense based experience factor (past and perfect tenses only) can be used as the measures of reliability. In this Section, we respectivel y show the methods of calculating the factors.  X  Volition based uncertainty factor To calculate the factor, the first problem is to obtain enough priori volitive auxiliaries, by which to identify the volitive auxiliaries in http://wordnet.princeton.edu/ http://incubator.apache.org/opennlp content of review. Here, we adopt a cross-language collaborative mining algorithm [16] to obtain the volitive auxiliaries. The algorithm firstly use several English volitive auxiliaries as seeds, and by using a machine translation tool [17] translate the seeds into Chinese; secondly, because Chinese words can be divided into characters, and the character in Chinese volitive auxiliaries normally can combine with other characters to generate new volitive auxiliaries, the algorithm expends the Chinese auxiliaries in this way; at last it translate the new Chinese auxiliaries into English and add them into the list of seeds. The algorithm iteratively executes the operations until the number of English volitive auxiliaries never increases. After checking the auxiliaries manually, we obtained a total of 156 English volitive auxiliaries (including words and phrases). We use the volition based uncertain ty factor as the measure of information reliability, by whic h to support reliability based helpfulness assessment. The original uncertainty factor (see the equation in Figure 2) is calculat e by the rate of sentences which involve at least one volitive auxiliary, within which syntactic structure (Stanford Parser [14] ) and part-of-speech [15] avoid the cases of multiple pragmatic functions and ambiguous sense. However, the original uncertainty factor neglects the influence of different uncertainty degrees of volitive auxiliaries. To improve the factor, we roughly divide volitive auxiliaries into three levels of uncertainty degree (see Table 3), and set the levels as the weighting coefficients: high uncertainty corresponds to 3, medium to 2 and low to 1, by which the improved uncertainty factor can be calculated as: where N s denote the total number of sentences in a review, n denotes the number of sentences which involve at least one volitive auxiliaries, u max is the maximum uncertainty degree occurring in a sentence, tf v denotes frequency of the verb adjacent to volitive auxiliary. By using tf v , the uncertainty factor has the capacity of determining whether the uncertainty occurs at an event that a review emphasizes on (As the definition of ACE event normally is triggered by verb).  X  Tense based experience factor As discussed in Section 4.2, the experience factor is measured by the rate of past and perfect tens es occurring in verbs because the tenses, to some extent, reflect th e possibility of reviewers having corresponding experiences to speci fic product. However, not all cases have the positive influence for the experience measurement. For example, the review  X  I want to buy the camera because my wife said she liked its color  X  has two past tenses, but only the verb  X  X iked X  can be regarded as the clue of usage experience (because  X  X olor X  is normally the first user experience). Therefore, to improve the tense based experience factor, we only extract the verbs link to product attributes or functions in syntactic structure. Besides, we involve the frequencies of attributes and functions into the calculation of experience factor: where N v denotes the total number of verbs in a review, n denotes the number of verbs link to attri butes or functions in syntactic structure, and tf p denotes the frequency of product attribute or function in review. As discussed in Section 4.3, th e divergence from mainstreaming opinion can be used to assess the helpfulness of review. Here, we propose a sentiment based diverg ence measurement, which only needs to use the item &lt;content&gt; of review. To calculate the sentiment based divergence, the first issue is to obtain sentiment words. For this, we extract the sentiment words from WordNet Affect [19] , which correspond to the WordNet synsets annotated with the six emo tions: anger, disgust, fear, joy, sadness, surprise. Secondly, we roughly divided the words into two classes: positive polarity and negative polarity. Thereinto, positive-polarity class involves the words of joy, and negative-polarity class involves the words of anger, disgust, fear and sadness. But the surprise, as neutral emotion, is filtered. On the basis, we estimate the polarity of a review as: where n pos denotes the number of wo rds of positive polarity, n denotes the number of words of negative polarity, L denotes the length of review,  X  is the threshold for positive polarity determination. By using Benchmark Corpus [20] to train the thresholds, we got their optimal values as:  X  equals to 0.02 and  X   X  equals to 0.015. Within the equation (6), we calculate the quantitative polarity force as: After determining the polarity fo r each review, we detect the mainstreaming opinion for each product. Here, the mainstreaming opinion is either positive polarity or negative polarity. For each product, we respectivel y count the number of positive reviews and negative reviews, and calculate average polarity force for the two classes of reviews. And then we regard the polarity of the class which involves more reviews, as the mainstreaming opinion, and use the corresponding average polarity force as the force of mainstreaming opinion. On the basis, for each review, we calculate its divergence from mainstreaming opinion by measuring the difference between its polarity force and that of mainstreaming opinion. In this Section, we firstly introduce the dataset for our experiments, secondly we give the evaluation metrics, and at last we show the AHV systems to be tested. We focused our experiments on 124,878 reviews associated with Amazon products from the Multi-Domain Sentiment Dataset The dataset collected most pro ducts and reviews released on Amazon.com in 2006. http://www.cs.jhu.edu/~mdr edze/datasets/sentiment/ In most commercial websites like Amazon.com, there are a large number of duplicate reviews, which often negatively influence machine learning algorithms. For this, we use a simple deduplication method to filter th e redundant reviews. The method matches bigrams between each pair of reviews. And a pair of reviews is deemed dupli cated if they have more than 80% bigram matching. Besides the Amazon.co m also have many duplicated products, such as the products can come in black or white models, and so reviews on such product ar e always duplicated. We filtered out the products whose all reviews are detected to be duplicates. The filtering process discarded a total of 3,404 products and 90,612 reviews. And at last we filtered out the reviews which never received any vote. The final dataset involves 34,26 6 reviews on 1289 products. The types of products and the number of reviews in each type are shown in Table 5. We followed Kim et al [1] to set the voting rate 0.6 as the boundary between he lpful and useless reviews and labeled the class (helpful or usel ess) of reviews beforehand. On the basis, we tested our helpfu lness classification system on the whole dataset. For testing AHV systems, we labe led helpfulness ranking for the reviews on the products of Digi tal Games, Audiobooks, Clothing and Sports (3,311 reviews on 116 products ). In evaluation process, we run 5-fold cross validation and each fold use 80% products along with their reviews as training se t and the rest as test set. We didn X  X  adopt original Amazon ranking because Amazon ranks the reviews according to the timeliness but not the helpfulness. In our experiments, we used accu racy to evaluate the performance of the helpfulness based review classification, and use the NDCG metric to evaluate the pe rformance of our AHV system. Here, the accuracy is the rate of the reviews whose helpfulness (helpful or useless) is correctly determined. NDCG@n is widely used to evaluate the performance of pseudo-feedback ranking in the field of information retrieval. In this paper, we translate it into an evaluation of review ranking system. NDCG@n [21] normalized discount cumulative gain , which can take into account the influence of rank to accuracy. NDCG at rank n is calculated as: where i is the rank in the ranking list of reviews, Z normalizing factor and chosen so that for the perfect list DCG at each rank 1, and r(u i ) equals 1 when u i is a review whose helpfulness is correctly determined , else 0. In our experiments, based on the given voting rate by Amazon.com, we obtain the correct ranking list of reviews, and on the other side, AHV systems output the ranking list of reviews based on helpfulness. Thus, we can use the NDCG@n to evaluate their performances. In our experiments, we totally built 10 systems: four helpfulness based review classification system, four AHV systems and two improved systems. The classification systems include:  X  System1 is a baseline classificat ion system which roughly regards all reviews in corpus as helpful reviews. Because the average priori probability of helpful review occurring (in the 5 test set) is approximately 0.55, thus the accuracy of the baseline is 0.55.  X  System2 follows the textual features based helpfulness assessment of Kim et al [1] . The system uses the LIBSVM 7
RBF kernel function (the rest pa rameters are default) as the classifier, and uses length, unigram and star as the features for classification.  X  System3 follows the work of Liu and Cao et al same LIBSVM as the classifier but with the features of informativeness, readability and subjectivity.  X  System4 uses our user-preference ba sed helpfulness assessment. 
The system also uses the LIBSVM as the classifier but with the features of needs fulfillment, information credibility and mainstreaming-opinion divergence. Thereinto, the needs fulfillment is measured by the capture rate (see equation 3), the credibility is measured by uncertainty factors (viz., volition based factor and experience ba sed factor, see equation 4 and equation 5) and the divergence is measured by the difference between polarity forces (see sec tion 5.3). Besides, we trained the discount factor  X  of the combination weight (see equation 1) in the measure of needs fulfillment, and set its optimal value to be 1.5, by which needs-fulfillment based classification can achieve best performance. 
In our experiments, the AHV systems include:  X  System5 is a baseline AHV system which randomly ranks the reviews for each product.  X  System6 is a Ranking SVM based AHV system. Thereinto, 
Ranking SVM employs support vector machine (SVM) to classify object pairs in consid eration of large margin rank boundaries. Here, we take pairs of reviews and their relative helpfulness derived from training data as training instances and apply Ranking SVM for learning better helpfulness assessment functions, by which to obtain op timal ranking list of reviews. 
Besides, we use the LIBSVM with the features of Kim et al implement the pair-wise classification.  X  System7 is a Ranking SVM based AHV system. But it uses the 
LIBSVM with the features of Liu and Cao et al [5] to implement the pair-wise classification.  X  System8 is also a Ranking SVM based AHV system. But it uses the LIBSVM with our features to implement the pair-wise classification. Finally, by jointly using all of the features of Kim et al, Liu and Cao et al and ours, we implem ent an improved classification system (System 9) and an improved AHV system (System 10) . 7 http://www.csie.ntu .edu.tw/~cjlin/libsvm We firstly run the helpfulness ba sed classification systems. The accuracies of the systems are shown in Table 6. Here, our system (System4) achieved promising perfo rmance with the accuracy of 71.91%. Compared to the textual feature extraction of Kim et al (System2) and Liu and Cao et al (System3), our user-preference based feature extraction additional ly contributes at least 6.77% correct helpfulness assessment. Table 6. Accuracies of helpfulness-based review classification System1 We individually used the user-pre ference based features and their combinations in the classifier, by which to inspect the respective contributions of the features. We show the corresponding accuracies in Table 7, within which  X  X F X  denotes the feature of needs fulfillment,  X  X C vol  X  denotes volition based information credibility,  X  X C exp  X  denotes experience based information credibility,  X  X O X  denotes di vergence from mainstreaming opinion and the sign  X   X   X  means the improvement when adding a feature into classifier. From the performances in Table 7, we can find two issues: one is that the classification accuracy when using the feature of  X  X O X  is very low; two is that the joint use of the features of  X  X O X  and  X  X C vol  X  contributes the lowest improvement, and on the contrary, the joint use of  X  X O X  and  X  X C exp  X  contributes the most improvement. 
Table 7. Respective contributions of user-preference based Feature Accuracy(%) Feature Combination Accuracy(%) NF 
MO divergence from mainstreaming opinion (viz., the feature of  X  X O X ), the correctness of mainstreaming opinion detection is very important, and the correctness mostly relies on whether there are enough reviews to calculate a steady average pol arity force. But actually the numbers of reviews for different products have very uneven distribution. Within the test dataset, there are approximately 25% products has only no more than 20 corresponding reviews. The curve of  X  X O X  in Figure 6 illustrates the negative influence of sparse reviews for the  X  X O X  based classifier. In the figure, the horizontal axis corresponds to different ranges of numbers of reviews a product received; the vertical axis corresponds to the average classification accuracies on different ranges. It is not hard to find, compared to other three features, the feature  X  X O X  cannot help the classifier accurately determine the helpfulness of reviews. The reason for the second issue is that the features  X  X O X  and  X  X C vol  X  make their classifiers have th e same determination on most reviews. By analyzing the result s, we found that approximately 79.2% reviews in test dataset receive the same helpfulness determination from  X  X O X  based classifier and  X  X C classifier. Thus, when adding the feature  X  X C vol  X  into the  X  X O X  based classifier, even if the feature has the ability to make better distinctions on review helpfulness, it has no enough space to offer this advantage. Compared to this,  X  X O X  based classifier and  X  X C exp  X  based classifier only have the same determination on 34.3% reviews. Thus the space for their respective superiority is bigger. The second issue also il lustrate that the volition and sentiment normally have a compa tible effect on reflecting user preference, e.g. people normally show positive sentiment to the things they preferred. We secondly run the AHV systems on the test dataset. The performances of the systems are shown in Table 8. Here, NDCG@10 means the NDCG on the top 10 of ranking list, which needs that only a product with at least 10 reviews can be used as test sample. Thus we restricted the participation of products with less than 10 reviews in test, and used the rest (26,962 reviews) to evaluate the AHV systems. The results show that our AHV system achieves the optimal NDCG@10. Compared to the AHV systems of Kim et al and Liu and Cao et al, our AHV system respectively improves 6.5% and 5.35%. At last, we run the final classification system (System9) and AHV system (System10), both of which use all existing features (including the textual features pr oposed by Kim et al, Liu and Cao et al, and our user-preference ba sed features). The results show further improvements on helpfulness assessment and ranking (See Table 9). As discussed in Section 4, we use parts of items provided by Amazon.com (see Table 1) to illustrate the availability of user-preference based features in helpfulness assessment. Thereinto, needs fulfillment, the item &lt;Buyer?&gt; is used to measure the feature of tense based information credibility, and the item &lt;Star&gt; is used to measure the feature of mainstreaming opinion consistency. And the capacity of the features in dividing helpful and useless reviews is obvious. But the features are restricted to use in AHV system to ensure pure automatic system without human intervention (our systems only use the items &lt;Content&gt; and &lt;Product&gt;). Here, we also us e the measures to generate the review classification and AHV system for evaluating the compatibility of our only &lt;Content&gt; based feature extraction to the human intervention based feature extraction. The performances of the systems ar e shown in Table 10 and Table 11, where  X  X  X  denotes the feature from human intervention,  X  X  X  denotes the feature only extracted from contents of review,  X  X F X  denotes the feature of needs fulfillment,  X  X C X  denotes information credibility (only based on tense here) and  X  X O X  denotes mainstreaming opinion consistenc y. It not hard to find the performances are very compatible. The only issue is the performances of  X  X (NF) X  and  X  X (IC) X  are better than that by human intervention. The reason is that the measure of  X  X F X  and  X  X C X  have been improved based on the language information in &lt;Content&gt;, but it is hard for the items from human. In this paper, we focus on discu ssing how to automatically assess review helpfulness and exploring the possibility of using user-preference based features to im prove previous textual feature based helpfulness assessment. We respectively illustrate the availabilities of the features of needs fulfillment, information credibility and divergence from mainstreaming opinion in helpfulness assessment, and give the quantitative measures of the features. By using the features, we respectively build a helpfulness based review classification system and an automatic review ranking system. The test results on a large scale of commercial reviews (from Amazon.com) show the user-preference based features contribute substantia l improvements for both review classification and ranking. The improvements demonstrate that user preference learning is useful to review helpfulness assessment. In future, we will further explore the features that reflect the preference on commercial reviews. For example, users no rmally only read and vote on the reviews on the top of original ra nking list by commercial websites, which will give steady voting rate to the top reviews but unreliable rate to the reviews on the bottom of ranking list. But, the similarity of contents can be used to detect the link between the relevant reviews wherever they locate. Therefore, the similarity can be used to build a collaborative helpfulness assessment, which use the steady voting rate to estimate the weak rate of the reviews which are sel dom or even never seen by users on the bottom of original ranking list. This research is supported by the National Natural Science Foundation of China (No. 60970056, 60970057, 61003152, 90920004), Special fund project of the Ministry of Education Doctoral Program (2009321110006, 20103201110021) and Natural Science Foundation of Ji angsu Province, Suzhou City (SYG201030). [1] Kim, S.M., Pantel, P., and Chkl ovski, T., Penn acchiotti, M. [2] Titov, I., and McDonald, R. 2008. Modeling online reviews [3] Goldensohn, S. B., Hannan, K. , McDonald, R., Neylon, T., [4] Popescu, A. M., and Etzioni, O. 2005. Extracting Product [5] Liu, J. J., Cao, Y. B., Lin, C. Y., Huang, Y. L., and Zhou, M. [6] Jinal, N., and Liu, B. 2008. Op inion Spam and Analysis. In [7] Cristian, D. N. M., Kossinets, G., Kleiinberg, Jon., and Lee, [8] Tsur, O., and Rappoport, O. 2009. REVRANK: a Fully [9] Liu, F., Yu, C., and Meng, W. Y. 2004. Pe rsonalized web [10] Liu, Y., Huang, X. J., An, A. , and Yu, X. H. 2008. Modeling [11] Jo, Y., and Oh, A. 2011. Aspect and Sentiment Unification [12] Cao, Y. B., Xu, J., Liu, T. Y., Li, H., Huang Y. L., and Hon, [13] Valizadegan, H., Zhang, R., Zhang, R., and Mao, J. 2009. In [14] De Marneffe MC., MacCartney B., and Manning CD. 2006. [15] Gim X nez, J., and M X rquez, L.2004. SVMTool: A general [16] Zhang, J. F., Hong, Y., Yang, Y. H., Yao, J. M., Zhu, Q. M. [17] Marcu, D., and Wong, W. 2002. A phrase-based joint [18] Doddington, G., Mitchell, A. , and Przybocki. 2004. The [19] Strapparava, C., and Valitutti, A. 2004. Wordnet-affect: an [20] Nasukawa, T., and Yi, J. 2003. Sentiment Analysis: [21] Jarvelin, K., and Kekalainen, J. 2000. IR evaluation methods 
