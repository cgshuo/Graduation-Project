 H.2 [ DATABASE MANAGEMENT Systems. ]: Distributed Databases Design, Performance Distributed Continuous Query Processing, Distributed Stream Query Engine. Self-Tuning.
Efficient continuous query processing is critical for many appli-cations, including monitoring remote sensors network traffic man-agement, and online transaction processing. To overcome resource limitations and achieve real-time responsiveness, continuous query systems research focuses on issues such as punctuation-driven operator-state purging [4], load shedding, operator scheduling [7] and plan optimization and on-line migration [9].

However, when real-time yet accurate results are critical such as in stock market analysis, then there is a limitation to what any of the above optimization techniques can accomplish. Hence, dis-tributed processing must be applied to support required scalability. It has been already proven in traditional database systems [3] that distributed processing results in high scale-up and speed-up capa-bilities due to aggregated resources. The design of a distributed continuous query system is characterized by an extra complexity, considered in the design of our D-Cape engine. In such a system, data streams may be infinite and initial cost statistics about the data streams are typically unknown. Moreover, cost statistics continue to change over time.

Our research on distributed stream processing addresses two crit-ical questions: (1) How to initially distribute query plans given little or possibly no cost information, and (2) How to efficiently adapt the query distribution corresponding to runtime environmen-tal changes. Even though research is now under way in designing distributed continuous query systems [2], [1], results to date, as far as we know, are based on simulations. Our work offers empirical results of distributed continuous query processing using an actual software system [5], [6].

D-Cape, a distributed continuous query processing architecture, employs stream query engines over a cluster of shared-nothing pro-cessors. We employ a dedicated distribution manager to manage the distribution, monitoring, and runtime redistribution of query plans, thus separating the control responsibility from the actual query processing task. Unlike Aurora*/Medusa [2], [1] which fo-cus on research issues of a large area network, D-Cape targets a local cluster environment connected with a high speed network. However, D-Cape X  X  controllers can be multi-tiered such that we can have multiple controllers, each controlling a cluster.Our exper-iments illustrate that D-Cape X  X  design is light-weight, yet effective. Contributions. Contributions of this work include:
Each machine in D-CAPE has a CAPE query processor [6] in-stalled, which performs the query processing tasks (Figure 1). A set of query processors is managed by a dedicated distribution man-ager (Figure 2).
 Each CAPE Query Processor is composed of seven modules. The execution engine oversees the execution of query plans based on information from statistics gatherer and decisions by the scheduler. The stream feeder is a separate thread responsible for taking tuples received by the stream receiver and placing them in the proper in-put queues of operators. The stream distributor sends tuples to the next query processor or to an end-user application. The connec-tion manager , the interface between the query processor and the distribution manager , handles requests such as activating operators on a processor, or sending the current status of the processor to the distribution manager.
 The Distribution Manager synchronizes the management of the installation and execution of query plans across the computing clus-ter. The runtime monitor listens for statistical updates from each query processor. The connection manager executes the connec-tion protocol to establish remote connections between every pair of operators assigned to different query processors. The distribution decision maker decides how to distribute the query plans. There are two phases to this decision. First, an initial distribution is created at startup when only limited cost statistics about the query plans are known. Then, at run-time, query operators are redistrib uted to other query processors.
Query plan distribution in D-CAPE is defined as the initial de -ployment of query plans across a set of query processors. Sin ce lit-tle or even no cost statistics about data streams and query op erators can be assumed, we make use only of the definition of queries to be processed and the number of available query processors. As o ur ex-periments show, the initial distribution significantly infl uences the overall performance [8]. Some distributions, if not carefu lly de-signed, will not always increase performance beyond that of a sin-gle query processor or even worse, they may degrade performa nce.
In D-CAPE, we introduce a balanced network-aware (BNA) dis-tribution algorithm. This distribution reduces network co nnections by keeping adjacent operators on the same processor while at the same time it balances the load per machine measured in number of operators allocated on a query processor.
Figure 4 shows how the choice of an initial distribution can s ig-nificantly influence query plan performance. For this experi ment a medium workload (a query plan with 40 operators) was distrib uted over 2 query processors using the BNA and the Random initial d is-tribution. For 30 minutes Q 2 produces only 50,000 tuples if the Random distribution is used to deploy operators across quer y pro-cessors. Whereas, the same query plan performs about 10 time s Figure 4: BNA and Random Distribution and Redistribution, 2 QPs (Medium Workload). better in terms of the number of tuples produced if the BNA dis tri-bution is used for initial query plan deployment.
D-CAPE is designed to have the capabilities of monitoring pr o-cessing performance in a non-obtrusive manner and of seamle ssly redistributing query operators during runtime even under fl uctuat-ing network conditions [8]. One of the policies implemented in D-Cape is the degradation-based redistribution policy. This policy alleviates load on machines that have shown a degradation in output rate since the last time operators were allocated to the mach ine by moving the most costly operators to other query processors. In this, we give highest preference to operators that will reduce the num-ber of network connections from the overall distribution if more than one operator is available to be moved. As Figure 4 shows, our runtime redistribution algorithm can improve the performa nce of a query plan even if a good initial distribution was used.

The protocol of moving query operators from one query proces -sor to another seamlessly deactivates the operators to be mo ved in the original processor and reactivates them in the new proce ssor. We package our protocol into a six-step protocol [8]. [1] D. Abadi, Y. Ahmad, and et. al. The design of the borealis [2] M. Balazinska, H. Balakrishnan, and M. Stonebraker.
 [3] D. DeWitt and J. Gray. Parallel database systems: The fut ure [4] N. M. L.Ding, E.Rundensteiner, and G. Heineman. Joining [5] B. Liu, Y. Zhu, M. Jbantova, and E. Rundensteiner. DAX: A [6] E. Rundensteiner, L. Ding, T. Sutherland, Y. Zhu, B. Piel ech, [7] T. Sutherland, B. Pielech, Y. Zhu, L. Ding, and E. A. [8] T. Sutherland and E. Rundensteiner. D X  X ape: A self-tunu ng [9] Y. Zhu, E. Rundensteiner, and G. Heineman. Dynamic plan
