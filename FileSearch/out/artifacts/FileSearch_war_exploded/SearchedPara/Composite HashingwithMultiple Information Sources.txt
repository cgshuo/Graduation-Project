 Similarity search applications with a large amount of text and image data demands an efficient and effective solution. One useful strategy is to represent the examples in databases as compact binary codes through semantic hashing, which has attracted much attention due to its fast query/search speed and drastically reduced storage requirement. All of the current semantic hashing methods only deal with the case when each example is represented by one type of fea-tures. However, examples are often described from several different information sources in many real world applica-tions. For example, the characteristics of a webpage can be derived from both its content part and its associated links.
To address the problem of learning good hashing codes in this scenario, we propose a novel research problem  X  Com-posite Hashing with Multiple Information Sources (CHMIS). The focus of the new research problem is to design an al-gorithm for incorporating the features from different infor-mation sources into the binary hashing codes efficiently and effectively. In particular, we propose an algorithm CHMIS-AW (CHMIS with Adjusted Weights) for learning the codes. The proposed algorithm integrates information from several different sources into the binary hashing codes by adjust-ing the weights on each individual source for maximizing the coding performance, and enables fast conversion from query examples to their binary hashing codes. Experimen-tal results on five different datasets demonstrate the superior performance of the proposed method against several other state-of-the-art semantic hashing techniques.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation Composite Hashing, Semantic Hashing, Multiple Informa-tion Sources
The explosive growth of the internet has generated a huge amount of data such as documents, images and videos. As the data sizes increase, the density of similar objects in the data space also increases. Therefore, nearest neighbor meth-ods for similarity search applications tend to be more reliable than before. However, two key problems for using nearest neighbor search in large datasets are: (1) Storage challenge . How to store the training data efficiently? (2) Retrieval chal-lenge . How to retrieve the desired data efficiently and ef-fectively? It is clear that the traditional methods, such as TF-IDF for document representations [34, 35], which com-pare word count vectors, are difficult to be directly used for large datasets, since they need to save the original examples and the computational cost of dealing with floating/integer features is often too high.

A clever way of solving these two challenges is through se-mantic hashing [17, 33, 47]. By using semantic hashing algo-rithms, each example in the database is re-represented by a compact binary code, which preserves its semantic meanings in the original feature space. These hashing methods also provide a way to efficiently transform query examples into the corresponding hashing codes. Then, the retrieving pro-cess can be simply done by selecting examples within a small Hamming distance of the codes for the query example. This method addresses the two challenges in the following ways: (1) By mapping examples in the database to a low dimen-sional binary space, it is much more efficient in terms of the storage cost, compared with saving the original examples. (2) The retrieval speed is fast, since an efficient scheme for mapping the query examples to their hashing codes is pro-vided, and the similarity computation can be simply done by using bit XOR operation and counting the number of  X 0 X  bits in the low dimensional space. This process is very fast, and nowadays even an ordinary laptop is capable of doing millions of Hamming distance computation in a short time.
Previous semantic hashing methods are successful in ad-dressing the two challenges. However, they do not address the case when examples are described from several differ-ent sources. Actually, in many real-world applications, it is often tru e that examples are derived from several different sources, and therefore are represented by multiple sets of fea-tures. For example, in web mining applications, each web-page has disparate descriptions, textual content, in-bound and out-bound links, etc. In image retrieval, each picture can be described by different kinds of features, such as the SIFT features [27], RGB features, texture features, etc. Dif-ferent sets of features could have different statistical proper-ties. Therefore, designing a hashing algorithm for examples from multiple information sources is necessary. The main challenge for this task is how to incorporate different sets of features together into one set of binary hashing codes.
To address this task, we propose a novel research prob-lem: Composite Hashing with Multiple Information Sources (CHMIS). The basic objective of CHMIS is to design some efficient and effective hashing algorithms that can encode the examples described from several different information sources. An intuitive way of designing a CHMIS method is to simply concatenate the features from several different sources, treat them as if they were from one information source, and apply the previous hashing method to these ex-amples. However, by doing so, the different statistical prop-erties from different individual sources may be lost. This paper proposes an elegant method  X  CHMIS-AW (CHMIS with Adjusted Weights) to intelligently integrate informa-tion from different sources. In particular, CHMIS-AW is an iterative method which preserves the semantic similari-ties between training examples, and ensures the consistency between the hashing codes and the corresponding hashing functions designed for different information sources. Fur-thermore, the importance of each individual source is rep-resented as a convex combination coefficient and can be au-tomatically learned through a joint optimization procedure. Our experiments on five real world datasets with different information sources show that the proposed method outper-forms state-of-the-art methods substantially.
Efficiency is a critical issue for many information retrieval applications with a large number of text documents, images or videos. For traditional ad-hoc text search with relatively short user queries, different types of structures and opera-tions of inverted indexing have been proposed [13, 39, 48, 54]. Distributed indexing [2, 3] has also been explored when there is sufficient computing resource. On the other side, similarity search with documents, images, or other entities are typically represented as feature vectors in a space of more than thousands of dimensions [25, 28], which demands different solutions especially when only limited computing resource is available.

The traditional similarity search is conducted based on these feature vectors by space partitioning index structures, like TF-IDF methods [34, 35], KD-tree, or data partitioning index structures, say R-tree [12]. However, when the dimen-sionality of feature space is too high, traditional similarity search may fail to work efficiently [46]. Semantic hashing [33] is used in the case when the requirement for the ex-actness of the final results is not high, and the similarity search in the original high dimensional space is not afford-able. More precisely, semantic hashing methods try to rep-resent the whole datasets by using a fixed small number of binary bits so that the queries can be answered in a short time (virtually constant time) [41], with some extent of pre-cision being guaranteed. Such hashing based fast similarity search can be considered as a way to embed high dimensional feature vectors into a Hamming space, while preserving the semantic similarity relationship between examples as much as possible. This is different from traditional dimensionality reduction methods, such as Principal Component Analysis (PCA) and Latent Semantic Analysis (LSI) [15, 20]. Hash-ing methods map original features to low dimensional binary codes, which enables the efficient search in Hamming space. One of the most well known hashing methods is Locality-Sensitive Hashing (LSH) [1, 14]. It uses random linear pro-jections to map close examples to similar codes. It has already been proved that the Hamming distance between different examples will asymptotically approach their Eu-clidean distance in the original feature space, with the in-crease of the hashing bits. Some LSH based methods are also used for near optimal duplicate detection. For example, min-Hash function [8 X 10] assigns numbers to each example, and, for two documents, the probability of being assigned the same number equals the ratio of the intersection and union of their word representations.

Besides LSH, some machine learning methods can be adap-ted to solve the hashing problem. The traditional dimen-sionality reduction methods can be adapted to solve the hashing problem via a simple thresholding [33, 45]. For ex-ample, PCA Hashing [26, 45] computes K -bit hashing codes by projecting each example to the K principal components of the training set, and then binarizing the coefficients, by setting each bit to 1 if it exceeds the median value seen for the training set, and  X  1 otherwise. In [33], the authors use stacked Restricted Boltzman Machine (RBM) [18, 19] to generate compact binary hashing codes, which can be considered as binarlized LSI. Later, in [51], the authors fur-ther improve this method. Some traditional classification algorithms can also be adapted. For example, in [4, 37], the authors adapted Adaboost [31] to the hashing method. More precisely, they consider the similar pairs of examples as positive examples, and otherwise negative. Then, a set of classifiers are trained by AdaBoost, the output of all of these weak learners are considered as the binary hashing codes. In [43, 44], the authors solve the problem of semi-supervised semantic hashing. In the following two sections, we mainly discuss two state-of-the-art hashing methods that most re-lated to the proposed method  X  Spectral Hashing [47] and Self Taught Hashing [52], which have been shown to outper-form several other types of hashing methods.
As a recently proposed method, Spectral Hashing (SH) [47] was proposed to design compact binary codes for search. This method can be considered as an extension of spectral clustering [49]. Its concrete formulation is as follows: where, S ij is the similarity measure between the example i and example j , y i is the hashing code for the i -th example, K is the number of bits. The basic motivation of this for-mulation is to preserve the original similarity relationship between examples in the Hamming space. However, solving this ab ove problem is NP hard. After relaxing the discrete constraints to continuous, it can be solved using traditional spectral analysis. Then, the optimal hashing code can be simply obtained by binarizing the solution of the relaxed optimization problem. Moreover, this formulation can be generalized to enable the effective computation for out of sample examples/queries, as introduced in [47].

SH tries to keep the similarity relationships, which are de-fined in the original feature space, between examples in the hashing codes. However, it is hard to find a good similarity measure which can consider the consistency of the different sources. Actually, some previous works demonstrate an im-provement for considering the consistency in multiple source problems [32, 40, 50], in which the authors designed a clas-sifier on each view, and require the outputs from different views should be consistent and not deviate too much.
Self Taught Hashing (STH) [52] can be considered as an extension to SH. To obtain the hashing codes for the train-ing set and the hashing function for efficiently mapping the query examples, STH uses two steps, an unsupervised step and a supervised step. In the unsupervised step, the authors construct a k-nearest-neighbor graph for the given dataset, embed the examples into a K ( K is still the number of bits) dimensional space through spectral analysis, and obtain the binary codes for each example through thresholding. In the supervised step, K SVM classifiers are trained based on the training examples, and their binary hashing codes learned from the previous step are used as labels. The hashing codes for the query example can then be obtained through a classi-fication problem and thresholding by using the K classifiers. Compared with SH, when dealing with query examples, STH does not assume that data are uniformly distributed in a hyper-rectangle, which is restrictive. The maximum mar-gin principle enables a good generalization ability. However, it still cannot avoid the same consistency problem of the sim-ilarity measures for different sources as in SH. Different from their method, the method proposed in this paper combines both the unsupervised and supervised steps for intelligently integrating features from multiple sources. In CHMIS, we are given a set of n training examples from M information sources, represented as: { x ( i ) 1 , x ( i ) R i  X  1 , i  X  { 1 , 2 , . . . , M } , where d i is the dimensionality of the i -th information source. The main objective of CHMIS is to find the optimal K binary hashing bits Y  X  X  X  1 , 1 } K  X  n for these training examples, as well as a hashing function f (  X  ) that deals with the query (out-of-sample) examples. The coding optimization criterion is to identify neighbors of the query examples in the training set accurately in a short time. For convenience, throughout this paper, the hashing codes for the i -th training example, i.e., the i -th column of Y , is denoted as y i . The hashing codes for the p -th hashing bit, which is the p -th row of Y , is denoted as Y p .
From the previous presentation, we can see that CHMIS is an important research problem. To solve it, this pa-per presents a novel optimization formulation  X  CHMIS-AW (CHMIS with Adjusted Weights).

The main objective of CHMIS-AW is to obtain hashing codes for training examples and to learn the hashing function simultaneously by preserving the similarity relationships be-tween training examples in the original feature space, and generating a consistent hashing function from different in-formation sources. In particular, the objective function of CHMIS-AW is composed of two parts: The first part is a similarity preservation term, which tries to preserve the sim-ilarity relationship in the original feature space using the learned hashing codes. The second part ensures the con-sistency between the learned hashing codes and the corre-sponding hashing function designed on multiple sources. In the following, we will first introduce how to construct the two parts respectively. Then the joint optimization problem and the corresponding optimization strategy will be pro-vided. Finally, we will discuss the distinctions of the pro-posed method and some related ones.
One of the key goals in most state-of-the-art hashing meth-ods, such as [33, 47], is to seek for compact binary codes so that the Hamming distance between codewords correlates with semantic similarity. This indicates that similar data points should be mapped to similar codes within a short Hamming distance. In SH, the authors demonstrated that the problem of finding such an optimal set of codes is closely related to the graph partition methods. They proposed to achieve the goal by using the spectral method.
 Different from the previous hashing problems, in CHMIS-AW, we need to deal with multiple information sources. To measure the similarity between examples represented by the binary hashing codes, one natural way is to measure the similarity quantity on each individual source and sum them together as follows: Here, S ( t ) n  X  n is the affinity matrix defined on the t -th source. To meet the similarity preservation criterion, we seek to min-imize this quantity, because it incurs a heavy penalty if two similar examples are mapped far away on the information source.

There are many different ways of defining the affinity ma-trix S ( t ) . In [47], the author used the global similarity struc-ture of all document pairs, while in [52], the local similarity structure, i.e., k-nearest-neighborhood, is used. In this pa-per, we use the local similarity, due to its nice property in many data mining and information retrieval applications [28]. In particular, the corresponding weights are computed by Gaussian functions, i.e., S The variance  X  ij is determined automatically by local scaling [49], and N k ( x ) represents the set of k-nearest-neighbors of the example x .

By introducing a diagonal n  X  n matrix D ( t ) for each indi-vidual source, whose entries are given by D ( t ) ii = Eq.(2 ) can be rewritten as: tr where L ( t ) is the graph Laplacian [11] defined on the t -th source, and tr (  X  ) is the trace function. Furthermore, similar to [52], we can replace the graph Laplacian with the normal-ized graph laplacian due to its superior performance [11, 38] indicated by: Then the objective function that needs to be minimized turns to: By minimizing this term, the similarity between different examples can be preserved in the learned hashing codes.
In Section 4.1, our focus is to find the optimal binary hashing matrix Y that preserves the data similarities in the original feature space. However, this is only a trans-ductive formulation, i.e., Y cannot be generalized to query examples directly. It is true that we can use the Nystr  X  om method [5] to deduct the hashing codes for the query ex-amples from the hashing codes in the training set and the similarity matrix between the query and the training ex-amples, but this operation is as expensive as doing exhaus-tive nearest neighbor search. We solve this problem by first introducing a linear hash function 1 , which is represented as: f ( x i ) = the weight vector for the classifier on the t -th source, and  X  = [  X  1 , . . . ,  X  M ] T is a M dimensional non-negative con-vex combination coefficient column vector that balances the outputs from each individual source, and
In this way, the output of this hashing function is a convex combination [7] of the linear hash function outputs on each individual source. A constraint here is that the outputs on the training set should be as close to Y as possible, i.e., the outputs of the hash functions from different sources should make an  X  X greement X  on the learned binary hashing codes. The importance of each individual source on this agreement is specified by the corresponding convex combination coeffi-cient. In this way, this methodology incorporates the differ-ent statistic properties of the different sources. The concrete formulation for the consistency part is given as follows: where, the regularization term to avoid overfitting [42].
 is a loss function, which measures the difference between the hashing codes and the outputs of the hash functions. C 2 is a trade-off parameter, balancing the loss function and the regularization term.
A lth ough [30, 43, 44, 52] use linear classifiers, as they have claimed, the kernel methods [36] can be easily incorporated into their formulations. So is the proposed method.
For convenience, we introduce the following concatenate matrix:
Here, after this concatenation, the new designed f W is a ( d 1 + d 2 + . . . + d M )  X  K dimensional matrix, and e x ( t ) one-dimensional column vector with only from the ( d 1 + d nonzero. It is clear that ( W ( t ) ) T x ( t ) i = f W T e Eq.(6) can be simplified as:
J ( Y , f W ,  X  ) = C 2
The overall objective function combines the similarity preser-vation part given in Eq.(5) and the consistency part in Eq.(8) as follows: where T ( Y , f W ,  X  ) = tr ( C 1 Y T  X  ( f W ) T e X ( t ) k 2 + k f W k 2 . Here, Y1 = 0 requires each bit to fire 50% of the time (with equal probability as positive or negative), and the constraint YY T = I requires the bits to be uncorrelated.

This is a hard optimization problem because of the dis-crete constraints. We propose to relax this constraint and drop the constraint Y1 = 0 first. However, even after the relaxation, the objective function T ( Y , f W ,  X  ) is still non-convex with respect to Y , f W ,  X  jointly, which makes it difficult to solve.

To solve this problem, we will show the optimal solution of f W has a closed form solution with respect to Y and  X  . For notational convenience, we rewrite e X  X  . Then, the optimal f W can be determined by solving the following optimization problem:
This is a standard regularized least square problem [6], and its optimal solution can be obtained by: f W = QY , back to T ( Y , f W ,  X  ), we can eliminate the optimization vari-able f W , and problem (9) can be rewritten as: , and Consistency trade-off parameter C 2 , as in Eq.(9). ) where, Here, H (  X  ) is a positive semi-definite matrix, which is re-lated to  X  and is introduced to simplify the formulation. I is an identity matrix.

The problem (11) is still non-convex, since  X  and Y are still coupled together. Fortunately, the problem is convex with respect to either of them, with the other one fixed, and therefore can be solved by alternative optimization with guaranteed convergence. In particular, after initializing  X  , the optimization problem can be solved by doing the follow-ing two steps iteratively, until convergence: 1. Fix  X  , optimize: 2. Fix Y , optimize: After obtaining the optimal Y , we can get the optimal hash-ing code for the training set by thresholding Y . More specif-ically, for Y p , which is the p -th row of Y , if the j -th element Y j is larger than the specified threshold, Y wise Y p j =  X  1.

Then, a natural question would be: how to pick these thresholds? In [45], the authors pointed out that a good semantic hashing should also maximize the entropy to en-sure efficiency. Using maximum entropy principle, a binary bit that gives balanced partitioning of the whole dataset al-ways provides maximum information. Therefore, we set the threshold for binarising Y p , p  X  { 1 , 2 . . . , K } to be the me-dian value of v p , p  X  X  1 , 2 . . . , K } (The median value for Y will be denoted as m p thereafter). Still, after binarising, since different eigenvectors are mutually orthogonal, the dif-ferent bits Y p will also be uncorrelated. In this way, the binary code achieves the best utilisation of the hash table and the contraint Y1 = 0 in Eq.(9) can also be satisfied.
After getting the hashing code for the training set, the hashing code for the query example q can be computed by first computing the hashing function f ( q ) = where, e q ( t ) is derived from q ( t ) , using exactly the same way ing binary hashing code can be obtained through threshold-ing the hashing function output, i.e., the j -th hashing code for q equals +1, if f ( q ) j &gt; m j and otherwise  X  1.
The training process of the proposed method is described in Table 1. The procedure of deriving hashing codes for query examples is summarized in Table 2. For hashing encoding, the training process is always conducted offline. Therefore, our focus of efficiency is on the prediction pro-cess. Since this process only involves some dot products Algo ri thm : CHMIS-AW Predicting
Input: Output: The bina ry hashing code h  X  X  X  1 , 1 } K  X  1 for q
Ini tiali zation : 1. Const ruct e q ( t ) similar to Eq.(7)
F or t = 1 : M 2. Calcu late the regression output by o ( t ) = ( f W ) T e
End Fo r 3. Calcu late the balanced output by h = f ( q ) = O  X  , where O = [ o (1) , o (2) , . . . , o ( M ) ]
F or p = 1: K 4. if h p &gt; m p , h p = +1 , and otherwise h p =  X  1 End Fo r and aggregations between two vectors, which can be done in O ( s ) time. Here, s is the average sparsity for features from the M sources.
The CHMIS problem is related to the traditional multi-view learning [16, 21, 24, 40, 50, 53]. The basic motivation for many previous multi-view learning methods [16, 24, 40, 50] is to design a classifier on each view/source, and requires that the soft labels on these views/sources to be consistent and should not deviate too much. These methods are mainly designed for classification problems. However, in CHMIS, we need to learn two things from the training data, i.e., (1) the hashing codes for the training data, (2) a hashing function that enables efficient mappings of query examples. CHMIS is an unsupervised problem, in which we can consider the hashing codes as unknown labels. Therefore, the traditional multi-view learning methods cannot directly be applied to solve CHMIS.

In CHMIS-AW, we learn both the hashing codes and the hashing functions together in one formulation. They are both considered as optimization objectives in this formula-tion, so that the solution can be tuned to the best point between good hashing codes that preserves the similarities between examples in the original feature space and a per-fect hashing function that approximates hashing codes for queries efficiently, since both of them are vital for efficient and effective retrieval. In contrast, in STH, the hashing codes and hashing functions are optimized in two different steps, which may cause a disconnection between them.
In this section, we will show the effectiveness of the pro-posed method through an extensive set of experiments 2 . 1. Cora [29] dataset consists of the abstracts and refer-
Th e co de and data can be found on the author X  X  homepage. 2. Reuters (Reuters21578) 3 is a collection of docu-3. ReutersV1 (Reuters-Volume I) : It is an archive 4. WebKB 5 consists of about 7000 webpages, collected 5. Healthcare dataset contains the information of 14199 h ttp ://daviddlewis.com/resources/textcollections/reuters2 1578/.
Actually, PLSA [20] can be considered as a dimensionality reduction method, which maps the documents into some fixed number of hidden topics. The topic distribution for each document can be used as low dimensional features.
CMU world wide knowledge base (WebKB) project. Avail-able at http://www.cs.cmu.edu/ WebKB/.
For each dataset, we use each document in the testing set as a query example to retrieve documents in the training set. We use two different metrics: the precision for the top 100 retrieved documents 6 , and the precision for the documents within the Hamming distance 2. Here, precision is defined as follows: precision = the number of retrieved relevant documents Throughout this paper, the relevant documents denote the ones with the same topics/labels as the query examples 7 . The averaged precision over all queries are recorded. As claimed in [22], once the number of bits is sufficiently high (e.g. 64), one would expect that distances with a Hamming distance less than or equal to two would correspond to near-est neighbors in the original data embedding.

For Reuters and healthcare dataset, we further report the precision-recall curves, where the recall rate is defined as:
To draw the precision-recall curves, for each query exam-ple, we vary the number of retrieved documents from 0 to the number of all training examples, while fixing the num-ber of hashing bits. The final results are the averaged results over all of the testing examples.
The proposed method CHMIS-AW is compared with four different algorithms on these datasets, i.e., Self Taught Hash-ing (STH), Spectral Hashing (SH), PCA Hashing (PCAH), and Latent Semantic Hashing (LSH).

These four comparison methods cannot be directly used for the multiple information sources. So, when conducting these comparison experiments, we first concatenate the fea-tures on different sources, use them as a single set of features and then apply the comparison algorithms. The parameters C 1 and C 2 in CHMIS-AW are tuned by 5-fold cross valida-
I f th ere are ties in the desired Hamming distance, some examples are randomly picked.
Please note that the labels are only used for evaluation purpose, but not for training.
 When constructing the graph laplacians for both CHMIS-AW and STH, the number of nearest neighbors is fixed to be 7 for all experiments. For LSH, we randomly select pro-jections from a Gaussian distribution with zero-mean and identity covariance to construct the hash tables.
First of all, we evaluate the performances of different algo-rithms by varying the number of hashing bits in { 8 , 16 , 32 , 64 , 128 } . The precision for the top 100 retrieved examples with different number of hashing bits is reported in Fig.1. And the precision for examples within hamming distance 2 is re-ported in Fig.2. From these comparison results, we can see CHMIS-AW shows the best performance among five hash-ing methods in most of the cases with information from different sources. CHMIS-AW also outperforms the other hashing methods in most cases when they use information from one specific source. In Fig.1(d), the precision of the proposed method is lower than that of SH and PCAH. How-ever, from the perspective of precision within hamming dis-tance 2, as shown in Fig.2(d), it gives the best performance. So, we can conclude that, in WebKB, CHMIS-AW achieves the best performance in retrieving the several most rele-vant examples, although probably not the top 100. The good performance of the CHMIS-AW is mainly because: (1) CHMIS-AW integrates the different information sources to-gether more naturally; (2) CHMIS-AW learns the hashing codes and hashing functions in one formulation, which ef-fectively avoid the noise that can be caused in a two step X  X  method as in STH.

As claimed in [33, 47], the LSH method is data-oblivious, and may lead to inefficient codes in practice. From the re-ported results, we can see that, LSH does not perform well in most cases, especially under the criteria of the precision of the top 100 examples. But for the criteria of the precision within Hamming distance 2, LSH does well in some cases. The similar behavior is also observed in [22]. As a spectral based method, SH borrows the idea of spectral analysis. Its basic motivation is reasonable. However, in CHMIS, SH can-not find a good similarity matrix that can be consistent on the different sources. As shown in the experimental results, the performance of SH is worse than that of CHMIS-AW. STH is a two step method. The first step is spectral cluster-ing step, and the second step employs SVM. However, it still cannot integrate the natural of multiple source problem into the algorithm and it needs to prepare the training data in a format for SVM, which may introduce some noise in this process. That X  X  why its performance cannot exceed that of CHMIS-AW.

By fixing the number of bits to 32, we further report the corresponding performances on all of the five datasets, along with the performance on each individual source in Tables 3 and 4. It seems that, in some cases, brute force con-catenating the features from different sources together may not necessarily improve the hashing performance. It further confirms that we need some more sophisticated algorithms specifically designed for CHMIS. Since the proposed method is specifically for CHMIS, it achieves the best performance in most of scenarios under both of the two criteria. The precision-recall curves with 32 hashing bits on Reuters and healthcare datasets are reported in Fig.3(a) and Fig.3(b) re-spectively. It is clear that among all of these comparison methods, CHMIS-AW shows the best performance. STH performs better than SH, PCAH and LSH. We have also observed similar results on the other three datasets. But due to the limit of space, they cannot be reported here.
There are two parameters in the proposed method, i.e., C 1 and C 2 . To prove the robustness of the proposed method, we conduct some parameter sensitivity experiments on Reuters. For each experiment, we tune only one parameter from the The results are reported in Fig. 4 and Fig. 5. It is clear from these experimental results that the performance of the pro-posed method are relatively stable with respect to C 1 and C . We have also observed similar patterns of the proposed method in the other four datasets.

The testing process of CHMIS-AW is fast, since the hash-ing function is a linear mapping and only involves some dot products. On an ordinary PC with Intel Core Duo CPU 2.5 GHZ and 4GB RAM, it takes about 0.0001 second per ex-ample for prediction in all datasets, which is similar to the comparison methods.
In this paper, in Eq.(3), we combined the graph Lapla-cian on each individual source with equal weights. However, a natural question would be: since we are tuning the impor-tance on each individual source by using a balancing factor  X  , can we do the same thing to combine the different graph Laplacian with different weights? One choice is to change the objective function in Eq.(9) to: tr ( C 1 Y T where g (  X  ) : R  X  R is a non-decreasing function. We be-lieve this is true. The problem is how to design a reasonable g (  X  ) function for this objective function. It is an intriguing problem that needs to be solved in the future.

Besides the problem of combining graph Laplacians, an-other future direction is to investigate how to solve the prob-lem when some sources of features are missing for some spe-cific examples.
To enable fast similarity retrieval, semantic hashing meth-ods represent the examples by a small number of binary bits, so that the storage space can be minimized, and the retrieval speed can be accelerated. However, previous hashing meth-ods only consider the case when each example is represented by one type of features. There is no prior work that can in-corporate different information sources together and learn the corresponding hashing codes. To address this problem, Figu re 3: Precision and Recall Curve on Reuters this paper proposes a novel research problem as Composite Hashing with Multiple Information Sources (CHMIS) for in-telligently combining information from different sources into final hashing codes. In particular, a method called CHMIS-AW (Adjusted Weights) is proposed to achieve this goal. CHMIS-AW is an iterative method, which optimizes the re-laxed hashing codes and the combination coefficients alter-natively. An extensive set of experiments clearly demon-strates the superior performance of the proposed method against several state-of-the-art techniques. We thank the anonymous reviewers for valuable comments. This research was partially supported by the NSF research grants IIS-0746830, CNS-1012208, IIS-1017837, CCF-0939370. Figur e 4: Parameter Sensitivity for C 1 and C 2 , with 32 Figur e 5: Parameter Sensitivity for C 1 and C 2 , with
