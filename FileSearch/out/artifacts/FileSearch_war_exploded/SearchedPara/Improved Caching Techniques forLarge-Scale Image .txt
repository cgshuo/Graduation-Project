 Commercial image serving systems, such as Flickr and Face-book, rely on large image caches to avoid the retrieval of re-quested images from the costly backend image store, as much as possible. Such systems serve the same image in di  X  erent resolutions and, thus, in di  X  erent sizes to di  X  erent clients, depending on the properties of the clients X  devices. The re-quested resolutions of images can be cached individually, as in the traditional caches, reducing the backend workload. However, a potentially better approach is to store relatively high-resolution images in the cache and resize them during the retrieval to obtain lower-resolution images. Having this kind of on-the-fly image resizing capability enables image serving systems to deploy more sophisticated caching poli-cies and improve their serving performance further.
In this paper, we formalize the static caching problem in image serving systems which provide on-the-fly image re-sizing functionality in their edge caches or regional caches. We propose two gain-based caching policies that construct a static, fixed-capacity cache to reduce the average serving time of images. The basic idea in the proposed policies is to identify the best resolution(s) of images to be cached so that the average serving time for future image retrieval re-quests is reduced. We conduct extensive experiments using real-life data access logs obtained from Flickr. We show that one of the proposed caching policies reduces the aver-age response time of the service by up to 4.2% with respect to the best-performing baseline that mainly relies on the access frequency information to make the caching decisions. This improvement implies about 25% reduction in cache size under similar serving time constraints.
Commercial image hosting services (e.g., Flickr, Face-book) rely on image serving systems that are geographically distributed in nature. A typical image serving system in-volves at least one data center, some cache proxies, and a number of edge caches. The main storage system is located in the data center(s) and hosts at least one copy of all avail-able images. Cache proxies are located in di  X  erent geograph-ical regions and have relatively smaller storage systems that host images requested by the clients in their region. 1 Finally, edge caches are general-purpose storage systems located at the network entry points of the image hosting service and may store images as well as other kinds of files.

The data centers, cache proxies, and edge caches form parts of a storage system hierarchy that is distributed over the network. When a client of the hosting service requests a particular image, the request is first tried to be resolved by looking up the image in the closest edge cache in the network. If the requested image is not found in the edge cache, the request is forwarded to the regional cache proxy responsible for the client X  X  region. If the requested image is found in the regional cache proxy, the image is served by the cache. If the requested image itself is not available, it may still be possible to obtain and serve the image by resizing a higher resolution version of the image previously cached in the proxy [9]. If both attempts fail (i.e., a cache miss), the request is finally forwarded to a data center, which is guaranteed to have the requested image, and the image is served by the backend image storage in that data center.
When designing and constructing cache hierarchies like the one mentioned above, an important optimization objec-tive is to minimize the miss rate of regional caches. 2 This is usually achieved by storing, in the respective regional cache, images that are frequently and/or recently requested by the clients of a particular region. Reducing the miss rate of re-gional caches relieves the request workload of the backend storage systems in data centers. Moreover, due to the close proximity of such caches to the clients, considerable reduc-tion can be achieved in the serving latency if more images are served by these caches. This, in turn, may make a pos-itive impact on users X  engagement with the image hosting service, especially when the user-perceived response latency values (in case of no regional caches) are beyond the range tolerable by the users. In practice, the financial cost of con-structing these regional caches often forms a constraint for image hosting companies. Therefore, a regional cache can have storage capacity (memory and disk) that is su cient to cache only a limited portion of the image collection.
Our paper investigates the static caching problem in geo-graphically distributed image hosting services. Despite the
Here, the client may refer to a real user, a crawling bot, or some automated software.
In the rest of the paper, we refer to cache proxies and edge caches together as regional caches. dynamic nature of the request streams, previous research [1] has revealed that static caches, which serve highly frequent requests, play an important role in building e  X  ective caching systems with high hit rates. Small dynamic caches usually lead to lower hit rates than static caches of similar size, and a combined static/dynamic cache always has its dynamic part built on top of its static part. 3 In this work, we partic-ularly focus on static caching, which is a more fundamental problem than dynamic caching. Adding a dynamic cache to serve images that are not found in the static cache is complementary to our problem.

The objective in our static caching problem is to select images to be stored in a fixed-capacity regional cache (in an o  X  ine manner) such that the average response time for fu-ture image retrieval requests is minimized. This particular caching problem is slightly di  X  erent from the static caching problem in similar domains (e.g., page caching in web prox-ies or result caching in web search engines) because the same image is often stored in many di  X  erent resolutions. 4 As a result, a cache hit does not necessarily lead to similar re-sponse time as in web search, rendering the cache hit rate not an ideal optimization target. In our case, it is possible to serve a retrieval request by generating the requested image after resizing one of its higher resolution versions previously stored in the cache [9]. Hence, a cache hit may be possible even if the requested resolution of the image is not found in the cache. As we will discuss, this di  X  erence leads to a slightly more complex cost model when caching images.
The contributions of this paper are as follows. We first provide a system model and a cost model, both guided by real-life observations, for the static caching problem in large-scale image hosting services. We then provide a formal def-inition for the static image caching problem where the opti-mization objective is to minimize the average serving time of the image hosting system under a cache capacity constraint. As a solution for this problem, we provide some frequency-based heuristics and propose two gain-based heuristics that aim to reduce the average serving time. We conduct ex-tensive experiments using real-life access logs obtained from Flickr. Under the same cache capacity constraint, the aver-age response time achieved by one of the proposed heuristics is up to 4.2% less than that of the best-performing baseline. This is equivalent to a cache size reduction of 25% under similar serving time constraints.

The rest of the paper is organized as follows. In Section 2, we formally define the static caching problem that we inves-tigate after presenting a realistic system model and a cost model. In Section 3, we provide some heuristic solutions for the problem. The experimental setting and results are pre-sented in Sections 4 and 5, respectively. The related work is surveyed in Section 6. We conclude the paper in Section 7.
We assume an image collection { I 1 ,...,I n } containing n images each with a unique id (referred to as logical images).
In general, the items in a static cache are updated period-ically, e.g., on a daily basis, and those in a dynamic cache may be updated upon a cache miss.
This is mainly because client devices di  X  er in terms of their resolution (e.g., a desktop computer versus a mobile phone). Figure 1: Architecture of an image serving system. Every logical image I i in the collection is available in exactly m di  X  erent resolutions { I 1 i ,...,I m i } (referred to as physical images). We denote by S ( I j i ) the size of a physical image I (in bytes). For convenience, we assume that S ( I k i ) &gt;S ( I if and only if k&gt;j , for all i .

Without loss of generality, we consider an abstract serving architecture composed of a single data center D and a single regional cache R . Data center D stores at least one copy of all physical images in the collection. Regional cache R can store a much smaller and fixed subset R of physical images. As we will discuss in Section 2.3, the selection of images to be stored on R forms the focus of our static caching problem. We illustrate the system architecture in Fig. 1.

The client requests are redirected to one of the nearby regional caches (e.g., using the IP addresses) in a quasi-deterministic way to ensure low network latency. Therefore, each regional cache has its own stream of requests and the related images to manage. Moreover, regional caches do not communicate with each other due to their high geographical (and network) distance. Communicating with a single data center or several data centers only a  X  ects the cost of fetch-ing an image from the backend as we will discuss shortly. Therefore, using this abstract serving architecture does not a  X  ect the design of caching policy.

A request issued by some client c to retrieve a physical image I j i is resolved in the following order.
Given a regional cache R with its image collection R and a physical image I j i , we compute the end-to-end response latency, T ( R ,I j i ), as Here, T PCH , T LCH , and T CM denote the response latency in the case of a physical cache hit, a logical cache hit, and a cache miss, respectively.

In the case of a physical cache hit, the latency is simply computed as where T read ( I j i ) denotes the time to read image I j memory or disk (depending on where the image is stored in the cache) and T Rc ( I j i ) denotes the time to send image I i from regional cache R to client c . Both times are posi-tively correlated with the size of image I j i . T Rc ( I correlated with the distance between R and c .
 For a logical cache hit, the latency is computed as where T resize ( I k i ,I j i ) denotes the time required to obtain the requested image I j i by resizing a higher resolution image I available in R . This cost increases with both S ( I k i S ( I j i ). We note that, if there are multiple images that have higher resolution than I j i , we always use the one that leads to the least time for resizing, i.e., the one closest to I terms of resolution.
 Finally, for a cache miss, the latency is computed as where T read ( I j i ) denotes the time it takes to read image I from the disk (of the backend image store), and T DR ( I j denotes the time to transfer image I j i from data center D to regional cache R over the network.

We note that this cost model assumes that conversion from I k i to I j i is time-wise cheaper than fetching I j backend image store in data center D , i.e., logical cache hits are always cheaper than cache misses. In practice, however, there may be cases where it is more beneficial to retrieve I from data center D instead of resizing at regional cache R (especially when the resolution di  X  erence between I k i is too large). In Section 4.4, we provide more detail for the computation of the costs given in Eqs. (2), (3), and (4).
We are given a sequence I of image retrieval requests. We denote by f ( I j i ) the frequency of requests in I for physical image I j i . Given a statically cached image collection R , the average response latency for serving the requests in I can be computed as
The objective of our static image caching problem is to fill a fixed-capacity static cache with a selected subset R of images so that the average response latency is minimized for a given sequence I of image retrieval requests. More formally, the objective is to find without violating the cache capacity constraint where C denotes the storage capacity of the cache (in bytes).
In this section, we introduce some heuristic solutions for the problem given in Section 2.3. All of the presented heuris-tics are greedy in nature. The basic idea behind them is to sort logical or physical images according to a greedy choice property and select images to be cached in that sorted or-der. The selection of images continues as long as the cache capacity constraint given in Eq. (7) is not violated. The heuristics di  X  er mainly in the way they define the greedy choice property. Herein, we present the heuristics under two headings: frequency-based and gain-based heuristics.
In this line of heuristics, the greedy choice property relies on the frequency with which images will be requested from the image hosting service. Since the future request frequency of an image is unknown, it is estimated using the image X  X  observed request frequency in a past time period. As in most other static caching problems, the fundamental assumption here is that the past and future request frequencies of images are highly correlated. We denote by f 0 ( I i ) the past request frequency of a logical image I i and compute it as where f 0 ( I j i ) denotes the past request frequency of physical image I j i . We next present three frequency-based heuristics.
Largest requested ( LR ). Logical images are sorted in decreasing order of their past frequency values ( f 0 ( I we iterate on images in this order. For each logical image I that is currently under consideration, we select its largest physical image that was requested at least once in the past, i.e., we cache a single physical image I k i such that f 0 and S ( I k i ) S ( I j i ), for all I j i , where 1  X  j  X  m . heuristic aims to increase the logical cache hit rate at the expense of more aggressive cache space consumption at each caching decision.

Most requested ( MR ). Similar to LR , logical images are considered in decreasing order of their past frequency val-ues. For each logical image currently being considered, we select the most frequently requested physical image associ-ated with that logical image, i.e., we cache a single phys-ical image I k i such that f 0 ( I k i ) f 0 ( I j i ), for all I 1  X  j  X  m . This heuristic aims to increase the physical cache hit rate while preserving high logical cache hit rates for physical images with lower resolutions (i.e., smaller sizes).
Most requested per byte ( MRPB ). Physical images are sorted in decreasing order of their size-normalized past fre-quency values ( f 0 ( I j i ) /S ( I j i )) and are admitted to the cache in this order. Normalizing frequency values with image sizes enables more e cient use of available cache space. Similar heuristics were previously used in other problem contexts, such as posting list caching [1], result caching [7, 14], and document replication [6, 10] for web search engines.
The ties are broken arbitrarily.
In this line of heuristics, instead of mainly relying on the observed request frequency of images, the greedy choice property prefers to select images that can ensure low re-sponse time, which is more inline with the objective of the static image caching problem defined in Section 2.3. Herein, we propose two gain-based heuristics. We note that di  X  erent gain-based heuristics were proposed before in the context of web search result caching [7, 14].

Largest gain per byte ( LGPB ). When making caching decisions, this heuristic prefers images that are expected to bring the largest reduction in response time over all image requests. Given a set R of statically cached images and a physical image I j i , we can estimate the total response time gain, G ( R ,I j i ), achieved by caching I j i as Here, we essentially compute the expected response time gain by only considering logical image I i . This is because caching physical image I j i may a  X  ect only the response time of requests for physical images ( I k i , for 1  X  k  X  m )associ-ated with logical image I i . The response times of requests for other physical images remain the same as none of those images can be obtained by resizing I j i . However, caching a physical image I j i may a  X  ect the response time gains esti-mated for other physical images associated with logical im-age I i . To be more specific, when the set R of cached images does not contain any physical image associated with logical image I i , the estimated response time gain for caching image I i for image I j i ( j&lt;k )is T CM ( I j i ) T LCH ( I j Once image I ` i ( ` &gt;k ) is cached in R , the estimated re-sponse time gain of caching image I k i for image I j i becomes T sponse time gain for each image in an iterative way, consider-ing physical images previously estimated to have higher pri-ority (i.e., those with larger estimated response time gains) for the corresponding logical image.

Similar to MRPB , the LGPB heuristic selects the images in decreasing order of their estimated response time gains nor-malized by the image size ( G ( R ,I j i ) /S ( I j i )) to enable more e cient use of the available cache space. The pseudo-code given in Algorithm 1 shows how the LGPB heuristic updates the estimated response time gain for each image and how it selects the images admitted to the cache. For every logi-cal image I i , we first compute the size-normalized response time gain for each of the physical images associated with I (Lines 3 X 10) and assign the largest gain to the correspond-ing image I j i (Lines 11 X 14). R i denotes the set of physical images that have been estimated to have high priority to be added to the cache for logical image I i . We then recompute the response time gains for every image I k i , where 1  X  k  X  m and k 6 = j , given that I j i has been added to R i (by repeat-ing Lines 6 X 15). We keep assigning the largest (updated) response time gain to the corresponding image and recom-puting the gain values for the rest of images whose gain values have not been assigned yet. We continue this process until a gain value is assigned to each of the physical images associated with the logical image (by repeating Lines 3 X 16).
The size-normalized response time gains for m physical images associated with a logical image can be computed by performing m  X  ( m 1) 2 updates. Therefore, the overall com-plexity to compute the response time gain for all n logical Algorithm 1 Gain-based caching heuristic ( LGPB ) images is O ( m 2  X  n ). Since an image usually has a dozen of di  X  erent resolutions in an image hosting service while a large-scale service may receive requests for millions of im-ages every day, we typically have m  X  n . Therefore, in practice, the complexity of the LGPB heuristic is comparable to the complexity ( O ( n )) of the frequency-based heuristics presented in Section 3.1.

Largest gain per byte with frequency adjustment ( LGPB-FA ). This heuristic is a variant of LGPB . Its key di  X  er-ence with LGPB , as well as the frequency-based heuristics, is in the way it estimates the future frequency of each request. The motivation stems from the observation that the past frequency of requests is not always strongly correlated with their future frequency, especially in the case of infrequent requests [7]. Table 1 shows the Pearson correlation coe -cient between the frequency of the physical image requests in the past (the training period of the experiments reported in Section 4.2) and the frequency of the same physical im-age requests in the future (the testing period of the exper-iments). As the request frequency follows a heavily skewed distribution, we group the requests according to their fre-quencies in the past and compute the Pearson correlation coe cient for each group respectively. We observe that the future frequency of a request is more correlated with its past frequency if it is a frequent query, and there is little corre-lation when a request only occurs a handful of times in the past. We further observe from Fig. 2 that the reason for the decreasing correlation at low frequency range is mainly be-cause the likelihood of not observing the infrequent requests in the future increases. In the figure, the requests with high frequencies are grouped by frequency to ensure each group has at least 100 requests to compute the probability. The adjacent two x-axis values represent the frequency range of the corresponding group.

To take this into account, we adjust the frequency estima-tion for computing the response time gain of caching image I ,i.e., G ( R ,I j i ) in Eq. (9), as follows. Instead of using the past frequency of each request directly in Eq. (9), we multi-ply f 0 ( I k i ) by the probability of the requests whose past fre-Table 1: Correlation between past and future fre-quencies of physical image requests Figure 2: The probability of a past request to reap-pear in the future with respect to its past frequency. quencies fall into the same frequency range [ a, b )as f 0 appearing in the future. The total response time gain due to caching of image I j i is now computed as where b f ( I k i ) denotes the future frequency of request for im-age I k i . The underlying intuition for this adjustment is that an image is only beneficial to maintain in the cache if it will appear again in the future.

The images to fill the cache are selected in the same way as LGPB (Algorithm 1) with the total response time gain of caching image I j i computed using Eq. (10). We note that, as shown in Fig. 2, the probability of a request ap-pearing at least 100 times in the past also appearing in the future is close to 1, we only need to compute the probability p ( b quency ranges. For instance, if we compute one probability for each frequency range [10 x , 10 x +1 ) for x 2, one probabil-ity for each frequency range [10 x, 10( x + 1)) for 1  X  x&lt; 10, and one probability for each frequency range [ x, x + 1) for each 1  X  x&lt; 10, at most 30 precomputed probabilities are enough to compute the largest gain with frequency adjust-ment. This adds a constant lookup overhead when comput-ing the response time gain per byte and does not increases the overall complexity of the gain-based heuristics, i.e., the complexity of LGPB-FA is still O ( m 2  X  n ).
As the dataset, we use an access log containing image download requests issued to an edge cache of Flickr during a period of three consecutive days. The entire log contains about 530 million requests. For each request, the access log Figure 3: Distribution of image sizes in bytes for each image resolution id. contains the id of the requested image (logical image), the id of the requested image resolution, the size of the requested image (in bytes), the timestamp of the request, and the IP address of the user who issued the request. We note that the id of the requested image and the id of the requested image resolution, together, uniquely identify a physical image. Each logical image is associated with 11 physical images. Di  X  erent image resolutions are denoted by ids s 1 , s 2 All physical images are obtained by resizing the original im-age uploaded by the user and stored in the backend image store. Table 2 shows the name and resolution information associated with each id. Since the aspect ratio of the origi-nal image is preserved during resizing, images with ids from s to s 11 may have variable pixel widths, which we denote by W .

The storage size of an image (in bytes) depends on the way the color is represented and the format of the image (e.g., PNG, JPG, etc.). In our case, all physical images are stored in JPG format, which uses lossy compression. Therefore, the storage requirement of images may be very di  X  erent even if their resolution is the same.

The box plot in Fig. 3 provides information about the distribution of image sizes (in KB) for each resolution id. The blue lines in the plot indicate the median storage sizes. The median values vary from several KBs to one MB in increasing order of ids. The boxes extend to the first and third quartiles while the whiskers extend to the 10th and 90th percentiles.

The bar chart in Fig. 4 shows the fraction of download requests with a particular image resolution id observed in the access log. According to the figure, about half of the requests are for s 6 (medium resolution images with a length of 500 pixels), which is the default resolution when serving Figure 4: Number of requests for each resolution id. images. The requests for other resolutions are usually due to the variation in user devices, the resolution of screens, particular user requirements on image quality, and available network bandwidth.

Fig. 5 shows the distribution of requests for logical and physical images with respect to the number of di  X  erent phys-ical image resolutions requested for a logical image. We ob-serve that, for more than 33% of logical images, at least two di  X  erent physical image sizes are requested. The number of requests for such images accounts for 60% of the total number of requests (for physical images). This implies that when on-the-fly resizing is enabled in the cache, it is pos-sible to improve the system performance for the requests for di  X  erent physical images of the same logical image by caching well-selected resolutions of that image rather than all possible physical images.
We use the first two days (training period) of the access log to compute the frequency of requests for each physical im-age. The probability of a past request in a given frequency range appearing in the future is computed by considering the first day as the past and the second day as the future. We then execute our heuristics using the corresponding fre-quency information to fill the cache with images. The re-quests in the third day (test period) are used to evaluate the performance of the caching heuristics. The test set contains about 155 million requests.

Our access log contains the size (in bytes) of each re-quested physical image. As shown in Fig. 5, often there are physical image resolutions that were not requested for a logical image. This means that for physical images that were not requested before, we need a mechanism to estimate their actual size in bytes. To this end, we use the following heuristic. If only one resolution s j of a (logical) image I was requested, the number of bytes for resolution s k ( k 6 = j ) of I k i is estimated as where n k denotes the number of images with resolution s k in the log. If more than one resolution of an image was requested, we use resolution s j whose index j is the closest to index k to estimate S ( I k i ) with the same equation. Our preliminary experiments show that using the closest index gives the most accurate estimation of the actual image size.
We define the cache size as a percentage of the total num-ber of bytes available in the backend image store. This def-Figure 5: Distribution of requests with respect to the number of unique physical images requested per logical image. inition takes into account the fact that a cache can be dis-tributed over a number of servers according to the backend size to make the system scalable. We assume that the cache is composed of an in-memory cache and a disk cache, which is widely used in modern caching systems. The cache size thus represents the total size of both caches. In our exper-iments, we assume that the backend image store maintains all 11 physical images associated with each logical image re-quested in the first two days of the access log. This results in a backend size of 287TB. Note that this assumption un-derestimates the size of the backend as the backend usually stores all images that were uploaded to the system in the past and is not limited to two days. Caching heuristics first fill the in-memory cache and then the on-disk cache until both caches are full.

We evaluate the performance of the caching heuristics us-ing a simulator written in Java. The simulator runs in a server with Quad-core 2.4GHz CPU and 48GB RAM. The information used by the simulator for selecting images to fill the cache (e.g., response time gain, frequency, sizes, etc.) is obtained by processing the training log using MapReduce jobs on Hadoop. This computation takes less than an hour for our training log and for each of the caching heuristics.
We use three di  X  erent metrics to evaluate the performance of the proposed caching heuristics.
An important variable in the experiments is the cache capacity. We set this parameter to 0.2%, 0.4%, 0.6%, 0.8%, and 1.0% of the total size of the backend image store. We set the sizes of the in-memory and on-disk caches as 1% and 99% of the cache capacity, respectively. This is a realistic setting since a cache server usually has tens of GBs of RAM and a few TBs of disk storage. In fact, given a fixed cache capacity, increasing the relative size of the in-memory cache does not change the overall miss rate of the cache, but only slightly reduces the response time for certain requests. Our preliminary experiments show that having a 10% in-memory cache reduces the average response time of a 1% in-memory cache by only 0.7%.

We compute the average time for resizing an image from size s i to size s j as follows: (i) download all 11 sizes of 100 random images from Flickr; (ii) repeat resizing each image from resolution s i to resolution s j 100 times for every (s pair with i&gt;j ; and (iii) compute the average time to resize the image from resolution s i to size s j . 6 The I/O times for reading the input image of resolution s i and writing the output image of resolution s j are not taken into account.
Fig. 6 shows the average time to resize an image from resolution s i to resolution s j . Images with resolution s not be resized to resolution s 1 since these images are not large enough in width (see Table 2). The average resizing time varies between 2ms and 186ms depending on input and output resolutions. We note that by using more advanced hardware, such as GPU, the resizing time can be reduced to less than 30ms. This increases the response time di  X  er-ence between logical cache hits and cache misses. We evalu-ate the caching heuristics without assuming the use of such advanced hardware. Therefore, the reported performance results are more likely to be lower bounds.

We use a cost model similar to those described in [13, 18] to compute the time for reading an image from the cache, i.e., T read ( I k i ) in Eqs. (2), (3), and (4). In the case of the in-memory cache, T read ( I k i ) is simply equal to the cache lookup
To resize the images, we use Ymagine, an open-source high-performance CPU-based tool. cost (set to 40ns). In the case of the on-disk cache, T read is computed as where D seek is the disk seek time (set to 5.80ms), D rotation is the rotational latency (set to 4.17ms), D block is the size of a disk block in bytes (set to 512 bytes), and D read is the average time to read a block from the disk (set to 4.883ns). The parameters are set to the same values as in [13] that are determined either empirically or by consulting the literature.
The time to transfer an image from the backend image store to the cache, i.e., T DR ( I j i ) in Eq. (4), is computed as where N latency denotes the network latency between the backend image store and the cache and N bandwidth denotes the network bandwidth between them. In the simulations, we assume that the backend is located in the U.S. and the regional cache is located in the U.S., Europe, or Asia. For these cache locations, we set the network latency between them and the backend to 25ms, 50ms, and 100ms, respec-tively. We also set the network bandwidth between the back-end and the cache to di  X  erent values: 0.1MB/s, 1MB/s, and 10MB/s. Table 3 shows the average response time, the 95th-percentile response time, and the 99th-percentile response time for the LGPB heuristic. Since a regional cache is more useful when placed in remote locations that are not close to the backend and the combination of 50ms network latency and 1MB/s bandwidth results in reasonable response times, we focus on this setting in our experiments.
In Fig. 7, we first compare the memory physical hit rate, disk physical hit rate, memory logical hit rate, and disk log-ical hit rate metrics for every caching heuristic and for vary-ing cache capacity values. We observe that MRPB achieves the highest memory physical hit rate and disk physical hit rate. This is not surprising as MRPB is among the best poli-cies in the literature in achieving high hit rates. MR also achieves high hit rates as it selects the most frequently re-quested physical images to fill the cache. Yet, given that several physical images of the same logical image may be requested frequently and with similar likelihood, caching only one of them misses some potential hits and thus re-sults in lower memory/disk physical hit rates than those of MRPB . The decrease in the hit rates of MR is partially com-pensated by resizing the cached physical images in both in-memory and on-disk caches as shown in the figure. In con-trast, LR results in the highest memory and disk logical hit rates, but the lowest memory and disk physical hit rates among all heuristics. This is because LR selects the largest requested physical image of the most frequently requested logical images to increase the likelihood of obtaining most of the requested physical images of these logical images in the cache (through on-the-fly resizing) rather than in the back-end image store. Di  X  erent from all these baseline policies, the proposed caching policy LGPB and LGPB-FA achieve more balanced physical and logical hit rates between the two ex-tremes that target high physical hit rates and high logical hit rates, respectively. LGPB-FA is slightly more in favor of Figure 8: Number of images in the in-memory cache for di  X  erent caching heuristics and di  X  erent cache capacity values. cache hits than resizing as the past frequencies of infrequent requests, which are more likely to benefit from resizing, are discounted more by the likelihood they may appear in the future. Depending on the size of the cache, from 39% to 51% of all requests can be served by the cache, being physi-cal or logical hits, using LGPB-FA . This is up to 9.7% higher than the amount of requests that can be served by a static cache constructed using the best frequency-based caching heuristic.

Figs. 8 and 9 further show the number of physical images stored in the in-memory and on-disk caches, respectively. We observe from both figures that, given the same cache capacity, LGPB-FA , LGPB , and MRPB admit much more images into the cache than MR and LR . This is because when selecting images with LGPB-FA , LGPB ,or MRPB , the response time gain or frequency of each image is divided by its size. They are thus more likely to add small images into the cache given the same response time gain or frequency. LGPB-FA feeds the cache with fewer images than LGPB . Since the frequency of each image is adjusted by its likelihood of appearing in the future, only images that have high likelihood of appearing again and high past frequency are selected. LGPB-FA thus ensures more e  X  ective use of the cache capacity than LGPB . Figure 9: Number of images in the on-disk cache for di  X  erent caching heuristics and di  X  erent cache capacity values.
In Fig. 10, we first compare the average response time of each caching heuristic for varying cache capacity values. As shown in the figure, by using the proposed caching heuristics LGPB-FA and LGPB , the average response time is reduced by up to 5.4% and 4.3%, respectively, compared to MRPB . In fact, independent of the cache capacity, LGPB-FA and LGPB consis-tently perform better than the best frequency-based heuris-tic and reduce its average response time by up to 4.2% and 3.1%, respectively. LGPB-FA leads to lower average response time than LGPB especially when the cache capacity is low. This conveys the e  X  ectiveness of the frequency adjustment as selecting the right images is more crucial under lower-capacity constraint. More importantly, while achieving sim-ilar average response time (e.g., around 115ms), LGPB-FA and LGPB considerably reduce the capacity of the cache (by about 25%). This implies a large reduction in the hard-ware investment needed for the caching system without any degradation in response latency and hence user experience.
Fig. 11 shows the cumulative distribution of response time for di  X  erent caching heuristics for a fixed cache capacity (set to 0.4%). We observe that MR and LR result in more requests to be served within 30ms, but fewer requests are Figure 10: Average response time for di  X  erent caching heuristics and cache capacity values. Figure 11: Cumulative distribution of response time for di  X  erent caching heuristics (cache capacity: 0.4%). served between 30ms and 100ms, compared to the proposed gain-based caching heuristics LGPB-FA and LGPB . As shown in Fig. 7, this is because MR prefers to select images that lead to direct hits in the cache while LR prefers to select large images to fill the cache so that more requests can be served through resizing. Although both physical hits and logical hits usually take less time than cache misses, the limited number of images in the cache filled by these two heuris-tics (Figs. 8 and 9) forces more images to be fetched from the backend, leading to high response time for more requests than LGPB-FA and LGPB . Finally, we observe in the inner plot in Fig. 11 that LGPB-FA and LGPB serve 57.5% and 56.9% of the requests under 140ms, respectively, outperforming all frequency-based caching heuristics.

Fig. 12 shows the cumulative distribution of response time for the LGPB-FA heuristic for varying cache capacity values. We observe that larger cache capacity values lead to lower response times. For instance, by using a cache capacity of 1.0%, more than 30% of the requests can be served under 100ms compared to using a cache capacity of 0.2%.
We quantify the cost of the backend communication us-ing the amount of data transferred between the cache and the backend image store. Fig. 13 shows the average num-ber of bytes fetched from the backend to serve a request. We observe that LR always incurs the lowest communica-tion overhead, and on average, it fetches at most 2.3% less bytes from the backend for each request than the gain-based caching heuristics LGPB-FA and LGPB . This is because, with LR , the requests for di  X  erent physical images associated with Figure 12: Cumulative distribution of response time for LGPB-FA with varying cache capacity values. Figure 13: Backend communication cost for di  X  er-ent caching heuristics and di  X  erent cache capacity values. the most frequently requested logical images can be resized on-the-fly in the cache rather than being forwarded to the backend. Despite its lower backend communication cost, as we have observed in Fig. 10, it takes more time to serve a request. This is because resizing a large image may itself be very costly (e.g., converting an image from resolution s 11 resolution s 10 as shown in Fig. 6). Finally, we observe that LGPB-FA and LGPB incur up to 8.9% and 7.0% lower commu-nication overhead than the other frequency-based caching heuristics MR and MRPB , respectively.
Caching has been an active research topic for many years, and hence the previous work on caching is vast. While ear-lier works were focused on more fundamental areas, such as operating systems [16] and databases [5], more recent works in the Web era have investigated the use of caching in ap-plications such as web servers [15, 19] and search engines [4, 13]. Media access patterns in the Internet were investigated in [8]. Herein, we prefer to keep our survey brief and limit it to the related work on static caching and image caching.
Static caching has found application especially in web search engines [1, 11, 13]. Cost-based heuristics were pro-posed for static caching of web search results, aiming to reduce the query workload of backend search systems [7, 14]. The gain-based heuristics we proposed in this work dif-fer from earlier cost-based heuristics in the way they try to reduce the average response latency. Moreover, their implementation requires iterative computation of the gains since the response times depend on which images are al-ready cached. A brief survey of caching techniques used in web search engines can be found in [3].

Regarding image caching, the only work we are aware of is [9], which presents the photo serving stack of Face-book. According to [9], Facebook has a highly distributed infrastructure involving client-side caches, edge caches, ori-gin caches, and data centers. 7 This architecture is slightly di  X  erent from the architecture we presented in this paper due to the presence of client-side caches and lack of regional proxies. While the hit rate of client-side caches is reported to be 65.5% for Facebook, the contribution of these caches is negligible in our application (as most client-side hits of Face-book come from the cached profile photos of users X  friends) and hence they are omitted. Another di  X  erence is that the work in [9] considers dynamic caching techniques while we explore static caching heuristics. More importantly, in terms of cache performance when using image resizing, their ob-jective is to improve the cache hit rate, which may not be ideal as the cost of responding to an image request upon a cache hit may be very di  X  erent (in contrast to result caching in web search engines). Indeed, as our experiments indicate, high hit rates do not always ensure low response times. For interested reader, some detail on the backend photo storage system of Facebook can be found in [2]. Facebook X  X  newer large binary object storage system is introduced in [12]. A recent work on photo caching in flash storage systems of Facebook is available in [17].
In this work, we formalized the static caching problem for large-scale image serving systems and proposed two caching heuristics that aim to improve the average response time. Extensive experiments using real-life access logs obtained from Flickr demonstrated that the proposed heuristics re-duce the image serving time and the system workload in terms of backend communication cost compared to simpler, frequency-based caching heuristics. More importantly, the proposed heuristics were shown to require much lower cache capacity than the baseline heuristics to achieve comparable response times, potentially leading to significant hardware savings when building image caching systems. A possible ex-tension that may further improve the performance of such systems would be to investigate the request dynamics and propose similar heuristics for dynamic caching. [1] R. Baeza-Yates, A. Gionis, F. P. Junqueira, [2] D. Beaver, S. Kumar, H. C. Li, J. Sobel, and [3] B. B. Cambazoglu and R. A. Baeza-Yates. Scalability
The origin cache is distributed across the data centers. [4] B. B. Cambazoglu, F. P. Junqueira, V. Plachouras, [5] W. E  X  elsberg and T. Haerder. Principles of database [6] G. Franc`es, X. Bai, B. B. Cambazoglu, and [7] Q. Gan and T. Suel. Improved techniques for result [8] L. Guo, E. Tan, S. Chen, Z. Xiao, and X. Zhang. The [9] Q. Huang, K. Birman, R. van Renesse, W. Lloyd, [10] E. Kayaaslan, B. B. Cambazoglu, and C. Aykanat. [11] E. P. Markatos. On caching search engine query [12] S. Muralidhar, W. Lloyd, S. Roy, C. Hill, E. Lin, [13] R. Ozcan, I. S. Altingovde, B. B. Cambazoglu, F. P. [14] R. Ozcan, I. S. Altingovde, and O. Ulusoy. Cost-aware [15] S. Podlipnig and L. B  X  osz  X  ormenyi. A survey of web [16] A. J. Smith. Cache memories. ACM Computing [17] L. Tang, Q. Huang, W. Lloyd, S. Kumar, and K. Li. [18] A. Trotman. Compressing inverted files. Information [19] J. Wang. A survey of web caching schemes for the
