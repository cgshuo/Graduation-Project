 Businesses and large organizations accumulate increasingly large amounts of customer interaction data. Analysis of such data holds great importance for tasks such as strategic planning and orchestration of sales/marketing campaigns. However, discovery and analysis over heterogeneous enter-prise data can be challenging. Primary reasons for this are dispersed data repositories, requirements for schema knowl-edge, and difficulties in using complex user interfaces. As a solution to the above, we propose a TEmplated Search paradigm (TES) for exploring relational data that combines the advantages of keyword search interfaces with the expres-sive power of question-answering systems. The user starts typing a few keywords and TES proposes data exploration questions in real time. A key aspect of our approach is that the questions displayed are diverse to each other and optimally cover the space of possible questions for a given question-ranking framework. Efficient exact and provably approximate algorithms are presented. We show that the Templated Search paradigm renders the potentially complex underlying data sources intelligible and easily navigable. We support our claims with experimental results on real-world enterprise data.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval keyword search, query recommendations  X 
The research leading to these results has received funding from the European Research Council under the European Union X  X  Seventh Framework Programme (FP7/2007-2013) / ERC grant agreement no. 259569.  X 
Dr. Hristidis was partially supported by NSF grant IIS-1216007 and a Samsung GRO grant.

Enterprises nowadays are sitting on billions of dollars X  worth of data equity. Historical transaction data (e.g., prod-ucts bought) and past interactional data with the client (e.g., responses to marketing campaigns) can offer valuable in-sights on the future buying preferences of the client. How-ever, distilling useful information from these data reposi-tories can be particularly challenging, especially for non-technical users. We can attribute this to three major fac-tors: -Access Issues : There exists a disparity between deci-sion makers and data analysts that severely hampers direct access to data. Although strategy and business insights are driven by upper-level management, data access is serviced by the IT division of an enterprise. Therefore, gleaning the useful information may require several progressive rounds of requests between decision makers and data accessors. These multiple iterations reduce the efficiency of an organization, incurring unnecessary costs and time delays. -Data Issues : Relating to the above, querying the data requires extensive knowledge of the underlying data and their schema to properly formulate the queries. Additional technical challenges that have to be addressed pertain to: (a) entity resolution problems (e.g., different client names of same entity per country) and (b) data scale. -Interface Issues : Even when simple graphical inter-faces do exist and offer data access to non-technical users, as the user interface progressively becomes more feature-rich, it will also become more difficult to operate. This induces a steep learning curve to adapt to new user interfaces. In addition, the various data tools cater for the needs of mul-tiple groups of people with diverse access interests to the data. The end result is that the users only use a small frac-tion of an interface X  X  functionality, but are exposed to its full complexity.

In this work, we propose a guided keyword search mech-anism for retrieving information over relational databases Given a database schema, the proposed search mechanism receives as input keywords from the user such as ( X  iphone  X  or  X  Boston  X ) and the system generates a list of queries that are valid on the given database schema, such as:  X  X ll clients who purchased an iphone  X ,  X  X ll clients located in Boston  X , or similar queries corresponding to the keywords. Com-pound queries can be formed such as  X  X ll customers who bought iPhone and are located in Boston  X  if both keywords
The proposed framework is applicable to any type of data source that consists of entities and relationships between them.  X  X phone X  and  X  X oston X  are inputs. These natural-language queries should be generated by the system and also seman-tically map to valid SQL queries on the database.

End users want to receive answers to their questions with-out delving into details of the database schema, a require-ment for issuing SQL queries, or learning how to use rigid and inflexible user interfaces [3]. Therefore, it is advanta-geous to put forward a new search paradigm that guides the user and eases the exploration and correlation over hetero-geneous data sources.

In TEmplated Search or TES for short (Figures 1 and 7), queries are formulated using a simple textual descrip-tion, allowing the user to ask queries like:  X  X ll clients who bought product X  X  or  X  X ll clients with expiring contracts for product Y X  which we call templates . Queries consists of both static text that is predefined (i.e., customers, contracts, who, product, etc.) and dynamic text that is retrieved from the database. Moreover, the original query primitives (tem-plates) can be compounded to form increasingly more com-plex query instances. We address the challenges that we identified above, in the following ways: -Using the technology presented, the user does not need to issue SQL queries or even have (complete) knowledge of the underlying schema. The universe of potential queries is captured in templates, and can be augmented as the system matures. -The system makes interactive recommendations about valid queries on the data, based on the templates created. The templates can be combined to articulate more elabo-rate query functionalities. The query recommendations are finally ranked so as to satisfy: relevance , coverage and di-versity . Note that the keywords that the user inputs as part of the query need not necessarily exist in the same tuple or even in the same table of the database. -To limit the interface complexity, we also use templates for the various visual components. The user is only pre-sented with a simple interface that is relevant to the partic-ular question. If the user asks for  X  X ontracts about Client X X  , then only a list of relevant contracts will be shown. In contrast, when the question is concerning  X  X roducts bought by Client X X  , a different view will be displayed that only captures the relevant information.

Here, we focus on relational datasets, mainly because most mature enterprises store historical data in relational tables. However, the notions we present are directly ap-plicable on any other data-storage infrastructure such as a graph database [31]. Our system is primarily targeted to-wards non-technical decision makers who would like to have direct query access to enterprise data.

Our work makes the following contributions : 1) We propose a tree-based query generation mechanism 2) We present an O ( n 2 log( n ) log( k )) algorithm for top k
Given a database schema that consists of a set of relation and their connections through primary-foreign keys, our goal is to search through the database with a simple keyword search. We propose a guided search mechanism that, given the user X  X  keywords, generates a list of valid questions in natural language format, where by valid we mean that they correspond to syntactically correct SQL statements. One such example could be:  X  X All clients] [who bought] [product X] X  (without the brackets), see Figure 1. TES will provide recommendations based on the keywords posed. The only assumption that we make about the user is that he/she has only a high-level knowledge on the entities that exist in the database (i.e., clients, contracts, products) and their rela-tionships (i.e., clients bought products, etc).

An important design question is how one defines the query generation mechanism given a database schema. We propose a tree-based query generation mechanism (see Figure 3 for an illustrative part of the tree-based mechanism). Given a database schema, we define a rooted tree in which each node is associated either with static text (i.e.,  X  X ll clients X ,  X  X ho bought X  together with a list of synonyms) or with text that is dynamically populated from the database (i.e.,  X  X roduct X X ,  X  X roduct Y X ). Syntactically, each rooted path models a query text by concatenation of its node text, therefore all possi-ble queries generated are defined as the union of all rooted paths of the tree. Note that the set of possible queries is not equal to the number of distinct paths, but grows with the number of entries in the database X  X  relations. Finally, each rooted path semantically corresponds to a specific query on the given database.

For the remainder of the paper, we will assume the database schema in Figure 2 2 . In this schema, there exist
It is possible to use a graphical database as the underlying schema without altering any of the statements in this paper. Figure 2: A simplified schema that we consider as our work-ing example throughout the paper. Links correspond to primary-foreign key dependencies. clients , products , contracts on products (e.g., maintenance), opportunities (a salesperson identified a product selling op-portunity for a product) and contacts (people within the client organization with whom we interact, e.g., CEO, CMO, etc). Other potential tables/entities include: news , which correspond to recent news events about the client assembled from RSS feeds and Twitter, and recommendations , which stores the result of elaborate propensity models regarding the probability of a client to express interest in a particular product (consider, for example, recommendations as offered in Netflix). The schema also comes with additional infor-mation such as where each entity is stored, and also with potential synonyms for the entities and their attributes. For example, keywords such as clients and customers are syn-onyms and are mapped to the node (entity) client . Fi-nally, all the text in all columns of interest in the database is indexed to support approximate or prefix search over all textual entities.

As input we also assume a set of template queries that we would like to answer on the system. For the above schema, a representative subset can be:
The templates consist of static nodes indicated in gray background color, and dynamic nodes , shown in white color, which can be populated from a column of a table in the database. In the above examples, Contracts of corresponds to a static node, whereas client.name is a dynamic node. In essence, a query template encapsulates a class of potentially infinite queries. The templates are also accompanied by a valid parameterized SQL statement, that guides their evalu-ation on the database. Templates and their respective SQL statements are derived via analysis of the database query logs [29]. We do not further focus on this important research aspect, but direct the interested reader to related work [21, 37, 5].

While templates are few in number and answer simple queries, they may be combined, based on primary-foreign key relationships of the data entities, to formulate more complex queries. For example, the user can ask a question:  X  X ll clients with expiring contracts for System P who bought SPSS X  , which combines two of the above templates. Tem-plates as the above can be combined with AND , OR and NOT operators for formulating increasingly more complex state-ments.

Given the list of templates, the schema and the data, we need to generate valid query recommendations interactively and rank them accordingly. This is the focus of the upcom-ing sections.
 Figure 3: A simplified example of the tree-based query gen-eration mechanism. For the sake of presentation, we demon-strates only 3 paths that have distinct return relations. Example: Let X  X  assume that the query  X  X acker X  is posed (see Fig. 3). The system will try to determine whether this matches approximately some entry in the database, the static part of a query template, or both. The query matches two records at the attribute name of the entity client :  X  X acker-Chemie X  and  X  X acker Neuson X . It also matches the address  X  X ackersdorfer X  of some client entity. Based on the above, an instance of the queries generated (query recom-mendations) is shown in Figure 3.

In what follows, we formally define the tree-based query generation mechanism (Section 2.2). It consists of two main steps: (a) Given user X  X  keywords, generate a list of  X  X alid X  question (b) Rank the valid questions/paths to satisfy three essential
Now, we formally define the tree-based query generation mechanism, which we call Query Tree. We construct a rooted tree that consists of a set of  X  X emplate paths X . A minimalistic example of a Query Tree is shown in Figure 3.
Definition 1 (Query Tree). Given a database schema, a Query tree (QT) is a rooted and directed tree T = ( V,E ) together with a labeling function L : V  X   X   X  7 X  ( X   X  ,  X   X  ,... ) where  X  is a fixed alpha-bet. QT has the following properties: 1. Root is  X  X tart X  node. 2. Children of start are all relations of the schema. 3. Each relation may have as children (or descendants) 4. AND , OR , NOT are special connector nodes that combine 5. Each v  X  V is a static or dynamic node. If v is a 6. Dynamic nodes correspond to a ranked subset of tuples In the above definition, the labeling function is required to frame the functionality of the dynamic nodes. For exam-ple, in Figure 3, q =  X  X acker X  and let u be the dynamic node corresponding to client.name , then L ( X  X acker X  ,u ) = { Wacker-Chemie AG , Wacker Neuson ,... } . The top-level static nodes (children of start) define the type of the re-turn relation , i.e., whether what will be returned is a client or a news element, etc. The lower-level nodes are in essence condition relations because they are used to constrain the re-sults of the return relations. Each valid query generated may have one or more condition relations. This can be achieved using connector nodes such as AND , OR and NOT .
 Example: Here, we explain the above terminology and demonstrate that each query contains all the information necessary for retrieving the semantically correct result set. Assume the query:
The system will parse the query and syntactically identify the following: (1) return type is a list of clients (based on the All clients static node); (2) client.city= X  X erlin X ; (3) product.name= X  X PSS X ; (4) fetch the client tuples that satisfy (2) and (3) as the connector is AND , i.e., clients with contracts for System P who also purchased SPSS.

Once the Query Tree is formed, we can use it to generate a set of relevant questions given a partial query from the user. The scope here is to provide variations on the current query, and also suggest ways of augmenting the query to form more complex and valid questions on the data.
 Problem 1 (Questions Formulation). Given a Query Tree and user keywords, create a list of valid questions (or paths).
 There are several challenges with Problem 1. Namely, there might be a match on schema and/or an instance or a tuple. For example, the keyword  X  X lients X  might match the static node  X  X ll clients X  but might also match a client tuple with the name  X  X lient Corporation X .
Here, we discuss how we compute the set P of all valid questions for Q with respect to a Query Tree. When we refer to a path on the Query Tree, we imply that it is rooted at  X  X tart X .

Definition 2 (Valid Path). Given a Query Tree T and a query Q, a path (question) p is valid if (i) p (approx-imately) matches at least one keyword in Q , and (ii) p is minimal, that is, removing the last node of p would decrease the number of keywords matched in condition (i).
 First, in the above definition, condition (i) is loose so that a large number of paths is generated. Second, condition (ii) is used so that very long paths will not be included in the valid set of paths. The valid path generation procedure op-erates in two steps and prioritizes the search over the static text nodes first. In the first step, the procedure matches keywords from Q with only static text nodes. If there is an exact match with a keyword in this case, the keyword is removed; otherwise not. In the second step, the remaining keywords are used for populating all the dynamic nodes. Re-call that for each dynamic node of the Query Tree, there is a corresponding attribute (or more) of some relation. Based on the text characteristics of any attribute (i.e., short com-pany name, postal code, city, etc), we construct an appro-priate text index over all tuples. Here, the search is based on a combination of approximate, prefix, edit distance and phonetic matching. In both steps, we store the information retrieval score between the query text and the node text for each node.
We describe a few properties and characteristics of paths, because paths will be the core object of study in this paper. At the end of this subsection (Section 2.4.3), we propose a simple adjustment of the MMR diversification criterion to take into consideration a graph theoretic covering the property of paths, i.e., favor shorter paths which potentially contain relevant longer paths.
Given two paths (questions) starting from the root node p = root  X  t 1 and p 2 = root  X  t 2 , our framework allows a restricted type of distance between p 1 and p 2 . Namely, for a fixed tree T with arbitrarily positive weights on its edges, we define their distance as where t 1 and t 2 are the final nodes of p 1 and p 2 , respec-tively and also treeDist( t 1 ,t 2 ) is the tree (or shortest path) distance 3 between node t 1 and t 2 .

At this point, the above restriction on the tree metric might seem unnecessary. However, we will see later in Sec-tion 3 that such a restriction is crucial for the development of our algorithmic solutions.
First, we define the IR score of a given node (dynamic or static). For a dynamic node v and a query Q , L ( v,Q ) will return a ranked list of string corresponding to an attribute of some entity. The score of a (dynamic or static) node v , nodeScore( v,Q ) equals the information retrieval score re-turned in the valid question generation step.

Our ranking formula is an adaptation of the formula in [19]:
The shortest path distance between two nodes of a tree is defined as the sum of the edge weights over their connecting path. where size( p ) is the number of nodes in p and nodeScore( w,Q ) is an appropriately chosen IR scoring func-tion depending on its corresponding data source text charac-teristics; by default we use Okapi BM25 [23]. More precisely, if node v is a relation, then we define IRScore( v,Q ) to be the maximum IR score of an attribute in the relation with respect to Q . If v is a select or project node, then we con-sider its text as a string (document) and compute the IR score as usual, i.e., BM25. Other ranking methods are also possible [35, 16]. Observe that the contribution of nodes that are far from the root node decreases linearly with their node distance.
 Example: Figure 4 provides an example where Q is  X  X olvo clients bought SPSS X . Here, three paths are displayed, most importantly the path r  X  v , for which the score computa-tion is expanded at the bottom panel of the figure. Another important characteristic of a path p that we would like to incorporate in our scoring function is the relevance of its descendants. The relevance of all descendants of a given partial path can be viewed as a covering property of a path. Namely, a path that has high relevance over its descendants should be more favorable to be included in the result list. For this reason, we defined the coverage score of a path p :
CovScore( p,Q ) := X where Desc( p ) is the set of all descendant paths of p . The covering condition is an important property on paths as it guides the user towards more relevant paths.
 Figure 4: Information retrieval scoring and distance of paths. nodeScore( h,Q ) is zero and therefore not included.
Now, we combine three aspects of any rooted path when one searches over the Query Tree: (i) relevance , the IR rel-evance of the path with respect to the query, (ii) coverage , the importance of all descendants of a sub-path (as the rele-vance of its descendants), and (iii) diversity , the dissimilarity or distance between two paths.

In most cases, one can expect the result set of valid poten-tial questions to be huge. Therefore, a ranking criterion has to be decided. Returning the top-k valid questions with the highest score may not be the best approach, because they may be very similar to each other. So, it is important to incorporate a diversity factor.

Computing diverse and relevant results is a rapidly grow-ing and active line of research in the database community [6, 17, 11, 14]. Most of the approaches are based on ranking documents/webpages given a query. Here, the main concep-tual difference to prior work is that we diversify and rank queries instead of results.

In contrast to the diversity of web documents, where se-lecting a set of relevant and diverse results might be a satis-fying solution [14], in our setting we must ensure that almost all questions can be (approximately) reached from the set of k questions presented, i.e., a covering condition must be sat-isfied by the selected paths. Past work has also examined how to incorporate topic coverage into a diversity function when the topics are organized as a hierarchy [38]; the intu-ition is similar to ours if we replace topics by queries, except that we compute the score based on the queries selected and not based on the results returned (which may be associated with several topics in [38]).

Search result diversification is a bi-criterion optimization problem in which one seeks to maximize the overall rele-vance of the document X  X  ranking, while minimizing the re-dundancy on the documents returned. In general, the above bi-criterion problem is formulated as an intractable max-imum coverage problem, and hence most algorithmic ap-proaches are based on greedy solutions that provide only approximate solutions [17].

We adopt a variant of the well-accepted maximal marginal relevance (MMR) method of Carbonell and Goldstein [6] whose diversity and relevance balancing formula aims to identify k elements S (paths in our framework) that maxi-mize: MMR-Path( Q,S ) = min where CovScore( p,Q ) is an arbitrary score of p given key-words Q ; | S | = k , dist(  X  ,  X  ) is a similarity measure between documents, and  X  &gt; 0 is a parameter specifying the trade-off between relevance and dissimilarity within S . This bi-criteria objective maximizes the minimum relevance and dis-similarity of the chosen paths S (e.g., see Figure 5). Other diversity definitions have also been proposed, e.g., [12, 15]. The problem can be casted as follows:
Problem 2 (Questions Ranking). Given a set of valid questions/paths in the Query Tree and user keywords, compute k diversified questions/paths that maximize Equa-tion (4) .
In the literature of document diversification, there is a well-understood connection between document diversifica-tion (using the maximal marginal relevance [6]) and dis-persion problems on graphs [17]: diversification problems are typically reduced to a graph problem, known as disper-sion problem [18, 24]). Here we follow the same approach, but with an essential difference to prior work: we diversify paths of trees . The simplicity of tree structures, as opposed to general graphs, allows us to benefit in terms of both accu-racy and efficiency. We present two provably accurate algo-rithms for Problem 2. Algorithm 1 is an O ( n 2 log( n ) log( k )) Figure 5: Input keywords are  X  X acker contracts X  and k is set to 9. Font size is proportional to IR score for each node. algorithm for top-k ranking of n paths that optimally solves the path-ranking problem. The algorithm discovers opti-mal solutions, but its quadratic dependence on the num-ber of paths makes it prohibitive for our application; we seek for interactive response times on the order of millisec-onds. To overcome this drawback of Algorithm 1, we also present a scalable, O ( nk 2 ) time, 2-approximate algorithm (Algorithm 2) .
 Table 1: Summary of provably accurate algorithms for the maximal minimum dispersion problem on general graphs and trees.

In Table 1, we put our results into context. The last two rows correspond to our contributions. The first row shows that the problem is NP-hard in general graphs. The second row corresponds to the greedy 2-approximate algorithm for general graphs (note the quadratic dependency on n ). The third row shows that the problem can be solved optimally on trees, whereas the last row demonstrates that compu-tational savings can be achieved on the restricted case of trees. Therefore, the idea is to reformulate Problem 2 as an instance of the maximum minimum dispersion problem [18, 24] over trees.

First, we revisit the MaxMin dispersion problem, which is an intractable problem, i.e., NP-hard, in general graphs [7].
Problem 3 (MaxMin Dispersion Problem). Given a positive integer k , a graph G = ( V,E ) with n = | V | and positive edge weights, compute a S  X  V of size k that maximizes where dist ( u,v ) is the shortest path distance on G .
Next we show how to reduce the objective value of Equa-tion (4) to Equation (5). The reduction works by modify-ing the distance function dist(  X  ,  X  ) to incorporate the scoring term of Equation (4) into the distance function. However, it is important to note that the results of Table 1 are in terms of nodes of a graph, they are not related to rooted paths as discussed here. Given the Query Tree restricted on the set of valid paths P , we identify each node v  X  V with its corresponding path starting from the start node of the Query Tree and ending at v , i.e., start  X  v . The following lemma makes formal the connection between Equation (4) on paths and the MaxMin dispersion problem on the under-lying Query Tree.
 Lemma 1. Let P be a set of valid paths over the Query Tree, Q be a query and k be an integer with 1  X  k  X  |P| . Algorithm 1 computes a set of size k that opti-mally maximizes Equation (4) over all k -subsets of P in O ( |P| 2 log( |P| ) log( k )) .
 Proof. For simplicity of notation, let w ( p v ) be equal to CovScore( p v ,Q ) as in Equation (3). First, observe that all paths have the same starting node, i.e., the start node of the Query Tree. Now, identify each path with its final node, i.e., the path p v := start  X  v is identified with node v . Naturally extend this identification to set of paths. Given P , let T = ( V,E ) be the tree that contains the union of all nodes and edges contained in the set of paths P . Moreover, for each e = ( u,v )  X  E , define d T ( u,v ) := 1 2 ( w ( p u  X   X  dist( p u ,p v ).

Now, note that for any P S  X  X  , it follows that min using the definition of d T and w (  X  ) in the first and second equality, respectively. The last equality follows by Equa-tion (4). Now the MaxMinDispersion problem can be solved exactly on T in O ( |P| 2 log( |P| ) log( k )) time [8, Section 2]. Using Equation (6), the resulting nodes of the algorithm can be translated into a set of paths P S of size k that maximize Equation (4) over all subsets of size k of P .
 Algorithm 1 solves the diversity ranking problem (Prob-lem 2) optimally 4 . Algorithm 1 depends on the procedure BoundedDisperse , which solves a decision version of Prob-lem 3, i.e., given T ,d,k and  X  , it returns S of size k such that cost( S )  X   X  . If such an S does not exist, it returns failure . Algorithm 1 uses Algorithm BoundedDisperse together with binary searching to compute the optimal objective value of Problem 3 with sufficient accuracy (Step 3), and then in-vokes Algorithm BoundedDisperse once more to return an optimal k -set S . Algorithm BoundedDisperse as presented here is a simplification of the implicit algorithm presented in [8, Section 2].
Now we show that the time complexity of the greedy algo-rithm (second row of Table 1) for solving Problem 3 on trees can be significantly improved. The main observation is that during the execution of the greedy algorithm only the dis-tances between the facilities and all other nodes are required.
Recall that the problem is NP-hard for general graphs. Algorithm 1 MaxMin Dispersion Algorithm on Trees Figure 6: A cluster C ( s,t ) of a 4-node tree (line 8 of Alg. 1) Moreover, the distance between any fixed node and all other nodes in a tree can be computed by a breadth-first search (BFS) in O ( n ) time, see updateMetaData in Algorithm 2.
Lemma 2. Given a tree T = ( V,E ) with n = | V | , an integer k &gt; 1 , Algorithm 2 returns a 2 -approximate solu-tion to the maximal minimum dispersion problem on T using O ( nk 2 ) operations.

Proof. The UpdateMetaData procedure is a breadth-first search which computes the distance between s and every other node in T . Hence, UpdateMetaData requires O ( n + | E | ) = O ( n ) time. Step 4 requires O ( nk ) time to compute the minimizer. In total, the algorithm re-quires O ( nk 2 ) operations. As the algorithm is an instan-tiation of the greedy algorithm, its output solution is a 2-approximation [30, Theorem 2].

Remark 1. The running time of the greedy algorithm on general graphs is  X ( n 2 ) if only the edge weights are given, because an all-pairs shortest path computation is required to compute all node pairwise distances.
For the experiments, we use real enterprise data from our host institution conforming to an expanded version of the schema in Fig. 2. The data cover a holistic view of clients including transactional and interactional data for a partic-ular geography of our enterprise. To support approximate Algorithm 2 Fast Greedy MaxMinDispersion on Trees search over all textual entries, text is indexed using Apache Lucene/SOLR [2]. Search over the dynamic nodes of the Query Tree is based on a combined weighted sum of approx-imate, prefix, edit distance and phonetic match. Our test system contains a total of approximately 16 million indexed entities. The index size of the textual terms was on the or-der of 20.2 GBytes. In Figure 7 we show snapshots of the Graphical Interface of the system.
 Path Scoring and Selection: First, we qualitatively demonstrate the effectiveness of our framework using a few representative examples. Given a set of keywords and a pos-itive integer k , we perform the following steps: First, using the input keywords, we generate the set of all valid paths. Then, we use Algorithm 1 to highlight k paths that maxi-mize the objective function defined in Section 2.4.3. Figure 8 demonstrates the generation of the Query tree and the paths selected for a progressively formed query search. Figure 5 depicts an actual query on the system with keywords  X  X acker contracts X  and k = 9, and also shows that (i) a diversified set of paths is selected, and (ii) dynamic nodes client.name client.address and client.city have been populated. Evaluation: Here, we present the experimental evalua-tion of the diversification algorithms we proposed in Sec-tion 3. We compare the efficiency in terms of running time for three diversification algorithms: OPTtree which is Al-gorithm 1, FastGreedy , which corresponds to Algorithm 2, and Greedy , which corresponds to a naive implementation of the greedy algorithm on trees, i.e., brute-force computation of all pairwise distances of the tree. Moreover, we evaluate Algorithm 1 in terms of the average path score (relevance) and average path distance (diversity).

Figure 9 depicts the running time of the algorithms versus the number of nodes selected for a tree with 1350 nodes, a maximum depth of 9 and 582 leaves. This figure suggests that both greedy solutions are superior to OPTtree . More-over, Figure 9 demonstrates the quadratic dependency of the greedy algorithms for large values of k . Figure 10 depicts the running time of the algorithms for k = 25 and trees having size between 30 and 1350 nodes. This figure confirms the scalability of Algorithm 2 for constant values of k . Parameter setting: Figure 11 depicts the average shortest path distance over k selected paths using Algorithm 2, see diversification as in Equation (4). Figure 9: Elapsed time versus number of paths selected ( k ) for a generated tree with 1350 nodes, 582 leaves, and maxi-mum depth of 9.
 Equation (1). It is evident that as k increases the average distance decreases after peaking out. This peak happens at the same value of k  X  20 for all values of  X  depicted. This suggests that a good value of k for our system is approxi-mately 20. A similar experiment can be used to tune one X  X  system before deployment.

Now, we turn our attention on how to set the parameter  X  , which controls the relative importance between relevance and diversity. Figure 12 depicts the average path score over the k paths selected using Algorithm 2. As anticipated, Figure 10: Elapsed time versus size of tree. We generate a tree with 30 to 1350 nodes and step 10. the average relevance score of the paths selected eventually decreases as k increases. For fixed small values of k , we observe that the average relevance score is sensitive to the values of  X  . We suggest fixing  X   X  0 . 6 so that a fair trade-off between diversity and relevance exists.
 User study: Even though our platform is intended for non-technical users, we also wanted to evaluate how effective the system can be for technical users. We gave 15 questions in plain English that can be answered by our system to 5 users, familiar with the database schema. Questions ranged from simple to elaborate. An indicative sample of questions that Figure 11: Average tree distance of paths for various values of k and  X  . Figure 12: Relevance of paths (IRScore( p,Q )) for various values of k and  X  . were given included the following: - X  X ll clients who bought [product name] X  - X  X ll clients with recommendations for [product name] who have not bought [product name] X  - X  X ll clients who bought [product name 1] and [product name 2] X  - X  X ews about [client name] X  - X  X xx contacts of [client name] X  - X  X ll clients who bought [product name 1] and with recom-mendations for [product name 2] X  - X  X ll clients in industry [industry name] who bought [prod-uct name] and with expiring contracts in the next 6 months for product [product name] X  ...and so on.
 We asked them to write the query for each question in SQL and also to use our system, after a brief introduction of the Templated Search interface. We measured the time taken by each user to formulate the question, when issuing the query in SQL, or when using the Templated Search in-terface. Fig. 13 shows boxplots of the time taken by the users to formulate the questions. It is apparent, that even for advanced database users the presented system can offer distinct advantages. Users also commented that they found the new search paradigm to be very powerful and at the same time simple to use, and particularly useful for answer-ing questions pertinent to the operations of our enterprise. Keyword search on databases: DISCOVER [20], DBXplorer [1], BANKS [4], and others, have proposed ways to search structured databases using keywords. The key idea is that, given a set of keywords, the system looks for trees of tuples connected through primary-foreign key links ( candidate networks ), that collectively contain all the query keywords. Other ranking methods have also been proposed: (i) Aggregation and OLAP : IBM X  X  SQAK [34] (SQL Ag-Figure 13: Boxplots of time of formulate proper search query in SQL and using the Templated-Search. gregates with Keywords) allows users to compute a variety of aggregate queries.Given such a keyword query and the schema graph, the system will compute candidate aggre-gate SQL queries and return the ones with highest score to the user. IBM X  X  Keyword-Driven Analytical Processing (KDAP) [36] combines intuitive keyword-based search with the power of aggregation in OLAP. (ii) Semantic Web : There are also works on using keyword search on seman-tic graphs (e.g., RDF graphs) [25]. SemSearch [25] assumes each keyword of the query is either a class (e.g., person), an instance (e.g., John) or a property (e.g., has-job-title). (iii) Web search : Given that Web search engines have an increasing number of vertical structured databases, such as tickets or products, there has also been work on finding the right database and attributes to match a web keyword query [33]. The above works do not consider progressive building of structured queries in an autocomplete-like fashion, nor distinguish between return and match relations. Moreover, query (candidate networks) semantics in keyword search on databases [20, 1] are different as each relation in the query must contribute (or link to) a query keyword. Instead, our queries (valid paths) are generated based on a combination of the templates and the query keywords, e.g., for a single-keyword query we may have a multi-relation question, but DISCOVER or DBXplorer would only have single-relation queries (the same holds for the results in BANKS). Also, query diversity is not considered. Finally, in our model the return relation is a primary class citizen, which allows our queries to be converted directly to natural language ques-tions.

Database query construction interface : In [37], given an initial keyword query, they construct a query interpreta-tion tree, each node corresponds to a decision  X  i.e., a ques-tion to the user. A sequence of such questions are asked until a query is constructed. In [13], this work is extended to leverage ontologies to group candidate queries, so fewer questions are posed to the user. The above works, neither support an autocomplete-style query construction, nor con-sider the diversity among the proposed queries.

Query forms: Another approach is to predefine a set of query forms (templates) and then let the user select a form and instantiate it. In [21] techniques are presented to auto-matically generate a set of forms for a relational database, using the data properties, without using a query workload. In [22] the authors show how to modify existing forms to create new forms. QURSED [27] is a tool to create query forms for XML data. In [9] keyword search on databases is combined with a form-based search. Limitations of using forms include: users rarely like to fill out forms; the number of forms to cover all possible queries is too high; no diversity measures are supported.

Natural language interfaces to databases : Much re-search was conducted especially in the 70 X  X  and 80 X  X , on au-tomatically converting natural language to SQL (e.g., [28]).
Facebook graph search: This is the project closest to our work. Users start with a few keywords and the system suggests possible search queries on the social graph. The Unicorn [10] system is the underlying index of the Face-book Social Graph on top of which Graph Search operates [32]. Although we do not know the details of how queries are generated, we speculate that the method has been de-veloped specifically for social search and may not be easily adapted to other schemas, e.g., enterprise search. Also, there is no mention of how (if at all) diversity and coverage are addressed [26].
We have introduced a scalable, templated search tech-nique over relational databases. Our approach guides the users and lets them easily query the potentially complex underlying data schema. The technical contributions of our work include: -A tree-based question generation framework that is based on information retrieval and graph theoretical tools. -Two provably accurate algorithms for path diversifi-cation over rooted trees: an optimal and a scalable 2-approximation algorithm.

Speaking in SQL lingo our system supports the following: multi-way joins, selections, projections, unions, intersections and exclusions over sets, and simple orderings of the results. Current limitations include: the absence of  X  X roup By X  and aggregate queries operations.  X  X roup by X  queries could easily be detected and expressed with the incorporation of static nodes using keywords such as  X  X er X  or  X  X very X . Finally, the ideas in [34] could be applied in the future to support aggre-gate queries. In the long term, we plan to include support for basic data-mining operations. For example, when the desired outcome of a query is not already present in the data, but is the result of some analytic operation (e.g., clus-tering or classification). This will broaden the scope of our solution, allowing it to answer even more complex tasks.
