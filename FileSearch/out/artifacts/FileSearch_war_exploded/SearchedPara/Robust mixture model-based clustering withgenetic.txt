 Division of Information Engineering, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 1. Introduction
In recent years, data clustering has become one of the most us eful and important activities in data mining and analysis. The amounts of data have nowadays becom e tremendous, generated from diversely different application domains. However, one aspect of data clustering that needs to be studied more result. Many efforts have been focused on developing and imp roving algorithms which mainly perform on non-contaminated datasets, whereas outlier problems in data clustering has now started to attract such as sampling errors, inaccurate measurements, uninter esting anomalous observations, distortions results of complicated and costly cluster analyses do not be come wasteful, or even misleading. observations. Such an example is Mahalanobis distance in Eq . (1). It measures the distance between a multivariate observation x covariance matrix  X  likelihood estimates.
 covariance estimate in their direction. For those reasons, D the presence of some outliers mask the appearance of another outlier. On the other hand, D and the clustering result is neither quality nor reliable.

Research related to outliers in multivariate data is not a ne w topic. More than a few methods have are the minimum volume ellipsoid (MVE) and minimum covarian ce determinant (MCD) estimators [28, up-to-date and complete review of this area, readers can ref er to [23,30]. Another approach, which is more related to probabilistic model, is the trimmed likelih ood estimator first suggested by Neykov and probabilistic mixture model-based clustering approach.
 clustering (M2C). We discuss how outliers affect its perfor mance, then introduce a new framework in order to integrate robustness into M2C. Next, in Section 3, a novel clustering algorithm based on the proposed framework is presented. Empirical experiments in Section 4 show the performance of our proposed algorithm with comparison to existing methods. Fi nally, conclusions and future work are given in Section 5. 2. Mixture model-based clustering ( M2C ) and outliers 2.1. Classical M2C
Finite mixture model is an approach to data modeling with str ong statistical foundation. It has been generated from a mixture of probability distributions. Let X = { x of size n . We say x can be written in the form: where each f probabilities (  X  normally assumed that all the components f
Under this model, the problem of identifying k clusters transforms into problem of determining the set of parameters  X  . The most well-known approach to fitting data into a mixture m odel is Maximum Likelihood (ML) [13]. Assuming x to the k -component mixture will be: and its logarithm form is: The aim of ML is to estimate the set of parameters  X  so as to maximize this function. A well-known technique for solving this optimization probl em is Expectation-Maximization (EM) [15]. This algorithm interprets X as  X  X ncomplete data X . What  X  X issing X  is a set of n vectors Z = { z corresponding to n elements of X . Each vector has k binary values, i.e. z observation x log-likelihood is: the conditional expectation of the complete log-likelihoo d in (6), using parameter estimates from the previous iteration b  X ( t ) . The result is a function Q of  X  : where  X  The M-step updates the parameter set  X  by maximizing function Q : parameter variables, the following updating formulas are o btained for the mixing probabilities, mean vectors and covariance matrices of the Gaussian components : field. Some recent research works continue to show its useful applications in high-dimensional data 2.2. Example Classical MLEs, and hence M2C methods, always try to fit the en tire set of data presented to them. since estimates of means and covariance matrices based on Eq s (11) and (12) are not robust enough to from a 3-component bivariate normal mixture with equal mixi ng probabilities and component parameters as: dimension, is added to the original data to form a new contami nated set of 150 samples. As shown in Fig. 1 and Table 1, classical M2C with Gaussian components pe rforms well on the former, but fails to yield correct result on the latter data.
 2.3. Toward robustness in M2C based framework. Banfield and Raftery [10] introduced an add itional component-a uniform distribution-robustness to outliers. Another approach is to employ Forwa rd Search technique [2,8,11]. A Forward Search-based method starts by fitting a mixture model to a sub set of data, assumed to be outlier-free. by repeated fitting and updating until all the samples are inc luded.
 combination of a robust estimator, the minimum volume ellip soid (MVE) introduced by Rousseeuw and Leroy [27], into clustering. Basically, they extend the application of MVE from robustly fitting a single group of data to clustering a mixture of groups of data . One drawback of MVE, though, is its high computational complexity and low rate of convergence. Cuesta-Albertos et al. [17] approached the problem in the opposite direction. They made use of clusteri ng method to provide estimate of normal was treated as the initial trimming. Then, the trimmed regio n was step-by-step expanded, with ML the clustering method used at the initial stage.
 Upon using a M2C method for clustering data, we intuitively a gree to the following assumption: be true with real-life data. Therefore, we adopted what is ca lled  X  X he weak assumption X  instead: A similar assumption called  X  X he weak Gaussian assumption X  was stated for the case of well-separated and spherical Gaussian mixture [33]. The weak assumption im plies an imperfection in the data, meaning fraction is within a given dataset depends on the nature of th e data itself.
So how can  X  X he weak assumption X  be incorporated into M2C? Th e framework given in Fig. 2 is what we propose for such a purpose. We call it the Partial Mixture M odel-based Clustering (Partial M2C), as The assumption leads to a subset selection step, where it is d ecided which data observations are to be included in the model, and which are not. ML estimation (MLE) is then carried out on the selected ones. another group, simply labeled as  X  X on X  X -care X , containing potential noises and outliers. There are two key issues in Partial M2C: 1) What should be the s election criterion in the Model Sample Selection stage? 2) How to make sure the EM X  X  monotonic prope rty is preserved, or how to guarantee algorithm X  X  convergence? As long as convergence is guarant eed, any suitable objective function can be considered as selection criterion.

Neykov et al. proposed a method based on trimmed likelihood e stimate (TLE) for robust fitting of mixtures [25]. They used an algorithm called FAST-TLE, whic h had previously been introduced for a contribution. Firstly, a random subgroup of the given sampl e is used to fit the model. In subsequent iterations, a new subset of predefined size is selected based on previously estimated model, and then used to refine the model. FAST-TLE can be explained by the fram ework, since each of the algorithm X  X  showed that the refinement procedure in FAST-TLE yielded mon otonically nondecreasing sequence of log-likelihood, and since the number of subsets is finite, co nvergence is always gauranteed.
In the next section, we introduce a novel and robust clusteri ng algorithm based on this Partial M2C framework. The proposed method, using Genetic Algorithm (G A) and TLE for Model Sample Selection, shows its effectiveness in overcoming noise and outlier pro blem in contaminated data. 3. GA-based Partial M2C ( GA-PM2C )
Genetic Algorithm (GA) [9] and its variants provide good sel ection methodologies. The GA X  X  repro-key role in GA. In Partial M2C, this is where we can use GA to find the model observations. It will be explained in details in a few more lines. Besides, as mention ed above, when the model observations are re-selected, the likelihood value of ML estimates might not be monotonically nondecreasing anymore. highest likelihood value from the current generation. Henc e, we think GA could be a suitable means to help us effectively search for the optimum set of model obs ervations in Partial M2C. Some recent examples of integrating GA into clustering framework inclu de: using GA to improve multi-objective effectively [18]. The proposed GA-based Partial M2C, or GA-PM2C, is given in Algorithm 1.
Before going into the algorithm, a few parameters need to be d eclared as follows: Each individual in a population is represented by a chromoso me, which is a binary vector of length n . The i -th bit of a chromosome is 1 if observation x the corresponding model. Attached to each chromosome is a Ga ussian mixture modeling the selected data. Hence, each chromosome (and its corresponding mixtur e model) is a possible solution, showing  X  X on X  X -care X  observations (i.e. outliers).

In Algorithm 1, P P than C cycles are needed. Besides, it is important to note that EM is only performed on the selected observations, corresponding to bits 1 of the individual. Se condly, the individuals undergo a process determined and stored for later comparison. In the followin g, we discuss our characterized GA-related operations used in the algorithm. 3.1. Guided Mutation
The original form of GA has three basic operators: Selection , Crossover and Mutation, which attempt we argue that Mutation is not helpful and powerful enough an e ngine. While occurring at a very low introduce another operator called Guided Mutation to repla ce the classical Mutation.
Guided Mutation applies on every individual during its deve lopment. This is where model observations some C cycles of EM, the chromosomes in a population are guided to mu tate toward maximizing their if
A represents the model sample, it is a subset of size m out of n original observations. From (4), let log f ( x maximize: where I if x i is trimmed off, and is equivalent to one refinement step of FAST-TLE. Hence, our G A-PM2C nicely inherits the monotonic property proven for FAST-TLE [24]. For each chromosome, the log-likelihood is always nondecreasing during EM cycles, as already known, and also after Guided Mut ation. Besides, since there is no random convergence of our GA-based algorithm. 3.2. Recombination
This process involves selecting potential pairs of parents and mating them to produce | P  X  | offspring individuals. The size of offspring population can be determ ined based on a percentage p of parent population, such that | P  X  | = p wheel rank weighting and single-point crossover [32]. 3.3. Selection considered. From the union of both the parent population P | P | best individuals are chosen to form the new generation P 3 ( t ) .

For each generation P generations G is reached; or iBest does not change within a certain number of consecutive gener ations. Once the GA evolution is terminated, a complete EM algorithm is performed one last time on the model process. 4. Empirical study
The experiments below are used to examine and demonstrate th e performance of GA-PM2C in cluster analysis of data with noise and outliers. Among various robu st methods that have been discussed so far in the previous sections, FAST-TLE is the one most relate d to our algorithm. Hence, we will make a close comparison between the two throughout the experimen ts. Classical MLE, however, had been include it in the comparison. Since we have been focusing on m ixture of Gaussians, we continue to use this distribution model in the experiments. Other models, h owever, such as regression model or mixture number of clusters in the following experiments, but assume that this value is known a priori. 4.1. Parameter setting GA-PM2C requires some additional parameters, as declared i n Section 3, for the GA-based processes. The population size | P | , the number of EM cycles C and the assigned contamination rate  X  affect the within a range to see the influence of this parameter. The numb er of EM cycles was set to 5 throughout EM algorithm does not lead to significantly better result. Th e assigned contamination rate specifies the amount of data observations being trimmed (trimming lev el). This was set at the true percentage robustness of the algorithms.

Besides, as pointed out by Neykov et al. [25], FAST-TLE shoul d actually be run  X  X initely many times X  after which the best solution is chosen. When comparing it wi th GA-PM2C, we followed the same procedure and repeated FAST-TLE as many times equal to | P | . So, in each trial, GA-PM2C was started with | P | chromosomes, whereas FAST-TLE was run | P | times simultaneously before its best outcome was recorded. The chromosomes in GA-PM2C and the subsamples in FAST-TLE were always initialized randomly. Finally, for EM algorithm, random initial assign ment strategy and Aitken acceleration-based stopping criterion [13] were used. The maximum number of ite rations in a complete EM process was 300 times. 4.2. Continue experiment 2.2 and the number of times the algorithms successfully identif y the three clusters are shown. From the table, it shows that GA-PM2C performs at least as well as FAST -TLE does on this dataset. The true contamination rate in this case is  X   X  is 15%, far below the true value, and | P | = 4 only, GA-PM2C does slightly better than FAST-TLE. When either  X  or | P | is set higher, the performance of FAST-TLE is improved. GA-P M2C with  X  of 15%, 25%, 35% and 45% are shown in Fig. 3. FAST-TLE, once fits co rrectly, yields the same results as GA-PM2C does. At 25% or 35%, which are quite close to the true r ate, the algorithms give excellent estimates of both means and covariances. The clustering res ult from GA-PM2C for the 35% case is too many outliers have been considered as model samples, or t oo many true model samples have been pruned off respectively.
 mentioned earlier, FAST-TLE should be run a certain number o f times to select the best outcome from there. Running FAST-TLE only once and immediately acceptin g that result may not be a good idea This practice appears equivalent to using | P | different parents in the initial population in GA-PM2C. will be demonstrated in the next experiments. 4.3. Mixture of five bivariate Gaussians with outliers
In the previous dataset, the clusters are well-balanced and almost equally separated. In this section, different size, of which group 5 has the most number of observ ations, and also the largest covariance 20 atypical points were added to create the outliers. The tru e contamination rate is, therefore,  X 
In this experiment, we assigned  X  to 3%, 4% and 5%, which are below, approximately equal and above the true contamination rate respectively. The number of parents in GA-PM2C (or the number of were executed. The number of times the algorithms correctly identified the 5 clusters is recorded in Table 5. It is shown that GA-PM2C outperforms FAST-TLE quite significantly in this case.
When  X  = 3%, which is a little below the true contamination rate, FAST -TLE almost completely fails On the other hand, GA-PM2C performs much better. With | P | = 4 only, its success rate is slightly 16, it gives correct results all the times. When  X  = 4%  X   X  than success one, whereas GA-PM2C has 100% success rate almo st since | P | = 4. When  X  is increased to 5%, higher than the true rate, FAST-TLE X  X  performance is t hen improved to be competent enough to components frequently estimated by the two algorithms with  X  = 3% and 4% are presented in Fig. 4. As shown, due to outlier effects, FAST-TLE mistakenly combi nes the two components 3 and 4 into one
The observation from this experiment clearly shows that FAS T-TLE is more sensitive to the assigned mixture of data, FAST-TLE may get trapped in local maxima due to the existence of outliers, even if just a few of them. It would be much safer for FAST-TLE to trim m ore data than the true percentage to get a higher chance of avoiding the  X  X asking X  and  X  X wamping X  effects of an outlier (although here,  X  = 4% is already greater than  X  found by Guided Mutation may be picked out through Recombina tion between chromosomes. Such a phenomenon can be demonstrated by an example given in Fig. 5. The figure presents two segments in the chromosomes Parent 1 and Parent 2, which have just been guide d-mutated and ready to mate to produce segments and produce Offspring 2 with all the correct assign ments. Consequently, Offspring 2 is more in GA-PM2C, the interaction among individuals is very usefu l for selecting model observations and identifying outliers. With FAST-TLE, although we can have m ultiple runs to select the best outcome, the drawback in this design is that each run is totally an inde pendent process and can not make use of any previous run to make any improvement.
 4.4. Simulated data in higher dimensions
Two datasets, A and B , were created from four-component Gaussian mixtures in  X  tively, for Monte-Carlo experiments. They are described in Table 6. For each dataset, 100 pairs of training sample and test sample were generated. The trainin g samples were added with 50 data points produced from a uniform distribution within (-10,10) in eac h dimension, but not the test samples. From simple calculation, we got  X  at 5%, 10% and 15% would be appropriate.

In these experiments, we included the results of classical G aussian mixture model (GMM) as to see GMM, FAST-TLE and GA-PM2C models are used to fit the training s et. A class label is then assigned to a component of the models if majority of the observations b elonging to that component have that same label. Afterwards, using the estimated models, each ob servation in the test sample is classified measure of performance.

From the results in Table 7, it is clearly seen that classical model could not cope with the problem success rates in both cases are low. The robust algorithms, F AST-TLE and GA-PM2C, show that they could produce better results. On both datasets, GA-PM2C out performs FAST-TLE. It gives significant improvement, starting from  X  = 10% on A , or from the lowest of 5% on B . 4.5. Classification of breast cancer data
Let us now examine the performance of GA-PM2C on a real-world problem, the popular Wisconsin diagnostic breast cancer data. This dataset can be found fro m the UCI Machine Learning repository [1]. 3 of the attributes, namely extreme area, extreme smoothnes s and mean texture, Fraley and Raftery [5] analyzed this dataset using three-group unconstrained-co variance Gaussian model. They pointed out that there were some  X  X ncertain observations X . Hence, also with three-component unconstrained model, we carried out a classification procedure similar to Section 4.4. The data were divided into 2 parts: 5.9%, and we set  X  at different levels, specifically 5%, 6% and 7%, around this v alue. repetitions of classical GMM, FAST-TLE and GA-PM2C. When co mpared with the classical model, both algorithms, the data models are estimated more precisely, a nd hence, yield better results. Among the three methods, GA-PM2C produces the best results. In the above experiments, we have been cautious when decidin g the amount of data to be trimmed. We have allowed this value to vary within a certain range arou nd the value of  X  a priori for the simulated datasets or determined by the guid eline given by Dasgupta and Schulman [33] case. In clustering performance point of view, trimming off more data observations without improving examine such circumstance, we repeated the experiments on d atasets A , B and Wisconsin for different methods become even worse than that of classical model after 25% of trimming. Thus, it is encouraging or not too high over the true contamination rate in data. 5. Conclusions and future work
In this paper, we implement a variant of classical M2C, named Partial M2C, in which  X  X he weak assumption X  is recommended over  X  X he strong assumption X . A new general framework for the Partial M2C is proposed. The framework has a Model Sample Selection s tage, where data observations are selected as either observations generated from a probabili stic model or outliers. We also propose GA-based Partial M2C algorithm, or GA-PM2C. The algorithm is ca pable of clustering data effectively in the presence of noise and outliers. We apply GA with a novel Gu ided Mutation operation to help filter GA-PM2C. When compared with a closely related work FAST-TLE , GA-PM2C is much less sensitive to initializations, and gives more stable and consistent re sults.

GA with trimmed likelihood as fitness function has been used f or Model Sample Selection in this potential direction to explore further in the future.
 References
