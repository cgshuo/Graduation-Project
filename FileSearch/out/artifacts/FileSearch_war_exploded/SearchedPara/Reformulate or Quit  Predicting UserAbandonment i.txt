 Detecting when a user is close to abandoning a search session is an important problem in search for complex information needs in order to take precautions and respond to users X  information needs. Session abandonment and its types has been studied extensively. Li et al. [ 1 ] introduced and made the first dis-tinction between good and bad abandonment. They defined good abandonment as an abandoned query for which the searcher X  X  information need was success-fully satisfied, without needing to clickthrough to additional pages. Chuklin and Serdyukov examined query extensions [ 2 ] and editorial and click metrics [ 3 ]for their relationship to good or bad abandonment. In [ 4 ] they developed machine learned models to predict good abandonment by using topical, linguistic, and his-toric features. Diriye [ 5 ] studied abandonment rationales and developed a model to predict abandonment rationale. Beyond the SERP (search engine result page), White and Dumais [ 6 ] studied aspects of search engine switching behavior, and develop and evaluate predictive models of switching behavior using features of the active query, the current session, and user search history.
 In this paper, we present a comparison of different feature sets for detecting session abandonment. In Sect. 2 , we describe the TREC Session track data that we use for our experiments X  X ince this data consists of clearly-segmented sessions on a single topic, it is low-noise and thus ideal for analyzing relative feature effectiveness. In Sect. 3 we describe our feature sets. In Sect. 4 we describe our experiments and results, and we conclude in Sect. 5 . We use the 2014 TREC Session track data [ 7 ], which consists of a large amount of logged user actions in the course of full search sessions for pre-defined topical information needs, for training and testing our models. As ensured by the track protocol, sessions in this data have clearly marked start and end points, and are entirely related to a given topic. Thus this data is much  X  X leaner X  than standard search engine log data. From this data we used 1063 full sessions, each of which is made up of a sequence of  X  X nteractions X . Each interaction consists of one query, up to 10 ranked results for that query from a search engine, and user clicks and dwell times on those results. Our main assumption for a proposed session abandonment prediction model is that users abandonment decision base primarily on their positive or negative search experience through the search session according to their information need. We formulate the search experience through the features of document titles, URLs, and snippets on the current search result page, the queries submitted to the search engine in the history of the users X  current session, the relevancy of seen documents on the current search result page and through out the current search session, dwell time on relevant, non-relevant and on whole clicked documents, total duration of the interaction and the session. Additionally, knowing that other users have seen and/or clicked on the same snippets in their own sessions may influence whether a user feel fulfilled about the information gathered in his/her own session.
 LETOR [ 8 ] and other datasets [ 9 , 10 ] for learning to rank: in particular, retrieval model scores between query and document URL/title/snippet; statistics about query term frequencies (normalized and non-normalized) in document URL/ title/snippet; URL length and depth; number of inlinks and outlinks; spam score as computed by Waterloo X  X  model for web spam [ 11 ]; and the Alexa ranking for the domain are all features that are either directly in LETOR data or are simi-lar to other LETOR features that we cannot compute in our own data. Most of the textual features can be computed using our Indri index of ClueWeb12; our index also stores spam scores and inlink counts. The Alexa ranking is available through Alexa X  X  API. Table 1 summaries all LETOR features.
 relevance, durations of actions, similarity between queries and topic statement, and similarities between a user X  X  actions and those of other users working on the same topic. We describe these in more detail below.
 guide their interactions with the search engine. We extract keyphrases from the topic description using the Alchemy API extraction tool. We also combined all queries that were submitted by other users working on the same topic. For each interaction, by using the previously submitted queries in prior interactions by the user, we calculate an approximation of submitted user queries to newly formed Alchemy topic keyphrases and other users X  combined query. The approximation of query Q i to Q j is computed as follows: We de-case all letters, replace all white space with one space, remove all leading and trailing white spaces, replace punctuation, remove duplicate terms, and remove stop words. Thus each query is represented as a bag of non-stopwords. In order to calculate | Q i  X  Q j | , we consider two terms equivalent if any one of the following four criteria are met: 1. The two terms match exactly. 2. The Levenshtein edit distance between the two terms is less than two. 3. The stemmed roots of the two terms match exactly. 4. The WordNet Wu and Palmer measure is greater than 0.5.
 For relevance features, we used graded relevance judgments of retrieved doc-uments to the topic description as provided. High positive scores of relevancy indicates that the document is more relevant to the topic. Features we used include relevancy grade of each document in the interaction, total relevant and non-relevant document counts in the interaction, and total relevant and non-relevant document counts appeared in the session up to the current interaction. Duration-based features extracted from the session data include start time of each interaction, time spent in each interaction, time spent in each clicked document, total time spent in clicked relevant documents which have relevance judgement of 1, total time spent in highly relevant clicked documents which have relevance judgement of 2 or higher, total time spent by user on scanning and reading documents including the last interaction.
 Other users X  sessions on the same topic are also used to extract features. Their queries, clicked documents, documents appeared on sessions were collected. The features related with other user sessions are, total number of other users X  clicked documents appearance in current and previous interactions, its ratio, total number of other users X  sessions X  documents appearance in current and previous interactions and its ratio.
 session. Actions including the first query and any clicks on results retrieved for that query are associated with interaction order number 1; actions starting from the second query up to just before the third query are associated with interaction order number 2; and so on. We compare our session abandonment feature sets by their effectiveness at pre-dicting actual user abandonments, using standard classification evaluation mea-sures like precision, recall, AUC, and classification accuracy. We trained and tested random forest models using all sessions from the TREC 2014 Session track. For each interaction that appears in the Session track data, we have one instance for training/testing: the 0/1 label indicating whether the session was abandoned immediately after that interaction or not.
 sessions), of which 1763 are non-abandonment and 637 are abandonment. Since the class distribution is so skewed, we re-balanced the training data with SMOTE, which creates artificial data for the under-represented class [ 12 ]. We trained and tested using four-fold cross-validation and report micro-averaged evaluation measures aggregated across all four testing splits. Note that only training data, not testing data, in each fold is rebalanced with SMOTE. We tested 4 models. The first model, a simple baseline, uses only the interac-tion order number. The second model uses the LETOR features listed in Table 1 for every document appeared in the interaction. The third model uses the session and interaction features listed in Table 2 , and the fourth model uses all features. Table 3 and Fig. 1 summarizes the performance of the four models. The baseline using only interaction order number for training performs better than model 2 (which only uses LETOR features) in all measures, and model 4 (which uses all features) in AUC and accuracy (though that model scores slightly higher in pre-cision and substantially higher recall). The best achieving model is the model that uses only session and interaction features. It improves precision over the baseline by 22 %, recall by 135 %, AUC by 17 % and accuracy by 7 %. Figure 2 shows the feature importance of the random forest that is trained with session and interaction based features. Mean importance of the features that are extracted by using the other sessions with the same topic is 17.46, only interaction id is 16.91, query approximation features is 16.77, relevancy based features is 12.89, duration based features is 12.89 and lastly duration and relevancy based features is 10.29.
 racy. The first three features are unsurprisingly based on session and interaction duration. The fourth feature is the number of clicked documents appeared in the current interaction. The fifth is the number of relevant documents seen in session including the current interaction. The last two features are query approx-imations. We have compared two different sets of features for predicting abandonment: one set is more like LETOR, including many features related to query/document similarity; the other includes a collection of features derived from the session and topic. We tested them in a setting with low noise: fully-segmented sessions on a single topic, with the topic description available as well as sessions by other users for the same topic. We found that the second set is far superior to the first set, which actually degrades effectiveness at predicting abandonment. We also found that the inclusion of durability-based features in the second set is not the sole reason they perform better.

