 } We propose a novel method for document clustering using charac-ter N-grams. In the traditional vector-space model, the documents are represented as vectors, in which each dimension corresponds to a word. We propose a document representation based on the most frequent character N-grams, with window size of up to 10 charac-ters. We derive a new distance measure, which produces uniformly better results when compared to the word-based and term-based methods. The result becomes more significant in the light of the robustness of the N-gram method with no language-dependent pre-processing. Experiments on the performance of a clustering algo-rithm on a variety of test document corpora demonstrate that the N-gram representation with n=3 outperforms both word and term representations. The comparison between word and term represen-tations depends on the data set and the selected dimensionality. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, I.2.7 [Natural Language Processing]: Text analysis, I.5.3 [Clustering]: Similarity measures, I.5.4 [Appli-cations]: Text processing General Terms: Algorithms, Experimentation, Performance. Keywords: Text clustering, text mining, N-gram text representa-tion.
Motivated by some recent positive results in using character N-grams in building author profiles and their use in automated author-ship attribution [3], we explore the idea of using frequent charac-ter N-grams as vector features in document clustering. We built a vector space for the documents and found that the results are bet-ter than using words, especially for low dimensionality. We also experiment with multi-word terms as the vectors X  features. Our hy-pothesis is based on the assumption that a multi-word term repre-sentation is a more compact representation of meaning in a domain than words, and therefore has the potential of providing better clus-tering performance at lower dimensionality. We perform automatic term extraction based on the C/NC value method [2] to retrieve the terms using a combination of linguistic and statistical criteria.
Since the document clustering we use (k-means) requires a dis-tance or dissimilarity measure rather than similarity, we use the following scaled sine distance measure, where d 1 and d 2 document vectors using TFIDF weights:
However, without feature selection, we can get very high dimen-sionality, which is one of the major challenges of document cluster-ing. Several approaches to dimensionality reduction are available; e.g., reduction to a set of the most frequent words or terms.
Character N-grams. A character (byte) N-gram is a substring of
N characters (bytes) of a longer string [1]. The N-gram models are based on systematically collecting and counting the N-grams using a  X  X liding window X  of length N over a document. Gener-ally, character N-grams produce better results than byte N-grams. Moreover, dimensionality and sparsity with character N-gram rep-resentation are lower than with byte N-gram representation. Our experiments were performed on character Tri-grams.
For each feature (N-grams, terms and words) two different fea-ture selection approaches were experimented with. The first feature selection approach is specific to each feature type (document fre-quency for N-grams, C-value for terms and collection frequency for words), and is explained in this section. The second feature se-lection approach used is generic, i.e. the same for all three feature types, and consists of a term-frequency variance measure. Let be the frequency of a feature (word/term/N-gram) t in document d , and n 0 be the total number of documents in the collection. The features are ranked by a quantity that is proportional to the term-frequency variance (Eq. 8 of [4]): Experiments with both feature selection approaches have been car-ried out.

Document Clustering using N-grams. In our approach, we at-tempted to follow the idea proposed in [3]. We retrieve the most frequent N-grams and their frequencies from each document as its profile. To explore a wider range of distance measures, we use the distance measure depending on parameter  X  . From our experimental results, we find that  X  = 1 produces the best clustering quality. N-gram size has impact on clustering quality as well as vector dimensionality and sparsity. Dimensionality and sparsity increase with increasing N-gram size. In other words, the larger the N-gram size, the smaller is the number of common N-grams among documents. The best clus-tering quality is given by Quad-gram representation or 5-gram. Tri-gram representation, however, produces comparable entropy and accuracy with a more practical dimensionality.
 An equivalent operation to stop-word removal in the context of N-grams is ignoring the N-grams that appear in all or almost all documents. This may be regarded as analogous to removing N-grams with high document frequency (DF), and we performed those experiments. They showed that having an upper bound on N-gram DF does not improve performance, and if the bound is made tighter, the performance started to decrease. The N-gram profiles are sig-nificantly different from word vectors as they are much more dense: If the N-gram size is small (e.g., less then 5), then most N-grams have high document frequency. If the N-gram size is large, then most N-grams have lower document frequency, and removing  X  X top  X  N-grams would not make significant changes. One of the advan-tages of the N-gram approach is independence of the specific lan-guage, while stop-word removal is language-specific.

Document Clustering using Terms. Terms are extracted auto-matically based on their C Value (a frequency-based weight that accounts for nested terms). In order to reduce the dimensionality, the n terms with highest C Value are chosen as the vector features. TFIDF is used as the weighting scheme and scaled sine is used as the distance measure. In this paper, K-means is the clustering method used.

Document Clustering using Words. In word based cluster-ing, we remove the stop-words and apply Porter X  X  stemming al-gorithm. We select the words that have the highest collection fre-quency (number of occurrences in the whole collection).
We apply the TFIDF weighting scheme to compute the elements of the vector representation of documents and the scaled sine dis-tance measure between documents in the K-means clustering algo-rithm.
Evaluation Methodology. Two evaluation measures are used to evaluate the clustering quality: entropy and accuracy . Two data sets are used in this paper: Reuters-21578 and CISI-CRAN-MED. After removing from Reuters the articles having less or more than one topic, there are 8,654 documents left which belong to 65 topics. CISI-CRAN-MED data set has 3,893 documents, which belong to three topics.
 N-gram Feature Selection Based on Document Frequency. We performed experiments on feature selection based on document frequency. Two values of document frequency, minimum docu-ment frequency df min and maximum document frequency df max are changed in our experiments. For any N-gram to be included in the vector space definition, it should appear in at least documents at most df max documents. Experiments show the im-portance of setting a minimal document frequency bound df even with small values of df min we can achieve significant dimen-sionality reduction. Generally, with increasing df min , the cluster-ing quality becomes better at first. However, the quality becomes worse if we keep increasing the df min .

Comparison of Word, Term, and N-gram Representations under feature-specific feature selection. We performed experi-ments on comparing clustering performance using Tri-gram, term and word representation with different dimensionalities, and using the two feature selection approaches. To get lower dimensionality for Tri-grams, firstly we chose the Tri-grams with maximum (Total number of documents) / 2; secondly, we sorted Tri-grams by decreasing df and chose the first n Tri-grams. For term-based representation, we use Filter 1 , C-Value and choose the with highest C-Value . For word-based representation, we perform stop-word elimination and word stemming (Porter X  X  stemming al-gorithm). The dimensionality was reduced by using the frequent words over the collection. The results are the means of 30 runs of the clustering algorithm to reduce the impact of the initial random centroids.

When compared to word and term representation, tri-gram rep-resentation produces the best clustering results on both Reuters and CISI-CRAN-MED data sets. A standard T-test confirms that clus-tering performance using Tri-grams is significantly better than us-ing terms on both of the Reuters and CISI-CRAN-MED data sets. Moreover, term representation generally gives significantly better results than word representation on the Reuters data set. On the other hand, there are not significant differences between the results of term and word representation when dimensionality is 1,000 or 2,000 on the CISI-CRAN-MED data set. When the dimensionality is 3,000 or 4,000, we see that word representation produces bet-ter clustering quality than term representation on the CISI-CRAN-MED data set.
In this paper, we presented a novel approach for document clus-tering. Using a character N-gram-based representation gives best clustering performance with the lowest dimensionality. A pos-sible justification for this result is that the N-gram method has less sparse-data problems, it finds common patterns between words with the same roots but different morphological forms (e.g., finance and financial), without treating them as equal, which happens with word stemming. It also detects phrasal patterns with N-grams that bridge two words. To reduce dimensionality with words, we have to ignore a large set of words, while frequent N-grams collect fre-quencies of multiple words, and hypothetically retain more infor-mation. In addition, we find that feature selection based on docu-ment frequency can be applied to document clustering with char-acter N-gram representation. More detail about the experiments summarized in this short paper is available as a technical report [5].

Acknowledgments. The research presented here was funded in part by grants from the Natural Sciences and Engineering Research Council of Canada and IT Interactive Services, Inc. [1] W. B. Cavnar. Using an n-gram-based document [2] K. Frantzi, S. Ananiadou, and H. Mima. Automatic [3] V. Ke  X  selj, F. Peng, N. Cercone, and C. Thomas. N-gram-based [4] J. Kogan, C. Nicholas, and V. Volkovich. Text mining with [5] Y. Miao, V. Ke  X  selj, and E. Milios. Document clustering using
