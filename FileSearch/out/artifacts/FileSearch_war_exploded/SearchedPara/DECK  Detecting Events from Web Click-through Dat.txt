 one of the dominant forms of self-publication on the Inter-net. In [20], tags indicating real events are extracted from a set of user-supplied tag data annotating pictures at the Flickr site 1 . Our focus in this paper is to detect events from Web click-through data , which is generated by users (pas-sively) when interacting with Web search engines. An ex-cerpt of click-through data containing five entries is shown in Table 1. Each entry of Web click-through data basically records the following four types of information: an anony-mous user identity, the query issued by the user, the time at which the query was submitted for search, and the URL of clicked search result [13].
 ing data source for event detection is trifold. Firstly, sim-ilar to the other resources on the Web, the click-through data can be regarded as a sensor of the real world as well. Click-through data containing query keywords and clicked pages of users often reflect users X  response to recent real world events. Secondly, the sharply accentuated role of Web search engines as an entry point to the Web has given rise to a huge volume of click-through data, which enables ef-fective knowledge discovery. Thirdly, click-through data is well formatted. Complicated data preprocessing on Web content and structure information can be avoided. nontrivial problem because of the following two observa-tions: 1) The information provided is limited, so that the semantic knowledge cannot be identified easily and clearly. As shown in Table 1, the URLs which represent the search results clicked by users are incomplete. Instead of referring to the addresses of pages, the URLs refer to the addresses of sites. Consequently, entries having the same URL may have different semantics and correspond to different events. Similarly, the same query keywords may have been issued by users to inquire about different events. 2) A large amount of click-through data do not represent any real event in the world. Table 2 illustrates the top 10 most frequent entries in the click-through data logged by AOL in March, 2006 . None of them are correlated with any real events. tion along the temporal dimension of query sessions in each subspace, we prune uninteresting subspaces which do not contain query sessions corresponding to real events. Finally, a non-parametric clustering method is used to cluster data points in each interesting subspace. The generated clusters will be returned as detected events.

The main contributions of this paper are summarized as follows.  X  We develop a novel algorithm, KNN-GPCA, to seg- X  We propose an interestingness measure, which simul- X  We evaluate the performance of DECK with extensive
The rest of the paper is organized as follows. In Sec-tion 2 , we illustrate the polar transformation of Web click-through data. We describe our linear subspace estimation algorithm, KNN-GPCA, in Section 3 . In Section 4 ,wefirst define the interestingness measure of subspaces and then discuss the detection of events from interesting subspaces. Experimental results based on real-life data sets are pre-sented in Section 5 . We review related work in Section 6 and conclude this paper in Section 7 .
Given a collection of Web click-through data, Zhao et al. [18] considered the data in unit of query-page pair which consists of a user query and a page clicked by the user. For example, the click-through data in Table 1 will be consid-ered as five distinct query-page pairs. However, in DECK, we study Web click-through data in unit of query session , which refers to an episode of interaction between a user and a Web search engine. A query session consists of a query is-sued by the user and a set of pages clicked by the user on the search result. The reason we adopt the unit of query session instead of query-page pair will be given in the below. Definition 1 ( Query Session ) A query session S =( Q, P ) , where Q = { q 1 ,q 2 ,  X  X  X  ,q m } is a bag of keywords repre-senting a user query issued to a search engine and P = { p 1 ,p 2 ,  X  X  X  ,p n } refers to the set of corresponding pages clicked by the user on the search result. In addition, a query session S is associated with a time point, T ( S ) , when the query is issued.
We first consider mapping the semantics of a query ses-sion S to an angle  X  . Since the angle between two query sessions reflects how similar they are in semantics, we need to define the semantic similarity between two query ses-sions. Recall that a query session contains a query and a set of clicked pages. We define the semantic similarity be-tween two query sessions by considering their similarities in not only query keywords but also clicked pages. For both queries and clicked pages, we use an adapted Jaccard coef-ficient to measure their similarity.
 Definition 2 ( Semantic Similarity ) Given two query ses-sions S 1 =( Q 1 ,P 1 ) and S 2 =( Q 2 ,P 2 ) , the semantic similarity between S 1 and S 2 , denoted as Sim ( S 1 ,S 2 ) ,is Sim ( S 1 ,S 2 )=  X   X  The weight coefficient  X   X  [0 , 1] can be determined ex-perimentally to assign different importance to the simi-larities of queries and clicked pages. For example, con-sider the query sessions in Figure 2 ( a ) , where each en-try of the table represents a query session. Let  X  =0 . 5 . Sim ( S 1 ,S 2 )=1 / 2  X  1 / 2+1 / 2  X  2 / 3=7 / 12 .
Given a set of n query sessions { S 1 ,S 2 ,  X  X  X  ,S n } ,a n  X  n semantic similarity matrix M can be computed such that each element m ij = Sim ( S i ,S j ) . In other words, the rel-ative semantics of a query session S i is represented by the n -dimension row vector R i = m i 1 ,m i 2 ,  X  X  X  ,m in of M . In order to map the semantics of S i to an angle  X  i in polar space, we need to reduce the dimension of R i to 1 . For di-mension reduction, we perform Principle Component Anal-ysis (PCA) on the semantic similarity matrix M . Then, the first principal component is used to preserve the dominant variance in semantic similarities. Let { f 1 ,f 2 ,  X  X  X  ,f n } be the first principal component which corresponds to the set of query sessions { S 1 ,S 2 ,  X  X  X  ,S n } . A query session S i can be mapped to a point (  X  i ,r i ) where  X  i is computed as
However, as analyzed in the following subsection, the performance of GPCA degrades in the presence of outliers. In order to improve the robustness of GPCA, KNN-GPCA takes into account the distribution of K nearest neighbors of data points, which yields a weighted least square estima-tion of subspaces. In this section, we first review the al-gorithm of GPCA. Then, the embedding of the distribution of K nearest neighbors is described. Finally, we estimate subspaces using weighted least square technique.
Given a set of sample data points, GPCA estimates a mixture of linear subspaces by the following three steps: fitting polynomials to data , estimating the number of sub-spaces and estimating the normal vectors of subspaces .
Based on the fact that each data point x  X  R D satisfies b x =0 where b i is the normal vector of the subspace to which x belongs, a data point lying on one of the n sub-spaces satisfies the following equation: where { b i } n i =1 are normal vectors of the subspaces. It can be converted to a linear expression by expanding the product of all b T i x and viewing all monomials x n = x  X  X  X  + n D = n ) as system unknowns. By introducing the Veronese map v n :[ x 1  X  X  X  x D ] T  X  [  X  X  X  , x n ,  X  X  X  ] where x n is a monomial of the form x n 1 becomes the following linear expression: should be demoted. We achieve this purpose by assigning weight coefficients to data points to distinguish true data from noise and outliers. Particularly, we weigh data points based on the distribution of its K nearest neighbors.
The K th Nearest Neighborhood Distance ( kNND ) metric proposed in [7] detects and removes outliers based on the following fact: in a cluster containing more than K points, the kNND for a data point is small; otherwise, it is large. Recall that our polar transformation has the cluster consistency property, which indicates that true data points (e.g., query sessions corresponding to real events) lie in clusters inside subspaces. The kNND of true data points should be small while the kNND of noise and outliers should be large.

However, instead of simply using the kNND to differ-entiate outliers and inliers, we consider the distribution of the K nearest neighbors of data points. Given a data point x , let its K nearest neighbors be NN k ( x i ) . Both the vari-ance of the K nearest neighbors along the direction of the subspace of x i (i.e., the direction from the origin to the point x ), denoted as svar ( NN k ( x i )) , and the variance of the K nearest neighbors along the direction which is orthogo-nal to the subspace direction, denoted as nvar ( NN k ( x i )) , can be computed by PCA. For example, Figure 3 shows the distribution of 3 nearest neighbors of three data points x , x 2 and x 3 respectively. The two directions along which svar ( NN 3 ( x 1 )) and nvar ( NN 3 ( x 1 )) are computed are in-dicated by svar and nvar respectively in the figure.
If x i is a true data point, it forms some cluster together with its K nearest neighbors. Thus, both svar ( NN k ( x i )) and nvar ( NN k ( x i )) should be small. Consequently, the sum S ( NN k ( x i )) = svar ( NN k ( x i )) + nvar ( NN k ( x i )) will be small as well. On the contrary, if x i is an outlier (e.g., x 2 in Figure 3), S ( NN k ( x i )) will be large. How-ever, even if a data point forms a cluster together with its K nearest neighbors, it may not be a true data point if its neighbors spread along the orthogonal direction of the subspace (e.g., x 3 in Figure 3). That is, the cluster does  X   X   X   X   X   X  W  X   X   X   X   X   X  The above equation can be succinctly written as WAc = d , where A is the matrix whose rows are v n ( x i )(2 ..M ) T , i =1 , 2 ..N and d is the right side of the above equation. By minimizing the objective function d  X  Ac W , we can obtain the weighted least square approximation of c i , i = 1 , 2 ..N as c =1 and [ c 2 ,  X  X  X  ,c M ] T =( A T W T WA )  X  1 ( A T W T Wd ) Note that, since we use the diagonal matrix of weight co-efficient W to demote the impact of noise and outliers, the estimation error of coefficient vector c is reduced.
To estimate the normal vectors { b i } n i =1 , we calculate them in the absence of noise using polynomial differen-tiation as in original GPCA. The computed vectors serve to initialize the following constraint nonlinear optimization which differs from GPCA in the introduction of the weight coefficients: where  X  x j is the projection of x j onto its closet subspace. By using Lagrange multipliers  X  j for each constraint, the above optimization problem is equivalent to minimizing the following function Taking partial derivatives with respect to  X  x j and equating it to 0 , we can solve for  X  j / 2 and W ( x j )  X  x j  X  x j 2 .Byre-placing them into the objective function ( 9 ) , the simplified objective function on the normal vectors can be derived as E n ( b 1 ,  X  X  X  ,b n )= After obtaining an initial estimate of normal vectors using polynomial differentiation, one can use standard nonlinear optimization techniques to optimize equation (11).
We now discuss some implementation and parameter is-sues in this subspace estimation step. In order to compute the weight of each data point, we need find its K nearest neighbors. Due to the high complexity of directly discov-ering K nearest neighbors, in our implementation, we op-timize to perform a bisecting k -means clustering first and discover K nearest neighbors in each cluster only. In esti-mating the set of normal vectors { b i } n i =1 , we observed that the convergence of equation (11) is slow. Hence, in our implementation, we employed a weighted k -means iteration
Note that, we cannot simply use the variance measure to define the interestingness of a subspace. The reason is as follows. A subspace may contain multiple events (e.g., pe-riodical events). If there are multiple events contained in a subspace, then the variance of data points in the orthogo-nal direction (e.g., the  X  X emantic X  dimension) will be small, while the variance of data points in the subspace direction (e.g., the  X  X emporal X  dimension) will be large. Hence, in our approach, we employ the entropy measure to define the interestingness of a subspace. Particularly, we project data points to the two directions respectively and calculate the respective histograms of the distributions. Let h 1 ,h 2 ,  X  X  X  , h m and v 1 ,v 2 ,  X  X  X  ,v n , where h i and v i are individual bins, be the two corresponding histograms. The interesting-ness of a subspace can be computed as follows.

I ( s i )=1  X  [  X  p where p  X  [0 , 1] is a weight which can be determined in experiments to assign different importance to the entropy values in the two directions. For example, if p =1 , then only the temporal  X  X urst X  is considered. The interesting-ness measure takes values from 0 to 1 . The more certain the distributions in two directions, the smaller the entropies in the brackets of Equation (12), the greater the value of interestingness. Given some threshold  X  , subspace s i will be pruned as an uninteresting subspace if I ( s i ) &lt; X  .We observed from our experimental results (Section 5 . 2 ) that I ( s i ) of an interesting subspace s i is usually much greater than I ( s j ) of an uninteresting subspace s j , which makes it easy to decide the value of  X  .

Note that, the calculation of the histograms may be bi-ased by noisy data points. In our implementation, we re-move noise by an inlier growing method before projecting data points. The details are given in a full version of this paper (the reference is omitted temporarily to comply with the double blind reviewing guidelines). The timestamp of the first clicked page of a query session is taken as the occurring time of the session. We manu-ally identified a set of events from the data set, including both predictable events (e.g., the Memorial Day on May 29 , 2006 ) and unpredictable events (e.g., the death of Dana Reeve, an American actress, on Mar 6 , 2006 ). For each event, we identified a set of query keywords and selected all query sessions which contain the query keywords and happen close to the date of the event. After filtering events which are represented by less than 50 query sessions, a total of 35 events are used in our experiments. The complete list of events is given in the full version of this paper. We then randomly select query sessions which do not represent any real events, together with the query sessions corresponding to real events, to generate five data sets, which respectively contain 5 K, 10 K, 20 K, 50 K and 100 K query sessions.
Performance of DECK . We first compare the perfor-mance of DECK with the existing two-phase-clustering al-gorithm [18]. Given the set of clusters returned as detected events, the existing algorithm [18] finds a best match be-tween discovered clusters and true events. A best match of a true event is defined as a cluster that has the maximum over-lap with the true event in terms of the number of common query-page pairs (query sessions). We further constrain that the number of common query-page pairs (query sessions) should be no less than some specified threshold (in our ex-periments, we set the threshold as 50% of the query-page pairs or query sessions representing the true event). Then, the evaluation metrics, precision and recall , can be com-puted as follows. Precision is the ratio of the number of correctly detected events to the overall discovered clusters. Recall is the ratio of the number of correctly detected events to the total number of events.

The experimental results are shown in Figures 5 ( a ) and ( b ) respectively. The existing algorithm is referred to as 2PClustering in the figures (please ignore the other two al-gorithms, DECK-GPCA and DECK-NP, temporarily). We observe that DECK outperforms 2PClustering in both pre-cision and recall. Since the algorithm 2PClustering did not discuss how to decide the number of clusters, we use the number of events generated by DECK. Then, since many clusters generated by 2PClustering do not represent any events, both the precision and recall values are low.
Since the best match approach used by the existing al-gorithm may not be objective enough, we further evaluate the performance using an entropy measure. For each gen-erated cluster i , we compute p ij as the fraction of query-page pairs (query sessions) representing the true event j . Then, the entropy the of cluster i is E i =  X  j p ij log p ij . The total entropy can be calculated as the sum of the en-tropies of each cluster weighted by the size of each cluster: E = m i n i  X  E i n , where m is the number of clusters, n is +  X  X  X  +  X  r &lt; . They recursively increase the number of subspaces until the condition is satisfied. Hence, in our ex-periments, the value r is automatically decided by fixing the value of as 1 . 0 E-3 for both GPCA and KNN-GPCA. Then, the ratios of the gap computed by KNN-GPCA to the gap computed by GPCA are shown in Figure 6. We notice that
KNN-GPCA does enlarge the gap and improves the possi-bility of selecting appropriate . Figure 7. Performance of subspace pruning.

Performance of Subspace Pruning . In order to exam-ine the subspace pruning step of DECK, we implemented another alternative version of DECK, referred to as DECK-
NP (stands for DECK with No Pruning), which skips the subspace pruning step. The performance of DECK-NP is shown in Figures 5 ( a ) , ( b ) and ( c ) also. Since DECK-
NP does not prune any uninteresting subspaces, more clus-ters will be generated by this algorithm. Hence, although it achieves similar recall as DECK, its precision is even lower than 2PClustering. Since it employs KNN-GPCA to esti-mate subspaces, the correctly discovered clusters are of high quality. Hence, its entropy is better than DECK-GPCA.
We further conduct experiments to examine whether the threshold  X  , which is used to prune uninteresting subspaces, can be selected easily. The experiments are conducted on the five datasets by varying the number of events. For each dataset, we order the estimated subspaces according to their interestingness values. We then compute the interesting-ness ratio between the pair of successive subspaces with the greatest difference in their interestingness values. The ex-click-through data. The main features of DECK are sum-marized as follows. Firstly, DECK simultaneously consid-ers the semantic information and temporal information of click-through data. By transforming click-through data to points in 2 D polar space such that the angles and radiuses of points respectively reflect the semantics and timestamps of queries, information of the two dimensions are consid-ered at the same time in both the subspace estimation step (e.g., each data point is weighted based on the distributions of its K nearest neighbors in both the subspace direction and the orthogonal direction) and the subspace pruning step (e.g., entropy in both the subspace direction and the orthog-onal direction are used to measure the interestingness of a subspace). Secondly, in the subspace estimation step, data points are weighed based on the distribution of its K near-est neighbors instead of kNND, so that a particular type of outliers can be identified. Thirdly, in the subspace pruning step, an interestingness measure based on entropy instead of variance was proposed, so that subspaces containing multi-ple events can be identified as interesting. The experimen-tal results on real-life Web click-through data show that our approach is accurate and effective in detecting events from Web click-through data.

