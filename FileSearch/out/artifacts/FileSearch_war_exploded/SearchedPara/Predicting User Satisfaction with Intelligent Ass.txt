 There is a rapid growth in the use of voice-controlled intelligent personal assistants on mobile devices, such as Microsoft X  X  Cortana, Google Now, and Apple X  X  Siri. They significantly change the way users interact with search systems, not only because of the voice control use and touch gestures, but also due to the dialogue-style nature of the interactions and their ability to preserve context across different queries. Predicting success and failure of such search di-alogues is a new problem, and an important one for evaluating and further improving intelligent assistants. While clicks in web search have been extensively used to infer user satisfaction, their signif-icance in search dialogues is lower due to the partial replacement of clicks with voice control, direct and voice answers, and touch gestures.

In this paper, we propose an automatic method to predict user satisfaction with intelligent assistants that exploits all the interac-tion signals, including voice commands and physical touch gestures on the device. First, we conduct an extensive user study to mea-sure user satisfaction with intelligent assistants, and simultaneously record all user interactions. Second, we show that the dialogue style of interaction makes it necessary to evaluate the user experience at the overall task level as opposed to the query level. Third, we train a model to predict user satisfaction, and find that interaction signals that capture the user reading patterns have a high impact: when in-cluding all available interaction signals, we are able to improve the prediction accuracy of user satisfaction from 71% to 81% over a baseline that utilizes only click and query features.

Spoken dialogue systems have been thoroughly studied in the lit-erature [37, 46 X 48]. However, it has only been in recent years that a new generation of intelligent assistants, powered by voice, such as Apple X  X  Siri, Microsoft X  X  Cortana, Google Now, have become common and popular on mobile devices. One of the reasons for the increased adoption is the recent significant improvement in ac-curacy of automatic speech recognition [38]. Intelligent assistants support multiple scenarios ranging from web search to proactive Figure 1: Example of search dialogue with intelligent assistant. user recommendations [42]. In this work, we focus on dialogue mode of interaction with intelligent assistants. In this mode, a conversation takes place between the user and the intelligent as-sistant: the user speaks to the intelligent assistant, it responds and user speaks back, frequently referring to the subject of the previous request. This method of interaction is a more natural way for peo-ple to communicate and is often faster and more convenient (e.g., while driving) than typing. We call this type of interaction with intelligent assistants X  search dialogue .

In search dialogue, users go through a sequence of steps in order to reach a desired goal: they solve one or more tasks, each of which consists of one or more search queries. As an example, consider the user dialogue in Figure 1: our user is trying to arrange a weekend in San Francisco. She has many tasks, from checking the weather to finding a hotel, or finding directions, etc. The user is engaged in a  X  X rue X  dialogue, i.e. the context is carried over across queries. When the intelligent assistant loses this context on Q 7 , the user has to repeat some of the queries to rebuild the context and most probably gets dissatisfied with the intelligent assistant. So search dialogues are complex interactions, powered by voice control, with longer sessions consisting of different tasks and changes of focus within the same context. This is very different from traditional search in the query-response paradigm, and here session context becomes of crucial importance.

Clearly, evaluation of user satisfaction is an essential part of de-velopment of any intelligent assistant, as well as any traditional web search application. The ability to measure user satisfaction provides an understanding of the direction to take in order to im-prove the system. We can see from the example in Figure 1 that user satisfaction with search on intelligent assistants makes sense only for the entire dialogue, not as satisfaction with each query of a dialogue separately.
This prompts the need to better understand how users interact with search dialogue and how to define success and failure in terms of user experience X  X hen are users (dis)satisfied? More specifi-cally, we want to understand how we can measure and predict user satisfaction with search dialogue in ways that reflect perceived user satisfaction, and whether we can use traditional methods of offline and online evaluation or need to take other factors into consider-ation. The common practice for evaluating is to create a  X  X old X  standard (set of  X  X orrect X  answers) judged by editorial judges [22]. In the case of search dialogue, there may be no general  X  X orrect X  an-swer since the answers are highly personalized and contextualized (e.g. to a user X  X  location or a user X  X  past searches) to match better user-information needs. Another way to evaluate web search per-formance is through the use of implicit relevance feedback such as clicks, query length and landing page dwell time [2, 13, 17, 25, 26].
User satisfaction is widely adopted as a subjective measure of the quality of the search experience [27]. We know that user satisfac-tion for mobile web search is already very different when compared to desktop search [34]. The case of search dialogue is even more challenging for the measurement of user satisfaction [24]. Due to voice input-output to obtain answers directly from search dialogue without clicking, implicit relevance signals become far more im-portant . The use of voice commands leads to a substantial increase in the length of queries: from 3 . 26 terms per query on average for mobile search to 4 . 48 for search dialogue, while also dramati-cally lowering the number of clicks per Search Engine Result Page (SERP): from 0 . 67 to 0 . 30 1 . Previous work [24] has modeled user satisfaction with intelligent assistants using generic explicit interac-tion signals (e.g. clicks, intelligent assistants request and response features, etc.) to simulate mobile search tasks, but the characteris-tics of more complex interactions and important touch-based sig-nals were left unexplored. And [32] investigated self-reported user satisfaction in a related user study with a range of intelligent assis-tant tasks: device control, web search, and search dialogue. In this paper, we encompass all touch-based physical gestures that control the mobile viewport location (visible region on the mobile device), and screen taps (clicks), for the purpose of inferring user satisfac-tion with search dialogue. Concretely, our main research problem is: We breakdown our general research problem into three specific re-search questions.
 As we show in Figure 1, a search dialogue is a sequence of user queries where each query is a step towards user satisfaction or frus-tration. We analyze interactions within search dialogue gradually increasing complexity of tasks and look at satisfaction with tasks. Clicks in web search have been extensively used to infer user sat-isfaction but clicks in search dialogue have lower significance due to the use of voice control and direct answers that does not require users to click. More insights can be gained by considering other in-teraction signals that characterize physical interaction with mobile devices. We investigate whether users X  touch interactions provide
Statistics are calculated based on two weeks traffic of a commer-cial intelligent assistant in July 2015 useful signals for modeling user satisfaction for search dialogue and if they are more effective than using of general query, session, and click-based features.
 We analyze if touch-based features are important, while training an interaction-based predictor of satisfaction for search dialogue. Fur-thermore, we investigate which interaction signals are more impor-tant to predict user satisfaction by performing a correlation analysis between the interaction features. To answer our research questions, we set up a lab study with realistic tasks [5] for search dialogue derived from real user logs of a commercial intelligent assistant, measuring a wide range of aspects of user satisfaction. We use the outcome of the user study to understand and predict user satisfac-tion with intelligent assistants.

The remainder of this paper is organized as follows. Section 2 describes earlier work and background. We define user satisfaction though interaction signals for search dialogues in Section 3. Then, Section 4 introduces an approach for modeling user interaction with search dialogues. Section 5 provides a detailed description of the user study design to gather satisfaction labels. Finally, Section 6 reports our results, findings, and limitations. We conclude and dis-cuss possible extensions of the current work in Section 7.
This paper is relevant to three broad strands of research. First, we discuss research on spoken dialogue systems which are prede-cessors of the current intelligent assistants on mobile devices. Sec-ond, our work is related to evaluation of search quality because we propose a new model to evaluate user satisfaction with search dia-logues. Third, our work is closely connected to the previous studies about user satisfaction in web search systems because we suggest a way to define and predict user satisfaction for search dialogues. Spoken Dialogue Systems The main difference between tradi-tional web search and intelligent assistants is their conversational nature of interaction. In the conversation mode of intelligent as-sistant, the technology can refer to the previous users X  requests in order to understand the context of a conversation. For instance, in Figure 1 by asking Q 4 the user assumes that the intelligent assis-tant will  X  X now X  that she is still interested in  X  X otels in Mountain View X . Therefore, spoken dialogue systems [37] are closely related to intelligent assistants. Spoken dialogue systems understand and respond to the voice commands in a dialogue form; this area has been studied extensively over the past two decades [46 X 48]. Most of these studies focused on systems that have not been deployed in a large scale and hence did not have the necessary means to study how users interact with these systems in real-world scenar-ios, which led to most of the effort in evaluating spoken dialogue systems focusing on offline evaluation. Moreover, intelligent as-sistants on mobile devices support multiple scenarios of use com-pared with traditional spoken dialogue systems. For example, in addition to voice system response, intelligent assistants on mobile devices provide web search results, direct answers or proactive rec-ommendations [42]. From these perspectives, intelligent assistants are similar to multi-modal conversational systems [20, 49].
This work is different from previous work on spoken dialogue systems in that, we study intelligent assistants on mobile devices and focus on analysis of user behavior that allows us to evaluate the system in an online setting, as well as identify instances of dis-satisfaction with the system performance.
 Search Quality Evaluation Historically, the key objective of in-formation retrieval systems is to retrieve relevant information, typi-cally in the form of documents or references to documents [40, 41]. In the simplest form, relevance can be defined as a score for a query-document pair. Given this query-document relevance score, many metrics have been defined, such as MAP, NDCG, DCG, MRR, P@n, TBG, etc. [22]. For such a setup, we have a collection of documents and queries that are annotated by human judges; such a setup is commonly used at TREC 2 .

Recently online controlled experiments, such as A/B testing, have become widely used technique for controlling and improving search quality based on data-driven decisions [33]. This methodology has been adopted by many leading search companies such as Bing [7], Google [45], Facebook [4], and Yandex [10]. An A/B test is de-signed to compare two variants of a method (e.g. ranking on SERP, ads ranking , etc.) at the same time by exposing them to two user groups and by measuring the difference between them in terms of a key metric (e.g. the revenue, the number of visits, etc.), also known as an overall evaluation criterion. There are many existing studies towards better online evaluation which were devoted to inventing new metrics [9, 11] or improving existing ones [10]. The main goal of these studies was to make these metrics more consistent with the long-term goals [33]. User engagement metrics show different aspects of user experience. For instance, they can reflect (1) user loyalty  X  the number of sessions per user [43], (2) user activity  X  the number of visited web pages [35] or the absence time [11]. The periodicity engagement metrics of user behavior, which resulted from the Discrete Fourier transform of state-of-the-art engagement measures were applied in [9].

Our work is related to the online evaluation line of work since our objective is to build models that can be used to evaluate intelligent assistants, possibly in A/B testing settings. Our work is different in that we do not focus on how to run A/B experiments, rather we only focus on creating models that can be used to predict satisfaction. User Satisfaction User satisfaction is widely adopted as a sub-jective measure of search experience. Kelly [27] proposes a defini-tion:  X  X atisfaction can be understood as the fulfillment of a specified desire or goal X . Furthermore, recently researchers studied different metrics reflective of user satisfaction, such as effort [52], and it has been shown that user satisfaction at the query-level can change over time [30, 31] due to some external influence. These changes lead to the necessity of updating the data collection. Query-level satisfac-tion metrics ignore the information about users X   X  X ourney X  from a question to an answer which might take more than one query [23]. Al-Maskari et al. [3] claim that query-level satisfaction is not ap-plicable for informational queries. Users can run follow-up queries if they are unsatisfied with the returned results; reformulations can
Text REtrieval Conference: http://trec.nist.gov/ lead users to an answer  X  this scenario is called task-level user satis-faction [8, 17]. Moreover, Kelly et al. [29] have provided evidence that the most complex search tasks were similar to the work [6] characterization of complex tasks with respect to having multiple interdependent parts that needed to be addressed separately.
Previous research proposed different methods for identifying suc-cessful sessions. Hassan et al. [17] used a Markov model to pre-dict success at the end of the task. Ageev et al. [1] exploited an expertise-dependent difference in search behavior by using a Con-ditional Random Fields model to predict a search success. Au-thors used a game-like strategy for collecting annotated data by asking participants to find answers to non-trivial questions using web search. On the other hand, situations when users are frustrated have also been studied. Feild et al. [12] proposed a method for un-derstanding user frustration. Hassan et al. [18] and Hassan Awadal-lah et al. [19] have found that high similarity of successive queries is an indicator of an unsuccessful task. Our work is different from this line of work in that we focus on intelligent assistants while all these methods focus on analyzing user behavior when users interact with traditional search systems.

Most recently, user satisfaction for intelligent assistants on mo-bile devices started to gain attention [24]. Jiang et al. [24] focused on simulated tasks for device control and web search, and iden-tify satisfactory and unsatisfactory sessions based on features used in predicting satisfaction on the web, as well as acoustic features of the spoken request. They do not focus on complex search dia-logues and use use generic signals commonly used in Web search satisfaction modeling such as clicks and queries.

Sometimes, the information displayed on a SERP is sufficient to satisfy the users X  information need. This phenomenon is called good abandonment [36, 44, 51] and was studied in [16] for mobile devices. The authors modeled viewing behavior based on touch in-teraction, and demonstrated the correlation of document relevance and viewport changing patterns on touch-enabled mobile devices. Recent research by Lagun et al. [34] extended this line of research to model the viewport for inferring user attention and satisfaction with SERPs. The absence of clicks is an emerging problem for in-telligent assistants as well because they are frequently controlled by voice input.
 formation retrieval studies in terms of task complexity and diffi-culty, and found that the number of tasks and the number of facets were the main dimensions of task complexity. Recently, Kelly [28] linked perceived task complexity and effort, suggesting that user satisfaction may depend on the amount of effort to complete a com-plex task.
Our work focuses on modeling user satisfaction for intelligent assistants. We specifically focus on complex types of interaction  X  X earch dialogue. We show that interaction signals are essential to infer user satisfaction with search dialogue and demonstrate how they can be used in practice. We also focus on studying new in-teraction signals (such as touch and viewport changes) to model user X  X  attention. We introduce a general notion of user satisfaction and exploit an extended list of interaction signals in order to predict user satisfaction with search dialogue
To summarize, the key distinctions of our work compared to previous efforts are: we studied a new method of user interaction with intelligent assistants on mobile devices, search dialogue, and we proposed a method to measure and predict user satisfaction for search dialogue using touch-interaction signals. Our metric is ap-plicable to evaluation both online (e.g., introducing a new ranker or answer type for the intelligent assistant) and offline (e.g., mining search dialogues where users are dissatisfied).
In this section we investigate RQ1: How can we define user satisfaction with search dialogues? In the case of search dialogue, the key distinction of this scenario is the ability of the intelligent assistant to maintain the context of the conversation. Moreover, re-sponses provided by intelligent assistants can be either in the form of a structured answer or in the form of the usual mobile SERP. Fig-ure 2 (A) and (B) illustrates examples of structured answers from a commercial intelligent assistant. Examples of tasks when this type of interaction is activated include requests about restaurants, hotels, travel, weather, etc. Structured answers differ significantly from the usual mobile SERP (e.g., Figure 2 (C)). We characterize different types of search dialogues based on our broad analysis of the logs from a commercial intelligent assistant (Section 3.1). We also present a generalized definition of user satisfaction with search dialogue using interaction signals (Section 3.2).
After intensive analysis of the logs of a commercial intelligent assistant, we split search dialogues into two types: single task search dialogues and multi-task search dialogues. Roughly 50-55% of in-teractions can be characterized as single task search dialogues, the rest as multi-task search dialogues.
 Single Task Search Dialogue Single task search dialogue has one underlying atomic information need and mostly consists of one query and one answer. An example of a single task search dialogue is the weather-related information need shown in Figure 2 (A). Sin-gle task search dialogues are very similar to mobile web search and follow the query-response paradigm. We expect that they can be evaluated using query-level satisfaction.
 Multi-Task Search Dialogue Multi-task search dialogue consists of multiple interactions with the intelligent assistant that lead to-wards one final goal e.g.  X  X lan a night out X . These long and com-plex interactions can be divided into a series of tasks. Obviously, multi-task search dialogues are more complex than other search di-alogues because of a greater number of interactions whereby the user speaks to the intelligent assistant, the intelligent assistant re-sponds, the user speaks back to it and so on.

An example of multi-task search dialogue is presented in Fig-ure 2 (B): the intelligent assistant is used to arrange a dinner. The user makes the following transitions in this search dialogue: Figure 3: An example of the search dialogue where structured answer and general web SERP are used.
We notice that some user needs turn out to be too complex to an-swer with the structured interface. An example where a user needs help with a stomach ache that is shown in Figure 3. In this case, the intelligent assistant used both general web search and structured di-alogue interface to respond to the user X  X  requests. The intelligent assistant redirects a user to general search if the intelligent assistant deems that general SERP will satisfy the user X  X  information needs better such as for queries: Q 1 , Q 2 , Q 5 and Q 6 in Figure 3.
A search dialogue is not just a sequence of  X  Q, SERP  X  pairs consisting of the SERP returned by intelligent assistant in response to the voice query Q . Search dialogue consists of one or more tasks, each of which consists of one or more queries. To better un-derstand requirements for the user study setup, we divide search dialogues into single-and multi-task. Our hypothesis is that it is important for evaluation of user satisfaction with intelligent assis-tants if a response to a voice query Q can be either in a structured form ( SERP str , see Figure 2 (A) and (B)) or in a form of a general web search ( SERP web , see Figure 2 (C)).
Based on our analysis of a commercial intelligent assistant logs we hypothesize that much of the frustration happens when the in-telligent assistant is not able to maintain the context and users need to start their search over in order to complete their tasks. As we present in the example in Figure 1, the intelligent assistant lost the context in the transition Q 6  X  Q 7 due to an automatic speech recognition error, and the user had to start over. Overall user satis-faction with the search dialogue decreases dramatically in this case despite the fact that the user seemed to be satisfied with the previ-ous transitions: Q 1  X   X  X  X   X  Q 6 . Furthermore, it is likely to be especially frustrating since the mistake happens at the end of the session.

Single task search dialogue has one main task T that can be rep-resented as follows: T =  X  Q 1 , SERP 1  X  , . . . ,  X  Q n For any given task T , there are a set of interaction signals (e.g. touch, viewport change, etc.) that we denote as I ( T ) and it can be defined as function f that combines all interactions for every  X  Q, SERP  X  pair in T :
I ( T ) = f I  X  Q 1 , SERP 1  X  , . . . , I  X  Q n , SERP n In the case of multi-task search dialogue, the search dialogue has more than one task and can be viewed as a sequence of tasks: T , . . . , T m . Interaction signals within the search dialogue are de-fined through the function g that aggregates user interaction over tasks happening during search dialogue: Table 1: Description of implicit features per search dialogue Feature Name Feature Description F 1 NumQueries Number of queries F 2 NumClicks Number of clicks F 3 NumSATClicks Number of clicks ( &gt; 30 sec. dwell time) F 4 NumDSATClicks Number of clicks (  X  15 sec. dwell time) F 5 TimeToFirstClick Time (seconds) until the first click F 6 MetaphoneLevenstein Levenstein similarity between pronunci-F 7 MetaphoneSubstring Substring similarity between pronuncia-
Our objective is to define a function h that given a set of inter-action signals would predict whether the user was satisfied or not. For multi-task search dialogues, h can be defined as: In the case of a single task search dialogue that consists of one task T , Equation 3 would be simplified to SAT ( T ) = h I ( T ) . If single task search dialogue consists of a single query, the equation can be further simplified to SAT ( T ) = h I (  X  Q, SERP  X  ) , like in standard query-level satisfaction.

In this section, based on extensive analysis of the logs of an in-telligent assistant, we characterized search dialogues as single-and multi-task, divided queries as giving either a structured or a general web search response, and conceptually modeled user satisfaction with search dialogues. Additionally we illustrated that the over-all user satisfaction with search dialogue cannot be reduced to the query or even task level satisfaction, because of the dependency be-tween them and the expectation that the intelligent assistant main-tains the context during the whole interaction within a dialogue.
This section addresses RQ2: How can we predict user satis-faction with search dialogues using interaction signals? First, we describe used interaction signals that are logged as the following two types of features: (1) general implicit features which have been used in previous work on characterizing user behavior with general Web search [2, 13, 17, 25] and intelligent assistants [24] (Section 4.1), and (2) touch and attention features which, we be-lieve, provide a different perspective for modeling satisfaction with search dialogues (Section 4.2). Note that some of these features were also shown to be useful for predicting the relevance of web search results [15, 16, 34]. These two types of features are used to define I (  X  Q, SERP  X  ) which is a component of Equation 1. Fi-nally, we present a method for modelling user interaction with the search dialogue task T to represent I ( T ) from Equation 1 (Sec-tion 4.3). Queries and Click Features ( F 1 , . . . , F 5 ) : In our case click means tapping a result item (e.g., the best answer from a list of candi-dates). We use the following features that are calculated across the entire search dialogue task: the number of queries ( F 1 ber of clicks ( F 2 ), the number of satisfied clicks, defined as clicks with dwell time &gt; 30 seconds ( F 3 ), as well as the number of dis-satisfied clicks, defined as clicks with dwell time  X  15 seconds ( F 4 ), and the total time (seconds) before the first click in search dialogue ( F 5 ). Note that previous work [13] has shown that long dwell time clicks ( &gt; 30 seconds) are highly likely to indicate satis-faction while quick-back clicks (  X  15 seconds) are highly likely to indicate dissatisfaction.
 Table 2: Description of touch features per search dialogue Feature Name Feature Description F 9 NumSwipes Number of Swipes F 10 NumUpSwipes Number of up-swipes F 11 NumDownSwipes Number of down-swipes F 12 SwipedDistance Total distance swiped (pixels) F 13 AvgNumSwipes Number of swipes normalized by time F 14 AvgSwipeDistance Total distance divided by num. of swipes F 15 DistanceByTime Total swiped distance divided by time F 16 DirectionChanges Number of swipe direction changes F 17 DurationPerAns SERP answer duration (seconds) which is F 18 FractionPerAns Fraction of visible pixels belonging to F 19 ReadTimePerAns Attributed time (seconds) to viewing a F 20 1DReadTimePerPix Attributed time (seconds) per unit height F 21 2DReadTimePerPix Attributed time (milliseconds) per unit Acoustic Features ( F 6 , F 7 ) : We utilize acoustic feature to char-acterize voice interaction happening in search dialogues. More specifically, we use the phonetic similarity between consecutive requests to identify patterns of repetition. Metaphone representa-tion [39] is a way of indexing words by their pronunciation that al-lows us to represent words by how they are pronounced as opposed to how they are written. Phonetic similarity is assessed by com-puting the edit distance between the Metaphone representation of two utterances. For example, a voice query  X  X hatsApp X  may be in-correctly recognized as  X  X hat X  X  up X  , but their metaphone codes are both  X  X TSP X  . In such cases, this phonetic similarity feature helps us detect repeated or similar requests that are missed by normal text similarity features based on recognized speech. As similari-ties metrics we use Levenstein Distance ( F 7 ) and Substring ( F Table 1 lists the utilized implicit features: ( F 1 , . . . , F
One of the main contributions of this work is the introduction of touch and attention features for detecting user satisfaction with search dialogues. We focus on touch-based features related to the way in which users interact with the screen and features based on elements visible to users. This serves as a surrogate for what the user is paying attention to on the page and how this changes throughout the search dialogue. Table 2 lists the used touch fea-tures. Capturing touch events is not easy in practice because of non-standard instrumentation [21]. We derive interaction features and the exact information that was displayed on the phone screen at any given time using mobile viewport logging . This allows us to record the portion of the answer/result currently visible on the screen, as well as bounding boxes of all results shown on the page. For instance, if an element is visible in the viewport at some point in time and then no longer visible, one can infer that a gesture must have taken place. Furthermore, if an element below the original element becomes visible, then one can infer that it must have been a downward swipe action. We use element-tracking in the view-port to infer features related to swipes happening during search di-alogue: F 9 , . . . , F 16 .

Lagun et al. [34] showed that there is strong correlation between the time for which a result is visible and its gaze time. Follow-ing this observation, we approximate how much attention different (A) F 19 = ABributed Reading Time per SERP Answer (B) ABributed Reading Time per Pixel for SERP Answer Figure 4: The illustration how to capture (A): F 19 TimePerAnswer and (B): F 20 , F 21 ReadingTimePerPixel .
 SERP elements get. Features F 17 , F 18 are used to characterize vis-ibility of SERP answers. The feature F 19 attempts to attribute the time the user spends looking at each stationary viewport to the dif-ferent elements based on their area. Features F 20 and F sponsible for reading time per pixel, they normalize the attributed reading time so that size of the content region does not introduce a systematic weight into the metric. Figure 4 illustrates how F is captured in the example (A) and how F 20 and F 21 are calcu-lated in the example (B). To aggregate the features F 17 , . . . , F the  X  Q, SERP  X  -level, we use four types of aggregation: average ( Avg ), maximum ( M ax ), minimum ( M in ), and standard devia-tion ( Stdev ).

We presented a list of implicit and touch features that are col-lected on  X  Q, SERP  X  -level during user interaction with search di-alogues on intelligent assistants. We define I (  X  Q, SERP  X  ) by the feature vector: ( F 1 , . . . , F 21 ) . Next, we will explain how to model user interaction on the task level X  I ( T ) .
We showed that search dialogue tasks have underlying seman-tic structure and potentially can be divided into single task search dialogues and multi-task search dialogues. There is no automatic search dialogue analyser available so we cannot split search dia-logues into tasks on the fly. The goal of this work is to deliver on-line metric for user satisfaction with search dialogues on intelligent assistants. Potentially, the semantic structure of a search dialogue task is not entirely flat and it might have a tree structure. Devel-oping an automatic tool to mine the search dialogue structure is a promising direction for future work.

The intelligent assistant has two types of responses to a voice query Q : either in a structured form, SERP str as illustrated in Figure 2 (A) and (B), or in the form of a general web search, SERP web as illustrated in Figure 2 (C). Our hypothesis is that the type of response of intelligent assistants can be used to approx-imately divide search dialogues into the different types of tasks. Our assumption relies on the internal logic of the intelligent assis-tant that returns SERP str when tasks are about different types of locations (restaurants, hotels, pharmacies, shops etc.), directions to locations, or weather. If the intelligent assistant deems that infor-mation from general web is more suitable for a query then it returns SERP str . This kind of intelligent assistant response still differs from general mobile search because it looks like a dialogue. For ex-ample, if a user voice query can be answered using the knowledge graph then, the intelligent assistant speaks the answer out aloud. We define the function f from Equation 1 through aggregation. Further, in our experiment we use geometric mean as aggrega-tion. We experimented with other aggregation functions, and they yielded similar or worse performance. We apply three techniques to define I ( T ) for the search dialogue consisting of n queries in total, m queries resulted in SERP str and k queries resulted in A : considering only interaction with  X  Q, SERP str  X  : A : considering interactions with all  X  Q, SERP  X  equally: A : separating interactions with  X  Q, SERP str  X  and  X  Q, SERP In this section we introduced the list of features to model user inter-actions. We focused specifically on presenting interaction signals which are promising for modeling user interaction with intelligent assistants. Next, we will describe the set up for our lab study with real-world tasks for search dialogue derived from real user logs. The outcome of the study will be used to understand how important interaction signals are for modeling user satisfaction with search di-alogue.
This section describes the design of the user study to collect user interactions for search dialogues. The collected data is used to in-vestigate our research questions: RQ2 X 3 . While designing tasks for our user study, we rely on the following requirements: (1) the suggested tasks should be realistic; (2) following Borlund [5] we construct the tasks so that participants could relate to them and they would provide  X  enough imaginative context . X  Participants We recruited 60 participants to participate in the study. All participants were college or graduate students residing in the United States. They all had good command of English. 75% of participants were male and the remaining 25% were female. The average age of participants is 25.5 (  X  5.4) years. They were reim-bursed $10 gift card for participating in the study.
 Tasks We analyzed over 400,000 search dialogues from the search logs of a commercial intelligent assistant to generate tasks for user study. Based on our analysis we generated eight tasks for the user study that were designed to cover approximately 70-80% of sub-jects queried by real users of the intelligent assistant. We formulate tasks in a free form in order to encourage query diversity and stim-ulate either genuine satisfaction or frustration with returned results. The final tasks for user study consist of: For each task, we recorded an audio that verbally described the task objective. Following the study [24], we did not show the partici-pants the written description while they were working on the task as it was demonstrated many participants directly used the sentences shown in descriptions as requests. We strongly wanted to avoid such outcome because our goal was simulate real user behavior. Study Setup Participants performed the tasks on a mobile phone with a commercial intelligent assistant installed. If the task needed access to specific device resources, functions or applications (e.g. maps), they were pre-installed to make sure users would not en-counter problems. The experiment was conducted in a quiet room, so as to reduce the disturbance of external noise. Although the real environment often involves noise and interruption, we eliminated those factors to simplify the experiment. While participants were doing the user study all their interactions were logged using an in-ternal API.

The participants watched a 4 minutes video with instructions that explained how to use the intelligent assistant. Then, partic-ipants worked on one training task and eight formal tasks. We instructed participants that they should stop a task when they had accomplished their goal or if they became frustrated and wanted to give up. After completing each task, participants were asked to answer the following four questions: 1. Were you able to complete the task? 2. How satisfied are you with your experience in this task in 3. Did you put in a lot of effort to complete the task? 4. How well did the intelligent assistant recognize your voice? Except for the first question which required a Yes/No answer, all questions were answered using a 5 point likert scale. Addition-ally, to stimulate participants X  involvement in search dialogues, we asked them to answer clarifying question(s) about task output. For example, if the task was about finding a restaurant the participant would need to indicate its name in the questionnaire. The total ex-periment time was about 30 minutes.
 User Study Summary We stimulated participants X  involvement by giving free form tasks. They needed to formulate their own goals for the task and it leads to satisfaction or frustration. For exam-ple, out of 60 responses for the Task C we extracted 46 references to unique places. As a result of free task formulation we obtained a diverse query set, characterized as follows: in total, participants
Link to instructions withheld to preserve anonymity. perform 540 tasks that involved 2 , 040 queries in total of which 1 , 969 were unique; the average query length was 7 . 07 . The sin-gle task search dialogue as Task A generated 130 queries in total, four multi-task search dialogues as Task B generated 685 queries, and three multi-task search dialogues as Task C generated 1 , 355 queries.
We now investigate our RQ3: Which interaction signals have the highest impact on predicting user satisfaction with search dia-logues? We begin by introducing our results on the prediction qual-ity of user satisfaction with search dialogues (Section 6.1). We con-clude by presenting a correlation analysis between the interaction features and user satisfaction (Section 6.2).
The purpose of this study is to predict overall user satisfaction with search dialogues. Therefore we do not utilize graded satisfac-tion in this work but it would be useful for future research. In our user study, users reported overall satisfaction using a 5 point likert scale. Due to the large difference in rating distributions between the single-and multi-task search dialogue we consider the evaluation as a binary classification problem. We divide the labeled search di-alogues into binary classes: satisfied (SAT)  X  users provided 5 or 4; dissatisfied (DSAT)  X  everything else. This resulted in the fol-lowing proportion of positively and negatively labeled search dia-logues: SAT  X  64% and DSAT  X  36%.

We formulate a supervised classification problem where, given a search dialogue, the goal is to classify it to SAT or DSAT. We train Gradient Boosted Decision Trees (GBDT) [14] as a satis-faction predictor h presented in Equation 3. We experiment with other classifiers (logistic regression, SVM), and they yield similar or worse performance. Hence we only report the results of GBDT. We use 10-fold cross validation. For each training fold, we use grid search to optimize for the number of leaves, tree depth, and number of leaves required to split. We train our predictors based on differ-ent subsets of features from ( F 1 , . . . , F 21 ) . For each experiment we report the overall accuracy (Acc), average F 1 score (Avg. F area under the curve (AUC); and precision (P), recall (R) and F score (F 1 ) for SAT and DSAT separately in Table 3.

The baseline is the classifier trained on queries and click features which are aggregated over a search dialogue using Equation 4. We observe that the baseline is overly optimistic with a low DSAT re-call (30%) and high SAT recall (93%), showing that it is effective in picking up the imbalance in SAT/DSAT distribution but far less ef-fective in distinguishing satisfaction from dissatisfaction. We train the predictor P 1 on an expanded feature set, adding the Metha-phone features ( F 6 , F 7 ) . From Table 3, we can see that the predic-tor P 1 shows statistically significant improvement ( p &lt; 0 . 05 ) in Acc , SAT P, DSAT R, DSAT F 1 , Avg. F 1 and AUC when com-pared against the baseline. Next, we expand feature set by adding the touch signals from Table 2.

We use the three proposed techniques for feature aggregation over task(s) while training based on ( F 1 , . . . , F tion 4) for the predictor P 3 , A 2 (Equation 5) for P 4 , and A tion 6) for P 5 . Based on results in Table 3, we can infer that the predictors P 2 , P 3 and P 4 demonstrate statistically significant im-provements ( p &lt; 0 . 05 ) in Acc, SAT P, DSAT R, DSAT F and AUC when compared against the baseline, indicating that the touch features incorporated in prediction models are fundamental to evaluation of user satisfaction with search dialogues. Also from Table 3, we can infer that the aggregation A 3 (when we separate user interactions: SERP str and SERP web ) is most beneficial one when compared against the baseline. In the next subsection, Features Description Accuracy (%) Precision SAT (%) Recall F P : A 1 ( F 1 , . . . , F 7 ) (Eq. 4) 78.53 (+4.25) P : A 1 ( F 1 , . . . , F 21 ) (Eq. 4) 78.78 (+4.88) P : A 2 ( F 1 , . . . , F 21 ) (Eq. 5) 80.21 (+6.07) P : A 3 ( F 1 , . . . , F 21 ) (Eq. 6) 80.81 (+6.03) Table 4: Pearson correlations between satisfaction (SAT) and implicit features. Results are statistically significant ( p &lt; 0 . 05 ) Feature Type Correlation
F 7 ( Q i , Q i +1 ) [MetaphoneSubstring] 0.45
F 4 [NumDSATClicks] 0.31
F 5 [TimeToFirstClick] 0.30
F 2 [NumClicks] 0.27
F 6 [MetaphoneLevenstein] 0.23
F 3 [NumSATClicks] 0.12
F 7 ( Q i +1 , Q i ) [MetaphoneSubstring] -0.16
F 1 [NumQueries] -0.49 we present features analysis to characterize the relative importance of different features.
To understand the impact of implicit features ( F 1 , . . . , F Table 1, we calculate the Pearson correlation between the user sat-isfaction label (SAT) and each feature. The results are presented in Table 4. Feature F 7 ( Q i , Q i +1 ) , which indicates that the sub-sequent query Q i +1 in the task contains prior query Q i tively correlated with SAT. Expanding the query, or rather refin-ing the query to better specify the intent, is a common user behav-ior and is expected to increase the probability of finding satisfac-tory content on the subsequent SERP. The complementary feature, F ( Q i +1 , Q i ) , however, reflects the case where the subsequent query Q i +1 in the task is contained within the prior query Q feature is negatively correlated with SAT. Speech recognition errors in Q i  X  Q i +1 can give rise to this type of feature, and the neg-ative correlation is expected from such transitions. Our findings are similar to the previously reported results [24]. Based on rela-tively high correlation between click-based features ( F 2 we infer that clicks during search dialogues can be interpreted as a sign of user satisfaction. We find that the search dialogue length, in terms of F 1 , is negatively correlated with satisfaction. Long con-versations can be result from two types of behaviors: (a) multiple attempts of users to have their speech properly recognized, or (b) the loss of context by the intelligent assistant during the conversa-tion, forcing users to restart the conversation; both of these explain the observed negative correlation.

Table 5 shows the results of correlation analysis for the touch features ( F 8 , . . . , F 21 ) using aggregations A 2 and A the top 5 positively correlated features and the top 5 negatively cor-related features. To explain the correlations, we present three hy-potheses: Table 5: Pearson correlations between satisfaction (SAT) and touch features. Results are statistically significant ( p &lt; 0 . 05 ) Feature Type Cor.
 Stdev ( F 18 ) [FractionPerAns] 0.23 M in ( F 20 ) [1DReadTimePerPix] 0.20 Stdev ( F 19 ) [ReadTimePerAns] 0.19 Avg ( F 20 )) [1DReadTimePerPix] 0.19
M ax ( F 20 )) [1DReadTimePerPix] 0.18
F 10 [NumUpSwipes] -0.10
F 9 [NumSwipes] -0.12
F 11 [NumDownSwipes] -0.12
F 12 [SwipedDistance] -0.13
F 15 [DistanceByTime] -0.18 I  X  Q, SERP str  X  : M ax ( F 18 ) [FractionPerAns] 0.35 I  X  Q, SERP str  X  : Stdev ( F 18 ) [FractionPerAns] 0.34 I  X  Q, SERP str  X  : M ax ( F 19 ) [ReadTimePerAns] 0.32 I  X  Q, SERP str  X  : Avg ( F 18 ) [FractionPerAns] 0.31
I  X  Q, SERP str  X  : Avg ( F 19 ) [ReadTimePerAns] 0.31 I  X  Q, SERP web  X  : M in ( F 20 ) [1DReadTimePerPix] -0.35 I  X  Q, SERP web  X  : Stdev ( F 18 ) [FractionPerAns] -0.28 I  X  Q, SERP web  X  : M in ( F 18 ) [FractionPerAns] -0.32 I  X  Q, SERP web  X  : Avg ( F 18 ) [FractionPerAns] -0.35 I  X  Q, SERP web  X  : M ax ( F 18 ) [FractionPerAns] -0.35
H 1 : The SERP for a query is ordered by a measure of relevance as determined by the system, then additional exploration is unlikely to achieve user satisfaction, but is more likely an indication that the best-provided results (i.e. the SERP top) are insufficient to address the user intent.

H 2 : In the converse case of H 1 , when users find content that satisfies their intent, their likelihood of scrolling is reduced, and they dwell for an extended period on the top viewport.

H 3 : When users are involved in a complex task, they are dis-satisfied when redirected to a general mobile SERP, as opposed to receiving an explicit structured answer from the intelligent assistant (e.g. the transition Q 4  X  Q 5 in Figure 3). Unlike H 2 , the absence of scrolling on this landing page is an indication of dissatisfaction.
The features in Table 5 are explained in more depth below. A large Stdev ( F 18 ) characterizes the situation where roughly half of the available answers are observed and the other half are not. This would occur when there is minimal or no scrolling behavior, since answers at the top of the SERP are visible and the answers toward the bottom are hidden from view. F 20 is well-defined only for observable content, and when users do not scroll, this value will be identical for all items on the SERP. As such, in the absence of scrolling, M in ( F 20 ) will be large, and therefore a positive correla-tion with SAT is consistent with our hypotheses. F 19 , on the other hand, is well-defined for all answers, observed or not, but will be equal to zero for answers that are not observed. When there is min-imal scrolling and a long dwell on the top viewport, F 19 positive and large for the observed answers, and zero for the un-observed content, giving rise to a large Stdev ( F 19 characterizes the same behaviour as M in ( F 20 ) when users do not scroll at all, but, when users do scroll small distances, M in ( F would drop substantially whereas Avg ( F 20 ) would remain rela-tively stable; a positive correlation with SAT is consistent with H A large M ax ( F 20 ) implies that users paused and dwelled on one portion of the page for an extended period, also consistent with H
Table 5 ( A 2 ) shows that SAT is negatively correlated with ( F . . . , F 12 ) , which describe user swipes. Swipe down, up, or both are signs of exploration of the result set and a negative correlation of number of swipes and swipe-distance with SAT is consistent with H . F 15 provides a measure of the speed of exploration of the content. The observed negative correlation implies that fast swip-ing indicates dissatisfaction, and it is consistent with users who are skimming through and exploring the results without success, sup-porting H 1 . These results are consistent with the findings of [34], who concluded that scrolling is negatively correlated with SAT.
For the aggregation A 3 (Equation 6), we separate interaction with structured answers, I  X  Q, SERP str  X  , and interaction with general mobile SERP, I  X  Q, SERP web  X  . The correlation be-tween SAT and F 18 , F 19 calculated though interaction with SERP is even stronger. The same set of features calculated for interac-tions with SERP web is negatively correlated with SAT, which is consistent with H 3 . Users who are redirected to SERP does not scroll likely land there unintentionally, as a consequence of a voice-misrecognition or loss of context by the intelligent assis-tant. While Table 5 only shows the top features, the entire list of correlations for A 3 are consistent with the H 1 , in agreement with our previous finding for the aggregation A 2 . Furthermore, we can see that swiping actions during interactions with SERP web higher negative correlation than with SERP str . Here, users are plausibly frustrated and perform quick swipes through SERP The above observations lead us to the following conclusion X  X hat users expect to find answers on the SERP without any  X  X dditional effort X  (e.g. scrolling), and users are not satisfied if the intelligent assistant cannot answer their request explicitly and redirects them to a general mobile SERP.

Although our paper shows that our method has a strong poten-tial, there are at least two limitations that can be improved in future work. The first limitation is the collected data during user study which can be improved in terms of size and diversity. One way to do that is to monitor users as they do their normal tasks via addi-tional instrumentation installed on their phones and prompt them to answer questions about their satisfaction. Another area of improve-ment is using data collected from multiple intelligent assistants. Most available intelligent assistants support search dialogues and the features we use are independent of the task subject and hence should be useful regardless of which tasks are supported by which assistants. Nevertheless, training and testing our models on data from different assistants can be very useful for proving their gener-ality. This is particulate challenging though given the difficulty of performing third-party instrumentation on mobile devices.
To summarize, extensively experimenting with the user study data, we concluded that touch and attention based features are ex-tremely helpful for predicting user satisfaction with intelligent as-sistants. Finally, we conducted feature analysis and concluded that active user interactions with the mobile device (e.g., scrolling) is the strong signal of user dissatisfaction with intelligent assistant.
The paper extends earlier work on desktop and general mobile search [16, 24, 34] and presents the first quantitative study for user satisfaction with the modern generation of intelligent assistants. In-telligent assistants allow for radically new means of information ac-cess: making a real dialogue with a context using voice commands and touch interactions. Evaluation of user satisfaction is crucial for intelligent assistants development. As the popularity of intelligent assistants rapidly grows, a strong need for better understanding and precise evaluating of user satisfaction grows correspondingly.
Our main research question was: How can we automatically pre-dict user satisfaction with search dialogues on intelligent assistants using click, touch, and voice interactions? First, we studied RQ1: How can we define user satisfaction with search dialogues? We studied search dialogues by analyzing real logs of a commercial intelligent assistant and introduced two types of the dialogues: sin-gle task search dialogues and multi-task search dialogues. We also illustrated that the dialogue queries can lead to responses either in the form of a structured interface or in the form of general mobile search, when a request is  X  X ut of scope X  of the search dialogue. We defined user satisfaction with search dialogues in the generalized form, which showed understanding the nature of user satisfaction as an aggregation of satisfaction with all dialogue X  X  tasks and not as a satisfaction with all dialogue X  X  queries separately. The introduc-tion of dialogue types and understanding which kinds of responses to queries exist helped us to set up a user study and make feature selection for answering the next research question.

Next we investigated RQ2: How can we predict user satisfaction with search dialogues using interaction signals? To predict user sat-isfaction, we used the following kinds of interactions: clicks (or  X  X aps X  in terms of touches on mobile platforms), other touch inter-actions and voice features. The baseline was predicting user satis-faction using clicks and queries features. We showed that features derived from voice and especially from touch interactions add sig-nificant gain in accuracy over the baseline. To understand how to efficiently select features depending on different types of queries, we proposed three techniques: using only features of queries re-sulting in structured interface; calculating a single set of features for queries resulting in structured interface and queries resulting in general SERP; and calculating separate sets of features for each group of queries resulting in structured interface and queries re-sulting in general SERP. We showed that the third technique is the most accurate to model user satisfaction. This technique improves accuracy from 71% to 81% over the baseline.

Finally, we analyzed the prediction quality of the classifier trained on various selections of interaction features, answering RQ3: Which interaction signals have the highest impact on predicting user satis-faction with search dialogues? We conducted a feature analysis and concluded that users expect to find answers on the SERP directly without putting in any  X  X dditional effort X  (e.g. scrolling). Our anal-ysis showed a strong negative correlation between user satisfaction and swipe actions. Additionally, we demonstrated that users are not satisfied if the intelligent assistant cannot answer their query explicitly and redirects them to a general mobile SERP.

Our general conclusion is that touch based features dramatically improve the prediction quality of user satisfaction with search di-alogue. Research on intelligent assistants on mobile devices is a new area, and this paper addresses some of the first important and necessary steps. We proposed a method for evaluating user satis-faction with intelligent assistants which can be applied in online evaluation of ranking results, offline mining of user dissatisfaction and understanding directions for their future development. We thank the help from Sarvesh Nagpal and Toby Walker for the help in collecting the internal API data for the user study.
