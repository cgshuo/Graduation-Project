 The curse of dimensionality has traditionally been the bane of nonparametric statistics, as reflected for instance in convergence rates that are exponentially slow in dimension. An exciting way out of real world problems the high dimensionality of the data is only superficial and does not represent the true complexity of the problem. In such cases data of low intrinsic dimension is embedded in a space of high extrinsic dimension.
 For example, consider the representation of human motion generated by a motion capture system. Such systems typically track marks located on a tight-fitting body suit. The number of markers, say N , is set sufficiently large in order to get dense coverage of the body. A posture is represented by a (3 N ) -dimensional vector that gives the 3D location of each of the N marks. However, despite this seeming high dimensionality, the number of degrees of freedom is relatively small, corresponding to the dozen-or-so joint angles in the body. The marker positions are more or less deterministic small dimension.
 In the last few years, there has been an explosion of research investigating methods for learning in the context of low-dimensional manifolds. Some of this work (for instance, [2]) exploits the low intrinsic dimension to improve the convergence rate of supervised learning algorithms. Other work (for instance, [12, 11, 1]) attempts to find an embedding of the data into a low-dimensional space, thus finding an explicit mapping that reduces the dimensionality.
 In this paper, we describe a new way of modeling data that resides in R D but has lower intrinsic dimension d &lt; D . Unlike many manifold learning algorithms, we do not attempt to find a single unified mapping from R D to R d . Instead, we hierarchically partition R D into pieces in a manner that is provably sensitive to low-dimensional structure. We call this spatial data structure a random adaptive. k -d trees, RP trees, and vector quantization splitting along one coordinate direction at a time. The succession of splits corresponds to a binary tree whose leaves contain the individual cells in R D . These trees are among the most widely-used methods for spatial partitioning in machine learning and computer vision. RP tree.
 figure by a circle. The partitioning together with these mean vectors define a vector quantization context of lossy compression methods). A good property of this tree-structured vector quantization the error introduced by replacing vectors with their representative.
 We quantify the VQ error by the average squared Euclidean distance between a vector in the set and As the depth of the k -d tree increases the diameter of the cells decreases and so does the VQ error. However, in high dimension, the rate of decrease of the average diameter can be very slow. In fact, as we show in the supplementary material, there are data sets in R D for which a k -d tree requires D of dimensionality.
 this; in fact the bad example mentioned above has intrinsic dimension d = 1 . But we show that a simple variant of the k -d tree does indeed decrease cell diameters much more quickly. Instead of splitting along coordinate directions, we use randomly chosen unit vectors, and instead of split-ting data exactly at the median, we use a more carefully chosen split point. We call the resulting data structure a random projection tree (Figure 1, right) and we show that it admits the following theoretical guarantee (formal statement is in the next section).
 vector quantization construction method for which the diameter of the cells depends on the intrinsic dimension, rather than the extrinsic dimension of the data.
 low-dimensional subspace. In fact, a recent experimental study of nearest neighbor algorithms [8] observes that a similar pre-processing step improves the performance of nearest neighbor schemes based on spatial data structures. Our work provides a theoretical explanation for this improvement and shows both theoretically and experimentally that this improvement is significant. The explana-tion we provide is based on the assumption that the data has low intrinsic dimension. Another spatial data structure based on random projections is the locality sensitive hashing scheme [6].
 Manifold learning and near neighbor search The fast rate of diameter decrease in random projection trees has many consequences beyond the  X  whether used for classification or regression  X  is centered around the rate of diameter decrease; for details, see for instance Chapter 20 of [7]. Thus RP trees generically exhibit faster convergence in all these contexts.
 close to the query. The classical work of Cover and Hart [5] on the Bayes risk of nearest neighbor methods applies equally to the majority vote in a small enough cell.
 Figure 2: Distributions with low intrinsic dimension. The purple areas in these figures indicate re-eas where data density is very low. The left figure depicts data concentrated near a one-dimensional manifold. The ellipses represent mean+PCA approximations to subsets of the data. Our goal is to mean+PCA. The right figure depicts a situation where the dimension of the data is variable. Some of the data lies close to a one-dimensional manifold, some of the data spans two dimensions, and some of the data (represented by the red dot) is concentrated around a single point (a zero-dimensional manifold).
 Finally, we return to our original motivation: modeling data which lie close to a low-dimensional manifold. In the literature, the most common way to capture this manifold structure is to create a graph in which nodes represent data points and edges connect pairs of nearby points. While this is a natural representation, it does not scale well to very large datasets because the computation time of closest neighbors grows like the square of the size of the data set. Our approach is fundamentally are small enough, the data in them can be well-approximated by an affine subspace, for instance that given by principal component analysis. In Figure 2 we show how data in two dimensions can be approximated by such a set of local ellipses. 2.1 Spatial data structures In what follows, we assume the data lie in R D , and we consider spatial data structures built by called C HOOSE R ULE . The core tree-building algorithm is called M AKE T REE , and takes as input a data set S  X  R D . procedure M AKE T REE ( S ) if | S | &lt; M inSize then return ( Leaf ) else principal component direction (for instance, see [9]). procedure C HOOSE R ULE ( S ) comment: PCA tree version let u be the principal eigenvector of the covariance of S
Rule ( x ) := x  X  u  X  median ( { z  X  u : z  X  S } ) return ( Rule ) This method will do a good job of adapting to low intrinsic dimension (details omitted). However, it has two significant drawbacks in practice. First, estimating the principal eigenvector requires a k of the tree. Second, when the extrinsic dimension is high, the amount of memory and computation required to compute the dot product between the data vectors and the eigenvectors becomes the dominant part of the computation. As each node in the tree is likely to have a different eigenvector this severely limits the feasible tree depth. We now show that using random projections overcomes these problems while maintaining the adaptivity to low intrinsic dimension. 2.2 Random projection trees We shall see that the key benefits of PCA-based splits can be realized much more simply, by picking eigenvector u will do a good job of reducing the diameter of the data. But a random direction v will also have some component in the direction of u , and splitting along the median of v will not be all that different from splitting along u .
 Now only medians need to be estimated, not principal eigenvectors; this significantly reduces the data requirements. Also, we can use the same random projection in different places in the tree; all projection direction for each node in the tree. In our experience setting the number of projections equal to the depth of the tree is sufficient. Thus, for a tree of depth k , we use only k projection vectors v , as opposed to 2 k with a PCA tree. When preparing data to train a tree we can compute the k projection values before building the tree. This also reduces the memory requirements for the training set, as we can replace each high dimensional data point with its k projection values (typically we use 10  X  k  X  20 ).
 distance between the two furthest points in the set), and  X  average distance between points of S : We use two different types of splits: if  X  2 ( S ) is less than c  X  2 use the hyperplane split discussed above. Otherwise, we split S into two groups based on distance from the mean. procedure C HOOSE R ULE ( S ) comment: RP tree version if  X  2 ( S )  X  c  X   X  2 then else { Rule ( x ) := k x  X  mean ( S ) k X  median {k z  X  mean ( S ) k : z  X  S } return ( Rule ) that maximally decreases average squared interpoint distance. In Figure 4.4, for instance, splitting the bottom cell at the median would lead to a messy partition, whereas the RP tree split produces two clean, connected clusters.
 Figure 4: An illustration of the RP-Tree algorithm. 1: The full data set and the PCA ellipse that manifold structure. Note: the ellipses are for comparison only; the RP tree algorithm does not look at them.
 were allowed, then a large number of splits would be devoted to uselessly subdividing this point mass. The second type of split separates it from the rest of the data in one go. For a more concrete remaining image patches will be spread out over a much larger space. The effect of the split is then to separate out these two clusters. 2.3 Theoretical foundations In analyzing RP trees, we consider a statistical notion of dimension: we say set S has local covari-To make this precise, start by letting  X  2 matrix; these are the variances in each of the eigenvector directions.
 covariance matrix satisfy  X  2 (1 / 2) X  2 A ( S ) .) Now, suppose an RP tree is built from a data set X  X  R D , not necessarily finite. Recall that there are two different types of splits; let X  X  call them splits by distance and splits by projection . Theorem 2 There are constants 0 &lt; c tree is built using data set X  X  R D . Consider any cell C for which X  X  C has local covariance dimension ( d, ) , where &lt; c contains it at the next level down. As a consequence, the expected average diameter of cells is halved every O ( d ) levels. The proof of this theorem is in the supplementary material, along with even stronger results for different notions of dimension. 3.1 A streaming version of the algorithm The version of the RP algorithm we use in practice differs from the one above in three ways. First that fall in an interval around the median are separated from data outside that interval. Second, update the tree) and immediately discarded. This is managed by maintaining simple statistics at each internal node of the tree and updating them appropriately as the data streams by (more details in the supplementary matter). The resulting efficiency is crucial to the large-scale applications we have in mind. Finally, instead of choosing a new random projection in each cell, a dictionary of a out and the best one (that gives the largest decrease in  X  2 effect of boosting the probability of a good split. 3.2 Synthetic datasets We start by considering two synthetic datasets that illustrate the shortcomings of k -d trees. We will see that RP trees adapt well to such cases. For the first dataset, points x generated by the following process: for each point x Figure 5: Performance of RP trees with k -d trees on first synthetic dataset (left) and the second synthetic dataset (right) For the second dataset, we choose n points from two D -dimensional Gaussians (with equal proba-bility) with means at (  X  1 ,  X  1 , . . . ,  X  1) and (1 , 1 , . . . , 1) , and identity covariances. chosen at random; (2) k -d trees in which at each split, the best coordinate is chosen (the one that most improves VQ error); (3) RP trees; and (4) for reference, PCA trees.
 In both cases, RP trees outperform both k -d tree variants and are close to the performance of PCA trees without having to explicitly compute any principal components. 3.3 MNIST dataset We next demonstrate RP trees on the all-familiar MNIST dataset of handwritten digits. This dataset consists of 28  X  28 grayscale images of the digits zero through nine, and is believed to have low Figure 6 (top) shows the first few levels of the RP tree for the images of digit 1 . Each node is represented by the mean of the datapoints falling into that cell. Hence, the topmost node shows the mean of the entire dataset; its left and the right children show the means of the points belonging to their respective partitions, and so on. The bar underneath each node shows the fraction of points we also show a histogram of the 20 largest eigenvalues of the covariance matrix, which reveal how closely the data in the cell is concentrated near a low-dimensional subspace. The last bar in the histogram is the variance unaccounted for.
 Notice that most of the variance lies in a small number of directions, as might be expected. And this rapidly becomes more pronounced as we go further down in the tree. Hence, very quickly, the cell means become good representatives of the dataset: an experimental corroboration that RP trees adapt to the low intrinsic dimension of the data.
 This is also brought out in Figure 6 (bottom), where the images are shown projected onto the plane defined by their top two principal components. (The outer ring of images correspond to the linear combinations of the two eigenvectors at those locations in the plane.) The left image shows how the close to what the PCA split would have been, corroborating our earlier intuition (recall Figure 3). The right image shows the same thing, but for the first two levels of the tree: data is shown in four colors corresponding to the four different cells. Figure 6: Top: Three levels of the RP tree for MNIST digit 1. Bottom: Images projected onto the (left) or after two levels of the tree (right).

