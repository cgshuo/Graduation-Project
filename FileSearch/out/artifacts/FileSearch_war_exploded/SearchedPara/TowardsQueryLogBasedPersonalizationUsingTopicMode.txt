 We investigate the utility of topic models for the task of personalizing search results based on information present in a large query log. We define generative models that take both the user and the clicked document into account when estimating the probability of query terms. These models can then be used to rank documents by their likelihood given a particular query and user pair.
 H.3.3 [ Information Storage &amp; Retrieval ]: Information Search &amp; Retrieval Design, Measurement, Experimentation Personalized Search, Topic Models, Query Log Analysis
Click-through data, in the form of query logs, is an abun-dant and important source of information for improving search engine retrieval performance. This data contains the search history of individual users and can therefore be uti-lized to personalize search results. Many studies have sug-gested that while difficult, if done correctly, personalization can indeed improve the quality of search results [2].
The aim of this paper is to investigate the applicability of topic modeling [3] approaches to the problem of query-log based personalization. The main contributions of this work are as follows:
Click-through data has been used extensively in the past as pairwise preference judgements for training learning-to-rank algorithms [4]. The aim of these rank learning tech-niques is to use the query log as a proxy for human rele-vance judgements. Each clicked-document for a particular query is assumed to be either a vote confirming its rele-vance or a preference for that document over other docu-ments present higher in the ranked list that were not clicked on. These pseudo-judgements are then used to calculate optimal weights for combining relevance features for each query-document pair.

In our work, we follow the same intuition; that each click on a URL represents an implicit vote for the relevance of the document to the query. Our work departs from that of learning-to-rank systems in that we are not attempting to learn a query independent weighting scheme for features de-fined over document-query pairs. Rather we are interested in a query dependent and personalized ranking function us-ing only the data present in the log itself. The score given by such a function could then be used alongside relevance information (such as content-based retrieval functions) as features for a learning-to-rank system.

There have been a number of recent studies on the viabil-ity of query log based personalization [2, 6]. Dou et al. [2] investigated a number of heuristics for creating user profiles and generating personalized rankings. In their work they used a preset set of interest categories and a K-nearest neigh-bor (KNN) approach for grouping users. They then used a borda-fuse approach to merge personalized and baseline rankings. The techniques we outline in this paper generalize these more heuristic techniques for clustering and smooth-ing user profiles by estimating the latter as parameters of a generative process. We also offer personalized ranking for-mulas which are principled in the sense that they are derived directly from the generative process.

In [6] the authors perform a large-scale study of query logs obtained from the Yahoo search engine to investigate user activity and determine if such data could be utilized to generate user profiles for personalization. The results of the study indicate that users X  queries exhibit a fairly consistent set of topics with each user varying in their specific topical interests. Furthermore the work showed that a user X  X  inter-ests tended to converge to a stable distribution, but only after a large number of queries. These results inform the work in this paper and lend credence to the notion that hid-den topic models might provide a workable solution to the problem of generating reliable and generalizable user profiles from query log data.
In this section we introduce and compare different latent variable generative processes for modeling the data found in a query click-through log. Before introducing these mod-els however, we first briefly discuss Latent Dirichlet Alloca-tion (LDA) [1, 3] which is a simple latent topic model that we extend to build more complicated models for query log analysis. We will also be using LDA as a non-personalized baseline for our personalization experiments later on. An LDA model consists of two parameter matrices  X  and  X  containing estimates for the probability of a word given a topic P ( w | z ) and a topic given a document P ( z | d ). Each col-umn of the respective matrices contains a probability distri-bution over words for a particular topic and over topics for a particular document (denoted  X  z and  X  d respectively). In or-der to prevent overfitting, LDA places a symmetric Dirichlet prior on both these distributions, resulting in the following expectations for the parameter values under the respective posterior distributions P (  X  z | w , z ) and P (  X  d | z , d ), where w is the vector of words occurrences w i in the corpus, z is an assignment of topics to each word position z i and d is the vector of documents d i associated with each word position: Here N wz , N zd and N z are counts denoting the number of times the topic z appears together with the word w , or with the document d or in total respectfully. W is the vocabulary and Z is the number of topics. Finally  X  and  X  are hyper-parameters, which determine the number of  X  X seudo-counts X  from a uniform distribution over the vocabulary/topics that are added to the observed counts of words/topics.

The Gibbs sampling procedure for LDA involves itera-tively updating the assignment of each topic z i in the topic vector z by sampling a value from the conditional distri-bution P ( z i | w , z  X  i , d ), which is conditioned on the current assignment to all topic variables except z i , (denoted z In LDA the word assignment is conditionally independent of the document given the topic assignment, so: Thus the expected value for the conditional distribution is simply: 1 Where the estimates  X   X  w | z and  X   X  z | d are calculated over z rather than z . After sufficient iterations of the sampler, the Markov chain converges (as seen by minimal change in the model likelihood Q i P z  X   X  w i | z  X   X  z | d i ) and the parameters of the LDA model can be estimated from z . For increased
Since the expectation of the product of two independently distributed random variables is the product of their expec-tations. has 3 observed variables and one latent variable per word position. The difference with LDA being the addition of an observed user variable u i , which like the word w i is dependent on the topic z i . accuracy, the parameter estimates are averaged over consec-utive samples (iterations) { z ( n  X  k ) ,..., z ( n ) } from the end of the chain.

We now investigate Topic Models which contain three ob-served variables: the document d i , the user u i and the word w i (as apposed to the two observed variables of LDA). The new user variable corresponds to the unique identifier for the user in the log who submitted the corresponding query.
We would like to estimate a model in which the topic assignment depends on both the document and the user. More precisely, we would like to directly estimate the prob-ablity P ( z | d,u ) as a parameter of our model. Unfortunately that would require estimating ( Z  X  1) DU different parame-ter values, where D is the number of documents and U is the number of users. Obviously, for any reasonable size dataset (containing many thousands of documents and users) there will be insufficient data to estimate all of the parameters. In other words, we are unlikely to see enough queries for each pair of documents and users to be able to estimate an entire distribution over topics. We thus need to investigate mod-els that factorize the conditional distribution by assuming conditional independence between u i and d i given z i .
Figure 1 shows the plate notation for our first model, a slight variation on LDA, which we will refer to as Person-alization Topic Model 1 (PTM1). In this model, the user assignment u i is chosen according to a multinomial distri-bution  X  z , which depends on the topic z i , in the same way as the word assignment w i is chosen from a topic specific multinomial  X  z . The user distributions are smoothed using a symmetric Dirichlet prior with concentration parameter  X  , resulting in the following estimate for the probability of a user given a topic: Thus the PTM1 model has the same parameters as LDA ( X  and  X ) plus an additional user probability matrix of size ZU which we denote  X . For the Gibbs sampling routine, the probability of choosing a topic assignment under the model can be factorized as follows: the topic z i is now dependent on the user u i rather than the clicked document d i which is instead itself now dependent on the topic z i .
 Thus we have the following update formula:
The PTM1 generative model is a little counterintuitive in that it supposes that a document chooses a topic and then the topic chooses a word and a user, making the user a byproduct of the process rather than the initiator of it. We can also consider a model in which the user first chooses the topic and then the topic chooses the document. We refer to this model as PTM2 and show a graphical representation of it in Figure 2. According to this model we need to estimate the parameters  X   X  d i | z i and  X   X  z i | u i instead of The Gibbs sampling update for the second model is then practically the same as before:
For the non-personalized case (using the LDA model) we rank documents according to their likelihood given the query, which can be estimated as follows:
In order to personalize the ranking we can rank documents according to their likelihood given both the query and the user: Thus our personalized ranking formula consists of a user specific  X  X ocument prior X  P ( d | u ) and a query-term likelihood P ( w | d,u ). The first term estimates the user X  X  probability to choose the document a priori , and the second models their likelihood to choose word w to describe it.
 For PTM1 we can calculate these two estimates as follows: Similarly for PTM2:
In order to evaluate our models on real data we made use of the AOL Query Log dataset. The log contains the queries of 657,426 anonymous users over 3 months from March to May, 2006. It is, as far as we know, the only publicly avail-able dataset of sufficient size to perform our analysis. We protected user privacy by analyzing results only over aggre-gate data.

To clean the data we first discarded all queries which didn X  X  result in a click on a URL. We then selected only those URLs which more than 100 users had clicked on, and selected from the remaining users only those with more than 200 queries. In order to parse the queries we first separated the words according to whitespace. All punctuation was re-moved and Porter X  X  algorithm was used for stemming. We did not remove any stopwords but did remove words that appeared less than 3 times in the dataset.

The resulting dataset contained 2152721 queries, 6581 users, 15996 documents and had a vocabulary of 53132 words. We note that the user profiles in the dataset should be consid-ered  X  X ong-term X  since they are over a 3 month period, (in contrast to  X  X hort-term X  profiles that are built using click data from only the previous week, day or even session). Per-sonalizing search results based on such long-term profiles is difficult since users X  interests can vary greatly over long time periods. 2
We separated the dataset into a training and testing set by retaining the last 5% of queries by each user for testing. To ensure the significance of the results all retrieval metrics were computed over the test queries for 1000 users from the log.

To evaluate retrieval performance we calculated the suc-cess at rank k (S@k) and the mean reciprocal rank (MRR): Here Q denotes the number of queries, r ( d,q ) is the rank of document d for query q and I () is an indicator function re-turning 1 whenever its argument is true and zero otherwise.
With regard to parameter settings, for LDA we set the concentration parameters  X  and  X  to be 50.0 and 0 . 1 W re-spectively, which are common settings used in the litera-ture [3]. (Here W is the size of the vocabulary.) For the personalization models, we set  X  to 50.0 or 0 . 1 D (depending on the model),  X  to 0 . 1 W ,  X  to 50.0 or 0 . 1 U .
For all models we ran the Gibbs sampler for 400 iterations, discarding the first 300 iterations as burn-in and averaging parameter estimates over the last 100. These settings ap-
We also performed experiments on short-term profiles with more positive results, but do not report these results in this short paper due to space limitations. Table 1: Initial retrieval performance on the three month dataset with 160 topics. The not personalized LDA baseline performs best overall. Figure 3: Retrieval performance on the three month dataset versus topic count. LDA consistently out-performs the PTMs across all topic count values. peared to give consistently good convergence in terms of model likelihood.

Table 1 gives the results for models with a topic count of 160. Surprisingly, the personalization topic models with their additional parameters and ability to leverage infor-mation about the user do not appear to improve perfor-mance over the baseline ranking. Across all metrics the not-personalized LDA-based ranking is preferred. We believe that there may be three reasons for this. Firstly, it may be the case that the learnt models have not converged on the best possible parameter settings since the generative mod-els all assume that the document label and the user label are equally important, when in reality the document is far more important for determining the query terms. Secondly, by blindly applying personalized ranking algorithms to all queries, we may be degrading performance on those queries whose meaning was not initially ambiguous. Thirdly, the long term profiles of the users may themselves be so het-erogenous as to not be useful for predicting the future rele-vance of documents to the user. These conjectures motivate our future work where we plan to investigate other conver-gence criteria, user profiles over shorter time periods and also query ambiguity measures.

We then investigated the effect of changes in the topic count on the relative performance of the personalization models. Figure 3 shows plots of performance against the number of topics for the long-term dataset. We see from the first plot that as the number of topics increases so does the performance of the different models. The LDA con-sistently outperforms the personalization models across the topic counts.
In this paper we have shown that it is possible to factor-ize a sizable search engine query log using topic modeling techniques. The results of our analysis applying two dif-ferent topic modeling based personalized ranking formulas to a query log dataset with three-month user profiles were largely negative, indicating the complexity of the personal-ization problem.

We are currently investigating performance on datasets containing shorter user-profiles to assess whether the co-herency of the user profile is an important factor in influ-encing personalization performance. Moreover we are inves-tigating more complicated topic models with two and three latent variables to see if the resulting factorizations better generalize the data for the personalization task. (The initial positive results from this analysis were not included in this short paper due to space limitations.)
Across the queries in the log, there were a large num-ber of occasions where the personalised models did outper-form LDA and in future work we plan to look into this in more detail. In particular, since certain queries are more amenable to personalization than others, and we will inves-tigate whether it is possible to identify a priori those queries which are most likely to benefit from personalization.
This research represents a first step in applying topic mod-eling techniques to the difficult task of query log based search engine personalization. There are many opportunities for fu-ture work, including: (1) investigating methods for  X  X olding in X  documents and users who were not in the training set, (2) integrating other forms of evidence into the ranking includ-ing the content of the documents, (3) extending the topic models in order to account for and make use of the tem-poral information about users, documents and terms in the log, (4) improving the cleaning of the log data in particular taking into account click bias in the models. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] Z. Dou, R. Song, and J.-R. Wen. A large-scale [3] T. L. Griffiths and M. Steyvers. Finding scientific [4] T. Joachims. Optimizing search engines using [5] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [6] S. Wedig and O. Madani. A large-scale analysis of
