 This paper proposes a learning approach for the merging process in multilingual information retrieval (MLIR). To conduct the learning approach, we also present a large num-ber of features that may influence the MLIR merging pro-cess; these features are mainly extracted from three levels: query, document, and translation. After the feature extrac-tion, we then use the FRank ranking algorithm to construct a merge model; to our knowledge, this practice is the first at-tempt to use a learning-based ranking algorithm to construct a merge model for MLIR merging. In our experiments, three test collections for the task of crosslingual information re-trieval (CLIR) in NTCIR3, 4, and 5 are employed to assess the performance of our proposed method; moreover, sev-eral merging methods are also carried out for a comparison, including traditional merging methods, the 2-step merging strategy, and the merging method based on logistic regres-sion. The experimental results show that our method can significantly improve merging quality on two different types of datasets. In addition to the effectiveness, through the merge model generated by FRank, our method can further identify key factors that influence the merging process; this information might provide us more insight and understand-ingintoMLIRmerging.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval -Selection Process Design, Experimentation, Performance FRank, Merge Model, MLIR Multilingual information retrieval (MLIR) is usually carried out by first performing cross-la nguage information retrieval (CLIR) on separate collections, each for a language. Once a list of monolingual results has been retrieved in each collec-tion, all the lists are merged to produce a multilingual result list. Because of various translation and retrieval qualities in different collections, how to merge a unique result list that includes more relevant documents from different collections has become one of the major issues in MLIR.
 In the literature, most traditional merging methods for MLIR are heuristic approaches such as raw-score [16], round-robin [16], normalized-by-top1 [12], and normalized-by-topk merg-ing [8]. These heuristic methods are based on a similar as-sumption that relevant documents are homogeneously dis-tributed over monolingual result lists; therefore, based on the assumption, these methods can locally adjust or normal-ize the scores of retrieved documents to produce a multilin-gual result list. However, if the assumption is invalidated, these heuristic methods have a large decrease of precision in the merging process. Instead of directly merging mono-lingual result lists, the 2-step merging strategy [10] uses re-indexing techniques on the documents retrieved with respect to each query term to indirectly obtain a multilingual result list. Although performing well, 2-step merging is also se-riously damaged by several factors such as the number of meaningful terms presenting in a collection and the propor-tion of relevant documents in a collection.
 This paper proposes a learning approach for the MLIR merg-ing process. To conduct the learning method, we also present a large number of features that may influence MLIR merg-ing; these features are mainly extracted from three levels: query, document, and translation. After the feature extrac-tion, we use a learning-based ranking algorithm, FRank [20], to construct a merge model from the extracted features. The merge model generated by FRank is then used to merge the monolingual result lists retrieved from different collections into a multilingual one. In our experiments, three CLIR test collections in NTCIR3, 4, 5 [3, 6, 7] are employed to assess the performance of our proposed method; in addition, for a comparison, several merging methods are also carried out, including the traditional merging methods, the 2-step merg-ing strategy [10], and the merging method based on logistic regression [18]. The experimental results show that the pro-posed method significantly improves merging quality on two different types of datasets. In addition to the enhancement, through the generated merge model, we can also recognize the crucial features that influence MLIR merging. The main contribution of our work includes the development of a learning scheme for the MLIR merging process. To our knowledge, this study is the first attempt to use a learning-based ranking algorithm to construct a merge model for the merging process. Under this scheme, several traditional methods can be regarded as special cases of our method. For example, if a merge model is a uniform model, then the cor-responding merging process ac ts like raw-score merging. In addition, our contribution also includes the presentation of a large number of features possibly affecting MLIR merging, and the important factor identification via the merge model generated by FRank. This information might provide us more insight and understanding into MLIR merging. The remainder of this paper is organized as follows. In Sec-tion 2, we briefly review previous work on MLIR merging. Section 3 presents several features and describes the use of a learning-based ranking algorithm to construct a merge model. Section 4 describes evaluation metric and the de-tails of experimental datasets. We then report and discuss experimental results. We conclude our paper and provide several directions for future work in Section 5. In the literature, several heuristic methods for the MLIR merging process have been proposed. We below review these heuristic methods. Given several monolingual result lists, raw-score merging [16] directly sorts the original scores of re-trieved documents to obtain a multilingual result list; round-robin merging [16] interleaves retrieved documents by their ranks only to produce a multilingual result list. Another well-known merging approaches also include the ways of us-ing normalization techniques on the scores of retrieved docu-ments. The main idea behind the normalized score methods is to map the scores into the same scale for a reasonable comparison. Normalized-score merging [12] uses the score of the top one document to normalize the other documents in the same list, and then sorts the normalized scores to obtain the final list. Instead of using the top one docu-ment, normalized-by-topk, uses the average score of the top k documents to produce a multilingual result list. These traditional methods are observed from our experiments to tend to have a similar performance because based on a simi-lar assumption that relevant documents are homogeneously distributed over the monolingual result lists.
 Lin and Chen [9] postulated the degree of translation am-biguity and the number of unknown words can be used to model the MLIR merging process. They presented the fol-lowing formulas to predict the effectiveness of MLIR: where W i is the merging weight of query i in a CLIR run, T i is the average number of translation terms in query i , U i is the number of unknown words in query i , n i is the number of query terms in query i ,and c 1 and c 2 are tunable parameters such that c 1 + c 2 = 1. However, by means of heuristic methods, it is quite difficult to obtain the optimal parameters for the above formulas.
 Mart  X   X nez-Santiago et al. [10] proposed a 2-step merging strategy to produce a multilingual result list. Instead of directly merging the monolingual result lists into a multi-lingual one, 2-step merging uses re-indexing techniques on the documents retrieved with respect to each query term, and then employs the re-indexed dataset to produce a mul-tilingual result list. By the re-indexing techniques, 2-step merging can globally consider relevant terms within data collections. Although performing well, the 2-step merging strategy would seriously be damaged by some factors such as the number of meaningful terms presenting in a collection and the proportion of relevant documents in a collection. The related work also includes the studies on collections fusion and search results merging in distributed informa-tion retrieval (DIR). Although DIR environments tend to be monolingual and uncooperative, some related techniques have been applied for MLIR merging with several degree of success. Si and Callan [17, 19] proposed a semisupervised learning solution for the DIR merging problem. Savoy [11, 15] proposed a merging approach based on logistic regression for predicting the probability of binary relevance according to a set of independent variables. Although also based on learning techniques, these methods lack the consideration of dataset with multiple relevance judgments. Therefore, we propose a novel merging method for MLIR merging by us-ing a learning-based ranking algorithm, in which multiple relevance judgments can be considered. Figure 1 illustrates a traditional framework for MLIR, in which each collection is a monolingual collection. This fig-ure shows that the traditional MLIR is typically carried out by first performing CLIR on separate collections. Once a monolingual result list has been retrieved in each collection, all the lists are merged into a multilingual result list. Thus, the traditional MLIR framework consists of two models: one is retrieval model for retrieving documents from each mono-lingual collection; another is merge model for merging all the monolingual result lists into a multilingual one. In the traditional MLIR framework, the retrieval model is usually set as several standard IR techniques such as bm 25 [13] and cosine similarity. Moreover, in recent years, sev-eral methods [1, 2, 4, 5, 20] based on learning techniques have also been proposed for the retrieval model. As for the merge model, conventional merging methods are mostly based on some heuristics. Therefore, this paper attempts to propose a learning scheme to generate a merge model. Under this learning scheme, several conventional merging methods can be regarded as special cases of our method. For exam-ple, if the generated merge model is a uniform model, then our method acts like raw-score merging. Furthermore, our method is similar to round-robin merging if the generated merge model predicts the merging weight of a document by means of its reciprocal rank.
 Below, we describe how to use a ranking algorithm based on leaning techniques to construct a merge model. In addition, we also present several features possibly affecting the MLIR merging process. Through the merge model generated by the learning-based ranking algorithm, we expect to identify critical features that really influence MLIR merging. Table 1 lists the features used to construct a merge model in this study. There are 62 fea tures extracted from three levels: query, document, and translation; moreover, all the features are represented by real numbers in our experiments. Accordingtotheextractionleve l, we describe these features in detail as follows.
 On document level, only two features are extracted in this study. These two features are document length and title length; they represent the number of words in a document and in a document title, respectively. We consider these two features mainly because of their abilities of indicating the amount of information within a document. In this study, no retrieval feature is used to construct a merge model, al-though retrieval features (e.g., tf and idf ) can also be re-garded as document-level features. Our experiments show that the retrieval features, if included, usually tend to dom-inate the generated merge model; as a result, this situation would lead to a difficulty in identifying the important fea-tures affecting MLIR merging. To emphasize the merging process, therefore, we use only document length (DLength) and title length (TLength) to construct a merge model. For query-level feature extraction, we first manually classify the terms within a query into several pre-defined categories, and then extract query-level features according to these cat-egories. In our experiments, we primarily consider the query terms as proper names since they usually play an important role in retrieving documents. For a query, therefore, each query term is labeled as one of the following categories: In addition, we also use an additional category: named en-tity (NET), which contains all the query terms classified into the above categories. For those terms unable to be precisely labeled, we simply classify them into two categories: con-crete nouns (CN) and abstract nouns (AN). Two types of verbs, i.e., intransitive (IV) and transitive (TV), are also considered in this study. Two query examples selected from our experimental datasets will be shown in Section 4.4; these two examples consist of their query descriptions and query terms with the corresponding labels.
 After the above labeling, we then extract query-level fea-tures from the labeled dataset. For a query, the feature set comprises the number of query terms (#QT) and compound words (#CW). In addition, the feature set also consists of the number of the query terms classified into the pre-defined categories (e.g., #PPN and #Loc), and the corresponding percentage with respect to total query terms (e.g., %PPN and %Loc). These query-level features are extracted mainly because we consider different types of query terms would influence the MLIR merging process differently. Through these features, therefore, we expect to realize the relation between query difficulty and merging performance. On translation level, we extract several features capable of indicating the translation quality of a query for a language. The translation-level features include the languages used in a query and in a document (i.e., QLanguage and DLanguage); thevaluesofthesetwofeaturesaresetto0forEnglish,1for Chinese, and 2 for Japanese in our experiments. In addition, the translation-level features also consist of the size of a bilingual dictionary used for various language (i.e., DictSize) and the average number of translation equivalents within a query (i.e., AvgTAD). For instance, for a language, if a query has two query terms both with three translation equivalents, then the value of AvgTAD of the query is (3 + 3) / 2=3. Furthermore, the translation-level features also consist of the number of translatable query terms (i.e., #TQT), the num-ber of translatable compound words (i.e., #TCW), and the corresponding ratios to total query terms (i.e., %TQT and %TCW). A query with more highly ambiguous query terms, whose number of translation equivalents  X  3, usually tends to be not well-translated. Therefore, the translation-level features also include the number of highly ambiguous terms (i.e., #HAT) and the corresponding ratio to total query terms (i.e., %HAT). In addition to translatable query terms, we also consider the features of out of vocabulary terms, such as #OOV and %OOV. The idea of OOV features is also extended to the terms classified into various categories, thereby generating the features such as #OLoc and %OLoc. Using these translation-level features, we expect to realize the effect of translation quality to merging performance. In the next subsection, we describe a learning-based ranking algorithm to construct a merge model. Through the merge model generated by the ranking algorithm, we can realize the effects of these extracted features to MLIR merging. The FRank ranking algorithm [20] is adopted to construct a merge model in this paper. FRank is a learning-based ranking algorithm with a novel loss called fidelity loss based on RankNet X  X  probabilistic ranking framework [1]. Because of the helpful properties in the fidelity loss, FRank is well-suited to ranking applications with multiple relevance judg-ments. In addition, since our experimental collection has four relevance judgments, merging on such a collection can be regarded as a ranking application. For the details of FRank, please refer to [20].
 According to FRank X  X  generalized additive model, a merge model can be represented as: where m t ( x ) is a weak learner,  X  t is the learned weight of m ( x ), and t is the number of selected weak learners. Thus, when obtaining the merge model generated by FRank, we can examine the effect of a feature to MLIR merging by means of its learned weight  X  t . Upon completion of the merge model, we combine it with a retrieval model by using linear combination. In our experiments, the retrieval model is set as bm 25; then the proposed method can be represented as: where  X  is the combination coefficient of bm 25. In the above model, the number of selected weak learners t and the com-bination coefficient  X  are two tunable parameters that can be determined on a validation dataset.
 In our method, the merge model M t ( x ) generated by FRank can be regarded as a supplementary model to enhance the merging quality of bm 25. This practice is consistent with the traditional procedure of MLIR, in which a retrieval model is for retrieving documents on each monolingual collection, and a merge model is for merging all the monolingual result lists into a multilingual one. Moreover, this practice also provides us a way independent from the retrieval model to examine the critical features influencing MLIR merging. Table 2: The Details of Experimental Collections In this section, we first describe the details of experimental collections and settings, including the evaluation metric and the comparison methods used in our experiments. Moreover, we also state the procedures of building datasets for training and testing. Then, we describe the experiments of using FRank to construct a merge model and report the results of all the comparison methods. Finally, to further identify the crucial features that influence MLIR merging, we provide several discussions as to the features found in FRank. In our experiments, three CLIR test collections in NTCIR3, 4,and5[3,6,7]wereusedtobuilddatasetsforMLIRmerg-ing. These collections are mainly for the evaluation of CLIR performance on four languages: Chinese (C), Japanese (J), Korean (K), and English (E). With respect to a query, doc-uments within the collections are labeled with four relevance judgments, including highly relevant, relevant, partially rel-evant, and irrelevant. Because of lack of Korean resources, we only used CJE documents to build experimental datasets. Table 2 lists the details of the CLIR test collections. We here describe how to build datasets for the MLIR merg-ing experiments. For each collection, we used English topics as source queries to retrieve English, Chinese, and Japanese documents; that is, there were three retrieval processes: E-E monolingual retrieval, and E-C and E-J crosslingual ones. In addition, query terms were mainly composed of the terms in the concept field of a topic description. When translating an English query term for crosslingual retrieval, we chose two translation candidates with the highest frequency in the corresponding language corpus. For example, if an En-glish query term has three Chinese translation candidates, we select the top two candidates that appear frequently in Chinese corpus. After the E-E, E-C, and E-J retrieval pro-cesses, we then obtained three lists of monolingual results for a query; for each monolingual result list, we only used the documents whose similarity scores are not zero to construct the experimental datasets.
 After obtaining the experimental datasets, we then con-ducted experiments on the datasets to merge the three mono-lingual result lists into a multilingual one. For a compari-son, several merging methods were carried out in our ex-periments, including raw-score, round-robin, normalized-by-top1, normalized-by-topk, and 2-step merging. Among these methods, we refer to the raw-score, round-robin, normalized-by-top1, and normalized-by-topk merging methods as tradi-tional ones. Instead of directly merging the monolingual re-sult lists, the 2-step merging strategy uses re-indexing tech-niques to indirectly obtain the multilingual result list; there-fore, we regard this strategy as a variant merging approach. Table 3: The Percentage of Retrieved Relevant Doc-uments to Total Retrieved Relevant Documents in Different Experimental Datasets In addition to the heuristic methods, two learning-based merging methods are also performed, including one based on logistic regression [18] and ours based on the FRank ranking algorithm. The corresponding settings for learning-based methods are described in Section 4.2.
 Traditional IR measure, Mean Average Precision (MAP), wasusedasevaluationmetricinourexperiments. Givena result list for query q i , the average precision for q i defined as follows: where N is the number of documents retrieved, P ( j )isthe precision value at position j ,and pos ( j ) is a binary function indicating whether the document at position j is relevant. Once all APs for all queries have been obtained, the MAP can be calculated by averaging the APs over all queries. In our experiments, the position j was set to 1000, and the pos ( j ) is set to true if the document at position j is labeled as highly relevant or relevant; this criterion setting is consistent with the rigid measure of NTCIR X  X  CLIR task. Table 3 lists the percentage of retrieved relevant documents to total retrieved relevant documents in different experimen-tal datasets. Regarding the distribution of retrieved relevant documents, NTCIR3 and NTCIR5 is more balanced than NTCIR4; in addition, as indicated in Table 2, the number of articles in NTCIR5 is also more than those in NTCIR3 and NTCIR4. Due to the above reasons, the monolingual result lists in NTCIR5 were chosen as training dataset, and those in NTCIR3 and NTCIR4 as two separate testing datasets; this practice assists us to further examine the performance of the comparison methods on two different types of datasets. In addition, for each testing datasets, we also selected 10 queries as a validation to deter mine the parameters within the learning-based merging methods.
 For the method based on logistic regression, we directly used the binary code of mySVM [14]. The parameter c within mySVM was tuned on the validation datasets; according to the validation results, the parameters c were set to 2.5 for the NTCIR3 testing dataset and 2.0 for NTCIR4. For our method, we implemented the FRank ranking algorithm [20] to construct a merge model from the proposed 62 features; in addition, all the features were normalized to the values between 0 and 1 in the experiments. The parameters of our methods were also tuned on the validation datasets; ac-cording to the results, th e number of weak learners t and the combination coefficient  X  were set to 7 and 0.07 for NTCIR3 as well as 13 and 0.01 for NTCIR4. Table 4: Experimental Results on Testing Datasets Merging Strategy NTCIR3 NTCIR4 Raw-score (baseline) 0.174 0.278 Round-robin 0.174 0.180 Normalized-by-top1 0.172 0.154
Normalized-by-topk 0.175 0.173 2-step 0.210 (0.048) 0.246 Logistic regression 0.189 (0.348) 0.152 Our method 0.222 (7.7e-3) 0.364 (3.7e-5) Table 4 lists the experimental results of all comparison meth-ods on two testing datasets. Numbers in brackets indicate the p -value from a paired t -test. Bold faced numbers indi-cate that the entry is statistically significant from the run of raw-score merging (baseline) on the same dataset at 95% confidence level. On the balanced NTCIR3 testing dataset, the performance of the traditional merging methods is sim-ilar to that of the baseline; in comparison, our method and the 2-step merging strategy both significantly outperform the baseline. The improvement of merging method based on logistic regression, however, fails to pass the significance test. On the unbalanced NTCIR4 testing dataset, in con-trast to other merging methods, ours also significantly out-performs the baseline; this consequence arises main because our method can overcome the problem of unbalanced dis-tribution by means of FRank X  X  ability of coordinating the proposed features.
 For our method, we also conducted an experiment to further examine the effect of combination coefficient  X  to merging performance. Figure 2 illustrates that the results of two baselines and those of our method with different  X  . Regard-less of the performance on NTCIR3 or NTCIR4, our method with various  X  is constantly above raw-score merging, except for  X  = 0; this exception is due to the fact that, when  X  =0, our method uses only merge model to produce a multilin-gual result list. Furthermore, as observed from the figure, the merging process of our method is the same as raw-score merging when  X  = 1. From this point of view, the proposed merge model can be considered a supplementary model to enhance the merging quality of raw-score merging. According to different types of merging methods, we offer some observations and discussions about the experimental results as follows. Figure 2: The Experimental Results of Our Method using Different Combination Coefficient  X  Table 5 lists two query examples in the NTCIR4 testing dataset. Each query example in the table consists of query id, query description, labeled query terms, and the distribu-tion of retrieved relevant documents. In addition, Table 6 lists the most effective features found in the first 100 itera-tions of FRank; according to their average learned weight, these features are selected, including the top 7 positive ones and 8 negative ones. Through these selected features, we at-tempt to find more clues about the MLIR merging process. Onthebasisoftheextractionlevel,webelowprovidethe analysis of the selected features.
Symbols in brackets indicate the sign of the average learned weight of a feature
The average precision is 0.68 for Qid 49 and 0.5 for Qid 15. The contribution of this work includes the proposition of a learning approach for the MLIR merging process. We use the FRank ranking algorithm to construct a merge model for merging monolingual result lists into a multilingual one. Experimental results demonst rate that, even performing on a dataset with the unbalanced distribution of relevant docu-ments, the proposed merge model can significantly improve merging quality. Moreover, the contribution of this work also includes the presentation of several features affecting MLIR merging, and the feature analysis via the merge model generated by FRank. In conclusion, the merge model indi-cates that for MLIR merging, the key factors are the number of translatable terms and compound words; in addition, the number of OOV terms as named entities, transitive verbs, and organization names also plays an important role in the merging process. This information provides us more insight and understanding into the MLIR merging process. Several research directions remain for future work: Research of this paper was partially supported by Excellent Research Projects of National Taiwan University (96R0062-AE00-02). [1] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [2] Z. Cao, T. Qin, T. Y. Liu, M. F. Tsai, and H. Li. [3] K. H. Chen, H. H. Chen, N. Kando, K. Kuriyama, [4] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An [5] T. Joachims. Optimizing search engines using [6] K. Kishida, K. H. Chen, S. Lee, K. Kuriyama, [7] K. Kishida, K. H. Chen, S. Lee, K. Kuriyama, [8] W.C.LINandH.H.CHEN.Mergingmechanismsin [9] W.C.LINandH.H.CHEN.Mergingresultsby [10] F. Mart  X   X nez-Santiago, L. Ure  X  na-L  X  opez, and [11] C. Peters. Cross-Language Information Retrieval and [12] A. Powell, J. French, J. Callan, M. Connell, and [13] S. Robertson and S. Walker. Some simple effective [14] S. Ruping. mySVM-Manual. University of Dortmund, [15] J. Savoy. Cross-language information retrieval: [16] J. Savoy, A. Le Calve, and D. Vrajitoru. Report on [17] L. Si and J. Callan. A semisupervised learning method [18] L. Si and J. Callan. CLEF 2005: Multilingual [19] L. Si and J. Callan. Modeling search engine [20] M.F.Tsai,T.Y.Liu,T.Qin,H.H.Chen,andW.Y.

