 In traditional clustering methods, a document is often rep-resented as  X  X ag of words X  (in BOW model) or n-grams (in suffix tree document model) without considering the natu-ral language relationships between the words. In this paper, we propose a novel approach DGDC (Dependency Graph-based Document Clustering algorithm) to address this issue. In our algorithm, each document is represented as a depen-dency graph where the nodes correspond to words which can be seen as meta-descriptions of the document; where-as the edges stand for the relations between pairs of words. A new similarity measure is proposed to compute the pair-wise similarity of documents based on their corresponding dependency graphs. By applying the new similarity measure in the Group-average Agglomerative Hierarchial Clustering (GAHC) algorithm, the final clusters of documents can be obtained. The experiments were carried out on five pub-lic document datasets. The empirical results have indicated that the DGDC algorithm can achieve better performance in document clustering tasks compared with other approaches based on the BOW model and suffix tree document model. I.5.3 [ Pattern Recognition ]: Clustering -algorithms, sim-ilarity measures ; H.3.1 [ Content Analysis and Index-ing ]: Linguistic processing; I.2.7 [ Artificial Intelligence ]: Natural Language Processing -text analysis Algorithms, Experimentation  X  This work is done when the first author is visiting Microsoft Research Asia.
 Document Clustering, Dependency Graph, Document Rep-resentation Model, Similarity Measure
Document clustering techniques usually rely on four mod-ules: document representation model, similarity measure, clustering model and the clustering algorithm which gener-ates clusters based on the document representation model [5]. Among all these modules, the document representation model is very fundamental and crucial for the clustering re-sults.
 The most basic model for document representation is the Vector Space Document (VSD) model. In this model, the document is regarded as  X  X ag of words X  (BOW), without considering the relationships between the words. In or-der to achieve better clustering results, many efforts have been made to seek more informative document representa-tion models. The n-gram model [12] is one of those efforts, which can be viewed as an extension of BOW model but in-cludes all ordered sequences of less than n words in the fea-ture vector. The suffix tree document model [14] considers a document to be a set of suffix substrings and constructs a suffix tree using substrings of all documents in the cor-pus. It provides a flexible n-gram approach by identifying all overlapping phrases among documents as Longest Com-mon Prefixes (LCPs) [9]. Some research works [7, 6, 13] also leveraged ontologies to enrich the representation of docu-ments. WordNet [6], Mesh [13] and Wikipedia [7] have been adopted and improvements were achieved in document clus-tering tasks. However, the pairwise relationships of words which are suggested in the natural language sentences are still ignored in these document representation models.
In this paper, we propose a more informative document representation model, namely the Dependency Graph-based Document (DGD) model. In this model, each document is represented as a dependency graph where each node corre-sponds to a word which can be seen as a meta-description of the document. The edges between nodes are used to catch the semantic relations between word pairs. A novel similar-ity measure is also proposed for calculating the similarity of documents based on their corresponding dependency graphs. The Dependency Graph-based Document Clustering (DGD-C) algorithm is conducted in the following steps: (1) Con-struct a dependency graph for each document. (2) Calculate the similarity of each pair of documents based on the novel similarity measure. (3) Generate the final clusters of doc-uments by the Group-average Agglomerative Hierarchical Clustering (GAHC) algorithm [14]. The experiments were carried out on five public document datasets. The empirical results have indicated that the DGDC algorithm can achieve better performance in document clustering tasks compared with other approaches based on the BOW model and the suffix tree document model.
Suppose d is a document with a vocabulary set W = { w 1 ,w 2 , ..., w n } ,where w i stands for a word which appears in d and w i = w j for  X  i = j . Since stopwords (e.g., the, is) count little for the meaning of the whole document, we ex-clude them from the vocabulary set. The dependency graph G corresponding to document d is denoted as G =( V,E ). each vertex v i corresponds to w i in the vocabulary set. E = { e 1 ,e 2 , ..., e m } is the collection of edges, where each edge e is associated to a pair of vertices, which indicates that there is some relationship between them.

Consider a document A , which has the content  X  X eijing is a big city. The city is very beautiful X . Figure 1 shows the dependency graph corresponding to the document. Assume that we have two documents A and B . A is  X  X eijing is a big city. The city is very beautiful X . B is  X  X eijing is a very beautiful city which is big X . These two documents have the same semantic meaning so that they should have high similarity between each other. However, as they are literally organized by different word sequences, the similarity is much lower than expected according to the n-gram model and the suffix tree document model. In our model, document A and B correspond to the same dependency graph which correctly indicates that they are semantically equal with each other. Figure 1: The dependency graph generated for  X  X ei-jing is a big city. The city is very beautiful. X 
In practice, a dependency parser (e.g. the Stanford pars-er [2]) is needed to obtain word relations from the original sentences. Using the parser, the dependency graph can be constructed in the following steps. Initially, there are no ver-tices and edges in the graph. Then the vertices and edges are added by processing each sentence in the document sequen-tially. For each sentence, we parse it using the dependency parser, which outputs a set of words and the identified pair-wise relations between them. The non-stopwords are then added to the vertex set if they are not contained in the graph before. Thus, each non-stopword in the output is associated with one vertex in the graph. Finally, for each pairwise re-lation suggested by the parser, a new edge will be generated between the corresponding vertices if it does not exist in the graph.
Given a dependency graph G =( V,E ), suppose the vertex is denoted by v i and w i is the word corresponding to v i The weight of vertex v i associated to document d can be calculated by the traditional tf-idf measure.

After representing the documents as weighted dependency graphs, the similarity of two documents can be calculated based on their corresponding graphs. Suppose there are two documents d 1 and d 2 in the global corpus D ,whosecor-responding graphs are G 1 =( V 1 ,E 1 )and G 2 =( V 2 ,E 2 respectively. V 1 V 2 is the collection of nodes contained in G 1 and G 2 , where the nodes corresponding to the same word are considered the same. The feature weight matrix R 1 for document d 1 is defined as R 1 = { r ij | 0  X  i, j &lt; | When i = j , r ij is actually the weight of single word fea-ture denoted by node v i .When i = j , r ij measures the importance of the relation between v i and v j .
Mathematically, r ij is defined as: r where c ( v i ,d 1 )istheweightofvertex v i associated to doc-ument d 1 .  X  is a constant coefficient to balance single word features (when i = j ) and relation features (when i = j ). f
G 1 ( v i ,v j ) is a penalty function which depicts how tightly the vertex v i and v j are connected with each other in G canbecalculatedby: where dist G 1 ( v i ,v j ) is the distance (length of the shortest path) between v i and v j in G 1 ,  X  is a penalty coefficient. The lengths of all the edges in the graph are set to 1. these two feature weight matrixes is computed by
Consequently, the similarity of two documents d 1 and d 2 is defined as: such that Sim ( d 1 ,d 2 )  X  [0 , 1].
The Group-average Agglomerative Hierarchical Cluster-ing (GAHC) algorithm is adopted in the cluster generating procedure. The algorithm considers each document as a u-nique cluster initially and selects a pair of clusters to merge repeatedly in the merging procedure. In each turn, the pair of most similar clusters are selected to be merged. The simi-larity of two clusters is calculated by the group-average mea-sure.

One important issue for hierarchical clustering is to de-termine which step to terminate in the merging procedure [11]. One way to solve this problem is using a pre-defined number of clusters or setting a constant threshold of simi-larity for termination. However, neither of the strategies are practical for various kinds of corpus in applications. Moti-vated by [11], we leverage the R 2 criterion to determine the terminating point in the merging procedure.

The R 2 value represents the proportion of variance ac-counted for by the clusters, which is estimated by: where C k stands for the k th cluster, Dist ( d i ,d j )  X  estimates the distance between document d i and d j .We define:
We can plot R 2 with respect to the number of remaining clusters in each step. The place where the curve levels off is selected to be the terminating point. We select five public document datasets: CSTR , Reuters3 , News-diff3 , News-sim3 and News-mod6 , which are common-ly used to evaluate document clustering and categorization methods. CSTR 1 consists of abstracts for technical reports. Reuters3 is generated from the Reuters-21578 2 corpus. We choose three popular topics (grain, trade, and crude) and randomly select 100 documents for each topic. News-diff3 , News-sim3 and News-mod6 datasets [1] are derived from the 20-Newsgroups corpus 3 .The News-diff3 dataset consists of three different newsgroups (alt.atheism, rec.sport.baseball, and sci.space) and the News-sim3 dataset contains three similar newsgroups (comp.graphics, comp.os.ms-windows.misc, and comp.windows.x). The News-mod6 dataset consists of 600 documents from 6 newsgroups (rec.sport.baseball, s-ci.space, alt.atheism, talk.politics.guns, comp.windows.x, and soc.religion.christian). In the News-mod6 dataset, some top-ics are similar while others are different from each other. Ta-ble 1 summarizes the characteristics of these five datasets. news-sim3 300 3 397.9 news-mod6 600 6 649.8 http://www.cs.rochester.edu/trs http://www.daviddlewis.com/resources/testcollections/ http://people.csail.mit.edu/jrennie/20Newsgroups/
Three metrics are adopted in our experiments to evaluate the performance of document clustering: normalized mutual information (NMI) [4], F-measure [3] and purity [15].
In our experiments, the Stanford parser 4 [8] is used to obtain word dependencies. The Porter stemming algorith-m [10] is utilized to stem the words in original documents. Instead of using a standard stopword list, we determine the stopwords by calculating the document frequency df of each word, similar to that in [3]. A word w i is considered to be uments containing w i in the global corpus and | D | is the size of global corpus. The parameters in the similarity measure are set as  X  =0 . 5and  X  =0 . 5 experimentally.

The baseline approach utilizes the  X  X ag of words X  (BOW) model for document representation and weights each word in the feature vector by tf-idf measure. We also compare our algorithm with NSTC [3] method, which represents the documents as a suffix tree and performs clustering based on a suffix tree similarity measure. According to [3], the maximum length of Longest Common Prefixes (LCPs) in the suffix tree is set to be 5. To obtain a fair compari-son between the DGDC algorithm and other approaches, we adopt the same way of word stemming and stopword recog-nition. Moreover, the Group-average Agglomerative Hierar-chical Clustering algorithm is utilized to generate clusters in all the three approaches and the R 2 criterion is adopted to determine the cluster number.
Firstly, we demonstrate the feasibility of using R 2 criteri-on to determine the actual stop point in the Group-average Agglomerative Hierarchical Clustering (GAHC) algorithm. An example is illustrated by applying the DGDC algorith-mon news-diff3 dataset. Figure 2 plots the R 2 value with respect to the number of remaining clusters in each step. It can be noticed that the curve levels off at point A (the number of clusters is 9 and the value of R 2 is 0.851) in the figure, so that the merging procedure will be terminat-ed when the number of remaining clusters equals to 9. As shown in Figure 3, it is exactly the point where the best performance can be achieved. Therefore, the R 2 criterion provides an appropriate and practical way to determine the point of termination in the merging procedure.

Secondly, we compare our algorithm with other approach-es. The clustering results of different algorithms on five datasets are shown in Table 2, measured by NMI , F-measure and purity respectively. The number of remaining clusters is also listed to indicate the place of termination. As shown in Table 2, the DGDC algorithm outperforms other approaches on all the five datasets.
In this paper, we propose a novel document representation model. A new similarity measure for documents is also pro-posed, which calculates the similarity based on dependency graphs. The similarity measure is applied to the Group-average Agglomerative Hierarchical Clustering (GAHC) al-gorithm and promising results are obtained in the document clustering experiments.
The parser is available for download as open source at: http://nlp.stanford.edu/downloads/lex-parser.shtml Figure 2: The plot of R 2 value with respect to the number of remaining clusters Figure 3: The plot of NMI value by DGDC algo-rithm on news-diff3 dataset
Although good results have been achieved, we have to point out the limitation of the algorithm. Firstly, it takes O ( Nm 2 log m + N 2 m 2 ) to calculate all the pairwise simi-larities of documents, where N stands for the number of documents in the corpus and m represents the average word length of the documents. This high time complexity will make the algorithm computationally prohibitive for cluster-ing tasks that deal with large corpus. Moreover, the R 2 cri-terion used in this approach for cluster number estimation is not fully automatic and sometimes needs manual efforts to help. [1] A. Banerjee, I. Dhillon, J. Ghosh, and S. Sra. [2] D. Cer, M.-C. de Marneffe, D. Jurafsky, and [3] H. Chim and X. Deng. A new suffix tree similarity [4] B. E. Dom. An information-theoretic external
Algorithm NMI F-measure Purity Number of [5] K. M. Hammouda and M. S. Kamel. Efficient [6] A. Hotho, S. Staab, and G. Stumme. Wordnet [7] X. Hu, X. Zhang, C. Lu, E. K. Park, and X. Zhou. [8] D. Klein and C. Manning. Accurate unlexicalized [9] U. manber and G. Myers. Suffix arrays: a new method [10] M. Porter. New models in probabilistic information [11] W. S. Sarle. Cubic clustering criterion. SAS Technical [12] A. Tomovic, P. Janicic, and V. Ke  X  Zelj. N-gram-based [13] I. Yoo, X. Hu, and I.-Y. Song. Integration of [14] O. zamir and O. Etzioni. Web document clustering: A [15] Y. Zhao and G. Karypis. Criterion functions for
