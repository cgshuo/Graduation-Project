 Relation extraction is an important info r mation extraction task in natural language processing (NLP) , with many practical applications. T he goal of relation extraction is to detect and characterize semantic relation s between pairs of entities in text . For example, a rel a tion extraction system need s to be able to extract an Employ ment relation between the entities US soldier and US in the phrase US soldier . 
Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing th e pair , both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic par sing, and dependency parsing. Then the f eature based method explicitly extracts a variety of lexical , syntactic and semant ic features for statistical learning , either generative or discriminative ( Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007 ) . In contrast, the kernel based method does not explicitly extract features; it design s kernel functions over the structure d sentence representation s ( sequence, dependency or parse tree) to capture the similarities between different relation instances ( Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman , 2005; Zhang et al., 2006; Zhou et al., 2007 ; Qian et al., 2008 ) . Both lines of work depend on effective features, either explicitly or implicitly. 
The performance of a supervised relation extraction syst em is usually degr aded by the spars ity of lexical features. F or example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relat ion or not. Because the syntactic feature of the phrase US soldier is simply a noun -noun compound which is quite general, the words in it are cr ucial for extracting the relation.

This motivates our work to use word clusters as additional features for relat ion extraction. The assumption is that even if the word soldier may never have been seen in the annotated Employment relation instances, other words which share the same cluster membership with soldier such as president and ambassador may have been observe d in the Employment instances . The absence of lexical features can be compensated by the cluster features. Moreover, word clusters may implicitly correspond to different relation classes. For example, the cluster of president may be related to the Employme nt relation as in US president while the cluster of businessman may be related to the Affiliation relation as in US businessman .

The main contributions of this paper are: w e explore the cluster -based features in a systematic way and propose several stati stical methods for selecting effective cluster s . We study the impact of the size of training data on cluster features and analyze the performance improvement s through an extensive experimental study .
 The rest of this paper is organized as follows: S ecti on 2 presents related work and S ection 3 provides the background of the relation extraction task and the word clustering algorithm. Section 4 describes in detail a state -of -the -art supervised baseline syst em . Section 5 describes the cluster -based features an d the cluster selection methods . We present experimental results in Section 6 and conclude in Section 7. The idea of using word clusters as features in discriminative learning was pioneered by Miller et al. (2004), who augmented name tagging training data with hierarchical word clusters generated by the Brown clustering algorithm (Brown et al., 1992) from a large unlabeled corpus. They used d ifferent thresholds to cut the word hierarchy to obtain clusters of various granularities for feature d ecoding. Ratinov and Roth ( 2009 ) and Turian et al. (2010) also explored this approach for name tagging. Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and r eported improvement s , we noticed that the clus ters used by Miller et al. (2004) were quite different from that of Ratinov and Roth ( 2009 ) and Turian et al. (2010). To our knowledge, there has not been work on selecting clusters in a principled way. We move a step further to explore several methods in choos ing effective clusters. A second difference between this work and the above ones is that we utilize word clusters in the task of relation extraction which i s very different from sequence labeling tasks such as name tagging and chunking .
 Though Boschee et al. ( 2005 ) and Chan and Roth (2010) used word clusters in relation extraction, they share d the same limitation as the above approach es in choosing clusters. For example, Boschee et al. ( 2005 ) chose clusters of different granularit ies and Chan and Roth (2010) simply used a single threshold for cutting the word hierarchy . Moreover , Boschee et al. ( 2005 ) only augmented the predicate ( typically a verb or a noun of the most importance in a relation in their definition) with word clusters while Chan and Roth (2010) performed this for an y lexical feature consisting of a single word . In this paper, we systematically explore the effectiveness of adding word clusters to different lexical features. 3.1 Relation Extraction One of the well defined relation extraction tas ks is the Automatic Content Extraction 1 (ACE) program sponsored by the U.S. g overnment . ACE 2004 defined 7 major entity types: PER (Person), ORG (Organization), FAC (Facility), GPE (Geo -Political Entity: countries, cities, etc.), LOC (Locatio n), WEA (Weapon) and VEH (Vehicle). An entity has three types of mention : NAM (proper name), NOM (nominal) or PRO (pronoun) . A relation was defined over a pair of entity mentions with in a single sentence . The 7 major relation types with examples are shown in Table 1. ACE 2004 al so d efined 23 relation subtypes. F ollowing most of the previous work, this paper only focuses on relation extraction of major types .

Given a relation instance ( , , ) m and sentence containing the pair, the goal is to learn a function which maps the instance x to a type c , where c is one of the 7 defined relation types or the type N il (no relati on exists). There are two commonly used learning paradigms for relation extraction:
Flat : This strategy performs relation detection and classification at the same time. One multi -class classifier is trained to discriminate among the 7 relation types plus t he N il type.

Hierarchical : This one separates relation detection from relation classification. One binary classifier is trained first to distinguish between relation instances and non -relation instances. This can be done by grouping all the instances of th e 7 relation types into a positive class and the instances of N il into a negative class. Then the thresholded output of this binary classifier is used as training data for learning a multi -class classifier for the 7 relation types ( Bunescu and Mooney, 2005 b ) . 3.2 Brown Word Clustering The Brown algorithm is a hierarchical clustering algorithm which initially assigns each word to its own cluster and then repeatedly merg es the two clusters which cause the least loss in average mutual information between adjacent clusters based on bigram statistics . By tracing the pairwise merging steps, one can obtain a word h ierarchy which can be represented as a binary tree. A word can be compactly represented as a bit string by following the path from the root to itself in the tree, assigning a 0 for each left branch, and a 1 for each right branch. A cluste r is just a branch of that tree. A high branch may correspond to more general concepts while the lower branches it includes might correspond to more specific ones. 
Brown et al. (1992) described an efficient impl emen tation based on a greedy algorithm which initially assigne d only the most frequent words into distinct clusters. It is worth point ing ou t that in this implementation each word occupies a leaf in the hierarchy, but each leaf might contain more than one word as can be seen from Table 2 . The length s of the bit strin gs also vary among different words. Give n a pair of entity mentions , sentence containing the pair , a feature based system extracts a feature vector v which contains diverse lexical, sy ntactic and semantic features. The goal is to learn a function which can estimate the conditional probabil ity ( | ) p c v , the probability of a relation type c given the feature vector v . The type with the highest probability will be output as the class label for the mention pair. 
We now descri be a supervised baseline system with a very large set of features and its learning strategy. 4.1 Baseline Feature S et We first adopted the fu ll feature set from Zhou et al. (2005) , a state -of -the -art feature based relation extraction system. For space reasons , we only show t he lexical features as in Table 3 and refer the reader to the paper for the rest of the features . 
At the lexical level, a relation instance can be seen as a sequence of tokens which form a five tuple &lt; Before, M1, Between, M2, A fter &gt;. Token s of the five members and the interaction between the heads of the two mentions can be extracted as features as shown in Table 3.

In addition , we cherry -picked the following features which were not included in Zhou et al. (2005) but were shown to be quite effective for relation extraction.
 Bigram of the words between the two mentions : This was extracted by both Zhao and Grishman (2005) and Ji ang and Zhai (2007), aiming to provide more order information of the tokens between the two mentions .

Patterns : Ther e are three types of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. ( 2005 ) ; 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between t he two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a) . These patterns can provide more structured information of how the two mentions are connected . 
Title list : This is tailored for the EMP -ORG type of relations as th e head of one of the mentions is usually a title . The features are decoded in a way similar to that of Sun (2009). 4.2 Baseline Learning S trategy We employ a simple learning framework that is similar to the hierarchical learning strategy as described in S ection 3.1. Specifically, we first train a binary classifier to distinguish between relation instances and non -relation instances. T hen rather than using the thresholded output of this binary classifier as training data, we use only the annotated relation instances to train a multi -class classifier for the 7 relation types. In the test phase, given a test insta nce x , we first apply the bi nary classifier to it for relation detection; if it is detected as a relation instance we then apply the multi -class relation classifier to classify it 4 . The selection of cluste r features aims to answer the following two questions: which lexical features should be augmented with word clusters to improve generalization acc uracy? How to select clusters at an appropriate level of granularity? We will describe our solutions in Sectio n 5.1 and 5.2 . 5.1 Cluster Feature Decoding While each one of the lexical features in Table 3 used by the baseline can potentially be augmented with word clusters, we believe the effectiveness of a lexical feature with augmentation of word clusters should be t ested either individually or incrementally according to a rank of its importance as shown in Table 4 . We will show the effectiveness of each cluster feature in the experiment section.

The importance is based on linguistic intuiti on s and observations of the contributions of different lexical features from various feature based systems . Table 4 simplifies a relation instance as a three tuple &lt; Context, M1, M2 &gt; where the Context includes the Before , Between and After from the five tup le representation. As a relation in ACE is usually short, the words of the two entity mentions can provide more critical indications for relation classification than the word s from the context. Within the two entity mentions, the head word of each mention is usually more important than other words of the mention; the conjunction of the two heads can provide an additional clue. And in general words other than the chunk head in the context do not contribute to establishing a relationship between the two entit y mentions.
The cluster based semi -supervised system works by a dding an additional layer of lexical features that incorporate word clusters as show n in column 4 of Table 4 . Take the US soldier as an example, if we decide to use a length of 10 as a threshol d to cut the Brown word hierarchy to generate word clusters, we will extract a cluster feature HM1_WC10= 1101111101 in addition to the lexical feature HM1=soldier given that the full bit string of soldier is 1101111101100 in Table 2. (Note that th e cluster feature is a nominal feature, not to be confused with an integer feature.) 5.2 Selection of Clusters G iven the bit string representation s of all the word s in a vocabulary , researchers usually use prefixes of different lengths of the bit string s to produce wor d clusters of various granularities. However, h ow to choose the set of prefix lengths in a principled way ? Th is has not been answered by prior work.

O ur main idea is t o learn the best set of prefix lengths , perhaps through the validation of their effective ness on a development set of data. To our knowledge, previous research simply uses ad -hoc prefix lengths and lacks this training procedure. The training procedure can be extremely slow for reasons to be explained below .

Formally, let l be the set of availabl e prefix lengths ranging from 1 bit to the length of the longe st bit string in the Brown word hierarchy and let m be the set of prefix lengths we want to use in decoding cluster features, then the probl em of selecting effective clusters transforms to finding a || m -combination of the set l which maximize s system performance. The training procedure can be extremely time consuming if we enumerate every possib le || m -combination of l , given that || m can range from 1 to the size of l and the size of l equals the length of the longest bit str ing which is usually 20 when inducing 1,000 clusters using the Brown algorithm. 
O ne way to achieve better efficiency is to consider only a subset of l instead of the full set . In addition, w e limit ourselves to use sizes 3 and 4 for m for matching prior work. This keeps the cluster features to a manageable size considering that ever y word in your vocabulary could contribute to a lexical feature. F or picking a subset of l , w e propose below two statistical measures for computing the importance of a certain prefix length .

Information Gain (IG): IG measures the quality or importance of a feature f by computing the difference between the prior entropy of classes C and the posterior entropy, given values V of the feature f (Hunt et al., 1966; Quinlan, 1986). For our purpose , C is the set of relation types, f is a cluster -based feature with a certain prefix length such as HM1_WC* where * means the prefix len gth and a value v is the prefix of the bit string representation of HM1 . More formally, t he IG of f is computed as follows: where the first and second terms refer to the prior and posterior entropies respectively.

For each prefix length in the set l , we can compute its IG for a type of cluster feature and then rank the p re fix lengths based on their IGs for that cluster feature. For simplicity , we rank the prefix lengths for a group of clus ter features (a group is a row from column 4 in Table 4) by collapsing the individual cluster features into a single cluster feature. For example, we collapse the 3 types : HM1_WC , HM2_WC and HM12_WC into a single type HM_WC for computing the IG.

Prefix Covera ge (P C): If we use a short prefix then the clusters produced correspond to the high branches in the word hierarchy and would be very general. Th e cluster features may not provide more informative information than the words themselves. Similarly, if we use a long prefix such as the length of the longest bit string, then maybe only a few of the lexical features can be covered by clusters. To capture this intuition, we define the PC of a prefix length i as below: where and HM1_WC i , (*) count is the number of occurrences of that feature in training d ata .
Similar to IG, we compute PC for a group of cluster features, not for each individual feature .
In our experiments, the top 10 ranked prefix lengths based on IG and prefix lengths with PC value s in the range [0.4, 0.9] were used.

In addition to the abo ve two statistical measures, for comparison, we introduce another two simple but extreme measures for the selection of clusters.
Use All Prefix es (UA) : UA produces a cluster feature at every available bit length wi th the hope that the underlying supervised system can learn proper weights of different cluster features during training . For example, if the full bit representation of  X  A pple  X  is  X  000  X  , UA would produce three cluster features: prefix1=0 , prefix2=00 and prefix3=000 . Because this method does not ne ed validation on the development set, it is the laziest but the fastest method for selecting clusters. 
Exhaustive Search (ES): ES works by trying every possible combination of the set l and picking the one that works the best for the development set. This is the most cautious and the slowest method for selecting clusters. In this section, we first present details of our unsupervised word clusters , the relation extraction data set and its preprocessing . We then present a series of experim ents coupled with result analyse s .
We used the English portion of the TDT5 corpora ( LDC2006T18 ) as our unlabeled data for inducing word clusters. I t contains roughly 83 million words in 3 . 4 million sentences with a vocabulary size of 450 K . We left case intact in the corpora. Following previous work, we used Liang  X  s implementation of the Brown clustering algorithm (Liang, 2005) . We induced 1,000 word clusters for words that appear ed at least twice in the corpora. The reduced vocabulary con tains 255K unique words. The clusters are available at http://www.cs.nyu.edu/~asun/data/TDT5_BrownW C.tar.gz .
 For relation extraction, we used the benchmark ACE 2004 training data. Followi ng most of the previous research, we used in experiments the nwire (newswire) and bnews (broadcast news) genres of the data containing 348 documents and 4374 relation instances. We extracted an instance for every pair of mentions in the same sentence which were separated by no more than two other mentions . T he non -relation instances generated were about 8 times more than the relation instances.
Preprocessing of the ACE documents : We used the Stanford parser 6 for syntactic and dependency parsing. We used ch unklink 7 to derive chunking information from the Stanford parsing. Because some b news documents are in lower case, we recover the case for the head of a mention if its type is NAM by making the first character into its upper case . This is for better matchi ng between the words in ACE and the words in the unsupervised word clusters .

We used the OpenNLP 8 maximum entropy (maxent) package as our machine learning tool . We cho o se to work with maxent because the training is fast and it has a good support for multi -class classification . 6.1 Baseline Performance Following previous work, w e did 5 -fold cross -v alidation on the 348 documents with hand -annotated entity mentions . Our results are shown in Table 5 w hich also lists the results of another three state -of -the -art fea ture based systems . For this and the following experiments, all the results were computed at the relation mention level .

Not e that a lthough all the 4 systems did 5 -fold cross -validation on the ACE 2004 data, the detailed data partition might be different. Also , w e were doing cross -validation at the document level which we believe was more natural than the instance level. Nonetheless, we believe our baseline system has achieved very competitive performance. 6.2 The Effectiveness of Cluster Selection We inves tigated the tradeoff between performance and training time of each proposed method in selecting clusters . In this experiment , we randomly selected 70 documents from the 348 documents a s test data which roughly equaled the size of 1 fold in the baseline in Section 6.1 . For the baseline in this section, all the rest of the documents were used as training data. For the semi -supervised system, 7 0 percent of t he rest of the documents were randomly selected as training data and 3 0 percent as development data . The set of prefix lengths that worked the best for the development set was chosen to select clusters. W e only used the cluster feature HM_WC in this experiment . 
Ta ble 6 shows that all the 4 proposed methods improve d baseline performance, with UA as the fastest a nd ES as the slowest. It was interesting that ES d id not always outperform the two statistical methods which might be because of its overfitting to the development set . In general, both PC and IG ha d good balance s between performance and training time. T he re was no dramatic difference in performance between using 3 and 4 prefix lengths .
For the rest of this paper, we will only use PC4 as our method in selecting clusters. 6.3 The Effectiveness of Cluster Features T he baseline here is the same one used in Sectio n 6.1 . For the semi -supervised system, each test fold was the same one used in th e baseline and the other 4 folds were further split in to a training set and a development set in a ratio of 7:3 for selecting clusters . We first added the cluster features ind ividually into the baseline and then added them incrementally according to the order specified in Table 4.

We found that adding clusters to the heads of the two mentions was the most effective way of introducing cluster features. Adding clusters to the words of the mentions can also help, though not as good as the heads. We were surprise d that the heads of chunks in context did not help. This might be because ACE relations are usually short and the limited number of long relations is not sufficient in generalizing cluster features. Adding clusters to every word in context hurt the performance a lot. Because of t h e behavior of each individual feature, it was not su rprising that adding them incrementally did not give more performance gain. For the rest of this paper, we will only use HM_WC as cluster features . 6.4 The Impact of Training Size We studied the impact of training data size on cluster features as shown in Table 8. The test data was always the same as the 5 -fold used in the baseline in Section 6.1. no matter the size of the training data. The training documen ts for the current size setup were randomly selected and added to the previous size setup (if applicable). For example, we randomly selected another 25 documents and added them to the previous 50 documents to get 75 documents. We made sure that every document participated in this experiment. The training docume nts for each size setup were split into a real training set and a development set in a ratio of 7:3 for selecting clusters.

There are some clear trends in Table 8. Under each training size, PC4 consistently outperformed the baseline and the system Prefix1 0 for relation classification . For PC4, the gain for classification was more pronounced than detection. The mixed detection results of Prefix10 indicated that only using a single prefix may not be stable.

We did not observe the same trend in the reductio n of annotation need with cluster -based features as in Koo et al. (2008) for dependency parsing . PC4 with size s 50, 125, 175 outperformed the baseline with size s 75, 175, 225 respectively . But this was not the case when PC4 was tested with size s 75 and 225 . This might d ue to the complexity of the relation extraction task. 6.5 Analysis T here were on average 69 cross -type errors in the baseline in Section 6.1 which were reduced to 56 by using PC4. Table 9 showed that most of the improvements involved EMP -OR G, GP E -AFF, DISC, ART and OTHER -AFF. The performance gain for PER -SOC was not as pronounced as the other five types. The five types of relations are ambiguous as they share the same entity type GPE while the PER -SOC relation only holds between PER and PER. This reflects that word clusters can help to dis tinguish between ambiguous relation types.

As mentioned earlier the gain of relation detection was not as pronounced as classification as shown in Table 8. The un balanced distribution of relation instances and no n -relation instances remains as a n obstacle for pushing the performan ce of relation extraction to the next level. We have described a semi -supervised relation extraction system with large -scale word clustering. We have systematic ally explored the effectiveness of different cluster -based features. We have also demonstrated that the two proposed statistical methods are both effective and efficient in selecting clusters at an appropriate level of granularity through an extensive expe rimental study.
Based on the experimental results, we plan to investigate additional ways to improve the p erformance of relation detection. Moreover, extending word clustering to phrase clustering (Lin and Wu, 2009) and pattern clustering (Sun and Grishman , 2010) is worth future investigation for relation extraction.

