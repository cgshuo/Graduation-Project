 A large num ber of question and answ er pairs can be col-lected from question and answ er boards and FAQ pages on the Web. This pap er prop oses an automatic metho d of nding the questions that have the same meaning. The metho d can detect seman tically similar questions that have little word overlap because it calculates question-question similarities by using the corresp onding answ ers as well as the questions. We dev elop two di eren t similarit y measures based on language mo deling and compare them with the traditional similarit y measures. Exp erimen tal results sho w that seman tically similar questions pairs can be e ectiv ely found with the prop osed similarit y measures.
 H.3.0 [ Information Searc h and Retriev al ]: General Algorithms, Measuremen t, Exp erimen tation Information Retriev al, FAQ retriev al, Language Mo dels
Man y web sites have question and answ er boards or FAQ pages. Retriev al of these human answ ered questions is very attractiv e since users can directly obtain answ ers rather than relev ant documen ts. In suc h retriev al systems accurate sim-ilarit y measures between questions are crucial. However, similarit y measures dev elop ed for documen ts do not work well for questions because questions are much shorter than documen ts. Traditional similarit y measures for sen tences suc h as the overlap coecien t, Dice coecien t and Jaccard coecien t work poorly when there is little word overlap be-tween sen tences.

Three di eren t types of approac hes has been dev elop ed in the literature to solv e this word mismatc h problem as fol-lows: The rst approac h uses kno wledge databases suc h as mac hine-readable dictionaries [3]. However, curren tly there are problems with the qualit y and structure of kno wledge databases. The second approac h emplo ys man ual annota-tions or rules, suc h as AskJeev es 1 . This approac h is ex-pensiv e and hard to expand to other domains. The nal approac h uses the statistical techniques of information re-triev al [1].

We think that the third approac h is the most promising if we can have a large num ber of seman tically similar but lexically di eren t question pairs. From these samples, we may extract statistically meaningful patterns to bridge the lexical chasm. The collections of similar question pairs can be further used in man y other IR and NLP researc h areas suc h as FAQ retriev al, question answ ering, example based mac hine translation and so on.

In this pap er, we study automatic metho ds of nding suc h pairs from existing question and answ er collections. Our as-sumption is if two answ ers are very similar, then the ques-tions connected to the answ ers should be seman tically simi-lar even though the two questions may be lexically very dif-feren t. We also study reliable similarit y measures between answ ers.
To nd similar answ er pairs, reliable similarit y measures between answ ers are required. The lengths of answ ers vary signi can tly. Answ ers can be very short esp ecially for fac-toid questions. Some answ ers are very long because some-times people generating answ ers just cop y multiple related documen ts from the web. Therefore, any similarit y measure seriously a ected by length is not appropriate for answ ers. In this pap er, we test three di eren t similarit y measures. The rst one is the cosine similarit y with TF.IDF weigh ts. This measure has been extensiv ely used for various IR and NLP tasks. An adv antage of using the cosine similarit y is that the measure is symmetric.
 The second one is the language mo deling technique [2]. The cross entrop y between two language mo dels is widely used. However, the cross entrop y values are not probabili-ties. A pair of answ ers that has a higher cross entrop y score than other pairs does not necessarily have stronger seman tic connections than the other pairs. For this reason, we do not use cross entrop y. Instead, we con vert every answ er into a Table 1: The ratio of correct answ er pairs in top 10, 100 and 1000 positions for each similarit y measure. query and retriev e other answ ers using the query likeliho od language mo deling technique. The outputs of the language mo deling technique are probabilities and can be used across di eren t pairs of answ ers. One prop erty of this measure is the scores are not symmetric. Every pair has two di eren t scores dep ending on whic h answ er becomes a query . In this study , we just pick the maxim um value of the two scores. We call this measure LM-SCORE.

The third measure is similar to the second measure in that it uses a language mo deling technique. This measure uses ranks instead of scores to resolv e the problem of non-symmetric scores. If answ er A retriev es answ er B at rank r 1 and answ er B retriev es answ er A at rank r 2 , then the similarit y between two answ ers is de ned as the rev erse of the harmonic mean of r 1 and r 2 . sim ( A; B ) = 1 2 ( 1 We call this measure LM-HRANK. We collected 5,200 question-answ er pairs from NHN Corp.'s Question and Answ er service 2 . All the questions are about email and written in Korean. The average length of ques-tions is 5.9 (words) and the average length of answ ers is 150.1. There are man y seman tically equiv alen t questions in the dataset because man y users do not carefully chec k whether there is the same question in the database before asking their questions. To calculate the cosine similarit y and the query likeliho od language mo dels, we used the LEMUR 3 toolkit.
In total, 1,351,700 pairs of answ ers are possible from 5,200 answ ers. All of these pairs are rank ed according to the three di eren t similarit y measures. We man ually evaluate the top 1000 pairs for eac h metho d. If a question pair connected to an answ er pair is seman tically iden tical or very similar, we judge the answ er pair is a correct matc h. Table 1 sho ws the ratio of the correct matc hes in the top 10, 100, and 1000 pairs for eac h similarit y measures.

The cosine similarit y works poorly because the measure favors short answ ers. For example, in our dataset, an answ er has only two words ("Korean homew ork") and answ er pairs con taining this short answ er usually have very high cosine similarit y scores. Because of this serious problem, the cosine similarit y can not be a good similarit y measure for answ ers.
The language mo deling technique based measures sho w good performance. In LM-SCORE, 90% of the answ er pairs in the top 10 connect seman tically equiv alen t questions. In the top 100, 67% of the answ er pairs are correct matc hes. LM-HRANK sho w better results than LM-SCORE in the comparison with the top 1000 pairs. Table 2 sho ws exam-3 http://www-2.cs.cm u.edu/lem ur/ Can I attac h a 5 mega byte le in my email? Sending big movie les to my friends over the net by email Wh y do we have to use only English for email addresses? Wh y can't I use Korean in email IDs? What is the best email service? Who pro vides the most popular and powerful email accoun ts? Who invented email? The rst person who used email Table 2: Examples of question pairs found using the LM-HRANK measure. (English translations). ples of the question pairs found using the LM-HRANK mea-sure. Eac h question pair in the examples con tains seman ti-cally similar questions but questions share very few common terms.

While LM-SCORE and LM-HRANK sho w comparable performance, they retriev e di eren t sets of answ er pairs. The num ber of overlapping answ er pairs between the top 100 pairs in LM-SCORE and the top 100 pairs in LM-HRANK is only 6. This implies more correct answ er pairs can be retriev ed when both measures are used together.

We have also tested the use of the scores generated from the traditional TF/IDF mo del and the Okapi BM25 mo del as similarit y measures. However, the results are not much better than the results of the cosine similarit y.
The exp erimen tal results sho w that we can automatically nd seman tically similar question pairs by measuring sim-ilarities between answ ers. By applying this technique to man y di eren t question and answ er collections, a large num-ber of similar question pairs can be gathered. We also nd language mo deling based similarit y measures are more ap-propriate than other similarit y measures in calculating sim-ilarities between answ ers. The prop osed similarit y measures can be used to cluster question-answ er pairs and the clus-ters can be further used to automatically generate FAQs or impro ve the performances of question and answ er retriev al systems. This work was supp orted by NHN Corp., the Cen ter for Intelligen t Information Retriev al and NSF gran t num ber DUE-0226144. Any opinions, ndings and conclusions or recommendations expressed in this material are the author(s)' and do not necessarily re ect those of the sponsor. [1] A. Berger, R. Caruana, D. Cohn, D. Freitag, and V. [2] J. M. Ponte, and W. B. Croft, A language mo deling [3] K. Hammond, R. Burk e, C. Martin and S. Lytinen,
