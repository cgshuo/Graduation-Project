 James T. Kw ok jamesk@cs.ust.hk Ivor W. Tsang ivor@cs.ust.hk Hong Kong In recen t years, there has been a lot of interest in the study of kernel metho ds (Sc h X olkopf &amp; Smola, 2002; Vapnik, 1998). The basic idea is to map the data in the input space X to a feature space via some non-linear map ' , and then apply a linear metho d there. It is now well-kno wn that the computational pro ce-dure dep ends only on the inner pro ducts 1 ' ( x i ) 0 ' ( x in the feature space (where x i ; x j 2 X ), whic h can be obtained ecien tly from a suitable kernel function k ( ; ). Besides, kernel metho ds have the imp ortan t computational adv antage that no nonlinear optimiza-tion is involved. Thus, the use of kernels pro vides el-egan t nonlinear generalizations of man y existing lin-ear algorithms. A well-kno wn example in sup ervised learning is the supp ort vector mac hines (SVMs). In unsup ervised learning, the kernel idea has also led to metho ds suc h as kernel-based clustering algorithms (Girolami, 2002), kernel indep enden t comp onen t anal-ysis and kernel principal comp onen t analysis (PCA) (Sc h X olkopf et al., 1998b).
 While the mapping ' from input space to feature space is of primary imp ortance in kernel metho ds, the re-verse mapping from feature space bac k to input space (the pre-image problem) is also useful. Consider for example the use of kernel PCA for pattern denois-ing. Giv en some noisy patterns, kernel PCA rst ap-plies linear PCA on the ' -mapp ed patterns in the fea-ture space, and then performs denoising by pro jecting them onto the subspace de ned by the leading eigen-vectors. These pro jections, however, are still in the feature space and have to be mapp ed bac k to the in-put space in order to reco ver the denoised patterns. Another example is in visualizing the clustering so-lution of a kernel-based clustering algorithm. Again, this involves nding the pre-images of, say, the cluster cen troids in the feature space. More generally , meth-ods for nding pre-images can be used as reduced set metho ds to compress a kernel expansion (whic h is a linear com bination of man y feature vectors) into one with few er terms. They can o er signi can t speed-ups in man y kernel applications (Burges, 1996; Sch X olkopf et al., 1998a).
 However, the exact pre-image typically does not ex-ist (Mik a et al., 1998), and one can only settle for an appro ximate solution. But even this is non-trivial as the dimensionalit y of the feature space can be in nite. (Mik a et al., 1998) cast this as a nonlinear optimiza-tion problem, whic h, for particular choices of kernels (suc h as the Gaussian kernel 2 ), can be solv ed by a xed-p oint iteration metho d. However, as men tioned in (Mik a et al., 1998), this metho d su ers from nu-merical instabilities. Moreo ver, as in any nonlinear optimization problem, one can get trapp ed in a local minim um and the pre-image obtained is thus sensitiv e to the initial guess.
 While the inverse of ' typically does not exist, there is usually a simple relationship between feature-space distance and input-space distance for man y commonly used kernels (Williams, 2001). In this pap er, we use this relationship together with the idea in multidimen-sional scaling (MDS) (Co x &amp; Cox, 2001) to address the pre-image problem. The prop osed metho d is non-iterativ e and involves only linear algebra.
 Our exp osition in the sequel will focus on the pre-image problem in kernel PCA, though it can be applied equally well to other kernel metho ds. The rest of this pap er is organized as follo ws. Brief introduction to the kernel PCA is given in Section 2. Section 3 then describ es our prop osed metho d. Exp erimen tal results are presen ted in Section 4, and the last section gives some concluding remarks. 2.1. PCA in the Feature Space In this Section, we give a short review on the kernel PCA. While standard exp ositions (Mik a et al., 1998; Sch X olkopf et al., 1998b) usually focus on the simpler case where the ' -mapp ed patterns have been cen tered, we will carry out this cen tering explicitly in the follo w-ing.
 Giv en a set of patterns f x 1 ; : : : ; x N g 2 R d . Kernel PCA performs the traditional linear PCA in the fea-ture space. Analogous to linear PCA, it also involves an eigen decomp osition HKH = U U 0 , where K is the kernel matrix with K ij = k ( x i ; x j ), is the cen tering matrix, I is the N N iden tity matrix, 1 = [1 ; 1 ; : : : ; 1] 0 is an N 1 vector, U = [ 1 ; : : : ; with i = [ i 1 ; : : : ; iN ] 0 and = diag ( 1 ; : : : ; Denote the mean of the ' -mapp ed patterns by ' = The k th orthonormal eigen vector of the covariance ma-trix in the feature space can then be sho wn to be where ~ ' = [ ~ ' ( x 1 ) ; ~ ' ( x 2 ) ; : : : ; ~ ' ( x jection of the ' -image of a pattern x onto the k th comp onen t by k . Then, where ~ k ( x ; y ) = ~ ' ( x ) 0 ~ ' ( y ) and k x = [ k ( x ; x 1 ) ; : : : ; k ( x ; x N )] 0 . Denote then (2) can be written more compactly as k = The pro jection P K ' ( x ) of ' ( x ) onto the subspace spanned by the rst K eigen vectors 3 is then P K ' ( x ) = where M = 2.2. Iterativ e Scheme for Finding the As P ' ( x ) is in the feature space, we have to nd its pre-image ^ x in order to reco ver the denoised pattern (Figure 1). As men tioned in Section 1, the exact pre-image may not even exist, and so we can only reco ver an ^ x where ' ( ^ x ) ' P ' ( x ). (Mik a et al., 1998) ad-dressed this problem by minimizing the squared dis-tance between ' ( ^ x ) and P ' ( x ): where includes terms indep enden t of ^ x . This, how-ever, is a nonlinear optimization problem. As men-tioned in Section 1, it will be plagued by the problem of local minim um and is sensitiv e to the initial guess of ^ x .
 For particular choices of kernels, suc h as Gaussian ker-nels of the form k ( x ; y ) = exp( k x y k 2 =c ), this non-linear optimization can be solv ed by a xed-p oint iter-ation metho d. On setting the deriv ativ e of (5) to zero, the follo wing iteration form ula is obtained: Here 4 , i = However, as men tioned in (Mik a et al., 1998), this it-eration scheme is numerically unstable and one has to try a num ber of initial guesses for ^ x .
 Notice from (6) that the pre-image obtained is in the span of x i 's. Besides, because of the exp onen tial drop rapidly with increasing distance from the pre-image. These observ ations will be useful in Section 3. For any two points x i and x j in the input space, we can obtain their Euclidean distance d ( x i ; x j ). Anal-ogously , we can also obtain the feature-space dis-tance ~ d ( ' ( x i ) ; ' ( x j )) between their ' -mapp ed im-ages. Moreo ver, for man y commonly used kernels, there is a simple relationship between d ( x i ; x j ) and ~ posed metho d is then as follo ws (Figure 2). Let the pattern to be denoised be x . As men tioned in Sec-tion 1, the corresp onding ' ( x ) will be pro jected to P ' ( x ) in the feature space. For eac h training pattern x , this P ' ( x ) will be at a distance ~ d ( P ' ( x ) ; ' ( x from eac h ' ( x i ) in the feature space. Using the dis-tance relationship men tioned above, we can obtain the corresp onding input-space distance between the de-sired pre-image ^ x and eac h of the x i 's. Now, in mul-tidimensional scaling (MDS) 5 (Co x &amp; Cox, 2001), one attempts to nd a represen tation of the objects that preserv es the dissimilarities between eac h pair of them. Here, we will use this MDS idea to embed P ' ( x ) bac k to the input space. When the exact pre-image exists, it would have exactly satis ed these input-space distance constrain ts 6 . In cases where the exact pre-image does not exist, we will require the appro ximate pre-image to satisfy these constrain ts appro ximately (to be more precise, in the least-square sense).
 Notice that instead of nding the pre-image of P ' ( x ) in kernel PCA, this pro cedure can also be used to nd the pre-image of any feature vector in the fea-ture space. For example, we can use this to nd the pre-images of the cluster cen troids obtained from some kernel clustering algorithm, as will be demonstrated in Section 4.
 The follo wing sections describ e these steps in more detail. Computation of the feature-space distances ~ uses the distance relationship to obtain the corre-sponding distances in the input space. Finally , Sec-tion 3.3 uses these distances to constrain the embed-ding of the pre-image. 3.1. Distances in the Feature Space For any two patterns x and x i , the squared feature-space distance between the pro jection P ' ( x ) and ' ( x i ) is given by: Now, from (3) and (4), we have k P ' ( x ) k 2 = = ~ k 0 = k x + and Thus, (7) becomes: ~ d 2 ( P ' ( x ) ; ' ( x i )) where K ii = k ( x i ; x i ). 3.2. Distances in the Input Space Giv en the feature-space distances between P ' ( x ) and the ' -mapp ed training patterns (Section 3.1), we now pro ceed to nd the corresp onding input-space dis-tances, whic h will be preserv ed when P ' ( x ) is em-bedded bac k to the input space (Section 3.3). Re-call that the distances with neigh bors are the most imp ortan t in determining the location of any point (Section 2.2). Hence, in the follo wing, we will only consider the (squared) input-space distances between P ' ( x ) and its n nearest neigh bors 7 , i.e., This in turn can o er signi can t speed-up, esp ecially during the singular value decomp osition step in Sec-tion 3.3. Moreo ver, this is also in line with the ideas in metric multidimensional scaling (Co x &amp; Cox, 2001), in whic h smaller dissimilarities are given more weigh t, and in locally linear embedding (Ro weis &amp; Saul, 2000), where only the local neigh borho od structure needs to be preserv ed.
 We rst consider isotropic kernels 8 of the form ship between the feature-space distance ~ d ij and the input-space distance d ij (Williams, 2001): ~ and hence, Typically , is invertible. For example, for the Gaus-sian kernel ( z ) = exp( z ) where is a constan t, we Similarly , for dot pro duct kernels of the form ship between the dot pro duct K ij = k ( x i ; x j ) in the feature space and the dot pro duct s ij = x 0 i x j in the input space (Williams, 2001): Moreo ver, is often invertible. For example, for the polynomial kernel ( z ) = z p where p is the polyno-mial order, s ij = K 1 p ij when p is odd. Similarly , for the sigmoid kernel ( z ) = tanh( vz c ) where v; c 2 R are parameters, s ij = (tanh 1 ( K ij ) + c ) =v . The corre-sponding squared distance in the input space is then Thus, in summary , we can often use (9), (11) for isotropic kernels, or (8), (12), (13) for dot pro duct kernels, to construct the input-space distance vector d 2 in (10). 3.3. Using the Distance Constrain ts For the n neigh bors f x 1 ; : : : ; x n g 2 R d obtained in Section 3.2, we will rst cen ter them at their cen-troid x = 1 tem in their span. First, construct the d n matrix H in (1), HX 0 will cen ter the x i 's at the cen troid (i.e. the column sums of HX 0 are zero). Assuming that the training patterns span a q -dimensional space (i.e., X is of rank q ), we can obtain the singular value de-comp osition (SVD) of the d n matrix ( HX 0 ) 0 = XH as: where U = [ e 1 ; : : : ; e q ] is a d q matrix with orthonor-mal columns e i and Z = [ z 1 ; : : : ; z n ] is a q n matrix with columns z i being the pro jections of x i onto the e 's. Note that the computational complexit y for per-forming SVD on an d n matrix is O ( kd 2 n + k 0 n 3 ), where k and k 0 are constan ts. Hence, using only the n neigh bors instead of all N training patterns can o er a signi can t speed-up. Besides, the squared distance of x i to the origin, whic h is still at the cen troid, is equal to k z i k 2 . Again, collect these into an n -dimensional vector, as d 2 0 = [ k z 1 k 2 ; : : : ; k z n k 2 ] 0 . Recall from Section 2.2 that the appro ximate pre-image ^ x obtained in (Mik a et al., 1998) is in the span of the training patterns, with the con tribution of eac h individual x i dropping exp onen tially with its distance from ^ x . Hence, we will assume in the follo wing that the required pre-image ^ x is in the span of the n neigh-bors. As men tioned in Section 3, its location will be obtained by requiring d 2 ( ^ x ; x i ) to be as close to those values obtained in (10) as possible, i.e., In the ideal case, we should have exactly preserv ed these distances. However, as men tioned in Section 1, in general there is no exact pre-image in the input space and so a solution satisfying all these distance constrain ts may not even exist. Hence, we will set-tle for the least-square solution ^ z . Follo wing (Go wer, 1968), this can be sho wn to satisfy: Now, Z11 0 = 0 because of the cen tering. Hence, the pre-image can be obtained as ^ z = This ^ z is expressed in terms of the coordinate system de ned by the e j 's. Transforming bac k to the original coordinate system in the input space, we thus have 4.1. Pre-Images in Kernel PCA In this Section, we rep ort denoising results on the USPS data set consisting of 16 16 handwritten digits 9 . For eac h of the ten digits, we randomly choose some examples (300 and 60 resp ectiv ely) to form the training set, and 100 examples as the test set. Kernel PCA is performed on eac h digit separately .
 Tw o types of additiv e noise are then added to the test set. The rst one is the Gaussian noise N (0 ; 2 ) with variance 2 . The second type is the \salt and pepp er" noise with noise level p , where p= 2 is the probabilit y that a pixel ips to blac k or white. The mo del selection problem for the num ber ( K ) of eigen vectors is side-stepp ed by choosing K = arg min n k P n ' ( x ) ' ( e x ) k where x is the noisy image and e x is the original (clean) image. 10 neigh bors is used in locating the pre-image. Moreo ver, comparison will be made with the tradi-tional metho d in (Mik a et al., 1998). 4.1.1. Gaussian Kernel We rst exp erimen t with the Gaussian kernel exp( z ), where we set 1 = 1 x k 2 . Figures 3 and 4 sho w some typical noisy test images for the two types of noise, and the corresp ond-ing denoised images. Tables 1 and 2 sho w the numeri-cal comparisons using the signal-to-noise ratio (SNR). As can be seen, the prop osed metho d pro duces better results both visually and quan titativ ely.
 num ber of SNR training noisy our 4.1.2. Polynomial Kernel Next, we perform exp erimen t on the polynomial kernel ( xy +1) d with d = 3. Follo wing the exp osition of (Mik a et al., 1998) in Section 2.2, (5) now becomes ( ^ x 0 ^ x + w.r.t. ^ x to zero, we obtain an iteration form ula for the num ber of SNR polynomial kernel: However, this iteration scheme fails to con verge in the exp erimen ts, even after rep eated restarts. On the other hand, our prop osed metho d is non-iterativ e and can alw ays obtain reasonable pre-images (Figure 5). Tables 3 and 4 sho w the resulting SNRs for the Gaus-sian noise and \salt and pepp er" noise.
 4.2. Pre-Images in Kernel k -Means Clustering In this Section, we perform the kernelized version of k -means clustering algorithm on the USPS data set, and then use the prop osed metho d to nd the pre-images of the cluster cen troids. We select a total of 3000 random images from the USPS data set and the same Gaussian kernel as in Section 4.1.1. Figure 6 sho ws the resultan t pre-images. For comparison, the cluster cen troids obtained by averaging in the input space are also sho wn. Note that, in general, these cluster cen troids may have to be interpreted with caution. As in ordinary k -means clustering, the (exact or appro ximate) pre-images of the cluster cen troids may sometimes fall outside of the data distribution. In this pap er, we address the problem of nding the pre-image of a feature vector in the kernel-induced fea-ture space. Unlik e the traditional metho d in (Mik a et al., 1998) whic h relies on nonlinear optimization and is iterativ e in nature, our prop osed metho d directly nds the location of the pre-image based on distance constrain ts. It is non-iterativ e, involves only linear al-gebra and does not su er from numerical instabilities or the local minim um problem. Moreo ver, it can be applied equally well to both isotropic kernels and dot pro duct kernels. Exp erimen tal results on denoising the USPS data set sho w signi can t impro vemen ts over (Mik a et al., 1998).
 In the future, other classes of kernel functions will also be investigated, esp ecially those that are de ned on structured objects, in whic h the pre-image problem then becomes an imp ortan t issue (Weston et al., 2003). This researc h has been partially supp orted by the Researc h Gran ts Council of the Hong Kong Special Administrativ e Region under gran ts HKUST2033/00E and HKUST6195/02E.
 Burges, C. (1996). Simpli ed supp ort vector decision rules. Proceedings of the Thirte enth International Confer ence on Machine Learning (pp. 71{77). San Francisco, CA: Morgan Kaufmann.
 Cox, T., &amp; Cox, M. (2001). Multidimensional scaling .
Monographs on Statistics and Applied Probabilit y 88. Chapman &amp; Hall / CR C. Second edition. Girolami, M. (2002). Mercer kernel-based clustering in feature space. IEEE Transactions on Neur al Net-works , 13 , 780{784.
 Gower, J. (1968). Adding a point to vector diagrams in multiv ariate analysis. Biometrika , 55 , 582{585. Mik a, S., Sch X olkopf, B., Smola, A., M X  uller, K., Scholz,
M., &amp; R X atsc h, G. (1998). Kernel PCA and de-noising in feature spaces. Advanc es in Neur al In-formation Processing Systems 11 . San Mateo, CA: Morgan Kaufmann.
 Roweis, S., &amp; Saul, L. (2000). Nonlinear dimensional-ity reduction by locally linear embedding. Scienc e , 290 , 2323{2326.
 Sch X olkopf, B., Knirsc h, P., Smola, A., &amp; Burges, C. (1998a). Fast appro ximation of supp ort vector ker-nel expansions, and an interpretation of clustering as appro ximation in feature spaces. Proceedings of the DAGM Symp osium Muster erkennung (pp. 124{ 132).
 Sch X olkopf, B., &amp; Smola, A. (2002). Learning with ker-nels . MIT.
 Sch X olkopf, B., Smola, A., &amp; M X  uller, K. (1998b). Non-linear comp onen t analysis as a kernel eigen value problem. Neur al Computation , 10 , 1299{1319. Vapnik, V. (1998). Statistic al learning theory . New York: Wiley .
 Weston, J., Chap elle, O., Elissee , A., Sch X olkopf, B., &amp; Vapnik, V. (2003). Kernel dep endency estima-tion. Advanc es in Neur al Information Processing Systems 15 . Cam bridge, MA: MIT Press.
 Williams, C. (2001). On a connection between kernel
PCA and metric multidimensional scaling. Advanc es in Neur al Information Processing Systems 13 . Cam-
