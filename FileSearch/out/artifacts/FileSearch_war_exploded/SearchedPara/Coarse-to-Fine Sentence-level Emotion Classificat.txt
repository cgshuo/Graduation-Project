 This paper proposes a novel approach using a coarse-to-fine analysis strategy for sentence-level emotion classifica-tion which takes into consideration of similarities to sen-tences in training set as well as adjacent sentences in the context. First, we use intra-sentence based features to de-termine the emotion label set of a target sentence coarsely through the statistical information gained from the label sets of the k most similar sentences in the training data. Then, we use the emotion transfer probabilities between neighboring sentences to refine the emotion labels of the target sentences. Such iterative refinements terminate when the emotion classification converges. The proposed algo-rithm is evaluated on Ren-CECps, a Chinese blog emotion corpus. Experimental results show that the coarse-to-fine emotion classification algorithm improves the sentence-level emotion classification by 19.11% on the average precision metric, which outperforms the baseline methods.
 I.2.7 [ Computing Methodologies ]: Artificial Intelligen-cenatural language processing, text analysis Emotion classification, Multi-label classification, Machine learning Co rresponding author.
 anger. It plays an essential role in decision making, percep-tion, learning etc., and it influences the very mechanisms of rational thinking [5]. In the past few years, with the popular-ity of Web 2.0, text emotion analysis has attracted growing attentions. Emotion analysis investigates how to classify a text fragment into a set of emotion labels such as anger, joy etc. To measure and recognize the emotional changes of population in large scale is one of the most important areas in social sciences and economics studies [2, 1].

Text emotion classification focuses on the writer or speech holder X  X  emotional state conveyed through the text. Previ-ous emotion classification researches mainly focused on blog postings [4, 8, 3] from author X  X  perspective. Evaluation for most of them use the LiveJournal 1 blog corpus which has 132 emotion categories and the blog text is classified accord-ing to the mood reported by its author during composition. Mishne [4] used the top forty frequent moods as classes and implemented a binary SVM classifiers for mood classifica-tion. Instead of using standard machine learning approach, Keshtkar and Inkpen [3] proposed a hierarchical approach which uses the hierarchy of possible moods and achieved a better result. Wu et al. [8] represented the emotional state of each sentence as a sequence of semantic labels and at-tributes. They first used the Apriori algorithm to derive the emotion association rules (EARs) for each emotion, and then developed a separable mixture model to estimate the similarity between the sentence and the EARs of each emo-tion.

In previous studies, text emotion classification (TEC) is always treated as a single-label classification problem. How-ever, a word or phrase may express more than one emo-tion. Examples include the idiom  X   X  X  X  X  X  X  (mixed hope and fear) X ,  X   X  X  X  X  X  X  X  (take the pleasure with pain) X , etc.. Statistics on Ren-CECps [6], a Chinese blog emotion an-notated corpus, shows that about 15.1% Chinese emotion words are multi-emotion words which express complex feel-ings in its usage [7]. Furthermore, most previous works clas-sified sentence emotion using intra-sentence features without considering their neighboring sentences. It is common sense that the emotions of neighboring sentences provide relevant information.

Base on the above two observations, a novel approach us-ing a coarse-to-fine analysis strategy is proposed for sentence-www.livejournal.com lev el emotion classification which incorporates features from training data using both Multi-Label k Nearest Neighbor (ML-k NN) approach [9] as well as contextual neighbors. ML-k NN is first used to obtain coarse multi-emotions of a target sentence. Then, emotion transfer probabilities of its neighboring sentences are incorporated to refine the emotion analysis of a target sentence. The emotions of the neighbor-ing sentences are also updated and used for refinement iter-atively. Such a coarse-to-fine analysis iterations terminate until the analysis results up to specified iteration number. The evaluation on the Ren-CECps shows that the proposed approach effectively improves the sentence-level TEC.
The rest of this paper is organized as follows. Section 2 de-scribes the ML-k NN based coarse-to-fine approach for TEC. Section 3 presents the performance evaluation. Section 4 is the conclusion.
In our proposed algorithm, the emotion label set of a sen-tence s is determined by features from two sources. Firstly, the emotion classification of s considers the emotions of its most k similar sentences in the training corpus. These sen-tences, referred to as the nearest neighbors in the k NN based algorithm, are compared to through intra-sentence features such as word unigram and character bigram. Secondly, the emotion classification of s considers the emotion of its con-textual sentences since the author X  X  emotion normally has its inertia. This is the motivation for using the inter-sentence features in emotion classification. In this work, only the im-mediate left and immediate right sentences of s are used as contextual information. In other words, the context window size is [-1,+1].

To make use of both the intra-sentence features from the training data and inter-sentence features from contextual sentences, a coarse-to-fine emotion classification strategy is used. ML-k NN is first applied to find out the most similar sentences, from the training data [6]. This step coarsely de-termines the emotion class of s . Then the emotion class of s is fine tuned by incorporating the emotion transfer proba-bilities between its neighboring sentences iteratively.
Multi Label k NN (ML-k NN) is used to coarsely determine the emotion label set of each sentence. ML-k NN is a variant of the single label k Nearest Neighbor lazy algorithm. For each test sentence s , ML-k NN first identifies its k nearest neighbors N ( s ) in the training set. Let Y be the finite set of emotion labels. For a emotion label  X  2 Y , let H 1  X  note the event that s has the emotion label  X  , H 0  X  denote the event that s does not have the emotion label  X  , and E ( j 2f 0 , 1 , ...k g ) denote the event that, among N ( s ), there are exactly j sentences which has label  X  . Following the maximum a posteriori principle, the category vector  X  X  s (  X  ) can be determined as: Here,  X  C s (  X  ) is a membership counting vector and  X  C a 2 N ( s )  X  X  a (  X  ). It counts the number of neighbors of s be-longing to the  X  -th emotion. By Bayes X  X  theorem, Eq. (1) Fi gure 1: An illustration of incorporating the emo-tion transfer probabilities between neighboring sen-tences, which accounts for all emotion transfer cases. can be rewritten as: The prior probabilities p ( H b  X  ) and probabilities p ( E f 0 , 1 , ...k g ) can be estimated from the training set based on frequency counting.
Based on the coarse emotion classification, the emotion transfers between neighboring sentences are used to refine emotion classification. Let s P denotes the previous sentence of s , and s N be the next sentence following s , and s P s
N must all be in the same paragraph. Let P 1  X  be the event that s P has the emotion label  X  , and P 0  X  be the event that s
P does not have the label  X  . Similarly, N 1  X  is the event that s
N has the label  X  , while N 0  X  is the event that s N does not have the label  X  . If s is the first sentence of a paragraph, event P 1  X  and P 0  X  will not happen, while the last sentence of a paragraph s is, event N 1  X  and N 0  X  will not happen.
Assuming that the emotion transfer be a sequence of events and each transfer event is independent, as the joint event E  X  for all emotional transfer,  X  X  s (  X  ) can be determined as: Eq. (3) is also referred to as the AET (All Emotion Transfer) as it accounts for all emotion transfer cases between neigh-b oring sentences. Figure.1 illustrates the different emotion transfers that determines the label of s .

If only the same emotion transfer is considered,  X  X  s (  X  ) is then determined by: Eq. (4) is also called SET (Same Emotion Transfer) as it counts for the same emotion transfer. In Eq. (3), four kinds of emotion transfer probabilities need to be estimated. They can be computed by: The other three can be estimated similarly. The conditional can be obtained from the above emotion transfer probabil-ities. For example, p ( P 1  X  j H 1  X  ) = p (  X  !  X  ) and p ( N p (  X  !  X  ) etc.
 A lgorithm 1 Coarse-to-fine emotion classification algorith-m.
 Inp ut: T , test corpus; I , iteration number. 1: for each sentence s 2 T do 2: Identify the k nearest neighbors N ( s ); 3: for each label  X  2 Y do 4: Count  X  C s (  X  ) = 5: Compute  X  X  s (  X , 0) using Eq.2 ; 6: end for 7: end for 8: for each t 2 [1 , I ] do 9: for each sentence s 2 T do 10: for each label  X  2 Y do 11: Iteratively refine  X  X  s (  X , t ) using Eq.3 (AET) or 12: end for 13: end for 14: end for
The pseudo code of the proposed Coarse-to-Fine Emo-tion Classification algorithm (C2FEC) is given above. ML-k NN is applied to classify the emotions of all sentences in the testing corpus (line 1-7). The emotions obtained for all the sentences by ML-k NN (result at line 7) are refined by using a complex classifier which incorporates the emotion transfer probabilities in iterations (line 8-14). The iteration terminates when the iteration number reaches the specified iteration number I .
To evaluate the sentence-level emotion classification ap-proaches, the Ren-CECps data [6] is used. Ren-CECps is an annotated Chinese blog emotion corpus with either eight emotional categories labeled at the sentence level (emotional hate ) or neutral if the sentence is factual. The corpus con-tains 1,487 documents, 11,255 paragraphs, 35,096 sentences, and 878,164 Chinese words.

To evaluate the proposed C2FEC algorithm, it is also com-pared to three other algorithms, namely the Binary Rele-vance method using Naive Bayes (BR N B), the Binary Rel-evance method using linear SVM (BR SV M) and ML-k NN. The BR approach is used because it is a different kind of approach in classification and it can perform as well or even superior than ML-k NN in some aspects. Intra-sentence fea-tures used are unigram of Chinese words after filtering stop words, and term frequency weighting for document repre-sentation. The cosine similarity measure is used to compute the similarity of two sentences.

The commonly used metrics for multi-label classification are different from those used in single-label classification due to the inherent differences of the classification problem. In this study, five popular evaluation metrics are adopted in the multi-label classification experiment include hamming loss, one-error, coverage, ranking loss, and average precision.
In the experiments, the Ren-CECps corpus is randomly split into 3 equal parts based on documents. 3 fold cross validation is performed. All the performance given below are the average of 3 folds.
The performance of the emotion classification by different approaches are given in Table 1. The result shown in the table used k =8 by ML-k NN, since k =8 gives the best perfor-mance. Items (4) and (5) are the results achieved by AET, while the iteration number is set to 1 and 100, respectively. Items (6) and (7) are the results obtained by SET, which on-ly considers the same emotion transfer (the iteration number is set to 1 and 100). The contribution of emotion transfer in emotion classification can be observed first by compar-ing ML-k NN with C2FEC with only one iteration ( I =1) as shown in Item (4) and (6) compared to Item (3). Results show that with only one iteration, C2FEC can improve the one-error by 11.5% and 12.1%, respectively. As shown in Item (7), the best performance is achieved by SET with 100 iterations. In fact, when iteration reaches 100, compared to ML-k NN, the one-error of SET decreases by 24.02% and av-erage precision increases by 19.11%, the most significant in the two proposed algorithms. All things considered, the pro-posed algorithm outperforms all the baseline methods due to the use of sentential context information.

As shown in Table 1, BR S VM outperforms BR N B on most evaluation metrics except coverage and ranking loss, and slightly outperforms ML-k NN on one-error metrics. It is shown that ML-k NN, a multi-label algorithm adapta-tion method, is not far superior to problem transfer method BR S VM. However, when sentential context is introduced, the proposed algorithms are obviously better because ML-k NN has the ability to incorporate the contextual features.
To estimate the performance of both AET and SET with different iterations, comparison is made between the two for iteration from 1 to 100 as shown in Fig. 2 on average precision and one-error, respectively. There are a number of observations. Firstly, both algorithms exhibit similar trend and converge well. Secondly, the iterations from 1 to 10 give the largest improvement and the rest of the improvement is monotonic. It is also important to know that SET always performs better than AET. One reason is that the transfer emotions between neighboring sentences with the highest probabilities are always the same. Another reason is that the assumption of AET that each emotion transfer event (3 ) ML-k NN 0 .1590  X  0.0157 0.6 107  X  0.0245 2. 7708  X  (4 ) C2 FEC-AET( I =1) 0 .1550  X  0.0125 0.5 404  X  0.0226 2. 4800 (5 ) C2 FEC-AET( I =100) 0 .1512  X  0.0095 0.4 715  X  0.0233 2. 1949 (6 ) C2F EC-SET( I =1) 0 .1507  X  0.0141 0.5 366  X  0.0226 2. 4762 (7 ) C2F EC-SET( I =100) 0. 1426  X  0.0126 0.4 640  X  0.0233 2. 1874 is independent and identically distributed, may not be as suitable as that of SET. Fi gure 2: Iteration curves by average precision and one-error, respectively.
In this paper, a novel approach which uses a coarse-to-fine analysis strategy for sentence-level emotion analysis is proposed by incorporating the features from the tradition-al similar measures and contextual sentences. The emotion label set of the target sentence is determined by ML-k NN coarsely and then refined with the emotion transfer probabil-ities following the coarse-to-fine analysis strategy iteratively. The ability of the ML-k NN to incorporate contextual infor-mation makes it possible to obtain further performance gain-s. The evaluation on a large Chinese blog emotion corpus shows that the proposed algorithms improve sentence-level emotion analysis by significant 24.02% on one-error metric, and 19.11% on average precision, outperform most of the existing methods.
This investigation is supported in part by the China Post-doctoral Science Foundation (No. 2011M500670), Nation-al Natural Science Foundation of China (No.61203378 and No.60973076), Hong Kong Polytechnic University(Z0EP) and HIT.NSFIR.2010124. [1] J. Bollen, H. Mao, and X.-J. Zeng. Twitter mood [2] P. S. Dodds and C. M. Danforth. Measuring the [3] F. Keshtkar and D. Inkpen. A hierarchical approach to [4] G. Mishne. Experiments with mood classification in [5] R. W. Picard. Affective Computing . MIT Press, 1997. [6] C. Quan and F. Ren. Construction of a blog emotion [7] C. Quan and F. Ren. An exploration of features for [8] C.-H. Wu, Z.-J. Chuang, and Y.-C. Lin. Emotion [9] M.-L. Zhang and Z.-H. Zhou. ML-kNN: A lazy learning
