
Sparse Metric Learning (SML), capable of learning both a good distance metric and low-dimension representations simultaneously, has received much attention recently. How-ever, performance of existing sparse metric learning ap-proaches is usually limited because they either made some relaxations or targeted the SML objective indirectly. In this paper, we propose a Generalized Sparse Metric Learning method (GSML). This novel framework offers a unified view for understanding many of the popular sparse metric learn-ing algorithms including the Sparse Metric Learning frame-work proposed in [15], the Large Margin Nearest Neigh-bor (LMNN) [21][22], and the D-ranking Vector Machine (D-ranking VM) [14]. Moreover, GSML also establishes a close relationship with the Pairwise Support Vector Ma-chine [20]. Furthermore, the proposed framework is ca-pable of extending many current non-sparse metric learn-ing models such as Relevant Vector Machine (RCA) [4] and a state-of-the-art method proposed in [23] into their sparse versions. We present the detailed framework, provide theoretical justifications, build various connections with other models, and propose a practical iterative optimization method, making the framework both theoretically important and practically scalable for medium or large datasets. A se-ries of experiments show that the proposed approach can outperform previous methods in terms of both test accu-racy and dimension reduction, on six real-world benchmark datasets.
Metric learning is an important concept in machine learning and data mining. Its objective is to learn a proper distance metric so as to improve prediction accuracy in su-pervised learning or benefit clustering performance in un-supervised learning. To this end, a number of approaches have been proposed, e.g. [5, 8, 19, 23].

Despite of its success, metric learning usually attempts to learn a distance function f with a full square transfor-mation matrix A from the given data set S = { x k  X  R
D | k =1 ,...N,N  X  N } (i.e., the distance between x i and x j is defined as f ( x i , x j )=( x i  X  x j ) AA ( x while satisfying certain extra constraints; these constraints can force similar (dissimilar) points to stay closer (further apart) with the new distance metric. However, learning a full matrix may cause some problems. Additionally, the presented data are often contaminated by noise, especially for high-dimensional datasets. A full matrix cannot sup-press such noise and may limit accuracy as a consequence. On the other hand, a full matrix, which is inefficient to com-pute, would result in practical difficulties for many real ap-plications.

In order to solve the above shortcomings involved in metric learning, many researchers proposed to learn a sparse metric from different perspectives. These work in-cludes the Sparse Metric Learning (SML) framework [15], the Large Margin Nearest Neighbor (LMNN) [21, 22], and the D-ranking Vector Machine (D-ranking VM) [14]. All these methods are able to learn a good distance metric as well as a sparse or low-dimensional representation. Specif-ically, Rosales and Fung targeted the shortcomings of met-ric learning directly [15]. Their method has demonstrated considerable improvement over traditional metric learning approaches [15]. LMNN is motivated from the large mar-gin concept and its solution can also lead to sparse met-rics. Although not designed for sparse learning, D-ranking VM is shown to force a low-rank (usually rank one) on the metric. However, there are some limitations to these mod-els. Although SML, proposed in [15], started from a rea-sonable motivation for achieving sparsity as well as good distance metric, they relaxed their model and did not lead to the optimal solution. LMNN touches the sparse concept marginally, which is shown not to hit the target of sparse metric learning directly. D-ranking VM made a strong re-striction on the rank of the distance matrix, and hence may not be flexible for real application.

In contrast to previous sparse metric learning ap-proaches, in this paper, we propose a general framework called Generalized Sparse Metric Learning (GSML) along with physical interpretation and practical optimization. This novel framework not only offers a unified view for under-standing many of the popular sparse metric learning al-gorithms including the above three methods, but also it leads to an optimal model which generalizes the SML ap-proach proposed in [15]. Specifically, the proposed GSML can learn both a good distance as well as the optimal low-dimensional representation. 1 Both theoretical justifi-cation and empirical verification demonstrate that the pro-posed GSML can outperform previous methods in many real datasets.

The proposed unified framework is important because it builds up various connections with other existing models. Besides the above mentioned three models, GSML is shown to have a close connection with the Pairwise Support Vector Machines (PSVM) [20]. In addition, we also demonstrate that many existing metric learning approaches such as RCA and the method proposed by Xing et al [23] can readily be extended to their sparse versions based on our novel frame-work. Another appealing feature is that a simple iterative optimization algorithm can be applied to solve the origi-nal convex but computationally difficult Semi-Definite Pro-gramming (SDP) problem involved in the proposed frame-work, making the proposed model scalable for medium or large datasets.

In summary, we have proposed a unified framework that establishes various connections with many famous models. We present this unified picture in Figure 1. More details can be seen in Section 2.2 on how these connections can be built.

Figure 1. A unified framework establishing various connections with other models.

The paper is organized as follows. In the next section, we present our novel unified framework. The model definition, theoretical justification, connections with other approaches, and practical optimization will be discussed in turn in this section. In Section 3, we evaluate our algorithm and report experimental results. In Section 4, we discuss the related work. Finally, we set out the conclusion with some remarks.
In this section, we present the general framework for sparse metric learning. Assume that we are given a training data set S containing ND -dimensional data points x k  X  R D , k =1 , 2 ,...,N . Assuming A is a matrix, we denote its i -th row vector as A . A 0 means that the matrix A is a positive semi-definite matrix. Moreover, we denote the trace of matrix A by tr ( A ) . For simplicity, let O D be the set of all D by D orthonormal matrices i.e. columns of the matrices are orthonormal vectors.

The basic target of metric learning is to learn an appro-priate distance metric f from S with extra constraints on a set of triplets T = { ( i, j, k ) | f ( x i , x j )  X  f ( Such triplets provide a certain relative comparison among pairs, i.e. x i is more similar to x j than to x k . 2 In the con-text of sparse metric learning, f is assumed to be a linear transformation A : R D  X  R d (with d D for ob-taining sparsity) such that  X  ( i, j, k )  X  X  , ||  X  x i  X   X  ||  X  x
The unified sparse metric learning framework is pre-sented as follows: min s.t.  X  ( i, j, k )  X  X  , ||  X  x i  X   X  x j || 2 2  X ||  X  x i where t indexes the set T and  X   X  R is a positive trade-off parameter. And L is a pre-defined matrix, which could be the identity matrix, the covariance matrix, or some other specified matrix. We will show shortly that different choice of L can lead to different sparse metric learning models. Additionally, in order to avoid a trivial solution, the con-straint (1) is often modified with an added margin, i.e., ||  X  x
Denoting W = AA , we can transform the above opti-mization to the following equivalent form: where x ij is defined as x i  X  x j , and x ik is similarly defined.
We interpret the unified model as follows. The basic mo-tivation is that, a good choice of a distance metric should generally preserve the distance structure of the data: the distance between examples exhibiting similarity should be relatively smaller, in the transformed space, than between examples exhibiting dissimilarity . Such preservation is im-plied by the constraint (3). Moreover, for the purpose of generalization ability or noise suppression, the distance ma-trix defined by A should be regularized. This can be seen in the second term of (2). We will see why this term can be used as a regularized term for forcing the sparsity in Sec-tion 2.2.1. Finally, we can observe that the above model is a typical Semi-definite Programming (SDP) problem, which is convex but also well-known for its high computational complexity. In Section 2.3, we will show how to apply an iterative sub-gradient method to solve this difficult op-timization problem.
We will demonstrate that the proposed general frame-work contains many existing sparse metric learning mod-els as special cases. Importantly, we show that the model with L = I corresponds to the optimal model that is able to achieve the goal of sparse metric learning directly. It also contains LMNN and D-ranking VM as special cases. In addition, it can also build a close connection with the Pairwise SVM model [20]. Finally, we demonstrate that many current non-sparse metric learning model including the method proposed in [23] and the Relevant Component Analysis (RCA) can readily be extended to their sparse ver-sions by using our proposed framework. 2.2.1 Connection with the Sparse Metric Learning [15] The Sparse Metric Learning model proposed in [15] is pre-sented as follows: s.t.  X  ( i, j, k )  X  X  , ||  X  x i  X   X  x j || 2 2  X ||  X  x i As a 1 -norm can produce sparse solutions [11, 8], many columns of A would be expected to become zero vectors due to the existence of the second term in the above objec-tive function. This is the basic motivation of SML.
However, there are several problems with the above model. Firstly, we observe that the low-dimensional map-ping is directly performed in the original input space. More specifically, enforcing the l -th column in A to become a zero vector will naturally discard the l -th dimension in the input data. However, redundant features unnecessarily ap-pear in the original input space. Instead, it might be very possible that certain transformed features are redundant. In other words, the useless features may be in the space U
T x i ,i =1 , 2 ,...,N , where U is an unknown D  X  D matrix. Secondly, the 1 -norm regularization may present a problem. We would like to enforce some columns of A to be zero vectors. However, the 1 -norm D m =1 || A m || zero but it does not necessarily force a whole column to be zero. Thirdly, in order to solve the above optimization problem, [15] further proposed to restrict the optimization of the matrix A in the space of diagonal dominance matri-ces. Although the final optimization is relaxed to a linear programming problem, such a restriction renders the final solution of A only sub-optimal instead of globally optimal even in the 1 -norm matrix regularized framework [9].
In contrast to the original SML model, a more general-ized SML can be described as follows: where || AA || (2 , 1) means the mixed (2 , 1) -norm for the matrix AA , which we will discuss shortly.

There are two major differences between our generalized model and the previous SML model. To see this, we let First, an extra matrix parameter U  X  O D is introduced. The input data samples are firstly transformed to a new space us-ing U . The feature selection or the low-dimensional map-ping is then pursued in the transformed space. More im-portantly, we do not need to specify the matrix U . Instead, the optimal U can be automatically learned from the model. Second, we use a mixed (2 , 1) -norm || B || (2 , 1) instead of the 1-norm. The mixed norm has been earlier used in multi-task learning [2] and the (2 , 1) -norm of B is obtained by first computing the 2-norm across the rows of B i , and then the 1-norm of the vector b ( B )=( || B 1 || 2 , || B 2 || 2 ,..., A 1-norm will impose a sparsity on a vector, meaning that some elements of b ( B ) will be zeroed. Let B k and ( A ) respectively denote the k -th row vector of B and k -th col-umn vector of ( A ) . Then it is easy to verify that Hence, the mixed (2 , 1) -norm of B ensures that some columns of A can become zero vectors and thus this natu-rally achieves a small d .

We propose the following theorem showing that the above original non-convex programming problem can be equivalently transformed to a convex form. Moreover, this transformed convex optimization is a special case of our proposed general framework, if L is set to the identity ma-trix.
 Theorem 1 The problem in (6) is equivalent to the opti-mization problem (3) with L equal to the identity matrix, or the following problem The proof can seen in the Appendix. From Theorem 1, we know immediately that our proposed unified framework contains the generalized SML as a special case. More-over, as the problem in (6) presents the optimal model that achieves the goal of sparse metric learning directly, we can have the following proposition.
 Proposition 1 The optimization problem (3) with L equal to the identity matrix presents an optimal model that is able to achieve the objective of sparse metric learning directly. 2.2.2 Connection with Large Margin Nearest Neigh-We show that a special problem with L set to the  X  X o-variance X  matrix  X  s among the similar pairs, i.e., L = pair set), is identical to the Large-Margin Nearest Neighbor (LMNN) approach. 4
This can be easily verified as follows: tr ( LW )=
Hence the modified problem can be changed to min Evidently, the above problem is identical to the optimization problem given in LMNN. 5
Remarks. As indicated in Section 2.2.1, it is the term tr (
W ) that conveys the sparsity directly. The regularization term given by LMNN is motivated from the large margin concept, which is however not directly related to the spar-sity of the distance matrix W . 2.2.3 Connection with D-ranking Vector Machine We propose Theorem 2 showing that the D-ranking Vector Machine proposed in [14] is a special case of our proposed framework.
 Theorem 2 When the transformation matrix W is of rank one and L is equal to the identity matrix, i.e., W = vv ,L = I ( v is a D  X  1 vector), problem (2) is equiv-alent to the D-ranking Vector Machine [14].
 Proof: If we substitute M = vv into (7), we can obtain the following optimization problem (12) can be transformed to x ij v  X  x ik v +  X  t in the sense that both forms imply the same meanings. On the other hand, the term v v ,theL 2 -norm, can be also defined as || v || H , which is an L 2 -norm in a reproducing kernel Hilbert space (RKHS). Hence, the above problem can be finally transformed to The above optimization problem is exactly the D-ranking Vector Machine [14].

From Lemma 2, we know that D-ranking VM actually restricts the rank of the transformation matrix to one. Al-though such restriction simplifies the model, it reduces the flexibility and consequently would limit the performance. 2.2.4 Connections with Pairwise Support Vector Ma-We show that the proposed general framework has a close relationship with the Pairwise Support Vector Machine. More specifically, when L is equal to the distance matrix W , the GSML is reduced to a model very similar to the Pairwise SVM [20].

With L = W , we can have: where || W || 2 Fro is the Frobenius norm of W .
Furthermore, suppose we change the triplet setting to an-other commonly-used setting, where a similar set T s (con-taining similar pairs) and a dissimilar set T d (containing dis-similar pairs) are defined. A good choice of distance met-ric should shrink the distance between each pair of similar points, while enlarging the distance between each pair of dissimilar points. In this sense, we have the following con-straints where  X  is a variable used to shift the margin among the dissimilar pairs and the similar pairs.

Consequently,the problem (2) can be written as The above model is exactly the pairwise SVM model [20]. 2.2.5 Extensions of Other Metric Learning Models We now examine how some other existing metric learning problems can be extended into a sparse learning problem. We use the model of Xing et al [23] as an example. Other models such as RCA can be similarly extended.

We first write the model of [23] as follows: where T d and T s are respectively a specified dissimilar set and a similar set.

By simply adding the regularization term, we transform the above problem to
It can be interestingly observed that the above sparse ver-sion can be regarded as a globalized sparse metric frame-work, while our proposed general framework is a local sparse metric learning method. In the above model, the constraint is constructed globally from the similar set and the dissimilar set. In comparison, our proposed GSML con-structs constraints between each similar pair and dissimilar pair, which is of the local fashion. Both models share simi-lar optimization forms. They can be efficiently solved based on the iterative method, which will be discussed shortly in Section 2.3. 2.2.6 Remarks In the above subsections, we have established various con-nections between our proposed unified framework and the other models. We now make some remarks on the men-tioned models. First, the generalized SML model (cf. prob-lem (6)) presents the optimal model which can learn an accurate distance metric as well as sparse representations. LMNN is motivated from the large margin concept. It can achieve the similar objective as the generalized SML. LMNN can also yield sparsity because of the inherited na-ture of the regularization term; however, the sparsity of  X 
W does not mean the sparsity of W . This point can also be observed later in the experimental section. The D-ranking Vector Machine can be regarded as a simplified ver-sion of the proposed framework, since the distance matrix is restricted to rank one. Finally, L could be defined as other matrices. In particular, it is possible to combine the mer-its of LMNN and Generalized SML with L set to  X  s +  X  I , where  X  is a positive trade-off parameter.
We have shown that the proposed sparse metric learn-ing problem can be formulated as an SDP problem. How-ever, SDP is still time-consuming, or even intractable for medium or large-scale problems due to the time complexity of O ( D 6 ) . In this section, in a similar fashion to Weinberger et al [22], we propose an iterative optimization technique which scales well for large-scale tasks.

The basic strategy is that we first solve a linear program-ming problem with standard sub-gradient descent methods by removing the semi-definite constraint W 0 . We then project the solution to the semi-definite matrices space. The process is iterated until a stable solution is obtained. We state this process as follows.

By defining the hinge loss [ z ] + = first transform the problem to an unconstrained optimiza-tion problem min
Let C ij = x ij x ij and let G t denote the gradient of the objective function at the t -th iteration, then G t can be cal-culated as follows: where H t is the set of triplets ( i, j, k )  X  X  with the positive slack variable. At each step, the metric matrix W can be updated by where  X  is a small positive step constant.

We then project the matrix W ( t ) to the cone of positive semi-definite matrices by finding the eigen-decomposition of the matrix W t , i.e., W ( t ) = P  X  P , where  X  is the diagonal matrix with the diagonal elements  X  i be-ing the eigen-values of W ( t ) , and P is the the eigen-vector matrix. The optimal W ( t ) satisfying W ( t ) 0 is the one, W ( t ) = P  X  + P , where  X  + = diag (max { 0 , X  1 } ,..., max { 0 , X  D } ) .

Following [22], we can further speed up the optimization by updating the gradient as follows: The second technique to speed up the optimization is to use the active set method. At each iteration, we do not check all the triplets if they violate the margin (i.e.,  X  t &gt; 0 we define the current active set H t = t  X  1 i =1 H i . When the algorithm converges, we then check if the current working set contains all the triplets satisfying  X  t &gt; 0 . Otherwise, we add the newly obtained triplets into the active set and continue the optimization until it converges.
In this section, we compare our proposed method with other competing models on six benchmark data sets,which were obtained from the UCI machine learning repository.
As we proved in Section 2.2.1, the model with L = I corresponds to the optimal model capable of simultaneously learning an accurate distance metric and a low-dimensional representation. In this section, our mention of GSML in nu-merical experiments will assume this L = I model. How-ever, we should still bear in mind that GSML is a general framework and thus other variations could have been used instead. With this choice we compare the proposed GSML method with four other competing methods including the naive Euclidean distance method (simply using Euclidean distance as the distance metric), the SML method [15], the algorithm proposed by Xing et al [23] (we call Xing from now on), and the Large Margin Nearest Neighbor method.
We follow [15] and use the category information to gen-erate the relative similarity pairs. More specifically, given a randomly chosen triplet { i, j, k } sampled from the training data, if two of them share the same label and a third has a different class, we then incorporate this triplet in the triplet set T . Namely, { x i , x j , x k } has the relative distance rela-tionship of x i is more similar to x j than to x k . For Xing X  X  method, two sets need be used for evaluating the algorithm: a similar set T s containing the similar pairs and a dissimilar set T d consisting of the dissimilar pairs. In a similar fash-ion to [15], we generate these two sets from the triplet set T by placing ( i, j ) into the similar set T s and ( i, k ) dissimilar set T d , provided ( i, j, k ) is a triplet in cussed in [15], such a strategy provides a fair level of super-vision for the comparison methods, in the sense that roughly the same information is presented to different methods. For LMNN, the nearest neighbor points for each training sam-ple need to be provided before training. In order to provide equal side information, for each training sample, we regard their similar samples appearing in the similar pairs as their nearest neighbors.

We use the same ratio, i.e., 0 . 85 / 0 . 15 as used by [15], to split the data sets into a training and test set. From the same data set, 1500 triplets are generated from the training set based on the strategy mentioned above, while 1000 triplets are sampled from the test set. During testing, if two of the similar elements in the triplet has a smaller distance than the dissimilar pair (calculated based on the learned metric), then this triplet is regarded as classified correctly. We count the ratio of correctly classified triplets to give the final ac-curacy score. This procedure was used for all five meth-ods. The final results are given as an average over 10 ran-dom splits of the data. The trade-off parameter  X  used in GSML, SML, Xing X  X  method, and LMNN is tuned in the tion, as suggested by previous research. All the experiments were pursued using Matlab V 7 . 1 on a PC with 2 GRAM, and a 3Ghz CPU.
We now report the experimental results. We will first present the test accuracy and then provide the performance in terms of dimensionality reduction.

We report the test accuracy in Table 1. In the first two rows of Figure 2, we also visualize the results. Several important points should be highlighted. First, almost all the metric learning methods can outperform the Euclidean distance method except Xing X  X  method of [23] which can sometimes be slightly worse than the Euclidean distance. This validates the data-dependant nature of distance or sim-ilarity metrics and indicates the importance of metric learn-ing. Second, GSML demonstrates overall better perfor-mance than the other remaining methods, showing that sparse metric learning appears to be more suitable and ap-propriate. On the one hand, enforcing sparsity in metric learning can suppress possible noise in the data and con-sequently avoid over-fitting for subsequent classification or clustering tasks. Additionally, it will lead to much more ef-ficient calculation of the learned distance functions. Third, our proposed GSML further lifts the performance of SML consistently across all six data sets used. The accuracy improvements are statistically significant for the data sets of Breast-Cancer, Pima Diabetes, Ionosphere, and Balance Scale according to a t-test at the level of 5% . Specifically, a roughly 5% and 7% accuracy lift has been observed respec-tively with the Ionosphere and Balance Scale data sets. Fi-nally, LMNN can also present competitive results, however not as good as our method. These results clearly demon-strate the advantages of the proposed generalized frame-work.

We now examine dimensionality reduction performance (i.e. the number of non-zero vector for the transformation matrix A ), used by different metric learning methods in the last two rows of Figure 2. 6 As observed, the optimal di-mensionality given by the proposed GSML is significantly fewer than those of the SML and LMNN across all six data sets. As the feature correlations may appear best in an un-known linearly transformed space and SML can only per-form feature selection in the original input space, the  X  X pti-mal X  dimensionality number given by SML is actually not optimal and still redundant. Moreover, although LMNN can impose partial sparsity, the associated term does not target sparsity directly. In comparison, our proposed GSML can learn the optimal low-dimensional feature representation as well as the distance metric at the same time. The dimension number given by GSML achieves the truly optimal solution within the framework of sparse metric learning and hence is much smaller than the other two methods.
Metric learning is an active research topic in ma-chine learning and data mining. Researchers have devel-oped many approaches in this field. Among them are Information-Theoretic Metric Learning [7] (ITML), Rele-vant Component analysis (RCA) [4], the method proposed by Xing et al. However, all the above methods can merely derive non-sparse metrics and only work within their spe-cial settings.

Some other related methods include non-Mahalanobis based metric learning, e.g., Neighborhood Component Analysis (NCA) [10], the method proposed in [5], Local Linear Embedding (LLE) [16], and Local Fisher Discrim-inant Analysis (LFDA) [18]. NCA and the method pro-posed in [5] are neural network based metric learning meth-ods, usually suffering from non-convexity or suboptimal performance. LLE and LFDA aim to preserve the local-ity or within-class covariances using low-dimensional map-pings. They do not try to improve the classification per-formance and hence are very different from our proposed method. Embedding approaches or manifold learning are also related to our framework in that both approaches seek low-dimensional representation. These approaches include Maximal Variance Unfolding (MVU) or its variant, Colored MVU [17], Multidimensional Scaling (MDS) [6], and its generalized case, Generalized MDS [1]. Principal Com-ponent Analysis (PCA) [12] and various other extensions can also be cast in this category. However, all these ap-proaches are differently motivated, leading to very different or even contrary objectives with our metric learning. The main purpose of embedding approaches is to find a low-dimensional embedding which maintains the distance or-dering. As shown by many authors, embedding approaches do not necessarily benefit classification performance [17]. In contrast, we are seeking a metric which simultaneously yields high classification or clustering accuracy as well as a low-dimensional representation [1] [17].
 Within sparse metric learning, SML [15], LMNN [21], D-ranking Vector Machine [14], Large Margin Component Analysis (LMCA) [19], and the method developed in [3] also target high classification accuracy and dimensionality reduction simultaneously. However, SML is sub-optimal and can merely search low-dimensional representations in the input space. Our proposed Generalized SML solves these two problems systematically and is shown to con-tain SML, LMNN, and D-ranking Vector Machine as spe-cial cases. On the other hand, LMCA controls the sparsity by directly specifying the dimensionality of the transforma-tion matrix and it is an extended version of LMNN [21, 22]. Its problem is that, one more parameter, i.e., the dimension d , needs tuning. In [3], a distance measure can be learned by combining multiple 1-D embeddings based on the Ad-aBoost [3]. However, it exploits some heuristics and cannot guarantee global optimum.
We propose a Generalized Sparse Metric Learning framework in this paper. This novel framework offers a uni-fied view for understanding many of the popular sparse met-ric learning algorithms including the Sparse Metric Learn-ing framework proposed in [15], the Large Margin Near-est Neighbor (LMNN) [21][22], and the D-ranking Vec-tor Machine (D-ranking VM) [14]. Furthermore, the pro-posed framework is capable of extending many current non-sparse metric learning models such as Relevant Vector Ma-chine [4] and a state-of-the-art method proposed in [23] to their sparse versions. We provide a conceptual interpreta-tion why the proposed framework can generate both a good distance metric and low-dimension representations simul-taneously. We also apply an iterative sub-gradient opti-mization method, making the original SDP problem solv-able even for medium-or large-scale data. Experimental results show that the proposed unified approach can outper-form previous methods in terms of both learning accuracy and dimension reduction on six real-world datasets. [1] S. Agarwal, J. Wills, L. Gayton, G. Lanckriet, [2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task [3] V. Athitsos, J. Alton, S. Sclaroff, and G. Kollios. [4] A. Bar-Hillel, T. Hertz, N. Shental, and D. Wein-[5] S. Chopra, R. Hadsell, and Y. LeCun. Learning a sim-[6] T. Cox and M. Cox. Multidimensional Scaling . Chap-[7] J. Davis, B. Kulis, P. Jain, S. Sra, and I. Dhillon. [8] G. Fung, O. L. Mangasarian, and A. J. Smola. Min-[9] G. Fung, R. Rosales, and R. B. Rao. Feature selection [10] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhut-[11] T. Hastie, R. Tibshirani, and R. Friedman. The Ele-[12] I. T. Jolliffe. Principal Component Analysis . Springer-[13] C. A. Micchelli and M. Pontil. Learning the ker-[14] H. Ouyang and A. Gray. Learning dissimilarities by [15] R. Rosales and G. Fung. Learning sparse metrics via [16] S. T. Roweis and L. K. Saul. Nonlinear dimension-[17] L. Song, A. Smola, K. Borgwardt, and A. Gretton. [18] M. Sugiyama. Dimensionality reduction of multi-[19] L. Torresani and K. Lee. Large margin component [20] Jean-Philippe Vert, Jian Qiu, and William S. Nobel. A [21] K. Weinberger, J. Blitzer, and L. Saul. Distance metric [22] K. Weinberger and L. Saul. Fast solvers and efficient [23] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance To prove Theorem 1, we first present Lemma 1.
 Lemma 1 , Consider the following optimization problem. The above sparse metric learning problem (6) is equivalent to the formulation in (16). In particular, if (  X  A,  X  U ) optimal solution of (6), then is an optimal solution of Problem (16). Moreover, the above problem (16) is a jointly convex problem with respect to W,  X  and V .
 Proof: Let W = UBU , then problem (6) is reduced to the following We know from [13] that min  X   X  i a where the minimization is achieved at  X   X  i = | a i | || a Let  X   X  1 =(  X   X  1 1 ,..., X   X  1 n ) . Then, replacing a i ( U WU ) i implies that Putting this back into equation (20) and recalling that  X   X  and U  X  O D are arbitrary, then letting V  X  1 = U diag (  X   X  1 ) U  X  S D ++ , implies the equivalent formulation (16).

To establish the convexity of the formulation in (16), we note that it follows directly from the observation that tr (
W V  X  1 W ) is jointly convex with respect to W and V and the constraint conditions are linear. This completes the proof.
 Proof of Theorem 1: If we recall that the set of all real-valued symmetric D  X  D matrices, denoted by S D ,isa Hilbert space with trace norm as the inner product defined by, for any A, B  X  S D , A, B tr = tr ( AB ) . Conse-quently, Cauchy-Schwarz X  X  inequality holds true, i.e. Applying the above inequality with A =( WW ) 1 2 V  X  1 2 Since tr ( V )  X  1 , we have that
If we further notice that W = UAA U is a symmetric and positive semi-definite matrix, we have tr (( WW ) V  X  1 )  X | tr (( WW ) 1 2 ) | 2 = tr ( W ) 2 . (21) Moreover, the minimum is achieved when V = (16), we can obtain the problem (7) immediately. Using
