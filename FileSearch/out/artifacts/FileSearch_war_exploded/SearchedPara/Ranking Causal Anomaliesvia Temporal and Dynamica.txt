 Modern world has witnessed a dramatic increase in our abil-ity to collect, transmit and distribute real-time monitoring and surveillance data from large-scale information system-s and cyber-physical systems. Detecting system anomalies thus attracts significant amount of interest in many field-s such as security, fault management, and industrial opti-mization. Recently, invariant network has shown to be a powerful way in characterizing complex system behaviours. In the invariant network, a node represents a system compo-nent and an edge indicates a stable, significant interaction between two components. Structures and evolutions of the invariance network, in particular the vanishing correlation-s, can shed important light on locating causal anomalies and performing diagnosis. However, existing approaches to detect causal anomalies with the invariant network often use the percentage of vanishing correlations to rank possi-ble casual components, which have several limitations: 1) fault propagation in the network is ignored; 2) the root ca-sual anomalies may not always be the nodes with a high-percentage of vanishing correlations; 3) temporal patterns of vanishing correlations are not exploited for robust detec-tion. To address these limitations, in this paper we propose a network diffusion based framework to identify significan-t causal anomalies and rank them. Our approach can ef-fectively model fault propagation over the entire invariant network, and can perform joint inference on both the struc-tural, and the time-evolving broken invariance patterns. As a result, it can locate high-confidence anomalies that are tru-ly responsible for the vanishing correlations, and can com-pensate for unstructured measurement noise in the system. Extensive experiments on synthetic datasets, bank informa-tion system datasets, and coal plant cyber-physical system datasets demonstrate the effectiveness of our approach. causal anomalies ranking, label propagation, nonnegative matrix factorization
With the rapid advances in networking, computers, and hardware, we are facing an explosive growth of complexity in networked applications and information services. These large-scale, often distributed, information systems usually consist of a great variety of components that work together in a highly complex and coordinated manner. One example is the Cyber-Physical System (CPS) which is typically e-quipped with a large number of networked sensors that keep recording the running status of the local components; anoth-er example is the large scale Information Systems such as the cloud computing facilities in Google, Yahoo! and Amazon, whose composition includes thousands of components that vary from operating systems, application softwares, servers, to storage, networking devices, etc.

A central task in running these large scale distributed sys-tems is to automatically monitor the system status, detect anomalies, and diagnose system fault, so as to guarantee sta-ble and high-quality services or outputs. Significant research efforts have been devoted to this topic in the literatures. For instance, Gertler et al. [9] proposed to detect anomalies by examining monitoring data of individual component with a thresholding scheme. However, it can be quite difficult to learn a universal and reliable threshold in practice, due to the dynamic and complex nature of information system-s. More effective and recent approaches typically start with building system profiles, and then detect anomalies via ana-lyzing patterns in these profiles [5, 13]. The system profile is usually extracted from historical time series data collected by monitoring different system components, such as the flow intensity of software log files, the system audit events and the network traffic statistics, and sometimes sensory mea-surements in physical systems.

The invariant model is a successful example [13, 14] for large-scale system management. It focuses on discovering stable, significant dependencies between pairs of system com-ponents that are monitored through time series recordings, so as to profile the system status and perform subsequent reasoning. A strong dependency between a pair of compo-nents is called invariant (correlation) relationship. By com-bining the invariants learned from all monitoring compo-nents, a global system dependency profile can be obtained. The significant practical value of such an invariant profile is that it provides important clues on abnormal system behav-iors and in particular the source of anomalies, by checking whether existing invariants are broken. Figure 1 illustrates one example of the invariant network and two snapshots of broken invariants at time t 1 and t 2 , respectively. Each Fig ure 1: Invariant network and vanishing correla-tions(red edges). node in the figure represents the observation from a moni-toring component. The green line signifies an invariant link between two components, and a red line denotes broken in-variant (i.e., vanishing correlation). The network including all the broken invariants at given time point is referred to as the broken network .

Although the broken invariants provide valuable informa-tion of the system status, how to locate true, causal anoma-lies can still be a challenging task due to the following rea-sons. First, system faults are seldom isolated. Instead, start-ing from the root location/component, anomalous behavior will propagate to neighboring components [13], and different types of system faults can trigger diverse propagation pat-terns. Second, monitoring data often contains a lot of noises due to the fluctuation of complex operation environments.
Recently, several ranking algorithms were developed to di-agnose the system failure based on the percentage of broken invariant edges associated with the nodes, such as the egonet based method proposed by Ge et al. [8], and the loopy be-lief propagation (LBP) based method proposed by Tao et al. [22]. Despite the success in practical applications, ex-isting methods still have certain limitations. First, they do not take into account the global structure of the invariant network, neither how the root anomaly/fault propagates in such a network. Second, the ranking strategies rely heavily on the percentage of broken edges connected to a node. For example, the mRank algorithm [8] calculated the anomaly score of a given node using the ratio of broken edges within the egonet 1 of the node. The LBP-based method [22] used the ratio of broken edges as the prior probability of abnormal state for each node. We argue that, the percentage of broken edges may not serve as a good evidence of the causal anoma-ly. This is because, although one broken edge can indicate that one (or both) of related nodes is abnormal, lack of a broken edge does not necessary indicate that related nodes are problem free. Instead, it is possible that the correlation is still there when two nodes become abnormal simultane-ously [13]. Therefore the percentage of broken edges could give false evidences. For example, in Figure 1, the causal anomaly is node i  X  . The percentage of broken edges for n-ode i  X  is 2 / 3, which is smaller than that of node h  X  (which is equal to 1). Since there exists a clear evidence of fault propagation on node i  X  , an ideal algorithm should rank i higher than h  X  . Third, existing methods usually consider
An egon et is the induced 1-step subgraph for each node. static broken network instead of multiple broken networks at successive time points together. While we believe that, jointly analyzing temporal broken networks can help resolve ambiguity and achieve a denoising effect. This is because, the root casual anomalies usually remain unchanged within a short time period, even though the fault may keep proro-gating in the invariant network. As an example shown in Figure 1, it would be easier to detect the causal anomaly if we jointly consider the broken networks at two successive time points together.

To address the limitations of existing methods, we propose several network diffusion based algorithms for ranking causal anomalies. Our contributions are summarized as follows. 1. We employ the network diffusion process to model 2. We further develop efficient algorithms which reduce 3. We employ effective normalization strategy on the rank-4. We develop a smoothing algorithm that enables users 5. We evaluate the proposed methods on both synthet-
In this section, we first introduce the technique of the invariant model [13] and then define our problem.
The invariant model is used to uncover significant pair-wise relations among massive set of time series. It is based on the AutoRegressive eXogenous (ARX) model [10] with time delay. Let x ( t ) and y ( t ) be a pair of time series under consideration, where t is the time index, and let n and m be the degrees of the ARX model, with a delay factor k . Let b y ( t ;  X  ) be the prediction of y ( t ) using the ARX model parametarized by  X  , which can then be written as where  X  = [ a 1 , . . . , a n , b 0 , . . . , b m , d ]  X   X  R [ y ( t  X  1) , . . . , y ( t  X  n ) , x ( t  X  k ) , . . . , x ( t For a given setting of ( n, m, k ), the parameter  X  can be esti-mated with observed time points t = 1 , . . . , N in the training data, via least-square fitting. In real-world applications such as anomaly detection in physical systems, 0  X  n, m, k  X  2 is a popular choice [6, 13]. We can define the  X  X oodness of fit X  (or tness score ) of an ARX model as where  X  y is the mean of the time series y ( t ). A higher value of F (  X  ) indicates a better fitting of the model. An invariant (correlation) is declared on a pair of time series x and y if the fitness score of the ARX model is larger than a pre-defined threshold. A network including all the invariant links is referred to as the invariant network . Construction of the invariant network is referred to as the model training. The model  X  will then be applied on the time series x and y in the testing phase to track vanishing correlations.
To track vanishing correlations, we can use the techniques developed in [6, 15]. At each time point, we compute the (normalized) residual R ( t ) between the measurement y ( t ) and its estimate b y ( t ;  X  ) by where  X  max is the maximum training error  X  max = max 1  X  | then we declare the invariant as  X  X roken X , i.e., the corre-lation between the two time series vanishes. The network including all broken edges at given time point and all nodes in the invariant network is referred to as the broken network .
Let G l be the invariant network with n nodes. Let G b be the broken network for G l . We use two symmetric matrices A  X  R n  X  n , P  X  R n  X  n to denote the adjacency matrix of network G l and G b , respectively. These two matrices can be obtained as discussed in Section 2.1. The two matrices can be binary or continuous. For binary case of A , 1 is used to denote that the correlation exists between two time series, and 0 denotes the lack of correlation; while for P , 1 is used to denote that the correlation is broken (vanishing), and 0 otherwise. For the continuous case, the fitness score F (  X  ) (3) and the residual R ( t ) (4) can be used to fill the two matrices, respectively.

Our main goal is to detect the abnormal nodes in G l that are most responsible for causing the broken edges in G b . In this sense, we call such nodes  X  X ausal anomalies X . Accurate detection of causal anomalous nodes will be extremely useful for examination, debugging and repair of system failures. In this section, we present the algorithm of Ranking Causal Anomalies (RCA), which takes into account both the fault propagation and fitting of broken invariants simultaneously.
We consider a very practical scenario of fault propagation, namely anomalous system status can always be traced back to a set of root cause anomaly nodes, or causal anomalies , as initial seeds. As the time passes, these root cause anoma-lies will then propagate along the invariant network, most probably towards their neighbors via paths identified by the invariant links in G l . To explicitly model this spreading pro-cess on the network, we have employed the label propaga-tion technique [16, 24, 26]. Suppose that the (unknown) root cause anomalies are denoted by the indicator vector e , whose entries e i  X  X  (1  X  i  X  n ) indicate whether the i th node is the casual anomaly ( e i = 1) or not ( e i = 0). At the end of propagation, the system status is represented by the anoma-ly score vector r , whose entries tell us how severe each node of the network has been impaired. The propagation from e to r can be modeled by the following optimization problem min where D  X  R n  X  n is the degree matrix of A , c  X  (0 , 1) is the regularization parameter, r is the anomaly score vector after the propagation of the initial faults in e . We can re-write the above problem as where I n is the identity matrix,  X  A = D  X  1 = 2 AD  X  1 = 2 degree-normalized version of A . Similarly we will use  X  the degree-normalized P in the sequel. The first term in Eq. (5) is the smoothness constraint [26], meaning that a good ranking function should assign similar values to nearby nodes in the network. The second term is the tting con-straint , which means that the final status should be close to the initial configuration. The trade-off between these two competing constraints is controlled by a positive parameter c : a small c encourages a sufficient propagation, and a big c actually suppresses the propagation. The optimal solution of problem (5) is [26] which establishes an explicit and closed-form solution be-tween the initial configuration e and the final status r through fault propagation.

To encode the information of the broken network, we pro-pose to use r to reconstruct the broken network G b . The intuition is illustrated in Figure 2. If there exists a broken Figure 2: Reconstruction of the broken invariant network using anomaly score vector r. link in G b , e.g.,  X  P ij is large, then ideally at least one of the nodes i and j should be abnormal, or equivalently, either r or r j should be large. Thus, we can use the product of r and r j to reconstruct the value of  X  P ij . In Section 5, we X  X l further discuss how to normalize them to avoid extreme val-ues. Then, the loss of reconstructing the broken link  X  P the whole broken network is then || ( rr  X  )  X  M  X   X  P || 2  X  is element-wise operator, and M is the logical matrix of the invariant network G l (1 with edge, 0 without edge). Let B = (1  X  c )( I n  X  c  X  A )  X  1 , by substituting r we obtain the following objective function. Considering that the integer programming in problem (7) is NP-hard, we relax it by using the  X  1 penalty on e with parameter  X  to control the number of non-zero entries in e [23]. Then we reach the following objective function.
In this section, we present an iterative multiplicative up-dating algorithm to optimize the objective function in (8). The objective function is invariant under these updates if and only if e are at a stationary point [17]. The solution is presented in the following theorem, which is derived from the Karush-Kuhn-Tucker (KKT) complementarity condition [3]. Detailed theoretical analysis of the optimization procedure will be presented in the next section.

Theorem 1. Updating e according to Eq. (9) will mono-tonically decrease the objective function in Eq. (8) until con-vergence. wher e  X  , [  X  ] [  X  ] and (  X  ) 1 4 are eleme nt-wise operators.
Based on Theorem 1, we develop the iterative multiplica-tive updating algorithm for optimization and summarize it in Algorithm 1. We refer to this ranking algorithm as RCA. Algo rithm 1: Ranking Causal Anomalies (RCA)
We derive the solution to problem (9) following the con-strained optimization theory [3]. Since the objective func-tion is not jointly convex, we adopt an effective multiplica-tive updating algorithm to find a local optimal solution. We prove Theorem 1 in the following.

We formulate the Lagrange function for optimization L = || ( Bee  X  B  X  )  X  M  X   X  P || 2 F +  X  1  X  n e . Obviously, B , M and are symmetric matrix. Let F = ( Bee  X  B  X  )  X  M , then  X   X  e m It follows that and thereby Thus, the partial derivative of Lagrange function with re-spect to e is: where 1 n is the n  X  1 vector of all ones. Using the Karush-Kuhn-Tucker (KKT) complementarity condition [3] for the non-negative constraint on e , we have The above formula leads to the updating rule for e that is shown in Eq. (9).
We use the auxiliary function approach [17] to prove the convergence of Eq. (9) in Theorem 1. We first introduce the definition of auxiliary function as follows.
Definitio n 3.1. Z ( h,  X  h ) is an auxiliary function for L ( h ) if the conditions are satis ed for any given h,  X  h [17].

Lemma 3.1. If Z is an auxiliary function for L , then L is non-increasing under the update [17].
Theorem 2. Let L ( e ) denote the sum of all terms in L containing e . The following function
Z ( e ,  X  e ) =  X  2 i s an auxiliary function for L ( e ) . Furthermore, it is a convex function in e and has a global minimum.
 Theorem 2 can be proven in a similar way as in [7] by vali- X  X  X  e Z ( e ; ^ e )  X  0 . Due to space limitation, the detail of the proof is omitted.

Based on Theorem 2, we can minimize Z ( e ; ^ e ) with respect updating formula wh ich is consistent with the updating formula derived from the KKT condition aforementioned.

From Lemma 3.1 and Theorem 2, for each subsequent iter-ly decreases. Since the objective function Eq. (8) is lower bounded by 0, the correctness of Theorem 1 is proven.
In Algorithm 1, we need to calculate the inverse of an n  X  matrix, which takes O ( n 3 ) time. In each iteration, the mul-tiplication between two n  X  n matrices is inevitable, thus the overall time complexity of Algorithm 1 is O ( Iter  X  n 3 ), where Iter is the number of iterations needed for convergence. In the following section, we will propose an efficient algorithm that reduces the time complexity to O ( Iter  X  n 2 ).
In this section, we will propose an efficient algorithm that avoids the matrix inverse calculations as well as the multi-plication between two n  X  n matrices. The time complexity can be reduced to O ( Iter  X  n 2 ).

We achieve the computational speed up by relaxing the objective function in (8) to jointly optimize r and e . The objective function is shown in the following.
To optimize this objective function, we can use an alter-nating scheme. That is, we optimize the objective with re-spect to r while fixing e , and vise versa. This procedure continues until convergence. The objective function is in-variant under these updates if and only if r , e are at a sta-tionary point [17]. Specifically, the solution to the optimiza-tion problem in Eq. (19) is based on the following theorem, which is derived from the Karush-Kuhn-Tucker (KKT) com-plementarity condition [3]. The derivation of it and the proof of Theorem 3 is similar to that of Theorem 1.
 Theorem 3. Alternatively updating e and r according to Eq. (20) and Eq. (21) will monotonically decrease the ob-jective function in Eq. (19) until convergence.
B ased on Theorem 3, we can develop the iterative mul-tiplicative updating algorithm for optimization similar to Algorithm 1. Due to page limit we skip the details. We re-fer to this ranking algorithm as R-RCA. From Eq. (20) and Eq. (21), we observe that the calculation of the inverse of the n  X  n matrix and the multiplication between two n  X  n matrices in Algorithm 1 are not necessary. As we will see in Section 7.4, the relaxed versions of our algorithm can greatly improve the computational efficiency.
In Section 3, we use the product r i  X  r j as the strength of evidence that the correlation between node i and j is vanishing (broken). However, it suffers from the extreme values in the ranking values r . To reduce the influence of the extreme values or outliers, we employ the softmax nor-malization on the ranking values r . The ranking values are nonlinearly transformed using the sigmoidal function before the multiplication is performed. Thus, the reconstruction error is expressed by || (  X  ( r )  X   X  ( r ))  X  M  X   X  P || 2 the softmax function with: The corresponding objective function in Algorithm 1 is mod-ified to the following
Similarly, the objective function for Eq. (19) is modified to the following
The optimization of these two objective functions are based on the following two theorems.
Theorem 4. Up dating e according to Eq. (25) will mono-tonically decrease the objective function in Eq. (23) until convergence. e  X  e  X  wh ere  X  =
Theorem 5. Updating r according to Eq. (26) will mono-tonically decrease the objective function in Eq. (24) until convergence. wh ere  X  =  X  ( r )  X   X  ( r ) and  X  =  X   X  ( r )  X  ( r ) .
Theorem 4 and Theorem 5 can be proven with a similar strategy to that of Theorem 1. We refer to the ranking algorithms with softmax normalization (Eq. (23) and Eq. (24)) as RCA-SOFT and R-RCA-SOFT respectively.
As discussed in Section 1, although the number of anoma-ly nodes could increase due to fault propagation in the net-work, the root cause anomalies will be stable within a short time period T [14]. Based on this intuition, we further devel-op a smoothing strategy by jointly considering the temporal broken networks. Specifically, we add a smoothing term || e e ( t ) are causal anomaly ranking vectors for two successive time points. For example, the objective function of algorith-m RCA with temporal broken networks smoothing is shown in Eq. (27). Here,  X  P ( t ) is the degree-normalized adjacency matrix of bro-ken network at time point t . Similar to the discussion in Section 3.3, we can derive the updating formula of Eq. (27) in the following. The updating formula for R-RCA, RCA-SOFT, and R-RCA-SOFT with temporal broken networks smoothing is similar. Due to space limit, we skip the details. We refer to the algorithms with temporal smoothing as T-RCA, T-R-RCA, T-RCA-SOFT and T-R-RCA-SOFT respectively.
In this section, we perform extensive experiments to e-valuate the performance of the proposed methods (summa-rized in Table 1). We use both simulated data and real-world monitoring datasets for validation. For comparison, we select several state-of-the-art methods, including mRank and gRank in [8, 13], and LBP [22]. For all the method-s, the tuning parameters were tuned using cross validation. We use several evaluation metrics including precision, recal-l, and nDCG [12] to measure the performance. The preci-sion and recall are computed on the top-K ranking result, where K is typically chosen as twice the actual number of ground-truth causal anomalies [12, 22]. The nDCG of the top-p ranking result is defined as nDCG p = DCG p IDC G DCG p = on the ground-truth, and p is smaller than or equal to the actual number of ground-truth anomalies. The rel i repre-sents the anomaly score of the i th item in the ranking list of the ground-truth.
We first evaluate the performance of the proposed meth-ods using simulations. We have followed [8, 22] in generating the simulation data.
We first generate 5000 synthetic time series data to sim-ulate the monitoring records 2 . Each time series contains 1,050 time points. Based on the invariant model introduced in Section 2.1, we build the invariant network by using the first 1,000 time points in the time series. This generates an invariant network containing 1,551 nodes and 157,371 edges. To generate invariant network of different sizes, we random-ly sample 200, 500, and 1000 nodes from the whole invariant network and evaluate the algorithms on these sub-networks.
To generate the root cause anomaly, we randomly select 10 nodes from the network, and assign each of them an anomaly score between 1 and 10. The ranking of these scores is used as the ground-truth. To simulate the anomaly prorogation, we further use these scores as the vector e in Eq. (6) and calculate r ( c = 0 . 9). The values of the top-30 time series with largest values in r are then modified by changing their amplitude value with the ratio 1+ r i . That is, if the observed values of one time series is y 1 , after changing it from y y , the manually-injected degree of anomaly | y 2  X  y 1 | | to 1 + r i . We denote this anomaly generation scheme as amplitude-based anomaly generation.
Using the simulated data, we compare the performance of different algorithms. In this example, we only consider the training time series as one snapshot; multiple snapshot cases involving temporal smoothing will be examined in the real datasets. Due to the page limit, we report the preci-sion, recall and nDCG for only the top-10 items considering that the ground-truth contains 10 anomalies. Similar results can be observed with other settings of K and p . For each algorithm, reported result is averaged over 100 randomly selected subsets of the training data.

From Figure 3, we have several key observations. First, the proposed algorithms significantly outperform other com-peting methods, which demonstrates the advantage of taking into account fault prorogation in ranking casual anomalies. We also notice that performance of all ranking algorithms will decline on larger invariant networks with more nodes, in-dicating that anomaly ranking becomes more challenging on http ://cs.unc.edu/  X  weicheng/synthetics5000.csv Figure 3: Comparison on synthetic data( K, p = 10 ). Figure 4: Performance with different noise ratio( K, p = 10 ). networks with more complex behaviour. However, the rank-ing result with softmax is less sensitive to the size of the in-variant network, suggesting that the softmax normalization can effectively improve the robustness of the algorithm. This is quite beneficial in real-life applications, especially when data are noisy. Finally, we observe that RCA and RCA-SOFT outperform R-RCA and R-RCA-SOFT, respectively. This implies that the relaxed versions of the algorithms are less accurate. Nevertheless, their accuracies are still very comparable to those of the RCA and RCA-SOFT method-s. In addition, the efficiency of the relaxed algorithms is greatly improved, as discussed in Section 4 and Section 7.4.
Practical invariant network and broken edges can be quite noisy. In this section, we further examine the performance of the proposed algorithms w.r.t. different noise levels. To do this, we randomly perturb a portion of non-broken edges in the invariant network. Results are shown in Figure 4. We observe that, even when the noise ratio approaches 50%, the precision, recall and nDCG of the proposed approaches still attain 0.5. This indicates the robustness of the proposed algorithms. We also observe that, when the noise ratio is very large, RCA-SOFT and R-RCA-SOFT work better than RCA and R-RCA, respectively. This is similar to those ob-servations made in Section 7.1.2. As has been discussed in Section 5, the softmax normalization can greatly suppress the impact of extreme values and outliers in r , thus improves the robustness.
In this section, we apply the proposed methods to detect causal abnormal components on a Bank Information System (BIS) data set [8, 22]. The monitoring data are collected from a real-world bank information system logs, which con-tain 11 categories. Each category has a varying number of
T abl e 2: Examples of categories and monitors. Fi gure 5: Two example monitoring data of BIS.
 time series, and Table 2 gives five categories as examples. The data set contains the flow intensities collected every 6 seconds. In total, we have 1,273 flow intensity time se-ries. The training data is collected at normal system states, where each time series has 168 time points. The invariant network is then generated on the training data as described in Section 2.1. The testing data of the 1,273 flow intensi-ty time series are collected during abnormal system states, where each time series contain 169 time points. We track the changes of the invariant network with the testing data using the method described in Section 2.1. Once we obtain the broken networks at different time points, we perform causal anomaly ranking in these temporal slots jointly. Properties of the networks constructed are summarized in Table 3.
Based on the knowledge from system experts, the root cause anomaly at t = 120 in the testing data is related to  X  X B16 X . An illustration of two  X  X B16 X  related monitoring data are shown in Figure 5. We highlight t : 120 with red square. Obviously, their behaviour looks anomalous from that time point on. Due to the complex dependency among different monitoring time series, it is impractical to obtain a full ranking of abnormal measurement. Fortunately, we have a unique semantic label associated with each measurement. For example, some semantic labels read  X  X B16:DISK hdx Request X  and  X  X EB26 PAGEOUT RATE X . Thus, we can extract all measurements whose titles have the prefix  X  X -B16 X  as the ground-truth anomalies. The ranking score is T able 5: Number of  X  X B16 X  related monitors in top 32 results on BIS data( t :120). determined by the number of broken edges associated with each measurement. Here our goal is to demonstrate how the top-ranked measurements selected by our method are related to the  X  X B16 X  root cause. Altogether, there are 80 measurements related to  X  X B16 X  in the invariant network, so we report the precision, recall with K ranging from 1 to 160 and the nDCG with p ranging from 1 to 80, respectively.
The results are shown in Figure 6. The relative perfor-mance of different approaches is consistent with the obser-vations in the simulation study. Again, the proposed algo-rithms outperform baseline methods by a large margin. To examine the top-ranked items more clearly, we list the top-12 results of different approaches in Table 4 and report the number of  X  X B16 X -related monitors in Table 5. From Table 4, we observe that the three baseline methods only report one  X  X B16 X  related measurement in the top-12 results, and the actual rank of the  X  X B16 X -related measurement appear lower (worse) than that of the proposed methods. We also notice that the ranking algorithms with softmax normaliza-tion outperform others. From Tables 4 and 5, we can see that top ranked items reported by RCA-SOFT and R-RCA-SOFT are more relevant than those reported by RCA and R-RCA, respectively. This clearly illustrates the effective-ness of the softmax normalization in reducing the influence of extreme values or outliers in the data.

As discussed in Section 1, the root anomalies could fur-ther propagate from one component to related ones over time, which may or may not necessarily relate to  X  X B16 X . Such anomaly propagation makes anomaly detection even harder. To study how the performance varies at differen-t time points, we compare the performance at t = 120 and t = 122, respectively in Figure 7 ( p , K =80). Clearly, the per-formance declines for all methods. However, the proposed methods are less sensitive to anomaly propagation than oth-ers, suggesting that our approaches can better handle the fault propagation problem. We believe this is attributed to the network diffusion model that explicitly captures the fault propagation processes. We also list the top-12 abnormal at t = 122 in Table 6. Due to page limit, we only show the re-sults of mRank, gRank, RCA-SOFT and R-RCA-SOFT. By comparing the results in Tables 4 and 6, we can observe that RCA-SOFT and R-RCA-SOFT significantly outperform m-Rank and gRank, the latter two methods which are based on the percentage of broken edges are more sensitive to the anomaly prorogation. T able 6: Top 12 anomalies on BIS data( t :122).
 T abl e 7: Top 12 anomalies reported by methods with temporal smoothing on BIS data( t :120-121).
 T abl e 8: Comparison on the number of  X  X B16 X  re-lated anomalies in top-12 results on BIS data.
We further validate the effectiveness of proposed method-s with temporal smoothing. We report the top-12 results of different methods with smoothing at two successive time points t = 120 and t = 121 in Table 7. The number of  X  X B16 X -related monitors in the top-12 results is summarized in Table 8. From Tables 7 and 8, we observe a significan-t performance improvement of our methods with temporal broken networks smoothing compared with those without smoothing. As discussed in Section 6, since causal anoma-lies of a system usually do not change within a short period of time, utilizing such smoothness can effectively suppress noise and thus give better ranking accuracy.
In this section, we test the proposed methods in the ap-plication of fault diagnosis on a coal plant cyber-physical Figure 8: Egonet of node  X  X 0146 X  and  X  X 0256 X  in invariant network and vanishing correlations(red edges) on coal plant data. system data. The data set contains time series collected through 1625 electric sensors installed on different compo-nents of the coal plant system. Using the invariant model described in Section 2.1, we generate the invariant network that contains 9451 invariant links. For privacy reasons, we remove sensitive descriptions of the data.

Based on knowledge from domain experts, in the abnormal stage, the root cause is associated with component  X  X 0146 X . We report the top-12 results of different ranking algorithms in Table 9. We observe that our algorithms all rank compo-nent  X  X 0146 X  the highest, while the baseline methods could give higher ranks to other components. In Figure 8(a), we visualize the egonet of the node X  X 0146 X  X n the invariant net-work, which is defined as the 1-step neighborhood around n-ode  X  X 0146 X , including the node itself, direct neighbors, and all connections among these nodes in the invariant network. Here, green lines denote the invariant link, and red lines de-note vanishing correlations (broken links). Since the node  X  X 0256 X  is top-ranked by the baseline methods, we also vi-sualize its egonet in Figure 8(b) for a comparison. There are 80 links related to  X  X 0146 X  in the invariant network, and 14 of them are broken. Namely the percentage of broken edges is only 17.5% for a truly anomalous component. In con-trast, the percentage of broken edges for the node  X  X 0256 X  is 100%, namely a false-positive node can have a very high percentage of broken edges in practice. This explains why baseline approaches using the percentage of broken edges could fail, because the percentage of broken edges does not serve as a reliable evidence of the degree of causal anomalies. In comparison, our approach takes into account the global structures of the invariant network via network propagation, thus the resultant ranking is more meaningful.
In this section, we study the efficiency of proposed meth-ods using the following metrics: 1) the number of iterations Figure 9: Number of iterations to converge and time cost comparison.
 for convergence; 2) the running time (in seconds) ; and 3) the scalability of the proposed algorithms. Figure 9(a) shows the value of the objective function with respect to the num-ber of iterations on different data sets. We can observe that, the objective value decreases steadily with the number of it-erations. Typically less than 100 iterations are needed for convergence. We also observe that our method with soft-max normalization takes fewer iterations to converge. This is because the normalization is able to reduce the influence of extreme values [21]. We also report the running time of each algorithm on the two real data sets in Figure 10. We can see that the proposed methods can detect causal anomalies very efficiently, even with the temporal smoothing module.
To evaluate the computational scalability, we randomly generate invariant networks with different number of nodes (with network density=10) and examine the computational cost. Here 10% edges are randomly selected as broken links. Using simulated data, we compare the running time of RCA, R-RCA, RCA-SOFT, and R-RCA-SOFT. Figure 9(b) plots the running time of different algorithms w.r.t. the number of nodes in the invariant network. We can see that the relaxed versions of our algorithm are computationally more efficient than the original RCA and RCA-SOFT. These results are consistent with the complexity analysis in Section 4.
In this section, we review the related work on anomaly detection and system diagnosis. In particular, we focus on the following two categories: 1) fault detection in distributed systems; and 2) graph-based methods.

For the first category, Yemini et al. [25] proposed to mod-el event correlation and locate system faults using known dependency relationships between faults and symptoms. In real applications, however, it is usually hard to obtain such relationships precisely. To alleviate this limitation, Jiang et al. [13] developed several model-based approaches to detect the faults in complex distributed systems. They further pro-posed several Jaccard Coefficient based approaches to locate the faulty components [14, 15]. These approaches generally focus on locating the faulty components, they are not capa-ble of spotting or ranking the causal anomalies.

Recently, graph-based methods have drawn a lot of in-terest in system anomaly detections [2, 5], either in static graphs or dynamic graphs [2]. In static graphs, the main task is to spot anomalous network entities given the graph structure [4, 11]. For example, Akoglu et al. [1] proposed OddBall to detect anomalous nodes in weighted graphs. Li-u et al. [18] proposed to use frequent subgraph mining to detect non-crashing bugs in software flow graphs. Howev-er, these approaches only focus on a single graph; in com-parison, we take into account both the invariant graph and the broken correlations, which provides a more dynamic and complete picture for anomaly ranking. In dynamic graphs, anomaly detection aims at detecting abnormal events [19]. Most approaches along this direction are designed to detect anomaly time-stamps in which suspicious events take place, but not to perform ranking on a large number of system components. Sun et al. proposed to use temporal graph-s for anomaly detection [20]. In their approach, a set of initial suspects need to be provided; then internal relation-ship among these initial suspects is characterized for better understanding of the root cause of these anomalies.
In using the invariant graph and the broken invariance graph for anomaly detection, Jiang et al. [14] used the ra-tio of broken edges in the invariant network as the anomaly score for ranking; Ge et al. [8] proposed mRank and gRank to rank causal anomalies; Tao et al. [22] used the loopy belief propagation method to rank anomalies. As has been discussed, these algorithms rely heavily on the percentage of broken edges in egonet of a node. Such local approaches do not take into account the global network structures, nei-ther the global fault propagation spreading on the network. Therefore the resultant rankings can be sub-optimal.
Detecting causal anomalies on monitoring data of dis-tributed systems is an important problem in data mining research. Robust and scalable approaches that can model the potential fault propagation are highly desirable. We de-velop a network diffusion based framework, which simultane-ously takes into account fault propagation on the network as well as reconstructing anomaly signatures using propagated anomalies. Our approach can locate causal anomalies more accurately than existing approaches; in the meantime, it is robust to noise and computationally efficient. Using both synthetic and real-life data sets, we show that the proposed methods outperform other competitors by a large margin.
Wei Wang is supported by the National Science Foun-dation grants IIS-1313606, DBI-1565137, by National Insti-tutes of Health under the grant number R01GM115833-01.
