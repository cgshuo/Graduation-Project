 1. Introduction national research community due to its impressive properties such as adaptability, ability to generalize and learning capability.
The process of training a NN is generally interested in adjusting the individual weights between each of the individual neurons. At the beginning of the learning process a dataset, which is named as training set, is presented to the inputs to determine the correct outputs. When the learning process is finished, an unseen dataset named as testing dataset is used to evaluate the generalization capability of the classifier. With the NN architecture fixed, training is an iterative process that continues until achieving the output value that is close to the desired output by adjusting the network weights accordingly ( Kattan et al., 2010 ).
NNs is the back-propagation technique, which is a gradient-descent method to minimize the mean-squared error between the desired outputs and the actual outputs for the particular inputs to the networks. However BP has some shortcomings: the first is that it requires a differentiable neuron transfer function and the second is the high possibility to converging into local minima. NNs generate complex error surfaces with multiple local minima and BPs tend to become trapped in local solution that is not global ( Montana and Davis, 1989 ).

In order to cope with the local minimum problem, many global optimization techniques have been adopted for the training of
NNs. Most of these techniques draw their inspiration from biological processes like evolutionary algorithms ( Castellani and
Rowlands, 2009 ) genetic algorithms ( Kim et al., 2005 ; Montana and Davis, 1989 ; Zhang and Bai, 2005 ), ant colony optimization ( Blum and Socha, 2005 ; Socha and Blum, 2007 ) particle swarm optimization ( Gudise and Venayagamoorthy, 2003 ; Zamani and
Sadeghian, 2010 ) differential evolution ( Ilonen et al., 2003 ; Slowik and Bialko, 2008 ) and artificial bee colony algorithm ( Karaboga and Ozturk, 2009 ). Harmony search (HS) algorithm, which is originated from the improvisation process of musicians not from biological or physical processes, is also adopted for the training of
NNs. Kattan et al. (2010) employed a variant of improved harmony search algorithm to train NNs for binary classification. They reported that HS performed better than the standard BP method.
In this paper, self-adaptive global best harmony search algorithm ( Pan et al., 2010 ) is used for training feed-forward NNs. SGHS algorithm employs a new improvisation scheme and adaptive parameter tuning methods. A suitable data representa-tion for NN is adapted to SGHS algorithm. SGHS algorithm for training NNs is not only applied to binary classification problems but also applied to n -ary classification problems in this study. Also a real-world case study is carried out in order to classify most frequently encountered quality defect in a major textile company in Turkey. Results obtained from the proposed algorithm are also compared with the results of standard BP algorithm, and other available variants of harmony search algorithm, namely standard HS, improved harmony search (IHS), modified IHS and global best harmony search (GHS) algorithms. The experimental analysis of SGHS presented that SGHS algorithm is a very efficient and promising candidate for training feed-forward NNs.

The rest of the paper is organized as follows. Section 2 sum-marizes the feed-forward networks in a general context. Section 3 gives an overview of harmony search algorithms. Section 4 describes the SGHS algorithm for training neural networks in detailed. The benchmark problems, experimental setup and performance analyses are given in Section 5 . A real-world classification problem and its experimental study are presented in Section 6 . Conclusions and remarks for future works are presented in Section 7 . 2. A brief overview of feed-forward neural networks Feed-forward NNs, which are also known as Multi-Layer Perceptrons (MLP), are one of the most popular and most widely used models in many practical applications due to their high capability to forecasting and classification. Different NN architec-tures can be used for classification purposes such as Self-Organiz-ing Maps (SOM) and Learning Vector Quantization (LVQ). But it should be indicated that if the class memberships of the training data are known, supervised classification methods such as MLP and LVQ should be used in such cases.

Feed-forward NNs have an input layer of source nodes and an output layer of neurons. In addition to these two layers feed-forward NNs generally have one or more hidden layers with hidden neurons, which extract important features embedded in the input data. A sample feed-forward NN with two hidden layers is shown in Fig. 1 .

In feed-forward NNs signals flow from the input layer through the output layer by unidirectional connections, the neurons being connected from one layer to the next, but not within the same layer ( Pham and Diego, 2007 ). Information flow from input layer to output layer is achieved by hidden layers using weights and activation functions. Each weight determines the influence of an input on the neuron and activation function controls the ampli-tude of the output of the neuron. In this study, sigmoid function given by Eq. (1) is used as activation function. Here NET weighted sum of all inputs and O j is the output of neuron j :
O  X  1  X  e NET j  X  1  X  Feed-forward NNs are usually trained with the BP algorithm.
The application of BP basically involves two phases: during the first phase inputs are presented and propagated forward through the NN to compute the output values for each output neuron. Then this output is compared with its desired value and error of each output is calculated. The second phase involves back-propagation of error to each unit in the NN and appropriate weight changes are computed to make the output values at the output neurons closer to desired outputs. Objective of the BP algorithm is to minimize the error. A widely used error function is the Sum of Squared Error ( SSE ). SSE is the sum of squared differences between the current obtained output and the real output value and can be calculated by SSE  X  3. Harmony search algorithms
Harmony search is a meta-heuristic algorithm inspired from the improvisation process of musicians ( Geem et al., 2001 ). Due to the fact that HS is developed for global optimization problems, handled problem should be formulated as an optimization problem with objective function, decision variables and constraints. Generally global optimization problems are formulated in the form of Eqs. (3) and (4),
N indicates the number of decision variables, LB and UB are the lower and upper bounds of decision variables, respectively. Optimize  X  Max or Min  X  f  X  x  X  X  3  X  Subject x  X  j  X  A  X  LB  X  j  X  , UB  X  j  X  , j  X  1 , 2 , 3 , ... , N  X  4  X 
In music, harmony at a time is analogous to a solution vector; each musical instrument is analogous to each decision variable; musical instrument X  X  pitch range is analogous to decision variable X  X  value range; audience X  X  esthetics are analogous to objective function and musician X  X  improvisations are analogous to local and global search schemes in optimization. Fig. 2 shows the analogy between music improvisation and optimization.
In HS algorithm each decision variable (musical instrument) generates a value (note) in order to find the global optimum solution (best harmony). The method uses a stochastic random search based on harmony memory consideration rate and pitch adjustment rate instead of a gradient search. Compared to earlier meta-heuristic optimization algorithms, the HS algorithm imposes fewer mathematical requirements and can be easily adopted for various types of optimization problems ( Lee and Geem, 2005 ).
Nowadays HS algorithm has been applied to many diverse optimization problems such as music composition, Sudoku puzzle, timetabling, tour planning, logistics, web page clustering, text summarization, Internet routing, robotics, power system design, structural design, vehicle routing, heat exchanger design,
RNA structure prediction and so on ( Geem, 2009a , Geem, 2009b ). 3.1. Standard harmony search algorithm
In standard HS algorithm, an initial population of harmonies is randomly generated and stored in a memory called harmony memory ( HM ). At each iteration a new candidate harmony is improvised using three rules:  X  X  X emory consideration rule X  X ,  X  X  X itch adjustment rule X  X  and  X  X  X andom selection X  X . This newly generated harmony is compared with the worst harmony at current iteration.
If candidate harmony is better than the worst harmony, then the worst harmony vector is replaced by the new candidate harmony vector; thus HM is updated. This process is repeated until number of improvisation ( NI )isreached.
 harmony memory size ( HMS ), harmony memory consideration rate ( HMCR ), pitch adjustment rate ( PAR ), distance bandwidth ( BW ) and the number of improvisations. HMS is the number of solution vectors in harmony memory. HMCR controls the balance between exploration and exploitation and takes value between 0 and 1. If memory consideration rule is performed, PAR deter-mines whether further adjustment is required according to BW parameter and can be visualized as local search. BW is the step size of PAR parameter and NI is the termination condition of harmony search. Table 1 summarizes the general procedure of standard HS algorithm ( Geem et al., 2001 ). 3.2. Improved harmony search algorithm due to the fact that PAR and BW parameters of HS control the convergence rate and the ability for fine-tuning. Unlike HS algorithm, which uses fixed values of PAR and BW parameters, IHS algorithm dynamically updates the values of PAR and BW as in
Eqs. (5) and (6). Other steps of the IHS algorithm are same as the standard HS algorithm: PAR  X  t  X  X  PAR min  X  PAR max PAR min NI t  X  5  X  3.3. Modified improved harmony search algorithm (MIHS)
Kattan et al. (2010) proposed a variant of IHS algorithm for training NNs. In modified IHS, PAR and BW parameters are determined dynamically based on the best-to-worst ( BtW ) harmony ratio in the current harmony memory instead of the improvisation count. BtW represents the ratio of the current best harmony fitness to the current worst harmony fitness in HM . They used BtW ratio for three purposes: dynamic setting of PAR value, dynamic setting of BW value and for determining the termination condition. The authors mentioned that this would be more suitable for training NN since parameters and termination would depend on the quality of the attained solution. Modified IHS algorithm has the same steps with IHS algorithm except the termination criterion and PAR and BW formulations. The termina-tion condition is based on BtW termination value, which is usually set to something close to unity. NI could be added as an extra optional termination criterion to the algorithm. PAR and BW parameters, which are functions of BtW , are calculated by PAR  X  BtW  X  X  m  X  PAR max PAR min 1 BtW BW  X  BtW  X  X 
BtW scaled  X  CB  X  BtW BtW thr  X  1 BtW
In Eqs. (7) X (10), BtW , BtW thr , PAR min , PAR max , BW min
CB indicate best-to-worst ratio, best-to-worst ratio threshold value, minimum pitch adjusting rate, maximum pitch adjusting rate, minimum distance bandwidth, maximum distance band-width and constant, respectively. 3.4. Global-best harmony search algorithm (GHS)
Omran and Mahdavi (2008) proposed the global best HS algorithm where concepts from particle swarm optimization are adapted to enhance the performance of the HS. GHS algorithm modifies the pitch adjustment rule of the HS in such a way that the new harmony can consider the best harmony in the HM. The GHS algorithm has exactly the same steps as the IHS except step 3. The modified GHS algorithm is presented in Table 2 . 3.5. Self-adaptive global best harmony search (SGHS) Inspired by the GHS algorithm, Pan et al. (2010) proposed the SGHS algorithm, which employs a new improvisation scheme and an adaptive parameter tuning methods. Novel improvisation scheme uses a modified pitch adjustment rule to well inherit good information from the best harmony vector and a modified memory consideration rule to avoid getting trapped into local optimum solution. The new improvisation scheme of SGHS algorithm can be summarized as shown in Table 3 .

In SGHS algorithm HMCR and NI parameters are fixed to a user-specified value. HMCR and PAR parameters are dynamically adapted to a suitable range by recording their historic values corresponding to generated harmonies entering the HM .By assuming that HMCR and PAR values are normally distributed, SGHS starts with HMCR and PAR values generated according to the normal distribution. During the evolution, HMCR and PAR values of generated harmony vector that replaced the worst member in the HM are recorded. After a specified number of iterations named as learning period ( LP ), means of HMCR and PAR are recalculated by averaging the recorded values. Using new means and given standard deviations new HMCR and PAR values are produced and used in the subsequent iterations. This process is repeated until reaching the maximum number of improvisations. Also BW parameter is dynamically changed with the increase in genera-tions to well balance the exploration and exploitation of the SGHS algorithm. Eq. (11) formalize dynamically changing of BW where BW max and BW min represent the maximum and minimum distance bandwidths, respectively: BW  X  t  X  X 
HMCR , PAR and BW parameters are critical parameters, which guide the search. In SGHS algorithm these parameters are self-adapted by a learning mechanism or dynamically decreased with improvisation counter. Therefore, the SGHS algorithm has potential to demonstrate consistently good performance on problems with different properties ( Pan et al., 2010 ). 4. Training neural networks by SGHS algorithm
The process of training feed-forward NNs mainly consists of determining the weights of connections between the neurons, which decreases the error. Since the weig hts of NNs are real-valued, SGHS algorithm, which especially proposed for continuous optimization problems, can be used to train feed-forward NNs. 4.1. Data representation
A harmony vector in HM represents the weights of NNs and involves separate data strings to represent weights of input through hidden layer processing elements, hidden through output layer processing elements, hidden biases and output biases as previously proposed by the authors ( Ozbakir et al., 2010 ). Fig. 3 illustrates such representation for a small scale problem with 3 inputs, 2 outputs and a hidden layer with 4 processing elements. 4.2. Problem initialization
Fitness function of SGHS algorithm for training NNs is the minimization of SSE . Upper and lower bounds of all weights are taken as [ 1, 1]. All weights in a harmony are randomly generated using Eq. (12) for all harmony vectors in HM : w 4.3. Computational procedure
Computational procedure for training NNs by SGHS algorithm can be summarized as it is shown in Table 4 . 5. Classification benchmark problems 5.1. Datasets
The performance of SGHS algorithm is tested using six datasets obtained from University of California at Irvine (UCI) Machine Learning Repository ( http://mlearn.ics.edu//MLRepository.html ). Table 5 shows the main characteristics of the data sets. Among these datasets two of them have binary classes and remaining four are n -ary classification problems.
 number of classes among the six datasets. It is used to classify glass types as float processed building windows, non-float processed building windows, vehicle windows, containers, table-ware or head lamps. This dataset contains 9 continuous inputs, 7 classes and 214 instances.
 16 high-frequency antennas with a total transmitted power of the order of 6.4 kW. Ionosphere dataset has two classes and these were free electrons in the ionosphere.  X  X  X ood X  X  radar returns are those showing evidence of some type of structure in the ionosphere.  X  X  X ad X  X  returns are those that do not; their signals pass through the ionosphere. Ionosphere contains 34 continuous inputs and 351 instances.
 used database to be found in the pattern recognition literature.
Iris contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two; the latter are not linearly separable from each other. The dataset also contains 4 inputs that are continuous.
Thyroid disease dataset : This dataset is the largest dataset with 7200 instances with 21 categorical and continuous inputs. Objec-tive is to determine whether a patient referred to the clinic is hypothyroid. The dataset contains 3 classes of patients thyroid function as being hyper-function, normal function or subnormal function. Thyroid dataset is originally split into single train and test sets with 3772 and 3428 instances in training and testing sets, respectively.

Wine dataset : Instances of Wine dataset are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. This dataset has 13 contin-uous inputs, which are used to classify 3 classes and 178 instances.
Wisconsin Breast Cancer ( WBC ) dataset : 699 instances of this dataset are arrived from Dr. Woldberg X  X  clinical cases reports.
WBC has 9 continuous inputs that classify a tumor as either benign or malignant. 5.2. Experimental setup
Partition of datasets : A well-known ten-fold cross-validation procedure is applied to the datasets except thyroid dataset. Each dataset is partitioned into ten data subsets and SGHS algorithm and compared other HS algorithms are executed once for each partition. Each time a different partition is used as testing set and the remaining 9 are grouped together to build training set. The training set is used to train the algorithm for good learning capability, while the testing set is applied to evaluate the general-ization capability of the proposed algorithm. Because thyroid dataset is originally partitioned into single training and testing sets ten-fold cross validation is not used for this dataset.
Performance metrics : In order to analyze and compare the training capability of the SGHS algorithm four performance metrics are taken into consideration. These are overall training time, sum of squared errors, training accuracy and testing accuracy. Accuracy measures the ability of the classifier to produce accurate results and can be computed using Eq. (13). As thyroid dataset is originally divided into a single train and test set, performance metrics on this dataset is obtained by evaluating ten different executions of the algorithm on the same training and testing sets. The performance metrics of the 10 runs are averaged and reported for all the datasets. Also standard deviations of the corresponding perfor-mance metrics are calculated:
Parameter settings : 3-layered feed forward NNs architecture that uses sigmoid activation function is designed for all datasets. 8, 33, 3, 17, 11 and 8 processing elements in hidden layers are used for glass, ionosphere, iris, thyroid, wine and WBC dataset, respectively. For comparison feed-forward NNs are also trained with standard BP, standard HS, IHS, modified IHS and GHS algorithms with the same NN architecture and objective function. Instances are classified by assigning a class to an output neuron using the winner-takes-all method, in which output with the largest value defines the class. All algorithms are coded and executed on the same computer. Table 6 shows the parameter settings of all HS algorithms including SGHS algorithm. 5.3. Performance comparisons
Table 7 shows ten-fold cross-validation results of the SGHS algorithm for each of the six data sets. Minimum, average and maximum values of training times, sum of squared errors, train-ing accuracies, testing accuracies and also standard deviations are shown in this table. SGHS algorithm X  X  performance is evaluated by comparing the training times, SSE  X  X , training and testing accuracies of the algorithm with standard back-propagation, standard harmony search, improved harmony search, modified improved harmony search and global best harmony search algorithms. Table 8 shows the comparison results of training times and the sum of squared errors. Table 9 shows comparisons on the training and testing accuracy.

As it can be seen from Table 8 training times of all studied algorithms are close to each other. So it can be said that SGHS algorithm can do training in a reasonable time. Also SSE of SGHS algorithm is smaller than the compared algorithms in three of the six datasets. When training and testing accuracies, which are shown in Table 9 , are taken in to consideration training accuracy of SGHS algorithm is better than other algorithms in four of the six datasets and testing accuracy is better in five datasets. When each dataset is considered individually, WBC appears to be the easiest dataset among the handled datasets. All algorithms give reasonably good results in this dataset. SGHS is the best perform-ing algorithm with 96.999% mean testing accuracy, BP is the second with 95.995% , MIHS is the third with 95.709% and HS 92.706%, IHS 92.282% and GHS has 89. 276% mean testing accuracy. For the wine dataset, SGHS algorithm outperforms the other algorithms with 96.633% mean testing accuracy and 3.907 standard deviations. Other algorithms give considerably lower testing accuracies: IHS 78.562%, HS 76.341%, GHS 64.66%, modified HIS 41.503% and BP 36.403%. Thyroid dataset is the largest dataset with 7200 instances among the six dataset. The results indicate that all algorithms except BP have very close results around 93% mean testing accuracies. For thyroid dataset
BP is the first with 97.45% and SGHS is the second with 93.49% mean testing accuracies. HS, IHS, MIHS and GHS have the same testing accuracies with 92.71% mean. Iris dataset has three classes and one class is linearly separable from the two; the latter are not linearly separable from each other. SGHS algorithm outperforms the other algorithms with 97, 999% mean testing accuracy and 3,222 standard deviation. HS and IHS are the second algorithms with 97.33% means. The latter algorithms are the BP, MIHS and
GHS with 91.332%, 90.37%, 82.001% mean testing accuracies, respectively. Ionosphere is the dataset with maximum inputs (34) among the six datasets. According to mean testing accuracies
SGHS algorithm is the first algorithm with 93.746%, BP is the second with 91.158%; MIHS is the third algorithm with 89.467%, HS is the fourth with 88.341%, IHS is the fifth with 86.34% and
GHS is the last with 85.493% mean testing accuracy. Glass dataset has the maximum number of classes (7) and having only 214 instances. Therefore this is the most difficult dataset among handled datasets. All algorithms except SGHS give lower than 50% mean testing accuracies on this dataset. SGHS algorithm outperforms all other algorithms with 64.977% mean testing accuracy. IHS, HS, MHIS, BP and GHS algorithms mean testing accuracies are 40.996%, 40.695%, 34.56%, 32.91% and 30.844%, respectively.

Statistical hypothesis test is also performed to show whether the difference between SGHS and compared algorithms testing accuracies is statistically meaningful. Statistical hypotheses are based on the concept of proof by contradiction. As variables (testing accuracies) are not distributed normally, groups are related (SGHS and compared algorithms all use same training and testing sets), variables are quantitative and sample size is small (ten test accuracies for each algorithm). Wilcoxon Signed-
Rank Test ( Wilcoxon, 1945 ), which is suitable for these situations, is used in this study. Significance level ( a ) is taken as 0.05.
Table 10 summarizes p values of the Wilcoxon Signed-Rank Test against BP, HS, IHS, MHIS and GHS algorithms.

In Table 10 , ( X ) sign means that compared algorithm gave better result than the SGHS algorithm. As can be seen from the table, SGHS algorithm gave better results than the compared algorithms for all of the datasets except thyroid dataset. In this dataset BP algorithm is better than SGHS algorithm according to testing accuracy measure. The bold values in Table 10 show that it is not statistically possible to prove that testing accuracies of the compared algorithms on related datasets are different from each other, because p values are higher than 0.05. 6. Case study
In this paper, the most frequently encountered quality defect, namely  X  X  X not defect X  X  in a textile manufacturer, is taken into consideration in order to show the training capability of proposed algorithm in real world problems. Knot defect is the appearance of the knots between broken yarns on the fabric and main causes of this defect are ball warping, rebeaming, sizing and weaving.
The attributes related with this defect are determined by making intensive interviews with production experts. Data related to the attributes are collected from different databases and combined into the single database. The attributes that have possible effects on this quality defect are shown in Table 11 ( Baykasoglu et al., in press ). 6.1. Experimental setup ten-fold cross-validation procedure is also applied to this dataset as in benchmark datasets. Overall training time, sum of squared errors, training accuracy and testing accuracy are again taken as performance metrics. Three-layered feed-forward NNs with 7 hidden neurons that use sigmoid activation function are designed for this dataset. For comparison feed-forward NNs are also trained with standard BP, standard HS, IHS, modified IHS and GHS algorithms with the same NN architecture and objective function.
The same parameter setting given in Table 6 is used for all HS algorithms in knot defect dataset. 6.2. Performance comparisons algorithm for knot defect dataset. Minimum, average and maximum values of training times, sum of squ ared errors, training accuracies, testing accuracies and also standard deviations are shown in this table. SGHS algorithm X  X  performance is evaluated by comparing the training times, SSE X  X  training and t esting accuracies of the algorithm with standard BP, standard HS, IHS, MIHS and GHS algorithms.
Table 13 shows the comparison results of training times, sum of squared errors, training and testing accuracies.

As it can be seen from Table 13 , training times of all implemen-ted algorithms are close to each other, namely there is not a significant difference between training times. SGHS algorithm gives lower SSE , higher training and testing accuracies at the nearly same training times when compared with the other algorithms.
Wilcoxon signed-rank test is al so performed on knot defect dataset in order to show whether t he difference between SGHS and compared algorithms testing accuracies is statistically meaningful.
Significance level ( a )isagaintakenas0.05. Table 14 summarizes the p values of wilcoxon signed-rank te st against compared algorithms.
As it can be seen from Table 14 , SGHS algorithm performed better testing accuracy than the compared algorithms for the knot defect dataset, because p values are lower than significance level ( a o 0.05). 7. Conclusion
In this study, a novel harmony search algorithm, SGHS, is used to train feed-forward type NNs for classification problem, which usually occurs in data mining applications. The training perfor-mance and the generalization capability of SGHS algorithm are empirically tested and verified on two binary and four n-ary classification benchmark problems and a real-world problem.
Overall training time, sum of squared errors, training and testing accuracies of SGHS algorithm are compared with other harmony search algorithms and the standard back propagation algorithm.
Experimental and statistical results show that SGHS algorithm can train feed-forward type NNs with high accuracies in a reasonable training time and SSE . SGHS is better than the compared algorithms in learning and classifying unseen patterns.
Consequently, we can say that SGHS algorithm is a good candidate for training feed forward type NNs for classification problems. Moreover, SGHS seems to be the most promising variant of harmony search algorithm available in the literature for the studied problem. Also it can be used in training self-organizing maps, learning vector quantization models, adaptive resonance theory networks and such supervised or unsupervised models, which are good in classification. If the class membership of training data is known, especially supervised models should be preferred. It is only necessary to adapt suitable network structure and learning algorithm to SGHS algorithm.
 References
