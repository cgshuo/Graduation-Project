 In this paper, we present a novel near-duplicate document detec-tion method that can easily be tuned for a particular domain. Our method represents each document as a real-valued sparse k -gram vector, where the weights are learned to optimize for a specified similarity function, such as the cosine similarity or the Jaccard co-efficient. Near-duplicate documents can be reliably detected through this improved similarity measure. In addition, these vectors can be mapped to a small number of hash-values as document signatures through the locality sensitive hashing scheme for efficient similar-ity computation. We demonstrate our approach in two target do-mains: Web news articles and email messages. Our method is not only more accurate than the commonly used methods such as Shin-gles and I-Match, but also shows consistent improvement across the domains, which is a desired property lacked by existing methods. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Performance, Reliability Near-duplicate Detection, Similarity Learning, Spam Detection
Inspired by the needs of many real-world tasks when handling large document collections, near-duplicate detection (NDD) has been an important research problem for more than a decade [4, 6, 10, 20]. In many scenarios, two documents that are not exactly identical may still contain the same content and should be treated as duplicates. However, which portion of the documents should be considered important in such comparison depends on the final  X  This work was done while the author was an intern at Microsoft Research.
 application and may vary from task to task. For example, Web pages from different mirrored sites may only differ in the header or footnote zones that denote the site URL and update time [7, 17]. News articles shown on different portals could come from the same source (e.g., Associated Press) and thus have identical content, but can be rendered in different site templates with advertisements [20]. In both cases, a search engine should not show these near-duplicate documents together since they carry identical information. In a plagiarism detection scenario, the definition of near-duplicate doc-uments may be even looser. When a portion of one document, such as a sentence or a paragraph, is contained in another document, these two documents could be seen as near-duplicates. In contrast, perhaps the most extreme definition of a near-duplicate exists in an anti-adversarial scenario. Spam messages that belong to the same campaign may look very different because spammers often need to randomize the messages by obfuscating terms or adding unrelated paragraphs to pass the filter [13, 14, 16]. However, as long as the core payload text (e.g., a URL pointing to the spammer X  X  site) is identical, two email messages are treated as near-duplicates.
There are two main considerations when solving an NDD prob-lem: efficiency and accuracy . Applications of NDD typically need to handle a very large collection of documents. A practical algo-rithm will need to determine whether a document is a duplicate of some other documents in the repository in real-time [17]. As a re-sult, efficiency has been the main focus of existing popular NDD approaches, where various techniques of generating short signa-tures using hash functions (e.g., [4, 2, 5]) and pruning inverted index searching (e.g., [20]) were invented. In contrast, the prob-lem of how to improve NDD accuracy has received less attention. Most existing NDD methods encode documents as sets of token sequences (i.e., k -grams). In such binary vector representation, all the elements, no matter where the corresponding fragments come from (e.g., header, body or navigation block) of the document, are treated equally when comparing two documents. Although heuris-tic approaches have been developed for selecting important terms using IDF values (e.g., I-Match [6]) or special patterns based on stopwords (e.g., SpotSigs [20]), such approaches do not perform consistently across different domains. As a result, manually tuning the configuration or even selecting the right NDD method becomes a necessity to achieve acceptable accuracy for the target domain.
In this paper, we introduce a novel Adaptive Near-Duplicate De-tection (ANDD) method to address the above issues. Given a small set of labeled documents that denote the near-duplicate clusters, ANDD learns a vector representation for documents in the target domain. Each element in the vector corresponds to a short token sequence (i.e., k -gram) associated with a real-valued weight that in-dicates its importance when determining whether two document are near-duplicate. Similarity scores, such as cosine or Jaccard, calcu-lated based on this new vector representation provide higher near-duplicate detection accuracy. Such improvement does not come at the cost of sacrificing the computational efficiency, as established signature generation techniques can be easily adapted to reduce the dimensionality of the original vector representation.

Our contributions in this work are twofold. First, by observing that existing NDD approaches consist of two main steps, raw vector construction and fast similarity computation, we identify the for-mer is critical to achieve better accuracy. Hashing schemes or trun-cated inverted index provide fast ways to approximate the similarity score calculated using the raw vectors. Theoretically, the prediction performance is upper bounded by the quality of the original rep-resentation. As we show experimentally, the accuracy difference between different hashing schemes is often limited and much less than the gain brought by a better document representation. Second, we extend a recently proposed similarity learning framework [21] to learn the vector construction. This allows ANDD to easily cap-ture the implicit and often vague notion of  X  X ear-duplicate X  through the editorial data for the target domain. As a result, comparable and often higher accuracy can be achieved compared to existing NDD methods, and such advantage is consistent across domains.
In this section, we briefly describe the representative NDD ap-proaches with focus on their unique characteristics.
As one of the earliest NDD methods, the Shingling [4, 2] al-gorithm views each document as a sequence of tokens and first encodes a document as a set of unique k -grams (i.e., contiguous subsequences of k tokens). For the ease of further processing, each k -gram is encoded by a 64-bit Rabin fingerprint [19] and is called a shingle . The similarity between two documents is measured us-ing the Jaccard coefficient between the shingle vectors. Documents with high similarity scores are considered near-duplicate.
One issue of applying the Jaccard similarity directly is the vari-able and potentially large size of the shingle sets, as they grow linearly in the number of document tokens. Broder et al. [4] intro-duced the technique of min-wise independent permutations to solve this problem by mapping each set of shingles to an m -dimensional vector, with m typically much smaller than the original number of tokens in a document. In this process, m different hash functions h ,  X  X  X  , h m are created and applied to all shingles. Let the set of shingles of the target document d be S ( d ) = { s 1 , s The j -th element in the final vector is defined as the minimum hash
Each m -dimensional vector can be further mapped to a smaller set of super shingles by first separating the elements into m joint subsets of equal size and then fingerprinting each subset of elements using a different hashing function. This process effec-tively reduces the dimensionality of each vector from m to m thus saves the storage space and also speeds up the computation.
Notice that in the standard shingling methods, the construction of document signature vectors is purely syntactic  X  all the k -grams in the documents are treated equally. Alternatively, Hoad and Zo-bel [11] experimented with various strategies of selecting k -grams when encoding shingles, such as based on their TFIDF scores.
Unlike Shingling, which creates a sequence of hash values based on a random sample of k -grams of the original document, I-Match maps each individual document into a single hash value using the SHA1 hash algorithm. Two documents are considered near-duplicate if and only if their hash values are identical [6]. The signature gen-eration process of I-Match views a document as a single bag of words (i.e., terms, unigrams). In addition, only the  X  X mportant X  terms are retained in the bag. It first defines an I-Match lexicon L based on collection statistics of terms using a large document corpus. A commonly used option is the inverse document fre-quency ( IDF ), where L consists of only terms with mid-range IDF values. For each document d that contains the set of unique terms U , the intersection S = L  X  U is used as the set of terms represent-ing d for creating the signature.

One potential issue with I-Match occurs when the retained word bag S is small (i.e., | S |  X  | U | ). Because the documents are ef-fectively represented using only a small number of terms, different documents could be mistakenly predicted as near-duplicates eas-ily. To deal with such cases, a constraint is placed on the mini-mum length of a document for which a valid signature can be pro-duced. To make I-Match more robust to such false-positive errors, Kolcz et al. propose using m randomized lexicons concurrently instead of one, following the same I-Match signature generation process [15, 14]. As a result, an m -dimensional vector is used to present a document. Two documents are considered near-duplicate only if they have enough number of signatures matched.
As pointed out by Charikar [5], the min-wise independent per-mutations method used in Shingling is in fact a particular case of a locality sensitive hashing (LSH) scheme introduced by In-dyk and Motwani [12]. The probability that the two hash values match is the same as the Jaccard similarity of the two k -gram vec-tors. In contrast, the random projection based approach proposed by Charikar [5] and later applied to the web document domain by Henzinger [10] is another special LSH scheme for the cosine simi-larity based on term (i.e., unigram) vectors. According to the imple-mentation described in [10], this algorithm generates a binary vec-tor with m bits to represent documents using the following steps. First, each unique term in the target document is projected into an m -dimensional real-valued random vector, where each element is randomly chosen from [  X  1 , 1] . All the random vectors generated from the terms in this document are then added together. The final m -dimensional binary vector representing this document is derived by setting each element in the vector to 1 if the corresponding real value is positive and 0 otherwise.

Noticing that the Shingling method often generates more false-positive cases, Henzinger invented a hybrid approach that applies Charikar X  X  random projection method to the potential near-duplicate pairs detected by Shingling [10]. As a result, the precision is sig-nificantly improved without sacrificing too much recall.
In determining which k -grams in a document should be used for creating signatures, Theobald et al.  X  X  SpotSigs method is perhaps the most creative and interesting one [20]. When developing near-duplicate detection methods for clustering news articles shown on various Web sites, they observe that stopwords seldom occur in the unimportant template blocks such as navigation sidebar or links shown at the bottom of the page. Based on this observation, they first scan the document to find stopwords in it as anchors. k tokens right after an anchor excluding stopwords are grouped as a special k -gram, or so called a  X  X pot signature X  in their terminology. The raw representation of each target document is therefore a set of spot signatures. To some extent, the construction of spot signatures can be viewed as a simple and efficient heuristic to filter terms in template blocks so that the k -grams are extracted from the main content block only. Once the spot signatures have been extracted, the same techniques of using hash functions as seen in other NDD methods can be directly applied to reduce the length of the spot signature vectors. In addition, Theobald et al. propose an efficient algorithm for directly computing the Jaccard similarity measures on the raw spot signature vectors, with the help of a pruned inverted index data structure [20].
Although their algorithm designs seem different, all the NDD methods surveyed in Sec. 2 can be described within a unified frame-work that consists of two main steps: (1) generating k -gram vec-tors from documents and (2) computing the similarity score effi-ciently based on the desired function operating on the vectors. Two documents are considered near-duplicate if their similarity score is above a predefined threshold. In order to have an efficient NDD method with improved prediction accuracy, our strategy is to fol-low the same pipeline but to ensure that the raw k -gram vector is a good representation of the document for computing reliable sim-ilarity scores. It will be a real-valued vector where the weights of the active k -grams are learned from labeled document pairs.
There are two main differences in our approach, compared to most existing NDD algorithms. First, each k -gram in the vector is associated with a real weight, which is used in computing docu-ment similarity scores. In contrast, existing methods typically de-compose documents into bags of k -grams (i.e., binary vectors). Al-though some weighting schemes (e.g., removing terms based on idf values) may be used to retain only a subset of k -grams, all the active k -grams are treated equally during signature generation and/or sim-ilarity computation. As in many NDD applications, not all terms in the same document are equally important (e.g., some may occur in the title; others may appear in the navigation sidebar), treating terms indistinguishably important will obviously impact the accu-racy of the NDD predictions [6].

The second difference of our approach is that we leverage la-beled data in the target domain to learn these weights. Unlike ex-isting NDD methods, which are more or less static , the learning approach provides a principled way to adjust the model to better fit the target domain. Specifically, we extend a recently proposed term-weighting learning framework [21] by using general k -gram vectors instead of just unigrams and by optimizing for different similarity functions such as the Jaccard coefficient. In what fol-lows, we describe the learning approach in detail and demonstrate how the better document representation can be encoded to signa-tures that support efficient similarity computation.
We first formally define the raw document representation in our approach, the real-valued k -gram vector. Let V = { g 1 , g be the vocabulary that contains all possible k -grams occurring in all documents. Each document d is mapped to a sparse vector v , which consists of all the k -grams G  X  V that can be found or selected in d . For each k -gram g  X  G , its score is decided by a function that depends on the k -gram g and/or the document d : gw  X   X  ( g, d ) , where  X   X  is the model parameters learned from the labeled data.
Conceptually, this weighting function indicates how important the k -gram is with respect to the document, when computing the similarity between two vectors. While there are many choices of the functional form, we use a simple linear combination of features extracted for each k -gram g i occurring in document d : Table 1: Features used in ANDD. DF-Avg and DF-Med are only used when k &gt; 1 . InURL is used only for HTML documents. where  X  j is the j -th feature function and  X  j is the corresponding model parameter. The goal of the training procedure is thus to de-termine  X   X  so that two near-duplicate documents can have a high similarity score.

The learning framework enables a principled way of incorporat-ing additional information regarding the raw text in features . For each k -gram, the features used are listed in Table 1. Note that these features are all very easy to compute. Some are statistics derived from scanning the whole document once (e.g., Loc and Len). Oth-ers can be retrieved from a direct table look-up (e.g., DF and QF).
Given two documents d p and d q , their similarity score is given by a specified similarity function f sim operating on their corre-sponding k -gram vectors v p and v q . We develop models for two commonly used similarity functions in the task of NDD, cosine and the (extended) Jaccard coefficient in this work.
Having human subjects score the importance of each k -gram that leads to a robust similarity measure is a difficult annotation task. Instead, we assume that we are given clusters of documents as the labeled data. Documents belonging to the same clusters are near-duplicate and unrelated otherwise. How to use such labeled doc-uments to train the model parameters depends on the learning set-ting. We choose the setting of learning the preference ordering be-cause of its slightly superior performance reported previously [21]. In this setting, the absolute similarity score of two documents is not important. Instead, we would like the model to assign higher scores to near-duplicate documents, compared to other unrelated pairs of documents. Due to the lack of space, we omit the derivations of gradients for both cosine and Jaccard functions here.

A training set of N examples in this setting is formally denoted as { ( y 1 , ( x a 1 , x b 1 )) , ( y 2 , ( x a 2 , x b 2 x uments and y k  X  { 0 , 1 } indicates the pairwise order preference, where 1 means x a k should be ranked higher than x b k and 0 other-wise. The loss function we used for this setting is: L (  X   X  ) = where  X  k is the difference of the similarity scores of two document pairs, computed based on the corresponding vectors. Namely, We use L-BFGS to minimize the loss function for its guarantee to find a local minimum. We omit the derivations of gradients for both cosine and Jaccard functions due to lack of space.
In many applications, the NDD method needs to handle a large collection of documents. Therefore, being able to efficiently deter-mine whether the similarity score of two documents is high enough to declare they are near-duplicate is not only crucial but also a re-quirement to a practical NDD algorithm. Based on the traditional vector space model and cosine similarity, one simple way to de-tect near duplicates is to sequentially submit each document in the collection as a query to search for highly similar documents in the collection. By the use of inverted index as adopted in most search engines, this kind of document search is sub-linear (to the num-ber of documents in the collection) in time complexity in practice, however it can be O ( n 2 ) in the worst case.

Recall that the weights in each vector can be interpreted as the importance scores of the corresponding k -grams, the straightfor-ward way to shrink the vector size is thus by eliminating k -grams with low weights. Efficient similarity computation can be sup-ported by techniques like pruned inverted index [20]. While this could be an effective strategy as shown in our experiment (see Sec. 4.2.2), having variable sizes of document representation is less appealing in terms of engineering. Meanwhile, although low-weight k -grams are not as important, they still contain information that could affect the similarity measure. In contrast, signature gen-eration techniques map vectors into strings of a fixed number of bits. Even with the same complexity order, the basic bit compar-ison operation is much faster than comparing whether two strings (i.e., terms) or two integers (i.e., term id) are identical. Moreover, efficient sub-linear algorithms exist for similarity search in the sig-nature space [5, 12, 1].

In this paper, we rely on locality sensitive hashing (LSH) schemes to map the raw vector to a sequence of hash values as the document signature. An LSH scheme has the following defining property:
D EFINITION 3.1 ([5, 12]). Let f sim (  X  ,  X  ) be a given similar-ity function defined on the collection of objects O . A distribution on a family H of hash functions operating on O is a locality sensitive hashing scheme if for x, y  X  X  , Using this scheme, hash functions h 1 , h 2 ,  X  X  X  , h m drawn from H are applied to raw vectors to encode them into signatures of m hash values. The similarity score of two documents is derived by counting the number of identical hash values, divided by m . As m increases, this scheme will approximate asymptotically the true similarity score given by the specific function f sim . Since the sim-ilarity functions that our learning method optimizes for are cosine and Jaccard, we apply the corresponding LSH schemes when gen-erating signatures.

For the cosine function, we use the random hyperplane based hash function, which is the essential part of Charikar X  X  random pro-jection algorithm [5]. For a given collection of vectors in R hash function is created by first choosing a random vector  X  r from the d -dimensional Gaussian distribution. When applied to a vector  X  u  X  R d , the corresponding binary hash function h  X  r LSH scheme has the following property: which has a monotonic mapping of the cosine function.

When applying this scheme to our k -gram vectors, each k -gram in the vocabulary is associated with m different random numbers drawn from the Gaussian distribution. The signature of each vec-tor/document is a bit-string of m bits. The value of the i -th bit is decided by the sign of summing the product of the i -th random number and the weight of each k -gram. Notice that this scheme works for both binary and real vectors, and the number of bits (i.e., m ) does not need to increase when handling real vectors.
For the Jaccard function, the LSH scheme that we use is the min-hash [12, 8] function, which are designed originally for bi-nary vectors. To handle our real k -gram vectors, we first transfer each real-valued weight to a binary vector as suggested by Gionis et al. [8]. The weight of each k -gram in the vector is multiplied by a big integer and then the number is bucketed and mapped to a bit-string. The original real vector thus becomes a binary vector by concatenating these bit-strings.

It is impractical to choose a hash function h uniformly among all the possible functions. Therefore, we limit the search among a specific class of functions (linear in our experiments) as suggested by [3, 20]. Each hash function h i in this family is defined by two random numbers  X  i and  X  i that are smaller than the length of the mapped binary vectors. Let X be the set of indices of the  X 1 X  bits in vector  X  u . The i -th hash value of this vector is defined as h (  X  u ) = min x  X  X (  X  i  X  x +  X  i mod p ) where p is the first prime number bigger than the length the mapped binary vector. Similarly, a complete document signature consists of m such min-hash val-ues, and the Jaccard coefficient is approximated by the fraction of identical hash values in the corresponding signature vectors.
In this section, we compare our near-duplicate detection methods with other state-of-the-art approaches on two important tasks: De-duping Web news articles and email campaign detection . Although in both tasks, the goal to identify semantically identical documents is the same, the syntactic presentation of near-duplicates in fact dif-fers significantly from one domain to the other. As we demonstrate, our approach can adapt to the target domain easily while existing methods often suffer from such domain change and their predic-tions cannot be consistently reliable. Other practical issues when applying our method, such as the number of training examples it needs and whether it increases the computational cost during run-time, will be discussed at the end of this section.
In this set of experiments, we focus on determining whether news articles shown on different sites are identical and thus are potentially from the same source. As observed by Theobald et al. [20], Web sites showing these articles have their own unique style or template. These news pages may differ substantially in their presentation and only the core content can decide whether they are near-duplicate articles. As a result, correctly distinguishing the important portion of the text and encoding such information in the document representation become a key to improve the accuracy for this task. Below, we start from describing the data used for this set of experiments and the detailed experimental settings. We then compare our method with others in detail.
The dataset we used in this task is the gold set of near-duplicate news articles collected by Theobald et al. [20], which contains 2,160 news articles crawled in 2006. These articles are manually clustered into 68 directories, where documents in the same direc-tory have the same content news and are considered near-duplicate. Table 2: MAX F 1 of ANDD-Raw k -gram vectors with k  X  { 1 , 3 } using the cosine and Jaccard similarity functions, com-pared to other k -gram weighting functions (binary and TFIDF) in the News domain.
 We use this dataset for a five-run experimental setting. In each run of the experiment, we divide these clusters randomly into two dis-joint sets of equal size; one set of clusters of documents is used for training and the other is used for testing. In other words, no cluster has documents in both the training and testing sets.

We construct our training instances as pairs of documents drawn from the training set. Each instance is associated with a binary label. If two documents are from the same cluster and thus near-duplicates, then the label is positive; otherwise, it X  X  negative. No-tice that the number of negative instances in the training set is much more than the number of positive instances. We balance the class distribution by randomly selecting the same number of negative in-stances as positive instances from the current set. 80,000 random pairs of these instances are used to train the model.

Likewise, testing instances are all the pairs of documents created from the testing set. To mimic the true distribution, we do not bal-ance the number of positive and negative instances as done previ-ously and evaluate various methods using all the testing instances. In particular, we evaluate the accuracy by the number of these in-stances with their labels correctly predicted. Following [20], we report the averaged Max F 1 scores of our experimental results as the major evaluation metric.
We first show the quality of the similarity measures computed on the k -gram vectors that include all the k -grams, when used for de-tecting near-duplicate documents. As discussed previously, exist-ing NDD methods essentially approximate the raw similarity mea-sure efficiently by either using the hashing trick or inverted index. In other words, the quality of the raw similarity score dictates the final NDD prediction accuracy.

We compare several configurations of the raw similarity mea-sures based on three variables: k  X  { 1 , 3 } (i.e., unigram or tri-gram), similarity function f sim  X  { cosine, jaccard } and the k -gram weighting function, which can be binary, TFIDF or learned (denoted as ANDD-Raw). Table 2 presents the averaged Max F scores by varying the decision threshold when using the similar-ity measures of all the configurations. As can be observed from the table, regardless of the choice of k and the similarity function, ANDD-Raw yields a better similarity measure for near-duplicate detection. Their Max F 1 scores are statistically significantly bet-ter than their TFIDF counterparts. Not surprisingly, without prun-ing k -grams using heuristics such as idf values, the raw similarity measure based on binary vector representation leads to the low-est Max F 1 scores in our experiments. Again, the differences are statistically significant 1 . We also found that on this dataset, the co-sine function performs slightly better than the Jaccard coefficient,
We run a student X  X  paired-t test on individual scores from the five rounds of the compared two configurations. The results are consid-ered statistically significant when the p-value is lower than 0.05. Figure 1: Averaged Max F 1 scores of ANDD-LSH-Cosine for the News domain when encoded using different numbers of bits. although the differences are not statistically significant. This phe-nomenon is shown on both unigram ( k = 1 ) and trigram ( k = 3 ) representations, where using unigram derives better NDD results. It is also worth noticing that with a small set of simple features, our approach can perform comparable to SpotSigs, which is based on a heuristic designed specifically for this problem domain.
While the raw similarity measures derived from ANDD-Raw k -gram vectors can achieve highly accurate results, reducing the size of such document representation is essential to efficient com-putation. In this section, we first apply locality sensitive hash-ing schemes described earlier to map unigram vectors to signa-tures with different lengths and examine the degree of performance degradation in terms of NDD accuracy. We then compare our ap-proach with other signature-based NDD algorithms.

Taking the unigram ANDD-Raw vectors where the weights are learned to optimize for the cosine function, we map each document to a signature of m bits via the LSH-Cosine scheme. The similarity score of two documents is then determined by the number of iden-tical bits their corresponding signatures share, divided by m . We vary m from 64 to 4096 and report the averaged Max F 1 scores of the 5 rounds in Figure 1. As can be observed from the figure, the accuracy of the NDD prediction based on the bit-string signatures improves quickly as the number of bits (i.e., m ) increases. The per-formance degradation compared to the raw vector representation is limited when m is not too small. In fact, when m  X  512 , the difference in Max F 1 when compared to either the raw vector or the SpotSigs method is not statistically significant. In practice, the choice of m is determined by the trade-off between the efficiency constraint and the desired prediction accuracy, as longer signatures take more storage space and processing time.

We conduct similar experiments applying the LSH-Jaccard scheme to the unigram ANDD-Raw vectors where the weights are learned to optimize for the Jaccard function. Each vector is mapped to a set of m min-hash values. Because each min-hash value takes roughly 20 bits to store, to get roughly the same size of signatures, we vary m from 8 to 256 and report the averaged Max F 1 scores of the 5 rounds in Figure 2. Compared with the LSH-Cosine scheme, LSH-Jaccard is less stable and also needs a larger size of signature to approximate the raw similarity measure. In this set of experiments, we found that the signature needs to have at least 80 min-hash val-ues to achieve a Max F 1 score that is not statistically significantly different from the one derived from the original Jaccard score.
We next compare our approach with other signature-based NDD methods, including Shingles, Charikar X  X  random projection algo-rithm and I-Match. Following the setting chosen by Henzinger [10], Figure 2: The averaged Max F 1 scores of ANDD-LSH-Jaccard for the News domain when encoded using different numbers of min-hash functions.
 ANDD-LSH-Cos [0,1] 48 0.965 0.923 0.943 ANDD-LSH-Jacc [0,1] 200 0.957 0.907 0.931 Table 3: Results of signature-based NDD methods for the News domain. ANDD-LSH-Cos achieves the best Max F 1 despite its short signature length. we use 384 bits (48 bytes) to encode the signatures in ANDD-LSH-Cos and Charikar X  X  random projection method, and represent each document using 84 shingles (672 bytes) and 6 super shingles (48 bytes) for the Shingles and 1-Shingles methods. Because the uni-gram model performs better than trigram model, we set the window size in Shingles to 1. For ANDD-LSH-Jacc, each signature consists of 80 min-hash values (200 bytes) since it gives the best trade-off between accuracy and storage based on Figure 2. I-Match uses only one hash-value for each document and the length is 24 bytes.
Table 3 summarizes the performance of these algorithms in terms of the averaged Max F 1 scores of the five rounds, as well as the cor-responding precision and recall numbers. Among them, ANDD-LSH-Cos achieves the best Max F 1 score despite the fact that its signature length is relatively short. The Jaccard-based counterpart achieves pretty competitive result, but with longer signatures. A regular Shingles method with 84 hash values performs better than Charikar X  X  random projection approach with 384 bits on this do-main. Both have Max F 1 scores higher than 0.8. Surprisingly, de-spite having impeccable precision, both 1-Shingles (at least 1 of the 6 super shingles matches) and I-Match achieve extremely low re-call. Consequently, the F 1 scores are also low. This might be due to the fact that we judge the accuracy using all document pairs, rather than whether each document is put in the same cluster.

Notice that the additional features other than TF and DF signif-icantly boost the performance. Even though the Max F 1 scores of ANDD-LSH are lower than ANDD-Raw, they are still much higher than those derived from raw TFIDF vectors (see Table 2).
Another application of near-duplicate document detection is cap-turing email campaigns, which has been shown an effective tech-nique for collaborative spam filtering and identifying abused email accounts [9, 18, 15, 13]. An email campaign is a large volume of semantically identical mail sent to a large number of recipients within a short period of time. Although some of these campaigns may be legitimate marketing newsletters, a big portion of them are in fact spam. To a large email service provider, such as Hotmail or GMail, as long as there are a few users reporting junkmail mes-sages, all other messages from the same campaign can be quickly classified as spam and removed from the inboxes of their recipients. Unfortunately, reliably detecting spam campaigns is not trivial. In order to escape from such spam filtering scheme, spammers typ-ically randomize their messages by obfuscating words or phrases in the email subject or body, or by appending random, unrelated para-graphs at the end of the mail. Figure 3 shows an example of two messages from the same campaign. As we can see here, the notion of  X  X ear-duplicate X  in this domain becomes quite different from the Web domain  X  two dissimilar messages may in fact come from the same campaign because they convey the same payload content.
To judge the effectiveness of different NDD algorithms when applied to a practical problem such as spam campaign detection, it is best to evaluate them in a realistic setting. We experiment with our approach using a set of 11,108,298 outbound messages served by Hotmail, randomly sampled during the period between Dec 10th, 2008 and Jan 24th, 2009. Detecting outbound email cam-paigns serves two purposes: reducing outbound spam and capturing abused accounts. Because legitimate marketing campaigns are usu-ally sent from the same sender, duplicate messages sent by multiple unrelated users can therefore be reliably classified as spam and in-tercepted by an outbound spam filter. In addition, accounts that send these messages are very likely to be abused or hijacked by adversaries. Detecting email campaigns in outbound messages can help find and suspend these compromised accounts.

Given the large number of messages, one difficulty we encoun-tered is how to manually find and label campaign messages in this collection. In order to efficiently construct the  X  X old X  set of email campaigns, we first ran both I-Match and Shingling on this dataset. Two messages were treated near-duplicate and put in the same clus-ter (i.e., campaign) if either I-Match or Shingling predicted so. As a result, we derived 31,512 initial clusters in total. Not surpris-ingly, email campaigns detected in this way were not totally correct and contain quite a few false-positive and false-negative cases. To further clean the campaign labels, we randomly selected 875 clus-ters with at least two messages and corrected their labels manually. Since in reality, a spam detection system can only be trained us-ing historical messages, we mimicked this scenario by splitting the clusters based on the time their messages were sent. 400 clusters (2,256 email messages) sent between Dec 10th, 2008 and Jan 5th, 2009 were used for training, and the remaining 475 clusters (658 email messages) sent between Jan 5th, 2009 and Jan 24th, 2009 were used for testing.
Table 4 shows the results of the similarity measures based on the unigram vectors. Contrary to the experiments on the news data, here we found that the Jaccard coefficient performs better than the cosine function. As for the comparisons against other weighting schemes, when the weights are learned using labeled data, the qual-ity of the raw similarity measure is still better than TFIDF, which is again better than using the binary vector representation.
Interestingly, we also found that the NDD accuracy of SpotSigs drops sharply on this dataset. After carefully examining the uni-grams it selects, we realized that although its heuristics of selecting the first non-stop words after some anchor stopwords works great Table 4: MAX F 1 of ANDD-Raw k -gram vectors with k = 1 using the cosine and Jaccard similarity functions, compared to other k -gram weighting functions (binary and TFIDF) in the Email domain. Figure 4: Max F 1 vs. the number of active terms in the Email domain for ANDD, TFIDF, and SpotSigs. on the Web news crawls, it is much less effective on email for two reasons. Email messages are typically shorter than news articles and are written somewhat informally. As a result, stopwords occur less frequently, and each email consists only 5.7 Spot signature in average. In addition, we hypothesize that the SpotSigs heuristic ef-fectively removes the terms in the template blocks such as ads or navigation bar, which typically don X  X  exist in email. It is therefore not clear whether SpotSigs can capture the main content any more.
To validate these conjectures, we conduct an experiment to test the performance degradation when removing some unigrams of the vectors based on their importance, judged by the weighting score. Figure 4 shows the Max F 1 scores given by the cosine similarity based on ANDD-Raw and TFIDF vectors with different numbers of active unigrams in them. Although the averaged number of ac-tive unigrams is 53.7, the performance of our method does not suf-fer much even when the number of active unigrams is restricted to 10. When the number of active unigrams is 5, the Max F 1 of ANDD and TFIDF are 0.621 and 0.525, respectively. Both out-perform SpotSigs substantially.
Table 5 reports the prediction accuracy of various NDD meth-ods in terms of Max F 1 , along with the corresponding precision and recall. Although the Jaccard similarity on the ANDD-Raw unigram vectors performs better than the cosine version, such ad-vantage is not fully carried after dimensionality reduction using the LSH scheme. As a result, ANDD-LSH-Cos achieves slightly ANDD-LSH-Cos [0,1] 48 0.723 0.599 0.656 ANDD-LSH-Jacc [0,1] 160 0.821 0.532 0.646 Table 5: Results of signature-based NDD methods for the Email domain. ANDD-LSH-Cos achieves the best Max F 1 despite its short signature length. higher Max F 1 score than ANDD-LSH-Jacc, and both outperform Charikar X  X  random projection method. Due to the selection bias introduced when sampling email messages for annotation, it is not entirely fair to compare our approach with Shingles or I-Match. We list the results of these algorithms here for reference only.
As shown previously, our ANDD approach not only outperforms existing methods, but can also adapt to a different domain better. Its advantages are mainly due to learning from the labeled training documents and from extracting more information from documents. In what follows we discuss how these two extra needs may impact the deployment of our algorithm in practice.

One natural question for a learning approach such as ours is how many training documents are needed to achieve satisfactory accu-racy. To answer this question, we repeat our experiments using the news data set by reducing the number of training documents and record the corresponding Max F 1 scores. Although in our regu-lar setting we use 1,005 labeled documents in average for training, surprisingly we found that the algorithm can in fact train a model that achieves the asymptotic result even with a small number of la-beled documents. Figure 5 shows the learning curve of the model on unigrams. The initial model of random parameter values gives a 0.856 Max F 1 score, which is worse than the cosine similarity based on binary unigram vector (see Table 2). With as few as 60 documents, it already improves the Max F 1 score to 0.953, which is very close to 0.956, the Max F 1 score of our regular model. Such a sharp learning curve may be due to the fact that we are learning only a few model parameters (23 in these experiments). However, we want to point out that manually tuning these model parameters is still difficult and may not be much better than the initial random model weights (from preliminary experiments not reported here). Also, adding more training documents still improves the perfor-mance although the gain is relatively small.

Another practical concern of deploying our approach is the run-time efficiency. Thanks to the effective LSH scheme, our approach can encode documents in short bit-string signatures that preserve the good accuracy result of the raw vectors. The storage require-ment of our method remains the same and standard indexing tech-niques applied to other NDD approaches can be adapted here as well (notice that training the k -gram weighting model is done of-Figure 5: Max F 1 vs. number of training samples compared to the random initialization (init) and fully learned model (full). Accuracy is only lowered slightly as the number of training ex-amples decreases. fline). The only place that our algorithm may need slightly more computational time is feature extraction. Fortunately, we can al-ready achieve good performance based on straightforward features listed in Table 1. Among them, the document frequency and query log frequency can be found using a simple table lookup operation. Other features such as term frequency and term position can be determined when the parser or tokenizer finishes scanning the doc-ument. Therefore, such additional computation cost should be neg-ligible. Of course, adding more domain-dependent features is usu-ally the most effective way to improve a learning system in prac-tice. Whether more such features should be added will depend on the trade-off between the computational cost of extracting such fea-tures and its potential benefits to the final NDD prediction accuracy.
In this paper, we presented a novel adaptive near-duplicate de-tection (ANDD) method that achieves high accuracy consistently across different target domains. Observing that the raw document representation dictates the prediction accuracy of an NDD algo-rithm, we extend a recently proposed term-weighting framework [21] to learn general k -gram vectors and to optimize for a different sim-ilarity function such as the Jaccard coefficient. Each document is represented by an informative real k -gram vector. Similarity mea-sures computed on these vectors can reliably predict near-duplicate documents. To the best of our knowledge, our method is the first that introduces similarity learning to the NDD problem. As demon-strated by the experiments done on two different problem domains, the Web news dataset and outbound email messages, our method can easily leverage a small number of labeled near-duplicate docu-ment clusters, and provide more accurate prediction results without tedious parameter tuning for the target domain. When this approach is applied to large-scale problems, efficiency is preserved by map-ping the vector representation to short signatures with the help of effective locality sensitive hashing schemes.

In the future, we would like to explore different possibilities of feature engineering and model improvement to further enhance our approach. For instance, lexical features such as the words contained in each k -gram can be easily added. Information from deeper doc-ument analysis modules, if can be efficiently executed during run-time, will be used for feature extraction as well. Applying our NDD method to more applications for Web search and online advertising is also on our agenda.
We thank Chris Meek and Susan Dumais for many useful discus-sions. We are also grateful to Martin Theobald for sharing the data and the SpotSigs package. We also would like to thank anonymous reviewers for their helpful comments. [1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for [2] A. Z. Broder. Identifying and filtering near-duplicate [3] A. Z. Broder, M. Charikar, A. M. Frieze, and [4] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. [5] M. S. Charikar. Similarity estimation techniques from [6] A. Chowdhury, O. Frieder, D. Grossman, and M. C.
 [7] D. Fetterly, M. Manasse, and M. Najork. On the evolution of [8] A. Gionis, P. Indyk, and R. Motwani. Similarity search in [9] R. J. Hall. A countermeasure to duplicate-detecting [10] M. Henzinger. Finding near-duplicate web pages: a [11] T. C. Hoad and J. Zobel. Methods for identifying versioned [12] P. Indyk and R. Motwani. Approximate nearest neighbors: [13] A. Kolcz and A. Chowdhury. Hardening fingerprinting by [14] A. Kolcz and A. Chowdhury. Lexicon randomization for [15] A. Kolcz, A. Chowdhury, and J. Alspector. Improved [16] D. Lowd and C. Meek. Good word attacks on statistical spam [17] G. S. Manku, A. Jain, and A. Das Sarma. Detecting [18] V. V. Prakash and A. O X  X onnell. Fighting spam with [19] M. Rabin. Fingerprinting by random polynomials. Report [20] M. Theobald, J. Siddharth, and A. Paepcke. Spotsigs: robust [21] W. Yih. Learning term-weighting functions for similarity
