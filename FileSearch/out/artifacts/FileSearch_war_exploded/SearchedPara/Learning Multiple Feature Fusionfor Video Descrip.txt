 With the dev elopment of artificial intelligence, people are increasingly interested in describing visual contents with natural language sentences, such as image caption or video description. For most people, it is easy to describe what happened in one video according to its visual information, but generating a sequence based on visual information automatically is still a complicated task for the machine. W hile video description is so challenging , it has a wide application prospect in many fields, such as vid eo retrieval, video caption based on semantic content, describing videos for the blind and automated video surveillance.

Video description is a transform from sequence to sequence in a sense as it inputs a sequence of raw frames and outputs a sequence comp osed of meaningful words. General model for video description usually uses two -step pipeline . Firstly, it extract s features of the video as an encoding stage and then generates sequence according to these features utilizing sequence generator as a decoding stage . Previous works [ 7, 9, 18 ] use a two -step pipeline to generate sequence. At first, they identify a fixed tuple of sematic roles, such as subject, verb object and scene, which are used to generate sequence. Then they use a template to translate these semantic contents to a sentence. While this simplified the process of video description, but this approach limits the diversity of natural languag e structures used in human descriptions to a large extent . Venugopalan et.al. proposed a video description models [ 2 0 ] F irst step of the method is to extract fixed length feature vector representing the frame or video from a CNN. LSTMs model following the language translation m odels of [ 17 ]. Similar approach in [ 2 2 ] integrates the Fully Convolutional Networks (FCNs) with Multiple Instance Learning mechanism (MIL) to form a multi -scale multi -instance network. Then it takes two -layer LSTM to generate sentence. These approaches onl y consider the spatial information of the video, but ignore the order of sequence and unable to exploit temporal information. To explore temporal information, Venugopalan et al. [ 1 9 ] use optical flow. They use one LSTM to encode visual features of both RGB images and flow images and decode them with sequence. When the model starts to decode, there is no visual input to the sequence generator. This might reduce relationship between visual information and words. Yao et al. [ 2 3 ] employ 3 -D CNN [ 5 ] to extract s patio -temporal motion features which have shown better performance at action recognition. Some dense trajectory features such as HoG, HoF and MBH [ 4 ] are extracted and concatenated to form the input. They also integrate soft -attention [ 2 1 ] mechanism with 3 -D CNN to automatically to select the most relevant temporal segments. Results show that 3 -D CNN alone gives limited performance improvement, if it does not conjunct with soft -attention mechanism.

In this paper, we use a VGG -16 [ 15 ] model pre -trained on 1. 2M images with category labels to extract features. In addition, we use the Lo ng Short Term Memory [8] (LSTM) , a special type of Recurrent Neural Network (RNN), to construct sequence generator. T aki ng inspiration from video description models in [ 1 9 ], we p ropose a novel model for video description with spatial and temporal features in the encoding stage and the mean pooling feature in the decoding stage. The main model structure is illustrated in Figure 1. Other than the spatial feature s extracted from each frame of video by VGG -16 model without changes , to further learn temporal information underlying the video, we generate SIFT flow images from the video using SIFT flow method [5] and then temporal features is extracted by our fine -tune d VGG -16 model. The model of video sentence description contains a two -layer LSTM which first encodes the frames one by one. O nce all frames are exhausted , it starts to generate words one by one. To make use of the holistic feature, we also input mean poo l feature during decoding stage. Otherwise, w e also replace simple one -hot vector with word2vec, which is achieved by training on text corpora, to learn similarity between words. Different from S2VT method [ 19 ] , our proposed model is an ensemble of the seq uence to sequence models trained on raw images and SIFT flow images. Moreover, o ur model inputs separate frame  X  s feature during encoding stage and inputs holistic feature during decoding stage, a ll other approaches mentioned [ 1 9 , 2 0 , 2 2 ] take input with only separate frame  X  s feature or holistic feature . This not only helps make full use of important features, but also helps establish relationship between visual information and words. In addition, our model can automatically learn similar ity between words through replaces one -hot vector with word2vec , while most previous works [ 1 9 , 2 0 , 2 2 , 2 3 ] choose one -hot vector where any two words are separate to be word representation.
 The model we proposed is a sequence -to -sequence process for video description, where the input is the sequence of frames , and output is the sequence of words . In our model, we estimate the conditional probability of an output sequence given an input sequence i.e.
Recently, related works [ 3 , 17 ] have shown that it is effective to resolve such sequence to sequence problem with an LSTM. In this section, we describe our video description model based on CNN and LSTM in details by three parts . 2.1 LSTMs for sequence generation The main idea for video description is to first encode the input sequence of frames, representing the video using a late nt vector representation, and then decode from that representation to a sentence.
Long Short Term Memory (LSTM) was proposed by Hochreiter and Schmidhuber in 1997 [ 8 ] and recently improved by Graves [ 6 ]. Traditional RNN can be difficult to learn long -range dependencies. However, LSTM which contains explicitly controllable memory units, are known to be able to learn long -range temporal dependencie s. Fig 2 depicts the LSTM structure used in this paper.

The core of LSTM is the memory cell c, which updates its state through the control gate at every time step . The cell is modulated by gates which is a sigmoid function with a range of [0, 1]. These gat es determine whether the LSTM keeps the value from the gate (if the layer evaluates to 1), or discards it (if it evaluates to 0). T he forget gate allows the LSTM to forget its previous memory . T he input gate allows the LSTM to determine wh at new information will be used for updating memory cell. Then a new candidate value  X  is created by a tanh function with input and previous hidden state . We multiply with to discard the information discarded, then multiply  X  w ith to keep the information needed. T he updated memory cell is computed by adding the result of the above two steps. The output gate decides how much of the memory to transfer to the hidden state . The final output of the result is obtained by multiplying with result which processes the memory cell by a tanh function. The formula group for LST M is defined as follows, where denote s the sigmoid function , denotes the hyperbolic tangent function tanh , denotees the e lement -wise product with the gate value, and weight matrices denoted by and biases are the trained parameters. 2.2 S equence to seq uence for video description to describe video with sentence . However, our model (Fig.1) input raw frames and SIFT flow images to train separate VGG -16 model for video description during encoding stage. After all frames of the video exhausted , the model input holistic feature of the video during decoding stage. Encoder with spatial feature . I n this work, we utilize the 16 -layer VGG model (VGG -16) [ 14 ] pre -trained on the ImageNet dataset [ 1 ] to extract 4096 dimensional output of the fully connection layer (fc7) as the spatial feature for the video frame. W e then learn a new linear embedding of the features to a 500 dimensional space as the input to top layer LSTM. The weights of e mbedding are learned jointly with the LSTM layer during training.
 Encoder with temporal feature . W e follow the approach in [ 11 ] and first extract SIFT flow field between two consecutive on UCF -101 dataset [ 16 ] . W e then visualize SIFT flow field as SIFT flow images. T hese images are used with the label same as the category of activity to fine -tune a specific VGG -16 model for SIFT flow images. On MSVD dataset , w e apply the same method to generate SIFT flow images. T hen, we use the specific VGG -16 model to extract 4096 dimensional output of the fully connection layer (fc7) as the temporal feature s . We take same approach to learn a new linear embedding of the features to a 500 dimensional space.
 Decoder with mean pool i ng features . W e extract features of all raw images and then get one holistic feature representing videos after mean pool these features extracted. order to make full use of the holistic feature and establish more relationship between visual information and word, we input mean pool feature at every time step during decoding stage.
 Training and Inference . F irstly , the first LSTM layer inputs a sequenc e of frames and encodes them while the second LSMT layer receives the hidden representation After all the frames are exhausted , the second LSTM is fed with begin -of -sentence (&lt;BOS&gt;) tag, which prompts it to start decoding. While training in the decoding stage, the model maximizes for the log -likelihood of the predicted output sentence given the hidden representation of the visual fram e sequence, and the previous words . T he model parameters and output sequence is formulated as: T he model use s stochastic gradient descent t o optimize this log -likelihood over t he entire training dataset . The model computes loss only when the LSTM is learning to decode. The output of second LSTM at each time step t is used to generate words. Then we employ a s oftmax function to get the probability distribution over all words tag, the decoding task is over. At train time, the previous word is ground truth during decoding stage. B ut at test time, the previous word is with the maximum probability after the s oftmax until it emits the &lt;EOS&gt; token. Fusing spatial feature with temporal feature . W hen models with raw frames and SIFT flow images have been trained, we use a shallow fusion technique to integrate spatial and temporal features. At each time step of the decodi ng stage, the model c omputes probability of every candidate words. W e then recomputed the p robability of each new word ) as: the hyper -parameter is tuned on the validation set. 2.3 W ord2vec for model optimization
Word2vec [ 4 ] is an open -source toolkit developed by Google Company for training word vector in 2013, and received wide acclaim in the field of natural language processing. Word2vec contains CBOW and Skip -Gram models, each of which has hierarchi cal s oftmax and negative sampling strategies. Please refer to [ 12 ] for more details of word2vec. In this paper, we take Skip -Gram with hierarchical s oftmax strategy to train language model and get word vectors. O ur optimal approach is depicted in the low part of Fig 1 . W e first transform wiki English corpus from xml format into txt format , and then use word2vec toolkit to train language model. W e can achieve the word vectors at the same time . At last, we replace the former word vector generated by one -hot vector with new word vectors in the model. 3.1 Datasets MSVD. W e evaluate our model on the Microsoft Video Description corpus [ 1 ] (MSVD), which also known as the YouTube2Text dataset. M S VD is a video description dataset that contains 1970 short videos from YouTube. Each short video takes between 10s and 25s, describes a single behav ior, and the data set covers a variety of scenes. Each video corresponds to more than 100 text descriptions, in cluding multiple languages. In our experiment s , we take descriptions of English about 40 descriptions for each video . W e pick 1200 videos for training , 100 video s for validation and 670 video for testing. 3.2 Evaluation Metrics To evaluate the generated sentence , we use the BLEU [13 ] and METEOR [10 ] scores against all ground truth sentences. The BLEU score is used to count number of n tuple appear in both hypothesis sentence and candidate sentences. T he METEOR score is computed based on the alignment betw een a give hypothesis sentence and a set of candidate reference sentences. W e employ the code released with Microsoft COCO Evaluation Server [ 2 ] for comprehensive comparison . 3.3 Results Analysis F eatures during encoding stage . Table 1 shows the results of dif ferent features during encoding stage on the MSVD dataset. W e can found that taking SIFT flow images as input achieve better result than optical flow. Though our model which takes RGB images show worse result than S2VT with RGB images, fusing models with i nput of RGB images and SIFT flow images outperformed fusing models with input of RGB images and optical flow images. Results demonstrate that fusion of CNN feature and SIFT flow feature can improve the performance of video description . Features during decoding stage . Table 2 shows the results of mean pool feature during decoding stage on the MSVD dataset. It justifies that adding mean pool feature when the model starts decoding can improve the performance of video description. W ord2vec as word representation . Table 3 shows the results of word2vec as word representation on the MSVD dataset. It can safely say that replacing one -hot vector with word2vec can improve the performance of video description.
 Final model . Table 4 shows the results of our final model compared with state -of -art works on the MSVD dataset. O ur final model outperforms most of models. To show the efficiency of our visual -sentence translation , we give some visualized results of our final model see Fig 3 and we found that generated sequence is very similar with reference sequence.
 In this paper, we propose a model for video description which fuses multiple features during encoding and decoding stage. Besides, we replace one -hot vector with word2vec when the model uses word representation. The approach is evaluated on MSVD dataset . T he modification of original model can improve the capa city of video description model . In the future, learning more text information underlying sentence from text corpora will be deserve d to research. This work was partially supported by N ational Natural Science Foundation of China (NSFC Grant No. 61170124, 61272258, 613012 9 9 , 61272005, 61572085), Provincial Natural Science Foundation of Jiangsu (Grant No. BK20151254, BK20151260), Key Laboratory of Symbolic Computation and Knowledge Enginee ring of Ministry of Education, Jilin University (Grant No. 93K172016K08), and Collaborative Innovation Center of Novel Software Technology and Industrialization.
