 Two distinct methodologies have emerged for comparing the usefulness of information retrieval systems: subject-based approaches that observe humans as they carry out some information seeking task while aspects of their behavior are monitored; and data-based approaches, in which standard corpora of documents, and the query topics and rele-vance judgments that accompany them, are used and re-used. The drawback of the first approach is that human experimentation requires great care in experimental design and in data interpretation, and is both difficult to reproduce, and expensive to carry out on a large scale. The drawback of the second approach is that it requires system  X  X seful-ness X  to be approximated by an effectiveness metric that may or may not adequately represent the facets of performance that can be elicited via a user study [10,19]. Data-driven experiments also require human input while the test sets are being prepared. The preparation of relevance judgments is expensive, and creation of exhaustive relevance judgments for non-trivial topics is beyond the resources of most individuals or organi-zations, even for relatively modest collection sizes and topic sets.

To identify a subset of the documents for judging, pooling is commonly used. Pool-ing has been the standard approach used in TREC for nearly two decades [20], and selects for judgment documents which are highly ranked by at least one of the con-tributing systems. The current practice is to pick the top d ranked documents for each system for each topic, identify the set of uni que document-topic pai rs, and judge them, with the choice of d controlling the cost of the evaluation in a non-linear manner.
This arrangement then gives rise to the question as to how unjudged documents should be handled when they are encountered during the evaluation of whatever effec-tiveness metric is being used. The conventional and default assumption is to presume that unjudged documents are not relevant, arguing that if they were relevant, they should have appeared within the top d for at least one of the systems that gave rise to the pool. This presumption is especially important when effectiveness metrics such as NDCG and AP are being used, since they include a normalization step by R , the total number of relevant documents for that topic. Studies have shown that when d is of the order of 100 to 200 ,thevalueof R derived from pooling is a relatively useful estimate [23]. But when a smaller number of judgments are being performed, care is required  X  it is not at all unusual, for example, for system runs to be generated and evaluated to depth 1 , 000 even though d = 100 is used for the judgments. In this case it is entirely possible for fully 90 % of the documents comprising a scored run to be unjudged.

Our work in this paper examines this tension. Specifically, we:  X  explore methods for estimating effectiveness scores in the face of missing relevance  X  compare the quality of the system comparisons that result from those estimation  X  explore the appropriateness of statistical tests when the number of judgments against In particular, we demonstrate that the assumption of  X  X ot relevant X  for unjudged docu-ments, while simplistic, and giving incorrect effectiveness scores, still leads to reliable experimental outcomes. The design of a retrieval experiment relies on a number of critical decisions [20]. This section briefly summarizes these different facets.
 Collection and topics. The starting point for data-based IR experimentation is a collec-tion of realistic documents, where realistic refers both to content and style, and to scale. In many IR experiments documents are sourced from the public web, but commercial entities may choose to make use of private collections, such as email repositories and the like. A set of realistic topics relative to that data is also required, where a topic is expressed as a query or more detailed i nformation need, and is accompanied by a statement as to the supposed intent of the searcher assumed to have authored that need.
The two systems being compared are then used to create a ranked list of answers, or a system run for each of the topics, evaluated in the c ontext of the collection. It is those system runs  X  containing a thousand or more documents in order, for each of the topics  X  that are then evaluated in the remaining steps of the experiment.
 Effectiveness metrics. A suitable effectiveness metric is then chosen to reflect the as-sumed model for the anticipated user behavior. For example, if users are presumed to be focussed primarily on quickly identifying a single possible resource in connection with the query, a metric such as  X  X it at 3  X  might be appropriate, which assigns  X  1  X  X o any retrieval run in which one or more of the t op-three ranked documents is an answer. The assessment of somewhat more patient user behavior might be modelled by the met-ric precision at 10 (P@ 10 ); and extended searching behaviors might be modelled by a weighted precision metric that reaches furt her down the ranking, such as that offered by Discounted Cumulative Gain (DCG) [11] evaluated to depth 100 in the rankings. Formation of relevance judgments. The next step is to form relevance judgments that allow the effectiveness metric to be evalu ated. A usual method for building relevance judgments is to pool the runs from the systems being compared down to some depth d . This approach yields incomplete judgmen ts, but is necessary because it is beyond most researchers X  resources to carry out comprehensive judgments across a non-trivial set of topics for realistic-sized document collections.
 Statistical testing. Another important facet of experime ntal design is the choice of statis-tical test, and a number of authors have commented in this regard [4,8,13,16,17,18,23]. Parametric statistics such as the t -test are used when assumptions can be made about the distribution underlying the system scores (or more correctly, underlying aggregates of scores), with predictions then possible about future behavior. For the most part, these rely on the scores being normally distributed. Parametric distributions can also be ap-plied to score differences when a sufficiently large set is available for analysis, even if the two underlying score distributions are not normal. For typical purposes, most sources agree that  X  X ufficiently large X  is attained at around 30  X  50 independent paired observations. When there is a pre-conceived notion as to which system is being tested for superiority (because, for example, it has the higher mean score across the set of observations) a one-tailed test is appropriate.

Statistical testing also assumes that the observations are independent and drawn ran-domly from some universe. In the case of an IR experiment, it is hard to provide evi-dence that the topics are a random subset of all queries; nevertheless, it is an assumption that is made in all statistical testing on IR system scores. Consider the simple metric precision at depth k , computed as the fraction of the top k ranked documents for each system that are relevant. If d , the pool depth, is larger than k , then all of the system-topic scores are fully defined, because every required document has been judged. But when k&gt;d , or when a new system that was not a contributor to the pool is being scored, there are three sets of documents, rather than two:  X  those that have been judged relevant, r in total;  X  those that have been judged irrelevant, n in total; and  X  those that have not been judged, k  X  ( r + n ) in total.
 The effectiveness score for the run can then be expressed as a range [ B, T ] ,where B = r/k and T =1  X  n/k are the bottom and top of the range, and  X  = T  X  B = 1  X  ( r + n ) /k is the uncertainty, or residual associated with the measurement. In the presence of unjudged documents other precision-based effectiveness metrics can also be evaluated to a [ B, T ] interval rather than a point, including rank-biased precision [12], and discounted cumulative gain [11]. Moffat and Zobel [12] make explicit ref-erence to the benefits of tracking score uncertainty via a residual, and highlight the lack of fidelity in effectiveness scores that arises when the pool depth d is shallow and significant numbers of unjudged documents are encountered.

In fact, what is desired in order for the system comparison to take place is a point estimate that reflects the interval. More precisely, if the interval is taken to be the domain of a probability density function that describ es the likelihood of the final score being any value in the interval, then the required point splits the probability density into two parts each of mass 0 . 5 .

When viewed this way, taking B as the representative point is a approach that is open to question, and there are other point estimates that could be considered. In the experiments that are described below, the following four methods are used. In all cases it is assumed that the representative point X is required to lie within [ B, T ]  X  [0 , 1] . Simplistic prediction. As already noted, the simplest approach is to take the minimum value of the range, X S = B .
 Background prediction. If a global estimate E can be computed for the background probability of a document being relevant, then t he unjudged documents can be pre-sumed to be relevant with that probability, In this approach, a fixed fraction of  X  is added to B , regardless of the value of B .The question now is to determine an appropriate value of E ;thevalue E =0 . 01 is defended below as being a reasonable one.
 Interpolated prediction. A third option is to split  X  , based on the ratio of B to 1  X  T , on the assumption that unjudged documents for a particular system are as likely to be relevant as the documents for which judgments are available. This approach yields: This method is similar to the RBP projection method discussed by Moffat and Zobel [12]. An obvious drawback is that it cannot be computed when  X  =1 (that is, when B =0 and T =1 ), and in this special case X I = E .
 Smoothed prediction. The smaller the value of  X  , the greater the confidence in the In-terpolated prediction. Conversely, the greater the value of  X  , the more attractive it may be to prefer the background model. This combination leads to point score calculated as: where  X  is a parameter that reflects the level of c onfidence in the Interpolated prediction. If  X  is chosen to be 1  X   X  , this simplifies to Pessimistic interval comparison. Rather than seek to represent an interval by a point value within the interval, it is also possible to compare corresponding score intervals directly. Suppose two systems are being compared, S 1 and S 2 , and their score ranges are [ B 1 ,T 1 ] and [ B 2 ,T 2 ] respectively. If T 1 &lt;B 2 then S 2 is clearly better (on this topic) than S 1 ; and vice versa if T 2 &lt;B 1 . On the other hand, when B 1 &lt;B 2 &lt;T 1 or B 1 &lt;T 2 &lt;T 1 (or either of two further symmetric cases) the outcome is inconclusive and there is neither evidence in support the hypothesis that S 1 is better than S 2 , nor evidence to contradict it.
 Estimating the bac kground pr obability. It was indicated above that E =0 . 01 would be used in our experimentation. As a justification for this, consider Figure 1, which shows the fraction of the documents in the TREC-9 Web Track that were judged relevant, categorized by the number of the pool-contributing systems that had included that doc-ument in their pool with d = 100 . For example, seven documents were each identified as being in the top d = 100 by 55 different systems, of which five were judged relevant, corresponding to the rightmost bar in the graph.

The average fraction across the graph is 0 . 202 , but is biased by the higher values at the right of the distribution. What is of more interest is the trend line, and where that trend line crosses the y -axis. That  X  X ero requests X  value can then be interpreted as being the likelihood of relevance for a document that has not been reported into the pool at d = 100 by any of the 59 systems. Fitting a cubic polynomial to the data gives a crossing value of 0 . 043 . Over the whole TREC-9 judgment set (the qrels file), the probability of a document being relevant is 0 . 038 .

Based on these values, E =0 . 01 is not an overestimate. Note that this is not a claim that a randomly selected document in the entire collection has a 1 % chance of being relevant for a randomly selected topic, that is clearly excessive. The claim is that, of the documents selected into the top d = 100 by a retrieval system of quality comparable to a mid-range TREC one, of the documents that have not already been judged, around 1 % can be expected to be relevant. Metrics. The estimation methods described above can be applied to all weighted-precision metrics. Other members of this family include rank-biased precision (RBP) [12], which in principle has no cutoff k because of the geometric weights that are used, but in practice is evaluated over a finite ranking and hence always has a residual; and discounted cumulative gain [11], which, like precision, must be evaluated over a finite prefix. In the results below we use both RBP and scaled discounted cumulative gain , SDCG, in which the DCG score is divided by the DCG score that would be achieve by an  X  X ll relevant X  ranking of that depth, so as to obtain scores bounded above by 1 . 0 . Our goal with this investigation was to determine the extent to which the quality of the outcome of an IR experiment is affected by the factors discussed in the previous sections, namely: the volume of judgments performed; the choice of score estimation technique; and the choice of metric. To meas ure quality, we adopt the approach that has been employed by a number of authors [13,16,23]. Using TREC data, we compare pairs of systems, and count the fraction of them tha t yield a significant outcome according to the 50 -topic comparison. For any chosen metric, if one experimental regime results in a greater fraction of the system pairs being statistically separable in this way than does another, it is more sensitive.

The experimentation is based around the 105 system submissions over 50 topics that comprise the TREC-9 Web Track [9], and the subset of 59 systems that were used to form the pool for the relevance judgments. The qrels file contains ternary judgments rather than binary ones; in our experiment both the  X  X elevant X  (category 1) and  X  X ighly relevant X  (category 2) document were taken to be relevant in a binary sense. The qrels file contains 69 , 100 judgments, of which 2 , 614 or 3 . 8 %, are  X  X elevant X .
Two sets of system pairs were used in the comparisons. In the first set, each of the 59 runs that contributed to the poolin g was compared to each of the other 58 ,asaset of 1 , 711 system pairs. In the tables and graphs that follow, this set is called  X  59 -con X . The second set,  X  46 -non X , was generated by applying the same process to the other 46 systems, to create a set of 1 , 035 system pairs in which neither of the two systems had contributed to the judgment pool. This latter set represents a typical  X  X udgment reuse X  situation, in which two non-contributing runs are to be compared in a post-TREC exper-iment. To evaluate of the effect of pool depth on metric usefulness, we sorted the qrels file according to the minimum depth at wh ich each document appeared in any of the 59 contributing runs for each topi c, with random ordering applied to ties. This arrangement mimics the effect of pooling and allowed, for example, the first 1 , 000 qrels to be used, simulating a highly resource-limited experiment in which only shallow judgments were undertaken.
 Scores and Residuals. Table 1 gives initial results for this experimental framework. Part (a) of the table shows the average base scores B computed for the two sets of systems, using three different effectiveness metrics , and evaluated using shallow, medium, and deep pooled judgments. In all cases the use of the X S = B approximation leads to non-decreasing score estimates as the number of judgments employed increases, a useful behavior; and 10 , 000 judgments is mostly sufficient to get the X S scores to within around 10 % of the values attained at 69 , 100 judgments.

Table 1(b) lists the average residuals  X  associated with those base scores. Unsurpris-ingly, the 59 -con set of systems has smaller average residuals than the 46 -non systems, since the documents to be judged to make the partial relevance judgments were cho-sen from the 59 -con runs. Note also that rank-biased precision has non-zero residuals even when all 69 , 100 judgments are used. A critical obs ervation in Table 1(b) is that on the 46 -non systems the residuals are, for the most part, comparable in magnitude to the scores B that they relate to; that is, T  X  2 B . It must be concluded that there is considerable uncertain ty associated with the 46 -non systems, and that the base scores in Table 1(a)  X  which are also simplistic point values X S  X  may not be at all accurate. The third section of Table 1 shows the average of the X I point estimates. Now the initial estimates based on shallow and medium judgment pools are uniformly overestimates of the final scores. Taking X I as the point estimate  X  and assuming that documents at the tail of a run have the same density of relevance as documents at the start of it  X  is clearly too generous. Finally, Table 1(d) shows the smoothed scores X M for the same combinations of metrics and judgments. After 1 , 000 judgments the point estimates are now all below the at-69 , 100 values; and at 10 , 000 judgments they are all somewhat higher than the at-69 , 100 values, indicating a reasonable compromise between the two options, but also perhaps indicating scope for further refinement. Significance outcomes. Figure 2 shows system separability as the number of judged documents varies, evaluated over the 46 -non set of system pairs. The different curves within each graph correspond to different w ays (Section 3) of rendering each of the [ B, T ] ranges into a single score value. The ver tical axis records the fraction of the system pairs that were significant at p =0 . 01 . The effectiveness metrics used in the three graphs are P@ 10 , SDCG@ 100 , and RBP (with parameter p =0 . 95 ).

Except for the pessimistic interval-overlap method of handling the residuals, even as few as 1 , 000 judgments is sufficient to obtain relatively high rates of system sep-arability. Perhaps surprisingly, it is the simplistic approach X S and the background approach X B that yield the greatest separability, followed by the smoothed approach X
M . Note that the different approaches disagree on outcomes even after the  X  X ull X  set of 69 , 100 judgments have been used; the equivalent P@ 10 graph for the 59 -con set of runs shows all lines converging by the time 20 , 000 judgments are being used, because of the top-centric nature of the P@ 10 metric. On the 59 -con set the SDCG@ 100 curves also converge, but only after the full set of 69 , 100 judgments.

Table 2 gives more details of these separability coefficients. The Interpolated score estimation approach gives low separability, and is consistently less useful than the other point estimation methods. This inferior behavior is presumably a consequence of the fact that it badly overestimates the actual scores (Table 1). On the other hand, the sim-plistic X S approach provides confident assessments even when startlingly few docu-ments have been judged. There are only 223 relevant documents (including 76 highly relevant documents) in the first 1 , 000 positions of the TREC-9 qrels file in the order-ing that is used in Figure 2 and Table 2, meaning that with 1 , 000 judgments, 90 %ofthe relevant documents are not part of the comparison.

The success of the X S approach to score estimatio n is because pooling ensures that it is tantamount to evaluating the same metric, but at a shallower depth  X  for example, P@ 10 on the shallow judgment sets is somewhat similar to evaluating P@ 3 (see the residuals listed in Table 1), and what is being observed in the separability graphs is that P@ 3 is a reasonably effective mechanism for separating systems across a set of 50 top-ics. Similar arguments can be made for SDCG@ 100  X  it is sufficiently well correlated with SDCG@ 10 (say), and the pooling approach to judgment discovery sufficiently well focussed on the top of the system runs that the l atter is evaluated reasonable accurately. Convergence. The simplistic X S point estimate yields high separability rates even when only shallow judgments are being used, despite the actual effectiveness scores generated being under estimates. An important question then becomes the extent to which the significant pairs that are identified after 1 , 000 judgments remain significant as more judgments are processed. Table 3 evaluates these relationships, using the 46 -non system pairs, the three precision-based metrics, and the shallow, medium, and deep judgments.
To compute each value in Table 3, the set of system pairs that gave t -test p values less than 0 . 01 according to the test environment noted in the left-most column were then checked again in the context of the te st environment record ed in the heading of the other columns. For example, with 1 , 000 judgments performed, P@ 10 resulted in 713 system comparisons (of a total of 1 , 035 system pairs) being deemed significant at the 0 . 01 level. Of these, 50 (or 7 . 0 %) were not identified as being significant when 10 , 000 judgments were used; and 60 (that is, 8 . 4 %) system pairs were no longer found to be significant when all 69 , 100 judgments were employed. What is apparent in these results is that both X S and X B appear to be relatively stable in their selections of sig-nificant system pairs, with less than 10 %  X  X ecanting X  of previous significance as further judgments are employed. The X I approach has higher revision rates, even though it is more conservative in awarded significance when using the shallower pool depths. The latter effect is particularly marked for the two effectiveness metrics that carry out deep evaluations and are intended to reflect th e behavior of very patient searchers. Other effectiveness metrics. The use of three weighted precision metrics in the various evaluations presented in this paper is de liberate  X  with each of these three metrics, discovery of additional relevant documents can only increase the effectiveness score, and so the score that is attained on partial relevance judgments is a lower bound.
But other effectiveness metrics are also a menable to this treatment, with varying de-grees of credibility. Average precision (AP), defined as the average of the precision val-ues at the ranks at which relevant documents occur, is particularly challenging. Because it is an average over relevant documents, t he discovery of new relevant documents can reduce as well an increase partial scores. A similar observation holds for normalized discounted cumulative gain NDCG [11], in which the scaling factor is the best DCG score attainable given the number of relev ant documents available for each topic. Other researchers have also considered the issue of partial relevance judgments, and carried out experiments in which TREC (and other) judgments are scaled back and simulated retrieval comparisons carried out. Buckley and Voorhees suggest the use of a modified effectiveness metric denoted as bpref in which unjudged documents are by-passed [5]; an approach commented on by Sakai [14] and by Sakai and Kando [15]. The latter work includes discussion and evaluatio n of a wide range of effectiveness metrics. Aslam and Yilmaz et al. [1,2,22] sample the system runs in order to derive approximate values for average precision and other metr ics, and show that the variance of the esti-mate can be reduced as the number of samples increases. B  X  uttcher et al. [6] consider the related problem of determining and allowing for the bias in favor of pool-contributing systems. Webber and Park [21] have also considered this issue. Bompada et al. [3] consider the similarity of system orderings when compared using incomplete relevance judgments, and demonstrate that partially-evaluated NDCG is more self-consistent than Buckley and Voorhees X  bpref metric. Carte rette and Smucker [7] quantify the tradeoff between pool depth and pool breadth, and conclude that shallow pooling over many topics is almost certainly more powerful t han deep pooling over a restricted set of top-ics. Our work here, in which shallow judgmen t pools are demonstrated to still yield significant system comparisons, are a further validation of these various findings. We sought to explore the extent to which the use of incomplete relevance judgments affected retrieval system comparisons. It is clear that the X S approach of assuming un-judged documents to be irrelevant affects numeric effectiveness scores, and results in values that (for weighted-precision metrics at least) markedly underestimate the true values that would arise from a more costly evaluation. In this sense, it is appropriate to explore other estimation techniques; of the ones considered here, the smoothing ap-proach X M gives reasonable approximations, but still leaves room for improvement.
When the effectiveness scores are being developed purely as input to a t -test in order to carry out a paired system comparison, the X S method shed the disadvantage of being inaccurate, and provided consistently relia ble outcomes. That is, despite the fact that the scores it produces are a low-fidelity approximation of the eventual scores for that metric, the relativities observed in the system scores can be relied on, and experimental outcomes reasonably determined.

We next plan to undertake similar experiments with AP and NDCG; with different document orderings for the purposes of creating the judgment set; and using other sta-tistical tests, including in situations in which only small numbers of topics are in use. Acknowledgment. This work was supported by the Australian Research Council, and by the Government of Malaysia.

