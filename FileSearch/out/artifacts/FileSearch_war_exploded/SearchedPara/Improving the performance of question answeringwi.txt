 1. Introduction
Today, an overwhelming quantity of textual information is available in electronic form. This makes the development of semi-automatic tools to mine the content of such documents a necessity. To answer this need, the natural language processing (NLP) community has harnessed together formerly independent technologies rizers, and question answering systems.

In this paper, we present a technique that uses semantic constraints to improve a reformulation-based ques-tion answering system. The objective of the research was to automatically acquire answer extraction patterns with semantic constraints that perform as well as manually created ones. We show how we generate these pat-terns from sentences retrieved from the Web based on lexical, syntactic and semantic constraints. Once these constraints have been defined, we present a method to evaluate and re-rank candidate answers that satisfy these constraints using redundancy. The two approaches have been evaluated independently and in combina-tion. The evaluation on 493 questions from TREC-11 shows that the automatically acquired patterns increase the precision by 16% and the MRR by 26%, the re-ranking using semantic redundancy increases the MRR by 67%, and the two approaches combined increase the precision by 28% and the MRR by 73%. This new tech-nique allows us to avoid the manual work of formulating semantically equivalent reformulations; while still increasing performance.

This paper is organized as follows: Section 2 reviews previous work in the area. We present both general the manual formulations that we used for comparison, then Section 4 presents in detail how we generate the semantic patterns and how we evaluated them. Section 5 then discusses how the patterns can be used to re-rank the candidate answers and again, improve the performance of the system. Section 6 finally evaluates presented in Section 7 . 2. Related work 2.1. Question answering
Information Retrieval (IR) systems are designed to find documents that satisfy user X  X  information needs in large document collections (e.g. the Web). Given a bag of keywords, an IR system will retrieve the most rel-(NIST) have added a new Question Answering (QA) track to their competition-style Text REtrieval Confer-a 3 GB document collection and are required to return anything from a 250-byte text snippet to the exact answer to the question extracted from the document collection. For example, given the question, Who founded
American Red Cross? a QA system will search the document collection and extract the answer Clara Barton , the retrieved documents.

Following TREC X  X A, standard measure to evaluate QA systems is the mean reciprocal rank (MRR). RR  X  q  X  score of 1 3 . The overall system score is the mean of RR  X  q  X  for all questions q [3] . and today QA has become a major application of NLP. Over the years, the TREC X  X A task has attracted more and more teams and the requirements have become more and more challenging. Novel techniques have domain QA and Voorhees and Harman [7] presents a complete historical perspective of the TREC competitions. 2.2. Reformulation-based QA
A standard technique used in QA working on large document collections such as the Web is to use question
For example given the question Who founded American Red Cross? , a reformulation-based QA system will tem X  X  confidence in them.

Most work on reformulations has used patterns based on lexical, syntactic or named entity features (e.g. person-name, organization, etc.). However, only a few studies have worked on semantically equivalent refor-in finding a more precise set of candidate answers. However writing semantic reformulations by hand is a ically from natural language questions and use these constraints to re-rank our candidate answers to improve the performance of a QA system. 2.3. Related work on reformulation acquisition by hand and were among the best scoring team at the TREC-10 QA track [10] . Their work shows that if enough human resources are available, hand-crafted rules can produce excellent results. On the other hand,
Brill et al. [9] generated patterns automatically by using simple word permutations to produce paraphrases mulations. Most were ungrammatical, but since they were mapped onto a large document collection (the
Web), ungrammatical permutations will, in principle, retrieve nothing, and grammatical and fluent contexts will retrieve candidates that can be ranked by frequency.

Given the success of these first attempts, much progress has then been made to acquire reformulations auto-whether they are created manually or learned automatically. More recently, Aceves-Prez et al. [13] tried to use simple word permutations and verb movements to generate paraphrases for their multilingual QA system.
In the work of [14 X 16] , answer formulations are produced for query expansion to improve information retrie-tions into sets of effective search engine queries, optimized specifically for each search engine.
Ravichandran et al. [17] use a machine learning technique and a few hand-crafted examples of question X  onyms , to enhance their TextMap QA system. However, many of these patterns are manual generalizations of patterns derived automatically by [17] .

Kwok et al. [19] use transformational grammar to perform syntactic modifications such as Subject X  X ux probabilistic QA system. Here again, the paraphrases are syntactic variations of the original question.
Duclaye et al. [21] , however, do try to learn semantically equivalent reformulations by using the web as a linguistic resource. They start with one single prototypical argument tuple of a given semantic relation and ate this process to progressively validate the candidate formulations.

More recently, Molla ` [22] tries to learn how to answer a question based on matching a graph representation of the question with a graph representation of the answer. His system translates questions and potential answer sentences into a graph-based logical form representation (inspired by Sowa [23] conceptual graphs) and applies operations based on graph overlaps to compute their similarity. Because a logical form is used, lexical and syntactic constraints, but impose a semantic constraint on the main verbs of the questions and ity to the semantic relation that it carries as computed by WordNet.

In addition to work in QA, much work in the automatic acquisition of reformulation patterns has been done in the context of information extractions. However, here again, semantic features are often reduced dard vector space model. Patterns consist of predicate X  X rgument structures. The subject, verb, object (SVO) item or semantic category (named entity). 2.4. Summary of previous work tures. When searching a huge document collection such as the Web, having only lexical and syntactic refor-mulations may be enough because the collection exhibits a lot of redundancy. However, we believe that in a smaller collection, semantic reformulations are necessary.
 Work in pattern acquisition that includes semantic features, usually only takes into account named entities. tions and answer sentences, but does not impose semantic constraints on the relations. Compared to Molla ` [22] , our work uses a more flexible and easy to compute question and answer sentence representations, yet imposes semantic constraints on the main verbs. We do not force a semantic match on the concepts of the sentences (only lexical, syntactic and named entity tags are used), but we do force a semantic match on the relations using WordNet X  X  hyponym/hypernym hierarchy. 3. The manual patterns Our work builds on our current reformulation-based QA system called lations were hand-crafted and only relied on named entities for semantic constraints.

Given a question, QUANTUM needs to identify which answer pattern to look for. It therefore uses two types of didate answer. For example, the question Who is George Bush? will be matched to the question pattern Who Vsf
PERSON? which will trigger the search for any one of these answer patterns in the document collection: h
QT ih Vsf ih ANSWER i h
ANSWER ih Vsf i by h QT i simple form.
 at least one formulation template is applicable for a question. In the current implementation, both question and answer patterns are based on named entity tags (e.g. PERSON ), part-of-speech tags (e.g. Vsf ), tags on strings (e.g. QT , ANY-SEQUENCE-WORDS ) and specific keywords (e.g. Who , by ). The templates generate formulations are produced per question.

These hand-made patterns will be used later for comparison in order to evaluate the patterns that we learned automatically. 4. Generating semantically equivalent patterns 4.1. Overall methodology
To generate semantically equivalent answer contexts, we try to find sentences from the Web that contain the corpus of question X  X nswer pairs from which we learn how to generalize each type of question. Each question X  tences through a part-of-speech tagger and a noun phrase chunker to generalize them.

More specifically, each question X  X nswer pair of the training corpus is processed to: (1) Extract the main arguments from the question. (2) Extract the main arguments from the answer. (3) Extract the relation between the question arguments and the answer arguments. The semantic constraint (4) Generate syntactico X  X emantic patterns for the answer sentences.
Let us now explain each step in details. 4.2. The training corpus
The training corpus consists of 1343 question X  X nswer pairs taken from the TREC-8, TREC-9, and TREC-10 collection data [2,27,10] . Each question X  X nswer pair is composed of one question and its corresponding answer. For example: Q: Where is the actress, Marion Davies, buried? A: Hollywood Memorial Park Q: When did Nixon die? A: April 22, 1994 Q: Who is the prime minister of Australia? A: Paul Keating
We divided the training corpus according to the question type. We used the classification used in [28] to categorize questions into seven main classes (what, who, how, where, when, which, why) and 20 subclasses (e.g. what-who, who-person, how-many, how-long, etc.). Fig. 1 shows the proportion of each type of questions in the training set. 4.3. Sentence retrieval
To generate semantic contexts, for each question X  X nswer pair, we define an argument set as the set of terms which a relevant document should contain. For example, consider the question X  X nswer pair: Q: Who provides telephone service in Orange County, California? A: Pacific Bell Any relevant document to this question X  X nswer pair must contain the terms  X  X  telephone service  X  X ,  X  X  Orange made up of all the arguments found in the question X  X nswer pair. The argument set is made up of all the base noun phrases in the question (found using the BaseNP chunker [29] ).

In the TREC-8 X 11 collections, the answers are typically constituted of a noun phrase. However, some sup-search for a combination of question arguments and each sub-phrase of the answer. We restrict each sub-phrase to contain less than four words and to contain no stop word. Finally, we assign a score to each ple, the sub-phrases and the score assigned for the previous question X  X nswer pair are: { Pacific Bell 1, the retrieved sentences.

Once the argument set is built, we construct a query using all the arguments extracted from the question, answer argument. 4.4. Semantic filtering of sentences that they contain. To do this, we need to find sentences that contain equivalent semantic relations holding between question arguments and the answer. We assume that the semantic relation generally appears as the ing question X  X nswer pair: Q: Who provides telephone service in Orange County, California? A: Pacific Bell
To check semantic equivalence, we examine all verbs in the selected sentences for a possible semantic equiv-alence using WordNet. We check if the main verb of the sentence is a synonym, hypernym, or hyponym of the original verb in the question.
 we also validate nouns and adjectives because the semantic relation may occur as a nominalization or another and then we check if it is equivalent to the stem of the original verb or one of its synonyms, hypernyms, or hyponyms. 2 For example, with our running example, both these sentences will be retained: Sentence 1 Pacific Bell, major provider of telephone service in Orange County, California ... 4.5. Generating the answer contexts
Once we have identified a set of semantically equivalent sentences, we try to generalize them into a pattern words except prepositions are removed. For example, the following sentence chunked with NPs: [California X  X /NNP Baby/NNP Bell,/NNP SBC/NNP Pacific/NNP Bell,/NNP]/NP still/RB provides/VBZ nearly/RB all/DT of/IN [the/DT local/JJ phone/NN service/NN]/NP]/NP in/IN [Orange/NNP County,/NNP California./NNP]/NP will generate the following pattern: h ORGANIZATION ih VERB ih QARG1 i in h QARG2 i | senseOf (provide)
The constraint senseOf (provide) indicates the semantic relation to be found in the candidate sen-tences through a verb, a noun or an adjective. 4.6. Weighting the patterns
As one pattern may be more reliable than another, the last challenge is to assign a weight to each candidate defined to have values between 0 and 1.
 compute each of the following factors:
Sub phrase score is the score of the candidate answer sub-phrase. The score of each answer sub-phrase Sem sim  X  V Q ; S P i  X  measures the similarity between the sense expressed in the candidate pattern  X  S The final weight of a pattern is based on the combined score of the previous four factors computed as: For the TREC-8, 9, 10 question sets, a list of 98 ranked patterns were created automatically by the system.
This can be compared to the bag of 77 hand-made patterns in the original system created using the TREC-8, 9 question set. 4.7. Evaluation
The system was implemented as a cascade of Perl scripts using the Google search engine. It was developed ular. Our main goal was to evaluate the quality of the results.

We evaluated the automatically created patterns using the 493 questions-answers of the TREC-11 collec-tion data [33] . The use of only the TREC question set for training and evaluating does create a bias in the system. The TREC X  X A question set contains factoid questions that are mostly answered by short noun phrases. Our patterns (both manual and induced) were developed using the TREC questions and are thus with a different corpus would probably lead to a lower MRR, especially if the questions are not factoid in nature. For example, more explanation-based or why / how questions as in closed-domain QA would have been harder to deal with.
The system was evaluated with the original 77 hand-crafted patterns and with the 98 learned ones; then the answers from both runs were compared. Table 2 shows the result of this comparison based on precision of the procal rank (MRR). The evaluation shows an increase in precision of about 16% with the generated patterns (from 0.50 to 0.58). This shows that the semantic constraints have filtered out some bad candidates that the original patterns accepted. The MRR, which takes the order of the candidates into account, increased by 26% from 0.32 to 0.40. In addition, since the patterns are generated automatically, no manual work is now necessary. 5. Semantic candidate re-ranking
A further analysis of the results, however, showed that although the semantic constraints imposed by the new patterns filtered out noisy candidates, quite a few bad answers still remained. This is because at least one document contained the semantic relation and the question arguments in the same sentence. Our next goal was then to improve these results by filtering out noisy candidates and re-rank the remaining candi-dates better.

To re-rank the candidates, we used a redundancy technique, but this time, based on the satisfaction of the semantic constraints. That is, we evaluate how many times the candidate answer satisfies the semantic con-the same sentence as the question arguments by chance, it should thus be given a lower rank or be removed completely. Let us describe this process in detail. 5.1. Sentence retrieval
We first run the QA system on the Web and retrieve its top 200 answer candidates. This first run can be California , the system retrieves the following candidates: Southwestern Bell Pacific Bell
Similarly to our approach for learning reformulations, we build a set of argument tuples composed of the from the noun phrases of the question and the candidate found by the QA system. In our example, the fol-lowing tuples are created: ( X  telephone service  X ,  X  Orange County , California  X ,  X  Southwestern Bell  X ) ( X  telephone service  X ,  X  Orange County , California  X ,  X  Pacific Bell  X )
Once we have built the set of argument tuples, we search for them in the document collection to identify the possible semantic relations relating them, and make sure that the relation that relates them in the documents is equivalent to what we were originally looking for in the question ( senseOf (provide) ).

In our experiment, we submitted all the tuples to the Web to find paragraphs that contained these tuples. Then we extracted only the paragraphs where both tuple elements are at a distance of N words or less (in our experiment, N  X  5). We used a context window size of N words between the tuple elements and N words on each side of them in the extracted paragraphs and then examined the words in these context windows for a possible similar semantic relation. This is shown in Fig. 2 . 5.2. Evaluating the semantic relation
Finally, we evaluate the relations expressed in the context windows to identify if at least one is seman-tically equivalent to the original semantic relation expressed in the question. To verify the semantic rela-in any context window is a synonym, a hypernym or a hyponym of the original verb in the question. If no verb has an equivalent semantic relation, we then back-off to validating nouns and adjectives. Any tuple that does not have a similar semantic relation in the question and in the documents is discarded. Thus if a candidate had been selected in the first QA run, but no further evidence is found in the re-ranking phase, it is filtered out. 5.3. Re-ranking candidates
The remaining candidates are re-ranked according to the proportion of passages in the collection contain-ing the same relation. For example, when we submitted the tuple ( X  X elephone service X ,  X  X range County, California X ,  X  X acific Bell X  ), we found 110 passages containing the elements of the tuple.
Among these, only 24 contained the tuples and the relation senseOf (provide) within five words of each other. We therefore gave a rank of (24/110) to the candidate Pacific Bell . By applying this procedure to all the argument tuples, all candidates can be easily re-ranked. 5.4. Evaluation We evaluated the semantic re-ranking alone again with the TREC-11 data. Table 3 shows the results.
Here again, the MRR improved (by 67%). This means that the candidates found are better ordered in the list so as to move the correct answers up in the list. In fact, with the TREC-11 data, 42% of correct answers were moved up in the candidate list by 3.8 positions on average while 4% were actually ranked worse by 5.7 positions.
 6. Evaluation of the combined approach 6.1. Results
Finally, we evaluated the combined approach: automatically acquired semantic patterns (Section 4 ) and in Tables 4 and 5 .

As Table 4 shows, with the combined approach ( A + B ), the MRR increased by 73% compared to the ori-when doing QA on a different domain or a new language.

Table 5 shows the details of the evaluation for each type of question. The proportion of each type of ques-how questions are particularly higher (the MRR more than doubled). We speculate that these types of ques-sufficient to cover a larger number of answers.
 6.2. Discussion
The evaluation of the QA system presented in the previous section shows that the acquired patterns improve the performance of the system. However, other more qualitative evaluations can be done. In the fol-lowing paragraphs, we will discuss a few interesting issues.
 Expressivity and ease of maintenance:
The generated patterns use the same formalism and vocabulary as the manual ones. In fact, this allows us to turn the automatic patterns on and off, and use each type interchangeably in the system. The automatic pat-terns are therefore as easy (or as hard) to read and modify as the manual ones.
 Complementarity of the manual and the induced patterns:
An interesting question is to determine if the manual patterns and the induced ones cover the same ques-lations (whether they are hand-made or generated).
 tions; whereas the induced patterns improve how , what and who questions the most. As mentioned earlier, we suspect that who and how (e.g. how many , how much , etc.) questions are answered by sentences that are more patterns do not answer where questions as one would have expected from the results of the hand-crafted pat-terns; a more detailed failure analysis is needed to understand why this is so.

As Table 6 shows, the two types of patterns do improve a few common question types, but they seem to complement each other for other types of questions. For those questions, one would expect that using the two types of patterns together should produce an even higher MRR. However, we have not tested this hypoth-esis experimentally, and a more detailed analysis would be needed.
 Scalability of the approach:
The approach presented here has been developed and tested with the TREC open-domain factoid ques-example with more explanation-based questions, the answer will not necessarily be formed of a short noun-phrase and within a strongly stereotypical answer pattern. Also, the approach needs a large document collec-much smaller [34] , it is not clear if the approach will be able to learn useful patterns.
On the other hand, the fact that semantic-based answer patterns can be learned automatically without degrading the system X  X  performance compared to hand-crafted patterns makes the approach portable to other application domains and other languages.
 Limitations of the approach:
As opposed to several other approaches that use the Web for answer redundancy; our approach is less strict crude; we only look for lexical, syntactic and semantic evidence within a window of words. We do not attempt to unify a proper semantic representation of the questions with that of candidate sentences. We chose not to ever, as we look for evidence anywhere in a window of words, we are more sensitive to mistakes. We are only
Negations and other modal words may completely change the sense of the sentence. When looking in a very large corpus such as the Web, this may lead to more noise. 7. Conclusion and future work
We presented a novel method for acquiring reformulation patterns automatically based on lexical, syntactic and semantic features then used these semantic constraints to re-rank the list of candidate answers using redundancy.

The experimental evaluation shows that using new semantic patterns increases the performance or our QA system. Together, the automatically acquired semantic patterns and the candidate re-ranking improve the per-formance significantly (28% for precision and 73% for MRR) and remove the need for human intervention. It is therefore very interesting when porting the system to a new language or a new domain.
The current implementation only looks at semantic relations holding between two or three arguments. It ation would be necessary to ensure that the approach does not introduce too many constraints and conse-quently filters out too many candidates.

Another interesting question is to what degree the results are bound to the thresholds we have used. For if or how changing this value will affect the results.

This work tried to learn answer patterns automatically without degrading the quality of the QA results. We essary to measure such things as response time and scalability to a real application.
 Acknowledgements
This project was financially supported by the Natural Sciences and Engineering Research Council of Can-ada (NSERC) and Bell University Laboratories (BUL). The authors would like to thank the anonymous ref-erees for their valuable comments.

References
