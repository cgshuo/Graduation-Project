 We discuss information retrieval methods that aim at serv-ing a diverse stream of user queries. We propose methods that emphasize the importance of taking into consideration of query difference in learning effective retrieval functions. We formulate the problem as a multi-task learning prob-lem using a risk minimization framework. In particular, we show how to calibrate the empirical risk to incorporate query difference in terms of introducing nuisance parameters in the statistical models, and we also propose an alternating optimization method to simultaneously learn the retrieval function and the nuisance parameters. We illustrate the ef-fectiveness of the proposed methods using modeling data extracted from a commercial search engine.
 H.3.3 [ Information Systems ]: Information Search and Retrieval X  Retrieval functions ; H.4.m [ Information Sys-tems ]: Miscellaneous X  Machine learning Algorithms, Experimentation, Theory relevance, retrieval function, machine learning, query depen-dence, least-squares regression, regularization
We consider D , the set of all the documents in consid-eration, L the set of labels which can be either a finite or infinite set, and Q the set of all potential user queries. We model each query q  X  X  as a probabilistic distribution P q over D X L , P q ( d, ` ) , d  X  X  , `  X  X  which specifies the probability of document d being labeled as ` under query q . Now we define a loss function L over the set L X L , L : L X L7 X  X  1 + , the set of nonnegative real numbers, and we also specify a class of functions H from which the retrieval function will be extracted, where for h  X  X  , h : Q X D7 X  X  . For each query q , we can then specify a learning problem (classification or regression problem): find h  X  q  X  X  such that
The goal of information retrieval, is to learn a retrieval function h  X  that will be good for all the queries q  X  X  . Therefore, we need to deal with potentially infinite number of related learning problems, each for one of the query q  X  X  . To this end, we specify a distribution over Q : P Q ( q ) can indicate, for example, the probability that a specific query q is issued to the information retrieval system which can be approximated. Then the optimization problem we need to solve is for the combined risk,
In practice, we sample a set of queries { q i } Q i =1 from the distribution P Q , and for each query q , we also sample a set of documents from D for labeling to obtain where l qj  X  X  are labels obtained from human judges for example after relevance assessment. Ideally, the sampling of the queries should be according to P Q and the sampling of the documents for each query q should be according to P , the former is relatively easy to do while the later is a much more difficult issue. The optimization problem for the empirical counterpart of (1) is where we also add a regularization term to control the com-plexity of h with  X ( h ) measuring the complexity of h , and  X  is the regularization parameter that balances the fit of the model in terms of the empirical risk and the complexity of the model.
To be concrete, we consider the risk minimization problem in the context of regression, i.e., we assume the labels are real numbers, and to be consistent with convention, we will use y qj to denote the label ` qj . We assume the labeled set here x qj denote the feature vector for the query-document pair { q, d qj } . To this end, we seek to find a function h  X  X  to minimize the following empirical risk, To incorporate query-dependent effects, we can consider the following modified empirical risk, and we seek to find h and g , q = 1 , . . . , Q , to minimize where g q (  X  ) is a general monotonically increasing function, and g = [ g 1 , . . . , g Q ], i.e., we seek The intention is that the function g q incorporate the dif-ference for queries when using h ( x ) to predict the label y . From another viewpoint, for suitably chosen g q , we seek to find h ( x ) to match g  X  1 q ( y ). In this work we focus on the simple case where g q (  X  ) is a linear function, i.e., g  X  +  X  q x, q = 1 , . . . , Q with  X  q  X  0.

As we mentioned before, to control the size of the param-eters  X  q ,  X  q and the complexity of h , we also need to add regularization terms to the modified empirical risk (2) to obtain the regularized empirical risk where  X  = [  X  1 , . . . ,  X  Q ] and  X  = [  X  1 , . . . ,  X  and  X  h are regularization parameters, and k X k p is the p norm of a vector. We will only consider the case for p = 1 or p = 2. In summary, we seek to find In general, we will not impose a parametric form for the function h , and we will employ the methodology of coor-dinate descent (alternating optimization) to solve the opti-mization problem (3). Specifically, we will alternate between optimizing against h and optimizing against  X  and  X  . The regularization parameter  X  h will be determined during the nonlinear regression process for finding h discussed below while regularization parameters  X   X  and  X   X  will be deter-mined by cross-validation. In what follows, we will refer the above algorithm as adaptive Target Value Tranforma-tion (aTVT). We will compare aTVT against a particular system that uses nonlinear regression to learn a retrieval function based on the gradient boosting methods.
In this section we report some experimental results on data generated from a commercial search engine. In our experiments, a set of queries are sampled from query logs, and a certain number of query-document pairs are labeled according to their perceived relevance judged by human ed-itors. The labels are mapped to numerical values and the goal is to learn a retrieval function that can best mimic the Table 1: Number of queries and query-url pairs on US and CN datasets Table 2: The dcgs and percentage dcg increases of retrieval function with aTVT over without aTVT on the English data set, and p values for different regularization parameters:  X   X  and  X   X  in the L 2 case. Notice that the dcg for retrieval function without aTVT are 11.30.  X   X  =1 11.48(+1.55%,0.01) 11.49(+1.68%,0.006) human judgment process. In Table 1, we list some basic statistics for the two data sets.
 In this work, we use the recently popularized Discounted Cumulative Gain (DCG) methodology which seems to be more appropriate for assess relevance in the context of search engines. For a ranked list of N documents, we use the fol-lowing variation of DCG, where G i represents the weights assigned to the label of the document at position i . We will use the symbol dcg to indicate the average of this value over a set of queries in our experiments.

Table 2 lists the dcg for retrieval function with aTVT as compared to retrieval function without aTVT in the L 2 case for the English data set (similar results hold for the chinese data set). The percentage dcg gains and the p -values from Wilcoxon signed rank tests are also presented. From thetable, we can see aTVT gives statistically significant dcg gains. The optimal regularization parameter combinations give about 2% dcg gain for the English data.
There are many ways to incorporate query difference in learning retrieval functions, the approaches of constructing appropriate query features being one of them even though it is usually not looked at from this viewpoint. In this pa-per, we present an approach through modifications of the empirical risk using nuisance parameters to accommodate the effects of query difference. The approach can be used even when there are query features contained in the feature vector of query-document pairs.
 We discuss information retrieval methods that aim at serv-ing a diverse stream of user queries such as those submitted to commercial search engines . We propose methods that emphasize the importance of taking into consideration of query difference in learning effective retrieval functions. We formulate the problem as a multi-task learning problem us-ing a risk minimization framework. In particular, we show how to calibrate the empirical risk to incorporate query dif-ference in terms of introducing nuisance parameters in the statistical models, and we also propose an alternating opti-mization method to simultaneously learn the retrieval func-tion and the nuisance parameters. We work out the details for both L 1 and L 2 regularization cases, and provide con-vergence analysis for the alternating optimization method for the special case when the retrieval functions belong to a reproducing kernel Hilbert space. We illustrate the effective-ness of the proposed methods using modeling data extracted from a commercial search engine. We also point out how the current framework can be extended in future research. H.3.3 [ Information Systems ]: Information Search and Retrieval X  Retrieval functions ; H.4.m [ Information Sys-tems ]: Miscellaneous X  Machine learning Algorithms, Experimentation, Theory relevance, relevance judgment, retrieval function, WWW search, discounted cumulative gain, machine learning, gra-dient boosting, risk minimization, query document feature, Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. query dependence, query specific feature, alternating op-timinization, least-squares regression, quadratic program-ming, regularization
Designing effective and efficient retrieval functions 1 is cen-tral to information retrieval, especially in the context of serving many user queries subm itted to commercial search engines. Many retrieval models and methods have been proposed in the past, including vector space models, prob-abilistic models and the more recently developed language modeling-based methods, with varying degree of success [28, 29, 27, 1, 24, 25, 31, 16]. In particular, algorithms and tech-niques from machine learning have been applied to informa-tion retrieval long before the recent advances of the World Wide Web [13, 14, 6, 15, 4, 17]. One key issue in designing retrieval functions is to find efficient and effective representa-tions of query-document relations. In this regard, Fuhr and coworkers considered dividing the design space for query-document representations into query-oriented, document-oriented and feature-oriented schemes [13, 14, 15]. They ad-vocated the use of feature-oriented method for probabilistic indexing and retrieval whereby features (relevance descrip-tions) of query document pairs such as the number of query terms, length of the document text, term frequencies for the terms in the query, etc. are extracted, and (least-squares) regression methods and decision-trees are used for learning the retrieval functions based on a set of query-document pairs represented as feature vectors with relevance assess-ment [13, 14, 15]. In a related work, Cooper and coworkers have developed similar approaches and used logistic regres-sion to build the retrieval functions and experimented with several retrieval tasks in TREC [6, 17] (see also [16]).
Compared with the query-oriented method and document-oriented method, the greatest advantage of feature-oriented method is its capacity at maintaining large effective sam-ple sizes. This is because the sample space for the feature-oriented method is the collection of all the feature vectors and two different query-document pairs can generate the same or similar feature vectors. The query-oriented method seems to be quite adequate or even advantageous especially in traditional information retrieval where retrieval functions
By retrieval functions we mean functions used to rank doc-uments in response in user queries. are usually designed to work for small and/or homogeneous text corpora. For more diverse corpora such as the World Wide Web and with the introduction of non-textual features including the information from hyperlinks such as anchor texts and user click-through data, it is more likely that sim-ilar feature vectors correspo nding to different queries are labeled very differently in terms of relevance assessment. Falling back on the query-oriented method or document-oriented method is not an option because the labeled data with relevance assessment will become even more sparse in the enlarged sample space under those methods, consider-ing that it is usually very expensive to acquire data with high-quality relevance assessment [14, 15]. To overcome this difficulty, we propose to incorporate query difference to sufficiently disambiguate feature vectors while maintaining the general framework of the feature-oriented method. The starting point for our work is the formulation of the infor-mation retrieval problem as a m ulti-task learning problem where the retrieval problem for each query is considered as a separate classification/ranking problem. The goal is to min-imize the expected retrieval risk over the whole set of user queries (Section 2). The retrieval problems for the individ-ual queries are closely related to each other, and in fact, in feature-oriented method, the individual retrieval problems are not sufficiently distinguished from each other at all. In section 3, we develop some novel schemes that can incor-porate query difference in the feature-oriented framework. This is done by introducing query dependent parameters in the empirical risk which are separated from the retrieval function. We discuss the details of solving the optimization problem and issues on regularizations to control the size of the query dependent parameters. A convergence anal-ysis is also given for the case when the retrieval functions are from a reproducing kernel Hilbert space. In section 4, we present several experimental results using data extracted from a commercial search engine. In section 5, we provide some concluding remarks and give pointers to topics for fu-ture research.
In this section we present a general formulation of the information retrieval problem using the risk minimization framework (a related work using risk minimization but em-phasizing document generation models and queries from in-dividual users was presented in [32]). We consider D ,the set of all the documents in consideration, L the set of labels which can be either a finite or infinite set, and Q the set of all potential user queries. 2 We model each query q  X  X  as a probabilistic distribution P q over D X L , which specifies the probability of document d being labeled as under query q . This probabilistic setting also includes the case where there is a deterministic function which specifies the  X  X orrect X  label f q ( d )foreachdocument d  X  X  .
To be more realistic, the model should also contain a tem-poral component to account for the time evolution of the document collection and user query stream.

Remark. With labels associated with documents, we are using the absolute relevance framework where judgments are made with respect to the fact that whether a document is or is not relevant to a query. We mention that there is also the possibility of using relative relevance judgment framework where judgments are in the form of whether a document is more relevant than other documents [20, 21, 22]. In certain situations, we may encounter judgments of both forms, abso-lute relevance judgments from human judgment process and relative relevance judgments e xtracted from click-through data.
 Now we define a loss function L over the set L X L , the set of nonnegative real numbers, and we also specify a class of functions H from which the retrieval function will be extracted, where for h  X  X  , For each query q , we can then specify a learning problem (classification or regression problem): find h  X  q  X  X  such that where the expectation E P q is with respect to the probability distribution P q corresponding to the query q .

The goal of information retrieval, however, is not just to learn a retrieval function h  X  q for some individual query q , but rather to learn a retrieval function h  X  that will be good for all the queries q  X  X  . Therefore, we need to deal with po-tentially infinite number of related learning problems, each for one of the query q  X  X  . To this end, we specify a distri-bution over Q : P Q ( q ) can indicate, for example, the prob-ability that a specific query q is issued to the information retrieval system which can be approximated, for example, by its frequency in the query logs of queries submitted to the information retrieval system in the past. Then the opti-mization problem we need to solve is for the combined risk,
In practice, we sample a set of queries { q i } Q i =1 from the distribution P Q , and for each query q ,wealsosampleaset of documents from D for labeling to obtain where l qj  X  X  are labels obtained from human judges for example after relevance assessment. Ideally, the sampling of the queries should be according to P Q and the sampling of the documents for each query q should be according to P , the former is relatively easy to do while the later is a much more difficult issue. The optimization problem for the empirical counterpart of (1) is In general we also add a regularization term to control the complexity of h with  X ( h ) measuring the complexity of h , and the optimization problem becomes here  X  is the regularization parameter that balances the fit of the model in terms of the empirical risk and the complexity of the model [9].

An important consequence of the above framework is that we are simultaneously dealing with multiple learning tasks each for an individual query, and there are substantial cor-relations and overlaps among those learning tasks and it is important to explore the solution of (1) in the context of multi-task learning. The feature-oriented method where the feature vector does not contain any query features can be considered as an extreme form of multi-task learning for (1) where the individual learning problems loss their identity and become a single learning problem. The main objective of this paper is to work within the multi-task learning by extending the feature-oriented method.

Remark. Many loss functions have been proposed for both classification and regression problems. For example, for the binary classification problem, we can use the expo-nential loss used in Adaboost [10] where  X  X  X  1 , 1 } denotes the class labels, and for the re-gression problem, we can use the squared-error loss, where is a real number.

Remark. Instead of considering classification and regres-sion problems, we can also consider directly the ranking problem for information retrieval: Each query corresponds to an ideal ranking function R q defined on D .Let R be a class of ranking functions, we define a loss function L that measures the difference between two ranking functions, then the optimization we need to solve is A recent work along those lines can be found in [3].
To discuss the subtle issue of query difference, we first consider a simple illustrative example. Consider two queries is more popular than q 2 generating about 13 million search results while results for q 2 are two orders of magnitude less. For simplicity we consider a retrieval function h ( x )usinga single feature x which counts the number of inbound links to a document. Now let us examine the top three results d 1 ,d i 2 ,d i 3 ,i =1 , 2 , for each of the query. Assume d ranked perfect, d i 2 is ranked excellent, and d i 3 is ranked good, and we convert the labels to numerical values as fol-lows, Since q 1 is very popular, each of the top three results gen-erate high feature values, say, while for q 2 the corresponding feature values are Assume x is negatively correlated with the label, i.e., small x values tend to indicate better relevance, then we will need to find a monotonically decreasing function h such that for and for q 2 , This is, however, impossible to do. As we can see the major issue comes from the difference of popularity of the queries and we need a framework for designing retrieval functions that can take this difference into account. The above is a simple illustrative example and for a single query we can use simple scaling to achieve the same effects. But in real-ity, a retrieval function involves several thousands or more features and simple scaling will not work. To this end, we discuss the issue in more formal terms.

To be concrete, we consider the risk minimization problem in the context of regression, i.e., we assume the labels are real numbers, and to be consistent with convention, we will use y qj to denote the label qj . We also use the squared-error loss function. Since we operate under the feature-oriented framework, we assume the labeled set is represented as here x qj denote the feature vector for the query-document pair { q, d qj } . Before we proceed to formally discuss the problem, we want to address an important and subtle issue. For a query q , we say a feature is query-dependent if it has the same value across all the documents d  X  X  . For exam-ple, the number of terms in q is a query-dependent feature. Other features are called document-dependent and query-document-dependent. So we can split the feature vector x qj into two parts with the first component indicates the set of query-dependent features (more details on those three types of features can be found in Section 4.1). One extreme form of a retrieval function is a function that only depends on x D qj ,x QD qj is used for ranking documents for all the queries. At the other end of the spectrum is to have one function for each query q  X  X  . On the other hand, to have a single function for all queries and using x Q qj to disambiguate the queries seems to be quite natural. But the problem with this ap-proach is that it is not known a-priori what set of query-dependent features are adequate for this purpose; for ex-ample, just using the number of query terms in a query is certainly too coarse. Our philosophy instead is to let the data implicitly capture this set of adequate query-dependent features and completely bypass its explicit construction. In particular, we allow the feature vector to contain any num-ber of query-dependent features or none at all.

To this end, we seek to find a function h  X  X  to minimize the following empirical risk, We will deal with regularization issue momentarily. To in-corporate query-dependent effects, we can consider the fol-lowing modified empirical risk, and we seek to find h and g ,q =1 ,...,Q , to minimize where g q (  X  ) is a general monotonically increasing function, and g =[ g 1 ,...,g Q ], i.e., we seek The intention is that the function g q incorporate the dif-ference for queries when using h ( x )topredictthelabel y . From another viewpoint, for suitably chosen g q , we seek to find h ( x )tomatch g  X  1 q ( y ). This has some flavor of response transformation used in general regression analysis [5]. How-ever, in our case, the response transformation g  X  1 q (  X  fixed in advance but is learned from the data, and it is also query dependent.

Now for a new query q n which is not in the given la-beled set, there is also a corresponding function g q n .How-ever, the retrieval function is used for ranking the documents and since g q n is monotonically increasing, rankings based on g q n ( h )and h are exactly the same, and for using the latter, there is no need to know g q n .

In this work we focus on the simple case where g q (  X  )isa linear function, 3 i.e., with  X  q  X  0. The query dependent parameters  X  q and  X  q are nuisance parameters because they do not appear in the retrieval function h . Interestingly, even though we incorpo-rate query dependent parameters in learning the retrieval function, to assess the relevance for a document d  X  X  ,we simply look at the learned retrieval function h . For exam-ple, to rank a list of documents d i  X  X  ,i =1 ,...,n ,fora query q , we can simply sort them according to the values h ( x i ) ,i =1 ,...,n ,where x i is the feature vector for the query document pair { q, d i } .

As we mentioned before, to control the size of the param-eters  X  q , X  q and the complexity of h , we also need to add regularization terms to the mo dified empirical risk (2) to obtain the regularized empirical risk where  X  =[  X  1 ,..., X  Q ]and  X  =[  X  1 ,..., X  Q ], and  X   X  and  X  h are regularization parameters, and  X  p is the p norm of a vector. We will only consider the case for p =1 or p = 2. In summary, we seek to find
In general, we will not impose a parametric form for the function h , and we will employ the methodology of coor-dinate descent (alternating optimization) to solve the op-timization problem (3) [2]. Specifically, we will alternate
We can use more complicated monotone functions by using more parameters or even monotone functions in nonpara-metric form. between optimizing against h and optimizing against  X  and  X  . The regulariza tion parameter  X  h will be determined dur-ing the nonlinear regression process for finding h discussed below while regularization parameters  X   X  and  X   X  will be de-termined by cross-validation (see section 4 for more details).
For fixed  X  and  X  , and for simplicity, we assume  X  q &gt; 0, define the modified residuals Then we seek to find h that solves the following weighted nonlinear regression problem Many nonparametric fitting algorithms can be used to find h including MARS, the recently proposed boosting meth-ods, and the general kernel-based methods [10, 11, 12]. For more detailed discussion in the context of learning retrieval function see [7, 8]. In section 3.3, we present a detailed discussion for the kernel-based methods. Here we briefly de-scribe the general framework of gradient boosting, assuming we are given a training set { x i ,y i } N i =1 with a loss function ( y  X  h ( x )) 2 [12]. The gradient boosting algorithm consists of the following steps. 1. Initialize h 0 ( x )= 2. For i=1,. . . , M: (number of trees in gradient boosting)
There are two parameters M , the number of regression trees and  X  , the shrinkage factor that need to be chosen by the user. We use cross-validation for choosing the two parameters.
Next we consider for fixed h , how to optimize against  X  and  X  . We need to solve the optimization problem min It is easy to see that we can break the above into Q separate optimization problems, for q =1 ,...,Q , We distinguish two cases in what follows.
For p = 2, the component  X  q and  X  q of  X  and  X  can be found by solving the following least squares problem, where to force nonnegativity on  X  q , we control the size of  X   X  q =  X  q  X  1 instead of  X  q and consequently,  X  q will be around 1. Let the data matrix for query q and the response be
X The optimal  X  q and  X  q can be obtained by solving the fol-lowing normal equation, here superscript T denotes matrix transpose.
For p = 1, we need to solve the following optimization problem, min We can turn the above into a quadratic programming prob-lem. To this end, we introduce the following notation, for a real number r , we define Then it is easy to see that r = r +  X  r  X  and | r | = r + Let  X   X  q =  X  q  X  1, and we rewrite (4) as subject to the constraints The above optimization problem is in the standard form of convex quadratic programming and can be solved by the available methods discussed in [26].
In this section, we present a convergence analysis of the above alternating optimization method for optimization prob-lem (3). We specialize to the case when H the set of func-tions in (3) is restricted to a reproducing kernel Hilbert space (RKHS) [30]. We first give a very brief description of RKHS just to introduce the relevant notations. A RKHS is charac-terized by a continuous, symmetric, positive definite kernel function K (  X  ,  X  ), where X is the domain of the functions, and K satisfies For a fixed x  X  X  , K ( x,  X  ) is a function on X .Consider the linear space H 0 spanned by { K ( x,  X  ) ,x  X  X } , i.e., consists of finite linear combination of the form f ( x )= P Define an inner-product on H 0 by It is easy to see that and this is where the name reproducing kernel comes from. Now we can define H K the RKHS uniquely associated with K as the completion of H 0 under the norm f 2 K = &lt;f,f &gt; For a fixed kernel function K ,wereplace H in (3) by the RKHS H K and the complexity measure  X ( h )by h 2 K .One advantage of using RKHS is the following representer theo-rem [30] which we rewrite in the context of the optimization problem (3).

Theorem 1. The optimal function h  X  for the optimization problem (3) has the following form, where c qj ,q =1 ,...,Q,j =1 ,...,n q , are real numbers.
In essence, Theorem 1 reduces the search over H K which is an infinite dimensional space to the search for c qj belonging to a finite dimensional Euclidean space, and we can write the emipirical risk L ( h,  X ,  X  )  X  L ( c,  X ,  X  )with c =[ c qj the above discussion, the alternating optimization algorithm discussed before can be more concisely described as
Algorithm. (Alternating optimization)
We now present a convergence result using the general convergence theorem for block coordinate descent in Section 2.7 of [2].

Theorem 2. Every limit point of { c k , X  k , X  k }  X  k =1 stationary point of L ( c,  X ,  X  ) .

Proof. According to proposition 2.7.1 in [2], we need to check that both the optimization problems (5) and (6) have unique solutions, and the feasible regions for both are con-vex. Convexity for the feasible regions for both (5) and (6) are obvious. For fixed c , it is easy to see that the objective function Table 1: Number of queries and query-url pairs on US and CN datasets is strictly convex for p&gt; 1, hence minimizer is unique. For fixed  X  and  X  , using the representer theorem of Theorem 1, the objective function for the optimization problem (5) can be written as which is also seen to be strictly convex in c .

Remark. One alternative to the Alternating Optimiza-tion method for the RKHS case is to directly optimize the function L ( c,  X ,  X  ).

Stopping criteria. In practice, we iterate the Alternat-ing Optimization Algorithm until  X  and  X  do not change very much, for example, we specify  X &gt; 0, and terminate the iterations once  X  k +1  X   X  k +  X  k +1  X   X  k  X   X  .
In this section we report some experimental results on data generated from a commerc ial search engine. We fo-cus on data for two popular languages English and chinese, respectively.
In our experiments, a set of queries are sampled from query logs, and a certain number of query-document pairs are labeled according to their perceived relevance judged by human editors. The labels are mapped to numerical values and the goal is to learn a retrieval function that can best mimic the human judgment process. In Table 1, we list some basic statistics for the two data sets.

The construction of the English and Chinese training sets mainly consists of the following steps. For each query-document pair ( q, d )with q  X  X  and d  X  D ,afeaturevector x =[ x Q ,x D ,x QD ] is generated and the features generally fall into the following three categories:
We use two data sets in our experiments, one mainly con-sists of English queries and the other mainly chinese queries, and the two sets are labeled as English and Chinese, repec-tively. In Table 1 we list the number of queries and query-document pairs for the two data sets. The numbers of fea-tures for Chinese and US datasets vary from a few hundreds to more than a thousand. The difference between the num-bers of features are mainly due to the different maximum query lengthes for the two datasets.
Many evaluation metrics have been proposed to assess the effectiveness of information ret rieval systems including the popular precision-recall method. In this work, we use the recently popularized Discounted Cumulative Gain (DCG) methodology which seems to be more appropriate for assess relevance in the context of search engines [19]. For a ranked list of N documents( N is set to be 5 in our experiments), we use the following variation of DCG, where G i represents the weights assigned to the label of the document at position i . Higher degree of relevance corre-sponds to higher value of the weight. We will use the symbol dcg to indicate the average of this value over a set of queries in our experiments.
In this section, we provide some more details on the learn-ing and evaluation methods used in our experiments.
In section 3 we mentioned that our proposed methods can also be considered as an adaptive way to adjust the response values. The alternating optimization process consists of fit-ting the  X  and  X  values and the fitting of the nonlinear function h which amounts to solving a weighted nonlinear least squares problem. For situations where it is not con-venient to incorporate weights, we can slightly modify the empirical loss to be (cf. section 3) min
For each choice of regularization parameters  X   X  and  X   X  we optimize the regression function and the parameters  X  q and  X  q as follows: 1) initialize y 0 qj to the assigned numerical values for each 2) iterate until the  X  k q and  X  k q do not change much (cf. Then  X  q =
In what follows, we will refer the above algorithm as adap-tive Target Value Tranformation (aTVT). We will compare aTVT against a particular system that uses nonlinear regres-sion to learn a retrieval function based on the gradient boost-ing methods [7, 8, 12]. It is the same fitting method used in Step 2/a) in the above. In our experiments, we will refer the retrieval function of this system as the retrieval function without aTVT. We should mention that many methods for nonlinear regression can be used for fitting the h function, we adopt the methods in [7, 8] so that we can provide a consistent comparison.
The main objective of this work is to demonstrate the po-tential advantage of incorporating query difference in learn-ing retrieval functions. In general, the idea can be applied to many existing information retrieval systems. We will only focus on the comparison between systems using aTVT and the same system without using aTVT. A more comprehen-sive comparison of the existing information retrieval systems is beyond the scope of this work.

The comparison is carried out in the following way, and in essence we seek to assess the significance of the observed difference between systems with aTVT and systems without aTVT in terms of the difference in the DCG values. Table 5: dcg gains and percentage of gains for re-trieval functions with aTVT over without aTVT at different percentiles on English data for the L 2 case with  X   X  =10 and  X   X  =1
Before we present our results we want to emphasize that the baseline result is based on a state of the art commer-cial search engine, and for all the top search engines the difference in dcg values is below 5%.

Table 2 and Table 3 list the dcg for retrieval function with aTVT as compared to retrieval function without aTVT in the L 2 case (cf. 3.2.1 ) for the English and Chinese data sets, respectively. The percentage dcg gains and the p -values from Wilcoxon signed rank tests are also presented. From the two tables, we can see aTVT gives statistically significant dcg gains for both English and Chinese data sets. The optimal regularization parameter combinations give about 2% dcg gain for the English data and slightly more than 1% dcg gain for the Chinese data.

Table 4 also shows the means and variances of the opti-mal  X  q and  X  q values for the queries in English data set. As is expected, increasing  X   X  and  X   X  will move  X  and  X  closer toward to 1 and 0, respectively. Retrieval function without aTVT corresponds to  X  =1and  X  =0,where  X   X   X  +  X  and  X   X   X  +  X  . Our experimental results also indicate that regularization with respect to  X  and  X  is very important to prevent overfitting. Compared with the dcg values for retrieval function without aTVT, aTVT without regulariza-tion on  X  and  X  , e.g.,  X   X   X  0and  X   X   X  0, degrade dcg .
Table 5 shows the absolute and percentage of dcg gains at different percentiles of queries for the English data form the L 2 case and the fixed regula rization parameters:  X   X  =10 and  X   X  =1.

Due to the limit of time, we only tried a few combinations of regularization parameters on English data for the L 1 case. The dcg for aTVT with one iteration K =1,  X   X  = 100 and  X   X  =0 . 2 are 11.45(+1.75%, p  X  value=0.001). The mean and variance of  X  q are 0.92 and 0.014 respectively, and those for  X  q are 0.22 and 0.179, respectively.

In Table 6, we list percentage dcg gains at different query percent intervals for the English data. We can see that for those queries where the dcg does not change much, the av-the dcg for retrieval function without aTVT are 11.30. aTVT is 7.87. Table 6: The average  X  ,  X  and percentage of gains for retrieval functions with aTVT at different per-centage intervals on English data for the L 2 case with  X   X  =10 and  X   X  =1 erage  X  tends to be bigger while the average  X  tends to be smaller, for those queries where the dcg changes signifi-cantly, the opposite is true. To gain better insights into the question X  X or what kind of queries, aTVT is more effective X  we notice that the average total number of anchor text lines for the top 10% and bottom 10% queries are about 15.42 and 10.83, respectively which is significantly different from the average for all queries (22.89). As a comparison, the av-erage total number of anchor text lines for the queries with small dcg change is about 24.71 which is very close to the average for all the queries. Certainly more work needs to be done to understand the effectiveness of aTVT at the query level.

A more telling story is what are presented in Table 7. Here we sort all the 910 English que ries according to the corre-sponding values of  X   X |  X  0 . 1  X  1 . 0 | + |  X / 10 | which measures how far the  X  and  X  deviate from their nominal values of one and zero. Those queries with larger  X  have larger adjustment to compensate query difference, and the corresponding im-provement in DCG is also larger. The adjustment for query Table 7: dcg gains and corresponding p -values for queries sorted according to |  X  0 . 1  X  1 . 0 | + |  X / 10 difference is purely data-driven and it is not easy to attribute the adjustments to some specific aspects of the query such as length and language identity etc. One the other hand, it is also possible to devise more targeted adjustments using query classes for example. We will mention this issue again in the conclusion remarks.
We cast the information retrieval problem as a multi-task learning problem which presents a natural setting for discussing the important issues of query difference and its impact in learning effective retrieval functions. There are many ways to incorporate que ry difference in learning re-trieval functions, the approaches of constructing appropri-ate query features being one of them even though it is usu-ally not looked at from this viewpoint. In this paper, we present an approach through modifications of the empirical risk using nuisance parameters to accommodate the effects of query difference. The approach can be used even when there are query features contained in the feature vector of query-document pairs. In this work we have also developed numerical algorithms for solving the resulted optimization problems for minimizing the modified empirical risk. Based on data sets extracted from a commercial search engine, we Case p =2 , X   X  =10 and  X   X  =1 also demonstrate that the proposed method gives good im-provements in terms of DCG gains. We believe our approach represents an initial step towards better understanding the issues of query difference and learning retrieval functions and there are many topics that deserve further investiga-tion including 1) explore the relation between construct-ing retrieval functions using query-dependent features and the approaches we used in this work, especially elucidate the concept of adequate query-dependent features; 2) prove generalization bounds for the multi-task risk minimization problem; and 3) explore query difference in other informa-tion retrieval models. In particular, the aTVT framework can be combined with other prior information, for example, we can devise a set of class structures for queries and use a set of  X  and  X  for each class of the queries instead of one set for each query, many sort of variations on this idea can be further investigated. The work of the first author was supported in part by Yahoo!. [1] A. Berger. Statistical machine learning for [2] D. Bertsekas. Nonlinear programming .Athena [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. [4] H. Chen. Machine Learning for information retrieval: [5] R. D. Cook and S. Weisberg. Residuals and influence [6] W. Cooper, F. Gey and A. Chen. Probabilistic [7] D. Cossock. Method and apparatus for machine [8] D. Cossock and T. Zhang. Subset ranking using [9] F. Cucker and S. Smale. On the mathematical [10] Y. Freund and R. Schapire. Experiments with a new [11] J. Friedman. Multivariate adaptive regression splines [12] J. Friedman. Greedy function approximation: a [13] N. Fuhr. Optimum polynomial retrieval functions [14] N. Fuhr and C. Buckley. A probabilistic learning [15] N. Fuhr and U. Pfeifer. Probabilistic information [16] J. Gao, H. Qi, X. Xia and J. Nie. Linear discriminant [17] F. Gey, A. Chen, J. He and J. Meggs. Logistic [18] M. Hollander and D. A. Wolfe. Nonparametric [19] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [20] T. Joachims. Optimizing search engines using [21] T. Joachims. Evaluating retrieval performance using [22] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and [23] K. L. Kwok. A neural network for probablistic [24] X. Liu and W. B. Croft. Cluster-based retrieval using [25] R. Nallapati. Discriminative models for information [26] J. Nocedal and S. Wright. Numerical Optimization. [27] J. Ponte and W. Croft. A language modeling approach [28] G. Salton. Automatic Text Processing. Addison [29] H. Turtle and W. B. Croft. Inference networks for [30] G. Wahba. Spline models for observational data. [31] C. Zhai and J. Lafferty. A study of smoothing methods [32] C. Zhai and J. Lafferty. A risk minimization
