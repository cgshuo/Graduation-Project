 Crowdsourcing is an effective method for collecting labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually. In this case, each single task needs large amounts of data to provide accurate estimations. How-ever, in practice, budgets provided by customers for a given target task may be limited, and hence each question can be presented to only a few users where each user can answer only a few questions. This data sparsity problem can cause previous approaches to perform poorly due to the overfit-ting problem on rare data and eventually damage the data veracity. Fortunately, in real-world applications, users can answer questions from multiple historical tasks. For exam-ple, one can annotate images as well as label the sentiment of a given title. In this paper, we employ transfer learning, which borrows knowledge from auxiliary historical tasks to improve the data veracity in a given target task. The moti-vation is that users have stable characteristics across differ-ent crowdsourcing tasks and thus data from different tasks can be exploited collectively to estimate users X  abilities in the target task. We propose a hierarchical Bayesian model, TLC (Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impact, TLC introduces task-specific factors to model task differences. The exper-imental results show that TLC significantly improves the accuracy over several state-of-the-art non-transfer-learning approaches under very limited budget in various labeling tasks.
 H.2.8 [ Database Management ]: Database Applications  X  Data Mining Crowdsourcing, Transfer Learning
Producing large-scale training, validation and test sets is vital for many machine learning and data mining applica-tions. Most often this task has to be carried out  X  X y hand X  and thus it is delicate, expensive, and tedious. Crowd-sourcing systems such as Amazon Mechanical Turk 1 ,re-CAPTCHA 2 and the ESP game 3 have made it easy to dis-tribute simple labeling tasks to hundreds of users(referred to as worker in Mechanical Turk). Crowdsourcing is widely used in tasks such as sentiment classification [12], object recognition [7], ranking [13] and clustering [6], etc.
A typical crowdsourcing pipeline can be divided into three main steps: 1: Task design, 2: Data distribution and 3: An-swer aggregation. In the answer aggregation step, one needs to aggregate these noisy and unreliable responses into a true answer. Answer aggregation is most important step because it is closely related to the veracity of the produced answers and the final performance of machine learning algorithms.
An intuitive way to aggregate the responses is via  X  X ajor-ity Voting X , in which one always selects the answer chosen by most users. However, this method is ineffective in practice. First, some users are more skilled and consistent in one spe-cific task, as they may master more background knowledge, or may be more patient. This implies that when dealing with the same task, different users may have different abilities to annotate instances/answer questions. Second, different in-stances may have different levels of difficulty. Third, since typical crowdsourcing tasks are tedious and the reward is small, errors are common even among users who made an effort. Thus, in order to achieve higher veracity for answer aggregation, one needs to take both user ability and question difficulty into account.

Several approaches have been proposed based on the above motivation [14, 6, 7, 15, 10]. These works model the user ability and question difficulty as two latent attributes, which affect the probability that one user provides the correct an-swer to a question. These approaches require the collecting of large amounts of data to guarantee the veracity of the final answers. However, in practice, due to the limited bud-get of each task provided by customers, each question may be presented to a few users and each user may answer only https://www.mturk.com/mturk/welcome http://www.google.com/recaptcha http://www.gwap.com/gwap/gamesPreview/espgame/ 677 a few questions. We define this problem as data sparisity. This data sparseness may make existing learning algorithms overfit the few responses. For example, a highly reliable user who only answers two questions in a new task may ac-cidentally make an obvious mistake in one question. This may cause the other answer from this user to be considered as useless due to overfitting, despite the fact that this user is an expert at doing similar tasks. Under this situation, previous approaches may fail to infer the user ability and question difficulty accurately and hence may provide incor-rect aggregated answers.

Fortunately, in most crowdsourcing services, many users have answered questions in multiple historical tasks. Ac-cording to a survey by Ross [11], 31% Mechanical Turk users have worked for more than half a year and 43% of users work more than 5 hours a week. The survey also stated that people often participate in various types of task such as image labeling and item comparison. In addition, the normal time span of a task is about 2 weeks, so we can in-fer that active users who have worked for more than half a year have answered questions in many different tasks with high probability. This phenomenon sheds light on solving the data sparsity problem: by considering the same users as a bridge, data from previous tasks can be borrowed to benefit the current task as most users may have relatively stable characteristics in completing related tasks.
However, due to the differences among tasks, merging the data from multiple tasks naively does not work well. For example, different tasks may require different background knowledge, such as the differences between image annota-tion and sentiment analysis tasks. There are also differences between different kinds of labels, e.g., binary vs. multi-class labels. Finally, we may have different display styles, e.g., texts vs. images. These differences may influence users X  performance and misguide the estimation process on user ability and question difficulty. Can we utilize the data from multiple previously labeled tasks for labeling the data in the current task, while avoiding the negative effects stemming from task differences?
In this paper, we consider the problem of making the max-imum use of the knowledge gained from previously solved crowdsourcing tasks to benefit the current crowdsourcing task. We propose a novel transfer-learning basd solution known as Cross-Task Crowdsourcing (CTC) for the prob-lem. In particular, we exploit transfer learning to extract knowledge from auxiliary domains to help learning in a tar-get domain [8]. As an example, in Figure 1, some new users in MTurk first answer many questions about images on ra-zors and then accept a task to annotate a set of new images related to the topic of  X  X akeups X . By observing users X  perfor-mance in the razor-recognition task, users X  characteristics, such as gender, can be inferred automatically. In addition, these characteristics are closely related to users X  abilities to answer questions in other tasks. For example, female users generally know about makeup products better than male users. Thus, in the target makeup task, users X  abilities can be estimated more accurately, which can be used to improve the data veracity of answer aggregation. Knowledge transfer seems particularly useful for crowdsourcing service providers such as Mechanical Turk because they can identify same users in different tasks using a unified identification.
To enable the transfer of knowledge between crowdsourc-ing tasks, we propose a hierarchical Bayesian model known as Transfer Learning for Crowdsourcing (TLC). We notice that most users have relatively stable characteristics in com-pleting related tasks, so the performance of these users in a target task can be estimated accurately. For each user, we introduce two shared variables, where one models the aver-age performance of a user while the other models the users X  variance in task performance in different tasks. For each task, we use another variable to model task-specific factors. Knowledge from different tasks is shared via these variables. To construct the TLC model, we propose a Markov chain Monte Carlo (MCMC) method to infer the latent variables. Our experiments confirm that TLC can effectively transfer knowledge from related auxiliary tasks while avoiding po-tential negative effects due to task differences.
We summarize the main contributions of this paper as follows: 1. We study a new transfer learning problem in crowd-2. We propose a novel hierarchical Bayesian model, TLC, 3. We conduct experiments on various real-world datasets.
In this section, we review related works on learning in crowdsourcing context. We summarize the related works in Table 1. 678
Transfer learning (TL) solves the lack of class label prob-lem in the target application by X  X orrowing X  X upervised knowl-edge from related problems [8]. It has been applied on clas-sification [9], clustering [17], ranking [5], collaborative fil-tering [3] and social network analysis [18]. For example, Daume et al. [4] proposed a graphical model by applying shared priors to the auxiliary and target tasks to represent common sentiment knowledge. Pan et al. [9] surveyed a series of algorithms on text classification. In [17], Yang in-troduced auxiliary knowledge from text to reduce the data sparsity in the image clustering task by building transla-tors from vocabulary to image features. Recently, Yahoo X  X  learning-to-rank challenge 4 also promotes transfer learn-ing X  X  application in the ranking field. In addition, more and more transfer learning research works have been proposed to cope with relational data. Cao et al. [17] proposed a novel generative model based on Gaussian Process, which introduced a new kernel to describe the task relations. Re-cently, Zhong et al. [18], incorporated knowledge in multiple social networks to help infer users X  behaviors by assigning social regulars in a topic model. However, these algorithms build models based on gold truths, which means the labeled data for training models are all reliable. This is not true for crowdsourcing tasks, where the labels provided by different users can be noisy and inaccurate. Thus, these previous ap-proaches cannot be directly applied to solve the cross-task crowdsourcing problem studied in this paper.
Crowdsourcing is a process that involves outsourcing tasks to a distributed group of people. Recently, several real and flexible systems have been launched, such as Amazon Me-chanical Turk, which makes related applications practical. In machine learning and data mining research fields, many researchers use crowdsourcing services to solve the lack of labeled-data problem. Related applications range from sen-timent classification [12], object recognition [7], ranking [13] to clustering [6]. To obtain labeled data from crowdsourcing systems, researchers commonly provide a certain budget and then distribute the data to different users/labelers accord-ing to certain principals or just randomly. Before utilizing crowdsourced labels, one needs to clean the data, since la-bels from the crowd are contaminated by errors and bias. To achieve this goal, several approaches have been proposed recently [14, 6, 7, 15, 10]. For example, Welinder et al. [15] presented a graphical model that discovers and represents groups of annotators with different sets of skills and knowl-edge, as well as groups of images that differ qualitatively. Wauthier et al. [14] focused on modeling users X  bias in a Bayesian model. Raykar et al. [10] improves the estimation accuracy by distinguishing spammers and normal users. In summary, most of these method sinferthetruelabelfora given instance by estimating both the user ability and the question difficulty. Then the probability that one user pro-vides a correct answer to a question is based on these two factors. However, these methods require a large number of answers for each user and each question. That makes the method impractical in many real-world applications, since the budget for each crowdsourcing task is limited. http://learningtorankchallenge.yahoo.com
We describe the problem of cross-task crowdsourcing (CTC) in this section. The notations can be found in Table 2. There are n users, K tasks, where the i -th task have m i questions.
We first consider single-task crowdsourcing problems. For the task T i , we try to infer answers in A i based only on the observations L i , which is referred to as ( A t )= STC ( However, as we stated in the introduction, as individual users can answer only a few questions, some L i can be very sparse, and hence the estimation of A i can be inaccurate. To solve this problem, the cross-task crowdsourcing (CTC) approach aims to exploit the knowledge in different source tasks T i that has abundant data, where i = t to estimate the 679
Blue: observed variable; Green: prior; Dashed: variable of answers A i in the target task T t collaboratively. Mathemat-ically, suppose that t -th task is the target task, the problem can be formulated as ( A t )= CTC ( L ). The challenge in CTC is how to transfer the knowledge effectively given that each user may have different performance in different tasks due to certain task-specific factors; for example, different tasks may require different background knowledge. In this paper, we focus on the annotation task where each question is an instance to label, such as a document or an image, whereas the answer to a question is its correspondent label. To simplify the discussion, we assume that in the same task, each question has only limited answers and the number of answer candidates to each question is a constant. Then, for each entry in L i , L i pq  X  [1 ,Z i ], Z i is the number of labels to the questions in T i .
In this section, we describe our proposed model, TLC (Transfer Learning for Crowdsourcing). We first present the main idea of performing knowledge transfer, then state the generation process of TLC, and finally introduce an effective inference algorithm to construct TLC models.
Our main idea is that the knowledge from related crowd-sourcing tasks can be utilized to regularize the estimation of each user X  X  performance by considering the overlapping users X  averaged performance and characteristics, as well as tasks-specific factors as a bridge. This allows us to better know which user is more trustworthy and infer true answers more accurately with fewer user responses.

A graphical representation of TLC can be found in Fig-ure 2. For each user U p in all tasks, the average performance of U p over all tasks is defined as M p , M p is generated from a single-dimension Gaussian distribution.
  X 
M is the mean of all users X  averaged performance and  X  M is the corresponding variance. M p is decided by task indepen-dent factors of a user which in practice, could be the user X  X  IQ, ability to focus, education level, etc. The characteristics of a user U p are defined as G p , which is generated from a multivariate Gaussian distribution.
  X  u is the mean of all users X  characteristics and  X  u is the corresponding covariance matrix. G p is decided by a task dependent factor of U p ,sothat G p accounts for the differ-ences of user performance in different tasks. In practice, G can be multidimensional, each dimension may be related to the users X  knowledge in a specific domain. We use B i to denote the task-specific factors for task T i , B i is generated from a multivariate Gaussian distribution.
  X  t is the mean of all task-specific factors and  X  t is the cor-responding covariance matrix. In reality, B i can be multi-dimensional, where each dimension can be considered as a factor, such as whether some specific knowledge is required. We use C i p to denote the ability of user U p answering ques-tions in T i ,andlet C  X  X  n  X  K denote the whole performance matrix containing the performance of all users in all tasks. C p is generated from a single-dimension Gaussian distribu-tion.
  X  c is the corresponding variance, which models how much the actual user ability will deviate from the expected user ability. A larger value in C i p means that U p is more likely to give correct answers to questions in T i , while a negative C means that U p is adversary and always gives wrong answers on purpose. The dot product of G p and B i can be viewed as the interaction of the user X  X  characteristics and how much this characteristic is needed to perform well in this specific task. For example, a male user may have sufficient knowl-edge about sports but little knowledge about how to apply makeups to oneself. If a task requires certain knowledge on sports but requires no knowledge on makeups, the user may perform very well; otherwise, he may perform badly. On the question side, we use D i q to denote the difficulty of the question Q i q ,where D i q &gt; 0.
 Here D i q =  X  means Q i q is so difficult that even the best users can only guess the answers. D i q =0meansthat Q i q is so easy that even the worst users can always answer it correctly. Eq.(5) can model the fact that very few questions are extremely easy or extremely difficult. In addition, we use A q to denote the true answer for Q i q . A i q is generated from a discrete distribution, which corresponds to a multiple-choice single answer question.
 p a is a common prior to all questions. Finally, let L i pq { X  p, q | I i ( p, q ) &gt; 0 } denote the response (label) given by U to Q i q . For a question with Z i possible answers, the prob-ability that a user can provide the correct answer to the question is computed through a logistic function as follows: 680 Algorithm 1 Inference of TLC 2: Output : A 6: Calculate W with Eq.(14) 7: for  X  =1to  X  do 8: Update every true answer A i q using Eq.(9) 11: Update every M p using posterior distribution Eq.(12) 14: for  X  =1to X  do 17: end for 18: ENDIF 19: end for 20: Return A When a user has a larger C i p , the answer he/she provides has a higher probability to be the same as the true answer to the question. If a user is adversarial, he may have a negative C so that the answer that he gives is more likely to disagree with the true answer. When the C i p is near 0, the user has around 1 Z i probability to answer the questions correctly. After inferring the latent variables, the true answer to each questioncanbecalculatedbyBayesiantheorem:
We need to infer a total number of six variables G i p , M C , B i , D i q , A i q . Toconstructthemodel,TLC,wepropose a mixture method of Markov Chain Monte Carlo (MCMC) and Gradient Descent (GD), where the MCMC is utilized to infer the low-level variables, user specific ability C averaged performance M p , problem difficulty D i q and true answer A i q , while the GD is to find the optimal G p and B user characteristics and task-specific factors respectively. In order to infer the latent variables from the observed vari-able L i pq , we first calculate the posterior distribution of each variable as follows: p ( M p |C p )= p ( Given the observed variable L i pq , we iteratively sample A using Eq.(9), sample D i q using Eq.(10), sample C i p using Eq.(11) and sample M p using Eq.(12). Since the prior dis-tribution of C i p and D i q are not conjugate with the likelihood algorithm when sampling C i p and D i q . Take the sampling process of C i p as an example. We firstly randomly initialize C p and then generate a new sample  X  C i p near the previous sample,  X  C i p  X  X  ( C i p , X  MH ), we use Gaussian distribution as the jumping distribution,  X  MH is the variance of the jumping distribution. Consequently, we compare the posterior prob-ability density of the new sample with the previous sample. If p (  X  C i p | L i pq ,A i q ,D i q ) &gt;p ( C i p | L obtains a higher posterior probability density, we accept the new sample and perform update: C i p =  X  C i p .Otherwise,we accept the new sample randomly with probability P MH .The above process is repeated until convergence when C i p does not change too much. The sampling process of D i q is similar to C i p , which firstly randomly initialize D i q , and then draw anewsample  X  D i q near D i q . Then, we compare the poste-rior probability density of  X  D i q and D i q and decide whether to accept the new sample or not.

For the user characteristic G p and task-specific factor B we exploit gradient descent to find the optimal solution via C . Since each user may give different numbers of answers in each task, we assign a weight to each C i p during the inference process. We denote the weight of C i p as W ip , W = { W ip i  X  K, 1  X  p  X  n } . The weight matrix can model our confidence on users X  performance C i p in each task: the more answers one user gives in a task, the higher confidence we can obtain from the estimation of user X  X  performance in this particular task. If W ip is small, we can predict C i p with the knowledge learned from related auxiliary task, which may have sufficient data.

The relation between the weight W ip and the number of answers in T i may not be linear. After a user answered a number of questions in a task, we gain high confidence in our performance estimation. That is, C i p can be trusted and W ip should be high. Thus, we make W ip  X  log( r i,p )where r i,p is the number of responses given by U p to all questions in T . We also normalize all W ip to make the maximum weight 1 and minimum weight 0. Let MAX =MAX( log ( r i,p )) and MIN =MIN( log ( r i,p )), The objective function is defined as follows: we minimize the following quantity where  X  is the Frobenius norm,  X  t and  X  u are two trade-off parameters, and  X  c is the mean of all C i p . The partial derivative of the objective function with respect to G p and 681 B i are  X 
J ( G , B )  X 
J ( G , B ) Then, these two equations are used to iteratively optimize G p and B i until convergence.
 Framework The whole inference process can be found in Algorithm 1. Overall, it is an iterative process. In each itera-tion, MCMC is utilized to infer the low-level variables, user-specific ability C i p , user-averaged performance M p ,problem difficulty D i q and true answer A i q , while the GD is to find the optimal user characteristics G p and task-specific factors B , respectively.  X  is the number of iterations,  X  controls how often to update hyper parameters, and  X  is the learning rate. Our algorithm is a kind of MCMC algorithm, which convergence guarantee is proved in [1].
 Time Complexity We analyze the time complexity of TLC. In the algorithm, the part for updating A and D takes O ( X  K i m i ) time, and for updating C takes O ( K time. Furthermore, the part for updating G and M takes O ( n  X   X ) time, and for updating B takes O ( K  X   X ) time. In addition, we update the parameters with  X  iterations. Thus the overall time complexity is O (  X   X  [( X  K i m i + n ( n + K )  X   X ]). The baseline model GLAD has a complex-ity of O (  X   X  ( X  K i m i + n  X  k )). TLC has a slightly higher complexity than the traditional aggregation model because TLC uses another two variables to model the relationship of user abilities in different tas ks. However, the running time of the aggregation algorithm is not our major concern in the whole crowdsourcing process, since most of the time is spent on collecting responses from the crowdsourcing users. This process takes weeks or even months to complete. By exploiting knowledge from related tasks, we can reduce the number of responses needed, which can also reduce the time of the whole process.
In this section, we experimentally verify that our TLC algorithm performs well. We first present some observa-tions and findings in our preliminary empirical study, which motivates us to design the proposed TLC algorithm. Con-sequently, we give a synthetic example to illustrate the in-trinsic properties of TLC. In addition, we compare TLC with several state-of-the-art methods on two realworld data collections, where the proposed method TLC improves the baselines significantly.

For evaluation metrics, we introduce two evaluation met-rics Root Mean Square Error (RMSE) and accuracy to mea-sure how accurate the inferred results match the ground truths. RMSE is used in the synthetic dataset as the task is regression, and the smaller the RMSE the better. Accuracy is used in two realworld data collections as our task is classi-fication where higher accuracy is better. We compare TLC with three baselines: (1) Majority Vote: A popular heuris-tic which does not model user ability and question diffi-culty(denoted as X  X ajority X  or  X  X V X ). (2) GLAD [16] model considers user ability and question difficulty. (3) DARE [2] model considers user ability, question difficulty and user X  X  advantage to a question. For GLAD and DARE, we imple-ment them in two different ways. One is to build models on only the target task(denoted as  X  X LAD X  and  X  X ARE X ), and the other is a naive transfer model where we put ques-tions from different tasks together directly and do not model task differences (denoted as X  X aiveTL GLAD X  X nd X  X aiveTL DARE X ).
We evaluate the effectiveness of TLC on three data col-lections. The first one is synthetic. The second one is about affective text analysis [12]. The third is Gender Hobby Dataset, which is collected by ourselves from Amazon Me-chanical Turk. The characteristics of the datasets are sum-marized in Table 3. The datasets are public 5 .

We describe the data-generation process of the synthetic datasets as follows. We build two tasks, where each task has 100 questions and each question has 2 possible answers. There are 40 users who have rated both tasks. The moti-vation to generate such a dataset is that we can control the performance of each user and hence we can test whether the proposed method TLC can recover these latent variables or not. In addition, we can adjust users X  performance in differ-ent tasks such that the task differences can be simulated. 1. In order to simulate two related tasks whose task-2. We generate C i p  X  X  ( G p  X  B i + M p , X  c ) and generate 3. We sample aLL possible responses L i pq by U p to Q i q
The affective text analysis datasets [12] are related to sen-timent analysis. After reading a news headline, users are asked to give an integer rating in [0,100] for each of six emotions to indicate how strong the emotion is. The six emotions are anger, disgust, fear, joy, sadness and surprise. Each emotion corresponds to one task. In total, there are 38 users and 100 headlines, where each user has given emotion ratings to all 6 tasks. Each emotion of each headline is rated by 10 users. A ground truth rating for each headline of each emotion is provided by expert labelers. Our goal is to infer the ground truth ratings with the possibly noisy and biased ratings provided by the users.

The Gender Hobby dataset is collected by us; this dataset is about different hobby of different genders. When a user http://www.cse.ust.hk/~kxmo/materials/ GenderHobbyDataSet.rar 682 accepted one of our tasks in Mechanical Turk, he/she is asked to answer 10 questions on 2 topics, half are about sports and half are about makeup and cooking. No user can answer the same question more than once. The user can choose from 2 answers if he knows the answer, or he can choose  X  X  don X  X  know X . 57 users gave 1400 responses to our 112 different questions. We choose a set of users with distinct gender related actions to conduct our experiment, these users are either male sports fans, or female housewives. Our goal is to infer the correct answer with the noisy answers provided by the users.
As we stated in the introduction, users X  responses to a single task can be very sparse. Our solution is to exploit the knowledge from related auxiliary tasks. We designed a simple strategy to show knowledge transfer based on the affective text analysis datasets. We compare two methods based on voting. The first one is majority voting that al-ways chooses the option chosen by most users as the ground truth. The second one is called Naive Transfer, which ex-ploits knowledge from other tasks. It first estimates the user ability based on users X  responses in other tasks. Specifically, it uses majority voting on other tasks to produce  X  X seudo X  truths. Then, each user X  X  common ability in all auxiliary tasks is calculated according to the distance between their responses to these  X  X seudo X  truths. Finally, the normalized common ability of each user is utilized as the weights to combine users X  responses in the target task. The result is shown in Figure 3(a). The experiment is repeated 10 times and we show the average performance on all tasks. We can observe that, using the same number of answers per ques-tion, the Naive Transfer method can achieve significantly lower RMSE than the non-weighted version. Alternatively, in order to achieve the same RMSE, the weighted majority voting saves up to 50% of user responses per headline. For example, the RMSE of majority voting is 16 when there are 10 ratings. On the other hand, weighted majority voting achieves the same RMSE with only 5 ratings! That means that if tasks are related, knowledge can be transferred across these tasks. However, we also observe that users X  perfor-mance levels are also different in different tasks, as shown in Figure 3(b). If the difference is too large, directly using users X  common ability may harm the final result. However, if we can predict the users X  performance in the target task more accurately, we may achieve better results.
We first describe a synthetic experiment to answer two questions: (1) Can TLC improve the performance of a tar-get task even when the source task and the target task are completely different? (2) Can TLC accurately recover hid-den parameters, e.g., users X  abilities?
We divided the T 1 and T 2 tasksinto4tasks. Q 2 t and Q 2 are two disjoint sets of questions in tasks T 2 and Q 1 is the set of questions in T 1 . L 2 t , L 2 s , L 1 t and L 1 s responses given by U t to Q 2 t , U s to Q 2 s , U t to Q 1 Q 1 respectively. L 2 the other L 2 s , L 1 t and L 1 s are the set of responses for aux-iliary tasks. L 2 s and L 1 s are dense and contain information about how the user X  X  performance in source task and in tar-get tasks are related. Specifically for our experiment, all users in three auxiliary source tasks L 2 s , L 1 s and L 1 swered 50 questions and each question in Q 2 t have received 2 ratings from users in U t . Our goal is to infer the true an-swer for each question in Q 2 t . We introduced three baselines: majority voting, single-task model, e.g., GLAD on L 2 t ;naive transfer model: using GLAD on L t = {L 1 t , L 2 t } .Theresult is shown in Fig.(4). The majority voting has about 75% accuracy and the single-task model, GLAD, achieves about 85% accuracy. Due to large task differences, the naive trans-fer model that combines data from different tasks simply does not work. However, by modeling users X  characteristics across tasks and tasks X  specific factors, TLC improves them to about 90%. The experiment result shows TLC can trans-fer knowledge from related but different auxiliary tasks and avoid potentially harm from task differences.

To answer the second question on how well TLC can re-cover user ability C i p in the sparse target task, we compare TLC with baselines. For evaluation, we compute the simi-larity between the predicted user ability C i p and the ground truth  X  C i p , with a normalized cosine similarity which is de-fined as in Eq.(17). The results are shown in Fig.(4). Note that the majority vote does not model user ability, so we do not take it into consideration.
 We observe that, as the number of iterations increases, the similarity of TLC goes steadily towards 1. That means TLC recovers user ability very well. On the other hand, the sim-ilarity of GLAD goes up before the 25-th iteration, but falls quickly and stays around 0 due to severe over-fitting. In addition, the similarity of naive transfer method goes down directly and stays close to  X  1. This is because the naive transfer method does not model task differences, thus it re-lates users X  abilities in the source tasks to that in the target task in the opposite way. This means the naive transfer method trusts the users with the lowest ability in the tar-get task. This analysis again shows that our proposed TLC model can effectively transfer knowledge from completely different yet related tasks, and the TLC model can recover users X  abilities very accurately. 683 (a)-(b)Result of Affective Text Dataset, (c)-(d) Result of
Here we present the performance of TLC in two real-world applications. Our experiment is divided into two parts. The first part is on the Affective Text Analysis dataset, which shows the effectiveness of TLC on similar tasks. The sec-ond part is on the Gender Hobby dataset, which shows that when users show a strong performance correlation in differ-ent tasks, we can use the auxiliary data from related tasks to improve performance in target task.

Firstly, we focus on the situation when source tasks and target tasks are very similar. We first convert the emotion score in the original Affection dataset to a true/false ques-tion. Emotion score in [0 , 50) is converted to false while score in [50 , 100] is converted to true. Let T i denote the i task. For each experiment, we pick a part of the questions t in task T i to be the target task, while the other questions s in T i and all questions source tasks. We left 2 answers to each question in the target task and stimulated more responses from each user in source tasks, while keeping the accuracy of each user unchanged. Specifically, users in target task U t answered 5 questions in target task Q i t and answered 50 questions in source task where j = i ; other users U s answer 50 questions both in and in each of Q j where j = i . The performance of TLC and all other baselines are presented in Table 4 and Figure 5. We ran each model 10 times and took the average performance. Due to the difference between tasks, the performances of the compared methods are different, but on the whole, Trans-fer Models have higher accuracy than Non-Transfer Models. Specifically when using Joy as the target task, the proposed model TLC achieves 82 . 5% accuracy, which improved the best non-transfer model by 6%. The results show that the information in auxiliary tasks can actually help the target task. TLC has comparable accuracy with Naive Transfer Models, which shows that the TLC model can effectively transfer knowledge from auxiliary tasks when source tasks and target tasks are very similar.

Secondly, we focus on the situation where users show strong performance correlation in different tasks using Gender Hobby datasets. The original dataset contains 400 responses given by 21 users related to 102 questions, where each task has Table 5: Accuracy on Gender Hobby Collections 51 questions. We enlarged the dataset by up-sampling: we sampled and added another 21 users with the same accuracy distribution as well as 51 questions with the same difficulty distribution to each task. We left 4 answers to each ques-tion in target task and stimulated more responses from each user in the source tasks, while keeping the accuracy of each user unchanged. Denote T 1 to be the sports task, T 2 to be the makeup and cooking task, respectively. We used both tasks as source and target. When we pick a part of the ques-tions Q i t in task T i as the target task, the other questions s in T i along with all questions source tasks. A part of the users, U t answer 50 questions in
Q 2  X  i and about 5 questions in Q i U s answer 50 questions in both Q 2  X  i and Q i s . Finally, we got 3,252 responses, 204 questions in 2 tasks, 42 users in total. Our goal is to infer the true answer for each ques-tion in target task Q i t . The result is shown in Fig.(5) and Table 5. We ran each model 10 times and took the average performance. Due to large differences between tasks, the two naive transfer models that do not model task differences do not work well. When using Sports as the target task, the proposed TLC model achieves 83 . 7% accuracy and outper-formed all the baseline models by at least 1 . 5%. When using Makeup and Cooking tasks as target tasks, the TLC model achieves 70 . 9% accuracy, improves the best baseline model by about 2%. This experiment demonstrates that the pro-posed TLC model can transfer knowledge from related but different tasks, and avoid possible negative affects brought about by task differences.
We answer three questions in the subsection to analyze the robustness of TLC: 1. Does TLC converge? 2. How does the data sparsity level affect the TLC performance? 3. Is TLC sensitive to model parameters? We answer these questions via an empirical test, where the results are the average from 10 repeated tests (see Fig.6).

We first plot the likelihood of the model in a target do-main. As the model iterates, the log-likelihood of the model becomes larger and larger, until it becomes stable. This shows that our model is able to converge. We then study the impact of target domain sparsity. We use different num-bers of responses to each question in the target domain, while keeping the source domain the same. In this exper-iment, TLC performs better than the non-transfer model in situations where there are fewer responses in the target task. This experiment shows that TLC can effectively trans-fer knowledge when the target task data is sparse.
We also experiment on the parameters of the model. A smaller  X  c makes prediction more accurate, because a smaller  X  better regularizes the difference between the inferred user 684 (b) Likelihood in target task ability C i p in the target task and the predicted user ability from the auxiliary task. This shows that the model X  X  predic-tive ability is accurate.  X  M and  X  d do not affect the result very much, the result is omitted due to page limitations. In reality, we can select the most appropriate parameters by testing the model performance on a validation set. We can keep a small set with gold labels as validation set and then select the parameters which achieve the best performance on it.
In this section, we briefly discuss situations where the TLC model is most suitable. We list a number of situations. First, some of the users in the target tasks have finished plenty of questions in source tasks in the history. Second, users in source and target tasks show strong performance correla-tion. TLC model is particularly suitable for crowdsourcing service-provider websites such as Mechanical Turk, as well as big crowdsourcing requesters who post many related tasks to a group of stable users. When most of the users in target tasks are new or when the target tasks have no correlation with source tasks, TLC still performs as well as single-task baseline models.
In this paper, we studied a new problem on how to trans-fer knowledge under the context of crowdsourcing, where the labels on data instances provided by various crowd-sourcing users can be sparse, noisy and unreliable. We con-sider this problem as cross-task crowdsourcing (CTC), where we exploit transfer learning to learn from related auxiliary tasks. Although the shared user may perform relatively sta-bly in similar tasks, the task-specific differences may degrade the performance. In response, we proposed a hierarchical Bayesian model to transfer the knowledge from related tasks adaptively. To the best of our knowledge, this is the first work to utilize multiple tasks in crowdsourcing applications via transfer learning. The proposed model is flexible in that it can be extended to any number of tasks. We conducted empirical studies on two real datasets: Affective Text Analy-sis dataset and Gender Hobby dataset collected from Ama-zon Mechanical Turk, where the proposed algorithm TLC outperforms several state-of-the-art non-transfer models by as high as 6% on accuracy.
 We thank the support of Hong Kong CERG Grants 621211 and 620812. We thank Rong Pan for discussions, and Shauna Dalton for revisions.
