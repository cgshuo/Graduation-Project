 Medical dischar ge summaries contain information that is useful to clinical researchers who study the interactions between, for e xample, dif ferent med-ications and diseases. Ho we v er , these summaries include e xplicit personal health information (PHI) whose release w ould jeopardize pri v ac y . In the United States, the Health Information Portability and Accountability Act (HIP AA) pro vides guide-lines for protecting the confidentiality of health care information. HIP AA lists se v enteen pieces of te xtual PHI of which the follo wing appear in medical dis-char ge summaries: first and last names of patients, their health proxies, and f amily members; doctors X  first and last names; identification numbers; tele-phone, f ax, and pager numbers; hospital names; ge-ographic locations; and dates. Remo ving PHI from medical documents is the goal of deidentification.
This paper presents a method based on a statis-tical representation of local conte xt for automati-cally remo ving e xplicit PHI from medical dischar ge summaries, despite the often ungrammatical, frag-mented, and ad hoc language of these documents, e v en when some w ords in the documents are am-biguous between PHI and non-PHI (e.g.,  X  X unting-ton X  as the name of a person and as the name of a disease), and e v en when some of the PHI cannot be found in dictionaries (e.g., misspelled and/or for -eign names). This method dif fers from traditional approaches to deidentification in its independence from dictionaries and hand-tailored heuristics. It applies statistical named entity recognition (NER) methods to the more challenging task of deidenti-fication b ut dif fers from traditional NER approaches in its hea vy reliance on a statistical representation of local conte xt. Finally , this approach tar gets all PHI that appear in medical dischar ge summaries. Experi-ments reported in this paper sho w that conte xt plays a more important role in deidentification than dic-tionaries, and that a statistical representation of lo-cal conte xt contrib utes more to deidentification than global conte xt. In the literature, named entities such as people, places, and or g anizations mentioned in ne ws arti-cles ha v e been successfully identified by v arious ap-proaches (Bik el et al., 1999; McCallum et al., 2000; Rilof f and Jones, 1996; Collins and Singer , 1999; Hobbs et al., 1996). Most of these approaches are tailored to a particular domain, e.g., understanding disaster ne ws; the y e xploit both the characteristics of the entities the y focus on and the conte xtual clues related to these entities.

In the biomedical domain, NER has focused on identification of biological entities such as genes and proteins (Collier et al., 2000; Y u et al., 2002). V arious statistical approaches, e.g., a maximum entrop y model (Fink el et al., 2004), HMMs and SVMs (GuoDong et al., 2005), ha v e been used with v arious feature sets including surf ace and syntac-tic features, w ord formation patterns, morphologi-cal patterns, part-of-speech tags, head noun triggers, and coreferences.

Deidentification refers to the remo v al of identi-fying information from records. Some approaches to deidentification ha v e focused on particular cat-e gories of PHI, e.g., T aira et al. focused on only patient names (2002), Thomas et al. focused on proper names including doctors X  names (2002). F or full deidentification, i.e., remo v al of all PHI, Gupta et al. used  X  X  comple x set of rules, dictionaries, pattern-matching algorithms, and Unified Medical Language System X  (2004). Sweene y X  s Scrub sys-tem emplo yed competing algorithms that used pat-terns and le xicons to find PHI. Each of the algo-rithms included in her system specialized in one kind of PHI, each calculated the probability that a gi v en w ord belonged to the class of PHI that it spe-cialized in, and the algorithm with the highest prece-dence and the highest probability labelled the gi v en w ord. This system identified 99-100% of all PHI in the test corpus of patient records and letters to ph ysi-cians (1996).

W e use a v ariety of features to train a support v ector machine (SVM) that can automatically e x-tract local conte xt cues and can recognize PHI (e v en when some PHI are ambiguous between PHI and non-PHI, and e v en when PHI do not appear in dic-tionaries). W e compare this approach with three others: a heuristic rule-based approach (Douglass, 2005), the SNoW (Sparse Netw ork of W inno ws) system X  s NER component (Roth and Y ih, 2002), and IdentiFinder (Bik el et al., 1999). The heuristic rule-based system relies hea vily on dictionaries. SNoW and IdentiFinder consider some representation of the local conte xt of w ords; the y also rely on informa-tion about global conte xt. Local conte xt helps them recognize stereotypical names and name structures. Global conte xt helps these systems update the prob-ability of observing a particular entity type based on the other entity types contained in the sentence. W e h ypothesize that, gi v en the mostly fragmented and ungrammatical nature of dischar ge summaries, local conte xt will be more important for deidentification than global conte xt. W e further h ypothesize that lo-cal conte xt will be a more reliable indication of PHI than dictionaries (which can be incomplete). The re-sults presented in this paper sho w that SVMs trained with a statistical representation of local conte xt out-perform all baselines. In other w ords, a classifie r that relies hea vily on local conte xt (v ery little on dictionaries, and not at all on global conte xt) out-performs classifie rs that rely either on global con-te xt or dictionaries (b ut mak e much less use of lo-cal conte xt). Global conte xt cannot contrib ute much to deidentification when the language of documents is fragmented; dictionaries cannot contrib ute to dei-dentification when PHI are either missing from dic-tionaries or are ambiguous between PHI and non-PHI. Local conte xt remains a reliable indication of PHI under these circumstances.

The features used for our SVM-based system can be enriched in order to automatically acquire more and v aried local conte xt information. The features discussed in this paper ha v e been chosen because of their simplicity and ef fecti v eness on both grammati-cal and ungrammatical free te xt. Dischar ge summaries are the reports generated by medical personnel at the end of a patient X  s hospi-tal stay and contain important information about the patient X  s health. Linguistic processing of these doc-uments is challenging, mainly because these reports are full of medical jar gon, acron yms, shorthand no-tations, misspellings, ad hoc language, and frag-ments of sentences. Our goal is to identify the PHI used in dischar ge summaries e v en when te xt is frag-mented and ad hoc, e v en when man y w ords in the summaries are ambiguous between PHI and non-PHI, and e v en when man y PHI contain misspelled or foreign w ords.

In this study , we w ork ed with v arious corpora consistin g of dischar ge summaries. One of these corpora w as obtained already deidentified 1 ; i.e., (man y) PHI (and some non-PHI) found in this cor -pus had been replaced with the generic placeholder [REMOVED] . An e xcerpt from this corpus is belo w:
W e hand-annotated this corpus and e xperimented with it in se v eral w ays: we used it to generate a corpus of dischar ge summaries in which the [REMOVED] tok ens were replaced with appropri-ate, f ak e PHI obtained from dictionaries 2 (Douglass, 2005); we used it to generate a second corpus in which most of the [REMOVED] tok ens and some of the remaining te xt were appropriately replaced with le xical items that were ambiguous between PHI and non-PHI 3 ; we used it to generate another cor -pus in which all of the [REMOVED] tok ens corre-sponding to names were replaced with appropriately formatted entries that could not be found in dictio-naries 4 . F or all of these corpora, we generated real-istic substitutes for the [REMOVED] tok ens using dictionaries (e.g., a dictionary of names from US Census Bureau) and patterns (e.g., names of people could be of the formats,  X  X r . F . Lastname X ,  X  X irst-name Lastname X ,  X  X astname X ,  X  X  . M. Lastname X , etc.; dates could appear as  X  X d/mm/yy X ,  X  X d Mon-thName, yyyy X ,  X  X dth of MonthName, yyyy X , etc.). In addition to these reidentified corpora (i.e., cor -pora generated from pre viously deidentified data), we also e xperimented with authentic dischar ge sum-maries 5 . The approximate distrib utions of PHI in the reidentified corpora and in the authentic corpus are sho wn in T able 1.

Class No. in reidentified No. in authentic Non-PHI 17872 112720 P atient 1047 287 Doctor 311 730 Location 24 84 Hospital 592 651 Date 735 1933 ID 36 477 Phone 39 32 4.1 Rule-Based Baseline: Heuristic+Dictionary T raditional deidentification approaches rely hea vily on dictionaries and hand-tailored heuristics.
W e obtained one such system (Douglass, 2005) that used three kinds of dictionaries:  X  PHI lookup tables for female and male first  X  A dictionary of  X  X ommon w ords X  that should  X  Lookup tables for conte xt clues such as titles, Gi v en these dictionaries, this system identifies k e y-w ords that appear in the PHI lookup tables b ut do not occur in the common w ords list, finds approx-imate matches for possibly misspelled w ords, and uses patterns and indicators to find PHI. 4.2 SNoW SNoW is a statistical classifie r that includes a NER component for recognizing entities and their rela-tions. T o create a h ypothesis about the entity type of a w ord, SNoW first tak es adv antage of  X  X  ords, tags, conjunctions of w ords and tags, bigram and trigram of w ords and tags X , number of w ords in the entity , bigrams of w ords in the entity , and some attrib utes such as the prefix and suf fix, as well as informa-tion about the presence of the w ord in a dictionary of people, or g anization, and location names (Roth and Y ih, 2002). After this initial step, it uses the possible relations of the entity with other entities in the sentence to strengthen or weak en its h ypothe-sis about the entity X  s type. The constraints imposed on the entities and their relationships constitute the global conte xt of inference. Intuiti v ely , information about global conte xt and constraints imposed on the relationships of entities should impro v e recognition of both entities and relations. Roth and Y ih (2002) present results that support this h ypothesis.
SNoW can recognize entities that correspond to people, locations, and or g anizations. F or deidenti-fication purposes, all of these entities correspond to PHI; ho we v er , the y do not constitute a comprehen-si v e set. W e e v aluated SNoW only on the PHI it is b uilt to recognize. W e trained and tested its NER component using ten-fold cross-v alidation on each of our corpora. 4.3 IdentiFinde r IdentiFinder uses Hidden Mark o v Models to learn the characteristics of names of entities, including people, locations, geographic jurisdictions, or g ani-zations, dates, and contact information (Bik el et al., 1999). F or each named entity class, this system learns a bigram language model which indicates the lik elihood that a sequence of w ords belongs to that class. This model tak es into consideration features of w ords, such as whether the w ord is capitalized, all upper case, or all lo wer case, whether it is the first w ord of the sentence, or whether it contains digits and punctuation. Thus, it captures the local conte xt of the tar get w ord (i.e., the w ord to be classifie d; also referred to as TW). T o find the names of all entities, the system finds the most lik ely sequence of entity types in a sentence gi v en a sequence of w ords; thus, it captures the global conte xt of the entities in a sen-tence.

W e obtained this system pre-trained on a ne ws corpus and applied it to our corpora. W e mapped its entity tags to our PHI and non-PHI labels. Ad-mittedly , testing IdentiFinder on the dischar ge sum-maries puts this system at a disadv antage compared to the other statistical approaches. Ho we v er , despite this shortcoming, IdentiFinder helps us e v aluate the contrib ution of global conte xt to deidentification. W e h ypothesize that systems that rely on dictionar -ies and hand-tailored heuristics f ace a major chal-lenge when particular PHI can be used in man y dif-ferent conte xts, when PHI are ambiguous, or when the PHI cannot be found in dictionaries. W e further h ypothesize that gi v en the ungrammatical and ad hoc nature of our data, despite being v ery po werful systems, IdentiFinder and SNoW may not pro vide perfect deidentification. In addition to being v ery fragmented, dischar ge summaries do not present in-formation in the form of relations between entities, and man y sentences contain only one entity . There-fore, the global conte xt utilized by IdentiFinder and SNoW cannot contrib ute reliably to deidentification. When run on dischar ge summaries, the strength of these systems comes from their ability to recognize the structure of the names of dif ferent entity types and the local conte xts of these entities.
Dischar ge summaries contain patterns that can serv e as local conte xt. Therefore, we b uilt an SVM-based system that, gi v en a tar get w ord (TW), w ould accurately predict whether the TW w as part of PHI. W e used a de v elopment corpus to find features that captured as much of the immediate conte xt of the TW as possible, paying particular attention to cues human annotators found useful for deidentification. W e added to this some surf ace characteristics for the TW itself and obtained the follo wing features: the TW itself, the w ord before, and the w ord after (all lemmatized); the bigram before and the bigram af-ter TW (lemmatized); the part of speech of TW , of the w ord before, and of the w ord after; capitalization of TW ; length of TW ; MeSH ID of the noun phrase containing TW (MeSH is a dictionary of Medical Subject Headings and is a subset of the Unified Med-ical Language System (UMLS) of the National Li-brary of Medicine); presence of TW , of the w ord before, and of the w ord after TW in the name, lo-cation, hospital, and month dictionaries; the heading of the section in which TW appears, e.g.,  X  X istory of Present Illness X ; and, whether TW contains  X - X  or  X / X  characters. Note that some of these features, e.g., capitalization and punctuation within TW , were also used in IdentiFinder .
 W e used the SVM implementation pro vided by LIBSVM (Chang and Lin, 2001) with a linear k er -nel to classify each w ord in the summaries as ei-ther PHI or non-PHI based on the abo v e-listed fea-tures. W e e v aluated this system using ten-fold cross-v alidation. Local conte xt contrib utes dif ferently to each of the four deidentification systems. Our SVM-based ap-proach uses only local conte xt. The heuristic, rule-based system relies hea vily on dictionaries. Identi-Finder uses a simplified representation of local con-te xt and adds to this information about the global conte xt as represented by transition probabilities be-tween entities in the sentence. SNoW uses local con-te xt as well, b ut it also mak es an ef fort to benefit from relations between entities. Gi v en the dif ference in the strengths of these systems, we compared their performance on both the reidentified and authentic corpora (see Section 3). W e h ypothesized that gi v en the nature of medical dischar ge summaries, Iden-tiFinder w ould not be able to find enough global conte xt and SNoW w ould not be able to mak e use of relations (because man y sentences in this cor -pus contain only one entity). W e further h ypothe-sized that when the data contain w ords ambiguous between PHI and non-PHI, or when the PHI cannot be found in dictionaries, the heuristic, rule-based ap-proach w ould perform poorly . In all of these cases, SVMs trained with local conte xt information w ould be suf ficient for proper deidentification.
 T o compare the SVM approach with Identi-Finder , we e v aluated both on PHI consistin g of names of people (i.e., patient and doctor names), locations (i.e., geographic locations), and or g aniza-tions (i.e., hospitals), as well as PHI consistin g of dates, and contact information (i.e., phone numbers, pagers). W e omitted PHI representing ID numbers from this e xperiment in order to be f air to Identi-Finder which w as not trained on this cate gory . T o compare the SVM approach with SNoW , we trained both systems with only PHI consistin g of names of people, locations, and or g anizations, i.e., the entities that SNoW w as designed to recognize. 6.1 Deidentifying Reidentifie d and A uthentic W e first deidentified:  X  Pre viously deidentified dischar ge summaries  X  Authentic dischar ge summaries with real PHI.
Our e xperiments sho wed that SVMs with local conte xt outperformed all other approaches. On the reidentified corpus, SVMs g a v e an F-measure of 97.2% for PHI. In comparison, IdentiFinder , ha v-ing been trained on the ne ws corpus, g a v e an F-measure of 67.4% and w as outperformed by the heuristic+dictionary approach (see T able 2). 6
W e e v aluated SNoW only on the three kinds of entities it is designed to recognize. W e cross-v alidated it on our corpora and found that its per -formance in recognizing people, locations, and or -g anizations w as 96.2% in terms of F-measure (see T able 3 7 ). In comparison, our SVM-based system, when retrained to only consider people, locations, and or g anizations so as to be directly comparable to SNoW , had an F-measure of 98%. 8 Method Class P R F SVM PHI 96.8% 97.7% 97.2% IFinder PHI 60.2% 76.7% 67.4% H+D PHI 88.9% 67.6% 76.8% SVM Non-PHI 99.6% 99.5% 99.6% IFinder Non-PHI 95.8% 91.4% 93.6% H+D Non-PHI 95.2% 95.2% 95.2% Method Class P R F SVM PHI 97.7% 98.2% 98.0% SNoW PHI 96.1% 96.2% 96.2% SVM Non-PHI 99.8% 99.8% 99.8% SNoW Non-PHI 99.6% 99.6% 99.6%
Similarly , on the authentic dischar ge summaries, the SVM approach outperformed all other ap-proaches in recognizing PHI (see T ables 4 and 5). 6.2 Deidentifying Data with Ambig uous PHI In dischar ge summaries, the same w ords can appear both as PHI and as non-PHI. F or e xample, in the same corpus, the w ord  X  X w an X  can appear both as the name of a medical de vice (i.e.,  X  X w an Catheter X ) and as the name of a person, etc. Ideally , we w ould lik e to deidentify data e v en when man y w ords in the Method Class P R F SVM PHI 97.5% 95.0% 96.2% IFinder PHI 25.2% 45.2% 32.3% H+D PHI 81.9% 87.6% 84.7% SVM Non-PHI 99.8% 99.9% 99.9% IFinder Non-PHI 97.1% 93.3% 95.2% H+D Non-PHI 99.6% 99.6% 99.6% Method Class P R F SVM PHI 97.4% 93.8% 95.6% SNoW PHI 93.7% 93.4% 93.6% SVM Non-PHI 99.9% 100% 100% SNoW Non-PHI 99.9% 99.9% 99.9% corpus are ambiguous between PHI and non-PHI. W e h ypothesize that gi v en ambiguities in the data, conte xt will play an important role in determining whether the particular instance of the w ord is PHI and that gi v en the man y fragmented sentences in our corpus, local conte xt will be particularly useful. T o test these h ypotheses, we generated a corpus by rei-dentifying the pre viously deidentified corpus with w ords that were ambiguous between PHI and non-PHI, making sure to use each ambiguous w ord both as PHI and non-PHI, and also making sure to co v er all acceptable formats of all PHI (see Section 3). The resulting distrib ution of PHI is sho wn in T able 6. Class T otal # W ords # Ambiguous W ords Non-PHI 19296 3781 P atient 1047 514 Doctor 311 247 Location 24 24 Hospital 592 82 Date 736 201 ID 36 0 Phone 39 0
Our results sho wed that, on this corpus, the SVM-based system accurately recognized 91.9% of all PHI; its performance, measured in terms of F-measure w as also significantly better than all other approaches both on the complete corpus containing ambiguous entries (see T able 7 and T able 8) and only on the ambiguous w ords in this corpus (see T able 9). Method Class P R F SVM PHI 92.0% 92.1% 92.0% IFinder PHI 45.4% 71.4% 55.5% H+D PHI 70.1% 46.6% 56.0% SVM Non-PHI 98.9% 98.9% 98.9% IFinder Non-PHI 95.0% 86.5% 90.1% H+D Non-PHI 92.7% 92.7% 92.7% Method Class P R F SVM PHI 92.1% 92.8% 92.5% SNoW PHI 91.6% 77% 83.7% SVM Non-PHI 99.3% 99.2% 99.3% SNoW Non-PHI 97.6% 99.3% 98.4% Method Class P R F SVM PHI 90.2% 87.5% 88.8% IFinder PHI 55.8% 64.0% 59.6% H+D PHI 59.8% 24.3% 34.6% SNoW PHI 91.6% 82.9% 87.1% SVM Non-PHI 90.5% 92.7% 91.6% IFinder Non-PHI 69.0% 61.3% 64.9% H+D Non-PHI 59.9% 87.4% 71.1% SNoW Non-PHI 90.4% 95.5% 92.9% 6.3 Deidentifying PHI Not F ound in Some medical documents contain foreign or mis-spelled names that need to be ef fecti v ely remo v ed. T o e v aluate the dif ferent deidentification approaches under such circumstances, we generated a corpus in which the names of people, locations, and hospitals were all random permutations of letters. The result-ing w ords were not found in an y dictionaries b ut fol-lo wed the general format of the entity name cate gory to which the y belonged. The distrib ution of PHI in this third corpus is in T able 10.

On this data set, dictionaries cannot contrib ute to deidentification because none of the PHI appear in dictionaries. Under these conditions, proper deiden-tification relies completely on conte xt. Our results sho wed that SVM approach outperformed all other approaches on this corpus also (T ables 11 and 12). Method Class P R F SVM PHI 94.0% 96.0% 95.0% IFinder PHI 55.1% 65.5% 59.8% H+D PHI 76.4% 27.8% 40.8% SVM Non-PHI 99.4% 99.1% 99.3% IFinder Non-PHI 94.4% 91.6% 92.9% H+D Non-PHI 90.7% 90.7% 90.7%
Of only the PHI not found in dictionaries, 95.5% w as accurately identified by the SVM approach. In comparison, the heuristic+dictionary approach ac-curately identified those PHI that could not be found in dictionaries 11.1% of the time, IdentiFinder rec-ognized these entities 76.7% of the time and SNoW g a v e an accurac y of 79% (see T able 13). Method Class P R F SVM PHI 93.9% 96.0% 95.0% SNoW PHI 93.7% 79.0% 85.7% SVM Non-PHI 99.6% 99.4% 99.5% SNoW Non-PHI 98.0% 99.5% 98.7% Method SVM IFinder SNoW H+D Precision 95.5% 76.7% 79.0% 11.1% 6.4 F eatur e Impo rtance As h ypothesized, in all e xperiments, the SVM-based approach outperformed all other approaches. SVM X  s feature set included a total of 26 features, 12 of which were dictionary-related features (e x-cluding MeSH). Information g ain sho wed that the most informati v e features for deidentification were the TW , the bigram before TW , the bigram after TW , the w ord before TW , and the w ord after TW .
Note that the TW itself is important for classifi-cation; man y of the non-PHI correspond to common w ords that appear in the corpus frequently and the SVM learns the f act that some w ords, e.g., the, ad-mit, etc., are ne v er PHI. In addition, the conte xt of TW (captured in the form of unigrams and bigrams of w ords and part-of-speech tags surrounding TW) contrib utes significantly to deidentification.
There are man y w ays of automatically capturing conte xt. In our data, unigrams and bigrams of w ords and their part-of-speech tags seem to be suf ficient for a statistical representation of local conte xt. The global conte xt, as represented within IdentiFinder and SNoW , could not contrib ute much to deiden-tification on this corpus because of the fragmented nature of the language of these documents, because most sentences in this corpus contain only one en-tity , and because man y sentences do not include e x-plicit relations between entities. Ho we v er , there is enough structure in this data that can be captured by local conte xt; lack of relations between entities and the inability to capture global conte xt do not hold us back from almost perfect deidentification. W e presented a set of e xperimental results that sho w that local conte xt contrib utes more to deidentifica-tion than dictionaries and global conte xt when w ork-ing with medical dischar ge summaries. These docu-ments are characterized by incomplete, fragmented sentences, and ad hoc language. The y use a lot of jar gon, man y times omit subjects of sentences, use entity names that can be misspelled or foreign w ords, can include entity names that are ambigu-ous between PHI and non-PHI, etc. Similar doc-uments in man y domains e xist; our e xperiments here sho w that e v en on such challenging corpora, local conte xt can be e xploited to identify entities. Ev en a rudimentary statistical representation of lo-cal conte xt, as captured by unigrams and bigrams of lemmatized k e yw ords and part-of-speech tags, gi v es good results and outperforms more sophisticated ap-proaches that rely on global conte xt. The simplicity of the representation of local conte xt and the results obtained using this simple representation are partic-ularly promising for man y tasks that require pro-cessing ungrammatical and fragmented te xt where global conte xt cannot be counted on. This publication w as made possible by grant num-ber R01-EB001659 from the National Institute of Biomedical Imaging and Bioengineering; by grant number N01-LM-3 -3513 on National Multi-Protocol Ensemble for Self-Scaling Systems for Health from National Library of Medicine; and, by grant number U54-LM0 08748 on Informatics for In-te grating Biology to the Bedside from National Li-brary of Medicine.
 W e are grateful to Professor Peter Szolo vits and Dr . Boris Katz for their insights, and to Professor Carol Doll, Sue Felshin, Gre gory Marton, and T ian He for their feedback on this paper .

