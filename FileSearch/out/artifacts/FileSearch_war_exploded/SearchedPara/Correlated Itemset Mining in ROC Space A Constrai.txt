 Correlated or discriminative pattern mining is concerned with finding the highest scoring patterns w.r.t. a correla-tion measure (such as information gain). By reinterpreting correlation measures in ROC space and formulating corre-lated itemset mining as a constraint programming problem, we obtain new theoretical insights with practical benefits. More specifically, we contribute 1) an improved bound for correlated itemset miners, 2) a novel iterative pruning al-gorithm to exploit the bound, and 3) an adaptation of this algorithm to mine all itemsets on the convex hull in ROC space. The algorithm does not depend on a minimal fre-quency threshold and is shown to outperform several alter-native approaches by orders of magnitude, both in runtime and in memory requirements.
 H.2.8 [ Database Management ]: Database applications X  Data Mining ; F.4.1 [ Mathematical Logic and Formal Languages ]: Mathematical Logic X  Logic and Constraint Programming Algorithms, Theory Itemset Mining, Constraint Programming, ROC Analysis
Correlated pattern mining is amongst the most popular data mining tasks. As opposed to frequent itemset mining, correlated pattern mining involves transactions that belo ng to two different classes and the task is to find those patterns that are correlated with the class attribute, that is, those patterns that are indicators of one of the two classes. In thi s paper, we formalize this task as that of finding the patterns that score high w.r.t. a correlation measure, such as  X  2 information gain, etc., without applying a support thresho ld.
A large number of publications is concerned with corre-lated pattern mining [21, 24, 22], but the problem is also known under the names of interesting itemset mining [3], contrast set mining [1], emerging itemset mining [12], sub-group discovery [26, 18, 16] and discriminative itemset min -ing [8, 9, 27, 13]. Further publications have extended these settings to structured domains, such as graphs, trees and sequences [5, 17, 6, 30]. The popularity of correlated pat-terns is in part due to their use for classification, where the y have been used both as classification rules [30, 5, 9, 27, 13] and as features for building classifiers [6, 11]. The key diffe r-ence between traditional rule learning and correlated patt ern mining approaches is that the former are typically heuristi c and the latter provide guarantees concerning completeness and optimality of the computed solutions.

We revisit the problem of correlated pattern mining using the principles of ROC analysis and Constraint Programming (CP). First, by reformulating correlation measures in PN space (a rescaling of ROC space common in ROC analysis), building on [22, 15], we contribute a theoretical framework that allows a better understanding and comparison of differ-ent bounds used for pruning in correlated pattern mining. More specifically, we investigate various n -support bounds that are used for pruning in correlated pattern mining; thes e are bounds that are based on n different support values. We show that the 2-support bound of [21] is tighter than the 1-support bound proposed by [8], and also show how the 2-support bound can be generalized into a novel 4-support bound, which allows for extra pruning.

Secondly, we formulate the task of correlated itemset min-ing as a constraint programming problem (cf. [10]). The main argument for this formalisation is that it allows us to incorporate additional constraints in the mining proces s in an easy and flexible manner, among which closure con-straints. This extends earlier results in the application o f constraint programming to data mining, by allowing to deal with optimization problems and correlation measures. The resulting approach is incorporated in both an off-the shelf constraint programming solver and a specialized itemset mi ner. Employing the 4-support bound, both implementations are shown to be highly efficient and orders of magnitude faster than the state-of-the-art systems of [21, 9].

Finally, we study how to find all optimal itemsets inde-pendent of the correlation measure. We do this by mining the itemsets on the convex hull in ROC space. We relate this approach to an algorithm by Bayardo et al. for mining interesting itemsets [3]. We show that the hull can be mined effectively and that this set of patterns is surprisingly sma ll.
This paper is organized as follows. First we review earlier work on branch and bound search for correlated patterns (Section 2), and review our approach for formalizing item-set mining in constraint programming (Section 3). Subse-quently, we show how to formalize correlated pattern mining in constraint programming, and prove the correctness of our formalization in Section 4. We perform experiments in Sec-tion 5 which explain in detail the reasons for the significant improved performance of our method compared to other al-gorithms for the basic setting. In Section 6 we study how to mine the convex hull of itemsets. We conclude in Section 7.
While introducing the problem of correlated pattern min-ing, we assume that the reader is familiar with the standard formulation of frequent itemset mining, and focus on the differences between frequent and correlated itemset mining .
We assume we are given a set of possible items I = { 1 , 2 ,...,m } and a dataset D consisting of n examples bel and X t  X  I . It will sometimes be convenient to rep-resent the database as a binary matrix, where D ti = 1 if and only if i  X  X t and D ti = 0 otherwise. Furthermore, T denotes the transaction identifiers T = { 1 , 2 ,...,n } . Given a set of transactions T , T c denotes the set of transaction identifiers in T having class c , { t | t  X  T,y t = c } . We use  X  ( I ) = { t | t  X  T ,I  X  X t } to denote the set of transactions in database D in which itemset I occurs and  X  c ( I ) as short-hand notation for (  X  ( I )) c . The vector  X  ( T ) = ( | T is called the stamp point of transaction set T ; similarly, we define the stamp points of an itemset I as  X  ( I ) =  X  (  X  ( I )) = vector ( p,n ) where p is the frequency of this itemset in T and n is the frequency of this itemset in T  X  .

In correlated itemset mining, we are given a function which computes a correlation score for an itemset, that is, a func-tion f (  X  ( I )), or short-handed f ( I ). One such measure is information gain : where
H ( a,b ) =  X  a Other, similar measures are  X  2 and Fisher scores. These scoring functions have two important properties.

Definition 1. A scoring function is zero diagonal con-vex (ZDC) if it has the following two properties:
Theorem 1. Fisher score, information gain, gini index, and chi-square are ZDC correlation measures.
 Definitions, as well as independent, alternative proofs of t his theorem, can be found in [8, 20, 21]. A plot of  X  2 is given in Figure 1, and illustrates these two properties. A character is-tic of these correlation measures is that they are symmetric : both f ( |T + | , 0) and f (0 , |T  X  | ) are local maxima. Asymmet-ric scoring functions, that is, functions in which only one of f ( |T + | , 0) and f (0 , |T  X  | ) is a local maximum, are also common; examples include Laplace value, confidence and conviction; furthermore, the growth rate used in emerging patterns is also asymmetric [12]. We focus our attention in this paper on the harder, symmetric case, even though our method can be extended to the asymmetric case.

The top-k correlated itemset mining problem can now be defined as: In this paper we focus mostly on this optimization version of the correlated itemset mining problem, for the case that k = 1. An alternative problem is to find all itemsets I  X  I for which f ( I )  X   X  , given a threshold  X  ; our technique can easily be adapted towards these other cases.

Notice that none of these formulations involves a mini-mum frequency threshold. Some authors have defined dis-criminative patterns as those patterns that are frequent in one class and infrequent in another, given two support thres h-olds [19]; in this work we do not consider approaches that require the explicit specification of frequency thresholds .
It is often useful to restrict one X  X  attention to closed item-sets [25, 29], which are itemsets whose supersets have a dif-ferent frequency. Given a set of transactions T , the largest itemset in common between these transactions is  X  ( T ) =  X  t  X  T X t . An itemset I is closed iff I =  X  (  X  ( I )). Closed itemsets form a condensed representation of the set of all frequent itemsets: if there is an itemset with correlation score  X  , there also is a closed itemset with this score. Usu-ally closed itemsets can be found faster. Taking closedness into account we obtain the problem of closed top-k correlated itemset mining .
Most correlated pattern miners employ a branch and bound algorithm to avoid having to enumerate all possible itemset s. They realize this by using bounds on correlation. In our ap-proach, the following result will be essential.

Theorem 2. Let f be a ZDC correlation measure and 0  X  p 1  X  p 2 and 0  X  n 1  X  n 2 . Then The bound states that to find the highest possible score in a rectangle of stamp points, it suffices to check two corners of the rectangle. A proof can be found in the appendix. The bound generalizes earlier bounds, e.g., those of [21, 8] whi ch cover cases where p 1 = n 1 = 0.

We will now first review the bounds that were used in earlier methods and then show how our bound improves on them.
Figure 1: A plot of the  X  2 scoring function, and a threshold on  X  2 .

Figure 3: The 2-support bound in PN-space.

In discriminative itemset mining [8, 9, 27, 13], the search i s performed depth-first, using similar data structures as tra -ditional frequent itemset miners (such as FP-Trees in the DDPMine algorithm [9]). The aim is to prune those item-sets I for which it can be shown that no J  X  I can have f ( J )  X  f ( I  X  ), where I  X  is the best itemset found so far. The main limitation of discriminative itemset miners is tha t during the search, only the total support values of itemsets are determined. The supports on the classes separately are not taken into account. Hence, given an itemset I , we need to derive a bound on f ( J ) for all J  X  I from the value |  X  ( I ) | only. Given this information, the stamp points ( p,n ) of the itemsets J which can possibly be reached satisfy where p = |  X  + ( J ) | and n = |  X   X  ( J ) | . This leads to the following result, which can be used for pruning itemsets. Theorem 3 (1-Support Pruning). Given an itemset I and a ZDC correlation measure f , if then  X  J  X  I : f ( J ) &lt; f ( I  X  ) .

In Figure 2 we visualize this in PN space [15]. Each point in PN space corresponds to a stamp point. An isometric in PN space is a line that connects stamp points with the same score according to a correlation function. For a given optimal itemset I  X  with score f ( I  X  ), we have plotted the isometric f ( p,n ) = f ( I  X  ); stamp points that improve the score f ( I  X  ) are found in the upper-left and lower-right corner (see Figure 1). Furthermore, we have drawn the isometrics p + n = |  X  ( I k ) | for three given itemsets I 1 ,I 2 ,I I 1 can be pruned using the 1-support bound but itemsets I 2 and I 3 cannot be pruned, because given only the total support information |  X  ( I k ) | ( k = 1 , 2), we cannot conclude whether the score of an itemset J  X  I k can exceed f ( I  X  not. The area of itemsets that can be pruned is indicated in gray.

The importance of this bound is that it relates minimum correlation to minimum support, and hence makes it possi-ble to apply traditional itemset miners to correlated items et mining. As can be seen in PN-space, given an itemset I  X  , there is a stamp point ( p  X  , 0) in which f ( p  X  , 0) = f ( I a stamp point (0 ,n  X  ) in which f (0 ,n  X  ) = f ( I  X  ). We can reformulate the pruning condition as follows: As a consequence a correlated itemset miner that uses 1-support pruning can never be more efficient than a frequent itemset miner run with support threshold min { p  X  ,n  X  } .
The restriction that only |  X  ( I ) | is available during the search is not present in the pruning strategy proposed in [21] and used in [21, 24, 22, 6, 30, 5, 17, 27]. This method the search, and, hence, it can be determined that an itemset J  X  I can only have stamp points ( p,n ) with This yields the following pruning strategy.
 Theorem 4 (2-Support Pruning). Given an itemset I and a ZDC correlation measure f , if then  X  J  X  I : f ( J ) &lt; f ( I  X  ) .
 This bound is visualized in Figure 3. In our example we can now also prune itemset I 2 , as the additional support information allows us to determine that the best stamp point we can reach from I 2 is either (0 , |  X   X  ( I 2 ) | ) or ( |  X  and these scores are lower than f ( I  X  ). Also in general the 2-support bound is more powerful than the 1-support bound.
Both 1-pruning and 2-pruning assume that the best reach-able itemset J  X  I covers zero examples in one of the two classes. We will now drop this assumption. Assume that we know for all J  X  I enumerable in the depth first search below I , there is a set U  X   X  ( J ) of unavoidable transac-tions (thus the unavoidable transactions are those that are covered by all refinements of I ). Then the additional sup-port information | U + | and | U  X  | can be used to obtain an improved 4-support pruning condition: This is illustrated in Figure 4, where we are computing a bound for itemsets J with U  X   X  ( J ). We have indi-cated the stamp point  X  ( U ). Observe that I 3 is now also pruned, as any itemset I  X   X  I 3 is assumed to cover at least | U + | positive and | U  X  | negative examples, and both f ( |  X  + ( I ) | , | U  X  | ) &lt; f ( I  X  ) and f ( | U + area of stamp points pruned, as indicated by the gray box in Figure 4, is largest for the 4-support bound. A practi-cal complication is however the need to compute the set U . The simplest solution would be to choose U =  X  , which re-duces the 4-pruning bound to the 2-support bound. Ideally, U should be as large as possible, without affecting the opti-mality of the search. To solve this problem, we will employ the principles of constraint programming that we introduce in the next section. Algorithm 1 Constraint-Search( Dom ( V ), C ) 1: Dom ( V ) :=propagate-till-fix-point( Dom ( V )) 2: if  X  v  X  V : dom ( v ) =  X  then 3: return 4: end if 5: if  X  x  X  V : | dom ( x ) | &gt; 1 then 6: select such an x using some heuristic 7: for all d  X  dom ( x ) do 8: Constraint-Search( Dom ( V ), C  X  { x = d } ) 9: end for 10: else 11: Output solution 12: end if
Constraint programming (CP) is a flexible programming paradigm in which problems are specified declaratively us-ing constraints on variables. More formally, a problem is specified by a set of variables V , for each variable v an asso-ciated domain dom ( v ), and a set of constraints C on possible assignments of values to these variables. In this paper, we shall only employ binary domains, that is, for all variables dom ( v ) = { 0 , 1 } . The problem then is to find an assign-ment of values to all the variables in V that satisfies all constraints in C . Solutions are found using general purpose solvers that combine two main ideas. First, solvers employ constraint propagation pervasively. This means that the do-mains of variables are iteratively reduced by exploiting th e constraints amongst the variables. By shrinking the domain s of the variables, one effectively reduces the number of pos-sible variable assignments to consider. To compute the re-duced domains, constraint programming techniques employ propagators . Secondly, when it is no longer possible to re-duce the domains (and hence a fix-point of the propagation is reached), the solver branches, which means that it select s a variable v (according to some heuristic) and recursively calls the solver while assigning a value to this variable. Th is process is continued until a domain becomes empty and the solver backtracks, or a solution is found. The resulting al-gorithm is summarized in Algorithm 1, where Dom ( V ) rep-resents the set of all domains of the variables V , and C the set of constraints.

In [10] we studied how to formalize constrained itemset mining in CP systems. We will illustrate this for the problem of frequent itemset mining . In its CP formulation we search in parallel for an itemset I and a transaction set T . We use as variables V = { I 1 ,...,I m }  X  { T , ...,T n } ; I i item i ; in a final solution, I i = 1 corresponds to i  X  I ; T t represents transaction t ; T t = 1 corresponds to t  X  T . Moreover, two constraints need to be satisfied: The combination of these constraints expresses the tradi-tional support constraint that |  X  ( I ) |  X   X  must hold. The problem of frequent itemset mining can be reformulated in terms of constraint programming as follows; cf. [10]. Given are a database D and a threshold  X  . The goal is to find I ,T t  X  { 0 , 1 } such that The first constraint is called the coverage constraint, the sec-ond the support constraint. The coverage constraint states that a transaction T t = 1 if and only if t is covered by I , that is, it belongs to  X  ( I ). The support constraint basically states that if an item I i = 1 then the support of i in T should be larger than  X  . To see why these CP constraints correspond to itemset mining constraints (2) and (3), con-sider that P i  X  X  I i (1  X  D ti ) = 0 iff  X  i  X  I : I i = 0  X  D iff I  X  X t iff t  X   X  ( I ). Also, knowing that T =  X  ( I ), we can  X  i  X  I : I i = 1  X  P t  X  X  T t D ti  X   X  .
 A constraint of the form containing a sum within an implication, is called a reified summation constraint . Reified constraints are readily avail-able in many CP systems and in our case allow for pervasive propagation. The propagation for the constraint of equa-tion (6) is performed as follows. Let us define the minimum propagator for constraint (6) is the following: if X This propagator states that when the highest possible value the sum can take is too low, dom ( I i ) is modified to not con-tain the value 1. In a similar way, propagators for other reified summation constraints can be defined; the CP sys-tem will execute a propagator whenever one of its variables is changed (even though the order of execution is often sys-tem dependent). Using these constraints and propagators in Algorithm 1 results in a search that is similar to many specialized depth-first itemset miners. Indeed, once we set dom ( I i ) = { 1 } in line 8 of Algorithm 1, we include item i in the itemset, and recurse for the resulting itemset; in line 1 the propagation ensures that first transactions that do not include item i are set to T max t = 0; then, all items which are no longer frequent in T are set to I max i = 0, and the search recurses over those items that have not been fixed yet.
The key advantage of constraint programming is that it is very general and flexible. In an itemset mining context this manifests itself in that it is easy to incorporate furth er constraints and also to combine them in a complex way. All that is needed to realize this is a formulation of the constraint and a propagator. The CP system takes care that solutions are found. In [10] it has been shown that the above CP formulation of frequent itemset mining can be extended to cope with anti-monotonic, monotonic, succinct , closed, free, fault-tolerant patterns, etc. One example of such a constraint that will be used later on in this paper is that for obtaining closed sets: This closedness constraint is very similar to the coverage constraint and the propagation of this constraint can be shown to emulate the search strategy of LCM [25].
Also for other constraints, other itemset miners are emu-lated; for instance, for monotonic constraints, CP emulate s the pruning strategies of ExAnte and DualMiner [4, 7].
While previously we have shown how to tackle a wide variety of constraint based itemset mining problems with constraint programming [10], the questions were left open as how to tackle top-k queries or to find correlated pat-terns. This section shows how the optimization version of correlated itemset mining can be realized using constraint programming. It can be specified as follows. Given is a database D and a ZDC correlation measure f ; the problem is to find I i ,T t  X  { 0 , 1 } and the maximal value  X   X  R such that the following constraints are satisfied. This formulation is called the CP version of the top-1 corre-lated itemset mining problem. The first constraint is again the coverage constraint, the second one is the correlation constraint. It is shown in the appendix that this reified for-mulation is correct, cf. Theorem 5. As argued before, it is easy to add further constraints to the CP engine in a flexible manner. One constraint that is useful in the context of cor-related pattern mining is the closedness constraint, which shall be used in our experiments. Furthermore, using the 4-support bound, the following propagator can be shown to be sound and complete for the correlation constraint (9), cf. Theorem 6 and its proof in the appendix.
To deal with the optimization criterion in CP, the thresh-old  X  is increased during the search whenever a solution is found. Most CP systems provide functionality for this.
An interesting question is now how the propagation of this propagator differs from the search strategies used in traditional correlated itemset miners. It is important to s ee that the propagator also considers the T min t variables; if T t = 1, transaction t is unavoidable and included in all transaction sets deeper down the search. To see how T min is set we need to consider the propagators for the coverage constraint (8). One propagator is the following: if X Following [7], let us denote by I tail = { i | I max i = 1 } the largest itemset that can still be reached at a specific point in the search space. This propagator states that if for all
Figure 5: Example of how CP prunes the search. items i  X  I tail we have that D ti = 1, i.e. I tail  X  X t know that T min t = 1, that is, transaction t is unavoidable.
CP systems propagate constraints whenever changes in a domain occurs. This leads to two types of propagation that are uncommon in existing correlated itemset miners. First, if the search branches over an item i and excludes it by setting I max i = 0, the data is scanned again, as the removal of this item might make certain transactions unavoidable, which may improve the bounds for remaining items (using the 4-support bound). This contrasts with most itemset miners that only scan the data when an item is added to an itemset. Second, propagation is repeated as long as changes in domains occur, as illustrated in the following example, which is visualized in Figure 5.

In this example we assume that the best itemset found so far is the itemset { 1 } . This itemset reaches a certain score  X  ; the curved lines indicate the isometrics for this score. Th e itemset which we are refining is the itemset { 2 } , the possi-ble items which we can add to this itemset are 3, 4, 5 and 6. Hence, we evaluate the support of itemsets { 2 , 3 } , { 2 , 4 } , is I tail in the initial situation and is the largest itemset that can still be reached; it determines the set of unavoidable transactions by propagating the coverage constraint using propagator (11). Assume that we find that the support of { 2 , 3 , 4 , 5 , 6 } is zero in both classes. The gray box in-dicates the stamp points of itemsets that will be pruned. In our example, { 2 , 6 } falls within this box. Consequently we know that no itemset containing { 2 , 6 } will achieve sufficient correlation. Hence we remove 6 from consideration. This means that the largest itemset we can reach now is item-set { 2 , 3 , 4 , 5 } , whose supports in the classes are evaluated (step 3) to determine the number of unavoidable transac-tions. Even though these supports may be small, the iso-metrics of many correlation measures are such that the set of prunable itemsets becomes significantly larger. In our example we now find that { 2 , 4 } cannot achieve significant correlation. Consequently, we remove item 4 from consider-ation and we evaluate the support of { 2 , 3 , 5 } (step 4). As a result, the prunable region is increased, and we find that no pattern can reach the desired correlation score; we prune the search for all itemsets below { 2 } .
In this section, we report on experiments for top-1 corre-lated itemset mining. We compare several variations of our approach and other state-of-the-art algorithms to get insi ght in the differences in performance between the approaches.
We use data from the UCI repository 1 . To deal with missing values we preprocessed each dataset by first elim-inating all attributes having more than 10% of missing val-ues and then removing all instances (transactions) for whic h the remaining attributes still had missing values. Numeri-cal attributes were discretized in a number of binary splits , setting thresholds for these splits using equal-frequency bin-ning. Nominal attributes are represented using one item for every value. For multi-class problems we combined all small -est classes together in one class. Experiments were run on PCs with Intel Core 2 Duo E6600 processors and 4GB of RAM, running Ubuntu Linux. The code of our implemen-tation and the datasets used are available on our website
To make an empirically fair comparison of the three differ-ent bounds, we implemented them all in a Constraint Pro-gramming system. The system we use is Gecode 3 , which is an open and efficient CP solver. Because of its generality, it was sufficient to plug in a propagator for each of the bounds, and model the problem as defined in Section 4. The result-ing system is called cimcp . The correlation measure used is information gain , although any ZDC correlation measure can be supplied. As variable-selection heuristic the most-constrained heuristic was used; this is comparable with an item ordering by increasing frequency. In our earlier work [10] we noticed that the Gecode system incurs extra over-head on very sparse data. In a specialized itemset min-ing implementation, based on the same principles, this over -head would not occur. To show this, and to show that any principle discovered in CP can be straightforwardly added to existing itemset miners, we implemented the specialized correlated itemset miner corrmine . It implements the same propagation as the CP system, including iterative 4-support pruning and closedness pruning; it is based on ECLAT [28], a memory-efficient depth-first miner. Also this algorithm is provided on our website. In all experiments we only mine for closed itemsets. We found that this is always more efficient.
We will compare our approach with the state-of-the-art ddpmine algorithm, presented in [9]. ddpmine is based on an implementation of FPGrowth by Zhu and Grahne [29], which stores closed itemsets in memory during the search to prevent duplicate itemsets from being found; it uses 1-support pruning. In our experiments the algorithm often ran out of memory; this will be indicated by a  X   X   X  in our result tables. In our experiments we answer the following questions.

In Figure 6 we compare the runtime of ddpmine , cimcp and corrmine on the same dataset, but discretized with a different number of bins. Using more bins, one can expect the correlation score of the correlated itemsets to increas e; http://archive.ics.uci.edu/ml/ http://www.cs.kuleuven.be/  X  dtai/CP4IM/ http://gecode.org Figure 6: Runtime for the anneal dataset, having 812 transactions. The number of items ranges from 53 to 147.
 Table 1: Statistics of UCI datasets, and runtimes, in seconds, of the CP model for the different bounds. however, the size of the transaction database will also in-crease, as will the runtimes. Using 20 bins, the ddpmine algorithm is no longer able to find the optimal solution be-cause of memory problems. To keep runtimes at reasonable levels for all algorithms we use 8 bins in further experiment s.
Table 1 shows the difference in runtime when using the different bounds. The density of a dataset, listed in col-umn 2, is the fraction of 1 X  X  in the binary representation of the transaction database.  X  &gt;  X  indicates that no solution was found within 900 seconds. Using the 1-support bound, we can only find the optima for 10 out of the 18 datasets. Using the 2-support bound, the same optima are found sig-nificantly faster, and for even more datasets. The 4-support bound finds these even faster, and for all datasets. For the mushroom dataset, the 4-support bound takes slightly more time than the 2-support bound, but the next experiment will show that this is due to the overhead incurred in CP sys-tems on data of low density. In general it is clear that using the 4-support bound results in the most pruning power, re-ducing the search-space enough to make exhaustive search feasible, and even fast.

Table 2 shows the result of comparing the corrmine , cimcp and ddpmine implementations on the different datasets. We also compare the runtimes with LCM, one of the fastest, most memory efficient closed itemset mining implementa-tion from the FIMI challenge [25]. We ran LCM with the
Name corrmine cimcp ddpmine lcm anneal 0.02 0.22 22.46 7.92 australian-credit 0.01 0.30 3.40 1.22 breast-wisconsin 0.03 0.28 96.75 27.49 diabetes 0.36 2.45  X  697.12 german-credit 0.07 2.39  X  30.84 heart-cleveland 0.03 0.19 9.49 2.87 hypothyroid 0.02 0.71  X  &gt; ionosphere 0.24 1.44  X  &gt; kr-vs-kp 0.02 0.92 125.60 25.62 letter 0.65 52.66  X  &gt; mushroom 0.03 14.11 0.09 0.03 pendigits 0.18 3.68  X  &gt; primary-tumor 0.01 0.03 0.26 0.08 segment 0.06 1.45  X  &gt; soybean 0.01 0.05 0.05 0.02 splice-1 0.05 30.41 1.86 0.02 vehicle 0.07 0.85  X  &gt; yeast 0.80 5.67  X  185.28 avg. when found: 0.15 6.55 28.88+ 81.54+ Table 2: Runtimes, in seconds, of 3 correlated and one frequent itemset miner. minimum support threshold of equation (1) (see Section 2), obtained by first running our system to find the optimal itemset. It therefore represents the minimal amount of time a 2-phase algorithm would need, which would first enumer-ate all frequent closed itemsets, and in a second step would select the most correlated one. corrmine , our specialized implementation using 4-support pruning, is by far the most efficient on all datasets. The dif-ference in runtime between corrmine and cimcp illustrates the overhead of using the general CP system. ddpmine does not find a solution for many of the datasets (indicated by  X   X   X ). This is a limitation of the pruning of the 1-support bound, as our implementation of the 1-support bound in Ta-ble 1 is not able to find the solution in a reasonable amount of time either. For the mushroom and splice datasets, ddp-mine is faster than cimcp , but the results of our special-ized implementation show convincingly that this is due to the overhead incurred in the Gecode system on sparse data. The runtimes of LCM confirm that a 2-phase algorithm can never be faster than a specialized 1-step algorithm like cor-rmine . In general, using the 4-support bound results in more effective algorithms, with some extra efficiency to be gained by creating a specialized implementation.
In the problem setting of correlated itemset mining we as-sumed we were given a ZDC correlation measure f . The cor-relation measure can be seen as a parameter of the method. In exploratory data mining parameter-free methods are of-ten preferred. We can obtain a parameter-free setting by informally formulating the following problem setting:  X  X a n we find all itemsets for which there exists a ZDC correlation measure under which the itemset is optimal? X  In ROC anal-ysis it is well-known that the problem of finding such item-sets (rules, classifiers) can be solved by determining which ones are on the convex hull in ROC space [14]. Exploit-ing the aforementioned relation between correlated itemse t mining and ROC analysis, we can now apply this result to correlated itemsets.
 Due to lack of space, we here skip a formal definition of a Figure 7: Comparison of an isometric, a convex hull and the border of Bayardo et al.: an isometric is a continuous curve; the convex-hull is piece-wise lin-ear; Bayardo X  X  border also contains points not on the convex-hull. convex hull; intuitively, given a set of points in PN -space, the convex hull consists of all stamp points on a rubber band that surrounds all stamp points as tightly as possible (including the (0 , 0) and ( |T + | , |T  X  | ) points). For symmetric measures, the hull has two sides, one for each of the two classes. An example is given in Figure 7. We are interested in finding all (closed) itemsets on the convex-hull.
To find itemsets on the convex hull we need to implement a new propagator. The main idea behind this propagator is that the convex hull is a curve in PN  X  space, similar to an isometric for a correlation score; this curve is piece-wi se linear between the stamp points on this curve (see Figure 7). Hence, instead of modifying the isometric during the search , by updating the correlation threshold, we will update the piece-wise linear curve when a new solution is found. When adding a new stamp point to the hull, previous hull points that are no longer on the convex hull are removed.
To find all itemsets on the hull, we replace propagator (10) with the following propagator: a stamp point outside the current convex hull. This test is performed as follows. We split the convex-hull in two sets of stamp points: one set of stamp points S  X  above the diagonal, i.e. stamp points ( p,n ) in which n/ |T  X  | &gt; p/ |T and one set of remaining stamp points below the diagonal, S + . Then we check the following:
In Table 3 we compare the runtime of finding the top-1 most correlated itemset, according to the information gain measure, with the runtime needed to find all itemsets on the
Name time (s) time (s) size of hull anneal 0.22 0.44 17 australian-credit 0.30 1.33 22 breast-wisconsin 0.28 0.83 20 diabetes 2.45 11.9 30 german-credit 2.39 3.93 21 heart-cleveland 0.19 0.37 20 hypothyroid 0.71 3.01 19 ionosphere 1.44 8.69 15 kr-vs-kp 0.92 1.75 17 letter 52.66 405.14 34 mushroom 14.11 32.45 10 pendigits 3.68 45.79 19 primary-tumor 0.03 0.07 16 segment 1.45 8.96 6 soybean 0.05 0.09 9 splice-1 30.41 40.13 10 vehicle 0.85 4.12 22 yeast 5.67 25.51 28 average: 6.55 33.03 18.61 Table 3: Mining the top-1 correlated itemset versus all itemsets on the convex hull in ROC space. convex hull. Both are implemented in Gecode. Although more time is always required to mine the entire convex hull, we believe most runtimes are within acceptable boundaries. Except for datasets that incur extra overhead in Gecode, mining all itemsets on the hull is even faster than existing approaches to finding the top-1 itemset. The results show that mining all itemsets on the convex hull is a realistic alternative to having to choose a correlation measure up front. Furthermore, the results in Table 3 also show that the number of patterns is reasonably small, on average 19 itemsets on the entire convex hull.

This algorithm for mining itemsets on the convex-hull is related to the algorithm of Bayardo et al. for finding inter-esting correlated itemsets [2]. Bayardo et al. X  X  method for finding interesting correlated itemsets is an indirect method, in which a border set of itemsets is computed even if the correlation function is fixed; the optimal pattern is always chosen from this border set. Bayardo et al. X  X  border is sim-ilar to the convex hull in ROC space. The main differ-ence is that they define a border of non-dominated item-sets in support-confidence space, which is a transformation of PN-space. Restricting ourselves to one half of PN-space, one itemset I dominates another itemset I  X  iff it dominates both in support and confidence, i.e. |  X  + ( I ) | &gt; |  X  |  X  tion is illustrated in PN-space in Figure 7. In this figure we have highlighted several isometrics for confidence. The border is along iso-support and iso-confidence lines; we see that the border of Bayardo et al. includes redundant stamp points compared to the convex-hull representation.
We have revisited the problem of correlated pattern min-ing starting from an analysis of correlation measures in PN-space and principles of constraint programming. This has al -lowed us to clarify the relationship between different bound s used for pruning and has also led to the introduction of a new bound that allows for significantly more pruning. The approach has been empirically evaluated and shown to out-perform state-of-the-art methods, either in terms of speed or memory requirements. Finally, we have adapted our tech-nique for finding the set of all itemsets on the convex hull, a problem of relevance to the machine learning and classifi-cation literature.

Several challenges still remain. First, it is unclear in how far our results can be extended towards structured domains, as the approach depends on an upper-bound which may not be present in some structured pattern domains. Sec-ond, even though our algorithm can be plugged into many existing classification algorithms, an interesting questi on is whether it can be integrated in some algorithms exploiting ROC spaces [23].

We are grateful to Albrecht Zimmermann for discussions, and to Hong Cheng for providing the implementation of DDPMine. This work was supported by a Postdoc and a project grant from the Research Foundation X  X landers, project  X  X rinciples of Patternset Mining X . [1] S. D. Bay and M. J. Pazzani. Detecting change in [2] R. J. Bayardo Jr. and R. Agrawal. Mining the most [3] R. J. Bayardo Jr., R. Agrawal, and D. Gunopulos. [4] F. Bonchi and C. Lucchese. Extending the [5] B. Bringmann and A. Zimmermann. Tree 2 -decision [6] B. Bringmann, A. Zimmermann, L. De Raedt, and [7] C. Bucila, J. Gehrke, D. Kifer, and W. M. White. [8] H. Cheng, X. Yan, J. Han, and C.-W. Hsu.
 [9] H. Cheng, X. Yan, J. Han, and P. S. Yu. Direct [10] L. De Raedt, T. Guns, and S. Nijssen. Constraint [11] M. Deshpande, M. Kuramochi, N. Wale, and [12] G. Dong and J. Li. Efficient mining of emerging [13] W. Fan, K. Zhang, H. Cheng, J. Gao, X. Yan, J. Han, [14] T. Fawcett. An introduction to ROC analysis. Pattern [15] J. F  X  urnkranz and P. A. Flach. ROC  X  X  X  rule learning  X  [16] H. Grosskreutz, S. R  X  uping, and S. Wrobel. Tight [17] M. Hirao, H. Hoshino, A. Shinohara, M. Takeda, and [18] B. Kavsek, N. Lavrac, and V. Jovanoski.
 [19] S. Kramer, L. De Raedt, and C. Helma. Molecular [20] Y. Morimoto, T. Fukuda, H. Matsuzawa, [21] S. Morishita and J. Sese. Traversing itemset lattice [22] S. Nijssen and J. N. Kok. Multi-class correlated [23] R. C. Prati and P. A. Flach. ROCCER: An algorithm [24] J. Sese and S. Morishita. Answering the most [25] T. Uno, M. Kiyomi, and H. Arimura. Lcm ver. 2: [26] S. Wrobel. An algorithm for multi-relational discover y [27] X. Yan, H. Cheng, J. Han, and P. S. Yu. Mining [28] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. [29] J. Zhu and G. Grahne. Reducing the main memory [30] A. Zimmermann and B. Bringmann. CTC -
Theorem 2. Let f be a ZDC correlation measure and 0  X  p 1  X  p 2 and 0  X  n 1  X  n 2 . Then
Proof. The proof is similar to that of [21]. First, we observe that the function is convex . Hence, we know that the maximum in a space [ p 1 ,p 2 ]  X  [ n 1 ,n 2 ] is reached in one of the points ( p 1 ,n 1 ), ( p 1 ,n 2 ), ( p 2 ,n 1 ) and ( p we need to show that we can ignore the corners ( p 1 ,n 1 ( p 2 ,n 2 ). For this we observe that the minimum is reached on the diagonal. We can distinguish several situations. diagonal. We know for the stamp point ( |T + | |T diagonal that f ( |T + | |T know then that f ( |T + | |T
Similarly, we can show that if ( p 1 ,n 1 ) is above the diago-nal that f ( p 1 ,n 1 )  X  f ( p 1 ,n 2 ); that f ( p 2 ,n ( p 2 ,n 2 ) is below the diagonal; and that f ( p 2 ,n 2 )  X  f ( p if ( p 2 ,n 2 ) is above the diagonal.

Theorem 5. The CP version of top-1 correlated itemset mining is equivalent to the top-1 correlated itemset mining problem.

Proof. We show that the itemset I = { i | I i = 1 } satis-fying the CP version of the problem is a solution to the orig-inal top-1 correlated itemset mining problem. We already showed that the two versions of the coverage constraint (4) and (2) are equivalent. Hence, we only need to show that the correlation constraint (9) ensures that f ( |  X  + ( I ) | , |  X   X  . To see this, observe that we can rewrite  X  ( I ) as follows: Using this observation, it follows that:  X  X  X  f (  X  X  X   X  i  X  I :  X  X  X   X  i  X  I : f (  X   X   X  + ( { i } )  X   X  + ( I )  X   X  ,  X   X  X  X   X  i  X  I : I i = 1  X  f ( X Finally, because  X  is required to be maximal in the CP for-mulation, an itemset reaching at least score  X  is a top-1 itemset.
 Theorem 6 (Propagator for Reified Correlation).
 The propagator in equation (10) is sound and complete for the correlation constraint (9).

Proof. (Soundness) Given the current values for T max t and T min t , We know that the stamp points  X  that we can orem 2, the best score we can still reach is given by the two corners checked in the propagator. If the condition of the propagator is no longer satisfied, we know that the condition I = 1 can no longer be satisfied, and change the domain. (Completeness) We need to show that if all variables are fixed, the constraint is not satisfied iff the propagator deriv es a contradiction. This follows from the fact that if all vari-ables are fixed, the two stamp points checked in the propaga-tor are the same stamp point, i.e., the stamp point occurring in the constraint.
