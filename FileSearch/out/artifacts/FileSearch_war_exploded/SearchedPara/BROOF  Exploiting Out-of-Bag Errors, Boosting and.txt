 Random Forests (RF) and Boosting are two of the most suc-cessful supervised learning paradigms for automatic classi-fication. In this work we propose to combine both strate-gies in order to exploit their strengths while simultaneously solving some of their drawbacks, especially when applied to high-dimensional and noisy classification tasks. More specifically, we propose a boosted version of the RF clas-sifier (BROOF), which fits an additive model composed by several random forests (as weak learners). Differently from traditional boosting methods which exploit the training er-ror estimate, we here use the stronger out-of-bag (OOB) error estimate which is an out-of-the-box estimate naturally produced by the bagging method used in RFs. The influ-ence of each weak learner in the fitted additive model is inversely proportional to their OOB error. Moreover, the probability of selecting an out-of-bag training example is increased if misclassified by the simpler weak learners, in order to enable the boosted model to focus on complex re-gions of the input space. We also adopt a selective weight updating procedure, whereas only the out-of-bag examples are updated as the boosting iterations go by. This serves the purpose of slowing down the tendency to focus on just a few hard-to-classify examples. By mitigating this unde-sired bias known to affect boosting algorithms under high dimensional and noisy scenarios X  X ue to both the selective weighting schema and a proper weak-learner effectiveness assessment X  X e greatly improve classification effectiveness. Our experiments with several datasets in three representa-tive high-dimensional and noisy domains X  X opic, sentiment and microarray data classification X  X nd up to ten state-of-the-art classifiers (covering almost 500 results), show that BROOF is the only classifier to be among the top perform-ers in all tested datasets from the topic classification domain, and in the vast majority of cases in sentiment and microar-T his work was partially supported by CNPq, CAPES, FINEP, FAPEMIG and INWEB.
 ray domains, a surprising result given the knowledge that there is no single top-notch classifier for all datasets. I.5.4 [ Applications ]: Text processing Algorithms; Experimentation Classification; Random Forests; Boosting
This paper advances the state of the art in automatic clas-sification by  X  X moothly X  combining, in an original way, char-acteristics of two learning strategies which excel in a variety of learning tasks: the Random Forest classifier and boost-ing. Proposed in [2], the Random Forest (RF) classifier has been surprisingly successful in a wide variety of automatic classification tasks, being considered by many [6, 14] as a top-notch supervised algorithm, comparable (sometimes su-perior) to Support Vector Machine (SVM) classifiers.
The RF classifier is a variation of the classic bagging (bootstrapped aggregation) procedure, where an ensemble of m decision tree classifiers are trained with sets of boot-strapped samples 1 drawn from the training set D train . The crucial aspects, as pointed out in [2], that make the RF classifier a good classifier are ( i ) the reduced correlation be-tween the decision trees composing the ensemble and ( ii ) the better-than-random-guess predictions of each tree. In fact, to achieve reduced correlation, each tree is learned with a bootstrapped sample of D train and each decision node is spec-ified by a chosen attribute drawn from a randomly chosen subset of features. To achieve strong decision trees (i.e., trees with better prediction performance than random guess-ing), each tree is typically grown to its maximum depth. Taken together, the RF classifier averages several de-corre-lated (rather complex) models, ultimately reducing variance while keeping the bias unchanged [14]. This typically brings up better models, with higher generalization power, that can successfully be applied to a wide variety of datasets.
A very important aspect for the sake of our work, is that the RF classifier is able to produce robust estimates of ge-neralization power, the so-called out-of-bag (OOB) error es-timate, which comes naturally from the bagging procedure
E xamples randomly sampled with replacement. this classifier exploits. As the RF classifier bootstraps the t raining dataset, roughly e  X  1 training examples are left out when learning a decision tree 2 . These examples, the so-called OOB samples, can be effectively used to assess gener-alization power of the tree. Considering the tree ensemble, an averaged estimate can be used to assess the generalization power of the RF classifier. As it is already known, the out-of-bag estimates are efficient to compute, since they can be measured when learning the classifier, with little additional effort. Moreover, it is capable of producing close approxima-tions of the expected error rate, which is unbiased (unlike the training error rate). As we shall see, the exploitation of the OOB error is one of the core aspects of our proposal.
However, RFs are not free of drawbacks. Despite being a classifier with great generalization capabilities, when learned from noisy data, common in datasets based on human lan-guage or imprecise measurements, RFs have been known to cause overfitting [25], degrading classification effectiveness on unseen test data. More specifically, it has been shown that the RF classifiers whose decision trees are grown to their maximum depth are deemed to perform poorly in the presence of many noisy attributes (e.g., rare, anomalous, irrelevant, with low discriminative power and low general-ization in the test sets).

In fact, [25, 27] validate the overfitting issue faced by ran-dom forest models when learning to classify high-dimensional noisy data. In both works, the authors showed that there ex-ist some data distributions where maximal unprunned trees used in the random forests do not achieve as good perfor-mance as the trees with smaller number of splits and/or smaller node size. This has to do with the unnecessary variance [14] incurred by the model: the bagged decision trees become plagued by irrelevant or noisy attributes, which introduces unnecessary model complexity. This ultimately hampers its effectiveness when applied to new unseen data. However, if one simply prunes the RF X  X  trees, the over-all classification effectiveness may be compromised in high-dimensional classification tasks plagued by irrelevant attri-butes, since the probability of selecting informative attributes is reduced [18]. This is a critical issue, since in real world problems it is entirely conceivable that one can be confronted with noisy, skewed, correlated and high-dimensional data.
As we shall see in our experiments, contrasting the ef-fectiveness of the traditional Random Forest classifier in a series of datasets X  X haracterized by high dimensionality and noise X  X gainst the baselines we can observe that the effec-tiveness of such a classifier is often not satisfactory, perform-ing poorly when compared to other classifiers in most cases. This confirms some of the posed arguments regarding the drawbacks of this algorithm in the target scenario.
Another well studied and successful classification tech-nique is boosting. Recent studies [3, 10] have shown that boosting provides excellent predictive performance across a wide variety of tasks. It is an iterative algorithm which com-bines several models, called weak-learners, which influence the final decision proportionally to their accuracy. Further-more, each successive classification model focuses on even more complex regions of the input space, by means of input data manipulation. Two aspects play a key role here: ( i ) the
T he constant e denotes the base of the natural logarithm. e  X  1  X  0 . 37. influence of each learner in the fitted additive model, and ( ii ) the focus on hard to classify regions of the input space. More importantly, since boosting combines the predictions of multiple weak-learners it also is able to significantly re-duce unnecessary variance. Thus, the fundamental aspects that must be defined include: ( i ) how to re-weight the train-ing examples in each subsequent iteration, and ( ii ) how to weight each single prediction in order to generate the final prediction. Such weights are usually based on the in-sample error rate. The downside is that it is an overly optimistic estimate of generalization performance.

The above discussion highlights potential opportunities to combine random forests and boosting in order to exploit their strengths while, at the same time, helping to solve some of their drawbacks, especially for automatic classifica-tion of high-dimensional noisy data. That is exactly what we do. In more details, we propose to exploit a boost-ing strategy to guide a sequence of random forest classi-fiers in specific sub-regions of the input space (those with higher uncertainty), ultimately constraining the input space search to relevant sub-regions and thus mitigating the over-fitting issue by avoiding noisy and irrelevant features. This has the potential to circumvent some of the aforementioned drawbacks that may degrade the performance of the tradi-tional RF classifier under high-dimensional noisy classifica-tion tasks. However, differently from traditional boosting methods, which uses the training error estimate as a basis to update the ensemble, we here propose to use the stronger out-of-bag (OOB) error estimate which is out-of-the-box es-timate naturally produced by the bagging method used in RFs. In such way, the random forest classifier can produce much more robust estimates for generalization capability, which can be exploited by the boosting process both to re-weight training examples and to weight the sequence of pre-dictors in order to come up with a more robust final clas-sification model. A second important difference is that we employ a less aggressive selective weight update. In other words, by selectively updating only the weights associated with the out-of-bag examples, we slow down the undesired bias of the boosting strategy towards a few hard-to-classify examples, which helps to prevent overfitting, while still driv-ing down misclassification rate of the ensemble.

In sum, the main contribution of this work is an advance in the state of the art in automatic classification by the proposition of a new, original classifier (BROOF), which smoothly combines properties of RFs and boosting. The originality of our proposal comes from two aspects: (i) we propose a new weighting scheme which is able to greatly improve the boosting strategy by means of a much more re-liable measure of generalization power, offered by the RF itself: the out-of-bag error estimate, and (ii) we also pro-pose to exploit a selective weight updating scheme, which slows down the undesired bias towards a few input points, thus preventing overfitting while boosting effectiveness. We demonstrate that both contributions, as well as their inter-actions, are equally important for our improvements. More-over, our experiments (covering almost 500 results) show that, in all cases, the proposed BROOF is the only classifier to be among the top performers in all tested datasets from the topic classification domain, and in the vast majority of cases in sentiment and microarray domains, each composed by a series of high-dimensional and noisy datasets and a rich s et of 10 state-of-the-art representative baselines. This is a surprising result given the knowledge that there is no single top-notch classifier for all datasets. This highlights the ben-efits of such a synergy between both widely used strategies. Moreover, we also empirically demonstrate that our classi-fier is more  X  X raining-efficient X , i.e., it requires less training than the baselines to produce its best results.
This work deals with two well known learning strategies, the RF classifier, as well as the meta-algorithm boosting. As we shall detail, we here propose to take advantage of a RF X  X  key characteristic to provide a very accurate boosting clas-sifier, which, as we empirically show, excels in automatic classification of high-dimensional and noisy data. In this section, we discuss some relevant work on these matters, pointing out our motivations and tying together with the literature the major contributions of this work.
The RF classifier was proposed in [2] as a variation of bagging of decision trees. An ensemble of low-correlated de-cision trees is built through a series of random procedures, such as bootstrapping of training data and random attribute selection to compose the decision nodes. A plethora of pro-posals aimed at improving the RF effectiveness can be found in the literature, usually characterized by reducing the corre-lation among the trees composing the ensemble. We review some of them here. Since our proposal is based on boosting, considering the RF classifier as weak-learner, we also discuss some important work on this matter.

The popularity of RF classifiers is highlighted by their successful application in several domains, such as object segmentation [24] and human pose recognition [26], outper-forming state-of-the-art strategies, such as the SVM classi-fier. Thus, it is natural to expect several extensions to such classifier, in order to improve its effectiveness even more.
In this direction, in [20] two strategies were proposed to improve RF by increasing the strength of the decision trees or reducing the correlation among them. The author pro-poses the use of several attribute evaluation metrics to select the attributes to be used in decision nodes, and the inclu-sion of a sophisticated weighted voting schema which takes into account the effectiveness of decision trees when applied to the training instances most similar to the test instance. In order to introduce more randomness (and thus, reduce the correlation among the decision trees), in [21] the so-called Rotation-RF was proposed. This strategy randomly groups the attributes, building subproblems which are pro-jected into new feature subspaces, reduced via PCA. The principal components are, then, used to learn the classifier.
Some previous work dynamically build the RF classifier, taking a closer look into the potential each decision tree in the ensemble has to increase the overall ensemble effective-ness. More specifically, in [29], the RF classifier is modified in order to keep just the most useful decision trees in the ensemble. When classifying a new example x , the decision trees with highest prediction effectiveness considering the out-of-bag examples most similar to x receive a higher vote in the final prediction average. That is, the final prediction is a weighted average of the individual decision trees, whereas each weight is proportional to the prediction performance of the trees considering the out-of-bag training examples in the neighborhood of x .

Aware of the potential overfitting issue faced by boost-ing strategies when confronted with noisy data, in [1] the authors propose an improved random forest classifier which exploits the adaptive re-sampling principle of boosting: the dynamic random forests classifier. Instead of boosting ran-dom forest classifiers, as we do here, the authors propose to proceed with the usual bagging approach of a traditional RF. However, when learning the decision trees composing the ensemble, an additional sample re-weighting schema (re-minding boosting by re-sampling) is followed in order to build new decision trees that focus more on hard-to-classify regions of the input space. Starting with a uniform probabil-ity mass assigned to every training example, as new trees are added to the ensemble, the current misclassification rate of the out-of-bag samples are computed and used to re-weight them. When learning the next decision tree to be added in the ensemble, the weights of each training sample are used to influence both the splitting criterion and the base prediction. As stated by the authors, this serves the pur-pose of guiding the ensemble to hard-to-classify regions of the input space in a slower and smoother way (one only updates the weights of out-of-bag samples), thus avoiding the bias towards a few hard-to-classify examples. Thus, in-stead of applying boosting and then bagging (such as in the proposed BROOF classifier), the authors propose to apply bagging and then boosting. As the authors observed in the reported experimental evaluation, such strategy is not ro-bust to the presence of many irrelevant attributes. In fact, the compromise between compensating previous errors (by means of sample re-weighting) and reinforcing correct pre-diction (by means of relating the weights to the accuracy of the entire ensemble on the previous iteration) may not be enough to guarantee robustness to datasets with many irrelevant attributes, since overfitted trees may lead to mis-leading weights due to the presence of many irrelevant paths on the decision trees, specially on the initial iterations. Since the evaluation of the entire ensemble is critical for the re-weighting step on the next iteration, and the previous en-semble state may be already overfitted, the errors may be unwittingly propagated as the random forest is built, being not robust to such high dimensional noisy data.

In sum, most of the previous work has tackled issues re-lated to improving the choice of features or the quality of the forest of trees. Here, we tackle a different issue, trying to overcome problems related to overfitting, known to hap-pen in RFs in the presence of noisy/irrelevant data mainly in highly dimensional problems.

In the light of the peculiarities observed in high dimen-sional noisy classification tasks, here we focus on exploring the RF classifier in a boosting strategy in order to provide a highly effective learner for such tasks, by means of a novel approach which is able to overcome the mentioned challenges regarding both the RF classifier and the boosting strategy. We also revisit the state-of-the-art in automatic classifica-tion, providing an in-depth comparison of RF-based classi-fiers (including BROOF) with a series of baseline algorithms known to produce highly accurate predictions, considering three representative automatic classification domains cove r-ing a variety of high dimensional and noisy datasets.
Before delving into our proposed strategy, we briefly dis-cuss some work regarding boosting algorithms for classifi-cation. Boosting [23] is a meta-algorithm which employs a general voting method for learning from a sequence of (sim-ple) models. It is a sequential algorithm which learns several weak-learners (that is, classifiers able to produce predictions slightly better than random guessing) to devise very accu-rate predictions. For each iteration i of the boosting algo-rithm, let  X  i be a probability distribution over the training set of size m . When i = 0,  X  0 ( i ) = 1 m ,  X  i | m i =1 i &gt; 0, for each training example x k , if x k is misclassified, its weight is increased so that, in the next iteration, an updated training set distribution  X  i +1 is considered, which puts more emphasis on the previously misclassified examples (i.e., the hardest to classify ones).

The perhaps most widely used strategy is AdaBoost, which has been shown to significantly improve the performance of base learners. Based on the traditional AdaBoost, sev-eral boosting strategies have been proposed in the literature, ranging from the updating strategy along the boosting iter-ations, to the choice of weak-learners. Regarding multiclass boosting, there are some variations of the AdaBoost algo-rithm, such as the AdaBoost.M1 and AdaBoost.M2, where the main distinctions have to do with the assumed weak-learning conditions, as detailed in [19].

Of special importance here, we cite the boosted version of the RF classifier, proposed in [13] (here called ADARF ), further studied in [32] and successfully applied in domains such as breast cancer survivability prediction [28] and face detection [31]. As the name suggests, it explores the ran-dom forest classifier as weak-learner in a boosting algorithm based on AdaBoost. In [13] such a technique was shown to perform well in a traffic flow problem, outperforming the traditional AdaBoost algorithm. In [31], ADARF outper-formed eight classifiers considering the UCI X  X  glass dataset. Finally, in [28], ADARF outperformed ten classifiers (in-cluding AdaBoost, Decision Trees, Na  X   X ve Bayes and SVM) considering a breast cancer survivability dataset. We here inspire ourselves in the promising synergy between boosting and Random Forests in order to tackle the challenging text classification task. As we shall see, there is opportunity to leverage automatic classification quality by exploring some advantages of both algorithms. However, we performed such integration with different strategies, which we believe are more suitable to our target domains.

In fact, boosting algorithms have their limitations. Al-though the boosting strategy was initially considered to be immune to overfitting, it is already known that such strategy suffers from this problem, specially in the presence of noisy data [22, 30]. The overfitting problem can be observed when inconsistent data (e.g., misclassified training data) is given to the learner. It becomes even more critical if the employed weak-learner is too strong to fit the model. That is, if instead of combining predictions of simple learners, one combines several complex hypothesis, such as done in [13, 28, 31, 32], chances are that the model becomes even more overfitted to noise data. Under an overfitted situation, the boosting al-gorithm may focus on erroneous regions of the input space, leading to suboptimal decision boundaries. In fact, such issue presents a challenge to boosting algorithms when ap-plied to real-world datasets [8, 12], and it can be even more challenging if one consider real-world data, such as textual data (due to not only noise but also to some aspects inher-ently related to natural languages, such as ambiguity and the dynamics of the language, to name a few).

We thus propose to use the RF classifier as weak-learner for a boosting algorithm. But, unlike previous work which did not deal with the challenging high dimensional and noisy automatic classification scenario, we here pay special atten-tion to the overfitting issue. Towards this end, we take advantage of the OOB estimation, readily available when building such a classifier, as well as of a selective weight up-dating strategy, in order to circumvent the overfitting issue which can arise from both the stronger nature of the RF clas-sifier and to the intrinsic characteristics of such challenging data. Next we describe in details our proposal.
In this section, we describe the proposed learning algo-rithm: the BROOF classifier, a smooth combination of boost-ing and random forests aimed at exploiting their strengths while simultaneously solving some of their drawbacks in noisy and high dimensional settings.

BROOF is an additive model composed of several random forest classifiers, which act as weak-learners. Each fitted model influences the final decision proportionally to its ac-curacy, focusing X  X s the boosting iterations go by X  X n ever more complex regions of the input space, in order to drive down the expected error. As usual in a boosting strategy, two aspects play a key role: ( i ) the influence of each learner in the fitted additive model, and ( ii ) the strategy to update the sample distribution  X  i based on the previous boosting iteration i  X  1. As we shall describe next, we deal with both aspects by means of the out-of-bag (OOB) error estimate computed when learning each random forest classifier.
The out-of-bag error yields an estimate of the expected error, sometimes referred to as the generalization error, and is directly computed when learning a random forest classi-fier. Recall that such a classifier uses bootstrapped exam-ples to build a series of decision trees. For each decision tree, roughly 1 e  X  3 7% training examples are left out by the sam-pling procedure [14]. Unlike the training error estimation, which considers the same data used to learn the model to assess misclassification rate, these out-of-bag examples gets classified by the corresponding decision trees, acting as an independent set (since they were not used to learn the tree). The final classification is given by averaging all predictions for each example. In fact, it can be shown [14] that the OOB error estimate is less biased than the training error estimate usually adopted to weight the weak-learners votes in traditional boosting methods. Consequently, it leads to better approximations for the expected error rate. This is rather an expected result since the out-of-bag examples are not used to learn a classifier, similarly to cross validation procedures to assess an unbiased estimate of expected error rate. Thus, we hypothesize that a less biased combination of weak-learners (by means of a proper weight calculation through OOB, as we shall see) potentially leads to an im-p roved boosting algorithm.

More formally, let D trn be the training set. Initially, all training samples have the same associated probability mass w . Such sample probability distribution (i.e., w i ,  X  i ) is em-ployed to drive the ensemble towards hard to classify re-gions of the input space. Let h i be a random forest classifier trained at iteration i of the boosting algorithm, consider-ing the bootstrapped samples D rnd  X  D trn . The out-of-bag samples O  X  D trn \ D rnd are used to assess the accuracy of h i . Considering the samples x  X  O , those misclassified by h i typically lie down in hard to classify regions of the input space. Thus, if the model focuses on such regions, the ensemble capability of accurately covering such complex regions increases. We do so by considering a weighted out-of-bag error estimate, denoted by OOB w err , when updating the sample distribution. In this case, the probability of correctly classifying the currently misclassified out-of-bag samples will tend to increase as the iterative process continues.
The proposed strategy to update the probability distribu-tion  X  i at boosting iteration i , which associates a probability mass with each training example x , is based on a weighted error estimate OOB w err . It considers not only the misclassifi-cation of x by the weak learner but also the probability mass in the previous boosting iteration, which is proportional to the complexity of classifying x . More specifically, the prob-abilities associated to the out-of-bag examples are updated is the indicator function. As the iterative process continues, the influence of h i predictions are properly updated (accord-ing to h i effectiveness when classifying the out-of-bag sam-ples), as well as the probability mass associated with each
Recall that, as previously discussed, boosting algorithms tend to overly increase the weights of a few hard-to-classify examples, specially in the presence of noisy data. In the proposed algorithm, just the probabilities w i related to the selected out-of-bag examples are updated at each round, mi-nimizing the undesired bias towards these hard-to-classify examples X  X hus reducing the negative effect of the overfit-ting problem X  X hile still driving down the misclassification rate. That is, we hope that the weak learner h i +1 will have better generalization power than the weak learner h i with-out overfitting to hard-to-classify regions, while maintaining high classification effectiveness due to the stronger nature of the base-learners. We summarize the proposed method in Algorithm 1. The final prediction rule is then given by an additive combination of the weak-learners, weighted by  X  m
In order to evaluate the effectiveness of our proposal, we contrast BROOF against a large set of baselines covering a variety of learning paradigms, including traditional ones X  SVM, KNN, Decision Trees (DT), Na  X   X ve Bayes (NB) X  X s well as  X  X atural baselines X : Random Forests, AdaBoost.M2, Gradient Boosted Decision Trees and the closest approach to ours: ADARF. In the following, we provide a brief de-Algorithm 1 B ROOF: Pseudocode 1: f unction Train ( D trn ) 3: for each m = 1 to M do 8: end for 9: end function scription of the experimental workload (i.e., classifiers and d atasets) 3 . Then, we report and discuss our results.
In our experiments, we considered six real-world textual datasets for topic classification, ten for sentiment classifica-tion, and six for microarray classification, covering overall 22 datasets. As we shall detail, we consider 11 classification algorithms and two evaluation metrics. This encompasses a set of 484 results, an extensive experimental evaluation.
As the traditional classifiers (SVM, KNN, DT, NB) are well known in the literature, here we only describe in details the less known ones or those more similar to our approach.
AdaBoost.M2 (ADA.M2) is an extension of the tradi-tional AdaBoost classifier for multiclass classification prob-lems [7]. For each boosting iteration, a bootstrap sample of the training set is picked in order to learn the weak classifier. A pseudo-loss function is then computed in order to update the weights of each training example and the weight associ-ated to the weak-learner which reflects its influence in the final model. We here adopt as weak-learner the so-called de-cision stumps X  X ecision trees with a single decision node X  as it is the most common strategy. We also consider what we call AdaBoost.FGDT (ADA.FGDT), where full grown decision trees are used as weak-learners [3]. Some previous work have shown that this strategy may outperform boosted decision stumps.

AdaBoost.RF (ADARF) is an AdaBoost-based clas-sifier in which a random forest classifier acts as a weak-learner [13, 32]. Unlike our approach, here the usual weight-ing strategy is adopted, where the in-sample error rate is key when updating the probability mass associated with each training example as the boosting iterations go by. Simi-larly to the AdaBoost algorithm, the weight of a misclassi-fied example is increased, enabling the boosting procedure to concentrate on the hard examples in the training set in subsequent rounds. Such weighting schema, which is based on training error and updates all training instances as the boosting iterations go by is prone to overfitting in scenar-ios such as the observed in text classification tasks, since it tends to focus on a few hard-to-classify examples, as previ-ously mentioned.

Gradient Boosted Decision Trees (GBDT) . Unlike the AdaBoost variants described above, here we fit an addi-tive model in a stage-wise manner, in the form of a typical
A ll reference datasets can be downloaded at http://homepages.dcc.ufmg.br/~tsalles/broof/ . Our implementation of the classifiers will be available soon under request. functional gradient descent procedure. At each iteration a n ew tree that best minimizes a loss function L is added to the model, according to L  X  X  negative gradient. Since our task is classification, we optimize for the deviance loss function [9]. We also consider its stochastic counterpart (SGBDT), by fitting trees considering a random subset of training data (thus reducing the variance of the final model).
One of the challenges when categorizing textual data into topics is that textual documents are usually represented by a great amount of features (high dimensionality) and most of them could be irrelevant or noisy, due to inherent proper-ties of natural languages, such as the presence of synonyms, ambiguities, to name a few. However, despite being a chal-lenging application domain, it is of great importance nowa-days, due to its wide applicability and demand. In order to evaluate the BROOF classifier for topic categorization, we consider here six real-world datasets, related to computer science articles (ACM), news (REUT and UniRCV1), web pages (4UNI) and medicine (MEDLINE). Due to space re-strictions, a detailed description of each can be found in an online appendix 4 .

For all datasets, we performed a traditional preprocessing task: we removed stopwords, using the standard SMART list, and applied a simple feature selection by removing terms with low  X  X ocument frequency X  (DF) 5 . In particular, in the case of the original RCV1 dataset, a multi-label one, the multi-label cases need special treatment, such as score thresh-olding, etc. (see [17] for details), in order to be properly consumed by unilabel classifiers. As our current focus is on unilabel tasks, to allow a fair comparison among the other datasets (which are also unilabel) and all baselines (which also focus on unilabel tasks), we decided to remove the docu-ments assigned to more than one class from RCV1, deriving a new dataset which we call UniRCV1 . This collection has 101 classes and about 20% less documents. Nevertheless, as we shall see, the effectiveness levels obtained by our method and the best baselines are still compatible with those of the original multilabel RCV1.

In terms of baseline implementations, for SVM we use the liblinear library [5] while for the other algorithms we developed our own implementations. In particular, for NB we adopt the Multinomial Na  X   X ve Bayes approach. The free parameters of these classifiers include the cost C for SVM, neighborhood size k for KNN and minimum number of ex-amples  X  min in a leaf node for DT. For the Random For-est (RF) algorithm, we considered unprunned trees, since it is already known that unprunned trees perform better than pruned trees when applied to high dimensional noisy datasets [18]. The free parameter of this classifier has to do with the number of trees to compose the ensemble. Fi-nally, all the training and test examples were represented by TFIDF vectors for all algorithms, except for Multinomial NB (whose optimal performance was obtained using TF).
The methods were compared using two standard informa-tion retrieval measures: micro averaged F 1 (microF 1 ) and http://homepages.dcc.ufmg.br/ ~tsalles/broof/ .
We removed all terms with DF  X  5. macro averaged F 1 (MacroF 1 ). While the MicroF 1 mea-sures the classification effectiveness over all decisions (i.e., the pooled contingency tables of all classes), the MacroF measures the classification effectiveness for each individual class and averages them.

All experiments were executed using a two-round 10-fold cross-validation procedure. This is a cross-validation proce-dure in which the free parameters of each classifier are set by means of an additional cross-validation step over the training set and the effectiveness of the algorithms are measured in the test partition 6 . Concerning the proposed BROOF classi-fier, we fixed the size of the weak-learners to 5 trees, setting the maximum number of iterations to 200. For the RF clas-sifier, we learned at most 200 trees and, for all datasets, convergence was reached with at most 100 trees. We as-sess the statistical significance of our results by means of a paired t-test with 95% confidence. The obtained results can be found in Table 1. The top-performers, with 95% confidence, are shown in bold.

Before diving into the results, we would like to notice that some of the results obtained in some datasets may differ from the ones reported in other works for the same datasets (e.g., [11, 16]). Such discrepancies may be due to several factors such as differences in dataset preparation 7 , the use of different splits of the datasets (e.g., some datasets have  X  X efault splits X  such as REUT and RCV1 8 ), the applica-tion of some score thresholding, such as SCUT, PCUT, etc., which, besides being an important step for multilabel prob-lems, also affects classification performance by minimizing class imbalance effects, among other factors. We would like to stress that we ran all algorithms under the same condi-tions, with the best standard weighting schemes for each of them, using standardized and well-accepted cross-validation procedures that optimize parameters for each of them and apply the proper statistical tools for the analysis of the re-sults. We consider that those decisions are in accordance with our main evaluation goal (i.e., to compare the learning algorithms), since its is fundamental to carefully isolate the important factors under study. Therefore, we believe our re-sults are a valid comparison across datasets and algorithms. Moreover, all our datasets (and implementations in the near future) are available for others to replicate our results and test different configurations.
Briefly, regarding our experimental results, they show that, overall, BROOF outperforms or ties all the state-of-the-art topical text classifiers (SVM, NB and DT), as well as the tra-ditional RF (that, to the best of our knowledge, had not been thoroughly investigated in this scenario). It also outper-forms, by large margins, the widely used lazy classifier KNN.
In our experiments, the proposed BROOF classifier was the only classifier to produce the best results in all tested
A more reliable procedure than setting up an single  X  X ali-dation set X , since it allows us to assess statistical significance when tuning the free parameters.
For instance, some works do exploit complex feature weighting schemes or feature selection mechanisms that do favor some algorithms in detriment to others.
In fact, we do believe that running experiments only in the default splits is not the best experimental procedure as it does not allow a proper statistical treatment of the results. datasets considering all metrics, a surprising result given the knowledge that there is no single universal top-notch classi-fier for all problems. Moreover, when compared to its tradi-tional counterpart (RF), we obtained gains of up to 24 . 53% in MicroF 1 (20NG) and 35 . 62% in MacroF 1 (REUT), being better than traditional RF in five out of six datasets (and tied in the other one). Notice also that the traditional RF does not perform so well, losing to traditional classifiers such as KNN and SVM in several datasets. These results corrob-orates two of our arguments: ( i ) the drawbacks of the tra-ditional RF for classification of noisy and high dimensional data; and ( ii ) our proposal was successful in overcoming these drawbacks.

Considering the boosting baselines, our experiments show that our proposal achieves consistently higher effectiveness. Particularly, when compared to the closest approach to ours (ADARF), we can see that our proposal beats this classifier in 5 out of 6 datasets, tying only in one case. In fact, large gains of up to 24 . 72% in MicroF 1 (UniRCV1) and 32 . 5% in MacroF 1 (also UniRCV1) can be obtained. The gains against GBDT and other boosting baselines are also similar. This clearly demonstrates the benefits of using the out-of-bag estimates, along with the selective weight updates, as described in Section 3. When compared to the overall best baseline (SVM), we statistically outperformed it in 2 out of 6 datasets, with gains of up to 8 . 84% in MicroF 1 and 5 . 90% in MacroF 1 (4UNI), tying in the other four datasets. Com-pared to the lazy baseline classifier (i.e., KNN), we obtained statistically significant improvements in most datasets, with gains of up to 149 . 87% and 131 . 76% in Micro-F 1 and Macro-F 1 (UniRCV1).

Overall, it is now clear the benefits of taking advantage of the successful boosting strategy smoothly coupled with the RF classifier, which is able to produce rich information through the out-of-bag samples.
An important aspect to analyze is the extent to which the two key components of our proposed algorithm, namely: ( i ) the use of OOB weights and ( ii ) the selective weight updates, influence the final results in order to better under-stand the obtained improvements. For this we run a full 2 factorial design experiment [15] in which the two factors un-der study are the use of OOB based weights and the use of selective updates. For each factor, we assume k = 2 levels: presence or absence. The response variable is the classifica-tion effectiveness, given by micro-averaged F 1 . Table 2: The explained variation of the OOB weight-i ng and the selective weight updates on MicroF 1 .
For each configuration, we run a 10-fold cross validation to assess classification effectiveness and account for experi-mental errors. The results of this experiment are shown in Table 2. For space reasons, in this analysis we only show re-sults for the datasets in which we obtained the largest gains. As we can observe, not only the main factors (the use of se-lective updates and an OOB based weighting schema) play an important role in improving the boosting algorithm with random forests as weak-learners, but also their interactions. As reported in Table 2, each factor explains roughly 33% of the variations observed in classification effectiveness, as well as their interactions. This means that, although one can achieve some improvements in classification effectiveness by considering a single factor in isolation, it is usually better to consider both, since their interactions also provide observ-able variations in the response variable.
Finally, we discuss issues related to the convergence and the need of training samples for our proposed method and the baselines. We have empirically found that in most cases BROOF needs less training data to achieve good classifica-tion results. This has positive implications in the practical application of the algorithm, mainly regarding labeling effort in real-world scenarios, which is usually very expensive and cumbersome. For space reasons, in here we only show results for MacroF 1 , but results for MicroF 1 follow somewhat sim-ilar patterns. Also for space reasons, we show results only for four datasets, as the patterns found in the other two datasets are similar. In our analysis, we compare BROOF with the best baselines in each dataset. Figure 1 shows the results of this analysis, when we vary the size of the training set to 10%, 20%,  X  X  X  , 100%. Results correspond to the av-erage of 10 test runs. We do not show confidence intervals for the sake of readability.

As it can be seen in Figure 1(a), for 20NG, the best base-line (KNN) is only competitive when using all the available training, while BROOF stabilizes around 70% of training data. Similarly, for 4UNI (Figure 1(b)), the best baseline in this dataset (RF) could only achieve competitive results when using the entire training data. However, differently from before, our proposed method keeps improving with more training. In any case BROOF results with around 70-80% of the training data are very competitive. For ACM, BROOF stability is achieved with about 60% of the training data, with small fluctuations for larger fractions, all within the statistical tie margins. The best baseline, SVM, be-comes competitive with BROOF when it is uses about 70% of training, loosing in all cases when smaller fractions are used. The second best baseline (NB) is only competitive using 100% of training. Notice also that in this dataset the effectiveness of BROOF is much more stable, with results with about 50% of training not far away from the peak in effectiveness. Finally, in UniRCV1 (Figure 1(d)), the curves for BROOF and the best baseline (SVM) are the closest ones among those analyzed, but, as before, SVM only ties with our proposed method when using about 100% of the training data. More importantly, differently from the previ-ous datasets, BROOF performance stabilizes around 30% of training, which means that only this fraction of the training data is enough for our solution to achieve its best effective-ness. In sum, BROOF needs much less training than the best baselines in all datasets to achieve its highest effective-ness, which usually occurs, when using around 30-80% of the original training set.
Topic categorization is not the only domain with noisy and high dimensional data where there is great benefit of pro-viding highly accurate classifiers. Clearly, other domains do exist and we here focus on two of them: sentiment analysis and microarray data classification.

In order to evaluate BROOF over such different domains, we adopted the same experimental protocol and setup dis-cussed in Section 4.2.1, evaluating the same set of nine clas-sifiers described in Section 4.1. In the following we detail the performed evaluation. In particular, for space reasons, we do not delve into the specific analyses we have performed in the previous section regarding the effects of the weighting strategies and sample size, but experiments in the domains analyzed in this section indicate that results are very similar.
Considering sentiment analysis, we tackle the problem of automatically identifying the polarity of user provided con-tent, which can be reviews for some topic or item (e.g., movies, products), posts on social networks (e.g., tweets), among others. The polarity of a user generated content refers to the degree to which such content express a posi-tive or negative opinion about a topic. As with any human generated content in natural language, there is a lot of noise (due to several factors such as misspellings, the presence of ambiguity and so on) generally embedded in a high dimen-sional space. Clearly, accurately predicting the polarity of such noisy high dimensional data is paramount to support decision making, being of great business importance.
We evaluate the effectiveness of BROOF considering ten datasets for polarity detection, consisting of reviews (e.g., Amazon), posts on online social networks (e.g., Twitter, De-bate), user comments (e.g, Youtube) and snippets of opinion news (NYT). Due to space issues, a detailed description of each can be found in the online appendix. The obtained results can be found in Table 3.

As we can observe in Table 3, BROOF is among the top performers in 8 out of 10 datasets, being the single classi-fier to be among the top performers in the vast majority of cases. Compared to the traditional RF classifier, BROOF achieved gains of up to 16 . 70% and 17 . 72% in MicroF 1 MacroF 1 , respectively. Compared to the ADA.RF baseline, BROOF achieved gains of up to 6 . 44% in MicroF 1 (Yelp) and 17 . 84% in MacroF 1 (MySpace). Compared to SVM clas-sifier, our approach obtained gains of up to 2 . 44% and 3 . 89% in MicroF 1 (BBC) and MacroF 1 (Amazon), respectively. The cases in which BROOF was not the top performer were Debate and Digg datasets. Considering Debate, SVM was the best performer, achieving gains of 4 . 23% and 3 . 91% over BROOF. The situation for Digg was a bit more competitive: neither SVM nor KNN were able to outperform BROOF in both metrics, being tied to BROOF in one of them.
Microarray analysis is a popular method in bioinformatics, allowing the investigation of thousands of genes simultane-ously. Gene expression microarray data usually contains a very large number of attributes, related to the expression of several genes, but a small number of samples (due to the associated costs of producing such samples, availability, pri-vacy issues, among others). Furthermore, such kind of data is usually plagued with both technical noise and the pres-ence of genes weakly correlated or not correlated at all with the outcome. Such a high dimensional characteristic, along with the presence of noise, provide a real challenge to come up with accurate prediction models capable of uncovering the relationship between gene expression and outcome (e.g., presence of cancer).

Unlike topic categorization and sentiment analysis domains, which deal specifically with textual data, the microarray analysis domain deals with continuous, real-valued, gene ex-pression measurements. Thus, for this particular domain, some of the previously adopted baselines may be subopti-mal. More specifically, instead of adopting a Multinomial NB classifier, which is specifically designed (and known to perform better) for textual data, we adopt the Gaussian NB approach. Furthermore, we adopt, for the microarray do-main, the libsvm implementation [4] for the SVM classifier since, unlike in textual data, non-linear class boundaries may be observed due to the much more complex relationship be-tween gene expression and the outcome 9 . We adopt a SVM with non-linear RBF kernel, tuning both the cost parameter and the gamma parameter for the kernel function.

We evaluate the effectiveness of BROOF considering six gene expression microarray datasets. Due to space limita-tion, their details can be found in an online appendix. The obtained results can be found in Table 4. As it can be ob-served, BROOF again was the top performer in the major-ity of evaluated cases (4 out of 6). BROOF outperformed RF with gains of up to 13 . 22% in MicroF 1 and 22 . 53% in MacroF 1 (both in the Prostate dataset). Considering the SVM classifier, BROOF achieved gains of up to 8 . 10% and 12 . 54% in MicroF 1 and MacroF 1 , also in the Prostate dataset. ADA.RF was outperformed in all but 9tumors and Brain1 datasets, in some cases by large margins. Brain2 was the only dataset in which BROOF was outperformed by an-other baseline (e.g., KNN) in both metrics, with a somewhat mild margin (just as observed with our previous sentiment
I n fact, linear SVM obtained poor results in this domain. analysis evaluation). Interestingly, in this domain, SVM was not the closest competitor as in the previous domains.
In this work, we propose an original classifier which ex-ploits the synergy between the classification paradigms ex-ploited by RF classifiers and boosting schemes, especially for classification of high dimensional noisy data. We have done that in an original way, by: ( i ) exploiting, in the boosting algorithm (which uses RFs as base learners), new weight-ing strategies that naturally come from the OOB sets pro-duced by RFs, and by ( ii ) applying a less aggressive, selec-tive weight updating strategy that mitigates overfitting, a common problem high-dimensional noisy task.

We summarize our findings regarding the behavior of all analyzed classifiers in the three explored application do-mains in Table 5, which counts the number of times each algorithm figured out as a top performer. As our experi-mental results clearly show, our proposal is in great advan-tage over the other explored baselines in terms of prediction accuracy, being the classifier of choice in the vast majority of cases. We also show that BROOF requires less training to achieve its best results, which is of great importance to guarantee its practical applicability.

As future work, we intend to provide theoretical bounds over our proposed setting, in order to provide some guar-antees regarding misclassification rates, complementing our extensive empirical comparison.
 DT not shown since they were not top performers in any evaluated case.
