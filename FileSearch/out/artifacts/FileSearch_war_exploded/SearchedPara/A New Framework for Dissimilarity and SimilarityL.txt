 The k-Nearest Neighbour (kNN) algorithm is an effective method to address classifica-tion problems that has proved its utility in many real-world applications [1]. Most com-mon kNN classifiers use the Euclidean metric to measure the dissimilarities between examples. This approach has the advantages of simplicity and generality, however, its main limitation is that the Euclidean metric implies that the input space is isotropic which is rarely valid in practical applications [2].

Since the Euclidean metric is not appropriate for many real-world learning problems different researchers have recently proposed methods for learning the parameters of a parametrized distance measure directly from the data, either in a fully supervised setting [2 X 7] or using side information [8 X 10 ]. The distances in the above methods are usually restricted to belong to a Mahalanobis metric family parametrized by a positive semi-definite (PSD) matrix A . 1 The goal in metric learning is to discover an  X  X ptimal X  matrix A that achieves a higher kNN predictive performance than the Euclidean metric.
In most of the above metric learning methods the input information is given in the form of equivalence relations. A widely used equivalence relation in the classification setting indicates whether two instances, x i and x j , belong to the same class or not. It is important to realize that the existing techniques do not require a direct access to the instances, instead they access them through their pairwise distances, or equivalently through their pairwise difference vectors | x | ij . The elements of the latter are the abso-lute differences of the attributes of the two instances. More formally, for x i , x j  X  IR p , | x | where x i the attributes vectors of the form of (1), or to their pairwise products in the case of quadratic metrics. The weighted differen ces are then aggregated to compute the final Mahalanobis metric. In general, the metric learning methods assign the weights so that under the new metric pairs of instances of the same class will be close together (i.e. their Mahalanobis metric will have a value close to 0), and pairs of instances of different classes will be far apart (i.e. their Mahalanobis metric will have larger values).
Based on the above observations we go one step further and use the pairwise differ-ence vectors of the form given by equation 1 as our learning instances. More precisely, our approach is based on casting the dissimilarity learning problem as a binary classi-fication or a regression task defined over the space of the absolute difference vectors. When we treat the problem as a classification task we assign negative labels to these difference vectors that correspond to pairs of instances of the same class and posi-tive labels to the difference vectors that co rrespond to pairs of instances from different classes. In the regression scenario we assign negative and positive target values, respec-tively. The construction of the learning problem in this new space is justified by the fact that for instances of the original space that belong to the same class, some attributes of | x | should have large values. Moreover, by exploring classification or regression algorithms that produce models based on weighted combinations of the input attributes (e.g. Sup-port Vector Machines or Logistic Regression), we expect that the non-discriminatory attributes will be assigned low weights, and hence instances in the new space with posi-tive and negative classes (or positive and negative numbers) will be moved respectively far from and towards the origin of the new space. In our framework the computation of a dissimilarity measure between two examples amounts to computing a prediction score in classification or a continuous value in regression; the higher the predicted score or the predicted value of the target variable, the more dissimilar are the corresponding two input instances.

The paper is organized as follows. In Sect. 2 we present the existing metric learning techniques under a common framework. This will directly motivate the main contribu-tion of this work presented in Sect. 3, where we propose a new framework for learning (dis-)similarity measures. Experimental results are reported in Sect. 4. Finally, we con-clude with Sect. 5 where we discuss major open issues and future work. In this section we will present the metric learning problem in a common framework that will help us to motivate the main contribution of this work in Sect. 3. We begin with a
Based on the notation from (1), we will represent in a compact way both Euclidean and Mahalanobis metrics between two instances x i , x j  X  IR p . More precisely, the Mahalanobis metric, parameterized by a positive semi-definite matrix A  X  IR p  X  p ( A , 0 ), can be represented as: We note that it is sometimes useful to reparametrize the Mahalanobis metric as: where A = W T W and W  X  IR p  X  p .Forany W we have A = W T W , 0 .In what follows, to emphasize that all the above metrics between x i and x j depend only and d 2 L ( | x | ij ) , respectively.

A common approach in the existing metric learning methods is to provide informa-tion in the form of equivalence relations as pairwise constraints on the input instances. In the classification framework there is a natural equivalence relation, namely whether two vectors share the same class label or not, i.e. S = { ( x i , x j ): c ij =0 } and D = { ( x i , x j ): c ij =1 } where c ij  X  X  0 , 1 } indicates whether or not the labels y i and y match: It should be stressed that the existing distance learning methods do not require a direct access to the pairs of instances in either of the above sets S and D ; instead, they access the data through the distance functions d A or d W and hence only though pairwise distance vectors | x | ij of the form given in (1). Consequently, to emphasize the above fact, we will consider only the following versions of the equivalence relations S = { | x | ij : c ij =0 } and D = { | x | ij : c ij =1 } .

The general problem of metric learning in a supervised setting can be now stated as the following optimization problem: where F L is a (possibly non-differentiable) cost function, L = A or L = W and  X  (  X  ) is a regularization term 2 whose importance is controlled by the  X  regularization parameter. Additionally, the above optimization problem is possibly subject to a number of constraints. For example, for the parametrization from (2) the optimization given in (5) has to be constrained by A , 0 . Depending on the actual form of the function F L different instantiations of the algorithm can be obtained.
 One possible problem with the optimization problem of (5) is that for full matrices L the number of parameters to estimate is p 2 .Forlarge p (i.e. in the order of few thou-sands), this would render the optimization task non tractable as there will be too many parameters to optimize over. We explore a solution to this problem that is based on re-stricting matrices L to be diagonal, resulting in a weight ed combination of features (this restriction can be seen as a simple form of re gularization since it reduces the effective number of parameters from p 2 to p ). It should be noted that the approach based on diag-onal matrices, although faster then the one based on full matrices, is also less expressive since it does not account for interactions between different attributes. On the other hand, it allows for the application of metric techniques also on high-dimensional datasets. 3
In the rest of this section we will present 3 different instantiations of the above frame-work which differ with respect to the objective function F L and hence the assumptions they make for the data distribution. More precisely, in this work we will focus on the following state-of-the-art metric learning algorithms: Large Margin Nearest Neighbor (LMNN) [11], Maximally Collapsing Metric Learning (MCML) [4] and Neighborhood Component Analysis (NCA) [3].
 LMNN. The cost function F A of LMNN [2] is constructed in such a way that it penalizes both large distances between each sa mple and its similarly labeled near-est neighbors, and small distances between differently labeled instances. Equivalently, the criterion of LMNN seeks for a metric in which each sample has a large margin between nearest neighbors of same class and samples in different classes. We use  X  indicates the sample x i is one of the neighbors of sample x j ,and  X  ij =0 other-wise. The LMNN method can be now formulated as the following optimization prob-MCML. The MCML algorithm is based on the simple geometric intuition that all points of the same class should be mapped onto a single location and far from points of the other classes [4]. To learn the metric which would approximate this ideal ge-ometrical setup a conditional distribu tion is introduced which for each example x i selects another example x j as its neighbor with some probability p A ( j | i ) ,and x i in-herits its class label from x j . The probability p A ( j | i ) isbasedonthesoftminofthe d
A distance measure, i.e. p A ( j | i )= exp( shown [4] that any set of points which has the distribution p 0 ( j | i )  X  1 if | x | ij  X  X  and p 0 ( j | i )=0 if | x | ij  X  X  exhibits the desired ideal geometry. It is thus natural to seek a matrix A such that p A (  X | i ) is as close (in the sense of the Kullback-Leibler divergence) to p 0 (  X | i ) . This, after a number of transformations [4], is equivalent to min-NCA. The NCA method attempts to directly optimize a continuous version of the leave-one-out error of the kNN algorithm on the training data. The main difference between NCA and the two previous methods is that optimization in NCA is done with respect to matrix W of (3). Its cost function F W is based on stochastic neighbor assignments in the weighted feature space, which is based on p A ( j | i ) defined above where A is replaced with W . Under this stochastic selection rule the probability p W ( i ) of correctly expresses the probability of obtaining an error free classification on the training set [3]. In this section we present the main contribution of this work and define a new frame-work for (dis-)similarity learning over vect orial data. As already mentioned in Sect. 2, most of the existing metric learning t echniques do not require a direct access to the training data; instead, the only  X  X nterface X  to the data is through the equivalence sets S and D , the elements of which are the pairwise difference vectors as these were defined in equation (1). Motivated by this observation, we will cast the problem of dissimilarity learning as a problem of learning a binary classification scoring (or regression) function from the training data constructed from S and D . In classification the new labels will be negative for elements of S and positive for elements of D . In regression pairs of instances of different and pairs of instances of the same class will be assigned positive and negative numbers, respectively. More fo rmally, the new training data is given as: where c ij is defined in (4) and O denotes a vector of zeros; ( 0 T , 0) is included in the new training data as it models for the fact th at duplicate instances share the same class. It should be noted that the size of the training dataset from (6) scales as O ( n 2 ) (it con-tains exactly ( n  X  1)( n  X  2) 2 +1 examples). In these settings, the learning phase amounts to training a learning algorithm over the training data I , whereas the computation of the (dis-)similarity between two examples x i and x j boils down to a computing pre-diction score (for classification) or predicted depended variable (for regression) for an instance | x | ij . In the remaining part of this work we will sometimes denote the scoring classifying an instance x new  X  X  by the kNN algorithm, that is based on computing score ( | x | ij ) to select nearest neighbours, is presented in Algorithm 1.
It is important to realize that for the definition of c ij from (4), we can interpret the scoring function score ( x i , x j ) as a dissimilarity measure since it assigns lower values to elements of S and higher values for elements of D . However, by redefining c ij from (4) so that it assigns negative labels for elements of D and positive labels for elements of S , the corresponding scoring functions can be interpreted as a similarity measure . In this study we will only focus on learning dissimilarity measures.
 Algorithm 1. kNN classification using scoring function.
The proposed framework has several advantages over existing metric learning meth-ods. First, it is very general as we are free to use almost any classification (or regression) algorithm as long as its decision is based on a classification score (the predictions of a regression algorithm could be interpreted right away as dissimilarities). The only re-striction we put on the learner is that it should scale well with respect to the number of input instances; this is due to the fact that the number of instances in the new train-ing set I scales as O ( n 2 ) , and hence any algorithm applied in the new space, whose computational complexity is higher than say log-linear would be prohibitive but for toy learning problems. Second, unlike most of the existing techniques, in general no semi-definite programming or eigenvalue comput ations are required, and hence depending on the employed learner the resulting dissimilarity can be efficiently learned. Finally, our formulation generally leads to a dissim ilarity that can be more expressive, and at the same time simpler to learn, than the standard Mahalanobis metrics.

We also mention that there are two main drawbacks of our approach. First, in gen-eral the learned dissimilarity measures are not valid metrics. 4 However, several au-thors have reported state-of-the-art classification performance of kNN over a variety of learning problems where the underlying distance measures were not valid metrics, see e.g. [12, 13]. In particular, most of the examined non-metric distance measures do not satisfy the triangle inequality; the latter guarantees that if a point x i is close to x j and close to x l then x j will be also close to x k . Moreover, as we will see in the experimen-tal part of this study, the kNN algorithm, where the nearest neighbors are selected using score ( | x | ij ) , generally outperforms kNN with adaptive metrics that are learned using different state-of-the-art metric learning techniques. This might suggest that the metric conditions of a dissimilarity function are not necessary for kNN to achieve good pre-dictive performance. However, the dissimila rity measures that are not metrics might be not adequate for some applications. 5 We will discuss both the forms and characteristics of our dissimilarities in the remainder of this section.

The second, and potentially more severe, problem is that the learning instances from (6) are not independent. This renders the application of the learning algorithms in the new space questionable, as the basic assumption that training instances should be in-dependently and identically distributed (i. i.d) does not hold. However, the good experi-mental performance of our framework reported in Sect. 4 suggests that this difficulty is lifted by the above mentioned flexibility of learned dissimilarities. In other words, the advantage of using very flexible dissim ilarities might overcome the problem of non-independently distributed data.

In the remainder of this section we will describe 3 different instantiations of our framework that differ with respect to the emp loyed learning algorithm. As already men-tioned, the two requirements we put on the learning algorithms applied in the new space are that they should (i) output a function indicating how similar are two instances and (ii) scale well with respect to the number of instances in I . To perform a fair compari-son with the existing metric learning techniques we will only focus on algorithms that produce linear models whose parameters are directly related with the parameters of the Mahalanobis metric. Based on the above considerations, in this study we focused on two classification (Linear Support Vector Machines and Logistic Regression) and one regression algorithm (Ridge Regression) which fulfil the above requirements. Support Vectors Machines. In Linear Support Vector Machines (L-SVM) the learn-ing phase amounts to solving the following unconstrained quadratic optimization prob-lem [15]: 6 min . w  X  IR p ij max { 0 , 1  X  c ij w , | x | ij } +  X  w 2 ,where  X  is a user-defined regularization parameter and i =1 ,...,n ; j = i,...,n . The dissimilarity be-tween x i , x j is given as: score ( x i , x j )= w  X  , | x | ij ,where w  X  is a solution of the optimization problem of SVM. It is easy to verify that the above function is not a metric as it can take negative values and does not satisfy the triangle inequality, however, it is reflexive, strict (assuming a non-degenerate w  X  ) and symmetric. For solving the opti-mization problem of L-SVM we exploit t he algorithm recently proposed in [15] that scales linearly with respect to the number of instances in I , i.e. its computational com-plexity is O ( n 2 ) . It is worth noting that the method from [5] also exploits the notion of SVM to learn a metric. The main difference from our framework is that this method is a local one that aims to determine a stretched neighbourhood around each query instance such that class conditional probabilities are likely to be constant. Moreover, [5] exploits the softmax function to obtain a valid metric. We plan to perform a detailed comparison of the two approaches in the future.
 Logistic Regression. Logistic Regression (LR) [1] is a well known binary classification method where the classification deci sions are based on a scoring function score ( | x | ij ) that can be interpreted as a probability p ij that an instance | x | ij belongs to the positive where w  X  IR p are parameters of the logistic regression model; score ( x i , x j ) is non-negative and symmetric, but does not satisfy the triangle inequality and is neither strict nor reflexive. We will exploit the regularized version of LR where the optimal solution w  X  is obtained by solving the following optimization problem: min . w  X  c Ridge Regression. We also experimented with one regression method, namely the Ridge Regression (RR) algorithm [1] that solves the following optimization problem: min . w  X  IR p ij ( w , | x | ij  X  c ij ) 2 +  X  w 2 . In this context, the dissimilarity func-tion has an identical form (and properties) as in the case of L-SVM score ( x i , x j )= w  X  , | x | ij ,where w  X   X  IR p is a solution of the optimization problem. We evaluated the performance of the proposed approach on a number of real-world classification problems. The goal is to examine whether the three instantiations of our dissimilarity learning framework from Sect . 3 (i.e. the L-SVM, LR and RL linear algo-rithms) achieve better predictive performan ce than a number of existing metric learning algorithms. The quality of the different dissimilarity measures will be only compared in the context of kNN (we will not use the underlying algorithms such as logistic regres-sion directly for classification).

We compared the above 3 methods with the LMNN, MCML and NCA state-of-the-art metric learning algorithms. We experimented with 2 instantiations of the above metric learning techniques, over full and diagonal matrices denoted respectively as METHOD full and METHOD diag ,where METHOD is LMNN , MCML or NCA .For comparison reasons we also provide the performance of the standard kNN algorithm with the Euclidean metric. We experimented with different values of k ( k =1 , 3 , 10 ); the relative performance of the different methods did not vary with k , we report results only for k =1 . In all the above methods we set the  X  parameter to 1. In all the experi-ments we estimate accuracy using 10-fold cro ss-validation and control for the statistical significance of observed differences using McNemar X  X  test [16] (sig. level of 0.05).
We experimented with 13 datasets. First, we used 4 standard datasets from the UCI repository (Liver, Wdbc, Wine, BalanceScale); these datasets are standard benchmarks used in the context of distance learning. Then, we have chosen to experiment with high-dimensional data from two different application domains, namely genomics and proteomics (description of these datasets can be found in [17]). The genomics datasets correspond to DNA-microarray experiments. We worked with three different datasets: colon cancer (Colon), central nervous system (Central) and Leukemia (Leuk). All pro-teomics datasets come from the domain of m ass spectrometry. We worked with 4 dif-ferent datasets: ovarian cancer (Ovarian), prostate cancer (Prostate), an early stroke diagnosis (Stroke), and MaleFemale (MaleF). In Ovarian, Prostate and Stroke we ex-perimented with 2 versions of each of the abo ve proteomics datasets where we used different parameters in the prepossessing step for feature extraction (in MaleF we had access only to one version of this dataset). Al l features correspond to intensities of mass values and are continuous. All the above genom ics and proteomics datasets, in addition to large number of features, are also charact erized by a small number of observations, making these datasets a difficu lt learning scenario. In all the above datasets the numeric attributes were normalized so that they takes values between 0 and 1. In Table 1 we provide the number of instances and attributes in the examined datasets.

To better understand the relative performances of the examined algorithms we es-tablished a ranking schema of these algorithms based on the results of the pairwise comparisons. More precisely, if an algorithm is significantly better than another it is credited with 1 point; if there is no significant difference between two algorithms then they are credited with 0.5 points; if an algorithm is significantly worse than another it is credited with 0 point. Thus, in the case m algorithms are examined, an algorithm that is significantly better than all the others for a given dataset is assigned a score of m  X  1 .
Experiments on these datasets have 2 goals. First, we study the relative performance of our methods with the existing metric learning algorithms. In these experiments we use the diagonal versions of the existing metric learning techniques as it allows for a fair comparison with the proposed framework; similar to the metric learning tech-niques based on diagonal matrices, L-SVM , LR and RR do not account for interactions between different attributes. Second, we compare the predictive performance of our method with the metric learning methods based on full matrices. For the latter methods applied on the high-dimensional datasets (i.e. all the genomics and proteomics clas-sification problems) we exploited the PCA method to reduce the data dimensionality; this procedure was widely used in the context of metric learning [2]. More precisely, the training instances are projected into a lower dimensional subspace accounting for at least 99 % of the data X  X  total variance.
 Results and Analysis. In Table 1 we present the results (with the score ranks) of the comparison between LMNN diag , MCML diag , NCA diag , L-SVM, LR, RR and the stan-dard kNN (recall that the maximum score for a n algorithm in a given dataset is 6). From these results we can make several observations. First, with the exception of the Liver and Balance datasets, there is at least one adaptive method that outperforms the stan-dard kNN algorithm (in Liver and Balance, kNN is placed in the first position according to our ranks). Second, the developed dissimilarity methods based on L-SVM, LR and RR generally outperform both MCML diag and NCA diag ; the advantage of our methods is most visible in the genomics and proteomics high dimensional datasets. Finally, the methods that most often win are LMNN diag , L-SVM, LR and RR.

To quantify the relative performances of the different algorithms we computed for each method its average rank over all the exami ned datasets. These ranks are presented in the last row of Table 1. We observe that the best performance of 3.86 points is ob-tained for the margin based methods (L-SVM and LMNN diag ), which have a similar computational complexity both in theory (they scale as O ( n 2 ) ) and in practice (they had similar running times). This result is remarkable since L-SVM, which is based on a simple idea, performs equally well as the more elaborate metric learning algorithm that has been reported to consistently outperform other metric learning techniques over a number of of non-trivial learning problems [2]. Finally, we mention that the surpris-ingly poor performance of NCA diag might be explained by the fact that its cost function is not convex and hence it is sensitive to the initializations of W .

In the second set of experiments we compare the performance of the metric learning methods with the full matrix to that of metric learning with diagonal matrix. Here we want to examine whether methods that account for feature interaction outperform the methods proposed in Sect. 3. As already mentioned, we first reduced the dimensional-ity of all the high-dimensional datasets by mapping the instances to lower dimensional subspaces defined by the PCA method; depe nding on the datasets, the dimensionali-ties of the subspaces, that account for at least 99 % of the data X  X  total variance, were between 50 and 178. The results are presented in Table 2. From these results we can see that with the exception of NCA full in Liver and Prostate1, all the metric learning with full matrix have similar or worse performances than their corresponding versions with diagonal matrices (the first signs in parenthesis are mostly  X = X  or  X - X ). This could suggest that even though the data dimensi onality is significantly reduced, the metric learning algorithms based on full matrices might still be prone to overfitting (the other explanation might be simply that by removing features that have low variance we also remove important discriminatory information).

We have also compared the performances of full matrix metric learning methods with that of L-SVM, LR and RR; the significance tests results corresponding to this comparison are given by the 2nd, 3th and 4th signs in parenthesis in Table 2. From these results we can see that the relative pe rformances between metric learning meth-ods that are based on full matrices and the methods from Sect. 3 depend on the actual dataset. Indeed, in the first 3 UCI datasets (Liver, Wdbc and Wine) there is an advan-tage of full matrix metric learning algorithm; in Balance, Central and Stroke1 there is almost no difference in performances; in Prostate1 no conclusions can be drawn; and in all the remaining datasets the L-SVM, LR and RR methods generally outperform the metric learning techniques based on full matrices. These results suggest that in the majority of the high dimensional datasets, the feature interactions are not important, and hence the methods that do not account for feat ure interactions have in general bet-ter performances. Alternatively, it might suggest that stronger regularization is needed. Moreover, it is interesting to note that the cases for which the full matrix metric learn-ing methods are good are mostly the UCI datasets that correspond to rather not difficult classification problems. This hints that there might be a bias of method development towards methods that perform well on UCI datasets; however, one can argue that they are really representative of the real world. In this paper we prosed a novel framework for learning a (dis-)similarity function over vectorial data, where the learning problem is cast as a binary classification or regression task defined over the space of pairwise differences of the input instances. Our approach generally leads to adaptive (dis-)similarities that are not valid metrics; however, we ar-gue that by learning (dis-)similarities that d o not fulfil metric cond itions (and are not of the Mahalanobis type) we might have more fr eedom in adapting these (dis-)similarities for a given problem. Our claim is supported by experimental evidence that, in terms of predictive performance, shows that our frame work compares favourably with alterna-tive state-of-the-art distance learning techniques which are trained to learn both full and diagonal Mahalanobis metrics.

In the future we want to exploit other learning techniques applied over the new data representation (e.g. decision trees). We also plan to investigate a Non-Linear version of Support Vectors Machines (NL-SVM) to learn the higher-order interaction between features, similar to that modelled in the Mahalanobis metric. This can be achieved by exploiting the polynomial kernel of degree 2 that induces a feature space that is in-dexed by all the monomials of degree 2. Since we will not be able to rely on efficient implementation of L-SVM, the main challenge here is to make NL-SVM scalable and practical. Moreover, we plan to test more car efully the pros and cons of the L-SVM and LMNN methods that in our experiments had similar performance and outperformed other algorithms. Finally, we would like to compare our framework with the approach from [5] as the latter also exploits the notion of SVM to locally learn a metric. Acknowledgments. This work was partially funded by the European Commission through EU projects DebugIT (FP7-217139) and e-LICO (FP7-231519). The support of the Swiss NSF (Grant 200021-122283/1) is also gratefully acknowledged.

