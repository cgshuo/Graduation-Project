 Advances in computer networking and database technologies have enabled the collection and storage of vast quantities of data. Data mining can extract valuable knowledge from this data, and organizations have realized that they can often ob-tain better results by pooling their data together. However, the collected data may contain sensitive or private informa-tion about the organizations or their customers, and privacy concerns are exacerbated if data is shared between multiple organizations.

Distributed data mining is concerned with the computa-tion of models from data that is distributed among multi-ple participants. Privacy-preserving distributed data mining seeks to allow for the cooperative computation of such mod-els without the cooperating parties revealing any of their in-dividual data items. Our paper makes two contributions in privacy-preserving data mining. First, we introduce the con-cept of arbitrarily partitioned data, which is a generalization of both horizontally and vertically partitioned data. Sec-ond, we provide an efficient privacy-preserving protocol for k -means clustering in the setting of arbitrarily partitioned data.
 H.2.8 [ Database Applications ]: Data Mining Algorithms; Security.
Many organizations collect large amounts of data about their clients and customers. Such data was initially used only for record keeping. There was soon a realization that  X 
This work was supported by the National Science Founda-tion under Grant No. CCR-0331584.
 these large collections of data could be  X  X ined X  for knowl-edge that could improve the performance of the organiza-tion. Well known data-mining tasks include clustering, pre-diction, association rule mining and outlier detection [13]. Data mining has been used extensively, for purposes includ-ing biomedical and DNA data analysis [8], financial data analysis [3], identification of unusual patterns, and analysis of telecommunications data [12].

While much data mining occurs on data within an organi-zation, it is quite common to use data from multiple sources in order to yield more precise or useful knowledge. How-ever, privacy and secrecy considerations can prohibit orga-nizations from being willing or able to share their data with each other. Privacy-preserving data mining, introduced by Agarwal and Srikant [1] and Lindell and Pinkas [9], arose as a solution to this problem by allowing parties to cooperate in the extraction of knowledge without any of the cooper-ating parties having to reveal their individual data items to each other or any other parties.

Techniques from secure multiparty computation [7] form one approach to privacy-preserving data mining. Yao X  X  gen-eral protocol for secure circuit evaluation [18] can be used, in theory, to solve any two-party privacy-preserving distributed data mining problem. However, since data mining usually involves millions or billions of data items, the communica-tion cost of this protocol renders it impractical for these purposes. This has led to the search for problem-specific protocols that are efficient in terms of communication com-plexity. In many cases (including our solution described in this paper), the more efficient solutions still make use of a general solution such as Yao X  X , but only to carry out a much smaller portion of the computation. The rest of the compu-tation uses other methods to ensure the privacy of the data. A complementary approach to privacy-preserving data min-ing uses randomization techniques. Although such solutions tend to be more efficient than cryptographic solutions, they are generally less private or less accurate.

Privacy-preserving data mining solutions have been pre-sented both with respect to horizontally and vertically par-titioned databases, in which either different data objects with the same attributes are owned by each party, or dif-ferent attributes for the same data objects are owned by each party, respectively. We introduce the notion of arbi-trarily partitioned data, which generalizes both horizontally and vertically partitioned data. In arbitrarily partitioned data, different attributes for different items can be owned by either party. Although extremely  X  X atchworked X  data is unlikely in practice, one advantage of considering arbitrarily partitioned data is that protocols in this model apply both to horizontally and vertically partitioned data, as well as to hybrid that are mostly, but not completely, vertically or horizontally partitioned. (These concepts are defined more precisely in Section 2.)
In this paper, we provide a privacy-preserving solution to an important data mining problem, that of clustering data. Privacy-preserving clustering has been previously ad-dressed [14, 16]. Oliviera and Zaiane [14] addressed the privacy-preserving data clustering problem using random-ization techniques. Vaidya and Clifton [16] presented a privacy-preserving k -means clustering protocol on vertically partitioned data using cryptographic techniques.

Informally, a clustering algorithm partitions a set of ob-jects into clusters so that all objects within any cluster are  X  X imilar X  or  X  X lose X , while objects in different clusters are  X  X issimilar X . Clustering has been successfully applied in many domains. In image processing, clustering algorithms are used for image segmentation, which is the problem of distinguishing objects from the background [17]. In bioinfor-matics, clustering has been used to group genes with similar expression profiles [15]. Astronomers have used clustering to create catalogs of objects in the sky [4].

The k -means clustering algorithm (also known as Lloyd X  X  algorithm ) [5, 10, 11] is a well known iterative algorithm that successively refines potential clusters in an attempt to minimize the k -means objective function , which measures the  X  X oodness X  of a given clustering. We present a privacy-preserving protocol for k -means clustering in the setting of arbitrarily partitioned data distributed between two parties. Our protocol is efficient and provides cryptographic privacy protection. In particular, our protocol provides the first privacy-preserving solution to k -means clustering for hori-zontally partitioned data. We also provide an analysis of the performance and privacy of our solution.

In Section 2, we review the k -means clustering algorithm and provide preliminary definitions that are used in the rest of the paper. Section 3 describes our privacy-preserving pro-tocol for k -means clustering when the data is arbitrarily par-titioned between two parties. The analysis of the commu-nication complexity and privacy of our protocol is given in Section 4.
In this section, we briefly review clustering and the k -means clustering algorithm, discuss the privacy properties we seek to achieve, and describe the concept of arbitrarily partitioned data.
We briefly review the k -means clustering algorithm. (For a more detailed description, see [13].) The k -means clus-tering algorithm partitions the set D of objects into the specified number k of disjoint subsets, called clusters .The clustering depends on a well defined notion of the distance between a given pair of objects. In our case, we assume data objects are elements of R d , and we define the distance between two objects as their Euclidean distance. Each clus-ter is represented by its center , which is the attribute-wise average of all objects in the cluster.

The k -means algorithm proceeds iteratively, successively refining k potential clusters until a specified termination condition is reached. Initially, the algorithm arbitrarily se-lects k of the objects in D as k initial candidate cluster centers. Each object in D is assigned to the cluster whose center is closest to it. Subsequently, the clusters centers and corresponding cluster assignments are modified with the in-tention of reducing the average distance of each object to its closest candidate cluster center. In each iteration, each object in the database D is reassigned to the cluster whose candidate center is closest. The current center of each clus-ter is then recomputed on the basis of the modified set of the objects in the cluster. This iterative process continues until a specified termination condition is met. A commonly used condition terminates the process when the change in the av-erage distance of each object to its closest cluster center is small. Another commonly used condition is the absence of any change in cluster composition. In this paper, we use a commonly used termination condition, terminating when the change in the centers of all clusters falls below a prespec-ified threshold value. This version is illustrated in Figure 1. Input : Database D , integer k
Output : Cluster centers  X  1 ... X  k
In the two-party distributed data setting, two parties (call them Alice and Bob) hold data forming a (virtual) database consisting of their joint data. More specifically, the vir-tual database D = { d 1 ,d 2 ,...,d n } consists of n objects ,or records .Eachobject d i is described by the values of nu-meric attributes. We denote the values of the attributes of object d i by ( x i, 1 ,x i, 2 ,...,x i, ). When convenient, we sometimes abuse notation and refer to this vector of values as the set d i = { x i, 1 ,x i, 2 ,...,x i, } .

We introduce in this paper the concept of arbitrarily par-titioned data , illustrated in Figure 2. In arbitrarily parti-tioned data, there is not necessarily a simple pattern of how data is shared between the parties. For each d i , Alice knows the values for a subset of the attributes, and Bob knows the values for the remaining attributes. That is, each d i partitioned into disjoint subsets d A i and d B i such that Alice knows d A i and Bob knows d B i . We emphasize that the set of attributes whose values are known to Alice for some object d does not have to equal the set of attributes whose values are known to Alice for some other object d j ( i = j ). In particular, it is possible that d A i =  X  or d A i = d i .Thatis,a given object may be  X  X ompletely owned X  by Bob or by Al-ice. If an object is completely owned by Alice (respectively, Bob), its existence is unknown to Bob (respectively, Alice). Both Alice and Bob know the names of all attributes. We assume that both parties either know the total number n of objects or they know the range of values for all of the at-tributes. Some data values may be known to both parties; we consider such values to be owned by one party, who will be responsible for handling the data value.
Clearly, both horizontally partitioned data and vertically partitioned data can be viewed as specific cases of arbi-trarily partitioned data. Specifically, in horizontally par-titioned data, each object in the virtual database is com-pletely owned by Alice or completely owned by Bob. In vertically partitioned data, each attribute is  X  X ompletely owned X  by either Alice or Bob except perhaps for some com-mon  X  X ata handle X . As previously mentioned, although ex-tremely  X  X atchworked X  data is unlikely in practice, the gen-erality of this model can make it better suited to practical settings in which data may be mostly, but not completely, vertically or horizontally partitioned. Our desired outcome is that clusters are computed on Alice and Bob X  X  joint data. Alice should learn the cluster number for each data object she owns completely, and Bob should learn the cluster number for each data object that he owns completely. For objects that both have attributes for, they should both learn the cluster number. In addition, the parties should either learn random shares (defined below) of the final cluster centers, or, depending on the desired out-come for the particular application, one or both party should learn the actual final cluster centers.

In an ideal world, Alice and Bob would have access to a trusted third party who could perform the necessary calcu-lations. Alice and Bob would securely send their data to this trusted party, which would then compute the cluster to which each object is assigned using the appropriate clus-tering algorithm, and send the desired information to the party that owns the object. However, trusted third parties are hard to find in the real world, and even then, such trust may be misplaced. In this work, we do not rely on a third party. Instead, we provide algorithms by which Alice and Bob can carry out the functionality that would be provided by the trusted third party without actually requiring such a party or requiring the parties to send their data to each other.

In this paper, we assume that both Alice and Bob are semi-honest . That is, both parties faithfully follow their specified protocols, but we assume they record intermediate messages in an attempt to infer as much information about the other party X  X  data as possible. Additional cryptographic techniques, such as zero knowledge proofs, can be used to provide privacy even against malicious behavior, but typi-cally at a significant performance cost.

As we discuss further in Section 4, our solution leaks some additional information beyond just the cluster numbers, in that it reveals some information about the progress of the computation as it is carried out. This information does not appear to be useful except perhaps in degenerate cases, and some protection can be provided by having the initial choice of candidate cluster centers be chosen randomly.
We describe the various cryptographic primitives used in this paper.
 Random Shares both parties should get their final output, but all the values computed in the intermediate steps of the algorithm should be unknown to both parties. So in our paper all the interme-diate values that are supposed to be unknown to the parties, such as the candidate cluster centers at the end of each iter-ation, are computed throughout the algorithm are shared as uniformly distributed random values between the two par-ties. Their sum is the actual intermediate value (i.e., the ones that would be computed using the centralized k -means algorithm on the union of the data).

More formally, we say that Alice and Bob have random shares of a value x drawn from a field F of size N (usually abbreviated to Alice and Bob have random shares of x )to mean that Alice knows a value a  X  F and Bob knows a value b  X  F such that where a and b are uniformly random in field F .Notethat the requirement that x  X  F implies that ( a + b )mod N is actually equal to x , not simply that a + b and x are congruent modulo N .

Throughout the paper, we assume that a finite field F of suitable size N is chosen such that all computations can be done in that field, and all computations throughout the remainder of the paper take place in F .
 Secure Scalar Product Protocol X =( x 1 , ..., x n )andBobhasavector Y =( y 1 , ..., y n wish to securely compute the scalar product as s A + s B = X.Y where s A and s B are the random shares of Alice and Bob respectively. (Recall that all computations in the paper are carried out modulo N .) We use the private homomor-phic scalar product protocol given by Goethals et al. [6]. This protocol uses a (semantically secure) homomorphic en-cryption scheme where Alice generates a private and pub-lic key pair and sends the public key to Bob. Alice com-putes the encryptions of x 1 , ..., x n and sends them to Bob. Bob chooses a random value s B and uses the properties of the homomorphic encryption to compute the encryption of X.Y  X  s B , and sends to Alice. Alice decrypts and obtains s
A = X.Y  X  s B . This protocol is secure when the parties are semi-honest.
 Yao X  X  Circuit Evaluation Protocol avector X =( x 1 , ..., x n )andBobhas Y =( y 1 , ..., y n use Yao X  X  circuit evaluation protocol [18] to securely com-pute the index j such that x j + y j is minimum. We also use Yao X  X  circuit evaluation protocol to compute a + b m + n where Al-ice knows a and m and Bob knows b and n . Alternatively the Bar-Ilan and Beaver X  X  protocol [2] can also be used to compute ( n + m )  X  1 and then a secure multiplication protocol can be used to compute the product.
We now describe our privacy-preserving k -means cluster-ing protocol. Alice and Bob share a data set in an arbitrary partition, as explained in Section 2.2. They wish to learn the k -means clustering of their joint data, without revealing their data to each other or anyone else. The final output of the algorithm should be an assignment of a cluster number between 1 and k to each object. If an object is shared, then both parties learn the assignment; otherwise, only the party who owns the object gets the assignment. The algorithm can terminate with the final mean of each cluster randomly shared by both parties, or if desired, one or both parties can learn the actual final centers.

As described in Section 2.1, the k -means clustering al-gorithm is an iterative algorithm that successively refines candidate cluster centers. Initially, k candidate cluster cen-ters are chosen from the data points in D .Thiscanbe done in one of several ways: for example, they can be taken as a pre-specified set of points, such as the first k data points, they can be chosen randomly by one or the other party, or they can be chosen randomly with randomness in-troduced by both parties. As we discuss in Section 4, the latter method provides the most robustness against certain kinds of potential privacy leaks.

In each iteration of the algorithm, each object is assigned to the closest candidate cluster center. Then, new candidate cluster centers are determined based on the actual centers of the points as they have been assigned. This procedure is repeated until a specified termination condition is reached. Our privacy-preserving distributed version follows the same iterative structure, but protects the data and intermediate values from being learned by the parties. This is done us-ing interaction, privacy-preserving subprotocols and random sharings as we now describe.

In each iteration of the privacy-preserving protocol, the candidate cluster centers are calculated as a random shar-Figure 3: Privacy-preserving k -means clustering protocol over arbitrarily partitioned data ing between the two parties. Let denote the number of attributes and d i =( x i, 1 ,...,x i, )for1  X  i  X  n denote the i th object in the database. Let  X  A ice X  X  share of the j th mean,  X  B j for 1  X  j  X  k denote Bob X  X  share. The candidate cluster centers are given by  X  A j +  X  for 1  X  j  X  k . For each object d i , Alice and Bob securely compute the distances dist ( d i , X  j )for1  X  j  X  k , between the object and each of the k cluster centers. The result of the distance calculation is learned as random shares be-tween Alice and Bob. Using these random shares, they then securely compute the closest cluster for each object in the database.

At the end of each iteration, Alice and Bob learn the clus-ter assignment for the objects, as follows. If an object d shared by both Alice and Bob, then both of them learn the cluster to which the object belongs. If not, it is revealed only to the party who completely owns d i . The iterative process is repeated until the change in the centers of all clusters falls below a certain specified threshold value.

When the termination condition is reached, the parties may wish to send each other their cluster center shares so that they can learn the actual cluster centers of the joint data. Whether this is desirable or not depends on the partic-ular application for which the clustering results are intended to be used.

Figure 3 illustrates the overall privacy-preserving k -means clustering protocol. In the remainder of this section, we de-scribe in detail each of the subprotocols used in the privacy-preserving k -means clustering algorithm. In Section 3.1, we present a secure protocol that computes the cluster assign-ment for each object. In Sections 3.2 and 3.3, we present secure protocols for recomputing the cluster centers and for iteration termination, respectively.
This subprotocol takes an arbitrarily partitioned object and k randomly shared candidate cluster centers as its in-put, and outputs to the appropriate party or parties the closest cluster to which the object belongs. It reveals no other additional information.

Consider an object d i =( x i, 1 ,...,x i, ). This object can be owned by Alice or Bob or shared by both. Suppose that the object is shared by both Alice and Bob. Without loss of generality, assume x i,p 1 ,...,x i,p s belong to Alice and the rest of the  X  s attributes belong to Bob. To compute the closest cluster the protocol first computes the distance be-tween the object d i to each of the k clusters. For each cluster 1  X  j  X  k , we compute the distance dist ( d i , X  j ) between the object d i to the j th-cluster mean  X  j . ( dist ( d i , X  j )) 2 =( x i, 1  X   X  j, 1 ) 2 +( x i, 2  X  Since  X  j,t =  X  A j,t +  X  B j,t ,where  X  A j,t is Alice X  X  share of the mean and  X  B j,t is Bob X  X  share of the mean, = To privately compute the term data, Alice computes the sum that involves the components x i,p 1 ,...,x i,p s , while Bob computes the sum of the rest of thecomponents. Thesecondterm pletely computed by Alice and similarly the third term is computed by Bob. They then use a secure scalar product protocol, such as the one described in [6], to compute ran-dom shares of the last three terms. Since a sum involving random shares results in random shares, we have where  X  i,j is the random share known to Alice and  X  i,j is known to Bob where and N is the size of the chosen finite field.

Alice has a k -length vector A =(  X  i, 1 ,..., X  i,k )andBob has B =(  X  i, 1 ,..., X  i,k ). They securely compute the index j such that  X  i,j +  X  i,j is minimum using Yao X  X  circuit eval-uation [18]. Since k is typically quite small, particularly in comparison to the number of data items, the overhead this requires should be sufficiently small. The object d i assigned to cluster j .
 Communication and Computational Complexity each object d i , this protocol computes the closest cluster. Each object has components. Let us assume that each component is represented by c bits. Communication is in-volved when the two parties engage in the secure scalar prod-uct computation to compute the distance between an object d and each of the cluster centers. For each distance com-putation the scalar product protocol is invoked three times between vectors of length . The communication complexity of each scalar product protocol is O ( c ). Hence it requires a communication of O ( kc ) bits to compute the distance of the object d i from all the k centers.
 Yao X  X  circuit evaluation [18] requires communication of O ( c ) bits per comparison and hence O ( c )bitsforcom-puting the minimum component. The total communication complexity for one invocation of the closest cluster protocol is O ( ck )).

The estimation of the computational complexity involves counting the number of encryptions, decryptions and multi-plications for Alice and Bob. To compute the closest cluster for each object the scalar product protocol is executed for each cluster three times between vectors of length .For each execution of the scalar product protocol Alice performs encryptions and one decryption. Bob performs expo-nentiations and one encryption. Alice performs at most 2 multiplications and Bob performs at most 3 multipli-cations. Since there are k clusters the total computational complexity for computing the closest cluster for an object is O ( k ) encryptions and at most O ( k ) multiplications for Alice and O ( k ) exponentiations and at most O ( k )mul-tiplications for Bob. The use of the secure scalar product protocol described in [6] increases the computational com-plexity. But the current hardware and optimization tricks makes the computational complexity for both Alice and Bob tolerable (see discussion in [6]).
 Privacy of the closest cluster for every object. The only informa-tion revealed is the output, which is the cluster index. The distance between each object d i to each of the k candidate centers is available only as a random share between Alice and Bob. Hence each cannot obtain any information about the other party X  X  data. For each distance computation, the communication between Alice and Bob occurs through the secure scalar product protocol. The scalar product proto-col leaks no information and returns only a random share to both Alice and Bob. Thus the distance function is computed as a random sharing between Alice and Bob without leaking any information. Finally, Yao X  X  circuit evaluation protocol securely computes the closest cluster. The output of the protocol is an assignment of the closest cluster of a given point. This is the only information available to the parties at the end of the protocol when a trusted third party is used.
At the end of each iteration Alice and Bob learns his or her share of the cluster centers. The next iteration re-quires recomputing each of the k -cluster centers. Let us assume that Alice has objects d A i 1 ,...,d A i p and Bob has ob-jects d B i 1 ,...,d B i q ,foreachcluster1  X  i  X  k .Eachofthe d and d B i is an -tuple, where d A i,j and d B i,j each denote the j th coordinate of the corresponding -tuple.

Alice calculates the shares s j and n j for 1  X  j  X  ,where s d ,...,d A i p for which Alice has the values for attribute A If Alice does not have the value for attribute j for some object d A i , she treats it as zero.

Similarly, Bob calculates the shares t j and m j for j, 1 j  X  ,where t j = of objects in d B i 1 ,...,d B i q for which Bob has the values for attribute A j . Again, if Bob does not have the value for attribute j for some object d i ,hetreatsitaszerointhe computation of the sum. The j th component of the i th cluster center is given by  X  i,j =( s j + t j ) / ( n j + m
Since this involves only four values, it can be computed efficiently using Yao X  X  [18] circuit evaluation protocol. We can also use Bar-Ilan and Beaver X  X  protocol [2] to compute the inverse of ( n j + m j )  X  1 as u + v where Alice learns u and Bob learns v . The secure multiplication protocol can then be used to compute a + b =( s j + t j )( u + v ) . Alice learns a and Bob learns b .
 Communication and Computational Complexity component of the mean requires communication of O ( c ) bits. Hence the communication complexity to recalculate the mean is O ( kc ). The computational complexity is O ( kc ). Privacy are secure and they do not leak any information. Each party learns only a random share of each mean and thus cannot infer any information about the centers of the clusters.
The k -means clustering algorithm is an iterative algo-rithm. At the end of every iteration, the cluster centers are recalculated. The iterative process is terminated when there are no substantial improvement in the approximations. In our case, the algorithm terminates when the Euclidean distance between the cluster centers between two consecu-tive iterations is less than a specified value ! .DenoteAl-ice X  X  share of cluster centers at the end of the i th iteration by  X  A,i 1 ,..., X  A,i k and Bob X  X  share by  X  B,i 1 ,..., X  larly, denote Alice X  X  share of cluster centers at the end of the ( i +1)st iteration by  X  A,i +1 1 ,..., X  A,i +1 k and Bob X  X  share by  X  B,i +1 1 ,..., X  B,i +1 k .Each  X  i is an -tuple and is denoted by (  X  i, 1 , ...,  X  i, ). For 1  X  j  X  k , we securely compute the square of the distances: where  X  j is Alice X  X  share of the distance and  X  j is Bob X  X  share. These are random shares. Alice and Bob have a k -length vector (  X  1 ,..., X  k )and(  X  1 ,..., X  k ) respectively. They communicate with each other and securely check if  X  +  X  j &lt;! for 1  X  j  X  k . As before, since the amount of input involved is small, this can be securely and efficiently implemented using Yao X  X  protocol.
 Communication and Computational Complexity check if the algorithm terminates, Alice and Bob has to check that the Euclidean distance between two consecutive iterations is less than ! for all the k -cluster centers. This involves executing the scalar product protocol four times to obtain the random shares of the distance vectors and then applying the Yao X  X  protocol. As explained in Section 3.1, it requires a communication of O ( kc ) bits to compute the dis-tance between two consecutive iterations of all the k centers. Yao X  X  circuit evaluation involves O ( kc ) bits of communica-tion to check if all the k distances are less than ! . The total computational complexity is O ( k ) encryptions and at most O ( k ) multiplications for Alice and O ( k ) exponentiations and at most O ( k ) multiplications for Bob. The discussion is similar to the one described in Section 3.1.
 Privacy uation protocol are secure and they leak no information. Therefore, the only information that Alice and Bob can ob-tain at the end of the execution of the protocol is the output whether the iterations can be terminated or not.
Putting all the subprotocols together as shown in Fig-ure 3 yields our complete privacy-preserving k -means clus-tering protocol over arbitrarily partitioned data. We have discussed the complexity and privacy of each component; we now discuss the overall complexity and privacy.
 Communication and Computational Complexity described in Section 3.1, the two parties need to commu-nicate O ( ck ) bits to determine the closest cluster for each point. Here c is the maximum number of bits needed to rep-resent each component of an object or its encryption and denotes the number of attributes. Hence the total communi-cation complexity to assign cluster numbers in each iteration is O ( nck ) bits. At the end of each iteration, the protocol recomputes the center of each of the k clusters, each of which is a -length vector. It takes O ( c ) bits to compute each com-ponent of a single center, so the communication complexity to compute all k centers is O ( ck ). Additionally, it takes O ( ck ) bits of communication for Alice and Bob to securely check the termination criterion at the end of each iteration. Thus, the overall communication complexity for the privacy-preserving k -means clustering protocol is O ( nck ) per iter-ation. It has been shown that for typical data and suitably chosen k , the number of iterations required by the k -means algorithm is typically small.

For each iteration the total computational complexity for computing the cluster centers is O ( nk ) encryptions and at most O ( nk ) multiplications for Alice and O ( nk )exponen-tiations and at most O ( nk ) multiplications for Bob. Privacy k -means clustering algorithm with one that makes use of a trusted third party (TTP) who receives all the data, locally runs the standard k -means algorithm, and returns the ap-propriate cluster numbers (and possibly cluster centers) to the parties. If one party completely owns an object, then only that party gets the cluster number assigned to that ob-ject. If both parties share an object then both parties get the assignment.

In contrast, the privacy-preserving k -means clustering pro-tocol is an interactive protocol. Although secret sharing is used to protect certain values, Alice and Bob learn some in-termediate results. At the end of each iteration, they learn the assignment of the cluster number to the objects. If one party holds the object completely, then only that party gets the assignment. Otherwise, both parties get the assignment.
This intermediate information can sometimes reveal in-formation about the parties X  data. For example, consider a database consisting of n objects, say d 1 ,...,d n , and sup-pose that each object has two attributes x and y .Al-ice has the values corresponding to the attribute x and Bob has values corresponding to the attribute y . Suppose d [ x ]= d 2 [ x ]=  X  X  X  = d n [ x ]=0 ,d n [ y ] = 4, and d d  X  1 [ y ] ranges from 0 to 3, and suppose d 1 , d 3 and d chosen as the initial choices for the centers ( k =3). Ifone of the clusters in the next iteration contains only d n ,then Alice can guess that d n [ y ] is close to 4. But the result of the final iteration may not have a cluster having d n [ y ]asa single point. The likelihood of this kind of leakage occurring can be reduced by choosing the initial centers at random.
We note that our protocol, when restricted to vertically partitioned data, is similar to the one given by Vaidya and Clifton [16]. The communication complexity and the privacy of our protocol is essentially the same as theirs.
This paper has two main contributions. First, we in-troduce the idea of arbitrarily partitioned data, which is a generalization of both horizontally and vertically parti-tioned data. Protocols in this model can be applied to both horizontally and vertically partitioned data, as well as to data anywhere in between. Second, we provide a privacy-preserving k -means clustering algorithm over arbi-trarily partitioned data.

As previously discussed, our algorithm potentially leaks some information through the intermediate cluster assign-ments, even though the intermediate cluster centers them-selves are not revealed. It is not clear except for some rare scenarios such as the one described in Section 4 that these values can yield anything useful, but obviously it would be desirable to find an efficient algorithm that did not leak the intermediate cluster assignments.

A more general direction for further work is to obtain privacy-preserving data mining protocols for other data min-ing algorithms over arbitrarily partitioned data. [1] R. Agrawal and R. Srikant. Privacy-preserving data [2] J. Bar-Ilan and D. Beaver. Non-cryptographic [3] S. Benninga and B. Czaczkes. Financial Modelling . [4] R. R. de Carvalho, S. G. Djorgovski, N. Weir, [5] E. Forgey. Cluster analysis of multivariate data: [6] B. Goethals, S. Laur, H. Lipmaa, and T. Mielikainen. [7] O. Goldreich. Foundations of Cryptography, Vol II . [8] D. Gusfield. Algorithms on Strings Trees and Strings . [9] Y. Lindell and B. Pinkas. Privacy preserving data [10] S. P. Lloyd. Least squares quantization in PCM. IEEE [11] J. MacQueen. Some methods for classification and [12] R. Mattison. Data Warehousing and Data Mining for [13] T. Mitchell. Machine Learning . McGraw Hill, 1997. [14] S. Oliveira and O. R. Za  X   X ane. Privacy preserving [15] Frank De Smet, Janick Mathys, Kathleen Marchal, [16] J. Vaidya and C. Clifton. Privacy-preserving k-means [17] O. Veksler. Image segmentation by nested cuts. In [18] A. C.-C. Yao. How to generate and exchange secrets.
