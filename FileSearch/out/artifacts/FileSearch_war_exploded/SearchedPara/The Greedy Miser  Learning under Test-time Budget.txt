 Zhixiang (Eddie) Xu XUZX @ CSE . WUSTL . EDU Kilian Q. Weinberger KILIAN @ WUSTL . EDU Washington University, St. Louis, MO 63130, USA Criteo, 411 High Street, Palo Alto, CA 94301, USA The past decade has witnessed how the field of machine learning has established itself as a necessary component in several multi-billion-dollar industries. The applications range from web-search engines (Zheng et al., 2008), over product recommendation (Fleck et al., 1996), to email and web spam filtering (Weinberger et al., 2009). The real-world industrial setting introduces an interesting new prob-lem to machine learning research: computational resources must be budgeted and costs must be strictly accounted for during test-time . Imagine an algorithm that is executed 10 million times per day. If a new feature improves the accu-racy by 3% , but also increases the running time by 1 s per execution, that would require the project manager to pur-chase 58 days of additional cpu time per day.
 At its core, this problem is an inherent tradeoff between accuracy and test-time computation. The test-time compu-tation consists of two components: 1. the actual running time of the algorithm; 2. the time required for feature ex-traction.
 In this paper, we propose a novel algorithm that makes this trade-off explicit and considers the feature extraction cost during training in order to minimize cpu usage during test-time. We first state the (non-continuous) global objective which explicitly trades off feature cost and accuracy, and relax it into a continuous loss function. Subsequently, we derive an update rule that shows the resulting loss lends itself naturally to greedy optimization with stage-wise re-gression (Friedman, 2001).
 While algorithms such as (Viola &amp; Jones, 2002) directly attack the problem of fast evaluation for visual object de-tection, in most machine learning application domains, such as web-search ranking or email-spam filtering, analy-sis and algorithms for on-demand feature-cost amortization are still in their early stages.
 Different from previous approaches (Lefakis &amp; Fleuret, 2010; Saberian &amp; Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), our algorithm does not build cascades of classifiers. Instead, the cost/accuracy tradeoff is pushed into the training and selection of the weak classifiers. The resulting learning algorithm is much simpler than any prior work, as it is a variant of regular stage-wise regression, and yet leads to superior test-time performance. We evaluate our algorithm X  X  efficacy on two real world data sets from very different application domains: scene recognition in images and ranking of web-search documents. Its accuracy matches that of the unconstrained baseline (with unlimited resources) while achieving an order of magnitude reduc-tion of test-time cost. Because of its simplicity, high ac-curacy and drastic test-time cost-reduction we believe our approach to be of strong practical value for a wide range of problems.
 Previous work on cost-sensitive learning appears in the context of many different applications. Most prominently, Viola &amp; Jones (2002) greedily train a cascade of weak clas-sifiers with Adaboost (Schapire, 1999) for visual object recognition. Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking. They learn a set of additive weak classifiers using gradient boosting, and remove data points during test-time using proximity scores. Although their algorithm requires almost no extra training cost, the improvement is typically limited. Lefakis &amp; Fleuret (2010) and Dundar &amp; Bi (2007) learn a soft-cascade, which re-weights inputs based on their probability of passing all stages. Different from our method, they em-ploy a global probabilistic model, do not explicitly incor-porate feature extraction costs and are restricted to binary classification problems. Saberian &amp; Vasconcelos (2010) also learn classifier cascades. In contrast to prior work, they learn all cascades levels simultaneously in a greedy fashion. Unlike our approach, all of these algorithms fo-cus on learning of cascades and none explicitly focus on individual feature costs.
 To consider the feature cost, Gao &amp; Koller (2011) pub-lished an algorithm to dynamically extract features during test-time. Raykar et al. (2010) learn classifier cascades, but they group features by their costs and restrict classifiers at each stage to only use a small subset. Pujara et al. (2011) suggest the use of sampling to derive a cascade of classi-fiers with increasing cost for email spam filtering. Most re-cently, Chen et al. (2012) introduce Cronus , which explic-itly considers the feature extraction cost during training and constructs a cascade to encourage removal of unpromising data points early-on. At each stage, they optimize the co-efficients of the weak classifiers to minimize the classifi-cation error and trees/features extraction costs. We pursue a very different (orthogonal) approach and do not optimize the cascade stages globally. Instead, we strictly incorporate the feature cost into the weak learners. Moreover, as our al-gorithm is a variant of stage-wise regression, it can operate naturally in both regression and multi-class classification scenarios. (Simultaneous with this publication, Grubb &amp; Bagnell (2012) also proposed a complementary approach to incorporate feature cost into gradient boosting.) Our training data consist of n input vectors { x 1 ,..., x R d with corresponding labels { y 1 ,...,y n }  X  Y drawn from an unknown distribution D . Labels can be continuous (regression) or categorial (binary or multi-class classifica-tion). We assume that each feature  X  has an acquisition cost c &gt; 0 during its initial retrieval. Once a feature has been acquired its subsequent retrieval is free (or set to a small constant).
 Further, we are provided an arbitrary continuous loss func-tion ` and aim to learn a linear predictor H  X  ( x ) =  X  &gt; to minimize the loss function, within some test-time cost budget, which will be defined in the following section. One example for ` is the squared-loss but other losses, for example the multi-class log-loss (Hastie et al., 2009), are equally suitable. The map-ping x  X  h ( x ) is a non-linear transformation of the in-put data that allows the linear classifier to produce non-linear decision boundaries in the original input space. Typ-ically, the mapping h can be performed implicitly through the kernel-trick (Sch  X  olkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al., 2004; Chapelle et al., 2010). In this paper we use the lat-ter approach with limited-depth regression trees (Breiman, 1984). More precisely, h ( x i ) = [ h 1 ( x i ) ,...,h h t  X  H where H is the set of all possible regression trees of some limited depth b (e.g. b = 4 ) and T = |H| . The resulting feature space is extremely high dimensional and the weight-vector  X  is always kept to be correspondingly sparse. Because regression trees are negation closed ( i.e. for each h  X  X  we also have  X  h  X  X  ) we assume through-out this paper w.l.o.g. that  X   X  0 .
 Finally, we define a binary matrix F  X  X  0 , 1 } d  X  T in which an entry F  X t = 1 if and only if the regression tree h t  X  X  splits on feature  X  somewhere within its tree. In this section, we formalize the optimization problem of test-time computational cost, and then intuitively state our algorithm. We follow the setup introduced in (Chen et al., 2012), formalizing the test-time computational cost of eval-uating the classifier H for a given weight-vector  X  . Test-time computational cost. There are two factors that contribute to this cost: The function evaluation cost of all trees h t with  X  t &gt; 0 and the feature extraction cost for all features that are used in these trees. Let e &gt; 0 be the cost to evaluate one tree h t if all features were previously extracted. With this notation, both costs can be expressed in a single function as where the l 0 -norm for scalars is defined as k a k 0  X  { 0 , 1 } with k a k 0 = 1 if and only if a 6 = 0 . The first term captures the function-evaluation costs and the second term captures the feature costs of all used features. If we combine (1) with (3) we obtain our overall optimization problem where B  X  0 denotes some pre-defined budget that cannot be exceeded during test-time.
 Algorithm. In the remainder of this paper we derive an algorithm to approximately minimize (4). For better clarity, we first give an intuitive overview of the resulting method in this paragraph. Our algorithm is based on stage-wise regression, which learns an additive classifier H  X  ( x ) = P During iteration t , the greedy Classification and Regression Tree (CART) algorithm (Breiman, 1984) is used to gener-ate a new tree h t , which is added to the classifier H  X  Specifically, CART generates a limited-depth regression tree h t  X  X  by greedily minimizing an impurity function, g : H X  X  + 0 . Typical choices for g are the squared loss (2) or the label entropy (Hastie et al., 2009). CART minimizes the impurity function g by recursively splitting the data set on a single feature per tree-node. We propose an impurity function which on the one hand approximates the negative gradient of ` with the squared-loss, such that adding the re-sulting tree h t minimizes ` , and on the other hand penalizes the initial extraction of features by their cost c  X  . To capture this initial extraction cost, we define an auxiliary variable  X   X   X  { 0 , 1 } indicating if feature  X  has already been ex-tracted (  X   X  = 0 ) in previous trees, or not (  X   X  = 1 ). We update the vector  X  after generating each tree, setting the corresponding entry for used features  X  to  X   X  := 0 . Our impurity function in iteration t becomes g ( h t )= where  X  trades off the loss with the cost.
 To combine the trees h t into a final classifier H  X  , our al-gorithm follows the steps of regular stage-wise regression with a fixed step-size  X  &gt; 0 . As our algorithm is based on a greedy opti miser , and is stingy with respect to feature-extraction, we refer to it as the Greedy Miser (short miser ). Algorithm (1) shows a pseudo-code implementation. Algorithm 1 Greedy Miser in pseudo-code Require: D = { ( x i ,y i ) } n i =1 , step-size  X  , iterations m
H = 0 for t = 1 to m do end for
Return H In this section, we derive a connection between (4) and our miser algorithm by showing that miser approximately solves a relaxed version of the optimization problem. 5.1. Relaxation The optimization as stated in eq. (4) is non-continuous, be-cause of the l 0 -norm in the cost term X  X nd hard to opti-mize. We start by introducing minor relaxations to both terms in (3) to make it better behaved.
 Assumptions. Our optimization algorithm (for details see section 5.2) performs coordinate descent and  X  start-ing from  X  = 0  X  increments one dimension of  X  by  X &gt; 0 in each iteration. Because of the extremely high dimen-sionality (which is dictated by the number of all possible regression trees that can be represented within the accu-racy of the computer) and the comparably tiny number of iterations (  X  5000 ) it is reasonable to assume that one di-mension is never incremented twice. In other words, the weight vector  X  is extremely sparse and (up to re-scaling by 1  X  ) binary: 1  X   X   X  X  0 , 1 } T .
 Tree-evaluation cost. The l 0 -norm is often relaxed into the convex and continuous l 1 -norm. In our scenario, this is particularly attractive, because if 1  X   X  is binary, then the re-scaled l 1 norm is identical to the l 0 norm X  X nd the re-laxation is exact. We use this approach for the first term: Feature cost. In the case of the feature cost, the l 1 norm is not a good approximation of the original l 0 -norm, be-cause features are re-used many times, in different trees. Using the l 1 -norm would imply that features that are used more often would be penalized more than features that are only used once. This contradicts our assumption that fea-tures become free after their initial construction. We therefore define a new function q , which is a re-scaled and amputated version of the ` 1 -norm: This penalty function q behaves like the regular ` 1 norm when | x | is small, but is capped to a constant when x  X   X  . With this definition, our relaxation of the feature-cost term becomes: Similar to the previous case, if 1  X   X  is binary, this relax-ation is exact. This holds because in (8) all arguments of q are non-negative multiples of  X  (as F  X t  X  X  0 , 1 } and  X   X  X  0 , X  } ) and it is easy to see from the definition of q that for all k = 0 , 1 ,... , we have q ( k X  ) = k k X  k 0 Continuous cost-term. To simplify the optimization, we split the budget into two terms B = B t + B f  X  X he tree-evaluation budget and the feature extraction budget X  X nd re-write (4) with the two penalties (6) and (8) as two indi-vidual constraints. If we use the Lagrangian formulation, with Lagrange multiplier  X  (up to re-scaling), for the fea-ture cost constraint and the explicit constraint formulation for the tree-evaluation cost, we obtain our final optimiza-tion problem: 5.2. Optimization In this section we describe how miser , our adaptation of stage-wise regression (Friedman, 2001), finds a (local) so-lution to the optimization problem in (9).
 Solution path. We follow the approach from Rosset et al. (2004) and find a solution path for (9) for evenly spaced tree-evaluation budgets, ranging from B 0 t = 0 to B 0 t = B Along the path we iteratively increment B 0 t by  X  . We re-peatedly solve the intermediate optimization problem by warm-starting (9) with the previous solution and allowing the weight vector to change by  X  , min s.t. k  X  k 1  X   X .
 Each iteration, we update the weight vector  X  :=  X  +  X  . Taylor approximation. The Taylor expansion of L is de-fined as If  X  is sufficiently small 2 , and because |  X  | X   X  , we can use the dominating linear term in (11) to approximate the opti-mization in (10) as Coordinate descent. The optimization (12) can be re-duced to identifying the direction of steepest descent. Let  X  X  (  X  ) t denote the gradient w.r.t. the t th dimension, and let us define to be the gradient dimension of steepest descent. Because H is negation closed, we have  X  X  (  X  ) t  X  =  X  X  X  X  (  X  ) k (If  X  X  (  X  ) t  X  = 0 we are done, so we focus on the case when it is &lt; 0 .) With H  X  older X  X  inequality we can derive the following lower bound of the inner product in (12), We can now construct a vector  X   X  for which (14) holds as equality , which implies that it must be the optimal solution to (12). This is the case if we set  X   X  t  X  =  X  and  X   X  6 = t  X  Consequently, we can find the solution path with steepest coordinate descent under step-size  X  .
 Gradient derivation. The gradient  X  X  (  X  ) t consists of two parts, the gradient of the loss ` and the gradient of the feature-cost term. For the latter, we need the gradient of q ( P t F  X t  X  t ) , which, according to its definition in (7), is not well-defined if P t F  X t  X  t =  X  . As our optimization al-gorithm can only increase  X  t , we derive this gradient from the right , yielding  X  q X Note that the condition | P t F  X t  X  t | &lt; X  is true if and only if feature  X  is not used in any trees with  X  t &gt; 0 . Let us define  X   X  = { 0 , 1 } with  X   X  = 1 iff feature | P t F  X t  X  t | &lt; X  . We can then express the gradient of L (with a slight abuse of notation) as Applying the chain rule, we can decompose the first term in prediction H  X  ( x i ) , and the partial derivatives of H w.r.t.  X  t . This results in  X  X  (  X  ) t = As H  X  ( x i )=  X  &gt; h ( x i ) is linear, we have  X  X   X  ( x If we define r i =  X   X  X   X  X  for every x i , we can re-phrase (17) as The Greedy Miser . For simplicity, we restrict H to only normalized regression-trees ( i.e. P i h 2 t ( x i ) = 1 ), which allows us to add two constant terms 1 2 P i h 2 t ( x i ) and r 2 to (18) without affecting the outcome of the minimization in (13), as both are independent of t . This completes the binomial equation and we obtain a quadratic form: with  X  0 =  X   X  . Note that (19) is exactly what miser mini-mizes in (5), which concludes our derivation.
 Meta-parameters. The meta-parameters of miser are sur-prisingly intuitive. The maximum number of iterations, m , is tightly linked to the tree-evaluation budget B t . The op-timal solution of (12) must satisfy the equality k  X   X  k 1 (unless  X  X  = 0 , in which case a local minimum has been reached and the algorithm would terminate). As k  X  k 1 is exactly increased by  X  in each iteration, it can be expressed in terms of the number of iterations m of the algorithm, and we obtain 1  X  k  X  k 1 = m . Consequently, in order to satisfy the l 1 constraint in (9), we must limit to the number of iter-ations to m  X  B t e . The parameter  X  0 corresponds directly to the feature-budget B f . The algorithm is not particularly sensitive to the exact step-size  X  , and throughout this paper we set it to  X  =0 . 1 . We conduct experiments on two benchmark tasks from very different domains: the Yahoo Learning to Rank Chal-lenge data set (Chapelle &amp; Chang, 2011) and the scene recognition data set from Lazebnik et al. (2006).
 Yahoo Learning to Rank. The Yahoo data set contains document/query pairs with label values from { 0 , 1 , 2 , 3 , 4 } , where 0 means the document is irrelevant to the query, and 4 means highly relevant. In total, it has 473134, 71083, 165660, training, validation, and testing pairs. As this is a regression task, we use the squared-loss as our loss function ` . Although the data set is representative for a web-search ranking training data set, in a real world test setting, there are many more irrelevant data points. Usually, for each query, only a few documents are relevant, and the other hundreds of thousands are completely irrelevant. There-fore, we follow the convention of Chen et al. (2012) and replicate each irrelevant data point (label value is 0 ) 10 times.
 Each feature in the data set has an acquisition cost. { 1 , 5 , 10 , 20 , 50 , 100 , 150 } . The unit of these costs is ap-proximately the time to evaluate a feature. The cheapest features (cost value is 1 ) are those that can be acquired by looking up a table (such as the statistics of a given docu-ment), whereas the most expensive ones (such as BM25F-SD described in Broder et al. (2010)), typically involve term proximity scoring.
 To evaluate the performance on this task, we follow the typ-ical convention and use Normalized Discounted Cumula-tive Gain (NDCG@5) (J  X  arvelin &amp; Kek  X  al  X  ainen, 2002), as it places stronger emphasis on retrieving relevant documents within a large set of irrelevant documents.
 Loss/cost trade-off. Figure 1 ( left ) shows the traces (dashed lines) of the NDCG@5/cost generated by repeat-edly adding trees to the predictor until 3000 trees in total  X  essentially depicting the results under increasing tree-evaluation budgets B t . The different traces are obtained under varying values of the feature-cost trade-off parameter  X  . The baseline, stage-wise regression (Friedman, 2001), is equivalent to miser with  X  = 0 and is essentially building trees without any cost consideration. The red circles indi-cate the iteration with the highest NDCG@5 value on the validation data set. The graph shows, that under increased  X  (the solid red line), the NDCG@5 ranking accuracy of miser drops very gradually, while the test-time cost is re-duced drastically (compared to  X  =0 ).
 Comparison with prior work. In addition to stage-wise regression, we also compare against Stage-wise regression feature subsets , Early Exit (Cambazoglu et al., 2010) and Cronus (Chen et al., 2012). Stage-wise regression feature subsets is a natural extension to stage-wise regression. We group all features according to the feature cost, and gradu-ally use more expensive feature groups. The curve is gen-erated by only using features whose cost  X  1 , 20 , 100 , 200 . Early Exit , proposed by Cambazoglu et al. (2010), trains trees identical to stage-wise regression X  X owever, it re-duces the average test-time cost by removing unpromising documents early-on during test-time. Among all methods of early-exit the authors suggested, we plot the best per-forming one (Early Exit Using Proximity Threshold). We introduce an early exit every 10 trees (300 in total), and at the i th early-exit, we remove all test-inputs that have a score of at least (300  X  i ) s 299 lower than the fifth best input (where s is a parameter regulating the pruning aggressive-ness). The overall improvement over stage-wise regression is limited because the cost is dominated by the feature ac-quisition, rather than tree computation. It is worth point-ing out that the cascade-based approaches of Early-Exits and Cronus are actually complementary to miser and fu-ture work should combined them.
 Since Cronus does not scale to the full data set, we use the subset of the Yahoo data from Chen et al. (2012) of 141397, 146769, 184968, training, validation and testing points respectively. In comparison to Cronus , which re-quires O ( mn ) memory, miser requires no significant oper-ational memory besides the data and scales easily to mil-lions of data points. Figure 1 ( right ) depicts the trade-off curves, of miser and competing algorithms, between the test-time cost and generalization error. We generate the curves by varying the feature-cost trade-off  X  (or the pruning parameter s for Early-Exits ). For each setting we choose the iteration that has the best validation NDCG@5 score. The graph shows that all algorithms manage to match the unconstrained cost-results of stage-wise regres-sion. However, the trade-off curve of miser stays consis-tently above that of Cronus and Early Exits , leading to bet-ter ranking accuracy at lower test-time cost. In fact, miser can almost match the ranking accuracy of stage-wise re-gression with 1/10 of the cost, whereas Cronus reduces the cost only to 1/4 and Early-Exits to 1/2. fraction of used features Feature extraction. To investigate what effect the feature-cost trade-off parameter  X  has on the classifier X  X  feature choices, Figure 2 visualizes what type of features are ex-tracted by miser as  X  increases. For this visualization, we group features by cost and show what fraction of features in each group are extracted. The legend in the right indicates the cost of a feature group and the number of features that fall into it (in the parentheses). We plot the feature fraction at the best performing iteration based on the validation set. With  X  = 0 , miser does not consider the feature cost when building trees, and thus extracts a variety of expensive fea-tures. As  X  increases, it extracts fewer expensive features and re-uses more cheap features ( c  X  = 1 ). It is interesting to point out that across all different miser settings, a few Accuracy expensive features (cost  X  150 ) are always extracted within early iterations. This highlights a great advantage of miser over some other cascade algorithms (Raykar et al., 2010), which learn cascades with pre-assigned feature costs and cannot extract good but expensive features until the very end.
 Scene Recognition. The Scene-15 data set (Lazebnik et al., 2006) is from a very different data domain. It contains 4485 images from 15 scene classes and the task is to classify im-ages according to scene. Figure 4 shows one example im-age for each scene category. We follow the procedure used by Lazebnik et al. (2006); Li et al. (2010), randomly sam-pling 100 images from each class, resulting in 1500 training images. From the remaining 2985 images, we randomly sample 20 images from each class as validation, and leave the rest 2685 for test.
 We use a diverse set of visual descriptors varying in compu-tation time and accuracy: GIST, spatial HOG, Local Binary Pattern, self-similarity, texton histogram, geometric texton, geometric texton, geometric color, and Object Bank (Li et al., 2010). The authors from Object Bank apply 177 ob-ject detectors to each image, where each object detector works independently of each other. We treat each object detector as an independent descriptor and end up with a to-tal of 184 different visual descriptors.
 We split the training data 30 / 70 and use the smaller subset to construct a kernel and train 15 one-vs-all SVMs for each descriptor. We use the predictions of these SVMs on the larger subset as the features of miser (totaling d =184  X  15= 2760 features.) As loss function ` , we use the multi-class log-loss (Hastie et al., 2009) and maintain 15 tree-ensemble classifiers H 1 ,...,H 15 , one for each class. During each iteration, we construct 15 regression trees (depth 3) and update all classifiers. For a given image, each classifier X  X  (normalized) output represents the probability of this data point belonging to one class.
 We compute the feature-extraction-cost as the cpu-time re-quired for the computation for the visual descriptor, the kernel construction and the SVM evaluation. Each visual descriptor is used by 15 one-vs-all features. The moment any one of these features is used, we set the feature extrac-tion cost of all other features that are based on the same vi-sual descriptor to only the SVM evaluation time ( e.g. if the first HOG-based feature is used, the cost of all other HOG-based features is reduced to the time required to evaluate the SVM). Figure 3 summarizes the results on the Scene-15 data set. As baseline we use stage-wise regression (Fried-man, 2001) and an SVM with the averaged kernel of all de-scriptors. We also apply stage-wise regression with Early Exits . As this is multi-class classification instead of re-gression we introduce an early exit every 10 trees (300 in total), and we remove test-inputs whose maximum class-likelihood is greater than a threshold s . We generate the curve of early exit by gradually increasing the value for s . The last baseline is original vision features with ` 1 regular-ization, and we notice that its accuracy never exceeds 0 . 74 , and therefore we do not plot it. The miser curve is gen-erated by varying loss/feature-cost trade-off  X  . For each setting we choose the iteration that has the best validation accuracy, and all results are obtained by averaging over 10 randomly generated training/testing splits.
 Both, multiple-kernel SVM and stage-wise regression achieve high accuracy, but their need to extract all features significantly increases their cost. Early Exit has only lim-ited improvement due to the inability to select a few expen-sive but important features in early iterations. As before, miser champions the cost/accuracy trade-off and its accu-racy drops gently with increasing  X  .
 All experiments (on both data sets) were conducted on a desktop with dual 6-core Intel i7 cpus with 2.66GHz. The training time for miser requires comparable amount of time as stage-wise regression (about 80 minutes for the full Ya-hoo data set and 12 minutes for Scene-15.) Accounting for the operational cost of machine learning al-gorithms is a crucial problem that appears throughout cur-rent and potential applications of machine learning. We be-lieve that understanding and controlling this trade-off will become a fundamental part of machine-learning research in the near future. This paper introduces a natural extension to stage-wise regression (Friedman, 2001), which incorpo-rates feature cost during training. The resulting algorithm, the Greedy Miser , is simple to implement, naturally scales to large data sets and outperforms previously most cost-effective classifiers.
 Future work includes combining our approach with Early Exits (Cambazoglu et al., 2010) or cascade based learning methods such as (Chen et al., 2012). KQW and ZX would like to thank NIH for their support through grant U01 1U01NS073457-01.

