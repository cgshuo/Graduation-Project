 Context-aware recommender systems (CARS) help improve the ef-fectiveness of recommendations by adapting to users X  preferences in different contextual situations. One approach to CARS that has been shown to be particularly effective is Context-Aware Matrix Factorization (CAMF). CAMF incorporates contextual dependen-cies into the standard matrix factorization (MF) process, where users and items are represented as collections of weights over var-ious latent factors. In this paper, we introduce another CARS ap-proach based on an extension of matrix factorization, namely, the Sparse Linear Method (SLIM). We develop a family of deviation-based contextual SLIM (CSLIM) recommendation algorithms by learning rating deviations in different contextual conditions. Our CSLIM approach is better at explaining the underlying reasons be-hind contextual recommendations, and our experimental evalua-tions over fi ve context-aware data sets demonstrate that these CSLIM algorithms outperform the state-of-the-art CARS algorithms in the top-N recommendation task. We also discuss the criteria for se-lecting the appropriate CSLIM algorithm in advance based on the underlying characteristics of the data.
 H.3.3 [ Information Search and Retrieval ]: Information fi ltering Recommendation; Context; Contextual; Context-aware Recommen-dation; SLIM; Matrix Factorization Recommender systems (RS) have been well developed in the past decades as an effective solution for the problem of information overload. There are also some new type of RS emerged in recen-t years, such as context-aware recommender systems (CARS) [1] and context recommender [27]. In contrast to traditional RS, CARS adapt the recommendations to users X  different contextual situation-s. The fundamental assumption of CARS is that a rating for an item is a function not just of the user and the item, but also of the context in which the item is evaluated or used. User X  X  preferences on a giv-en item may vary from context to context. For example, companion is an in fl uential context in the movie domain; you may make a dif-ferent decision in selecting movies if you intend to watch the movie on a date, than if you plan on seeing a movie with children.
Matrix factorization (MF), an effective approach in recommen-dation, can be extended to incorporate contextual variables, such as time-aware MF. Karatzoglou et al. [8] proposed to use tensor fac-torization (TF) to integrate context in recommendation. Their ap-proach assumed that contextual variables are independent of other dimensions. But the computation cost in TF is prohibitive increas-ing exponentially as the number of contextual variables increases. Another approach, context-aware matrix factorization (CAMF) [5] was developed to adapt to contextual recommendations by mod-eling contextual dependencies with the user or item dimensions. CAMF was built based on traditional MF [9], where both users and items are represented by factor weights in a matrix.

Most recently, a new matrix factorization method, called sparse linear method (SLIM) [10] was proposed and shown to be more accurate than the state-of-the-art MF algorithms in the top-ommendation task. In this paper, we extend this work to create a series of contextual SLIM (CSLIM) model incorporating contex-tual conditions. More speci fi cally, contexts are incorporated into the SLIM by estimating the rating deviations in different contextual conditions. Thus we name these CSLIM models as deviation-based CSLIM approaches. We compare the performance of our proposed approaches to the state-of-the-art CARS algorithms through exten-sive empirical evaluations on multiple data sets, and demonstrate that they are able to outperform the popular CARS algorithms in the top-N context-aware recommendations. The standard formulation of the collaborative recommendation prob-lem begins with a two-dimensional (2D) matrix of ratings, orga-nized by user and item: Users  X  Items  X  Ratings . Recommenda-tion then becomes a prediction problem, interpolating new ratings not present in the original matrix. Context-aware recommender systems (CARS) add contextual variables to this question, making use of the context in which a rating was made and the context in which a recommendation is sought, in order to add nuance to the re-sulting recommendations [1]. CARS algorithms have been created based on collaborative fi ltering (CF), including both neighborhood-based CF (NBCF) [16, 17], and matrix factorization (MF) [9].
Differential context modeling (DCM) [22] is one approach incor-porating context into NBCF, and there are two series of approaches in DCM: differential context relaxation (DCR) [20, 21] and differ-ential context weighting (DCW) [23]. In DCM, contexts are con-sidered as fi lters or constraints being applied to different functional components in NBCF: DCR uses a relaxation strategy where cer-tain contextual dimensions are fi ltered out, while DCW associates weights with context dimensions based on their importance. Those models are good for explanations: for example, demonstrating the emotional effects in recommender systems [24, 25]. Although D-CM may only work well in dense context-aware data sets in terms of prediction errors, it does less well in ranking metrics (e.g., pre-cision, etc) as shown in [26]. It is because context-aware data sets are usually sparse  X  users may not rate the same items in multiple contexts at different times, thus it is easy to produce over fi tting.
Context has also been incorporated into the MF-based approach-es. Karatzoglou et al. [8] proposed to use tensor factorization (TF) to integrate context in recommendation. Their approach assumed that contextual variables are independent of other dimensions. But the computation cost in TF is prohibitive, increasing exponential-ly as the number of contextual variables increases. Context-aware matrix factorization (CAMF) [5] was then developed to adapt to contextual recommendations by modeling contextual dependencies with the user or item dimensions. CAMF was built based on tradi-tional MF [9], where both users and items are represented by factor weights in a matrix. There are also other factorization approaches, such as those based on factorization machines [15].

The MF-based CARS algorithms usually work better than DCM but the common issue in those algorithms is that it is dif fi cult to understand or interpret the latent factors. Therefore, it is impor-tant to develop a CARS algorithm working effectively in terms of recommendation accuracy while at the same time being fl exible for explanation or pattern discovery. Sparse linear method (SLIM) is an approach designed for top-recommendations. It improves upon the traditional item-based nearest neighbor (ItemKNN) collaborative fi ltering [17] by learn-ing directly from the data, a sparse matrix of aggregation coef fi -cients that are analogous to the trad itional item-item similarities [10].
Assume that there are M users, N items, and let us denote the associated 2-dimensional rating matrix by R .Weuse u i to denote user i and t j to denote the item j .Anentry, R i,j ,inmatrix represents u i  X  X  rating on t j . A row vector in R representing ratings over all items is denoted by R i, : ,and R : ,j denotes a column vector in Matrix R representing ratings on item t j from all users.
In SLIM, the ranking score for user u i on item t j is represented by
S i,j , which is calculated by a sparse aggregation of the ratings on the other items that have been rated by u i . We denote this ap-proach as SLIM-I, since it assigns a N by N non-negative matrix W to represent the aggregation coef fi cients between each two item-s. Accordingly, S i,j can be estimated by Equation 1.
 An example of how the SLIM-I approach works is provided in Figure 1. To estimate the ranking score of u 2 on item t 1 SLIM extracts u 2  X  X  ratings on the other items, and aggregates the ranking score by multiply those r atings by the corresponding coef fi -cients of items t 1 and t h (where t h belongs to the set of other items has rated) in matrix W . The estimated ranking score will be used to rank the items to fi nally provide top-N recommendations. A SLIM-U approach can be also be derived similarly, where is calculated by a sparse aggregation of ratings on this speci fi c item , given by other users, where W is a M  X  M non-negative matrix representing the coef fi cients between every two users. A similar example of calculating S 2 , 1 in SLIM-U can be viewed in Figure 2.
Traditional matrix factorization techniques, such as those pro-posed by Koren et al. [9], usually represent users and items by a set of weights on the assigned latent factors, where both the number of latent factors, learning parameters and training iterations are re-quired to tune the model and fi nd the optimal solution. In contrast to those approaches, the sizes of matrices R and W are fi xed and there is only a single matrix, W , required to be learned in SLIM-I and SLIM-U. Due to that the coef fi cients in matrix W are anal-ogous to the item-item (or user-user) similarities in ItemKNN (or UserKNN), these coef fi cients learnt by the SLIM models can be directly used for explanations or pattern discovery. Analogously to context-aware matrix factorization techniques [5], we propose to incorporate context into the SLIM approach for top-N recommendations. We refer to this as the contextual SLIM (CSLIM) approach. In the following, we use contextual factor to refer to the contextual variables such as "Time", "Location", and "Companion". The term contextual condition refers to a speci fi c value in a contextual factor. For example, "weekend" and "week-day" are two contextual conditions for the "Time" contextual factor. A sample of context-aware data set can be found in Table 1.
Assume there are F contextual factors and L contextual condi-tions in total in a context-aware data set, in addition to the and N items. The primary task in the CSLIM approach is to es-timate the ranking score S i,j,c for user u i on item t j tual situation. For example, assume all contextual conditions with L =4 can be represented by { Time= weekend ,Time= weekday ,Lo-cation= school , Location= home } . Then, the vector c = &lt;1, 0, 1, 0&gt; indicates that the current contextual situation is { Time= weekend , Location= school } .

Users X  preferences may vary from context to context for the same item. Therefore, it is necessary to make predictions based on rating pro fi les in the same context c . Recall that the estimated ranking score matrix S in SLIM-I is derived based upon the idea in Item-KNN  X  aggregation of users X  ratings on other items. As a result, it is possible to estimate the ranking score S i,j,c by an aggregation of ratings on other items in the same contexts c . However, a context-aware data set, with multiple ratings for each item, is usually very sparse  X  it is not guaranteed that a user has rated other items in the same context c . In this case, we estimate u i  X  X  ratings on an item in context c (i.e., R i,j,c ) based on the user X  X  non-contextual rating on this item (i.e., R i,j , ratings without considering contexts) and the aggregated contextual rating deviations (CRD, i.e., the rating deviations in different contextual conditions). Accordingly, we can build different CSLIM models based on how to estimate the CRD. In this section, we present three CSLIM-I models based on how to estimate the CRD. First, we estimate contextual ratings (i.e., based on the non-contextual rating R i,j and aggregated CRD. Specif-ically, we model CRD as the contextual rating deviations on item-s. In other words, we assume there is a rating deviation for each &lt;item, context condition&gt; pair. Thus, the predicted contextual rat-ing R i,j,c can be computed as follows: where R is the 2D non-contextual rating matrix.

Typically, users may provide non-contextual ratings in addition to the ratings for an item in given contexts. But it is not necessary that both non-contextual ratings and contextual ratings are assigned to a same &lt;user, item&gt; entry. The average rating of each user on each item rated within multiple contexts can be added to the rating matrix R if there is no knowledge about the non-contextual ratings. Note that we suggest to use Equation 2 to estimate users X  ratings in a speci fi c context c , even if we already know user X  X  real ratings in c , so that more entries in the D will be learnt and fi nally results in a more accurate ranking model for context-aware recommendations.
We build a N  X  L matrix D where each row represents an item, and each column represents an i ndividual contextu al condition as shown in Figure 3. Thus, D j, : is a row in D representing CRD for the item t j in L different contextual conditions.

We can then use the similar idea in the SLIM-I approach to es-timate the ranking score S i,j,c for user u i on item t j c . This is described in Equation 3, where t h belongs to the set of items for which user u i has provided non-contextual ratings (i.e., R i,h &gt; 0). The ranking score is estimated by an aggregation of user X  X  ratings on other items in the same context c . We call this ap-proach CSLIM-I-CI because the CRD are assumed for each &lt;item, context condition&gt; pair, and D is built as a CI matrix (i.e., columns and rows represent contexts and items respectively).

Similar to SLIM-I model shown in Figure 1, the visualization of the CSLIM-I-CI can be shown in Figure 4. R is the 2D matrix with non-contextual ratings fi lled in. S i,j,c is estimated by on the other items within the same contexts c multiplying by the corresponding item-item coef fi cients which can be extracted from matrix W , where CRD matrix D coupled with R is used to estimate u  X  X  ratings on the other items in c by Equation 2, because it is not guaranteed that u i has left those ratings in the same context Generally, the CSLIM-I-CI model will learn the parameters in D and W based on each entry of known contextual ratings R i,j,c in the training set, and make predictions only relying on three ma-trices: non-contextual rating matrix R , CRD matrix D ,andthe aggregation coef fi cient matrix W .

Theoretically, Equation 2 can be used for prediction, but we be-lieve it is not reliable to use this way, due to the fact that deviations are usually very small. It would be easier to predict a user X  X  con-textual ratings on an item if we already know that user X  X  preference on the item. A more typical situation is one in which the recom-mender does not have any ratings for an item in any context. In our CSLIM-I models, as shown in Equation 3, we set h = j to avoid the model learning from u i  X  X  existing non-contextual rating on t j , where the ranking score is predicted as in the original SLIM-I models  X  aggregation of users X  ratings on the other items within thesamecontexts.

Using squared error as the optimization criteria, the loss function with regularization terms can be described in Equation 4.  X  parameters are the learning rates. Both F (Frobenius) terms (e.g., W 2 F )and 1 terms (e.g., W 1 ) are included, where the regularization term is usually applied for sparse models.
Minimize D,W
Note that S i,j,c in Equation 3 is just the aggregated rating, which is not equivalent to the predicted contextual rating, but it can be viewed as a ranking score and can be used in the loss function in Equation 4 for the optimization purpose, where the similar oper-ation (i.e., R i,j,c  X  S i,j,c ) was also adopted in the original SLIM algorithms [10]. Using stochastic gradient descent (SGD) for opti-mization, the parameters can be updated by Equations 5-7: where  X  ,  X  and  X  parameters are the learning rates. Note that in W h,j , we only update parameters for corresponding position where user u i has rated the corresponding items (i.e., R In D h,l , we only update parameters in position l when c l =1. The pseudo-code for CSLIM-I-CI can be present in Algorithm 1. Algorithm 1: CSLIM-I-CI, training phase based on R i,j,c Input : sizes of users, items &amp; contextual conditions:
Input : Build non-contextual rating matrix R , Initialize CRD
Input : Learning parameters:  X  1 ,  X  2 ,  X  2 ,  X  2 ,  X  1 ,
Input : Binary vector c , Training iteration T 1for iter =1; iter  X  T ; iter ++ do 2 /* Below is the prediction function */; 3for h =1; h  X  N ; h ++ do 5if h = j then 7i f k&gt; 0 then 8f or l =1; l  X  L ; l ++ do 9 k += D h,l  X  c l ; 10 end 12 end 13 end 14 end 16 /* Below is the learning process for parameter updates */; 17 for h =1; h  X  N ; h ++ do 18 if h = j then 19 w = W h,j ; 20 if R i,h &gt; 0 then 21 Update W h,j based on Equation 7; 22 end 23 for l =1; l  X  L ; l ++ do 24 if c l =0 then 25 Update D h,l based on Equation 6 and w ; 26 end 27 end 28 end 29 end 30 end
In CSLIM-I-CI, D is modeled as a CI matrix, where a deviation is learnt for each &lt;item, contextu al condition&gt; pair. The deviations can also be considered for users  X  we assume there is a rating de-viation for each &lt;user, contextual condition&gt; pair, where in a CU matrix (i.e., columns denote contexts and rows represent users). The latter model is called CSLIM-I-CU.

In addition to these two approaches, the rating deviations can also simply be viewed as the deviations only associated with each contextual condition rather than as being paired with users or items. In other words, each contextual condition will brin g a rating devi-ation and the value is assumed as the same regardless who is the user and which item it is. As a result, the CRD can be represented by a L -length vector d instead of a matrix D . We call this approach CSLIM-I-C.

Note that the only difference among those three CSLIM-I models is the matrix D , while the matrix W is still the same  X  a matrix of item-item coef fi cients. Context can also be incorporated into the SLIM-U models. In this case, W is a M  X  M matrix representing user-user coef fi cients. The matrix D can be built using a similar approach as introduced in the CSLIM-I models. In other words, we can build three CSLIM-U models: CSLIM-U-CI, CSLIM-U-CU, CSLIM-U-C.

The CSLIM-U-CU model, for example, utilizes user-based K -nearest-neighbor (UserKNN) collaborative fi ltering [16] to estimate S i,j,c using a sparse aggregation of the ratings on item t j texts c by other users who have rated it. Meanwhile, the CRD ma-trix D is modeled by a CU matrix which is of size M  X  L ,and is a M  X  M matrix, with zero values in the diagonal, which is used to estimate coef fi cients between each pair of users.

The loss function for CSLIM-U-CU model is the same as Equation 4 and the estimation for ranking score can be computed by E-quation 8. The parameters in matrices W and D can be updat-ed accordingly using SGD as the optimizer, as shown in Equa-tions 5, 9, 10, where u h belongs to the set of users who have rat-ed item t j except u i , W measures coef fi cients between every two users.

In a summary, CSLIM-I and CSLIM-U models utilize the intu-itions behind ItemKNN and UserKNN, respectively, with the CRD matrix computed in one of three ways: CRD associated to each individual contextual condition, CRD associated with each &lt;item, contextual condition&gt; pair, or CRD associated with each &lt;user, contextual condition&gt; pair, which results in six CSLIM models. In contrast to other CARS algorithms, CSLIM takes advantage of both NBCF-based and MF-based algorithms  X  CSLIM is derived by the intuitions behind ItemKNN or UserKNN, where the CRD matrix D and coef fi cient matrix W were learned in a similar way as matrix factorization using SGD as the optimizer. As a result, CLSIM is able to obtain good accuracy in recommendation while retaining transparency. D interprets how the rating is deviated from non-contextual rating when user is within different contextual con-ditions, and W measures the coef fi cients between every two items (or users) which are analogous to item-item (or user-user) similari-ties in NBCF. These values could be used, for example, in explain-ing a recommendation.

Besides, CSLIM helps alleviate the sparsity problem in contextu-al ratings by making use of the non-contextual ratings. In CSLIM, the contextual ratings are used to train the model, and the ranking score is estimated by the non-contextual ratings in R , the CRD ma-trix D and the coef fi cient matrix W . The model can be trained and built incrementally as contextual ratings are added and is therefore can deliver useful recommendations while contextual ratings are still being gathered.

CSLIM are based on co-ratings among items or users. Thus, this approach may be affected by the cold-start problem. For example, consider a situation where a user has no ratings or an item has not been rated before. This, of course, is a long-standing problem in recommender systems research. One possible solution is to use the item X  X  or the user X  X  average rating or even the global average rating in the data to fi ll the non-contextual rating matrix and then make predictions based on CSLIM models. Once a user has at least one contextual rating, CSLIM is able to learn the corresponding CRD to make recommendations in the future. Context-aware matrix factorization (CAMF) [5] has to learn con-textual rating deviations, user and item matrices, as well as other parameters, such as user or item biases. In CSLIM, however, there is only one matrix W must be learned in addition to contextual rating deviations.

The complexity of CSLIM models is associated with the num-ber of users or items and the number of contextual conditions. For example, too many contextual conditions will result in a large ma-trix D ; large scale of ratings will create a large matrix matrix W will be large if there are many items. This same situation also happens in the original SLIM approach [10].

There are different solutions to tackle this problem. First of all, computational costs in D can be easily reduced by selecting con-texts in advance based on contextual relevance [12]. In addition, when it comes to large numbers of users or items, feature selection can be applied to reduce the number of user or item dimensions as instructed in SLIM approach [10]. For example, the implementa-fi nd a neighborhood of the item to be predicted (i.e., target item), where only the ratings given to those neighbors and the coef fi cients between the target item and the individual item in the neighborhood will be involved in the computation process [10]. In addition, co-ordinate descent can be applied to replace the SGD optimization, to allow parallel computing for large-scale applications. However, our research has been limited in this respect because of the small size of existing public data sets containing contextual ratings. Recall that the CRD matrix D in the CSLIM algorithms estimates the rating deviation between a contextual rating and the correspond-ing non-contextual rating. Actually, the non-contextual rating can be considered as a special contextual rating, where the contextual conditions are all empty. Thus, the deviations in D can be viewed as the ones deviated from empty (denoted by  X ) contexts. In addition to the CSLIM models mentioned above, a general CSLIM (GCSLIM) approach can be built, where the contextual de-viations can be the ones deviated from any contexts other than the  X  contexts. In this case, the matric D is expected to be a CC matrix, where the rows and columns represent the L contextual conditions, and each entry in D represents the rating deviation from a contex-tual condition t o another contextual condition.
 Accordingly, we can build two GCSLIM models: GCSLIM-I-CC and GCSLIM-U-CC. Take the GCSLIM-I-CC model for ex-ample, it is modi fi ed from the CSLIM-I model, where user X  X  con-textual rating on the items in c -1 (Equation 2) can be estimated by user X  X  rating on the items within other contexts. As shown in Table 3, we have no knowledge about R u,i,c -1 , but we know the  X  contexts. Previous CSLIM models only estimate the contextual rating deviations from c -3 to c -1 ; but in GCSLIM, the of a CC matrix is able to estimate the R u,i,c -1 from any contextual ratings, such as R u,i,c -2 ;inotherwords, R u,i,c -1 = (Kids, Partner) + D (Weekday, Weekend).
 A visualization of GCSLIM-I-CC model is shown in Figure 5. The multi-dimensional rating space is represented by space context factors are the extra dimensions in addition to the user and item dimensions. In GCSLIM models, the u i  X  X  rating on item context c can be estimated from u i  X  X  rating on t j within any oth-er contexts (except c ) to replace the original prediction function in Equation 2. Note that a contextual rating deviation is expected to be applied for each contextual factor where the conditions in this factor are different. For example, in Table 3, we may only take the rating deviations between Time=Weekday and Time=Weekend into consideration and skip the rating deviations between condi-tions in different contextual factors, e.g., Companion=Kids and Time=Weekend . As a result, multiple contextual ratings can be used to train the GCSLIM-I-CC m odel instead of using the non-contextual rating only. And the estimated ranking score can be computed in a similar way in order to fi nally provide the top-context-aware recommendations.

In CSLIM, as shown in Equation 2, the contextual deviations are learned and trained from only one non-contextual rating. Howev-er, GCSLIM enables the model to learn contextual rating deviations from any context, where a user may give multiple ratings on an item within different contexts  X  it will result in a challenge on compu-tational ef fi ciency if using all contextual ratings for training, which was also experienced in our experiments. In this case, it is nec-essary to develop some strategies to pre-select limited contextual ratings for training. Here, we introduce three strategies as follows:
Note that the all three strategies above just select one contextual rating each time, it is also possible to select multiple contextual ratings, but that would lead to higher computational costs.
The CC matrix can be paired with either users or items, which is similar to the CSLIM-I-CU and CSLIM-I-CI approaches. In this case, the rating deviations can be represented by a tensor. However, it would be expensive to assign a CC matrix to each user or item. A possible solution is to assign a CC matrix to each group of users or items, where users or items can be put into different groups by clustering. We plan to explore such solutions in our future work.
In short, the only difference between GCSLIM and CSLIM mod-els is the estimation of contextual rating shown in Equation 2, where CSLIM learns the CRD deviated from  X  contexts, but GCSLIM en-ables to learn the ones deviated from any contexts. However, it is possible that the context-aware data set is too sparse, where ratings were not given multiple times in different contexts. In that case, it is likely that only a limited number of entries in the CRD matrix will be learned. Thus, it is necessary to compare the performance between CSLIM and GCSLIM models to fi nd out which ones are more appropriate for the top-N recommendations. In this section, we introduce our data sets and evaluation protocols. We have successfully evaluated our CSLIM and GCSLIM models over fi ve real-world context-aware data sets in different domain-s 2 : Food data set is the AIST Food data [13] with users X  ratings on food menus. Restaurant data [14] collected users X  ratings on restaurants in city of Tijuana, Mexico. Music data [3] captures users X  ratings on music tracks in different driving and traf fi c condi-tions. The Tourism data [4] collects users X  places of interest (POIs) from mobile applications. And the Movie one [2] collects students X  ratings on movies within various situations. Notice that the num-ber of context-aware data sets is limited and they are usually either small or sparse. A summary of those data sets can be viewed in Ta-ble 2. More speci fi c information about the contextual factors and conditions, please refer to the original papers using those data sets.
We use two density metrics (see Equation 11 and 12) to describe the level of sparsity in the data. Assume there are F contextual factors, D cr measures the density of contextual ratings, where |  X  | is used to measure the size of the user, item or context dimension. on unique &lt;user, item&gt; entries. D mr = 100% indicates each unique &lt;user, item&gt; entry was given multiple ratings (i.e., more than one rating) in different contexts. Thus the Food, Music and Tourism data are dense in multiple contextual ratings  X  contextual effects are expected to be signi fi cant and helpful in those data sets. We have no knowledge about the non-contextual ratings in the Food data, where we use user X  X  average rating on each item across multiple contexts to fi ll the non-contextual rating matrix we generated 1060 non-contextual ratings given by 212 users on the 20 items for this data. It is reliable, because D mr = 100%. Even if D mr is small, we can still use average ratings to fi ll is because the diagonal values in W are 0s so that the model will never make a prediction based on user X  X  existing ratings on the item we X  X  like to predict. We claim that users X  average rating on items within multiple contexts can be used as the non-contextual ratings, which is also examined in our following experiments. We compare the CSLIM algorithms with state-of-the-art context-aware recommendation algorithms, including TF [8], CAMF (i.e., CAMF-CI, CAMF-CU, CAMF-C) [5] and context-aware splitting approaches (CASA) using biased MF as the recommender (i.e., item splitting, user splitting and UI splitting) [6, 26]. To better present the results based on CAMF and CASA, we just present the results by the best performing CASA and CAMF approaches, where we use "CASA" and "CAMF" to denote them in the follow-ing fi gures.

In addition, we also add the original SLIM (non-contextual SLIM approach) as baseline, where SLIM approaches are able to perform better than the traditional state-of-the-art CARS algorithms intro-duced above in some situations, but we can always fi nd one CSLIM algorithm beats the SLIM approach. For a clearer representation in the fi gures below, we did not include the results of SLIM, please refer to our previous work [28] for more details.

Since SLIM was built for top-N recommendations, we continue to evaluate CSLIM and GCSLIM models by comparing with base-line algorithms in the top-N recommendation task. We split the set of contextual ratings and non-contextual ratings to fi ve pieces respectively and then blend the corresponding pieces to create the fi ve folds of data in order to have equal proportion of contextual and non-contextual ratings in each fold.

We measure the quality of recommendations using 5-fold cross validation based on three metrics: precision, recall and mean av-erage precision (MAP). Precision and recall are two popular met-rics in evaluating the top-N recommendations, where precision is de fi ned as the ratio of relevant items selected to number of item-s recommended, and recall presents the probability that a relevant item will be selected. MAP [18] is another popular ranking met-3 Note that D mr is calculated based on contextual rating only. ric which additional takes the ranks of the recommended items into consideration, and it can be calculated by Equation 13 and 14.
M denotes the number of the users, and N is the size of the recommendation list, where P ( k ) means the precision at cut-off k in the item recommendation list, i.e., the ratio of number of users followed up to the position k over the number k, where m in denotes the number of relevant items.

In CARS, we add contexts as an additional constraint for each recommendation action. Speci fi cally, we provide a list of top-recommended items for each &lt;user, context&gt; pair, instead of a sin-gle user. Then, we measure the hit ratio averaged by each context for each user. Recall and MAP can be measured in a similar man-ner. The resulting metric values may be smaller than the ones in traditional RS, because it is not very common for users to rate mul-tiple items within a same context.

Basically, our goal is to explore the following questions: The results in precision, recall and MAP are shown in Figure 6. The results for GCSLIM models shown in the fi gure are the best ones we obtained among the three strategies (MostSimilar, LeastSimilar and Random) mentioned above. First, we examine how our CSLIM models work competing against the baseline approaches. From an overall view, the CSLIM model-s generally outperform the baseline approaches, including CASA, CAMF and TF algorithms. Some CSLIM approaches may not work better than the baselines, for example, CAMF works better than CSLIM-U models in the Restaurant data in terms of precision, and CASA works better than CSLIM-I models in the Movie data in terms of MAP. However, the best performing CSLIM models can beat all baseline approaches in those fi ve context-aware data set-s across all three metrics. Take precision for example, the best performing CSLIM model obtain a 90%, 12%, 318%, 21% and 45% improvement on precision@10 for the Food, Restaurant, Mu-sic, Tourism and Movie data sets respectively, compared with the best performing baseline CARS algorithm. When it comes to the MAP values, the advantages of CSLIM models are more signi fi -cant  X  clearly, we can see the gaps between CSLIM models and the baseline approaches. We conclude that CSLIM models are promis-ing algorithms in the top-N context-aware recommendations, and they are able to outperform the state-of-the-art CARS algorithms in terms of precision, recall and MAP in our experiments.
We argue above that we can use users X  average rating on each item within multiple contexts as the non-contextual ratings if we have no knowledge about the non-contextual ratings, like the Food data. To further con fi rm this, we generate non-contextual ratings using this way for the other four data sets, split those non-contextual ratings into 5 pieces and blend them to the 5-pieces contextual rat-ing sets to create a new 5-folds for evaluations. We simply test the best performing CSLIM algorithm (selected for each data set) over the new 5-folds sets (i.e., using average ratings as non-contextual ratings), and compare the performance based on precision@10 and MAP@10, where the results can be shown in Figure 7. We use Average to denote the new created data, and Original data which used the real non-contextual ratings. The bars and left y-axis rep-resent precision@10, where the curves and the right y-axis denote MAP@10. We can observe that the differences are small except the Restaurant data. Even so, the performance by those CSLIM al-gorithms based on the Average data are still able to outperform the baseline approaches. Accordingly, we expect a better result if we have the real non-contextual ratings for the Food data. The next question we explore is the comparison between CSLIM-I and CSLIM-U models. Basically, there are no unique patterns to say whether CSLIM-I or CSLIM-U models work better. Based on the experimental results, we observe that the CSLIM-U models are better than CLSIM-I models in the Food and Movie data, but CLSIM-I models are better for the other three data sets.
We believe that there are at least three factors in fl uencing the performance of CSLIM models: First of all, whether UserKNN or ItemKNN is more appropriate for the data is a relevant issue. Recall that CLSIM-I models measure item-item coef fi cients in Matrix which utilizes the intuition behind ItemKNN, while CSLIM-U can be viewed as analogous to UserKNN. Thus, the ratio of ratings given by users to items can be used to make a decision on which model will perform better in advance.
 We develop two metrics to examine those data characteristics. Average Density of User Pro fi les (ADUP) is obtained by the num-ber of unique users who gave ratings to a speci fi c item divided by the number of all unique users in the data set. We calculate this metric for all unique items and analyze the distributions (simply we choose the mean and max values) as shown in the Table 4. Av-erage Density of Item Pro fi les (ADIP) is calculated in a similar way  X  by the number of unique items each user rated divided by the total number of all unique items in the data. Obviously, ADUP is correlated with the performance of CSLIM-U models, because the larger ADUP it is, the more user-user coef fi cients (i.e., entries in W ) will be learnt. Accordingly, ADIP is correlated with CSLIM-I models.

We use the mean and max values of ADUP and ADIP to infer whether CSLIM-U or CSLIM-I models are more appropriate. The max value is important; for example, that the max value of ADUP equals to 1.0 implies that there is at least one item has been rated by all users. In this case, the coef fi cient between the target user and all the other users will be learnt in the training phrase. In other words, more entries in W will be learnt. Based on these two metrics, we can observe that ADUP are larger than ADIP in the Food and Movie data, which results in CSLIM-U models will work better than the CSLIM-I models in those two data sets. The pattern also con fi rms the advantages of CSLIM-U over CSLIM-I models for the other three data sets, where ADIP values are larger than ADUP.
The second factor in fl uencing the performance of CSLIM mod-els is the density of ratings in contexts, i.e., the D mr duced previously. The larger a D mr is, it implies &lt;user, item&gt; pairs were assigned multiple ratings within different contexts, thus it is possible that more entries in the CRD matrix D can be learnt in the training process. E.g., Food and Music data obtain a 100% value, and the performance of CSLIM models is signi fi cant better than the baselines. Movie data is the one with the least D mr ue, the improvement is not that signi fi cant as Food and Music data, especially when it comes to MAP values, where CASA approach works pretty good, and only the CSLIM-U-CI can beat it.
Finally, contextual dependency is another in fl uential factor in the performance of the models. For example, CSLIM-I-CU outper-forms CSLIM-I-CI, and CSLIM-U-CU outperforms CSLIM-U-CI in the food data, which implies that contexts are more dependent on the user dimension than the item dimension. This observation is also con fi rmed in previous work [26] using this data.
In summary, to select an appropriate CSLIM model, those three factors should be taken into account, especially the comparison be-tween ADIP and ADUP values, as well as the D mr metric. Based on the experimental results shown in Figure 6, we fi nd GC-SLIM models generally outperforming the baseline approaches (i.e., CAMF, TF and CASA) except in the Movie data where CASA out-performs GCSLIM models in MAP. Recall that GCSLIM models are more general versions of the CSLIM approaches. It is neces-sary to compare the performance between those two series of mod-els. As the results indicate, GCSLIM outperforms CSLIM mod-els in the Food data and Restaurant data  X  offering additional 2% and 42% improvement on precision@10, 4% and 2% improvement on MAP@10, compared with the best performing CSLIM model-s. Also, GCSLIM-U-CC is able to provide a 6% improvement on MAP@10 in the Tourism data but not that good in terms of the precision, compared with the best performing CSLIM models.
These results are not surprising, since measuring rating devia-tions from any context increases the learning space  X  it is possible that several entries in the matrix D will not be learned because rat-ings are only given in limited contextual conditions. So, it may only be effective when it is applied to a data set with limited num-ber of contextual conditions. GCSLIM works better than CSLIM models in the Food and Restaurant data sets because the number of contextual conditions (i.e., the values of L ) in those two data set-s are the smallest among the fi ve data sets. We suggest using the CSLIM models unless there are limited contextual conditions and ratings are given within multiple data sets (i.e., D mr is large).
In addition, we measure the performance of GCSLIM models using the three strategies: MostSimilar, LeastSimilar and Random. We examine those three strategies, and show the results in Figure 8 where we measure precision@10 and MAP@10 using GCSLIM-U-CC model. Basically, there is no general pattern. MostSimilar works the best for Restaurant, Music and Movie data sets, Least-Similar works the best in Tourism data, and the Random beats other two choices in the Food data in Precision but works slightly worse in MAP. We suggest to use the Random way because it is able to obtain similar performance and it is not necessary to measure the similarity of the contexts, which further reduces the computational costs especially when it comes to a data set with several contextual conditions. We X  X  like to explore more effective and ef fi cient ways to improve the GCSLIM models in our future work.
 In a summary, our results demonstrate that both CSLIM and GC-SLIM are able to outperform the state-of-the-art CARS algorithm-s including CASA, CAMF and TF in the top-N recommendation task through experiments over fi ve real-world contextual data sets. GCSLIM may work better than CSLIM models when the number of contextual conditions is limited. In addition, we also discover some underlying patterns to select the appropriate CSLIM models in advance based on the data characteristics. In this paper, we introduced a new approach to incorporate context into the sparse linear model recommendation technique, resulting in CSLIM and GCSLIM models that are based on estimating con-textual ratings from rating deviations across contexts. We demon-strate the effectiveness of CSLIM and GCSLIM models through a detailed comparative evaluation with the state-of-the-art CARS algorithms using fi ve context-aware data sets. Our proposed ap-proaches are promising for the top-N context-aware recommenda-tion task. We also identi fi ed several underlying factors that can help pre-select appropriate models in advance.

One possible weakness of CSLIM and GCSLIM models is that very few coef fi cients in matrix W may be learned if the number of co-ratings by users is small. This also happens in the original SLIM algorithm, where Kabbur et al. [7] proposed FISM approach to solve this problem, and the solution can be reused to alleviate the same problems in our CSLIM and GCSLIM models. We will explore this option in our future work.

In addition, there would be several ways to improve CSLIM models. For example, side information, such as item features [11] (e.g., movie genre, music album, etc) and user pro fi les (e.g., age, gender, etc) can be further incorporated into CSLIM and GCSLIM models to improve the performance. Beyond deviations, contexts can be incorporated into SLIM model by modeling similarity of the contexts: One way is to develop the similarity-based CSLIM algo-rithms [19], where matrix D is able to measure the similarity of each two contextual situations; also, contexts can be directly incor-porated into the coef fi cient matrix W instead of adding the CRD matrix D to the SLIM models. We believe those models are more straightforward and promising because they are able to mine the coef fi cients among different context conditions directly. [1] G. Adomavicius, B. Mobasher, F. Ricci, and A. Tuzhilin. [2] G. Adomavicius, R. Sankaranarayanan, S. Sen, and [3] L. Baltrunas, M. Kaminskas, B. Ludwig, O. Moling, F. Ricci, [4] L. Baltrunas, B. Ludwig, S. Peer, and F. Ricci. Context-aware [5] L. Baltrunas, B. Ludwig, and F. Ricci. Matrix factorization [6] L. Baltrunas and F. Ricci. Experimental evaluation of [7] S. Kabbur, X. Ning, and G. Karypis. Fism: factored item [8] A. Karatzoglou, X. Amatriain, L. Baltrunas, and N. Oliver. [9] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization [10] X. Ning and G. Karypis. Slim: Sparse linear methods for [11] X. Ning and G. Karypis. Sparse linear methods with side [12] A. Odic, M. Tkalcic, J. F. Tasic, and A. Ko X ir. Relevant [13] C. Ono, Y. Takishima, Y. Motomura, and H. Asoh.
 [14] X. Ramirez-Garcia and M. Garca-Valdez. Post-fi ltering for a [15] S. Rendle, Z. Gantner, C. Freudenthaler, and [16] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [ 17] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based [18] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support [19] Y. Zheng. Deviation-based and similarity-based contextual [20] Y. Zheng, R. Burke, and B. Mobasher. Differential context [21] Y. Zheng, R. Burke, and B. Mobasher. Optimal feature [22] Y. Zheng, R. Burke, and B. Mobasher. Differential context [23] Y. Zheng, R. Burke, and B. Mobasher. Recommendation [24] Y. Zheng, R. Burke, and B. Mobasher. The role of emotions [25] Y. Zheng, R. Burke, and B. Mobasher. Emotions in [26] Y. Zheng, R. Burke, and B. Mobasher. Splitting approaches [27] Y. Zheng, B. Mobasher, and R. Burke. Context [28] Y. Zheng, B. Mobasher, and R. Burke. Cslim: Contextual
