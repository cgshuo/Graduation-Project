 We study the problem of detecting coordinated free text campaigns in large-scale social media. These campaigns  X  ranging from coordinated spam messages to promotional and advertising campaigns to political astro-turfing  X  are grow-ing in significance and reach with the commensurate rise of massive-scale social systems. Often linked by common  X  X alking points X , there has been little research in detecting these campaigns. Hence, we propose and evaluate a content-driven framework for effectively linking free text posts with common  X  X alking points X  and extracting campaigns from large-scale social media. One of the salient aspects of the framework is an investigation of graph mining techniques for isolating coherent campaigns from large message-based graphs. Through an experimental study over millions of Twitter messages we identify five major types of campaigns  X  Spam, Promotion, Template, News, and Celebrity cam-paigns  X  and we show how these campaigns may be extracted with high precision and recall.
 Categories and Subject Descriptors: H.3.5 [Online In-formation Services]: Web-based services; J.4 [Computer Ap-plications]: Social and behavioral sciences General Terms: Algorithms, Design, Experimentation Keywords: social media, campaign detection
Social media is inherently a persuasive technology, sup-porting the rapid insertion of new memes, near instanta-neous global reach, and unprecedented leveraging of massive-scale interpersonal connections. On the one hand, many users of social media organically engage with social media to share opinions and interact with friends; on the other, social media is a prime target for strategic influence.
For example, there is widespread anecdotal evidence of  X  X stroturfing X  campaigns [3], in which political operatives in-sert memes into sites like Twitter and Facebook in an effort to influence discourse about particular political candidates and topics. In addition, there are large campaigns of coordi-nated spam messages in social media [4], templated messages (e.g., auto-posted messages to social media sites from third-party applications announcing a user action, like joining a game or viewing a video), high-volume time-synchronized messages (e.g., many users may repost news headlines to social media sites in a flurry after the news has been ini-tially reported), and so on. In the case of spam and promo-tion campaigns, the relative openness of many social media sites (typically requiring only a valid email address to reg-ister) suggests coordinated campaigns could be a low-cost approach for strategically influencing participants.
User-driven campaigns  X  often linked by common  X  X alking points X   X  appear to be growing in significance and reach with the commensurate rise of massive-scale social systems. However, there has been little research in detecting these campaigns. While there has been some progress in detecting isolated instances of long-form fake reviews (e.g., to promote books on Amazon), of URL-based spam in social media, and in manipulating recommender systems [4, 5, 6, 7], there is a significant need for new methods to support web-scale detection of campaigns in social media.

Hence, we focus in this paper on detecting one particular kind of coordinated campaign  X  those that rely on  X  X ree text X  posts, like those found on blogs, comments, forum postings, and short status updates (like on Twitter and Facebook). For our purposes, a campaign is a collection of users and their posts bound together by some common objective, e.g., promoting a product, criticizing a politician, or inserting disinformation into an online discussion. Our goal is to link messages with common  X  X alking points X  and then extract multi-message campaigns from large-scale social media. De-tecting these campaigns is especially challenging consider-ing the size of popular social media sites like Facebook and Twitter with 100s of millions of unique users and the inher-ent lack of context in short posts.

We explore in this paper several content-based approaches for identifying campaigns from the massive scale of real-time social systems. Concretely, we propose and evaluate a content-driven framework for effectively linking free text posts with common  X  X alking points X  and extracting cam-paigns from large-scale social media. We find that over millions of Twitter messages, the proposed framework can identify 100s of coordinated campaigns, ranging in size up to several hundred messages per campaign.
In this section, we describe the problem of campaign de-tection in social media, introduce the data, and outline the metrics for measuring effective campaign detection.
We consider a collection of n participants across social media sites U = { u 1 ,u 2 ,...,u n } , where each participant u may post a time-ordered list of k messages M u i = { m i 1 ... , m ik } . Our hypothesis is that among these messages, there may exist coordinated campaigns .

Given the set of users U , a campaign M c can be defined as a collection of messages and the users who posted the mes-sages: M c = { m ij ,u i | u i  X  U  X  m ij  X  M u i  X  theme ( m t } such that the campaign messages belong to a coherent theme t k . Themes are human-defined logical assignments to messages and application dependent. For example, in the context of spam detection, a campaign may be defined as a collection of messages with a common target product (e.g., Viagra). In the context of astroturf, a campaign may be defined as a collection of messages promoting a particular viewpoint (e.g., the veracity of climate change). Addition-ally, depending on the context, a message may belong to one or multiple themes. For the purposes of this paper and to focus our scope of inquiry, we consider as a theme all mes-sages sharing similar  X  X alking points X  as determined by a set of human judges.
To evaluate the quality of a campaign detection approach, we would ideally have access to a large-scale  X  X old set X  of known campaigns in social media. While researchers have published benchmarks for spam webpages, ad-hoc text re-trieval, and other types of applications, we are not aware of any standard social media campaign dataset. Hence, we take in this paper a twofold approach: (i) a small-scale validation over hand-labeled data; and (ii) a large-scale validation over 1.5 million Twitter messages for which ground truth is not known.
 CD Small : First, we sample a small collection of messages (1,912) posted to Twitter in October 2010. Over this small campaign dataset ( CD Small ), two judges labeled all pairs of the 1,912 tweets as sharing similar  X  X alking points X  or not, finding 298 pairs of messages sharing similar  X  X alking points X . Based on these initial labels, the judges considered all com-binations of messages that may form campaigns consisting of four messages or more, and found 11 campaigns ranging in size from four messages to eight messages. While small in size, this hand-labeled dataset allows us to evaluate the precision and recall of several campaign detection methods. CD Large : Second, we supplement the small dataset with a large collection of messages (  X  1.5 million) posted to Twitter between October 1 and October 7, 2010. We sampled these messages using Twitter X  X  Streaming API, resulting in a rep-resentative random sample of Twitter messages. Over this large campaign dataset ( CD Large ), we can test the preci-sion of the campaign detection methods and investigate the types of campaigns that are prevalent in-the-wild. Since we do not have ground truth knowledge of all campaigns in this dataset, our analysis will focus on the campaigns detected for which we can hand-label as actual campaigns or not.
To measure the effectiveness of a campaign detection method, we use variations of average precision, average recall, and the average F 1 measure. The average precision (AP) for a campaign detection method is defined as: where n is the total number of predicted campaigns by the campaign detection method, PC is a predicted campaign, and TC is an actual (true) campaign. MaxCommonMessage function returns the maximum of the number of common messages in both the predicted campaign i ( PC i ) and each of the actual (true) campaigns ( TCs ). For example, suppose a campaign detection method identifies a three-message cam-paign: { m 1 ,m 10 ,m 30 } . Suppose there are two actual cam-paigns with at least one message in common: { m 30 ,m 38 ,m = 2 / 3. In the aggregate, this individual precision will be av-eraged with all n predicted campaigns.
 Similarly, we can define the average recall (AR) as: where n is the number of the predicted campaigns, and TC j is a true campaign which has the largest common messages with the predicted campaign i ( PC i ). Continuing the ex-ample from above, the Recall would be max (2 , 1) / 5 = 2 / 5. Finally, we can combine precision and recall as the average F 1 measure (AF):
An effective campaign detection approach should identify predicted campaigns that are composed primarily of a single actual campaign (i.e., have high precision) and that contain most of the messages that actually belong to the campaign (i.e., have high recall). A method that has high precision but low recall will result in only partial coverage of all campaigns available (which could be especially disastrous in the case of spam or promotional campaigns that should be filtered). A method that has low precision but high recall may identify nearly all messages that belong to campaigns but at the risk of mislabeling non-campaign messages (resulting in false positives, which could correspond to mis-labeled legitimate messages as belonging to spam campaigns).
To detect coordinated campaigns, we explore in this paper several content-based approaches for identifying campaigns. Our goal is to identify methods that can balance both pre-cision and recall for effective campaign detection. We pri-marily consider a graph-based framework, where we model messages in social media as a message graph . Each node in the message graph corresponds to a message; edges corre-spond to some reasonable notion of content-based correla-tion between messages, corresponding to pairs of messages with similar  X  X alking points. X  Formally, we have:
Definition 1 (Message Graph). A message graph is a graph G = ( V,E ) where every message in M corresponds to a vertex m ix in the vertex set V . An edge ( m ix ,m exists for every pair of messages ( m ix ,m jy ) where corr ( m ) &gt;  X  , for a measure of correlation and some parameter  X  .
A message graph which links unrelated messages will nec-essarily result in poor campaign detection (by introducing spurious links). Traditional information retrieval approaches for document similarity (e.g., cosine similarity, KL-divergence) as well as efficient near-duplicate detection methods (e.g., Shingling [1], I-Match [2] and SpotSigs [8]) have typically not been optimized for the kind of short posts of highly-variable quality common in many social media sites (including Face-book and Twitter). Hence, we shall investigate experimen-tally several possible approaches for determining pairwise message correlation which guides the formation of the mes-sage graph.

Given a message graph, we propose to explore three graph-based approaches for extracting campaigns:(i) loose extrac-tion; (ii) strict extraction; and (iii) cohesive extraction. Ex-perimentally, we compare these graph-based approaches ver-sus a traditional k-means clustering approach and reach poor results for clustering as compared to the graph methods. For now, we focus our attention on extracting content-driven campaigns via graph mining.
The first approach for content-driven campaign detection is what we refer to as loose campaign extraction . The main idea is to identify as a logical campaign all chains of messages that share common  X  X alking points X . In this way, the set of all loose campaigns is the set of all maximally connected components in the message graph:
Definition 2 (Loose Campaign). A loose campaign is a subgraph s = ( V 0 ,E 0 ) , such that s is a maximally con-nected component of G , in which s is connected, and for all vertices m ix such that m ix  X  V and m ix /  X  V 0 there is no vertex m jy  X  V 0 for which ( m ix ,m jy )  X  E .

As an example, Figure 1 illustrates a collection of 10 mes-sages, edges corresponding to messages that are highly corre-lated, and the two maximal components (corresponding to loose campaigns): { 1, 2, 3, 6, 7, 8, 9 } and { 4, 5 } . Such an approach to campaign detection faces a critical chal-lenge, however: not all maximally connected components are necessarily campaigns themselves (due to long chains of tangentially-related messages). For example, a chain of sim-ilar messages A X  X  X  X  X ... X  X , while displaying local similarity properties (e.g., between A and B and between Y and Z) will necessarily have low similarity across the chain (e.g., A and Z will be dissimilar since there is no edge between the pair, as in the case of messages 9 and 1 in Figure 1). In prac-tice, such maximally connected components could contain disparate  X  X alking points X  and not strong campaign coher-ence.
A natural alternative is to constrain campaigns to be max-imal cliques, what we call strict campaigns :
Definition 3 (Strict Campaign). A strict campaign s = ( V 00 ,E 00 ) in a message graph G = ( V,E ) , in which V 00  X  V and E 00  X  E , such that for every two vertices m and m jy in V 00 , there exists an edge ( m ix ,m jy )  X  E the clique cannot be enlarged by including one more adjacent vertex (corresponding to a message in M ).

To identify these strict campaigns, we can first identify all loose campaigns  X  by identifying all maximally connected components over the message graph, we can prune from consideration all singleton messages and are left with a set of candidate campaigns. Over these candidates, we can identify the strict campaigns through maximal clique min-ing. However, discovering all maximal cliques from a graph is an NP-hard problem (i.e., the time complexity is expo-nential). Finding all maximal cliques takes O (3 n/ 3 ) in the worst case where n is the number of vertices [9]. Over large graphs, even with parallelized implementation over MapReduce-style compute clusters, the running time is still O (3 n/ 3 /m ) in the worst case, where n is the number of ver-tices and m is the number of reducers [11].

And there is still the problem that even with a greedy ap-proximation, strict campaign detection may overconstrain the set of campaigns, especially in the case of loosely-connected campaigns. Returning to the example in Figure 1, the max-imal cliques { 1, 2, 3 } and { 2, 3, 6 } would be identified as strict campaigns, but perhaps { 1, 2, 3, 6, 7 } form a coherent campaign even though the subgraph is not fully-connected. In this case the strict approach will identify multiple overlap-ping campaigns and will miss the larger and (possibly) more coherent campaign. In terms of our metrics, the expectation is that strict campaign detection will favor precision at the expense of recall.
Hence, we also consider a third approach which seeks to balance loose and strict campaign detection by focusing on what we refer to as cohesive campaigns , which relaxes the conditions of maximal cliques:
Definition 4 (Cohesive Campaign). Given a message graph G = (V,E), a subgraph G X  is called a cohesive cam-paign if the number of edges of G X  is close to the maximal number of edges with the same number of vertices of G X .
The intuition is that a cohesive campaign will be a dense but not fully connected subgraph, allowing for some varia-tion in the  X  X alking points X  that connect subcomponents of the overall campaign. There are a number of approaches mining dense subgraphs and the exact solution is again NP-hard in computation complexity, so we adopt a greedy ap-proximation approach following the intuition in [10]. The approach to extract cohesive campaigns requires a notion of maximum co-clique CC ( m ix ,m jy ) for all neighbors: Definition 5 (Maximum co-clique: CC ( m ix ,m jy ) ). Given a message graph G = (V,E), the maximum co-clique CC ( m ix ,m jy ) is the (estimated) size of the largest clique containing both vertices m ix and m jy , where m jy  X  V and m jy is a neighbor vertex of m ix (i.e., they are connected).
Considering all of a vertex X  X  neighbors, we define the largest of the maximum co-cliques as C ( m ix ):
Definition 6 ( C ( m ix ) ). Then, C ( m ix ) is the largest value between m ix and any neighbor m jy , formally defined as C ( m ix ) = max { CC ( m ix ,m jy ) ,  X  m jy  X  Neighbor ( m
With these definitions in mind, our approach to extract cohesive campaign is as follows: 1. Estimate each vertex X  X  C ( m ix ): In the first step, our goal is to estimate the C values for every vertex in a candidate campaign which indicates the upper bound of the maximum clique size the vertex belongs to. Starting at a random vertex m ix in s , we compute the maximum co-clique size CC ( m ix ,m jy ), where m jy  X  V 0 and m jy is a neighbor vertex of m ix . Then, we compute C ( m ix ). We insert m into a priority queue and sort all m jy by CC ( m ix Next, we greedily advance to the m jy , which has the largest CC ( m ix ,m jy ) among all m jy , and remove it from the queue. Finally, we compute C ( m jy ). We repeat this procedure for every vertex in the candidate campaign. At the conclusion of this procedure, we have an estimated C ( m ix ) for every vertex. 2. Cohesive campaign extraction : Given the estimated C ( m ix ) for every vertex in a candidate campaign, by con-sidering the order in which the greedy algorithm in Step 1 encounters each vertex, we can consider consecutive neigh-bors as potential members of the same coherent campaign. Intuitively, the C ( m ix ) values should be high for vertices in dense subgraphs but should drop as the algorithm encoun-ters nodes on the border of the dense subgraph, then rise again as the algorithm encounters vertices belonging to a new dense subgraph. We identify the first vertex with an increasing C ( m ix ) over its neighbor as the initial boundary of a cohesive campaign. We next include all vertices be-tween this first boundary up to and including the vertex with a C ( m ix ) value larger than or equal to some threshold (= the local peak value *  X  ). By tuning  X  to 1, the extracted cohesive campaigns will be nearly clique-like; lower values of  X  will result in more relaxed campaigns (i.e., with less den-sity). We repeat this procedure until we extract all cohesive subgraphs in the candidate campaign.

The output of the cohesive campaign extraction approach is a list of cohesive campaigns, each of which contains a list of vertices forming a cohesive subgraph.
In this section, we explore campaign discovery over social media through an application of the framework to messages sampled from Twitter. We begin by examining how to accu-rately and efficiently construct the campaign message graph, which is the critical first step necessary for campaign detec-tion. We find that a short-text modified Shingling-based ap-proach results in the most accurate message graph construc-tion. Based on this finding, we next explore campaign detec-tion methods over the small hand-labeled Twitter dataset, before turning our sights to analysis of campaigns discovered over the large (1.5 million messages) Twitter dataset.
Recall that each node in the message graph corresponds to a message; edges correspond to some reasonable notion of  X  X elatedness X  between messages corresponding to human-labeled similar  X  X alking points X . Our first goal is to answer the question: can we effectively determine if two messages are correlated (i.e., algorithmically determine if they share similar  X  X alking points X ) across hundreds of millions of short messages for constructing the message graph in the first place? This step is critical for accurate message graph for-mation for discovering campaigns.

Using the small campaign dataset ( CD Small ), we consider the 298 pairs of messages sharing similar  X  X alking points X  (as determined by human judges) as the ground truth for whether an edge should appear in the message graph be-tween the two messages. We can measure the effectiveness of a message correlation method by precision, recall, and F
We investigate the identification of correlated messages through a comparative study of five distinct techniques: unigram-based overlap between messages, edit distance, and three representative near-duplicate detection algorithms (Shingling [1], I-Match [2], SpotSigs [8]). Near-duplicate detection ap-proaches have shown great promise and effectiveness by web search engines to efficiently identify duplicate web content, but their application to inherently short messages lacking context is unclear.

In our experiment, we see that the Shingling approach performs the best, with an F 1 = 0 . 81. To improve the per-formance of the Shingling approach with Jaccard coefficient, we propose as a measure of correlation the overlap coefficient cient, we get F 1 = 0 . 88. In the further experiments, we use the Shingling approach with overlap coefficient.
In the previous experiment, we evaluated several approaches to measuring message correlation. Now we turn our atten-tion to evaluating campaign detection methods. We begin in this section with the small data set (which recall allows us to measure precision and recall against ground truth) before considering the large data set.

Over the hand-labeled campaigns in CD Small , we apply the three graph-based campaign extraction methods: (i) loose; (ii) strict; and (iii) cohesive, over the message graph generated via the best performing message correlation method identified in the previous section. We also compare cam-paign extraction using a fourth approach based on text clus-tering. For this non-graph-based approach, we consider k-means clustering, where each message is treated as vector with 10K bag-of-words features, weighted using TF-IDF, with Euclidian distance as a distance function. We vary the choice of k value, and report the best result.

Table 1 presents the experimental results of the four cam-Table 1: Effectiveness Comparison of Campaign De-tection Approaches paign detection approaches. The cohesive campaign detec-tion approach found 11 campaigns ( NumC ) like the ground truth, but missed a message in two campaigns. The strict approach found 12 campaigns, missed one message in a true campaign, and divided a true campaign to two predicted campaigns because the approach due to the strict campaign rule (all nodes in a campaign should be completely con-nected). The loose approach found 12 campaigns, one of which is not an actual campaign (false positive) and some predicted campaigns contain dissimilar messages due to long chains. The k -means clustering algorithm found only 5 cam-paigns. Overall, the cohesive and strict approaches outper-formed the loose and cluster-based approaches. In practice, the ideal approach should return the same number of cam-paigns of the ground truth in order to reduce post-labeling time and to further analysis. In this perspective, the cohe-sive approach may be preferred over the strict approach.
We next examine campaign extraction from the large Twit-ter data set, CD Large . Can we detect coordinated cam-paigns in a large message graph with 1.5 million messages? What kind of campaigns can we find? Which graph tech-nique is the most effective to find campaigns? Message Graph Setup: Based on the best message graph construction approach identified in the previous section, we generated a message graph consisting of 1.5 million vertices (one vertex per message). Of these, 1.3 million vertices are singletons, representing messages without any correlated messages in the sample (and hence, not part of any cam-paign). Based on this sample, we find 199,057 vertices have at least one edge; in total, there are 1,027,015 edges in the message graph.
 Identifying Loose Campaigns: Based on the message graph, we identify as loose campaigns all of the maximally connected components, which takes about 1 minute on a single machine (relying on a breadth-first search with time complexity O ( | E | + | V | ). Figure 2 shows the distribution of the size of the candidate campaigns on a log-log scale. We see that the candidate campaign sizes approximately follows a power law, with most candidates consisting of 10 or fewer messages. A few candidates have more than 100 messages, and the largest candidate consists of 61,691 messages. On closer inspection, the largest candidate (as illustrated in Fig-ure 3) is clearly composed of many locally dense subgraphs and long chains. Examining the messages in this large can-didate, we find many disparate topics (e.g., spam messages, Justin Bieber retweets, quotes, Facebook photo template) and no strong candidate-wide theme, as we would expect in a coherent campaign.
 Identifying Strict Campaigns: To refine these candi-dates, one approach suggested in Section 3 is strict cam-paign detection , in which we consider only maximal cliques as campaigns (in which all message nodes in a subgraph are connected to each other). While maximal clique detection may require exponential time and not be generalizable to all social message datasets, in this case we illustrate the maxi-mal cliques found even though it required  X  7 days of com-putation time (which may be unacceptable for campaign de-tection in deployed systems). Considering the top-10 strict campaigns discovered in order of size: [559, 400, 400, 228, 228, 227, 227, 217, 217, 214], we find high overlap in the campaigns discovered. For example, the 2nd and 3rd strict campaigns (each of size 400) have 399 nodes in common. Similarly, the 4th, 5th, 6th, 7th, and 10th strict campaigns have over 200 nodes in common, suggesting that these five different strict campaigns in essence belong to a single coher-ent campaign (see Figure 4). This identification of multiple overlapping strict campaigns  X  due to noise, slight changes in message  X  X alking points X , or other artifacts of short mes-sages  X  as well as the high cost of maximal clique detection suggests the cohesive campaign detection approach may be preferable.
 Identifying Cohesive Campaigns: We next applied the cohesive campaign extraction approach to the set of candi-date campaigns corresponding to maximal connected com-ponents. We assign  X  to 0.95 and use the CSV tool [10] for an efficient implementation of computing each vertex m ix C ( m ix ) by mapping edges and vertices to a multidimen-sional space. Although computing C ( m ix ) of all vertices takes O( | V | 2 log | V | 2 d ) where d is a mapping dimension, the performance for real datasets is typically sub-quadratic. Like the candidate campaign sizes, the distribution of the size of the cohesive campaigns follow a power law. Since the cohesive campaign extraction approach can isolate dense subgraphs, we see that the large 61,691 message candidate has been broken into 609 sub-components. Compared to Figure 4: An Example Dense Subgraph Campaign: Strict Campaign Detection Identifies 5 Different Maximal Cliques; Cohesive Campaign Detection Identifies a Single Coherent Campaign strict campaign detection, the cohesive campaign extraction approach required only 1/7 the computing time on single workstation.

Examining the top-10 campaigns (shown in Table 2) we see that the cohesive campaign detection approach over-comes the limitations of strict campaign detection by com-bining multiple related cliques into a single campaign (recall Figure 4). The biggest campaign contains 560 vertices and is a spam campaign. The  X  X alking point X  of this campaign is an Iron Man 2 promotion of the form:  X #Monthly Iron Man 2 (Three-Disc Blu-ray/DVD Combo + Digital Copy) ... http://bit.ly/9L0aZU X , though individual messages vary the exact wording and inserted link.

Based on a manual inspection of the identified campaigns, we categorize the campaigns into five categories:  X  Spam campaigns : These campaigns typically post dupli- X  Promotion Campaigns : Users in these campaigns pro- X  Template Campaigns : These are automatically-generated  X  News Campaigns : Participants post recent headlines along  X  Celebrity Campaigns : Users in these campaigns send
Some of these campaigns are organic and the natural out-growth of social behavior, e.g., a group of Justin Bieber fans retweeting a message, or a group posting news articles of in-terest. On closer inspection, we observe that many of the less organic campaigns (e.g., spam and promotion campaigns) are driven by a higher ratio of messages to participants. For example in Table 2, the Iron Man 2 spam campaign consists of 560 messages posted by only 34 different participants. In contrast, the Justin Bieber retweet campaign consists of 153 messages posted by 153 different participants.
In this paper, we have investigated the problem of cam-paign detection in social media. We have proposed and eval-uated an efficient content-driven graph-based framework for identifying and extracting campaigns from the massive scale of real-time social systems. Based on the success of the sys-tem we are extending this work to incorporate adaptive sta-tistical machine learning approaches for isolating artificial campaigns from organic campaigns. Do we find that strate-gically organized campaigns engage in particular behaviors that make them clearly identifiable? Our results suggest that campaigns are not necessarily  X  X nvisible X  to automated detection methods. We are also interested in exploring if campaigns are centralized around common types of users or are they embedded in diverse groups. How early in a cam-paign X  X  lifecycle can a strategic campaign be detected with high confidence? Do we find a change in campaign mem-bership and detection effectiveness after it reaches a critical mass? These challenges motivate our continuing research.
