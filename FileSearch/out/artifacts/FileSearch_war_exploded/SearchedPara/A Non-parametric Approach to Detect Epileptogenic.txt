 Visual detection of lesional areas on a cortical surface is criti-cal in rendering a successful surgical operation for Treatment Resistant Epilepsy (TRE) patients. Unfortunately, 45% of Focal Cortical Dysplasia (FCD, the most common kind of TRE) patients have no visual abnormalities in their brains X  3D-MRI images. We collaborate with doctors from NYU Langone X  X  Comprehensive Epilepsy Center and apply ma-chine learning methodologies to identify the resective zones for these MRI-negative FCD patients. Our task is partic-ularly challenging because MRI images can only provide a limited number of features. Furthermore, data from dif-ferent patients often exhibit inter-patient variabilities due to age, gender, left/right handedness, etc. In this paper, we introduce a new approach which combines the restricted Boltzmann machines and a Bayesian non-parametric mix-ture model to address these issues. We demonstrate the ef-ficacy of our model by applying it to a retrospective dataset of MRI-negative FCD patients who are seizure free after surgery.
 mixture models, Bayesian non-parametric, Restricted Boltz-mann Machine, predictive medicine, semi-supervised learn-ing and application
Epilepsy is a common neurological disorder, affecting ap-proximately 1% of the population [14]. It is characterized by profound abnormal neural activity during seizures and interictal (between seizures) periods. Uncontrolled epilepsy can have harmful effects on the brain and has increased risk of injuries and sudden death [3]. About one third of epilepsy patients remain resistant to medical treatment [21]. Our re-search addresses the identification of lesions in the MRI X  X  of patients with focal cortical dysplasia (FCD), which is rec-ognized as the most common source of pediatric epilepsy [3, 35] and the third most common source in adults having medically intractable seizures [20, 22]. Early detection and subsequent surgical removal of the FCD lesion area is the most effective and is often the last hope for these patients.
The most widely used technology in identifying the epilep-tic lesions is MRI coupled with intracranial EEG (iEEG). For MRI-positive patients, (i.e., patients with visible ab-normal areas in the MRI), the placement of electrodes on the cortex is informed by the pinpointed problematic re-gions detected by visual inspection of the MRI. However, for MRI-negative patients there is no visible lesion to guide precise electrode implantation [17]. The final target for sur-gical resection is based on both of these findings if available coupled with other clinical data. Consequently, the post-surgical success (i.e., seizure-freedom after surgery) ratio of MRI-positive to MRI-negative patients is 66% to 29%. Un-fortunately, 45% of FCD patients are MRI-negative [37]. For this reason the surgical resection procedure remains highly underutilized as most practitioners are unwilling to operate in the absence of a visually detected lesion [34].

The machine learning task is to detect the lesional re-gion(s) in MRI-negative patients. Specifically, our model identifies the abnormal areas in a patient X  X  brain which in turn serve as a focus of attention mechanism for the neuro-radiologists in placing the iEEG sensors on the patient X  X  cor-tex. The work presented in this paper adopts a new Bayesian non-parametric approach as compared to our previous logis-tic regression (LR) based model [1]. Our LR model has been in use at NYU X  X  Comprehensive Epilepsy Center since 2013. The new approach, as we present in Section 6, achieves im-proved performance compared to the LR model.

Our training data comes from the 3D-MRI images of MRI-negative FCD patients who underwent resective brain surgery at NYU and were seizure free after surgery. Furthermore, the resected tissue was histopathologically verified to con-tain FCD. We also have access to the MRI X  X  of healthy con-trols who underwent the same MRI protocol. One challenge in applying machine learning to this dataset is the paucity of features available from the MRI that are predictive of lesional tissue (see Section 2.1 for a complete list of the available features). To address this issue, we employ the Restricted Boltzmann Machines (RBMs) [32, 40] to learn a new set of nonlinear features. An RBM is an undirected graphical model [18] which can be interpreted as a stochastic neural network. Through training, an RBM learns a closed-form distribution of the input data. The units in the bottom layer of the network correspond to the observable attributes and the top layer consists of hidden units that can be viewed as nonlinear feature detectors [16]. Thus, an RBM is often employed to extract more meaningful features from input data [29].

Another challenge while learning with this dataset is the impact of inter-patient variability, i.e., each human brain has its own characteristics depending on the severity of the disease, age, gender, left/right handedness, lifestyle and ge-netics many of which are not uniformly available in our data. Thus, learning from data collected from all patients does not lead to satisfactory performance. To address this issue, we partition the data into subgroups with the goal that each subgroup will contain instances with similar brain charac-teristics. Because we do not have sufficient domain knowl-edge to estimate the number of subgroups, we employ a Bayesian non-parametric Dirichlet process mixture model (DPM, [33]) to infer the number of clusters automatically from the data.
 Our proposed approach leads to an infinite mixture of RBMs. Because a typical mixture (finite or infinite) of RBMs is computationally intractable [23], we propose a two-stage method. We first apply an RBM to the entire dataset and cluster the patients based on the hidden layer of this model using a DPM. The second stage applies a weighted version of RBM to each non-empty cluster using the weights obtained from the DPM model. We discuss the rationale be-hind this two-stage method in Section 5.1.
 We evaluate our model on MRI-negative patients from NYU who were seizure free after resective surgery. Our evaluation includes a successful detection indicator (Y or N) within the resected region and the performance (true negative rate, TNR) outside the resection zone. We need to measure the performance of the two regions separately because we not only want to locate the lesion but also don X  X  want to misclassify non-lesional benign tissue. Note that, instead of using the true positive rate (TPR), we use an indication to measure whether there is any lesion detected inside the resection zone. This is because in the absence of any visual information to locate the lesion precisely, very generous margins are employed during the surgery to en-sure that the patient is seizure free afterwards. As a result, the TPR is not a meaningful evaluation metric because the resection zone is much larger than the actual lesion and con-tains benign areas. Instead, we are interested in assessing whether there is any detection that can potentially guide the placement of iEEG sensors with the ultimate goal of reduc-ing the size of the resection zone, which will lead to a much safer yet effective surgery. In our experiments, our model has accuracies of greater than 99.4% in the benign region and a successful detection in the resection region in 7 out of 12 (i.e., 58%) patients (see details in Section 6). This is in contrast to neuroradiologists X  visual MRI inspections that have a detection rate of 0 out 12.

In this paper, we first describe our method of constructing training data from 3D-MRI images of human brains using surface morphometry [10, 12]. We then present a brief in-troduction to RBM and DPM models in Sections 3 and 4 respectively. In Section 5, we outline our model which in-tegrates the RBM and DPM algorithms. We present and discuss our results in Section 6 and conclude in Section 7.
Surface based morphometry (SBM) provides the means to characterize and analyze the human brain by explicitly mod-eling the cortex using a suitable geometric model [10]. The cortical surface represents the outer layer of the brain mod-eled as a folded two-dimensional surface. It is extracted by delineating the boundary between the gray and white mat-ter using T1-weighted MRI images [10]. The reconstructed surface is represented as a triangulated surface [10], and at each vertex on the surface different morphological features such as cortical thickness, curvature, sulcal depth, etc., can be calculated to characterize the cortex. Similarly, different morphological transforms can be applied to register the cor-tical surface to a standard surface also known as a group-atlas. Registration is achieved by aligning specific sulcal and gyral landmarks across the reconstructed cortical sur-faces allowing for a precise comparison of individual cortical structures across subjects [11]. SBM has been used success-fully for analyzing and detecting neurological abnormalities in various neurological disorders such as schizophrenia [28], autism [25], and epilepsy [35, 17].
In this work we use five features to represent each vertex on the reconstructed surface: 1. Cortical thickness represents the thickness of the cor-2. Gray/white-matter contrast (GWC) represents the de-3. Sulcal depth characterizes the folded structure of the 4. Curvature is measured as 1 r , where r is the radius of an 5. Jacobian distortion measures the magnitude of the non-
The machine learning task is to develop a classifier that can distinguish between normal and abnormal cortical tissue using the patient X  X  MRI data. SBM has been used in con-junction with machine learning and statistical techniques to identify lesions in FCD patients. Besson et al. [4] use tex-ture, GWC and a number of morphological features includ-ing cortical thickness to represent each vertex on the recon-structed cortical surface. They then train a neural network to classify each vertex as being normal or lesional. Similarly, Thesen et al. [35] use a univariate z-score based threshold-ing approach on registered SBM data to classify each vertex as being lesional or normal. Recently, Hong et al. [17] devel-oped a two-stage Fisher linear discriminant analysis (LDA) [5] classifier to detect FCD lesions in MRI-negative patients. Initially they train a vertex-level classifier that classifies each vertex on the reconstructed cortical surface as being lesional or non-lesional for both controls and patients. These detec-tions are further refined using another LDA classifier that is trained to distinguish between actual FCD lesions (de-tections made inside the manually refined resection zones of patients) and false lesional detections made on controls.
One of the major confounding factors inhibiting the de-velopment of an effective classifier for detecting FCD lesions is inter-patient variability . The morphology of the human brain such as its thickness, curvature and the overall struc-ture in general are affected by different factors such as age, gender, handedness, etc. [30]. This causes a co-variate shift in data as the data from different patients is pooled to learn a common classifier. Similarly, the distribution of patholog-ical features that define an FCD lesion differs across FCD subtypes. For example in addition to causing other morpho-logical abnormalities, FCD type I lesions appear on MRI as abnormally thin regions of cortex, while FCD type II is char-acterized by abnormally thick regions. This heterogeneity of feature distributions that define the target concept (FCD le-sion) for the learner must be taken into account to develop an effective supervised lesion detection scheme.

To counter the effects of differences in feature distributions across FCD subtypes, Hong et al. deal only with FCD type-II [17], while Ahmed et al. [1], stratify the training data into thick and thin lesions based on manual inspection of data, while other schemes bypass this training bias by posing le-sion detection as an outlier/anomaly detection problem [35, 2]. However, none of the lesion detection schemes cited pre-viously explicitly address the co-variate shift arising from inter-patient variability. To overcome this co-variate shift in the underlying data distributions, we train an ensemble Figure 1: An RBM with D visible units and H of classifiers on a training set consisting of an equal number of patients from each of the three FCD subtypes, and con-trol data taken from fifty neurotypical controls. In order to discover subgroups in data, that align with meaningful com-binations of patient and FCD subtype characteristics we use a Dirichlet process based mixture of RBMs. Before describing our method in detail, we first review RBMs and DP mixture models.
A restricted Boltzmann machine (RBM, [32, 40]), illus-trated in Figure 1, is an undirected graphic model that consists of two layers: a visible layer x = { x 1 ,x 2 ,...,x which represents the attributes of the input data, and a hid-den layer h = { h 1 ,h 2 ,...,h H } . Units across different layers are fully connected. However, connections within the same layer are restricted. The goal of an RBM is to model the distribution of the observations x with the help of hidden units h .
 We define the energy of an RBM as: where W  X  R H  X  D represents the weights connecting hidden and visible units and b  X  R D , and c  X  R H are the offsets of the visible and hidden layers respectively. (1) directly leads to the following formulation for the probability density of x : where F ( x ) is often referred to as the free energy. H is the total number of hidden units and Z = P function. It can be shown that the conditional probability of h i  X  X  given x is independent, i.e., and furthermore, p ( h j = 1 | x ) = 1 where  X  (  X  ) is the sigmoid function. Similarly, we have p ( x k = 1 | h ) = 1 Equations (4) -(7) allow us to make approximations to the inference algorithm described next.
Given the training data, the learning objective of an RBM is to adjust parameters { W , b , c } such that the free energy defined in (3) is minimized, which is equivalent to maximiz-ing p ( x ). A standard approach is to use stochastic gradi-ent descent to minimize the average negative log-likelihood
P with respect to each parameter and set them to zero to ob-tain a set of update rules. A learning algorithm iteratively updates the parameters until convergence. Specifically, where  X   X  { c , b , W } . The computational difficulty in (8) is the second term which is an expectation over an expo-nential number of configurations of the input x under p ( x ). Hinton introduced the Contrastive Divergence (CD) learn-ing algorithm [15] which makes this computation tractable by estimating the expectation using a sample,  X x from the model. This sample is obtained using k -steps of Gibbs sam-pling. Thus, (8) can be simplified to:
For RBMs, because the hidden and visible units are con-ditionally independent (Equation (4), (6)), we can perform block Gibbs sampling, i.e., we sample all hidden (visible) units simultaneously given the values of the visible (hidden) units. In particular, starting with a given visible observation x Consequently, the Contrastive Divergence learning algorithm with k steps of Gibbs sampling (CD-k ) can be summarized as follows: 1. For each training example x ( t ) 2. Go to Step 1 until the stopping criterion is met.
We set k = 1 in our model because it has been shown empirically that even when k is not large (e.g., k = 1), the CD-k algorithms often gives good results [9].
Dirichlet process is a family of Bayesian nonparametric models in which the model representations grow as more data are observed [39, 38, 26]. In particular, DP used as a prior in a generative mixture model allows the number of mixing components to adapt to the individual dataset automatically. DP can be interpreted as an extension to the traditional generative model with an arbitrary (infinite) number of mixing components.

Formally, a Dirichlet process is an infinite dimensional discrete distribution with two parameters  X  and H denoted as: where H is the base distribution and scalar  X  is the strength parameter. H serves as the mean of G and  X  controls the convergence of G towards H . A Dirichlet process can be constructed using the stick-breaking process [31] as follows: where k = 1 , 2 ,.... and  X  is the Dirac delta function. A DP mixture model uses G (  X ,H ) as the prior under the Bayesian framework. The entire dataset is modeled as a mix-ture of components and each component is parameterized by a random draw (  X  ) from G . Each data observation belongs to one of the components and is modeled as a function of the parameter of its component, i.e., f i (  X  i ). Specifically, Consider drawing N samples of  X  i ( i = 1, 2, . . . , N ) from G . Because G is a discrete distribution, the probability at any given point in the probability space can be non-zero. This implies that the values of the  X  i  X  X  will repeat with a positive probability. Hence, these  X  i  X  X  exhibit clustering behavior (Polya Urn Scheme)[33]. Given the first N samples of  X  from G , we assume they have produced a set of k distinct values: It can be shown that the next new sample  X  N +1 can be either a new value drawn from base distribution H with probability  X   X  or can be taken from one of the existing members from  X   X  with probability  X  c i , where c i is the number of times  X  has been repeated. Specifically, where  X   X  i denotes the distribution concentrated at a single point  X  i .
 Equation (11) illustrates two important properties of a DP. First, the concentration parameter  X  controls the num-ber of distinct values of  X  i  X  X , i.e., the number of mixing com-ponents. Second, DP exhibits a  X  X ich get richer X  property: the more frequently a  X   X  i has been adopted (i.e., the larger the c i ), the more likely it will be chosen again as the next  X  value.
 There are various techniques such as Markov chain Monte Carlo (MCMC) sampling[24] and variational inference[7] to conduct inference for a non-parametric model under the Bayesian framework. The variational approach can be more advantageous due to its scalability and guaranteed conver-gence. In this paper, we adopt the mean-field variational approach outlined in [7].
In this section, we describe how to combine the RBM and DPM algorithms for our classification task, including a modified weighted RBM training algorithm. The inference of a typical mixture (finite or infinite) of RBMs is intractable due to the difficulty of estimating the partition function Z in density function p ( x ) defined in Equa-tion (2). We propose to take a two-stage approach. We first employ a DPM model to partition the data into k clusters. We then apply a modified version of RBM training algo-rithm to each non-empty cluster using the weights obtained from the DPM model.

For stage one, we note that it is the top layer hidden units (i.e., the non-linear features extracted by the RBM) that serve as the input to our classifier. Thus, directly clustering in the input x may not be effective for a task for which the inputs are the h  X  X . For example, in image processing, we can interpret the h  X  X  as a projection of x  X  X  on some other di-mension. In Figure 2, the three clusters in the original input space are no longer valid when projected to either of the two dimensions. Input data, therefore, should be partitioned ac-cording to the characteristics of the projections (i.e., the top layer hidden units) rather than the original observations.
We propose applying an infinite mixture model to cluster the data at the top layer of the neural network, i.e., the h  X  X  in an RBM, and then learn k classifiers for those partitions produced by the clustering algorithm. To predict the label for a new instance, we will feed the instance to each classifier and take a majority vote from their predictions. Our RBM-DPM model can be outlined as follows: INPUT: Instances x = { x 1 , x 2 ,..., x n }
OUTPUT: Classifiers R 1 ,R 2 ,...,R k on input
In this section, we give the formulation of the DP mix-ture model of the hidden units h = { h 1 , h 2 ,..., h corresponds to Step 3 of the RBM-DPM model. We as-sume instances in the k th cluster follow a multivariate Gaus-sian distribution N (  X  k ,  X  k ). Our model employs the latent variables z = { z 1 ,z 2 ,... } ( z i  X  X  are the rows of the indi-cator matrix z mentioned previously), v = { v 1 ,v 2 ,..., } ( v  X  X  are the stick-breaking proportions [31] in a DP) and  X  = {  X  1 , X  2 ,...,  X  1 ,  X  2 ,... } . Hyper-parameters are  X  ,  X  and  X  0 .

The joint distribution p ( h , z , v ,  X  | x ) can be factored as follows: where
This is a standard DPM of Gaussian distributions, for which we use the variational inference method [6, 7] to es-timate the parameters. The inference algorithm iteratively computes the values of latent variables until convergence. In particular, the mixture model produces a soft mixture of h  X  X  as follows: where each column is a cluster and P 1 , 2 ,...,n .
 We next proceed to the second stage, i.e., Step 4 of the RBM-DPM model, and train a weighted RBM (described below) for each non-empty cluster. The weights for cluster k are defined by the z i,k  X  X  ( i = 1 , 2 ,...,n ).
The mixing proportions for the i th cluster are: Because the h i  X  X  are obtained from the corresponding x i we modify the energy function using weighted x i  X  X : F ( x ) = c T D x + where D = Consequently, we can modify the CD-k learning algorithm as follows: Table 1: Epilepsy Type and Data Instances Patient Type Positive Instances Negative Instances NY143 2 629 629 * 50 NY148 1 6,569 6,569 * 50 NY149 2 7,035 7,035 * 50 NY159 1 5,111 5,111 * 50 NY186 3 5,869 5,869 * 50 NY294 3 14,107 14,107 * 50 NY226 1 6,485 6,485 * 50 NY255 2 14,394 14,394 * 50 NY259 1 6,792 6,792 * 50 NY315 2 3,434 3,434 * 50 NY338 3 9,046 9,064 * 50 NY343 3 10,463 10,463 * 50 NY351 1 3,522 3,522 * 50 NY371 2 6,915 6,915 * 50 NY394 2 14,197 14,197 * 50 NY46 1 18,972 18,972 * 50 NY67 3 14,932 14,932 * 50 NY72 3 15,448 15,448 * 50 Total 163,920 8,196,000
In this section we describe in detail the construction of our dataset, the machine learning techniques used to address domain-specific issues related to our task, and the perfor-mance evaluation of our proposed RBM-DPM model.
As described in Section 1, we had access to the MRI data of MRI-negative patients from NYU X  X  Comprehensive Patient NY226 Y 96 . 3 Y 99 . 0 Y 99 . 6 Y 98 . 7 N 98 . 3 Y 99 . 9 NY255 Y 96 . 8 Y 97 . 5 Y 99 . 6 Y 98 . 6 Y 99 . 3 Y 99 . 9 NY259 N 96 . 3 Y 98 . 3 Y 99 . 7 N 99 . 4 N 97 . 6 Y 99 . 9 NY315 N 97 . 7 Y 97 . 9 N 99 . 6 N 98 . 5 N 99 . 0 N 99 . 9 NY338 Y 98 . 5 Y 97 . 6 Y 99 . 4 N 99 . 8 Y 99 . 3 Y 99 . 8 NY343 Y 97 . 1 Y 97 . 6 Y 99 . 6 Y 99 . 1 Y 98 . 4 Y 99 . 9 NY351 N 97 . 9 N 97 . 2 N 99 . 7 N 98 . 3 N 98 . 3 N 99 . 9 NY371 Y 97 . 5 Y 97 . 2 N 99 . 6 Y 99 . 5 Y 98 . 1 N 99 . 9 NY394 N 98 . 1 N 97 . 5 N 99 . 7 N 99 . 6 N 98 . 1 N 99 . 9 NY46 Y 97 . 4 Y 98 . 7 Y 99 . 7 Y 99 . 2 Y 97 . 9 Y 99 . 9 NY67 Y 97 . 3 Y 98 . 6 Y 99 . 6 Y 99 . 3 Y 99 . 3 Y 99 . 8 NY72 N 98 . 1 N 99 . 8 N 99 . 8 N 99 . 7 N 98 . 4 N 99 . 9
Mean 58% 97 . 4 75% 98 . 1 58% 99 . 6 50% 99 . 1 50% 98 . 6 58% 99 . 9 Epilepsy Center who underwent resective surgery and were completely seizure free after surgery. Furthermore, their re-sected cortical tissue was histopathologically verified to con-tain FCD. These patients are further categorized into three subtypes (type I, II and III) [8]. Six representative patients were selected from each subtype group resulting in a total of eighteen patients available to our research. This might seem like a small collection of patients, however it should be noted that only a few MRI-negative patients proceed to surgical resection and out of these few only a third achieve complete post-surgical seizure freedom. Indeed, the six type II patients in our collection represent the entire population of FCD type-II MRI-negative patients treated at NYU during the past three years .

In terms of the actual data instances, each patient con-tributes a different number of positive instances to our dataset, depending on the size of his/her resection area. All the vertices within the resected region are labeled as positive (lesional, label = 1). The negative (non-lesional, label = 0) instances for our dataset are extracted from the MRI scans of fifty neurotypical healthy controls who underwent the same MRI protocol. In particular, for each patient, we extract data from each healthy image from the same loca-tion as the patient X  X  resection region. As a result, if a patient contributes n lesional samples to our dataset, we will have fifty corresponding sets of non-lesional samples with n in-stances in each set (i.e., a total of 50  X  n instances). Table 1 presents the subgroup type, total number of positive and negative instances associated with each patient. We have a total of 163,920 and 8,196,000 positive and negative in-stances respectively. Note that the negative instances in our dataset are taken from the healthy controls instead of from non-lesional areas outside the patient resection zones. This approach encourages our model to learn a more accurate representation of normal human brains, which is essential to our task of abnormality detection.
The majority of the FCD patients in our dataset have temporal lobe resections, which is the most prevalent local-ization of FCD in adults [19]. Training on all patients would therefore be biased toward a specific cortical region limit-ing its generalization to differentiate between lesional and non-lesional vertices in other cortical regions. Training on all patients (and performing evaluation via leave-one-patient out cross validation) would result in low accuracy in regions other than the temporal lobe. Furthermore, it is desirable to have a balanced training set of patients from the three differ-ent FCD subtypes to give a good distribution over different FCD lesion types. Under these two constraints, we selected two patients from each FCD subtype (i.e., six patients in to-tal) as our training patients such that their resected regions optimize coverage of the cortex beyond the temporal lobe 1
There is a fifty to one ratio between negative and positive instances in our data. In order to overcome this class im-balance, we apply bagging with under-sampling [36]. Each bag includes all positive instances from our training patients (i.e., all minority instances). At the same time, we ran-domly pick one control for each patient and include all the corresponding negative instances from the selected control.
In experiments, not reported in this paper due to space, we later verified our assumptions that training on a distribution skewed toward lesions in the temporal lobe did indeed lead to lower performance than a more balanced dataset. We omit these results due to space limitations. the white outlined region represents the actual resected region. We repeat this process fifty times and create fifty balanced training sets. We learn a classifier for each training set re-sulting in fifty independent classifiers. A standard ensemble method in machine learning will perform a majority vote among all fifty classifiers when making a prediction. In our case, however, we are biased towards the learners that boast high TNRs on the training data. This is because, although we are aiming at a high lesion detection rate, our detection is only meaningful if we don X  X  sacrifice the performance on non-lesional areas. To address this preference issue specific to our task, we establish an Ensemble Threshold and dis-card classifiers whose TNRs fall below this threshold on the training data . The goal is to weed out classifiers that fail to capture the characteristics of healthy cortical tissue.
We have twelve test patients (excluding the six training patients) and we present our model X  X  performance on each test patient X  X  entire brain i.e., vertices from both inside and outside the resected region.We measure the performance on the lesional and non-lesional instances separately to ensure that we are not only detecting the lesions, but also are not misclassifying normal cortical vertices. As discussed in Sec-tion 1, we use an indication flag to signal a successful detec-tion within the resection zone and a TNR to measure our performance on non-lesional instances. Note that the be-nign instances of the patients are not part of our dataset constructed in Section 6.1. Thus, a good performance on these instances (i.e., a high TNR) demonstrates the efficacy of our model in recognizing new healthy brain structures.
We compare the performance of our RBM-DPM model to two baseline models. The first is our recently reported logistic regression (LR) based approach [1] which deals ex-clusively with MRI-negative patients. This model has been in use at NYU X  X  Comprehensive Epilepsy Center since 2013 and entails a number of pre-processing steps which include manual reduction of the resected region, stratifying the data based on sulcal depth, and a post-processing step that man-ually discards all detected clusters that fall below a surface area of 50 mm 2 . The performance of this model reported in Table 2 which includes all the pre-processing and post-processing steps, and thus represents this method X  X  best per-formance.

For the second baseline model, we choose to train an RBM over the entire dataset without applying the DPM clustering algorithm to the data. The purpose of the comparison is to verify our conjecture that DPM is able to capture the interpersonal variations arising from different morphologies among the patients.
Table 2 presents the main results of our model on the twelve test patients. We experimented with two thresholds (90 and 95, indicated by the first row) for retaining the clas-sifiers (see details in Section 6.3). Lower thresholds are not interesting because they lead to lower TNR. For each thresh-old value, we compare the performance of our model to the two baseline models. The third row shows the number of classifiers retained in each case. Under columns  X  X etected X , a value  X  X  X  indicates a successful detection. The correspond-ing entry in the last row shows the successful detection rate out of the twelve patients in each model.

We have developed our approach keeping in mind its final use as a focus of attention mechanism for neuroradiologists. The detections made using our model would be used to in-form effective electrode placement in iEEG, and constitute a source of secondary evidence for determining the resection target with an ultimate goal of reducing the resection region which leads to a safer and effective surgery. Thus, there are two performance metrics that need to be analyzed: i) performance on the non-lesional regions (i.e., TNR) and ii) detection rate on the lesional regions. Figures 3(a) and 3(b) plot the detection results of RBM-DPM model on a test patient for the two different threshold values using an inflated model of their cortical surface. The yellow areas are detected lesional regions from the model and the white outlined region indicates the actual resected area. Thus, yellow clusters within the resected zone are correct de-tections, while those outside the zone are false positives. As explained in Section 1, the resection zones are determined in a  X  X enerous X  manner for MRI-negative patients to maximize the chances of a seizure free outcome. Indeed, we observe in Figures 3(a) and 3(b) that the detected lesional regions are considerably smaller than the size of the actual resec-tion. Thus, the results of our model can be used to guide electrode placement during a patient X  X  iEEG evaluation and obtain a potentially refined resection target, which leads to reduced chances of removing healthy cortical tissue.
In this paper, we proposed a non-parametric approach to detect MRI-elusive epilepsy lesions using restricted Boltz-mann machines (RBMs). In particular, we transform 3D-MRI images of human brains into a standard 2D surface using the Surface-Based Morphometry methodology and ex-tract five features that characterize human cortical surfaces. Our model addresses both issues of limited available features and inter-patient variabilities in the input data. For the for-mer, we used an RBM as a pre-training step. For the latter, we applied a Dirichlet process based clustering algorithm and estimated its parameters via variational inference. To accomplish our classification task, we collect multiple clas-sifiers by training an augmented RBM for each non-empty component from the clustering algorithm and take a ma-jority vote among all classifiers while making a prediction. We evaluated our model on brain images of twelve MRI-negative patients Our model correctly detected abnormal regions within the resected areas in 58% of the patients, with 99.9% accuracy of correctly classifying the non-lesional vertices. Based on these findings, we are evaluating a re-placement of our current LR model with our new approach in the clinical treatment for epilepsy patients at NYU X  X  Com-prehensive Epilepsy Center.
This work is partially supported by NSF IIS-1546428. We would like to thank Hugh Wang at the neurocognitive labo-ratory, New York University for his help with mapping the resection masks.
Additional authors: Ruben Kuzniekcy (New York Univer-sity, email: ruben.kuzniecky@nyumc.edu ) and Orrin Devin-sky (New York University, email: od4@med.nyu.edu ). [1] B. Ahmed, C. E. Brodley, K. E. Blackmon, [2] B. Ahmed, T. Thesen, K. Blackmon, Y. Zhao, [3] A. Bernasconi, N. Bernasconi, B. Bernhardt, and [4] P. Besson, N. Bernasconi, O. Colliot, et al.
 [5] C. M. Bishop. Pattern Recognition and Machine [6] D. M. Blei and M. I. Jordan. Variational methods for [7] D. M. Blei and M. I. Jordan. Variational inference for [8] I. Blumcke, M. Thom, E. Aronica, et al. The [9] M. A. Carreira-Perpinan and G. Hinton. On [10] A. Dale, B. Fischl, and M. Sereno. Cortical [11] B. Fischl and A. Dale. Measuring the thickness of the [12] B. Fischl, M. Sereno, and A. Dale. Cortical [13] C. D. Good, J. Ashburner, and R. Frackowiak.
 [14] W. A. Hauser and D. C. Hesdorffer. Epilepsy: [15] G. E. Hinton. Training products of experts by [16] G. E. Hinton. Boltzmann machine. Scholarpedia , [17] S. J. Hong, H. Kim, D. Schrader, N. Bernasconi, B. C. [18] D. Killer and N. Friedman. Probabilistic graphical [19] P. Krsek, B. Maton, B. Korman, E. Pacheco-Jacome, [20] R. I. Kuzniecky and A. Barkovich. Malformations of [21] P. Kwan and M. J. Brodie. Early identification of [22] J. T. Lerner et al. Assessment and surgical outcomes [23] V. Nair and G. E. Hinton. Implicit mixtures of [24] R. M. Neal. Markov chain sampling methods for [25] C. Nordahl, D. Dierker, I. Mostafavi, et al. Cortical [26] J. Paisley, C. Wang, D. Blei, and M. I. Jordan. A [27] R. P. R, B. Fischl, V. C. V, N. Makris, and P. E. [28] L. Rimol, R. Nesv  X  A X eg, D. Hagler Jr., et al. Cortical [29] D. Rumelhart, G. Hinton, and R. Williams. Learning [30] D. H. Salat, R. L. Buckner, A. Snyder, et al. Thinning [31] J. Sethuraman. A constructive definition of dirichlet [32] P. Smolensky. Information processing in dynamical [33] Y. W. Teh. Dirichlet process. Encyclopedia of machine [34] J. F. Tellez-Zenteno, R. Dhar, and S. Wiebe.
 [35] T. Thesen, B. Quinn, C. Carlson, et al. Detection of [36] B. C. Wallace, K. Small, C. E. Brodley, and T. A. [37] Z. I. Wang, A. V. Alexopoulos, S. E. Jones, Z. Jaisani, [38] E. P. Xing, M. I. Jordan, and R. Sharan. Bayesian [39] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. [40] F. Y. and D. Haussler. Unsupervised learning of
