 Salah Rifai (1) rifaisal@iro.umontreal.ca Yoshua Bengio (1) bengioy@iro.umontreal.ca Yann N. Dauphin (1) dauphiya@iro.montreal.ca Pascal Vincent (1) vincentp@iro.umontreal.ca A central objective of machine learning is to gener-alize from training examples to new configurations of the observed variables, and in the most general setup this means deciding how to redistribute the probabil-ity mass associated with each training example from the empirical distribution. Classical non-parametric density estimation does this by convolving the empir-ical distribution with a smoothing kernel such as the Gaussian (giving rise to the Parzen density estimator). This spreads each point mass into the nearby volume surrounding each training point, but unfortunately it does so isotropically (in the same way in all directions around each training point). This means that if the true density tends to concentrate near low-dimensional manifolds (this is called the manifold hypothesis (Cay-ton, 2005; Narayanan and Mitter, 2010; Rifai et al., 2011a)), then a lot of densely packed training exam-ples will be required to achieve a high model density near the manifold and a low model density away from it.
 Whereas Principal Components Analysis (PCA) dis-covers a linear manifold near which the density may concentrate, Non-Linear Manifold Learning algo-rithms (Sch  X olkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000) attempt to discover the struc-ture of manifolds (which may be non-linear) near which the true density concentrates. In some cases a mani-fold learning algorithm can be generalized to obtain a model that tells us (explicitly or implicitly) how to al-locate probability mass everywhere, and not just iden-tify the manifold. For example, PCA corresponds to a Gaussian model, where the variances in directions orthogonal to the manifold are small and constant (across all these directions), and this can be extended to non-linear structures with a mixture model (Tip-ping and Bishop, 1999) or the kernel trick (Sch  X olkopf et al., 1998).
 In many manifold learning algorithms, the local shape of the manifold is specified by a local basis which in-dicates the plausible directions of variation, i.e., the tangent plane at any point on the manifold. Different algorithms propose different ways of stitching these lo-cal planes or local pancakes in order to construct a global manifold structure or a global density. How-ever, a potentially serious limitation of most manifold learning algorithms is that they are based on local gen-eralization : they infer these local tangent planes based mostly on the training examples in the neighborhood of the point of interest. As discussed in Bengio and Mon-perrus (2005) and Bengio et al. (2006b), this raises a curse of dimensionality issue: if the manifold of in-terest has many ups and downs then the number of examples required to capture the manifold increases exponentially with manifold dimension and the man-ifold curvature. The model cannot generalize in an up or down of the manifold for which there are no examples to map out its variations. A first non-linear manifold learning algorithm with non-local generaliza-tion was proposed by Bengio et al. (2006a), stimulat-ing the work presented here, which also follows up on more recent work in the area of Deep Learning (Hinton et al., 2006; Bengio, 2009), more specifically the Con-tractive Auto-Encoder (CAE) (Rifai et al., 2011b;a), described in more detail in the next section. The CAE is a representation-learning algorithm which also cap-tures local manifold structure and has the potential for non-local generalization.
 The first main contribution of this paper (section 3) is a novel way to use the CAE to construct a generative procedure which implicitly specifies a density concen-trating near the captured manifolds. The second main contribution of this paper (section 4) is inspired by the ideas of the first and consists in a novel way to train a pooling layer on top of the features learned by a CAE, so as to make them invariant to the local di-rections of variation captured by the lower-level CAE features. Another contribution of this paper regards the training of a multi-layer CAE, in particular us-ing the above trained pooling layer, whereas previous CAE papers involved only single-layer CAEs, possibly stacked to get deeper representations. Section 5 shows results from the proposed generative process and from the invariance-seeking pooling algorithm for the CAE, illustrating the advantages brought by these two con-tributions. Deep Learning algorithms learn a representation of input data x , which is typically used either to con-struct a classifier 1 or in order to capture the structure of P ( x ). They are deep if these representations have multiple levels, and the number of levels is a hyper-parameter that can be chosen in a data-dependent way. Typically, higher levels are defined in terms of lower levels and are expected to represent more ab-stract features, e.g., better capturing structure present in the input distribution such as invariance (Goodfel-low et al., 2009) or manifold structure. See Bengio (2009) for a review of Deep Learning algorithms. A major breakthrough in this area occurred in 2006 (Hin-ton et al., 2006) with the successful idea that deep architectures could be trained by stacking single-level unsupervised representation learning algorithms such as the Restricted Boltzmann Machine (RBM) or auto-encoder and sparse coding variants.
 The Contractive Auto-Encoder (CAE) is an unsuper-vised feature learning algorithm that has been success-fully applied in the training of deep networks, i.e., CAE layers can be stacked to form deeper representations. Its parametrization is essentially the same as that of a Restricted Boltzmann Machine, but contrary to RBMs its training procedure is deterministic, and consists in minimizing, through gradient descent, a simple objec-tive that can be efficiently computed analytically and exactly.
 In this section, we briefly review the CAE algorithm, its interpretation as modeling a data manifold, and how one can use a trained CAE to extract the local tangent space to that manifold at a point. 2.1. The Contractive Auto-Encoder training Let us briefly introduce the notation that we will be using throughout, and formalize the operations of the CAE algorithm, following Rifai et al. (2011b). From an input x  X  [0 , 1] d , a k -dimensional feature vec-tor is computed as a hidden layer, e.g., where W  X  IR k  X  d and b h  X  IR k are parameters of the model, and s is the element-wise logistic sigmoid s ( z ) = 1 1+ e  X  z . From hidden representation h , a recon-struction of x is obtained as where b r  X  IR d is the reconstruction bias vector. Al-though the encoder f (  X  ) and decoder g (  X  ) can them-selves have multiple layers, previous work on the CAE has focused on training only single-layer CAEs (and then stacking them to form deeper representations). A reconstruction loss L ( x,r ) measures how well the input is reconstructed from the hidden representation. Following Rifai et al. (2011b), we will be using a cross-entropy loss: The set of parameters of this model is  X  = { W,b h ,b r } . The training objective being minimized in a traditional auto-encoder is simply the average reconstruction er-ror over a training set D . The Contractive Auto-Encoder adds a regularization term to this objective, that penalizes the sensitivity of the features to the in-put, measured as the Frobenius norm of the Jacobian In summary, the parameters  X  of the CAE are learned by minimizing:
J CAE (  X  ; D ) = X where  X  is a non-negative regularization hyper-parameter that controls how strongly the norm of the Jacobian is penalized. 2.2. How the CAE models a data manifold A useful non-parametric modeling hypothesis for high dimensional data with a complicated structure, such as natural images or sounds, is the so-called mani-fold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010; Rifai et al., 2011a). It hypothesizes that, while in its raw representation such data may appear to live in a high dimensional space, in reality its probability density is likely to be relatively high only along stripes of a much lower-dimensional non-linear sub-manifold, embedded in this high-dimensional Euclidean space 2 . This should not be taken to mean that all data must lie strictly on said sub-manifold (that we will from now on simply call manifold), but merely that the probabil-ity density is expected to decrease very rapidly when moving away from it. Since this manifold is thought to support the high data density regions, capturing and exploiting its precise structure and location within the high dimensional space is viewed as a key to better generalization.
 To interpret the CAE from this perspective, as is done in Rifai et al. (2011b), it is useful to think of an au-toencoder as projecting an input point x onto a low di-mensional manifold, and of the hidden representation as the coordinates of that projection in a coordinate system within the manifold. One may view this as a non-linear generalization of PCA, where PCA would correspond to projecting onto a linear sub-manifold. Let us consider a point x and the manifold-coordinates of its projection (i.e. its hidden representation h ( x )), and how they change as we slightly move x . For move-ments  X  X arallel X  to the manifold (i.e. in directions within the tangent space at the projection), the pro-jection and thus h will follow suit. Ideally, small move-ments  X  X rthogonal X  to the manifold however should not change the projection or h .
 Now CAEs X  contractive penalty pressures the hidden representation not to change when moving x , whatever the direction (it is an isotropic pressure). But this is counterbalanced in the CAE training criterion by the need to correctly reconstruct training points, so that h has to be sensitive at least to moves in directions that lead to other likely points (such as training set neighbors). As a result, of this trade-off, the CAE X  X  hidden representation will learn, at training points, to be sensitive only to movements alongside the manifold of high density (i.e. spanning the tangent space to the manifold), because these can yield points it must be able to reconstruct well, while sensitivity to all other directions (those orthogonal to the manifold) will have been shrunk.
 This interpretation is supported in Rifai et al. (2011b) by an empirical analysis of the sensitivity of f ( x ) to input directions: a Singular Value Decomposition of Jacobian J ( x ) reveals that the singular values spec-trum has a rapidly decreasing shape, with but a few large singular values. This shows that, indeed, the hid-den representation learned to be sensitive to changes in only a few input-space directions, which is coherent with the hypothesis of the data concentrating along a low-dimensional manifold.
 Note that while J ( x ) contains all information to com-pute the sensitivity of h = f ( x ) to movements in any input direction, performing a SVD yields a more di-rectly informative orthonormal basis of directions, or-dered from most to least sensitive. The subset of most sensitive directions in this basis can be interpreted as spanning the tangent space to the manifold at point x .
 Rifai et al. (2011a) have successfully used these ex-tracted  X  X angent X  directions in a subsequent super-vised classification training to encourage class predic-tion probabilities to be invariant to these directions, by using the tangent-propagation technique (Simard et al., 1992).
 Consider a pretrained CAE with hidden units h i = f ( x ), e.g., computed according to Eq. 1. Consider a local variation  X  x of x obtained by moving infinitesi-mally along the tangent plane defined by the CAE at x , i.e., with greater movement in the directions of the Jacobian J associated with larger singular values: where i is a Gaussian perturbation in h -space with small variance  X  2 , and the vector is isotropic with  X  N (0 , X  2 I k ).
 We can now calculate the hidden units representation associated with the perturbed sample  X  x : By using a first order Taylor expansion, taking ad-vantage of the assumption that  X  is small and thus is small, we obtain a movement in the space of h as follows: f j x + Hence, to first order, when we move x along the direc-tions captured by the CAE X  X  Jacobian, it corresponds to a movement from h = f ( x ) to h + JJ T , where is a small isotropic perturbation. This idea is exploited in Algorithm 1 below.
 Algorithm 1 : Sampling from a CAE.
 Let inputs x  X  [0 , 1] d and representations h  X  [0 , 1] k with encoding function f : [0 , 1] d  X  [0 , 1] k from input space to representation space, and decoding function g : [0 , 1] k  X  [0 , 1] d from representation space back to input space.
 Input: f , g , step size  X  and chain length T Output: Sequence ( x 1 ,h 1 ) , ( x 2 ,h 2 ) ,..., ( x T ,h
Initialize x 0 arbitrarily and h 0 = f ( x 0 ). for t = 1 to T do end for If the singular values of J had a sharp drop-off, only movement on the manifold would be allowed. Further-more, if the singular value spectrum was constant by parts (some large constant value for the first m sin-gular values, and 0 for the remaining ones), and in the limit where  X   X  0, then we claim that Algorithm 1 would do a random walk on the manifold. What if some singular values are non-zero or if  X  &gt; 0? We would take some steps  X  X ff X  the manifold. However the reconstruction step (last step) of the algorithm would always bring us back towards the manifold (since g is trained to output values on a set of high-density points, i.e., the training examples). We show below that in fact Algorithm 1 defines a stochastic process that asymptotically generates examples according to a well defined distribution. Note that the CAE used to compute J does not have to be a single-layer CAE (as was the case in earlier CAE papers), and below we show experimental results with 2-layer CAEs that im-prove on single-layer CAEs. In that case, J represents the Jacobian of the top-level layer with respect to the model X  X  input.
 Theorem : Algorithm 1 defines a stochastic process generating a sequence h 1 ,h 2 ,... that is an ergodic Har-ris chain with a stationary distribution  X  , so long as J J T t is full rank.
 Proof : Note first that  X  h is a zero-mean Gaussian with full rank covariance matrix E [ X  h  X  h T ] = E Since  X  h can be anywhere in R k , so can h t  X  1 +  X  h . Let H = { f ( g ( h )) : h  X  R k } the domain reachable in representation space when starting from anywhere in representation space and applying the decoder and en-coder. This is the set in which all h t belong in Algo-rithm 1. Hence, by definition of H , for any h t  X  H , there exists h  X  R k such that h t = f ( g ( h )), and since there exists  X  &gt; 0 such that  X  h can take any value with probability density p ( X  h ) &gt;  X  so long as  X  h is in a bounded sphere around the origin, there exists h t  X  1  X  X  such that p ( h t | h t  X  1 ) &gt;  X  . The sequence of h t  X  X  defines a time-homogeneous Har-ris chain (a Markov chain with uncountable state) be-cause it puts probability mass everywhere in H , and it has a stochastic kernel K s.t. K ( u,v ) = p ( h t +1 = v | h and K ( u,V ) = P ( h t +1  X  V | h t = u ) for a set V . In order for it to have a stationary distribution  X  it is enough to show that it is irreducible, aperiodic and recurrent, all being true in virtue of the fact that, be-cause H is a compact set,  X   X  &gt; 0 such that we can go from any element in H to any element in H in just one time step with probability greater than  X  : we can go from any region to any region in finite time (ir-reducibile chain), we can return to a just visited re-gion (recurrent chain), and we can return to it in any number of time steps (aperiodic chain). These are the sufficient conditions for a stationary distribution  X  to exist, i.e., such that  X  ( h 0 ) = R K ( h 0 ,h )  X  ( h ) dh . Since the sequence of h t converges in distribution, and x t = g ( h t  X  1 +  X  h ) it can be similarly shown that the sequence of x t  X  X  also converges in distribution, i.e., that Algorithm 1 defines a Markov chain with a stationary distribution in the input space. Previous work (Rifai et al., 2011a) has shown that the CAE X  X  Jacobian characterizes a low dimensional tan-gent space that e.g., for images might correspond to small local deformations of the image such as transla-tions. The CAE is able to learn these automatically instead of having to provide them as prior knowledge. Note that from the point of view of classification, the leading tangent space directions are often conceived as representing directions to which higher level features should be mostly invariant (assuming that each class is associated with a separate manifold, this is the  X  X ani-fold hypothesis for classification X  (Rifai et al., 2011a)). Now if we follow the standard procedure for greedy layer-wise pre-training of deep networks, and stack a second similar CAE layer on top of the features learned by a first CAE layer, it is unlikely that this second CAE layer will learn to be highly invariant to these directions, to which the first layer is, on the contrary, very sensitive, as it would harm its reconstruction er-ror. This behavior is to be contrasted with a very successful element of many deep architectures (espe-cially for machine vision tasks), which is the addi-tion of pooling layers in convolutional networks (LeCun et al., 1989). This powerful approach builds on prior knowledge of the 2D topology of images to pool the responses of several nearby receptive fields and yield units that are invariant, e.g., to small local transla-tions of the input image. Since CAEs appear able to capture relevant local tangent directions without using prior domain knowledge, we wanted to find a way to automatically learn units that are, similarly, invariant to the corresponding deformations. This corresponds to learning a form of pooling (see also Coates and Ng (2011) for recent work on learning to pool). Hence we propose a slight addition to the training criterion of the second layer CAE to achieve this goal.
 The normal CAE criterion for learning the second layer would be J where f 0 and g 0 are defined similarly to f and g of the first layer, but using a different set of parameters  X  To encourage representation h 0 = f 0 ( h ) = f 0 ( f ( x )) to be most invariant to the directions captured by the first layer X  X  J , we add a further term to the objective: J where expectation E is over  X  X  (0 , X  2 I k ), and  X  p  X  0 is a hyper-parameter. Note that  X  h = J ( x ) J ( x ) T the same as was used in our CAE sampling procedure, which served as inspiration for this novel criterion. In practice, we will use a stochastic version of this crite-rion, where we sample a single each time we consider a different x .
 Naturally this addition will likely worsen the achieved reconstruction error. But it is to be expected that as more abstract and invariant higher layers are learned, reconstruction from their representation alone will be further from the exact input.
 The intended effect of this criterion is similar to Rifai et al. (2011a) X  X  motivation for using tangent-propagation with extracted  X  X angent X  directions to yield class prediction probabilities that are invariant to these directions. There are however two major dif-ferences. First the criterion we propose is not tied to a supervised classification task, as it is local to the CAE+ layer. It can be viewed instead as a fully unsu-pervised learning of a form of pooling. Second, it does not require performing a SVD for each data point, and is thus much more efficient computationally. In this section, we will first evaluate empirically the sampling procedure defined in Section 3 for a trained CAE, by comparing it to samples generated by a trained Restricted Boltzmann Machines (RBM). Sec-ond we will show empirical results supporting the ef-fectiveness of learning more invariant units with the criterion explained in section 4 in terms of classifi-cation performance. All our experiments were con-ducted on the MNIST, Caltech 101 Silhouettes, and the Toronto Face Database (TFD) . The latter is the largest dataset used for facial expression recogni-tion. The dataset is composed of 100 , 000 unlabeled images 48x48 pixels which makes it particularly in-teresting in the context of unsupervised learning al-gorithms, and 4000 labeled images with 7 facial ex-pressions. We use the same preprocessing pipeline de-scribed in Ranzato et al. (2011). 5.1. Evaluating sample generation We used the sampling procedure proposed in Section 3 to generate samples from two layer stacks of ordi-nary CAE (denoted CAE-2), that were trained on the MNIST and TFD datasets. To verify the importance of basing the stochastic perturbation of the hidden units on the CAE X  X  Jacobian, we also run an alter-native technique where we instead add isotropic noise. For comparison we also generated samples with Gibbs samling from a 2-layer Deep Belief Network denoted DBN-2 (i.e. stacking two RBMs). For the first RBM layer we used binary visible units for MNIST, and Gaussian visible units for TFD. Hidden units were binary in both cases. Figure 1 shows the generated samples for qualitative visual comparison. Figure 3 shows the evolution of the reconstruction error term, as we sample using either Jacobian-based or isotropic hidden unit perturbation. We see that the proposed procedure is able to produce very diverse samples of good quality from a trained CAE-2 and that properly taking into account the Jacobian is critical. Figure 2 shows typical first layer weights (filters) of the CAE-2 used to generate faces in Figure 1.
 To get a more objective quantitative measure of the quality of the samples, we resort to a procedure pro-posed in Breuleux et al. (2011) that can be applied to compare arbitrary sample generators. It consists in measuring the log-likelihood of a test set (not used to train the samplers) under the density obtained from a Parzen-Windows density estimator 3 based on 10000 generated samples. Table 1 shows the thus measured log-likelihoods.
 5.2. Evaluating the invariant-feature learning Our next series of experiments investigates the effect of the training criterion proposed in section 4 to learn more invariant second-layer features. The correspond-ing model is denoted CAE-2 p .
 How to measure invariance to transformations known a-priori? In order to validate that the pro-posed criterion does learn features that are more in-variant to transformations of interest, we define the following average normalized sensitivity score, follow-ing the ideas put forward in Goodfellow et al. (2009). Let T ( x ) represent a random deformation of x in di-rections of variability known a priori, such as affine transformations of the ink in an image (corresponding for example to small translations, rotations or scaling of the content in the image). Then ( f i ( x )  X  f i ( T ( x ))) measures how sensitive is unit i to T (when large) or how invariant to it (when small) it is. Following Good-fellow et al. (2009) we also need some form of normal-ization to account for features that do not vary much (maybe even constant): we will normalize the sensi-tivity of each unit by V [ f i ], the empirical variance of f ( x ) over the data set. This yields for each unit i and each input x a normalized sensitivity Like Goodfellow et al. (2009), we focus on a fraction of the units, here the 20% with highest variance, and de-fine the normalized sensitivity  X  ( x ) of the layer for an example x as the average of the normalized sensitivity of these selected units. Finally, the average normal-ized sensitivity score  X   X  is obtained by computing the average of  X  ( x ) over the training set. When comparing layers learned by two different models A and B , statis-tical significance of the difference between  X   X  A and  X   X  is assessed by computing the standard error 4 of the mean of differences  X  A ( x )  X   X  B ( x ).
 Experimental comparison of sensitivity to a-priori known deformations. We considered stochastic affine deformation T ( x ) for MNIST test-set digits, controlled by a set of 6 random parame-ters. These produced slightly shifted, scaled, rotated or slanted variations of the digits.
 Table 2 compares the resulting average normalized sensitivity score obtained by the second layer learned by the CAE-2 p algorithm, to that obtained for sev-eral alternative models. These results confirm that the CAE-2 p has learned features that are significantly more invariant to this kind of deformations, even though they have not been explicitly used during train-ing. DBN-2 is a Deep Belief Network obtained by stacking two RBMs.
 Effect of invariant-feature learning on classi-fication performance. Next we wanted to check whether the ability to learn more invariant features of the CAE-2 p could translate to better classification per-formance. Table 3 shows classification performance on the TFD dataset of a deep neural network pretrained using several variants of CAEs, and then fine-tuned on the supervised classification task. For comparison we also give the best result we obtained with a non-pretrained multi-layer perceptron (MLP). We see that the criterion for learning more invariant features used in CAE-2 p yields a significant improvement in classifi-cation. We get the best performance among methods that do not explicitly use prior domain knowledge. 5 We have proposed a converging stochastic process which exploits what has been learned by a CAE to generate samples, and we have found that these sam-ples are not only visually good, they  X  X eneralize X  well, in the sense of populating the space in the same ar-eas where one tends to find test examples. A related idea has also allowed us to train the second layer of a 2-layer CAE, acting like pooling or invariance-seeking features, and this has yielded improvements in classi-fication and invariance.
 The invariance criterion for training pooling units pro-posed here works well for feature extraction for classifi-cation problems (and so do classical pooling strategies) but is not optimal for pure unsupervised learning. In-deed, it throws away some of the information along the first layer X  X  leading direction of variation (which are typically not useful for classification, but would be useful to reconstruct the input). A more general ap-proach that we propose to investigate is based on the idea of learning a representation that does not throw these variations away but instead disentangles them from each other (Bengio, 2009). We believe this could be achieved by criteria which encourage the active fea-tures to be invariant to the input variations captured by other features.
 Another direction of future work regards linking the CAE to a density model giving rise to the appropriate local covariance (i.e., the one used in the generative process). If we consider a small neighborhood around a point x with density p ( x ), then the local variance is the variance associated with the density that is the product of p with a local kernel (such as as Gaussian or uniform ball) centered at x . The generating pro-cess we have proposed here essentially corresponds to moves according to a Brownian motion in which the lo-cal mean and covariance are location-dependent. The interesting question is how one should pick those lo-cal means and covariances so as to replicate a target density. Intuitively, these should depend on the lo-cal structure of the density, so that the mean moves points towards the manifold (in the direction of the log-likelihood gradient) and the covariance disallows moving out of the manifold. Note that both of these are captured by the CAE.
 Y. Bengio. Learning deep architectures for AI. Foun-dations &amp; Trends in Mach. Learn. , 2(1):1 X 127, 2009. Y. Bengio and M. Monperrus. Non-local manifold tan-gent learning. In NIPS X 2004 , pages 129 X 136. MIT Press, 2005.
 Y. Bengio, H. Larochelle, and P. Vincent. Non-local manifold Parzen windows. In NIPS X 2005 , pages 115 X 122. MIT Press, 2006a.
 Y. Bengio, M. Monperrus, and H. Larochelle. Nonlocal estimation of manifold structure. Neural Computa-tion , 18(10):2509 X 2528, 2006b.
 O. Breuleux, Y. Bengio, and P. Vincent. Quickly generating representative samples from an rbm-derived process. Neural Computation , 23(8):2053 X  2073, Aug. 2011.
 L. Cayton. Algorithms for manifold learning. Techni-cal Report CS2008-0923, UCSD, 2005.
 A. Coates and A. Y. Ng. Selecting receptive fields in deep networks. In NIPS X 2011 , 2011.
 I. Goodfellow, Q. Le, A. Saxe, and A. Ng. Measuring invariances in deep networks. In NIPS X 2009 , pages 646 X 654, 2009.
 G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation , 18:1527 X 1554, 2006.
 Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropa-gation applied to handwritten zip code recognition. Neural Computation , 1(4):541 X 551, 1989.
 H. Narayanan and S. Mitter. Sample complexity of testing the manifold hypothesis. In NIPS X 2010 . 2010.
 M. Ranzato, J. Susskind, V. Mnih, and G. E. Hin-ton. On deep generative models with applications to recognition. In CVPR X 11 , pages 2857 X 2864, 2011. S. Rifai, Y. Dauphin, P. Vincent, Y. Bengio, and X. Muller. The manifold tangent classifier. In NIPS X 2011 , 2011a. Student paper award.
 S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Ben-gio. Contracting auto-encoders: Explicit invariance during feature extraction. In ICML X 2011 , 2011b. S. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science , 290 (5500):2323 X 2326, Dec. 2000.
 B. Sch  X olkopf, A. Smola, and K.-R. M  X uller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation , 10:1299 X 1319, 1998.
 P. Simard, B. Victorri, Y. LeCun, and J. Denker. Tan-gent prop -A formalism for specifying selected in-variances in an adaptive network. In NIPS X 1991 , 1992.
 J. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimen-sionality reduction. Science , 290(5500):2319 X 2323, Dec. 2000.
 M. E. Tipping and C. M. Bishop. Mixtures of proba-bilistic principal component analysers. Neural Com-
