 Caching is a widely used technique to boost the performance of search engines. Based on the observation that the speed gap between the random access of flash-based solid state drive and its sequential access is much inapparent than that of magnetic hard disk drive, we introduce a new static list caching algorithm which takes the block-level access latency into consideration. The experimental results show that the proposed policy can reduce the average disk access latency per query by up to 14% over the state-of-the-art algorithms in the SSD-based infrastructure. Besides, the results also reveal that our new strategy outperforms other existing al-gorithms even on HDD-based architecture.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process ; H.3.4 [ System and Software ]: Performance evaluation X  efficiency and effec-tiveness Algorithm, Performance, Experimentation Search Engines, Static caching, Solid State Drive, Latency
Caching technologies have been widely employed by Web search engines, as they can reduce the query latency and result in higher query throughput. In a static list cache, the cache typically keeps the posting lists of the most-frequent query terms that are extracted from the previous query log. It is general to treat the static list caching problem as a classic KNAPSACK problem [1]: The fixed size cache memory is viewed as a fixed capacity knapsack, while the data to be filled in the cache memory are regarded as commodities that have their own value and volume. For a give term t , its posting list is denoted as ` ( t ). Since the volume (the length of ` ( t ), counted in bytes, denoted as | ` ( t ) | ) is fixed, estimating its  X  X alue X  (the I/O cost that can be saved if it is cached) as precisely as possible is crucial to the performance of a caching algorithm.
 In the past, the traditional magnetic hard disk drives (HD-D) are used as external storage media. Due to the mechan-ical nature of HDD, the cost of a random read is about two orders of magnitude larger than that of a sequential read [13]. Thus, the access latency caused by a cache miss is dominated by the expensive random seek operation. Con-sequently, most of the existing static caching techniques are designed and tuned to achieve higher hit ratio, as the lower cache miss ratio indicates less random reads required.
Recently, solid state drive (SSD) has been largely deployed as secondary storage media in Web search engines [6, 7]. S-ince the internal structure of SSD involves no mechanical operations, the random read is no longer a determinant of the overall access latency. We repeat the read latency test in [13] with an SSD (OCZ Vertex-3 120GB) and a HDD (Seagate Barracuda 500GB, 7200rpm) and get Figure 1. As indicated in Figure 1, different from the case of HDD, the latency (in block level) of a random read in SSD is now com-parable to that of a sequential read. Therefore, in an SSD-based infrastructure, caching algorithms should now aim to save not only random reads but also sequential reads. In other words, with SSD as the external storage media for the posting lists, the existing static list caching algorithms may not perform as good as they do with HDD.
 Figure 1: Read access latency on HDD and SSD.
 Each read fetches a block (4KB). The page cache is by-passed.

In this paper, we introduce a new static list caching strat-egy that takes the block-level access latency into account. The proposed method aims to reduce the overall cost of both the random reads and sequential reads. The experimental results show that the proposed strategy outperforms other state-of-the-art static caching algorithms. To the best of our knowledge, this is the first study to develop a new caching policy specific to SSD-based search engine infrastructure.
Static caching has been studied extensively in the litera-tures [1, 2, 3, 5, 8, 9, 10, 11]. Baeza-Yates and Saint-Jean [3] presented a hierarchical memory organization for invert-ed lists. In [3], the posting lists of the most frequent terms in the previous query log were stored in main memory, act-ing as static list caching. We will use the policy, referred as Query-Term-Frequency (QTF) , as one of the baselines to compare with. Another static caching policy for posting lists was proposed by Baeza-Yates et al. [1]. They proposed an algorithm which caches the posting lists of the terms with frequency of a term t in the previous query log. We refer this algorithm as QTFDF and use it as another baseline.
Apart from the work described above, some researchers focused on the cost-based static caching algorithms. Ozcan et al. [11] proposed a cost-aware strategy for static query result caching, while a multi-level static cache architecture for various item types was introduced in [9]. The cost models described in these two work are analogous to ours, except that (i) we are dedicated to posting list caching and (ii) we introduce a new cost function which takes the block-level access latency in SSD into consideration.

Work by Wang et al. [13] was the first to investigate the impact of SSD on cache management of Web search engines. They also discussed the impact of the narrower speed gap (compare to that of HDD) between the random access and sequential access in SSD on the existing caching algorithms, which is very similar to our work. However, they did not come up with better caching policies that are appropriate for SSD-based infrastructure.
In this paper, the static list caching problem is treated as a KNAPSACK problem and solved by a greedy approach like [1]. That is, for any term t , we calculate the benefit B ( t ), i.e., the I/O cost that can be saved during the whole query evaluating process if ` ( t ) is kept in the cache. Then we sort the terms according to their B ( t ) in descending order and fill as many  X  X igh-beneficial X  terms as possible into the static cache until it is full. The benefit B ( t ) is computed by a benefit function (1), with a cost function C ( t ) estimating the access cost that will be saved by a single cache hit. Note yielded by per cached byte of ` ( t ).

B ( t ) = As shown in (2), by adopting different cost functions C ( t ), all the existing static list caching policies can be modeled by the benefit function. If the access cost of any ` ( t ) is simply considered as its length or just a constant, then we get exactly the QTF or QTFDF policy, respectively. Besides these two cases, the access latency can be estimated by a more elaborated formula T ( t ), which considers the working principle of the drives as well. In a HDD-based architecture, due to HDD X  X  mechanical nature, the access latency consists of a seek time D seek , a rotational delay D rotate and also the time D read of reading per block data (whose size is D block Thus, T hdd ( t ) can be computed by (3), while D seek , D D read and D block has the same value as those in [9, 11]. We refer the strategy that applies T ( t ) to estimate the access latency as MECH (abbreviated for mechanical) and use it as the third baseline. Note that SSD does not carry out the read operation in a mechanical way but a electronical manner, thus, instead of (3), its T ( t ) should be computed by (4), where S read represents the time of fetching data of S page size [14] 1 . Nevertheless, we still refer this method as MECH in SSD-based setting. As T ssd ( t ) is only proportional to | ` ( t ) | , the items it keeps in the cache are exactly the same as QTF does. Therefore, MECH and QTF will perform identically in SSD-based setting.
 As revealed by Figure 1, however, in SSD-based infrastruc-ture, not each part of | ` ( t ) | contributes equally to the access latency of a positing list ` ( t ). That is, even though a read op-eration in SSD involves no mechanical operations (like seek and rotation in HDD), it still can be viewed as a compound of a relative expensive random read and a few low-cost se-quential reads. Thus, we develop a new caching strategy (called BLOCK ) that takes into account the block-level la-tency gap between random read and sequential read. The C ( t ) is now represented by (5), which measures the latency of accessing ` ( t ) in terms of the numer of random reads. Here  X  is the ratio of the random read latency to the sequen-tial read latency of a certain drive (  X  ssd =11.5 and  X  hdd see Figure 1), and B s equals to the block size in the system. With C block ( t ), we can now obtain a more precise benefit B ( t ) for each term t in SSD-based infrastructure. We also expect that such estimation will work for HDD-based sys-tem, and this is conformed by experimental results reported in Section 4.2.
To test our method, we extract 5 million web pages 2 from the GOV2 document set to generate the inverted index. The index consists of only the document identifiers (no frequency or positional information are included), with a size of 4.9GB.
Our query trace is extracted from a large log of queries is-sued by about 650,000 AOL users over three months 3 . After removing the followup queries (queries requesting successive page of results) [5] and eliminating the queries containing terms that do not appear in the document set, the log con-tains 14.4 million queries. We draw a random sample of 10
Usually, S read and S page are considered as 50  X  s and 2KB.
In our testbed, the GOV2 collection is evenly distributed to 5 servers by URL hashing. We only use the first subset since we focus on the caching techniques in a centralized system.
The queries are all de-identified and anonymized, so no personal info would be revealed from the query trace. million queries for our experiments. There are 5,753,735 dis-tinct queries and 397,447 distinct query terms in the sampled query set, while the average query length is 3.3. As expected, both the distributions of query frequency and term frequen-cy of the sampled query set follow power-law distributions, with skew-factor  X  = 0.43 and 1.63, respectively.

We then conduct a number of experiments by replaying the sampled query log over the generated index. We employ SvS algorithm [4] as the posting list intersection algorithm to evaluate the queries. We divide the 10 million queries into two parts: 90% of queries are used as training data to obtain reliable statistics such as term frequency, while the rest 10% are used as testing data. Since the index involved in the testing query set only amounts to 3.897 GB, thus the largest cache size was set as 3.5 GB.
 The experiments are conducted on a PC running Centos Linux version 6.0, with an Intel i7 930 processor at 2.8 GHz and 12 GB of main memory. The SSD and HDD used are the same as those in Figure 1. In order to minimize the effectiveness of the OS buffer, the page cache is by-passed The block size in the system is set as 4KB [13]. We report the results in terms of term hit ratio , byte hit ratio 5 that used in [12]), as well as the wall-clock time .
We now present the results from the experimental evalu-ation of different static caching algorithms. First, we load all the index involved into main memory and execute the in-memory evaluation, which provides us with the elapsed time of the intersection operations solely. Second, for each static caching algorithm, the queries are evaluated with a static cache module and the total query response time is recorded. Then, by subtracting the intersecting time from the overall run time, we get the accurate disk access latency during the query processing. The wall-clock times reported in this section are all disk access latencies.

Figure 2 depicts the average access latency of four caching algorithm in the SSD-based infrastructure. In this figure, using O DIRECT flag in open() system call. Note that the 1 million testing queries require around 5.76 TB data in total during the query processing, provided no index is cached. Thus, a slight difference in byte hit ratio means a substantial difference in the amount of I/O data. we can see that BLOCK method outperforms other policies under all the cache size settings, while QTFDF policy has the longest access latency. More detailed numerical results can be found in the left part of Table 1, where the term hit ratio and byte hit ratio are also presented. Figure 2: Average access latency (per query) on SSD Table 2: Number of lists kept in static cache on SSD platform (counted in thousands)
As shown in Table 1, QTFDF has the highest term hit ratio among all the existing static cache algorithms. This echoes the results in [1]. However, it has the poorest byte hit ratio as well as the disk access latency. In contrast, QTF (also MECH 6 ) achieves the best byte hit ratio, though its term hit ratio is markedly worse than other algorithms. Table 2 shows the number of lists that are loaded into the static cache memory. Each numerical value in this table is counted in thousands. We can see that, QTF keeps the Since MECH has exactly the same performance as QTF on SSD, we do not distinguish them in the following discussions, as long as in the context of SSD-based infrastructure. fewest lists in the static cache, which explains why it has the lowest term hit ratio. In addition, it can be inferred that, with QTF policy, the lists in the cache are relatively longer than those of its counterparts. Thus, the average size of the posting lists that are fetched when cache misses occur is relatively smaller (which is confirmed by Figure 3). This accounts for the highest byte hit ratio it gets. In the same way, it is easy to understand why QTFDF ends up with a much lower byte hit ratio while achieving a rather higher term hit ratio. Figure 3: Average list size (in terms of blocks) of all cache misses on SSD platform
The reason that BLOCK has the shortest average disk access latency can be inferred from Figure 3. Take 1024 M-B cache memory for instance, a cache miss under BLOCK method would save about 535-65=470 sequential reads com-pared with QTFDF, which is equivalent to around 41 ran-dom reads on SSD. Even though the term miss ratio of QTFDF is around one third (0.174/0.454, actually) over that of BLOCK, it can not make up for the 41 extra ran-dom reads saved during each cache miss by BLOCK. On the other hand, although QTF results in smaller number of disk accesses (55 blocks per cache miss in average), the slight superiority is outweighed by the random accesses saved by BLOCK due to its lower term miss ratio (0 . 539  X  [ 55  X  1 0 . 454  X  [ 65  X  1 11 . 5 + 1]). Therefore, these explain why BLOCK yields a shorter access latency than other baselines. Fur-thermore, the results indicate that higher term hit ratio or byte hit ratio do not necessary lead to lower access latency, which echoes the conclusion in [13]. Figure 4: Average access latency (per query) on HD-D
We also examine BLOCK strategy in HDD-based experi-mental environment. Note that MECH is no longer the same as QTF in this context. Figure 4 illustrates the average disk access latency of the 1 million queries in HDD-based platfor-m, while the term hit ratio and byte hit ratio are presented in the right part of Table 1 7 . Figure 4 shows that our new strategy also beats all the state-of-the-art algorithms (even MECH) under HDD-based infrastructure, though by a small margin. The other details, like the number of lists kept in the static cache by each policy and the reason why BLOCK outperforms other algorithms, are almost the same as those of SSD and are omitted due to the space constraint.
In this paper, we investigate the speed gap between ran-dom access and sequential access on both SSD and HDD, and then propose a block-level latency-aware static policy for inverted list caching. Our results show that the new strategy can reduce the disk access latency during query processing in both SSD and HDD based search engine infrastructures. The results also demonstrate that neither term hit ratio, nor byte hit ratio, are reliable reflections of the actual query la-tency, no matter the search engine infrastructure is HDD or SSD based. In other words, the wall-clock time is the only reliable metric to measure the efficiency of a cache policy. We would like to thank Jianguo Wang, Hao Li and Fan Zhang for their helpful discussions and comments. This work is partially supported by NSFC of China (60903028, 61070014) and Key Projects in the Tianjin Science &amp; Tech-nology Pillar Program (11ZCKFGX01100).
Note that QTF performs identically on SSD and HDD, in terms of term hit ratio and byte hit ratio. So does QTFDF.
