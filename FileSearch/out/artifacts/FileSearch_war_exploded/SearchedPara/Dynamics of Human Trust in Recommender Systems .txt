 Dynamic Decision Making 
Carnegie Mellon University Department of Computer University of Illinois at The trust that humans place on recommendations is key to the success of recommender systems. The formation and decay of trust in recommendations is a dynamic process influenced by context, human preferences, accuracy of recommendations, and the interactions of these fact ors. This paper describes two psychological experiments (N=400) that evaluate the evolution of trust in recommendations over time, under personalized and non-personalized recommendations by matching or not matching a participant X  X  profile. Main fi ndings include: Humans trust inaccurate recommendations more than they should; when recommendations are personalized, they lose trust in inaccurate recommendations faster than when recommendations are not personalized; and participants re port less trust and lower overall ratings of personalized but ina ccurate recommendations compared to not-personalized inaccurate recommendations. We make connections to the possible implic ations of these psychological findings to the design of recommender systems. Categories and Subject Descriptors [H.3.3] Information Storage and Retrieval: Information Search and Retrieval; [J.4] SOCIAL AND BEHAVIORAL SCIENCES General Terms Performance, Experimentation, Human Factors, Theory. Keywords Trust, decision making, cognitive processes Over the last 25 years, automated recommender systems (RS) have attempted to help users find the right information at the right time [15]. Recently, the social web has made massive amounts of user-provided content available for analysis, making the task of identifying reliable information sources increasingly difficult. In many cases, only a small window of information exists upon which a human can make a decision about whether or not to trust a recommendation and the RS that produced it. Furthermore, that window of information changes dynamically, and humans adapt their trust judgments and thei r preferences accordingly. automated accuracy metrics are not enough for evaluation of RS [7, 14, 16]. A main reason is that the overall user experience with a RS should be accounted for. We believe that a key component of this overall user experience is the trust that humans place on the RS and its recommendations. perspective [4, 12, 13] at the ne twork [4], temporal [10], and algorithmic [14] levels. However, accounting for human trust in order to build RS that dynamically adjust to the human preferences and experiences is a challenge. This is largely due to the lack of research regardi ng how humans develop and adjust their trust in a RS. The current research is an interdisciplinary effort, bringing together behavior al and computer scientists, to collaborate on the development of recommender systems that are aware of and can adapt to the dynamics of human behavior. into the dynamics of human trus t in RS. We use psychological experiments in simple learning paradigms to develop theoretical insights of the process of learning, choice, and trust that could inform the development of RS. options of differing quality in the presence of accurate or inaccurate recommendations that are impersonal in terms of pre-defined human preferences. Results indicate that participants decrease their trust when the RS is inaccurate, yet even after extended practice with inaccurate recommendations, they continue to trust the RS more th an they should objectively. recommendations that match the user X  X  profile exactly. As in Experiment 1, participants chose among options that differ in their outcome quality under accurate and inaccurate recommendations. Our results indicate that particip ants were able to abandon the inaccurate RS more rapidly than in Experiment 1. Furthermore, participants reported lower levels of trust and overall quality of the RS in the inaccurate condition compared to the first experiment. applications to the design of cognitively-aware RS. Trust in recommendations systems has been approached mostly from a computational perspective. For example, early trust metrics for RS include Golbeck X  X  metrics in social networks [4]; O X  X onovan and Smyth [14] evaluated a trust model to accompany similarity scores in a collaborative filtering algorithm; and this model was extended by Liu [11] to account for temporal sequences. A common thread among these studies is that they evaluate trust in a RS using automated accuracy metrics while placing little attention to human-b ased trust decisions and to the role of trust from the overall human experience. have explored how contextual elem ents of RS influence trust. For example, Swearing and Sinha [16] examined interaction design for RS focusing on user require ments and available system features. They argue that a familiar recommendation can increase trust, and that transparency of the recommendation process plays an important role. These findings are supported by the psychology literature on conformity [1] where systems designed to help people make choices can actuall y change people X  X  subjective opinions. Researchers also argue for the benefits of providing explanations to recommendations th at may convince users to trust the system. Knijnenburg [9] and Bostandjiev [2] report similar findings using an interactive visu al recommender system. Finally, Cosley et al. find that recommendations can influence users X  ratings towards the predicted rating, whether it is an accurate one or not [3]. highlights trust perceptions and act ions that signal trust from the human perspective. We present experiments where humans make decisions about several sources in the presence of a recommender system that provides accurate or inaccurate recommendations. We aim at advancing the theoretical basis for building RS that account for the dynamics of human trust a nd preferences. Particularly, we are concerned with providing some theoretical basis for building RS that dynamically adapt to the preferences and changes in human trust. This is largely a challenge in the RS community. We believe that addressing this challenge can begin with knowledge about how humans adjust their preferences and how they learn to trust or not trust RS. experiments where we used si mplistic paradigms of decisions from experience (DFE) [5, 6], expanded to include simple accurate or inaccurate recommendations. DFE paradigms are designed to study how humans make small daily decisions that they face repeatedly. In these well studied paradigms, participants learn from making repeated choices under conditions of uncertainty, where explicit info rmation about outcomes and the probabilities of good outcomes from different options is not descriptively provided but rather learned from experience. The most basic DFE paradigm is pre sented as a money machine, where participants choose between two unlabeled buttons over several trials, receiving feedback for each choice. The buttons provide feedback based on underlying outcome distributions and over time, participants learn through experience which button provides better outcomes on average [8]. Given the research summarized above, we expected that humans would trust inaccurate recommenders more than they should, but that with extended experience, they would learn to not trust inaccurate recommendations. In our experimental paradigm, partic ipants are presented with four options (represented as buttons on the screen) that they are asked to choose from. On each of 200 trials, participants choose one of the four options and receive feed back (positive or negative) from their selected option before moving to the next trial. Positive feedback is recorded and added to a running total which determines the participant X  X  compensation at the end of the experiment. The four options varied in the frequency with which they provided a quality outcome as: .2, .4, .6, and .8 for the four respective buttons (presentat ion order and labeling were randomized). The feedback probability of the four options was determined through extensive prete sting to ensure that most participants would be able to le arn to discriminate between the four buttons by the end of 200 trials. participant by an accurate (recommendations always led to a good quality outcome) or inaccurate (recommendations led to a good quality outcome only half of the ti me) recommender. Note that our definition of accuracy represents a probability, and differs from the typical rating-based concept of accuracy in RS, e.g., from [3]. Thus, for accurate recommendations, each chosen option that was recommended on a trial produced a positive outcome (and each source not recommended produces a negative outcome); and in the inaccurate condition, recommended options produced a positive outcome half of the time and a negative outcome half of the time (the same is true for options not recommended). proportion of recommendations followed (as a measure of trust in the recommendations), total outc ome (sum of positive feedback), and choice proportion for each of the four options (as a measure of detection of more profitable options). After participants complete 200 trials, they answer qu estions designed to elicit their perceptions of both the recommender and the quality of outcomes from the four options, in addition to basic questions about trust and trust in RS. Two hundred participants completed Experiment 1 on Amazon MTurk; 80 participants were female with an overall mean age of 34.7. experiment, similar to paradigms common in cognitive science [6]. The four options were labeled A-D and the outcome feedback was composed of 1 (positive) or 0 (negative). In the experimental instructions, participants were told that the study was designed to explore how intelligence analysts collect and acquire information, and that the four options could represent sources of information such as military reports or social media that could provide information at a given time th at is useful or not useful. Additionally, participants were told that a RS would highlight specific sources on each trial that could provide useful information on that trial. The left side of Figure 1 plots the proportion of recommendations followed on each trial for the accurate and inaccurate conditions. In the accurate recommendation condition, participant immediately chose from the recommended options and continued to choose recommended options across the 200 trials. In the inaccurate recommendation condition, participants began choosing recommended options but decreased their choice of recommended options as trials progressed, choosing recommended options about 60% of the time by the end of the experiment, indicating that participants trust inaccurate recommendations more than they objectively should (50%). Figure 1. The proportion of choices from recommended options are plotted over time by condition (accurate recommender, inaccurate recommender) for each experiment. condition mirror the distribution of recommendations for each option (as recommendations were predominantly followed) with clear distinction between the high probability options and the low probability options. In the inaccurate condition, choice proportions between the four options (left side of Figure 2) differed across trials with part icipants choosing more from the best option, despite in accurate recommendations. Figure 2. For the inaccurate recommender conditions only, the proportion of choices for each option (.8[best], .6, .4, .2[worst]) are plotted across ti me for each experiment. Experiment 2 was designe d to provide personalized recommendations in a more detailed scenario. Two hundred participants completed Expe riment 2 on Amazon MTurk (81 female participants, M age = 33.5). Participants were told that the experiment was designe d to test a personalized social media app that provided recommendations on which venue to visit on a given night to have the best opportunit y to meet a person they are compatible with. Before beginni ng the experiment, participants rated their preferences in a pote ntial partner on three attributes with three levels in each attribute: attractiveness, education, and common interests. The participants X  ratings were used to determine what feedback was considered a match (positive outcome) or not (negative outcome). The choice portion of Experiment 2 was identical in stru cture to Experiment 1 with two exceptions. First, the four options, instead of being labeled A-D, were labeled: Alpha Club, Beta Bar, Common Club, and Delta Bar. Second, the feedback was presented as the profile (levels of the three attributes represented by stars) of a person they meet. If the attribute levels were identical to the participant X  X  stated preferences, it was considered a match and added to their total, which determined their monetary compensation at the end of the experiment. The right side of Figure 1 plots the proportion of recommendations followed in each trial for the accurate and inaccurate conditions. In the ac curate recommendation condition, the proportion following recommendations mirrored that of Experiment 1. That is, people trusted and followed the recommendations almost all of the time. In the inaccurate recommendation condition, participants decreased their choice in the recommended options more over time. Compared to Experiment 1 X  X  results, the pe rsonalization with individual profiles led to participants abandoning recommendations more quickly, evident in the difference in recommendations followed in the first 25 trials (72% vs. 84% in Exp. 1). Yet, even in this case and after 200 trials, participan ts trusted the recommendations more than they should have (about 60% of the time by the end of the experiment). choice proportions between the four options in the inaccurate recommender condition (right side of Figure 2) is similar to the matching condition in Exp. 1, fa voring the best option over time, and reducing the proportion of choices from the less favorable options over time. This is consistent with people learning the best experienced outcome from repeated trials [6]. For each condition in both experiments, Table 1 shows the total proportion (and Standard Deviation) of the total number of choices made from recommended options, the final outcome (measure of overall performance), and the proportion of choices from the best option and the worst option. The accurate recommendations conditions did not differ from one another, and we focus our analysis on comparisons between the inaccurate conditions in Experiments 1 and 2. conditions in Experiments 1 and 2 are significant for both the number of choices from recommended options ( t (199) = 8.77, p &lt; .001) and the obtained outcome ( t (199) = 3.28, p &lt; .001). There is no difference between the two groups in terms of choices from the best option 1 ( t (199) = 0.69, p = .488) however the difference in choice proportion for the worst option is significant ( t (199) = 3.18, p &lt; .01). Table 1. Mean (SD) dependent variables for each condition. To further explore the role of trust in choice behavior for the inaccurate conditions, we examined dynamic choice behavior in response to choosing a recommended option and receiving either a positive or negative outcome. We used two time points (trial 25 and trial 175) to explore early and late changes in trust and reactions to feedback. At each time point, we looked at participants who chose a recommended option and whether they received a positive outcome (the recommendation was correct) or whether they received a negative outcome (the recommendation was incorrect). We then calculate d how many times participants chose a recommended option over the next ten trials to calculate the probability of choosing a recommended option after a successful or unsuccessful recommendation. These contingent choice dynamics are plotted in Fi gure 3 (the same analysis was performed using only the following trial and the following 25 trials, both of which produce the sa me results presented here). of choosing a recommended option following either a successful or unsuccessful recommendation from time 1 to time 2 96) = 71.13, p &lt; .001). In Experiment 2, this main effect is in the presence of an interaction wh ere the probability of choosing a recommended option after a successful recommendation is lower than after an unsuccessful recommendation at time 2 ( F (1, 96) = 39.29, p &lt; .001). This interaction did not reach significance in Experiment 1. Figure 3. Probability of using the recommendation system after a correct or incorrect rec ommendation over time in the inaccurate recomme ndation condition. After the experiment, we asked participants to estimate out of ten recommendations, how many would be successful. We also asked them to rate, on a 1-5 scale, the qu ality of the RS and their trust in the recommendations. Participants in the inaccurate condition of both experiments correctly estimated that the rate of accurate recommendations was about 50% (Experiment 1: M = 5.2/out of 10; SD=1.2; Experiment 2: M = 5.06, SD= 1.3) . However, when asked for their opinion of the RS, participants in Experiment 2 rated it significantly worse than those in Experiment 1 ( t (200) = 7.377, p &lt; .001). Additionally, participants in the inaccurate condition of Experiment 2 reported trusting the recommendation system less than those in Experiment 1 ( t (200) = 5.189, p &lt; .001). Several important conclusions aris e from the results of these two experiments. First, humans trust recommendations, even when they are inaccurate, and they trust them more than they objectively should. This result agrees with Cosley et al. X  X  study [3], which found that people X  X  opinions can be influenced based on what the RS predicts, rega rdless of predictive accuracy. However, in contrast to [3], we observe a difference between accurate and inaccurate conditions. Second, people lower their trust in the recommendations when they are inaccurate, but they do so more when recommendati ons are personalized. Third, people learn to select more accurate options even in the presence of inaccurate recommendations. And fourth, even when people are sufficiently aware of the inaccuracy of recommendations, participants feel more dissatisfi ed and trust the recommendations less when they are personalized. believe that the results of these cognitive experiments are a step towards understanding when reco mmendation consumers are at higher risk of losing trust in a system X  X  predictions, and conversely when their trust is high and they can be most influenced by the system X  X  predictions. This work was partially supported by the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 The views and conclusions contained in this document are those of the authors and should no t be interpreted as representing the official policies, either ex pressed or implied by the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Gove rnment purposes notwithstanding any copyright notation here on. [1] S. E. Asch. Effects of group pressure upon the modification [2] S. Bostandjiev, J. O X  X onovan, and T. H X llerer. TasteWeights: [3] D. Cosley, S. K. Lam, I. Albert, J. A. Konstan, and J. Riedl. [4] J. A. Golbeck. Computing and applying trust in web-based [5] C. Gonzalez. The boundaries of instance-based learning theory [6] C. Gonzalez and V. Dutt. Instance-based learning: Integrating [7] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. [8] R. Hertwig, G. Barron, E. U. Weber, and I. Erev. Decisions [9] B. P. Knijnenburg, S. Bostandjiev, J. O X  X onovan, and A. [10] N. K. Lathia. Evaluating collaborative filtering over time. [11] D.-R. Liu, C.-H. Lai, and H. Chiu. Sequence-based trust in [12] P. Massa and P. Avesani. Trust-aware recommender systems. [13] P. Massa and P. Avesani. Trust metrics in recommender [14] J. O X  X onovan and B. Smyth. Trust in recommender systems. [15] P. Resnick and H. Varian. Recommender systems. Commun. [16] K. Swearing and R. Sinha. Interaction design for 
