
In recent years, the number of digital infor mation storage and retrieval systems has increased immensely. Many of those database systems are available in a networked environment. Having access to such interc onnected digital source databases makes the task of integrating distributed data increasingly important. Solutions to this prob-lem include data warehouses, a term that, in the database community refers to mate-rialized views over (possibly distributed) information sources. Numerous publications deal with problems of maintenance of su ch views under data updates of the under-lying sources (Blakeley et al. 1986; Gupta and Mumick 1995; Zhuge et al. 1996). independence of data producers from data c onsumers. Independent data producers or providers have control over the capabilities (schema) of their information sources.
This raises the question of the influence of meta data updates (deletions of attributes or relations in underlying da tabases) on a view. In traditional views as introduced (1997)), meta data updates typically render a view definition undefined. the literature. One approach taken for example by Levy et al. (1995b), Arens et al. (1996), or Duschka and Genesereth (1997) is to create a global domain model, i.e. an a priori-defined-type system th at defines all possible attributes and relations in a given domain (world view). Over such a domain model, information providers define views that specify which part of the world X  X  data they provide. Consumers also query this domain model instead of directly referring to the actual data sources (or customized views defined upon them). An algorithm is provided to rewrite a consumer X  X  query in terms of the currently available providers X  views and thus provides the consumer with whatever data happens to be available at that moment. This approach is sometimes also referred to as the local-as-view integration approach.
 a globally fixed domain nor on an ontology of permitted classes of data. Rather, views are built in the traditional way over a number of base schemas. Users are then directly aware of such views and, in fact, specific integration views are typically be-ing designed to meet the information needs of particular user groups or applications.
Those views are then adapted after base schema changes by rewriting them using information space redundancy and relaxable view queries. There are many systems in the literature that follow this approach, including the TSIMMIS-project (Garc  X   X a-
Molina et al. 1995), the Garlic-Project (Haas et al. 1997), mediators (Wiederhold 1992), and also our EVE system (Nica et al. 1998; Lee et al. 2002) X  X hich will be used as the basis of this work. The benefit of this approach is that no prede-fined domain (which is hard to define and t o maintain) is necessary and yet that changes in the schema can still be accommodated by automatically rewriting user queries (without human intervention). However, previous approaches do not fully ad-dress the problem of schema changes in the sources, especially if those changes are temporary or if subsequent changes have inverse effects (i.e. cancel out). steiner 1998b; Koeller et al. 1998; Lee et al. 2002), on which this paper is based, deals with the rewriting of views under schema updates in the underlying sources in the case of schema element deletions. In EVE , algorithms have been defined that can rewrite a view under meta data changes of underlying sources and retain all or a part of the view extent in the new rewritten view. The notion of nonequivalence of view rewritings has been defined (Lee et al. 2002) and a model for a numeric assessment of the quality and cost of such rewritings has been developed (Lee et al. 1999b). namely the SVS (Rundensteiner et al. 1997), POC/SPOC (Nica and Rundensteiner 1998b), CVS (Nica et al. 1998), and GRASP (Nica and Rundensteiner 1998b) al-gorithms. They generate a new view defin ition after each individual schema change in an information source without taking any information about earlier IS changes into account. Therefore, all those algorithms fall into the category of one-step syn-chronization algorithms. That is, after the deletion of an underlying relation that had initially been used by a view, they rewrite the view to not refer to that relation any more. They also do not support any synchronization under add-capability changes.
For example, when an information source IS 1 becomes unavailable to the data ware-house, data is either completely remove d or at best replaced from a different IS
That is, no history is being kept about prio r modifications of th e view definitions or the view states. Thus, reappearing ISs are not being recognized as such, and hence, when IS 1 joins the information space again, it will not be used again in any view.
This work describes a way to overcome both problems. In particular, we propose an algorithm capable of handling all information source schema changes, namely adds, renames, and deletes of attributes and relations. Also, we rewrite views under changes of constraints across the source databases, such as a containment relation-ship, defining that the relation IS1.Hotels contains the relation IS2.Boston-
Hotels . We refer to all those changes as meta data updates. The main contribution of this paper is the utilization of additional available meta data to keep views as close to their original definition as possi ble, under a sequence of arbitrary meta data changes that occur over time. We will demonstrate that the addition of attributes, re-lations and other knowledge about the information sources can indeed be beneficial to a view-synchronization environment.

This technology can be applied to a variet y of problems in views over multiple in-dependent information sources (which could be WWW sources with SQL-Wrappers).
It is most beneficial in situations in which the meta data of the underlying informa-tion may evolve or in which attributes or relations may be temporarily unavailable.
Also, the usefulness of materialized view s to a user could be increased when im-proved or additional information becomes available.

Initial research on the topic in this paper has been published in the Workshop on Data Warehousing and Knowledge Discovery, DaWaK 2000 (Koeller and Run-densteiner 2000). In addition to this initial w ork, this paper now contains a formal treatment of the meta data model, including a meta data change taxonomy, a full description of the synchronization algorithm, including a detailed description of treat-ing all cases of meta data changes, a running example of the view synchronization process, proofs of correctness of the synchronization as well as of the quality of the synchronized view, an analysis of the algorithm X  X  complexity and a discussion of the implementation issues of realizing the algorithm within the EVE data warehousing system.

The rest of the paper is organized as fo llows: Section 2 reviews needed back-ground in view synchronization, Sect. 3 defines the meta knowledge base, gives the change taxonomy and the notion of history. Section 4 introduces the main history-driven view-synchronization (HDVS) algorithm. Section 5 contains proofs of correct-ness and demonstrates the improvements achieved by our approach. A discussion of the implementation of the HDVS algorithm and a case study we have conducted using the EVE data warehousing system is given in Sect. 6. Section 7 reviews re-lated work and Sect. 8 concludes the paper.
Because our work is based on the EVE -Project (Rundensteiner et al. 1997), we give a short overview of those elements of the EVE system that we need as background in this paper. In particular, we introduce one of the one-step synchronization al-gorithms (Lee et al. 2002; Rundensteiner et al. 1997; Nica et al. 1998; Nica and
Rundensteiner 1998b; Koeller et al. 1998) because the strategy we propose in this work makes use of such algorithms when performing one or a sequence of single-step view synchronizations as necessary.
In order to support the evolution of views under schema changes in a data-warehouse-like environment, two kinds of information need to be stored: the view definitions themselves and information about the data on which those views are defined. In the
EVE system, such meta data is stored in the View Knowledge Base (VKB) and the Meta Knowledge Base (MKB), respectively. In addition to the view definitions (written in a query language such as relati onal algebra or SQL), the VKB contains information about how views are allowed to c hange while still being useful to a user.
For example, the deletion of a table central to the view might not be allowed, while the replacement of such a table with anothe r table, containing similar information, might be acceptable to a user. In the case of s uch a replacement, joins, predicates and/or projections of the base tables might also have to be adapted. Alternatively, it might also be acceptable to a user to remove certain attributes from a view. One way to classify such acceptable changes is through an extension of the SQL query language called E-SQL (Lee et al. 2002; Rundensteiner et al. 1997). Because the focus of this paper is not on E-SQL, we will use traditional relational algebra for our view definition. The only aspect of E-SQL view definitions we will use is the decision (made by a user) whether accepta ble rewritings of a view are those that produce subsets or supersets of the previous view extent (or both). For this purpose, each view in a view-knowledge base will be marked with a view evolution parameter ( VE  X  X  X  ,  X  ,  X  ,  X  X  ). A parameter of  X  means only rewritings producing identical view extents are acceptable (which is the uni versal assumption in traditional view maintenance), whereas  X  and  X  allow the rewriting X  X  view extent to be a subset or superset of the original extent, respectively.  X  producing rewritings are acceptable.
 schema changes), our system also needs to b e able to identify view-element replace-ments from other ISs. For this purpose, we express relationships between ISs through constraints (e.g. agreeing data types, func tional dependencies between attributes, ex-tent overlaps between relations) stored in the Meta Knowledge Base (MKB). One important constraint used in this paper is the containment-constraint. A containment constraint between two relations, R 1 and R 2 , states that a (horizontal and/or vertical) fragment of R 1 stands in a certain set relationship to a (horizontal and/or vertical) fragment of R 2 .
View synchronization refers to the proces s of rewriting views under capability chan-ges in underlying information sources 1 . In our architecture, view definitions are stored in a View Knowledge Base (VKB). Whenever a view V is affected a schema change, a view-synchronization algorithm is executed. The general view-synchronization process cons ists in attempting to find a replacement for missing view elements using the MKB and view definition and generating a rewritten view, V , that uses those replacements but is as sim ilar as possible to the previous view. For example, after a delete-relation capability change on a relation R is detected, the al-gorithm aims to find possible repl acements for those attributes of R that were used by the view V . We generally denote the original view by V and its rewriting by V .
While all of the view-synchronization algorithms previously presented are com-patible with the algorithm presented in this paper, we give an idea of one of the existing view-synchronization algorithms, namely POC, developed in the context of the EVE system (Nica and Rundensteiner 1998a, 1998b) (Example 1). As POC is used by our new algorithm, a basic understanding of it is necessary. Other one-step view-synchronization algorithms differ mainly in the strategy used to replace miss-ing schema elements and in the size of the search space explored in order to find replacements.

Example 1. We define an information space (meta knowledge base) describing a travel agency in which the administrat ors of information spaces IS1, IS2 and IS3 have a common interest in maintaining the integrity of their databases (Figs. 1 and 2).
The containment constraint shown in Fig. 2 asserts that the projection on the Holder and Age attributes of relation Insurance forms a subset of the projection on attributes
Name and Age of relation Customer for all tuples in Insurance for which Amount is over 1,000,000 and for which Age is under 50. Semantically, this could mean that the maintainer of IS3 is allowed to sell some insurance policies on her own, while the younger customers with expensive policies have to be entered into the main customer database first.
 We consider the view Customer-Passengers in Fig. 3 to show how to apply the
POC algorithm to find a replacement under the delete relation Customer change and then rewrite the view Customer-Passengers.

The POC algorithm (Nica and Rundensteiner 1998b) uses containment constraints in the MKB that connect a relation used a s a replacement to the remaining rela-tions in the existing query. Here, we can replace the attribute Customer.Age by the similar attribute Insurance.Age in relation Insurance. We then join the new table with FlightRes using the containment constraint from Fig. 2 (Fig. 4). Then all view the relation name Customer) are replaced by view elements usin g the new relation.
A possible rewriting of the query in Fig. 3 using this substitution and satisfying the subset-requirement is given by the query in Fig. 4. Note that the query in Fig. 4 contains additional predicates derived from the containment constraint used for the query rewriting (Insurance.Amount &gt; 1,000,000 and Insurance.Age
These additional predicates are inserted into the query by POC to satisfy the sub-set requirement stated in the original view definition (Fig. 3). So, in fact, the views computed by the queries in Figs. 3 and 4, respectively, are not equivalent, but the rewritten query represents an acceptable rew riting based on the original query given. rithm, including the full definition of use r preferences through the E-SQL language, which is not repeated here.
 literature are  X  A synchronization occurs after each meta data change, without taking previous  X  If a view can be rewritten from being valid on the old information space into  X  All deletes and renames of attributes and relations are handled, while additions  X  If a view is rewritten, it may deteriorate over time, i.e. the quality of a view to  X  If a meta data item is deleted, the algorithms cannot recognize its return. Tem-
Because view-synchronization algorithms ma y generate many possible query rewrit-ings, one needs to be selected as the new vi ew definition. To select between alterna-tive rewritings, the QC-Model (Lee et al. 1999b) has proposed a quality measure of view rewritings. This quality measure estimates the preservation of amount (extent) and types (interface) of information.

Quality refers to the similarity (vs. divergence) between a view V and its rewrit-ing, V . Quality is expressed for a view rewriting V by Q a distance metric for the extents of view rewritings V to a previous view V .Two factors are measured in order to determine the quality of a view. The degree of diver-gence in terms of the view interface ( DD attr ) determines numerically how different the view interfaces of the two queries are (low values mean low divergence, the value 0 means equal view interfaces). The degree of divergence in terms of the view extent ( DD ext ) is determined by the relative numbers of missing and additional tuples in the extent of a view rewriting (as compared with the extent of the previous view). The quality factor is then computed as Q ( V ) = 1  X  ( attr  X  with the factors attr and ext used to trade off and normalize the raw divergence values. The final Q-value is a number between 0 and 1. In order to estimate the overlap between old and new views, containment constraints are used that spec-ify statements about relationships between relations. Precise formulas can be found in Lee et al. (1999b). In an evolvable data warehouse, as introduced by Rundensteiner et al. (1997) and
Lee et al. (2002), two types of data stores exist to maintain data warehouses over changing information sources. Those two structures are the View Knowledge Base (VKB) and the Meta Knowledge Base (MKB). While the VKB holds view defini-tions, the MKB records meta data about the underlying information space
EVE -Project, such information has been encoded in the form of several different constraints (Nica et al. 1998), with the sch ema of the underlying relational sources being only implicitly defined through type-integrity constraints.

For the purpose of this paper, we will generalize the notion of an MKB constraint to mean not only the dependencies (such as containment constraints or functional dependencies), but also the schema of the underlying information sources. In this sense, we will describe schemas of underlyi ng information sources by attribute con-straints and relation constraints as well as containment constraints and containment pair constraints. We will begin with a formal definition of meta knowledge, which is necessary for a clear definition of meta data changes. Example 2 gives an example MKB.

Definition 1 (Meta knowledge base). A meta knowledge base MKB is a 6-tuple ( RC , AC , CC , CPC , rel , con ) with the following properties:  X  RC is a finite set of relation constraints { RC  X  AC is a finite set of attribute constraints { AC  X  rel: AC  X  RC is a function that associates attribute constraints AC  X  CC is a finite set of containment constraints { CC  X  CPC is a finite set of containment pair constraints  X  con: CPC  X  CC is a function that associates c ontainment pair constraints
AC i has OID i ). Naturally, those IDs must be uni que for each class of constraints ( AC , RC , CC , CPC ). We do not require a globally unique ID because the four classes of constraints are mutually exclusive and the type of a constraint cannot change.
 a pair ( AC i , RC j ) denoted by AC i  X  RC j as an element of function rel iff rel computes the function value RC j for argument AC i . An analogous syntax applies to function con .

Example 2. Let us assume a travel agency with a national customer database. For performance reasons, the Massachusetts br anch of our travel agency maintains its own customer database, whic h is a subset of the national database. Also, the Boston office has a local copy of customers locat ed in the Boston area, which is a subset of the Massachusetts database. Due to the transitivity of  X  , this means that the Boston database is also a subset of the national cus tomer database (at least when projected onto the common attributes of all three relations). Another information source is a flight reservation database. A possible MKB for this setup is given in Fig. 5, together with a graphical representation.
Changes to the schema of information sources cause changes in the Meta Know-ledge Base (MKB) and generally also in the View Knowledge Base (VKB). This paper describes how to synchronize view definitions with such meta data changes under consideration of previous meta data ch anges in the information space and, in particular, how to make use of added meta knowledge by reinstalling a previously deleted schema element. In previous work (Nica et al. 1998), algorithms were de-fined to adapt views to deletions of information only, but views would not be able to return to previous states when temporarily deleted information would again be available to the view user. In order to precisely specify the meaning of previously deleted element, below, we will introduce the concept of history of view and of meta knowledge (Definition 4).

Definition 2 (Meta data change). Let c be a constraint c see Definition 1. A meta data change, MDC i , is the addition or deletion of c to or from the meta knowledge base, transforming the MKB from a previous state MKB i  X  1 to a subsequent state MKB i . Valid meta data changes are add-CPC (c) , add-CC (c) , add-RC (c) , add-AC (c) , delete-CPC (c) , delete-CC (c) , delete-RC (c) , delete-AC (c) 4 .
 i.e. the constraint has to be instantiated as an object with a new OID. This is an implementation detail that does not affect our algorithm. We will assume for this discussion that new constraints are available when necessary and that unique object
IDs have been assigned to them. We will give a detailed explanation of meta data changes in Sect. 4.6 after some more necessary definitions.

Definition 3 (Inverse meta d ata change operations). Two meta data change oper-ations MDC 1  X  X  add-CPC ( c ), add-CC ( c ), add-RC ( c { delete-CPC ( c ), delete-CC ( c ), delete-RC ( c ), delete-AC change operation MDC i ( c ) by MDC  X  1 i ( c ) .

The MKB stores constraints as objects, which in the cases of AC and RC have names as fields (variables). When a new view is added to the VKB, the view defin-ition is parsed, and the correct AC and RC constraints are identified for each at-tribute or relation mentioned in the view definition. Rather than storing the (current) names of those attributes and relations in the VKB, references to the AC and RC constraint objects are stored.
 because names are simply labels stored with their respective AC and RC constraints in the MKB. When an attribute or relation is renamed, its label in the appropriate
AC or RC constraint description is updated. The view definitions in the VKB are implicitly adapted as well. The (SQL) queries sent to underlying databases or dis-played to the users upon request are constructed from the VKB view definitions, with the attribute and relation names inserted from the MKB constraints. To the
HD-VS algorithm, described in this paper, as well as to the VKB, rename changes are therefore transparent.
We now want to model the history of meta data and views, i.e. the sequence of changes in meta data that have occurred on a MKB and VKB. Because our new algorithm is able to deal with multiple subsequent rewritings of a view, we introduce the notation V ( i ) for the i th rewriting of a view V . In this sense, V with the previously used notation V for a (one-step) rewriting of view V .
Definition 4 (History). Let a view V be rewritten into V change sequence S =[ MDC 1 , MDC 2 ,... , MDC n ] . The history H
V is a sequence H ( V ) =[ N 0 , N 1 ,... , N n ] with N representing the states of the MKB and the view definition at different points in time. Each state N i is a pair ( MKB i , V ( i ) ) and each transition between two states
E = ( N from state N i to state N j . MKB 0 is the original MKB and V with V , the original view definition. N n represents the current state of the system.
An obvious space optimization (at the expense of execution time) on this structure is the storage of only meta data changes rather than the entire MKB at each node and generating the other MKB states at runtime. We are modeling the history in the way described above to simplify the algorithm given later.
 Furthermore, note that, in this definition, each view keeps its own copy of the
MKB. That is also a simplification for the purpose of presentation. In an implemen-tation, one would keep a single MKB history reflecting all changes and refer to the appropriate MKB version from each view X  X  history.
 Example 3. We now introduce an example to be used in the subsequent discussion.
Using the MKB from Example 2, Fig. 6 contains a part of a history of view Cus , namely the original view Cus and two of its subsequent rewritings Cus We abbreviated the views in the figure X  X he actual views are defined as follows:
The original view Cus is defined on an information space containing RC
CC ,and CC 2 . We assume two meta data change operations, namely add-RC (which adds a relation named Customer) and delete-RC ( RC itly generated deletions of AC sand CPC s. The MKB is updated after each of these operations and the view is synchronized (rewritten) at the delete operation (edge E according to the meta information availabl e (using the POC algorithm, Sect. 2.2).
We will now introduce the notion of dependency of views. In an environment holding multiple views (in the VKB), not every view may be affected by every meta data change. In previous view-synchronization algorithms (Rundensteiner et al. 1997; Nica et al. 1998; Koeller et al. 1998; Nica and Rundensteiner 1998b), the rewriting of a view is accomplished with the help of constraints that state (partial) redundancy in the information space. For instance, a c ontainment constraint could provide the information that relation R is a subset of relation S . This information can be used in certain cases to replace relation R by relation S in a view if the former becomes unavailable. The semantics of such a constraint are that the information providers of relations R and S guarantee that the constraint is and remains valid. We define a rewritten view V ( 1 ) to be dependent on a CC -constraint c if c wasusedinthe above sense in the rewriting of a view V .

Definition 5 (Constraint dependent). A view rewriting V straint dependent on a constraint c  X  CC  X  CPC  X  AC  X  RC if one of the following conditions is satisfied: 1. A view rewriting V ( n ) is constraint dependent on a containment constraint c 2. A view rewriting V ( n ) is constraint dependent on c 3. A view rewriting V ( n ) is constraint dependent on an attribute or relation constraint 4. A view rewriting V ( n ) for a view V ( m ) (with n 5. Constraint-dependency for c = CPC k ( AC i . AC j ) has the analogous property as view X  X  constraint dependency on any constraint (point 4 in the definition).
Example 4. In our running example, assume that a view rewriting V
FlightRes ) is constraint dependent on the containment constraint CC i.e. relation Boston is the replacement for relation MassBranch, which was part of
V at an earlier stage. A further meta data change, such as delete-RC relation Boston or delete-CC ( CC 1 ) , can make a V dependent on CC 1 . Other changes, such as delete-RC constraint dependency of V on CC 1 . In this section, we will give the history-driven view-synchronization algorithm (HD-
VS). The main idea of the algorithm is to review the history of a view to find better ways of rewriting a view definition than simply replacing missing elements (relations, attributes) with others. There are two main cases: When a relation or an attribute that is added to the information space has previously been deleted, we can return to a previous view definition. Vice versa, when an element that is deleted from the information space has not been part of the view from the beginning but has only been introduced by a previous view-synchronization step at point i (state N can regenerate view rewritings from point i on, assuming that the recently deleted element has never been available. Our al gorithm uses three main concepts: We use backtracking to review the view X  X  history after a schema change. We then reapply part of a view X  X  change history after backtracking stops and we have explored the necessary subset of the view X  X  history. Last, we reconstruct the history H process of reapplication of changes.
 After a detailed explanation of the algorithm, Fig. 10 (p. 51) shows the complete HD-VS algorithm in pseudocode.

Example 5. Figure 7 gives an idea of the algorithm X  X  main concepts that are de-scribed in detail later in this section. Continuous lines ( while dashed lines ( ) denote actions of the HD-VS algorithm. Four meta data changes are executed on a view V . After each meta data change, HD-VS checks whether the history of the view contains information that could be used to rewrite view V in an appropriate way. After the first delete-constraint change, there is no useful information in the view X  X  history, such that we have to call the POC algo-rithm to keep the view valid. As another view element ( Y ) is deleted in step 2, the same situation occurs. The third operation, an add-constraint operation, also leads to unsuccessful backtracking (in the sense that no previous delete operation is found to cancel out the add operation). In this case o f an addition to the information space, no POC view synchronization is executed (because a view synchronization is not necessary for the view to remain valid). The fourth operation, an add operation in-verse to a previous delete operation, finally leads to successful backtracking. Here, the two operations add-X and del-X cancel out. So the idea of the algorithm is to apply a meta data change sequence to V that contains all operations except those two. The two schema changes remaining in the shortened meta data change sequence (del Y and add U in our example) are applied in the same fashion, but do not lead to further cancellations. Thus, the final view obtained is the new V add-X-operation is still executed on the MKB (i.e. X is added to the MKB), but is not contained in the view X  X  history after this step. View synchronization is performed when a view is affected by a schema change.
Views whose definitions are still valid after the change are not synchronized. There-fore, we will define the concept of affected views.

Definition 6 (Affected view). Aview V with a history of H a meta data change MDC to MKB i iff: 1. a constraint c is added to MKB i ( MDC  X  X  add-CPC , add-CC , add-RC , add-AC } ) and the view V ( i ) was at some point in its history constraint dependent on c but is not now or 2. a constraint c is deleted from MKB i ( MDC  X  X  delete-CPC , delete-CC , delete-RC , delete-AC } ) and the view rewriting V ( pendent on c .

In order to perform view synchronization, we apply a technique that we call backtracking. Recall that the application of a meta data change sequence S [
MDC 1 , MDC 2 ,... , MDC n ] to an original view V leads to a sequence of view rewritings [ V ( 1 ) , V ( 2 ) ,... , V ( n ) ] , with its history being recorded in the history H
The purpose of the backtracking process is to find a view rewriting V from which to reapply or redo a changed meta data update sequence. Backtracking now has the goal to find the most recent node in H ( V ) for which a certain condition holds.

Definition 7 (Backtracking). Backtracking in the history H a meta data change MDC n + 1 to a constraint c corresponds to the traversal of the sequence H ( V ) , starting from state N n through states N lowing condition is satisfied: 1. If MDC n + 1 is an add-constraint operation, V ( i 2. If MDC n + 1 is a delete-constraint operation, V ( i
The backtracking process returns the node N i with the highest index i for which condition.
The motivation behind this definition of backtracking is as follows: If a con-straint c is added to the MKB, we are trying to determine whether this constraint has been used at some point in the history of view V . In this case, c must have been deleted from the MKB earlier and we can cancel the add with the delete of c .
Therefore, we backtrack to the most recent view V ( i ) that used c because V precede the deletion of constraint c . In the opposite case of a delete-constraint opera-tion on constraint c , we look for the last view V ( i ) that was not constraint dependent on c because we can then reapply the remain ing schema change sequence starting from V ( i ) under the assumption that c was never part of the MKB.

Note that backtracking does not primarily involve finding the inverse meta data change to a given change, but rather looks for properties in view definitions. Adding a new constraint to the MKB does not make any views constraint dependent on it, so backtracking after a delete-operation MDC i will generally not reach the inverse of MDC i . Rather, it will stop before the first view that actually used the constraint deleted by MDC i . This is then the place from which we need to reapply subsequent meta data changes.

Also note that backtracking is a very ch eap operation, as each backtracking step involves only checking for constraint dependency (which can be determined based on information in each MKB i ). It is possible to implement backtracking in a variety of ways. In particular, the order in which the history is traversed is not relevant and subject to optimization, as long as the conditions in Definition 7 are satisfied.
Example 6. Figure 8 shows backtracking in a schema change sequence for our run-ning example, with views as defined in Example 3. Assume a meta data change add-RC ( RC 3 ) has removed relation Customer , followed by a meta data change delete-RC ( RC 1 ) that removed Boston . The POC-view-synchronization algorithm would have rewritten V into V ( 2 ) = MassBranch (cf. Fig. 8). This rewriting made use of containment constraint CC 1 ( RC 1 , RC 2 ,  X  ) provided by it to ensure the validity of the new view. By Definition 5, i.e. view rewriting, V ( 2 ) is now constraint dependent on CC 1 .

Let us now assume another meta data change delete-CC of that meta data change are that the information source provider that ensured the superset relationship between MassBranch and Boston does not provide this service anymore. Therefore, it can no longer be guaranteed that the view V
MassBranch will be a superset of the original view V to find the most recent view that is not constraint dependent on CC in our example. The next step in the algorithm will then reapply meta data changes, as discussed in the next section.
The general principle of reappli cation is to generate a new history H changes are removed). The process o f reapplication proceeds as follows. continue to the beginning of H ( V ) (if no view satisfies the stopping condition in Definition 7). We handle the first case here and the latter in the next section. to be applied to V ( i ) to obtain a correct view. For this purpose, we first define a subsequence of S in the following way: S =[ MDC i + 1 , MDC found through backtracking, we now find the meta data change MDC k in H ( V ) such that MDC k = MDC  X  1 n + 1 . Note that, if the inverse meta data change (a delete) exists in H ( V ) , k = i + 1, because a view can only cease to be constraint dependent on a constraint c by deleting c . We now remove MDC then apply S to MKB i and V ( i ) . That is, we apply all meta data changes in S and execute a one-step view-synchronization algorithm for all delete -RC and delete -AC changes, however, without further backtracking. After this reapplication, the history contains a total of n  X  1 nodes (see the note on complex meta data changes in
Sect. 4.6.3). In other words, MDC k (the delete-operation inverse to MDC canceled out by MDC n + 1 .
 quence S on V ( i ) , but on an altered MKB. MKB i is altered by applying the meta data change MDC n + 1 to it, and S is then applied assuming this altered MKB. After applying S , we furthermore find MDC  X  1 n + 1 (an add constraint) and remove it from
H ( V ) .Iftheinverseto MDC will now contain n  X  1 nodes (if the inverse existed) or n nodes (otherwise). In other words, MDC n + 1 is processed by assuming that the constraint deleted by MDC was never available to begin with.
 tracking is necessary. The reason is that all operations that are members of S have been successfully evaluated b efore the application of MDC cellations can be found. Any candidates for cancellation that may remain in S have been processed before, and all other meta d ata changes that led to backtracking have been processed, such that ne w backtracking does not occur.

Example 7. Continuing from Example 6, Fig. 9 now shows the reapplication of the schema change sequence and the reconstruction of the history. Remember that the view-synchronization algorithm backtracked in H ( V ) and found view V most recent view not constraint dependent on CC 1 . We now construct a sequence
S as defined above and obtain S =[ delete-RC ( RC 1 ) ] changed MKB 1 , from which CC 1 has been removed. The only element in S is a delete operation. We can conclude that backtracking beyond that delete operation will not yield better rewritings and therefore apply the POC algorithm on N the changed MKB 1 . POC will use the only remaining containment constraint CC and thus lead to a new synchronized view V ( 2 ) = Customer . The node N deleted from H ( V ) and replaced with a new node (also called N
In some cases, we may backtrack to the b eginning of the meta data change se-quence, i.e. find no view that satisfies the condition from Definition 7. In those cases, we simply add the current meta data change to the view X  X  history and if ne-cessary (i.e. in the case of a delete-RC or delete-AC operation) apply a single-step view-synchronization algorithm. In this case, the view X  X  new history will have n nodes.

Note that a na X ve approach at history-driven view synchronization might be to always restart the entire view-synchronization process from the beginning and apply the complete meta data change sequence after every new meta data change. However, considering that view synchronization is a relatively expensive process (Koeller et al. 1998) and is only defined for one meta data change at a time, this approach is inefficient in practice. In contrast, our algorithm avoids repeating a large number of such view-synchronization steps when it stops backtracking at a node in H
Sect. 5.2, we will show that this strategy saves processing time while retaining or improving on the quality of views.

Figure 10 gives an overview of the complete HD-VS algorithm in pseudocode. We will now discuss the meta data change operations introduced earlier in detail.
Some operations imply others. For example, deleting a relation implicitly requires the deletion of its attributes. As we model relations and attributes separately, we must require that some meta data change ope rations are dependent on others, defined below.

Definition 8 (Dependent constraints). AC i is dependent on RC  X  rel. Likewise, CPC i is dependent on CC i iff CPC i  X  CC each case, the view-synchronization algor ithm will be executed only on the affected views.
For the delete -AC and delete -RC operations deleting a constraint c , backtracking will generally find a view that is not constraint dependent on c . If the original view was already dependent on c , no such view can be found and backtracking will lead to the beginning of the view X  X  history. Then, a single-step view-synchronization al-gorithm (Sect. 2.2) has to be executed on the view V
V n + 1 ) . Note that backtracking to the beginning of H of delete -CC and delete -CPC changes. By our definition of constraint dependency, an initial view V cannot be dependent on either type of constraint and backtracking stops at the most recent view not constraint dependent on c , i.e. in the worst case at the initial view V . Note that backtracking that stops at view V is not considered backtracking to the beginning. Only backtracking that does not stop at any view satisfies this condition.
The cases of add -RC , add -CC ,and add -AC are straightforward. Let us look at an example for an add-RC obtaining an alternative view rewriting V
Example 8. Assume the MKB from Example 2 and an original view V which has undergone two meta data changes: delete-RC ( RC
MassBranch constraint dependent on CC 1 and RC 2 and delete-RC ( RC ing to V ( 2 ) = Customer constraint dependent on CC 2 and RC neither of the two containment constraints is deleted from the MKB. Subsequently, letusassumeametadatachange add-RC ( RC 2 ), i.e. relation MassBranch is coming back into the inf ormation space. Because CC 1 still holds, we can intuitively just return to V ( 1 ) to obtain a valid view. Algorithm HD-VS achieves just that by backtracking to the node containing V ( 1 ) because this is the most recent view con-straint dependent on relation MassBranch and reapplying the remaining meta data changes. In this case, this is an empty sequence because S is empty.
For the complex add operations add-RC and add-CC , subsequent explicit additions of dependent attribute or containment pai r constraints are requi red because relations must have attributes and containment constraints must have attribute pairs. Likewise, the deletion of relations by delete-RC or containment cons traints by delete-CC re-quires the removal of dependent constraints. For delete operations, our system will automatically remove those dependent constraints (cf. Fig. 11).

Similar to the cancelled operations themselves, no view synchronization will have to be executed for any of those operations because correctness is assured by the synchronization for the parent meta data changes. However, these constraints must be removed from the MKB when their parent constraints are removed to maintain consistency. For readability reasons, our algorithm in Fig. 10 does not contain pseu-docode for this case, as this is really an implementation issue. The algorithm would need to be adjusted in that the returned new history actually may contain more or less than n  X  1 nodes but those additional or m issing nodes will all be additions or deletions of constraints dependent on the RC i or the CC the current view-synchronization step. Thos e dependent constraints are simply re-moved from or added to the MKB before the HD-VS algorithm is executed. View synchronization will then proceed as described in Fig. 10.

Figure 11 shows a summary of the dependencies of the different meta data update operations, i.e. the updates to H ( V ) , rel, con, AC , RC , CC ,and CPC that need to be performed in addition to the original change and the removal of its inverse change. As mentioned before, dependent me ta data change operations are executed on the MKB before the view-synchronization step triggered by the original meta data change is executed.
 trigger removals of containment constraints that are defined over that RC  X  X  relation.
This ensures that a relation that is temporarily unavailable can be reintroduced into the information space, such that the view synchronizer can recognize it as, in some sense, the same as the relation deleted earlier. If a relation is completely removed from the information space and its containment relationship to other relations will no longer be maintained, the appropriate CC has to be deleted from the MKB ex-plicitly. Therefore, it is not an inconsistency in the MKB if it contains a CC that refers to nonexisting RC s. However, we require that any RC that is referred to in an existing CC must have at some point existed in H meaningful semantics for the OIDs of those RC s.

Example 9. To illustrate our approach for handling possibly complex changes, we now give an example of MKB evolution under such a complex meta data change.
Consider the state of the MKB as defined in Example 2 (cf. node N
We execute the operation delete -RC ( RC 1 ) on that MKB. By Fig. 11, this will trig-ger the removal from H ( V ) of all add and delete operations concerning the three constraints AC 10 , AC 11 and AC 12 . Those are the constraints related to RC function rel. Those change operations are simply removed from the history. That is, no view synchronization algorithm is executed. The function rel is updated to a function computing the same values as before for all AC s except the deleted ones (for which it is undefined). In addition, the t hree attribute constraints mentioned are removed from the set AC .
While we allow a user to explicitly rename an attribute or a relation by changing the name field in the corresponding meta data object ( AC or RC ), it is still possible that a user might attempt to rename an attribute or relation through the deletion of a constraint, followed by an addition of the same constraint with a different name label. In the HD-VS algorithm, such a sequence of operations would cancel out through backtracking, as long as the newly added constraint is identified to be the same as the old deleted one. The only effective change would be a change of the label on an AC or RC . The notion of sameness is given by the object identity of the constraint, e.g. add-RC ( RC 1 ) cancels out delete-RC ( RC assigned to this constraint has changed be tween add and delete. If the sameness is not explicitly specified by the user during the addition of the new constraint, then the system will treat the delete and ad d as unrelated and proceed accordingly.
In this section, we prove that the HD-VS algorithm produces a valid view (in a sense to be defined shortly) and we also illustrate that HD-VS is in fact an improvement on previous view-synchronization strategies in terms of the quality of the view that can be generated.
HD-VS satisfies two invariants that hold after each run of the algorithm: 1. Applying all meta data changes in H ( V ) to the initial MKB will yield the current
MKB state MKB n . 2. At every node N i in H ( V ) , the rewritten view V
MKB i without errors. We refer to this property as validity , formally defined be-low.

The first invariant is easy to see because the meta data changes in H the actual changes that occurred in the info rmation space minus the cancelled inverse operations and their dependents. On the o ther hand, the second property requires a closer look.

As a notational issue, recall that each meta data-change MDC exactly one constraint of type AC , RC , CC ,or CPC . In order to simplify our definitions, we will refer to this constraint as constraint below.

Definition 9 (Valid view). Aview V ( i ) is valid over an information space defined by MKB i if all constraints on which V ( i ) is constraint dependent (Definition 5) in fact exist in MKB i .

Definition 10 (Valid meta data change). A MDC i is valid over MKB  X  constraint ( MDC i ) exists in MKB i in the case of a delete -meta data change, or  X  constraint ( MDC i ) does not exist in MKB i in the case of an add -meta data change.
Theorem 1. Using HD-VS, view synchronization of a valid rewriting V a valid meta data change MDC i + 1 that does not lead to backtracking will lead to a valid rewriting V ( i + 1 ) .

Proof. View synchronization as introduced in this paper is an operation defined on arbitrary SPJ-views V ( i ) , as long as they are valid over the current state MKB the meta knowledge base. Any meta data change that does not lead to backtracking will either do only trivial changes to the information space (adds that do not affect the views or renames that simply change labels in constraints) or defaults back to a single-step view-s ynchronization algorithm accordin g to Sect. 2.2. Those algorithms have been proven to produce valid views under any given schema change (Nica and Rundensteiner 1998a). Therefore, a valid meta data change that does not cause backtracking will lead to a valid view rewriting. 
Theorem 2. Assume a meta data change sequence S =[ MDC
MDC n ] that led to a valid view rewriting V ( n ) and assume all previous MKB and V ( j ) with j = 1 ,... , n  X  1 were valid as well. Furthermore, assume another meta data change MDC n + 1 that leads to backtracking in H V to view rewriting V in Sect. 4.4 on the view V ( i ) defined over MKB i rewritings [ V ( i + 1 ) ,... , V ( j + 1 ) ] (with some i
Proof. We prove by considering the different views V meta data changes. Note that V is trivially valid. We consider a meta data change
MDC n + 1 and assume that sequence S consists of meta data changes that previously led to valid view rewritings V ( 1 ) , V ( 2 ) ,... ,
Definition 10). We now show that the addition of MDC creation of invalid views.

Then, all change operations on any of t he constraints that are dependent on MDC by Definition 8 are removed from S . After a meta data change MDC to Sect. 4, there are then two ways of applying the correct S to the view V 1. After add-constraint operations ( add-c ): 2. After delete-constraint operations ( delete-c ):
V ( i + 1 ) constraint dependent on c .Rather, c must have been added at an earlier stage, before V ( i ) ,i.e. MKB i must contain c as well.

Thus, we can apply MDC n + 1 to MKB i .Then S is applied on MKB without further backtracking. By Theorem 1, this sequence of valid meta data changes will then lead to a sequence of valid views. Furthermore, the inverse to MDC n + 1 will be removed from H ( V ) if found. Let this inverse be MDC Note that none of the views V ( k ) ... V ( i ) can be constraint dependent on c because
V ( i ) is not constraint dependent on c and no older view has this property either (because that would imply the e xistence of another delete-c operation, which is a fallacy). Therefore, the removal of MDC k , should it occur, would not lead to invalid views.

Because S creates valid views, the new history H ( V views. The valid meta data changes representing the transitions between states in
H ( V ) form the new and valid sequence S := [ MDC 1 , MDC 2 ,... ,
In order to evaluate the usefulness of our approach, we have to assess if this view-synchronization process produces views that better satisfy users X  needs. The previous approach at view synchronization (Sect. 2.2) takes only delete-capability changes into account and ignores adds, which causes views to deteriorate over time and become increasingly different from their originals.

Figure 12 illustrates this issue by an example of a meta data change sequence that uses backtracking. Previous approaches (Rundensteiner et al. 1997; Nica et al. 1998; Nica and Rundensteiner 1998b; Koeller et al. 1998) would simply ignore the add-capability change. Here, instead, we also consider the cancellation of the del-X operation, and subsequently reapplying del-Y and del-Z will lead to a higher quality view  X  V ( 3 ) than simply ignoring the add-X operation, obtaining V would have to compare the quality of those two alternate rewritings, to the user. Quality is measured by the Q value (Sect. 2.3, fully defined in Lee et al. (1999b)). In this example, we would have to evaluate whether Q
Q ( V ( 4 ) ) . Generalized, the question has to be answered if the quality Q a view rewriting will improve when the history-based algorithm described in this paper instead of a one-step algorithm is applied. We will show this separately for add-and delete-meta data changes.

Theorem 3. Let V ( n  X  1 ) be a view rewriting of view V obtained after a meta data change sequence S =[ MDC 1 , MDC 2 ,... , MDC n  X  1 ] , and let an add change
MDC n be the next meta data change. Then, a view rewriting the history-based view synchronization will have a higher or equal quality Q than a rewriting V n obtained with previous view-synchronization algorithms as de-scribedinSect.2.2,i.e. Q ( V ( n ) 2 )  X  Q ( V ( n ) ) .

Proof. Note that a comparison is only meaningful for an X CC and CPC changes are not even handled by the one-step approach (and thus HD-
VS will always do better). Assume a meta data change sequence S [
S ]+[ add -X ] with | S |= n and S a sequence that does not contain add -X or delete -X for some constraint X . Applying S on a view V leads to a number of view rewritings. The one-step view synchronizer described in Sect. 2.2 would apply the
POC algorithm after each schema change, including MDC a sequence of rewritings [ V ( 1 ) ,... , V ( n ) ] . HD-VS cancels MDC therefore applies only S (reverting back to POC for some or all steps in S )inorder or better, i.e. that Q ( V ( n  X  2 ) 2 )  X  Q ( V ( n ) ) to a rewriting in only one algorithm (the one-step approach) but not in the other. In the one-step algorithm, MDC 1 = delete -X could lead to a view rewriting V which the algorithm (based on POC) eventually arrives at a view V of which is higher than the quality of the best V ( n  X  (using POC at its different steps) without applying MDC this is not possible.
 exactly one relation or attribute Y or drops X if no replacement is found (Nica and
Rundensteiner 1998b). If X is replaced by Y , a containment constraint CC must have been used to justify this rep lacement (by definition of POC). There are now three cases for the remaining meta data change sequence S , depending on the existence of the operation delete-Y in S : 1. | S |= 0. In this case, HD-VS does not execute any view synchronization op-2. If S does not contain meta data change delete-Y , both algorithms perform the 3. If S contains a delete-Y operation, POC drops or replaces Y with Z given the POC case). HD-VS can also find a rewriting that does not drop X at all, but only Y . The latter cannot be found by POC and gives HD-VS an additional choice for a rewriting. If X is dropped by POC, the resulting rewriting must be worse than any rewriting that replaces X with another element. Therefore, with the previous argument, the resulting V ( n ) must also be worse than Theorem 4. Using the notation from Theorem 3, the HD-VS algorithm achieves
Q (
Proof. In the case of delete changes ( delete -CC , delete -CPC , delete -AC , delete -RC ) the HD-VS algorithm produces the exact same views as the one-step POC-based algorithm in all comparab le cases, namely, for delete-AC and delete-RC . Note that deletion of CC and CPC is not handled in the one-step approach, so a comparison is not meaningful. 
In summary, the HD-VS algorithm achieves the following:  X 
In cases where a relation or attribute that has been deleted is reintroduced into the system, HD-VS will generate a view, the quality of which is better than or as good as the quality of any view that a one-step algorithm could produce. If a relation has been deleted, HD-VS will consider two different ways of synchro-nizing the view and choose the better one, where previous algorithms considered only one way.  X 
In the case of other deleted meta information (containment constraints), one-step algorithms will not recognize such a change, in effect keeping the same view.
This would not be meaningful anymore becau se containment relationships in the view X  X  data can no longer be guaranteed. HD-VS will correctly adapt views to those deletions and produce the best vi ew consistent with the available meta information.  X 
In the case of additions of meta information without previous deletions, one-step algorithms will not perform any synchronization. HD-VS will attempt to make use of such additional information, thus potentially leading to improved views and, in the worst case, keeping the quality of rewritings equal to the level achieved in previous algorithms.

In summary, the HD-VS algorithm will never perform worse than the one-step approach, but in many cases better.
A prototype dynamic data warehouse has been developed by us at the Database Sys-tems Research Group (DSRG) at Worcester Polytechnic Institute (Rundensteiner et al. 1999). The EVE graphical user interface, the MKB, the MKB evolver, the VKB, and the view synchronizer are implemented using Java. The participating ISs are all assumed to be JDBC-compliant SQL databases, with the communication between
EVE and the information space being via JDBC. The view-synchronization mod-ule currently implemented in the system performs (one-step) view synchronization using the POC algorithm described in Sect . 2.2. Versions of this system have been demonstrated at ACM SIGMOD  X 99 (mainly the view-synchronization capability) and ACM SIGMOD  X 01 (mainly the concurrency handling of both data and meta data changes).
 per and materializes its views through a query engine. MKB evolver and MKB con-sistency checker modules track schema changes and adapt the MKB accordingly.
A view synchronizer module rewrites views, a view maintainer updates view extents after rewritings, and a concurrency control module (Chen et al. 2001, 2002, 2004) tracks and resolves concurrency issues between data and schema updates. The EVE system also uses a quality-and-cost (Q C) computation module (Lee et al. 1999a), inferring approximate view qualities and costs from prerecorded system parameters (such as relation sizes and join selectivitie s). It is not necessary for the QC evaluation to store view extents with each version of the view.
 tem (Rundensteiner et al. 1999). The HD-VS implementation augments two mod-ules (View Synchronizer and MKB/VKB data store) with HD-VS-specific extensions.
Thus, building a system for history-driven view synchronization is relatively straight-forward because most of the necessary modules already exist. Figure 13 shows the
EVE -architecture augmented by the necessary changes for the history-driven view-synchronization algorithm (adapted modules shaded).
 bines the data stores of VKB and MKB becau se HD-VS builds sequences, the elem-ents of which are pairs (MKB, V). The history manager is responsible for keeping all necessary historical states of the VKB and MKB as well as for providing opera-tions to support backtracking and reapplication and reconstruction of the history. An
HD-VS view synchronizer wraps the old one-step view synchronizer (e.g. running
POC) and uses it when necessary. It also makes use of the QC computation module in order to decide about the quality of the g enerated views and interacts with the history-manager during synchronization.

When a view synchronization becomes necessary, the HD-VS module executes the algorithm HD-VS (Fig. 10), which may include building new states in the his-tory. Each new node contains a snapshot of VKB and MKB, respectively, as well as a list of constraints on which each view in the VKB is constraint dependent.
Note that we do not store data, only meta data, in each node. At the end of the view synchronization process, we obtain a valid query, i.e. a query that can even be executed against the current state of the information sources. Thus, the extent of the data warehouse at that point can be recomputed. If a previous extent of the data warehouse is available, the current extent can be incrementally determined by view adaptation (Nica and Rundensteiner 1999).

One obvious trade-off of the implementation is whether to store complete snap-shots of the MKB or only the schema cha nge that leads to each node. In the latter case, we only maintain the differential action that led us to transition from one state formance of the algorithm is expected to be similar for both implementation choices.
The system described in this paper has the requirement on DBAs to supply some information for the system. In particular, participating data providers have to supply schemas as well as containment information and guarantee that this information is correct at all times. Also, when an informa tion provider deletes one of its information sources, it must reidentify the source during a subsequent add as the same as before.
This can be accomplished by presenting a lis t of previously deleted information sources and their constraints to the prov ider. The provider can then select among those sources and constraints or can also create new sources and constraints, for which the system will generate new identifiers. In summary, substantial aids can be implemented for a participating DBA to facilitate providing data to the system.
In order to evaluate the system, we have conducted a case study on an example information space. The inf ormation spaces involved were modeled similarly to the running example in this paper. They captu re data of different travel agencies, each holding different subsets of data. The relations involved included Customer(CName, Age, Street, City, State) , MassBranch(CName, Phone, Age, Street, City, State) , Flight-Res(PName, Airline, FNo, Source, Dest, DepartDate) , CarRental(Name, Address, City, State, Phone) ,and BBHotel(Name, Address, City, State, Phone) ,where Mass-
Branch (CName, Age, Street, City, State)  X  Customer(CName, Age, Street, City, State) and BBHotel (Name, Address, City, State, Phone)  X  Hotel(Name, Address, City, State, Phone) . An example view, PotentialCustomer, is given in Fig. 14.
 This information system was subjected to a number of schema changes using the
EVE system with QC evaluation. Whenever the POC view-synchronization algorithm was executed, QC computation was used to select the highest ranking view rewriting for a set of given parameters. For example, after a sequence of 7 schema changes adding, deleting, and renaming attributes and relations, out of which 2 delete-attribute changes (on MassBranch.Phone and MassBranch.Street ) affected view PotentialCus-tomer, the view definition in Fig. 14, below, was obtained. According to the QC (quality and cost) model in EVE, this view has a QC value of about 0.48 relative to the original view (where the QC value is a number between 0 and 1, and 1 repre-(with abbreviations M = MassBranch , F = FlightRes , and C sents a view identical to the original). After an additional schema change readding the attribute MassBranch.Street to the information space, the system rewrote view PotentialCustomer to be essentially identical to the original view, minus the missing Phone attribute, and achieved a QC value of 0.765.
 attribute, the POC algorithm has the choice between dropping the attribute from the view (and thus reducing the view quality) or replacing the attribute from the
Customer table (and thus incurring substantial additional view maintenance cost due to the join with the Customer table). Th erefore, POC might choose to drop the attribute from the view.
 is chosen by POC to replace the missing MassBranch table, the Street will still be missing from the view. In this case, the HD-VS algorithm ensures through its backtracking and cancelling ac tions that the Street attrib ute will be reintroduced into the view. This now improves its quality. Thus, unnecessary deteriorations of views introduced by the greediness of the single-step view-synchronization algorithms are prevented by HD-VS.
As illustrated above (and earlier in Sect. 5), the algorithm HD-VS (Fig. 10) provides significant improvements of the applicability and versatility of view synchronization.
We now discuss its overhead, namely its potential increase in view-synchronization time. Most operations in HD-VS are cheap: b acktracking is only a linear traversal of a typically relatively short sequence, and updating the schema change history consists of simple MKB updates. The HD-VS algorithm also has no loops. The main increase in complexity is due to the reapplication of meta data change sequences, which leads to more single-step view synchronization operations than previously.
 the expensive operation of view extent recomputation (or adaption) (Nica and Run-densteiner 1999) does not have to be executed until the meta data change sequence has been completely reapplied. Therefore, a sequence of meta data changes can be executed fast. In the worst case, the runtime is the sum of runtimes for synchronizing all views in H ( V ) using the original view-synchronization algorithm. We have meas-ured how much additional runtime is require d for the additional view-synchronization steps in our system. We have observed that a single schema change in our example system could be processed in about 50 ms per affected view (including all calls to the POC view-synchronization algorithm). That is, in the worst case, the n th schema change incurs about ( n  X  1 )  X  50 ms of additional runtime per view. This is minimal cost compared with the original cost of the recomputation time of a single view, which has typically been in the order of several seconds to minutes.
Thus, for most applications, this minimal (one-time) cost in terms of added run-time performance is well worth the potential gain of having identified a higher quality view that can be of a long-term value by better meeting the needs of the application.
In short, whenever the increase in versatility is more important to an application than this small additional processing cost for view synchronization, the HD-VS should be used.

It is important to note that, whenever HD-VS produces the same result as previ-ous view-synchronization algorithms, no additional single-step synchronizations are executed. The only additional step is backtr acking. This is extremely cheap, as it requires a simple backwards traversal of in-memory history list, and thus can be ignored in practical settings. Only whe n HD-VS produces views that could not be found by previous algorithms, may its view-s ynchronization step take additional time over the simpler algorithms. In addition, our prototype implementation leaves much room for future performance improvements, as we have developed this prototype first as proof of feasilibity rather than tuning it to be maximally efficient for deployment.
Materialized views over distributed in formation sources have been explored for a number of years. Early work focused on questions of materialized view main-tenance under data updates in the sources (Blakeley et al. 1986; Gupta and Mu-mick 1995; Quass and Widom 1997). Work on maintenance of mediator-based (con-strained) heterogeneous database systems was done, for example, in Lu et al. (1995).
Questions of optimizing view queries given varying parameters or capabilities of un-derlying sources have also been explored. Generally, work in this area assumes that the rewritten view query computes a view extent equivalent to the original one (Jarke and Koch 1984; van den Berg and Kersten 1994; Agrawal et al. 1997).
Papakonstantinou et al. (1995, 1996) are pursuing the goal of information gath-ering across multiple sources. Like our meta knowledge model, their data model allows information sources to describe their capabilities, but the authors do not as-sume that these capabilities could be changed and thus do not address the view-synchronization problem. The same authors have also done work on query rewriting without using views, for example, in capabilities-based query rewriting for mediator-based systems (Papakonstantinou et al. 1998), in which queries are formulated based on query capabilities of underlying sources.

Some work has been done on rewriting queries using materialized views (Levy et al. 1993, 1995a, 1996; Levy and Sagiv 1992; Rajaraman et al. 1995; Srivastava et al. 1996; Rajaraman and Ullman 1996). This work deals with rewriting queries into equivalent ones using underlying views. Cohen et al. (1999) discuss the problem of rewriting aggregate queries using views.

Work on the world-view concept by Levy e t al. is closely related to ours in terms of its goal, but not the approach taken. In Levy et al. (1995b), the notion of the world view is introduced as a global, fixed domain model of a certain part of the world on which both information providers and consumers must define views. This work is in some sense an approach inverse to ours (Rundensteiner et al. 1997). Where Levy et al. describe information sources in terms of a world model, we incrementally es-tablish our world model in terms of the available sources. The model by Levy et al. provides an elegant solution to a subs et of our problem. It is nevertheless neces-sary to establish a world model before any source can provide information. Changes to the world model are not possible in this approach or would require a manual re-definition of both information providers X  source descriptions and consumers X  queries.
Another drawback of the approach is the insufficient handling of redundancy in the information space. If two information pro viders define partially overlapping view extents, Levy X  X  algorithm finds the minimal cover for the queried data, i.e. uses in-formation from a randomly picked information source that satisfies the user X  X  query.
In contrast with this approach, we can make use of known overlaps of source data to provide nonequivalent rewrites of queries in the case of the possible unavailability of one of the sources and can also assess a user X  X  intent through our user preference model E-SQL. With the help of a quality measure (QC value (Lee et al. 1999b)), we can also decide which of a number of given information sources provides the best answer to a query.
 1994) provide similar approaches as Levy et al., which solve similar problems, but neither SIMS nor SoftBot address the particular problem of evolution under capa-bility changes of participating external information sources.
 tion of relaxation of the query extent, similar to our E-SQL view-specification lan-guage (Rundensteiner et al. 1997). Chu et al. established an SQL extension called
CSQL (cooperative SQL), which relaxes the strictness of SQL where conditions, i.e., it relaxes restrictions on the extent, but not the interface, of a view query, as E-SQL does.
 siliou 1997; Jarke et al. 1998; Jeusfeld et al. 1998; Quix 1999) addresses many prob-lems related to the quality of data warehouses. In the ConceptBase system, using a language called O-Telos, many issues of data warehouse quality and evolution are addressed, such as basic (data) view maintenance (Staudt et al. 1998) and the assessment of data quality (Vassiliadis et al. 1999). In the latter paper, quality is defined as  X  X itness for use, X  which is very similar to our definition of  X  X sefulness for a user X  used throughout the EVE Project. The goal-question metric adopted in the DWQ project is somewhat more sophisticated than the E-SQL preference model (Lee et al. 2002). It could be used in our algorithm as long as single-step view-synchronization algorithms are provided that take that metric into account. tions related to data warehouse evolution (Vassiliadis et al. 1999, 2000, 2001). How-ever, to the best of our knowledge, the work does not provide algorithms to accom-plish such evolution. In Vassiliadis et al. (2001), some basic rules for the adaptation of view queries to some source changes are given, but no comprehensive analysis is provided.
 any component of a data warehouse management system into a meta repository.
Such changes may include the addition or removal of a view by the (human) data manager. Thus, like EVE , they make use of a meta repository in support of data warehouse evolution. However, their focus is on the methodology and management of the process of the meta repository design to assure quality of a data warehouse in terms of software engineering issues. The problem we tackle here of developing algorithms for generating nonequivalent view rewritings over evolving warehouses has not been addressed in the DWQ project.

Another, more recent, interesting dev elopment is the notion of peer database-management systems, also developed at the University of Washington (Halevy et al. 2003). PDMS arise from the notion of P2P file-sharing systems and add database functionality to such an architecture. A query-reformulation algorithm rewrites view queries based on the information content of the source that happen to be available approach, to ours. It solves a different problem (namely the modelling of source capabilities rather than the question of how to adapt to source changes once the system is established).

To the best of our knowledge, there is no other work that addresses the problem of canceling out adds and deletes in an evolvable view environment such as ours.
Work by Duschka (1997), Genesereth et al. (1997), Manolescu et al. (2001) and the work by Levy et al. mentioned above requires the existence of a world schema that never changes, such that schema or capab ility changes in underlying databases are in some sense hidden from the views.

Initial work on this topic has been published in DaWaK 2000 (Koeller and Run-densteiner 2000).
In this paper, we present an approach at view synchronization under meta data up-dates. With our algorithm, a view that is defined over a given set of relations and at-tributes and other meta knowledge that is defined in a Meta Knowledge Base (MKB) can be synchronized with additions and deletions of elements in that MKB. Previ-ous approaches (Rundensteiner et al. 1997; Nica and Rundensteiner 1998a, 1998b) to this problem could only adapt views to deletions in the information, but not to additions. Hence, for example, temporary unavailability of information sources could not be accounted for. The main contributions of this work are (1) formal founda-tions of view synchronization, including the precise definition of meta data changes, (2) the investigation of the effects of m eta data changes on meta data (i.e. depen-dencies between meta data entities), (3) the definition of the notion of history of views and meta data, leading to versioning of both, (4) the definition of a back-tracking algorithm making use of the view X  X  history for future view rewritings and (5) a proof of correctness and assessment of the improvements of that algorithm for view synchronization over previous approaches.

The technology presented in this paper builds on previously described view-synchronization algorithms and is able to directly make use of them. It is more powerful than the previous approaches, yet simple to implement given an existing implementation of one of the single-step view-synchronization algorithms in the liter-ature. This technology can be applied to views over distributed information sources, especially as those sources are independe nt and prone to change their schemas or query capabilities. We have extended our current EVE demo software (Rundensteiner et al. 1999) to incorporate the strategies presented in this paper in a prototype data warehouse system.

