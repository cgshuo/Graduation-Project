 1. Introduction
Today 98% of all computing systems are embedded systems [1]. Frequently cited examples are sensors, smartcards, and cellphones. Applications for these systems have different requirements on data management, ranging from simple data stor-age functionality, over stream processing, to complex data management including transactions, recovery, and replication.
Separation of data management and application logic is needed to avoid redevelopment of data management in these sys-tems. This can be achieved with a general data management infrastructure [2].
 the example of automotive systems. The amount of data that is processed in automobiles increases by 7 X 10% per year [3].
Data is captured in sensors and distributed and stored in working or persistent memory. Depending on the application sce-nario, different data management functions are needed including transaction management, query processing, and security mechanisms. The data stored in such systems ranges from single values and simple structures like arrays, as in sensors [4], to few tables or complete databases, as in navigation systems. Also advanced mechanisms like transaction processing or recovery are required in certain situations. The actually needed functionality depends on the application scenario and data in EEPROM. 1.1. Scale invariance of data management
As in automobiles, data management is required in most computing systems. In contrast to contemporary desktop and server systems, the resources of embedded devices are very limited. This includes memory, computing power, and power consumption. When developing data intensive applications such constraints have to be taken into account.
The memory limitations in embedded systems range from a few kilobytes (e.g., in sensors) to moderate restrictions of a operate under restricted resources. This trend seems to be continued with ubiquitous computing [5] and in the future with developments such as smart dust [6]. It may continue up to the smallest possible devices limited by the currently known physical laws [7]. We formulate this trend as the law of scale invariance of data management :
There will be always small computing devices that operate with very constrained resources and independent of the size of these systems there is a need for dedicated data management. 1.2. Data management for embedded systems
Considering the limited resources and very special requirements on data management, traditional database management systems (DBMS) are inappropriate for use in embedded environments [8 X 11] . There have been approaches that try to scale down data management technology for embedded systems [9,10] . These approaches concentrate on supporting special hard-ware or application scenarios by manually creating customized solutions. When developing such customized systems, data management is often reinvented to satisfy computational and memory constraints as well as new kinds of requirements gue that appropriate techniques are needed to develop tailor-made DBMS that attain high customizability and reuse without loss of performance.

Customizable software can be built with a number of different approaches such as components or preprocessors [12]. All these approaches have benefits and drawbacks. For example, components allow to modularize functionality but often de-grade performance [11]. Furthermore, the crosscutting structure of some features hinders the use of components [13]. For example, encapsulation of a transaction management system as well as a B-tree into dedicated components is difficult to achieve because the transaction management system cuts across many other components of a DBMS including the B-tree.
Preprocessors, e.g., the C preprocessor using #ifdef statements, do not have these problems but are known to pollute cessors are an optimal solution to build applications for embedded systems and new programming paradigms might be em-ployed [13,16] . 1.3. Contribution
We argue that feature-oriented programming (FOP) [17,18] is a promising technique to develop highly customizable DBMS for embedded systems in order to avoid the problems mentioned above. Using FOP, the functional requirements on a DBMS erating different DBMS variants in a short period of time, and (iii) decreases resource consumption by including only required functionality. We also show that, in contrast to other approaches, the customizability has no negative impact on performance and resource consumption. We evaluate our approach by a refactorization and analysis of Oracle X  X  embedded increase performance by about 16% for a reading benchmark. We could achieve these improvements while increasing custom-izability, i.e., we provide customizability also of small features in the refactored DBMS. 2. Tailor-made data management systems
The development of tailor-made data management software has been in the focus of research in recent years. Approaches of DBMS. Especially for embedded systems there are a number of manually tailored data management systems that address special application scenarios. For example, Bobineau et al. have developed PicoDBMS, a DBMS that supports special algo-rithms for smartcards [9]. Sen et al. have developed DELite, a data management system to be used on embedded devices [10]. They propose special storage mechanisms and query processing. TinyDB is a DBMS intended for sensor networks tomizability in the embedded domain. However, such systems do not provide a general approach for customization that al-such systems and analyze their benefits and drawbacks. 2.1. Component-based approaches
Component-based approaches are getting popular for development of DBMS. They allow to encapsulate functionality of components introduce an overhead to support dynamic composition and require an infrastructure to manage components develop and maintain; however, when decreasing the size of components the communication overhead further increases grained customizability as proposed by Geppert et al. with KIDS [20]. Hence, using components on embedded devices is lim-ited to special use cases because they do not address the resource constraints of embedded systems and do not provide the required customizability. 2.2. Components and embedded systems
In order to overcome these limitations, component systems can be combined with approaches that provide fine-grained is hard to encapsulate in a component. With COMET, Nystr X m et al. provide a component-based approach that uses AOP to et al. examined AOP for DBMS customization by modularizing crosscutting code into aspects [16]. They evaluated their ap-proach using Berkeley DB but have shown only customizability for small parts and not the whole system. Unfortunately, none of these approaches (and there is no other approach that we are aware of) could show concrete implementations of a complete DBMS nor detailed evaluations. Furthermore, the customizability of these approaches is very limited (less than 2.3. Preprocessors As stated above, also preprocessors can be used to create customizable DBMS as it is done in Berkeley DB, a customizable
DBMS written in the C programming language. The Berkeley DB developers use fine-grained customizability. In contrast to components, preprocessors do not introduce additional overhead because the composition of a concrete DBMS occurs before compilation and does not hinder compiler optimizations. Furthermore, also we would like to point out that preprocessor statements are known to have a negative affect on maintenance of software lematic [15]. 2.4. Kernel systems, frameworks, and others Beside approaches that can be used on embedded systems, there are approaches to provide customizable or extensible
DBMS intended for server systems. For example, Batory and Thomas used code generation to customize DBMS [24]. They concentrated on customizing DBMS with Genesis [25]. In contrast to FOP, as it is known today, there was no support for object-oriented programming (OOP) and the complexity of the technique decreased usability. There have been many other developments to support extensibility of DBMS in the last 20 years. These approaches found their way into current DBMS (e.g., kernel systems) but cannot provide appropriate customizability to support embedded systems. Other disadvantages include detailed knowledge that is needed to implement a concrete DBMS or extend existing ones (e.g., kernel systems, frameworks) [21].
 2.5. AOP and infrastructure software AOP has shown to be appropriate to provide customizability, e.g., in operating systems [26 X 28] and middleware [29 X 31].
These studies show that AOP can be used to decompose infrastructure software with respect to crosscutting features. Eval-uations of these solutions show that AOP can be used with negligible impact on performance and resource consumption.
In this paper, we present an approach that focuses on software product lines and is based on FOP, which is similar to AOP but provides support for modularizing the features of software. We will show that FOP can also be used to separate cross-cutting concerns without negative impact on performance. AOP and FOP are conceptually different and some studies propose that collaboration based designs like FOP should be preferred when developing customizable software [32 X 35] . 3. Methodology
In this section, we introduce feature-oriented programming (FOP) [18,17] , a programming paradigm for the development of customizable software. With FeatureC++ [36] we developed an FOP language extension for the C++ programming language.
This allows us to apply FOP to software systems intended for resource constrained environments. Software development based on features was applied successfully in different domains [35,37 X 40,31,27,41 X 43,30] , but there was less research regarding the application to data management or embedded systems [44]. We will demonstrate that FOP can be used to de-velop customizable data management software that satisfies the requirements of embedded systems. 3.1. Feature-oriented programming ples for features of a DBMS are transaction management or query processing. But there may also be smaller features like a lock protocol used to implement a transaction management system. The idea behind FOP is to modularize the features of an
SPL and generate programs by selecting only required features. 3.1.1. Feature modules
A feature module implements a feature as an increment in program functionality [18]. In FOP, feature modules are kept separate from each other to comply with the principle of separation of concerns [45]. FOP supports implementing different ules can be freely composed to derive different programs which is the reason for scalability of the approach.
A simple example of a feature-oriented DBMS design is given in the left part of Fig. 1 . It shows a base program and two features T RANSACTION and R EPLICATION . Basic database functionality is implemented in module B T
RANSACTION and R EPLICATION that implement transaction processing and replication functionality of a DBMS. Based on this matically by composing the required feature modules. In our example, possible DBMS support transactions as well as rep-lication  X  DB TxnRep  X  , only basic functionality, or one of the features (e.g., DB 3.1.2. Decomposition of classes
FOP can be used as an extension of object-oriented programming (OOP) . When considering features of object-oriented soft-tionality is separated from feature-specific functionality. Classes in module B R EPLICATION , which we denote by an arrow. In this example, the basic implementation of class feature T RANSACTION , which is in contrast to class DB that is refined in both features. Features T across the entire source code of a DBMS and are called crosscutting features [23]. 3.1.3. FeatureC++
For refactoring Berkeley DB we use FeatureC++, 2 a feature-oriented programming language [36,46,47] . As FeatureC++ is extended by refinements in features T RANSACTION and R EPLICATION refinements can introduce new members (Lines 7 and 19) and extend existing methods (Lines 9 X 15). In method extensions from a generated program to reduce its binary size, the amount of needed working memory, and to increase performance. For example, the code in Lines 5 X 16 of Fig. 3 is not present in a DBMS if feature T selection process, the code size of class DB is reduced and also the size of instantiated objects can be decreased. FeatureC++ source code is processed by the FeatureC++ precompiler. It uses a source-to-source transformation from FeatureC++ code into C++ code which is then compiled by an ordinary C++ compiler. An example for the code transformation of class DB (cf. Fig. 3 ) is shown in Fig. 4 . Depending on the feature selection, different variants of class For example, a simple variant (Fig. 4 b) is derived by using only the implementation defined in module B
The FeatureC++ precompiler performs source code optimizations to avoid any overhead at runtime that might be intro-duced by decomposing a class into multiple refinements. For example, the generated C++ code is optimized for inlining of method refinements to achieve performance that is equal to a C++ implementation. For the composed class shown in
Fig. 4 c this means that code of refinements of method put method by the C++ compiler. This avoids an overhead for additional method calls as it is usually found when using ob-ject-oriented concepts for extensions like virtual methods. 3.1.4. Expected benefits of FOP
FOP has a number of benefits that are of interest for software development, which have been observed in several studies [39,44,40 X 42] . Especially of interest for embedded systems are: Application footprint is important especially for the embedded domain and performance is highly important for DBMS.
Therefore, we analyze these properties in detail in our evaluation. 3.2. Refactoring
Most contemporary data management systems are written in the C programming language. In some cases also object-ori-ented languages like Java or C++ have been used. Implementations of DBMS are usually highly tuned and cannot be reim-plemented from scratch using a novel programming paradigm like FOP without a huge amount of work and the risk of degrading performance or introducing errors. We will demonstrate that a refactoring approach can be used to decompose legacy DBMS. That means, we show how an existing DBMS can be restructured with respect to features to yield a DBMS product line.

For refactoring, we propose minimal invasive code transformations that do not change the design in order to avoid errors applications into a product line. In our case study, we present the refactoring of Berkeley DB using FeatureC++. 4. Berkeley DB: a case study
Berkeley DB is an embedded DBMS for use in server systems but also in embedded systems. In our case study, we refac-from C to C++ and (ii) the conversion from C++ to FeatureC++. 4.1. An overview of Berkeley DB
The source code of Berkeley DB contains 96 thousand lines of C code, excluding comments. To use Berkeley DB in resource source code. The configuration mechanism is implemented using C preprocessor statements and macros. The low binary size of Berkeley DB and the high performance achieved by its integration into client applications enable
Berkeley DB to be used in a wide range of computing systems. Examples are Amazon X  X  inventory of products, set top boxes from Sony, Samsung X  X  digital videorecorder, Motorola X  X  smartphone A768.
We demonstrate that FOP is an appropriate approach to scale down data management systems like Berkeley DB for the use in deeply embedded systems. 4.1.1. Comprehensibility of source code
Berkeley DB developers maximized the performance by avoiding function calls with macros, goto -statements, and very long methods (up to several hundred LOC). Preprocessor statements are used for the configuration of Berkeley DB. In
Fig. 5 , we show a code sample from Berkeley DB where different features are surrounded by preprocessor statements. Very has been observed that nesting of preprocessor statements degrades readability, comprehensibility, and maintainability of 4.1.2. Customizability and reusability 4.2. Refactoring Berkeley DB As FeatureC++ builds on C++, we discuss some issues related to performance before we present our results of refactoring Berkeley DB.
 4.2.1. C++ and performance
The C programming language is still widely in use. In some cases the use of C++ is refused because poor performance and high resource consumption is assumed. According to Stroustrup there is no evidence for this argument [49]:
Contrary to popular myths, there is no more tolerance of time and space overheads in C++ than there is in C. The emphasis on runtime performance varies more between different communities using the languages than between the languages themselves. In other words, overheads are found in some uses of the languages rather than in the language features.
It is important to use the language constructs of C++ in an appropriate way. Especially in embedded systems one should sidered this and avoided those language constructs. 4.2.2. Transformation from C to C++
Software written in C is usually based on a decomposition of the source code into files of similar functionality and structs that encapsulate data. To simplify the process of refactoring C to C++, we transformed existing files and and thus a conversion process that can be automated. There are some tools that support a C to C++ conversion to the original C source code, but uses classes with static methods. The following overview describes the used code transformations: methods of the generated classes. ented design. The classes generated in (1) usually contain methods that operate on these structs. This is caused by an object-based programming style in C which is used in Berkeley DB. In this case, fields of structs have to be moved to the according generated class that operates on this data. This conversion means to merge structs and the newly generated classes. can be removed because they are implicitly provided as this ers refer to C functions that usually operate on fields defined in these structs thus emulating objects. The pointers can usually be removed since non-static methods have been generated from the according C functions as described in (3). Code for initializing function pointers can be removed as well since functions are replaced by ordinary OOP methods.
The presented conversions can be applied in a step-wise manner. Using only a conversion into classes with static methods (1) is the simplest case and can be fully automated. Conversions (2) X (4) replace the object-based programming style often found in C source code and introduce regular classes. This means avoiding function pointers and manual pointer initialization. because code replication in some methods was obvious and a simple inheritance based redesign could remove the redun-dancies. We limited our redesign to only seven classes to form an inheritance hierarchy in order to avoid fundamental changes. For example, classes used for implementing different index structures now reuse the same code implemented in a common base class. However, we did not use any virtual methods to avoid a performance impact and to preserve real-time capabilities. 4.2.3. Transformation to FeatureC++
In a second step, we transformed the C++ code into FeatureC++ code. We developed a tool that transforms the code semi-automatically and extracts user-defined features of Berkeley DB into separate modules.
DB and is used to separate feature code from the base implementation. For example, a folder crosscut other parts of the source code (e.g., other structs or classes) cannot be modularized this way. Nevertheless, we utilize this inherent structure and our tool creates a feature for each existing directory as an initial feature-oriented decomposition.

This basic decomposition of the source code into features is extended with a user-defined mapping of classes to features, modularization that provide functionality which is not necessarily needed for every application and can significantly de-crease the size of Berkeley DB, e.g., for transaction management and recovery. We furthermore selected features that are mandatory for a DBMS but increase the modularity of the source code. This basic transformation does not include a decom-the feature a class was assigned to, (iii) removes includes not needed for FeatureC++, and (iv) merges implementation ( .cpp ) and header files ( .h, .hh ) of a class.

This basic transformation from C++ to FeatureC++ does not include a decomposition of classes into refinements, but it results in FeatureC++ code which is equivalent to the underlying C++ code. In combination with our transformation from
C to C++ the source code of the refactoring is equal to the original C code. gram also other features. An examples is the transaction management that crosscuts many classes and features in a DBMS action management. Due to the absence of appropriate tool support, we extracted the features manually. That is, we decom-posed the classes into base implementation and refinements according to crosscutting features. This means that code a refinement.

In some situations we needed to create hook methods to decompose methods according to features. Hook methods, often used in frameworks, are usually abstract methods that introduce extension points into other methods that call these hook use a hook method InitNewCursor that can be overridden by subsequent features to provide initialization code. This hook method is called while creating new cursors and its base implementation is empty and is overridden in features like B-to hook methods to allow their use in method extensions [51].
 with static configuration based on preprocessors. For example, when distributed transactions are used, function pointers semantically equal to method refinements of FOP. For example, code for distributed transactions can be implemented in
Replacing such function pointers with refinements of FOP means an automation of the manual configuration process, i.e., replacing manual pointer initialization by FOP code transformation. original version). The remaining 11 features are mandatory for each variant of Berkeley DB and thus cannot be removed able to modularize it and thus can generate variants of Berkeley DB without transaction management. 5. Evaluation
For evaluation we analyze the feature-refactored version of Berkeley DB with respect to customizability and resource con-sumption. We compare several variants with the original C version of Berkeley DB. 5.1. Customizability In its original version 11 features are optional and can be individually disabled when generating a concrete instance of
Berkeley DB after refactoring; mainly optional and alternative features. Features denoted with an empty dot are optional For our analysis we use eight different variants of Berkeley DB embedded into a small benchmark application, as shown in versions of Berkeley DB because features like T RANSACTION original version.
 5.2. Footprint
The binary size (footprint) of an application depends on the size of executable code and static data. Comparing our fea-664 KB to 636 KB in Configuration 1) are mainly differences in the programming paradigm. For example, in the C version of
Berkeley DB function pointers are used to mimic an object-based programming; these have to be manually initialized when instantiating objects and increase the binary size. These differences are negligible compared to the overall size of the application.
 The binary size of applications that use Berkeley DB can be further decreased by removing features that are not needed. (6), we see significant differences (code reduction of about 50%). 5.3. Size of source code In order to get more insight into the reasons for a reduced binary size, we analyzed the source code of Berkeley DB. In
Fig. 8 , the number of lines of Berkeley DB code before and after feature-oriented refactoring are depicted. configurations (1) the features are nearly of equal size, they differ when comparing minimal configurations (6).
The difference between the minimal configurations (6) is not only caused by differences in the programming paradigm but also by additional extraction of crosscutting features (e.g., R version. For example, feature B-TREE as shown in configuration (1) is about 7 KLOC larger when compared to configuration (7) which is caused by removing all features that crosscut the B-example, feature R ECOVERY makes up a large part of the B-
By omitting the feature R ECOVERY the size of the B-TREE 5.4. Performance
As discussed earlier, C++ has to be used carefully to avoid performance penalties. We considered this when imposing an parisons between the C and FeatureC++ variants of Berkeley DB. formance for configurations 1 X 6 is roughly equivalent when comparing C with FeatureC++ variants. Configuration 7 is the smallest FeatureC++ configuration using the B-tree index structure. is caused by the removal of dynamic checks that are evaluated even when features like T case for Configurations 1 X 6. 6. Discussion
Our evaluation has shown that we are able to preserve the performance characteristics of Berkeley DB when transforming it from C into FeatureC++ and even when applying a more fine-grained decomposition. This also shows that C++ and
FeatureC++ do not necessarily have a negative impact on performance. In contrast to dynamic configuration, e.g., using statements and function pointers, a performance improvement is possible by using static composition of features. However, sented decomposition. 6.1. Separation of concerns
All features that we modularized in Berkeley DB cut across large parts of the base program. Crosscutting features also
Berkeley DB, we made some observations regarding structure and modularity of the source code. 6.1.1. Static customization
The C version of Berkeley DB already supports static composition of some features using C preprocessor directives. The programmers of Berkeley DB use function pointers to support object-based programming and to exchange the implementa-manually by the programmer.
 When using FeatureC++, object-oriented concepts, i.e., classes with methods, eliminate the need for function pointers. composition based on the C preprocessor. This automatic composition of methods reduces implementation effort and is less
Technical aspects like memory consumption and performance also benefit from using FeatureC++. The reason is that function tions referenced by pointers. 6.1.2. Decomposition of large methods
Many methods in Berkeley DB are quite large and contain entangled functionality of different features. Reasons for this ing capabilities of modern compilers. For those methods, we created up to seven methods each by using the extract method refactoring [48]. In some cases, we used hook methods to decompose large methods. This results in additional effort for decomposing such methods but tool support could significantly ease the refactoring of large applications [53,54] .
Decomposing methods into smaller methods and method extensions, including the use of hook methods, does not have a negative effect on performance. The reason is method inlining which can be employed because all methods are part of the same compilation unit in the generated C++ code. 6.1.3. Variable method signatures
Crosscutting features may not only extend a method body but also affect parameters of a method, e.g., add a parameter that is required for executing the method. We also observed this in Berkeley DB when extracting the transaction manage-ment. Some method extensions require an additional transaction parameter and thus extend the signature of refined meth-ods. For example, there is a method put in Berkeley DB similar to the method shown in Fig. 3 . In order to support transactions, an additional parameter of type TXN is required which is introduced in feature T mentation without any transaction code, the parameter is not needed and should not be part of the method signature. This proaches to handle such method extensions and found that the C preprocessor (i.e., using ments if these classes are defined in optional features [57].

Overall, the effort to modularize a feature depends strongly on its crosscutting nature. Thus when deciding to extract a reduce the refactoring effort and handle fine-grained decompositions of the source code [58]. 6.2. Comprehensibility of source code
Comprehensibility of source code depends on a number of properties, such as separation of concerns, size and complexity understood by inspecting large parts of the code [14].

Using FOP, features are modularized and separated from each other. Parts of classes and methods can be related easily to major features that crosscuts large parts of the source code is the transaction management system (4208 LOC). In Berkeley DB, we extracted the transaction management system, as other features, and separated it from the remaining functionality. This decreases the code size of other features. For example, feature B-features (cf. Fig. 9 ). As a result, one has to inspect only about 50% of the complete feature to understand the B-mentations. On the other hand, to understand a particular feature sometimes also the code of other features has to be mentation of the B-tree, a programmer has to understand the features B-that crosscut the B-tree, which make up about 25% (cf. Fig. 9 ).

FOP can also have a negative effect on readability of the source code when comparing it to techniques that do not support customization. For example, if hook methods are used to introduce extension points into methods, these may degrade the cannot be completely modularized if these occur at the level of statements within methods. In this case, the decomposed source code may be difficult to understand since the program flow switches between a number of methods and refinements.
On the other hand, hook methods can improve comprehensibility since they can avoid decomposition of a method into a high number of smaller methods. Finally, the degradation of readability is a result of increasing customizability and can not only be found in FOP.

C/C++ macros can also be used to implement functionality that is only available if the according feature is present in a
In contrast to macros, hook methods can be easily refined by multiple features when using FeatureC++. Furthermore, hook methods are part of the programming language and the type system. They support type safety, allow for overloading, and are encapsulated in the corresponding class. Additionally, complex we are able to properly modularize such code into feature modules and separate it from other features. 6.3. Comparison to other approaches
As discussed in Section 2, there are different approaches for development of customizable data management software. In the following, we review these approaches and compare them to our approach. Specifically, we compare properties like sep-from our experience with Berkeley DB or result from analysis of previous research. The comparison should be considered only as an evaluation of approaches with respect to their applicability for developing customizable data management sys-tems for the embedded domain and not as a general evaluation of these approaches. Some of the analyzed properties are difficult to evaluate and cannot be generalized. We begin with an overview of the compared properties:
Performance and footprint represent the impact of the respective approach on execution time and binary size of an appli-cation. A negative value (empty dot) means a negative impact and a positive value means that performance might be increased or binary size might be reduced due to optimizations.

Granularity means the potential degree of decomposition of a software. A fine granularity results in high customizability which is needed for highly resource constrained environments. For example, a decomposition of the Berkeley DB core into further features might be necessary for deeply embedded devices to reduce the binary size but is usually not required for source code modules. Separating and modularizing functionality of an application is important when independent devel-with a different implementation. Especially crosscutting concerns are often difficult to separate from other code [23].
Comprehensibility of source code is difficult to measure. When estimating comprehensibility we consider only our experi-ence with Berkeley DB and do not intend to rate the approaches in general. Maintainability of software is affected by its comprehensibility and not considered separately here.

Based on these properties, we rated the approaches introduced in Section 2 as shown in Table 3 . The following overview reviews them to explain the ratings. 6.3.1. Kernel systems
Kernel systems have been used to develop server DBMS and might also be applied to embedded systems but achieve only coarse-grained customizability because a fixed DBMS core is used that provides functionality required by most DBMS [21]. scenarios in the embedded domain in general. However, a small kernel system might provide a foundation for implementing
DBMS for a range of applications with similar requirements for embedded devices. Furthermore, FOP might be used as an extensibility mechanism for kernel systems thus avoiding performance degradation. Comprehensibility of the source code kernel systems are not a general mechanism for achieving customizability. They obey similar properties like components and components can also be used to implement extensions for kernel systems [21]. 6.3.2. Components
In contrast to kernel systems, components provide a better customizability since also the kernel of a DBMS can be decom-using virtual methods) and introduce an overhead when considering performance and footprint [21,13] . Hence, the granu-larity of decomposition that can be achieved with components is limited due the introduced runtime overhead which dra-matically increases for very small components [11]. Furthermore, components can be used only to modularize functionality discussed for FOP, e.g., caused by hook methods that are used in both approaches. 6.3.3. Components and AOP In order to avoid some drawbacks of the component approach, AOP can be used to implement crosscutting concerns [23].
This was also shown by Nystr X m et al. who modularized crosscutting concerns like concurrency control to allow modular-ization of the transaction management subsystem [13]. The result is improved separation of concerns and fine-grained cus-tomizability without negative impact on performance and footprint. Unfortunately, the comprehensibility of source code may be degraded when using aspects as it is suggested by studies that analyzed AOP [59,51,60] . However, currently there is too less known about the comprehensibility of AOP code for a detailed evaluation. 6.3.4. C preprocessor (CPP)
When using #ifdef statements and the C preprocessor there is no negative impact on performance or resource con-sumption as long as the approach is used correctly. The reason is that the preprocessor is working on the source code and does not introduce additional overhead as it is the case for components. A negative impact on performance as it might allows a developer to provide fine-grained customizability by modularizing even single statements which is not possible with any of the other analyzed approaches. In contrast to components or FOP, the C preprocessor does not directly support separation of concerns in source code. This is partially possible using separately defined macros instead of scattered nificantly degraded by nested #ifdefs which might be reduced by using macros but cannot be completely avoided [14]. 6.3.5. FOP
Similar to the C preprocessor, static composition of feature modules avoids any negative impact on performance or foot-print when using FOP [61]. Feature modules are similar to components but they can be also used to modularize crosscutting combination of components and AOP, FOP provides a uniform mechanism for customization, i.e., refinements , and not a com-instead. We argue that this is needed only seldom but a combination of annotative approaches with FOP might be beneficial ming language [63]. Separation of concerns can improve the comprehensibility of large software systems because not the have a negative impact on code comprehensibility, e.g., when hook methods have to be used. For that reason we think that hensibility of FOP code.

The presented comparison does not mean that a particular approach cannot be used for the embedded domain at all or that FOP is always the best approach. For example, even though components introduce an overhead with respect to resource consumption they can be used for devices that provide more memory or when lower performance and higher power con-each concrete scenario which approach should be applied. The comparison presented above provides a basic guideline for such decisions. 6.4. Summary The key results of our case study can be summarized as follows:
We used a refactoring approach to decompose a medium size DBMS into features to reduce the code size of tailored instances.
 We modularized also complex crosscutting features, e.g., transaction management, that crosscut up to 11 other features.
The binary size of Berkeley DB did not increase when applying an FOP approach. By removing unused features we could decrease the binary size by about 50%.

Comparing different variants of the refactored Berkeley DB to the original version, the performance did not decrease even though a more fine-grained decomposition was used. By extending dynamic configuration with static configuration we could achieve a performance improvement of about 16% for a reading benchmark.

These results show that when FOP is used, it is indeed appropriate to build highly customizable applications for use in embedded systems. In resource constrained environments, the requirements on resource consumption and specialization macros in C/C++ code or for hooks as commonly applied in frameworks [50]. 7. Conclusion and perspective
We have presented an approach to customize and downsize DBMS in order to tailor data management for embedded sys-tems. We used FOP to generate specialized DBMS based on a common architecture and code base. The fine-grained custom-izability supported by FOP is the basis for tailoring DBMS to satisfy the resource constraints of embedded systems.
We have evaluated our approach by means of the medium-sized commercial DBMS Berkeley DB. While other approaches for developing customizable data management software have drawbacks regarding resource consumption and customizabil-ity, we have shown that performance is not degraded but can be enhanced by about 16% when comparing reading operations with the original DBMS. Furthermore, we reduced the minimal binary size of the DBMS by about 50% by removing features.
Comparing FOP to other approaches, it combines high customizability, similar to that of preprocessors, with proper modu-to replace preprocessors usually applied to provide customizability for embedded systems and seems promising to be ap-plied to other domains.

The presented approach is also promising for developing customizable DBMS in general because it overcomes the limi-cation of FOP to DBMS development in the embedded domain and also in other domains as we discuss in the following. 7.1. Granularity of decomposition
Currently, there is little known about the appropriate granularity for decomposing DBMS. While FOP supports a fine-management solutions from scratch [44,64 X 66] . For example, we used FeatureC++ to develop F strained resources, e.g., with a program memory of less than 128KB. We could show that an extremely fine-grained decompo-of Berkeley DB as well as an extension of the F AME -D BMS 7.2. Query processing
Structured Query Language (SQL) grows with every new standard and supports many features while only a small subset is actually used by applications. Developments like Structured Card Query Language (SCQL) [69] and SQL extensions for sensor networks [19] or stream processing [70] show that the current SQL standard is not the appropriate solution for every use case and extensions are needed as well. In prior work, we could show that SQL (i.e., the grammar and the parser) can be tomizable DBMS [72]. 7.3. DBMS architecture and other domains
By combining tailor-made SQL dialects and customizable DBMS we think that one can build DBMS that can be fully tailored to an application domain or a special use case within a domain while providing high reuse. The need for tailor-made data management is not only apparent in the embedded domain but also in other domains [8,73] . Examples are tailor-made lock protocols for XML databases [74] or special solutions needed in the domain of distributed databases [75].
Creating a fully customizable DBMS to support tailor-made solutions for different domains is a challenging task. It in-volves customizability on all levels of a DBMS such as the query processor and optimizer, the transaction subsystem, or to handle the resulting complexity. For example, changing the query language affects the whole DBMS including the query optimizer which is highly connected with all other parts of a DBMS. New extensibility mechanisms as provided by feature-oriented programming can help in building such variable systems including a variable architecture. Based on the F constrained devices. In the long run, this could be done for many other domains as well.
 Acknowledgements We thank Christian K X stner, Martin Kuhlemann, and Norbert Siegmund for comments on drafts of this paper. Marko
Rosenm X ller is funded by German Ministry of Education and Research (BMBF), project number 01IM08003C. Sven Apel X  X  work is funded partly by the German Research Foundation (DFG), project AP 206/2-1. The presented work is part of projects F AME -D BMS , 12 ViERforES, 13 and FeatureFoundation. 14
References
