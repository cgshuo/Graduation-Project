 Analysing the document scores returned fr om information retrieval (IR) systems is a very useful, yet challenging problem. Work in this area can be dated back to the early days of IR [16]. Modelling the document scores returned for different queries (and from different systems) i s an important task because it has been noticed that the distribution of relevan t document scores is different than that of non-relevant document scores. For example, if given the entire score distribu-tion (SD) returned from a system, the distribution of relevant documents could be accurately determined, it would be particularly useful for automatic query performance prediction and/or meta-search (fusion) tasks [8,4]. Regardless, the problem of correctly modelling the distribution of relevant and non-relevant doc-uments remains an open, and theoretically important, area in IR.

Over the last decade, the predominant distributions [1] for modelling relevant and non-relevant document scores have been a normal and an exponential re-spectively. There has been relatively little justification as to why relevant and non-relevant document scores should be drawn from two different families of distributions. Nevertheless, these distributions have best fit the data for many years now. More recently, it has been sugge sted that the normal-exponential mixture is not theoretically valid under certain assumptions [14], and in fact, a more theoretically valid approach might be to model the scores using two gamma distributions [9].

This paper deals with determining the best distribution for use in a mixture model by using the inference of performance (i.e. average precision) as a measure of utility , when relevance information is available. While the task of inferring average precision might be viewed as only one measure of utility ,itisoneofthe most important tasks in IR. This measure of utility is very important as it is linked to a model X  X  ability to accurately model the relevance contained within, and is not only of practical concern but is of theoretical importance. We show that the log-normal distribution is the best distribution to use in a mixture model of score distributions for both goodness-of-fit and utilty .

The remainder of the paper is organised as follows: Section 2 reviews related work on modelling document score distributions. Section 3 outlines four mixture distributions used in this paper, before the formulae for calculating the average precision from a mixture model are intro duced. Section 4 presents empirical re-sults comparing the four mixture models for a number of metrics when relevance information is known. Section 5 presents an empirical analysis of the four mix-tures based on Robertson X  X  recall-fallout convexity hypothesis. Finally, section 6 outlines our conclusions and future work. In this section, we review related work in document score distributions. 2.1 Related Work Early work into SD modeling has shown that the distribution of relevant doc-uments somewhat follows a normal curve [16]. Approaches over the years have tried various curves and  X  X its X  to try and uncover the underlying distributions. More recent work has shown that modelling relevant and non-relevant document scores using a normal and exponential distr ibution respectively, fits for the scores at the head of the ranked list (i.e. top 1000 documents) [1]. Indeed, this has been the predominant trend o ver recent years [14].

Others have addressed more theoretical aspects of the underlying distribu-tions, and have developed hypotheses under which certain distributions can be theoretically rejected [14]. The aforemen tioned work develops a recall-fallout hy-pothesis which states that the recall-fallout curve for good systems should be upper convex and has shown that if the probability ranking principle [13] holds, then certain distributions should be rejected on theoretical grounds. Further work [2] has hypothesised that a theoretically valid distribution must be able to approach the Dirac delta function (i.e. it must be able to approach an impulse under which the entire mass of documents can reside).

Some of the theoretical problems associated with the infinite support that some distributions allow w ere addressed recently [1] using truncated forms of distributions. Some novel approaches [10] to modelling the score distribution have used multiple normal distributions for the relevant documents and a gamma distribution of the non-relevant ones. This approach uses these distributions because they are a good  X  X it X  given the data. Important work in analysing the generation process (i.e. ranking functions) of document scores and their resultant distributions has also been conducted [9]. On a practical note, research has been conducted to use the score distributions for data fusion [12] and score threshold optimisation [1]. 2.2 Contributions This work has a number of contributions. Firstly we show how average precision can be inferred from a mixture distribution. Secondly, we conduct an extensive evaluation of several mixture models for a number of metrics (one of which is the task of inferring average precision accurately), and advocate the use of the log-normal model in particular. Interestingly, we show that the best method of estimating parameters for the task of inferring average precision, is the method of moments (MME), rather than maximum likelihood estimates (MLE). Finally, we show that the despite its superior performance the log-normal mixture does not adhere to Robertson X  X  recall-fallout convexity hypothesis as well as the gamma mixture. In this section, we present four mixture distributions used in this paper to model the scores of both relevant and non-relevant documents. 3.1 Assumptions and Restrictions Consider an IR system that retrieves a returned set of N documents, and thus N scores given a query ( Q ). Firstly, we assume that an IR system ranks documents independently of each other, in accordance with the probability ranking principle (PRP) [13]. While this may not be true for certain systems (e.g. for those that wish to promote diversity), it is a widely held principle in IR. Secondly, we assume a binary view of relevance. While score distributions can be modelled as mixtures of a multiple of differently graded relevance distributions, this work only models a binary view of relevance.

We used the following two criteria to select the distributions that are pre-sented in section 3.2. Firstly, under on the strong SD hypothesis [2], the distri-bution of both relevant and non-relevant documents should be able to approach Dirac X  X  delta function (these distributions are valid under that hypothesis). And secondly, there is no theoretically valid reason why relevant and non-relevant documents should be drawn from two different families of distributions, given that the document score of relevant and non-relevant documents is generated using the same process (ranking function) within an IR system. 3.2 Mixture Distributions The distributions that we consider are the normal distribution ( N ), the expo-nential distribution ( E ), the log-normal distribution ( L ), and the gamma distri-bution ( G ) [11]. For most of the mixtures outlined in this section both relevant and non-relevant documents are modelled using the same distribution. We only include the normal-exponential ( N 1 E 0 ) mixture as it has been used in many studies to model score distributions for various tasks. Therefore, the next step is to outline the mixture model that can be used in conjunction with any dis-tribution. For most mixtures, we model both sets of documents using the same distribution, where P ( s | 1) is the pdf (probability density function) for the scores (s) of relevant documents, and P ( s | 0) is the pdf for the scores of non-relevant documents. Therefore, similar to previ ous approaches, the document score dis-tribution can be thought of as a mixture of relevant and non-relevant documents as follows: where  X  = R N is the proportion of relevant documents R in the entire returned set N . In practice, no form of document scor e normalisation is necessarily needed for the upper limit for any of the distributions. Although, negative values are not supported for the log-normal or gamma distributions, for the information retrieval models used in this work, the issue of supporting negative scores is not a problem in practice 1 .

In this paper, we study four mixtures. Table 1 outlines the mixtures and the parameters that need to be estimated for each model. For the parameters of each model, we use the subscript 1 to imply that the parameter is used with the distribution of relevant document score s, whereas we use the subscript 0 to imply that the parameter is used with the distri bution of non-relevant document scores. For three of the mixtures, there are a total of five parameters (i.e. the mixture parameter, two parameters to model th e relevant scores and two parameters to model the non-relevant scores), while the normal-exponential model ( N 1 E 0 ) has only four parameters. This is important for comparison purposes, as models (and distributions) with more parameters have more flexibility in modelling the observed data. Therefore, some models may have less flexible in terms of their ability to model scores from different systems. Although we have included the normal-exponential ( N 1 E 0 ) model in this study, it is in the authors opinion that document scores of relevant and non-relevant documents should not be drawn from two different families of distribution. For the N 1 E 0 and N 1 N 0 mixtures, the MME (method of moments estimates) and MLE (maximum likelihood) estimates are equivalent. However, for the L 1 L 0 and G 1 G 0 mixtures, the MME and MLE estimates will lead to different parameter settings. 3.3 Inferring Average Precision In this section, we will show how average precision (a standard metric for the effectiveness of a query) can be calculate d directly from the mixture of continuous distributions. Firstly, it is worth noting that average precision is an informative measure. As average precision can be viewed geometrically as the area under the precision-recall curve [3] 2 , we know that it summarises the performance over a large portion of the ranked list, and therefore, conveys a broad view of the effectiveness of a query. Secondly, it is a stable measure [5], and is probably the most prevalent metric of both query and system performance used in IR literature. The interested r eader is referred to researc h which strongly outlines the theoretical importance of average precision [15].

As recall is the proportion of relevan t returned documents compared to the entire number of relevant documents, the recall at score s can be defined as follows: which is the cumulative density function (cdf) of the distribution of relevant documents (viewed from  X  ). Under the distributions outlined earlier for our model, we know that recall ( s ) will vary between 0 and 1, (i.e. when s =0, recall ( s ) = 1 as ensured by the cdf). Similarly, the precision at s (the proportion of relevant returned documents over the number of returned documents) can be defined as follows: Now that we can calculate the precision and recall at any score s in the range [0 :  X  ], we can create a precision-recall curv e. Furthermore, as average precision can be estimated geometrically by the area under the precision-recall curve [3], the average precision ( avg.prec ) of a query can be calculated as follows: where r ( s )= recall ( s ) which is in the range [0:1]. This formulation is an elegant and intuitive way of calculating average precision using the score distributions. As these expressions are not closed-form, they can be calculated using relatively simple geometric numerical integration methods. In this section we perform a comparative analysis of the four mixture models across a number of different IR models (i.e. vector space, classic probabilistic, language model, learned model, and axiomatic model). First, however, we will motivate our choice of comparison metrics. 4.1 Goodness-of-Fit, Correlation, and RMSE Usually, the performance of a mixture model is determined by how well the model  X  X its X  actual data. For different fields of study and for different problems, differ-ent metrics may be applicable. Usually, goodness-of-fit tests (e.g. Kolmogorov-Smirnov test) are used to either a ccept or reject certain models as a  X  X ood fit X  . However, in IR, it is well-known that doc uments, and therefore document scores, at the head of a ranked list are more important than those further down the list 3 . These goodness-of-fit tests do not make a distinction between observations (i.e. scores) at various locations and they do not measure the amount of relevant information that can be correctly maintained in the model.

We propose that better mixture models are better able to model the infor-mation regarding relevance. An intuitive way of measuring this is by trying to infer the average precision of a query using the model (and its known param-eters). Average precision is a natural candidate for capturing the performance (as discussed earlier). Therefore, over a set of topics, the correlation between the inferred average precision from the mixture model and the actual average preci-sion of the query from the IR system, gives us a measure of how much relevance information is contained in each model. From an information theoretic point of view, it also gives us an indication of ho w much relevance information is lost when modelling each ranking with a particular mixture model.
 4.2 Comparative Analysis We now compare the four mixture models introduced earlier (i.e. Table 1) over a range of IR systems and settings. Different distributions may better be able to model different IR systems and so for a broader comparison, we compared the four mixtures across five IR models. We chose the vector space model using pivoted document normalisation (PIV) [7], the probablistic model (BM25) [7], a language modelling (LM) approach (Jelinek-Mercer smoothing) [17], a learned approach (ES) [6], and the axiomatic approach (F2EXP) [7], as these represent a broad range of classical and more modern ranking functions. Table 2 shows the test collections used in this research.
 Goodness-of-fit. Table 3 shows the Kolmgorov-Smifnoff D-statistic 4 (a mea-sure of goodness-of-fit ) on each of the collections averaged over the five systems. The D-statistic measures the maximum distance between the cumulative den-sity function of the theoretical distribution (i.e. one of the mixtures) and the empirical distribution (i.e. the actual scores). Firstly, we can see that Table 3 shows that the log-normal model has a significantly 5 better fit compared to the gamma model on two collections for the en tire returned set of document scores. The results also show that the log-normal models fits non-web collections very well, but the gamma model has a better fit for some IR systems on web col-lections. The normal-exponential model is the third best model in terms of fit, while the normal-normal model is particularly poor. We can also see from Table 3 that the MLE parameter estimation technique provides better fits, in general, than MME.
 Correlations and RMSE. Now we analyse the amount of relevance informa-tion that can be correctly contained within each mixture model across the five IR systems using correlation measures. Using the MME and MLE approaches we can estimate the five parameters in each mixture model assuming relevance information is known (i.e. labelled data). We then compare the correlation of the inferred average precision (calculated from equation 4) for the mixture model with the actual average precision from the IR system in question.

Table 4 shows the average Spearman and Pearson correlation of the four mixture models averaged across the five systems 6 . Firstly, it is worth noting that the correlation coefficients for some of the mixtures are quite high, indicating that much of the information regarding average precision (relevance) are correctly modeled by some of the mixtures. We can also see that the mixture model comprised of a normal and exponential (i.e. the predominant model over the last decade) is the lowest performing model of the four that we have studied. The normal-normal model outperforms the exponential-normal model in terms of utility despite having a worse fit (see Table 3). In general, we can also see that the log-normal mixture model tends to outperform the gamma model across a variety of settings and parameter estimation techniques (i.e. for both MME and MLE estimates). In general, the results show that the log-normal model is the more general and consistent model for preserving relevance information across a variety of IR systems.

Table 5 shows the root mean squared error (RMSE) 7 of the inferred average precision compared to the act ual average precision for a set of queries for both the BM25 and LM systems (the other systems tested showed comparable results). We can see that the actual average precision predicted by the log-normal model is closer to the true average precision. While the RMSE is not of major importance in terms of the predictive quality of a model, it does inform us that the raw output of the log-normal mixture model is closer to the actual average precision of a query. The RMSE results of all other IR systems are comparatively similar to those in Table 5. One reason for this error is that the formulae given for inferring average precision from score distributions (Section 3.3) will actually over-estimate the actual average precision value on TREC data due to the fact that recall is calculated as the number o f relevant documents in the returned set, rather than the total number of relevant documents in the collection. MME vs MLE. Another interesting point is that the MME approach to pa-rameter estimation consistently outperforms the MLE approach in terms of util-ity (i.e. for the task of inferring performance as measured by the correlations in Table 4). However, when all sample observations are treated equally (as for goodness-of-fit tests), the D-statistic in Table 3 shows that models derived from MLE are closer to the observed samples. This provides further proof that the correlation coefficients and goodness of fit tests measure different aspects. As we are dealing with IR systems, and models of relevance, we argue that a standard measure of utility is more apt. Of the mixtures studied in this paper, we have empirically determined that the mixture of two log-normals is one of the better mixtures for modelling document scores for a number evaluation metrics. Furthermore, our results suggest that it is very robust and can accurately model rankings returned from many systems. However, it is unclear if this mixture adheres to useful theoretical properties. In this section, we analyse all of the mixtures using the recall-fallout convexity hypothesis [14]. Interestingly, we show that the gamma mixture violates the recall-fallout hypothesis less often than the log-normal mixture near the head of the ranked list (i.e. where it is more important). 5.1 Locating Points of Non-convexity The recall-fallout hypothesis states that as we traverse a ranked-list, the recall should always be greater than fallout. This seems theoretically justifiable, as IR systems should at least provide a better than random ranking. Therefore, when modelling document rankings as continuous distributions, the recall-fallout hypothesis can be more formally stated as  X  s P ( s | 1)  X  ds &gt;  X  s P ( s | 0)  X  ds for all s . A detailed analysis of the recall-fallout convexity hypothesis for all of the mixtures studied in this paper (except the two log-normal mixture) can be found in the original work [14]. Using notation similar to the original work, the convexity condition that must be satisfi ed to ensure that reca ll is greater than fallout for all scores, can be written as follows: where where f ( s ) is the probability density function of a particular distribution. Now assuming this hypothesis to be valid, it would be interesting to see how closely the better mixture models adhere it.
 Gamma Mixture. Therefore, as g ( s )=(( k  X  1) /s )  X  1 / X  for the gamma mixture [14], the score at which the condition is violated is found by simplifying the following: which simplifies to We can see that if  X  1 =  X  0 ,therearenorootsfor s , and so no violations occur. Furthermore, if k 1 = k 0 , s = 0 and so, the violation occurs at the point at which both recall and fallout ar e 1 (which is acceptable). F or the two-gamma mixture, if s&gt; 0, the violation occurs at a score that can be encountered by the mixture, otherwise the violation does not occur.
 Log-Normal Mixture. For the log-normal mixture g ( s )= condition is violated is found at: by following a similar simplification process as the gamma mixture. We can see that if  X  1 =  X  0 , the function has no roots, and therefore, no violations (similar to the normal distribution [14]). If the variances are not exactly equal, a violation of the convexity condition, will occur at a score above zero. The score at which a violation occurs can be translated to a point of recall using equation (2). 5.2 Empirical Results and Discussion We analysed the four mixtures models by calculating the points of recall at which the convexity condition was violated for each query on the te st collections. It is reasonable to assume that a violation at the head (i.e. low point of recall) of the ranked list is more serious than if it occurs at high recall. However, if the convexity condition is violated at a sco re that is rarely, or never, encountered by an IR metric (at high recall), it is deemed less serious. Table 6 reports the average point of recall at which a violation of the convexity condition occurs for a set of queries averaged across the five IR systems. The results in Table 6 are from the four models when using MME as the parameter estimation technique.
In general, we can see that the two-gamma model is the more theoretically sound as violations occur, on average, at a higher point of recall (e.g. at a lower score s ). Surprisingly, violations occur at a low point of recall for the log-normal model (even lower than the two-normal model), which suggests that it is theo-retically less sound that either the two-gamma model or the two-normal model. The exponential-normal mixture has violations at both ends of the relevant dis-tribution (i.e. both high and low recall) 100% of the time, and therefore, we can see from Table 6 that the violations occur very early on in the ranking (i.e. low point of recall). The results from Table 6 confirm previous analysis [14] with regard to many of these models.

The average results across the five IR systems in Table 6 are highly represen-tative of each individual system. More work is needed to understand the reasons for the apparent shortcoming in the theoretical behaviour of the two log-normal model (especially as it outperforms other mixtures in terms of goodness-of-fit and utility ). In this work, we have performed a comparative analysis of different distributions that comprise mixtures for document score distributions in IR systems. We have determined that the log-normal distribution is the best performing model in terms of both its accuracy in infe rring average precision, and its goodness-of-fit . The log-normal model has been used in relatively few practical works. Interest-ingly, we have shown despite its good performance the log-normal model is the-oretically less sound than the two-gamma model towards the head of a ranking. Interesting future work would be to create mixture models that unconditionally adhere to the recall-fallo ut convexity hypothesis (e.g. by ensuring  X  1 =  X  0 for the two log-normal model) and then compare the utility of those valid models.
