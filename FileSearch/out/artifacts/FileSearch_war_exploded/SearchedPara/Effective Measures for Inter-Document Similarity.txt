 While supervised learning-to-rank algorithms have largely supplanted unsupervised query-document similarity meas-ures for search, the exploration of query-document mea-sures by many researchers over many years produced in-sights that might be exploited in other domains. For exam-ple, the BM25 measure substantially and consistently out-performs cosine across many tested environments, and po-tentially provides retrieval effectiveness approaching that of the best learning-to-rank methods over equivalent features sets. Other measures based on language modeling and di-vergence from randomness can outperform BM25 in some circumstances. Despite this evidence, cosine remains the prevalent method for determining inter-document similar-ity for clustering and other applications. However, recent research demonstrates that BM25 terms weights can signif-icantly improve clustering. In this work, we extend that result, presenting and evaluating novel inter-document sim-ilarity measures based on BM25, language modeling, and divergence from randomness. In our first experiment we an-alyze the accuracy of nearest neighborhoods when using our measures. In our second experiment, we analyze using clus-tering algorithms in conjunction with our measures. Our novel symmetric BM25 and language modeling similarity measures outperform alternative measures in both experi-ments. This outcome strongly recommends the adoption of these measures, replacing cosine similarity in future work. I.5.3 [ Pattern Recognition ]: Clustering X  similarity mea-sures, algorithms Experimentation Clustering; Similarity Measures
In recent years, supervised learning-to-rank algorithms have largely replaced unsupervised query-document mea-a wealth of information from years of research into unsu-pervised query-document measures that can be leveraged to enhance results in none-search domains. An example is the use of BM25 [13] as an inter-document similarity measure, such as those used in clustering, as well as many other ap-plications.

Document clustering is a heavily study area of data mining research [2, 7, 8, 14, 15, 16, 20, 21, 24]. In much of this re-search cosine with tf-idf term weighting remains the defacto standard inter-document similarity measure, as well as in many other domains. This fact is surprising, given that it is known that it performs very poorly in the search domain relative to other measures, such as BM25 [13]. BM25, on the other hand, is competitive with supervised learning-to-rank algorithms in some situations [18]. It might therefore be a more reasonable basis for an inter-document similarity measure. Recent research results corroborate this idea [20]. Alternatively, one could also use other unsupervised query-document measures  X  known to outperform cosine with tf-idf term weighting for search  X  and expect similarly im-proved results.

To explore this idea, we present a number of novel inter-document similarity measures based on BM25 [13], language modeling [22], and divergence from randomness ( DFR ) [1]. We provide experiments, including one in the clustering do-main, demonstrating the effectiveness of several of our novel inter-document similarity measures. Our experiments indi-cate that our novel symmetric BM25 and language modeling inter-document similarity measures are the most effective of those we consider in this paper, both for clustering and for more general tasks. This suggests that these measures should replace cosine with tf-idf term weighting in all future work.

The remainder of this paper proceeds as follows. In Sec-tion 2, we discuss preliminaries. In Section 3 we present our novel BM25, language modeling, and DFR based inter-document similarity measures. In Section 4, we show that the nearest neighbors of documents, when using several of our novel measures, are more likely to share the same labels than when using competing measures. This result suggests that they more accurately capture how humans categorize documents. In Section 5, we show similarly positive results with respect to actual clusterings. This result is demon-strated using a number of clustering algorithms and well-studied datasets. For both of our experiments, our symmet-ric BM25 and language modeling inter-document similarity measures out-perform the others. We consider parameter sensitivity in our experiments, showing that our best per-forming methods that require parameters have reasonable universal parameterizations. Finally, we provide a conclud-ing discussion in Section 6.
This section provides preliminaries for the rest of the pa-per, starting with some notation: Let X be a set of doc-uments, and let X i be the i th document of X . Let m be the number of distinct terms in X . We assume each X i is represented using the standard  X  X ag-of-words X  vector space model representation: where x il is the weight of term l to document i .
When using the above formula to measure inter-document similarity, the x il weights are typically based on two values: 1) tf il , the term frequency of l in i , and 2) idf l , the inverse document frequency of l . A standard document clustering definition for idf l , as well as for other domains, is [20, 25]: where n is the number of documents in X , and n l is the number of documents in X that contain term l . Less com-monly, some form of collection frequency is used as part of an x il weight [12]:
The x il weights may be length normalized. This is most often achieved through Euclidean length normalization: but Manhattan length normalization may also be used: A substantial fraction of document vector representations used in inter-document similarity measures are formed us-ing some combination of the above formulae, including the nearly ubiquitous tf-idf term weighted vectors: which are often Euclidean length normalized.

Using vector representations, the inter-document similar-ity between a pair of documents X i and X j can be computed with any number of measures. The most common is cosine: where || X i || 2 is the Euclidean length of X i : Note that this formula makes cosine invariant to Euclidean length normalization. Alternatives to Eq. 7 include extended Jaccard similarity [17]: inverse Euclidean distance: where d euc ( X i ,X j ) is the Euclidean distance between X and X j : and the inverse of the Jensen-Shannon divergence if the vec-tors can be treated as probability distributions: where D ( X i || M ) is the Kullback-Leibler divergence between X i and M , the average of the probability distributions X i and X j : For all the measures discussed above higher values indicate higher similarity. As noted previously, the most widely used inter-document similarity measure is cosine (Eq. 7) with tf-idf representations (Eq. 6).
In this section we present our novel inter-document simi-larity measures. We begin with our BM25 based measures, followed by our language modeling based measures, and fi-nally we present our DFR based measures.
Since its introduction in the early 1990s, the BM25 for-mula [13] has been widely adopted, and it has repeatedly proved its value as a ranking function across a large variety of search domains. If Q is a query consisting of terms, then X  X  X  BM25 score with respect to Q is: where: q is the term frequency of l in the Q , dl i is the count of the tokens in document X i : and avgdl is the average dl for documents in X . The values b , k 1 , and k 3 are tuning parameters, with k 1  X  0, k 3 and 0  X  b  X  1. Typical values are k 1 and k 3 between 1 . 2 and 2, and b = 0 . 7. The q l ( k 3 +1) q often omitted, except when queries are long.

Previous work [9] peripherally suggests that BM25 should be effective in document clustering, but the idea was not carefully explored. More recently, we performed an in-depth study of document clustering term weighting strategies [20], in which we used BM25 term weights of: and: in combination with cosine as inter-document similarity mea-sures. We had positive results using fixed parameter values of k 1 = 20 and b = 1 . 0. In this paper, we implement BM25 directly as a general inter-document similarity measure (i.e., without cosine or any other secondary measure). Previous research has shown that cosine is a poor ranking function relative to BM25 [6], we therefore had a strong rationale for believing that our purely BM25 measure would outperform one synthesizing BM25 and cosine. Our experimental results in later sections support this idea.

To use BM25 in an inter-document similarity measure di-rectly we perform the following adjustments to Eq. 14: 1) We replaced Q with a second document X j , and 2) We replaced b used in each component be equal. The first adjustment is just a notational change. The second adjustment ensures symmetry (S( X i ,X j ) = S( X j ,X i )) by enforcing k 3 equating b . With the above adjustments, Eq. 14 becomes:
OK( X j ,X i ) = This is our first inter-document similarity measure using BM25. Considering that an idf component may not always be beneficial [20], we also implemented a second, tf-only, version: This is our second inter-document similarity measures using BM25.

It is worth noting that for any term l to have a non-zero contribution to OK and OKTF, it must have tf il &gt; 0 and tf jl &gt; 0. This property is preserved from Eq. 14. In con-trast, a previous use of BM25 in an inter-document similar-ity measure [20] did not have this property. Indeed, none of other inter-document similarity measures that we discuss in this paper have this property, making a strong distinction between OK/OKTF and other measures.

In Section 4 and 5, we will show that OK is a highly effective inter-document similarity measure, performing as well as our novel language modeling approaches, and better than all the other alternatives. We show its b parameter may be fixed at 1, regardless of k 1 , with a minimal loss in performance, and that wide range of k 1 values provide reasonable performance. With respect to OKTF, we show it is approximately as effective as the previous use of BM25 with no idf component and cosine [20].
Language modeling ranking functions usually take a fixed query Q and rank X i s by their estimated p ( X i | Q )s, i.e., the probability that their language model generated Q . Using Bayes X  rule, p ( X i | Q ) can be written as: Combining the assumption that p ( X i ) is uniform [22] with the fact that Q is fixed gives: Thus log( p ( Q | X i )) estimates can be (and typically are) used to rank by document relevance against a fixed query in lan-guage modeling ranking functions. Similarly, we use such es-timates in our new general language modeling inter-document similarity measure: where X i and X j are two documents, and X is the entire dataset treated as a single document. Before computing this measure, all documents are Manhattan length normalized to account for varying document lengths.

Specific versions of Eq. 23 are obtained by translating their log( p ( Q | X i )) estimates into a form applicable to a pair of documents, then substituting that form for each of the logs in the equation. LM is a symmetrizing of language model ranking functions that accounts for varying document length and similarity by random chance, where we model chance using log( p (  X  X  X )). Being based on language model ranking functions, which have a strong theoretical backing, makes LM intuitively reasonable as an inter-document sim-ilarity measure.

Many of the commonly discussed log( p ( Q | X i )) estimates in the language modeling literature are amenable to oper-ating on document vectors of Manhattan length normalized term counts, and can thus easily be used with LM. In this paper, we consider using two such estimations: 1) a Dirichlet smoothed estimation and 2) a Jelinek-Mercer smoothed es-timation, both of which can found in Zhai and Lafferty [22]. We chose these because they are well-known and have been shown to provide good performance in a large number of ranking experiments.
 The Dirichlet smoothed estimation of log( p ( Q | X i )) from Zhai and Lafferty [22] may be rewritten as: where X i and X j are two document vectors, p ( l | X ) is the collection probability of term l : and x il = tf il (in our case, this is Manhattan length normal-ized).  X  is the smoothing parameter of the function, with  X   X  0.
A similar rewriting of the Jelinek-Mercer smoothed esti-mation from Zhai and Lafferty yields:  X  is the smoothing parameter, 0  X   X   X  1. For details on the base forms of Eq. 24 and 26, and more details on language modeling ranking functions in general, readers can consult Zhai and Lafferty [22]. In this paper we will denote the use of Eq. 24 in LM as DLM, and Eq. 26 in LM as JMLM.

We will show in our experiments that both our language modeling inter-document similarity measures are competi-tive with OK, and out perform all other measures in this paper. As well, we will show that they function well with a wide range of parameter settings. JMLM will be shown to be particularly robust with respect to  X  variation.
DFR models [1] provide a method of assigning term weights to documents based on a comparison of their within-document frequency and their collection frequency (Eq. 3). The first step in DFR term weighting is usually to apply length nor-malization to raw term counts (as opposed to after term weighting, as is typical in other domains such as document clustering). The standards for this are Manhattan (Eq. 5) and a log normalization of the form: where c is some positive constant (1 is a common value). After normalization, the final DFR term weight is typically assigned as: P R estimates the risk in assessing l  X  X  relevance to X i . P computes the log of the probability of seeing tf il occurrences of l in X i under a specific model of randomness. There are various methods of computing P R and P M . Table 1 lists the ones we consider in this work, for others readers can consult Amati and van Rijsbergen [1].
 Table 1: Our DFR term weighting components.

By summing over query terms, Eq. 28 produces the rank-ing function: The above is known to be competitive with the BM25 rank-ing function (Eq. 14) when using a variety of P R and P M functions [1]. Given this experience, it makes sense that one could also use DFR for term weighting in inter-document similarity measures. In the same vein, we combined Eq. 28 term weightings with cosine, Jaccard, and inverse Euclidean distance to produce inter-document similarity measures.
We crossed all the P M s and P R s from Table 1 with four length normalizations: none, Manhattan (Eq. 5), Euclidean (Eq. 4), and log (Eq. 27 with c = 1). Length normalizations were applied before Eq. 28, as is standard in DFR. Each weighting was used with cosine, Jaccard, and inverse Eu-clidean distance to produce a total of 72 (2x3x4x3) distinct DFR similarity measures. We denote a specific DFR inter-document similarity measure as: DFR-A -B -C -D , where A is the P M used, B is the P R used, C is the length normal-ization, and D is the measure.

Our experiments will show that a variety of our DFR methods are effective, with the best out-performed only by our BM25 and language modeling similarity measures. This result, combined with the fact that DFR methods require no tuning parameters, makes them appealing as inter-document similarity measures.
In order to evaluate our inter-document similarity mea-sures, we conducted two experiments. Our nearest neighbor experiment is detailed in this section. The next section dis-cusses our clustering experiment.

The idea behind our nearest neighbor experiment is as follows: The better a measure is for a given domain, the better it will reflect expert human notions of similarity in that domain . To evaluate how well our inter-document sim-ilarity measures performed in this respect, we selected eight datasets that have been used in many previous document clustering experiments [2, 7, 16, 20, 21, 23, 24, 25]. Table 2 summarizes their properties 1 . These datasets are not clus-tering specific. The fbis, k1b, tr31, tr41, and tr45 datasets are from TREC 2 , the wap dataset is from the Web-Ace project [3], and the re0 and re1 datasets are taken from the Reuters-21578 text categorization text collection 1.0 3 . For more details on these datasets, readers should consult Zhao and Karypis [25].

For each dataset, for r = 1 to 100, we computed the r -nearest neighbors for each document of the dataset when using our inter-document similarity measures. We then com-puted the average fraction of r -nearest neighbors per docu-ment for that dataset sharing the same label as that docu-ment for each r value. Finally, we averaged the by-dataset results to obtain a single accuracy value for each r . The better our measures are performing, the higher their accu-racies. With respect to parameter ranges tested, for OK and OKTF, we performed this test using a range of k 1 = 0 to 50
All of these are available at: http://glaros.dtc.umn.edu/ gkhome/views/cluto/download http://trec.nist.gov http://www.research.att.com/~lewis Table 2: The datasets used in our experiments.
 in increments of 2, combined with a range of b = 0 to 1 in increments of 0 . 1. For DLM, we tested using  X  values of 0.1, 0.2, 0.4, 0.8, 1.6, etc., up to 6553 . 6. For JMLM, we tested using  X  = 0 . 1 to 0 . 9 in increments of 0 . 1.

We performed an analysis identical to the above using a wide variety of inter-document similarity measures. Ta-ble 3 provides details. Among the measures are the nearly ubiquitous cosine with tf-idf term weighting (TIC), and a use of cosine with BM25 term weights known to outperform it (OKC) [20]. For OKC and K1C, we investigated the same parameter ranges as we did for OK and OKTF.

Table 4 presents the top 10 measures, by average accuracy from r = 1 to r = 100, from our nearest neighbor experi-ment. TIC is included in the table for comparison. Only the single best global parameter setting for each measure is reported in the table, as that is our estimation of the performance one can expect in practice (without by-dataset tuning).

The key result to take from Table 4 is the significance hierarchy of the best performers:
OK &gt; { JMLM , DLM } &gt; All of the members of this hierarchy, except OKC, were de-signed for this paper, and all produce significantly and sub-stantially better nearest neighborhoods than tf-idf with co-sine. This outcome strongly suggests that any of them would make a better inter-document similarity measure than tf-idf with cosine, independent of the application. Note that pa-rameter effects on this hierarchy will be discussed below. We will show that it holds for a range of settings for each method, and that parameter selection for individual meth-ods is straightforward.

From Table 4, it is clear that OK performed the best. It was significantly better than all the other top inter-document similarity measures, and might be viewed as the best inter-document similarity of those we consider, at least from an application-independent standpoint.

OK was significantly and substantially better than OKC, corroborating our suggestion in Section 2 that it would be better to use BM25 directly in an inter-document similarity measure instead of combining it with cosine. For the BM25 measures we tested that had no idf component, OKTF did not perform significantly different from K1C (0.671 versus 0.672 average accuracy, with individual results always within 1%). Both were significantly worse than TIC (.676).
With respect to parameters, we found that OK exhibited uniformly strong by-dataset performance with its optimal parameter setting, being at the worse 2.5% behind the high-est accuracy method for any r value on any dataset. Fix-ing b = 1 . 0 in OK produced excellent results. At most, this resulted in a 2% degradation in accuracy relative to the optimal b for a particular k 1 value (see Fig. 4). Al-though OK performed the best when b = 1 . 0 and k 1 = 8, it was still significantly superior to all the other methods when 4  X  k 1  X  12 and b = 1 . 0. OKTF X  X  best parameter settings were similar to OK.
 Figure 1: OK X  X  average accuracy as k 1 and b vary.
Both our language modeling inter-document similarity mea-sures performed well, obtaining the second and third high-est average accuracies respectively (.691 and .691). Their results were not significantly different from each other, but were significantly better than all the other top measures ex-cept OK.

Fig. 4 shows the parameter sensitivity of our language modeling measures with respect to average accuracy. It is notable from the figure that JMLM behaves almost identi-cally from  X  = 0 . 4 to 0 . 7, with no statistically significant differences in that range. This suggests that any  X  in that range is a reasonable universal parameter setting. For DLM, it is apparent that larger values of  X  are harmful to results, with a reasonable range being 0.8 to 3.2. Note that these  X  values are substantially smaller than those performing well in DLM X  X  corresponding ranking function because we length normalize documents before computing DLM, whereas the ranking function version does not.

Many of our DFR inter-document similarity measures had robust performance in our nearest neighbor experiment. Six of the top 10 methods by average accuracy were DFR mea-sures, with all P M and P R s being used at least once in those six. DFR-In-B-LOG-COS, DFR-In-B-LOG-JAC, and DFR-In-L-EUC-JAC performed significantly better than any of the other DFR methods, but were inferior to OK and our language modeling measures.

DFR measures that did not use length normalization, or used inverse Euclidean distance similarity, exhibited poor performance in the experiment. It is worth noting that P seemed to dictate the optimal normalization. When B was used, log length normalization was best, whereas when L was used, Euclidean normalization was best. The difference from using a non-optimal normalization was often large.
Considering just the inter-document similarity measures in Table 3, OKC performed the best. It was significantly higher than the nearest competitor, TIJM (.682 versus .677). TIJM, on the other hand, was a member of a group of methods with no statistical difference between them: TIJM, TIJ (.676), LIJM (.676), TIEN (.676), TIC (.676), and TIJN (.676). It is noteworthy that TIC was among these methods. Table 3: Alternative inter-document similarity measures. cf Short Length Short Length Name x il Norm. Measure Name x il Norm. Measure BE 1, if tf il &gt; 0, else 0 None sim euc BEN 1, if tf il &gt; 0, else 0 Euclidean sim euc TE tf il None sim euc TEN tf il Euclidean sim euc LE log(1 + tf il ) None sim euc LEN log(1 + tf il ) Euclidean sim euc
TIE tf il log( n n TIEN tf il log( n n
LIE log(1 + tf il ) log( n n LIEN log(1 + tf il ) log( n n
IE log( n n IEN log( n n BJM 1, if tf il &gt; 0, else 0 Manhattan sim jac BJN 1, if tf il &gt; 0, else 0 Euclidean sim jac TJM tf il Manhattan sim jac TJN tf il Euclidean sim jac LJM log(1 + tf il ) Manhattan sim jac LJN log(1 + tf il ) Euclidean sim jac
TIJM tf il log( n n TIJN tf il log( n n
LIJM log(1 + tf il ) log( n n LIJM log(1 + tf il ) log( n n
IJM log( n n IJN log( n n BC 1, if tf il &gt; 0, else 0 None sim cos BJ 1, if tf il &gt; 0, else 0 None sim jac TC tf il None sim cos TJ tf il None sim jac LC log(1 + tf il ) None sim cos LJ log(1 + tf il ) None sim jac
TIC tf il log( n n TIJ tf il log( n n
LIC log(1 + tf il ) log( n n LIJ log(1 + tf il ) log( n n
IC log( n n IJ log( n n TJS tf il Manhattan sim js
Our nearest neighbor experiment from this section was not clustering specific. As such, its key result strongly suggests that its best performers (OKC, DLM, and JMLM) should replace tf-idf with cosine as default application independent inter-document similarity measures. In the following sec-tion, we will present a document clustering specific experi-ment that corroborates this key result, and others discussed in this section, for the domain of document clustering.
For our clustering experiment, we used the same datasets, inter-document similarity measures, and parameter ranges as our nearest neighbor experiment. We drew 10 samples of each dataset, of size equal to half the dataset, and com-puted a document similarity matrix for each sample us-ing each inter-document similarity measure/parameter set-ting. These matrices were clustered using four algorithms: UPGMA, complete-linkage, direct e1, and agglomerative i2. We selected these four as they are well-known and tested in document clustering [2, 7, 16, 20, 23, 24, 25]. For each clus-tering algorithm, we obtained its clustering of each matrix with five, 20, 40, and 80 clusters.

We then computed the correspondence each clustering had with its sample labeling using clustering purity ( PQ [23]), clustering entropy ( EQ [20]) adjusted mutual information ( AMI [19]), and the Rand index ( RI [11]). In order to per-form a similarity-method focused analysis, we further nor-malized each correspondence measure result by the maxi-mum that correspondence measure obtained on the same parameters, except when using any similarity method. This procedure was similar to that used by Zhao and Karypis [23] to aggregate correspondence measures from clusterings on different datasets with different numbers of clusters. To avoid confusion with the base correspondence measures, we refer to the normalized correspondence measures as relative .
As a final step we obtained the combined average of the four relative correspondences for each inter-document sim-ilarity measure over its 1280 clusterings (8 datasets x 10 samples x 4 clustering algorithms x 4 number of clusters). The higher the combined average for a particular measure, the better it was performing as its clustering tended to bet-ter reflect true labelings.
 Table 5 presents the top 10 measures by combined average. As in our nearest neighbor experiment, we only report the best performing parameterizations in the table. normalized documents.
 normalized documents.

Similar to the results in Table 4, those in Table 5 indicate that several of our novel inter-document similarity measures are superior to the alternatives. The best performers in Table 5 have a significance hierarchy of: Again, OKC is the only method in the hierarchy not de-signed for this paper. One can see that the top three mem-bers of the clustering hierarchy, DLM, JMLM, and OK, are identical to those from the previous experiment. Further, when considering each individual correspondence measure, those three still take up the top three ranks (although their order relative to each other changes). This indicates that DLM, JMLM, and OK are robust inter-document similarity measures for clustering. They can and should be adopted as the default inter-document similarity measures in clustering.
OK performed 3rd best overall in the experiment (.916 combined average). OKTF, on the other hand, exhibited very poor performance (.874 combined average). It was no-tably worse than many of measures in Table 4.

For parameters, we found that for OK, fixing b = 1 . 0, in-dependent of k 1 , was reasonable. Given this corroboration with the previous experiment, we argue that it makes sense to view OK as a single parameter function, having a fixed b and taking just a k 1 parameter. Again, similar to the pre-vious experiment, we found that a wide range of k 1 values performed well. However, for this experiment OK was even less sensitive to k 1  X  X eyond a threshold of approximately k 1 = 8, results were very similar. Fig. 5 gives an exam-ple of this behavior. The behavior of k 1 is consistent with previously the observed behavior of k 1 for OKC [20].
Our language modeling similarity measures performed well in the clustering experiment, with JMLM being 2nd in com-bined average (.917), and DLM being 1st (.922). Fig. 5 shows their combined average as their parameters vary. It is notable from the figure that JMLM X  X  combined average is only mildly sensitive to  X  . It varied by less than 1% over the entire range of  X  we tested, although we observed a slight in peak at  X  = 0 . 7. This insensitivity is consistent with that of the previous experiment. It makes using JMLM less wor-risome in practice, as none-optimal  X  s seem to not degrade its results much. On the other hand, DLM X  X  performance fluctuated much more noticeably, with its optimal range be-F igure 2: JMLM and DLM X  X  average accuracy as their respective parameters change.  X  is based on Manhattan length normalized documents.
 F igure 3: OK X  X  combined average as k 1 varies and b = 1 . 0 . ing higher than the previous experiment (1.6 to 12.8 versus 0.8 to 3.2 previously). However, it also had a significantly higher combined average for all parameter settings in that range than the highest combined average from JMLM.
With respect to our DFR inter-document similarity mea-sures, they performed slightly worse than in the previous ex-periment. Their best performer, DFR-In-L-EUC-COS (.907), was significantly worse than DLM, JMLM, and OK. How-ever, three DFR measures were still significantly superior to TIC. We noticed that DFR measures using log length nor-malization performed poorly relative to the last experiment, whereas those using Euclidean length normalization ranked approximately the same as the previous experiment. For ex-ample, DFR-In-L-EUC-JAC was ranked 7th in the previous experiment, and 6th in this one.
 Figure 4: JMLM and DLM X  X  combined averages as their respective parameters change.  X  is based on Manhattan length normalized documents.

Considering just the methods in Table 3, OKC was, again, the best performer (.909). TIEN was the next best (.899), ranking 8th, with only a small (but still significant) differ-ence between it and LIEN and TIC. That TIC was in the top 10 is noteworthy. Given that all the inter-document similarity measures that substantially outperformed TIC in our experiments were either designed in this paper, or only recently shown to be superior to it in clustering (OKC [20]), provides some rationale for its nearly ubiquitous use in clus-tering to date. All the other measures in Table 3 were no-tably worse than TIC (the next best was LIEN at .892).
Besides those results previously discussed, we found a few noteworthy points about the inter-document similarity mea-sures in Table 3 from our experiments that we believe may apply in general: 1. Inter-document similarity measures using binary term 2. Euclidean and Manhattan length normalizations were 3. Including an idf component with a term frequency 4. The collection frequency component, as we implemented 5. Logging term frequency had only a small effect on
Summarizing the key result from the previous sections, our experiments indicate that OK, DLM, and JMLM are the best of all the inter-document similarity measures that we considered in this paper. Each has easily selected pa-rameters and similar overall performance. They should re-place cosine with tf-idf, as well as the other measures we considered, in clustering, as well as in general, as standard inter-document similarity measures. We also showed that best of our DFR measures were competitive with the best inter-document similarity measure we found in previous re-search, OKC (BM25 term-weighting with cosine). It should be noted that these conclusion are based on our specific datasets, each of which contains relatively long documents. It is possible that different similarity measures are better for shorter text sources such as tweets or forum posts. Addi-tionally, length normalizations might need to be altered, or even omitted, in most of the similarity measures we consid-ered in this paper to effectively handle the smaller length of such text sources. If this is the case is an avenue of future research.
Despite query-document measures having moved beyond simple techniques such as cosine with tf-idf term weight-ing, the related area of inter-document similarity measure research still makes frequent use of such measures. In this paper, we focused on improving inter-document similarity measures by leveraging knowledge available from research on query-document measures. We implemented novel inter-document similarity measures based on BM25, language mod-eling, and divergence from randomness ranking functions, all of which are known to substantially and significantly outper-form tf-idf with cosine in search.

We tested our novel inter-document similarity measures in a general experiment, as well as one focused on clustering specifically. The key result from those experiments was that OK, JMLM, and DLM are highly effective inter-document similarity measures, outperforming cosine with tf-idf term weighting, as well as a large variety of other inter-document similarity measures. Furthermore, our experiments show that reasonable universal parameter ranges exist for each of those measures, allowing them to be applied in practice more effectively. Together, these facts led us to conclude that they should replace the current standard inter-document similar-ity measures such as cosine with tf-idf term weighting.
One interesting avenue of future research is investigating previous research conclusions based on older inter-document similarity measures. For example, Zhao and Karypis [23, 24, 25] present a series of objective functions for document clustering which are evaluated using cosine with tf-idf term weighting. However, our preliminary tests suggest that when replicating their experiments with our better inter-document similarity measures, results will not only globally improve, but also change some conclusions drawn from their experi-ments. Specifically, the optimal objective functions change. We believe investigations like this one are necessary for inter-document similarity measures to improve.

Another possible avenue of research is the analysis of inter-document similarity measures in datasets where ground truth similarity is (or will be) defined (or tested) on a document pair level using concepts such as relevance. For example, in the TREC Web Track diversity task dataset 4 , documents have relevance judgements with respect to subtopics. The overlap in subtopics might be viewed as a supervised inter-document similarity measure, and could be compared to unsupervised inter-document similarity measures using ex-periments similar to the ones we performed in this paper. Finally, we are considering the design of document cluster-ing algorithms based on language modeling, as well as those exploiting learning-to-rank methods. [1] G. Amati and C. J. V. Rijsbergen. Probabilistic [2] F. Beil, M. Ester, and X. Xu. Frequent term-based [3] D. Boley, M. Gini, R. Gross, E.-H. Han, K. Hastings, [4] C. J. C. Burges, R. Ragno, and Q. V. Le. Learning to [5] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, [6] C. L. A. Clarke, G. V. Cormack, and S. B  X  uttcher. [7] B. C. M. Fung, K. Wang, and M. Ester. Hierarchical [8] X. Hu, X. Zhang, C. Lu, E. K. Park, and X. Zhou. [9] R. Jin, C. Falusos, and A. G. Hauptmann.
 [10] T. Joachims. Optimizing search engines using [11] W. M. Rand. Objective criteria for the evaluation of [12] J. W. Reed, Y. Jiao, T. E. Potok, B. A. Klump, M. T. http://plg.uwaterloo.ca/~trecweb/2011.html [13] S. E. Robertson, S. Walker, S. Jones, [14] X. Sevillano, G. Cobo, F. Al  X  X as, and J. C. Socor  X o. [15] N. Slonim and N. Tishby. Document clustering using [16] M. Steinbach, G. Karypis, and V. Kumar. A [17] A. Strehl and J. Ghosh. Value-based customer [18] K. M. Svore and C. J. C. Burges. A machine learning [19] N. X. Vinh, J. Epps, and J. Bailey. Information [20] J. S. Whissell and C. L. A. Clarke. Improving [21] W. Xu, X. Liu, and Y. Gong. Document clustering [22] C. Zhai and J. Lafferty. A study of smoothing [23] Y. Zhao and G. Karypis. Criterion functions for [24] Y. Zhao and G. Karypis. Evaluation of hierarchical [25] Y. Zhao and G. Karypis. Empirical and theoretical [26] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen,
