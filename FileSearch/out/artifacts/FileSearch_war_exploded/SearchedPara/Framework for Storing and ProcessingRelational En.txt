 Stream mining algorithms are gaining in importance on many research and appli-cation fields. However, traditional stream mining makes the limiting assumption that a data instance is seen only once. In relational learning, the same entity (a customer, a patient, etc.) is observe d many times, each time associated with additional data that should be taken into account during learning. This raises new challenges: (a) how to enrich a relational entity with all information known about it, although space and time are limited? (b) how to choose the entities that are most important for learning, since we can never consider all of them? In this study, we propose a multi-threaded framework that prepares and prioritizes relational entities for stream mining.

Relational learning is a mature field, but has only recently been perceived in the stream learning context [SS09, FCAM09, SS10, SS11, IDDG11], although learning on a drifting stream of relational entities is demanded in many ap-plications. Consider, for example, churn prediction as experienced e.g. in the telecommunications industry or in banking. Companies attempt to predict the likelihood that a customer will continue or discontinue a contract. Obviously, this prediction is regularly done for each customer, each time considering the new activities of the customer and combining them to past activities. Making the right prediction for a customer x allows to use the predictor on customers, who later behave similarly to x . We generalize this example to the Stream Learning Problem for Relational Entities as follows: In the churn example, the customers constitute the target stream T ; their service invocations, hotline requests, money transfers etc. constitute further streams of activities. These activities are exploited by  X  T ( t ) to predict whether the label of customer x observed at t is continue or discontinue (i.e. quit the contract).
The stream learning problem for relatio nal entities requires a tight and effi-cient coupling of stream preparation wi th stream mining. As mentioned at the beginning, space is limited and execution speed is an issue: how should a re-lational entity be expanded with all information seen on it thus far, so that it is made available to the learner? We propose a four-layer architecture with parallelizable components. Two layers of the architecture are responsible for (a) accommodating and (b) fetching relational entities from secondary storage, whenever activities referencing them are encountered; the other two layers are responsible for (c) queuing activities and (d) extracting information from the ac-tivities before discarding them. A core el ement of the framework is the weighting scheme responsible for the prioritization of relational entities that are expanded and delivered to the learner at each mome nt. The prioritization is motivated by the fact that the stream learner can never process all entities that have ever been seen, and by the fact that the model to be learned at a given moment should not consider entities that have not been referenced for a long while. Hence, our framework can be coupled with an arbitr ary stream learner , whereby it handles the management and prioritization of past information for the relational stream.
In the next section we explain the context of this publication and related works. The new framework and its archit ecture are presented in section 3. Section 4 encompasses evaluation of the new framework and a description of a data generator, which has been developed for this purpose. In the last section we conclude and summarize the work and discuss a possible, further development. Learning on multiple interrelated streams is a very new problem, mostly called  X  X elational stream mining X  or  X  X ulti-relational stream mining X . In [SS09], Sid-diqui et al. stress the fact that the relational entities re-appear, by calling them perennial , as opposed to the ephemeral transactions that reference these entities. We adopt this terminology, and use the terms  X  X erennial entity X ,  X  X erennial X  and  X  X elational entity (of the target stream) X  interchangeably hereafter; the terms  X  X nstance X  or  X  X phemeral instance X  we us e for elements of the other streams.
First solutions to the Stream Learning Problem for Relational Entities, as we described it in the introduction, have been proposed by Siddiqui et al. in [SS09], where they studied stream clustering, and in [SS10], where they proposed a decision tree classifier for multi-relati onal streams. More r ecently, Ikonomovska et al. proposed following task:  X  X or each stream, determine the amount of facts that a relational incremental learner needs to observe at any point in time in order to be able to infer a correct model of the target function X  [IDDG11, page 698]. Their goal is to minimize the information delivered to the learner, while our goal is to deliver only the entities relevant at some moment -but keep all information on these entities intact for the learner to exploit. Accordingly, we concentrate on lossles s reconstruction of the relational entities.

However, the demand for recalling past entities does not always appear. For example, the method of Fumarola et al. on a ssociation rules discovery over multi-relational streams [FCAM09] only deals with the entities that are inside the win-dow. The demand of recovering past enti ties emerges through the need to adapt the model on instances that reference entities seen earlier. For example, consider a decision tree based strea m classifier that distinguishes between low-risk and high-risk customers. The customers constitute the target stream , the entities of which should be associated to the fastest transaction stream(s) of the cus-tomers 1 Whenever a new transaction for this customer appears, the customer X  X  entity must be reconstructed and and its label must be predicted again, given the entire information known on this customer -the classifier may then predict for this customer a different label than it has predicted earlier. Multi-stream classification algorithms [ IDDG11, SS10] either oper ate on the most recent data (at the cost of information loss, especially for entities that re-appear at very slow pace) or require an entity reconstruction algorithm.

Incremental entity reco nstruction algorithms have been proposed in [SS09, SS11]. The former is an incremental version of a join-like operation called propositionalization [KRv + 03], which essentially extends the schema by turning values into columns/features.

The incremental propositionalization method of [SS09] ensures that the schema does not grow in an unbounded way by fixing the size of the feature space and clustering the values of similar entities into the fixed features. The CRMPES method of [SS11] rather identifies values that predict the label with classifica-tion rule mining, and uses these values as features. While this method requires much less space than the former, it is by nature sensitive to drift, and may lose important information on rarely re-appearing entities. In this study, we focus on entity reconstruction for multi-stream classification without information loss .We therefore build upon the former method, by establishing a database architecture that prepares the interrelated streams for adaptive learning and ranks entities on their expected importance for adaption.

In studies on database querying over streams, we find heuristics that prioritize the entities to compute approximate joins [DGR03, XYC05]. These heuristics predict which of the entities seen thus far will be needed for the join computation and which will not: the former are retained in the cache, the latter are kept in secondary storage. Their objective is to minimize the number of accesses to the secondary storage. Das et al. propose several heuristics that rank entities on the likelihood of being needed for the join computation [DGR03]: PROB assigns higher rank to those entities of the one stream that are frequently referenced from the other stream. LIFE [DGR03]computes for each entity the time it will stay inside the window 2 ; LIFE chooses entities whose expected remaining  X  X ifetime X  in the window is longer. The  X  X euristic of Estimated Expected Benefit X  HEEB [XYC05] exploits past knowledge to compute the likelihood that an entity being observed now will be needed in near future. HEEB comes closest to the demands of our stream classification scenario: if an entity x of the target stream has been often referenced from the other stre ams in the past, it is likely that many instances refering to it will arrive in the future.
 One pitfall of the HEEB weighting scheme is that it does not account for drift. In this study, we abstract drift as the ageing of some part of the model, until this part becomes completely obsolete. The anytime stream learner ClusTree uses an exponentially non-increasing function  X  (  X t )=  X   X   X  X t to assign an age-based weight to micro-clusters [KABS09]. One-stream classifiers based on VFDT [DH00] discard subtrees that have no t received instances for a while [HSD01, GRM03], while the multi-stream classifier TrIP computes the age of a subtree on the basis of the age of the entities in it, since entities may re-appear [SS10]. We similarly take account of an entity X  X  age with help of an exponentially non-increasing decay function, and re-juvenate entities as they re-appear. Conventional stream mining algorithms learn a model by sliding a window over the arriving data instances. For the learn ing problem introduced in Section 1, the model is learned on the permanent relational entities and is updated as new instances referencing these entities a rrive. Hence, instead of sliding a window over the arriving instances, we need to fetch the referenced entities for learning and adaption. We use a cache to accommodate th e referenced entities: if a rela-tional entity is referenced by an arrived instance, then this entity is fetched from the database to the cache, and is extended with the information of the arrived instance. This aggregation of new information is implemented as an incremental propositionalization mechanism [KRv + 03, SS09], which is part of our Four-Layer Architecture for relational entity preparation. This architecture and the func-tionalities of its components are depicted in Figure 1 and described below. In the following, we use the term  X  X erennial X  for the relational entities of the target stream, to stress their permanent nature, and  X  X phemeral X  for the arriving in-stances, to stress that they are seen and forgotten, as is the case for conventional stream data [GMM + 03]. 3.1 Four-Layer-Architecture The Ephemeral layer consists of streams of ephemeral instances. It is respon-sible for supplying the framework with data. This layer can be physically dis-tributed over many computers in a networ k. In the churn prediction scenario, for instance, the ephemerals represent mon ey transfers, transactions or hotline requests recorded at different servers.
 The Queue layer consists of three queues. The ephemerals delivered by the pre-vious layer are stored in the Ephemeral-Queue, from where they are extracted by further threads of the framework. The extracted ephemerals are then propo-sitionalised into the corresponding perennials using the mechanism of [SS09]. A selection of perennials is stored in the cache and used for model learning; all other perennials remain stored in the database, and fetched through an update action (carried out by a further thread group). The Ephemeral-Queue guarantees the separation of the mining algorithm from the stream management process. We have further queues, used for actions for adaption and for updating ,which are performed by other threads:  X  Actions for adaption encompass learning and forgetting. They process a  X  Update actions serve the purpose of updating perennials stored in a database The Queue Layer is responsible for the parallelization of processes within the framework. Each queue is served by its own thread group. The distribution of the tasks among many thread groups leads to the reduction of the computation time on multi-core processors.
 The Cache layer serves following purposes. First, it speeds up the access to the perennials used for learning; these are selected with help of the weighting function presented thereafter. By delivering perennials to the learning algorithm, the cache layer is responsible for the process of model adaption to concept drift.
Second, the cache implements the operation of sliding a window over arriving stream instances. Since we st udy a learning problem upon relational entities, we cannot use a conventional window. Rather, as stream instances, i.e. ephemerals, arrive, the entities referenced by them (i n the churn prediction example: already seen customers, or new ones) are presented to the mining algorithm. To prevent cache overflow and stick to the most important entities for learning, we define a weighting function that decides which perennials should enter the cache and which ones should be moved to the database.

Our framework allows to define an arbitrary weighting function. For exam-ple, in the churn prediction scenario , the human expert may decide that the most important customers for learning a re not those observed most recently but those most active during the whole observation period. In this study, we propose following exponential function to compute the weight of a perennial p : where p a is the age of p , i.e. the elapsed time since the perennial was referenced for the last time, while p s is the support of p , defined as the number of ephemerals referencing p thus far. The parameter  X  expresses the preferences of the analyst (or the decision maker) regarding relative importance of age versus support. The Persistent layer is responsible for the management of perennials. The rela-tional entities seen thus far may not be deleted, but it is not possible to keep all of them in main memory. Therefore, we use a database as a persistent storage. When a perennial is referenced by an ephemeral that has just arrived in the stream, then an  X  X pdate action X  is created and stored in the Update-Queue. From there the update actions are extracted by a thread group that is respon-sible for aggregating new information to the perennials, using the incremental propositionalization of [SS09]. After such an update, the weight of a perennial may change. If the new weight is larger th an the lowest weight in the cache, then the perennial with the highest weight in the database replaces the perennial with the lowest weight in the cache. To avoid too frequent switches between database and cache, we perform a  X  X witch-test X  , where we check whether the difference between the two weights is significant.
 Example 1. Consider the aforementioned churn prediction scenario, where the label of a customer has to be predicted. The challenge is here to combine all data that comes from multiple streams and relations belonging to this customer efficiently and learn upon those data in real time.

The process starts at the Ephemeral Layer, i.e. as a new transaction by the given customer arrives. Subsequently, this ephemeral object is stored in the Queue Layer, where it awaits the propositionalization. Hence, the correspond-ing perennial (the customer) has to be retrieved from the cache or from the database (Persistent Layer) and updated using the new transaction in course of the propositionalization. After that, the customer object can be stored back into the cache or into the database. The data mining model is kept consistent with the data in the cache, which plays the role of sliding window : every change of the data in the cache has to be followed by an update of the model. Therefore, an update action is created and stored in the Queue Layer. From there it is re-trieved and performed by a different, parallel thread. Then, if the weight of the customer object (cf. Eq. 1) is high, the u pdate action concludes by storing the object into the cache and incorporating it into the model. Finally, the object X  X  label is derived. 3.2 Data Flow within the Framework After explaining the internal structure and architecture of our framework, we now focus on the data flow and processes within the framework, as depicted in Figure 2. The work of the framework starts with the arrival of a new ephemeral instance, which is subsequently stored in the Ephemeral-Queue. From there it is extracted by another thread that carries out the propositionalisation. First, the location of the referenced perennial has to be determined. The thread checks whether the perennial is in the cache. If this is the case, the propositionalisation is performed immediately, the weight of the perennial is updated, and the perennial is saved back into the cache. If the referenced perennial is not in the cache, then it must be in the database. In this case, an update action has to be created and stored in the Update-Queue. Another thr ead group extracts the update actions from the queue and picks the referenced perennial(s) from the database.
Following case may occur: the perennial has been already moved into the cache, while the update action was waiting in the Update-Queue. Therefore, a further check is necessary. Thereafter, the perennial is updated using propo-sitionalisation, and a switch-test is p erformed to check whether the perennial should be moved to the cache.

Cache overflow prevention: If the perennial has high weight (Eq. 1) and must be kept in the cache, we check whether the cache is overfilled. In such a case, we move the perennial with the lowest weight back to the database. Additionally, we launch a forgetting action (stored in the Adaption-Queue), so that this perennial is no more considered in the current model.
If a new perennial is saved in the cache, then also an adaption action is put into the Adaption-Queue. The last group of threads executes the actions in the Adaption-Queue and predicts the perennials X  labels. This is repeated as long as there are new unprocessed ephemerals, whereby the threads run in parallel. To evaluate the performance of our framework in a controlled way, we developed a data generator that creates streams that change their speed and exhibit drift. The generator is presented first. We then present the experiments on execution speed and result quality, for which we coupl ed our framework with the relational stream classifier TrIP [SS10]. We used an Intel i5 with 2.4 GHz (2 cores and 4 parallel threads) and 4 GB RAM. 4.1 Data Generator Our data generator takes as input a number of perennials and generates a stream of ephemerals which reference them. Although the streams are by definition infinite, an obviously finite number of ephemerals is specified in order to ensure that the generator terminates its work and results can be seen.

The number of classes of perennials and the number of feature space dimen-sions are also input parameters. The classes follow the Gaussian distribution. Furthermore, the gener ator simulates concept drift by shifting the  X  parameter of the class distribution by a given value. We introduce several parameters to govern concept drift, and use a simple notation, which we call  X  X rift string X , to set them. The parameters are: the time point s when the concept drift starts; the time point e when the concept drift ends; the velocity of the drift v (number of units a class center is shifted at each time point); the class c affected by the drift, and the attribute a affected by the drift. Hence, the drift string has the form: s&lt;s&gt;e&lt;e&gt;v&lt;v&gt;c&lt;c&gt;a&lt;a&gt; . 4.2 Reducing Computation Time through Multi-threading To measure how our four-layer-architecture (4LA) reduces computation time, we implemented a baseline one-thread-architecture (1TA) with the same func-tionality as 4LA.

The first parameter affecting the computation time of the framework is the cache size. When the cache is small, then many perennials have to be moved to the database, thus increasing computat ion time. The other im portant parameter is the number of perennials: if they are only few, then it is more likely that a new ephemeral will reference a perennial that is already stored in the cache.
In Table 1, we show the impact of the parameters on the computation time of 4LA and 1TA. Two cases have to be distinguished. In the first case, the cache is so large that it accommodates all perennials (black numbers, below the diagonal). In the second case, the number of perennials exceeds cache size (blue numbers, above the diagonal), so some perennials had to be moved to the database, increasing computation time. Red numbers denote the best relative improvement of 4LA over 1TA. In the first case (below the diagonal), the im-provement on computation time reached 97.13%. In the second, more realistic case, the relative improvement reach ed 94.96%, i.e. 4LA needed only 5.04% of 1TA X  X  computation time. 4.3 Dealing with Speed-Up of the Stream A further advantage of our 4LA framework is that it smooths temporal speedups of the ephemeral streams. We have simulated such a speedup by reducing the elapsed time between the arrival of two ephemerals, and compared the compu-tation time of 4LA to 1TA. When the stream speed becomes higher than the processing speed of the miner, then further actions that cannot be carried out immediately, are cumulated in the queues. Following experiment shows that our new architecture can cope with a t emporal speed-up of the stream.

A peak in the speed of the stream of ephe merals was simulated using nor-mal distribution (cf. Figure 3a) over the waiting time between generating two ephemerals. Thus, the peak of the distribution points to the bottom of the page. Thereafter, the processing speed of th e 4LA and 1TA were measured. The stream speed is represented by the blue curve in Figure 3b. It is apparent that the pro-cessing speed of the 4LA (red curve) is not as high as the stream speed. An advantage of the new framework shows at this stage -the framework does not collapse under the high load of ephemerals, but it rather starts to cumulate the ephemerals in queues. When the speed of the stream becomes lower again, the framework processes the ephemerals from the queues, what is apparent from the shift of the red curve to the right of the blue curve. For the contrast, the maximal processing speed of the 1TA has been depicted by the orange line in Figure 3b. We described a novel framework for storing and processing relational entities in stream mining, based on a four-layer-architecture. Due to a partial parallelization of processes within the framework an essential reduction of computation time was possible. The computation time was reduced by up to 97.13%. Thanks to the usage of the queue layer the framework gained the ability to cope with streams witch changing speed -an issue of particu lar relevance for real-world scenarios. Furthermore, we used a new weighting function that prioritizes the entities to be used for learning, giving preferen ce to those most recently referenced.
Our framework is scalable and appropriate for multi-core processors. As future work, we want to extend it so that it runs on many computers in a network. This will not only reduce computation time, but it also will allow us also to delegate part of the stream classification itself to a network of computers. While distributed data mining has been widely investigated for static data, stream mining and the incremental propositionalization step in the preprocessing phase incur additional coordination overhead that has yet to be investigated.
