 In this paper we address the problem of detecting topics in large-scale linked document collections. Recently, topic de-tection has become a very active area of research due to its utility for information navigation, trend analysis, and hi gh-level description of data. We present a unique approach that uses the correlation between the distribution of a term that represents a topic and the link distribution in the ci-tation graph where the nodes are limited to the documents containing the term. This tight coupling between term and graph analysis is distinguished from other approaches such as those that focus on language models. We develop a topic score measure for each term, using the likelihood ratio of binary hypotheses based on a probabilistic description of graph connectivity. Our approach is based on the intuition that if a term is relevant to a topic, the documents con-taining the term have denser connectivity than a random selection of documents. We extend our algorithm to detect a topic represented by a set of terms, using the intuition tha t if the co-occurrence of terms represents a new topic, the ci-tation pattern should exhibit the synergistic effect. We tes t our algorithm on two electronic research literature collec -tions, arXiv and Citeseer. Our evaluation shows that the approach is effective and reveals some novel aspects of topic detection.
 H.3.1 [ Content Analysis and Indexing ] Algorithms, Languages, Measurement topic detection, graph mining, probabilistic measure, cit a-tion graphs, correlation of text and links Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
The availability of large-scale linked document collectio ns such as the Web and specialized research literature archive s[6, 3] presents new opportunities to mine deep knowledge about the community activities behind the document collections. Topic discovery is one example of such knowledge mining that has recently attracted considerable research interes t [12, 14, 9, 22, 23, 24, 17, 4, 16, 7, 15]. Topics are semantic units that can function as basic building blocks of knowledg e discovery. Once discovered they can be used in a number of ways including information navigation, trend analysis, an d high-level descrption of data [22, 15].

In this paper, we present a unique approach to topic de-tection that uses the correlation between the distribution of terms representing a topic and the distribution of links in the citation graph among the documents containing these terms. This distinguishes our work from other approaches to topic detection that focus on textual data alone [9, 22, 23, 14] or which detect topics and communities by studying graph properties without considering text features [10, 19 , 11, 20]. Our appoach is based on the intuition that docu-ments related to a topic should be more densely connected in the citation graph than a random selection of documents are connected in the citation graph. We therefore extract topics from the corpus by examining the structure of the term citation graph for each term in the corpus. A term citation graph of a term A is a subgraph of the full citation graph restricted to the documents that contain the term A and the edges between these term-specific nodes. If the term citation graph of a term A shows denser connectivity than a random subgraph of the full citation graph, it is likely that the term A represents a topic.

An illustration of our approach to topic detection is as follows. Let X  X  imagine that we have a set of all documents containing a term  X  : for example  X  X ensor network X  or  X  X s-sociation rule mining X . Intuitively, if  X  represents a topic, then the documents containing this term will be intercon-nected in a relatively dense citation network (Figure 1. a) ) . This contrasts with another term  X  , for example  X  X ractical examples X  or  X  X ix months X , that are non-topic terms (i.e., general terms) for which the citation links among contain-ing documents will be relatively sparse (Figure 1. b) ). The notions of  X  X ense X  and  X  X parse X  connectivity are relative t o the connectivity of a citation graph consisting of a random selection of documents and their citation edges from the ful l citation graph.

We develop topic score measures that are log odds ratios of binary hypotheses based on a probabilistic description o f graph connectivity. For each term in the corpus, we take a look at its term citation graph. Our topic score measure tells, with statistical confidence, whether the connectivi ty of the term citation graph is significantly denser than what is expected from the citation graph of a random selection of documents. As a first approximation, we assume that a topic can be represented by a single term. We then extend our algorithm to detect topics that are not represented by a single term, but by the relation of a set of terms.
We test our algorithms on two electronic research litera-ture collections, arXiv and Citeseer. Our experiments pro-duce a ranked list of terms that on examination by field ex-perts and based on our observations match prevailing topics in the corpus. Our evaluation of the lists uncovers a number of interesting characteristics of the lists of terms, inclu ding the discovery of topics in varying scale, the prevalence and specificity of topics, and the time evolution of topics.
This paper is structured as follows. Section 2 and Sec-tion 3 present our algorithm to detect topics represented by a single term and by a set of terms, respectively. Section 4 shows the results obtained by applying the algorithm to arXiv and to Citeseer. Section 5 reviews the related work. Section 6 discusses a few issues raised in the work. Section 7 concludes.
The problem statement of this paper is  X  X ow do we detect topics in a linked textual corpus, such as a collection of research papers? X . We address this research problem by producing a ranked list of terms where terms are ordered according to how likely a term represents a topic and how significant the topic represented by a term is. To accomplish this goal, we look at the citation graph of the corpus at the resolution of an individual term level.

Definition 1. In this paper, a  X  X erm X  is defined as an n-gram phrase that consists of any n consecutive words from a document, where n is any positive integer. For example,  X  X etwork X ,  X  X or the X ,  X  X ssociation rule mining X  are all val id examples of a term.
 Conventionally, the citation graph of a corpus is a directed graph with nodes being the documents or research papers in the corpus, and with edges being the hyperlinks or the citation links. In this paper, we only consider the undirect ed version of the citation graph. We denote the undirected citation graph of the entire corpus as G all .

The term citation graph of a term A , G A , refers to a sub-graph of the entire citation graph G all with nodes restricted to the documents that contain the term A and the links between these documents. Precisely,
Definition 2. G A , the term citation graph of a term A , is defined by V ( G A ) = { d | document d contains a term A , d  X  V ( G E ( G A ) = { e ( d i , d j ) | d i , d j  X  V ( G A ) , e ( d where V ( G ) denotes the set of vertices in G , E ( G ) denotes the set of edges in G , and e ( d i , d j ) is an edge between the nodes d i and d j . Figure 1: The term citation graphs. a)  X  : a term representing a topic. (e.g.  X  X ensor network X ,  X  X s-sociation rule X ), b)  X  : a term not representing a topic. (e.g.  X  X ractical examples X ,  X  X ix months X )
Given a term, we want to make a binary decision with statistical confidence about whether the term is relevant to a topic or not. We use the following intuition. If a term represents a topic, then the document nodes in its term ci-tation graph will be well-connected by citations. On the other hand, if a term does not represent a topic, the doc-uments in its term citation graph are not related to each other, thus their distribution is random with respect to ci-tation patterns. Figure 1 shows this intuition. Figure 1 a) is the term citation graph for a topic term  X  showing dense connectivity. Figure 1 b) is the term citation graph of a non-topic term  X  showing sparse connectivity comparable to that of a random selection of documents.
 We formalize this notion by setting up two hypotheses. Given a term A , hypothesis H1 says that A is relevant to a topic, and hypothesis H0 says A is not. We make an obser-vation O ( G A ) about the connectivity of the term citation graph of A , G A . We compute the loglikelihood of the ob-servation O ( G A ) under hypothesis H1 and the loglikelihood of O ( G A ) under hypothesis H0. The difference of the two loglikelihoods becomes the topic score for the term A . The topic score represents how well hypothesis H1 explains the connectivity observation, compared to hypothesis H0.
We take the observation O ( G A ) to indicate, for each node in G A , whether the node has at least one link to the rest of the graph or not. Under hypothesis H1, it is very likely that a node in the graph is connected to the rest of the graph by at least one link. The document either cites or is cited by another document that shares the topic. We use the parameter p c , with a value close to 1, to denote this probability of a node in G A having at least one link to any other node in G A . We present the result with p c set to 0.9 in Section 4. 1 Then, the loglikelihood of O ( G A ) with
Our experiment with several values of p c shows that the result is not sensitive to a particular choice of values for p as long as the value is close to 1. Figure 2: The term citation graphs from arXiv. a) for a topic term  X  X lack hole X , b) for a stop phrase  X  X e show X  hypothesis H1 is given as follows. where n A is the number of nodes in G A , and n c,A is the number of nodes in G A that have at least one link that points to another node within G A , and O i ( G A ) is the per-node observation for node i .

The loglikelihood of O ( G A ) with hypothesis H0 is more interesting. Under the null hypothesis H0 that a term A is not relevant to a topic, the documents in G A are not related to each other. Thus, given a node i in G A and one of its citation links, the probability that the other end of this link points to any node within G A is n A  X  1 N  X  1 , where n is the number of nodes in G A and N is the number of nodes in the entire corpus. That is, determining which node a citation link of a node i connects to can be considered as a random process with respect to G A , where any node in the entire corpus is equally likely to be the destination of the link. Then, the probability that a node i in G A is connected to any other nodes in G A by at least one link is given as, node i .

The loglikelihood of O ( G A ) with hypothesis H0 is given as follows. where V c ( G A ) denotes the set of nodes in G A that has at least one link to any other node in G A .

It should be noted that our null hypothesis H0 is based on the randomness of the citation connectivity, not on the ab-solute sparseness of the connectivity. This enables our top ic score to effectively filter out high-frequency common phrase s as non-topic terms. This is illustrated in Figure 2. 2 Figure 2 a) shows the term citation graph derived from arXiv for a prevalent topic term  X  X lack hole X  and Figure 2 b) for a stop phrase  X  X e show X . As shown, it is not easy to discern from the graph visualization that the topic relevance of  X  X lack hole X  is much greater than that of  X  X e show X . However, as will be seen in Section 4, our topic score measure assigns the highest score to  X  X lack hole X , and the lowest score to  X  X e show X . This is because, for the term  X  X e show X , the random connectivity assumption of the null hypothesis H0 defaults to the dense connectivity as shown in Figure 2 b), while the hypothesis H1 assumes even denser connnectivity.

If we generate the topic scores in Eq.1 for all possible terms in the corpus and order them, we get a ranked list of terms, where terms are ranked according to how likely they represent the topics of the corpus. The terms at the top ranks are the terms representing the topics prevalent in large scale. This is because the term citation graphs of the topics prevalent in large scale have many instances of per-node observations that support the hypothesis H1 over H0.

As hinted above, the bottommost ranked terms have clear intuitive interpretation as well. These terms are the stop words or common phrases, as their term citation graphs ex-hibit the large scale statistical evidence that can be bette r explained by H0 than by H1.
Some topics are not detectable by a single term but by the appearance of a set of terms. This may occur, for ex-ample, when a new term is not coined for a topic, but the topic is represented by the relation between a few general terms. For example, let X  X  consider a topic M represented by the co-occurrence of two terms  X  X uantum computer X  and  X  X uantum dot X . This is a research topic in physics about using  X  X uantum dot X  as a hardware device for  X  X uantum computer X . However, each individual term  X  X uantum dot X  or  X  X uantum computer X  represents a much broader research topic than the given topic M . The term  X  X uantum com-puter X  represents any topic related to quantum computing: examples are quantum computer algorithms, fault tolerant quantum computing, and many kinds of hardware devices for quantum computer.  X  X uantum dot X  is a nano-scale semi-conductor material. The term  X  X uantum dot X  represents a broad research topic including material property study, an d using quantum dot to make applications such as laser, quan-tum computer logic gate, etc. Thus, looking at a single term is not going to reveal the topic M .

The problem of detecting a topic represented by a set of
To aid the visualization, the term citation graphs from arXiv are illustrated in the following ways. The vertical axis is a time scale where time follows downward. The hor-izontal axis spans 7 research fields of arXiv. A paper at a particular time and a field is placed in the small rectangle at the corresponding position. The darkness of a rectangle represents the number of papers contained in the rectangle. The links between rectangles are the citation links between the papers in the rectangles. Figure 3: The term citation graphs of terms  X  and  X  , and their intersection terms but not by an individual term is different from finding the co-occurrence counts of terms. The mere high count of co-occurrence is not what we want. The co-occurrence count of two stop words might be high, but it does not carry topic information. Also, the normalized co-occurrence, defined a s the co-occurrence count divided by the occurrence count of a single term, is not what we want either. At the extreme, we may think of terms A and B that always occur together with high frequency. But this topic is detectable by looking at a single term A or B by the method explained in Section 2. Finally, it should be noted that our goal is different from association rule mining [1]: the above example of terms A and B co-occurring with high frequency qualifies for an as-sociation rule, but not for detecting a topic represented by a set of terms. Then, how do we detect topics represented by a set of terms?
We again look at the term citation graphs and use the following intuition that is illustrated in Figure 3. In Figu re 3, a small rectangle containing  X  is a document containing a term  X  . Similarly, a small rectangle containing  X  is a doc-ument containing a term  X  . A small rectangle containing both  X  and  X  is a document containing both terms. A link connecting two documents is a citation link. The left big circle encloses the term citation graph of the term  X  , which is G  X  . The right big circle encloses G  X  . The documents and links within the intersection of the two circles consti-tute the citation graph for the documents containing both terms, which we denote as G  X   X   X  . Figure 3 shows that the documents containing both terms  X  and  X  are significantly more densely connected than G  X  or G  X  . This indicates that there is a nontrivial topic represented by the co-occurrenc e of  X  and  X  , but not by one of them. On the other hand, if there is no significant topic represented by the marriage of  X  and  X  then the occurrence of the term  X  within G  X  or the occurrence of the term  X  within G  X  will not be cor-related to the citation pattern. In this case, G  X   X   X  should have the link connectivity comparable to that of the same size random subgraph of G  X  or G  X  .

We formalize this notion as follows. Given a term A and a term B , we want to detect whether the connectivity of G
A  X  B is significantly higher than what we could normally expect from the connectivity of G A or G B . To account for the connectivity of any term citation graph G , we use an ob-servation 3 that considers, for each citation link of each node
Note that this observation O ( G ) is different from the ob-in G , whether the link ends with a node within G or outside G . If for each link of a node in G the probability that it ends with another node within G is p , then the loglikelihood of the connectivity observation on G is, where l i is the total number of citation links of a node i , and c i ( G ) is the number of citation links of a node i that fall within G . Let p  X  ( G ) be the value of p that maximizes Eq.4. With the number of nodes in G fixed, p  X  ( G ) tends to increase, as the connectivity of G gets denser.

Let X  X  consider G A and G A  X  B under the hypothesis that the co-occurrence of terms A and B does not represent a new topic. Under this null hypothesis, the generative process of determining which document in G A contains the term B is an independent random process with respect to the distribution of the citation links of G A . Thus, if we let p 0 A be our guess for p  X  ( G A  X  B ) under the null hypothesis, our best guess for p 0 A is the probability that maximizes the average loglikelihood of the following subgraphs of G A . The subgraphs we consider are any subgraphs of G A that have the same number of nodes as that of G A  X  B , and the citation links between them. Formally, p 0 A is given as follows. where n is the number of nodes in G A , k is the number of nodes in G A  X  B , and the summation over G  X  runs over any graph G  X  that satisfies the following p 0 A can be analytically obtained to be
Now, we think of the alternative hypothesis that says the co-occurrence of terms A and B represents a new topic. Un-der this hypothesis, our guess for p  X  ( G A  X  B ), which we de-note as p 1 A , should be significantly higher than p 0 A it as p 1 A = m  X  p 0 A , where m is a mulplicative parameter greater than 1.

The following score T A ( A, B ) is our confidence about how likely the co-occurrence of terms A and B represents a new topic, with respect to a term A .
 Note that our guess for p 1 A need not be exactly p  X  ( G nor even close to it. The actual value of p  X  ( G A  X  B ) only needs to be relatively closer to p 1 A than to p 0 A to make T A ( A, B ) positive. In particular, if the actual p  X  ( G significantly larger than p 0 A , T A ( A, B ) will be positive for servation O ( G ) for a single term topic detection in Section 2. The choice is made so that the new observation O ( G ) can account for graph connectivity in a continuous spectrum. a wide range of m . Thus, with large m , we could filter false positives, while we may only lose false negatives with weak confidence. We experimented on several values for m in the range of [2 , 10]. While the result does not sensitively change over a wide range of m , the choice of m = 6 seems to provide a good balance between false positives and false negatives. In Section 4, we present the result with m = 6.
 We then get T B ( A, B ) in the similar way by looking at G
A  X  B and G B . Our final score for judging whether the co-occurrence of terms A and B represents a new topic or not is given by taking the minimum of T A ( A, B ) and T B ( A, B ), reflecting our belief that the link density of G A  X  B should show a significant departure from that of both G A and G B We use arXiv and Citeseer for evaluation.
 We restrict the terms we consider to all possible bigrams in the corpus. We choose bigram as our term unit, because bi-grams typically convey more concrete ideas than unigrams, yet higher grams might suffer from the explosion of the num-ber of terms and sparseness of data for each term. But, it is only a choice of convenience and our algorithm can be applied to any n-grams. We further restrict the terms by pruning out low frequency terms that appear in less than 5 documents in the corpus and by pruning out 35 stop words. arXiv is an actively maintained online repository of re-search papers in physics. We take papers from year 1991 to year 2006 that span 7 major arXiv areas. This is in total 214,546 papers and 2,165,170 citation links between them, which amounts to 10.09 per-document citations. For each paper, we use its abstract as its document.

We perform the following experiments. First, for all pos-sible terms appearing in the corpus, we compute the single term topic score measure of Eq.1, and get a ranked list of topics. Second, for all possible term pairs in the corpus, we compute the topic score of two terms as in Eq.9, and get a ranked list of topics.
 The running time is reasonable. We used a pentium IV PC with 2GB memory. It took 45 minutes to generate the term citation graphs for all terms and their inverted index. It took 4 minutes to compute the single term topic scores for all terms. It took about 10 hours to compute the topic scores of two terms for all possible term pairs.

To consider all pairs of terms for two term topic scores could be prohibitive as there are a huge number of terms. But, we need to consider a pair of terms only when the two terms appear together in at least one paper. This co-occurrence matrix is pretty sparse when a document is an abstract. Thus we achieve a reasonable running time for the two term topic score experiment.
Computing the topic scores for each term in the corpus ac-cording to Eq.1 gives a ranked list of topics. The ranked list of terms has 137,098 entries (terms), where top entries con-stitute topic terms and bottom entries constitute non-topi c terms. Table 1 shows the top 15 entries from the ranked list. The first 2 columns represent the rank and the topic term respectively. The third column labeled as &lt; n, n c , | E | &gt; is an information about the term citation graph of the topic term: n is the number of nodes in the citation graph of a topic term, n c is the number of nodes that has at least one link connecting to any other node within the graph, | E | is the number of edges in the graph.

An objective and quantitative evaluation of the result is difficult due to the lack of standard formal measures for topic detection tasks. However, when the results were examined by the domain experts, they recognized the topics presented in Table 1. Also, we have an informal evidence that these top ranked terms do represent highly prevalent topics in the physics literature. When we typed in each topic term of the top 20 ranks as a search query to www.google.com, 19 of them returned Wikipedia entries within the top 5 of the google search results. The inspection of the Wikipedia arti -cles reveals that most of them have serious physics research oriented contents. The one topic term that did not return the Wikipedia entry was  X  X eavy quark X . But, the second rank entry of its google search result is  X  X he 5th interna-tional workshop of heavy quark physics X , indicating that it also is a prevalent research topic in physics.

The topic terms at the top ranks are topics in large scale, as we can see from the term citation graph information of &lt; n, n c , | E | &gt; column. The topic term entries down to a few thousand X  X h level of the ranked list still present meaningf ul topics. Table 2 shows a few entries of topic terms around 100 X  X h, 500 X  X h, 1000 X  X h, 2000 X  X h ranks. There is an appar-ent trend of topic scale getting smaller as we go down to lower ranked topic terms, as seen from &lt; n, n c , | E | &gt; col-umn. Topics discovered at these levels could be more inter-esting as they tend to represent more specific ideas than the more generic and prevalent top ranked topic terms. Figure 4 shows the term citation graphs of the topic terms at 100 X  X h, 990 X  X h, 1971 X  X h ranks, respectively (Refer to Footnote 2 fo r how to read the graphs). We see the scale difference of the topic terms at different ranks. Figure 4 c) suggests that even at 1971 X  X h rank, there is still a meaningful topic that binds the papers in the term citation graph.

As explained in section 2, the bottommost entries of the ranked list are stop words or common phrases, whose term citation graphs are much better explained by hypothesis H0 than by hypothesis H1. Table 3 shows the bottommost 15 terms of the ranked list.

It should be noted that the topics discovered by our al-gorithm have a varying degree of prevalence and specificity, that are natural in the given corpus. This is because we do not assume a predefined number of topics to discover, as language model approaches or graph-based clustering ap-proaches do. Fixing the number of topics to discover has the effect of determining the scale of topics in advance.
To see the overall property of the entire ranked list, we present two plots, Figure 5 and Figure 6. Figure 5 is a plot of term rank vs. the log size of the term citation graph averaged over 100 consecutive terms. It shows that the term frequency gets higher, as the rank gets close to either the highest or the lowest ranks. This is because in a large-scale term citation graph one hypothesis is strongly preferred ov er the other due to many instances of per-node observations that support the hypothesis.

To show the connectivity of term citation graphs, we de-vise the following measure and use it in the plot of Figure 6. Given a term citation graph G A , c i ( G A ) denotes the num-ber of links of a node i that falls within G A , l i denotes the top topic (term) &lt; n, n c , | E | &gt; rank 1 black hole &lt; 4978 , 4701 , 38952 &gt; 2 quantum hall &lt; 1863 , 1493 , 4862 &gt; 3 black holes &lt; 3131 , 2896 , 22824 &gt; 4 higgs boson &lt; 2079 , 1896 , 12607 &gt; 5 renormalization group &lt; 3738 , 2920 , 8490 &gt; 6 quantum gravity &lt; 2014 , 1724 , 9693 &gt; 7 standard model &lt; 7848 , 7145 , 53829 &gt; 8 heavy quark &lt; 1671 , 1473 , 6570 &gt; 9 cosmological constant &lt; 2141 , 1815 , 7134 &gt; 10 quantum dot &lt; 1366 , 1031 , 2926 &gt; 11 chiral perturbation &lt; 1132 , 1050 , 5578 &gt; 12 form factors &lt; 1578 , 1354 , 5616 &gt; 13 lattice qcd &lt; 1425 , 1265 , 5240 &gt; 14 string theory &lt; 3818 , 3539 , 26250 &gt; 15 hubbard model &lt; 1702 , 1167 , 2678 &gt; . . . . . . . . .
 Table 1: The topic terms of top 15 ranks from arXiv total number of links of a node i , n A denotes the size of G
A , and N denotes the size of the full citation graph. We call P i c i ( G A ) / P i l i as edge containment. It reflects how clustered G A is with respect to the rest of the full citation graph. We normalize the edge containment by the relative size of the term citation graph. We call the resulting quan-normalized edge containment should default to 1 if the ci-tation pattern of G A is random. Figure 6 shows a plot of term rank vs. the normalized edge containment. As ex-pected, the topic terms at high ranks show high normalized edge containment, while the non-topic terms at low ranks show low normalized edge containment. What is interesting to note is that the graph is not monotonically decreasing: up to the top few thousand ranks, the normalized edge con-tainment keeps increasing. This agrees with our observatio n that the middle rank topics are more specific than the top rank topics. Figure 5: A plot of term rank vs. log(size of term citation graphs)
By computing the term pair topic scores of Eq.9 for all possible pairs of terms in arXiv corpus, we get a ranked list Table 2: The topic terms at various ranks from arXiv Table 3: The terms with the lowest topic scores from arXiv where each entry is a pair of terms that might represent a topic. Since we are looking at the intersection citation gra ph of two terms, we get a sparser graph to look at. In order to alleviate the sparseness, we stemmed our corpus. Table 4 shows the top 12 entries of the ranked list. These entries are the topics that are represented not by a single term, but by the relation involving a set of terms. For example, the rank Figure 6: A plot of term rank vs. normalized edge containment 1 entry has  X  X hase transit(ion) X  and  X  X tandard model X  as its topic terms.  X  X hase transition X  is a general term meaning a change in macroscopic state of a large-scale system.  X  X tan-dard model X  is a prevalent theory of particle physics that de -scribes the fundamental interactions of elementary partic les. It turns out that the papers at the intersection of the two terms talk about the  X  X hase transition X  occuring in  X  X tan-dard model X  or the  X  X hase transition X  occuring in minimal supersymmetric  X  X tandard model X  which is an extension of  X  X tandard model X . The individual term  X  X hase transition X  or  X  X tandard model X  has a much broader research context than the topic identified. The rank 2 entry has  X  X auge the-ory X  and  X  X atrix model X  as its topic terms. It turns out that there was a heavily cited paper that started the whole idea of analyzing  X  X auge theory X  using the computational tech-niques from  X  X atrix model X , and the majority of papers in the intersection graph talk about the further development o f this idea. As explained in the previous section 3, the papers of the rank 7 entry talk about using  X  X uantum dot X  as a hardware implementation of  X  X uantum computer X .

The last three columns of table 4 show the citation graph tersection, respectively. They show that the connectivity of the intersection graph G A  X  B exhibits significant departure from the same size random subgraph of G A or G B .
Our Citeseer data contains 716,771 papers, with 1,740,326 citations. This amounts to 2.43 citations per paper. For each paper, we use its title and abstract combined as its doc-ument. The number of bigrams in the corpus after pruning out the low-frequency bigrams and 35 stop words is 631,839. The majority of papers are from year 1994 to year 2004. We divided the documents into two different document sets. One set contains all the documents up to year 1999, and the other set contains all the documents since year 2000.
We performed the single term topic score measure of Eq.1 to each set. The top 25 topic entries of each set are shown in parallel in Table 5. We see that the top rank topics have changed significantly between the two time periods. We see that many top rank topics of the time frame since 2000 carry recent trends that were not significant before. Ex-amples are  X  X ensor networks X ,  X (ad) hoc networks X ,  X  X ire-less sensor X ,  X  X ntrusion detection X ,  X  X emantic web X ,  X  X ml data X , and  X  X mage retrieval X .  X  X upport vector (machine) X  was ranked 35th in the document set up to 1999, and it has risen to the rank 5 in the document set since 2000.  X  X onges-tion control X  more or less maintains its topic rank through the different time periods. We observe the fall of many top ranked topics of the document set up to 1999, in the time period since 2000. The ranked list of topic terms is quite in-structive as well: initially, we did not recognize the 7th ra nk  X  X nterior point X  of the time frame up to 1999 as a topic. But, it turns out  X  X nterior point X  represents an important family of algorithms in linear programming.

As in the case of arXiv evaluation, we see that the ranked list of topic terms from Citeseer has meaningful topics even around a thousand X  X h level with the apparent trend of topic scale getting smaller as we go down the ranks. Due to the space limitation, however, we do not present the result.
In order to see the time evolution of topics more clearly, we performed the following experiment. We ran the single term topic score measure for the entire Citeseer document collection. Then, for each term in top 70, we generated a plot where x-axis is years spanning from 1994 to 2004 and y-axis is the number of documents of the term citation graph in a particular year normalized by the total number of doc-uments in that year. Figure 7 shows the plots for 12 such topic terms out of top 70 terms. We see a sharp recent rise of  X  X ensor networks X  and  X  X emantic web X , a significant rise of  X  X upport vector X  and  X  X nergy consumption X , a rise of  X  X ml data X  in a smaller scale, the fall of  X  X ogic programs X ,  X  X etr i nets X ,  X  X nterior points X .  X  X ongestion control X ,  X  X ssocia tion rules X , and  X  X enetic programming X  show less dramatic dy-namics.
Our work is distinguished from previous work on topic detection in two ways. First, we look at the correlation rank topic (term) topic (term) 1 logic programs sensor networks 2 model checking hoc networks 3 semidefinite programming logic programs 4 inductive logic image retrieval 5 petri nets support vector 6 genetic programming congestion control 7 interior point model checking 8 kolmogorov complexity decision diagrams 9 automatic differentiation wireless sensor 10 complementarity problems ad hoc 11 congestion control instrusion detection 12 complementarity problem vector machines 13 conservation laws mobile ad 14 linear logic binary decision 15 timed automata sensor network 16 situation calculus energy consumption 17 real-time database content-based image 18 motion planning semantic web 19 duration calculus fading channels 20 volume rendering xml data 21 chain monte source separation 22 association rules timed automata 23 term rewriting signature scheme 24 posteriori error volume rendering 25 active database xml documents . . . . . . . . .
 Table 5: The top 25 topic terms of two different time periods from Citeseer between the term distribution and the citation link distri-bution for a topic. Second, we use for a topic measure the log odds ratio of binary hypotheses based on a probabilistic description of graph connectivity.

Previous work on topic detection can be largely divided into two groups. The majority of papers take a language model based approach. This approach tends to focus on text, but a few papers extend the model to incorporate links. Another group of work is based on studies of graph prop-erties. Most of these papers address a problem related to topic detection: community detection. They tend to use the non-probablistic aspects of graph properties. There ar e also related papers that share some of the ideas used in this paper. Specifically, these ideas are examination of pattern s at individual term level, usage of log odds ratio to detect patterns, and investigation of the notion of term informa-tiveness.

The language modeling approach [9, 22, 23, 24, 7, 16] as-sumes a multi-stage generative process where semantically meaningful modalities such as topics or authors are chosen as an intermediate step, and then the words are drawn from the multinomial distribution conditioned on these modali-ties. These papers differ in the design choice for the gener-ative process. Examples of design decisions are the choice of modalities or the final features that will be produced. [9] uses the document generation process conditioned on topic distributions. [22] uses authors as distribution over topi cs as additional modalities. [23] detects topics over time by let -ting the generative process produce the timestamps of words as well as the words themselves. A number of papers extend the model to incorporate links. [7] treats the reference lis t of a paper as another final feature to produce, in addition to the bag of words. [16, 24] apply the language model approach to social network analysis where documents are the communication links such as e-mail messages between people. [4] aims to overcome the inability of latent dirich-let allocation used in the papers above for describing the correlation of topics, by including the correlation matrix of topics in the generative process. [17] computes the themes of a document collection by the mixture model using the EM algorithm.

Graph properties are used to study community structures by [10, 8, 11, 20, 19, 13, 2]. As a distance metric [10] uses the similarity of citation patterns, [8, 11] use the notion t hat nodes have more links to the members of the same com-munity than to other nodes, [20] introduces the concept of edge betweenness, and [19] uses the measures from bibliom-etry and graph theory. Some papers in this group combine the information from text as well. [13] extracts storylines for a query by identifying densely connected bipartites fro m the document-term graph of the search results. [2] improves the document categorization performance by starting from a text-based categorization result and then iteratively rel abel-ing the documents to further satisfy the constraints impose d by the link proximity relation.

Our approach of looking at citation patterns at an indi-vidual term level and using the loglikelihood to explain the observation is inspired by [12]. [12] detects a topic as a burst of activities represented by the state transition in a markov chain. In an experiment on paper titles and pres-idential speeches the paper shows that topics can be effec-tively detected as time bursts in a single term level. The idea of anomaly detection by log odds ratio is used in a number of papers related to topic detection. [18] uses the log odds ratio of event frequencies to detect space-time clu s-ters. [14] discovers a set of words as topic signature in a supervised learning setting by comparing the log odds ratio of word frequency in topic documents and non-topic docu-ments. The ranked list of terms for topics produced by our algorithm shows a continuous spectrum of term informative-ness in representing topics. The notion of term informative -ness is explored in a number of related contexts. [5] detects the terms informative about the citation links to use them as features for document categorization. For this purpose, they use the expected entropy loss measure, which resem-bles the one used in the decision tree feature selection. [21 ] detects the informative terms for named entity detection, using the idea that informative terms are better modeled by a mixture of two unigram models while non-informative terms are better modeled by a single unigram model.
It is worthwhile to note that graph connectivity observa-tions used in our algorithms are pluggable. One can plug in an observation that best suits one X  X  need. As different observations may represent different aspects of graph con-nectivity, the choice of an observation affects the topic sco re result. For example, the observation used in Section 2 for a single term topic score concerns whether a node has a connection to the rest of the graph or not, but it does not distinguish how many connections a node has. Thus, the observation is generous on loosely connected topics. As a result, the highest topic scores are given to the large-scal e prevalent topics even though these topics are not as tightly connected as the more specific smaller scale topics.
Another point to note is that because our algorithms dis-cover topics without imposing any constraint on the relatio n-ship among topics such as restricting the number of topics to be discovered or assuming implicit mutual exclusion among topics, the topics discovered are suitable for expressing t he complex relationship among topics. Specifically, with our topics, a single document can be involved in multiple topics , and topics could have hierarchical covering relations or no n-hierarchical overlapping relations among themselves. Un-derstanding the structure of the relationship between the topics is left as future work.
In this paper, we presented algorithms to detect topics from a linked textual corpus based on the unique approach of using the correlation between the term distribution and the link distribution for topics. Our algorithms produce a ranked list of terms for topics represented by a single term and for topics represented by a set of terms. Our evalua-tion on arXiv and Citeseer data show that the method is effective. Topics discovered by our algorithms reveal novel aspects of topic detection. The ranked list shows a continu-ous spectrum of topics of varying prevalence and specificity that are natural in the given corpus. The relations among the terms that represent topics are revealed by the two term topic score measure. As an interesting by-product, our algo -rithm can discern common phrases without the prior knowl-edge of stop word notion. The possibility of discovering complex topic relations and the pluggable characteristic o f graph connectivity observations are discussed.
We would like to thank Simeon Warner for providing arXiv data, and Isaac G. Councill for providing Citeseer data. We would like to thank Prof. John E. Hopcroft for his in-put on graph properties and his encouragement, and Prof. Thorsten Joachims for valuable discussions. This work was supported in part by National Science Foundation under award numbers IIS-0430906, 0227648, 0227888, and 0424671. [1] R. Agrawal, T. Imielinski, and A. Swami. Mining [2] R. Angelova and G. Weikum. Graph-based text [3] arXiv. http://arxiv.org . [4] D. M. Blei and J. D. Lafferty. Correlated topic [5] L. Bolelli, S. Ertekin, and C. L. Giles. Clustering [6] Citeseer. http://citeseer.ist.psu.edu . [7] E. Erosheva, S. Fienberg, and J. Lafferty.
 [8] G. W. Flake, S. Lawrence, and C. L. Giles. Efficient [9] T. I. Griffiths and M. Steyvers. Finding scientific [10] J. Hopcroft, O. Khan, B. Kulis, and B. Selman. [11] H. Ino, M. Kudo, and A. Nakamura. Partitioning of [12] J. Kleinberg. Bursty and hierarchical structure in [13] R. Kumar, U. Mahadevan, and D. Sivakumar. A [14] C.-Y. Lin and E. Hovy. The automated acquisition of [15] G. S. Mann, D. Mimno, and A. McCallum.
 [16] A. McCallum, A. Corrada-Emmanuel, and X. Wang. [17] Q. Mei and C. Zhai. Discovering evolutionary theme [18] D. B. Neill, A. W. Moore, M. Sabhnani, and [19] M. Newman. Scientific collaboration networks. i. [20] M. E. J. Newman and M. Girvan. Finding and [21] J. D. M. Rennie and T. Jaakkola. Using term [22] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. Griffiths. [23] X. Wang and A. McCallum. Topics over time: A [24] D. Zhou, E. Manavoglu, J. Li, C. L. Giles, and
