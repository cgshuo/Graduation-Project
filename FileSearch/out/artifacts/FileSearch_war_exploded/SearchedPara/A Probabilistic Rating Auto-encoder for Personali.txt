 User profiling is a key component of personalized recom-mender systems, and is used to generate user profiles that describe individual user interests and preferences. The in-creasing availability of big data is driving the urgent need for user profiling algorithms that are able to generate ac-curate user profiles from large-scale user behavior data. In this paper, we propose a probabilistic rating auto-encoder to perform unsupervised feature learning and generate la-tent user feature profiles from large-scale user rating data. Based on the generated user profiles, neighborhood based col-laborative filtering approaches have been adopted to make personalized rating predictions. The effectiveness of the pro-posed approach is demonstrated in experiments conducted on a real-world rating dataset from yelp.com .
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval-Information Filtering Algorithms; Experimentation; Performance User profiling; Recommender Systems; Auto-encoder; Di-mensionality Reduction
Recommender systems are popular for personalization, and can help to reduce information overload for users in on-line communities, i.e., making suggestions regarding which information is most relevant to an individual user [1]. User profiling is the foundation of personalized recommender sys-tems, and provides the basis to discover knowledge about users X  interests, preferences, and information needs, from user-generated social data including user-generated content c  X  and behavioral information [8, 9]. With the rapid growth of social data, methods that support efficient and effective extraction of features from big social data are becoming in-creasingly important.

Rating (explicit or implicit) is one type of user behaviour information that has been popularly used to profile users X  in-dividual preferences and interests. For example, a user may use a rating score ranging from 1 to 5 to express their pref-erences or sentiment orientation with respect to an item. The typical approach is to profile users directly based on their rating behavior, and based on this, to use neighborhood based collaborative filtering approaches [11] to make rating predictions or top-N item recommendations [1]. Learning latent representations of users or items from user rating be-havior can find latent or hidden relations between users or items, and reduce data dimensionality, thus contributing to scalable recommender systems. Although some dimension-ality reduction approaches such as matrix factorization [5] have been proposed, the development of methods for learn-ing latent representations from large-scale user rating be-haviour data remains an open research question.

Deep learning techniques [4] have recently emerged as a promising framework for automatically training models over large-scale unlabeled and labeled datasets. Notably, auto-encoders [12] can learn latent representations for unlabelled data, and have been successfully applied to text, image, and audio information [7]. However, there has been less work on applying auto-encoders to rating data [13]. In this paper, we propose a rating auto-encoder to learn latent features from user rating behavior, for the purpose of constructing more accurate user profiles. The generated latent user representa-tions can then be used in recommender systems to perform personalized recommendation.
Recommender systems have been an active research area for more than a decade, with the main focus being recom-mendation approaches based on explicit ratings. Popular recommender systems applications include predicting rat-ings and recommending items to a user. Rating prediction is the task of predicting the rating a user will give to an item, while item recommendation is the task of recommending a set of unrated or new items to a target user [1]. Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) are widely used to measure the accuracy of rating predic-tions, while precision and recall are commonly used to eval-uate top-N item recommendation. For explicit ratings, both tasks are applicable, while for implicit ratings, top-N recom-mendation is more applicable [1]. Recommender systems can be broadly classified into three categories: content-based, collaborative filtering ( X  X F X ), and hybrid approaches [1]. Neighborhood methods [11] and latent factor methods [5] are the two primary collaborative filtering approaches.
Learning latent representations or latent factor models of users and items has generated recent attention, given the rapid growth of rating data in online communities [3]. Matrix factorization [5] is a state-of-the-art latent factor method. It jointly learns latent user and item features, and uses the dot product of the latent user and item features to predict ratings. However, there is less discussion about whether the learned latent user features themselves can be used to accurately represent user profiles. The general ques-tion of accurately learning latent user representations is still relatively unexplored. Recently, deep learning techniques [4] and neural network models [2] such as auto-encoders have been applied to various application areas to learn latent or abstract representations of words, documents, images, au-dio, and videos. For example, word2vec [10, 6] can be used to efficiently learn latent word representations based on large-scale, unlabelled text corpora. Our contribution in this work is to apply deep learning methods to user rating data, and learn latent user representations.
To describe the proposed approach, we define some key concepts used in this paper:  X  Users: U = { u 1 ,u 2 ,...,u | U | } is the set of all users in an  X  Items (e.g., Products or Businesses): P = { p 1 ,p 2 ,...,p  X  Ratings: R = { ( u i ,p j ,r ij ) | u i  X  U,p j  X  P,r ij
A user profile is used to describe an individual user X  X  in-terests and preferences. Typically, an explicit or implicit item rating vector with numeric or binary values is used to profile the preferences or interests of a user to these items [1]. Let ~u i denote the rating vector representation of user u , for example in the form of a set of items with ratings, ~u u and target item p j , the rating prediction task is defined as predicting the rating value r ij , given a user u i  X  X  profile.
For a given user u i , if two items p i and p j are assigned the same rating score, then these two items are similar in terms of the sentiment orientation implied in the user rating infor-mation from this user X  X  viewpoint. Based on this assump-tion, we propose a probabilistic auto-encoder [10] to cap-ture the co-occurrence of items with the same rating score for the same user and learn the latent feature representa-tion of items. For a given user and rating score, we can use the items with the same rating score from a given user as context, and use a neural probabilistic model to predict the occurrence of other items given an item in this context. For example, if user u i rated items p 1 , p 2 , p 3 with the same rating score of 3, the rating auto-encoder can predict the occurrence of item p 2 or p 3 given item p 1 in this context. If the rating scores are of very fine granularity, it will result in a small context set containing very few items. To avoid data sparsity, we bin the rating scores and map them to rating categories with coarser granularity.

Let P ix  X  P denote the items in the context of user u i and rating category c x . The objective of the rating auto-encoder is then to maximize the average log probability:
Inspired by the skip-gram model of word2vec [10], the rat-ing auto-encoder contains two layers: an input and output layer. The input to the rating auto-encoder is the vector rep-resentation of an item p s  X  P ix with random initialization, while the output of the rating auto-encoder is every other item in the same context p t  X  P ix \{ p s } . Let W be the weight matrix between the input layer and output layer; W is randomly initialized and shared across all contexts. Let y denote the un-normalized log-probability for each output item p l , which can be computed as: We use a multi-class classifier (e.g., softmax) to conduct the prediction task:
We use the squared difference error as the loss function, stochastic gradient descent to train the auto-encoder model, and back propagation to obtain the gradient to update each input vector of item p s and weight matrix W . After con-vergence over the training data, items with similar ratings are mapped to a similar position in the vector space. We generate latent representations of both items and users, as detailed below.

The item representation represents each item X  X  latent rating behavior by users. Let L = { 1 , 2 ,...,d } denote the latent factor, v ja denote the relevance of item p j to latent factor l a  X  L (i.e., the value of dimension l a of the learned or updated vector of item p j ), and 2 L  X  R denote the power set of L  X  R . The relationship between an item and a set of latent factors can then be defined as the mapping R p : P  X  2 L  X  R such that R p ( p j ) = { ( p j ,v ja ) | l a  X  L } . R p item representation of item p j .

Based on the item representation and the rating profile of each user, we can derive the latent representation profile of a given user u i in a rating category c x  X  C . The user repre-sentation represents the latent rating behavior of each user. Let w c x ia denote the value of how much user u i is interested in latent factor l a  X  L in rating category c x  X  C . The relation-ship between a user and a set of latent factors in rating cate-gory c x  X  C can then be defined as the mapping R u,c x : U  X  2 is the user representation of u i in rating category c x  X  C , representation of u i .

We adopt mean pooling [7] to get the summary statistics of the latent features of u i in category c x  X  C . The weight w ia can be calculated as:
Based on user profiles, we use the neighborhood based col-laborative filtering approach to find a set of similar users, then perform rating prediction based on neighboring users X  ratings. This will be discussed in the following section.
The neighborhood based collaborative filtering approach is a common method for making recommendations. We ap-ply the generated user profiles in neighborhood based rec-ommender systems to make rating predictions. Neighbor-hood formation is the task of generating like-minded peers (i.e., the k nearest neighbors,  X  k -NN X  ) for a target user u i  X  U . The distance or similarity measure can be cal-culated through various kinds of proximity computing ap-proaches such as cosine similarity or Pearson X  X  correlation. The more accurate a user profile is, the higher quality the user neighborhood.

Typically, the similarity of two users u i and u j can be measured by the similarity of their user profiles (i.e., user representations). Cosine is used to measure the similarity of two user latent representation vectors. The similarity of u and u j  X  U can be calculated based on the maximum simi-larity of their representations over all the rating categories:
The neighborhood of user u i is denoted as N ( u i ) = { u maxK u j  X  U { sim ( u i ,u j ) }} ,u j  X  U , where maxK {} returns the top-k most similar users to u i . For each target user u the prediction score of how much u i will be interested in item p j is calculated by considering the similarities between user u i and those users who are neighbors of u i and have rated item p j [11]: where b ij =  X  + b u i + b p j ,  X  is the average rating score of all known ratings, b u i is the observed rating deviation of of user u i . b p j is the observed rating deviation of item p b j =  X  r p j  X   X  , where  X  r p j denotes the average rating of item p . b zj =  X  + b u z + b p j , where b u z is the observed rating deviation of user u z .
To evaluate the effectiveness of the proposed approach, we conducted experiments on a real-world rating dataset from yelp.com .
The Yelp dataset is a customer review dataset. 1 Each user rates businesses with a score ranging from 1 to 5. To reduce sparseness in the dataset, we filtered out those users who have rated no more than 3 items, and those items that http://www.yelp.com.au/dataset_challenge have been rated by no more than two users. The dataset includes 37,915 users, 40,021 items (i.e., businesses), and 713,291 ratings. For each user, we randomly selected 80% of ratings as training data, and 20% of ratings as test data. The task is to predict ratings for the test set based on the training data. The descriptive statistics of this dataset are shown in Table 1.
We use root mean squared error ( X  X MSE X ) to measure the accuracy of rating prediction:
We compared the RMSE value of the proposed approaches (i.e, RA-implicit and RA-explicit ) with neighborhood meth-ods over the Yelp dataset. To evaluate the effectiveness of user profiles, we compare the following user profile repre-sentations using a standard user-based k -NN collaborative filtering approach:
For all of RA-implicit , RA-explicit , Random and MF , we set the dimensionality parameter d to 100. Figure 1: The RMSE of rating predictions for the different approaches over the Yelp dataset
Figure 1 shows the RMSE results of the different ap-proaches under varying values of k . We can see that RA-implicit performs slightly better than RA-explicit , while RA-explicit and RA-implicit achieved better performance (i.e., lower RMSE scores) than the other models. Given that the accuracy of rating prediction relies on the quality of the target user representation, this indicates that RA-implicit and RA-explicit result in better user representations. De-spite having the same dimensionality as RA-implicit and RA-explicit , Random performed the worst, indicating that our results are not simply an artefact of the more expressive representation, but rather due to the quality of the learned representation. The user profiles generated by the proposed rating auto-encoder are also more accurate than the latent factor user profiles generated by MF .
 Based on a one-tailed paired t -test, both RA-explicit and RA-implicit are significantly better than the other neigh-borhood baselines ( p &lt; 0 . 005), but there is no significant difference between the two RA methods ( p &gt; 0 . 1). Because the dataset is sparse (81.2% users have provided less than 5 ratings), coarser granularity leads to higher co-occurrence of items in the same context, explaining why RA-implicit is slightly better than RA-explicit .
In this paper, we have proposed a probabilistic rating auto-encoder to conduct unsupervised feature learning for learning latent user profiles from large-scale rating data. The rating auto-encoder learns item similarity based on anal-ysis of items which have been given similar ratings by a given user. Based on the generated latent item representations, we generate latent user representations in each rating category. After that, we applied the generated latent user profiles to neighborhood based collaborative filtering recommender sys-tems to predict rating scores. The experiments were con-ducted on real-life rating data from yelp.com shows that the latent user profiles generated by the proposed rating auto-encoder are able to find more similar neighboring users with-out further fine tuning or optimization, and make more ac-curate rating predictions than traditional rating-based user profiles and latent user profiles generated by state-of-the-art latent factor learning models such as matrix factorization.
In future work, we plan to explore how to learn latent user features from multi-modality social data such as both rating and user-created text information. We are also interested in combining the proposed approach with matrix factorisation for rating prediction, to see whether further gains can be achieved.
 This research was supported by the Australian Research Council. The authors would like to acknowledge the sup-port of Pitchwise ( www.pitchwi.se ), and valuable feedback from the anonymous reviewers and Joel Nothman. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. [3] X. Chen and X. Lin. Big data deep learning: [4] G. Hinton and R. Salakhutdinov. Reducing the [5] Y. Koren, R. Bell, and C. Volinsky. Matrix [6] Q. V. Le and T. Mikolov. Distributed representations [7] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. [8] H. Liang, Y. Xu, Y. Li, R. Nayak, and X. Tao.
 [9] H. Liang, Y. Xu, D. Tjondronegoro, and P. Christen. [10] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [11] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [12] J. Schmidhuber. Deep learning in neural networks: An [13] H. Wang, N. Wang, and D. Yeung. Collaborative deep
