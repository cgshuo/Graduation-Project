 In this paper, we describe VIRLab , a novel web-based vir-tual laboratory for Information Retrieval (IR). Unlike ex-isting command line based IR toolkits, the VIRLab system provides a more interactive tool that enables easy implemen-tation of retrieval functions with only a few lines of codes, simplified evaluation process over multiple data sets and pa-rameter settings and straightforward result analysis inter-face through operational search engines and pair-wise com-parisons. These features make VIRLab a unique and novel tool that can help teaching IR models, improving the pro-ductivity for doing IR model research, as well as promoting controlled experimental study of IR models.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models General Terms: Algorithms, Experimentation Keywords: virtual lab; IR models; teaching
Information Retrieval (IR) models determine how to com-pute the relevance score of a document for a given query, thus directly affecting the search accuracy. Developing op-timal IR models has been one of the most important re-search problems in information retrieval. Over the past decades, many retrieval models have been proposed and studied. However, there is no single winner, and we end up having a few different retrieval models that all seem to perform equally well [2]. Moreover, it has proven very hard to further improve these state of the art retrieval models [1]. Unfortunately, experimenting with any new retrieval model is inevitably time consuming and requires significant amount of resources available since the new model needs to be em-pirically evaluated and validated over as many data sets as possible. Thus, it would be necessary to develop a tool that facilitates the development and study of IR models.
IR toolkits such as Lemur, Terrier and Lucene have been developed to enable IR research and successful technology transfer of IR models to industry and various applications. They are often designed as off-the-shelf systems that allow users to take advantage of the implemented retrieval func-tions to their own applications. In particular, users can use command lines to build indexes, retrieve documents based on a specified retrieval function, and evaluate the retrieval results.

Unfortunately, existing IR toolkits do not offer an easy so-lution for users to implement, evaluate and analyze new re-trieval models, which often pose unnecessary extra burdens to the users. For example, users need to learn how to access various term or document statistic information from the in-dex, and read through source codes or API documentations to figure out how to implement a new retrieval function. Af-ter implementing the function, the users often need to write their own scripts to evaluate and analyze the search results over multiple collections. This kind of process adds unnec-essary burden to the users and could discourage them from experimenting with more functions over more collections. Naturally, it is also very challenging to use such toolkits for a course assignment.

This paper describes our efforts on developing a web-based tool for IR students or researchers to study retrieval func-tions in a more interactive and cost-effective way. We will demonstrate that the developed system, i.e,. Virtual IR Lab (VIRLab) , can offer the following new functionalities:
Empowered by these new functionalities, the VIRLab sys-tem is a novel IR tool that can (1) help teaching IR models to students with limited programming experience; (2) im-prove the productivity for doing research on IR models; and (3) promote controlled experimental study of IR models by establishing baselines on various data collections.
Our prototype system is available at http://infolab.ece.udel.edu:8008 . Please contact the authors to obtain the login information.
Figure 1 shows the screenshots of three major functional-ities including creating a retrieval function, evaluating the function and comparing the results of two functions. We now provide more details about these functionalities.
The front end of the system is a Web interface that allows users to create retrieval functions. Specifically, a user can implement a retrieval function by simply combining multiple features (i.e., collection statistics) from a provided list based on C/C++ syntax. As an example, the left part of Figure 1 shows how the Dirichlet prior retrieval function [2] is imple-mented. Moreover, instead of specifying a single parameter value, the users can also specify a set of values for retrieval parameters, and then the system will automatically create a group of functions with these parameter settings. Once a retrieval function has been created, the user can select test collections and evaluate the effectiveness of the retrieval function over the collections (as shown in the middle part of Figure 1).

The front end also enables users to use or evaluate the re-trieval function through a Web-based search interface. The user first needs to create a search engine by selecting a re-trieval function and a document collection. After that, the user can either enter his or her own query or select a query from existing test collections when queries are available. If the query is from the test collections, we will display not only search results but also the relevance judgment of these re-sults as well as the evaluation results for the query. This fea-ture would allow users to easily see when their search engines fail or succeed and encourage them to identify the problems and try to fix them by changing the retrieval function. More-over, we also empower users to compare the search results of two search engines side by side so that they could analyze them and identify how to revise one of the search engines accordingly. The right part of Figure 1 shows the screenshot of this functionality.

To promote controlled experimental study of IR, we gen-erate a leader-board to report the best performed retrieval functions for each collection. This functionality is similar to the evaluatIR system 1 . One key difference is that VIRLab enables users to conduct more result analysis such as side-by-side comparison between the results of the best system with those of their own retrieval functions.

The back end of the system includes several basic com-ponents such as indexer, ranker and evaluation script. The indexing process is done offline. Several standard TREC ad hoc collections have been indexed and ready for users to choose from. The ranker is determined by the retrieval function that the user provided through the front end.
We expect to demonstrate all these functions of VIRLab, particularly, how one can easily (1) modify/implement a re-trieval function and immediately evaluate its performance on the fly; (2) create an operational search engine with the implemented function with one click; and (3) compare the search results of two retrieval functions side by side to ana-lyze their relative weaknesses and strengths.

We plan to extend the developed system by enabling flex-ible implementations of other system components. http://wice.csse.unimelb.edu.au:15000/evalweb/ireval/
