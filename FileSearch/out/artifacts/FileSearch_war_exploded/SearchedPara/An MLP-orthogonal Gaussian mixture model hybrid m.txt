 Hui Zhu, X.L. Tang, Peng Liu Abstract. A hybrid model based on the combination of an orthogonal Gaussian mixture model (OGMM) and a multilayer perceptron (MLP) is proposed in this pa-per that is to be used for Chinese bank check machine printed numeral recognition. The combination of MLP with OGMM produces a hybrid model with high recog-nition accuracy as well as an excellent outlier rejection ability. Experimental results show that the proposed model can satisfy the requirements of Chinese bank check printed numeral recognition where high recogni-tion accuracy, high processing speed, and high reliability are needed.
 Keywords: Orthogonal Gaussian mixture model  X  Multilayer perceptron  X  Multiple classifier systems  X  Chinese bank check recognition  X  Outlier rejection 1 Introduction As a kind of applied science, pattern recognition has been widely applied in human affairs, for example personal identification systems based on fingerprint, iris, or face classification, automatic car detection systems, etc. Op-tical character recognition (OCR) is one of the most suc-cessful applications of pattern recognition. Many com-mercial OCR products for automatic information pro-cessing already exist.
 thought to be a resolved problem of OCR, but problems still exist for certain applications. Chinese bank check recognition is an application where reliability is even more important than accuracy. To recognize a check, first, the format of the check should be analyzed and then the recognition area should be searched. Failure in area searching will present false samples to the recognition system including Chinese characters, English letters, and certain types of symbols on the bank check. So a bank check printed numeral recognition system needs to be able to reject those inputs that are significantly different from printed numerals, i.e., the outliers. As a practical application system, a Chinese bank check printed nu-meral recognition system should also possess high recog-nition accuracy as well as high processing speed. multilayer perceptron (MLP) has been widely used for character recognition [2, 3]. For bank check printed nu-meral recognition, the number of classes is small and the number of training samples can be very large, so MLP is well suited for this application. By training with large numbers of samples, MLP can obtain excellent recog-nition performance[9]. Small numbers of classes require small numbers of nodes, which makes the architecture of MLP simple, so the network can be easily trained and a high processing speed predicted.
 to reject outliers [4, 16, 17]. Some studies suggest that MLP X  X  poor outlier rejection ability is caused by its hid-den node number and the activation function it uses. With the use of a sigmoid as the activation function, the active space of each node is an infinite open space, which causes poor outlier rejection capabilities [17]. Gori and Scarselli [4] have studied the separation space of MLP and proven that the architecture of MLP affects the shape of its separation space. They have also pointed out that although MLP with the thresholding criterion can exhibit excellent discrimination performance, it may fail in the task of pattern rejection, and the reliability of pattern verification systems based on the thresholding criterion can be very poor.
 outlier rejection ability. Vasconcelos et al. [16] add ex-tra guard nodes to MLP to judge whether or not the input pattern belongs to the needed classes. Liu et al. [10] suggest that using some outlier samples as train-ing samples and increasing the number of hidden nodes would improve MLP X  X  outlier rejection ability. Vascon-celos et al. [17] use Gaussian function as an activation function to improve MLP X  X  outlier rejection ability. Five human experts X  knowledge is injected into training sam-ples. They set target values of these samples manually. Trained with these samples, MLP presents a better out-lier rejection performance [9]. Vasconcelos et al. compare the outlier rejection ability of MLP trained with out-liers with that of MLP with Gaussian activation func-tion (GMLP), MLP with additional direct connections from the input to the output layers (DMLP), and ra-dial basis function (RBF) networks. Their experimental results show that these modification methods can im-prove MLP X  X  outlier rejection ability [18]. A comprehen-sive survey of methods to improve MLP X  X  outlier rejec-tion ability can be found in [13].
 lier rejection ability to some extent, they also have many drawbacks. Designing and training become very compli-cated for methods that change a network X  X  architecture and introduce human expert knowledge. Because outliers are not entirely predictable, using them as training sam-ples is not an effective method. These modifications also lead to the decrease of pattern recognition performance [18]. Although RBF has been reported with good outlier rejection ability, its computational cost is high and its recognition accuracy cannot be guaranteed [9, 18]. statistical methods have been presented in the literature. Most of these statistical approaches approximate the dis-tribution of samples according to their statistical prop-erties and use this information to estimate whether or not a test sample comes from the same distribution [12]. Because a Gaussian mixture model (GMM) can model general distributions using fewer kernels than the num-ber of patterns in the training set, it has been widely used in outlier detection [12]. Bishop proposed using GMMs to estimate the probabilistic density of inputs; the esti-mated probability is then used for determining whether the input is an outlier or a normal sample [1]. thogonal Gaussian mixture model (OGMM) can be ob-tained [11]. An OGMM can approximate class condi-tional density more accurately than a GMM and has been successfully used in speaker recognition and offline handwritten numeral recognition [11, 14, 20].
 and MLP is proposed in this paper. During the past two decades, multiple classifier systems have been used suc-cessfully in many applications [5, 7, 8, 15]. By using some existing or easily attainable classifiers as well as some fu-sion methods, better performance can be obtained. The proposed hybrid model uses the idea of multiple clas-sifier systems. It recognizes input samples with MLP and uses OGMM to reject outliers. Experiments are car-ried out to test the effectiveness of this model. Machine-printed numerals gathered from real Chinese bank checks are used as normal samples, and Chinese characters in bank checks and simulated English letters and symbols are used as outliers. The experimental results show that the proposed model possesses high recognition accuracy, high processing speed, and good outlier rejection ability. a brief overview of the Chinese bank check recognition system. Section 3 gives a brief description of OGMM and the reason why the MLP model with a sigmoid activa-tion function presents a half-open decision region. Sec-tion 4 compares decision regions of MLP and OGMM. The MLP/OGMM model is introduced in Sect. 5. In Sect. 6, a series of experiments is carried out to confirm the effectiveness of the model. Section 7 concludes the work. 2 A brief overview of the Chinese bank check recognition system The main purpose of this Chinese bank check recognition system is to record daily bank operations automatically. Now it can handle 13 categories of commonly used types of bank checks. An example bank check image is shown in Fig. 1.
 corresponding category, and then the recognition area (arrow line in Fig. 1) is searched; finally, the information in the searched area unique to that check is recognized. The contents of a Chinese bank check are very com-plicated and include Chinese characters, symbols, and sometimes English letters. Even in the same check cate-gory, different check formats will appear. These difficul-ties, together with poor scan quality for certain checks, make area searching and numeral recognition difficult, hence the need for a recognition system with high recog-nition accuracy, high recognition speed, and high relia-bility. 3 A brief description of OGMM and MLP 3.1 Orthogonal Gaussian mixture model A Gaussian mixture model (GMM) is a weighted sum of multivariate Gaussian functions as follows: where x is the input vector,  X  is the corresponding class, and p i ,i =1 ,...,M is the weight satisfying M i =1 p i =1; b ( x ) has the following form: where D is the vector dimension and the parameters of each Gaussian model and  X  i and  X  i are estimated by an Expectation-Maximization (EM) algorithm [14]. The covariance matrix can be transformed into a diagonal matrix with eigenvectors of the original covariance ma-trix. A GMM with a transformed diagonal covariance matrix is called an orthogonal Gaussian mixture model (OGMM), and it presents a better approximation to the distribution of feature vectors [11, 20].
 each class with M Gaussian components is trained with training samples of the corresponding class. For an un-known pattern x , the recognition class label  X  c is assigned by the following equation: 3.2 MLP with sigmoid activation function A multilayer perceptron trained with the well-known backpropagation (BP) is shown in Fig. 2.
 tivation function, MLP possesses a good generalization. The sigmoid function has the following form: Let y j be the output of the j th node in the current layer. y j has the following form: where  X  ij , i =1 , 2 ,... is the the weight vector from all nodes of the forward layer to the j th node in the current layer, x i is the output of the i th node in the forward layer, and  X  is the threshold of the current layer. The current layer can either be a hidden layer or an output layer. A sigmoid function curve is shown in Fig. 3. sigmoid activation function is a poor outlier rejection method. For a 2D feature (Fig. 4), point A is a normal sample, point B is very far from each class and should be considered as an outlier, and w is the weight vector. illustrated as follows: y j ( A )= f (( w, x A )+  X  )= f ( w  X  x A  X  cos(  X  )+  X  ) (6) y ( B )= f (( w, x B )+  X  )= f ( w  X  x B  X  cos(  X  )+  X  ) , where ( w, x ) is the inner product of vectors w and x ,  X  is the vector module, and  X  is the angle between input vector and weight vector. For 0  X   X   X   X / 2, because x
B  X  x A , the following relation can be obtained: will fall into the dark area in Fig. 3, then point B can also activate it. But point B is an outlier and should not activate the neuron. 4 Decision regions of MLP and OGMM Vasconcelos et al. [17] introduce a method to study de-cision regions of neural networks. The steps in their method are as follows. Design a two-class problem in 2D space and generate training and test samples in it. Train the network, and then classify all the points in this space with a threshold. A sample whose recognition output is larger than the threshold will be accepted; otherwise it will be rejected.
 per are shown in Fig. 5. Table 1 gives a description of classifiers whose decision regions will be investigated on these samples. hidden layer and two input nodes. For classifiers 4, 5, and 6, 0.9999 is used as the rejection threshold. For other classifiers, the threshold is obtained from the training process, i.e., the minimum value of recognition outputs of each class is recorded. Figure 6 shows the decision regions of each classifier.
 of hidden nodes, decision regions of MLPs undergo vir-tually no change. By using a higher threshold, 0.9999, the gap between each class decision regions has been en-larged, but their shape has not changed. The shape of MLP decision region is an open infinite half-space. By contrast, the decision regions of OGMMs are closed, and they are more similar to the original training samples X  distribution with more Gaussian components.
 gation show that decision regions of MLPs cannot be changed easily even by using higher thresholds and more hidden nodes. It is hard to predict whether they are open or not [4]. With an open decision region, MLPs will trust samples that are far from each class and should be rejected as outliers. OGMMs possess a closed deci-sion region and can approximate samples X  distributions smoothly, so they must have a good outlier rejection ca-pability. 5 A hybrid model based on the combination of an MLP and an OGMM Based on the results and analyses in Sects. 3 and 4, the MLP and OGMM hybrid model is proposed. For a C -class problem, let x be the input unknown sample. The processing of the proposed model can be concluded as follows:  X  Use MLP to classify unknown patterns, and let i  X   X  Calculate the probability density of the recognition  X  Use the following expression to decide whether to ac-two separate parts, the MLP training process and the OGMM training process. Thus we can use the existing classifier that has been trained in previous applications without having to design and train a new one. For the testing process, we first recognize the sample with MLP that possesses high recognition accuracy, and then we only check the recognition result with the OGMM that can reject outliers easily.
 6 Experiments 6.1 Data sets The effectiveness of this hybrid model is tested on printed numerals gathered from real Chinese bank checks. Printed and handwritten Chinese characters gathered from bank checks together with simulated En-glish letters and symbols are selected as outliers. Ran-dom noise is added to these simulated samples, which are printed out and then scanned as images. Table 2 gives more details of these samples. Figure 8 shows some samples used in this experiment.
 fonts. Scanned numerals are usually distorted and con-tain noise, which makes recognition difficult. The se-lected outliers are contents in a bank check that will be input into the recognition system when the error area is searched and most of the outliers are significantly dif-ferent from numerals. 6.2 Feature and recognition models A directional element feature [6] is adopted, and the original 196-dimensional feature is converted into a 64-dimensional feature with a Karhunen X  X o`eve transfor-mation. A three-layer MLP trained by backpropagation with 64 input nodes, 150 hidden nodes, and 10 output nodes is used. An OGMM with 9 Gaussian components of each numeral class trained by the EM algorithm with the same feature as that of MLP is adopted.
 outlier rejection abilities including MLP with direct connections (DMLP) [18], MLP with additional guard nodes (MLP GUARD) [16], and MLP trained with both printed numerals, and printed Chinese characters (MLP NEG). The architecture of these networks is the same as that of the aforementioned MLP. For printed Chinese character training samples, we set all the target values of MLP NEG at 0. 6.3 Results and discussions A. Reco gnition accuracy. Recognition rates of each model are presented in Table 3. The results in Table 3 show that MLP possesses the strongest discrimination ability. By adding negative training samples, the recogni-tion rate is decreased. OGMM shows a lower recognition accuracy than other models.
 B. Outlier rejection results. We set the threshold of MLP, MLP NEG, and DMLP at 0.9999; samples whose recognition outputs are less than this threshold will be rejected. The threshold of MLP GUARD, OGMM, and MLP OGMM is obtained from the training process. The outlier rejection results are shown in Table 4. capability. Figure 9 shows some incorrectly rejected out-liers of MLP OGMM and of MLP with the thresholding criterion. For the hybrid model, only samples that are similar to numerals fail to be rejected, but for outliers that are significantly different from numerals, MLP still gives a large output value ( &gt; 0 . 9999).
 cellent. Trained only with printed numerals, OGMM can reject outliers such as Chinese characters and English letters effectively. MLP GUARD can reject outliers that are significantly different from numerals, such as Chinese characters, more effectively than MLP. But for similar outliers such as printed symbols and letters, its outlier rejection ability is even worse than that of MLP. By using Chinese characters as negative training samples, MLP NEG possesses a better Chinese character rejec-tion ability than MLP does. But for outliers not included in training samples such as symbols and letters, its rejec-tion performance is poor. MLP NEG cannot even reject English letters as effectively as MLP does. So for situ-ations where outliers cannot be predicted and collected entirely, training MLP with outliers is not an effective method to reject outliers. The outlier rejection ability of DLMP is quite poor. This result contradicts sharply the results in the literature [18]. Thus by changing the internal architecture of MLP, not only does the training process become hard, but also the performance cannot be guaranteed.
 C. Reco gnition reliability. We investigated the match-ing time cost and recognition reliability of each model. The results are given in Table 5. Let N n be the total number of testing numerals, N r n the number of numer-als accepted and recognized correctly, N a n the number of numerals that are accepted, N o the total number of out-liers, and N r o the number of outliers rejected correctly. We calculate the final recognition rate (F REC RATE), recognition reliability (RR), and final reliability (FR) as follows: a larger output value ( &gt; 0 . 9999). MLP OGMM, MLP GUARD, and OGMM can reject more numerals than MLP and MLP NEG. Because MLP OGMM re-jects samples with poor qualities such as with heavy noise, etc., its final recognition rate decreases slightly. Although the final recognition rate of MLP OGMM is lower than that of MLP, it is higher than that of OGMM with the thresholding criteria alone. Those rejected nu-meral samples are heavily distorted. Figure 10 shows some of these samples. These samples are correctly rec-ognized by MLP, but because they possess a low proba-bility estimated by OGMM, they are rejected. By reject-ing these ambiguous samples, the reliability of the whole system can be improved.
 ognized, MLP is the most reliable model. But for ap-plications where outliers are introduced, such as bank check recognition, MLP OGMM is the most reliable model. According to Eq. 12 and the results in Tables 4 and 5, MLP OGMM is more reliable than MLP when N o /N n &gt; 0 . 000478. slightly, but it is still faster than that of OGMM and can satisfy the requirement of real-time recognition. 7 Conclusions In this paper a hybrid model based on a combination of MLP and OGMM for Chinese bank check printed nu-meral recognition is proposed. With the participation of MLP and OGMM, the model possesses high accu-racy as well as good outlier rejection ability. Although most Multiple Classifier Systems (MCS) aim to improve recognition accuracy, MCSs can increase other kinds of performance like outlier rejection performance. The pro-posed hybrid model utilizes MLP to recognize printed numerals and OGMM to reject outliers.
 eral advantages. First, each class can be modeled by a series of Gaussian models separately, which gives a more accurate distribution approximation than if the whole distribution of all classes were modeled together. Second, a high processing speed is attained simply by estimat-ing the probability density for one class. Experimental results have shown that for situations where outliers are introduced during recognition, MLP OGMM is a better choice than MLP because of its high reliability. The pro-posed hybrid model has been successfully used for Chi-nese bank check printed numeral recognition in real ap-plications. Experimental results show that the proposed hybrid model is excellent in the following respects.  X  High recognition accuracy and high processing speed.  X  Excellent outlier rejection ability.  X  Good utilization of existing classifiers. The architec-References
