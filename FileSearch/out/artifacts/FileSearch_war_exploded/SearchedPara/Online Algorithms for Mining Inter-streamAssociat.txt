 Data mining is an area of active database research because of its applicability in various areas. One of the important tasks of data mining is to extract frequently occurring patterns or associations hidden in very large datasets. In recent years, stream processing and in particular sensor networks has attracted much research interest [ 1 , 3 ]. Stream processing poses challenging research problems due to large volumes of data involved and, in many cases, on-line processing requirements. be regarded as a sensor. In our model, we assume that a sensor only takes on a finite number of discrete states. Also, we assume that a sensor only reports state changes. A sensor stream can thus be considered as a sequence of updates such that each update is associated with a time at which the state change occurs. Figure 1 shows a system of six sensors ( S 1 , ..., S 6 ), each could be in one of the two possible states  X  X ow X  ( L ) and  X  X igh X  ( H ). Our goal is to discover associations among sensor values that co-exist during a significant portion of time. quent value sets is similar to mining frequent itemsets. One possible approach is to transform the stream data into a dataset of transactions, and then apply a tra-ditional mining algorithm like Apriori to the resulting dataset. A straightforward data transformation would generate transactions by taking snapshots of sensor states at regular intervals. Alternatively, one can derive a transaction only when there is an update (from any sensor). With this approach, different snapshots of the sensor states could have different life-spans, which are taken as the weights of respective transactions. Tables 1 and 2 demonstrate these transformations. tain a lot of redundancy. One can observe from Tables 1 and 2 that successive transactions only differ by at most one sensor value. The redundancy causes traditional mining algorithms to perform badly.
 sent a stream by an interval list . Given a sensor S and a value v , the interval list IL ( S = v ) is a list of ( start-time, end-time ) pairs. Each pair specifies the start time and the end time of a time interval during which S assumes the value v . efficient mining algorithm in a large sensor network environment over the tradi-tional Apriori-based approaches. This is because the representation avoids data redundancy which leads to a much smaller dataset. Moreover, determining the support of a sensor value set is achieved by list intersection. This avoids the large number of redundant subset testing performed by Apriori-based algorithms. generated by a massive sensor network, data analysis algorithms should be online and one-pass. In [ 4 ], Manku et al. proposed the Lossy Counting algorithm, which is an online, one-pass procedure for finding frequent itemsets from a data stream of transactions. In this paper, we study how the Lossy Counting framework can be used to derive online one-pass algorithms for mining large sensor streams under the two data representations (weighted transactions and interval list). of finding frequently co-existing sensor value sets in Section 2 . In Section 3 , we review the Lossy Counting algorithm. In Section 4 , we define interval list and propose an interval-list-based algorithm for solving the problem. Section 5 reports the experimental results. Finally, Section 6 concludes the paper. We define a sensor as a device for monitoring and reporting the states of some physical attribute. The set of all possible states of a sensor S is called the domain of S . We assume that every sensor has a finite domain. (If the domain of a sensor is continuous, we assume that it can be appropriately quantized and mapped to a finite one.) We assume that the state of a sensor changes at discrete time instants called updates . The state of a sensor stays the same between updates. S = v where S is a sensor and v is the state reported. If the state of sensor S is v at a certain time t , we say that the sensor value S = v is valid at time t . Given a time interval I , if S = v is valid at every instant of I , we say that S = v is valid in I . A sensor network consists of a number of sensors. A set of sensor values V is valid in an interval I if all sensor values in V are valid in I . 0. At any time instant T ( &gt; 0), the support duration of a value set V , denoted by SD ( V ), is the total length of all non-overlapping intervals within [0 , T ] in which V is valid. We define the support of V , denoted by sup ( V ), as SD ( V ) /T . A value set V is frequent if sup ( V )  X   X  s , a user specified support threshold. would require keeping track of all value sets that have ever occurred. The high memory and processing requirements render this approach infeasible. As an al-ternative, we adopt the Lossy Counting framework proposed in [ 4 ] and report all value sets that are frequent plus some value sets whose supports are guaranteed to be not less than  X  s  X   X  for some user-specified error bound  X  . In [ 4 ], Manku and Motwani propose Lossy Counting, a simple but effective al-gorithm for counting approximately the set of frequent itemsets from a stream of transactions. Since our algorithms use the framework of Lossy Counting, we briefly describe the algorithm in this section.
 bound  X  . Itemsets X  support counts are stored in a data structure D . We can consider D as a table of entries of the form ( e, f,  X  ), where e is an itemset, f is an approximate support count of e , and  X  is an error bound of the count. The structure D is maintained such that if N is the total number of transactions the system has processed, the structure D satisfies the following properties: P1: If the entry ( e, f,  X  ) is in D , then f  X  f e  X  f +  X  , where f e is the exact P2: If the entry ( e, f,  X  ) is not in D , then f e must be less than  X N . vided into batches. The size of a batch is limited by the amount of memory available. Figure 2 (a) illustrates the update procedure of D . Let B be a batch of transactions. Let N 1 denote the number of transactions before B and let D 1 denote the data structure D before B is processed. Lossy Counting enumerates itemsets that are present in B and counts their supports in the batch. Let e be an itemset that appears in B whose support count w.r.t. B is f B . D is then updated by the following simple rules ( D 2 denotes the updated D in the figure):
Insert: If D 1 does not contain an entry for e , the entry ( e, f B ,  X N 1 ) is created in D unless f B +  X N 1  X   X N 2 , where N 2 is the total number of transactions processed including those in B .
 Update: Otherwise, the frequency f of e in D 1 is incremented by f B . Delete: After all updates, an entry ( e, f,  X  ) in D is deleted if f +  X   X   X N 2 . done. Details of which can be found in [ 4 ]. Besides, to apply Lossy Counting to our frequent value set mining problem, a few modifications have to be made. Due to space limitation, readers are referred to [ 2 ] for details. In this section we formally define interval lists and discuss how they could be used to mine frequent value sets under the Lossy Counting framework.
 where t and  X  t are the start time and the end time of the interval, respectively. The duration of I is given by  X  ( I ) =  X  t  X  t . Given two intervals I 1 = ( t 1 ,  X  t 1 ) intersection is given by I 1  X  I 2 = ( t 2 , min(  X  t 1 ,  X  t 2 )). interval list are ordered by their start time. The duration of an interval list IL is given by  X  ( IL ) = intersection is defined as: IL 1  X  IL 2 = list that contains all and only those intervals in which the value set V is valid. We call such an interval list the interval list of V . Given two sensor value sets, V 1 and V 2 , it can be easily verified that the interval list of V 1  X  X  2 can be obtained by the Lossy Counting framework in the following way. First of all, time is parti-tioned into a number of intervals, each corresponds to a batch of sensor updates (see Figure 2 (b)). Instead of representing a batch of updates as a set of weighted transactions, the updates are represented by the interval lists of the sensor val-ues. Similar to the case of Lossy Counting, the size of a batch is limited by the amount of buffer memory available. Also, a data structure D is again used that keeps track of certain sensor value sets X  support durations. The function and properties of D is the same as those described in Section 3 .
 in a value set V is its size . The procedure starts by collecting all size-1 value sets into a set of candidates, C 1 . The batch B is represented by a set of interval lists, one for each sensor value. The procedure then executes a while loop. During each iteration, a set of candidate value sets, C i , is considered. Essentially, each value set V in C i is of size i and that V  X  X  support duration up to time T 2 has the potential of exceeding  X T 2 . The procedure then verifies whether V should be included in D by finding its support duration in batch B . This is achieved by computing IL ( V ) in B through intersecting the interval lists of relevant sensor values, followed by determining the total length of all the intervals in IL ( V ). D is then updated by function Update (), described in Figure 4 , which essentially follows the three update rules listed in Section 3 .
 sets are properly updated. These entries are collected in D i . The set D i is used to generate the candidate set C i +1 for the next iteration. More specifically, a size-( i +1) value set V is put into C i +1 unless there is a size-i subset V 0 of V that is not in D i . This is because by Property P2 of D (see Section 3 ), if the entry ( V 0 , f,  X  ) is not in D i , we know that the support duration of V 0 w.r.t. time T 2 must be smaller than  X T 2 . Since the support duration of V cannot be larger than the support duration of its subset V 0 , the support duration of V is smaller than  X T 2 as well. That is, V should be left out of D and needs not be considered. procedure of processing a batch. Interested readers are referred to [ 2 ] for details. We performed extensive experiments comparing the performance of the min-ing algorithms using different data representations. Due to space limitation, we highlight some of the findings here. A detailed analysis can be found in [ 2 ]. resentation is that it is more space-efficient than the weighted representation. Figure 5 shows the size of the dataset generated for a stream history of 92000 time units when the number of sensors in the network varies from 100 to 600. The dataset size grows linearly w.r.t. the number of sensors under the Interval-List-Based Lossy Counting algorithm (ILB) described in Section 4 . The weighted transformation representation (LC), however, does not scale well.
 an example, Figure 6 shows that ILB is much more efficient than LC for a 600-sensor network, especially when  X  s is small. It is because, as shown in Figure 5 , the dataset size for LC is much (31 times in this case) larger than that for ILB. Hence, for LC, a batch contains 31 times fewer updates than that of ILB. The small batch size is undesirable for the Lossy Counting framework because of the false alarm effect, as discussed in [ 2 ].
 work. ILB still outperforms LC although the margin is less drastic than the 600-sensor case. It is because, for a 400-sensor network, the dataset size is much smaller for LC. This allows a larger batch and thus the effect of false alarm is ameliorated. We study the problem of mining frequent sensor value sets from a massive sen-sor network. We discuss methods for representing sensor stream data. We derive online mining algorithms, namely, LC and ILB and evaluate the algorithms X  per-formance through experiments. The results show that ILB could outperform LC by a significant margin, particularly for large sensor networks. The interval list representation is thus a viable option in representing a massive sensor network.
