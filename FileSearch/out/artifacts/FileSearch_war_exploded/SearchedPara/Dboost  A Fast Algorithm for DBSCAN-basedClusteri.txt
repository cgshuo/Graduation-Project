 DBSCAN is a classic density-based clustering technique, which uses the density property of data points to identify groups or connected structures. The density in DBSCAN can be defined as the number of the neighbours ( -neighborhood) within a predefined distance to a given point. By utilizing the density infor-mation, DBSCAN is not limited to finding only compact and spherical shaped clusters [ 1 ], nor does it need a predefined k as total cluster numbers or be sus-ceptible to noisy outliers. However, DBSCAN requires much time in its running due to the density calculation, which becomes much worse when facing high dimensional data.
 High dimensional data is usually referred to the data with hundreds of dimen-sions. They are widely appearing in many areas of data analysis such as textual data, DNA microarray data and daily voltage data on wires. Due to the high dimensionality, the -neighborhood querying of DBSCAN becomes very time consuming.
 For example, in our work with National Grid, we analysis electric wire voltage data to find the similar power consuming wire regions (by DBSCAN based clus-tering). We find a bare 1-year record can get over 1000 dimensions, let alone the huge number of records in a big area, which makes a DBSCAN process consume nearly a whole day to assure only one possible result.
 However, most current accelerating approaches for DBSCAN have not yet fitted in circumstances on such high dimensionality. Usually, they either work when dimensionality is lower than 50 [ 2 ] or fail in guaranteeing the generality of application, which usually leads to unacceptable clustering results [ 3 , 4 ]. In this paper, we propose an algorithm named Dboost, to accelerate DBSCAN-based clustering without accuracy loss on high dimensional data. Dboost aims at reducing and improving the density calculations of DBSCAN. To achieve this, it first uses an adaption ( WAND # ) of a ranked retrieval technique WAND [ 5 ] to speedily fetch a larger neighborhood for a checking point p ,in its density calculation, then it checks all p  X  X  -neighbors X  density based on only p  X  X  larger neighborhood, instead of giving each member in p  X  X  -neighborhood a global searching as the original DBSCAN does.
 Major contributions of Dboost are from two aspects: Firstly, we novelly apply the idea of the ranked retrieval technique WAND [ 5 ] in information retrieval (IR) field to handle the density-calculation task of DBSCAN, which dramati-cally increases DBSCAN X  X  speed performance on high dimensional data. Sec-ond, we reduce the invoking times of our WAND adaption by taking advantages of DBSCAN X  X  characteristics, to further improve the acceleration substantially. Dboost can be used in a single-machine setting, or at individual node in parti-tioned implementations for parallel computing.
 Experiments were conducted on Netflix data [ 6 ], wire voltage data and microblog corpus, which are typical high dimensional data. We compared Dboost with DBSCAN, LSH-based DBSCAN and k-means on the speed efficiency. The results showed that over 50 times speeding up without accuracy loss were achieved on wire voltage data and Netflix dataset. We also found that about 99.9 % of data would be filtered out during data X  X  density calculation in the microblog corpus, which implies that much more than 100 times acceleration should be achieved with the data size growing. DBSCAN DBSCAN was first proposed by Ester et al. [ 7 ] and has been used as a base for many other techniques [ 2 ]. There are much work on enhancing the speed of DBSCAN, which can be divided into two categories. Accelerating without loss in clustering accuracy and accelerating with accuracy loss. trees can reduce the complexity to O (NlogN), but they work well only when the dimensionality is lower than 50 [ 8 ]. [ 9 ] was proposed to accelerated by clus-tering in parallel, but it was still inefficient with each executing node on high dimensional data.
 sampling [ 4 ]. Recently, Zhang et al. proposed Linear DBSCAN algorithm based on locality sensitive hashing [ 10 ]. The main problems in this technique locates in adding more input parameters [ 3 ] and lacking of generality.
 Ranked Retrieval. Ranked retrieval strategies are used as query evaluation strategies to reduce data in the calculations for a query search. The strategies can be divided into two main classes: Term at a time (TAAT) and document at a time (DAAT). A recent survey and a comparative study of in-memory TAAT and DAAT algorithms was reported in [ 11 ]. A large study of known TAAT and DAAT algorithms was conducted by [ 12 ]. They found that the Moffat TAAT algorithm [ 13 ] had the best performance, but it came at a tradeoff of loss of precision compared to naive TAAT approaches and other approaches.
 memory and they exploit I/O parallelism more effectively. So, we choose DAAT. Our method finally chooses WAND [ 5 ], because the characteristics of WAND is well fit for the large scale data and numbers of features in the query [ 11 ]. In this section we describes the very details of Dboost.
 as DBSCAN, for it only focuses on reducing and improving the unnecessary density calculations. Consequently, it preserves DBSCAN X  X  clustering accuracy. The parts Dboost differs locate in the density-fetch related subroutines: distance comparisons with all of the rest points, to get their corresponding -neighborhood. However, Dboost treats each -neighborhood search as a querying issue in IR, and novelly applied the ranked retrieval technique WAND for such search implementation (our WAND # ). Later, we figure out there is no need to treat each ( WAND # ) search as an independent new search, based on DBSCAN X  X  characteristics. Thus we further improve ( WAND # ) to reduce its invocation times.
 To better explain our work, we first introduce the distance metric we choose, then, the background and soundness of our WAND adaption ( WAND is the base idea Dboost takes advantage of. Finally, in Sect. 3.3 , we describe the improvements of ( WAND # ) as well as the whole Dboost algorithm. Algo-rithms 1  X  3 gives a basic description of Dboost and they will be explained in Sect. 3.3 . 3.1 The Distance Metric We choose our distance metric based on the Cosine similarity in order to make usage of WAND as well as get fast comparison speed. For two points p,p X  their standard cosine similarity is calculated as: where  X  denotes the vector dot product, and denotes the vector X  X  length. Recall that, since the cossim is always between 0 and 1, so we define =1-cossim(p,p X ) as the distance metric , which is non-negative, symmetric and satisfies the triangle inequality.
 The usage of Cosine similarity based metric has the same effect as using Euclidean distance metric. For the fact that, when cossim ( a, c ) &gt; cossim ( a, b ), the Euclidean distance between a and c is exactly closer than a and b ,whichis easy to prove. As for the situations when cossim ( a, c ) = 0, then a and c are too dissimilar (sharing no common features) to be neighbors and they shouldn X  X  be considered as neighbors under Euclidean distance metric, too. 3.2 The Background and Soundness of WAND # WAND # is adapted by the WAND algorithm [ 5 ], which is a method allowing for fast retrieval of the top-k ranked documents for a query in the IR field. Although the task of WAND # is to fetch a neighborhood within a given distance d ,which is different from the top-k retrieval issue, the conversion of the task is not difficult for WAND: WAND makes comparisons by Cosine similarity. A global view of WAND mechanism can be seen in Fig. 1 (a). When searching, WAND keeps a heap H of nearest top-k points among the points it checked so far. The similarity score of the k-th best result acts as a threshold  X  . WAND works by repeatedly calling its subroutine next to fetch a next point (in order of point ID) whose similarity score surpasses  X  and using it to update H . The final results in H is then the top-k neighbors.
 To convert WAND to achieve the task of WAND # , we can change the heap H in Fig. 1 (a) into a set with unlimit size, and change the  X  to be fixed to the distance d . This is just the previous version of our WAND of our final WAND # version.
 WAND, which is also the core idea of WAND. WAND aims at using an upper bound of each term X  X  similarity contribution to remove points that are too dis-similar from the query to become a member of the top-k list. To explain this better, we first introduce some preliminaries, then the details of WAND logic. point in our situation, and term corresponds to feature or dimension .If a document doesn X  X  contain a term, its corresponding weight under that term (dimension) is zero (otherwise, above zero). WAND runs on top of inverted index , which is an efficient indexing way for high dimensional sparse data. In inverted index , each term (dimension) has an associated points list which contains all of the documents (points) that contain this term. The document (point) are expressed as its vector value the same as the point in our clustering issue and they are sorted by their ID in ascending order in points list .
 query terms (dimensions) that points at an entry in the corresponding points list (see Fig. 1 (b)). During the searching of the next point in H ,the cursors are kept sorted by the document (point) ID they point to, in a list called cursors list . (non-zero weight dimension), WAND fetches the upper bound of its entry points X  weights under this dimension (term), and uses it to calculate the similarity con-tribution upper bound UB t for each cursor with respect to their corresponding terms. Next, all cursors will be initialized to point at the first entry in their corresponding points lists (the one having the minimum point ID in the list). be over  X  , then the point can never be a true member in the top-k list, and we should skip to a next possible point. The first point that has the upper bound of its similarity score higher than  X  is denoted as the pivot point . The finding of pivot point relies on next X  X  helper function findPivotTerm , which returns the earliest cursor index, o*, in the cursors list , such that the sum of the upper bound UB t , for all cursors preceding o* in cursors list ,isatleast  X  (See Fig. 1 (b), the sum of UB t s preceding 3-th cursor is 0.6) Given o*, we then check whether the cursor in the first and the o*-th position point to the same point. Since the cursors are sorted by the point ID they point at, if the above is false, the point whose ID smaller than ID* cannot be a pivot point (like the point (id = 3) in Fig. 1 (b)), and we should advance the corresponding cursors. In next , a function named PickTerm then selects one of these cursors to advance to the first point with ID at least ID*. On the other hand, if the above is true, then the point pointed by o*-th cursor is a real pivot point , and it should be fully checked for the similarity score and return to update heap H if the score really surpasses.
 The WAND algorithm is fully described in [ 5 ]. 3.3 Improvements for WAND # The improvements for WAND # is in two aspects: to reduce its invocation times and to diminish its searching scope. All of which based on the characteristics of DBSCAN.
 Invocation Reduce. A short description of DBSCAN can be seen in Sect. 2 . During the clustering of DBSCAN, each point in the data set shall be checked for density, which means a WAND # search of overall data shall be repeated as many times as the number of data points. On the other hand, many of these searching results are overlapped: In Fig. 2 , P and P X  are two points in the data set, with their -neighborhood within circle yellow and circle grey. The blue points are the common neighbors for both P and P X  , yet they have been searched from overall data for at least two times during the density check for P and P X  and other points in Fig. 2 .
 of P first, then check the density of each P X  X  -neighbor within just the larger neighborhood. This relies on Theorem 1 which is easy to prove, since the distance metric we use here follows the triangular inequality (see Sect. 3.1 ). Theorem 1. For any point p # in point p X  X  -neighborhood, the -neighborhood of p # is within p X  X  2 -neighborhood.
 Algorithm 1. Dboost Algorithm 2. ExpandCluster is two times the range of .If p is a core point, then we should check all its -neighbor X  X  density as described in DBSCAN. Based on Theorem 1 , all -neighbors of p X  X  -neighbors are now within ES .Soall p X  X  -neighbors X  checking can be operated in the scope of ES , instead of the entire data space. To make this better, we can combine density checking with the ES querying process: Algorithm 3. WAND # When we fetch a ES member m , we compare it with , and identity it as the inner-neighbor (neighbor whose distance &lt; )orthe outer-neighbor (distance ). We then update the -neighbor number (density info) of p X  X  inner-neighbors by calculating their distance scores with m and plus 1 if the score exceeds .As for the core points among p X  X  inner-neighbors , we use a special data structure called moonSet to store their -neighbors for later cluster expansion (see Sect. 2 ). The moonSet is described in Fig. 2 . Notice the moonSet of P X  in Fig. 2 only stores the outer-neighbors of P , for the inner-neighbors are all checked by the end of ES fetching and got added into the corresponding cluster.
 Scope Diminishing. This improvement is simple: To make WAND more effec-tive, we reduce its searching scope by repeatedly eliminating the points in one cluster from the global search scope, once the cluster is fully established. Because now the clustering problem subjects to the sub problem of the rest of the points. The diminishing step is in Algorithm 1 (Lines 8, 9).
 searching scope to it every time it is invoked, and so as to its subroutine next (Lines 3). Each time it fetches a 2 -neighbor of the query point, it identifies the neighbor as inner-neighbor or outer-neighbor and updates the density info of query point X  X  inner-neighbors (Lines 8 X 28). Notice the function f() in Line 8 is our distance metric defined in Sect. 3.1 .
 Rest Detail of Dboost. The top level of Dboost is described in Algorithm 1 , once it identifies a cluster and expands it (Lines 6, 7), it removes the points in the cluster from the global searching scope (Lines 8, 9). Algorithm 2 is a DBSCAN expanding subroutine, but it only adds the points in the moonSet to the expanding set (Lines 6, 9 X 11) as described above. In this section, we will first give a brief introduction to the experiment setup, including the experiment environment, datasets, comparison approaches. We then evaluate the performance of Dboost over other approaches in different aspects. Finally we will display the experimental results and make some dis-cussion. 4.1 Experiment Setup The experiment is built on a common PC with an Intel(R) Core(TM) i3 X 2120 CPU, and 8 G RAM. The operating system is a 64bit-windows 7.
 we use three datasets in our experiment, wire voltage data, Netflix dataset and microblog corpus.
 on time series. The Netflix dataset consists of movie rating score data from Netflix Prize competition [ 6 ]. We use its rating info of 420 thousand customers over 13 thousand movies (dimensions), and we cluster customers by their movie ratings. The microblog corpus consists of 120 k articles from Sina weibo and it contains 305 thousand terms (dimensions).
 ing methods.  X  DBSCAN: is the classic density-based clustering approach, we make it run under inveted index .  X  k-means: is the classic centroid-based clustering approach, we make it run under inveted index .  X  LSH: is the current fastest DBSCAN approach but with accuracy loss.  X  Dboost-p: is a poor implemention of Dboost, which directly uses WAND for each point X  X  -neighborhood searching.  X  Dboost: the standard Dboost in this paper.
 The DBSCAN and k-means methods are implemented with inverted index as a result of the high dimensionality of our experiment data sets, the original version of the two algorithms cannot run the data with their original data struc-tures. Due to the same reason, the indexing or partition based methods, e.g. R-tree, are not suitable as well, but they cannot be applied on inverted index and only work when the dimension is below 50. We implement LSH with the hash technique mentioned in [ 14 ].
 To establish a fair comparisons, we set the = 0.4 (which means the similarity threshold for WAND is 1  X  = 0.6. See first head in Sect. 3 ),  X  = 10 for all the DBSCAN-based algorithms, and set k = 10 for k-means with randomly initial centers. The k-means here is used for speed comparison only, and its predefined cluster numbers k is equal to the smallest number of DBSCAN X  X  results. The extra parameters of LSH were set as the value to achieve the best performance. We doesn X  X  show the accuracy comparison in this paper, as the outputs on experiment data of Dboost are exactly the same to DBSCAN and accuracy is not our aim. As described in Sect. 3.2 , Dboost doesn X  X  change the identifying logic for clusters of DBSCAN. 4.2 Comparison for Speeding up We first compared the efficiency on the three datasets, and the experimental results were shown in Fig. 3 (a) and (b).
 of data size. On the contrary, DBSCAN was with the temporal complexity O ( N 2 ), so the time it cost was increasing dramatically. The performance of k-means only defeated DBSCAN, when the cluster numbers limited to only 10, however, it performed poor when the k became larger. Meanwhile the cluster numbers for DBSCAN varied from 10  X  306 with different data scale.
 than the LSH. Note that, Dboost achieved acceleration without loss in accuracy while LSH cannot guarantee the generality of quality.
 is because the wire voltage data is much denser than the other two data sets, which lows down the effect of inverted index .
 Influence of . Since WAND uses Cosine similarity threshold (1 points X  similarity calculations, the smaller the threshold, the less the dissimilar points get skipped. Thus the setting of 1  X  do influent the accelerating effect of Dboost. Figure 3 (c) shows a tint that Dboost X  X  performance may not be restricted too much by 1  X  when 1  X  value is in a reasonable range. The results in above section shows a significant speeding up even with 1  X  set to be 0.6. 4.3 Discussion To provide an insight of Dboost, Fig. 3 (d) demonstrates the average ratio of the promising candidate points that participate a fully similarity calculation, which explains Dboost X  X  high acceleration effect. The results show that our improved Dboost can achieve around 0.1 %, and the ratio is empirically independent to the growth of the data size. That means 99.9 % data can be removed from density calculation. Note that compared with the real world data, the size of the corpus with 120 k microblogs is quite low. Since the condition judgment weights less and less significant with the growing of data size. It implied that much more than 100 times speeding up against the traditional DBSCAN would be achieved. In this paper, we target to improve the efficiency of DBSCAN-based approach on high dimensional data. An efficient approach, named Dboost, is proposed to accelerate clustering speed through the ranked retrieval strategy WAND. WAND adaption. The experimental results showed that 70 times speeding up without accuracy loss were achieved on the Netflix dataset. We also found that about 99.9 % of data would be filtered out during data X  X  density calculation in the microblog corpus, which implies much more than 100 times acceleration will achieved with the data size growing.
 Though Dboost is adaptable for many kinds of high dimensional data, it was originally proposed to fast detect similar wire regions in our work with National Grid. In the future, we will focus more on its performance on denser data.
