 1. Introduction
Automatic text classification (TC) is essential for information sharing and management. One popular way to TC is to build a classifier for each category. Upon receiving a document, each classifier autonomously makes a yes X  X o decision for its corresponding category. To make such decisions, each classifier is associated with a threshold. A document is  X  X  X ccepted  X  by the classifier only if its degree of acceptance (DOA) with respect to the category (e.g., similarity with the category or probability of belonging to the category) is higher than or equal to the corresponding threshold; otherwise it is  X  X  X ejected.  X  With the help of the thresholds, text filtering is actually achieved in the course of TC. Each document may be classified into zero, one, or several categories.
The ideal goals of TC are to achieve high-quality classification: (1) accepting almost all documents that should be accepted and (2) rejecting almost all documents that should be rejected. More specifically, the ideal goals aim to achieve high recall and precision , respectively. Recall is measured by (total number of correct clas-sifications/total number of correct classifications that should be made), while precision is measured by (total number of correct classifications/total number of classifications made).

Unfortunately, the ideal goals were rarely achieved simultaneously, since most classifiers could not be per-due to two common problems: (1) imperfect selection of training documents (e.g., noises, over-fitting and con-tent ambiguities) and (2) imperfect system setting (e.g., parameter setting and feature selection). These prob-lems incur improper DOA estimations, and hence high-quality TC is seldom achieved. 1.1. Problem definition
In this paper, we explore a way to help classifiers to achieve high-quality TC by conducting intelligent inter-users are consulted only when necessary, reducing the cognitive load on the users. Our goal is to achieve high-quality TC: achieving high performance in both recall and precision, with a controlled amount of cognitive load on the users.

Interactive high-quality TC differs from traditional classifier-improving tasks, which often aimed at improv-&amp; Buckley, 1997 ), and document labeling ( Godbole, Harpale, Sarawagi, &amp; Chakrabarti, 2004; Nigam &amp;
Ghani, 2000 ). Actually, it complements these classifier-improving processes: when the classifiers cannot be improved any more, system X  X ser interaction is a promising (and perhaps final) approach to achieve high-qual-ity TC. 1.2. Motivation
Interactive high-quality TC is particularly helpful for information archiving and recommendation. Its con-firmation requests are actually indications of possible TC errors, which help users to process information more properly, no matter whether the users may conduct confirmation online. More specifically, for each category c , the system directs users to check two types of information items: definitely relevant items (i.e., those that should be classified into c ) and potentially relevant items (i.e., whose classifications require confirmation).
Information managers may thus enrich c both more precisely and completely, and those readers that are inter-ested in c may read what they should read, and avoid missing much information, by simply checking a limited amount of information that is potentially relevant.

Moreover, interactive high-quality TC is also essential for those applications in which an erroneous TC decision may incur high cost and/or serious problems. Healthcare information support is an example of the applications (ref. Fig. 1 ). The Internet has been a major information source for patients, who require rich information in several aspects (e.g., cause, symptom, curing, and prevention of each disease, Fahey &amp; Wein-berg, 2003 ). Automatic gathering and classification of healthcare information are thus helpful. However, information from the Internet may not be scientifically-based ( Tang, Hawking, Craswell, &amp; Griffiths, 2005 ), while patients require information verified by healthcare professionals ( Kittler, Hobbs, Volk, Kreps, &amp; Bates, 2004 ), who have only a limited amount of time to make verification. With interactive high-quality TC (1) patients may enter interest descriptions (e.g., symptoms), which are classified to retrieve relevant information (e.g., related diseases) or send inquiries to corresponding healthcare professionals, and (2) healthcare profes-sionals may be consulted only when necessary to verify information gathered and classified, or respond to patients X  inquiries. 1.3. Main challenges and organization of the paper
Main challenges of interactive high-quality TC lie on the tradeoff between the quality of TC and the cog-nitive load on the users. Obviously, a straightforward way to achieve high-quality TC is to consult the user very frequently. However, this approach dramatically increases the cognitive load of the user in making ver-ification. Conversely, to reduce the cognitive load on the user, the system should make most decisions by itself, and hence deteriorates the quality of TC. To simultaneously achieve high-quality TC and control the cognitive load, the system needs an intelligent way to determine when to consult the user.
 In the next section, we further clarify the problem by defining evaluation criteria for interactive high-quality TC. A framework ICCOM (Intelligent Confirmation by Content Overlap Measurement) is then presented to achieve interactive high-quality TC (ref. Section 3 ). To measure the contributions of ICCOM, we conduct empirical evaluation using a public textual data collection (ref. Section 4 ). The results show that ICCOM may help various kinds of classifiers to achieve high TC performance in both recall and precision, with a con-trolled amount of cognitive load to users. 2. Evaluation criteria for interactive high-quality TC
The problem of interactive high-quality TC may be clarified by defining its evaluation criteria. Since the ideal case is to achieve high-quality TC with a limited amount of interactive confirmations, there should be two criteria: Confirmation Precision (CP) and Confirmation Recall (CR).
 CP is measured by (number of necessary confirmations conducted)/(number of confirmations conducted). Since a confirmation for a decision is necessary if and only if the decision is wrong, CP may also be defined by (number of wrong decisions identified)/(number of decisions identified as potentially wrong). On the other hand, CR is measured by (number of necessary confirmations conducted)/(number of confirmations that should be conducted). Similarly, it may also be defined by (number of wrong decisions identified)/(number of wrong decisions that should be identified).

It is interesting to note that, CR is related to the quality of TC, while CP is related to the cognitive load incurred to the user. To help the classifier to achieve perfect performance (both precision and recall of TC are 100%), CR should be 100%, indicating that all wrong decisions have been identified for the user to confirm (and hence be corrected by the user. 1 ) On the other hand, an extremely low CP may incur a very heavy cog-nitive load on the user (e.g., when all decisions call for confirmations, CP will approach 0 and hence the clas-sifier becomes nearly unusable). Therefore, interactive high-quality TC aims to achieve nearly 100% in CR, under the requirement that CP should be as high as possible.

Based on the criteria, there are two straightforward confirmation strategies to pursue interactive high-qual-ity TC: Uniform Confirmation (UC) and Probabilistic Confirmation (PC). For each category c , both strategies are based on threshold tuning documents (i.e., validation documents, Yang, 2001 ), which are either positive (belonging to c ) or negative (not belong to c ). As illustrated in Fig. 2 , UC sets a confirmation range. Once a document X  X  DOA value falls in the range, a confirmation is conducted. Since the goal is to achieve high CR,
UC sets a range that is large enough to cover the DOA values of those documents for which the classifier might make mistakes. The lower limit of the range may be set to the maximum DOA value below which no DOA values of positive documents lie (ref. the  X  X  X ejection threshold  X  ). Similarly, the upper limit may be set to the minimum DOA value beyond which no DOA values of negative documents lie (ref. the  X  X  X cceptance threshold  X  ). Obviously, UC may incur lower CP, and hence incur heavier cognitive load.

On the other hand, as illustrated in Fig. 3 , PC works for those classifiers that tune a threshold to optimize performance in some criterion, such as the popular F 1 measure, which integrates precision and recall by (2 precision recall/(precision + recall)). PC is based on the observation that those documents whose
DOA values are closer to the threshold tend to have a higher probability of leading the classifier to make erro-neous decisions. Obviously, PC hopes to promote CP, but may incur lower CR, and hence has difficulties in guaranteeing the classifier X  X  performance.

Therefore, both UC and PC are not good enough to achieve interactive high-quality TC, which aims to achieve very high CR, under the requirement that CP should be as high as possible. To develop a more effec-tive confirmation strategy, the main technical issue lies on, among the decisions made by the classifier, intel-ligently identifying those decisions that deserve confirmations. The confirmation strategy should also be applicable to various kinds of classification techniques.
 3. ICCOM: an intelligent confirmation strategy for high-quality TC We present an intelligent confirmation technique for interactive high-quality TC. The technique is named ICCOM (Intelligent Confirmation by Content Overlap Measurement). Fig. 4 illustrates introduction of ICCOM to various kinds of classification techniques. ICCOM works with the classifier for each category. In training, the classification technique follows its way to build the classifier and tune its threshold, and ICCOM employs the same set of threshold tuning documents to set its thresholds for identifying potential TC errors (those that deserve confirmations). In testing, once a document is entered, the classifier is invoked to make its decision (either acceptance or rejection), and based on the decision, ICCOM is invoked to deter-mine whether a confirmation is required. The integrated system may thus make three kinds of decisions: accep-tance (definitely relevant), rejection (definitely irrelevant), and confirmation (potentially relevant). There are two key issues in the design of ICCOM: (1) the basis on which ICCOM makes its decision and (2) the way ICCOM collaborates with the classifier, which are presented in the next subsections. 3.1. Content overlap measurement: the basis on which ICCOM makes decisions
ICCOM bases its decision on content overlap measurement (COM), which was shown to be helpful in improving performances of text classifiers ( Liu, 2007a ). We explore how COM may be a good basis on which potential TC errors are identified to achieve high-quality TC. The basic idea of COM is to measure the degree of content overlap (DCO) between a category c and a document d . If DCO between c and d is low, d should not be classified into c , even though d mentions some content of c .

Table 1 presents the algorithm for COM. Given a category c and a document d , the algorithm considers two kinds of terms: those terms that are positively correlated with c but do not appear in d (ref. Step 2), and those terms that are negatively correlated with c but appear in d (ref. Step 3). Both kinds of terms lead to the reduc-tion of DCO (ref. Step 2.1 and 3.1). Therefore, a smaller DCO indicates that d talks more information not in c , and vice versa. In that case, it is less proper to classify d into c .

The correlation strengths is estimated by v 2 (chi-square). For a term t and a category c , v 2 ( t , c )= ( N ( A D B C ) 2 )/(( A + B ) ( A + C ) ( B + D ) ( C + D )), where N is the total number of docu-ments, A is the number of documents that are in c and contain t , B is the number of documents that are not in c but contain t , C is the number of documents that are in c but do not contain t , and D is the number between t and c . We say that c and t are positively correlated if A D &gt; B C ; otherwise they are negatively correlated .

Note that the set of terms considered by COM is dynamic in the sense that the set is reconstructed with respect to each input document d (ref. Step 2 and Step 3). Moreover, a term t may even appear in d but not in any training document. It needs to be considered when measuring DCO between d and c (ref. Step 3). However, its v 2 value is incomputable (since both A and B are zero). ICCOM tackles the problem by treat-ing d as a training document, and hence incrementing N and B by 1. 3.2. Collaboration with the classifier
Based on COM, the next challenge is to design a way for ICCOM to collaborate with the underlying clas-sifier. As noted above, ICCOM is invoked after the classifier has made a decision for a document (acceptance or rejection). It determines whether the decision deserves confirmation. The goal is to achieve extremely high CR (and hence extremely high TC performance in both precision and recall).

For each category, ICCOM collaborates with the classifier by tuning three thresholds: rejection threshold (RT), positive confirmation threshold (PCT) and negative confirmation threshold (NCT). As illustrated in
Fig. 5 , the three thresholds work together to help the system to make decisions: rejection, acceptance, or con-firmation. RT is used to identify those documents whose DOA values are too low, and hence may be rejected without any confirmation (i.e. rejection). PCT is used to check whether a document is accepted by both the classifier (i.e., DOA value P the classifier X  X  threshold T ) and COM (i.e., DCO value P PCT). If so, the doc-ument is accepted without confirmation (i.e., acceptance); otherwise a confirmation is required (i.e., confirma-tion). Similarly, NCT is used to check whether a document is rejected by both the classifier (i.e., DOA value &lt; the classifier X  X  threshold T ) and COM (i.e., DCO value 6 NCT). If so, the document is rejected with-out confirmation (i.e., rejection); otherwise a confirmation is required (i.e., confirmation).
More specifically, Table 2 presents the procedure to identify the thresholds for each category c . Validation documents (ref. parameter V ) used by the classifier in tuning its threshold (ref. parameter T ) are used by ICCOM as well. The documents are either positive (belonging to c ) or negative (not belonging to c ). RT is set to the maximum DOA value below which no DOA values of positive documents lie (ref. Step 1). To set PCT and NCT, COM is invoked to compute DCO values of validation documents. To set PCT, only those documents whose DOA values are larger than or equal to the classifier X  X  threshold T are used (ref. Step 2). PCT is simply set to the minimum DCO value beyond which no DCO values of negative documents lie (ref. Step 4). Similarly, to set NCT, only those documents whose DOA values are between RT and T are used (ref. Step 5). NCT is simply set to the maximum DCO value below which no DCO values of positive documents lie (ref. Step 7).

With the help of the thresholds, ICCOM and the classifier may collaborate to make decisions for incoming documents. Table 3 summarizes the collaboration. Once a document d is entered, the classifier is invoked to compute its DOA value (ref. Step 1). RT is then consulted to check whether d should be rejected without con-firmation (ref. Step 2). If no, COM is invoked to compute DCO value of d . Based on the DOA value and the DCO value, PCT and NCT are consulted to check whether d should be accepted and rejected, respectively (ref. Step 3.2.1 and Step 3.3.1). If no, a confirmation is required (ref. Step 3.2.2 and Step 3.3.2).
It is interesting to analyze the contributions of ICCOM. Obviously, with ICCOM, the classifier X  X  TC per-formance is improved if the user provides correct confirmations. Moreover, the TC performance is expected to reach the ideal goal of high-quality TC: accepting almost all documents that should be accepted (i.e., high recall), and rejecting almost all documents that should be rejected (i.e., high precision). This is because ICCOM employs a  X  X  X onservative  X  confirmation strategy: the thresholds are tuned to make confirmation for any possible mistake (ref. Step 1, Step 4, and Step 7 in Table 2 ). Obviously, the conservative strategy may increase the number of confirmations required. ICCOM reduces the number by identifying the cases where no confirmation is required. These cases fall into two types: (1) those that the classifier rejects confidently (by RT), and (2) those that both the classifier and COM accept (by PCT) or reject (by NCT). Empirical evaluation is helpful to precisely measure the contributions of ICCOM.

It is also interesting to note that ICCOM collaborates with the underlying classifier without needing to change the classifier X  X  operation, making the collaboration easy to implement, and hence promoting portabil-ity of ICCOM to various classifiers. Another collaboration way is to mix the results from ICCOM (DCO) and the classifier (DOA), and then accordingly set thresholds for confirmation. However, its main difficulties lie on properly setting relative weights of DOA and DCO for the mixing. 4. Empirical evaluation
ICCOM is evaluated on real-world collections of data, and the results show that ICCOM may help various kinds of classifiers to achieve high-quality TC with a much smaller number of confirmations. 4.1. Experimental data and underlying classifiers
We follow Liu, 2007a to set experimental data (including different sources and splits of experimental data) and underlying classifiers (including different kinds and settings of classification methodologies). 4.1.1. Experimental data
Sources of experimental data include Reuters-21578 2 and a text hierarchy from Yahoo!. 3 There are 135 cat-egories (topics) in Reuters-21578. The ModLewis split is adopted, which separates data into a test set and a the in-space subset, which consists of 3022 test documents that belong to some of the categories (i.e., fall into the category space), and (2) the out-space subset, which consists of 3168 documents that do not belong to any of the categories.
 idation) subset. The former is used to build the classifiers, while the latter is used to tune a threshold for each category. Therefore, to guarantee that each category has at least one document for classifier building and one document for threshold tuning, we remove those categories that have fewer than two training documents, and hence 95 categories remain. After removing those documents to which no categories are assigned (i.e., not belonging to any of the 95 categories), the training set contains 7780 documents. To perform classifier building of training data is used for classifier building, and the remaining 50% (20%) is used for threshold tuning, and the process repeats 2 (5) times so that each training document is used for threshold tuning exactly one time.
On the other hand, documents from a text hierarchy of Yahoo! are used for testing only. There are 370 documents randomly sampled from the categories of science , computers and Internet , and society and culture of Yahoo!, and hence the documents are less related to the content of the Reuters categories. Experiments on the Yahoo out-space documents aim to measure the system X  X  performances in processing those out-space doc-uments that have different degrees of relatedness to the categories of training documents. 4.1.2. Underlying classifiers
ICCOM is applied to two classification methodologies: (1) vector-based methodology, and (2) probability-based methodology. For the former, we implement a Rocchio classifier with thresholding (RO), while for the latter, we implement two Naive Bayes classifiers: a Naive Bayes classifier with thresholding (NB) and a Naive Bayes classifier with a fixed threshold of 0.5 (NBFix05). All systems associate each category c with a classifier. Upon receiving a document d , the classifier estimates the similarity between d and c (i.e. DOA of d with respect to c ) in order to make a binary decision for d : accepting d or rejecting d .
 RO was commonly employed in text classification (e.g., Wu et al., 2002 ), filtering (e.g., Schapire et al., 1998; Singhal et al., 1997 ), and retrieval (e.g., Iwayama, 2000 ). It was shown to be promising in TC ( Liu &amp; Lin, 2005 ). RO constructs a vector for each category, and the similarity between a document d and a category c for a category c is constructed by considering both relevant documents and non-relevant documents of c : g 1 the documents in c ), while N is the set of vectors for non-relevant documents (i.e., the documents not in c ). In the experiment, g 1 = 16 and g 2 = 4, since previous studies (e.g., Wu et al., 2002 ) showed that such a setting is promising.

On the other hand, NB and NBFix05 are based on the Naive Bayes technique, which was frequently employed and evaluated with respect to various techniques, including text filtering ( Kim, Hahn, &amp; Zhang, were shown to be competitive (and even better) when compared with various state-of-the-art TC techniques (e.g., neural networks and support vector machine, Dhillon et al., 2002; Yang &amp; Lin, 1999 ). In particular, a standard Laplace smoothing to avoid the probabilities of zero). The  X  X  X imilarity  X  between a document d and a c ) uments irrelevant to c .

To make TC decisions, both RO and NB also require a thresholding strategy to set a threshold for each category. As in many previous studies ( Callan, 1998; Chai, Ng, &amp; Chieu, 2002; Lewis, Schapire, Callan, &amp; Papka, 1996; Schapire et al., 1998; Yang, 2001; Yang &amp; Lin, 1999; Zhang &amp; Callan, 2001 ), RO and NB tune a relative threshold for each category by analyzing document-category similarities. The threshold tuning doc-uments were used to tune each relative threshold. As suggested by many studies (e.g., Yang, 2001 ), the thresh-olds are tuned in the hope to optimize the system X  X  performance with respect to F 1 . On the other hand, NBFix05 predefines a fixed threshold of 0.5 for each category (i.e., no threshold tuning). It was tested in sev-eral previous studies (e.g., Chai et al., 2002 ) as well.
All the classifiers require a fixed (predefined) feature set, which is built using the documents for classifier building. Each term that is not a stop word may be a candidate feature. No phrases extraction routine is invoked. Features are selected according to their weights, which are estimated by the v 2 (chi-square) weighting technique. The technique was shown to be more promising than others ( Yang &amp; Pedersen, 1997 ). Moreover, there is no perfect way to determine the size of the feature set. Setting a proper feature set size was often an experimental issue in previous studies ( McCallum et al., 1998; Yang &amp; Pedersen, 1997 ). Therefore, we try five feature set sizes, including 1000, 5000, 10,000, 15,000, and 20,000, since there are about 20,000 different fea-tures in the 2-fold training data. 4.2. Baseline confirmation strategies
To measure the contribution of ICCOM with respect to other confirmation strategies, we implement the two baseline confirmation strategies presented in Section 2 : UC and PC. As noted above, interactive high-quality TC did not get much attention in previous studies, and the two strategies may represent common approaches to pursue interactive high-quality TC. They are applied to all the classifiers (i.e. RO + UC,
RO + PC, NB + UC, NB + PC, NBFix05 + UC, and NBFix05 + PC), and then compared with those corre-sponding classifiers associated with ICCOM (i.e., RO + ICCOM, NB + ICCOM, and NBFix05 + ICCOM). 4.3. Evaluation criteria
To measure the systems X  performances, we employ two groups of criteria: (1) criteria to evaluate the effec-tiveness of confirmation, and (2) criteria to evaluate the quality of TC. For the former, we employ the criteria defined in Section 2 : CP and CR. For the latter, we employ the popular criteria: precision (P), recall (R), and
F . P is equal to (total number of correct classifications/total number of classifications made), R is equal to (total number of correct classifications/total number of correct classifications that should be made), and F 1 is equal to 2PR/( P + R ). Moreover, to measure the systems X  performances on filtering out-space documents, we define misclassification ratio (MR), which is equal to (number of misclassifications for the out-space docu-ments/number of the out-space documents). A system should avoid misclassifying out-space documents into many categories (i.e., lower MR).

Since the experiment investigates the contributions of ICCOM to TC, we assume that each confirmation request issued by the system may get a correct answer (rejection or acceptance). By comparing performances of the classifier with and without ICCOM, we may precisely measure the contribution of ICCOM to the clas-sifier. Moreover, with the assumption, CR is directly related to the system X  X  performances in all aspects includ-ing MR and F 1 (and hence both P and R), since a higher CR means that more errors are identified and hence corrected (if necessary) by confirmations. Therefore, for the quality of TC, we report results on MR and F 1 , while for the effectiveness of confirmation, we report results on CP, which is related to the number of confir-mations requests issued. 4.4. Result and analysis
We report and discuss experimental results on (1) classification of in-space documents, and (2) filtering of out-space documents. 4.4.1. Identifying the best versions of the classifiers
As noted in Section 1.1 , confirmation strategies aim to further promote performances of those classifiers that have been tuned and improved to their best extents. Therefore, we first identify the best versions of the classifiers. Table 4 summarizes micro-averaged F 1 performances of RO, NB, NBFix05 under their best training conditions, including training data splits (2-fold and 5-fold validations) and feature set sizes (1000, 5000, 10,000, 15,000, and 20,000). RO, NB, and NBFix05 achieved their best performances in 5-fold, 2-fold, and 5-fold environments, respectively. RO and NBFix05 have similar best performances (0.7246 vs. 0.7496), while the best performance of NB is quite poor (0.3351).
 4.4.2. Performances on in-space documents
We thus analyze the performances of the systems that apply ICCOM, UC, and PC to the best versions of the classifiers. Fig. 6 illustrates their performances in processing in-space documents. 4 PC fails to help NBFix05 to achieve high-quality TC (F 1 = 0.7680 only), while ICCOM and UC successfully help both better classifiers (i.e., RO and HBFix05 whose F 1 performances are higher than 0.72, recall Table 4 ) and poor clas-sifier (i.e., NB whose F 1 performance is only 0.3351, recall Table 4 ) to achieve high-quality TC. They have similar contributions (in F 1 ) to RO, NB, and NBFix05 (RO: 0.9615 vs. 0.9661; NB: 0.9782 vs. 0.9918; NBFix05: 0.9575 vs. 0.9580), however, ICCOM simultaneously fulfills the requirement and maintains much better CP, which is 2.6 21.8 times better than that of UC (RO: 0.1582 vs. 0.0608; NB: 0.4232 vs. 0.1318; NBFix05: 0.1632 vs. 0.0075).

To analyze the contributions of ICCOM to individual categories, Fig. 7 shows F 1 performances of RO on each category before and after ICCOM is employed. The categories are shown in descending order of F 1 performances achieved by RO, and those categories whose F 1 is incomputable (i.e., P is incomputable, R is incomputable, or both P and R are 0) are ranked last. The results indicate that ICCOM may help RO to achieve high F 1 perfor-mances ( P 0.95) on many categories. Moreover, as shown in Fig. 8 , CP of RO + ICCOM oscillates among the categories, indicating that ICCOM tries to comprehensively promote performances on all categories.
As noted above, confirmation strategies aim to further promote performances of those classifiers that have been improved to their best extents. Therefore, we are also particularly interested in those categories for which RO has been well trained. Therefore, we are interested in the first 5 categories for which RO achieves F 1 P 0.85. For the first category, F 1 achieved by RO has been perfect (=1.0), and RO + ICCOM successfully avoids conducting any confirmation. For the other four categories, RO + ICCOM achieves an average F 1 of 0.9720 and an average CP of 0.2584. The results justify the contributions of ICCOM to well-tuned classifiers, which are believed to be more difficult to improve by other means.
 4.4.3. Performances on out-space documents Figs. 9 and 10 show the systems X  performances on Reuters and Yahoo out-space documents, respectively. As noted in Section 4.3 , since out-space documents should not be classified into any category, we focus on the extent to which the confirmation strategies help the classifiers to reduce MR. The results show that both UC and PC fail to help all the classifiers to achieve low MR. With UC, NBFix05 has poor performance on Yahoo out-space documents (MR = 1.9432. ref. Fig. 10 ). With PC, both NB and NBFix05 have poor performances on both Reuters and Yahoo out-space documents (MR ranges from 1.0865 to 11.5703).

Again, only ICCOM successfully helps all the classifiers to achieve low MR (ranging from 0.0108 to 0.1730) and maintains better CP, which is 1.3 20.2 times better than that of UC on Reuters out-space documents (RO: 0.1580 vs. 0.0630; NB: 0.4046 vs. 0.1087; NBFix05: 0.2781 vs. 0.0138) and Yahoo out-space documents (RO: 0.0099 vs. 0.0078; NB: 0.5652 vs. 0.2378; NBFix05: 0.4575 vs. 0.0328). 5. Conclusion
Before text classifiers may be launched to provide services, they are often tuned to their best extents. Unfor-tunately, even the well-tuned classifiers often have an unrealizable ideal: high-quality TC, which aims to accept almost all documents that should be accepted (i.e., very high recall), and reject almost all documents that should be rejected (i.e., very high precision).

One way to approach high-quality TC is to collect confirmations from users so that potential errors of the classifiers may be corrected. Its challenge lies on achieving high-quality TC without conducting many confir-mations. We thus develop an intelligent confirmation strategy ICCOM to tackle the challenge. Empirical results show that ICCOM may help various kinds of classifiers to achieve high-quality TC with a much smaller number of confirmations. It may even improve performances of well-tuned classifiers, which are more difficult to improve by other means.

Main contributions of ICCOM lie on the intelligent identification of potential TC errors, which is the key to process information more properly. Therefore, typical applications of ICCOM are information archiving and recommendation. ICCOM may help users to read what they should read and avoid missing much information, by simply checking a limited amount of information that is potentially relevant. Moreover, ICCOM is partic-ularly essential for those applications in which erroneous TC decisions may incur high cost and/or serious problems. Healthcare information support through the Internet is an interesting example of the applications. ICCOM is being applied to classifying symptom descriptions into diseases ( Liu, 2007b ). Subsequent health-care decision support activities may then be triggered based on the results of high-quality TC. Acknowledgement This research was supported by the National Science Council of the Republic of China under the Grants NSC 95-2221-E-320-002.
 References
