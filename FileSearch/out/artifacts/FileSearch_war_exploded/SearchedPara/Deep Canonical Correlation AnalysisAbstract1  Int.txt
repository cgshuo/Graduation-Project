 Galen Andrew galen@cs.washington.edu University of Washington Raman Arora arora@ttic.edu Toyota Technological Institute at Chicago Jeff Bilmes bilmes@ee.washington.edu University of Washington Karen Livescu klivescu@ttic.edu Toyota Technological Institute at Chicago Canonical correlation analysis (CCA) (Hotelling, 1936; Anderson, 1984) is a standard statistical technique for finding linear projections of two random vectors that are maximally correlated. Kernel canonical cor-relation analysis (KCCA) (Akaho, 2001; Melzer et al., 2001; Bach &amp; Jordan, 2002; Hardoon et al., 2004) is an extension of CCA in which maximally correlated nonlinear projections, restricted to reproducing kernel Hilbert spaces with corresponding kernels, are found. Both CCA and KCCA are techniques for learning rep-resentations of two data views, such that each view X  X  representation is simultaneously the most predictive of, and the most predictable by, the other.
 CCA and KCCA have been used for unsupervised data analysis when multiple views are available (Hardoon et al., 2007; Vinokourov et al., 2003; Dhillon et al., 2011); learning features for multiple modalities that are then fused for prediction (Sargin et al., 2007); learn-ing features for a single view when another view is available for representation learning but not at pre-diction time (Blaschko &amp; Lampert, 2008; Chaudhuri et al., 2009; Arora &amp; Livescu, 2012); and reducing sam-ple complexity of prediction problems using unlabeled data (Kakade &amp; Foster, 2007). The applications range broadly across a number of fields, including medicine, meteorology (Anderson, 1984), chemometrics (Mon-tanarella et al., 1995), biology and neurology (Vert &amp; Kanehisa, 2002; Hardoon et al., 2007), natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011), speech processing (Choukri &amp; Chollet, 1986; Rudzicz, 2010; Arora &amp; Livescu, 2013), computer vision (Kim et al., 2007), and multimodal signal processing (Sargin et al., 2007; Slaney &amp; Covell, 2000). An appealing property of CCA for prediction tasks is that, if there is noise in either view that is uncorrelated with the other view, the learned represen-tations should not contain the noise in the uncorrelated dimensions.
 While kernel CCA allows learning of nonlinear repre-sentations, it has the drawback that the representation is limited by the fixed kernel. Also, as it is a nonpara-metric method, the time required to train KCCA or compute the representations of new datapoints scales poorly with the size of the training set. In this paper, we consider learning flexible nonlinear representations via deep networks. Deep networks do not suffer from the aforementioned drawbacks of nonparametric mod-els, and given the empirical success of deep models on a wide variety of tasks, we may expect to be able to learn more highly correlated representations. Deep net-works have been used widely to learn representations, for example using deep Boltzmann machines (Salakhut-dinov &amp; Hinton, 2009), deep autoencoders (Hinton &amp; Salakhutdinov, 2006), and deep nonlinear feedforward networks (Hinton et al., 2006). These have been very successful for learning representations of a single data view. In this work we introduce deep CCA (DCCA), which simultaneously learns two deep nonlinear map-pings of two views that are maximally correlated. This can be loosely thought of as learning a kernel for KCCA, but the mapping function is not restricted to live in a reproducing kernel Hilbert space.
 The most closely related work is that of Ngiam et al. on multimodal autoencoders (Ngiam et al., 2011) and of Srivastava and Salakhutdinov on multimodal restricted Boltzmann machines (Srivastava &amp; Salakhut-dinov, 2012). In these approaches, there is a single network being learned with one or more layers con-nected to both views (modalities); in the absence of one of the views, it can be predicted from the other view using the learned network. The key difference is that in our approach we learn two separate deep encodings, with the objective that the learned encodings are as correlated as possible. These different objectives may have advantages in different settings. In the current work, we are interested specifically in the correlation objective, that is in extending CCA with learned non-linear mappings. Our approach is therefore directly applicable in all of the settings where CCA and KCCA are used, and we compare its ability relative to CCA and KCCA to generalize the correlation objective to new data, showing that DCCA achieves much better results.
 In the following sections, we review CCA and KCCA, introduce deep CCA, and describe experiments on two data sets comparing the three methods. In principle we could evaluate the learned representations on any task in which CCA or KCCA have been used. However, in this paper we focus on the most direct measure of performance, namely correlation between the learned representations on unseen test data. Let ( X 1 ,X 2 )  X  R n 1  X  R n 2 denote random vectors with covariances ( X  11 ,  X  22 ) and cross-covariance  X  12 CCA finds pairs of linear projections of the two views, ( w Since the objective is invariant to scaling of w 1 and w 2 the projections are constrained to have unit variance: sequent projections are also constrained to be un-w
 X  22 w j 2 = 0 for i &lt; j . Assembling the top k projection vectors w i 1 into the columns of a matrix A we obtain the following formulation to identify the top k  X  min( n 1 ,n 2 ) projections: There are several ways to express the solution to this objective; we follow the one in (Mardia et al., 1979). the matrices of the first k left-and right-singular vectors of T . Then the optimal objective value is the sum of the top k singular values of T (the Ky Fan k -norm of T ) and the optimum is attained at ( A tion assumes that the covariance matrices  X  11 and  X  22 are nonsingular, which is satisfied in practice because they are estimated from data with regularization: given centered data matrices  X  H 1  X  R n 1  X  m ,  X  H 2  X  R n 2 can estimate, e.g. where r 1 &gt; 0 is a regularization parameter. Estimating the covariance matrices with regularization also reduces the detection of spurious correlations in the training data, a.k.a.  X  X verfitting X  (De Bie &amp; De Moor, 2003). 2.1. Kernel CCA Kernel CCA finds pairs of nonlinear projections of the two views (Hardoon et al., 2004). The Reproducing Kernel Hilbert Spaces (RKHS) of functions on R n 1 , R n 2 are denoted H 1 , H 2 and the associated positive definite kernels are denoted  X  1 , X  2 . The optimal projections are those functions f  X  1  X  H 1 ,f  X  2  X  H 2 that maximize the correlation between f  X  1 ( X 1 ) and f  X  2 ( X 2 ): ( f  X  1 ,f  X  2 ) = argmax To solve the nonlinear KCCA problem, the  X  X ernel trick X  is used: Since the nonlinear maps f 1  X  H 1 f 2  X  X  2 are in RKHS, the solutions can be expressed as linear combinations of the kernels evaluated at the whose i th element is  X  1 ( x,x i ) (resp. for f 2 ( x )). KCCA can then be written as finding vectors  X  1 , X  2  X  R m that solve the optimization problem K 1 = K  X  K 1  X  1 K + 1 K 1 , K ij =  X  1 ( x i ,x j ) and Subsequent vectors (  X  j 1 , X  j 2 ) are solutions of (7) with with the previous ones.
 Proper regularization may be critical to the perfor-mance of KCCA, since the spaces H 1 , H 2 could have high complexity. Since  X  0 1 f 1 (  X  ) plays the role of w KCCA, the generalization of w 0 1 w 1 would be  X  0 1 K 1  X  . Therefore the correct generalization of (5) is to use K 1 + r 1 K 1 in place of K 2 1 in the constraints of (7) , for regularization parameter r 1 &gt; 0 (resp. for K 2 2 ). The optimization is in principle simple: The objective is maximized by the top eigenvectors of the matrix The regularization coefficients r 1 and r 2 , as well as any parameters of the kernel in KCCA, can be tuned using held-out data. Often a further regularization is done by first projecting the data onto an intermediate-dimensionality space, between the target and original dimensionality (Ek et al., 2008; Arora &amp; Livescu, 2012). In practice solving KCCA may not be straightforward, as the kernel matrices become very large for real-world data sets of interest, and iterative SVD algorithms for the initial dimensionality reduction can be used (Arora &amp; Livescu, 2012). 2.2. Deep learning  X  X eep X  networks, having more than two layers, are capable of representing nonlinear functions involving multiply nested high-level abstractions of the kind that may be necessary to accurately model complex real-world data. There has been a resurgence of interest in such models following the advent of various successful unsupervised methods for initializing the parameters ( X  X retraining X ) in such a way that a useful solution can be found (Hinton et al., 2006; Hinton &amp; Salakhutdinov, 2006). Contrastive divergence (Bengio &amp; Delalleau, 2009) has had great success as a pretraining technique, as have many variants of autoencoder networks, includ-ing the denoising autoencoder (Vincent et al., 2008) used in the present work. The growing availability of both data and compute resources also contributes to the resurgence, because empirically the performance of deep networks seems to scale very well with data size and complexity.
 While deep networks are more commonly used for learn-ing classification labels or mapping to another vector space with supervision, here we use them to learn non-linear transformations of two datasets to a space in which the data is highly correlated, just as KCCA does. The same properties that may account for deep net-works X  success in other tasks X  X igh model complexity, the ability to concisely represent a hierarchy of features for modeling real-world data distributions X  X ould be particularly useful in a setting where the output space is significantly more complex than a single label. Deep CCA computes representations of the two views by passing them through multiple stacked layers of nonlinear transformation (see Figure 1). Assume for simplicity that each intermediate layer in the network for the first view has c 1 units, and the final (output) layer has o units. Let x 1  X  R n 1 be an instance of the first view. The outputs of the first layer for the R biases, and s : R 7 X  R is a nonlinear function applied componentwise. The outputs h 1 may then be used to compute the outputs of the next layer as h 2 = s ( W 1 2 h b )  X  R c 1 , and so on until the final representation f network with d layers. Given an instance x 2 of the second view, the representation f 2 ( x 2 ) is computed the potentially different architectural parameters c 2 and d ). The goal is to jointly learn parameters for both views W l and b v l such that corr ( f 1 ( X 1 ) ,f 2 ( X 2 )) is as high as possible. If  X  1 is the vector of all parameters W 1 l and b 1 l of the first view for l = 1 ,...,d , and similarly for  X  2 , then (  X   X  1 , X   X  2 ) = argmax objective as estimated on the training data. Let H 1  X  R top-level representations produced by the deep models on the two views, for a training set of size m . Let  X  H H 1  X  1 m H 1 1 be the centered data matrix (resp. define  X   X  12 = 1 for regularization constant r 1 (resp.  X   X  22 ). Assume that r &gt; 0 so that  X   X  11 is positive definite.
 As discussed in section 2 for CCA, the total correlation of the top k components of H 1 and H 2 is the sum of the top k singular values of the matrix T =  X   X   X  1 / 2 11  X   X  If we take k = o , then this is exactly the matrix trace norm of T , or 1 The parameters W v l and b v l of DCCA are trained to optimize this quantity using gradient-based optimiza-tion. To compute the gradient of corr ( H 1 ,H 2 ) with respect to all parameters W v l and b v l , we can compute its gradient with respect to H 1 and H 2 and then use backpropagation. If the singular value decomposition of T is T = UDV 0 , then  X  corr( H 1 ,H 2 ) where and and  X  corr ( H 1 ,H 2 ) / X  X  2 has a symmetric expression. The derivation of the gradient is not entirely straight-forward (involving, for example, the gradient of the trace of the matrix square-root, which we could not find in standard references such as (Petersen &amp; Pedersen, 2012)) and is given in the appendix. We also regularize (10) by adding to it a quadratic penalty with weight  X  b &gt; 0 for all parameters.
 Because the correlation objective is a function of the entire training set that does not decompose into a sum over data points, it is not clear how to use a stochastic optimization procedure that operates on data points one at a time. We experimented with a stochastic method based on mini-batches, but obtained much better results with full-batch optimization using the L-BFGS second-order optimization method (Nocedal &amp; Wright, 2006) which has been found to be useful for deep learning in other contexts (Le et al., 2011). As discussed in section 2.2 for deep models in general, the best results will in general not be obtained if param-eter optimization is started from random initialization X  some form of pretraining is necessary. In our experi-ments, we initialize the parameters of each layer with a denoising autoencoder (Vincent et al., 2008). Given centered input training data assembled into a matrix data  X  X = W 0 s ( W  X  X + b  X  1 0 ) is formed. Then we use L-BFGS to find a local minimum of the total squared error from the reconstruction to the original data, plus a quadratic penalty: where || X || F is the matrix Frobenius norm. The min-imizing values W  X  and b  X  are used to initialize opti-mization of the DCCA objective, and to produce the  X  a are treated as hyperparameters, and optimized on a development set, as described in section 4.1. 3.1. Non-saturating nonlinearity Any form of sigmoid nonlinearity could be used to deter-mine the output of the nodes in a DCCA network, but in our experiments we obtained the best results using a novel non-saturating sigmoid function based on the cube root. If g : R 7 X  R is the function g ( y ) = y 3 / 3 + y , then our function is s ( x ) = g  X  1 ( x ). Like the more pop-ular logistic (  X  ) and tanh nonlinearities, s has sigmoid shape and has unit slope at x = 0. Like tanh , it is an odd function. However, logistic and tanh approach their asymptotic value very quickly, at which point the derivative drops to essentially zero (i.e., they sat-urate). On the other hand, s is not bounded, and its derivative falls off much more gradually with x . We hy-pothesize that these properties make s better-suited for batch optimization with second-order methods which might otherwise get stuck on a plateau early during optimization. In figure 2 we plot s alongside tanh for comparison.
 Another property that our nonsaturating sigmoid func-tion shares with logistic and tanh is that its deriva-tive is a simple function of its value. For example,  X  ( x ) =  X  ( x )(1  X   X  ( x )), and tanh 0 ( x ) = 1  X  tanh This property is convenient in implementations, be-cause it means the input to a unit can be overwritten by its output. Also, as it turns out, it is more efficient to compute the derivatives as a function of the value in all of these cases (e.g., given y = tanh ( x ), 1  X  y 2 can be computed more efficiently than 1  X  tanh 2 ( x )). In shown with implicit differentiation. If y = s ( x ), then x = y 3 / 3 + y, To compute s ( x ), we use Newton X  X  method. To solve for g ( y )  X  x = 0, iterate For positive x , initializing y 0 = x , the iteration de-creases monotonically, so convergence is guaranteed. In the range of values in our experiments, it converges to machine precision in just a few iterations. When x is negative, we use the property that s is odd, so s ( x ) =  X  s (  X  x ). As a further optimization, we wrote a vectorized implementation. We perform experiments on two datasets to demon-strate that DCCA learns transformations that are not only dramatically more correlated than a linear CCA baseline, but also significantly more correlated than well-tuned KCCA representations. We refer to a DCCA model with an output size of o and d layers (including the output) as DCCA-o -d .
 Because the total correlation of two transformed views grows with dimensionality, it is important to compare only equal-dimensionality representations. In addition, in order to compare the test correlation of the top k components of two representations of dimensionality o ,o 2  X  k , the components must be ordered by their correlation on the training data. In the case of CCA and KCCA, the dimensions are always ordered in this way; but in DCCA, there is no ordering to the output nodes. Therefore, we derive such an ordering by per-forming a final (linear) CCA on the output layers of the two views on the training data. This final CCA produces two projection matrices A 1 ,A 2 , which are applied to the DCCA test output before computing test set correlation. Another way would be to compute a new DCCA representation at each target dimension-ality; this is not done here for expediency but should, if anything, improve performance. 4.1. Hyperparameter optimization Each of the DCCA models we tested has a fixed num-and b v l are trained as discussed in section 3. Several other values are treated as hyperparameters. Specifi-cally, for each view, we have  X  2 a and  X  a for autoencoder pretraining, c , the width of all hidden layers (a large integer parameter treated as a real value) and r , the CCA regularization hyperparameter. Finally there is a single hyperparameter  X  b , the fine-tuning regulariza-tion weight. These values were chosen to optimize total correlation on a development set using a derivative-free optimization method. 4.2. MNIST handwritten digits For our first experiments, we learn correlated repre-sentations of the left and right halves of handwritten digit images. We use the MNIST handwritten image dataset (LeCun &amp; Cortes, 1998), which consists of 60,000 train images and 10,000 test images. We ran-domly selected 10% (6,000) images from the training set to use for hyperparameter tuning. Each image is a 28x28 matrix of pixels, each representing one of 256 grayscale values. The left and right 14 columns are sepa-rated to form the two views, making 392 features in each view. For KCCA, we use a radial basis function (RBF) kernel for both views: k 1 ( x i ,x j ) = e  X  X  x i  X  x j k similarly for k 2 . The bandwidth parameters  X  1 , X  2 are tuned over the range [0 . 25 , 64]. Regularization parame-ters r 1 ,r 2 for CCA and KCCA are tuned over the range [10  X  8 , 10]. The four parameters were jointly tuned to maximize correlation at k = 50 on the development set. We use a scalable KCCA algorithm based on incremen-tal SVD (Arora &amp; Livescu, 2012). The selected widths of the hidden layers for the DCCA-50-2 model were 2038 (left half-images) and 1608 (right half-images). Table 1 compares the total correlation on the develop-ment and test sets obtained for the 50 most correlated dimensions with linear CCA, KCCA, and DCCA.
 4.3. Articulatory speech data The second set of experiments uses speech data from the Wisconsin X-ray Microbeam Database (XRMB) (West-bury, 1994) of simultaneous acoustic and articulatory recordings. The articulatory data consist of hori-zontal and vertical displacements of eight pellets on the speaker X  X  lips, tongue, and jaws, yielding a 16-dimensional vector at each time point. The baseline acoustic features consist of standard 13-dimensional mel-frequency cepstral coefficients (MFCCs) (Davis &amp; Mermelstein, 1980) and their first and second deriva-tives computed every 10ms over a 25ms window. The articulatory measurements are downsampled to match the MFCC frame rate.
 The input features X 1 and X 2 to CCA/KCCA/DCCA are the acoustic and articulatory features concatenated over a 7-frame window around each frame, giving acoustic vectors X 1  X  R 273 and articulatory vectors X the articulatory data (e.g., due to mistracked pellets), resulting in m  X  50 , 000 frames for each speaker. For KCCA, besides an RBF kernel (described in the previ-ous section) we also use a polynomial kernel of degree d , with k 1 ( x i ,x j ) = x T i x j + c d and similarly for k We run five independent experiments, each using 60% of the utterances for learning projections, 20% for tun-ing hyperparameters (regularization parameters and kernel bandwidths), and 20% for final testing. For this set of experiments, kernel bandwidths for the RBF kernel were fixed at  X  1 = 4  X  10 6 , X  2 = 2  X  10 4 to match the variance in the un-normalized data. For the polynomial kernel we tuned the degree d over the set { 2 , 3 } and the offset parameter c over the range [0 . 25 , 2] to optimize development set correlation at k = 110. Hyperparameter optimization selected the number of hidden units per layer in the DCCA-50-2 model as 1641 and 1769 for the MFCC and XRMB views respec-tively. In the DCCA-112-3 model, 1811 and 1280 units per layer, respectively, were chosen. The widths for the DCCA-112-8 model were fixed at 781 and 552 as discussed in the last paragraph of this section. Table 2 compares total correlation captured in the top 50 dimensions on the test data for all five folds with CCA, KCCA with both kernels, and DCCA-50-2. The pattern of performance across the folds is similar for all four models, and DCCA consistently finds more correlation.
 Figure 3 shows correlation obtained using linear CCA, KCCA with RBF kernel, KCCA with a polynomial kernel ( d = 2 ,c = 1), and various topologies of Deep CCA, on the test set of one of the folds as a function of number of dimensions. The KCCA models tend to detect slightly more correlation in the first few compo-nents, after which the Deep CCA models outperform them by a large margin. We note that DCCA may particularly have an advantage when k is equal to the number of output units o . We found that DCCA mod-els with only two or three output units can indeed find more correlation than the top two or three components of KCCA (results not shown). This is also consistent with the observation that DCCA-50-2 has the highest performance of any model at k = 50. To determine the impact of model depth (number of layers) on performance, we conducted an experiment in which we increased the number of layers from three to eight, while reducing the number of hidden units in each layer in order to keep the total number of parameters approximately constant. The output width was fixed at 112, and all hyperparameters other than the number of hidden units were kept fixed at the values chosen for DCCA-112-3. Table 3 gives the total correlation on the first fold as a function of the number of layers. Note that the total correlation of both datasets increases monotonically with the depth of DCCA and even with eight layers we have not reached saturation. layers (d) 3 4 5 6 7 8 Dev set 66.7 68.1 70.1 72.5 76.0 79.1 Test set 80.4 81.9 84.0 86.1 88.5 88.6 We have shown that deep CCA can obtain improved representations with respect to the correlation objective measured on unseen data. DCCA provides a flexible nonlinear alternative to KCCA. Another appealing fea-ture of DCCA is that, like CCA, it does not require an inner product. As a parametric model, representa-tions of unseen datapoints can be computed without reference to the training set.
 In many applications of CCA, such as classification and regression, maximizing the correlation is not the final goal and the correlated representations are used in the service of another task. A natural next step is therefore to test the representations produced by deep CCA in the context of prediction tasks and to compare against other nonlinear multi-view representation learning ap-proaches that optimize other objectives, e.g., (Ngiam et al., 2011; Srivastava &amp; Salakhutdinov, 2012). This research was supported by NSF grant IIS-0905633 and by the Intel/UW ISTC. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funders. To perform backpropagation, we must be able to com-pute the gradient of f = corr ( H 1 ,H 2 ) defined in Equa-tion (10) . Denote by  X  ij the matrix of partial deriva-tives of f with respect to the entries of  X   X  ij . Let the singular value decomposition of T =  X   X   X  1 / 2 11  X   X  12 be given as T = UDV 0 . First we will show that and (resp.  X  22 ). To prove (15) , we use the fact that for a matrix X ,  X  X | X || tr = UV 0 , where X = UDV 0 is the singular value decomposition of X (Bach, 2008). Using the chain rule: is easily derived from Theorem 1 of (Lewis, 1996): Theorem 1. A matrix function f is called a spectral function if it depends only on the set of eigenvalues of its argument. That is, for any positive definite matrix X and any unitary matrix V , f ( X ) = f ( V XV 0 ) . If f is a spectral function, and X is positive definite with eigendecomposition X = UDU 0 , then Now we can proceed from (Petersen &amp; Pedersen, 2012) for the derivative of an inverse,  X  ( T  X  (  X   X  So continuing from (18), (  X  Using  X  12 and  X  11 , we are ready to compute  X  X / X  X  1 . First (temporarily moving subscripts on H 1 and  X   X  11 to superscripts so subscripts can index into matrices) Also, Putting this together, we obtain  X  X   X  X  Using the fact that  X  11 is symmetric, this can be written more compactly as
