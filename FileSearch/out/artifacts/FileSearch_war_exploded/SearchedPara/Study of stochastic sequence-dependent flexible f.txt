 1. Introduction
Scheduling in manufacturing syst ems is typically associated with allocating a set of jobs on a set of machines in order to achieve some objectives. It is an important decision making process in the operation level. Manufacturing environments in the real world are subject to many sources of change which ar e typically treated as random occurrences, such as new job rele ases, machine breakdowns, etc.
In this paper, we will focus on Flexible Flow shop Scheduling system with Sequence Dependent Setup Times (SDST/FFS). In the considered problem, jobs arrive dynamically over the scheduling period and the objective is to find a schedule that minimizes average this problem.

Dispatching rules are very common techniques to deal with dynamic scheduling problems and especially flow shops. Hunsucker and Shah (1992) examined the performance of six different dispatch-ing rules to minimize two tardiness-based criteria in a dynamic flow shop. Lodree et al. (2004) suggested a new rule to minimize the number of tardy jobs, while Branke and Mattfeld (2005) demon-strated that avoiding early idle times helps in mi nimizing total tardiness in a dynamic flow shop. More recently, Swaminathan et al. (2007) examined minimization of total weighted tardiness in a dynamic flow shop where new jobs arrive at every shift change, while Alfieri (2007) studied the interaction between a number of dispatching rules and due-date quoting policies in a simple dynamic flow shop. Rajendran and Alicke (2007) developed dispatching rules to take into account the presence of bottleneck machines in a flow shop system. Another work that deals specifically with the mean flow time criterion in a dynamic flow shop is that of Rajendran and
Holthaus (1999) . They compared 13 existing and proposed dispatch-ing rules in a 10-machine flow shop and also concluded that the SPT dispatching rule gives the lowest mean flow times for all the machine utilization levels examined. Kianfar et al. (2009) proposed four dispatching rules to minimize the sum of tardiness and rejection costs and through a statistical simulation model showed the perfor-mance of their new methods.

Jayamohan and Ra jendran (2000) provide a set of new dispatching rules to minimize various performance measures such as mean, maximum and variance of flow time and tardiness in dynamic shops.
A static rule which minimizes the number of tardy jobs is also proposed. To evaluate these proposed rules, their relative perfor-mance is analyzed in open job shops and reported in comparison with the standard benchmark rules such as the SPT and EDD, popular rules like ATC and MOD, and the best performing rules in literature such as RR, PT  X  WINQ, PT  X  WINQ  X  SL and AT  X  RPT. When job
Hunsucker and Shah (1992 , 1994) . Their studies were limited by the selection of dispatching rules such as FIFO, SPT and LPT, and efficient rulessuchasMOD( Baker and Kanet, 1983 )andATC( Vepsalainen and
Morton, 1987 ) were not included. Moreover, a comprehensive set of measures involving mean and variance of flow time, and mean and variance of tardiness of jobs was not considered. In comparison to these studies, the study by Rajendran and Holthaus (1999) appears more complete with respect to many dispatching rules such as COVERT ( Russell et al.,1987 ), RR ( Raghu and Rajendran, 1993 )and
PT  X  WINQ ( Holthaus and Rajendran, 1997 ).Fromarelativeevalua-tion of many dispatching rules from the literature survey, it appears that there is a scope to develop efficient dispatching rules that could address a number of performance measures such as the minimization of mean, maximum and variance of flow time and tardiness of jobs, and also minimizing the percentage of tardy jobs.
 the area of dynamic scheduling. Hmida et al. (2006) presented a heuristic for hybrid flow shop to minimize makespan. They present a climbing discrepancy search that is an adaptation of the depth-bounded discrepancy search method to obtain near-optimal solu-tions. Fattahi and Fallahi (2010) consider dynamic scheduling in flexible job shop systems by considering simultaneously efficiency and stability. They develop a meta-heuristic algorithm based on GA and state that the proposed algorithm is capable to achieve the optimal solutions for the small size problems and near optimal solutions for the medium size problems.
 setup times are really scarce. Kurz and Askin (2003) study dispatch-ing rules for the SDST/FFS. They investigate three classes of heuristics based on simple greedy methods, insertion heuristics and adaptations of Johnson 0 srule.Later, Kurz and Askin (2004) formulate the SDST/HFFS as an Integer Programming (IP) model.
Because of the difficulty in solving the IP model directly, they develop a random keys genetic algorithm. Zandieh et al. (2006) propose an immune algorithm, and compare it against the random keys genetic algorithm of Kurz and Askin (2004) . Naderi et al. (2009) study a hybrid flexible flow shop with respect to sequence depen-dent setups. They propose a dynamic dispatching rule and an iterated local search algorithm. The new algorithms are evaluated by comparison against 7 other high performing algorithms from the literature. Statistical experiments show that the proposed algo-rithms are very competitive for the studied problem. Kia et al. (2010) suppose the same problem and present a discrete-event simulation model as well as eleven heuristic algorithms, including 9 dispatching rules and two construc tive heuristics. It is notable that all the studies in this paragraph except the paper by Kia et al. (2010) consider the related problems in deterministic conditions in which the arrival times of jobs are known in advance; however, we have defined the proposed problem in stochastic condition in which the jobs information is unknown before they are received.
 flexible flow shop problem with setup times to minimize a convex combination of makespan and the number of tardy jobs. In this study, several basic dispatching rules and constructive heuristics are generalized to the considered problem and a genetic algo-rithm is developed as well. In 2003, Reddy and Narendran (2003) studied a five machine permutation flow shop problem with dynamically arriving jobs belonging to different families in a stochastic environment where both the process times and the inter-arrival times are assumed to be exponential random vari-ables. Sequence dependent setup times occur between the jobs of distinct families. Reddy and Narendran compared the quality of 9 heuristics with regard to the objectives of minimizing the average job flow time, the average job tardiness and the percentage of tardy jobs.

Vinod and Sridharan (2009) conduct a simulation study that investigates several performance measures in the dynamic job shop scheduling with sequence dependent setup times. They also proposed five setup-oriented scheduling rules that one of them performs better for the mean flow and mean tardiness measures.
The rest of the paper is organized as follows. Section 2 deals with the problem definition. The Neighborhood Search-based
Dispatching Rule (NSDR) for the flexible flow shop problem under consideration is introduced in Section 3 .In Section 4 , a hybrid genetic algorithm for the problem is proposed; and the important aspects of the developed simulation model are described in Section 5 .
Section 6 presents the details of the experimentation and the results and analysis of different scenarios. Finally, Section 7 is devoted to conclusions and recommendations for future studies. 2. Problem definition
This paper considers the problem of scheduling jobs in a flexible flow shop environment. A flexible flow shop is a generalization of the classical flow shop model. There are M stages and some stages may have only one machine, but at least one stage must have multiple machines. The jobs have to visit all of the stages in the same order string from stage one through stage M . A machine can process at most one job at a time and a job can be processed by at most one machine at a time. Preemption of such a processing is not allowed and each stage has a set of identical parallel machines. The problem consists of assigning jobs to machines at each stage and sequencing the jobs assigned to the same machine so that some optionally criteria are minimized.

If all jobs are known before processing starts a scheduling problem is said to be static, while, to classify a problem as dynamic it is sufficient that job release times are not fixed at a single point in time, i.e. jobs arrive to the system at different times. Scheduling problems can also be classified as either deterministic, when processing times and all other parameters are known and fixed, or as non-deterministic, when some or all parameters are uncer-tain. The dynamic shop environment considered here is one where jobs arrive continually over time, without advance knowledge of their process time requirements. Hence, the products 0 combination is in continues state of change as new jobs arrive and completed jobs leave the shop.
 Another assumption made for this problem is about setup times.
Setup times considered in this problem are sequence-dependent which means the setup time between two different jobs on the same machine might have different values depending on both the job just completed and the current job to be processed.
The objective of this paper is to determine the job schedules that average tardiness of jobs is minimized.

The following are other problem characteristics and assump-tions in this paper. 1. All jobs require processing on each stage. 2. Job preemption is not allowed. 3. There are no precedence constraints between jobs. 4. There are unlimited buffers between stages. 5. There is no interruption in the shop floor, e.g. no machine breakdowns.

Here, the necessary notations are provided for the scheduling problem discussed in this study. We adapt the three-field notations a / b / g of Graham et al. (1979) to describe the scheduling problem.
The a field describes the shop environment. The b field describes the setup information, other shop conditions, and details of the processing characteristics, which may contain multiple entries. Finally, the g field contains the objective to be optimized. According to this system of notation, the problem is shown as type of flexible flow shop. Symbols in the second field denote that all of the jobs are not available at the beginning of scheduling period and setup times are sequence dependent. The last field of the notation describes that the objective of the problem is to minimize average tardiness of jobs. 3. Proposed dispatching rule based on neighborhood search
This section proposes a Neighborhood Search-based Dispatch-ing Rule (NSDR) for the considered problem. The main idea of this rule is to develop a scheduling method in which dispatching selection at any stage is taken collectively after consultation with the other stages. This consultation is initiated every time a machine loading decision is needed. According to this, NSCDR is classified as a state dependent dispatching rule.

The proposed method has two phases where the first phase is developed based on Cooperative Dispatching (CD) rule, intro-duced by El-Bouri et al. (2008) . CD was designed to minimize mean flow time in a dynamic flow shop and, as they asserted, the rule gives improvements ranging between 1.5% and 11.4% over other dispatching rules considered in that study. NSDR uses the main idea of CD rule and customizes it to the flexible flow shop environment, setup times and tardiness performance measure. 3.1. First phase: Construction procedure
At first, the notation used in the proposed dispatching rule (NSDR) is defined. The symbol prime ( 0 ) is related to the current decision condition such as the stage in which the dispatching rule is applied. Also, the symbols [ ] and 99 are, respectively, used to show a set in ordered form and number of elements in a set. According to this, we introduce the terminology used here. i index for stages m index for machines j index for jobs I total number of stages in the flexible flow shop p j , i process time for job j on stage i r j release time for job j d j due date for job j y i set of dispatching candidate jobs ready for processing on f i set of jobs being processed on stage i j 0 index for job which heads set y (lead job) i 0 stage where the current dispatching decision is pending R i ready time for stage i y hi C j , i y completion time for job j on stage i according to the S j , i setup time between job j and the last job on the related ji earliest possible completion time for job j on stage i T j , i tardiness for job j on stage i subject to job j 0 be the T i tardiness for the lead job j 0 on stage i f j 0 , i total local tardiness of jobs on stage i , in the event job j 0 z j 0 , i tardiness cost to stage i, resulting from selecting job j 0 at u i minimum tardiness cost for stage i i penalty cost for stages i 0 through I , resulting from the w i weighting factor for stage i
NSDR is designed such a way that in each step one job is selected from a set y i 0 of queued candidates to process next on current stage. The downstream of the current stage is consulted in a pooling process that seeks from each stage a priority ranking of the candidate jobs. A consulted stage endeavors to receive the jobs contained in y i 0 according to a sequence that allows it to minimize its local total tardiness. The lead job in a stage desired sequence for y i 0 is the job it favors to be loaded next at the current stage, s 0 .

At the first step of NSDR, total tardiness matrix (matrix Z ) should be constructed from the local tardiness costs. The rows of the matrix Z represent, in order, the jobs included in y i 0 columns are related to stages i 0 through I .
 Element z j , i represents the local tardiness cost at stage i Hence, matrix Z can be shown as follows:
Z  X  2 6 6 6 6 6 6 6 4
According to the above matrix, the minimum tardiness of stages and the final cost of scheduling jobs, headed by job i , are obtained using Eqs. (1) and (2) given below. u  X  min  X 
According to the Eq. (3), a job j * , dispatched at stage i 0 , is the one that minimizes the total penalty of jobs X  tardiness from the current stage to the last one. j n  X  j 9 min f r j g X  3  X  3.1.1. Calculating local tardiness costs, z i,m
Each z j , i is obtained by solving a parallel machine scheduling problem to find an optimal order of the current jobs at set y stage i 0 , subject to job j takes the first position in that sequence. a weighting factor which indicates stage 0 s importance in schedul-ing the flexible flow shop. Thus, the next step of the algorithm is to determine the total tardiness values, f j , i , and the importance factor of stages, w i .

Determining f j , i values needs knowledge about the minimum sum of job 0 s tardiness in the sequence y i 0 on stages, headed by job j 0 . For this purpose, we should first find the completion time of jobs.

The completion time of lead job j 0 from the current stage to the last one is given by where R i is the ready time for stage i and is equal to the ready time of the machine in stage i which releases first.

The completion time of jobs contained in  X  y j 0 (ordered set of dispatching candidate jobs that excludes job j 0 ) is computed by simulating a EDD flow of parts through the shop, from current state until all in-process jobs ahead of job j in the system are flushed. After sorting jobs by use of EDD dispatching policy, the completion times of jobs in y j 0 are
Here, it is notable that in the above procedure some other policies despite the EDD rule were applied, but experiments did not show any significant difference between applied rules; so, the EDD rule is selected due to requiring less computation effort.
 modified due date of the related jobs are computed according to the Eq. (6) and then the tardiness of each job will be obtained.
Eqs. (7) and (8) show how to calculate tardiness of the lead job j 0 and all jobs in set  X  y j 0 .
 stage i and the term l j 0 , i equals the mean setup times between job j and all jobs being processed or queued at buffer of stages i 0  X  1to I .
 the current stage and then using the Eq. (9), total tardiness of jobs included in y i 0 (headed by job j 0 ) is obtained. factor of stages. The effect of important factor is to give candidate jobs preferred by the more important stages a greater claim in the dispatching process. A stage importance is influenced by two factors: its remaining workload an d its sequential distance down-stream. The first factor impresses the ratio of a stage 0 soutstanding workload (the sum of the remaining job 0 s local process time), related to the bottleneck stage, which has the most remaining work. The second factor also implies that the farther downstream a stage is, the less influence it has on the dispatching decision. This is because the validity of the assumptions on which the sequence costs are calculated is strongest for the st age immediately downstream, and weakens with increasing distance from the current stage. is given by stage i and jobs being processed on stage i , respectively, and M shows the number of machines in stage i .
 distance from the current stage. According to limited experimental trials using different a values, the recommended value for this parameter is 0.5. Although this empirical study cannot be con-sidered conclusively, this value appears to work fairly well and will be used henceforth. 3.2. Second phase: Improvement procedure
The improving phase of NSDR is developed based on two main ideas. The first idea is that a job with a high tardiness may have large setup times and displacing this job with its previous jobs in the way that setup times decrease, may improve the final solution.
The second idea is taken from the Wilkerson-Irvin theory (1971) about minimizing tardiness in single machine systems. To do this purpose, the due dates of jobs are adjusted for stages and then each machine is considered as a single machine environment.
These two ideas are included in an improving method that is described in the following steps: 1. List the uncompleted jobs in a set F according to the decreas-ing order of their tardiness. 2. Select the first job in set F and consider it as job a . 3. In each stage, test the relation (11) for job a and the job scheduled immediately before job a on the related machine.
In the case, that the non-equalities were hold and if possible, displace the two jobs. Do this procedure for all stages that job a has not been processed yet. In this relation, parameter e stands for the job immediately dispatched before job [ a 1] and t indicates its finish time calculated as t  X  C J  X  1 e , i 0 indicates the first job of set F .D a , i is the modified due date for job a on stage i which is obtained by Eq. (6). 4. Update start and finish times of jobs and recalculate the total tardiness. If new tardiness is decreased, then take the new schedule. 5. Define a list c i that includes unscheduled jobs on stage i in an ascending order of start times. In each stage i , suppose the job before a in list c i as job b . Examine the following non-equality and if it holds, schedule job a before job b on stage i ; otherwise consider job before b in the list c i as a new job b . Return to step 5.
 6. Update start and finish times of jobs and recalculate the total tardiness. If new tardiness is decreased, then accept the new schedule, otherwise keep the previous one. 7. Take the next job in list F as job a and go back to step 3.
Primary experimental tests specify that at the seventh step of the algorithm, it is enough to consider 1 3 of jobs from the beginning of the list F . Also, in step 5 of the algorithm, only the jobs that the difference between their start times and the start time of job a is less than 2 p a , i are regarded as b in stage i. It is because of the tradeoff between solution qualities and the time required. Experimental tests show that considering this para-meter greater than 2 increases the computation time without any significant effect on the solution quality. 4. The proposed hybrid approach based on GA
In this paper, genetic algorithm is adapted by a novel method to create initial populations and also integrate simulation approach into it. The other contributions are the dynamic structure of operators X  rates and also the way of defining fitness function that includes both tardiness and number of completed operations.
The Hybrid Genetic Algorithm (HGA) proposed for the problem under consideration is based on the general framework for this metaheuristic method ( Crama et al. 1995 ; Sastry et al. 2005 ). The algorithm starts with an initial population of chromosomes, where each chromosome represents a single solution to the problem. All of the parameters in standard GA are deterministic. In each generation, current population is modified by applying two genetic operators: first  X  crossover and subsequently  X  mutation. The crossover operator allows for generating new offspring by recombining (usually) a pair of existing solutions. They are selected from the population in the number determined by a given crossover rate. Then, according to the given mutation rate, some solutions are chosen for mutating, which modifies their structure in a random way.

In our approach, some parallel populations run at the same time in each generation and the best solution from a population migrates to the next population. In the other hand, in order to improve GA 0 s efficiency, the rate of operators in each generation will be specified dynamically according to the fitness function improvement in the last generation. At the end of each genera-tion, new solutions are evaluated and a subset of chromosomes is selected for the further search, this procedure will be repeated until certain termination conditions are met. 4.1. Simulator algorithm
In stochastic conditions, each job arrives in a non-determinis-tic way without any knowledge of the arrival time, process time and other specification of it. So, the problem cannot be solved by GA unless it is changed to deterministic form. The simplest approach is to reschedule the jobs one by one, after each of them arrives. In this approach, the time and cost of rescheduling have been neglected and will result in complex calculations. In the other hand, if number of jobs to perform the rescheduling increases, the received jobs have no operation and will wait for scheduling in the period between entering the first job until the last one. So, the delay time will increase and no good solutions are obtained. In our approach, the jobs enter to the shop and we put them into Future Events List (FEL) matrices until one of the rescheduling criteria is met. Rescheduling criteria are: 1. A given number of jobs is placed in the FEL. 2. In a given period of time, job number 0 s constraint has not met, and at least one job is placed into the FEL.
 These two parameters will be defined by management decisions. This approach divides the stochastic problem into some determi-nistic problems. Jobs information is updated at the moment of rescheduling and if a job 0 soperationiscompletedorisbeing processed at this time, we remove the operation and update the sequence. The new jobs and remaining operations of old jobs will be assigned to machines by running the GA again.

As an example, consider a flow sh op system with four machines and seven initial jobs to be scheduled. Fig. 1 shows the best sequence which was found by GA before rescheduling. Here, we assume that three new jobs have entered until the rescheduling point. Fig. 2 shows the finished operations befor e the rescheduling point that will be removed from sequence and Fig. 3 shows the best sequence that was found by GA for new jobs and the remaining operations.
For example, five initial jobs 1, 2 , 3, 4 and 7 are completed on the second machine before rescheduling. According to Table 3 ,thetwo remaining initial jobs 5 and 6 and also the three new jobs (8, 9 and 10) are sequenced as 6 X 5 X 9 X 10 X 8 on this machine.

Before we describe the algorithm, it is essential to notice some points.

In stages where there is more than one machine, jobs are randomly assigned to the first idle machine.

When a job enters the shop, simulator specifies next job arrival time from exponential distribution.

At each rescheduling point, GA is run again and best solution will be applied to the shop.
 Fig. 4 illustrates a general procedure of simulation.

According to this procedure, GA can be run at each scheduling and rescheduling point. Fig. 5 shows the outline of hybrid genetic algorithm.

The above procedure will be repeated until we fairly sure that the population has reached a good position in the search space. 4.2. The structure of initial populations
One of the most important decisions in genetic algorithms is creating initial populations. In the proposed HGA, a new pool of chromosomes should be created each time that the algorithm is required to modify the previous schedule. In this paper, a number of the most popular priority rules are applied for this purpose.
Table 1 gives a list of the priority rules used as a member of the initial populations in each time that HGA reschedules the pre-vious solution. The symbol Z j , i shows the relative priority of job j on stage i and a job with the minimum value of Z is selected at each iteration to be dispatched first.

A part of each population is created by geometric sampling method. In this method, a good chromosome (e.g. dispatching rules of Table 1 ) is considered and some other ones are generated based on the initial chromosome. This task is done by assigning a probability to each operation in a stage. Assume the sequence { J , J , y , J n } as the initial order of jobs on a stage. We want to calculate the probability of putting jobs in the first place in the new sequence of jobs. First and second rows of Fig. 6 show the jobs list and related probabilities, respectively.

In the above figure, o and O are the parameters of the method where o must be tuned and O is obtained from Eq. (13).  X  O : o j  X  X  1 ) O  X  chromosomes includes the following steps: chromosome, with a little difference. Because the specified initial populations are based on some popular and effective dispatching rules, this approach helps us to find some good sequences in the
GA procedure. 4.3. Objective function
In dynamic scheduling systems, the scheduling horizon is open and the makespan gives no credit for jobs that finish well before the last one finishes and minimizing makespan is of less interest.
Reducing turnaround time through the shop or reducing the amount of tardiness is usually the primary objective.

It is possible that some jobs are not to be completed in a rescheduling point. So we cannot use the original due dates to calculate the tardiness of chromosomes when all operations of a job have not finished. To this purpose, we calculate the modified due dates of jobs in any stage by the Eq. (6) given in Section 3.1.1 .

In the other hand, HGA procedure produces a number of chromosomes with different properties such as finish times, tardiness and number of finished operations. For example, suppose a chromosome with low tardiness and a few number of finished operations to be selected, although there are some other chromo-somes that have not much greater tardiness but greater number of finished jobs that is moderately better than the first one. Regarding now? to this, in our approach, relative tardiness measure is used to compare chromosomes. Relative tardiness is calculated by dividing the tardiness of chromosome to the number of finished operations through the following:.
 T  X  where T z is the relative tardiness for chromosome z , N z number of finished jobs in a chromosome. Note that tardiness of each job is calculated in the last stage on which the job 0 s operation is completed.

T seems to be a good criteria, however, it is calculated during the simulation process and according to any increase in the number of finished operations, the tardiness will increase too.
Experimental tests show that among some chromosomes with almost equal relative tardiness ( T z ), the one with more completed operations has a greater chance to be the best. According to this, the proposed fitness function is edited as Eq. (15). f  X  T z  X  Q  X  N best N z  X  X  15  X  where Q and N best are the relative tardiness per operation and the maximum number of finished operations in the current popula-tion, respectively.

This fitness function is only used to compare the chromosomes in roulette wheel procedure and the final result of algorithm is calculated based on mean tardiness as the original performance measure. 4.4. Genetic operators
The crossover constitutes an important step of the genetic algorithm since it is through this step that new individuals will be created and that GA is able to explore deeply the solutions space.
In this paper, two crossover operators and one mutation operator are applied to produce new generations. 4.4.1. PMX crossover
The Partially Mapped Crossover (PMX) method may be the most popular crossover operator when operating with scheduling problems. The way this method works is declared by an example.
Consider the two chromosomes of Fig. 7 . Firstly, choose two parents P 1 and P 2 and two cutting sites along the string are randomly chosen, e.g. 3 and 7. The substrings defined by the two cut points are called the mapping sections. Secondly, exchange the two substrings between the parents to produce childs. It is clear that childs will often lead to infeasible solutions. Then, one needs to determine the mapping relationship between the two mapping sections and finally, we legalize the offspring using this mapping relationship. In the first child, we can map the two infeasible genes 2 and 8 outside the mapping section, by using the mapping swaps, for instance, 2 in the first child 0 s mapping section can be mapped to 5 in the second child 0 s mapping section corresponding to the position. It does, however, not finish, because 5 is in the first child 0 s mapping section as well. Again, 5 in the first child can be mapped to 7 in a similar way. At last, 2 in the first child can be swapped to 7. Similarly, 8 in the first child can be mapped to 4. Consequently, two Childs are created as shown in the Fig. 7 . 4.4.2. One point crossover This kind of crossover is also described by a simple example.
Fig. 8 shows two randomly selected parents P 1  X  [123456789] and P 2  X  [9 3 7 8 2 6 5 1 4], and one cutting site along the string, e.g. 3. Each parent is divided into two substrings where the first one consists of the jobs from the first job to the cutting point, called S 1 , and the second substring (S 2 ) is defined by the cutting point until last job. Now, exchange the two substrings S 1 between the parents to produce offspring. As same as PMX, offspring will often lead to infeasible solutions. In this crossover, a simple way is used to legalize the sequence. We keep the original sequence in the S 1 parts of offspring which directly come from parents, then remove the jobs that appear in S 1 from S of each offspring and shift all the remaining jobs to the left. Then find the jobs that are not appeared in the current sequence of offspring 1 and put them to the end of this offspring according to their order in Parent 2. This will create Child 1 and doing the same procedure for offspring 2 will lead to the Child 2 as well. 4.4.3. Mutation offspring solution by randomly modifying the parent solution 0 s features. It helps to preserve a reasonable level of population and provides a mechanism to escape from local optima. Ho et al. (2007) define a new mutation operator. In this operator, two random jobs J 1 and J 2 are selected and swapped; where 2 o J 1 o J 2 o n 1 and n is the number of jobs in a stage. As an example, consider a sample chromosome of Fig. 9 in which jobs 1 and 8 are randomly chosen to be swapped. Fig. 9 shows the resultant offspring. 4.5. Migration be run at the same time and at each generation a population with the best fitness function will migrat e to the next population. Studies indicate that the migration algorithm is found to improve the reliability of a GA optimization run and an effective linear speedup for constant work is achieved. In this paper, as declared in Fig. 10 the best instances of each population migrate to the next one. maximum value N mig . For each subpopulation of GA, migration is accomplished as follows. At the end of each generation, a uniformly distributed random number w is generated and if w o pmig migration is initiated. During migration, a uniform random number determines the number of instances n mig between 1 and N mig to send. The best n mig instances in the subpopulation are sent to the nearest neighbor in the ring and the neighbor subpopulation replaces the n mig instances. The migration probability is set as 0.05 and the value of
N mig will be tuned in Section 4.7 . 4.6. Dynamic child 0 s parameters
In the standard GA procedure, operators 0 rates are predefined and fix, hence the operators X  ratio remains constant in all generations. In the initial iterations, crossover makes effective improvement in fitness function value. However, GA operators cannot make much improvement in the last iterations because mutation rate is defined small with respect to the crossover rate and all chromosomes in a population converge to one chromo-some. In standard GA that has been used so far, the number of iterations that fitness function does not have any improvement is defined as stopping criterion.

In our approach crossover, mutation and migration rates dynamically change during the GA procedure. Rates of operators in each generation will be modified according to the fitness function changes in the last generation. In the proposed GA, when the improvement percentage between two consecutive genera-tions is low it indicate that the chromosomes have converged and so the mutation rate is increased to prevent the procedure to be fallen in local optima. In the other hand, in the condition that higher improvements occur through the algorithm, imperial studies suggest that it is better to increase the cross over rate. 4.7. Tuning HGA parameters Parameter tuning is vital part of designing any algorithm.
According to different levels of parameters and different combi-nation of them, the quality of algorithm will be changed. Para-meter tuning in this algorithm, adopted from Zandieh et al. (2006) , is performed on the five parameters of  X  X  X nitial population size X  X ,  X  X  X umber of steps without improvement for stopping criteria X  X  and  X  X  X ynamic child parameters of crossover, mutation and migration X  X . At first, some levels for the first two parameters are selected as follows:  X  Initial population size: 70, 80, 90, 100, 110, 120, 150, 200, 300.  X  Number of steps without improvement for stopping criteria: 50, 100, 200, 300, 400, 500.

Now, the tuning algorithm can be described in the following 9 steps. 1) Set the size of initial population at 110. 2) Set the number of steps without improvement at 200. 3) Set the crossover, mutation and migration rates at 0.7, 0.1 and 0.07, respectively. 4) Test the nine levels of initial population size and select the one makes the best objective function. 5) Test the six levels of number of steps without improvement and select the one makes the best objective function. 6) Calculate the improvement percentage between the last con-secutive iterations ( IP 0 ) 7) Multiply the last crossover rate by 0.95 and recalculate the improvement percentage ( IP 1 ). If IP 1 4 IP 0 select the new rate for crossover else multiply the last selected crossover rate by 1.05 and recalculate the improvement percentage ( IP 2 ). If
IP 2 4 IP 0 select the new rate for crossover. 8) Do the step 7 for mutation and migration parameters, respectively. 9) Repeat the steps 4 to 8 for one hundred times.

The above tuning procedure is performed on 80 test problems and for the first two parameters (size of initial populations, number of steps without improvement for stopping criteria) the most selected level is the tuned one. In the case of other three parameters (crossover, mutation and migration), the average of selected rates for each improvement percentage is considered as the tunes rate.

The experimental tests show that the optimum value for the initial population size and number of steps without improvement is 100. Table 2 shows the tuned crossover, migration and muta-tion rates used in this paper based on different improvement percentages. 5. Experimental design for the simulation study
In this paper, a discrete event simulation model is developed for the operation of the flexible flow shop production system. The simulation model is developed using C  X  X  programming language and run on a PC with 3.1 GHz Pentium IV processor and 4 GB of RAM. 5.1. Parameter setting of the simulation model
Here, the processing times are randomly sampled from integer numbers in the range 5 X 35. In the literature, setup times in flexible flow shop problems are typically 20 X 40% of mean process times ( Kurz and Askin, 2003 ). Hence, based on setup time 20% and 40% of the process time, it follows uniform distributions [1,7] and [2,14], respectively.

In this paper, the number of stages is considered at three levels of I  X  5, I  X  10 and I  X  15 and the number of machines in each stage follows the discrete uniform distribution between 1 and 3 when the number of stages is less than or equal to 10, otherwise it is sampled from discrete uniform distribution between 1 and 5.
It has been observed in the literature that in flow shops, the distribution of the job arrival process closely follows the poisson distribution ( Rangsaritratsamee et al., 2004 ). Hence, the time between arrivals of jobs is exponentially distributed. The mean of this exponential distribution is determined for a specified shop utilization level and the processing requirements of the jobs. Thus, the mean inter-arrival time of jobs is obtained using the Eq. 16: R  X  p : I where R is the mean inter-arrival time, p is the mean process time per operation, I is the number of stages, U is the shop utilization and m i is the number of machines in stage i . Three levels of shop utilization are used in the experiments, viz 70%, 80% and 90%.
When a job enters the shop for processing, its due date needs to be assigned. Dynamic due date assignment methods such as
Dynamic Processing Plus Waiting (DPPW) time make use of shop status information and job characteristics for setting due dates.
The DPPW method assigns due dates to jobs based on the predicted completion time of jobs. This method is found to provide good performance for the measures based on flow time and tardiness ( Vinod and Sridharan, 2011 ). Thus, the due dates in this paper are determined by this method and according to the following: d  X  r where r j is the arrival time of job j and J t is the number of uncompleted jobs in the shop at arrival time t .

In order to make the results of simulating different methods comparable, it is required to use the same parameters in all cases.
To do this, using the EDD rule, a simulation model is run and the related parameters are generated dynamically. Then, the pro-duced parameters are applied for comparison purposes. 5.2. Specifying rescheduling criteria for HGA
As mentioned before, two criteria of job number and time limit are used to specify rescheduling periods. Job number criterion is set as receiving 7 jobs. In order to defining time limit criterion, the mean time between two consecutive job entrances ( t ) is calcu-lated and then we set t  X  7 : t as time criterion. With respect to intrinsic nature of exponential distribution, the value of t is better not be deterministic and should be updated by new jobs entrance.
To calculate t , we calculate the MSE for 3 moving average methods. Simple average, moving average and simple exponential smoothing methods are used in order to find the best one and using MSE 0 s values, simple exponential method is selected.
According to this, if job number criterion is not met until time t , rescheduling will be performed. 6. Experimentation In this paper, four dispatching rules from the literature (SSPT,
RR, HW&amp;ICH, ATC) are customized regarding to setup times condition and compared with two new algorithms proposed in Sections 3 and 4 . These rules are described in Table 3 .
In Table 3 , s j , i is the slack time of job j on stage i and p average process time of the imminent operations of the compet-ing jobs at the current stage. In HW&amp;ICH rule from Kia et al. (2010) at any time and in each stage, the jobs waiting to be processed are first sorted according to the Wilkerson and Irwin 0 s rule, and then the job that leads to the minimum completion time is assigned last to the machine. In ATC rule, c is the exponential look-ahead parameter to scale the slack according to the expected number of competing jobs. In this study, parameter c is set equal to 3 based on our experimental tests and also as Jensen et al. (1995) recommends for dynamic shops.
 only SSPT and HW&amp;ICH consider the setup times and the others are developed ignoring setups. Thus, we customize these rules in such a way that when a job is selected to be dispatched, it will be scheduled on a machine that needs the least setup time with the dispatching job. Note that in the original version of these rules, a selected job is dispatched on the machine that releases first. from the literature. SSPT is a setup-oriented scheduling rule proposed by Vinod and Sridharan (2009) and they showed that it has the best performance to minimize mean tardiness. Raghu and Rajendran (1993) proposed a new dispatching rule called RR and compared it with a number of other rules. Lastly, they concluded that RR performs quite well with respect to perfor-mance measures such as flow time, tardiness and root mean square tardiness in dynamic scheduling problems. The HW&amp;ICH is considered as another rule with which our proposed algorithms are compared, it is because the study by Kia et al. (2010) shows that this rule is the best performing rule to minimize total tardiness in comparison with some other rules. Finally, the ATC heuristic was proposed as a composite dispatching rule by
Vepsalainen and Morton (1987) which combines the weighted shortest processing time rule with the minimum slack rule in a single ranking index. This rule is a very popular benchmark rule in literature in view of reducing mean tardiness and Swaminathan et al. (2007) showed the effectiveness of this rule in comparison with some other methods. Hence, this rule was taken as the fourth rule from literature to be compared with our proposed methods.
 the predefined parameters values to evaluate the performance of four adapted dispatching rules from the literature and the two proposed new methods. 6.1. Identification of steady state
The first step in the simulation experimentation is determin-ing the initial transient period. For this purpose, moving averages of the output performance measure are plotted to identify the start of steady-state period. Steady-state period starts at a time since then moving averages reach a level value. It is found that the moving averages for the performance measure in this paper approaches a level value when about 300 jobs are completed. 6.2. Identifying different scenarios
In this study, each run of the simulation model includes 10 replications and each replication simulates the operation of the system for completion of 1300 jobs. Four scenarios are defined based on the parameter values of Section 5.1 and summarized in
Table 4 . The purpose of defining various scenarios is to evaluate the performance of methods under various conditions.

In the scenarios, jobs are numbered with respect to their arrival time and jobs in the range 300 X 1300 are used for the computation of the performance measure, because the first 300 jobs should be passed to reach the system to steady-state period.
The scenario 0 s results show that the completion times for all of the six methods are less than 3 seconds and therefore it is not necessary to report the running times. 6.3. Results and discussion for scenario I
Scenario 1 represents the base case wherein the purpose of analysis is to investigate the performance of rules in dynamic flexible flow shop environment. In this scenario, a single factor is assessed which is methods and includes 6 levels. This experiment is implemented in 10 replications, considering setup time ratio, s  X  20%, shop utilization level, U  X  80% and the number of stages, M  X  10.

Fig. 11 shows the average values of the performance measure (mean tardiness) in 10 replications. As shown in Fig. 11 ,thetwo methods proposed in this study provide better performance in comparison with the four dispatchi ng rules from the literature. Also, among all of the rules, HGA has the best performance measure.
Table 5 summarizes the analysis of variance results for scenario I. F -test is used to determine if there is any significant difference among mean values. Since the P -value in ANOVA of
Table 5 is less than 0.05, the difference among means at 95% confidence level is significant. Using the LSD method, a multiple comparison test among mean values is conducted.

Table 6 gives the LSD results. The LSD method classifies the methods into three different groups labeled a , b and c . For the mean tardiness criteria, HGA and NSDR methods belong to group a and seems to be the best performing rules. The three other rules
HW&amp;ICH, ATC and SSPT are classified into group b and the final rule of RR is grouped as c . As it is obvious from this table, the two proposed rules have less standard deviation in comparison with the other methods; this validates the effectiveness of HGA and NSDR rules again. 6.4. Results and discussion for scenario II
In this scenario, two levels of setup time ratio are used to investigate how the system performance is affected when the setup times change. Simulation results are obtained for the two-factor experiments wherein the 6 scheduling methods form the first factor and the two levels of setup time form the second factor. Table 7 shows the results of two-factor ANOVA. According to this table, main effects of scheduling method and setup time ratio are both statistically significant. As evident from Fig. 12 , HGA and NSDR perform better than the other rules when the setup time ratio is low ( s  X  20%) or high ( s  X  40%).
Table 8 shows the LSD test results for this scenario. Based on the results from this table, the two proposed methods are grouped together in both cases of 20% and 40% setup ratios which declares the better performance of these rules against the other methods. 6.5. Results and discussion for scenario III
In the base case (scenario I), the shop utilization factor is set equal to 80% but here, in order to investigate the effect of utilization level, this factor has been set at 70%, 80% and 90% to represent low to high utilization levels. Simulation experiments are conducted using a two-factor full factorial design. The experimental factors are scheduling method (6 levels) and shop utilization (3 levels). Ten replications are made for each of the 18 (6 scheduling method 3 utilization level) simulation experi-ments. Table 9 gives the results of two-factorial ANOVA. The two main effects of scheduling method and shop utilization are significant in this experiment; whereas the interaction effect of main factors is not significant that can be recognized from Fig. 13 .
In this figure, in the case of low utilization there is no sizeable difference between methods, but for higher utilization levels, HGA and NSDR obviously provide better performance measures.
It is clear that there is an increase in the mean tardiness values when the shop utilization increases. Based on Fig. 13 , HGA and NSDR have the best performance especially when utilization level is high.
The LSD test in Table 10 shows that, especially in higher utilization levels, there is a significant difference between our proposed methods and ones adopted from literature.
 6.6. Results and discussion for scenario IV considered to show how the performance of dispatching rules is influenced when the number of stages in the shop changes. Two-factor simulation experiments are conducted with the scheduling methods constituting the first factor and the three number of stages ( M  X  5, M  X  10, M  X  15) forming the second factor. Ten replications are made for each of the 18 simulation experiments arising out of the combination of 6 scheduling methods and the 3 levels for number of stages.
 effect of scheduling methods is significant but the two others are not significant for the related performance measure. Fig. 14 shows the means of the 10 replications for the 18 experiments. Table 12 shows that when the number of stages is low (5 stages in this scenario), the three methods of RR, HGA and NSDR create the best results and in the cased of 10 and 15 stages, the proposed rules are grouped as the bests among all methods. 7. Conclusions and recommendations for future studies in a dynamic environment. Since the flexible flow shop problem is
NP-hard, algorithms to find an optimal solution in polynomial time are unlikely to exist. This paper presents two methods to achieve near optimal solutions. The first proposed method (NSDR) is a kind of cooperative dispatching rule combined with neighborhood search techniques. In this paper, a hybrid genetic algorithm is considered as the second proposed method. The suggested HGA has some novel contributions in creating initial solutions, dynamic modifying of parameters and the way the fitness function is presented.

Moreover, a discrete-event simulation experiment has been developed for this production system. The simulation output has been subjected to steady-state analysis to ensure that further investigations are free from initial bias. Four dispatching rules from the literature and two new proposed methods are included in the simulation study. Different scenarios characterized by variations in setup time ratio, shop utilization and number of stages are investigated. The results of simulating different sce-narios indicate that scheduling methods proposed in this paper have a significant impact on the shop performance and provide better performance under all of shop conditions.

Further research can be done using other methods such as neural networks. The results presented in this paper should be interpreted with reference to the dynamic flexible flow shop system considered for the experimental conditions described.
Hence, there is a need for further research to develop new scheduling rules under different conditions. For example, the scheduling rules can be designed considering limited buffers between stages or system disruptions such as breakdowns of machines. Moreover, Taguchi 0 s design of experiments method can be used for simulation experiments.
 References
