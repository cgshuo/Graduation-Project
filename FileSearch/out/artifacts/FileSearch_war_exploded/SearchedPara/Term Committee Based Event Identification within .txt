 Topic Detection and Tracking (TDT) [1] has given the definitions of news Topic and related events and activities X  [2]. An Event is defined as  X  X omething (non-trivial) sinks in an ocean, it is the seminal event which triggers the topic. Other events within the topic may include salvaging efforts, environmental damage and so on. 
Nallapati et al. first presented the concepts of event identification within news top-ics [4]. In their work, cosine formula was used to compute news story similarity. Fi-nally agglomerative clustering was employed to identify news events. However, the methods widely used in Topic Detection (e .g. agglomerative clustering) can hardly achieve satisfying accuracy in event identification. Based on analysis of data, we have two observations: (1) within the same topic, even two stories describing different usually only has a small number of key terms which are strongly related to the event other than the whole topic. Therefore document similarity contributed by event key terms is usually drowned out by the similarity contributed by topic related terms. 
This paper aims at resolving the problems described above, and has the following contributions: (1) We define term committee to represent event key terms. (2) We propose a clustering based method to discover term committees of difference events within a topic. We compute the similarity between terms according to the set of sto-intra-group similarity), that are well scattered in the similarity space (low inter-group committees to adjust story representation and similarity computing in event identifica-tion. The experimental results show that our proposed event identification method improves 16.7% in accuracy compared to the method used in paper [4]. Yang [5] employed an agglomerative clustering algorithm named Group Average Clustering to identify events. Li [6] believed that news stories are always aroused by events; therefore, they proposed a probabilis tic model to incorporate both content and time information in a unified framework. Gabriel [7] used some probabilistic models features and use them to determine the hot periods of the bursty events. Although these methods are called  X  X vent identification X , their concept of  X  X vent X  is more like the concept of  X  X opic X  in TDT, bigger than the concept of  X  X vent X  in TDT. For exam-ple, the methods can detection a topic  X  X inter Olympic Game 1998 X , but they are not good at identifying and differentiating two events within a topic:  X  X he open ceremony of Olympic Game X  and  X  X  hockey match in Olympic Game X . Another related work is a new clustering algorithm named CBC (Clustering By Committee) [8]. Our method is different from CBC: the committee elements are terms (features) in our algorithm, while the committee elements are documents (samples) in CBC. And we use term committee to re-weight the representation of stories. We use the same definition of event identification as paper [4]: Event Identification (EI): event identification detects even ts within a news topic. Let the document vector and t is the publication time. The results of event identification is in topic T . And the elements have the following constrains: 
Event identification is more challenging than traditional TDT tasks, because stories corpus of TDT2 from LDC to make an investigation about the similarities between stories from the same events or different events. 
In table 1, we use S-event to represent the average similarity of all pairs of stories in the same events and use D-event to represent the average similarity of all pairs of stories belonging to different stories. S-event ( de ) and D-event ( de ) denote the similar-ity obtained by using time decay according to the difference between two story X  X  publication time. The time decay method is defined as follows: story in the given topic.  X  is the time decay factor and set to 1 here. From table 1, we topics. Even when time decay is used, the difference between S-event ( de ) and D-event ( de ) is still not significant. 
In Table 2, we give the average story similarities of all pairs of stories in the same topics tend to have low similarities. Obviously, traditional method for topic detection is not suitable for event identification. 
By analyzing the data, we have classified the terms into three classes: nor a specific event. Terms of this class should be given low weights. They are strongly related to the topic other than a specific event. Obviously, terms of this class should be assigned low weights too. frequently in other events. They are strongly related to a specific event. Through data analysis, we found an event usually only have a small number of event key terms. 
To decrease the similarity contributed by term class A and B , we have to find event terms (term class C ) of the corresponding event. Term committees are captured at first and then used for later event identification steps. In this section, we describe our approach to event identification. Our event identifica-tion method consists of three phases. At the first phase, we preprocess the news sto-ries and generate the vector representation for each news story. At the second phase, committees, that are well scattered in the similarity space (low inter-group similarity). At the third phase, we use the term committees to help re-weight key terms in stories, proach are given by subsequent subsections. 4.1 News Story Representation Preprocessing is needed before generating story representation. For preprocessing, we tokenize words, recognize abbreviations, no rmalize abbreviations, and remove stop-words, replace words with their stems using K-stem algorithm [9], and then generate word vector for each news story. 
Thus, each story d is represented as follows: terms in story d . And ) , ( w d weight means the weight of term w in story d : where N means the total number of news stories, and tf ( d , w ) means how many times term w occurs in news story d . And df ( w ) is the number of stories containing term w . 4.2 Term Committee Discovery For each term w , we create a story set at first: 
The similarity of two terms w i and w j is defined as follows: The details of term committee discovery algorithm are presented in Figure 1. 
Because the number of terms that only appear in a single news story is very large, use the terms appear in at least two stories as the input of this phase. At step 1, we put empty set. At step 3, we use agglomerative clustering algorithm to cluster the terms in reflects a preference for tighter term sets. Step 4 selects the cluster c with the highest score at first. If the similarity between cluster c and each previous term committee in residues R , and remove all the terms in R whose similarity with c is above threshold  X  . If R is empty then return term committee set C as result, otherwise the algorithm jumps to step 3. 4.3 Event Id entification To reduce the similarity contributed by term class A and B , we use term committees for potential events to re-weight terms in similarity calculation. For two stories d and d  X , their similarity is defined as follows: where C is the term committee set of the current topic obtained in the last phase, and key terms of the corresponding potential event. Therefore, the more overlapping terms two stories have in the same committee c , the more probable that the two stories be-long to the same event. 
At last, we also use agglomerative cluste ring method for event identification. Simi-larity between two clusters cl 1 and cl 2 is computed using average link: where | cl | is the number of stories in cluster cl . 5.1 Datasets The datasets include 28 topics selected fr om TDT2 corpus, and 25 topics selected from TDT3 corpus [10]. Nallapati et al. annotated event membership for each selected story, and then created a training set of 26 topics and a test set of 27 topics by merg-ing the 28 topics from TDT2 and 25 from TDT3 and then splitting them randomly. Table 3 shows some statistics for the training and test datasets. There are more details about the annotation spec in paper [4]. 5.2 Evaluation Metric We use the same evaluation metrics as paper [4]. For an automatically generated we examine a pair of stories at a time and verify whether the generated model and the given in detail as follows: z Event Recall ER : this is the probability that a pair of two randomly selected sto-
And the well known F1-measure is used to combine the above measures: 5.3 Experimental Results We implemented and tested four systems. SYSTEM 1 and SYSTEM 2 are two previ-ous systems with which we want to compare. SYSTEM-3 is based on topic-specific stopword removal and SYSTEM-4 is based on our approach. 
SYSTEM-1: This system uses cosine distance as the similarity of stories, and em-ploys agglomerative clustering based on average-link to identify events. This system is used as baseline system. [4] 
SYSTEM-2: This system is the same as SYSTEM-1, except that it uses formula (1) to adjust similarities according to time difference between news stories. [4] 
SYSTEM-3: this system is based on topic-specific stopword removal. The idea was firstly presented in paper [11]. Since topical common terms (term class A ) cause events in the same topic to be mutually confusing, a natural choice is to remove those terms. We obtained a stopword list for each t opic by thresholding on the training set document frequency of a term t in T i : SYSTEM-2. 
SYSTEM-4: This system is implemented based on our approach. It has three phases: story preprocessing, term committee discovery, event identification. The results of the four systems on training set and test set are shown in Table 4 and Table 5 respectively. Each value in the tables is the average score over all topics. P-the compared system (95% confidence level, one tailed T-test). The results of SYS-TEM-1 and SYSTEM-2 listed in these tables are obtained from paper [4]. 
For SYSTEM-3, we tested it on the test set with the optimal parameter  X  obtained from training set. For SYSTEM-4, we tested our method on the test set with the two parameters  X  1 and  X  fixed at their optimal values learned from training set. 
When tested on training set, SYSTEM-3 is slightly better than SYSTEM-2. How-ever, it is even worse than SYSTEM-2 on test set. By analyzing the cases of SYS-determine. If  X  is set to a relative small value, some terms of class C (related to a big event) may be removed falsely. Otherwise, if  X  is set to a relative great value, most of the terms of class B cannot be removed. (2) when some terms of class B and C are removed, terms of class A will get heavier weights and make more noise. The EF result of SYSTEM-4 on training set is 0.62 which is 34.8% higher than SYSTEM-1, and 17.0% higher than SYST EM-2. On the test set, SYSTEM-4 X  X  EF value is 26.0% higher than SYSTEM-1 and 16.7% higher than SYSTEM-2. On both training and test sets, SYSTEM-4 shows statistically significant improvement com-pared to SYSTEM-2 which performs the best in paper [4]. Most of the previous work, such as TDT, organizes news stories by topics which are same topic (usually stories within a topic share lots of terms about the topic), we pro-posed an event identification method based on term committee. We first capture some weight key terms in a story. The experimental results show that our approach for event identification has significant improvement over previous methods. This work is supported by the National Natural Science Foundation of China under Grant No. 90604025 and the National Basic Research Program of China (973 Pro-gram) under Grant No. 2007CB310803. 
