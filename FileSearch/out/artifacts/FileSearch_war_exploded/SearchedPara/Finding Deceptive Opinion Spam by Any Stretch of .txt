 With the ever-increasing popularity of review web-sites that feature user-generated opinions (e.g., TripAdvisor 1 and Yelp 2 ), there comes an increasing potential for monetary gain through opinion spam  X  inappropriate or fraudulent reviews. Opinion spam can range from annoying self-promotion of an un-related website or blog to deliberate review fraud, as in the recent case 3 of a Belkin employee who hired people to write positive reviews for an other-wise poorly reviewed product. 4
While other kinds of spam have received consid-erable computational attention, regrettably there has been little work to date (see Section 2) on opinion spam detection. Furthermore, most previous work in the area has focused on the detection of DISRUPTIVE OPINION SPAM  X  X ncontroversial instances of spam that are easily identified by a human reader, e.g., ad-vertisements, questions, and other irrelevant or non-opinion text (Jindal and Liu, 2008). And while the presence of disruptive opinion spam is certainly a nuisance, the risk it poses to the user is minimal, since the user can always choose to ignore it.
We focus here on a potentially more insidi-ous type of opinion spam: DECEPTIVE OPINION SPAM  X  X ictitious opinions that have been deliber-ately written to sound authentic, in order to deceive the reader. For example, one of the following two hotel reviews is truthful and the other is deceptive opinion spam :
Typically, these deceptive opinions are neither easily ignored nor even identifiable by a human reader; 5 consequently, there are few good sources of labeled data for this research. Indeed, in the ab-sence of gold-standard data, related studies (see Sec-tion 2) have been forced to utilize ad hoc procedures for evaluation. In contrast, one contribution of the work presented here is the creation of the first large-scale, publicly available 6 dataset for deceptive opin-ion spam research, containing 400 truthful and 400 gold-standard deceptive reviews.

To obtain a deeper understanding of the nature of deceptive opinion spam, we explore the relative util-ity of three potentially complementary framings of our problem. Specifically, we view the task as: (a) a standard text categorization task, in which we use n -gram X  X ased classifiers to label opinions as either deceptive or truthful (Joachims, 1998; Sebastiani, 2002); (b) an instance of psycholinguistic decep-tion detection , in which we expect deceptive state-ments to exemplify the psychological effects of ly-ing, such as increased negative emotion and psycho-logical distancing (Hancock et al., 2008; Newman et al., 2003); and, (c) a problem of genre identification , in which we view deceptive and truthful writing as sub-genres of imaginative and informative writing, respectively (Biber et al., 1999; Rayson et al., 2001).
We compare the performance of each approach on our novel dataset. Particularly, we find that ma-chine learning classifiers trained on features tradi-tionally employed in (a) psychological studies of deception and (b) genre identification are both out-performed at statistically significant levels by n -gram X  X ased text categorization techniques. Notably, a combined classifier with both n -gram and psy-chological deception features achieves nearly 90% cross-validated accuracy on this task. In contrast, we find deceptive opinion spam detection to be well beyond the capabilities of most human judges, who perform roughly at-chance X  X  finding that is consis-tent with decades of traditional deception detection research (Bond and DePaulo, 2006).
Additionally, we make several theoretical con-tributions based on an examination of the feature weights learned by our machine learning classifiers. Specifically, we shed light on an ongoing debate in the deception literature regarding the importance of considering the context and motivation of a decep-tion, rather than simply identifying a universal set of deception cues. We also present findings that are consistent with recent work highlighting the difficul-ties that liars have encoding spatial information (Vrij et al., 2009). Lastly, our study of deceptive opinion spam detection as a genre identification problem re-veals relationships between deceptive opinions and imaginative writing, and between truthful opinions and informative writing.
 The rest of this paper is organized as follows: in Section 2, we summarize related work; in Section 3, we explain our methodology for gathering data and evaluate human performance; in Section 4, we de-scribe the features and classifiers employed by our three automated detection approaches; in Section 5, we present and discuss experimental results; finally, conclusions and directions for future work are given in Section 6. Spam has historically been studied in the contexts of e-mail (Drucker et al., 2002), and the Web (Gy  X  ongyi et al., 2004; Ntoulas et al., 2006). Recently, re-searchers have began to look at opinion spam as well (Jindal and Liu, 2008; Wu et al., 2010; Yoo and Gretzel, 2009).

Jindal and Liu (2008) find that opinion spam is both widespread and different in nature from either e-mail or Web spam. Using product review data, and in the absence of gold-standard deceptive opin-ions, they train models using features based on the review text, reviewer, and product, to distinguish between duplicate opinions 7 (considered deceptive spam) and non-duplicate opinions (considered truth-ful). Wu et al. (2010) propose an alternative strategy for detecting deceptive opinion spam in the absence of gold-standard data, based on the distortion of pop-ularity rankings. Both of these heuristic evaluation approaches are unnecessary in our work, since we compare gold-standard deceptive and truthful opin-ions.

Yoo and Gretzel (2009) gather 40 truthful and 42 deceptive hotel reviews and, using a standard statis-tical test, manually compare the psychologically rel-evant linguistic differences between them. In con-trast, we create a much larger dataset of 800 opin-ions that we use to develop and evaluate automated deception classifiers.

Research has also been conducted on the re-lated task of psycholinguistic deception detection . Newman et al. (2003), and later Mihalcea and Strapparava (2009), ask participants to give both their true and untrue views on personal issues (e.g., their stance on the death penalty). Zhou et al. (2004; 2008) consider computer-mediated decep-tion in role-playing games designed to be played over instant messaging and e-mail. However, while these studies compare n -gram X  X ased deception clas-sifiers to a random guess baseline of 50%, we addi-tionally evaluate and compare two other computa-tional approaches (described in Section 4), as well as the performance of human judges (described in Section 3.3).

Lastly, automatic approaches to determining re-view quality have been studied X  X irectly (Weimer et al., 2007), and in the contexts of helpful-ness (Danescu-Niculescu-Mizil et al., 2009; Kim et al., 2006; O X  X ahony and Smyth, 2009) and credibil-ity (Weerkamp and De Rijke, 2008). Unfortunately, most measures of quality employed in those works are based exclusively on human judgments, which we find in Section 3 to be poorly calibrated to de-tecting deceptive opinion spam. While truthful opinions are ubiquitous online, de-ceptive opinions are difficult to obtain without re-sorting to heuristic methods (Jindal and Liu, 2008; Wu et al., 2010). In this section, we report our ef-forts to gather (and validate with human judgments) the first publicly available opinion spam dataset with gold-standard deceptive opinions.

Following the work of Yoo and Gretzel (2009), we compare truthful and deceptive positive reviews for hotels found on TripAdvisor. Specifically, we mine all 5-star truthful reviews from the 20 most popular hotels on TripAdvisor 8 in the Chicago area. 9 De-ceptive opinions are gathered for those same 20 ho-tels using Amazon Mechanical Turk 10 (AMT). Be-low, we provide details of the collection methodolo-gies for deceptive (Section 3.1) and truthful opinions (Section 3.2). Ultimately, we collect 20 truthful and 20 deceptive opinions for each of the 20 chosen ho-tels (800 opinions total). 3.1 Deceptive opinions via Mechanical Turk Crowdsourcing services such as AMT have made large-scale data annotation and collection efforts fi-nancially affordable by granting anyone with ba-sic programming skills access to a marketplace of anonymous online workers (known as Turkers ) will-ing to complete small tasks.

To solicit gold-standard deceptive opinion spam using AMT, we create a pool of 400 Human-Intelligence Tasks (HITs) and allocate them evenly across our 20 chosen hotels. To ensure that opin-ions are written by unique authors, we allow only a single submission per Turker. We also restrict our task to Turkers who are located in the United States, and who maintain an approval rating of at least 90%. Turkers are allowed a maximum of 30 minutes to work on the HIT, and are paid one US dollar for an accepted submission.

Each HIT presents the Turker with the name and website of a hotel. The HIT instructions ask the Turker to assume that they work for the hotel X  X  mar-keting department, and to pretend that their boss wants them to write a fake review (as if they were a customer) to be posted on a travel review website; additionally, the review needs to sound realistic and portray the hotel in a positive light. A disclaimer indicates that any submission found to be of insuffi-cient quality (e.g., written for the wrong hotel, unin-telligible, unreasonably short, 11 plagiarized, 12 etc.) will be rejected.

It took approximately 14 days to collect 400 sat-isfactory deceptive opinions. Descriptive statistics appear in Table 1. Submissions vary quite dramati-cally both in length, and time spent on the task. Par-ticularly, nearly 12% of the submissions were com-pleted in under one minute . Surprisingly, an inde-pendent two-tailed t-test between the mean length of these submissions (  X  ` t&lt; 1 ) and the other submissions (  X  ` t  X  1 ) reveals no significant difference ( p = 0 . 83 ). We suspect that these  X  quick  X  users may have started working prior to having formally accepted the HIT, presumably to circumvent the imposed time limit. Indeed, the quickest submission took just 5 seconds and contained 114 words. 3.2 Truthful opinions from TripAdvisor For truthful opinions, we mine all 6,977 reviews from the 20 most popular Chicago hotels on TripAdvisor. From these we eliminate:  X  3,130 non-5-star reviews;  X  75 reviews with fewer than 150 characters  X  1,607 reviews written by first-time authors  X 
Finally, we balance the number of truthful and deceptive opinions by selecting 400 of the remain-ing 2,124 truthful reviews, such that the document lengths of the selected truthful reviews are similarly distributed to those of the deceptive reviews. Work by Serrano et al. (2009) suggests that a log-normal distribution is appropriate for modeling document lengths. Thus, for each of the 20 chosen hotels, we select 20 truthful reviews from a log-normal (left-truncated at 150 characters) distribution fit to the lengths of the deceptive reviews. 14 Combined with the 400 deceptive reviews gathered in Section 3.1 this yields our final dataset of 800 reviews. 3.3 Human performance Assessing human deception detection performance is important for several reasons. First, there are few other baselines for our classification task; indeed, re-lated studies (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009) have only considered a random guess baseline. Second, assessing human perfor-mance is necessary to validate the deceptive opin-ions gathered in Section 3.1. If human performance is low, then our deceptive opinions are convincing, and therefore, deserving of further attention.
Our initial approach to assessing human perfor-mance on this task was with Mechanical Turk. Un-fortunately, we found that some Turkers selected among the choices seemingly at random, presum-ably to maximize their hourly earnings by obviating the need to read the review. While a similar effect has been observed previously (Akkaya et al., 2010), there remains no universal solution.

Instead, we solicit the help of three volunteer un-dergraduate university students to make judgments on a subset of our data. This balanced subset, cor-responding to the first fold of our cross-validation experiments described in Section 5, contains all 40 reviews from each of four randomly chosen hotels. Unlike the Turkers, our student volunteers are not offered a monetary reward. Consequently, we con-sider their judgements to be more honest than those obtained via AMT.

Additionally, to test the extent to which the in-dividual human judges are biased, we evaluate the performance of two virtual meta-judges. Specifi-cally, the MAJORITY meta-judge predicts  X  decep-tive  X  when at least two out of three human judges believe the review to be deceptive, and the SKEP -TIC meta-judge predicts  X  deceptive  X  when any hu-man judge believes the review to be deceptive. Human and meta-judge performance is given in Table 2. It is clear from the results that human judges are not particularly effective at this task. In-deed, a two-tailed binomial test fails to reject the null hypothesis that JUDGE 2 and JUDGE 3 per-form at-chance ( p = 0 . 003 , 0 . 10 , 0 . 48 for the three judges, respectively). Furthermore, all three judges suffer from truth-bias (Vrij, 2008), a common find-ing in deception detection research in which hu-man judges are more likely to classify an opinion as truthful than deceptive. In fact, JUDGE 2 clas-sified fewer than 12% of the opinions as decep-tive! Interestingly, this bias is effectively smoothed by the SKEPTIC meta-judge, which produces nearly perfectly class-balanced predictions. A subsequent reevaluation of human performance on this task sug-gests that the truth-bias can be reduced if judges are given the class-proportions in advance, although such prior knowledge is unrealistic; and ultimately, performance remains similar to that of Table 2.
Inter-annotator agreement among the three judges, computed using Fleiss X  kappa, is 0.11. While there is no precise rule for interpreting kappa scores, Landis and Koch (1977) suggest that scores in the range (0.00, 0.20] correspond to  X  slight agreement  X  between annotators. The largest pairwise Cohen X  X  kappa is 0.12, between JUDGE 2 and JUDGE 3 X  X  value far below generally accepted pairwise agreement levels. We suspect that agreement among our human judges is so low precisely because humans are poor judges of deception (Vrij, 2008), and therefore they perform nearly at-chance respective to one another. We consider three automated approaches to detect-ing deceptive opinion spam, each of which utilizes classifiers (described in Section 4.4) trained on the dataset of Section 3. The features employed by each strategy are outlined here. 4.1 Genre identification Work in computational linguistics has shown that the frequency distribution of part-of-speech (POS) tags in a text is often dependent on the genre of the text (Biber et al., 1999; Rayson et al., 2001). In our genre identification approach to deceptive opinion spam detection, we test if such a relationship exists for truthful and deceptive reviews by constructing, for each review, features based on the frequencies of each POS tag. 15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular au-tomated text analysis tool used widely in the so-cial sciences. It has been used to detect personality traits (Mairesse et al., 2007), to study tutoring dy-namics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007).

While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC di-mensions, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text 2. Psychological processes: Includes all social, 3. Personal concerns: Any references to work, 4. Spoken categories: Primarily filler and agree-
While other features have been considered in past deception detection work, notably those of Zhou et al. (2004), early experiments found LIWC features to perform best. Indeed, the LIWC2007 software used in our experiments subsumes most of the fea-tures introduced in other work. Thus, we focus our psycholinguistic approach to deception detection on LIWC-based features. 4.3 Text categorization In contrast to the other strategies just discussed, our text categorization approach to deception de-tection allows us to model both content and con-text with n -gram features. Specifically, we consider the following three n -gram feature sets, with the corresponding features lowercased and unstemmed: superscript + indicates that the feature set subsumes the preceding feature set. 4.4 Classifiers Features from the three approaches just introduced are used to train Na  X   X ve Bayes and Support Vector Machine classifiers, both of which have performed well in related work (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Zhou et al., 2008).

For a document ~x , with label y , the Na  X   X ve Bayes (NB) classifier gives us the following decision rule:
When the class prior is uniform , for example when the classes are balanced (as in our case), (1) can be simplified to the maximum likelihood classi-fier (Peng and Schuurmans, 2003):
Under (2), both the NB classifier used by Mihal-cea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI Language Modeling Toolkit (Stolcke, 2002) to esti-mate individual language models, Pr( ~x | y = c ) , for truthful and deceptive opinions. We consider all three n -gram feature sets, namely UNIGRAMS , language models smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1996).

We also train Support Vector Machine (SVM) classifiers, which find a high-dimensional separating hyperplane between two groups of data. To simplify feature analysis in Section 5, we restrict our evalu-ation to linear SVMs, which learn a weight vector ~w and bias term b , such that a document ~x can be classified by:
We use SVM light (Joachims, 1999) to train our linear SVM models on all three approaches and feature sets described above, namely POS , LIWC , evaluate every combination of these features, but for brevity include only LIWC + BIGRAMS + , which performs best. Following standard practice, doc-ument vectors are normalized to unit-length. For LIWC + BIGRAMS + , we unit-length normalize LIWC and BIGRAMS + features individually before com-bining them. The deception detection strategies described in Sec-tion 4 are evaluated using a 5-fold nested cross-validation (CV) procedure (Quadrianto et al., 2009), where model parameters are selected for each test fold based on standard CV experiments on the train-ing folds. Folds are selected so that each contains all reviews from four hotels; thus, learned models are always evaluated on reviews from unseen hotels.
Results appear in Table 3. We observe that auto-mated classifiers outperform human judges for every metric, except truthful recall where JUDGE 2 per-forms best. 16 However, this is expected given that untrained humans often focus on unreliable cues to deception (Vrij, 2008). For example, one study ex-amining deception in online dating found that hu-mans perform at-chance detecting deceptive pro-files because they rely on text-based cues that are unrelated to deception, such as second-person pro-nouns (Toma and Hancock, In Press).

Among the automated classifiers, baseline per-formance is given by the simple genre identifica-tion approach ( POS SVM ) proposed in Section 4.1. Surprisingly, we find that even this simple auto-mated classifier outperforms most human judges (one-tailed sign test p = 0 . 06 , 0 . 01 , 0 . 001 for the three judges, respectively, on the first fold). This result is best explained by theories of reality mon-itoring (Johnson and Raye, 1981), which suggest that truthful and deceptive opinions might be clas-sified into informative and imaginative genres, re-spectively. Work by Rayson et al. (2001) has found strong distributional differences between informa-tive and imaginative writing, namely that the former typically consists of more nouns, adjectives, prepo-sitions, determiners, and coordinating conjunctions, while the latter consists of more verbs, 17 adverbs, 18 pronouns, and pre-determiners. Indeed, we find that the weights learned by POS SVM (found in Table 4) are largely in agreement with these findings, no-tably except for adjective and adverb superlatives , the latter of which was found to be an exception by Rayson et al. (2001). However, that deceptive opin-ions contain more superlatives is not unexpected, since deceptive writing (but not necessarily imagi-native writing in general) often contains exaggerated language (Buller and Burgoon, 1996; Hancock et al., 2008).

Both remaining automated approaches to detect-ing deceptive opinion spam outperform the simple genre identification baseline just discussed. Specifi-cally, the psycholinguistic approach ( LIWC SVM ) pro-posed in Section 4.2 performs 3.8% more accurately (one-tailed sign test p = 0 . 02 ), and the standard text categorization approach proposed in Section 4.3 per-forms between 14.6% and 16.6% more accurately. However, best performance overall is achieved by combining features from these two approaches. Par-ticularly, the combined model LIWC + BIGRAMS + SVM is 89.8% accurate at detecting deceptive opinion spam. 19 Surprisingly, models trained only on UNIGRAMS  X  X he simplest n -gram feature set X  outperform all non X  X ext-categorization approaches, and models trained on BIGRAMS + perform even better (one-tailed sign test p = 0 . 07 ). This suggests that a universal set of keyword-based deception cues (e.g., LIWC ) is not the best approach to de-tecting deception, and a context-sensitive approach (e.g., BIGRAMS + ) might be necessary to achieve state-of-the-art deception detection performance.
To better understand the models learned by these automated approaches, we report in Table 5 the top 15 highest weighted features for each class ( truthful and deceptive ) as learned by LIWC + BIGRAMS + SVM and LIWC SVM . In agreement with theories of reality monitoring (Johnson and Raye, 1981), we observe that truthful opinions tend to include more sensorial and concrete language than deceptive opinions; in particular, truthful opinions are more specific about spatial configurations (e.g., small, bathroom, on, lo-cation). This finding is also supported by recent work by Vrij et al. (2009) suggesting that liars have considerable difficultly encoding spatial information into their lies. Accordingly, we observe an increased focus in deceptive opinions on aspects external to the hotel being reviewed (e.g., husband, business, vacation).

We also acknowledge several findings that, on the surface, are in contrast to previous psycholinguistic studies of deception (Hancock et al., 2008; Newman et al., 2003). For instance, while deception is often associated with negative emotion terms, our decep-tive reviews have more positive and fewer negative emotion terms. This pattern makes sense when one considers the goal of our deceivers, namely to create a positive review (Buller and Burgoon, 1996).
Deception has also previously been associated with decreased usage of first person singular, an ef-fect attributed to psychological distancing (Newman et al., 2003). In contrast, we find increased first person singular to be among the largest indicators of deception, which we speculate is due to our de-ceivers attempting to enhance the credibility of their reviews by emphasizing their own presence in the review. Additional work is required, but these find-ings further suggest the importance of moving be-yond a universal set of deceptive language features (e.g., LIWC ) by considering both the contextual (e.g., BIGRAMS + ) and motivational parameters underly-ing a deception as well. In this work we have developed the first large-scale dataset containing gold-standard deceptive opinion spam. With it, we have shown that the detection of deceptive opinion spam is well beyond the ca-pabilities of human judges, most of whom perform roughly at-chance. Accordingly, we have introduced three automated approaches to deceptive opinion spam detection, based on insights coming from re-search in computational linguistics and psychology. We find that while standard n -gram X  X ased text cate-gorization is the best individual detection approach, a combination approach using psycholinguistically-motivated features and n -gram features can perform slightly better.

Finally, we have made several theoretical con-tributions. Specifically, our findings suggest the importance of considering both the context (e.g., BIGRAMS + ) and motivations underlying a decep-tion, rather than strictly adhering to a universal set of deception cues (e.g., LIWC ). We have also pre-sented results based on the feature weights learned by our classifiers that illustrate the difficulties faced by liars in encoding spatial information. Lastly, we have discovered a plausible relationship between de-ceptive opinion spam and imaginative writing, based on POS distributional similarities.

Possible directions for future work include an ex-tended evaluation of the methods proposed in this work to both negative opinions, as well as opinions coming from other domains. Many additional ap-proaches to detecting deceptive opinion spam are also possible, and a focus on approaches with high deceptive precision might be useful for production environments.
 This work was supported in part by National Science Foundation Grants BCS-0624277, BCS-0904822, HSD-0624267, IIS-0968450, and NSCC-0904822, as well as a gift from Google, and the Jack Kent Cooke Foundation. We also thank, al-phabetically, Rachel Boochever, Cristian Danescu-Niculescu-Mizil, Alicia Granstein, Ulrike Gretzel, Danielle Kirshenblat, Lillian Lee, Bin Lu, Jack Newton, Melissa Sackler, Mark Thomas, and Angie Yoo, as well as members of the Cornell NLP sem-inar group and the ACL reviewers for their insight-ful comments, suggestions and advice on various as-pects of this work.

