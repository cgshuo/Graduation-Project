 Data stream applications such as network monitoring, on-line transaction flow analysis, intrusion detection and sensor networks pose tremendous challenges to traditional database systems. Unbounded continuous input streams require specific processing techniques different from fixed-size stored data sets.
As for  X  X oin X , a traditional important operator, it is not practical to compare every tuple in one infinite stream with every tuple in another, thus, sliding window join is put forward [1]. It restricts the set of the most recent tuples that participate in the join within a bounded-size window, and produces acceptable approximate join outputs. There are mainly two types of windows: time-based window and tuple-based window [3]. As for time-based window, the number of tuples in window is not fixed. The higher the stream arrival rate is, the more tuples the window memory holds. For tuple-based window, the number of tuples in window is fixed. The higher the arrival rate is, the newer the window tuples are. In our paper, we primarily focus on tuple-based window, and time-based window is reserved for future work.

Note that even with a window predication, join operator may lack of CPU or memory resources when streams have high arrival rates. Therefore, we need load shedding (drop some tuples to reduce system load) to facilitate the join processing, so as to keep pace with the incoming streams. There are two types of join approximation [5]: max-subset r esults and sampled results. We take max-subset approximation as the evaluation criterion for shedding strategies.
For time-based window joins, we have two kinds of resource limitations, CPU deficiency and memory shortage [4]. For tuple-based joins, the two limitations can attribute to CPU deficiency exclusively, because the buffer memory that holds tuples will not overflow if CPU is fast enough. Considering the evaluating process of joins, since probes(checking the opposite window for matching tuples) take up most of the CPU resources, we develop a novel shedding strategy by letting part of the tuples enter window without performing probes. We  X  X rop X  the tuples in this way rather than discard them directly, for the sake that future tuples from the other stream may produce join results with these ones. Further-more, we implement a semantic selection of the shedding tuples based on sta-tistics of aux-windows(Section 2), which shows good performance on producing max-subset outputs and is denoted as rear-shedding stategy(Section 3). If stream arrival rates are high, a large percen t of tuples will be dropped. CPU resources are primarily spent on the operation of e ntering/leaving windows. Considering an extreme case, stream speeds are so high that no probes can be performed, then no join outputs will be obtained. Paradoxically, if we discard part of tuples beforehand, some CPU resource will be saved to perform probes, with a subset of join outputs gained. We name the shedding strategy here front-shedding, and address the problem of how to cooperate these two shedding processes through a series of calculations(Section 4). Experiment results are shown in Section 5. Related literatures are fully summarized in Section 6. Our goal is to process a sliding window equi-join between two streams A and B producing maximum subset of join outputs with load shedding if necessary.
We adopt the join process similar to those presented in [1,5]. Assume the two streams are Stream A and Stream B. On each arrival of a new tuple from Stream A, three tasks must be performed: 1. Scan Stream B X  X  window, looking for matching tuples, and propagate them 2. Insert the new tuple into Stream A X  X  window. 3. Invalidate the oldest tuple in Stream A X  X  window.

From the above, we conclude that there are two kinds of tasks for CPU to perform: probe (1) and updates (2,3). As for tuple-based window, for one tuple, updates (replacing the oldest tuple in the join window with a new coming one) can be performed more efficiently than probe. In cases of high stream speeds, CPU is unable to perform the whole join process (both probe and updates) for every arriving tuple, therefore we need to shed load by letting part of the tuples enter window without performing probes, yet the other tuples perform probes as normal. Notice that we do not discard the tuples that do not perform probes, for future tuples from the other stream may produce join results with these tuples. Consequently, CPU can keep pace with the streams whose speeds are faster than CPU X  X  processing ability. Figure 1 shows our model on window joins. We divide the memory into three parts. For each stream, we have : 1. join-window , the join window holding the tuples with which a new arriving 2. aux-window , auxiliary window, which is the same size as the join-window. 3. queue , serves as a buffer. We can detect stream speeds by monitoring the
When the queue length reaches the threshold when buffer is about to overflow, and the stream speeds are still faster than CPU processing rate, we start load shedding by keeping part of the tuples from performing probes. Hence CPU can process more tuples per time interval, though some join results are left out. We denote this load shedding process as rear-shedding, whose evaluation is executed when a tuple leaves aux-window, and preparing to enter join-window. If the in-coming stream speeds further increase, exceeding another threshold (interpreted in sections below), we start front-shedding to cooperate with rear-shedding to produce max-subset join results. For convenience, in Table 1, we introduce notations for the constants and vari-ables used in this paper. These notations are also used in the following sections. W , D are set up according to specific application, while V j and V w are deter-mined by CPU processing ability and can be tested from experiments. 3.1 Determining k r We do not need load shedding if CPU can perform the joins of every tuple. In order to keep the queue from overflow, we need to maintain an approxi-mately constant queue length. Based on this prerequisite, we have the following deduction:
Thetimeforonetupletoenterqueueis 1 V leave aux-window and to join is 1 V for one tuple to leave aux-window is equal to that for one tuple to enter aux-window, which is equal to the time for one tuple to leave queue. For a constant queuelength,thetimeforonetupletoenterqueueandthetimeforonetuple toleavequeueareequal,thuswehave: Likewise, we can get the following equation when performing load shedding: Then k r is determined as:
Furthermore, for a constant queue length, we obtain V q = V s . V s can be de-tected by the system, thus we can find a shedding rate k r to let CPU coordinate with the incoming streams. The faster the streams are, the higher k r is adopted. 3.2 Determining Which Tuple to Shed Suppose we perform joins on tuple X  X  attribute Attr , and take integer as data type for simplicity. For each stream, we build a window-histogram based on its aux-window by mapping the values of Attr into an array of counters. The array size is D . Figure 2 gives an example. There are two 1s, four 2s, one 3, zero 4, and one 5 in aux-windowB. Window-histogram is maintained dynamically, and when a new tuple enters aux-window or when an old one leaves, updates will be carried out by means of increasing or decr easing the corresponding counter of the tuple X  X  attribute value.

Assume the two streams have the same speed(Processing of different speed ratio is omitted due to space limitation. Readers can refer to our technical report [14].), for such speed ratio 1:1, we let the aux-windows and join-windows of the two streams have the same size. CPU alternatively takes out a tuple from one of the aux-windows and performs joins with the opposite join-window. Now we introduce the strategy to determine which tuple should be shed. Take Stream A for example. When a tuple g is about to enter aux-windowA, we check its join attribute value in the window-histogram of the opposite window (window-histogramB), find how many join outputs it will produce, and save this number N . Accordingly, we construct an array C for all the tuples in aux-windowA, recording the number of join outputs that each tuple will produce. Moreover, a frequency array is built on the array C , counting how many tuples in aux-windowA will produce a specific number( C [ i ]) of join outputs. Therefore, when g leaves aux-windowA, it is able to count how many tuples in aux-windowA will produce less join outputs than N , denoted as n . Figure 3 provides an example: the tuple being judged will produce 4 join outputs, we need to count how many tuples in its aux-window will produce less than 4 join outputs. There are 4 tuples will produce 0 join outputs, 3 will produce 1, 6 will produce 2, and 8 will produce 3, there are 4+3+6+8=21 (n=21) tuples in all to produce less than 4 outputs. From Algorithm 1, we know that for a tuple g leaving aux-windowA, if the aux-windowA has k r W or more tuples that will produce join outputs fewer than N , g will not be shed.
 Algorithm 1. SheddingAlgorithm()
Considering a case that many tuples in aux-windowA have the same number of join outputs. For example, suppose window size is 100, and of the 100 tuples in aux-windowA, 20 tuples will produce 0 join result, 50 tuples will produce 1 join result, and 30 tuples will produce 2 join results. Now we have the shedding rate k r =0.6, thus we should shed all 20 resultless tuples and also 40 tuples from 50 which will produce 1 join result. The 40 tuples will be chosen randomly. The algorithm is easy and omitted here. Suppose stream speeds are extremely high, e.g. V s &gt; V w .Wehavetoshedall the tuples ( k r =1), and CPU resources are all spent on performing updates, with no join results produced. Nevertheless, the speed of tuples entering queue is still higher than the speed of tuples leaving queue. The queue length will increase with no limit, and the system will become unstable. However, if we discard part of the incoming tuples before pushing them into the queue, letting V q &lt; V w , some CPU resources will be saved to perform probes, and some join results will be obtained. We introduce front-shedding, controlling V q &lt; V w .Figure4 shows an approximate join outputs curve without front-shedding. When V s &lt; = V  X  V ( k r =0), and output results will increase in proportion to the stream speeds. As the stream speeds become higher, shedding rate increases correspondingly. When the stream speeds reach V w , the shedding rate k r is 1, and the output results are 0. Since the stream speeds and the shedding rate k r are continuous, there exists a maximum number of join outputs at a certain speed, V opt .(optmeans optimal.) Next, we will calculate V opt .
Suppose the two incoming stream have the same distribution. Take uniform distribution as an example. Other types of distribution, such as Zipfian can be deduced similarly. For a tuple g with attribute value a , the probability it appears is 1 /D ,where D is the value domain of the attribute. The probability that in the opposite window, there are exactly i tuples with the same attribute value a as tuple g is: Suppose we will shed all the tuples producing outputs fewer than M ( M  X  [0 , W ]), and part of the tuples producing M outputs, denote the ratio as r , i.e. among the tuples that will produce M outputs, the number of tuples to be shed divided by all such tuples. Thus the shedding rate k r is determined as: And tuples joined per time interval is: V q (1  X  k r ). The average number of join outputs each tuple can produce is: Thus the total number of outputs per time interval is: Our goal is to achieve the max-subset of join output results, letting O reach the maximum O max .Asisknowninequation(1), k r is a function of V q . Substitute k r with V q , we obtain: Let V q = V opt ,when O reaches its maximum O max .let  X  = V j / V w ,then V q = V / (  X  +1  X  k r ). Substitute V q with k r and  X  ;andlet  X  = M  X  =  X  + 1, we can get: In equation (3), for a definite M ,  X  ,  X  , V j are all constant. Hence O changes monotonically with k r . As a result, there is no such k r : that produces O max . O max is obtained only at endpoints, i.e. the ratio r =0. The following equation can be deduced: Equation (2) can be reduced to: M is in [0, W ]. For a given window size and distribution, W and P i are fixed; only M is variable. Therefore O max can be easily found through a search of M among W +1 values. V opt and k r can be then determined by M .Furthermorewe can use a binary search to reduce the searching cost remarkably, for the function O has the shape like  X   X   X , which means it first increases, and then decreases. The proof is omitted due to page limitation.

Based on the discussions above, we summarize the applying of front-shedding and rear-shedding strategies as follows:  X  X f V s &lt; = V opt , only rear-shedding will be adopted.  X  X f V s &gt; V opt , rear-shedding and front-shedding will cooperate. Control V q by
Front-shedding rate k f is determined as k f =1  X  V opt V is ignored in front-shedding, because the long queue may impair its efficacy in prediction over joins. Therefore, we ch oose a subset of the streams in a random way, namely a simple but efficient way. To assess the practical performance of our model, we perform several sets of ex-periments on both synthetic and real life datasets. We compare the performance of our strategies (referred to as DUAL) with another two load shedding strate-gies. One is dropping tuples randomly from the join input buffers (referred to as RAND); the other is a heuristic strategy [4] (referred to as PROB). Additionally, we use an optimal offline strategy [4] (referred to as OPT) to better evaluate the results. All the experiments are performed on P4 3.2G, 512M, Windows XP. The experiments indicate that our dua l-window model histogram-based load shedding strategy works surprisingly well in practice. 5.1 Experiments on Front-Shedding Our first set of experiments is focused on studying the function of front-shedding. We compare two strategies, both front-shedding and rear shedding (referred to as DUAL), and rear-shedding only (referred to as REAR). We use window size 400, domain size 50, and input data generated from Zipfian distribution with skew parameter 1. From the tested speed of join probes and that of the tuples entering/leaving window, we obtain that V opt = 117.396 by calculation, which accords with our experiment results. V opt is determined similarly in subsections 5.2 and 5.3, with respect to fixed data distributions and predefined window sizes. Figure 5 shows the comparison between the two strategies.

When stream speeds are lower than V opt , front-shedding has not been started, thus two strategies have the same results. As the stream speeds increase, we can easily see the difference: the result from DUAL keeps approximately a constant number, because front-shedding controls the tuples entering queue at a constant speed, and rear-shedding drops tuples at a constant shedding rate. 5.2 Effect of Window Size Figures 6 and 7 show the number of join outputs for window sizes of 400 and 800 respectively. In this set of experiments, we use input data generated from Zipfian distribution with skew parameter 1, domain size 50. Four load shedding strategies are to be compared: OPT, RAND, PROB, and DUAL.

As shown in the figures, DUAL works much better than PROB and RAND, especially when stream speeds are high. The performance of the different strate-gies do not change much as the window size is varied. Increased window size only produces more join outputs at one stream speed, for a tuple needs to probe more tuples in the opposite window; but not impacts the performance of the load shedding strategies.
 5.3 Effect of Distribution Figure 8 shows the performance of the different load shedding strategies for a window size of 400 when both the incoming streams have a uniform data distribution in a domain size of 50. The experiment results indicate that for less regular input data, shedding by heuristic information is not a good option, while our strategy has a significant advantage over shedding by heuristic information or random selection.

The input data streams consist of tuples with uniformly distributed attribute values have different affects on the performance of different load shedding strate-gies. Since all the tuples have the same probability of finding a tuple with equal attribute value in the opposite window, heuristic information is trivial in judg-ing which tuple will produce more join results. Therefore PROB will be as poor as RAND, however, DUAL is able to perform as well as on Zipfian distributed input data. Aux-windows are introduced to predict the number of join outputs that each tuple can produce, therefore enable the selection among tuples within a range of window size. Such preferences are accumulated through large streams, and finally lead to the advantage over the other two strategies. 5.4 Real Life Dataset Experiments We use CO 2 data available at [10] as our real life datasets for experiments. We perform a streaming sliding window join using the air temperature at 38.2 meters in two years -1995 and 1998 -as two datasets, and we set window size as 1000. After deleting invalid data items and considering the warmup phase [4], 15471 tuples are left for join queries. Such join query results can be potentially used to research the change of ambient CO 2 concentration at the same temperature in the three years. For the calculation of V opt ,weperformasamplingofthe datasets, and then obtain an approximate distribution of the input data, thus V opt can be determined as described in Section 4. Figure 9 shows the results from different strategies as a percentage of ideal case, namely the results produced by fast enough CPU.

From the figure, it is observed that our strategy DUAL performs much better than PROB and RAND. The real life datasets are neither as random as uniform distribution data, nor as regular as Zipfian distribution data. Therefore, heuristic information may be used to judge which tuple will produce more join outputs, but the judgment might not be accurate, in other words, the tuples with attribute value that produced more join outputs in the past might not produce more join outputs in the future. At the same time, DUAL performs well because the judgment is within one window instead of among all the tuples, and therefore more accurate than selection by heuristic information. There has been considerable work on data stream processing. The survey in [11] gives an overview of stream work, an d has summarized the issues of build-ing a data stream management system. Specialized systems have been built to process streaming data, such as Aurora [6], STREAM [2], NiagaraCQ [7] and TelegraphCQ [9].

The papers [1, 4, 5, 12, 13] focus on performing joins over streaming data. [1] introduces an implementation of join process, and addresses the cost models of nested loop joins and hash joins, which adopts the simplest random shedding strategy. [4] provides an architectural model, primarily discusses the offline load-shedding strategies, and introduces some heuristic online strategies. [5] puts forward the concepts of sampled results and age-based model, apart from max-subset results and frequency-based model in [1,4]. Our work consider max-subset results and frequency-based model. We also construct an architectural model, and develop an online shedding strategy a ccording to window statistics. In the literature of multi-joins, [12] analyzes the cost of nested loop joins and hash joins, and proposes join ordering heuristics to minimize the processing cost per unit time. [13] provides a symmetric multi-join operator for multiple joined streams to minimize memory usage as opposed to using multiple binary join operators. In this paper, we addressed a novel load shedding technique over sliding window joins. We propose a dual window architectural model, and build statistics based on the aux-windows. Effective semantic load shedding can be implemented, for the number of join outputs can be predicted by window-histograms in advance. With the cooperation of front-shedding and rear-shedding, we can deal with high stream arrival rate scenarios, and manage to produce max-subset results. A promising direction for future work is to consider time-based window joins in order to serve for different kinds of applications.
 Acknowledgments. This research was partially supported by the National Nat-ural Science Foundation of China (Grant No. 60273079 and 60573089) and Spe-cialized Research Fund for the Doctoral Program of Higher Education (SRFDP).
