 Anumberofretrievalmodelsincorporatingtermdependenci es have recently been introduced. Most of these modify existing  X  X a g-of-words X  retrieval models by including features based on the p roxim-ity of pairs of terms (or bi-terms). Although these term depe ndency models have been shown to be significantly more effective tha nthe bag-of-words models, there have been no previous systemati ccom-parisons between the different approaches that have been proposed. In this paper, we compare the effectiveness of recent bi-ter mde-pendency models over a range of TREC collections, for both short (title) and long (description) queries. To ensure the repro ducibil-ity of our study, all experiments are performed on widely ava il-able TREC collections, and all tuned retrieval model parame ters are made public. These comparisons show that the weighted se -quential dependence model is at least as effective as, and often sig-nificantly better than, any other model across this range of collec-tions and queries. We observe that dependency features are m uch more valuable in improving the performance of longer querie sthan for shorter queries. We then examine the effectiveness of de pen-dence models that incorporate proximity features involvin gmore than two terms. The results show that these features can impr ove effectiveness, but not consistently, over the available da ta sets. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models ;H.3.4[ Information Storage and Retrieval ]: Systems and Software X  Performance and evaluation Retrieval models; proximity; evaluation
Recent research has demonstrated that retrieval models inc orpo-rating term dependencies (dependency models) [6, 16, 19, 21 ,30] can consistently outperform benchmark  X  X ag-of-words X  mod els [1, 22, 26] over a variety of collections. We define a dependency model here as any model that exploits potential relationships bet ween two or more words to improve a document ranking. Using a depen-dency model requires a query processing component and a scor ing component. In query processing, groups of words that are poten-tially related are selected. This can be done in a variety of w ays, but the most common is to select words that satisfy some prox-imity relationship, such as being next to each other in the qu ery. An alternative would be to use linguistic analysis to identi fy words that have specific syntactic relationships. The scoring com ponent of a dependency model modifies the scores of documents to take into account the presence of the selected query words in a spe cified relationship, such as satisfying a proximity or linguistic constraint. Most of the best-performing dependency models are based on p rox-imity (or position-based) features and we focus on these mod els in this paper.

Although there have been a number of comparisons of depen-dency models to bag-of-words baselines, there has been surp ris-ingly little comparison between these models. Given the imp or-tance of dependency models, it is critical to provide comparisons and baselines that can be used to establish the effectivenes sofnew models, instead of showing an improvement compared to relat ively weak bag-of-words baselines. In this study, we compare the e ffec-tiveness of recent retrieval models that use term dependencies and, in addition, study the impact of different proximity featur es.
In the first part of the paper, we describe a systematic compar ison of state-of-the-art dependency models that use features ba sed on the proximity of pairs of terms (bi-terms). For this compari son, we use a range of TREC collections, including both short (title)and long (description) queries. By using these collections, qu ery sets, and open source software, our results can be easily reproduced and used as baselines or benchmarks in future studies. The param eters for each model are extensively tuned to maximize performanc e, and 5 -fold cross validation is used to avoid overfitting. 1
Some dependency models have been proposed recently that use proximity features involving more than two terms [5, 30, 32, 33]. We define a many-term dependency as a set of three of more terms that are assumed to be dependent. The studies comparing many -term dependency features to bi-term features have been inco nclu-sive, and providing more comprehensive evidence of the rela tive effectiveness of these proximity features is another goal o fthispa-per. To do this, we compare the best bi-term dependency model s to both existing many-term dependency models and models cre ated by adding many-term features to the best bi-term dependency mod-els. Similar to the first part of the paper, the parameters for each model are extensively tuned and 5 -fold cross validation is used.
To ensure fair comparisons between the models, we restrict t he process of selection or generation of term dependencies fro mthe
For space reasons, details of parameter settings and query f olds are provided in a companion technical report. input query so that it does not rely on external information, or a pseudo-relevance feedback algorithm [12]. These restrict ions en-sure that each tested model has access to identical informat ion, and computing resources, thus allowing the direct attribut ion of re-trieval effectiveness improvements or degradations to dif ferences in model formulation and the features used. Further, these res trictions make these models widely applicable in many different information retrieval problems.

Results from our experiments show that the performance of al l dependency models can be improved significantly through app ro-priate parameter tuning. This may not be a new or surprising con-clusion, but the extent of the improvements possible is quit eno-ticeable, and there are many published results where this tu ning does not appear to have been done [2]. We also confirm the pre-vious results showing that dependency models using bi-term scon-sistently improve effectiveness compared to bag-of-words models. The comparison between the bi-term dependency models shows that the variant of the weighted sequential dependence mode l[6] tested in this study exhibits consistently strong performa nce across all collections and query types. In regards to the compariso nbe-tween short and long queries, we observe that dependency fea tures have more potential to improve longer queries than shorter queries. We also show that many-term proximity features have the pote ntial to improve retrieval effectiveness over the strongest bi-t erm depen-dency models. However, more research, and probably more tra in-ing data, is required to fully exploit these features.

The major contributions of this study are:
This paper is structured as follows. Section 2 discusses bac k-ground and related work. Each of the tested dependency model s are discussed in Section 3. The experimental setup is descri bed in Section 4. Section 5 presents a detailed comparison of the re trieval models, and results and analysis from the investigation of m any-term dependency features.
The simplest method of identifying groups of dependent term s in a query is to assume that all query terms depend on all other query terms. Several recently proposed dependency models make this assumption. BM25-TP [24] extracts all pairs of terms fr om the query. This results in a polynomial relationship betwee nthe number of query terms and the number of extracted dependenci es. Similarly, the full dependence model (FDM) [19] uses each gr oup in the the power set of query terms. This model produces an exp o-nential number of query features, relative to the number of query terms. The large number of query features makes longer queri es impractical for both these models. For this reason, these mo dels are not included in this study.

Two retrieval models, BM25-Span [30] and the positional lan -guage model (PLM) also make the assumption that all query ter ms depend upon all other query terms. However, both models produce just a single group of dependent terms, the group of all query terms. As we will discuss in Section 3, we include both BM25-Span and PLM in our comparison of many-term dependency models.

To improve efficiency, a common alternative is to assume that only adjacent pairs of query terms are dependent. The n -gram language models approach presented by Song and Croft [29] is an early example of this method. Indeed, this approach has been used effectively by many recent dependency models [4, 6, 19, 21, 3 2, 33]. A key advantage of this dependency assumption is that ev en long queries remain computationally feasible after the inclusion of all dependency features.

While, in general, pairs of adjacent terms capture useful in forma-tion in the query, the assumption of positional dependence also has the potential to introduce misleading pairs of terms. For ex ample, consider extracting all sequential term pairs from the quer y: desert fox news stories .Whilethequeryintentmayhavebeentofindcon-temporary articles about the WWII field marshal, Erwin Rommel, the assumption of sequential dependence leads to the extraction of  X  X ox news X  .Thispairoftermshasthepotentialtomisleadare-trieval model to focus on the news channel, rather than the de sired information. Even so, retrieval models that use the sequential de-pendence assumption have been shown to improve average retr ieval performance compared to bag-of-words retrieval models.

Several different linguistic techniques have been used in a nat-tempt to improve dependency extraction. Srikanth and Sriha ri [31] use syntactic parsing to identify concepts in queries. Gao e tal. [13] propose a method of extracting dependencies using a lin kage grammar. Park et al. [20] use quasi-synchronous parsing to g en-erate syntactic parse trees, from which dependencies are ex tracted. Maxwell and Croft [17] have recently shown retrieval perfor mance can be improved through the use of dependency parsing techni ques in the extraction of non-adjacent subsets of dependent term s. A common requirement of these methods is a natural language query.
Query segmentation has also been proposed as a method of ex-tracting only the most informative dependencies from an inp ut query [5, 7, 15, 25]. Typically, these methods use a set of features to deter-mine where to segment (or separate) the query. Detected segm ents are then considered term dependencies. Bendersky and Croft [3] extend this work to classify a subset of detected query segme nts as key concepts.

None of these techniques have been shown to be consistently better than the simpler adjacent pairs method for identifying good candidate term dependencies. In addition, they commonly re ly on external data sources such as linguistic models, Wikipedia articles, and query logs. For this reason, as stated in the introductio n, we decided to simplify the comparison presented here by leavin gan investigation of the non proximity-based methods to future work. This restriction eliminates the possibility that an observ ed perfor-mance difference can be attributed to unique, external data sources. Further, the investigations presented in this paper can be u sed to de-termine an appropriate benchmark for a future study investi gating the utility of external data sources in dependency retrieva lmodels.
Given that a group of terms has been identified as dependent, a dependency model must also specify how to match and evaluate each group of dependent terms for each scored document. Matc h-ing methods commonly focus on measuring how often the depen-dent group of terms occurs in the document, subject to a proxi mity constraint.

One of the simplest approaches is to match dependencies as n -grams or phrases. This method reports a match when all terms i n adependentgroupoccursequentiallyinadocument. Thistyp e of feature has been used in several dependency models [6, 19, 29, 31]. From a scoring and weighting perspective, an occurrenc eofa dependent group is generally treated similarly to the occur rence of asingleterm.

Another common method of matching a dependent group to in-stances in a document is to count text windows, or constraine dre-gions of the document, that contain all the dependent terms. Window-based features have been used in a number of retrieval models [19, 21, 24, 33]. Similar to the phrase-type features, window-based oc-currences may be evaluated as terms [6, 19, 21]. Alternative ap-proaches have also been proposed. For example, Rasolofo and Savoy [24] propose a method that scores windows of term pairs proportional to the inverse, squared width of the matched te rms.
Tao and Zhai [33] propose several different aggregate dista nce functions over the set of all matched query terms in the docum ent. In this study, documents are scored using KL divergence or BM 25 combined with a retrieval score  X  X djustment factor X . The ad just-ment factor transforms an aggregate measure of distance, ov er all matched term instances in the document into a  X  X easonable X  s core contribution. The aggregate functions tested include the m inimum, maximum, average, span and min-cover distances. They obser ve that the minimum distance function in this model optimizes p erfor-mance. However, the authors also observe that this best perf orming model is not statistically different from the sequential de pendence model. For this reason, we omit the evaluation of these model s from this work.

Another approach is to cluster all dependent term occurrenc es into text spans [30, 32]. Each span is evaluated according to the number of matching terms in the span and size of the span, where spans of a single term use the maximum span size. For these mod -els, the definition of a span is implied by the specific algorit hm proposed to cluster term occurrences into spans.

Positional language models [16] propose a method of modelin g dependencies between all query terms. Dependencies are imp licitly modeled through the evaluation of a specific document positi on. Each matching query term instance in the document propagate sa partial count to the position to be evaluated, through a kern el func-tion. If several queried terms occur near this position, the nthe probability of relevance for the position increases.

Some learning-to-rank retrieval models also use dependenc yfea-tures. For example, Svore et al. [32] investigate the effect iveness of several BM25-based dependency models, that are decomposed into features in a learning-to-rank retrieval framework. Their study also introduces new features that are detected in matched spans. Using alargesetoftrainingdata,( 27 , 959 queries), they are able to show large retrieval effectiveness improvements over the origi nal formu-lations. A combination of a lack of training data, and the lar ge number of parameters in these proposed learning-to-rank retrieval models, make evaluation of these models infeasible for this study.
In this paper, we compare the relative effectiveness of bi-term dependency models, and the effectiveness of bi-term models com-pared to many-term models. We start by performing a systemat ic comparison of a variety of bi-term dependency models. This c om-parison allows us to determine the most effective benchmark bi-term dependency model. We then compare this benchmark model against existing many-term dependency models and variatio ns of the benchmark model that include many-term dependencies. I nthis section, we present and discuss each of the dependency model sthat will be compared empirically in Section 5. Note that we do not claim to evaluate all existing dependency models, instead, we lim-ited our evaluation to more  X  X opular X  models that have been u sed in multiple studies. We also, as previously noted, do not mak euse of external data sources or pseudo-relevance feedback.
 Table 1: Feature functions used by the WSDM-Internal retrie val model.
 Language Modeling: The language modeling framework for in-formation retrieval [22] has been shown to be an effective ba g-of-words retrieval model. This model allows for various assump tions in the estimation of the probability of relevance. Query lik elihood (QL) [22] is commonly used as a strong bag-of-words baseline re-trieval model. Several different methods have been propose dto model dependencies between terms in this framework.

Metzler and Croft [19] propose the Markov random field re-trieval model for term dependencies. This framework explicitly models the relationships between query terms. The sequenti al de-pendence model (SDM) assumes that all pairs of sequential te rms extracted from the query are dependent. It models each bi-te rm dependency using two types of proximity features; an orderedand an unordered window. The ordered windows matches n -grams or phrases in each evaluated document, the unordered window matches each pair of terms that occur in a window of 8 terms or less. SDM scores each term, ordered window and unordered window featu re using a smoothed language modeling estimate. A weighted lin ear combination is then used to produce a final estimate of the pro ba-bility of relevance for a document, given the input query.
Bendersky et al. [6] extend SDM to incorporate term and win-dow specific weights (WSDM). Each term and window is assigned aweightusingalinearcombinationoffeatures,extractedf or each term and window. The parameters for this model control the co n-tribution of each feature to the weight of a specific term or wi ndow. In their initial study, a total of 18 features are used to estimate the optimal weight for each term and window. In later studies, th e feature set is reduced to 13 features without exhibiting diminished performance [4]. Several of these features are computed ove rexter-nal data sources, including a query log, Wikipedia and the Google n-grams collection. In accordance with our restrictions, w elimit this model to the set of features that are computed over the ta r-get collection only. We label this model variant WSDM-Internal (WSDM-Int). The subset of features used to estimate the weig ht of each term and bi-term feature are listed in Table 1.
 Divergence from Randomness: Retrieval models based on the di-vergence from randomness (DFR) framework [1] have been used in a number of studies. Term dependencies have recently been in-troduced to this framework [21]. Similar to SDM, the proximity divergence from randomness model (pDFR) assumes that all ad ja-cent pairs of terms are dependent. In their formulation, ter ms are scored using the PL2 scoring model, and bi-terms are scored us-ing the BiL2 scoring model. The score for each document is the weighted sum of the term and bi-term components. The authors state that other DFR models can be used to score each componen t, which we intend to investigate in future work.

In this study, we test two pDFR models. First, we investigate the model proposed by Peng et al. [21] that uses PL2 to score un -igrams, and BiL2 to score bigrams (pDFR-Bil2). We also investi-gate a variation that uses PL2 to score both unigrams and bigr ams (pDFR-PL2). BM25: BM25 [26] is an effective extension of the Binary Indepen-dence Model [27] (BIM). It is important to note that BIM makes a strong assumption of term independence [10]. Specifically, the the-oretical underpinnings of BIM, and therefore BM25, preclud ethe inclusion of term dependency information into the estimation of the probability of relevance. Even so, several heuristic retri eval models that combine BM25 components with term proximity-based fea -tures have been proposed.

Rasolofo and Savoy [24] present an version of the BM25 model that includes term dependency features. Their model (BM25-TP) extends the BM25 model with a feature that is proportional to the inverse square of the distance between each pair of queried t erms, up to a maximum distance of 5 .B X ttcheretal.[8]investigatethe optimization of this function for efficient retrieval. Svor eetal.[32] further investigates this model in comparison to other BM25-based dependency models. We note that, as proposed, BM25-TP uses all possible term pairs extracted from the query. The polyno mial growth in the number of query features, as the size of the quer y grows, makes the model infeasible for longer queries. In thi sstudy, we use the variation of BM25-TP proposed by Svore et al. [32] t hat only assumes that all sequential pairs of query terms are dep endent.
We also include a variant of this retrieval model (BM25-TP2) , also proposed by Svore et al. [32]. This model is similar to BM 25-TP, except that the score for each bi-term feature is no longe rpro-portional to the inverse squared distance between each matched term pair. Instead, the feature is scored according to the nu mber of matching instances using a BM25-like function.
Only a small number of many-term dependency models have been proposed. This, in itself, is considered evidence that many-term dependencies may be less effective than bi-term depend en-cies. In this study, we investigate two recently proposed mo dels; BM25-Span, and the positional language model (PLM). We also study variations of the SDM and WSDM-Int models that include many-term dependency features.

Song et al. [30] propose the BM25-Span model. This model as-sumes that all queried terms are dependent. They model this s ingle set of dependent terms by grouping term instances in a given d ocu-ment into spans .TheBM25-Spanmodelscoresthedetectedspans using a modified BM25 scoring function. The BM25-Span scor-ing function evaluates each span by interpolating between t he span width and number of query terms in each span.

Further extensions to this model have also been proposed. Fo r example, Svore et al. [32] extend this model with 14 new features extracted from each span. They observe significant improvem ents over the original BM25-Span model. However, many of these fe a-tures require external data sources, such as training data f or phrase, sentence detection algorithms, and various word lists. For this rea-son, we leave the investigation of this extended span-based re-trieval model to future work.
 The positional language model (PLM) was proposed by Lv and Zhai [16]. Similar to BM25-Span, this model assumes that all queried terms are dependent on each other. PLM operates by prop-agating each occurrence of each query term in the document to neighboring locations. The document is then scored at each loca-tion, or term position, in the document. Kernel functions ar eused to determine the exact propagated frequency of each term in the document to the specific position to be scored. Several methods are used for producing an aggregate score from the individual po sition scores.

In this study, we consider the two best performing variants pro-posed in the original study; the best-position strategy, us ing the Table 2: Descriptions of the potential functions used in var ious ex-tensions to SDM. Unordered window widths are held constant a t 4 times term count [19].
 Feature Description O2 / U2 Ordered / Unordered window of pairs of terms O3 / U3 Ordered / Unordered window of sets of three terms O4 / U4 Ordered / Unordered window of sets of four terms Table 3: Feature functions used by the WSDM-internal-3 retr ieval model, in addition to the parameters shown in Table 1.
 cf (#1( t 1 ,t 2 ,t 3 )) Collection frequency of trigram; t df (#1( t 1 ,t 2 ,t 3 )) Document frequency of trigram; t gaussian kernel (PLM), and the multi- X  strategy, using two gaus-sian kernels (PLM-2). The best-position strategy scores ea ch doc-ument using the score from the maximum scoring position in th e document. The multi- X  interpolates between two best-position strate-gies, with different kernel parameters. As suggested by Lv and Zhai [16], the second kernel in our PLM-2 implementation is a whol e document language model,  X  2 =  X  .Wenotethatthisparticular kernel is equivalent to the query likelihood model (QL). Bot hmod-els are implemented using Dirichlet smoothing.

We now describe many-term dependency models that are vari-ants of the SDM and WSDM models. We start by extending SDM [19] to include many-term dependencies. As mentioned previousl y, SDM is a variant of the Markov random field model that uses three ty pes of potential functions; terms, ordered windows and unorder ed win-dows.

We extend this model by adding new potential functions to the model that evaluate larger sets of dependent terms. Table 2 l ists each of the potential functions in the extended model. Each f unc-tion is computed in a similar manner to the functions in the original SDM. Note that the width of each unordered window feature is s et at four times the number of terms, as in the original study [19 ]. Various many-term dependency models are constructed by sel ect-ing different subsets of features. For example, the Uni+O23+U23 model includes the unigram function; two, bi-term function s(O2, and U2); and two, 3 term functions (O3, and U3). In these mod-els O indicates an ordered window is used, and U indicates an un-ordered window is used. In each model, each function is assoc iated with one weight parameter. All features are computed as smoo thed language model estimates, using Dirichlet smoothing. Similar to SDM, for an input query, each potential function is populate dwith all sequential sets of terms extracted from the query.

We note that each of these features is also present in the full de-pendence model (FDM) [19]. However, in FDM all ordered win-dows are weighted with the parameter  X  O ,andsimilarly,allun-ordered windows are weighted the parameter  X  U .

We also construct a variant of the WSDM-Int model that incor-porates three-term dependencies (WSDM-Int-3). WSDM-Int-3ex-tends WSDM-Int by including three-term features, similar t othe existing two-term features. The weight for each three-term depen-dency is determined using a linear combination of the featur es in Table 3. Term and bi-term features are weighted as in WSDM-In t. This extension adds three new parameters to the WSDM-Int mod el, one for each three-term weighting feature. 50 new topics.

To test the effectiveness of dependency retrieval models, w euse three TREC collections, Robust04, GOV2, and ClueWeb-09-Ca t-B. The details of these collections are provided in Table 4. B ased on results presented at TREC 2009 and TREC 2010, ClueWeb-09-Cat-B is filtered to the set of documents with spam scores i n the 60 th percentile, using the Fusion spam scores distributed with ClueWeb-09 [11]. Retrieval effectiveness results are repo rted for both the title and the description of each TREC topic. Robust -04 topics are collected from TREC Robust Track 2004. The GOV2 topics are accumulated over TREC Terabyte Tracks 2004, 2005 , and 2006. The Clueweb-09-Cat-B topics are accumulated from TREC Web Tracks 2009, 2010, and 2011. The INQUERY stop-words [9] are removed from all topics. Indexed terms are stem med using the Porter 2 stemmer. 2 All retrieval models are implemented using the Galago Search Engine 3 .

Given the limited number of queries for each collection, 5 -fold cross-validation is used to minimize over-fitting without r educ-ing the number of learning instances. Topics for each collection are randomly divided into 5 folds. The parameters for each mo del are tuned on 4 -of-5 folds. The final fold in each case is used to evaluate the optimal parameters. This process is repeated 5 times, once for each fold. For each fold, parameters are tuned using a coordinate ascent algorithm [18], using 10 random restarts. Mean average precision (MAP) is the optimized metric for all retr ieval models. Throughout this paper each displayed evaluation st atistic is the average of the five fold-level evaluation values.
For each collection, each type of query, and each retrieval m odel, we report the mean average precision (MAP), normalized disc ounted cumulative gain at rank 20 (nDCG@20), precision at rank 20 (P @20), and expected reciprocal rank at 20 (ERR@20). For each collec tion, each metric is computed over the joint results, as combined f rom the 5 test folds. Statistical differences between models are com -puted using the Fisher randomization test as suggested by Smucker et al. [28], where  X  =0 . 05 .

Due to space constraints in this paper, the full details of th equery folds, learned parameters, fold-level evaluation metrics ,andmea-sured p-values for statistical tests are published in a sepa rate tech-nical report [14].
Most of the models investigated in this study were originall ypro-posed and tested over a variety of different collections and queries, and any parameters suggested in each corresponding study ar eun-likely to be optimal for other collections. In this section, we present acasestudyontuningtheSDMparameters. MetzlerandCroft [19] suggest that the SDM parameters, (  X  T ,  X  O ,  X  U ) ,shouldbe set to (0 . 85 , 0 . 1 , 0 . 05) .Thesmoothingparameter  X  is assumed to be 2 , 500 ,thedefaultsmoothingparameterinthecanonicalimple-mentation. 4 http:/ /snowball.tartarus.org/algorithms/english/ste mmer.html http://www.lemurproject.org/galago.php
Indri Search Engine, http://www.lemurproject.org/indri .php Figure 1: Average parameter settings across the 5 tuned folds for each collection, and each type of query. Left axis indicates values for each of the  X  parameters, right axis indicates values for  X  .
The 5 -fold cross-validation method used in this study learns 5 settings for each parameter, one setting per test fold. The a verage learned parameter settings for each collection, and type of query is shown in Figure 1. This graph suggests that, with the excep -tion of  X  ,theSDMparametersarerelativelystable. Theoptimal parameters are similar for all collections and query types, and the parameters are close to the suggested default parameters. Further, we observe very low standard deviation (  X  p &lt; 0 . 02 )acrossquery folds, for each feature parameter, p ,andeachcollectionandquery set. These observations imply that the learned parameters a re sta-ble, and that optimal parameters for one collection could be used effectively on another collection.

We next investigate the retrieval performance differences between default and tuned parameters. Table 5 shows the retrieval pe rfor-mance of the default parameter settings, and the tuned param eter settings. Results over the smallest collection, Robust-04 ,donot change significantly after tuning the model parameters. Thi sis likely to be because the original paper presented results ov er some subsets of this collection. However, appropriate tuning of model parameters results in significant improvements in effectiv eness for the larger GOV2 and Clueweb-09-Cat-B collections.

We make similar observations for all tested retrieval model sthat have suggested parameter settings. These observations demonstrate that even small changes in parameter values can have a signifi cant effect on retrieval model effectiveness. Further, it is clear that if the suggested parameter values are na X vely used in a benchma rk retrieval model, its performance may be significantly dimin ished. Reduced or low performance for a benchmark method is likely t o produce erroneous conclusions about significant improvements for aproposedmodel. Whilethisisnotanewobservation[2],itd e-serves restating.
The section presents results from the systematic compariso nof bi-term dependency models. This comparison allows us, for the indicates no significant difference is observed.
  X  S  X   X   X  W W  X  first time, to determine the relative effectiveness of the di fferent de-pendency models that have been proposed. Recall that each model is evaluated using 5 -fold cross validation, with each fold tuned us-ing a coordinate ascent algorithm [18], for each collection ,andeach type of query.

AsummaryofresultsisdisplayedinTable6.Thistableshows significant differences, for the MAP metric, between each of the bi-term dependency models and SDM as the baseline model. Sig -nificant differences are evaluated using the Fisher randomi zation test and indicated in each table (  X  =0 . 05) .Detailsofoptimized results, as aggregated across query folds, for each model, e ach col-lection, and each type of query, are shown in Table 7. Signific ant improvement or degradation, relative to the query likeliho od (QL), is indicated in this table.

It is clear from this data that WSDM-Int is the most consisten tly effective bi-term dependency model. It significantly outpe rforms all other retrieval models in several settings. We also note that SDM is a very strong retrieval model, it significantly outperforms several other models on the Robust-04 and GOV2 collections, as is sho wn in Table 6. This data also shows that WSDM-Int significantly o ut-performs SDM in several settings.

All other bi-term retrieval models show some significant im-proved performance, relative to the QL baseline. Interestingly, the BM25-TP2 model does not perform well, even showing significant degradation of performance in some cases. This may be becaus e this model does not control the contribution of bi-term features us-ing a parameter, resulting in the score contribution of bi-t erms is being overvalued, relative to the contribution of terms.
These results also confirm previously published findings; bi -term models can consistently improve information retrieval on t he Robust-04 and GOV2 collections. Interestingly, the significant imp rove-ments observed on the Robust-04 and GOV2 collections, are mu ch lower on the Clueweb-09-Cat-B collection. The relatively l ow per-formance improvements with dependency models using the current Clueweb-09 queries has also been observed at TREC and in rece nt publications [4, 23]. As more queries are developed for this corpus, we plan to study this issue further.

When comparing the performance of short and long queries, we observe that the use of bi-term proximity dependencies produces much larger improvements for description topics, than for t itle top-ics, for the Robust-04 and GOV2 collections. One obvious cau se for this is that many more bi-term dependencies are extracte dfrom the longer description topics. We also observe that for the R obust-04 collection, the effectiveness of the best performing mod el on description topics, (WSDM-Int), is significantly better th an the ef-fectiveness with title topics, using the MAP and ERR@20 metr ics. For this collection, WSDM-Int is able to extract more inform ative features from the long queries compared to the associated sh ort queries.
Based on the results from the comparison of bi-term dependen cy models, we select SDM and WSDM-Int as benchmark methods against which to evaluate the benefit of many-term dependenc ies for retrieval effectiveness. We evaluate this by investiga ting three existing many-term dependency retrieval models; BM25-Spa ns, PLM, and PLM-2. We also study variants of the SDM and WSDM-Int retrieval models using many-term dependencies. We cons truct SDM variants that use dependent sets of three and four terms, as described in Section 3. Finally, we construct a variant of WS DM-Int, WSDM-Int-3 which includes three-term dependencies. N ote that for the extended SDM and WSDM-Int models, all term depen -dencies are extracted sequentially from the query. dation with respect to the query likelihood model (QL) is ind icated ( + 0.229  X  0.389 0.329 0.115 + 0.278 + 0.428 + 0.365 + 0.123 + + 0.261 0.401 + 0.484 0.161 + + 0.272 + 0.407 + 0.510 + 0.160 + + 0.266 + 0.394 + 0.486 0.155 + 0.270 + 0.403 + 0.494 + 0.161 + + 0.077 + 0.194 0.247 0.108 + 0.078 0.200 0.255 0.114 + 0.083 + 0.199 0.255 0.118 + with respect to SDM are indicated ( + /  X  ).
 0.123 + 0.297 + 0.425 0.531 0.167 0.205 0.261 0.118 indicates no significant difference is observed.

Similar to the comparison of bi-term models, each many-term dependency model is evaluated using 5 -fold cross validation, where each fold is tuned using a coordinate ascent algorithm [18]. We focus on topic descriptions in this section, as title querie sarefre-quently too short to extract dependent sets of more than two t erms.
Note that PLM and PLM-2 are only tuned for the Robust-04 col-lection. To evaluate a given document for these retrieval mo dels, for a particular query, each position in the document must be eval-uated, and the maximum score is returned. In the worst case, e very position in the collection must be scored independently. To reduce this overhead, we implement a simulated annealing algorith mto reduce the number of positions tested to locate the maximum s cor-ing position in the document. This algorithm reduced the tim eto evaluate queries by a factor of 20 ,withoutanymeasurablechange in retrieval effectiveness. However, even with this efficie ncy op-timization, and the document-length estimation optimizat ion pre-sented by Lv and Zhai [16], tuning the parameters of this retr ieval model using coordinate ascent over larger collections rema ins in-feasible. In lieu of tuned optimal parameters, we report res ults for GOV2, and Clueweb-09-Cat-B using the optimal parameters fr om Robust-04.

The results from these experiments are displayed in Table 8. Sig-nificant differences are indicated in the table with respect to the SDM benchmark. We also show significance testing with respec t to the WSDM-Int benchmark in Table 9.

We observe that many-term dependency models do not consis-tently improve retrieval performance over bi-term models. Small improvements can be observed in some cases. WSDM-Int-3, in particular, shows significant improvements over benchmark models for the Robust-04 collection. Investigation of the perform ance of WSDM-Int-3 for the GOV2 and Clueweb-09-B collections shows some evidence that the model is overfitting to the 4 folds of train-ing data, thereby reducing performance on the test fold. How ever, in general, many-term features do not improve aggregate ret rieval metrics with respect to the benchmark bi-term dependency mo dels.
Figure 2 shows two per-query result graphs, one comparing SD M with two variant many-term models, and one comparing WSDM-Int to WSDM-Int-3. Data shown is only for the topic descripti ons from the Robust-04 collection. Similar trends were observe dfor the other two collections.

This data shows that each of these models improves and degrad es different queries, resulting in similar average performance. It also suggests that many-term proximity features may still be abl eto significantly improve retrieval performance. However, opt imizing the use of many-term dependencies may require a more selecti ve approach to the generation of sets of dependent terms, or to t he selection of model features for an input query. Further, the avail-ability of more training data, such as a large query click log ,would be likely to make a significant difference to our results.
Future work will include the relaxation of some of the restri c-tions we made to do comparisons between models. This will in-clude studying the interactions between dependency models and methods of identifying groups of dependent terms from the qu ery; methods of exploiting document structures; and the incorpo ration of information from external data sources. Each of these techniques have been demonstrated to improve retrieval over a variety o fdif-ferent baseline bag-of-words models, missing from the literature is astudyofanyrelativeimprovementsthesetechniquesmaypr ovide to strong benchmark dependency models.

We have already started the investigation into the utility o fexter-nal data sources. There is some evidence that external data sources can help significantly improve ad-hoc retrieval performanc e. For example, 6 of the 7 top performing models at the TREC 2012 Web Track use external data sources.

As originally proposed, WSDM [6] uses three external data sources; Wikipedia titles, the MSN query log, and Google n-g rams. This model uses these features to determine term and window s pe-cific weights. By design, WSDM allows the inclusion of arbitr ary external features into the weighting of each term and window ,mak-ing this model appropriate for further investigation of any benefit of external data sources for dependency models.

Table 10 shows a comparison between WSDM, and one of the strongest performing dependency models, as determined in this paper, WSDM-Int. We test both models for each collection and query type, where both models are tuned using 5 fold cross vali-dation. We observe that the external data sources used in WSD M can significantly improve the performance of this model. How ever, the improvements are relatively small. Future work will inc lude the isolation and identification of the external features th at provide the largest benefits for this model, and investigate alternative data sources.
In this study, we perform a systematic comparison of the retr ieval effectiveness of state-of-the-art bi-term dependency mod els. We also present a comparison of many-term dependency models, u sing the best bi-term models as benchmarks. Additionally, we pro vide tuned parameters for a wide range of popular dependency mode ls, for three standard test collections.

The retrieval models investigated in this study were selected to make the comparison as fair as possible. We restrict the comp arison to models that use proximity-based dependencies between se quen-tially extracted sets of queried terms, that do not require e xternal data sources, and do not require the use of pseudo-relevance feed-back algorithms.

Our results support the well-established finding that bi-te rm de-pendency models can consistently outperform bag-of-words mod-els. We observe that dependency models produce the largest i m-provements over bag-of-words models on longer queries. For the first time, we can also make some conclusions on the relative e f-fectiveness of different dependency models. The best performing bi-term model, given the restrictions applied, is a variant of the weighted sequential dependence model. Due to our extensive pa-rameter tuning on all models, the differences between model scan be small, but they are significant. Given that all models make use of similar proximity features, it appears that it does make a di fference exactly how those features are incorporated into the model. Our experiments also show that many-term dependency models do n ot consistently outperform bi-term models. However, these mo dels do perform better on one of the collections (Robust-04) and per -query analysis shows that many-term proximity features have the p oten-tial to improve retrieval performance, if used in a more sele ctive manner. Demonstrating the effectiveness of these features will, we believe, require larger query sets than are available in TRE Ccol-lections.
 This work was supported in part by the Center for Intelligent Infor-mation Retrieval and in part by NSF grant #CNS-0934322. We would also like to thank Michael Bendersky for several helpf ul technical discussions. Any opinions, findings and conclusi ons or recommendations expressed in this material are those of the au-thors and do not necessarily reflect those of the sponsor. [1] Gianni Amati and C. J. Van Rijsbergen. Probabilistic mod els [2] Timothy G. Armstrong, Alistair Moffat, William Webber, and [3] Michael Bendersky and W. Bruce Croft. Discovering key [4] Michael Bendersky and W. Bruce Croft. Modeling [5] Michael Bendersky, W. Bruce Croft, and David A. Smith. [6] Michael Bendersky, Donald Metzler, and W. Bruce Croft. [7] Shane Bergsma and Qin Iris Wang. Learning noun phrase [8] Stefan B X ttcher, Charles L. A. Clarke, and Brad Lushman. [9] James P. Callan, W. Bruce Croft, and John Broglio. TREC an d + 0.435 + 0.383 0.119 + 0.426 0.536 0.165 [10] William S. Cooper. Some inconsistencies and misnomers in [11] Gordon V. Cormack, Mark D. Smucker, and Charles L. A. [12] W Bruce Croft, Donald Metzler, and Trevor Strohman. Search [13] Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Guihong [14] Samuel Huston and W. Bruce Croft. Parameters learned in the [15] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley [16] Y. Lv and C.X. Zhai. Positional language models for [17] K. Tamsin Maxwell and W. Bruce Croft. Compact query term [18] Donald Metzler. Using gradient descent to optimize lan guage [19] Donald Metzler and W. Bruce Croft. A Markov random field [20] Jae Hyun Park, W. Bruce Croft, and David A. Smith. A [21] Jie Peng, Craig Macdonald, Ben He, Vassilis Plachouras ,and [22] Jay M. Ponte and W. Bruce Croft. A language modeling [23] Fiana Raiber and Oren Kurland. Ranking document clusters [24] Yves Rasolofo and Jacques Savoy. Term proximity scorin g [25] Knut Magne Risvik, Tomasz Mikolajewski, and Peter Boro s. [26] S. E. Robertson and S. Walker. Some simple effective [27] Stephen E Robertson and K Sparck Jones. Relevance [28] Mark D. Smucker, James Allan, and Ben Carterette. A [29] Fei Song and W. Bruce Croft. A general language model for [30] Ruihua Song, Michael J. Taylor, Ji-Rong Wen, Hsiao-Wue n [31] Munirathnam Srikanth and Rohini Srihari. Incorporati ng [32] Krysta M. Svore, Pallika H. Kanani, and Nazan Khan. How [33] Tao Tao and ChengXiang Zhai. An exploration of proximit y
