 Controlling the dissemination of an entity (e.g., meme, virus, etc) on a large graph is an interesting problem in many disciplines. Ex-amples include epidemiology, computer security, marketing, etc. So far, previous studies have mostly focused on removing or inoc-ulating nodes to achieve the desired outcome.

We shift the problem to the level of edges and ask: which edges should we add or delete in order to speed-up or contain a dissem-ination? First, we propose effective and scalable algorithms to solve these dissemination problems. Second, we conduct a theo-retical study of the two problems and our methods, including the hardness of the problem, the accuracy and complexity of our meth-ods, and the equivalence between the different strategies and prob-lems. Third and lastly, we conduct experiments on real topologies of varying sizes to demonstrate the effectiveness and scalability of our approaches.
 H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithm, experimentation edge manipulation, immunization, scalability, graph mining
Managing the dissemination of an entity (e.g., meme, virus, etc) on a large graph is a challenging problem with applications in vari-ous settings and disciplines. In its generality, the propagating entity can be many different things, such as a meme, a virus, an idea, a new product, etc. The propagation is affected by the topology and the properties of the entity: its  X  X irality X , its speed, its  X  X tickiness X  or the duration of the infection of a node. Our focus here is the topology , since we assume that we cannot alter the properties of the propagating entity.

The problem we address is how we can affect the propagation by modifying the edges of the graph. In fact, we address two different problems. First, in the NetMelt problem , we want to contain the dissemination by removing a given number of edges. For example, we can consider the distribution of malware over a social network. Deleting user accounts may not be desirable, but deleting edges ( X  X nfriending X  people) may be more acceptable. More specifically, we want to delete a set of k edges from the graph to minimize the infected population. Second, in the NetGel problem ,wewant to enable the dissemination by adding a given number of edges. Specifically, we want to add a set of k new edges into the graph to maximize the population that adopt the information. For example, we could extend the social network scenario using the recent  X  X rab spring X  which often used Facebook and Twitter for coordinating events: we may want to maximize the spread of a potential piece of information. Note that an additional, key requirement for both problems is computational efficiency: the solution should scale to large graphs.

Both problems are challenging for slightly different reasons. For the NetMelt problem, most of the existing methods operate on the node-level , e.g., deleting a subset of the nodes from the graph to minimize the infected population from a propagating virus. In the above social spam example, this means that we might have to shut-down some legitimate user accounts. Can we avoid this by op-between users to slow down the social spam spreading? For the NetGel problem, things are even more challenging because of its high intrinsic time complexity. Let n be the number of the nodes in the graph. There are almost n 2 non-existing edges since many real graphs are very sparse. In other words, even if we only want to add one single new edge into the graph, the solution space is O ( n 2 ) . This complexity  X  X xplodes X  if we aim to add multiple new edges collectively, where the solution space becomes exponential . To date, there does not exist any scalable solution for the NetGel problem.

The overarching contribution of this paper is the formulation and theoretical study of the dissemination management via edge manip-ulation: how to place a set of edges 1 to achieve the desired outcome. The main contributions of the paper can be summarized as follows: Figure 1: Comparison of maximizing the outcome of the in-formation dissemination process. Larger is better. The pro-posed method (red) leads to the largest number of  X  X nfected X  nodes (e.g., having more people in the social networks to adopt a piece of good idea, etc). Notice that all the alternative methods are mixed with the result on the original graph (yellow), which means that they fail to affect the outcome of the dissemination process. See Section 6 for detailed experimental setting.
The rest of the paper is organized as follows. We introduce nota-tion and formally define the NetGel and NetMelt problems in Sec-tion 2. We present and analyze the proposed algorithms in Section 3 and Section 4, respectively. We provide experimental evaluations in Section 5. We review the related work in Section 6 and conclude in Section 7.
Table 1 lists the main symbols used throughout the paper. We consider directed, irreducible unipartite graphs. For ease of pre-sentation, we discuss the unweighted graph scenario although the algorithms we propose can be naturally generalized to the weighted case. We represent a graph by its adjacency matrix. Following the standard notation, we use bold upper-case for matrices (e.g., bold lower-case for vectors (e.g., a ), and calligraphic fonts for sets (e.g., I ). We denote the transpose with a prime (i.e., transpose of A ). Also, we represent the elements in a matrix us-ing a convention similar to Matlab, e.g., A ( i, j ) is the element at In this paper, we use the terms  X  X ink X  and  X  X dge X  interchangeably. the i th row and j th column of the matrix A ,and A (: ,j column of A ,etc.

When we discuss the relationship between the two different strate-gies (node deletion vs. edge deletion) for the NetMelt problem, it is helpful to introduce the concept of line graph, where the nodes represent the edges in the origin al graph. Formally, each edge in the original graph A becomes a node in the line graph L ( A ) there is an edge from one node to the other in the line graph if the target of the former edge is the same as the source of the latter edge in the original graph A . It is formally defined as follows:
D EFINITION 1( L INE G RAPH ). Given a directed graph A its directed line graph L ( A ) is a graph such that each node of L ( A ) represents an edge of A , and there is an edge from a node e to e 2 in L ( A ) iff for the corresponding edges i 1 ,j 1 in A , j 1 = i 2 .

With the notation of the line graph L ( A ) , we have two equivalent ways to represent an edge. Let e x ( e x =1 , ..., m ) be the index of represent the edge e x by the pair of its source and target nodes in the original graph A : i x ,j x , i.e., the edge e x starts with the node i and ends at node j x .

In order to design an effective strategy to optimize the graph structure to affect the outcome of an information dissemination pro-cess, we need to answer the following three questions. (1) (Key graph parameters/metrics) What are key graph metrics/parameters that determine/control the dissemination process? (2) (Graph oper-ations) What types of graph operations (e.g., deleting nodes/edges, adding edges, etc) are we allowed to change the graph structure? (3) (Affecting algorithms) For a given graph operation, how can we design effective, scalable algorithms to optimize the corresponding key graph parameters?
For information dissemination on real graphs, a major finding [41, 33] is that, for a large family of dissemination processes, the largest (in module) eigenvalue  X  of the adjacency matrix A or an appropri-ately defined system matrix is the only graph parameter that deter-mines the tipping point of the dissemination process, i.e., whether or not the dissemination will become an epidemic (see Section 6 for a review of related work). In principle, this gives a clear guid-ance on the algorithmic side, that is, an ideal, optimal strategy to affect the outcome of the information dissemination process should change the graph structure so that the leading eigenvalue  X  is min-imized or maximized .

Based on this observation, now we can transform the original problem of affecting the dissemination process to the eigenvalue optimization problem ,thatis, (1) minimize the leading eigenvalue  X  for NetMelt ; (2) maximize the leading eigenvalue  X  for NetGel .

In this paper, we focus on operating on the edge-level to design affecting algorithms. With the above notation, our problems can be formally defined as the following two sub-problems: P ROBLEM 1. NetMelt (Edge Deletion) Given: Alarge n  X  n graph A and an integer (budget) k ; Output: Asetof k edges from A whose deletion from A creates P ROBLEM 2. NetGel (Edge Addition) Given: Alarge n  X  n graph A and an integer (budget) k ; Find: Asetof k non-edges of A whose addition to A creates the
As we will show soon, both problems are combinatorial.
In this section, we address the NetMelt problem (Prob. 1), that is, to delete k edges from the original graph A so that its leading eigenvalue  X  will decrease as much as possible. We first study the relationship between two different strategies (edge deletion vs. node deletion), and then present our algorithm, followed by the analysis of its effectiveness as well as efficiency.
Roughly speaking, in the NetMelt Problem (Edge Deletion), we want to find a set of k  X  X mportant X  edges from the graph A With the notation of the line graph L ( A ) , intuitively, such  X  X mpor-tant X  edges in A might become  X  X mportant X  nodes in the line graph L ( A ) . In this section, we briefly present the relationship between these two strategies (node deletion vs. edge deletion).

Our main result is summarized in Lemma 1, which says that the eigenvalues of the original graph A are also the eigenvalues of its line graph L ( A ) .

L EMMA 1. Line Graph Spectrum. Let  X  be an eigenvalue of the graph A .Then  X  is also the eigenvalue of the line graph L P ROOF . Omitted for brevity.

By Lemma 1, it seems that edge deletion (Prob. 1) can be trans-formed to the node deletion problem on the line graph  X  that is, select a subset of k nodes from the line graph L ( A ) whose dele-tion creates the largest decrease in terms of the leading eigenvalue of L ( A ) . However, by the following lemma, the node deletion problem itself is still a challenging task.

L EMMA 2. Hardness of Node Deletion. It is NP-Complete to find a set of k nodes from a graph A , whose deletion will create the largest decrease of the largest eigenvalue of the graph A
P ROOF . The proof can be done by the reduction from the inde-pendent node set problem, which is known to be NP-Complete [17]. The detailed proof is omitted for brevity.
 That said, we seek an effective algorithm that directly solves the NetMelt problem next.
The key to solving Prob. 1 ( NetMelt ) is to quantify the impact of deleting a set of edges in terms of the leading eigenvalue  X  .The naive way is to recompute the leading eigenvalue  X  after deleting the corresponding set of edges -the smaller the new eigenvalue, the better the subset of the edges. But it is computationally infeasible for large graphs since it takes O ( m ) time for each of the sible sets, as in general, the impact for a given set of the edges (in terms of decreasing the leading eigenvalue  X  )is not equal to the summation of the impact of deleting each individual edge.
Let u and v be the leading left eigenvector and right eigenvector of the graph A , respectively. Intuitively, the left eigen-score and the right eigen-score v ( j ) ( i, j =1 , ..., n ) provide some im-portance measure for the corresponding nodes i and j .Thecore idea of the proposed K-E DGE D ELETION algorithm is to quantify the impact of each edge by the corresponding left and right eigen-scores independently (step 9) . Our upcoming analysis in the next subsection shows that this strategy (1) leads to a good approxima-tion of the actual impact wrt decreasing the leading eigenvalue; and (2) naturally de-couples the dependence among the different edges. As a result, we can avoid the combinatorial enumeration in Prob. 1 by picking the top-k edges with the highest individual impact scores (step 9).

Note that steps 2-7 in Alg. 1 are to ensure that all the eigen-ing to the Perron-Frobenius theorem [10], such eigenvectors v always exist.
 Algorithm 1 K-E DGE D ELETION Input: the adjacency matrix A and the budget k Output: k edges 1: compute the leading eigenvalue  X  of A ;let u and v 3: assign u  X  X  X  u 4: end if 6: assign v  X  X  X  v 7: end if 8: for each edge e x :( i x ,j x ) e x =1 , ..., m ; i x ,j 9: score ( e x )= u ( i x ) v ( j x ) ; 10: end for 11: return top-k edges with the highest score ( e x ) Here, we analyze the accuracy and the efficiency of the proposed K-E DGE D ELETION algorithm.

The accuracy of the proposed K-E DGE D ELETION is summa-rized in Lemma 3. According to Lemma 3, the first-order matrix perturbation theory, together with the fact that many real graphs have large eigen-gap, provides a good approximation to the impact of a set of edges in terms of decreasing the leading eigenvalue. What is more important, with such an approximation, the impact of the different edges are now de-coupled from each other. Therefore, we can avoid the combinatorial enumeration of Prob. 1 by simply returning the top-k edges with the highest individual impact scores (step9inAlg.1).

Notice that by Lemma 3, there is an O ( k ) gap between the ap-proximate and the actual impact of a set of edges in terms of de-creasing the leading eigenvalue. Our experimental evaluations show that the correlation between the approximate and the actual impact is very high (See Section 6 for details), indicating that it indeed pro-vides a good approximation for the actual decrease of the leading eigenvalue.

L EMMA 3. Let  X   X  be the (exact) first eigenvalue of  X  A ,where is the perturbed version of A by removing all of its edges indexed by the set S .Let  X  =  X   X   X  2 be the eigen-gap of the matrix where  X  2 is the second eigenvalue of A , and c =1 / ( u v ) is the simple first eigenvalue of A , and  X   X  2 c
P P ROOF .Let  X  i ( i =1 , ..., n ) be the ordered eigenvalues of (i.e., |  X  | = |  X  1 | X |  X  2 | ...  X |  X  n | ). Let  X   X  i responding eigenvalues of  X  A . Notice that we omitted the subscripts for the leading eigenvalues (i.e.,  X  1 =  X  ,and  X   X  1 =  X 
Let  X  A = A + E .Wehave E Fro =
According to the first-order matrix perturbation theory (p.183 [38]), we have
Next, we will show that  X   X  1 is indeed the leading eigenvalue of To this end, again by the matrix perturbation theory (p.203 [38]), we have
Since  X  =  X  1  X   X  2  X  2 other words, we have that  X   X  1 =  X   X  is the leading eigenvalue of Therefore, which completes the proof.

The efficiency of the proposed K-E DGE D ELETION is summa-rized in the following lemma, which says that with a fixed budget k ,K-E DGE D ELETION is linear wrt the size of the graph for both time and space cost.

L EMMA 4. Efficiency of K-E DGE D ELETION . The time cost of Alg. 1 is O ( mk + n ) . The space cost of Alg. 1 is O P
ROOF . Using the power method, step 1 takes O ( m ) time. Steps 2-7 take O ( n ) time. Steps 8-10 take O ( m ) time. Step 11 takes O ( mk ) time. Therefore, the overall time complexity of Alg. 1 is O ( mk + n ) , which completes the proof of the time cost.

We need O ( m ) to store the original graph A . It takes O and O (1) to store the eigenvectors and eigenvalue, respectively. We need additional O ( m ) to store the scores (Step 9) for all the edges. Finally, it takes O ( k ) for the selected k edges. Therefore, the overall space complexity of Alg. 1 is O ( m + n + k ) completes the proof of the space cost.
In this Section, we address the NetGel problem (Prob. 2), where we want to add a set of new links into the graph A so that its leading eigenvalue  X  will increase as much as possible. We first present the proposed K-E DGE A DDITION algorithm, and then analyze its accuracy as well as efficiency.
Let T be a set of non-existing edges in A , that is, for each e x : i x ,j x  X  X  ,wehave A ( i x ,j x )=0 .Let ing eigenvalue of the new adjacency matrix  X  A by introducing the new edges indexed by the set T . By the similar procedure as in the proof of Lemma 3, we can show that the impact of the new set of edges T in terms of increasing the leading eigenvalue  X   X  approximated as Therefore, it seems that we could use a similar procedure as K-E DGE D ELETION to solve the NetGel problem (referred to as  X  Naive-Add  X ): for each non-existing edge e x : i x ,j x edges with the highest scores.

However, many real graphs are very sparse, i.e., m&lt;&lt;n Therefore, we have O ( n 2  X  m )  X  O ( n 2 ) possible non-existing edges. In other words, Naive-Add requires quasi-quadratic time wrt the number of the nodes ( n ) in the graph, which does not scale to large graphs.

To address this issue, we propose an efficient algorithm, which is summarized in Alg 2. The core idea of K-E DGE A DDITION to prune a large portion of the non-existing edge pairs based on their left and right eigen-scores. As in Alg. 1, we take the same procedure to make sure that the left and right eigenvectors ( are non-negative. We omit these steps in Alg 2 for brevity. Algorithm 2 K-E DGE A DDITION Input: the adjacency matrix A and the budget k Output: k non-existing edges 1: compute the left ( u ) and right ( v ) eigenvectors of 2: calculate the maximum in-degree ( d in ) and out-degree ( d 3: find the subset of k + d in nodes with the highest left eigen-4: find the subset of k + d out nodes with the highest right eigen-5: for each edge e x : i x ,j x i x  X  X  ,j x  X  X  , A ( i x ,i 6: score ( e x )= u ( i x ) v ( j x ) . Index them by P ; 7: end for 8: return top-k non-existing edges with the highest scores among Here, we analyze the accuracy and efficiency of the proposed
The accuracy of the proposed K-E DGE A DDITION is summa-rized in Lemma 5, which says that K-E DGE A DDITION selects the same set of edges as Naive-Add .

L EMMA 5. Effectiveness of K-E DGE A DDITION . Alg. 2 out-puts the same set of non-existing edges as Naive-Add .
 P ROOF . Omitted for brevity.

The efficiency of the proposed K-E DGE A DDITION is summa-rized in the following lemma.

L EMMA 6. Efficiency of K-E DGE A DDITION . The time cost of Alg. 2 is O ( m + nt + kt 2 ) . The space cost of Alg. 2 is O m + t 2 ) ,where t = max ( k, d in ,d out ) . P
ROOF : Using the power method, step 1 takes O ( m ) time. Step 2 takes O ( m + n ) time. Steps 3-4 take O ( n ( d in + k )) k )) time respectively, both of which can be written as O ( 5-7 take O (( k + d in )( k + d out )) = O ( t 2 ) time. Step 8 takes O (( k + d in )( k + d out ) k )= O ( kt 2 ) . Therefore, the overall time cost is O ( m + nt + kt 2 ) , which completes the proof of the time complexity.

We need O ( m ) to store the original graph A . It takes O store the eigenvectors u and v . Step 2 takes additional O space. Steps 3-4 take O ( d in + k ) and O ( d out + k ) tively, both of which can be simplified as O ( t ) . Steps 5-7 take at most O (( k + d in )( k + d out )) = O ( t 2 ) space. Step 9 takes O space. Therefore, the overall space cost (by omitting the smaller terms) is O ( m + nt + kt 2 ) , which completes the proof of the space complexity. In this section, we provide empirical evaluations for the proposed K-E DGE D ELETION and K-E DGE A DDITION algorithms. Our eval-uations mainly focus on (1) the effectiveness and (2) the efficiency of the proposed algorithms.
Data sets . We used a popular set of real graphs for our ex-periments -the Oregon AS (Autonomous System) router graphs, which are AS-level connectivity networks inferred from Oregon route-views 2 . These were collected once a week, for 9 consecu-tive weeks. Table 2 summarizes the nine graphs we used in our evaluations.

Evaluation criteria . As mentioned before, the leading eigenvalue  X  of the graph is the only graph parameter that determines the epi-demic threshold for a large family of information dissemination processes. Therefore, we report t he change of the leading eigen-value for the effectiveness comparison -for both NetMelt and Net-Gel problems. A larger change of the leading eigenvalue is better, which suggests that we can affect the outcome of the dissemination process more. In addition, we also run virus propagation simula-tions to compare how different methods affect the actual outcome of the propagation. For the computational cost and scalability, we report the wall-clock time.

Machine configurations . All the experiments ran on the same machine with four 2.4GHz AMD CPUs and 48GB memory, run-ning Linux (2.6 kernel). Approximation Quality . For both K-E DGE D ELETION and K-E
DGE A DDITION , we want to approximate the actual change of the leading eigenvalue by the first order matrix perturbation the-ory. This is the only place we introduce the approximation. By Lemma 3, it says that the quality of such an approximation de-pends on both the budget k as well as the eigengap of the orig-inal graph, with an O ( k ) gap. Here, let us experimentally eval-uate how good this approximation is on real graphs. We com-pute the linear correlation coefficient between the actual and ap-proximate leading eigenvalue after we randomly remove k ( k 10 , 50 , 100 , 500 , 1000 ) edges. The results are shown in table 3. It can be seen that the approximation is very good -in all the cases, the linear correlation coefficient is greater than 0 . 92 very close to 1 .

The Impact of Decreasing the Leading Eigenvalue . Here, we evaluate the effectiveness of the proposed K-E DGE D ELETION http://topology.eecs.umich.edu/data.html terms of decreasing the leading eigenvalue  X  of the graph. Lemma 1 suggests that the  X  X mportant X  edges on the original graph become  X  X mportant X  nodes on the line graph L ( A ) . We follow this intuition to design the following comparative strategies: (1) ran-domly select k edges from the original graph A (referred to as  X  X and X ); (2) select k edges with the highest degrees in the line graph L ( A ) (referred to as  X  X ine-Deg X ); (3) select k edges with the highest eigen-scores in the line graph L ( A ) (referred to as  X  X ine-Eig X ); and (4) select k edges with the highest PageRank scores in the line graph L ( A ) (referred to as  X  X ine-Page X ). For  X  X and X , we run the experiments 100 times and report the average result. For  X  X ine-Deg X , we have two variants by using out-degree or in-degree. In our evaluation, we found that these two variants give the similar results. Therefore, we only report the results by out-degree. For the same reason, we only report the results by the right eigen-scores for  X  X ine-Eigs X . For  X  X ine-Page X , there is an additional parameter of the teleport probability. We run the experiments with the different teleport probabilities and report the best results.

For brevity, we only present the results on Oregon-A , Oregon-B and Oregon-C since the results on the rest six graphs are similar. From Fig. 2, it can be seen that our K-E DGE D ELETION leads to the biggest decrease in terms of the leading eigenvalue. For example, on Oregon-C graph, the proposed K-E DGE D ELETION decreases the leading eigenvalue by 3.8 with the budget k which is almost double of the second best method (e.g., 2.0 by  X  X ine-Deg X ). Therefore, we expect that K-E DGE D ELETION would affect the outcome of the dissemination processes better than the alternative choices, e.g., having less number of infected nodes in the graph, etc. We validate this next.

Affecting Virus Propagation . Next, we evaluate the effectiveness of the proposed K-E DGE D ELETION in terms of minimizing the outcome of the information dissemination processes. To this end, we simulate the virus propagation for the SIS model (susceptible-infective-susceptible) on the graph [41]. For each method, we delete k = 200 edges from the original graph. Let s =  X b/d be the nor-malized virus strength (bigger s means stronger virus), where b and d are the infection rate and death rate, respectively. The results are presented in Fig. 3, which is averaged over 1,000 runs. It can be seen that the proposed K-E DGE D ELETION is always the best -its curve is always the lowest which means that we always, as desired, have the least number of infected nodes in the graph with this strat-egy. In Fig. 3,  X  X riginal X  (the yellow curve) means that we simulate the virus propagation on the original graph without deleting any edges. Notice that when the virus becomes stronger (Fig. 3(b)), all the curves except the proposed method mix with  X  X riginal X , which means that they all fail to affect the virus propagation in this case. In contrast, our propos ed method (the red curve) can still signifi-cantly reduce the number of infected nodes.

Node Deletion vs. Edge Deletion . Finally, in some applications, e.g., to stop malware propagation on the computer networks, both node deletion (e.g., shu tting down some machin es) and edge dele-tion (e.g., blocking some links between machines) are feasible. In this case, we want to know which strategy (node deletion or edge deletion) is more effective in affecting the outcome of such propa-gation process. To this end, we use an effective node immuniza-tion algorithm [39] to delete  X  k =1 , 10 nodes respectively (re-ferred to as  X  X ode-Del X ). For each  X  k , we then use our proposed K-E DGE D ELETION to delete the same amount of edges from the original graph (referred to as  X  X dge-Del X ). We compare the de-crease of the leading eigenvalues of the two methods. The results are summarized in Fig. 4. It can be seen that  X  X dge-Del X  always leads to a bigger decrease of the leading eigenvalue -which sug-gests that by operating on the edge level, we can design a more Oregon-A 0.999 0.997 0.995 0.973 0.924 Oregon-B 0.999 0.999 0.998 0.993 0.988 Oregon-C 1.000 0.999 0.999 0.996 0.991 Oregon-D 0.999 0.999 0.999 0.994 0.988 Oregon-E 1.000 0.999 0.999 0.998 0.995 Oregon-F 1.000 0.999 0.999 0.998 0.997 Oregon-G 1.000 0.999 0.999 0.999 0.998 Oregon-H 1.000 1.000 0.999 0.999 0.999 to the biggest decrease of the leading eigenvalue. effective algorithm with the same budget to affect the outcome of the information dissemination process. The results are consistent nodes, which the node immunization algorithm aims to delete, are also  X  X mportant X  (e.g., many edges adjacent to an  X  X mportant X  node might link to/from some degree-1 nodes). In other words, edge deletion enables us to optimize the underlying graph structure on a finer granularity by picking each individual edge one by one.
To our best knowledge, there are no existing methods to add k new links into an existing graph in order to increase its leading eigenvalue. Let  X  A be the complementary graph of A , which has the same node set as A ,and  X  A ( i, j )=1 iff A ( i, j )=0 the notation of the complementary graph, we use the following intuition to design the comparative methods: to select k  X  X mpor-tant X  edges from the complementary graph  X  A and add them into the original graph A . More specifically, we compare the proposed K-E DGE A DDITION with the following strategies: (1) randomly se-lect k edges (referred to as  X  X and X ); (2) select k edges with the highest out-degrees in the line graph of the complementary graph  X  A (referred to as  X  X ompDeg X ); (3) select k edges with the high-est right eigen-scores in the line graph of the complementary graph  X  A (referred to as  X  X ompEigs X ); (4) select k edges with the high-est PageRank scores in the line graph of the complementary graph  X  A (referred to as  X  X ompPage X ); and (5) select k edges by running K-E DGE D ELETION in the complementary graph  X  A (referred to as  X  X ompDelete X ). Again, for  X  X and X , we run the experiments 100 times and report the average result. We only report the results of  X  X ompDeg X  by out-degree and those of  X  X ompEig X  by right eigen-scores, respectively, since the other variants give the similar perfor-mance. For  X  X ompPage X , we run the experiments with the different teleport probabilities and report the best results.

The Impact of Increasing the Leading Eigenvalue .Wefirsteval-uate the effectiveness of the proposed K-E DGE A DDITION in terms of increasing the leading eigenvalue of the graph. For brevity, we only present the results on Oregon-A , Oregon-B and Oregon-C since the results on the rest of the graphs are similar. From Fig. 5, it can be seen that the proposed K-E DGE A DDITION always leads to the biggest increase in terms of the leading eigenvalue of the graph. Notice that for all the comparative methods, they behave like  X  X and X  (blue curve), especially when the budget k is small.
Affecting Virus Propagation . We also evaluated the effective-ness of the proposed K-E DGE A DDITION in terms of maximizing the outcome of the information dissemination process. To this end, again, we simulate the virus propagation for the SIS model on the graph. For each method, we add k = 200 new edges into the graph. Again, let s =  X b/d be the normalized virus strength, with bigger s being stronger virus. Here, our goal is to increase the number of  X  X nfected X  nodes (e.g., having more people in the so-cial networks to adopt a piece of good idea, etc) by introducing a set of new links into the graph. The result is presented in Fig. 6, which is averaged over 1,000 runs. It can be seen that the pro-posed K-E DGE A DDITION is always the best -its curve is always the highest which means that we always have the largest number of  X  X nfected X  nodes in the graph with this strategy. Notice that when the strength of the virus is weak (Fig. 6(a)), all the curves except the proposed method mix with or are very close to  X  X riginal X  (yel-low curve), which means that t hey have little impact to boost the outcome of the propagation in this case. In contrast, our proposed method (the red curve) can still significantly increase the number of  X  X nfected X  nodes. Therefore, we conclude that our proposed K-scale. E
DGE A DDITION is much more effective to guild the outcome of the dissemination process.
We use the subsets of the largest data set Oregon-I to evaluate the scalability of the p roposed algorithms. The results are pre-sented in Fig. 7. We can see that the proposed K-E DGE D ELETION and K-E DGE A DDITION scale almost near-linearly wrt m ,which means that they are suitable for large graphs. Notice that for both cases, we also observe a slight super-linear trend. This is due to the following two reasons: (1) for both K-E DGE D ELETION E
DGE A DDITION , we use the power method to compute the leading eigenvalue and the corresponding eigenvectors. When m increases, the actually iteration number in the power method also tends to in-crease; (2) for K-E DGE A DDITION when m increases, the max-imum degree (max ( d in ,d out ) ) also increases even though we fix the number of the nodes ( n ).
In this section, we review the related work, which can be cate-gorized into three parts: information dissemination, affecting algo-rithms and node/edge importance measure.

Information Dissemination . Many research works in virus prop-agation have been devoted to studying the so-called epidemic thresh-old, that is, to determine the c ondition under which an epidemic will break out. While earlier works [13] focus on some specific types of graph structure (e.g., random graphs, power-law graphs, etc), Wang et al. [41] and its follow-up paper by Ganesh et al. [8] found that, for the flu-like SIS model, the epidemic threshold for any arbitrary, real graph is determined by the leading eigenvalue of the adjacency matrix of the graph. Prakash et. al. [33] further dis-covered that the leading eigenvalue (and a model-dependent con-stant) is the only parameter that determines the epidemic threshold for all virus propagation models (more than 25 models, including H.I.V.) in the standard literature. In this work, we aim to take one step further, i.e., how to optimize (minimize or maximize) the lead-ing eigenvalue of the graph by deleting or adding a set of links.
There are also many research interest in studying other types of information dissemination processes on large graphs, including (a) information cascades [1, 9], (b) blog propagations [24, 11, 21, 35], and (c) viral marketing and product penetration [18, 23].
Affecting Algorithms . Hayashi et al. [12] derived the extinc-tion conditions under random and targeted immunization for the SHIR model (Susceptible, Hidden, Infectious, Recovered). Tong et scale. al. [39] proposed an effective node immunization strategy for the SIS model by approximately minimizing the leading eigenvalue. Briesemeister et al. [3] studied the defending policy in power-law graphs. Prakash et. al. [34, 40] proposed effective algorithms to perform node immunization on time-varying graphs. Other algo-rithms to affect the outcome of the information dissemination in-clude the influence maximization [18, 6, 5], finding effectors in social networks [22], etc. Notice that all these works focus on op-nodes) to affect the outcome of the dissemination. In contrast, we study the equally important, but much less studied affecting algo-rithms by operating on the edge level.

There exist some empirical evaluations on edge removal strate-gies for slightly different purposes, such as, slowing down the in-fluenza spreading [26], minimizing the average infection probabil-ity [36], evaluating and comparing the attack vulnerability [14], etc. The closest related work to our K-E DGE D ELETION algorithm is [2], which proposed a convex optimization based approach to approximately minimize the leading eigenvalue of the graph. How-ever, the method is based on semi-definite programming and does not scale to large graphs. Moreover, for all these methods, it re-mains unclear if they can be generalized to address the even more challenging NetGel problem, where we want to add new edges to promote the information dissemination.

Measuring the Importance of Nodes and Edges. In the liter-ature, there are a lot of node importance measurements, including betweenness centrality, both the one based on the shortest path [7] and the one based on random walks [29, 16] PageRank [30], HITS [19], and coreness score [28]. Our work is also related to the so-called k-vital edges problem, which aims to delete a set of links from the graphs to increase the shortest path length [25] or the weight of the minimum spanning tree of the remaining graph [37]. K-vital edge problem itself is known to be NP-Hard. Other remotely re-lated work includes graph augmentation [31, 4], graph sparsifica-tion [20], network inhibition [32] and network-interdiction [42, 15]. Both network inhibition and network interdiction are NP-Hard.
In this paper, we study the problem of how to optimize the link structure to affect the outcome of information dissemination pro-cesses. The main contributions of the paper are:
This material is based upon work supported by the Army Re-search Laboratory under Cooperative Agreement No. W911NF-09-2-0053, the U.S. Defense Advanced Research Projects Agency (DARPA) under Agreement Number W911NF-12-C-0028, and the National Science Foundation under Grant No. IIS-1017415. The content of the information in this document does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. The U.S. Government is autho-rized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. Figure 4: Comparison between node deletion vs. edge deletion. Larger is better. With the same amount of edges deleted, our proposed K-E DGE D ELETION (red) leads to a bigger decrease in terms of the leading eigenvalue. [1] S. Bikhchandani, D. Hirshleifer, and I. Welch. A theory of [2] A. N. Bishop and I. Shames. Link operations for slowing the [3] L. Briesemeister, P. Lincoln, and P. Porras. Epidemic profiles [4] V. Chaoji, S. Ranu, R. Rastogi, and R. Bhatt.
 [5] W. Chen, C. Wang, and Y. Wang. Scalable influence [6] S. Datta, A. Majumder, and N. Shrivastava. Viral marketing [7] L. C. Freeman. A set of measures of centrality based on [8] A. Ganesh, E. Massouli, and D. Towsley. The effect of [9] J. Goldenberg, B. Libai, and E. Muller. Talk of the network: [10] G. H. Golub and C. F. V. Loan. Matrix Computations .The Figure 7: Scalability of proposed algorithms. Both K-E
DGE D ELETION and K-E DGE A DDITION scale near-linearly wrt the size of the graph. [11] D. Gruhl, R. Guha, D. Liben-Nowell, and A. Tomkins. [12] Y. Hayashi, M. Minoura, and J. Matsukubo. Recoverable [13] H. W. Hethcote. The mathematics of infectious diseases. [14] P. Holme, B. J. Kim, C. N. Yoon, and S. K. Han. Attack [15] E. Israeli and R. K. Wood. Shortest-path network [16] U. Kang, S. Papadimitriou, J. Sun, and H. T ong. Centralities [17] R. M. Karp. Reducibility Among Combinatorial Problems . [18] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the [19] J. M. Kleinberg. Authoritative sources in a hyperlinked [20] A. Kolla, Y. Makarychev, A. Saberi, and S.-H. Teng. [21] R. Kumar, J. Novak, P. Raghavan, and A. Tomkins. On the [22] T. Lappas, E. Terzi, D. Gunopulos, and H. Mannila. Finding [23] J. Leskovec, L. A. Adamic, and B. A. Huberman. The [24] J. Leskovec, M. McGlohon, C. Faloutsos, N. Glance, and [25] W. Liang, X. Shen, and Q. Hu. Finding the most vital edge [26] J. Marcelino and M. Kaiser. Reducing influenza spreading [27] A. Milanese, J. Sun, and T. Nishikawa. Approximating [28] J. Moody and D. R. White. Social cohesion and [29] M. Newman. A measure of betweenness centrality based on [30] L. Page, S. Brin, R. Motwani, and T. Winograd. The [31] M. Papagelis, F. Bonchi, and A. Gionis. Suggesting ghost [32] C. A. Phillips. The network inhibition problem. In STOC , [33] B. A. Prakash, D. Chakrabarti, M. Faloutsos, N. Valler, and [34] B. A. Prakash, H. Tong, N. Valler, M. Faloutsos, and [35] M. Richardson and P. Domingos. Mining knowledge-sharing [36] C. M. Schneider, T. Mihaljev, S. Havlin, and H. J. Herrmann. [37] H. Shen. Finding the k most vital edges with respect to [38] G. W. Stewart and J.-G. Sun. Matrix Perturbation Theory . [39] H. Tong, B. A. Prakash, C. E. Tsourakakis, T. Eliassi-Rad, [40] N. Valler, B. A. Prakash, H. Tong, M. Faloutsos, and [41] Y. Wang, D. Chakrabarti, C. Wang, and C. Faloutsos. [42] R. K. Wood. Network interdiction problem. Mathematical Higher-Order NetMelt . From Lemma 3, it can be seen that the only place we introduce the approximation in Alg. 1 is to approxi-mate the actual decrease of the leading eigenvalue by the first-order matrix perturbation theory. The readers might wonder if we can fur-ther improve the quality by using higher-order matrix perturbation theory, while maintaining the linear scalability of the algorithm.
We explored second-order matrix perturbation theory to approx-imate the actual decrease of the leading eigenvalue, and found that (1) it generates very similar results as the proposed K-E algorithm and (2) it requires 5-10x more wall-clock time. The rea-son might be that for the NetMelt problem, the first-order perturba-tion already gives a very good approximation. Therefore, in prac-tice, we recommend K-E DGE D ELETION for simplicity.

Nonetheless, the new algorithm based on the second-order per-turbation exhibits some interesting theoretic properties. It also helps understand the relationship between edge deletion and node dele-tion on the algorithmic level. We present it here for the complete-ness.

Let c = 1 u v , with second-order matrix perturbation, we can approximate 3 the impact of deleting a set of edges S in terms of the leading eigenvalue as:
Compared with the first-order perturbation (eq. (3)), we have an additional penalized term in eq. (5): u ( i x ) v ( j y ) cent edges e x and e y . The intuition is to encourage the edges in the set S to be far away (not adjacent) from each other.

By eq. (5), the impact of different edges in the set S is no longer independent with each other. At the first glance, this might compli-cate the algorithm since now we need to optimize at the set level, that is, to find a set of edges that collectively maximize eq. (5). However, by the following lemma, the impact defined in eq. (5) exhibits some nice diminishing return properties.
 L EMMA 7. Second-Order Approximation Properties .The Impact ( S ) defined in eq. (5) has the following properties: (1) Impact ( X ) = 0 ,where  X  is an empty set; (2) Impact ( S ) is monotonically non-decreasing wrt the set (3) Impact ( S ) is sub-modular wrt the set S .
 P ROOF . Omitted for brevity.

Thanks to such diminishing return properties, it naturally leads to the following greedy algorithm (K-E DGE D ELETION ++ )tofind a near-optimal subset of edges to delete from the original graph A . And it can be shown that the overall time complexity of K-E DGE D ELETION ++ remains linear wrt the size of the graph. Algorithm 3 K-E DGE D ELETION ++ Input: the adjacency matrix A and the budget k Output: k edges indexed by set S 1: compute the first eigen-value  X  of A ; compute the correspond-2: initialize the set S to be empty; 3: score ( e x )= u ( i x ) v ( j x ) ( e x : i x ,j x , e 4: for k 0 =1 , ..., k do 5: find e 0 = argmax e x ,e x /  X  X  score ( e x ) ; 6: add the new edge e 0 :( i 0 ,j 0 ) into S ; 7: for each edge e y : i y ,j y s.t. j y = i 0 do 8: score ( e y )  X  score ( e y )  X  1 / (2  X  ) u ( i y ) v ( 9: end for 10: for each edge e y : i y ,j y s.t. i y = j 0 do 11: score ( e y )  X  score ( e y )  X  1 / (2  X  ) u ( i 0 ) v ( 12: end for 13: end for
An interesting property of Alg. 3 is that it builds the equivalence between edge deletion and node deletion on the algorithmic level: L EMMA 8. Equivalence of Alg. 3 to Node Immunization .Let S be the set of edges by running Alg. 3 on graph A ; T be the set of edges by running the node immunization algorithm [39] on the line graph L ( A ) ; and |S| = |T | . We have S = T .
 P
ROOF . Omitted for brevity.
This formulas is similar as the one in [27]
