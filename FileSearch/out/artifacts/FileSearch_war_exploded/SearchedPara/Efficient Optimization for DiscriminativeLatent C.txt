 Latent representations of data are wide-spread tools in sup ervised and unsupervised learning. They are used to reduce the dimensionality of the data for two main reasons: on the one hand, they provide numerically efficient representations of the data; on the other hand, they may lead to better predictive performance. In supervised learning, latent mo dels are often used in a generative way, e.g., through mixture models on the input variables only, wh ich may not lead to increased predictive performance. This has led to numerous works on supervised di mension reduction (e.g., [1, 2]), where the final discriminative goal of prediction is taken ex plicitly into account during the learning process.
 In this context, various probabilistic models have been pro posed, such as mixtures of experts [3] or discriminative restricted Boltzmann machines [4], where a layer of hidden variables is used between the inputs and the outputs of the supervised learning model. Parameters are usually estimated by expectation-maximization (EM), a method that is computati onally efficient but whose cost function may have many local maxima in high dimensions.
 In this paper, we consider a simple discriminative latent class (DLC) model where inputs and outputs are independent given the latent representation.We make th e following contributions:  X  We provide in Section 2 a quadratic (non convex) local appro ximation of the log-likelihood of our model based on the EM auxiliary function. This approxima tion is optimized to obtain robust initializations for the EM procedure.  X  We propose in Section 3.3 a novel probabilistic interpreta tion of discriminative clustering with added benefits, such as fewer hyperparameters than previous approaches [5, 6, 7].  X  We design in Section 4 a low-rank optimization method for no n-convex quadratic problems over a product of simplices. This method relies on a convex relaxat ion over completely positive matrices.  X  We perform experiments on text documents in Section 5, wher e we show that our inference tech-nique outperforms existing supervised dimension reductio n and clustering methods. We assume that each observation x n has a certain probability to be in one of K latent classes, mod-eled by introducing hidden variables z n  X  X  1 , . . . , K } , and that these classes should be predictive of the label y n . We model directly the conditional probability of z n given the input data x n and the probability of the label y n given z n , while making the assumption that y n and x n are independent given z n (leading to the directed graphical model x n  X  z n  X  y n ). More precisely, we assume that, given x n , z n follows a multinomial logit model while, given z n , y n is a multinomial variable: with w k  X  R p , b k  X  R and P M m =1  X  km = 1 . We use the notation w = ( w 1 , . . . , w K ) , b = ( b by replacing implicitly or explicitly x by the image  X ( x ) of a non linear mapping.
 Related models. The simple two-layer probabilistic model defined in Eq. (1), can be interpreted and compared to other methods in various ways. First, it is an instance of a mixture of experts [3] where each expert has a constant prediction. It has thus weak er predictive power than general mix-tures of experts; however, it allows efficient optimization as shown in Section 4. It would be inter-esting to extend our optimization techniques to the case of e xperts with non-constant predictions. This is what is done in [8] where a convex relaxation of EM for a similar mixture of experts is con-sidered. However, [8] considers the maximization with resp ect to hidden variables rather than their marginalization, which is essential in our setting to have a well-defined probabilistic model. Note also that in [8], the authors derive a convex relaxation of th e softmax regression problems, while we derive a quadratic approximation. It is worth trying to comb ine the two approaches in future work. Another related model is a two-layer neural network. Indeed , if we marginalize the latent vari-able z , we get that the probability of y given x is a linear combination of softmax functions of linear functions of the input variables x . Thus, the only difference with a two-layer neural network w ith softmax functions for the last layer is the fact that our last layer considers linear parameterization in the mean parameters rather than in the natural parameters of the multinomial variable. This change allows us to provide a convexification of two-layer neural ne tworks in Section 4.
 Among probabilistic models, a discriminative restricted B oltzmann machine (RBM) [4, 9] mod-els p ( y | z ) as a softmax function of linear functions of z . Our model assumes instead that p ( y | z ) is linear in z . Again, this distinction between mean parameters and natur al parameters allows us to derive a quadratic approximation of our cost function. It wo uld of course be of interest to extend our optimization technique to the discriminative RBM.
 Finally, one may also see our model as a multinomial extensio n of reduced-rank regression (see, e.g. [10]), which is commonly used with Gaussian distributi ons and reduces to singular value de-composition in the maximum likelihood framework. We consider the negative conditional log-likelihood of y n given x n (regularized in w to avoid over-fitting) where  X  = (  X , w, b ) and y nm is equal to 1 if y n = m and 0 otherwise: 3.1 Expectation-maximization A popular tool for solving maximum likelihood problems is th e EM algorithm [10]. A traditional way of viewing EM is to add auxiliary variables and minimize t he following upperbound of the negative log-likelihood  X  , obtained by using the concavity of the logarithm: F (  X ,  X  ) =  X  where  X  k = (  X  k 1 , . . . ,  X  km ) T  X  R M and  X  = (  X  1 , . . . ,  X  K ) T  X  R N  X  K with  X  n = (  X  n 1 , . . . ,  X  nK )  X  R K . The EM algorithm can be viewed as a two-step block-coordina te descent procedure [11], where the first step (E-step) consists in find ing the optimal auxiliary variables  X  , given the parameters of the model  X  . In our case, the result of this step is obtained in closed for m of parameters  X  , given the auxiliary variables  X  . Optimizing the parameters  X  k leads to the closed form updates  X  k  X  P N n =1  X  nk y n with  X  T k 1 M = 1 while optimizing jointly on w and b leads to a softmax regression problem, which we solved with Newton met hod.
 Since F (  X ,  X  ) is not jointly convex in  X  and  X  , this procedure stops when it reaches a local minimum, and its performance strongly depends on its initialization . We propose in the following section, a robust initialization for EM given our latent model, based o n an approximation of the auxiliary cost function obtained with the M-step. 3.2 Initialization of EM Minimizing F w.r.t.  X  leads to the original log-likelihood  X  (  X  ) depending on  X  alone. Minimizing F w.r.t.  X  gives a function of  X  alone. In this section, we focus on deriving a quadratic appr oximation of this function, which will be minimized to obtain an initia lization for EM.
 We consider second-order Taylor expansions around the valu e of  X  corresponding to the uniformly is motivated by the lack of a priori information on the latent classes. We briefly explain the calcu-lation of the expansion of the terms depending on ( w, b ) . For the rest of the calculation, see the supplementary material.
 Second-order Taylor expansion of the terms depending on ( w, b ) . Assuming uniformly dis-tributed variables z n and independence between z n and x n implies that w T k x n + b k = 0 . There-fore, using the second-order expansion of the log-sum-exp f unction  X  ( u ) = log( P K k =1 exp( u k )) around 0 leads to the following approximation of the terms depending on ( w, b ) : J wb (  X  ) = cst + third-order term O ( k Xw + b k 3 F ) can be replaced by third-order terms in k  X   X   X  0 k , which makes the minimization with respect to w and b correspond to a multi-label classification problem with a square-loss [7, 10, 12]. Its solution may be obtained in clos ed form and leads to: where A ( X,  X  ) =  X  N I  X  X ( N  X I + X T  X  N )  X  1 X T  X  N . Quadratic approximation. Omitting the terms that are independent of  X  or of an order in  X  higher than two, the second-order approximation J app of the function obtained for the M-step is: where B ( Y ) = 1 N Y ( Y T Y )  X  1 Y T  X  1 N 1 N 1 T N and Y  X  R N  X  M is the matrix with entries y nm . Link with ridge regression. The first term, tr (  X  X  T B ( Y )) , is a concave function in  X  , whose maxi-mum is obtained for  X  X  T = I (each variable in a different cluster). The second term, A ( X,  X  ) , is the matrix obtained in ridge regression [7, 10, 12]. Since A ( x,  X  ) is a positive semi-definite matrix such that A ( X,  X  )1 N = 0 , the maximum of the second term is obtained for  X  X  T = 1 N 1 T N (all variables in the same cluster). J app (  X  ) is thus a combination of a term trying to put every point in the same cluster and a term trying to spread them equally. Note that in general, J app is not convex. Non linear predictions. Using the matrix inversion lemma, A ( X,  X  ) can be expressed in terms of the Gram matrix K = XX T , which allows us to use any positive definite kernel in our fra me-work [12], and tackle problems that are not linearly separab le. Moreover, the square loss gives a natural interpretation of the regularization parameter  X  in terms of the implicit number of param-eters of the learning procedure [10]. Indeed, the degree of freedom defined as df = n (1  X  tr A ) provides a intuitive method for setting the value of  X  [7, 10].
 Initialization of EM. We optimize J app (  X  ) to get a robust initialization for EM. Since the entries of each vector  X  n sum to 1, we optimize J app over a set of N simplices in K dimensions, S = { v  X  R
K | v  X  0 , v T 1 K = 1 } . However, since this function is not convex, minimizing it d irectly leads to local minima. We propose, in Section 4, a general reformul ation of any non-convex quadratic program over a set of N simplices and propose an efficient algorithm to optimize it. 3.3 Discriminative clustering The goal of clustering is to find a low-dimensional represent ation of unlabeled observations, by assigning them to K different classes, Xu et al. [5] proposes a discriminative c lustering framework based on the SVM and [7] simplifies it by replacing the hinge lo ss function by the square loss, leading to ridge regression. By taking M = N and the labels Y = I , we obtain a formulation similar to [7] where we are looking for a latent representati on that can recover the identity matrix. However, unlike [5, 7], our discriminative clustering fram ework is based on a probabilistic model which may allow natural extensions. Moreover, our formulat ion naturally avoids putting all variables in the same cluster, whereas [5, 7] need to introduce constra ints on the size of each cluster. Also, our model leads to a soft assignment of the variables, allowi ng flexibility in the shape of the clusters, whereas [5, 7] is based on hard assignment. Finally, since ou r formulation is derived from EM, we obtain a natural rounding by applying the EM algorithm after the optimization whereas [7] uses a coarse k-means rounding. Comparisons with these algorithm s can be found in Section 5. To initialize the EM algorithm, we must minimize the non-convex quadratic cost function defined by Eq. (3) over a product of N simplices. More precisely, we are interested in the followi ng problems: where B can be any N  X  N symmetric matrix. Denoting v = vec ( V )  X  R NK the vector obtained by stacking all the columns of V and defining Q = ( B T  X  I K ) T , where  X  is the Kronecker product [13], the problem (4) is equivalent to: Note that this formulation is general, and that Q could be any N K  X  N K symmetric matrix. Tradi-tional convex relaxation methods [14] would rewrite the obj ective function as v T Qv = tr ( Qvv T ) = tr ( QT ) where T = vv T is a rank-one matrix which satisfies the set of constraints: We note F the set of matrix T verifying (7-8). With the unit-rank constraint, optimizin g over v is exactly equivalent to optimizing over T . The problem is relaxed into a convex problem by removing the rank constraint, leading to a semidefinite programming p roblem (SDP) [15].
 Relaxation. Optimizing T instead of v is computationally inefficient since the running time com-plexity of general purpose SDP toolboxes is in this case O ( KN ) 7 . On the other hand, for prob-lems without pointwise positivity, [16, 17] have considere d low-rank representations of matrices T , of the form T = V V T where V has more than one column. In particular, [17] shows that the n on convex optimization with respect to V leads to the global optimum of the convex problem in T . In order to apply the same technique here, we need to deal with the pointwise nonnegativity. This can be done by considering the set of completely positive matrices , i.e., This set is strictly included in the set DN K of doubly non-negative matrices (i.e., both pointwise nonnegative and positive semi-definite). For R  X  5 , it turns out that the intersection of CP K and F is the convex hull of the matrices vv T such that v is an element of the product of simplices [16]. This implies that the convex optimization problem of minimizing tr ( QT ) over CP K  X  X  is equivalent to our original problem (for which no polynomial-time algorit hm is known).
 However, even if the set CP K  X  X  is convex, optimizing over it is computationally inefficien t [18]. We thus follow [17] and consider the problem through the low-rank pointwise nonnegative matrix V  X  R NK  X  R instead of through matrices T = V V T . Note that following arguments from [16], if R is large enough, there are no local minima. However, because of the positivity constraint one cannot find in polynomial time a local minimum of a differentiable fu nction. Nevertheless, any gradient descent algorithm will converge to a stationary point. In Se ction 5, we compare results with R &gt; 1 than with R = 1 , which corresponds to a gradient descent directly on the sim plex.
 Problem reformulation. In order to derive a local descent algorithm, we reformulate the con-straints (7-8) in terms of V (details can be found in the supplementary material). Denot ing by V r condition by using a rescaled cost function which equivalen t. Finally, using the notation D : we obtain a new equivalent formulation: where Diag ( A ) is the matrix with the diagonal of A and 0 elsewhere. Since the set of constraints for V is convex, we can use a projected gradient method [19] with th e projection step we now describe. Projection on D . Given N K -vectors Z n stacked in a N K vector Z = [ Z 1 ; . . . ; Z N ] , we consider the projection of Z on D . For a given positive real number a , the projection of Z on the set of all U  X  X  such that for all n , k U n k 1 = a , is equivalent to N independent projections on the  X  1 ball with radius a . Thus projecting Z on D is equivalent to find the solution of: where (  X  n ) n  X  N are Lagrange multipliers. The problem of projecting each Z n on the  X  1 -ball of radius a is well studied [20], with known expressions for the optimal Lagrange mul-tipliers, (  X  n ( a )) n  X  N and the corresponding projection for a given a . The function L ( a ) is Figure 1: Comparison between our algorithm and R independent optimizations. Also comparison between two rounding: by summing and by taking the best colum n. Average results for K = 2 , 3 , 5 (Best seen in color). convex, piecewise-quadratic and differentiable, which yi elds the first-order optimality condi-tion P N n =1  X  n ( a ) = 0 for a . Several algorithms can be used to find the optimal value of a . We found iteratively. This method was found to be empirically f aster than gradient descent. Overall complexity and running time. We use projected gradient descent, the bottleneck of our algorithm is the projection with a complexity of O ( RN 2 K log( K )) . We present experiments on running times in the supplementary material. We first compare our algorithm with others to optimize the pro blem (4). We show that the per-formances are equivalent but, our algorithm can scale up to l arger database. We also consider the problem of supervised and unsupervised discriminative clu stering. In both cases, we show that our algorithm outperforms existing methods.
 Implementation. For supervised and unsupervised multilabel classification , we first optimize the second-order approximation J app , using the reformulation (9). We use a projected gradient de scent method with Armijo X  X  rule along the projection arc for backt racking [19]. It is stopped after a maximum number of iterations (500) or if relative updates ar e too small ( 10  X  8 ). When the algorithm stops, the matrix V has rank greater than 1 and we use the heuristic v  X  = P R r =1 V r  X  X  as our final solution ( X  X vg round X ). We also compare this rounding with a nother heuristic obtained by taking v  X  = argmin Section 2.
 Optimization over simplices. We compare our optimization of the non-convex quadratic prob-lem (9) in V , to the convex SDP in T = V V T on the set of constraints defined by T  X  X N K , (7) and (8). To optimize the SDP, we use generic algorithms, CVX [ 21] and PPXA [22]. CVX uses interior points methods whereas PPXA uses proximal methods [22]. Both algorithms are com-putationally inefficient and do not scale well with either th e number of points or the number of constraints. Thus we set N = 10 and K = 2 on discriminative clustering problems (which are described later in this section). We compare the performanc es of these algorithms after rounding. For the SDP, we take  X   X  = T 1 NK and for our algorithm we report performances obtained for bo th rounding discuss above ( X  X vg round X  and  X  X in round X ). On the se small examples, our algorithm associated with  X  X in round X  reaches similar performances t han the SDP, whereas, associated with  X  X vg round X , its performance drops.
 Study of rounding procedures. We compare the performances of the two different roundings,  X  X in round X  and  X  X vg round X  on discriminative clustering pr oblems. After rounding, we apply the EM algorithm and look at the classification scores. We also co mpare our algorithm for a given R , to two baselines where we solve independently problem (4) R times and then apply the same round-ings ( X  X nd -min round X  and  X  X nd -avg round X ). Results are sho wn Figure 1. We consider three Figure 2: Classification rate for several binary classificat ion tasks (from top to bottom) and for different values of K , from left to right (Best seen in color). different problems, N = 100 and K = 2 , K = 3 and K = 5 . We look at the average perfor-mances as the number of noise dimensions increases in discri minative clustering problems. Our method outperforms the baseline whatever rounding we use. F igure 1 shows that on problems with a small number of latent classes ( K &lt; 5 ), we obtain better performances by taking the column associated with the lowest value of the cost function ( X  X in r ound X ), than summing all the columns ( X  X vg round X ). On the other hand, when dealing with a larger n umber of classes ( K  X  5 ), the per-formance of  X  X in round X  drops significantly while  X  X vg round  X  maintains good results. A potential explanation is that summing the columns of V gives a solution close to 1 K 1 N 1 T K in expectation, thus in the region where our quadratic approximation is valid. Mo reover, the best column of V is usually a local minimum of the quadratic approximation, which we hav e found to be close to similar local minima of our original problem, therefore, preventing the E M algorithm from converging to another solution. In all others experiments, we choose  X  X vg round X .
 Application to classification. We evaluate the optimization performance of our algorithm ( DLC) on text classification tasks. For our experiments, we use the 20 Newsgroups dataset (http://people.csail.mit.edu/jrennie/), which contain s postings to Usenet newsgroups. The postings are organized by content into 20 categories. We use the five bi nary classification tasks considered in [23, Chapter 4, page 91]. To set the regularization parame ter  X  , we use the degree of free-dom df (see Section 3.2). Each document has 13312 entries and we tak e df = 1000 . We use 50 random initializations for our algorithm. We compare our me thod with classifiers such as the lin-ear SVM and the supervised Latent Dirichlet Allocation (sLD A) classifier of Blei et al. [2]. We also compare our results to those obtained by an SVM using the features obtained with dimension-reducing methods such as LDA [1] and PCA. For these models, we select parameters with 5-fold cross-validation. We also compare to the EM without our init ialization ( X  X and-init X ) but also with 50 random initializations, a local descent method which is c lose to back-propagation in a two-layer neural network, which in this case strongly suffers from loc al minima problems. An interesting result on computational time is that EM without our initiali zation needs more steps to obtain a local minimum. It is therefore slower than with our initialization in this particular set of exper iments. We show some results in Figure 2 (others maybe found in the sup plementary material) for different values of K and with an increasing number N of training samples. In the case of topic models, K represents the number of topics. Our method significantly ou tperforms all the other classifiers. The comparison with  X  X and-init X  shows the importance of our con vex initialization. We also note that our performance increases slowly with K . Indeed, the number of latent classes needed to correctly separate two classes of text is small. Moreover, the algorit hm tends to automatically select K . Em-pirically, we notice that starting with K = 15 classes, our average final number of active classes is around 3. This explains the relatively small gain in perform ance as K increases. Figure 3: Clustering error when increasing the number of noi se dimensions. We have take 50 different problems and 10 random initializations for each of them. K = 2 , N = 100 and R = 5 , on the left, and K = 5 , N = 250 and R = 10 , on the right(Best seen in color).
 Figure 4: Comparison between our method (left) and k-means ( right). First, circles with RBF kernels. Second, linearly separable bumps. K = 2 , N = 200 and R = 5 in both cases.
 Application to discriminative clustering. Figure 3 shows the optimization performance of the EM algorithm with 10 random starting points with ( X  X LC X ) and without ( X  X and-init X ) our initial-ization method. We compare their performances to K-means, G aussian Mixture Model ( X  X MM X ), Diffrac [7] and max-margin clustering ( X  X MC X ) [24]. Follow ing [7], we take linearly separable bumps in a two-dimensional space and add dimensions contain ing random independent Gaussian noise (e.g.  X  X oise dimensions X ) to the data. We evaluate the ratio of misclassified observations over the total number of observations. For the first experiment, w e fix K = 2 , N = 100 , and R = 5 , and for the second K = 5 , N = 250 , and R = 10 . The additional independent noise dimensions are normally distributed. We use linear kernels for all the meth ods. We set the regularization parame-ters  X  to 10  X  2 for all experiments but we have seen that results do not chang e much as long as  X  is not too small ( &gt; 10  X  8 ). Note that we do not show results for the MMC algorithm when K = 5 since this algorithm is specially designed for problems wit h K = 2 . It would be interesting to com-pare to the extension for multi-class problems proposed by Z hang et al. [24]. On both examples, we are significantly better than Diffrac, k-means and MMC. We sh ow in Figure 4 additional examples which are non linearly separable. We have presented a probabilistic model for supervised dime nsion reduction, together with associ-ated optimization tools to improve upon EM. Application to t ext classification has shown that our model outperforms related ones and we have extended it to uns upervised situations, thus drawing new links between probabilistic models and discriminative clustering. The techniques presented in this paper could be extended in different directions: First , in terms of optimization, while the embed-ding of the problem to higher dimensions has empirically led to finding better local minima, sharp statements might be made to characterize the robustness of o ur approach. In terms of probabilistic models, such techniques should generalize to other latent v ariable models. Finally, some additional structure could be added to the problem to take into account m ore specific problems, such as multiple instance learning [25], multi-label learning or discrimin ative clustering for computer vision [26, 27]. Acknowledgments. This paper was partially supported by the Agence Nationale d e la Recherche (MGA Project) and the European Research Council (SIERRA Pro ject). We would like to thank Toby Dylan Hocking, for his help on the comparison with other meth ods for the classification task.
