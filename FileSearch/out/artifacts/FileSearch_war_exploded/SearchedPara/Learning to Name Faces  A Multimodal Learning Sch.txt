 Automated face annotation aims to automatically detect human faces from a photo and further name the faces with the corresponding hu-man names. In this paper, we tackle this open problem by investi-gating a search-based face annotation (SBFA) paradigm for mining large amounts of web facial images freely available on the WWW. Given a query facial image for annotation, the idea of SBFA is to first search for top-n similar facial images from a web facial im-age database and then exploit these top-ranked similar facial im-ages and their weak labels for naming the query facial image. To fully mine those information, this paper proposes a novel frame-work of Learning to Name Faces (L2NF)  X  a unified multimodal learning approach for search-based face annotation, which consists of the following major components: (i) we enhance the weak labels of top-ranked similar images by exploiting the  X  X abel smoothness" assumption; (ii) we construct the multimodal representations of a facial image by extracting different types of features; (iii) we opti-mize the distance measure for each type of features using distance metric learning techniques; and finally (iv) we learn the optimal combination of multiple modalities for annotation through a learn-ing to rank scheme. We conduct a set of extensive empirical stud-ies on two real-world facial image databases, in which encouraging results show that the proposed algorithms significantly boost the naming accuracy of search-based face annotation task.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation web facial images, auto face annotation, supervised learning
Automated face annotation aims to automatically detect human faces from a photo image and name the facial image with the corre-sponding human names, which sometimes is termed as  X  X ace nam-ing" or  X  X ace tagging" in some existing studies. It is an important yet very challenging problem in multimedia information retrieval, which is highly desirable for many real-world applications, such as online photo management or face annotation for video sum-marization. One possible way is to directly apply some classical face recognition methods [ 3, 30 , 33 ]. For exmaple, one can ap-p ly supervised machine learning techniques to train face classifi-cation models from a collection of well-controlled labeled facial images and then apply the models to name a new facial image. However, such kinds of  X  model-based face annotation " techniques suffer from some common drawbacks, e.g., being difficult and ex-pensive to collect large high-quality training data and being non-trivial for adding new training data.

Recent years have witnessed an emerging promising direction to tackle the automated face annotation challenge, i.e., the  X  Search-Based Face Annotation " (SBFA) paradigm [ 37 , 38 ] which attempts t o explore content-based image retrieval (CBIR) techniques [ 16 , 43 ] in mining massive WWW facial images freely available on the i nternet, such as popular social sharing web sites (e.g., Flickr or Facebook). Due to the noisy nature of web images, the raw labels of web facial images are often noisy without extra manual effort, in which some facial images are tagged with incorrect/incomplete names. We refer to such kind of raw facial image database as  X  X eakly labeled web facial image database".

Figure 1: The framework of Search-Based Face Annotation. Figure 1 illustrates the basic framework of the  X  S earch-based Face Annotation " paradigm, which consists of three main stages: (i) given a query facial image, it typically involves a pre-processing stage, including face detection, face alignment, and facial feature extraction; consequently, the input facial image is represented as feature vectors in the facial feature space; (ii) we retrieve the top-n similar instances of the query facial image from the large-scale weakly labeled web facial image database using content-based im-age retrieval techniques; and (iii) finally, we aim to name the query image by mining the top-ranked similar images and the correspond-ing weak name labels. Such a paradigm was inspired by the search-based image annotation [ 39 ] for generic image annotation as face a nnotation can be generally viewed as a special case of image an-notation [ 9, 10 , 32 , 36 , 41 , 49 ], which has been extensively studied, b ut remains still an technically challenging problem. In the follow-ing, we explain the main challenges of this task to motivate the proposed new technique.

As shown in Figure 1, there are two key challenging tasks for t he search-based face annotation framework: (i) how to efficiently retrieve the top-n most similar facial images from a large facial im-age database given a query facial image, that is, how to develop an effective content-based facial image retrieval solution; and (ii) how to effectively exploit the short list of candidate facial images and their weak labels for naming the faces automatically. In general, these two tasks can be solved separately, though the second task can be affected by the results of the first task. As one can tackle the first task by adapting existing CBIR techniques [ 24 , 43 , 7, 42 ], in t his paper we focus on the second challenge, which is critical due to the nature of noisily labeled web facial images.
 In this paper, we propose a novel framework of  X  Learning to Name Faces " (L2NF) for search-based face annotation, which at-tempts to learn both the optimal weight vector in combining dif-ferent query-neighbor similarity functions for face naming and the refined labels for enhancing the initial weak labels simultaneously in a unified learning framework. In particular, the key challenge of naming the query facial image is to effectively measure the similar-ity between the query image and its nearest instances by combin-ing diverse facial feature representations and their proper distance measurements. To tackle this challenge, we propose a multimodal learning scheme that (i) first constructs multiple diverse facial fea-tures for representing the faces, (ii) further optimizes the distance measure on each feature space (modality) using distance metric learning, and (iii) finally learns the optimal fusion of the multiple representations by adapting the structural SVM algorithm. Besides, we suggest a graph-based label refinement scheme to enhance the weak labels of top-ranked similar facial images by exploiting the  X  X abel smoothness" assumption. The main contributions of this work include:
The remainder of this paper is organized as follows. Section 2 reviews related work. Section 3 presents the proposed algorithms of Learning to Name Faces (L2NF). Section 4 shows the experimental results of our empirical studies. Section 5 discusses the limitations, and finally Section 6 concludes this paper.
Automated face annotation can be directly solved by general face recognition and verification techniques, which have been exten-sively studied for many years [ 20 ]. However, the success of such  X  m odel-based face annotation " scheme often relies on a large set of high-quality facial images collected in well-controlled environ-ments, which can be difficult and expensive to obtain. This draw-back has been partially addressed in recent benchmark studies of unconstrained face detection and verification techniques on facial image testbeds collected from the web, such as LFW [ 18 , 5].
B y focusing on the facial image domain, the studies for auto-mated face annotation can be further classified into four groups. The first group of studies aim to handle the collections of personal photos [ 8], where rich context clues, such as social context, GPS t ags, time tamps, are available. Some techniques have already been successfully deployed in the commercial applications, e.g., Apple iPhoto, 1 Google Picasa, 2 and Facebook face auto-tagging solution. The second group of works consider to refine the text-based fac ial image retrieval results, where a human name is used as the input query [ 27 , 22 , 17 , 12 , 13 ]. Such problems are closely related to the i mage re-ranking problems, and part of top-ranked facial images are tagged with the name query. For example, Ozkan and Duygulu proposed a graph-based model for finding the densest sub-graph as the most related result [ 27 ], which is improved by adding an extra c onstraint such that a face can only appear once in an image [ 12 ] o r by introducing the images of  X  X riends" of the query name in a query expansion scheme [ 26 ]. Following the graph-based ap-p roach, Le and Satoh [ 22 ] proposed to represent the importance of e ach returned image. Recently, the generative approach has also been adopted in this problem and achieved better performance [ 12 , 13 ]. The third group of works have attempted to directly annotat e each web facial image with the names extracted from its caption information. For example, Berg et al. [ 3] proposed a probability m odel which is combined with a clustering algorithm to estimate the relationship between the facial images and the names in their captions. Guillaumin et al. [ 12 ] proposed to iteratively update the a ssignment between the facial image and detected names in cap-tions based on a minimum cost matching algorithm, which is fur-ther improved by using supervised distance metric learning tech-niques to grasp the important discriminative features in low dimen-sional spaces in their subsequent work [ 13 ]. Recently, Bu et al. [ 4] p roposed to estimate the distance between faces and names with  X  X ommute distance". The last group of studies is the  X  search-based face annotation " (SBFA), which was inspired by the search-based image annotation and is fundamentally different from the previous three groups of research. In particular, the SBFA framework aims to solve the generic content-based face annotation problem, where facial images are directly used as the input query images and the task is to return the corresponding human names for the query im-ages. There are rather few studies in this group. For example, by attempting to mine the large-scale noisy web facial image with weak labels, Wang et al. [ 37 ] proposed an Unsupervised Label Re-fi nement (URL) algorithm to enhance the initial weak label matrix over the entire facial image database. In their subsequent work [ 38 ], t hey further proposed the Weak Label Regularized Local Coordi-nate Coding (WLRLCC) algorithm, which aims to fully exploit the top-ranked similar images of the query facial image via a unified optimization scheme of learning both local coordinate coding and refined labels. http://www.apple.com/ilife/iphoto/ http://picasa.google.com/ http://www.facebook.com/
Recently, a few of emerging works have attempted to attack the a utomated face annotation problem via the  X  search-based face an-notation " (SBFA) paradigm [ 37 , 38 ]. It was generally inspired by t he search-based image annotation that attempts to infer the cor-relation or joint probabilities between query images and annota-tion keywords based on existing object recognition techniques and semi-supervised learning algorithms in mining massive free web images on the WWW [ 9, 10 , 6, 32 , 15 , 39 , 29 , 35 , 28 ]. Several stud-i es have attempted to develop efficient content-based indexing and search techniques to facilitate image tagging tasks. For example, Russell et al. [ 29 ] developed a large collection of web images with g round truth labels to facilitate object recognition tasks. More stud-ies in this area aim to address the final annotation process by explor-ing effective label propagation algorithms. For example, Wright et al. [ 40 ] proposed a classification algorithm based sparse repre-s entation technique, which predicts the label information based on the class-based feature reconstruction. Tang et al. [ 32 ] presented a sparse graph-based semi-supervised learning (SGSSL) approach to annotate web images. Wang et al. [ 36 ] proposed another sparse c oding based annotation framework, where the label-based graph is used to learn a linear transformation matrix for feature dimension reduction, and sparse reconstruction is employed for the subsequent label propagation step. Wu et al. [ 41 ] proposed to select heteroge-n eous features with structural grouping sparsity and suggested a Multi-label Boosting scheme (denoted as  X  X tBGS" for short) for feature regression, where a group sparse coefficient vector is ob-tained for each class (category) and further used for predicting new instances. Wu et al. [ 43 ] proposed a multi-reference re-ranking s cheme (denoted as  X  X RR" for short) for improving the retrieval process.

Our work differs from the above existing works for search-based face annotation in several aspects. First of all, the ULR algorithm aims to refine the noisy labels over the entire facial image database, which is extremely time-consuming for the large-scale database. Unlike the ULR algorithm, our work tackles such a computation-ally expensive task by mining only the top-ranked similar images for each query, which follows the similar approach of the WL-RLCC algorithm. Further, both ULR and WLRLCC algorithms are designed to explore only one single type of facial feature descriptor, e.g., the GIST features, while our work is designed to explore more clues by constructing multiple types of facial features descriptors and further learning to optimize the fusion of the multimodal rep-resentations.
In this section, we present the proposed  X  X earning to Name Faces" (L2NF) framework for search-based face annotation in detail.
Throughout the paper, we denote matrixes or sets by upper case letters, e.g., X and D ; we denote vectors by bold lower case let-ters, e.g., x , x i , x ij ; we denote scalars by normal letters, e.g., x X ij , where x i is the i -th element of vector x which could also be denoted as x [ i ] , and X ij is the element in the i -row and j -column of matrix X , which could also be denoted as X [ i, j ] .
Let us denote by Q = { q i | i = 1 , 2 , . . . , N t } the set of query facial images, and assume there are a total of m names (classes) in the whole retrieval database, denoted by C = { c 1 , c Each query facial image q i  X  Q is associated with one name (class label) c q i  X  C . Notice that we assume that there is only one name for each person. We denote by y q i  X  { 0 , 1 } m the label vector for the query instance q i , which contains only one non-zero item: k y q i k 0 = 1 . If the query instance q i is annotated with the k -th name ( c q i = c k ), then y q i [ k ] = 1 . In the SBFA framework, the name (label) of the query face is predicted based on its nearest facial images. Assume the top-n retrieval results of the query image q are { ( d ij , y ij ) | j = 1 , 2 , ..., n } , where d ij image in the retrieval result and y ij  X  X  0 , 1 } m is its corresponding label vector. We denote by Y i = [ y i 1 , y i 2 , . . . , y label matrix for the i -th query q i .

For each query-neighbor pair ( q i , d ij ) , we can create one query-neighbor similarity based feature vector: where  X  k ( , ) represents the k -th query-neighbor similarity func-tion and N f is the number of the query-neighbor similarity func-tions. Typically, the query-neighbor similarity function is related to three factors: (1) the facial feature representation, (2) the distance metric, and (3) the mapping function between the distance value and the similarity value. For example, we can extract the Local binary patterns (LBP) as the facial feature, apply the L2-norm (Eu-clidean) distance as the distance metric, and the radial basis func-tion with  X  = 0 . 1 as the similarity-mapping function: To estimate the similarity more accurately by exploring more infor-mation, we can leverage multiple diverse query-neighbor similarity functions. More details about query-neighbor similarity function construction will be presented in Section 3.4 . Based on the pre-d efined query-neighbor similarity function and the achieved query-neighbor similarity based feature vector, for the i -th query instance q , we denote its query-neighbor similarity matrix by X i = [ x with k = 1 , 2 , . . . , n and X i  X  R N f  X  n .
The basic idea of the SBFA paradigm is to exploit the weak la-bels of top-ranked similar facial images for naming the query face. The crux for the face naming task lies in how to effectively estimate the confidence values for the weak label vectors of the top-ranked similar instances. Given a query image q i and its top-n retrieval results { ( d ij , y ij ) | j = 1 , 2 , . . . , n } , we denote by v fidence value for its j -th similar image d ij . Then, the estimated label vector of q i , denoted as  X  y q i , can be generated as: v ij is related to both query q i and the j -th similar instance d our problem, we assume it linearly depends on the query-neighbor similarity based feature vector x ij , that is, v ij = x  X  confidence vector v i can be achieved as follows: aims to combine different features of X generated by the N verse similarity functions. In other words, each confidence value v ij is a weighted linear combination of the corresponding query-neighbor similarity based feature vector x ij .

Remark. This aforementioned assumption is not difficult to un-derstand as follows: each item in x ij (e.g. the k -th item x only related to the corresponding similarity function (e.g.  X  A large x ij [ k ] indicates that the j -th retrieved instance is more similar to the query instance based on the k -th similarity function. Hence, it is more possible that the query instance has the same label q , its top-n similar samples are { ( d ij , y ij ) } j =1 , 2 ,...,n sample d ij . x ij is the feature vector between the query instance q and the similar example d ij . The k -th item of x ij is constructed with the k -th query-neighbor similarity function  X  k ( q vector with the j -th retrieved instance. As a result, the similarity value x ij [ k ] is correlated with the the confidence value v cally, different similarity functions can perform very differently in practice, hence they should be combined appropriately by a proper weight vector w
By combining Eq.( 1) and Eq.( 2), the estimated label vector  X  y f or the query image q i can be computed as follows: where Y i and X i vary for different query instances, and w is in-dependent of the query instances. We show a visual example in Figure 2 (a) to help understand this formula.

T o generate the final annotation results (a sorted candidate name list), we can rank all the m names by sorting the predicted label vector  X  y q i in a descending order, as shown in Figure 2 (b). We d enote by  X   X  q i the ranked name list, in which the item  X  is the j -th annotated name. Given the correct name of the query instance q i is c q i , a good annotation system should ensure c appears at the top-ranked position or ideally at the first position. Hence, our problem aims to minimize the ranking position of the correct name c q i , which can be formulated as follows: In general, the loss value should be zero if the correct name c the first position of  X   X  q i , and the loss value of c q i position should be smaller than the one of c q i at a lower-ranked position. The goal of the whole learning to name faces scheme is to minimize the loss values over all the query instances by addressing the following three key factors: (i) the noisy label matrix Y the query-neighbor similarity matrix X i , and (iii) the combination weight vector w , as shown in Figure 2 (b). In particular, we attempt t o address each of them respectively in the following approach: In the following, we will introduce the solutions of the aforemen-tioned three problems respectively.
In this section, we aim to refine the initial weak label matrix for each query independently. In particular, for a query q ( the sub-script of query index value is omitted ), its top-n similar samples are { d 1 , . . . , d n } and the corresponding noisy label matrix is de-noted by  X  Y . We enhance the initial label matrix  X  Y in a manifold learning scheme based on the key assumption of  X  label smooth-ness ", which means that the more similar the visual contents of two facial images are, the more likely they share the same labels [ 37 ].
I n particular, for two images d i and d j in the top-n nearest sam-ples, we can compute their similarity value vector based on the query-neighbor similarity function:  X  ( d i , d j )  X  R N the weight vector w learned in Section 3.5 , we can get the similar-i ty value between d i and d j as S ij = w  X   X  ( d i , d j of S ij indicates that d i is more similar to d j . Hence, a larger value of S ij implies that the label vectors of d i and d j are more likely to be the same. Based on the above motivation, we can obtain the following formulation to enhance the initial weak label matrix where  X  denotes the Hadamard product of two matrices, and M is the mask matrix indicating the non-zeros values in  X  Y . In Eq.( 5), the fi rst term enforces the  X  label smoothness " assumption. Following the previous work [ 38 ], the second term is a regularization term that p revents the refined label matrix being deviated too much from the initial weak matrix  X  Y . Notice that the label refinement problem in Eq.( 5) depends on the weight vector w a chieved by Eq.( 9), while l earning the weight vector w depends on the input data Y , as shown in Eq.( 9). In our problem, we update the label matrices Y 1 , . . . , n and the weight vector w iteratively.
I n this section, we aim to construct the multimodal represen-tation between the query instances and their corresponding top-ranked similar samples, which is based on the query-neighbor simi-larity function:  X  = {  X  k } k =1 , 2 ,...,N f . Generally, we can represent one facial image in different feature space, e.g. LBP feature, GIST feature, and Gabor feature. Suppose there are K kinds of features in total, we can represent by ( q ( k ) , d ( k ) i ) the query-neighbor fea-ture pair between the query image q and its i -th nearest sample d in the k -th feature space. Following the existing works on distance metric learning [ 47 , 46 ], we can define a distance metric M the k -th feature space, hence, the distance between q ( k ) can be expressed by a nd the inner product between q ( k ) and d ( k ) can be expressed by
Based on the k -th feature space and distance matrix M ( k ) are two ways to compute the similarity values between two in-stances: one way is using the heat kernel to transform the dis-tance value into a similarity value which is widely used in semi-supervised learning [ 2]. In detail, the similarity value between q a nd d ( k ) i can be computed as follows: where the query-neighbor similarity function  X  depends on the fea-ture type k and the parameter  X  . As a result, we can obtain k  X  X   X  | query-neighbor similarity functions, where  X  is the set of all possi-ble parameters  X  during the experiments.

Another way to compute the similarity value is using the sparse representation technique which has been adopted to construct ad-jacency matrix in some recent works [ 32 ]. In detail, in the k -t h feature space with M ( k ) as the distance metric, we can ob-tain the sparse representation s ( k ) for q ( k ) based on the dictionary D rithm [ 11 ], which can be formulated as follows: s  X  = arg min where s ( k )  X  is the achieved sparse representation with parameter  X  , K
DD is an n  X  n matrix with { K The i -th item in the sparse represent s ( k )  X  presents the represen-tative ability of the i -th dictionary instance d ( k ) i instance q ( k ) . Hence, the similarity value between q ( k ) computed as follows: where the query-neighbor similarity function  X  depends on the fea-ture type k and the parameter  X  . As a result, we can obtain k  X  X   X  | query-neighbor similarity functions, where  X  is the set of all possi-ble parameters  X  during the experiments.
 Finally, for each feature space, we must choose a distance metric M ( k ) . We can use the original feature space by setting M the identify matrix. To keep all the data points within the same classes close and separate all the data points from different classes far apart, it is better to adopt distance metric learning (DML) tech-niques to learn a better distance metric for each feature space re-spectively. Generally, any supervised DML algorithms can be used since the query-neighbor similarity function  X  is independent of the DML algorithms. In our problem, we adopt  X  X etric Learning to Rank" (MLR) algorithm [ 25 ] that learns a metric such that rank-i ngs of data induced by the learned distance are optimized against a ranking loss measure (e.g. ROC area (AUC) or MAP). In this setting, the  X  X elevant" results (in the same class) should lie close in space to the query, and  X  X rrelevant" results should be pushed far away.
In this section, we aim to find the optimal weight vector w for optimizing the multimodal fusion. In particular, given a label ma-trix Y i and a query-neighbor similarity matrix X i , we can directly achieve the annotation result of q i with the fusion vector w ac-cording to Eq.( 3). Hence, finding the optimal fusion vector w t hat achieves the best ranked name list in Eq.( 4) is equivalent to learning a multimodal annotation function with parameter w as follows: based on a set of training samples { ( q i , y q i , L i , X 1 , 2 , . . . , N t by minimizing the annotation errors. The input space contains all the multiplication results between label matrix Y query-neighbor similarity matrix X i . The output space  X  contains all the possible annotation results (the ranked name list ). Obvi-ously, the result of the function f is a structural output instead of a scalar value. Hence, it could be formulated as a structural SVM problem [ 34 , 19 ], which has been extensively studied in several re-s earch works and has been used for ranking problems in [ 48 , 25 , 45 ]. To specialize a general structure SVM algorithm for a parti c-ular problem, we define two functions: the  X  loss function "  X  and the  X  feature combination function "  X  .
The loss function is denoted as  X (  X  ,  X   X  ) in our problem, where  X  is the ground-truth ranked name list generated by the ground-truth label vector y , while  X   X  is the predicted ranked name list gen-erated by the predicted label vector  X  y . Notice that we omit the subscript for the query index for clarity. The  X  hit rate " at the top t annotated names is used as the performance metric, which mea-sures the likelihood of having the correct name among the top t annotated names. In real world applications, we prefer a high hit rate value with a small t value, that is, the correct names are at the top-ranked position in the ranked name list  X   X  .

For one query facial image, suppose it has t 1 correct names ( t 1 in our problem since we assume there is only one name for each person), all the correct names are at the top positions of the ground-truth name list  X  , followed by all the incorrect names. For the predicted name list  X   X  , if we only consider its top t 2 function can be formulated as follows: where h 1 ( , ) is a judgement function that equals 1 if the i -th name  X  in  X  is the same with the j -th name  X   X  j in  X   X  , and 0 otherwise. For example, if t 2 = 1 , we focus on only the first annotated name which means that if the first name in  X   X  is correct, then the loss value is 0 , otherwise, the loss value is 1 . If t 2 = m , the loss function in Eq.( 6) becomes a special case of MAP loss.
T ypically, the feature combination function aims to combine a set of feature vectors based on a ranking result. In our problem, the ranking result is denoted by  X  , which contains all the m names in the name set C = { c 1 , . . . , c m } . For a query facial image q , its name vector is y  X  X  0 , 1 } m with k y k 0 = 1 since each facial image has only one correct name. Hence, y k = 1 indicates that the k -th name c k in C is the correct name for the query image q . We denote by I 1 the index set of all the correct names, which contains only one item ( k ) according the previous case. Similarly, we denote by I the index set of all the incorrect names, which contains m  X  1 items w e define the feature combination function  X  to combine the input label matrix Y and the input query-neighbor similarity matrix X based on a ranked name list  X  , as shown in Eq.( 7):
 X  ( Y, X,  X  ) = 1 | I where Y k : is the k -th row of the label matrix L, and h is a ranking judgement function. If the name c i is ranked be-fore c j in the ranked name list  X  , h 2 ( c i , c j ,  X  ) = 1 ; otherwise, h ( c i , c j ,  X  ) =  X  1 if c j is ranked before c i .

Remark. For one group of input data ( Y , X , w ), we can compute the label vector with Eq.( 3): y = Y X  X  w , which is used to gen-erate the ranked name list  X  subsequently following the previous discussion. As shown in [ 45 ], based on the feature combination f unction in Eq.( 7), we can obtain the same ranked name list by s olving the following problem: where F ( w , Y, X,  X  ) is the discriminant function. It indicates that we can learn the weight vector w by maximizing the discriminant function F ( w , L, X,  X  ) over the a set of correct ranked label lists, and predict the new label vector of the unseen query with Eq.( 3).
U sing the loss function in Eq.( 6) and the f eature combination function in Eq.( 7), we can obtain the objective function to learn the w eight vector w based on the structural SVM, which is shown as follows: Algorithm 1: C utting plane algorithm for Eq.( 9)
Input : ( q i , y q i , X i , Y i ) , i = 1 , 2 , . . . , N
Output : weight vector w 1 W i  X  X  X  , for all i = 1 , . . . , N t 2 repeat 3 for i = 1 to N t do 4 E (  X  ; w )  X   X  (  X  q i ,  X  ) + w  X   X ( Y i , X i ,  X  ) 5  X   X  = arg max  X   X   X  E (  X  ; w ) 6 if E (  X   X  , w ) &gt;  X  i +  X  then 7 W i  X  W i  X  X   X   X  } 8 Get ( w ,  X ) by solve Eq.( 9) over W = S i W i 9 end 10 end 11 until no W i has changed during iteration ;
In the above formulation, the objective function of Eq.( 9) is sim-i lar to that of the general SVM algorithm, where C is a regulariza-tion parameter to tradeoff between the training error and the model complexity. For the constraints, if the value of discriminant func-tion F in Eq.( 8) for an incorrect ranking  X   X   X  \  X   X  i i s greater than that for one true ranking  X  q i  X   X   X  i , the slack variable  X  at least  X (  X  q i ,  X  ) , which indicates the sum of slacks P per bounds the empirical risk for the training samples based on the loss function defined in Eq.( 6). Since the number of constraints in E q.( 9) is extremely large, we adopt the cutting plane algorithm [ 19 , 48 ] to efficiently solve the optimization in Eq.( 9), as shown in Al-g orithm 1. More details about the cutting plane algorithm can be f ound in [ 19 ].
In the above, we separately discuss the three key factors that af-fect the final annotation result of the proposed SBFA framework, including the label matrix Y , the query-neighbor similarity matrix X and the weight vector w , which collectively determine the an-notation result as y = Y X  X  w . In this section, we will present the overall training for unifying all these three factors, and how to apply the models learned by L2NF for on-the-fly face annotation of a novel query facial image.
 Algorithm 2: L 2NF X  X lgorithm for training the models
Input : Training set ( q i , y q i , d ij , y ij ) in K feature spaces
Output : weight vector w and similarity function set  X  1 for k = 1 to K do 2 L earn the optimal distance metric M ( k ) in Section 3.4 3 end 4 B uild query-neighbor similarity functions  X  with varied combinations of  X   X   X  ,  X   X   X  , and M ( k ) , k = 1 , 2 , . . . , K 5 Construct query-neighbor feature matrix X i , i = 1 , . . . , N 6 repeat 7 G et the weight vector w by solving Eq.( 9) 8 for i = 1 to N t do 9 R efine the label matrix Y i by solving Eq.( 5) 1 0 end 11 until CONVERGENCE ;
Algorithm 2 shows the overall algorithmic framework for train-i ng the models by L2NF. At the beginning, we attempt to optimize each of the distance metrics for each facial feature space using the distance metric learning technique as discussed in Section 3.4 . Af-t er obtaining the set of optimal distance metrics M ( k ) construct the set of query-neighbor similarity functions  X  based on the set of multiple diverse facial feature representations and their distance measures. Using the query-neighbor similarity functions  X  , we can generate the query-neighbor feature matrices X each query q i in the training query set. Finally, we optimize both the optimal weight vector w and the refined label matrices Y by an iterative scheme. At the end of the whole training scheme, we obtain the final model that consists of the set of query-neighbor similarity functions  X  and the optimal weight vector w for multi-modal fusion.

The above training framework as shown in Algorithm 2 can be d one in an off-line learning manner. After completing the training, we can apply the model for online face annotation for naming a novel query facial image on-the-fly. Algorithm 3 summarizes the p roposed algorithm for on-the-fly annotation of an unseen query facial image. Specifically, given a new query facial image, we first find a short list of most similar faces based on CBIR techniques. Algorithm 3: A lgorithm of on-the-fly face annotation by L2NF
Input : Novel query q in K feature spaces, query-neighbor
Output : Annotation result  X  1 Retrieval the top-n similar images the { ( d i , y i ) } 2 Construct the query-neighbor similarity matrix X based on the query-neighbor similarity function  X  3 Obtain Y by refining the initial label matrix with Eq.( 5) 4 y = Y X  X  w  X  5 Get the ranked names list  X  by sorting y in descending order After that, we construct the query-neighbor similarity matr ix X based on the query-neighbor similarity function  X  . We then refine the initial label matrix Y of the current query using Eq.( 5). Finally, w e compute the label vector y by Eq.( 3) and obtain the final anno-t ation result  X  by sorting the label vector y in descending order.
Some web facial image databases are available on the WWW, which are used in some previous research works, e.g, LFW [ 18 ], Label Yahoo!News [ 14 ], 5 and FAN-Large. 6 Although the number o f persons in these three databases is large, the number of images for each person is quite small. For example, there are 13 , 233 im-ages of 5 , 749 people in the LFW database. The recent database PubFig [ 21 ] 7 is different from these databases. In detail, it was c onstructed by collecting online news sources. It contains 200 per-sons and 58 , 797 images. Due to the copyright issue, only image URL addresses are released. As some URL links are not available any more, 41,609 images are collected by our crawler in total. For each downloaded image, we crop the face image out according the provided face position rectangle and resize all the face images into the same size (128  X  128). We construct the query set by randomly collecting 10 images per person from the whole PubFig database. Hence, there are a total of 2 , 000 test query images used for per-formance evaluation, while the rest 39 , 609 images are used as the retrieval database. To construct the training set, we randomly col-lect 2 , 000 images in the same way from the retrieval database, with the rest 37 , 609 images as the retrieval database for the training set. Several facial images samples are shown in the first row of Figure 3. Figure 3: Face image examples in Pubfig database (the first r ow) and WDB database (the second row)
To evaluate the L2NF framework on weakly labeled web facial images, we use another western celebrity database:  X  X eakly la-beled web facial image database" (WDB for short), which has been released in [ 38 ]. There are a total of 1 , 6 00 query images with ground truth in the WDB database. In our experiment, we divide t hese queries into two parts of equal size, and randomly choose one part for model training. In  X  X DB" database, there are a to-tal of four retrieval databases of different sizes. In our experiment, we use two sub-databases of different scales:  X  X DB-040K" and  X  X DB-600K".  X  X DB-040K" is a smaller database with 53 , 448 images belonging to 400 persons, while  X  X DB-600K" is a large-scale database with 714 , 454 belonging to 6 , 000 persons. All the facial images were aligned into the same well-defined positions by the face alignment algorithms in [ 44 ], as shown in the second row o f Figure 3.

T o construct the query-neighbor similarity functions, we adopt three kinds of features as the facial descriptors: the LBP feature [ 1], t he GIST feature [ 31 , 38 ], and the Gabor feature [ 23 ]. In par-t icular, the 2891 -dimensional LBP feature is extracted by divid-ing the face images into 7  X  7 blocks. To reduce the computa-tion complexity, the LBP feature is further projected into a lower 500 -dimensional feature space using Principal Component Anal-ysis (PCA). Both GIST features and Gabor features are extracted over the whole aligned facial images. The parameter set for heat kernel is  X  = { X  1 , 0 , 1 , 2 , 3 , 4 } , while the parameter set for sparse The parameters  X  and C of L2NF are set as 10 and 1000 , respec-tively. For the distance metric learning algorithm (MLR), the pa-rameter C is set as 10 .

Following the previous works, we adopt the hit rate at top-T annotated results as the performance metric to evaluate the annota-tion performance, which measures the likelihood of having the true label among the top-T annotated names for a query facial image. We compare the proposed  X  X 2NF" framework with several exist-ing works that are proposed for web-scale face annotation or gen-eral image annotation, including  X  X LRLCC" [ 38 ],  X  X GSSL" [ 32 ],  X  MtBGS" [ 41 ],  X  X RR" [ 43 ], and a simple baseline algorithm that s imply adopts the weighted majority voting  X  X MV". We also extend the WLRLCC algorithm and the WMV algorithm into a multimodal scheme, by equally combining the face naming results from different facial feature spaces, denoted as  X  X LRLCC mm  X  X MV mm ".
This experiment aims to evaluate the face naming performance of the proposed  X  X 2NF" framework on the database  X  X DB-040K" by comparing with the aforementioned seven existing algorithms. For the facial image retrieval task in L2NF, we adopt the JEC algo-rithm to combine the distances from different face descriptors [ 24 ], w hich allows each individual distance to contribute equally. The same retrieval scheme is used to find the top-ranked similar images for the multimodal extensions:  X  X LRLCC mm " and  X  X MV mm ". For the single model solution, we use the GIST feature as the facial descriptor, which is similar to the experiment setting in [ 38 ]. For t he 1 , 600 query facial images, we randomly select half of them to learn the distance metrics of different facial features and the multi-modal representation combination w . Such a procedure is repeated 10 times and the average performance is computed over the 10 tri-als, as shown in Table. 1.

S everal observations can be drawn from the results. First, for the the single model solution, the WLRLCC algorithms achieves the best performance by using only one type of facial feature (GIST). In detail, the simple baseline WMV is about 60 . 9% with T = 1 , which is boosted to 76 . 7% by WLRLCC. Second, if multiple facial features are available, the performance of the multimodal WLRLCC mm increases to 80 . 9% , and 65 . 6% for the multimodal WMV mm . It indicates that using multiple facial representations is Table 1: Face naming performance on database  X  X DB-040K". helpful for the face naming task which validates the importan ce of this study. More specifically, the improvements of these two algo-rithms ( WLRLCC mm and WMV mm ) in the multimodal scheme are mainly gained from two aspects: (i) the retrieval result becomes better when multiple features and distance measures are combined by JEC [ 24 ]. For example, for the WMV algorithm, if we use the m ultiple features for the retrieval step but only use GIST feature for the annotation step, its performance is 64 . 2% , which is higher than the one that uses only GIST feature for both retrieval and annotation steps( 60 . 9% ). (ii) the combination enlarges the probability that the correct name is chosen. Both of the two aspects are beneficial for the L2NF framework. Last but not least, the proposed L2NF frame-work can further improve the face naming performance to 86 . 6% , which indicates the constructed multimodal representations are dis-criminative and the learned fusion vector can efficiently combine various query-neighbor similarity function in different facial fea-ture spaces. The performance improvement is mainly gained from three aspects: the refined label matrix, the constructed multimodal representation based on distance metric learning techniques, and the learned optimal combination of various query-neighbor similar-ity functions. More details will be further discussed in Section 4.5 .
This experiment aims to evaluate the face naming performance of the proposed L2NF framework on two different larger facial im-age databases:  X  X DB-600K" and  X  X ubFig". The two databases were collected under very different approaches and settings, which can help us evaluate the generalization of the proposed technique on real-world data under different scenarios. For clarity, we mainly focus on the evaluation of the algorithms using multimodal rep-resentations. The experimental results are shown in Figure 4 and T able 2.

W e can make several observations from the results. First of all, similar to the previous observations, the proposed L2NF framework consistently achieves the best annotation performance among all the compared algorithms. It shows that for different databases the proposed L2NF algorithm is always helpful to improve the anno-tation performance. Secondly, the performance on  X  X DB-600K" is lower than the one on  X  X DB-400K" which is consistent with the previous observation in [ 38 ], since increasing the number of p ersons leads to a larger database of more images, which makes the retrieval task more challenging. Finally, it is interesting to ob-serve the overall annotation performance on the PubFig database is worse than the results on WDB-series databases. There are several F igure 4: Face naming performance (hit rate @ top-T) on the two databases:  X  X DB-600K" and  X  X ubFig". reasons for this observation: (i) the number of images per person varies a lot in the PubFig database, in which several persons own only about 20 images. It is insufficient for the data-driven scheme, hence the annotation performance is reduced; (ii) All the facial im-ages in the PubFig database are cropped according to the face posi-tion without adopting any face alignment algorithms, which makes the facial descriptor sensitive for face views.
This experiment aims to evaluate the impact of the training query sizes in the L2NF framework based on the  X  X DB-040K" database. In the previous experiments, we adopt half of the 1 , 600 query im-ages as the training set. In this experiment, we evaluate the an-notation performance under varied number of training query im-ages. Specifically, instead of using all the training samples (to-tally 800 ), we build three small training sets by randomly collect-ing 200 , 400 and 600 query images, respectively. The experimen-tal results of hit-rate @ top-1 performance are shown in Figure 5. F rom the results, it is obvious that the face naming performance increases when more training samples are available, and the final performance tends to become saturated when the training query size is above 600 . Finally, even with a small number of queries for training, e.g., only 200 training samples, the L2NF algorithm can achieve a good performance ( 83 . 4% ), which remains much better than the state-of-the-art  X  X LRLCC mm " scheme.
 Figure 5: Face naming performance (hit rate @ top-T = 1 ) of L2NF with varied sizes of training queries.
This experiment aims to analyze how different factors affect the face naming performance by the proposed L2NF scheme as shown in Figure 2 (b). In particular, there are three key factors: the refined label matrix, the constructed multiple representations, an d the op-timal weight vector for multimodal fusion.
 Table 3: Evaluation and analysis of the performance gains.
First of all, to examine the efficacy of the refined label matrix Y , w e compare it with the initial raw label matrix  X  Y using the simplest baseline algorithm WMV by excluding other factors in affecting annotation performance. Our result indicates that the refined label matrix can boost the performance from 60 . 9% (without refinement) to 62 . 0% (after refinement). Further, we examine the efficacy of another two factors as shown in Table 3. We denote by M  X  learned distance metric and w  X  the optimal multimodal fusion vec-tor. When the weight vector w is fixed to 1 and the distance metrics are based on Euclidean distance ( M is set to an identity matrix), the hit-rate @ top-1 performance of the resulting L2NF algorithm (de-noted as L2NF w =1 M = I ) is 79 . 4% . This value can be boosted to 84 . 3% if we adopt the optimized metric M  X  (denoted as L2NF w =1 and further boosted to 86 . 6% if we also use the optimal weight conclusion, the proposed L2NF framework is able to leverage all the three factors for achieving the state-the-art performance in a systematic and synergic scheme.
Despite the promising results on the benchmark search-based face annotation tasks, our work still have some limitations. First, we assume each name corresponds to a unique single person. How-ever, this is not always true for real-life scenarios. For example, it is possible that two persons have the same name or one person may have multiple names. Such kind of practical duplicate name is-sues may be partially solved by extending our algorithms, e.g., via learning the similarity between any two names both in the name space and visual space. Second, we assume the top retrieved web facial images are related to the query name. This is clearly true for celebrities who have many photos on the internet. However, when the query facial image is not a well-known person, there may not exist many relevant facial images on the WWW. This is a common limitation of all existing data-driven annotation techniques. Finally, although the performance of L2NF is much better, more facial fea-ture are used which means more computational cost and storage space. We may overcome the limitation by adopting hashing tech-niques in our further work.
This paper investigated an emerging paradigm of search-based face annotation for automated face naming through mining large-scale web facial images freely available on the WWW. We pro-posed a novel framework of  X  X earning to Name Faces" (L2NF) by exploring multi-modal learning on weakly labeled facial image data. In particular, our framework has three major contributions: (i) we suggest enhancing the initial weak labels by a graph-based re-finement scheme based on the  X  X abel smoothness" assumption; (ii) we propose to explore multiple facial feature representations, and further optimize the distance metric on each facial feature space using distance metric learning techniques; and (iii) finally, we pro-pose to learn the optimal multimodal fusion of diverse facial fea-tures by formulating the problem as a learning to rank task, which can be efficiently solved by the existing structural SVM algorithm. We conduct a set of extensive empirical studies on two benchmark real-world facial image databases, in which encouraging results show that the proposed L2NF model significantly boosts the face annotation performance.
 This research was supported in part by MOE tier 1 project grant (RG33/11), Microsoft Research grant, Interactive and Digital Me-dia Programme Office (IDMPO) and National Research Founda-tion (NRF) hosted at Media Development Authority (MDA) under Grant No.: MDA/IDM/2012/8/8-2 VOL 01. Jianke Zhu was sup-ported by National Natural Science Foundation of China under the Grant (61103105). [1] T. Ahonen, A. Hadid, and M. Pietik X inen. Face description [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [3] T. L. Berg, A. C. Berg, J. Edwards, M. Maire, R. White, [4] J. Bu, B. Xu, C. Wu, C. Chen, J. Zhu, D. Cai, and X. He. [5] Z. Cao, Q. Yin, X. Tang, and J. Sun. Face recognition with [6] G. Carneiro, A. B. Chan, P. Moreno, and N. Vasconcelos. [7] B.-C. Chen, Y.-Y. Chen, Y.-H. Kuo, and W. H. Hsu. Scalable [8] J. Y. Choi, W. D. Neve, K. N. Plataniotis, and Y. M. Ro. [9] P. Duygulu, K. Barnard, J. de Freitas, and D. Forsyth. Object [10] J. Fan, Y. Gao, and H. Luo. Multi-level annotation of natural [11] S. Gao, I. W.-H. Tsang, and L.-T. Chia. Kernel sparse [12] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. [13] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. Face [14] M. Guillaumin, J. Verbeek, and C. Schmid. Multiple instance [15] A. Hanbury. A survey of methods for image annotation. J. [16] S. C. Hoi, R. Jin, J. Zhu, and M. R. Lyu. Semi-supervised [17] A. Holub, P. Moreels, and P. Perona. Unsupervised clustering [18] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. [19] T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane [20] M. G. Kresimir Delac and M. S. Bartlett. Recent Advances in [21] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. [22] D.-D. Le and S. Satoh. Unsupervised face annotation by [23] C. Liu and H. Wechsler. Gabor feature based classification [24] A. Makadia, V. Pavlovic, and S. Kumar. A new baseline for [25] B. Mcfee and G. Lanckriet. Metric learning to rank. In [26] T. Mensink and J. J. Verbeek. Improving people search using [27] D. Ozkan and P. Duygulu. A graph based approach for [28] X. Rui, M. Li, Z. Li, W.-Y. Ma, and N. Yu. Bipartite graph [29] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. [30] S. Satoh, Y. Nakamura, and T. Kanade. Name-it: Naming [31] C. Siagian and L. Itti. Rapid biologically-inspired scene [32] J. Tang, R. Hong, S. Yan, T.-S. Chua, G.-J. Qi, and R. Jain. [33] J. Zhu, S. C. Hoi, and M. R. Lyu. Face annotation by [34] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. [35] C. Wang, F. Jing, L. Zhang, and H.-J. Zhang. Image [36] C. Wang, S. Yan, L. Zhang, and H.-J. Zhang. Multi-label [37] D. Wang, S. C. Hoi, and Y. He. Mining weakly labeled web [38] D. Wang, S. C. H. Hoi, Y. He, and J. Zhu. Retrieval-based [39] X.-J. Wang, L. Zhang, F. Jing, and W.-Y. Ma. Annosearch: [40] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. [41] F. Wu, Y. Han, Q. Tian, and Y. Zhuang. Multi-label boosting [42] S. C. Hoi and M. R. Lyu. A multimodal and multilevel [43] Z. Wu, Q. Ke, J. Sun, and H.-Y. Shum. Scalable face image [44] J. Zhu, S. C. Hoi, and L. V. Gool. Unsupervised face [45] J. Xu, T.-Y. Liu, M. Lu, H. Li, and W.-Y. Ma. Directly [46] S. C. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma. Learning [47] L. Yang and R. Jin. Distance metric learning: A [48] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support [49] L. Wu, S. C. Hoi, R. Jin, J. Zhu, and N. Yu. Distance metric
