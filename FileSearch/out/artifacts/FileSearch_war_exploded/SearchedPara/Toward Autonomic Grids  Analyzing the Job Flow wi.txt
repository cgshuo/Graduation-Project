 The Affinity Propagation (AP) clustering algorithm proposed by Frey and Dueck (2007) provides an understandable, nearly optimal summary of a dataset, albeit with quadratic compu-tational complexity. This paper, motivated by Autonomic Computing, extends AP to the data streaming framework. Firstly a hierarchical strategy is used to reduce the complex-ity to O ( N 1+  X  ); the distortion loss incurred is analyzed in relation with the dimension of the data items. Secondly, a coupling with a change detection test is used to cope with non-stationary data distribution, and rebuild the model as needed. The presented approach Strap is applied to the stream of jobs submitted to the EGEE Grid, providing an understandable description of the job flow and enabling the system administrator to spot online some sources of fail-ures.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms, Similarity measures Algorithms, Experimentation Affinity Propagation, Autonomic Computing, Online Clus-tering
The clustering of large-scale dynamic datasets is a key is-sue for most application domains, at the crossroad of data-bases, data mining and machine learning [3]. High perfor-mance computers and large-size memory storage do not per se sustain scalable and accurate clustering. Typically, ad-vances in large-scale clustering (see e.g., [12]) mainly proceed by distributing the dataset and processing the subsets in parallel; when dealing with dynamic datasets, such Divide-and-Conquer approaches face some limitations in terms of latency and/or communication costs.

Furthermore, the choice of a clustering method must re-flect the applicative needs. Our motivating application per-tains to the strategic field of Autonomic Computing [19], aimed at providing large computational systems with self-modelling, self-configuring, self-healing and self-optimizing facilities. More specifically, the applicative goal of the present paper is to enable the administrator of a large-scale grid system, the EGEE Grid 1 , to analyze the flow of jobs sub-mitted to and processed by the grid. The input data thus is made of the Logging and Bookkeeping (L&amp;B) files, auto-matically generated by the grid middleware. As noted by [7], modern data mining is more and more concerned with au-tomatically generated datasets ( X  X omputers are fueling each other X ); building understandable summaries thereof is even more critical. For this reason, it is highly desirable that a job cluster be summarized by an actual job (as opposed to an artefact, as done in K -means; more in Section 4).
Affinity Propagation (AP), a message passing-based clus-tering algorithm proposed by Frey and Dueck [6], does sat-isfy the above interpretability constraint. Akin K -centers, AP maps each data item onto an actual data item, called ex-emplar , and all items mapped onto the same exemplar form one cluster. Contrasting with K -centers, AP builds quasi-optimal clusters in terms of distortion (section 2.1), thus enforcing the cluster stability [6]. The price to pay for these understandability and stability properties is AP quadratic computational complexity, severely hindering its usage on large scale datasets.

In an earlier work [22], a 2-level hierarchical approach was proposed to decrease AP complexity from O ( N 2 ) to O ( N 3 / 2 ), where N denotes the number of items. Indepen-dently, some coupling with a change detection test was in-The EGEE grid was established in the EU project Enabling Grid for E-SciencE, http://www.eu-egee.org . It involves 41,000 CPUs, 5 Petabytes storage and concurrently supports 20,000 jobs on 24/24, 7/7 basis. vestigated to extend AP to dynamic data distributions, en-abling online clustering aka Data Streaming [22].
This paper features three contributions along these same lines. Firstly, a straightforward generalization of the 2-level hierarchical approach is proposed, showing that a h -level hierarchical approach would reduce the computational com-plexity to O ( N h +2 h +1 ) up to poly-logarithmic terms. Secondly, the price to pay for this complexity reduction, namely the distortion loss incurred along the hierarchical Divide-and-Conquer, is analyzed; the distortion loss is shown to be negligible except in the particular case of two-dimensional datasets. Thirdly, an adaptive mechanism inspired from [21] is used to optimize the change-detection test parameters.
The extended Strap algorithm ( Streaming Affinity Prop-agation ) is finally applied to a challenging real-world prob-lem, the online monitoring of the EGEE grid. The Strap specificity compared to prominent data streaming algorithms (e.g., [2, 3, 7]) is twofold. On the one hand, Strap inherits AP understandability, modelling the data stream through exemplars (actual data items) as opposed to artefacts. On the other hand, this model is available at any time step contrasting with e.g., [2]. Strap thus makes it feasible to provide the EGEE administrator with a real-time dashboard of the job data flow, enabling the discovery of anomalies.
The paper is organized as follows. Section 2 first briefly describes AP for the sake of self-containedness, before pre-senting Divide-and-Conquer AP. The computational com-plexity thereof is derived, and the distortion loss is analyzed. The Strap algorithm extending Hi-AP to data-streaming is presented in Section 3, and the self-adaptive change de-tection test is detailed. Section 4 describes a large scale real-world application: the profiling of 5M+ jobs submitted to the EGEE grid. Strap is assessed in terms of algorithmic robustness and performance compared to a k -centers base-line approach, and the added value for the grid administrator is discussed. The paper concludes with some perspectives for further research.
This section presents the Affinity Propagation algorithm, referring the reader to [6] for a comprehensive description. How to make AP scalable, and what is the price to pay for the complexity reduction, are described thereafter.
Let E = { e 1 , . . . , e N } denote a set of N items, and let d ( e i , e j ) denote the distance or dissimilarity between items e and e j . Letting K denote a positive integer, the K -center problem consists of finding K items e i 1 , . . . , e i K ferred to as exemplars, minimizing the dataset distortion L : defined as the squared distance between e j and its closest exemplar, summed over all e j in E .

Affinity Propagation is a message-passing algorithm tack-ling the above optimization problem as follows. Let c be defined as a mapping from E onto E , associating to each item e i its exemplar c i ( c i  X  X  ).
 with where Parameter s  X  , the penalty for having an additional exem-plar, controls the clustering granularity: s  X  = 0 leads to the trivial solution where every item is an exemplar; s  X  =  X  leads to the single-cluster solution. Contrasting with K -centers, AP only indirectly controls the number of clusters through s  X  (usually set to the median value of d 2 ( e i  X  i [ c ] is a set of constraints initially meant to enforce the fact that, if e i is chosen as exemplar, it must be its own ex-emplar (clusters are ball-shaped) [6]. A relaxation of the  X  constraints enabling tree-structured clusters, was proposed by [13].

The Divide-and-Conquer approach presented next relies on the Weighted AP algorithm (WAP) [22], extending AP to the case of multiply-defined items, and/or dense subsets of items. Let F denote a subset of E , made of n items with small pair distance (  X  e i , e j  X  F , d 2 ( e i , e proceeds by replacing all items in F with a single example e . The clustering problem defined on E is made equivalent to the one defined on ( E\F )
The WAP algorithm will be used in the remainder of the paper to iteratively cluster exemplars produced in former clustering steps.
AP computational complexity 2 is N 2 log ( N ); it involves the matrix S of pair distances, with quadratic complexity in the number N of items, severely hindering its use on large-scale datasets.

This AP limitation can be overcome through a Divide-and-Conquer heuristics inspired from [9]. Dataset E is ran-domly split into b data subsets; AP is launched on every sub-set and outputs a set of exemplars; the exemplar weight is set to the number of initial samples it represents; finally, all weighted exemplars are gathered and clustered using WAP (the complexity is O ( N 3 / 2 ) [22]). This Divide-and-Conquer strategy  X  which could actually be combined with any other basic clustering algorithm  X  can be pursued hierarchically in a self-similar way, as a branching process with b repre-senting the branching coefficient of the procedure, defining the Hierarchical AP ( Hi-AP ) algorithm.

Formally, let us define a tree of clustering operations, where the number h of successive random partitions of the data represents the height of the tree. At each level of the hierarchy, the penalty parameter s  X  is set such that the ex-pected number of exemplars extracted along each clustering step is upper bounded by a constant K .
Except if the similarity matrix is sparse, in which case the complexity reduces to NKlog ( N ) with K the average con-nectivity of the S matrix [6].
Proposition 2.1. Let us define the branching factor b as Then the overall complexity C ( h ) of Hi-AP is given by up to logarithmic terms.

Proof. M = N/b h is the size of each subset to be clus-tered at level h ; at level h  X  1, each clustering problem thus involves bK = M exemplars with corresponding complexity The total number N cp of clustering procedures involved is with overall computational complexity: It is seen that C (0) = N 2 , C (1)  X  N 3 / 2 ,. . . , and C ( h )  X  N for h  X  1 .
Let us examine the price to pay for this complexity re-duction. As mentioned earlier on, the clustering quality is assessed from its distortion, the sum of the squared distance between every data item and its exemplar:
The distortion loss incurred by Hi-AP w.r.t. AP is ex-amined in the simple case where the data samples follow a centered distribution in IR d . By construction, AP aims at finding the cluster exemplar r c nearest to the center of mass of the sample points noted r cm : The distortion loss incurred by Hi-AP can be assessed from the relative entropy, or Kullback Leibler distance, between the distribution P c of the cluster exemplar computed by AP, and the distribution P c ( h ) of the cluster exemplar computed by Hi-AP with hierarchy-depth h :
In the simple case where points are sampled along a cen-tered distribution in IR d , let  X  r c denote the relative position of exemplar r c with respect to the center of mass r cm : The probability distribution of  X  r c conditionally to r cylindrical; the cylinder axis supports the segment (0, r where 0 is the origin of the d -dimensional space. As a result, the probability distribution of r cm +  X  r c is the convolution of a spherical with a cylindrical distribution.

Let us define the following notations. Subscripts sd refer to sample data, ex to the exemplar, and cm to center of mass. Let x  X  denote the corresponding square distances to the origin, f  X  the corresponding probability densities and F their cumulative distribution. Assuming and exist and are finite, then the cumulative distribution of x of a sample of size M satisfies by virtue of the central limit theorem. In the meanwhile, x f ex = x ex  X  x cm has a universal extreme value distribution (up to rescaling): where  X   X  6 =  X  stands for the fact that the extreme value parameter is possibly affected by the displacement of the center of mass. To see how the clustering error propagates along with the hierarchical process, one proceeds inductively. At hierarchical level h , M samples, spherically distributed with variance  X  ( h ) are considered; the sample nearest to the center of mass is selected as exemplar. Accordingly, at hier-archical level h +1, the next sample data is distributed after the convolution of two spherical distributions, the exemplar and center of mass distributions at level h . The following scaling recurrence property (proof in appendix) holds:
Proposition 2.2. with It follows that the distortion loss incurred by Hi-AP does not depend on the hierarchy depth h except in dimension d = 2 .
Fig. 1 shows the distribution of the clustering distortion depending on the hierarchy-depth h and the dimension d of the dataset. The distortion curve for h = 1 corresponds to the AP case, showing that the distortion loss due to the hi-erarchical approach is moderate to negligible in dimension d 6 = 2 provided that the number of samples per cluster at each clustering level is  X  X ufficient X  (say, M &gt; 30 for the law of large numbers to hold). In dimension d &gt; 2, the distance of the center of mass to the origin is negligible with respect to its distance to the nearest exemplar; the distortion be-haviour thus is given by the Weibull distribution which is stable by definition (with an increased sensitivity to small sample size M as d goes to 2). In dimension d = 1, the dis-tribution is dominated by the variance of the center of mass, yielding the gamma law which is also stable with respect to Figure 1: Radial distribution plot of exemplars ob-tained by clustering of Gaussian distributions of N = 10 6 samples in IR d in one single cluster exem-plar, with hierarchical level h ranging in 1,2,3,6, for diverse values of d : d = 1 (upper left), d = 2 (upper right), d = 3 (bottom left) and d = 4 (bot-tom right). Fitting functions are of the form f ( x ) = the hierarchical procedure. In dimension d = 2 however, the Weibull and gamma laws do mix at the same scale; the overall effect is that the width of the distribution (of the distortion) increases like h 2 , as shown in Fig. 1 (top right).
This section is concerned with adapting Hi-AP to on-line clustering and dynamic data distributions, defining the Strap algorithm. Strap combines Hi-AP with a change detection test: if the test is triggered, it is likely that the data distribution has changed and the stream model is re-built. Two extensions have been brought to Strap initial version [22]. An adaptive mechanism inspired from [21] is used to automatically optimize the parameters of the change detection test. Secondly, the exemplars built by Strap fuel an offline clustering process, enabling some multi-scale de-scription of the data stream. This section finally discusses the strengths and weaknesses of Strap with respect to the state of the art.
The early Strap algorithm is summarized for the sake of self-containedness, referring the reader to [22] for more detail. The model of the stream is initialized by applying Hi-AP to the first data items. Formally, the stream model is made of a set of clusters C i = ( e i , n i ,  X  i , t i ), where e cluster exemplar, n i and  X  i respectively stand for the clus-ter size and distortion, and t i is the last time stamp when a data item joined the cluster.

As the stream flows in, current data item e t is checked against the model. If its distance to the nearest exemplar e is less than a threshold computed in the initialization step, e joins the C i cluster. The C i time stamp is set to the cur-rent time step t , while C i size and distortion are updated by relaxation. The model update is parameterized from a (user supplied) time length  X ; the idea is that clusters which have not received any additional item during  X  consecutive time steps [22] should disappear. If data item e t does not fit the model, it is considered to be an outlier and put in the reservoir. The reservoir gathers the last M outliers.
A change point detection test is used to monitor the sta-bility of the data distribution. The so-called Page Hink-ley (PH) statistical test [16, 11] is applied to the outlier rate (section 3.2). Upon triggering the PH test, the stream model is rebuilt using WAP from the current model (exem-plars weighted by the current size of the associated cluster) and the outliers in the reservoir.
 Algorithm 1 Strap Algorithm Data streams e 1 , . . . e t , . . . ; fit threshold  X  Init for t &gt; T do end for
Among the main change detection tests are Wald tests, also referred to as SPRT (sequential probability ratio test [15]), and CUSUM test (cumulative sum [16]); kernelized versions of the CUSUM test have also been developed, see e.g., [10]. The Page-Hinkley test has been used within the Strap algorithm because it minimizes the time expectancy before detecting a change, conditionally to a given false alarm rate [16, 11].

Formally, the PH test monitors a scalar random variable p . The test is parameterized after a threshold  X  , classi-cally governing the rate of false alarms; additionally, a small constant tolerance parameter  X  has been used to cope with slowly varying distributions: In its current state, the PH test is only triggered when p tends to increase, as the monitored variable p t relates to the presence of outliers. When the rate or severity of outliers decreases, there is no need to rebuild the stream model. Several scalar indicators p t have been considered, among which the distance of the current data item to the nearest exemplar, possibly normalized by the associated distortion. Empirically, the best performing indicator is found to be where 1 o t is set to 1 if the current data item is an outlier and 0 otherwise, and O t is the fraction of data items considered to be outliers since the model was last (re)built.
Threshold  X  is adjusted in order to optimize the model representativity, in the spirit of the Bayesian Information Criterion [20]. The optimization criterion is set to:
F  X  =  X  1 where | C | is the number of clusters, e  X  i and n i respectively the exemplar and size of the i -th cluster, d the dimension of the data stream, N the number of data items recognized by the stream model since the last restart,  X  and  X  are two constants to make the penalty term on the same scale as the distortion item.

The optimization of  X  has been tackled in a discrete (con-sidering a finite set of values) and a continuous (considering a continuous domain) setting, respectively using  X  -greedy optimization and a Gaussian Process-based estimate of F  X  [21].
While data streaming aims at providing an accurate de-scription of the instant flow distribution, it might be desir-able to also provide X  X he big picture X , depicting the evolution of this distribution on a larger time scale.

The fact that at each time step the stream model is based on exemplars makes it natural to apply Hi-AP on the over-all set of exemplars gathered along time 3 , thus extracting  X  X uper-exemplars X . These super-exemplars capture the var-ious trends of the data stream along time, enabling to char-acterize any period (day, week or month) after the represen-tativity of each such super-exemplar (number of data items falling in each super-cluster).
 Figure 2: Online (1st level) and retrospective (2nd level) representation of a data stream with Strap .
Among the prominent challenges of Data Streaming (see e.g., [8, 3]) are the computational time and space resources needed, on the one hand, and the efficient modeling of non-stationary distributions on the other hand. There is lit-tle doubt that the computational requirements of a data streaming algorithm govern its usability; typically when the system under examination generates the equivalent of 27 CDs per minute, linear or quasi-linear computational com-plexity is the maximum one can afford to keep up with real-time processing. The second challenge, namely the pursuit
The exemplars with low representativity are filtered out. of a moving target distribution, has been extensively consid-ered in the Signal Processing and Data Analysis literature [1]. It however needs to be reconsidered in the Data Stream-ing context, subject to the above mentioned computational limitations. This challenge can be viewed as yet another in-stance of the Exploration vs Exploitation dilemma; indeed, a competent data streaming algorithm should simultaneously be able to catch up with any true change in the data distri-bution (exploration) while discarding outliers in order not to spoil the model (exploitation).

A third and equally important challenge is to provide the user with understandable results. As ill-defined as under-standability might be, it remains that providing understand-able results is mandatory in order to keep the user in the loop [5]. The presented Strap algorithm aims at understand-able, stable and computationally efficient Data Streaming, through the selection of the exemplars best representing the (majority of) data items at any time step. The Continuous Distributed clustering (CDC) algorithm presented by Cor-mode et al. [3] is most related to Strap , with two impor-tant differences. Firstly, CDC is interested in  X  X onquering the divide X , i.e. building a model of a distributed stream, whereas Strap is interested in splitting the data stream to overcome the complexity barrier. Secondly, CDC is based on K -centers and its goal is to minimize the radius (maximal distance between a point and its exemplar) or the diameter (maximal distance between two points with same exemplars) of the clustering, whereas the Strap goal is to minimize the distortion; the difference between both criteria can be un-derstood as the difference between L  X  and L 1 or L 2 norms.
To our best knowledge, Strap is the only Data Stream-ing algorithm modeling a centralized data flow through a set of exemplars. This unique feature of Strap is both a strength and a weakness compared to the Data Streaming algorithms at the state of the art. On the weak side, Strap was shown to be slower by an order of magnitude than Den-Stream [2] on the KDD Intrusion Detection Dataset [22]. This lesser computational efficiency is blamed on two facts. Firstly, DenStream , extending the DBScan clustering algo-rithm [4] to the streaming context, constructs an artefact-based model, smoothly updating an implicit K -means-like model at any time step, whereas Strap explicitly rebuilds the model whenever some change in the underlying data dis-tribution is detected. Secondly, the data model is available at any time step in Strap , whereas it is only computed upon request by DenStream .

In counterpart, to our best knowledge Strap is the only applicable Data Streaming algorithm when i) the data repre-sentation makes it impossible to build artefacts; ii) some per-formance and stability/reproducibility guarantees are needed, barring the use of K -centers. Domains such as molecular chemistry, image processing, or social networks fall in the first category (e.g., defining an  X  X verage X  molecule still is an open problem). Safety-critical domains fall in the second category. This section reports on the application of Strap to the Autonomic Grid context, specifically the monitoring of the jobs submitted to the EGEE grid. After briefly describing the application, the section describes the goal of experiments and the experimental setting, before discussing the empirical results.
Grid Monitoring involves two main functionalities: acqui-sition and usage of the relevant information. The acquisition functionality includes sensors that instrument grid services or applications, and data collection services that filter, cen-tralize and/or distribute the sensor data to the usage func-tionality. Usage, which is more specifically investigated in this paper, includes consumer services such as real-time pre-sentation and interpretation. It also includes middleware services as far as feedback loops are considered, typically in the Autonomic Computing framework. Many architec-tures and integration frameworks offer advanced presenta-tion, user interaction and reporting facilities, such as the EGEE dashboard [23] and Real Time Monitor [24]. Data interpretation, aimed at revealing meaningful (compound) features which go beyond elementary statistics, is much less developed in the grid area.

The goal of the proposed Job Streaming facility, enabling the real-time inspection of the jobs submitted to and pro-cessed by the grid, is to provide some interpretation of the grid running status. The job stream considered in the fol-lowing is the log of 39 Resource Breakers (RB) of all gLite-operated jobs in the whole EGEE grid from early January 2006 to end of May, including a total of 5,268,564 jobs. Through the Real Time Monitor system (RTM) [24], the acquisition module provides a real-time description of the jobs through XML records (available at http://www.grid-observatory.org/ ). Each job is labeled after its final sta-tus, successfully finished ( good job ) or failed. Circa 45 error classes (e.g.,  X  X ancel requested by WorkloadManager X ,  X  X B Cannot plan X ) exist; about 25 error classes are significantly represented (with more than 1,500 occurrences) in the job stream. These labels will not be accounted for in the cluster-ing process, for the following reason. Following grid experts, error types do not necessarily relate to operational aspects and could blur the picture of the grid status. For instance, although cannot plan means that the Resource Broker was unable to find a matching resource, the real cause might be that the user X  X  requests were truly unreachable; or the Bro-ker information is stalled and does not see that resources have been released. Therefore, the job labels will only be used a posteriori to assess the clustering performance.
Each job is described by 6 continuous and 6 boolean at-tributes 4 . The first 6 attributes describe the time cost du-ration spent in different services along the job lifecycle: 1. Submission Time : time for submission to Workload Man-agement System (WMS) 2. Waiting Time : time to find a matching resource 3. Ready for Transfer Time : time acceptation and transfer to the found resource, reported by JobController (JC) 4. Ready for CE accept Time : the same as Ready for Transfer Time, but reported by LogMonitor (LM) 5. Scheduled Time : queuing delay in local cite 6. Running Time : execution time.
An additional categorical attribute, the name of the queue visited by the job will not be considered in this paper, al-though a proper handling of categorical attributes was the main motivation for using exemplar-based clustering as op-posed to K  X  means approaches.

In principle, attributes 3 and 4 are redundant (JC is a standalone logging service, while the LM integrates various logs, and returns them in the L&amp;B database); as will be seen, the discrepancy between both attributes however pro-vide useful clues about grid misbehaviors. All numerical attributes are centered and normalized; the first data sub-set is used to estimate the average and standard deviation, which are thereafter updated using an additive relaxation scheme as the dataflow goes in.

In case the job does not reach a given service due to a failure in the job lifecycle, the durations of all subsequent services are set to 0. Six additional boolean attributes are thus considered, indicating whether the job reaches the cor-responding service. The job dissimilarity is the Euclidean distance on IR 12 .
The goal of the experiments is to assess the Strap algo-rithm from an algorithmic and an applicative perspectives. On the one hand, Strap is assessed from its ability to pro-vide useful hints on the grid state. On the other hand, the algorithmic performance of Strap is assessed with compari-son to hierarchical k -centers streaming 5 , measured after four criteria: The Clustering Accuracy and Clustering Purity are classically measured with respect to the job labels: the ac-curacy is the percentage of jobs with same class label as their exemplar; the purity is the fraction of the jobs in each clus-ter belonging to the majority class of the cluster, averaged over all clusters. The clustering purity is known to be more robust than the clustering accuracy in case of imbalanced clusters and/or classes.
 The Clustering Stability measures the clustering perfor-mance after [14]. Specifically, a set of super-exemplars com-puted for a given setting s  X  of Strap induces a partition C ( s  X  ) of the jobs; the sensitivity of the algorithm is mea-sured from the independence of the partitions obtained for different settings, defined as the sum taken over all clusters C i  X  C ( s  X  ) , C 0 j  X  C ( s  X  0 ) of the quantity The Streaming Stability measures the stability of the stream model w.r.t. the change detection test, and parame-ter  X  . The model dynamics is reflected by the restart sched-ule (number of restarts per day); its stability is assessed by computing the correlation of the restart schedules obtained for various values of  X  ; the significance of the correlation is measured after a permutation test (considering the correla-tion values obtained for 100 restart schedules with randomly ordered days).

In all experiments, the penalty parameter s  X  of AP is ini-tially set to the median similarity value in the first bunch of the data stream, and updated by relaxation from the se-quence of data considered.

The outlier threshold is set to  X  = 0 . 25. The PH tolerance threshold is set to  X  = 0 . 01. The PH threshold parameter  X  is adjusted online, using discrete or continuous optimisation. In the discrete case,  X  ranges in 40 , 50 , . . . 120 and a  X  -greedy
The baseline algorithm is defined by replacing AP with k -centers with multiple restarts; the best performance out of 30 restarts is kept, ensuring that Strap and the baseline algorithm have same computational runtime. optimization of the empirical average distortion (section 3.2) is achieved (  X  = 5%). In the continuous case, a Gaussian Process-based estimate of the distortion (Eq. (3)) is built, the the  X  value with minimal estimated empirical distortion is selected and the model is updated [21].
The accuracy and purity of the Strap modelling are re-spectively displayed in Fig. 3 and 4. With respect to the 21 classes of jobs, the accuracy is consistently over 85%, signif-icantly outperforming the baseline algorithm. The adaptive adjustment of the  X  parameter, based on discrete or contin-uous optimization, preserves Strap accuracy while decreas-ing the number of restarts (omitted for space limitations). The clustering purity (Fig. 4) is over 90% and confirms the quality of the clustering model. The relatively high number of clusters (circa 200) must be understood in relation with the average number of jobs per day (circa 15,000). Unex-pectedly, the clustering purity is higher than the accuracy, although the former indicator usually is a pessimistic one (since all clusters, including those related to rare classes, have same weight); experimentally, the difference in perfor-mance is explained as rare failure classes are associated to pure clusters.
The stability of the data stream model is measured consid-ering various values of the AP penalty parameter s  X  , ranging the partitions induced by the super-exemplars (clusters with representativity less than .5% of the jobs have been filtered out): columns 3 and 4 respectively indicate the number of clusters obtained for s  X  values in columns 5 and 6. The actual correlation (column 1) is assessed from the reference value (column 2), computed as the best correlation of clus-tering C i with 100 random perturbations of C 2 .
Finally, the stability of the model dynamics is measured by varying  X  in 40, 50, 75 and 100, respectively inducing 699, 558, 371 and 284 restarts. The correlation between the restart schedules is significant up to the confidence level 99% (Table 2): Figure 4: Strap : Clustering Purity and Number of Clusters, measured at each restart step (discrete op-timization of the change detection test parameter). Table 1: Stability of the Data Stream Clustering Model Table 2: Correlation between the restart schedules for different  X  values (Reference = maximal corre-lation out of 100 permutation tests)
The typical summaries of the job flow provided by Strap to the EGEE system administrator are displayed in Fig. 5. The top snapshot corresponds to a standard situation, with a few outliers ( Reservoir ), circa 10% jobs stopping after reg-istration (exemplar (7 0 0 0 0 0)), circa 15% stopping before arriving at the CE (exemplar (10 47 54 129 0 0)), about 60% successful short jobs and 10% computationally heavy jobs. Two days later (bottom snapshot), a new cluster appear in-cluding about 40% of the jobs. The corresponding exemplar (10 18 29 20091 395 276) is immediately interpreted by the administrator as an alarm signal; LM is getting clogged (ex-emplar value 20091s, higher than the standard one by two orders of magnitude).

The load dynamics and trends can be assessed from the number of model restarts per day (Fig. 6), comforted by the robustness analysis of this indicator presented in the previous section. This indicator however only provides a coarse feedback, for frequent restarts can be explained from several causes: i) the load is abruptly increasing; ii) new job patterns appear; iii) job patterns oscillate, frequently appearing and disappearing.
A more detailed view of the load dynamics in a long time-scale is based on using super-exemplars (section 3.3). The overall stream is visualized as a tapestry, each row corre-sponding to a given super-exemplar, and each column corre-sponding to a day (or a time period; a zooming functional-ity allows the administrator to adjust the granularity of the visualization). The color of the super-exemplar indicates the percentage (or number) of jobs associated to this super-exemplar in the time period, enabling the administrator to spot the load regularities. Resuming an earlier work devoted to Data Streaming with Affinity Propagation [22], this paper shows that the compu-tational complexity of the Strap algorithm can be reduced to a quasi-linear complexity through a generalized Divide-and-Conquer approach  X  without incurring a significant dis-tortion loss except in dimension 2. Further, an adaptive pro-cedure automatically adjusting the parameters of the change detection test has been proposed. Besides its theoretical analysis, the Strap algorithm has been validated on a chal-lenging real-world application, specifically the modelling of the EGEE Grid status from the job data flow. The first Figure 7: Visualization of the Stream Model along time (x axis: time; y axis: super-exemplars ordered by attribute 4) results reported in this paper, made possible by the Grid Observatory initiative 6 , have been considered to provide rel-evant and useful hints into the types and hidden causes of the grid traffic jams.

A current limitation of the approach remains its compu-tational cost; while it meets the real-time constraint of the EGEE job stream, its batch performances still are about 8 hours by Matlab code and 2 hours by C/C++ (on Intel 2.66GHz Dual-Core PC with 2 GB memory) for 5M+ jobs.
Another limitation, deeply rooted in the AP frame, is that the number of exemplars is not easily controlled from the penalty parameter s  X  . How to address this limitation is our main perspective for further study. Independently, the Strap framework will be enhanced with visual mining fa-cilities to support a flexible multi-scale dashboard. This work has been partly supported by the Pascal-2 Euro-pean Network of Excellence and the EGEE-III Infrastruc-ture Project. [1] F. Bergeaud and S. Mallat. Matching pursuit of [2] F. Cao, M. Ester, W. Qian, and A. Zhou.
 [3] G. Cormode, S. Muthukrishnan, and W. Zhuang.
 [4] M. Ester. A density-based algorithm for discovering [5] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. From
Deployed in the EGEE-III European Infrastructure Project (2008-2013) http://www.grid-observatory.org/ . [6] B. Frey and D. Dueck. Clustering by passing messages [7] J. Gama, R. Rocha, and P. Medas. Accurate decision [8] J. Gama and P. P. Rodrigues. Stream-based electricity [9] S. Guha, A. Meyerson, N. Mishra, R. Motwani, and [10] Z. Harchaoui, F. Bach, and E. Moulines. Kernel [11] D. Hinkley. Inference about the change-point from [12] D. Judd, P. K. McKinley, and A. K. Jain. Large-scale [13] M. Leone, Sumedha, and M. Weigt. Clustering by [14] M. Meila. The uniqueness of a good optimum for [15] S. Muthukrishnan, E. v. d. Berg, and Y. Wu.
 [16] E. Page. Continuous inspection schemes. Biometrika , [17] N. Palatin, A. Leizarowitz, A. Schuster, and R. Wolff. [18] C. E. Rasmussen and C. K. Williams. Gaussian [19] I. Rish, M. Brodie, and S. M. et al. Adaptive diagnosis [20] G. Schwarz. Estimating the dimension of a model. The [21] J. Villemonteix, E. Vazquez, M. Sidorkiewicz and [22] X. Zhang, C. Furtlehner, and M. Sebag. Data [23] J. Andreeva, B. Gaidioz, J. Herrala, and et al. [24] Real Time Monitor: [25] X. Zhang, C. Furtlehner, and M. Sebag. INRIA
For the sake of readability and to lighten the argument, the influence between the center of mass and extreme value statistics distribution is neglected, enabling us to use a spher-ical kernel instead of cylindrical kernel and making no dis-tinction between ex and  X  ex , to write the recurrence (see [25] for a complete discussion). Between level h and h + 1, one has: with where K ( x, y ) is the d-dimensional radial diffusion kernel, with I d The selection mechanism of the exemplar yields at level h , and with a by part integration, (4) rewrites as: with lim At this point the recursive hierarchical clustering is described as a closed form equation. Proposition 2.2 is then based on (5) and on the following scaling behaviors, so that Basic asymptotic properties I d/ 2  X  1 yield with a proper choice of  X  , the non degenerate limits of proposition 2.2. In the particular case d = 2, taking  X  = 1, it comes: with help of the identity Again in the particular case d = 2, by virtue of the expo-nential law one further has  X  ( h ) = 1 / X  ( h ) , finally yielding:
