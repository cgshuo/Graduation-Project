 Stemming has been widely used as a fundamental technique in information re-trieval, especially in the context of web search engines [19], in text mining [20] and sentiment analysis [1] as a preprocessing technique, and for databases when building large indices on documents [18]. The main reasons are reduced mem-ory and computing demands which are necessary to handle large amounts of data, however once stemmed it is hard for humans to work with the underly-ing material for manual inspection. Further, methods which directly operate on character sequences, like string kernels [10], or computer-assisted approaches for content analysis of textual data which involve exact word matching, like the Gen-eral Inquirer [17], may return unexpected results. One can address this problem in multiple ways. First, we could avoid the  X  X esign error X  of deleting informa-tion we need later on, e.g., store both the original and stemmed representation. However, this introduces significant space overhead for large corpora, and ig-nores the fact that many text mining routines only return stems due to their internal representation. Second, one can use the context, i.e. the order of the stems appearing in the original texts, to better reconstruct the original words. Computational morphology also provides techniques for implementing reversible methods, nevertheless, the stemming process cannot be directly reverted. A far more conservative assumption, which will ground our further considerations, is that the original words to stems can only be restored from a dictionary. This is especially relevant for large amounts of (very) short texts (like blogs or Twitter) as context is hardly preserved due to abbreviations or stemming. Based on a dictionary the main challenge is now to find completions for individual stems along compatible semantic groups.
 Stemming denotes the process of conflating words to their stems, e.g. by deleting word suffixes. Formally stemming ca n be seen as a surjective function s :  X   X   X   X   X  mapping words from an alphabet  X  to words from the same alphabet. Stem-ming functions can be evaluated e.g. by under-and overstemming errors [14], by checking whether words of the same conceptual group are actually conflated to the same stem.
 Example 1. Table 1 shows words (completions) and their word stems ( c i and s i are shortcuts for completions and stems , respectively), and the effect of over-vs. understemming. All words starting with exp are conflated to the same stem experi (= s 1 ) although the first two belong to another conceptual group ( over-stemming ) as the rest, whereas adhe words have different stems ( s 2 and s 3 ) although describing a similar semantic concept ( understemming ).
 Prominent stemmer implementations are the Porter [15], Lovins [11], Paice [13], Dawson [2] and Krovetz [7] stemming algorithms. Given two sets of terms X  X ord stems and possible completions X  X e want to find an assignment of stems to completions in such a way that as many of the original, i.e. before initial stemming, words (or at least its semantics) are restored. Conceptually, the best solution to this optimization problem is exactly the inversion of the implemented stemming procedure. However, since stemming is surjective a one-to-one inversion is i n general not always possible, and we need a notion of optimality in terms of choosing completions for given stems. We formalize this as follows: Definition 1 (Stemming Inversion Problem). Let S be a set of stems, and C be a set of completions. Note that both sets are not necessarily disjoint, i.e. S X  X  =  X  .Let G C be a collection of sets (groups) of completions of related semantic interpretation, and there may exist sets of stems G S which group stems of related meaning (e.g., when stems are considered as equivalent which is useful for combining stemming and lemmatization tasks). The sets in G C can be used to indicate that a single completion of the same group suffices to participate in the matching. Finally, let L contain all links between stems and valid corresponding possible completions. Note that not all stems need to have a link to a completion (e.g., if there is a completion missing due to an incomplete dictionary).
The stemming inversion problem is to choose the minimal number of links l  X  X  and groups in G C , G S , such that all stems s  X  X  and completions c  X  X  which are present in at least one link or group in G C , G S in the input, are covered. An element is covered if it is present in at least one link or group in the solution. Note that this definition allows isolated stems or completions (and e.g. C to be a dictionary) which are simply ignored. Also note that it is allowed to choose mul-tiple completions for a given stem. We present completion strategies for choosing a single element out of multiple completions at the end of this section. Example 2. A corresponding instance to the stemming inversion problem for { c { c s correspond to the shortcuts defined in Tab. 1. Figure 1 depicts the situation for the subset induced by the terms starting with experi . The assignment of stems, completions, and links (solid lin es) between stems and possible comple-tions follows directly by definition. The assignment of groups (dashed lines) is given by the observation that c 1 and c 2 have a similar semantics as a comple-tion, whereas c 3 and c 4 are different and thus are in another group. Similarly, we define c 5 and c 6 to be in the same semantic group. A solution to this instance is given by choosing the links l 3 , l 5 , l 6 , and the groups g 1 and g 2 ,asthiscovers all stems and completions. Note that the groups allow us to choose one out of several choices, but only if the links allow us this.
 This formalism allows us to model important notions for stemming inversion. However this comes at a computationally high price as we show the problem to be NP-hard via a polynomial-time reduction from the set cover problem. Definition 2 (Set cover problem). Let U be a universe of elements, and S be a family of subsets of U .The set cover optimization problem is now to find the minimal number of subsets T i of S such that T i = U .
 This problem is NP-hard [6,4]. For the reduction we need to encode every possible instance of the set cover problem to our stemming inversion problem: Proof. For a given set cover problem with the universe U we define C = U , i.e., all elements of the set cover problem are assumed to be completions, and we set G
C = S , i.e., the groups of the completions directly correspond to the subsets of the set cover problem.
 Note that we do not even need links between completions and stems since already the problem of choosing a minimal number of semantically equivalent groups is expensive. However we argue that the inversion problem in its full generality cannot be simplified as we observe that stemming functions can be arbitrary surjective functions, and as such can have one or multiple completions, and that groups of completions and stems depend on semantic notions and language properties, and as such can ove rlap and intersect each other.

Nevertheless, in many languages, the underlying problem is not that complex since there is normally no deep nesting or overlapping between groups. That means that naive brute-force algorithms will work reasonably well in practical applications since the search space that needs to be explored is separated in local components (e.g. see the two clusters in Ex. 1).

So far we were only concerned about finding optimal assignments between stems and completions respecting the sema ntics of related conceptual groups. In addition we need a local criterion which tells us which element should be chosen out of multiple completions. E.g., in Ex. 2 we could prefer to choose c 3 or c 4 out of group g 2 as completion for stem s 1 , depending on our notions of optimality. A description of possible completion heuristics follows: Prevalent. Choose the completion c with the maximal number of occurrences First. Choose the completion c which is first found in a dictionary, i.e., c 1 = c . Shortest. Choose the completion c with the minimal number of characters of Longest. Symmetric to the previous case, i.e., the completion with the maximal Random. Choose a random item c out of the possible completions, i.e., c i = c The actual implementation of the strategies is slightly more complicated due to some intricacies of prominent stemming algorithms. In detail it does not suffice just to search for completions only but also consider insertion or deletion of characters. E.g., the Porter stemming algorithms produces berri out of berry . A solution is to use completions with minimal Levenshtein distance [8] regarding insertion/deletion for such special cases. To evaluate our presented approaches we implement a benchmark consisting of three test suites working on real data in an information retrieval setting. Reconstruction. Given a text corpus we build a dictionary consisting of the Text clustering. Our second test suite measures the performance of our ap-Sentiment Analysis. Our final procedure tests for the effect of inversion in 4.1 Data We use the Reuters-21578 data set [9] as the basis for our experiments. It contains stories covering a broad range of topics, like mergers and acquisitions, finance, or politics, and was collected by the Reuters news agency. The data set is publicly available 1 and has been widely used in text mining and information retrieval within the last decades. It contains 21578 short to medium length documents. 4.2 Procedure The experiment is carried out using the R [16] statistical computing and graph-ics environment since it provides a text mining infrastructure [3] which makes it easy to implement a prototype and apply user-defined algorithms, in our case the five presented completion heuristics, provides a broad spect rum of clustering and classification tools, and has support t o access the General Inquirer word lists and score documents accordingly. For the first part ( Reconstruction ) we built a dictionary out of the corpus, resulting in X  X fter some preprocessing like removal of punctuation marks, numbers, and common stopwords X  X bout 45000 unique terms. Next, we drew samples consisting of 100 randomly chosen terms out of this dictionary. The terms were stemmed and heuristically completed with the five presented procedures. We independently repeated the sample, stemming, and completion steps up to 100 times, resulting in up to 100 different runs. The results of the individual runs are analyzed such that for a given sample we compare the completions of each individual completion heuristics with the original unstemmed terms, and compute the percentage of matching terms. This corresponds to the relative amount of t erms where a perfect inversion (recon-struction) was possible. In the second experiment ( Text clustering ) we extracted all documents of the Reuters-21578 corpus with topics acq (acquisitions; 2125 documents) or crude (crude oil; 355 documents). These documents are the input for a k -means clustering procedure (averaging over 100 runs to compensate for local minima due to unfortunate (randomized) initial start assignments) with two desired clusters ( k = 2, modeling the two chosen topics). Since we have the true class/topic ids from the annotations in the corpus, we compute cross-agreements using maximal co-classification rate (i.e., the maximal rate of objects with the same class ids in both clusterings). The third part ( Sentiment analy-sis ) implements a sentiment analysis by scoring individual documents according to occurrences of matching terms from w ord lists as provided by the General Inquirer. These word lists are carefully chosen for specific categories (we use Positive , Negative , Strong , Weak , Active , Passive , Rise and Fal l ) by the cre-ators of the General Inquirer, and are as such highly sensitive to individual word forms. I.e., it might happen that a word matches, but its stem does not, but also vice versa. We then compute word score s for documents consisting of stemmed words and compare their scores when using our presented inversion heuristics. For the first part of our experiment, Fi g. 2 depicts the percentage of matching terms for the five completion strategies prevalent , first , shortest , longest , and random for 15 independent runs. Note that although the runs are indepen-dent we connect points of the same method to visualize their relative positions among each other throughout the experiment. Also be aware that we only visu-alize 15 runs instead of 100 to see more details but the results hold for the full experiment in the same way. For comparison we also have a none entry which corresponds to the stemmed terms. I.e., if there is a match for none this means the stem and the completion is identical, which is the case for about 43% of the terms in average in Fig. 2. We see that prevalent , first ,and shortest are comparable in performance with about 67% which corresponds to a relative increase of about 55%. Interestingly, even random outperforms longest which has the lowest matching rate of all strategies.
Table 2 gives exact numbers for this ob served behavior. The column  X  X econ-struction X  lists the percentage of matc hes between the original terms and the stemmed terms with completion for all 100 runs.

Column  X  X lustering X  is relevant for our text clustering experiment and shows the relative performance of k -means clustering for the two ( k =2)clusters of known topics acq and crude both for the stemmed corpus and the corpora obtained by first applying our completion heuristics. Relative performance mea-sures the cross-agreements (between cluster labels obtained from the k -means clustering procedure and the true class ids available as annotations from the cor-pora) using maximal co-classification rates. The zero entry for none forms the base line as no relative improvement can b e achieved (obviously, since no heuris-tics is applied). Interestingly, longest has a negative impact on the clustering results. I.e., it is better to use no heuristics than this one on the Reuters-21578 data set. Strategies shortest and first perform about 3% and 6% better than none .Heuristic prevalent outperforms all others with a relative performance increase of almost 16%.

For the third part, the application of our methods in the context of sentiment analysis, Fig. 3 depicts summarized score penalties for the whole Reuters-21578 corpus for individual categories from the General Inquirer word lists. Summa-to the sum of word frequencies which are different from the original corpus  X  O considering that the stemmed corpus  X  S has negative impact on the individual scores. A penalty of 0 means that an inversion method produces exactly the same score as the original document, i.e., the smaller the penalty, the better. Table 3 shows corresponding numbers for Fig. 3.

By combining the results from the different conducted experiments we observe that the method prevalent performs best and yields a notable improvement compared to just using a stemmed corpus, i.e., without any completion strategy. This result is in so far surprising since it holds in all of our experiments; it could be expected for reconstruction since the terms which occur most often are also likely to be the correct ones for completio n. However, this is not necessarily true for text clustering, and especially not for text mining on sentiments. Clusterings are selective to large numbers of identical observations; a strategy in favor of prevalent terms can merge different st ems to a single one occurring quite of-ten. Similarly, joining different stems to a common word might trigger different semantics related to sentiments in text mining. Nevertheless, our experiments show that a prevalent strategy works in these two settings satisfactorily. For special settings shortest is a viable alternative, especially for runtime sensitive applications. Given a sorted dictionary finding the shortest completion is easy compared to determining the most prev alent terms in a corpus which involves counting word frequencies. We note that both the random (as expected) but also the longest completion strategy are unsatisfactory, and should not be used in general. We presented the stemming inversion p roblem which is motivated by the fact that stemming is a central technique in information retrieval but the reversal of this process is often desirable, either for human consumption, or for word sensitive methods. Although being a natural and important research question, to the best of our knowledge this problem has not been addressed in the literature in a systematic way we do. We proved that the problem in its full generality is NP-hard. We introduced efficient approximation algorithms which solve the inversion problem based on a set of heuristics. Experiments on real data show the applicability of the presented heuristics for word reconstruction, text clustering, and text mining. We obtain strong results for a strategy based on prevalent term completions, yielding observable better results than just the stemmed words without completion heuristics, in all categories of our benchmarks. It remains open to investigate to which extent alte rnative strategies depend on the corpus language. This is highly relevant in an int ernational context, e.g. for Internet search engines [19] due to user provided queries. Another remaining issue is the extension of our implementation into a stand-alone library. This would allow easy integration of the proposed inversion strategies into existing information retrieval frameworks. Finally there are plans to i ncorporate context-aware techniques in our setting.
 Acknowledgments. This work is supported by the Vienna Science and Tech-nology Fund (WWTF), project ICT08-032.

