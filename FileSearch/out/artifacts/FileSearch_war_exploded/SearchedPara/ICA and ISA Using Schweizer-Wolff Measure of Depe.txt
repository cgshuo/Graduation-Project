 Sergey Kirshner sergey@cs.ualberta.ca Barnab  X as P  X oczos poczos@cs.ualberta.ca
Independent component analysis (ICA) (Comon, 1994) deals with a problem of a blind source sep-aration under the assumptions that the sources are independent and that they are linearly mixed. ICA has been used in the context of blind source separa-tion and deconvolution, feature extraction, denoising, and successfully applied to many domains including finances, neurobiology, and processing of fMRI, EEG, and MEG data. For a review on ICA, see Hyv  X arinen et al. (2001).

Independent subspace analysis (ISA) (also called multi-dimensional ICA and group ICA) is a generaliza-tion of ICA that assumes that certain sources depend on each other, but the dependent groups of sources are still independent of each other, i.e., the indepen-dent groups are multidimensional. The ISA task has been the subject of extensive research (e.g., Cardoso, 1998; Theis, 2005; Bach &amp; Jordan, 2003; Hyv  X arinen for instance, to EEG-fMRI data.

Our contribution, SWICA, is a new ICA algorithm based on Schweizer-Wolff (SW) non-parametric depen-dence measure. SWICA has the following properties:  X  SWICA performs comparably to other state of the  X  SWICA is extremely robust to outliers as it uses  X  SWICA suffers less from the presence of noise  X  SW measure can be used as the cost function to  X  SWICA is simple to implement, and the Mat- X  On a negative side, SWICA is slower than other
The paper is organized as follows. An overview of the ICA and ISA problems and methods is pre-sented in Section 2. Section 3 motivates and describes Schweizer-Wolf dependence measure. Section 4 de-scribes a 2-source version of SWICA, extends it to a d -source problem, describes an application to ISA, and mentions possible approaches for accelerating SWICA. Section 5 provides a thorough empirical evaluation of SWICA to other ICA algorithms under different set-tings and data types. The paper is concluded with a summary in Section 6.

We consider the following problem. Assume we have d independent 1-dimensional sources (random vari-ables) denoted by S 1 , . . . , S d . We assume each source emits N i.i.d. samples denoted by s i 1 , . . . , s i N . Let S = n s j i o  X  R d  X  N be a matrix of these samples. We assume that these sources are hidden, and that only a matrix X of mixed samples can be observed: where A  X  R d  X  d . (We further assume that A has full rank d .) The task is to recover the sample matrix S of the hidden sources by finding a demixing matrix W and the estimated sources Y 1 , . . . , Y d are mutually in-dependent. The solution can be recovered only up to a scale and a permutation of the components; thus we assume that the data has been pre-whitened, and it is sufficient to search for an orthogonal matrix W (e.g., Hyv  X arinen et al., 2001). Additionally, since jointly Gaussian sources are not identifiable under lin-ear transformations, we assume that no more than one source is normally distributed.

There are many approaches to solving the ICA prob-lem, differing both in the objective function designed to measure the independence between the unmixed sources (sometimes referred to as a contrast function) and the optimization methods for that function. Most commonly used objective function is the mutual infor-mation (MI) J ( W ) = I Y 1 , . . . , Y d = where h is the differential entropy. Alternatively, one can minimize the sum P d i =1 h Y i of the univari-ate entropies as the joint entropy is constant (e.g., Hyv  X arinen et al., 2001). Neither of these quantities can be evaluated directly, so approximations are used instead. Among effective methods falling in the former category is KernelICA (Bach &amp; Jordan, 2002); RAD-ICAL (Learned-Miller &amp; Fisher, 2003) and FastICA (Hyv  X arinen, 1999) approximate the sum of the univari-ate entropies. There are other possible cost functions including maximum likelihood, moment-based meth-ods, and correlation-based methods.

While ICA problems has been well-studied in the above formulation, there are a number of variations of it that are subject of active research. One such formulation is a noisy version of ICA where multivariate noise is often assumed normally distributed. Another related problem occurs when the mixed samples X are corrupted by a presence of out-liers. There are many other possibilities that go be-yond the scope of this paper.

Of a special note is a generalization of ICA where some of the sources are dependent , independent sub-space analysis (ISA). For this case, the mutual in-formation and Shannon entropies from Equation 1 would involve multivariate random vectors instead of scalars. Resulting multidimensional entropies are ex-ponentially more difficult to estimate than their scalar counterparts, making ISA problem more difficult than ICA. However, Cardoso (1998) conjectured that the ISA problem can be solved by first preprocessing the mixtures X by an ICA algorithm and then grouping the estimated components with highest dependence. While the extent of this conjecture is still on open is-sue, it has been rigorously proven for some distribution types (Szab  X o et al., 2007). Even without a proof for the general case, a number of algorithms apply this heuris-tics with success (Cardoso, 1998; Theis, 2007; Bach &amp; Jordan, 2003). There are ISA methods not relying on Cardoso X  X  conjecture (e.g., Hyv  X arinen &amp; K  X oster, 2006) although they are susceptible to getting trapped in lo-cal minima.
Most of the ICA algorithms use an approximation to mutual information (MI) as their objective func-tions, and the quality of the solution thus depends on how accurate is the corresponding approximation. The problem with using MI is that without a parametric assumption on the functional form of the joint distri-bution, MI cannot be evaluated exactly, and numerical estimation can be both inaccurate and computation-ally expensive. In this section, we explore other mea-sures of pairwise association as possible ICA contrasts. To note, most commonly used measure of correlation, Pearson X  X  linear correlation coefficient, cannot be used as it is invariant under rotations (once the data has been centered and whitened)
Instead, we are focusing on measures of dependence of the ranks . Ranks have a number of desirable proper-ties  X  they are invariant under monotonic transforma-tions of the individual variables, insensitive to outliers, and not very sensitive to small amounts of noise. We found that a dependence measure defined on copulas (e.g., Nelsen, 2006), probability distributions on con-tinuous ranks, has the right properties to be used as a contrast for ICA demixing. 3.1. Ranks and Copulas
Let a pair of random variables ( X, Y )  X  R 2 be dis-tributed according to a bivariate probability distribu-tion P . Assume we are given N samples of ( X, Y ), D = { ( x 1 , y 1 ) , . . . , ( x N , y N ) } . Let the rank r the number of x i , i = 1 , . . . , N such that x &gt; x i let r y ( y ) be defined similarly.

Many non-linear dependence measures are based on ranks. Among most commonly used are Kendall X  X   X  and Spearman X  X   X  rank correlation coefficients. Kendall X  X   X  measures the difference between propor-( x i  X  x j ) ( y i  X  y j ) &gt; 0) and discordant pairs. Spear-man X  X   X  measures a linear correlation between ranks of r ( x ) and r y ( y ). Both  X  and  X  have a range of [  X  1 , 1] and are equal to 0 (in the limit) if the X and Y are independent. However, the converse is not true, and both  X  and  X  can be 0 even if X and Y are not inde-pendent. While they are robust to outliers, neither  X  nor  X  make for a good ICA contrast as they provide a noisy estimate for dependence from moderately-sized data sets when the dependence is weak (See Figure 1 for an illustration).

Rank correlations can be extended from samples to distributions with the help of copulas , distributions over continuous multivariate ranks. We will devise an effective robust contrast for ICA using a measure of dependence for copulas which is closely related to Spearman X  X   X  .

Let I denote a unit interval [0 , 1]. A bivariate cop-ula C is probability function (cdf) defined on a unit square, C : I 2  X  I such that its univariate marginals are uniform, i.e., C ( u, 1) = u, C (1 , v ) = v,  X  u, v,  X  I . Let U = P x ( X ) and V = P y ( Y ) denote the corre-sponding cdfs for previously defined random variables X and Y . Variables X = P  X  1 x ( U ) and Y = P  X  1 y ( V ) can be defined in terms of the inverse of marginal cdfs. Then, for ( u, v )  X  I 2 , define C as It is easy to verify that C is a copula. Sklar X  X  theorem (Sklar, 1959) states that such copula exists for any distribution P , and that it is unique on the range of values of the marginal distributions. A copula can be thought of as binding univariate marginals P x and P y to make a distribution P .

Copulas can also be viewed as a canonical form of multivariate distributions as they preserve multivari-ate dependence properties of the corresponding fami-lies of distributions. For example, the mutual informa-tion of the joint distribution is equal to the negentropy of its copula restricted to the region on which the cop-ula density function (denoted in this paper by c ( u, v )) is defined: Such negentropy is minimized when C ( u, v ) =  X  ( u, v ) = uv . Copula  X  is referred to as the product copula and is equivalent to variables U and V (and the original variables X and Y ) being mutually indepen-dent. This copula will play a central part in definition of contrasts in the next subsection.

Copulas can also be viewed as a joint distribution over univariate ranks , and therefore, preserve all of the rank statistics of the corresponding multivariate dis-tributions; rank based statistics can be expressed in terms of the copula alone. For example, Spearman X  X   X  has a convenient functional form in terms of the cor-responding copulas (e.g., Nelsen, 2006):
As the true distribution P and its copula C are not known, the rank statistics can be estimated from the available samples using an empirical copula (De-heuvels, 1979). For a data set { ( x 1 , y 1 ) , . . . , ( x an empirical copula C N is given by C Well-known sample versions of several non-linear de-pendence measures can be obtained using an empirical copula (e.g., Nelsen, 2006). For example, sample ver-sion r of Spearman X  X   X  appears to be a grid integration evaluation of its expression in terms of a copula (Equa-tion 3): r = 3.2. Schweizer-Wolff  X  and  X 
Part of the problem with Kendall X  X   X  and Spear-man X  X   X  as a contrast for ICA is a property that their value may be 0 even though the corresponding vari-ables X and Y are not independent. Instead, we sug-gest using Schweizer-Wolff  X  , a measure of dependence between two continuous random variables (Schweizer &amp; Wolff, 1981):  X  can be viewed as an L 1 norm between a copula for the distribution and a product copula. It has a range of [0 , 1], with an important property that  X  = 0 if and only if the corresponding variables are mutually inde-pendent, i.e., C =  X . The latter property suggests an ICA algorithm for a pair of variables: pick a rotation angle such that the corresponding demixed data set has its  X  minimized. A sample version of  X  is similar to that of  X  (Equation 5): s =
We note that other measures of dependence can be potentially used as an ICA contrast. We also experimented with an L  X  version of  X  ,  X  = 4 sup I 2 | C ( u, v )  X  uv | , a dependence measure similar to Kolmorogov-Smirnov univariate statistic (Schweizer &amp; Wolff, 1981), with results similar to  X  .
In this section, we present a new algorithm for ICA and ISA demixing. The algorithm uses Schweizer-Wolff  X  estimates as a contrast in demixing pairs of variables; we named this algorithm Schweizer-Wolff contrast for ICA, or SWICA for short. 4.1. 2-dimensional Case First, we tackle the case of a two-dimensional signal S mixed with a 2  X  2 matrix A . We, further assume A is orthogonal (otherwise achievable by whitening). The problem is then reduced to finding a demixing rotation matrix W =
For the objective function, we use s (Equation 7) computed on 2  X  N matrix Y = WX of rotated sam-ples. Given an angle  X  , s ( Y (  X  )) can be computed by first sorting each of the rows of Y (  X  ) and computing row ranks for each entry of Y (  X  ), then computing an empirical copula C N (Equation 4) for ranks of Y , and finally computing s ( Y (  X  )) (Equation 7). The solution is then found by finding angle  X  minimizing s ( Y (  X  )). Similar to RADICAL (Learned-Miller &amp; Fisher, 2003), we find such solution by searching over K values of  X  in the interval 0 ,  X  2 . This algorithm is outlined in Figure 2. 4.2. d -dimensional Case
A d -dimensional linear transformation described by a d  X  d orthogonal matrix W is equivalent to a composi-tion of 2-dimensional rotations (called Jacobi or Givens rotations) (e.g., Comon, 1994). The transformation matrix itself can be written as a product of correspond-ing rotation matrices, W = W L  X  . . .  X  W 1 where each matrix W l , l = 1 , . . . , L is a rotation matrix (by angle  X  l ) for some pair of dimensions ( i, j ). Thus a d -dimensional ICA problem can be solved by solving 2-dimensional ICA problems in succession. Given a current demixing matrix W c = W l  X  . . .  X  W 1 and a current version of the signal X c = W c X , we find an angle  X  corresponding to SWICA X ( i,j ) c , K . Taking an approach similar to RADICAL, we perform a fixed number of successive sweeps through all possible pairs of dimensions ( i, j ).

We should note that while d -dimensional SWICA is not guaranteed to converge, it converges in practice a vast majority of the time. A likely explanation is that each 2-dimensional optimization finds a transfor-Algorithm SWICA ( X , K )
Inputs: X , a 2  X  N matrix where rows are mixed signals (centered and whitened), K equispaced evaluation angles in the [0 ,  X / 2) interval
For each of K angles  X  in the interval [0 ,  X / 2) (  X  =  X k 2 K , k = 0 , . . . , K  X  1 . ) Find best angle  X  m = arg min  X  s ( Y (  X  ))
Output: Rotation matrix W = W (  X  m ), demixed signal Y = Y (  X  m ), and estimated dependence measure s = s ( Y (  X  m )) mation that reduces the sum of entropies for the corre-sponding dimensions, reducing the overall sum of en-tropies. In addition to this, Learned-Miller and Fisher (2003) suggest that the minimization of the overall sum of entropies in this fashion (by changing only two terms in the sum) may make it easier to escape local minima. 4.3. Complexity Analysis and Acceleration 2-dimensional SWICA requires a search over K an-gles. For each angle, we first sort the data to com-pute the ranks of each data point ( O ( N log N )), and then use these ranks to compute s by computing the empirical copula and summing over the N  X  N grid (Equation 7), requiring O N 2 additions. Therefore, running time complexity of 2-d SWICA is O KN 2 . Each sweep of a d -dimensional ICA problem solves a 2-dimensional ICA problem for each pair of variables, O d 2 of them; S sweeps would have O Sd 2 KN 2 complexity. In our experiments, we employed K = 180 , S = 1 for d = 2, and K = 90 , S = d for d &gt; 2. The most expensive computation in SWICA is O N 2 needed to compute s ( Y (  X  )). Reducing this complexity, either by approximation, or perhaps, by an efficient rearrangement of the sum, is left to fu-ture research. We used several other tricks to speed up the computation. One, for large N ( N &gt; 2500) we estimated s using only N 2 s ( N s = b N d N the sum corresponding to equispaced gridpoints on I 2 . Two, when searching for  X  minimizing s ( Y (  X  )), it is unnecessary to sum over all N 2 terms when evaluat-ing a candidate  X  if a partial sum already results in a value of s ( Y (  X  )) larger than the current best. This optimization translates into a 2-fold speed increase in practice. Three, it is unnecessary to complete all S sweeps if the algorithm already converged. One possi-ble measure of convergence is the Amari error (Equa-tion 8) measured for the cumulative rotation matrix for the most recent sweep. 4.4. Using Schweizer-Wolff  X  for ISA
Following Cardoso X  X  conjecture, ISA problems can be solved by first finding a solution to an ICA prob-lem, and then by grouping resulting sources that are not independent (Cardoso, 1998). We propose em-ploying Schweizer-Wolff  X  to measure dependence of sources for an ICA solution as it provides a compu-tationally effective alternative to mutual information, commonly used measure of source dependence. Note that ICA solution, the first step, can be obtained using any approach, e.g., FastICA due to its computational speed for large d . One commonly used trick for group-ing the variables is to use a non-linear transformation of the variables to  X  X mplify X  their dependence as in-dependent variables remain independent under such transformations. 2
For the experimental evaluation of SWICA, we con-sidered several settings. For the evaluation of the quality of demixing solution matrix W , we computed the Amari error (Amari et al., 1996) for the resulting transformation matrix B = WA . Amari error r ( B ) measures how different matrix B is from a permuta-tion matrix, and is defined as  X  where  X  = 1 / (2 d ( d  X  1)). r ( B )  X  [0 , 1], and r ( B ) = 0 if and only if B is a permutation matrix. We compared SWICA to FastICA (Hyv  X arinen, 1999), KernelICA-KGV (Bach &amp; Jordan, 2002), RADICAL (Learned-Miller &amp; Fisher, 2003), and JADE (Cardoso, 1999).
For the simulated data experiments, we used 18 dif-ferent one-dimensional densities to simulate sources. These test-bed densities (and some of the experiments below) were proposed by Bach and Jordan (2002) to test KernelICA and by Learned-Miller and Fisher (2003) to evaluate RADICAL; we omit the description of these densities due to lack of space as they can be looked up in the above papers.

Table 1 summarizes the medians of the Amari er-rors for 2-dimensional problems where both sources had the same distribution. Samples from these sources were then transformed by a random rotation, and then demixed using competing ICA algorithms. SWICA outperforms its competitors in 8 out of 18 cases, and performs comparably in several other cases. However, it performs poorly when the joint distribution for the sources is close to a Gaussian (e.g., (d) t -distribution with 5 degrees of freedom). One possible explana-tion for why SWICA performs worse than its com-petitors for these cases is that by using ranks instead of the actual values, SWICA is discarding some of the information that may be essential to separating such sources. However, given larger number of sam-ples, SWICA is able to separate near-Gaussian sources (data not shown due to space constraints). SWICA also outperformed other methods when sources were not restricted to come from the same distribution (Ta-ble 2) and proved effective for multi-dimensional prob-lems ( d = 4 , 8 , 16).

Figure 3 summarizes the performance of ICA algo-rithms in the presence of outliers for the d -source case ( d = 2 , 4 , 8). Distributions for the sources were cho-sen at random from the 18 distributions from the ex-periment in Table 1. The sources were mixed using a random rotation matrix. The mixed sources were then corrupted by adding +5 or  X  5 to a single component for a small number of samples. SWICA significantly outperforms the rest of the algorithms as the contrast used by SWICA is insensitive to minor changes in the sample ranks introduced by a small number of outliers. For d = 2, we tested SWICA further by significantly increasing the number of outliers; the performance was virtually unaffected when the proportion of the out-liers was below 20%. SWICA is also less sensitive to noise than other ICA methods (Figure 4).
 We further tested SWICA on sound and image data. We mixed N = 1000 samples from 8 sound pieces of an ICA benchmark 3 by a random orthogonal 8  X  8 matrix. Then we added 20 outliers to this mixture in the same way as in the previously described outlier experiment and demixed them using ICA algorithms. Figure 5 shows that SWICA outperforms other meth-ods on this task. For the image experiment, we used 4 natural images 4 of size 128  X  256. The pixel intensi-ties we normalized in the [0 , 255] interval. Each image was considered as a realization of a stochastic variable with 32768 sample points. We mixed these 4 images by a 4  X  4 random orthogonal mixing matrix, resulting in a mixture matrix of size 4  X  32768. Then we added large +2000 or  X  2000 outliers to 3% randomly selected points of these mixture, and then selected at random 2000 samples from the 32768 vectors. We estimated the demixing matrix W using only these 2000 points, and then recovered the hidden sources for all 32768 samples using this matrix. SWICA significantly out-performed other methods. Figure 7 shows an example of the demixing achieved by different ICA algorithms.
Finally, we applied Schweizer-Wolff  X  in an ISA set-ting. We used 6 3-dimensional sources where each variable was sampled from a geometric shape (Figure 6a), resulting in 18 univariate hidden sources. These sources ( N = 1000 samples) were then mixed with a random 18  X  18 orthogonal matrix (Figure 6b). Apply-ing Cardoso X  X  conjecture, we first processed the mixed sources using FastICA, and then clustered the recov-ered sources using  X  computed on their absolute values (a non-linear transformation) (Figure 6c). The hidden subspaces were recovered with high precision as indi-cated by the Hinton diagram of WA (Figure 6d).
We proposed a new ICA and ISA method, SWICA, based on a non-parametric rank-based estimate of the dependence between pairs of variables. Our method frequently outperforms other state of the art ICA al-gorithms, is very robust to outliers, and only moder-ately sensitive to noise. On the other hand, it is some-what slower than other ICA methods, and requires more samples to separate near-Gaussian sources. In the future, we plan to investigate possible accelera-tions to the algorithm, and statistical characteristics of the source distributions that affect the contrast.
This work has been supported by the Alberta Inge-nuity Fund through the Alberta Ingenuity Centre for Machine Learning.
 Amari, S., Cichocki, A., &amp; Yang, H. (1996). A new learning algorithm for blind source separation. NIPS (pp. 757 X 763).
 Bach, F. R., &amp; Jordan, M. I. (2002). Kernel indepen-dent component analysis. JMLR , 3 , 1 X 48.
 Bach, F. R., &amp; Jordan, M. I. (2003). Beyond inde-pendent components: Trees and clusters. JMLR , 4 , 1205 X 1233.
 Cardoso, J.-F. (1998). Multidimensional independent component analysis. Proc. ICASSP X 98, Seattle, WA. Cardoso, J.-F. (1999). High-order contrasts for inde-pendent component analysis. Neural Computation , 11 , 157 X 192.
 Comon, P. (1994). Independent component analysis, a new concept? Signal Proc. , 36 , 287 X 314.
 Deheuvels, P. (1979). La fonction de d  X ependance em-pirique et ses propri  X et  X es, un test non param  X etrique d X  X nd  X ependance. Bulletin de l X  X cad  X emie Royale de Belgique, Classe des Sciences , 274 X 292.
 Hyv  X arinen, A. (1999). Fast and robust fixed-point al-gorithms for independent component analysis. IEEE Trans. on Neural Networks , 626 X 634.
 Hyv  X arinen, A., Karhunen, J., &amp; Oja, E. (2001). Inde-pendent component analysis . New York: John Wiley. Hyv  X arinen, A., &amp; K  X oster, U. (2006). FastISA: A fast fixed-point algorithm for independent subspace analysis. Proc. of ESANN .
 Learned-Miller, E. G., &amp; Fisher, J. W. (2003). ICA using spacings estimates of entropy. JMLR , 4 , 1271 X  1295.
 Nelsen, R. B. (2006). An introduction to copulas . Springer Series in Statistics. Springer. 2nd edition. P  X oczos, B., &amp; L  X orincz, A. (2005). Independent sub-space analysis using geodesic spanning trees. Proc. of ICML-2005 (pp. 673 X 680).
 Schweizer, B., &amp; Wolff, E. F. (1981). On nonparamet-ric measures of dependence for random variables. The Annals of Statistics , 9 , 879 X 885.
 Sklar, A. (1959). Fonctions de r  X epartition `a n dimen-sions et leures marges. Publications de l X  X nstitut de Statistique de L X  X niversit  X e de Paris , 8 , 229 X 231. Szab  X o, Z., P  X oczos, B., &amp; L  X orincz, A. (2007). Under-complete blind subspace deconvolution. JMLR , 8 , 1063 X 1095.
 Theis, F. J. (2005). Blind signal separation into groups of dependent signals using joint block diagonaliza-tion. Proc. of ISCAS. (pp. 5878 X 5881).
 Theis, F. J. (2007). Towards a general independent subspace analysis. Proc. of NIPS 19 (pp. 1361 X 
