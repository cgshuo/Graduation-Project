 Matrix is a fundamental structure to represent structured data, including feature-vector sets in information retrieval, relational t ables in databases, graphs in social network analysis, and user-item associations in recommender systems [5]. Nowadays, abundant data yield huge matrices, which consume a large amount of storage space and take a long time to be computed. This situation motivates a classical problem: how to reduce the data size and computational time of huge matrices?
There is a lot of work that tackled the above-mentioned problem [8, 11, 13], and most of the work are based on the technique called sparse matrix representation [8]. Sparse matrix representation concisely represents a sparse matrix that is mostly filled with a default value . When the default value is zero, we can omit computations on matrix by utilizing algebraic laws such as multiplying any values to zero yields zero.
However, actual matrices are not only sparse but also dense. Unfortunately, meth-ods that depend on sparse matrix representation cannot represent and compute dense matrices concisely and e ffi ciently. Thus, in this paper, we tackle a challenging prob-lem: reducing the data size and computational time for both sparse matrices and dense matrices.
 To this end, we propose MOARLE (Matrix Operation Accelerator based on Run-Length Encoding) , a novel matrix computation framework. MOARLE compresses an input matrix with run-length encoding and conducts computations on the compressed matrix directly without decompression, t hus reducing computational time and memory usage. Furthermore, MOARLE can improve the compression rate of the input matrix without changing computational results. When MOARLE finds that the result of a computation to a matrix is not a ff ected by row-or column-reordering, it reorders the rows or columns of the matrix in order to improve the compression rate, in a grace way.
We summarize our contributions: 1. We propose MOARLE , a matrix computation framework that compresses an in-2. For several representative computations, we propose techniques to conduct the 3. We propose a way to improve the compression rate without changing computational 4. We implement MOARLE and present an experimental analysis demonstrating the
The rest of this paper is organized as follows. Section 2 describes how MOARLE compresses and computes input matrices. Section 3 describes how MOARLE improves the compression rate of input matrices. Section 4 evaluates MOARLE by experiments with various datasets. Section 5 describes related work. Section 6 concludes the paper. In this section, we present the mechanism of MOARLE  X  X  computational part, which compresses a matrix with run-length encodi ng and conducts computations without de-compression to reduce computational time. 2.1 Run-Length Encoding of Matrix We e m p l o y run-length encoding ( RLE ), a basic data compression technique, in compres-sion part of MOARLE . RLE represents a sequence of consecutive same data elements with a pair (length, element) ,where length is the number of the data elements, and ele-ment is the data element. For example, RLE encodes a data sequence 1, 1, 1, 1, 1, 2, 2, of elements to represent the data from 17 to 6. One downside of RLE is that it cannot e ffi ciently encode a sequence of data in which the number of consecutive same data elements is small. To alleviate this problem, MOARLE can optionally employ Pa ck -Bits [1], an improved version of RLE. PackBits handles two types of length: positive length and negative length . When PackBits finds a sequence of consecutive same data elements, it replaces the sequence by a pair with positive length, as in RLE. Otherwise, PackBits meets a sequence of consecutive di ff erent data elements and inserts the length of the sequence before the sequence in negat ive format. For example, PackBits encodes adatasequence1,2,3,4,1,1,1,1to(-4,1,2,3,4),(4,1)successfullyreducingthedata size, whereas RLE encodes the sequence to (1, 1), (1, 2), (1, 3), (1, 4), (4, 1) increasing the data size. We will compare the perform ance of RLE and PackBits in Section 4.
MOARLE encodes a matrix with RLE in two ways: either encoding row-vectors (row-compression) or column-vectors (column-compression). For example, a matrix is encoded to in row-compression, or in column-compression. 2.2 Computation over RLE Vectors without Decompression
MOARLE conducts several computations on RLE vectors directly without decoding the vectors. This technique reduces the computational time and the memory usage, achieving our original goal. The key insight in the technique is that an RLE-encoded data sequence contains preliminary knowledge that an element x appears n times. In the rest part of this section, we demonstrate how MOARLE uses this knowledge to perform computations without decoding.
 Square Sum of RLE Vector. First, we demonstrate how MOARLE computes square sum of a compressed vector. Square sum of a vector is a basic but essential computation because it appears in many complex computa tions including Euclidean distance, cosine similarity, coordinate descent [7], and so forth.

Let us consider square sum computation of the first row of the matrix in Eq.1. A na  X   X ve approach is to compute the square sum as where it needs to conduct five product and four addition operations.
 In MOARLE , we can process RLE vectors directly without decoding. Given the RLE vector of the first row of the matrix in Eq.1, (1 , 1) , (4 , 2), MOARLE computes the square sum of the vector as where it needs to conduct three product and one addition operations, of which the com-putation cost is less than the na  X   X ve one.
 DotProductofRLEVectors. Next, we will demonstrate how MOARLE computes dot product of a compressed vector. Dot product of a vector is also an important com-putation, which appears in a lot of more complicated operations.

Let us consider dot product computations of the first row-vector and the second row-vector of the matrix in Eq.1. A na  X   X ve approach is to compute the dot product as where it needs to conduct five product opera tions and four addition operations. Given RLE-encoded vectors for the first row and second row of the matrix in Eq.1, MOARLE computes the dot product of the vectors as where it needs to conduct two product operati ons and four addition operations, of which the computation cost is less than the na  X   X ve one. 2.3 Euclidean Distance of RLE Vectors So far, we have described two basic computations of RLE vectors: square sum and dot product. Here, we show that some complex operations of RLE vectors can be done by composing basic computations.

Let us consider the Euclidean distance of two RLE vectors. The Euclidean distance of vector a = ( a 1 ,..., a n ) and vector b = ( b 1 ,..., b n )isdefinedas Since Eq.6 can be expanded to and it consists only of aforementioned square sum and dot product, MOARLE can compute the Euclidean distance without decoding. This kind of approach can also be applied to other computations like cosine similarity, but we do not show concrete ex-amples here owing to space limitations.
 In the previous section, we have described how MOARLE compresses a matrix with RLE and conducts computations directly without decoding. In this section, we show that the RLE compression rate of a matrix can be improved by reordering the row-or column-vectors, and such reordering does not a ff ect results of the computations we have seen. 3.1 Reordering and Computation By reordering rows or columns of a matrix properly, we can improve the RLE X  X  com-pression rate of the matrix, reducing the memory usage and the computational time. Consider the matrix X  X  R 6  X  3 in Fig.1. By applying column compression to the matrix X , we get the collection of RLE column-vectors in Fig.2, which consists of 15 pairs. If we reorder the rows of the matrix X , we can further reduce the number of pairs to rep-resent X , thus improving the compression rate. Let us consider the matrix X  X  in Fig.3, which is derived from X by reordering rows. If we apply column-compression to X  X  , we get the collection of the RLE column-vectors in Fig.4, which consists of 11 pairs, smaller than the 15 pairs needed by the original matrix X .
MOARLE reorders a matrix to improve its compression rate, based on an insight that the results of certain computations are not a ff ected by row-or column-reordering. We call such a computation an order-insensitive computation . For example, square sum and dot product computations illustrated in Sec tion 2.2 are order-insensitive computa-tions. Relational operations in relational algebra also are order-insensitive computations when we consider a matrix as a relational table. Furthermore, even more complex com-putations such as convex-optimization algorithms including coordinate descent [7] and stochastic gradient descent [6] also are order-insensitive computations. 3.2 Problem Definition Since reordering a matrix can improve the RLE compression rate and the reordering does not change the results of order-insensitive operations, MOARLE reorders the ma-trix if computations are known to be order-insensitive. However, the number of possible reordering patterns for a matrix is huge, and thus finding the best pattern from possi-ble patterns is not a trivial task. In the rest of this section, we limit the discussion to row-reordering for ease of exposition, and tackle the following problem: Problem 1 (Row-reordering). Given a matrix X  X  R m  X  n , find a matrix X that satisfies the following condition where Size : R m  X  n  X  N is a function that counts the number of pairs to represent a given matrix with column-order RLE and X is a set of all matrices derived from X by reordering rows. 3.3 Exhaustive Search The most na  X   X ve solution for the Problem 1 is exhaustive search method. Exhaustive search method checks all reordering matrix patterns X , and picks up the matrix with minimum Size . Since the number of patterns | X | is m ! and each Size process traverses all elements in X , which checks m  X  n elements, the computational complexity of exhaustive search method is O ( m  X  m ! n ). Although exhaustive method can find the best matrix, it is not feasible for real-world problems b ecause the computational complexity is non-polynomial.

In the rest of this section, we introduce two reordering methods: greedy method and scored-lex-sort method, which run in polynomial time. 3.4 Greedy Method First, we introduce greedy method , a greedy approach to find a feasible solution for the Problem 1. Greedy method selects a row as the beginning row, and then repeatedly selects the row that has minimum hamming distance with the previously selected row. Finally the matrix is reordered in the selection order. Algorithm 1 shows the complete algorithm of greedy method. In Algorithm 1, we use a i  X   X  b j  X  to denote replacing i -th row of the matrix A with j -th row of the matrix B ,and O m , n to denote a zero matrix whose size is m  X  n .

Greedy method runs in polynomial time. In greedy method, we can choose a begin-ning row from m rows, and for each beginning row, there are ( m  X  1) + ( m  X  2) + ... + 1 = Algorithm 1. Greedy method m ( m  X  1) / 2 patterns of selecting remaining rows in the greedy way based on hamming distance. Each hamming distance computation needs n element comparison. Thus, the computational complexity of greedy method is O ( m 3 n ). Although O ( m 3 n ) is a polyno-mial time, it contains m 3 and thus is not appropriate for processing large matrices. 3.5 Scored-Lex-Sort Method Second, we introduce scored-lex-sort method ,ane ffi cient approach to find a feasible solution for the Problem 1 based on lexicographical sort. scored-lex-sort method runs in O ( nm log m ), which is better than greedy method X  X  O ( m 3 n ). To introduce scored-lex-sort method, we first describe lexicographical sorting of a matrix. Lexicographical sorting of a matrix is to reorder rows of a matrix where the order of two rows is defined as lexicographical order. For example, if we sort the matrix X in Fig.1 lexicographically, then the sorted matrix X lex is as follows:
Since lexicographical sorting of a matrix preferentially sorts left-side columns, it sometimes does not much improve the compression rate of the matrix. For instance, Algorithm 2. Row-compare function in scored-lex-sort method X lex  X  X  compression rate is worse than the original matrix X  X  X  one. To alleviate this prob-lem, we introduce scored-lex-sort method . scored-lex-sort method first computes scores of all columns of the matrix, and then preferentially sorts columns that have high score. E ff ectiveness of scored-lex-sort method highly depends on the definition of the score. counts the number of distinct elements in column x . This score definition is based on the insight that a column with low-cardinality contains a lot of same elements, and preferentially sorting such columns improves the compression e ff ect of RLE.
Algorithm 2 shows the algorithm of row comparison function in scored-lex-sort part, we can use arbitrary sorting algorithms. This allows us to reorder a huge matrix by using external sorting algorithms like merge-sort.

Scored-lex-sort method runs in polynomial time, and its computational complexity is smaller than of the greedy method. As mentioned before, scored-lex-sort method can use arbitrary general sorting algorithms. General sorting algorithms are known to sort m records in O ( m log m ) [4]. In a comparison operation of two rows, scored-lex-sort method needs to compare n elements as described in Algorithm 2. Thus, the computa-tional complexity of scored-lex-sort method is O ( nm log m ), which is su ffi ciently appli-cable to large matrices. 4.1 Experimental Setup System. All of our experiments were run on a machine that has 16GB RAM and dual-core 3.6GHz CPU running Linux 3.8.0. Our proposed system, MOARLE ,isimple-mented in C ++ and compiled by GNU g ++ 4.7.3.
 Datasets. We used both sparse matrices and dense matrices for experiments. Table 1 shows the information of the datasets we have used. The sparse matrices are bag-of-words model matrices [2]. For the details of the dense matrices, see [10]. 4.2 Compression Rate First, we evaluate MOARLE  X  X  compression part, checking how RLE is e ff ective for real-world matrices and how our scored-lex-sort method improves the compression rate. In this experiment, we applied row-order RLE described in Section 2.1 to the matrices In the experiment, we used two compression methods: normal RLE and PackBits de-scribed in Section 2.1. We measured the compression rate of these methods for both an input matrix and reordered by scored-lex-sort method described in Section 3.5.
Table 2 shows the compression rates of di ff erent compression methods. In the table,  X  X LE X  or  X  X ackBits X  means the compression method, and  X  X ort X  indicates whether we applied scored-lex-sort method or not. Here, we summarize the insights from Table 2 as follows:  X  X LE / PackBits are especially e ff ective for sparse matrices: PackBits yielded the  X  X ackBitsise ff ective for dense matrices: Compared to RLE, PackBits succeeded  X  Scored-lex-sort is especially e ff ective for dense matrices: Scored-lex-sort suc-4.3 Runtime Speedup Second, we evaluate MOARLE  X  X  computation part, check ing how the computations on compressed matrices outperform the computations on original matrices. In this ex-periment, we measured the performance of the computations described in Section 2.2 for both compressed matrices and original m atrices. Specifically, we computed square sums of each row-vectors, and dot products and Euclidean distances between the first row vector and each of the rest row-vectors. Fig.5 shows runtime speedup factors of the computations on compressed matrices compared to the computations on original matrices.

In the results of square sum for sparse matrices (left part of Fig.5(a)), we confirmed the notable speedup by a factor of 10x to 120x, and further speedups by scored-lex-sort method (e.g., factor increased from 115x to 120x in enron ). Although PackBits yielded more compact data compared to R LE, speedup factors of PackBits are lower than those of RLE. We conducted a profiling and found that this phenomena is caused by branch-prediction misses: PackBits sw itches processes based on whether a run X  X  length is positive or negative, causing freque nt branch-prediction misses. To reduce branch-prediction misses is one of our future work. For dense matrices (right part of Fig.5(a)), we also confirmed the speedup by a factor of 1.5x to 3.7x and the e ff ect of scored-lex-sort method (e.g., factor increased from 3.2x to 3.7x in gisette ).
In the results of dot product for sparse matrices (left part of Fig.5(b)), we confirm the notable speedup by a factor of 3.5x to 124x, and further speedups by scored-lex-sort method (e.g., factor increased from 76x to 124x in enron ). In contrast to square sum, PackBits outperformed RLE in both the compression rate and the runtime speedup. This is because RLE and PackBits versions of dot product impleme ntations are a bit complicated, and both implementations incur branch-prediction misses. Overheads of current complicated implementations can also be seen in the results for dense matrices (right part of Fig.5(b)). We plan to optimize the implementations in the future.
Fig.5(c) shows the results of Euclidean distance. We omit the discussion for the results here because Euclidean distance is composition of square sum and dot product we have discussed.
 4.4 Correlation between Compression Rate and Runtime Speedup Finally, we discuss the correlation of compression rates and runtime speedups. Fig.6 shows the correlations between the compression rates and the runtime speedups mea-sured in experiments in Section 4.2 and 4.3. In the figure, each sample corresponds to a dataset. Since correlation charts in Fig.6 are both log-scaled and show linear correla-tions, we can say that compression rates and runtime speedups follow power-law. By using this knowledge, we can estimate the corresponding runtime speedup from a com-pression rate, allowing us to choose wheth er to conduct a computation on a compressed matrix or an original matrix; if the estimated speedup by the compression is below 1.0, we can choose the na  X   X ve computation. MOARLE relates to computation frameworks that utilize sparse matrix and sparse vec-tor representations. For example, Eigen [9], a vector computation library, can represent sparse matrices and sparse vectors by concise data structures, and e ffi ciently conduct computations on them. In contrast to Eigen, which aims to process sparse data e ffi -ciently, MOARLE tries to compress and process dense matrices e ffi ciently as well. We plan to compare MOARLE with Eigen through experiments in the future.

Rendle proposed a way to accelerate machine-learning algorithms by utilizing block structures in a matrix [12]. His method assumes certain structures in input matrices, the block structures generated by relational joins, whereas MOARLE does not have any assumptions for the input matrices.

Brodie et al. tackled the row-reordering problem we have defined in Section 3.2, and proposed a method that is similar to our greedy method, whose computational complex-ity is O ( m 3 n ) [3]. In their situation, greedy method was enough, because their aim was to compress the state-transition tables of regular expression, and commonly such tables are not so large. However, we also targets huge matrices that cannot be processed by greedy method in realistic time. In this case, our scored-lex-sort method, whose com-putational complexity is O ( nm log m ), is better. In this paper, we proposed MOARLE , a matrix computation framework. MOARLE compresses an input matrix with RLE / PackBits, and conducts computations on the com-pressed matrix directly without decoding, t hus reducing the computational time and the memory usage. We first proposed techniques that directly conduct several representa-tive computations over compressed matrices. Second, we proposed a way to improve the compression rate without changing computational results. The insight is that results of certain computations over a matrix are not a ff ected by row-or column-reordering, allowing us to reorder the rows / columns of matrix in order to improve the compres-sion rate. We defined the reordering problem formally, and proposed a solution named scored-lex-sort , which runs in O ( nm log m ). Our experimental results confirmed the ef-fectiveness of MOARLE , showing the computational time improvement from na  X   X ve one by a factor of 10x to 120x and the memory usage reduction up to 98%.

