 Thin Nguyen, Dinh Phung, Brett Adams, Truyen Tran, and Svetha Venkatesh Mood is a state of the mind such as being happy, sad or angry. It is a com-plex cognitive process which has received e xtensive research effort, and debate, among psychologists about its nature and structure [9,10,6]. But better scien-tific understanding of what constitutes a  X  X ood X  has ramifications beyond psy-chology alone: for neuroscientists, it might offer insight into the functioning of the human brain; for medical professionals working in the domain of mental health, it might enable better monitoring and intervention for individuals and communities.

Research like that cited above aims to understand psychological drives and structures behind human mental states, and typically does so with expensive methodologies involving questionnaires or interviews that limit the number of participants. By contrast, our work aims to classify and cluster mood based on pre-existing content generated by us ers, which is collected unobtrusively  X  a sub-problem known as mood analysis in sentiment analysis [8]. Text-based mood classification and clustering, as a sub-problem of opinion and sentiment mining, have many potential applications identified in [8].

However, text-based mood analysis poses additional challenges beyond stan-dard text categorization and clustering. The complex cognitive processes of mood formulation make it dependent on the specific social context of the user, their idiosyncratic associations of mood and vocabulary, syntax and style which re-flect on language usage, or the specific genre of the text. In the case of weblogs, these challenges are highlighted by bloggers in the expression of diverse styles, relatively short text length, and informal language, such as jargon, abbrevia-tions, and grammatical errors. This leads us to investigate whether machine learning-based feature selection methods for general text classification are still effective for blog text. Feature selection methods available in machine learning are often computationally expensive, relying on labeled data to learn discrimi-native features; but the blogosphere is vast (reaching almost 130 million 1 )and continuing to grow, making desirable a feature set that works without requiring supervised feature training to classify mood. To this end, we turn our atten-tion to the result of a study that intersects psychology and linguistics known as affective norm for English words (ANEW) [1], and propose its use for mood classification.

In addition to classification, clustering mood into patterns is also an impor-tant task as it might provide vital clues about human emotion structure and has implications for sentiment-aware applications. While the structure of mood organization has been investigated from a psychological perspective for some time [9], to our knowledge, it has not been investigated from a data-driven and computational point of view. We provide an analysis of mood patterns using an unsupervised clustering approach and a dataset of more than 17 millions blog posts manually groundtruthed with users X  moods.

Our contribution is twofold. First, we provide a comparative study of ma-chine learning-based text feature select ion for the specific problem of mood clas-sification, elucidating insights into what can be transferred from a generic text categorization problem for mood classification. We then formulate a novel use of a psychology-inspired set of features for mood classification which does not require supervised feature learning, and is thus very useful for large-scale mood classification. Second, we provide empirical results for mood organization in the blogosphere on the largest dataset with m ood groundtruth available today. To our knowledge, we are the first to consider the problem of data-driven mood pattern discovery at this scale.

The rest of this paper is organized as follo ws. Related work to feature selection methods for classification and clusteri ng tasks in general text and in sentiment analysis is presented in Section 2. Work r elated to emotion measures in psychol-ogy is also examined in this section. Next, we present machine learning based feature selection schemes, together with the proposed ANEW feature set and lin-guistic analysis, applied to mood classification in two large datasets in Section 3. Section 4 presents the results of mood pattern discovery using an unsupervised learning technique, and is followed by some concluding remarks. For generic text categorization, a wide range of feature selection methods in machine learning has been studied. Mos t noticeably, Yang and Pedersen [14] conduct a comparative study on different feature selection schemes including information gain ( IG ), mutual information ( MI ), and  X  2 statistic ( CHI ).
Other than term-class interaction based, another approach for selecting fea-tures is to consider term statistic s. Thresholds for term frequency ( TF )ordocu-ment frequency ( DF ) are commonly used in feature reduction in data mining. A joint from these two, the term frequency X  X nverse document frequency ( TF.IDF ) scheme, is also popular in text mining which often outperform TF and DF .Some works perform feature searching in narrower sets than over the entire vocabulary such as linguistic groups like the parts of speech ( POS ).

Related work making use of emotion bearing lexicon for sentiment analysis includes [2], where Dodds and Danforth use the valence values of ANEW [1] for estimating happiness levels in song lyrics, blogs, and the State of the Union.
Work related to mood clustering includes [5], where Leshed and Kaye group blog posts based on their moods to find mood synonymy.

Generally, emotions have been represen ted in dimensional and discrete per-spectives. In the first methodology, emotion states are coded as combinations of some factors like valence and arousal. In contrast, the latter argues that each emotion has unique coincidence of experience, psychology, and behaviour [6]. We base our work on the dimensional mode for estimating the emotion sphere in blogosphere. Specifically, we use the circumplex model of affect [9,10] since it conceptualizes emotion states simply via valence and arousal dimensions, which can be computed using ANEW. 3.1 Feature Selection Methods Denote by B the corpus of all blogposts and by M = { sad, happy, ... } the set of all mood categories. In a standard feature selection setting, each blogpost d  X  X  is also labeled with a mood category l extract from d a feature vector x ( d ) being as discriminative as possible for d to be classified as l d . For example, if we further denote by V = counting with its i -component x ( d ) i represents the number of times the term v i appears in blogpost d , a scheme widely known as bag-of-word representation. Term-based selection. These are features derived with respect to a term v . Two common features are term an d document frequencies where term frequency TF ( v, d ) represents the number of times the term v appears in document d , whereas document frequency DF ( v ) is the number of blogposts containing the term v . It is also well-known in text mining that TF.IDF ( v, d ) weighting scheme can potentially improve discriminative power where TF.IDF ( v, d )= TF ( v, d )  X  IDF ( v )with IDF ( v )= |B| /DF ( v ) is the inverse document frequency. In this work, a term v will be selected if it has high DF ( v ) value, or high average values of TF ( v, d )or TF.IDF ( v, d ) across all documents d over a threshold. Term-Class interaction-based selection. The essence of these methods is to capture the dependence between terms and corresponding class labels during the feature selection process. Three common selection methods falling into this cate-gory are information gain IG ( v ), mutual information MI ( v, l )and  X  2 -statistics CHI ( v, l )[14]. IG ( v ) captures the information gain (measured in bits) when a term v is present or absent; MI ( v, l ) measures the mutual information between a term v and a class label l ; and lastly CHI ( v, l ) measures the dependence between a term and a class label by comparing against one degree of freedom  X  2 distribution. Affective Norms for English Words ( ANEW ). Apart from feature sets learned from data, for sentiment analysis, some emotion bearing lexicons have been subjectively chosen by labor power could help. Among them is ANEW [1], a set of 1034 sentiment conveying English words. These words are rated in terms of valence, arousal, and dominance they could convey. We apply the proposed set of 1034 words in ANEW exclusively as the feature vector, which means each blogpost is represented as a sparse counting vector for these ANEW words. 3.2 Mood Classification Results It has been shown that linguistic components such as specific use of adverbs, adjectives or verbs can be a strong indicator for mood inference [8]. Therefore, in this paper, we further run a part-of-speech tagger to identify all terms that can be tagged as verbs, adjectives and adverbs. The tagger used is the SS-Tagger [12] ported to the Antelope NLP framew ork, giving a reasonable accuracy 2 .Three term weighting-based ( TF , DF , TF.IDF ) and three term-class interaction-based ( IG , MI , CHI ) selection methods are employed i n this experiment. These feature selection methods shall be applied either on all terms (unigrams) or with respect to a subset of terms tagged with a specific POS .

Our experimental design is to compare and contrast which feature selection methods work best and to examine the effect of specific linguistic components in the context of mood classification. For classification methods, we have exper-imented with many off-the-shelf classifiers such as SVM, IBK, C4.5 and so on, however, the naive Bayes classifier (NBC) consistently outperforms these meth-ods and therefore we shall only report the results w.r.t NBC. For each run, we use ten-fold cross-validation and repeat 10 runs and report the average result. To evaluate the results, we repor t two commonly-used measures: accuracy and F-score (which is measured based on recall and precision ).
 Effect of feature selection schemes and linguistic components. We use two datasets, namely WSM09 and IR05, for the task of mood classification. The first, WSM09, is provided by Spinn3r as the benchmark dataset for ICWSM 2009 conference 3 which contains 44 millions blogposts crawled between August and October 2008. We extract a subset from this dataset consisting only blog-posts from LiveJournal and query LiveJournal to obtain the mood groundtruth entered by the user when the posts were composed. We only consider the moods predefined by LiveJournal and discard others, resulting in approximately 600,000 blogposts. To validate the generalization of a feature selection scheme, we also run it on another dataset (IR05) created in [7] which contains 535,844 posts tagged the predefined moods. To make comparison with previous results in [11], we examine three popular moods { sad, happy, angry } in this experiment. The full set of 132 mood categories will be reported in the next section.
We run the experiment over combinat ion of feature selection methods on different linguistic subsets and report the top ten best results in Table 1.
With respect to feature selectio n scheme, information gain ( IG )isobserved to be the best selection scheme. Other ter m-class interaction based methods do not perform well, noticeably mutual information ( MI ) does not appear in any of the top ten results. These observations are consistent with what reported in [14] for text categorization problem. How ever, different with c onclusions in [14], we found that CHI performs badly for mood classification task and does not ap-pear in any top ten results. Surprisingly, both TF and DF performs better than TF.IDF in all-term (unigram) cases, which otherwise has been known oppositely in text mining that IF.IDF is often superior although much more computation-ally expensive. Thus, TF or DF should be the alternative candidates for IG for the trade-off of computational cost.

The performance of featur e selection schemes experimented is also agreeable well across two datasets as can be seen in Table 1, except for the first few rows. Our best result stands at 77.4% F-score for WSM09 and 78.8% for IR05, which is higher than what reported in [11] (66.1%). With respect to the effect of linguistic components (which are not experimented in [11] and [5]), a combination of adjectives, verbs and adverbs (AdjVbAdv) dominates the top ten results and gives a very close performance to usin g all terms; noticeably using verbs or adjectives alone shows a good performance.
 Performance of ANEW. Without the need of superv ised feature selection stage, the result of ANEW feature is found to be very encouraging, appears in both top ten results across two datasets. The results across two datasets are also consistent, stand at approximately 70% F-score (still better than the best result reported in [11]). While most of existing work has focused on supervised classification of mood, we are interested in discovering intrinsic patterns in mood structure using unsuper-vised learning approaches. Using a large, groundtruthed dataset of more than 17 millions posts introduced in [5], we aim to seek empirical evidences to answer various questions which have often posed in psychological studies. For example, does mood follow a continuum in its transition from  X  X leasure X  to  X  X ispleasure X , or from  X  X ctivation X  to  X  X eactivation X  ? Is  X  excited X  closer to  X  X  roused X  or  X  X appy X  ? Does  X  X epressed X  transit to  X  X a lm X  before reaching  X  X appy X  ?
We use a total of 132 predefined moods defined by livejournal.com 4 for the clustering task. Given a corpus of more than 17 millions posts, it means that feature selection schem es presented in section 3.1 are very expensive to per-form; for example, computing MI ( v, l ) for each pair (term, mood label) will take O ( |M|  X  |V| )where |M| = 132 (number of moods) and |V| is the number of unique terms which could be in the order of hundreds of thousands. Since our results in section 3.2 have shown that the proposed ANEW feature set gives comparable results, marginally lower (  X  8%) in the classification compared to the best result but can totally avoid the expensive feature selection step, we shall employ ANEW as the feature vector in this section.

We choose multidimensional scaling, in particular, self-organizing map (SOM) [3] for clustering purpose. We use the SOM-PAK package [4] and the SOM Toolbox for Matlab [13] to train and visualize the map. For training, an 9  X  7 map is used which accounts for nearly a half of the mood classes. Using the recommendations in [3], the horizontal axis is roughly 1.3 that of the vertical axis; the node topology is hexagonal, and the number of training steps is 32,000 (about 500 times of the number of nodes).

Due to space restriction, we omit coarse -level results and present in the Fig-ure 1 the structures of the clusters discovered in which top six moods in each cluster are included.

Several interesting patterns emerge from this analysis. At the highest level, one can observe the general transition of mood from an extreme of pleasure (clusters II, III, and V) to displeasure (clusters IV, VI, VII). On the pleasure polar we observe the moods having very high valence values 5 such as good (7.47), loved (8.64) or relaxed (7), whereas on the displeasure end, we observe the moods having low valence values such as enraged (2.46) or stressed (2.33). Certain mood transition is also evidential, for example the cluster path IV-II-III presents a transition pattern from infuriated to relaxed andthento good . Though not strongly emerging as in the case of pleasure  X  displeasure, a global pattern of activation  X  deactivation is also observed based on the analysis of the arousal measure as shown in Figure 1. Our results are indeed favorable of the core affect model for human emotion structure studied in psychology [9,10], generally agreeable with the global mood structure proposed in there. We addressed the problem of mood classification and pattern discovery in we-blogs. While the problem of machine lea rning based feature selection for text categorization has been intensively investigated, little work is found for textual based mood classification which is often more challenging. Our first contribution is a comprehensive comparison of different selection schemes across two large datasets. In addition, we propose a novel use of ANEW features which do not require a supervised selection phase, and thus, can be applied for mood analy-sis at a much larger scale. Our results have recalled similar findings in previous results, but also brought to light discoveries peculiar to the problem of mood clas-sification. Our newly proposed feature set has also performed comparatively well at a fraction of the computational cost of supervised schemes, and was further validated by the results of an unsupervise d clustering exercise, which clustered 17 million blog posts, and provided a unique view of mood patterns in the blogo-sphere. In particular, this study manifests global patterns of mood organization that are analogous to the pleasure X  X ispleasure and activation X  X eactivation di-mensions proposed independently in the psychology literature, such as the core affect model for the structure of human emotion. This data-driven organization of mood could be of interest to a wide range of practitioners in the humanities, and has many potential uses in sentiment-aware applications.

