 szhong@yahoo-inc.com Semi-supervised clustering employs limited supervision in the form of labeled instances or pairwise instance constraints to aid unsupervised clustering and often significantly im-proves the clustering performance. Despite the vast amount of expert knowledge spent on this problem, most existing work is not designed for handling high-dimensional sparse data. This paper thus fills this crucial void by developing a S emi-supervised C lustering method based on sphe R ical K-m E ans via f E ature projectio N (SCREEN). Specifically, we formulate the problem of constraint-guided feature projec-tion, which can be nicely integrated with semi-supervised clustering algorithms and has the ability to effectively re-duce data dimension. Indeed, our experimental results on several real-world data sets show that the SCREEN method can effectively deal with high-dimensional data and provides an appealing clustering performance.
 H.2.8 [ Database Management ]: Database Applications -Data Mining; I.5.3 [ Pattern Recognition ]: Clustering Algorithms, Experimentation Semi-Supervised Clustering, Pairwise Instance Constraints, Feature Projection
Semi-supervised clustering, learning from a combination of labeled and unlabeled data, has recently become a topic of significant interest to data mining and machine learning communities. Indeed, in many application domains, addi-tional information such as some labeled instances or pair-wise instance constraints are available and can be used to aid the unsupervised clustering process [4, 5, 10, 25, 26]. Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
Existing methods for semi-supervised clustering can be generally grouped into three categories. First, the constraint-based methods aim to guide the clustering process with pair-wise instance constraints [25] or initialize cluster centroids by labeled instances [4]. Second, the distance-based meth-ods employ metric learning techniques to get an adaptive distance measure used in the clustering process based on the given pairwise instance constraints [26]. Finally, the hy-brid method proposed by Basu et al. [5] unifies the first two methods under a general probabilistic framework.

However, most existing semi-supervised methods are not designed for handling high-dimensional data. It is well-known that the traditional Euclidean notion of density is not meaningful in high-dimensional data sets [7]. Since most semi-supervised clustering techniques are based on proxim-ity or density, they often have difficulties in dealing with high-dimensional data. Therefore, it is necessary to inte-grate feature reduction into the process of semi-supervised clustering. The key challenge is how we can incorporate supervision into dimensionality reduction such that the re-duced data can still capture the available class information.
To this end, we propose a S emi-supervised C lustering method based on sphe R ical K-m E ans via f E ature projec-tio N (SCREEN). Specifically, we first formulate the prob-lem of constraint-guided feature projection and provide an analytical solution to the associated optimization problem. Then, we exploit this constraint-guided feature projection technique to reduce the dimensionality of the original dataset and use constrained spherical K-means algorithm on the low-dimensional projected data for clustering.

In this paper, we consider supervision provided in the form of must-link and cannot-link constraintsonpairsof instances. A must-link constraint means that the pair of instances involved must reside in the same cluster while a cannot-link constraint means that the pair of instances should always be placed in different groups. Indeed, the use of must-link and cannot-link constraints is a natural and practical choice, because the labeled instances may not be available and are harder to collect than pairwise constraints. For example, in the context of clustering GPS data for lane-finding [25] or grouping different actors in movie segmenta-tion [3], the complete class information may not be available in these cases, but the pairwise instance constraints can be extracted automatically with minimal effort. Also, a user who is not a domain expert is more willing to provide an answer to whether two objects are similar/dissimilar more than to specify explicit labels. Moreover, pairwise instance constraints are more general than class labels in that we can always generate equivalent pairwise instance constraints from labeled instances, but not vice versa.

Finally, we have conducted experiments on some real-world datasets from different application domains. Our ex-perimental results show that, for high-dimensional data, the SCREEN method can achieve better clustering performance than the state-of-the-art, semi-supervised clustering meth-ods. In addition to this, we provide an analysis on the rel-ative importance of must-link and cannot-link constraints. This analysis indicates that cannot-link constraints are much more important than must-link constraints in providing su-pervision for the clustering process.

Overview. The remainder of this paper is organized as follows. In Section 2, we introduce the general framework of our proposed SCREEN algorithm. Section 3 discusses the experimental results on several real-world datasets. Related work on the existing methods of semi-supervised clustering is discussed in Section 4. Finally, in Section 5, we draw conclusions and make suggestions for future work. Symbols Description
X = { x i } N U = {  X  i } K 1 set of K cluster centroids
F = { F l } k 1 projection matrix calculated from M m  X  m
Before we describe the SCREEN method, we summarize the main notations used in this paper in Table 1. Figure 1: The Framework of the SCREEN Method.
 Figure 1 shows the framework of the SCREEN method. Given a set of instances and a set of supervision in the form of must-link constraints C ML = { ( x i ,x j ) } where ( x must reside in the same cluster, and cannot-link constraints C
CL = { ( x i ,x j ) } where ( x i ,x j ) should be in the different clusters, the SCREEN method is composed of three steps. In the first step, a pre-processing method is exploited to reduce the unlabelled instances and pairwise constraints according to the transitivity property of must-link constraints. In the second step, a constraint-guided feature projection method, called SCREEN PROJ , is used to project the original data into a low-dimensional space. Finally, we apply a version of semi-supervised clustering algorithms based on constrained spherical K-means on the projected low-dimensional dataset to produce the clustering results.

Therestofthissectionisorganizedasfollows. InSec-tion 2.1, we introduce our initialization method for the un-labeled instances and pairwise constraints in detail. Sec-tion 2.2 presents the constraint-guided feature projection method X  SCREEN PROJ  X  X hich gives an analytical solution to the optimization problem in finding the projection ma-trix. Finally, in Section 2.3, we describe our semi-supervised clustering based on constrained spherical K-means to pro-duce the final clustering results.
If there is no error in the pairwise constraints, it is easy to demonstrate that the must-link constraints represent an equivalence relation. This enables us to replace each transi-tive closure of must-link constraints with its average instance as demonstrated in Figure 2, where the solid line represents the must-link constraint and the dashed line represents the cannot-link constraint. In Figure 2, original instances are shown in white nodes and the average instances of transi-tive closures are represented by grey nodes. Sets { a 1 ,a { b tive closures forced by must-link constraints. After initial-ization, we can eliminate all must-link constraints and use the average instances a , b and c in each closure to rep-resent the original cannot-link constraints. In the trans-formed dataset, the size of each transitive closure becomes the weight of the representative instance. Note that if there are some erroneous constraints, we need to identify and re-move them in the initialization step. However dealing with the mis-specified constraints is out of the scope of this paper.
The benefit of initialization is that we simplify the prob-lem of constraint-guided clustering where we only need to focus on the cannot-link constraints in the optimization pro-cess as described in the following subsection. Another ben-efit is that we can further reduce the size of unlabelled in-stances and cannot-link constraints. This is helpful for deal-ing with some large datasets.
In the previous work [5, 26], the pairwise constraints were used for learning an adaptive metric between the prototype of instances. However, learning a distance metric among high-dimensional instances is very time consuming. More importantly, recent research on high-dimensional space has shown that the concept of distance in high-dimensional space may not be meaningful [7]. Instead of using constraint-guided metric learning, in this paper we propose a constraint-guided feature projection approach (SCREEN PROJ )tofur-ther improve the performance of semi-supervised cluster-ing in the high-dimensional datasets. The objective is to learn the projection matrix F d  X  k = { F 1 ,...,F k } containing k orthogonal unit-length d -dimensional vectors, which can project the original datasets into a low-dimensional space such that the distance between any pair of instances in-volved in the cannot-link constraints are maximized while the distance between any pair of instances involved in the must-link constraints are minimized. The objective function we try to maximize is: f = subject to the constraints where  X  denotes L 2 norm and F is the projection matrix whose column vectors are orthogonal to each other.
As we described in Section 2.1, by applying the initial-ization methods shown in Figure 2, we can eliminate each transitive closure from must-link constraints with its equiva-lent average instance. Then, the objective function in Equa-tion (1) can be further reduced to: where { w i } N i =1 is the set of weights (which is measured by the number of instances in each transitive closure) for the reduced instances after pre-processing to the must-link con-straints and N is the reduced size of instances ( N  X  N ).
Note that in Equation (3), we still adopt the Euclidean distance instead of the cosine similarity as in the SPKM al-gorithm to calculate the objective value. This is because we work on the unit-length instances that satisfy the property in the following equation:
From Equation (4), we can see that using the cosine sim-ilarity is equivalent to using the Euclidean distance when operating on the unit-length instances.

There exists an analytical solution to the above optimiza-tion problem of finding the optimal projection matrix F in Equation (3). The following theorem shows that the opti-mal projection matrix F is given by the first k eigenvec-tors of the covariance matrix M d  X  d for a difference matrix C  X  m ,whereeachcolumnof C is a weighted difference vec-tor w 1 w 2  X  ( x 1  X  x 2 )  X  R d for a pair ( x 1 ,x 2 )in C is the number of pairs of cannot-link constraints.
Theorem 1. Given the desired dimensionality k ( k&lt;d ) , the set of cannot-link constraints C CL , and the covariance matrix M = cov ( C ) (where C is defined as above), the opti-mal projection matrix F d  X  k is comprised of the first k eigen-vectors of M corresponding to the k largest eigenvalues.
Proof. Consider the objective function f = where F l  X  X  are subject to constraints F T l F h = 1 for l = h and 0 otherwise.

Using the traditional Lagrange multiplier optimization tech-nique, we write the Lagrangian
By taking the partial derivative of L F 1 ,...,F k with respect to each F l and setting it to zero, we get
It is clear from Equation (6) that solution F l is an eigen-vector of M and  X  l is the corresponding eigenvalue of M. To maximize f , F must be the first k eigenvectors of M which makes f the sum of the k largest eigenvalues of M .
After the constraint-guided feature projection as described above, we can represent the original instances in a low-dimensional space which conforms to the class information given in the form of pairwise constraints.
Since we eliminate the must-link constraints and shrink the original dataset in the initialization step, our version of constrained spherical K-means for semi-supervised cluster-ing is slightly different from the one as shown in [25]. In this section, we introduce our version of constrained spher-ical K-means algorithm for semi-supervised clustering.
Given a set of reduced instances X = { x 1 ,...,x N } with the corresponding weights W = { w 1 ,...,w N } ,asetof cannot-link constraints, and a pre-specified number of clus-ters K , we aim to find K disjoint partitions. As shown in [12], finding a feasible solution for the cannot-link con-straints is much harder than that for the must-link con-straints. It is computationally intractable to find an exact cluster assignment which does not break any cannot-link constraints. Therefore, we adopt a local greedy heuristic to update cluster centroids as follows.

Given each cannot-link constraint ( x i ,x j )  X  C CL , we find two different cluster centroids  X  x is maximized and assign x i and x j to these two different centroids to avoid violating the cannot-link constraint. Fig-ure 3 shows the pseudo-code of our Pairwise Constrained Spherical K-means (PCSKM) clustering algorithm.
 Figure 3: The Pairwise Constrained Spherical K-means Clustering Algorithm.

It is worth noting that our algorithm for semi-supervised clustering differs from the one used in [5] in that we do not consider the relative importance among cannot-link con-straints, and we do not utilize pairwise constraints to su-pervise the cluster centroids initialization. However, these techniques can be easily incorporated into our algorithm. Finally, an overview of our SCREEN method is shown in Figure 4. Please note that since the pre-process step to the pairwise constraints will shrink the original dataset, we need to post-process the resultant K partitions (clusters) in order to be in accordance with the original dataset.
In this section, we study the performance of the SCREEN method. Specifically, we demonstrate: (1) the effectiveness of constraint-guided feature projection, (2) the relative im-pact of Must-link and Cannot-link constraints on the per-formance of the SCREEN method, (3) the choice of reduced dimensionality, (4) the computational performance of the SCREEN method, and (5) the clustering performance of SCREEN, compared with several existing semi-supervised clustering algorithms.

Figure 4: An Overview of the SCREEN Method.
Experimental Datasets. Our experiments were per-formed on a couple of real-world datasets from different ap-plication domains. There are six UCI datasets 1 [20], six datasets derived from TREC collections 2 and nine datasets constructed from 20-Newsgroup [18]. The descriptions of these data sets are summarized as follows. 1. Six UCI datasets: balance-scale , ionosphere , iris , soy-2. Six data sets: tr11 , tr12 , tr23 , tr31 , tr41 ,and tr45 from 3. In order to evaluate the overall performance of our
Evaluation Measures. In this paper, we use normal-ized mutual information (NMI) as the clustering validation measure. NMI, an external validation metric, estimates the quality of clustering with respect to the given true labels of http://www.ics.uci.edu/  X  mlearn/MLRepository.html http://trec.nist.gov the datasets [24]. If  X  Z is the random variable denoting the cluster assignments of the instances and Z is the random variable denoting the underlying class labels, then NMI is defined as where I (  X  Z ; Z )= H ( Z )  X  H ( Z |  X  Z ) is the mutual information between the random variables  X  Z and Z , H ( Z ) is the Shan-non entropy of Z ,and H ( Z |  X  Z ) is the conditional entropy of Z given  X  Z [11]. The range of NMI values is 0 to 1. In gen-eral, the larger the NMI value is, the better the clustering quality is. NMI is better than other external clustering vali-dation measures such as purity and entropy, since it does not necessarily increase when the number of clusters increases.
Finally, we implemented the SCREEN algorithm in Mat-lab and conducted our experiments on a machine with 4 Intel Xeon 2 . 8 GHz CPUs and 2G main memory running under the GNU/Linux operating system. For each test dataset, we repeated experiments for 20 trials. For the UCI datasets, we randomly generated 100 pairwise constraints in each trial. For the Trec datasets and data sets from the 20-Newsgroup collection, we randomly generated 500 pairwise constraints from half of the dataset, and tested the performance on the whole dataset. Also, the final result is the average of the results from the 20 trials.
In this section, we compare SCREEN PROJ with some ex-isting dimensionality reduction methods such as PCA and RCA 3 . In order to do a thorough comparison, we used both relatively low-dimensional datasets from the UCI repository and high-dimensional datasets from the Trec corpus. For the low-dimensional UCI datasets, we used the standard K-means algorithm as the baseline clustering algorithm. For
Thanks to the authors for providing their code on-line at http://www.cs.huji.ac.il/  X  tomboy/code/RCA.zip for [3] the high-dimensional Trec datasets, we chose the spherical K-means algorithm [14] instead.

For six UCI datasets, Figure 5 shows the clustering perfor-mance of standard K-means applied to the original as well as projected data by different dimension reduction algorithms with different numbers of pairwise constraints. As can be seen, RCA performs well on low-dimensional data. Also, the performance of RCA significantly improves as the number of available constraints increases. However, we can also ob-serve that the performance of RCA can be worse than that of PCA when there is a small number of constraints in the datasets such as Vehicle and Wine (we know that PCA is unsupervised and does not use any pairwise constraints). In contrast, the performance of SCREEN PROJ is always com-parable to, or better than that of PCA. Finally, we observe that the performance of SCREEN PROJ is comparable to that of RCA on Soybean and Iris datasets.

However, our experiments on six high-dimensional Trec data sets show that the performance of RCA heavily de-pends on the dimensionality of the original data. Also, it is computationally expensive to directly apply RCA to high-dimensional datasets. Indeed, for data sets with extremely high dimensions, we need to first reduce their dimension to a lower level before applying the RCA method. Therefore, for the purpose of a fair comparison, we first used PCA to project the original data into a 100-dimensional space, and then applied the different algorithms to further reduce the dimensionality to 30. Figure 6 shows the results of this ex-periment on six Trec datasets. In the figure, we observe that SCREEN PROJ nearly always achieves the best performance on all six test datasets. In contrast, although we first reduce the dimension of six data sets to 100 using PCA, the perfor-mance of RCA is still the worst among all the algorithms. In other words, RCA may not be a good dimension reduction method for high-dimensional data.
Here, we compare the relative impact of must-link and cannot-link constraints on the performance of the SCREEN method. In this experiment, we incorporate a parameter  X  to the objective function in Equation (3) to adjust the rela-tive impact between must-link and cannot-link constraints:
From Equation (9), we observe that  X  = 0 is equivalent to only using cannot-link constraints in finding the optimal feature projection matrix. When  X  =1,weonlyuse must-link constraints to perform the feature projection. In our experiments, we varied the value of  X  instepsof0 . 1from0to 1. The clustering results, as measured by NMI, are plotted in Figure 7 with respect to different values of  X  . In the figure, the x -axis denotes the different values of parameter  X  and the y -axis denotes the clustering performance measured by NMI.

As can be seen in Figure 7, there is no significant difference on the clustering performance when  X  is in the range of 0 . 1to 0 . 9. However, when using only must-link constraints (  X  =1) the clustering performance deteriorates sharply. This indi-cates that the cannot-link constraints are more important than the must-link constraints in guiding the feature pro-jection to get meaningful representation for each instance in the low-dimensional space.
In this section, we empirically evaluate the impact of dif-ferent number of reduced dimension k on the performance of Figure 7: The relative impact of must-link and cannot-link constraints.

Figure 8: The Choice of Reduced Dimension k . the SCREEN method. Specifically, we test the performance of SCREEN in terms of NMI with respect to different values of k varing from 10 to 100. Given a specified k , we repeat ex-periments 20 times. In each trial, we randomly generate 250 pairwise constraints as additional information. The experi-mental results on the datasets from 20-Newsgroup and Trec collections are shown in Figure 8 as four boxplots, which cor-responds to the experimental results on sub-datasets from Binary, Multi5, Multi10, and Trec.
 There are three sub-datasets: Binary 1 , Binary 2 ,and Binary 3 from the Binary data collection. The Binary box-plot shows the k values when SCREEN achieves the best clustering performance on these three sub-datasets. Also, the Multi 5 boxplot shows the k values when SCREEN has the best clustering performance on three sub-datasets: Multi 5 Multi 5 2 , Multi 5 3 . In addition, the Multi 10 boxplot presents the k values for the case that SCREEN has the best cluster-ing performance on three sub-datasets: Multi 10 1 , Multi 10 Multi 10 3 . Finally, there are six sub-datasets: tr 11, tr 12, tr 23, tr 31, tr 41, and tr 45 from the Trec corpus. The Trec boxplots shows six k values which lead to the best clustering performance of SCREEN on these datasets.

In Figure 8, we observe that SCREEN achieves the best performance at different k values for different datasets. How-ever, we notice that the clustering performance is maximized when the median of k values is between k =20and k = 40. In all our experiments, we use this as a guideline for the choice of k values.
In the following two subsections, we evaluate the overall computational and clustering performance of our SCREEN method. The benchmark algorithms are listed as follows.
First, we evaluate the computational performance of the semi-supervised clustering algorithms on the selected 20-Newsgroup datasets with respect to different numbers of pairwise constraints. Due to the space limit, for each cat-egory of the datasets we only give out one result since the datasets from the same category usually lead to similar out-puts. The experimental results are summarized in Figure 9, where the x -axis denotes the number of pairwise constraints and the y -axis denotes the elapsed running time in log scale. Please note that since the MPCSKM algorithm is not im-plemented in Matlab, we did not include this algorithm for this comparison.

As demonstrated in Figure 9, the execution time of SPKM is consistently the lowest among all the methods since it does not perform extra work in addition to enforcing the clus-tering process based on the pairwise constraints. The PC-SKM+M algorithm via metric learning method is always the slowest when compared to other methods. This is because the metric learning method has to learn a different weight for each individual dimension. When the dimensionality is very high, the cost of metric learning will be very high com-pared to the constraint-guided feature projection. In Fig-ure 9, we can see that the SCREEN method, which utilizes the feature projection method, is only slightly slower than the PCSKM algorithm due to the extra work on the super-vised dimensionality reduction, but much faster than that of PCSKM+M algorithm. This is because it only involves an eigen value decomposition of a covariance matrix formed by cannot-link constraints, which can be implemented ef-ficiently by Singular Value Decomposition (SVD). In addi-tion, one can still explore some iterative methods, such as
Thanks to the authors for putting their implementation on-line at http://www.cs.utexas.edu/users/ml/risc/ for [5]. EM algorithm for PCA [23] or Nystrom method [9], to fur-ther improve the efficiency.
Here, we compare the clustering performance of various semi-supervised clustering algorithms. The results are shown in Figure 10, where the x -axis denotes the number of pair-wise constraints, and the y -axis denotes the clustering per-formance in terms of NMI. We have tested various values of k -the number of reduced dimension. Due to the space limit, we select k = 30 to report the results.

In general, it is clear that on most datasets, the cluster-ing performance of all algorithms constantly improve with the increase of the number of pairwise constraints. How-ever, the clustering performance of the SCREEN method is more stable compared to the other methods, and always outperforms the PCSKM+M algorithm via metric learning and MPCSKM algorithm via HMRF model. This is mainly due to the fact that constraint-guided feature projection can easily produce more condensed and meaningful representa-tions for each instance. In addition, PCSKM+M is not much better than the PCSKM algorithm except for the Multi10 dataset. This is because it is a big challenge for metric learn-ing to learn a reasonable distance measure between any pair of sparse instances in the original high-dimensional space.
The related literature on semi-supervised clustering can be grouped into three categories: constraint-based methods, distance-based methods, and a combination of constraint-based and distance-based methods.

For constraint-based methods, the cop-k means algorithm [25] guides the cluster allocation process by a constraint moti-vated heuristic objective function. However, this algorithm strictly enforces the clustering process such that any vio-lation of the given pairwise constraints is forbidden, which limits its use, especially in a noisy environment. In con-trast, our version of semi-supervised clustering algorithm allows some relaxation of the pairwise constraints. Also, Basu et al. [4] proposed a seeded K-means which tries to get better initial cluster centroids from the labeled instances in addition to constraining the clustering process, while their supervised cluster initialization is based on the labeled in-stances instead of pairwise constraints.

For distance-based methods, Cohn et al. [10] used gradi-ent descent for weighted Jensen-Shannon divergence in the context of EM clustering. Xing et al. [26] combined the Newton Raphson method and iterative projection together to learn a Mahalanobis distance for K-means clustering. De Bie et al. [8] proposed a more efficient algorithm for learn-ing the distance metric with side information, which uti-lized Canonical Correlation Analysis (CCA) to approximate LDA. In general, the metric learning used in the distance-based method, which is equivalent to learning an adaptive weight for each dimension, is either based on iterative al-gorithms, such as gradient descent and Newton X  X  method, or involves some matrix operations. However, the distance-based method has high computational cost when applied to the high-dimensional data. Indeed, data represented in ma-trix is often singular when the sparsity of the data is high. This makes some matrix operations, such as inversion, com-putationally intractable.

For hybrid methods, Basu et al. [5] introduced a general probabilistic framework which unifies the constraint-based and distance-based method into the Hidden Markov Random Field (HMRF). The proposed HMRF-EM algorithm can in-terweave the constrained clustering and distance learning interactively in the process of semi-supervised clustering. Also, the related literature on feature reduction includes Principle Component Analysis (PCA) [22] which tries to find a low rank approximation to represent the high-dimensional data, and Fisher X  X  Linear Discriminant Analysis (LDA) [15] which tries to find one or more directions along which differ-ent classes can be best separated while the variance of each class is minimized given the label for each instance. The PCA method works in an unsupervised manner where the class information is not available, which makes the reduced dataset incapable of capturing the original class informa-tion. In contrast, the LDA method needs to know the exact information in order to calculate the between/within-class scatter matrix. Our constraint-guided feature projection method differs from the traditional LDA method in that we incorporate a more general supervision in the form of pair-wise constraints instead of the complete class information which may be unavailable in certain application domains. To the best of our knowledge, the most related work is the Relevant Component Analysis (RCA) algorithm [3], which is based only on must-link constraints and tries to learn a Mahalanobis distance using Whitening transform [16].
In this paper, our major focus is to provide an alter-native way to improve the semi-supervised clustering for high-dimensional sparse data by constraint-guided feature projection instead of metric learning. Existing approaches for clustering high-dimensional data usually involve the use of feature projection and feature selection. Feature projec-tion techniques, as we described above, attempt to repre-sent a dataset by its latent variables which are usually much fewer than the number of original features. Feature selec-tion methods select only the most relevant dimensions from a dataset to summarize its instances. The typical algorithms of feature selection include  X  X rapper methods X  and  X  X ilter methods X . The recently proposed subspace clustering algo-rithm [21] can be regarded as an extension to the feature selection methods which attempts to find clusters in differ-ent subspaces of the same datasets. Existing subspace clus-tering algorithm can be grouped into two categories: the top-down method such as projected clustering [1] and the bottom-up method such as CLIQUE [2] etc. An excellent survey on techniques of subspace clustering is available in [21]. It is worth noting that the recently proposed method on semi-supervised projected clustering [27] utilized the lim-ited supervision in the form of labeled instance in subspace clustering. The motivation of this work is to find a more compact representation for each cluster to efficiently repre-sent the instances within it. Also, they used feature selection instead of feature projection in applying the supervision in each cluster. This is different from our method.
In this paper, we proposed a S emi-supervised C lustering method based on sphe R ical K-m E ans via f E ature projec-tio N (SCREEN), which is tailored for handling sparse, high-dimensional, data. Specifically, the SCREEN method first uses the constraint-guided feature projection to reduce the dimensionality and then applies the constrained spherical K-means algorithm to cluster data with reduced dimension.
In the development of the SRCEEN method, we formu-late the problem of constraint-guided feature projection as an optimization problem. The goal is to find a feature pro-jection matrix based on the pairwise instance constraints, and give an analytical solution which can be implemented without too much effort. In addition, for the constrained spherical K-means algorithm, we introduce a heuristic solu-tion to loosely enforce the pairwise constraints, which enable it to be applied in much wider application domains.
Finally, we have compared the SCREEN method with ex-isting semi-supervised clustering methods using real-world datasets. The experimental results indicate that SCREEN can achieve a better clustering performance with a smaller computational cost. We also studied the relative impact of must-link and cannot-link constraints in guiding the cluster-ing process. Our analysis shows that the cannot-link con-straints are more important than the must-link constraints in providing meaningful class information.
 There are several potential directions for future research. First, we are interested in automatically identifying the right number for the reduced dimensionality based on the back-ground knowledge other than providing a pre-specified value. Second, we plan to explore alternative methods to employ supervision in guiding the unsupervised clustering, e.g., su-pervised feature clustering.
We thank Dr. Xingquan Zhu at Florida Atlantic Univer-sity for his insightful comments. This research was sup-ported in part by an IBM Faculty Award, by a Faculty Research Grant from Rutgers Business School-Newark and New Brunswick, and by a Motorola Research Grant. [1] C. C. Aggarwal, C. M. Procopiuc, J. L. Wolf, P. S. [2] R. Agrawal, J. Gehrke, D. Gunopulos, and [3] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. [4] S. Basu, A. Banerjee, and R. J. Mooney.
 [5] S.Basu,M.Bilenko,andR.J.Mooney.A [6] M. Berry, Z. Drmac, and E. Jessup. Matrics, vector [7] K. Beyer, J. Goldstein, R. Ramakrishnan, and [8] T. Bie, M. Momma, and N. Cristianini. Efficiently [9] C. Burges. Geometric methods for feature extraction [10] D. Cohn, R. Caruana, and A. McCallum.
 [11] T. M. Cover and J. A. Thomas. Elements of [12] I. Davidson and S. S. Ravi. Clustering with [13] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [14] I. S. Dhillon and D. S. Modha. Concept [15] R.O.Duda,P.E.Hart,andD.H.Stork. Pattern [16] K. Fukunaga. Statistical pattern recognition .Academic [17] G. Karypis. Cluto -a clustering toolkit, 2002. [18] K. Lang. News weeder: learning to filter netnews. In [19] A. K. McCallum. Bow: A toolkit for statistical [20] D.J.Newman,S.Hettich,C.L.Blake,andC.J.
 [21] L. Parsons, E. Haque, and H. Liu. Subspace clustering [22] K. Pearson. On lines and planes of closest fit to [23] S. Roweis. EM algorithms for PCA and SPCA .In [24] A. Strehl, J. Ghosh, and R. Mooney. Impact of [25] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. [26] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. [27] K. Y. Yip, D. W. Cheung, and M. K. Ng. On
