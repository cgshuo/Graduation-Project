 Department of Computer Science and Engineering, School of Engineering, Shiraz University, Shiraz, Iran 1. Introduction
Mining frequent itemset (or pattern) is an interesting problem in the context of knowledge discovery and data mining. The Apriori algorithm for frequent itemset mining was fi rst introduced by Agrawal et al. [1,3] for static databases. Since then, frequent itemset mining has been the focus of many researchers in the data mining community [12,22,23,26,30,32,33]. An itemset (set of items) is frequent in a database if the number of its occurrences in the database is greater than a speci fi ed threshold. This threshold is determined by the user of the mining process. The well known application of frequent itemset mining is in retail markets where the user is interested in identifying products that are purchased together frequently. The frequent pattern mining is not limited to static databases but it is extended to incremental databases and data streams.

Due to characteristics of data streams with respect to static databases, data stream mining of frequent patterns has special requirements. In data stream mining, data elements have to be processed in one scan since it is impossible to store the entire data stream. Moreover, due to huge amount of data and rapid data arrival rate, it is infeasible to rescan the data multiple times. Thus, the mining process must be performed at a rapid rate using limited amount of main memory. In addition, the stream data mining process must be able to adapt with occurrence of concept change. Concept change(or concept drift) in the context of frequent pattern mining is referred to as the change in status of itemsets from frequent to infrequent or vice versa over data streams as time goes by. This change of status may occur multiple times for each itemset, thus making frequent itemsets mining in data streams even more challenging than traditional static databases.

Commonly used approaches to handle and model data streams in frequent itemsets mining methods are based on window models. The three main window based models include the landmark model, the damped model and the sliding window model. In the landmark model, data between a point of time namely landmark and the current time are considered. In the damped model weights are assigned to data elements based on their arrival order within the data stream. That is, the more recent data elements have higher weights assigned to them. In the sliding window model, a fi xed length of recently arrived data is speci fi ed for the mining purpose. For example, having window W on a stream of transactions, only | W | transactions or all of the transactions within the last | W | units of time are used in the mining process. Here, | W | is the size of the window and can be determined by the user.

In this model, by the incoming of a new transaction, the oldest transaction is removed from the window and the new transaction is inserted to the window. As new transactions arrive from the data stream, the window slides forward; thus, it always contains the newest transactions. The window is always stored and maintained within the main memory. Due to unbounded amount of incoming transactions and limited amount of memory, the window size must be limited to | W | . The length of the window can be determined either by the time interval or the number of transactions. In the fi rst case, it is called a time sensitive window and in the second it is referred to as transaction sensitive ( transactional ) window . Since the cost of insertion and deletion of transactions is signi fi cant, batches (panes) of transactions can be added or removed from the window instead of individual transactions. In other words, the window includes batches (or panes) of transactions. There are a number of research studies for mining recent frequent patterns based on the sliding window model [6,14,16,18,21,25].

In this paper, we propose a novel method based on the sliding window model for frequent patterns mining over data streams. The idea is quite simple. Each item in the current window is represented by using its corresponding transactions X  numbers. The window content is adjusted when the concept change occurs during the processing. This adjustment is performed to reduce the amount of memory usage. An adopted version of dEclat [30] algorithm is used to extract frequent patterns from the most recent transactions when the user submits a mining reque st. Our proposed method provides exciting results in terms of memory usage and run time with respect to recently proposed algorithms. The reminder of paper is as follows. The next section presents a review on recently proposed frequent pattern mining algorithms over data stream. Problem statement is described in Section 3. In Section 4 the new method is presented after overview of the Eclat and dEclat algorithms. The experimental results are presented in Section 5. Finally Section 6 concludes the paper. 2. Related works
The problem of mining frequent pattern over data streams is introduced in [20] where the authors start with frequent items mining and then extend their idea to frequent itemsets mining. They proposed two approximate algorithms namely Lossy Counting and Sticky Sampling for mining frequent items and then extended the Loosy Counting for mining frequent itemsets. These algorithms provide approximate results with an error bound. In frequent items mining every singleton item incoming from a data stream is considered as a transaction. Frequent items mining algorithms are reviewed and experimentally compared with each other in [19].

Over the past few years, a number of exact and approximate methods have been proposed for mining frequent patterns in data streams. Most of them are based on the landmark window model [17,29,34] and the sliding window model [14,16,18,25]. DSM-FI [17] is a landmark based algorithm. In this algorithm every transaction is converted into smaller transactions and inserted into a summary data structure called item-suf fi x frequent itemset forest which is based on pre fi x-tree. In [29] the authors used the Chernoff Bound [7] to produce an approximate result of frequent patterns over the landmark window. Zhi-Jun et al. [34] used a lattice structure, referred to as a frequent enumerate tree , which is divided into several equivalent classes of stored patterns with the same transaction-ids in a single class. Frequent patterns are divided into equivalent classes, and only those frequent patterns that represent the two borders of each class are maintained; other frequent patterns are pruned.

Giannella et al. [9] developed an FP-tree based algorithm, called FP-Stream , to mine frequent patterns at multiple time granularities using a novel titled-time window technique. Lin et al. [18] proposed a new method for mining frequent pattern over time sensitive sliding window. In their method the window is divided into a number of batches for which itemset mining is performed separately. Chang and Lee [5] proposed the estWin that fi nds recent frequent patterns adaptively over transactional data streams using sliding window model. This algorithm requires the signi fi cant support in addition to the minimum support threshold to adaptively maintain the approximate frequent patterns. In [24] the authors proposed an algorithm called AP of transactions. The AP data and uses an effective method for inferring the global support of previously infrequent itemsets. In this algorithm, upper bound, lower bound and interpolated support of each pattern found are returned. Tsai [27] proposed a framework for data stream mining, called the weighted sliding window model. This model allows the user to specify the number of windows for mining, the size of a window, and the weight for each window. Thus users can specify a higher weight to a more signi fi cant data section, which will make the mining result closer to user X  X  requirements.
 In [16] an algorithm namely MFI-TransSW was proposed which is based on the Apriori algorithm. This algorithm mines all frequent itemsets over recent window of transactions. It uses sequence of bits for every item in the window. Appearance of an item in a transaction is shown using 1 and absence of the item in a transaction is represented by 0. For inserting a new transaction and removing the oldest transaction from the window, left shift is performed on the bit string of all items. Similar to the Apriori , this method uses candidate generation and test in order to mine frequent patterns.

Some stream mining algorithms use the pre fi x-tree to store the sliding window information. Most of them are extended version of the well known algorithm of FP-Growth [12] which is a tree based method without candidate generation proposed for mining in static databases. One of these methods is the DSTree [14]. In this algorithm, transactions within a window are divided into a number of batches (or panes) and the information about every batch is maintained in a pre fi x tree. Nodes of the pre fi xtreeshow items in the transactions which are sorted in a canonical order. Transactions are inserted to and removed from the tree in a batch by batch manner. Frequent itemsets are mined using the FP-Growth method when the user requests it. Another recently proposed pre fi x tree based algorithm is the CPS-Tree [25] which is more ef fi cient than the DSTree . This method is similar to the DSTree algorithm but dynamically reconstructs the pre fi x tree. In the CPS-Tree , nodes are sorted in descending order of their support. The tree is dynamically reconstructed to maintain support descending order of nodes. Tree reconstruction is performed to reduce the amount of memory usage for the tree structure. The pre fi xtreemustbe monitored continuously during the data stream processing in order to determine when the restructuring must be performed. The tree reconstruction is time consuming especially when the concept change occurs frequently in the data stream. In this case, the order of items in the pre fi x tree continuously changes due to change in their support values. This increases the number of tree reconstructions. In DSTree and CPS-Tree the incoming panes of transactions are inserted to and old panes of transactions are removed from the pre fi x tree structure. The CPS-Tree uses an ef fi cient extraction mechanism for old panes. Additionally, the CPS-Tree has better mining time due to its high compact pre fi x tree structure. Mozafari et al. [21] propose an approach which divides the window into a number of panes. This algorithm is based on the partitioning algorithm [22] proposed for static databases. In this method, itemsets which are frequent in at least one of the pane are considered for further analysis.
 Some proposed data streammining algorithms concentrate on fi nding special types offrequent itemsets. The Moment algorithm [8] fi nds closed itemsets by maintaining a boundary between frequent closed itemset and other itemsets. In [15] the authors present an algorithm for mining non-derivable frequent itemsets over data streams. The estMax [4] is an algorithm proposed to mine maximal itemsets over data streams. In [13] the authors propose a different approach for mining frequent patterns over data stream based on concept drift. This approach continuously monitors the data stream to detect any occurrence of a concept shift. The frequent itemset mining is triggered only when any signi fi cant change is observed. This change is detected according to an estimated changing rate of frequent itemsets. There are some studies which use frequent pattern mining in data stream in speci fi c application. For example, Wan et al. uses frequent patterns in different categories to diagnose different types of events in sensor network data streams [28].
 In this study, a new method to extract set of all frequent itemsets over sliding window is proposed. Similar to most frequent itemset mining algorithms over data streams, our algorithm is also based on a previously proposed method for static databases. 3. Problem statement
Let I = { i 1 ,i 2 ,...,i each transaction is a subset of I . For an itemset X , which is a subset of I , a transaction T in S is said to contain the itemset X if X  X  T . The support of X is de fi ned as the percentage of transactions in S that contain X . For a given support threshold s , X is frequent if the support of X is greater than or equal to s%, i.e., if at least s% of transactions in S contain X . Transaction sensitive sliding window over data stream S contain | W | recent transactions in the stream, where | W | is the size of the window. The window slides forward by inserting a new transaction and deleting the oldest transaction from the window. Due to ef fi ciency issues, instead of a single transaction, the unit of insertion and deletion can be a pane (or batch) of transactions. The current transactional window over the data stream S is de fi ned as TSW i th transaction in the S , respectively. In fact the window contains the n most recent transactions of the data stream. An itemset X is said to be frequent in TSW if Sup ( X ) | W | .s ,where Sup ( X ), | W | and s are support of X , size of the window and the minimum support threshold, respectively. Thus, having a transactional sliding window TSW and a minimum support threshold speci fi ed by the user, the problem is de fi ned as fi nding all the frequent itemsets that exists in the recent TSW .

Figure 1 shows a data stream of transactions, where the left column shows the transaction id, i.e., Tid of incoming transactions and the right column shows the items within each transaction. Two consecutive transactional sliding windows W 1 and W 2 are de fi ned over the so far received transactions. As shown in the fi gure, transactions are inserted to and deleted from the window, pane by pane. Each window consists of two panes and each pane contains two transactions. In the other words, both of the window size and pane size are equal to 2. The aim is to mine all frequent itemsets that exists within the recent window after the user makes a mining request. 4. The proposed method
The new method proposed here is based on the dEclat [30] algorithm which is an improved version of the Eclat algorithm [32,33]. The Eclat algorithm is a depth fi rst method for mining all frequent itemsets in static databases operating on the vertical layout of a database. In the vertical layout, a database is represented by a transaction id list ( tidlist ) for every item. The tidlist for each item is a linked list containing the Tid s of transactions in which the item appeared. If a transaction contains an item, its tid will appear in that item X  X  tidlist .The Eclat algorithm obtains the support of itemsets by intersecting tidlists of its items.
 The Eclat algorithm is described here by an example. Figure 2 shows all the steps required by the Eclat for mining all frequent patterns (with minimum support of 1) in the whole database of Fig. 1. As shown in Fig. 2, the algorithm starts by transforming the database into its vertical layout D (the root of the tree), where each item is stored together with its tidlist in a column. Then recursively, conditional databases are formed. The arcs indicate the recursions. The tree is traversed depth-fi rst, from left to right. In conditional databases, items are lexicographically ordered from left to right. The conditional databases are formed by intersecting the tidlists . For example, by using D a the conditional database D ad is formed. In D a ,the tidlist of d i.e. [1 X 3] is intersected with the tidlist s of all items that come after d in D a . In this conditional database only item e having tidlist [3] comes after d . Therefore, D ad contains only one column namely e which is the result of intersection of tidlists of d and e ([1 X 3]  X  [3] = [3]).
The vertical layout of database represents the root conditional database of  X  itemset. At any time, only the parents of the conditional database being constructed are kept in the memory. For example, when D ad is constructed, the databases D a and D are in the memory; once a conditional database is no longer needed, it is removed from the memory. In order to obtain a better run time, the items can be sorted using their supports in an ascending order. In a conditional database, if the size of tidlist becomes lower than the support threshold, the corresponding item is deleted from the database (e.g., item e is deleted from D b ) and it is not used in the next recursion. Each D  X  contains information about the support of itemsets pre fi xed by  X  and terminated by the items within the D  X  . Thus, the support of an itemset  X  iis equal to the size of tidlist of item i in D  X  . For example the support of abc is equal to 2 because the size of tidlist of item c ([2,4]) in the conditional database of D ab is equal to 2.

In some databases known as dense databases, some items may have long tidlist s and thus the generation of conditional database may become expensive in terms of memory usage and processing time. In order to overcome this shortcoming, a modi fi ed version of Eclat algorithm called dEclat [30] was proposed in which for every k-itemset of I the difference between the tidlists of I and its k  X  1pre fi x is stored. This of its k-1 pre fi x. For an item, the dif fl ist contains tid s of transactions for which the item does not occur in them. This technique reduces the memory usage and enhances the run time. The dEclat algorithm can start its operation either by the tidlist sor dif fl ist s of items [30]. Other aspects of the dEclat algorithm are similar to the Eclat but the dEclat is more ef fi cient than the Eclat algorithm on different datastets. Zaki and other authors utilize the ideas of tidlist and dif fl ist representation of items in transactional database to propose new and ef fi cient algorithms for mining closed and maximal itemsets in static databases [10, 31].

Although the dEclat algorithm is designed for static databases, it can be adapted for data streams, since it is a single scan algorithm, which is the main requirement of all the data stream mining methods. In this paper, we show how the dEclat algorithm can be adapted for mining of frequent patterns in data streams. In the rest of the paper we call the adapted version, the EclatDS algorithm. In this algorithm for every item, a list of transactions in which the item has appeared, is maintained in a linked list. The tid for each transaction is generated by the algorithm. Since the number of transactions in the data stream is unbounded, we use a restricted range of generated numbers for transaction tid s. The generated numbers in this range are less than or equal to the window size. This prevents generation of big numbers for transaction tid s in transactional data stream processing. We start the tid s from 1 and increment it, up to the window size. By an incoming new transaction from a data stream, a new tid number is generated using the previous tid . For items contained in the new transaction, their tidlist s are extended by insertion of the new tid . In addition to tidlist s of items, another list called Tlist is maintained to store all tid sof the transactions within the window.
 De fi nition 1. Tlist is a list which stores all generated tid s corresponding to transactions belonging to the most recent window. Similar to tidlist sand dif fl ist s, Tlist is ordered according to the order of incoming transactions.
 Once the numberof transactions r eaches the window size, the window i nitialization phase is completed. After that, the window sliding phase starts. In this phase, transactions are removed from and inserted to the window, pane by pane. The pane size is a number which is less than the window size and it determines the unit of deletion and insertion for the window. Similar to the window size, the pane size is determined by the user. After a transaction with a generated tid number equal to the size of window is added, the tid number generation is reset to 1. By using this limited form of number generation, the generated tid s are always within the speci fi ed range i.e., from 1 to the window size. This prevents the generation of very large numbers for tid s which could exceed the variable storage size of the computer.
In data stream mining, we are faced with a limited amount of main memory due to unbounded and continuous arrival of data. Therefore, one of the main goals in the development of data stream mining algorithms is to reduce their memory requirements. In our window model, tidlist s of some items may become very long, thus requiring a large amount of memory. In order to reduce the amount of memory required by our sliding window model, we have developed the following mechanism. In this mechanism, when the length of an item X  X  tidlist becomes greater than half of the window size, the complement of the tidlist , i.e., dif fl ist is used for the item. The dif fl ist is obtained by complementing the original tidlist . During the data stream processing if this situation arises for dif fl ist ,again tidlist is computed by complementing the dif fl ist . In this way, for low frequency items in the window the tidlist is used, and for high frequency items the dif fl ist is maintained.
 De fi nition 2. List Complementation: Complementing a list involves tid s which are not contained in the list but belong to Tlist .Both tidlist and di f fl is t can be complemented. If a tidlist is complemented, a dif fl ist is obtained and vice versa. The process of getting a complement is performed using Tlist of the window. For example, complementing the tidlist [1,2,4] in a window having Tlist [1 X 4] generates the dif fl ist [3].

In the EclatDS , during a data stream processing, some item X  X  list are either in the form of tidlist or in the form of dif fl ist . The following two lemmas summarize window adjustments of the EclatDS algorithm to reduce the size of the lists.
 Lemma 1. For the items that appear in more than half of the transactions of the sliding window, it is more memory ef fi cient to store them in the dif fl ist format than the original tidlist format. Proof: If an item appears in more transactions than half of the window size, then the number of transactions which do not contain the item would be less than half of the window size. Therefore, the amount of memory than the original tidlist .
 Lemma 2. For items that appear in less than half of the transactions of the sliding window, it is more memory ef fi cient to store them in tidlist format than the dif fl ist format.
 Proof: If an item appears in fewer transactions than half of the window size, then the number of transactions which do not contain the item would be more than half of the window size. Therefore, the size of tidlist would be smaller than the size of the dif fl ist . Thus, storing the tidlist requires less amount of memory than the dif fl ist .

These lemmas show that storing all items in only the dif fl ist or tidlist formats is not ef fi cient. Thus the formats that items have to be stored are based on their occurrences within the window. Additionally, due to dynamic nature of data streams, these lists must be converted to their complement in the presence of the concept change. The concept change is the result of change in frequency of items. A low frequent item is represented using tidlist . Once the frequency of an item is increased, its tidlist is converted to dif fl ist by the algorithm. On the other hand, an item with high frequency is represented in the form of dif fl ist . Once its frequency is reduced, its dif fl ist is converted by the algorithm to tidlist . In the data streams which do not have very high concept drift, this process is effective. In contrast, if concept change is preferred. In our method, it is assumed that the concept change is not so dramatic and occasional changes in list types are acceptable.

Based on the above two lemmas, here we propose our new sliding window method. In this method, fi rst the window is initiated. The window initiation includes the insertion of the incoming transactions tid s into the Tlist . Therefore, the Tlist would contain tid s of the current transactions within the window. Moreover, when a new item appears within the window for the fi rst time, a new tidlist is formed to store the corresponding tid s of the incoming transactions in which the item appears in. During the window initialization phase all the stored item lists are in the original tidlist format. The second phase namely the window sliding phase starts after the window initialization. In this phase, by the arrival of new transactions, before a new pane of transactions is inserted, the oldest pane has to be deleted. Whenever a mining request is initiated by a user, the third phase namely the mining phase starts to extract frequent itemsets within the current window. Figures 3, 4 and 6, show algorithms of the pane insertion, the pane removal and the mining process respectively. The notations used in these algorithms are summarized in Table 1. The process of removing a pane from the current window is very simple. By using an important characteristic of the Tlist (i.e., its order), the pane removal process is performed. Similar to the tidlist and dif fl ist ,inthe Tlist ,the tids of transactions are placed in their arrival order. This characteristic ensures that the oldest pane is placed at the tail of the Tlist . Once the oldest pane is removed from the window, the tid s of the removed pane are also deleted from the existing tidlist sand dif fl ist s. Due to this deletion, if any tidlist becomes empty, it is removed from the existing lists. The pseudo code of the pane removal algorithm shown in Fig. 3 is started by determining tid s of oldest pane in line 1. In this process a number of tid s (equal to pane size) are removed from the tail of the Tlist and are stored in a temporary list called R . In lines 2 through 5 for every tid contained in R , it is deleted from dif fl ist sand tidlist s. In lines 6 and 7allempty tidlist s are removed. However, the empty dif fl ist are not deleted since they represent items occurring in all transactions of the window. It is important to note that the process of pane removal can be ef fi ciently performed by just looking at the tail of the tidlists and dif fl ists since the lists have the same order as the Tlist .

After the pane deletion, in the window sliding phase, the new pane of transactions is inserted to the window. For every item in an added transaction, if the corresponding list of the item is in the tidlist form, the tid of the added transaction is inserted to the tidlist . On the other hand, for every dif fl ist ,if its corresponding item does not appear in an added transaction, the tid of this transaction is inserted to the dif fl ist . For new items that appear for the fi rst time in an added transaction, a new tidlist is created and the tid of the transaction is inserted in the newly created tidlist . After inserting all transactions of a new pane, the window must be adjusted. Based on lemma 1 and 2 some dif fl ist sand tidlist smustbe complemented. For dif fl ist sand tidlist s which their length exceed | W | /2 they must be converted to the other format. Using the Tlist , the conversion is carried out by complementing the dif fl ist sand tidlist s.
Figure 4 shows the pane addition algorithm. This algorithm inserts a new pane of transactions. As shown in Fig. 4, for all transactions of the new pane ( D ), lines 2 through 9 are applied. In lines 2 and 3 based on the current tid , a new number is generated. In order to prevent generation of large numbers, if the current tid is equal to the window size, the numbering is reset to 1. The generated tid is inserted to the Tlist at line 4. In line 5, for all items included in a new transaction, if corresponding lists are in are not included in an inserted transaction, tid of transaction is inserted to dif fl ists . In this algorithm, ST and SD represent all tidlist sandall dif fl ist s, respectively. In line 7, if a new item appears in an inserted transaction for the fi rst time, a tidlist is created for this item and then the tid of the transaction is inserted to the newly created tidlist .

As show in lines 10 through 16, the window adjustm ent is performed after addition of a new pane of transactions. In line 10, for every tidlist or dif fl ist which its size is greater than | W | /2, it must be complemented using Tlist of the window. After the conversion, ST and SD are updated. In lines 12 through 14, if a list is converted from tidlist to dif fl ist format, it is removed from SD and inserted to ST accordingly. In lines 15 and 16, the inverse operation is carried out for lists converted from dif fl ist format to the tidlist format. According to lemmas 1 and 2 these conversions improve the memory usage. In our toy example, the process of window sliding can be tracked by comparing the W1 and W2 in Fig. 5.
Figure 5 shows the content of windows W 1 and W 2 of Fig. 1 in EclatDS method before and after the window adjustment. In Fig. 5, List ( x )andList(  X  x )represent tidlist and dif fl ist for an item x, respectively. In Fig. 5.a, the left side (i.e., Before column) shows the content of the window following the window initialization phase. The right side (i.e., After column) shows the content of window after the adjustment. As shown in Fig. 5.a, after window initialization phase, the fi rst window (W 1 ) is formed. Most of the tidlist sinW 1 are long (greater than half of the window size). For all items except e, tidlist s are converted to dif fl ist , thus, reducing the amount of memory usage. Figure 5.b shows the content of the window after a sliding process, i.e., transition from W 1 to W 2 . Once window slides forward, tidlist s are changed and the long list of b and e are complemented and converted to tidlist and dif fl ist form, respectively. This process is continuously performed after every window slide. Therefore at all times, each list is in its shortest form. It should be noted that, after window initialization, the number of lists which are adjusted can be large. However, in the window sliding phase only a few of these lists have to be adjusted. For example in Fig. 5.a four out of fi ve lists are adjusted whereas in Fig. 5.b only two lists are adjusted (i.e., List (  X  ) and List (e)). This characteristic resembles the concept change in real data streams, where the change from one window to another is not that great.

The mining phase starts when a frequent pattern mining request is initiated by a user. The mining is performed using the dEclat algorithm. As mentioned in [30] the dEclat can start with all the items in the dif fl ist sor tidlist s format. However it continues and works with dif fl ist s. Therefore, in our method, before a mining process is initiated, a copy of all the lists is made. In this copy all the lists are transformed into a uniform format. That is, in this copy if the number of dif fl ist s is greater than or equal to the number of Having all the lists in the same format, the dEclat algorithm extracts all the frequent patterns in the current window. Figure 6 depicts the algorithm of frequent pattern mining using the dEclat . In lines 1 and 2, if most of the items are represented by their dif fl ist s, the remaining items are also converted to their corresponding dif fl ist s. In lines 5 and 6 on the other hand, if most items are in the tidlist sformat, the dif fl ist s are converted to their corresponding tidlist s.

Although our above proposed method operates in the sliding window model, it can be adapted to work in the landmark model by neglecting the deletion procedure. Experimental evaluations in the next section show the effectiveness of the EclatDS . 5. Experimental evaluation
We have implemented the EclatDS and two recently proposed window sliding algorithms of DSTree and CPS-Tree . All programs were written in C++ and executed in Windows XP on a 2.66 GHz CPU with 1 GB memory. Both DSTree and CPS-Tree use pre fi x trees as described in Section 2 to store sliding window information. These two recently proposed algorithms are selected due to their ef fi cient run time and memory usage and also since these methods divide the window into a number of equal panes similar to EclatDS . Additionally, similar to EclatDS , both of them store and maintain the sliding window information and perform the mining when the user makes a request. Real life and synthetically generated sparse and dense datasets are used in the experiments to show the generality of the proposed algorithm. The datasets speci fi cations are summarized in Table 2. The fi rst 3 datasets of Table 2 are real and the last one is a synthetically generated dataset [2]. The datasets are read once by the algorithm, transaction by transaction, similar to reading from a stream.

Similar to CPS-Tree and DSTree, the fi rst active window is formed after window initialization. The other active windows are formed using their old windows by deleting an old pane and inserting a new pane of transactions. This process of sliding continues until the data stream generated by reading a t r ansactional dataset is fi nished. However, this is only a simulation and a real data stream is unbounded and could continue forever. In our experiments, fi rst the accuracy of our mining method is evaluated by comparing its mining result on a number of speci fi c windows with that of direct mining on the same set of transactions. This reveals that the EclatDS can accurately extract all frequent patterns from the each window on a data stream. Runtime and memory usage are the key features of every data stream mining algorithm since the incoming transactional data streams are arriving with very high speed. On the other hand, data is unbounded and we have limited amount of main memory. Here, time and memory usage are measured for all active windows and the results are averaged and reported. In order to show how the value of minimum support affects the runtime, for each runtime experiment a range of minimum support values are used. Similarly, the size of window has a direct effect on memory usage in sliding window based methods. This is due to the maintenance of window information in the main memory. Therefore, in our memory usage experiment, different values of window size in term of number of pane in each window are applied. In EclatDS method the window adjustment is performed after the current window slides forward by inserting a new pane of transactions. In the window adjustment, the size and type of some lists may be changed in order to adapt the model of window content to new concepts in data stream. Since the pane size and window size have direct impact on the content of windows, an experiment is performed by various pane sizes and window sizes to see their effect on window adjustment.

The fi rst experiment compares the run time of EclatDS , CPS-Tree and DSTree . In this experiment, the average runtime of all active windows are computed for the three algorithms on all datasets. After each window sliding, the mining is performed on the current window. For the BMS-POS, every window contains three panes for which the pane size is 50k transactions. In the Kosarak dataset, the pane size and window size are set to 50K transactions and 4 panes respectively. These values for the Connect-4 dataset are 10K and 2 panes. For T40I10D100K dataset, every window contains 4 panes for which the pane size is 10k transactions. Figure 7 shows the result of this experiment for different minimum support thresholds. In this Figure, for every chart, horizontal axis shows minimum support values and vertical axis shows the run time in second. In order to better compare the runtimes of the algorithms, the vertical axis is in logarithm scale. As shown in Fig. 7, the EclatDS is more ef fi cient with multiple others of magnitude with respect to DSTree and CPS-Tree algorithms for all of the datasets. As the minimum support decreased, the ef fi ciency of the EclatDS is more revealed and the performance gap becomes more signi fi cant. Figure 7 also shows that the EclatDS is very fast even for low support thresholds where the number of generated frequent itemsets is very high.

The ef fi ciency of the EclatDS is due to its ef fi cient sliding window mechanism. In the pre fi xtree based algorithms of DSTree and CPS-Tree , tree construction, tree restructuring (in the CPS-Tree ), tree update and old pane extraction from the tree, are time consuming tasks while in EclatDS these tasks are replaced by simple operations on tid s of transactions. Another reason for this ef fi ciency is due to use of dEclat which is enhanced by item ordering.

Runtime distribution of all algorithms includes pane insertion, pane deletion, window adjustment and the mining times. The pane insertion in DSTree and CPS-Tree is the time required to construct the pre fi x tree using the incoming transaction. The pane insertion time in the EclatDS is the time required to insert the incoming transactions tid s to lists of items contained in the window. The window adjustment of the CPS-Tree is the process of reconstructing the pre fi x tree to maintain support descending order of nodes in every path. The window adjustment time for the EclatDS is the time required for converting the long lists to short lists as described in the previous section. The DSTree does not perform any window adjustment. Finally the pane deletion time in the CPS-Tree and the DSTree , includes the time to extract the old pane from the pre fi x tree. The pane deletion time for the EclatDS includes the time to remove the tid sofanold pane from lists of items contained in the window. Runtime distributions of these algorithms are reported in Table 3 for two values of minimum support, i.e., one low and one high, in the performed experiment.
In all these cases, the EclatDS has signi fi cantly better total runtime. As shown in Table 3, in almost all cases, the EclatDS has also the least recorded time for pane insertion, pane deletion, mining and window adjustment. This shows that the supremacy of the EclatDS is the result of the ef fi ciency of all these steps. Pane additions and removals are very simple and ef fi cient. In EclatDS , window adjustment does not happen so much and it requires simple complementing of only a few lists. However, in CPS-Tree this task requires complex process of branch sorting or path adjusting in the pre fi x tree. Moreover, the EclatDS has ef fi cient mining time. In the DST r ee and CPS-Tree ,the FP-Growth algorithm is used to extract frequent patterns from the pre fi x tree. This method constructs a number of conditional trees proportion to the number of frequent itemsets. Instead, in the EclatDS , dEclat algorithm is used. The fi rst and one of the most time consuming step of the dEclat algorithm in static databases is the generation of the tidlist sor dif fl ist s from transactions stored in a disk. In our sliding window mechanism these lists are continuously maintained in the main memory. Therefore, they are ready to be used when a mining process is requested by a user. Moreover, in the dEclat algorithm the intermediate conditional databases are formed during the mining process. These conditional databases incorporate simple dif fl ist structures. Construction of these conditional databases using in memory tidlist sor dif fl ist sisveryef fi cient. Furthermore, items are reordered in these conditional databases in support ascending order. This technique is commonly used in Eclat based algorithms and leads to reduce the size of intermediate conditional databases which optimizes the mining process. Thus the dEclat results in a better runtime in sliding window with respect to the FP-Growth method used in DSTree and CPS-Tree.

The second experiment is about the memory usage. In this experiment, the memory usages of all algorithms are measured with respect to different window sizes on the above datasets. For BMS-POS and Connect-4, the pane size is set to 50K. For T40I10D100K and Kosarak, the pane size is set to 10K. Sizes of window for each dataset are varied by using different number of panes. Memory sizes for holding all active windows are measured and the averaged result is computed and reported. The results are shown in Fig. 8. In this fi gure, memory usage (in mega byte) is plotted with respect to each window size for all the algorithms. Window sizes can be computed by multiplying pane size and the number of panes existing in every window. As can been seen from Fig. 8, memory usage of the EclatDS is signi fi cantly lower than other methods. This is true for all datasets and almost all window sizes.
As can be inferred from Fig. 8 the amounts of memory enhancement with respect to other algorithms in every dataset are almost fi xed for different window sizes. This is due to the simple structure of tidlist sand moreover by using tidlist and dif fl ist formats in appropriate situations. Both the DSTree and CPS-Tree use the complex pre fi x tree data structure. This tree requires considerable memory for each node, header table, and pointers for children of every node and node links in different paths. The CPS-Tree uses the branch sorting and the path adjusting methods to enhance its memory usage. Moreover it uses the support descending order of every path to reduce the memory requirements. The support descending order in the pre fi x tree enhances memory consumption considerably. However as discussed in [12], this method of ordering is not the best way of tree construction and there probably are better ordering of nodes in the tree which lead to better memory usages. The DSTree does not use any method to enhance its memory usage during window sliding. In EclatDS , simple short linked lists for tidlist sand dif fl ist sareusedwhich require little amount of memory for every link, thus improving the memory usage. Additionally the memory is adjusted to shorten the long lists. Therefore, the EclatDS outperforms CPS-Tree and DSTree in term of memory usage. As can be seen in Fig. 8.d the only exception is about dataset of Connect-4 in large window sizes with respect to CPS-Tree . In the largest window size the CPS-Tree is slightly better than EclatDS . This is due to very high pre fi x sharing of CPS-Tree in this dense dataset.

The third experiment is about the impact of pane size and window size on the number of list changes which are performed to adjust the window. These list changes are performed to adapt the model of sliding window information based on changes that occur during the streaming data processing. In this experiment different sizes of pane and window are set for Connect-4 dataset. For every pane size, a different number of window sizes are applied and the average number of list changes with respect to window size is measured and reported in Table 4. The window size which has 5 panes of 15K transactions cannot be applied due to the size limitation of the dataset. As mentioned previously these changes are performed by EclatDS to reduce the size of lists of the current window. As shown in Table 4, there are no signi fi cant differences in the number of list changes for different pane sizes and window sizes. This is due to the fact that according to Lemma 1 and 2, window adjustment decision for every list is performed with respect to the window size. However as shown in Table 4 when the window becomes smaller in terms of the number of panes and pane size, the number of list changing are slightly increased. These slight differences are due to higher sensitivity of smaller windows to changes of the content during the data stream processing. Left upper corner cell of Table 4 has higher value of list changes for pane size 2K transactions and window size 2 panes, respectively. This setting has the smallest value of window size 4K transactions for the experiment. 6. Conclusion
In this paper, the EclatDS is proposed for frequent patterns mining in data streams. The new algorithm is implemented and experimentally evaluated on real-life and synthetically generated data streams with different window sizes and minimum support thresholds. The EclatDS mines the complete set of frequent patterns in recent sliding window remarkably faster than recently proposed algorithms. Moreover due to use of a simple data structure and ef fi cient maintenance of this structure, the proposed algorithm requires less memory with respect to other algorithms. In fact, the sliding window information is continuously monitored and adjusted after every window sliding process. By adjusting the content of window, the EclatDS resolves the concept change during data stream processing. Although, the EclatDS is a sliding window based method, it can be adapted to wor k in the landmark window model w ith little modi fi cation. References
