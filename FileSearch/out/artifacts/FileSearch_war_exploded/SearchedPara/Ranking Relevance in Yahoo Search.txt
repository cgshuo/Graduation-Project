 Search engines play a crucial role in our daily lives. Relevance is the core problem of a commercial search engine. It has attracted thousands of researchers from both academia and industry and has been studied for decades. Relevance in a modern search engine has gone far beyond text matching, and now involves tremendous challenges. The semantic gap between queries and URLs is the main barrier for improving base relevance. Clicks help provide hints to improve relevance, but unfortunately for most tail queries, the click information is too sparse, noisy, or missing entirely. For comprehensive relevance, the recency and location sensitivity of results is also critical.

In this paper, we give an overview of the solutions for relevance in the Yahoo search engine. We introduce three key techniques for base relevance  X  ranking functions, semantic matching features and query rewriting. We also describe solutions for recency sen-sitive relevance and location sensitive relevance. This work builds upon 20 years of existing efforts on Yahoo search, summarizes the most recent advances and provides a series of practical relevance solutions. The reported performance is based on Yahoo X  X  commer-cial search engine, where tens of billions of URLs are indexed and served by the ranking system.
 learning to rank; query rewriting; semantic matching; deep learning
The continuing growth of the web consistently expands the in-formation pool available to us and we must increasingly rely on search engines to find useful web documents. Indeed, search en-gines play an ever more crucial role in information consumption in our daily lives. Meanwhile, search engines are one of the most suc-cessful businesses in industry, where the sponsored search model is a predominant form of online advertising.

Ranking relevance has been the most critical problem since the birth of web search. As web content was explosively generated, modern search engines have been required to efficiently and effec-tively retrieve the relevant URLs from a prohibitively large corpus. This raises tremendous challenges for both industrial and academic researchers. Early works on search relevance concentrated on text matching between queries and web documents such as BM25 [28], probabilistic retrieval model [22], and vector space model [22]. Subsequently, user behavior demonstrated great potential for rel-evance improvement in the industrial setting, and user behavior modeling has been extensively explored for improving search rele-vance, e.g., click modeling [9, 20, 19, 31, 26].

The advancing state of the art for search engines presents new relevance challenges, which drives us to think beyond direct text matching and click modeling. First, the semantic gap between queries and web documents is a major barrier for improving base relevance [24]. Query language is usually different from document language. For instance, for the query  X  X ow much tesla X , relevant documents may contain the term  X  X rice X  rather than  X  X ow much X , so direct text matching methods do not work well. Second, search queries follow a long tail distribution and the majority of queries are tail queries that have very low frequency or are completely new to the search engine. As a result, user behavioral information (e.g., clicks) is often not available so click modeling is not applicable to these queries. Third, users tend to treat search engines as general Q&amp;A systems where they can locate useful information directly; thus an increasing number of queries are in the style of natural lan-guage, which presents obstacles to many aspects of search technol-ogy such as query understanding and semantic matching.

In addition to base relevance, comprehensive relevance includes both temporal and spatial dimensions. First, many queries are best served with up-to-date information, requiring search engines to take freshness into account. For example, for the the query  X  X afest cars X , users prefer information about the latest tests for recent car models. It would be a poor user experience if a search engine returned a page stating  X  X he safest car is the 1998 toyota celica X . Second, with the increasing use of mobile search, more queries are location sen-sitive, such as  X  X almart X  and  X  X estaurant X , requiring search engines to consider location information in relevance.

This paper focuses on sharing our experiences in tackling these relevance issues and introduces a series of practical techniques that have been successfully implemented and deployed to power the Ya-hoo search engine. We first revisit the state of the art ranking func-tions on both offline static evaluation data and the Yahoo search engine. We reveal the key factor leading to the performance dif-ference of ranking functions in these two scenarios, which moti-vates us to design an effective ranking function for a real web-scale search engine. Second, to bridge the semantic gap between queries and web documents, we take full advantage of users X  behavior data in various ways, and develop three types of semantic matching fea-tures to transfer click information from head queries to tail queries. Finally, we present a rewriting system that remarkably boosts rel-evance performance, especially for tail queries. The three compo-nents together significantly enhance the Yahoo search engine. In addition, we provide solutions for recency sensitive relevance and location sensitive relevance. The key contributions include,
The Yahoo search engine can retrieve the most relevant docu-ments from a corpus of billions within a fraction of a second. It achieves this by (1) parallelizing the work for one query across many servers; and (2) repeatedly taking the best candidate docu-ments from a cheaper ranking function and reranking them using a better one. The corpus is segmented into equal-sized shards which are served by index servers. Documents are assigned to a shard based upon the MD5 of their URL. Each query is processed by one complete set of shards, each of which returns its best candidate doc-uments. These are then merged and reranked to produce the final result set.

Within a shard, the first step is to find all documents that match the query. This is called recall . Typically, document sets for each query term are intersected to obtain documents containing all terms. These documents are then sorted by the first round , a lightweight function that scores each document. Here uniquing is then applied, enforcing simple diversity constraints such as a limit on the num-ber of documents from a single host or domain. In addition, query-dependent features are extracted from the top candidates, which are then ranked using a much more expensive second round function, also named Core Ranking Function [33].

To build a core ranking function, the model performance strongly depends on the distribution and the number of labeled samples in the training set. Obtaining labeled samples is very expensive and time-consuming, and active learning plays an important role in se-lecting informative and representative samples with limited budget as suggested by previous works [21].
There are several major groups of features in traditional search engines, which, when taken together, comprise thousands of fea-tures [6]. The ranking functions are built on top of these features. Web graph : This type of feature tries to determine the quality or the popularity of a document based on its connectivity in the web graph. A famous example is PageRank [25]. Other features in-clude distance or propagation of a score from known good or bad documents [12, 18]. Document statistics : These features com-pute some basic statistics of the document such as the number of words in various fields. Document classifier : Various classifiers are applied to the document, such as spam, adult, language, main topic, quality, type of page (e.g., navigational destination vs infor-mational). Query Features : which help in characterizing the query type: number of terms, frequency of the query and of its terms, click-through rate of the query. Text match : Basic text matching features are computed from different sections of the document (ti-tle, body, abstract, keywords) as well as from the anchor text and the URL. These features are then aggregated to form new compos-ite features. The match score can be as simple as a count or can be more complex such as BM25 [28]. There are also proximity fea-tures which try to quantify how far in the document are the query terms (the closer the better) [23]. Topical matching : This type of feature tries to go beyond similarity at the word level and compute similarity at the topic level. In the context of contextual advertising, details can be found in [2]. Click : These features try to incorpo-rate user feedback, most importantly the clicked results [1]. They are derived either from the search or the toolbar logs. For a given query and document, different click probabilities can be computed: probability of click, first click, last click, long dwell time click or only click. Time For time-sensitive queries, the freshness of a page is important. There are several features which measure the age of a document as well as the one of its inlinks and outlinks. More information on such features can be found in [7].
There are several ways to evaluate search results, including hu-man labeling (e.g., professional editor X  X  judgment) and user be-havioral metrics (e.g., click-through rate, query reformulation rate, dwell time). Usually, user behavioral metrics are complicated, since they may be affected by other factors, such as presentation (e.g., fonts, bold, size, abstract summary etc.) and result diversity. In this paper, we focus on base relevance, which is the most impor-tant aspect of search results. Good, relevant URLs are a necessary foundation for impacting behavioral metrics.

To assess base relevance, we leverage professional editors X  judg-ment. Editors manually judge each query-URL pair, assigning one of five grades: Perfect, Excellent, Good, Fair or Bad. We then use Discounted Cumulative Gain (DCG) as the metric to evaluate the search relevance performance. DCG has been widely used to as-sess relevance in the context of search engines [15]. For a ranked list of N documents, we use the following variation of DCG, where G i represents the weight assigned to the label of the doc-ument at position i . Higher degrees of relevance correspond to higher values of the weight. We use the symbol DCG to indi-cate the average of this value over a set of test queries in our ex-periments. DCG will be reported only when absolute relevance judgments are available. In the following sections, we will report DCG1, DCG3, DCG5 with N in {1,3,5}, respectively. For signifi-cance, we employ the Wilcoxon T-test to report p-value.

We sample 2,000 queries from a one year query log as our test queries and evaluate their search results in the Yahoo search engine by capturing the results (query-URL pairs) real time and asking editors to judge them. According to the popularity of the query, the evaluation query set is split into three parts: top, torso and tail. Top queries have very high impressions and, in terms of relevance, they are relatively easy, since lots of click information provides hints for ranking. For torso queries, the click information is very limited and sparse because torso queries occur only a few times in a year. Tail queries generally recur less than once a year, so click information is not available. Tail and torso queries are relatively hard queries for search relevance [8]. Lots of efforts on tail and torso queries have been made [32, 34, 14]. In this paper, we mainly focus on performance of torso and tail queries and introduce the practical solutions in Yahoo X  X  web search engine, where we have taken implementation and efficiency into consideration.
In a realistic scenario such as a commercial search engine, given a query, there is a seeming infinity of irrelevant documents which may dominate the relevant results. However, it is infeasible to use only negatives to train the model, and negative results in training data usually cannot cover all aspects of irrelevancy. As a result, at run time, the percentage of bad results is often higher than we expect. The percentage of bad results is also very important  X  when users see embarrassing results at top positions, they may abandon the current search engine and switch to its competitors.
Search can be treated as a binary problem. In our experiments, we observe that gradient boosting trees (GBDT) with logistic loss usually are able to reduce the bad URLs at top positions. It finds the decision boundary between relevant and irrelevant URLs for a given query. However, this binary classification cannot rank URLs perfectly. We here introduce a unified method for web search which is based on logistic loss and incorporates the Perfect, Excellent and Good information into the model though scaling the gradient for GBDT. Online evaluation of the Yahoo search engine shows that this framework decreases the percent of bad URLs by 40% and in-creases relevance by 5% in terms of DCG5, compared to the lead-ing ranking algorithms [33, 4].
We adopt GBDT (Gradient Boosting Decision Tree) [10] into the framework. Based on the gradient boosting framework, we first introduce logistic loss at the core ranking where we aim to reduce bad/fair URLs in top results. We first flat the labels  X  X erfect X ,  X  X x-cellent X  and  X  X ood X  to  X  X ositive X  (+1) and  X  X air X ,  X  X ad X  to  X  X eg-ative X  (-1). The log likelihood (loss) is Then the pseudo-response of stage m is Logistic loss targets on binary classification, which can decrease the number of bad URLs for top retrieved results. Logistic loss is not limited to classification, and it can provide reasonable ranking, comparing to other classification loss such as hinge loss since it al-ways places the force on positive/negative towards positive/negative infinite. For the samples far from the decision boundary (e.g.,  X  X er-fect X  samples are usually far apart from decision boundary), it is highly likely that the predicted values have larger absolute values (very positive/negative). However, only simple binary classifica-tion is insufficient for ranking. To bring the orders of  X  X erfect X ,  X  X xcellent X  and  X  X ood X  back to the model, we introduce gradient scale for GBDT. According to the gradient of logistic loss, given a Perfect/Excellent/Good sample, we know that the gradient is al-ways positive, while for Fair/Bad URLs, the gradient of logistic loss is always negative. To distinguish Perfect/Excellent/Good, we scale the gradient, (which is also known as the pseudo-response of stage i ), in different levels (e.g., 3 for Perfect, 2 for Excellent, and 1 for Good). As a result, Perfect data samples get relatively higher forces to positive infinite than Excellent ones, which are higher than the Good ones. The pseudo-response of stage m will become where scale( label ) can be empirically set scale( Perfect )=3 scale( Excellent )=2 and scale( Good/F air/Bad )=1 to dis-tinguish Perfect/Excellent/Good. This loss function takes advan-Table 1: Performance comparison of models using different learn-ing algorithms. * denotes p-value&lt;=0.01;  X  denotes p-value&lt;=0.05. tage of both gradient boosting and logistic loss. It not only is a binary classification but also has ability to distinguish the different positive samples. For Fair and Bad samples, since their gradients are always negative, we needn X  X  make any modification/scale for each pseudo-response. This method is actually equivalent to adding different weights on loss for different labels, but it is different from adding sample weights, which actually affect the tree growth of each stage m but do not change pseudo-response.

We name our learning algorithm LogisticRank and compare with the leading ranking algorithms: GBRank [33] and LambdaMart [4] in the Yahoo search engine. All three methods share the same train-ing data, which is collected through active learning and editor la-bels and includes about 2 million query-URL pairs. The parame-ters are carefully tuned on a validation query set. The results are shown in Table 1. Interestingly, the overall performance of Lo-gisticRank is the best and significantly better than GBRank and LambdaMart. Especially, on top queries, we obtain the largest im-provement through LogisticRank. GBRank, which mixes pairwise loss and pointwise loss, performs consistently better than Lamb-daMart, which is a listwise loss function and only learns relative orders. In tail queries, GBRank is able to generate similar perfor-mance (no significance) as LogisticRank. On the other aspect, the percentage of bad/embarrassing results is reduced by 40% through LogisticRank. We observe that the improvement of LogisticRank is mainly from bad results removal.

We tried to further understand the above results by performing experiments on an offline dataset X  X he learning to rank challenge data set (LTR) [6]. The results of LambdaMart and GBRank are consistent with [6], where LambdaMart is around 2% better than GBRank in terms of DCG5. However, we find that both Lamb-daMart and GBRank are better than LogisticRank on this task: LambdaMart is 2.6% better than LogisticRank while GBRank is 0.6% better than LogisticRank in terms of DCG5. The main differ-ence between LTR data and a real search engine is the data distri-bution. In static LTR data, there are 27%  X  X ad X  query-URL pairs, compared to &gt;99%  X  X ad X  query-URL pairs in the real search en-gine, since for a given query, there are usually only tens of relevant URLs among tens of billions of indexed URLs. The performance on the static LTR data set is not matched by real search engine per-formance due to this big difference in data distribution. Unfortu-nately, it is prohibitively expensive to generate a static data set with editorial labels which has the same distribution as the real search engine.

In the remaining sections of this paper, we use LogisticRank with the existing features described in Section 2 as our base model, which is referred to as  X  X ase X . Table 2: Performance of systems with and without contextual reranking. * denotes p-value&lt;=0.01;  X  denotes p-value&lt;=0.05.
The core ranking only considers query-URL pair features, while ignoring contextual information from other candidate results for the same query. There are practical reasons  X  the result set at this point is very large, and it is distributed over hundreds of machines, hence calculating even a very simple feature like the average popularity of all candidate documents would be prohibitively slow.

The reranking phase is applied after the results from the core-ranking phase have been sorted, aggregated on one machine, and trimmed down to the best tens of results, e.g., 30. That allows us to extract features capturing contextual information about the entire result set, which turn out to be the most important features in the model.

Based on the top tens of results (e.g., 30) of a given query, we extract following contextual features for specific existing features  X  (1) Rank : sorting URLs by the feature value in ascending order to get the ranks of specific URLs. (2) Mean : calculating the mean of the feature values of top 30 URLs. (3) Variance : calculating the variance of the feature values of top 30 URLs. (4) Normalized feature : normalizing the feature by using mean and standard devia-tion. (5) Topic model feature : aggregating the topical distributions of 30 URLs to create a query topic model vector, and calculating similarity with each individual result.

Beside contextual features, another advantage of reranking is that the training data for this phase is less prone to sample selection bias, since it is possible to label a complete set of candidate docu-ments for a given query. By contrast, for the core ranking phase, this is infeasible since the candidate set could consist of hundreds of thousands of documents. In practice, we found that it is more robust to use the ranks of the results as a feature instead of directly using the core-ranking function X  X  scores. Though the core-ranking phase score is more informative than the rank, its value can drift as the index refreshes, degrading third phase performance. Since the reranking function is running on top 30 results, its main pur-pose is to further distinguish relevant results ( X  X erfect X ,  X  X xcel-lent X ,  X  X ood X ) rather than to identify and remove bad results. The impact is shown in Table 2. We see that the overall performance is significantly and consistently improved by reranking.
The core ranking function is run on the index serving nodes. This allows it to run in parallel on billions of documents, and to have local access to posting lists and document properties for quick fea-ture generation. By contrast, the third phase ranking function must be run on a single blending node , after merging and sorting the top results from each index serving node, in order to calculate the contextual features. To support this, we have added the ability to export features from the index serving nodes. When processing the query, we augment it with the list of primitive features that are used to calculate contextual features. Each index serving node includes the requested features of its top-k documents in its response. This approach is feasible because the reranking phase also has access to the rank and ranking score of each document in core-ranking phase, so it typically requires only between 30 and 50 primitive features, keeping communication overhead to a minimum.
The features described in Section 2 have some failings. For tail queries, the user behavior features do not work well due to sparsity and noise. The documents relevant to tail queries are often lacking anchor text. Text matching features suffer from the vocabulary gap between queries and documents [24]. To overcome these issues, we introduce three novel features: click similarity, translated text matching and deep semantic matching, which are calculated from click logs in different ways and complement each other.
From the click log, we can extract a bipartite click graph to con-nect queries and documents by gross judgments from users. A vector propagation framework is developed on this graph to learn vector representations for both queries and documents in a shared space. Based on these vectors, relevance scores can be calculated. Traditional content-based models such as VSM [30] represent queries and documents in a vector space by merging their vocabularies. One weakness of such models is the mismatch between query vo-cabulary and document vocabulary [24]. To bridge this gap, the proposed model represents both queries and documents using terms in query vocabulary.

Edges in the bipartite click graph link queries and their corre-spondingly clicked documents. They are weighted by the number of clicks since more clicks could imply more confidence in poten-tial relevance. We extract terms from co-clicked queries to repre-sent documents. Terms that are from more co-clicked queries are likely to be more representative of documents. In such a space, the more queries two documents share, the  X  X loser X  they are. Sim-ilarly, queries that share many co-clicked documents are likely to share similar intent that should also be reflected by the query vec-tors. Thus we in turn propagate terms in document vectors to their co-clicked queries so that related queries can share high similarity in this vector space as desired.

With aforementioned assumptions, a vector propagation algo-rithm is proposed in this section. We first construct the bipartite click graph G with nodes V = D [ Q , where D and Q are the sets of documents and queries, respectively. For a document d 2 D and a query q 2 Q , an edge is formed between them if there are co-clicks between them. The weight of the edge is indicated by the number of co-clicks. We use E to denote the set of edges and represent the adjacency matrix of G . An example of the bipartite click graph is shown in Figure 1. QV n is a |Q|  X  V matrix where the i -th row QV n i is the query vector of q i at iteration the vocabulary size. Likewise, DV is a |D|  X  V matrix where each row is a document vector.

We initialize the representation of each query by its terms with weights proportional to their frequencies in the query. Each vector is normalized to 1. The initial matrix for the queries is n -th iteration, we compute document vectors DV n by aggregating their co-clicked queries. Formally, DV n j of d j is calculated as: where DV n j is normalized by ` 2 norm. Then the query vectors in q : cvs pharmacy
During the propagation process, queries are enriched with new and relevant terms. The propagation also re-learns the weights to indicate term importance that helps distinguish the informative terms, and is especially helpful for short queries. Document vectors are enriched with terms in queries and adjusted in a similar man-ner. In this way, query and document vectors mutually enhance each other towards a better representation that reveals more reli-able query-document relevance. More details about the proposed framework can be found in [16].

In practice, we use a sparse representation by only considering terms with non-zero weights. The set of non-zero terms could be still quite large. If we use their weights to rank the terms in a vector, we can observe  X  the first few terms have large weights and the rest are very small. We find that we can keep the top K terms with the highest weight for both query and document vectors in each iteration without losing much accuracy. This optimization is vital for very large click logs. In addition, we have noted that this simple trimming speeds up the convergence of the vectors. Both query and document vectors stay stable after a few iterations.

Evaluation Given the learned query and document vectors in the same space, we can simply compute the inner product as a click similarity feature between queries and documents, which is referred as CS and evaluated in a real search engine. As shown in Table 3, through CS feature, the overall performance obtains 2 . 87% ative DCG5 gain, comparing with the base model (LogisticRank with the existing features described in Sec 2). For all query bands (top, torso and tail), CS significantly and consistently improves the performance, especially on the torso and tail queries where and 5 . 14% DCG5 gain respectively. This improvement shows that this vector propagation algorithm successfully reduces the vocab-ulary gap between queries and documents, and also introduces re-lated terms to learn better representations for both queries and doc-uments. We also study the importance of features and the proposed CS feature is ranked as the top one feature among thousands.
Implementation and deployment We have created a pipeline that refreshes the query and document vectors monthly to include the latest click log data. This pipeline automatically constructs the click graph from the click log and generates the query and docu-ment vectors, which are further quantized to bytes to save space. The quantized document vectors are stored in the forward index on the index serving nodes, and the quantized query vectors are maintained in a table on a blending node. At run time, the similar-ity between the query and the document is computed as a ranking feature, which is used in the ranking models.
The click similarity feature, although very effective, cannot be directly calculated for queries and documents that have never been observed in the click log. Statistical machine translation X  X ranslating Table 3: Performance of models with and without new features. * denotes p-value&lt;=0.01;  X  denotes p-value&lt;=0.05.
 queries to documents or reverse X  X rovides an intuitive method to reduce the vocabulary gap, and it can be applied to all queries and documents. We harness the power of machine translation by intro-ducing a set of new features, named  X  X ranslated Text Matching X  (TTM).

The TTM features are constructed as shown in Figure 2. We first create a translation model, using clicked (query, document title) into k alternate queries { q 1 w ,...,q k w } , which are in document title space. Given a document d , we then compute a cosine similarity s between the i -th alternate query q i w and the document title as the similarity between q i w and d . The final similarity score between the original query q and the document d is computed as: S ( q, d )= F ( { s 1 ,s 2 ,...,s k } ) where F denotes a function to ag-gregate scores in { s 1 ,s 2 ,...,s k } e.g., max, average, median, etc.
In our experiments, we consider various strategies to construct the translation features and we choose k =10 written candidates. Due to space limits, we omit the details and only report the suc-cessful features: EXT_Q_TLM 1 is the maximum cosine similarity scores { t_qlm i } 10 i =1 between the rewritten queries and document titles, where the rewritten queries are obtained using a translation model and a language model based on document titles; AGG_Q_TLM is obtained by computing the cosine similarity between the rewrit-ten queries (of the original query) using a title language model and the set of the 10 rewritten titles (of the document) using a query language model.
Evaluation We combine the proposed features EXT_Q_TLM and AGG_Q_TLM with existing features described in Section 2 to train a ranking function and evaluate the test data by DCG scores. The results are reported in Table 3, which show that the proposed TTM features significantly and consistently improve the relevance performance, compared with the baseline (LogisticRank with the existing features described in Sec 2). We also evaluate the fea-ture importance, and the proposed two features EXT_Q_TLM 1 AGG_Q_TLM are ranked 7th and 10th respectively. These results show that the proposed translation features contain valuable dis-criminative information for the core relevance modeling task. Implementation and deployment The implementation of the TTM features is based on a caching strategy which is described in more depth in Section 5. For cache misses, we use a pruned version of the translation model to keep latency low.
Top queries are relatively easier in machine learning based rank-ing, since many text or click-through features can be used. It may face challenges for torso and tail queries since they occurred only few times in users X  historic log. The aforementioned CS and TTM features to some extent mitigate the problem: the CS feature smooths the click information and removes the noise of clicks, while the TTM features are the soft version of query rewriting and record the equivalence of n-grams. However, both methods stay on the word level. To further improve the ranking results, we need to deeply understand the semantic information and users X  intentions behind them. Deep learning techniques can help extract semantic and con-textual information from top queries, and then we can generalize such information for torso and tail queries.

The rareness of torso and tail queries are mainly caused by mis-spelling, synonyms, abbreviations, long or descriptive queries. Gen-erally speaking, tail queries tend to be substantially longer than top ones. For example, tail queries on average have four to five words. Longer queries are harder to be matched with the title or content of a web page. However, it might contain more semantic information, compared against shorter queries with one or two words.

We adopted a feed-forward neural network following the work of DSSM [13] as our ranking model. The structure of our network is illustrated in Figure 3. The queries Q and their candidate docu-ments D  X  are fed as inputs, and the embedded vectors V Q are used to calculate the likelihood function of a clicked document D + , given by the following softmax:
For the training purpose, we collected one year of users X  query logs of Yahoo X  X  desktop search (US market), from May 1st 2014 to May 1st 2015. We removed spamming queries using simple rules, e.g., whether the number of displayed results per page is larger than 100 . A query session is generated by aggregating logs which have the same key of  X  X rowser_cookie+time_stamp+view_id X . We then aggregated all query sessions based on the query text. We discarded all abandoned sessions, where users did not click on any of the displayed documents. After aggregation, we obtained more than 20 billion unique query-document pairs.

To generate a list of positive and negative documents for train-ing, we use a 10 -slot window and slide it down from the top ranked document. The document in the first slot of this window is a pos-itive D + , and the following ones are regarded as negative As a result, we generate a large training set of 3 billion query-10-documents samples. The input feature representation is crucial in this model. For documents, we include both document titles and domain names as inputs. As discussed in section 4.2, title is the most informative and representative content about the document, but in a deep understanding model, domain provides another im-portant signal for specific queries, e.g., wiki, weather, imdb, etc. To reduce the vocabulary size, we use 3-letter shingling of words as proposed in DSSM [13], and normalized bag-of-words vectors of dimension 30 , 000 are used as input features. The neural network is then trained on a GPU cluster. After training the deep neural network, at runtime, the top level neurons will generate the em-beddings of queries and URLs. Then the deep semantic matching (DSM) value is calculated by the inner product of the embeddings of queries and URLs.

Evaluation We evaluate the proposed DSM in our ranking frame-work on the Yahoo search engine. As shown in Table 3, DSM obtains 1 . 69% relative DCG5 gain. It shows significant DCG im-provement on all three query bands (top, torso and tail). We also study the importance of features and the proposed DSM ranked as the eighth feature among thousands of features.

Implementation and deployment Deploying the deep neural network model is an engineering challenge. Latent vector embed-dings of document titles and domain names are pre-computed via forward propagation and stored on index servers. In order to re-duce memory footprint, we have to utilize vector quantization to further compress the features represented as vectors of single pre-cision floats. At runtime, we only need to obtain the embedded vector V Q for the query on the fly. The dot product between and the embedded document vector V D is used as the deep learn-ing feature, which is then fed to the GBDT model to calculate the final relevance scores.
Online users play two roles in commercial search engines. Users are information creators that generate web documents and they are also information consumers that retrieve documents for their infor-mation needs. Hence web documents and queries often use differ-ent language styles and vocabularies; consequently search engines could be unable to retrieve documents matching queries even when queries can perfectly describe users X  information needs. For exam-ple, a user forms a query  X  how much tesla X , but web documents in search engines use the expression  X  X rice tesla X . In previous sec-tions, semantic matching features try to solve this issue in a soft way that is mainly focused on improving precision. However, in a commercial web-scale search engine, to boost efficiency, before feeding the query-URL pairs into the ranking function, some steps, e.g., filtering, preselection, are performed. For instance, typically only URLs which contain all query terms are allowed to pass the preselection phase. This results in a recall problem. Query rewrit-ing (QRW) is the task of altering a given query so that it will get better results and, more importantly, to help solve the recall prob-lem. Query rewriting has been proven to be effective in improving search performance [27, 11]. In this section, we will describe the techniques and deployment of QRW in the Yahoo search engine.
We treat query rewriting as a machine translation (MT) problem which translates from a source language of user queries S get language of web documents T . In other words, we use QRW to bridge the language gap between queries and documents for search engines. The proposed framework consists of two phases  X  (1) the learning phase that learns phrase-level translations from queries to documents; and (2) the decoding phase that generates candidates for a given query. Next we detail each phase.

The Learning Phase : The immediate challenge for the learn-ing phase is to obtain a large parallel training data with queries and the corresponding rewritten queries that improve relevance. It is difficult, if not impossible, to use human labeling for the training data because (a) a good translation model requires a prohibitively large bitext; and (b) editors are not very effective at choosing which query will improve relevance. To tackle this challenge, we make use of click graphs. A click graph is a bipartite weighted graph where queries and documents are nodes, edges indicate the co-click between queries and documents and weights are co-click numbers. Typically titles of documents are very informative and representa-tive while queries are usually very short. Titles are more similar to queries than body texts, which suggests that we consider titles of documents as rewritten queries. These observations motivate us to extract query-title pairs from click graphs. Since it is highly pos-sible for us to collect a click graph with millions of queries and titles, we can prepare a very large-scale parallel training data for the learning phase. With query-title pairs, we follow the common steps for a typical phrase-based machine translation framework to learn phrase-level translations  X  word alignment, phrase extraction and phrase scoring. Note that titles are usually longer than queries; hence we need to adjust the null-word alignment probability to rel-ative large values (say 0 . 9 ) to filter out noisy alignments.
The Decoding Phase : Given a query q , there could be many ways to segment it into phrases, and each phrase could then have many translations, leading to hundreds of candidates. The decoding phase should generate the most confident candidate q w . Typically each candidate q c is scored via a linear combination of m functions and the decoding is formally defined as: where h i ( q c ,q ) is the i -th feature function and i controls the con-tribution of h i ( q c ,q ) . i can be either manually tuned or learned through a defined loss function. We here adopt widely used fea-ture functions in traditional statistical machine translation systems including translation scores provided by the learning phase, lan-guage model of original queries, word penalty, phrase penalty and distortion. We also develop feature functions specific to the QRW problem that aim to improve search relevance performance. For each pair ( q c ,q ) , we can build three groups of feature functions:
Query feature functions: h 1 -number of words in q , h 2 -number of stop words in q , h 3 -language model score of the query query frequency of q , h 5 -average length of words in q ;
Rewrite query feature functions: h 6 -number of words in q h -number of stop words in q c , h 8 -language model score of the query rewrite q c , h 9 -query frequencies of the query rewrite average length of words in q c and
Pair feature functions: h 11 -Jaccard similarity of URLs shared by q and q c in the query-URL graph, h 12 -difference between the frequencies of the original query q and the rewrite candidate h 13 -word-level cosine similarity between q and q c , h 14 between the number of words between q and q c , h 15 -number of common words in q and q c , h 16 -difference of language model scores between q and q c , h 17 -difference between the number of stop words between q and q c , h 18 -difference between the average length of words in q and q c .

From our empirical study, we found that h 11 , h 12 , h 13 top three most influential feature functions. As mentioned earlier, it is difficult to prepare training data for QRW by human labeling. It is also challenging to assess the perfor-mance of QRW by comparing original queries and rewritten queries directly via editors. For example, it is hard for editors to reach an agreement about the quality of  X  X ow much tesla X  vs.  X  X rice tesla X  as queries. Intuitively a good QRW should lead to an improve-ment in search performance. Given the original query and rewritten query, we have two options for ranking strategies. Intuitively, we can replace the original query with the rewritten query. However, our empirical finding is that direct replacement is risky, since some-times some poor rewrites will hurt relevance significantly. Hence we evaluate the quality of QRW in a blending mode, which is able to overcome most poor rewrites: (1) Suppose that q and q w original and rewritten queries, respectively. We use q and trieve top-N documents from a search engine separately. Assume that O = { D i ,S i } N i =1 is the set of documents and their corre-sponding scores (which is from the ranking function described in Section 3) for q and R = { D w j ,S w j } N j =1 for q w ; (2) We join R  X  if a document D appears in both, i.e., ( D, S i ) and ( D, S max( S i ,S w j ) is the ranking score of D after joining; and (3) We rank documents in the union according to their ranking scores in a descending order and choose the Top-N documents as final results for the original query q .

The DCG improvement over the set of queries is demonstrated in Table 4. Note that QRW is the blending mode while QRW-RE is using the rewritten queries to replace original queries for rank-ing. Both QRW and QRW-RE are able to significantly improve relevance especially for tail queries, and over 5% DCG5 gain is obtained through QRW. Since rewritten queries may change the in-tent of original queries, the performance of QRW-RE is worse than QRW.

Implementation and deployment The main issue that needs to be handled before deploying statistical machine translation-based QRW is the latency impact of running the decoder. To minimize latency, we cache the best translation candidate for each query af-ter decoding it. Since the rewrites generated by a given model are constant, the cache entries only expire when a new model is de-ployed. In addition, we preload the cache with rewrites of the most frequent 100 million queries. For cache misses, there is still some latency impact, however, we are able to keep it within our limit by pruning the phrase table before deployment, limiting beam width in the decoder, and simply abandoning the rewrite in the few cases where it is too slow.
We here report on the performance of all of the proposed tech-niques when used together. Performance numbers are shown in Table 5.  X  X BRank X  is the GBRank loss function with the baseline features, essentially a snapshot of Yahoo Search in 2010 [6];  X  X ase X  is the LogisticRank with the baseline features;  X  X ase + feat X  is the LogisticRank with both baseline features and the three types of se-
Table 4: The relative DCG performance improvement for QRW. Table 5: Performance comparison of models with and without the proposed components.  X  X eat X  includes CS, TTM and DSM;  X  X ll X  includes feat, query rewriting and logistic rank. * denotes p-value&lt;=0.01;  X  denotes p-value&lt;=0.05. mantic matching features (CS, TTM and DSM); and  X  X ase + all X  combines all techniques, including proposed ranking functions, se-mantic matching features and query rewriting. All relative perfor-mance numbers are calculated with respect to  X  X ase X .

The combination of all new features  X  X ase+feat X  significantly and consistently outperforms the baseline  X  X ase X  on all the three DCG scores and all three query groups. The semantic matching features together also perform better than each individual semantic matching feature. This confirms that the three semantic matching features are complementary to each other. When we combine QRW models described in Section 5 with the new features in  X  X ase the results are further improved. Comparing with  X  X BRank X , the proposed framework improves the performance by 7% in terms of DCG5. For tail queries, it achieves 9% improvement in DCG5. As Table 6 shows, with the help of the proposed ranking function, new semantic features and the query writing, our search engine signifi-cantly improves the ranking result for query  X  X ow much tesla X .
So far, we have discussed only base relevance issues, the most fundamental aspect of a commercial search engine. In the follow-ing sections, we discuss the solutions for recency sensitive ranking and location sensitive ranking in Yahoo Search.
In this section, we discuss how to improve ranking quality for recency-sensitive queries, which covers a large portion of users X  daily search activities. For such queries, users need the search re-sults to be not only relevant, but also fresh. For example, for the query  X  X afest cars X , users prefer information about latest tests for recent car models; for  X  X uperbowl X , it is likely that users search for the last or next superbowl. If a search engine were to return a page that said  X  X he next superbowl will be in 1989 X  or  X  X he safest car is the 1988 toyota celica X , it could lead to poor user experience.
In order to jointly optimize the relevance and freshness of rank-ing results for recency-sensitive queries, we define a metric called recency-demoted relevance to integrate both relevance and recency scores. Specifically, for each document, we characterize its recency with a five-level label: X  X ery fresh (VF) X ,  X  X resh (F) X ,  X  X lightly out-dated (SO) X ,  X  X tale (S) X  and  X  X on-time-sensitive (NT) X , and adjust its relevance as follows, As the table shows, we keep or promote a document X  X  relevance score when it is a very fresh, fresh, or non-time-sensitive document, and demote its score when its slightly outdated or stale. Our goal is to optimize the DCG metric defined based on the relevance score after adjustment, i.e., recency-demoted DCG .

To build such a new ranker with distinct goal, the challenge lies in the cost of collecting training data. It is not possible to have as many recency labels as relevance labels, because the recency labels quickly go stale. We must somehow leverage both a large rele-vance dataset without recency labels and a small recency dataset for building the recency ranker.

We create the recency-sensitive ranker upon the base relevance ranker by learning an additive freshness component to adjust doc-ument score based on its freshness. The freshness component is trained based on the recency dataset. We use a time-sensitive clas-sifier to decide whether the component should be added, which prevents changing the score of non-recency queries and non-time-sensitive documents. The new recency ranker f ( x ) for query-URL feature vector x is formally defined as, where f rel ( x ) represents the base ranker trained from general rele-vance labels, r fresh ( x ) denotes the freshness component and notes the time-sensitivity classifier . r fresh ( x ) is added only when c shows that x is time-sensitive query-URL pair.

Based on the equation, the problem boils down to training time-sensitivity classifier c ts ( x ) and freshness component train c ts ( x ) , we use the recency dataset and transform the freshness labels into binary labels, i.e.,  X  X on-time-sensitive (NT) X  to negative and other labels to positive and train a binary classifier. To build r fresh ( x ) , we use f rel ( x ) as the base ranker, and add more trees to optimize the goal of recency-demoted relevance  X  the newly added trees become c ts ( x ) .

We compare the performance of recency ranker f ( x ) with base ranker f rel ( x ) on 500 recency-sensitive queries, which are also sam-pled and selected from the whole year query log. We compute three different metrics: 1) DCG to measure the result relevance; 2) DCR, which uses the same formula with DCG but replaces the relevance label with a freshness label, to measure result freshness; 3) RD-DCG (Recency-demoted DCG) to measure overall quality consid-ering both relevance and freshness.

The result in Table 7 shows consistent improvement by the re-cency ranker on relevance and freshness metrics for recency-sensitive queries, verifying the effectiveness of our proposed method. Table 8 compares the recency ranking example of a newsy query  X  X olly bobo X , which shows that our algorithm helps boost fresher results to the top. The deployment of recency ranking is relatively easy, since it shares the same framework with the core ranking function. Note that this recency ranker is only triggered when the query is Table 7: Performance comparison: recency ranker v.s. base ranker. classified as a recency-sensitive query by a run time recency clas-sifier.
Web search results typically are identical for all users. How-ever, for some queries, contextualized search results are more use-ful for users. In this section, we will discuss location sensitive rank-ing. For example, given the query  X  X estaurants X , users want to find the pages of restaurants which are near to their current locations. We refer to such queries that are closely related with locations as location-sensitive queries . Queries with specific location names (e.g.,  X  X estaurants Boston X ) are referred to as explicit local queries , and queries without locations but with location-sensitive intention (e.g.,  X  X estaurants X ) are referred to as implicit local queries .
To improve the ranking for location-sensitive queries, a straight-forward way is to treat the distance d ( QUERY , URL ) between the query and the URL as an extra feature in the learning-to-rank frame-work [3, 5]. However, typically the text-matching features [29] and the click-based features [17] dominate learning-to-rank mod-els, thus this distance feature is likely to be buried due to the its coverage. To overcome this problem, we propose a novel loca-tion boosting ranking model to improve the ranking for location-sensitive queries.

To compute the distance between queries and web pages, we first need to extract the locations from both sides. We parse the location from the explicit local queries and use users X  locations as the query locations for implicit local queries. The locations of web pages are extracted based on the query-URL click graph from search logs. Given a URL, we parse the locations from the queries that this URL is connected to, and use these locations to describe the URL. We keep all the locations as a location vector weighted by the clicks. In addition, we also parse the locations from URLs directly.
Once we have the locations for both queries and URLs, we can easily compute the distance d ( QUERY , URL ) , which is further nor-malized to the range of [0 , 1] , denoted as  X  d ( QUERY , URL ) that when d ( QUERY , URL )=0 ,  X  d ( QUERY , URL )=1 . The query-URL relevance is measured by the existing base ranking function denoted as f b ( x ) , where x 2 R n is the input feature vector. We as-sume f b ( x ) is already available (trained by learning-to-rank meth-ods [3, 5]). We then introduce the logistic function to combine the two conditions and define the ranking function f ( x ) for location-sensitive queries as follows: The idea is similar to recency ranking. Here, the logistic function controls the distance boosting based on the base relevance score f ( x )  X  if the URL is close to user and the content well matches the query, the boosting term (second term in Equation 7) is more positive, thus the total ranking score f ( x ) for this URL is larger and the ranking for this URL is boosted; however, if the URL is very close to the user but is not relevant to the query, or if the URL matches the query well but it is too far from the user, the boosting term is close to 0 , thus the ranking score is only decided by the base ranking function and the ranking for this URL is not boosted.
The parameters w ,  X  and are trained using pair-wise data by minimizing: where P = { ( p i ,p j ) | p i p j } is a set of preference URL pairs to the same query, obtained by human experts; p i p j denotes that p i is more preferred than p j . We solve the above optimization problem by a standard gradient descent approach.

Evaluation We compare the search relevance of our new ranking function f ( x ) and the base ranking function f b ( x ) on 500 location-sensitive queries, which are sampled from query logs. The URLs are judged by human experts. The new ranking function improves DCG5 compared to the base ranking function by +6.92% . We also conduct online experiments to observe how users interact with the new ranking function. We perform  X  X ucket tests X  in a certain period to compare the base ranking function and the new ranking function in the Yahoo search engine. We use click-through rate (CTR) as our user experience metric to compare the two functions. Higher CTR implies a better user experience. The bucket test result shows that our new ranking function improves CTR by +4.78% compared to the base ranking function. This result is consistent with the of-fline experimental results (DCG) and shows that the new ranking function outperforms the base ranking function due to the effec-tive location features. Table 9 compares the search results for a query  X  X vs X  from a user in  X  X unnyvale, CA X . While the baseline ranks URLs with no specific location context on top (Note that cvs is a chain business that has many locations), the proposed distance boosting function ranks local results relatively higher.
Implementation and deployment To implement this location boosting model at run time, we extract the URL locations offline and store them in the forward index. The model parameters and are learned offline and stored in a dictionary. At run time, the implementation is similar to the core ranking function intro-duced in Sec 3. The distance between queries and URLs and the location boosting scores are computed in parallel on distributed index serving nodes. Then the boosting scores are added to the base ranking function to get the final ranking score. For URLs that are not labeled with a location, the distance is set to be zero,  X  d ( QUERY , URL )=0 , thus only the base ranking function is used for ranking. Note that this location boosting module is only trig-gered when the query is classified as a location-sensitive query by a run time classifier.
In this paper, we introduce the comprehensive relevance solu-tions of Yahoo search. The proposed solutions are effective, prac-tical, and have been deployed and tested at scale in Yahoo X  X  com-mercial search engine.

The proposed solutions are not limited to web search relevance, but also can be leveraged for vertical search engines, e.g., shop-ping, news, local, etc. We hope this work is able to provide useful insights to the whole research community for both industry and academia.
