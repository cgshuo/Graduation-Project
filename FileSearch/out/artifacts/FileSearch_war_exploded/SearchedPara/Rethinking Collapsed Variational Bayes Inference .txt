 Issei Sato sato@r.dl.itc.u-tokyo.ac.jp The University of Tokyo Hiroshi Nakagawa n3@dl.itc.u-tokyo.ac.jp The University of Tokyo Latent Dirichlet allocation (LDA) ( Blei et al. , 2003 ) is a well-known probabilistic latent variable model. It is used to model the co-occurrence of words by using latent variables called topics where a document is rep-resented as a  X  X ag of words X  . It has a wide variety of applications in many fields. Originally, the variational Bayes (VB) inference was used for learning LDA. The collapsed variational Bayes (CVB) inference was devel-oped as an alternative deterministic inference for LDA ( Teh et al. , 2007 ). The CVB inference is a variational inference improved by marginalizing out parameters as in a collapsed Gibbs sampler ( Griffiths &amp; Steyvers , 2004 ). ( Sung et al. , 2008 ) generalized the CVB infer-ence for conjugate-exponential family models, called latent-space variational Bayes (LSVB) inference. Since the CVB inference requires intractable integrals, Teh et al. ( Teh et al. , 2007 ) used a second-order Taylor expansion to perform the integrals. Asuncion et al. ( Asuncion et al. , 2009 ; Asuncion , 2010 ) pro-posed another approximation that uses only the zero-order information, called the CVB0 inference. The CVB0 inference does not have the drawbacks that Inference Marginalization  X  -divergence VB NA  X  ! 0 CVB X  X  ! 0
CVB0 X
EP NA  X  = 1 other inferences do: VB contains digamma functions which are computationally expensive, while CVB re-quires the maintenance of variance counts. In contrast, the stochastic nature of the collapsed Gibbs sampler causes it to converge more slowly than the determin-istic algorithms. Asuncion et al. X  X  empirical results suggest that the CVB0 inference learns models that are as good as or better than those learned by the VB and CVB inferences and the collapsed Gibbs sam-pler in terms of perplexity. Furthermore, as shown in ( Asuncion , 2010 ), when the asymmetric Dirichlet pa-rameters are estimated over document-topic distribu-tion, the predictive performance of the CVB0 inference clearly outperforms that of the CVB inference. We have the question of why CVB0 outperformed CVB, even though the approximation of CVB is more accurate than that of CVB0. In this paper, we propose an interpretation of the CVB0 inference for LDA by using the  X  -divergence. Using the  X  -divergence helps clarify the properties of the CVB0 inference. We also experimentally show the performance of the subspecies of the CVB0 inference, which is derived with the  X  -divergence projection framework. Our analysis of the relationship between existing inference algorithms and  X  -divergence is summarized in Table 1 , the meaning of which is revealed in later sections.
 The remainder of this paper is organized as follows. Sections 3 and 4 review LDA and the CVB / CVB0 inference for LDA, respectively. Sections 5 and 6 ex-plain  X  -divergence and its local projection, respec-tively. The key sections 7 and 8 describe local  X  -divergence projection for LDA and its connection to the CVB0 inference. Section 9 introduces other local projections inspired by the CVB0 inference. Section 10 evaluates algorithms in terms of document model-ing. Section 11 concludes this paper. Suppose that we have N documents, V vocabularies, and T topics. w = f w d g N d =1 denotes a set of docu-ments and z = f z d g N d =1 is a set of assigned topics.  X  denotes the probability of topic t appearing in docu-ment d .  X  t;v denotes the probability of word v appear-ing in topic t . n d;t ( z ) denotes the number of observations of topic t in document d . n d denotes the total number of words in document d . n t;v ( w , z ) denotes the num-ber of observations of word v assigned to topic t and n t; ( z ) = note them by n d;t , n t;v and n t; . The superscription  X  n d, i  X  denotes the corresponding variables or counts with w d;i and z d;i excluded, e.g., w n d;i = w nf w d;i z tions of word v assigned to topic t leaving out z d;i . E [ x ] denotes the expectation of x and V [ x ] = E [ x 2 ] E [ x ] 2 the variance. Multi( ) denotes the multinomial distribution. Dir( ) denotes the Dirichlet distribution. The following generative process is assumed with LDA. First, document-topic distribution  X  d and topic-word distribution  X  k are generated by where  X  = (  X  1 , ,  X  T ) is a T -dimensional vector and  X  = (  X  1 , ,  X  V ) is a V -dimensional vector. For each document d , generate the i -th topic z d;i and Wallach et al. ( Wallach et al. , 2009 ) explored the ef-fects of choosing  X  and  X  in LDA. They found in Markov chain Monte Carlo (MCMC) simulations that using asymmetric  X  and symmetric  X  results in bet-ter predictive performance for held-out documents. Therefore, we use asymmetric  X  = (  X  1 , ,  X  T ) and symmetric  X  = (  X , ,  X  ).
 The assignment probability of topic t to the i -th word in document d given w n d;i , z n d;i ,  X  and  X  is / p ( w / p ( w / n This is used for the collapsed Gibbs sampler. ( Teh et al. , 2007 ) proposed the CVB inference to LDA inspired by the collapsed Gibbs sampler and showed that the CVB-LDA outperformed the VB-LDA in terms of perplexity. They only introduced a varia-tional posterior q ( z ) by marginalizing out  X  and  X  . The free energy of the CVB-LDA is given by F Thus, the updates for q ( z ) are obtained by taking derivatives of F CV B [ q ( z )] with respect to f q ( z d;i equating to zero: q ( z d;i = t ) / exp E [log p ( w / exp / exp This update equation for q ( z ) requires approxima-tions to compute intractable expectation. By us-ing the central limit theorem, the expectation should be closely approximated using Gaussian distributions with means and variances, e.g.,
Moreover, using the second order Taylor expansion, we can approximately calculate where the superscription X  n d, i  X  denotes subtracting q ( z d;i = t ) and q ( z d;i = t )(1 q ( z d;i = t )). ( Asuncion et al. , 2009 ) showed the usefulness of an ap-proximation using only zero-order information, called the CVB0 inference. The update using only zero-order information is given by We derive this CVB0 inference by using  X  -divergence, which enables us to reveal the relationship among other inference algorithms. This section reviews  X  -divergence. A readable intro-duction is provided in ( Minka , 2005 ).
 Let our task be to approximate a complex probabilistic proximate p ( x ) as q ( x ), which is a simple probabilis-tic distribution, such as fully factorized distribution, i.e., q ( x ) = q ( x ) is to minimize information divergence such as the Kullback-Leibler divergence:
KL [ p jj q ] = where p ( x ) and q ( x ) do not need to be normalized. By using the KL-divergence, the estimation of q ( x ) is defined by the KL-projection of p ( x ) onto a family of q ( x ) as follows:  X  -divergence is a generalization of the KL diver-gence ( Amari , 1985 ; Trottini &amp; Spezzaferri , 2002 ; Zhu &amp; Rohwer , 1995 ), indexed by  X  2 ( 1 , 1 ). The  X  parameter can be used in different ways by different authors. In this paper, we define  X  -divergence by the convention used in ( Minka , 2005 ): D [ p jj q ] = where p ( x ) and q ( x ) do not need to be normalized. If p = q ,  X  -divergence is zero. Some special cases are The case  X  = 0 . 5 is known as the Hellinger distance, and  X  = 2 is the  X  2 distance. Since  X  = 1 swaps the position of p and q of the  X  2 distance, we call the case  X  = 1  X  X he inverse  X  2 distance X , which is the key divergence in this paper. In this section, we introduce a local divergence projection-based inference.
 Suppose that the approximate distribution q ( x ) is fully factorized. We derive the update q ( x i ) mini-mizing  X  -divergence as follows. Taking derivatives of  X  -divergence ( 12 ) with respect to q ( x i ) and equating them to zero, we obtain the following fixed point iter-ation equations: In many cases, this update is intractable and thus we introduce an approximation for Eq. ( 18 ).
 Since Eq. ( 18 ) is we replace p ( x n i ) with q ( x n i ), obtaining In the case  X  = 1, the update ( 20 ) is similar to belief propagation, and the factorized neighbors algorithm ( Rosen-Zvi et al. , 2005 ).
 The update ( 20 ) means that it locally minimize  X  -divergence, i.e., for each i , In the case  X  = 1, i.e., KL divergence, this local projection-based inference is equal to the EP algo-rithm. We describe the connection of this  X  -divergence projection with the CVB0 inference in the next sec-tion. In this section, we derive the CVB0 inference by using the local  X  -divergence projection. First, we describe how the case  X  = 1, i.e. EP, cannot be applied for the collapsed LDA. Second, we derive a divergence projec-tion applicable to the collapsed LDA and explain the relationship between this projection and the CVB0 in-ference.
 We apply Eq. ( 21 ) with  X  = 1(EP) to the collapsed LDA. For each z d;i , we perform q ( z d;i ) = argmin The update for q ( z d;i ) is q ( z d;i = t ) / E , The problem is that we cannot analytically execute this expectation. ( Asuncion , 2010 ) derived Eq.( 23 ) in a different way where he changed the CVB free en-ergy by moving the logarithm out of the expectations, and pointed out the relationship between Eq.( 23 ) and the CVB0 inference, which inspired this work. How-ever, the intractable expectation in Eq.( 23 ) was not executed. This intractability makes interpreting the CVB0 inference difficult.
 Here, we derive another approach by using the  X  -divergence projection. The key idea is to construct q ( z d;i ) by using the novel three parameters . We define q ( z d;i ) as follows: where we do not assume that  X  n n d;i d;i ,  X  n n d;i t;v expected counts, i.e., these are parameters of q ( z d;i ). We also define Since our definition of  X  -divergence does not require normalization of the probabilistic distribution, we can introduce the following local projection: optimization (see Appendix A ), we obtain minimizing the  X  -divergence:
Thus, we have When we use  X  -divergence projection with  X  = 1 for estimating a ( z d;i ) and b ( z d;i ), we have a b When we use  X  -divergence projection with  X  = 1 for estimating c ( z d;i ), we have Therefore, we have the following update for q ( z d;i ) Although the updates are performed in order, i.e., up-date a given b and c , b given a and c , and c given a and b , this update is equal to the CVB0 update in Eq.( 9 ). In this section, we explain why the CVB0 inference outperforms the CVB inference. To sum up this dis-cussion, in the CVB0 inference, the  X  X ero-forcing ef-fect X  works only with the n t; estimation, while in the CVB inference it works with the q ( z ) estimation. The previous section showed that the CVB0 inference is composed of the three projections with a mixture of  X  = 1 and  X  = 1: This projection-based update with a different diver-gence measure reveals the properties of the CVB0 in-ference. Ideally, we use the (  X  = 1)-divergence pro-E use the inverse  X  2 divergence D 1 [ p jj q ] for estimating D 1 [ p jj q ] = 1 2 forcing divergence ( Minka , 2005 ) which emphasizes q to be small when p being small, i.e., p ( x ) = 0 forces q ( x ) = 0, which means that it avoids  X  X alse positive X . In our case ( 43 ), the zero-forcing effect on the n t; es-timation means that the emphasis in the estimation is on high-frequency topics or low-frequency topics tend to be estimated as zero in an entire corpus. We think that affecting n n d;i t; matters much less than affecting n explain the zero-forcing effect of CVB0 in more detail in the next section.
 Returning to Eq.( 20 ), i.e., q ( x i ) / E we describes the relationship between the CVB infer-ence and  X  -divergence projection. First, we introduce the following theorem: Theorem 1 (Liapunov's inequality) If x is a non-negative random variable, and we have two real num-bers  X  2 &gt;  X  1 , then and Using Eq.( 20 ) and Theorem 1 , we obtain q ( x i ) / lim This is the variational inference minimizing KL [ q jj p ]. In LDA, we have q ( z d;i = t ) / lim = exp( E [log p ( z d;i j w d;i = v, w n d;i , z n d;i )]) / exp E / exp( E [log( n n d;i The update Eq.( 47 ) is the same update as the CVB inference in Eq.( 5 ). (  X  ! 0)-divergence is also known to induce the zero-forcing effect.
 In this section, we consider other projection-based al-gorithms that help clarify the property of the zero-forcing effect in CVB0. 9.1. CVB with (  X  = 1) -divergence From our view point, the CVB0 inference is com-posed of two different-type divergence projections:  X  = 1 , 1. We consider using only  X  = 1 for the projections. To do this, we have to calculate the ex-pectation given by Since we cannot derive the analytical solution for this expectation, we propose two approximation methods. The first is a stochastic approximation called sample averaging given by where S denotes the number of samples and z ( s ) is the s -th samples generated from q ( z ). This method is accurate but not practical when S takes a large value. We use this approximation to investigate the accuracy of the next approximation.
 The second is a deterministic approximation that uses the same approximation of CVB with the second-order Taylor expansion and Gaussian approximation given by  X  c As shown in the experiments (Sec. 10 ), we find that the second term of Eq.( 50 ) is vanish-ingly small.  X  c ( =1) in Eq.( 50 ) is calculated denotes the number of all words (tokens). For example, the variance takes the largest value when q ( z d;i = t ) = 1 / 2 for all d and i . In this case, E [ n t ] = n/ 2 and V [ n t ] = n (1 1 / 2) / 2 = n/ 4. Therefore, we consider c ( = 1) is similar to c ( =1) , which means that CVB0 is rarely affected by the zero-forcing effect. 9.2. Type-base CVB0 Inference We derive a type-based inference as an application of our framework. In a type-based inference, we only estimate the probabilistic distribution for each type in a document not each token; this is beneficial for computation cost and memory usage.
 We exclude all counts of word v from document d , denoted by superscription  X  n d, v  X . The appearance probability of word v given w n d;v d and z n d;v d is Moreover, we have Here, we consider obtaining an approximation dis-tribution q ( z d;v ). Instead of z d;i , we define q ( z ) factorized by using q ( z d;v ), i.e., q ( z d;i  X  v =1 q ( z d;v )  X  ( w d;i = v ) and q ( z ) = The update of q ( z d;v ) is obtained by q ( z d;v = t ) / E which is derived by minimizing the  X  -divergence as in Using the local  X  -divergence projection with  X  = 1 for n have We call this update the type-based CVB0 (TCVB0) inference. We compared CVB0 with its subspecies on document modeling in terms of perplexity to investigate the effect of  X  = 1. All results are averaged values from five experimental runs with random initialization. We set the number of iterations to 100 for each inference. We use a fixed point equation for updating  X  intro-duced in ( Minka , 2000 ). We set  X  = 0 . 01 because ( Asuncion et al. , 2009 ) showed that CVB0-LDA with  X  = 0 . 01 worked well when compared with other set-2004 )).
 In this section X  X  figures,  X  X VB X  indicates the second order approximation of the CVB inference.  X  X VB1s X  indicates the stochastic approximation in Eq.( 49 ) with S = 50.  X  X VB1d X  indicates the deter-ministic approximation in Eq.( 50 ).
 We used four sets of text data with different proper-ties. The first was  X  X IPS corpus (NIPS) X  from which the number of documents was N = 1 , 500 and the vo-cabulary size was V = 12 , 245. The second was  X  X he Wall Street Journal (WSJ) X  from which we randomly chose N = 5 , 000 ( V = 38 , 272) documents. The third was  X  X nron email corpus (Enron) X  from which we ran-domly chose N = 5 , 000 ( V = 14 , 758) documents. The fourth was  X 20 news group corpus (20ng) X  from which we randomly chose N = 5 , 000 ( V = 13 , 176). Stop words were eliminated.
 The comparison metric we used for document model-ing was the perplexity used by ( Teh et al. , 2007 ; 2008 ) that indicates the prediction performance for held-out words. We randomly split the words in a document into training words w train d (80%) and test words w test (20%).
 Figure 1 shows the experimental results. The bar graph indicates the results for test set perplexity in-terms of ( T = 40 , 80 , 120) in each corpus. CVB0, CVB1s, CVB1d and TCVB0 outperformed CVB in terms of perplexity. Although we compared VB with others, we eliminated the VB results to clarify the dif-ferences of inference algorithms because CVB outper-formed VB and the VB results change the scale of a bar-graph in some corpora.
 The performances of CVB1s and CVB1d were similar to that of CVB0. Since the results of CVB1d were similar to those of CVB1s, the approximation used in CVB1d seemed to be accurate. When we analyzed pus when T = 120 was about 3 . 17 e 4 , which is neg-ligible compared with 1. Therefore, as discussed in Sec. 9.1 , CVB0 was not affected by the zero-forcing ef-fect. We believe this is the reason CVB0 worked better than CVB. Moreover, the performance of TCVB0 was similar to that of CVB0. Consequently, the TCVB0 inference was practical. In this paper, we reviewed existing inference algo-rithms of LDA in terms of the  X  -divergence projec-tion. We showed that the CVB0 inference is com-posed of (  X  = 1 , 1)-divergence projections and that  X  = 1 is similar to  X  = 1 in LDA, which means that CVB0 is not affected by the zero-forcing effect in LDA. Combining the marginalization of parameters and the heterogeneous  X  -divergence projection is use-ful because it is easy to apply to other topic models learned by the collapsed Gibbs sampler. Future work is to develop an online-update extension, such as that by ( Hoffman et al. , 2010 ; Sato et al. , 2010 ; Wang et al. , 2011 ). From the relationship between EP and assumed density filtering, we can extend the local  X  -divergence projection into an online algorithm, which leads to the online CVB0 inference. A convergence analysis is also important remaining work.
 Taking derivatives of with respect to a ( z d;i ) and equating them to zero, and we obtain the following fixed point iteration equa-tions: a ( z d;i ) = Since
