 1. Introduction
Semantic similarity between concepts is becoming a common problem for many applications of Computational Linguis-ment of concept similarity improves the understanding of textual resources and increases the accuracy of knowledge based stood as the degree of taxonomic proximity between concepts (or terms, words). In other words, semantic similarity states how taxonomically near two concepts (or terms, words) are, because they share some aspects of their meaning. Technically, terms, words) has been and continues to be widely studied, and is a central and common issue in many research areas such  X  speaking, semantic similarity measurement relates to computing the similarity between concepts (or words, terms, short text expressions), having the same meaning or related information, but which are not lexicographically similar ( Li, very active and many results have been achieved ( Martinez-Gil, 2014 ).

Making judgments about the semantic similarity of different concepts is a routine yet deceptively complex task. To perform it, people draw on an immense amount of background knowledge about the concepts. Any attempt to compute semantic similarity automatically must also consult external sources of knowledge. Most of the work dealing with semantic similarity measures has been developed using taxonomies and more general ontologies, which provide a formal and machine-readable way to express a shared conceptualization by means of a unified terminology and semantic inter-relations
Blettner, 1989 ); (2) information content measures: which consists of measuring the difference of the information content of the two concepts (or terms) as a function of their probability of occurrence in a text corpus (or an ontology)
Egenhofer, 2003; Sanchez et al., 2012 ); (4) hybrid measures: which consists of combining all of the above ( Batet et al., 2013; Pirro, 2009; Schickel-Zuber &amp; Faltings, 2007 ).

In a nutshell, edge counting measures base the similarity assessment on the number of taxonomical links of the minimum counting measures is their simplicity. They only rely on the graph model of an input ontology whose evaluation requires a low computational cost. Due to their simplicity, these approaches offer a limited accuracy due to ontologies model a large amount of taxonomical knowledge that is not considered during the evaluation of the minimum path ( Batet, Sanchez, &amp; graph may be finely classified and others only coarsely defined ( Mathur &amp; Dinakarpandian, 2012 ).
To acknowledge some of the limitations of edge counting measures, Resnik (1995) proposes to complement the taxono-mical structure of an ontology with the information distribution of concepts evaluated in input corpora ( Sanchez et al., 2012 ). Information Content (IC) based approaches assess the similarity between concepts as a function of the information content that both concepts have in common in a given ontology. In the past, IC was typically computed from concept dis-on corpora availability and manual tagging that hampered their accuracy and applicability due to data sparseness ( Sanchez
Isern, 2011 ). However, the fact that intrinsic IC-based measures only rely on ontological knowledge is also a drawback
To overcome the limitations of edge counting measures regarding the fact that taxonomical links in an ontology do not necessary represent uniform distances, feature based measures are addressed by considering the degree of overlapping the contrary to edge counting measures which are based on the notion of minimum path distance, feature based approaches estimate similarity between concepts according to the weighted sum of the amount of common and non-common features.
By features, authors usually consider taxonomic and non-taxonomic information modeled in an ontology, in addition to con-1977 ). Due to the additional semantic evidences considered during the assessment, they potentially improve edge counting approaches ( Sanchez &amp; Batet, 2013 ).

There are still some limitations in the above-mentioned ontology based similarity measures ( Batet et al., 2011; Batet mation is based on the extraction of semantic evidence from one or several knowledge sources. The more available the back-ground knowledge is and the better its structure is, the more accurate the estimation will potentially be. Usually, these knowledge sources can be well-defined semantic networks such as WordNet ( Ahsaee, Naghibzadeh, &amp; Naeini, 2014; Fellbaum, 1998; Liu et al., 2012 ) or more domain-dependant ontologies such as Gene Ontology ( Couto et al., 2007; Mathur &amp; Dinakarpandian, 2012 ) and biomedical ontologies MeSH or SNOMED CT ( Batet et al., 2013; Pedersen, by a panel experts in the given domains. Clearly, the construction process of domain ontologies is time-consuming and similarity measures are limited in scope and scalability. On the other hand, while WordNet (or domain ontology) represents
Hooper, 2012 ), a lot of (sets of) concepts or terms (proper nouns, brands, acronyms, new words, conversational words, technical terms and so on) are not included in WordNet and domain ontologies (in fact Web users can publish whatever they want to share with the rest of the world by using Wikis, Blogs and online communities at present), therefore, similarity measures that are based on these kinds of knowledge resources (i.e., WordNet or domain ontologies) cannot be used in these tasks. These limitations are the motivation behind the new techniques presented in this paper which infer semantic similarity from a kind of new source of information, i.e., a wide coverage online encyclopedia, namely Wikipedia ( Hovy,
Gabrilovich and Markovitch (2007, 2009), Zesch, Muller, and Gurevych (2008), Taieb, Aouicha, Tmar, and Hamadou (2012), and Yazdani and Popescu-Belis (2013) have worked on the problem of semantic relatedness measures for concepts two separate notions. Semantic relatedness is a more general notion of the relatedness of concepts, while similarity is a (or Wikipedia concepts) are not considered, so in the following we will use the term of semantic similarity.
The purpose of this paper is to present several new feature based similarity measures to solve the shortcomings of exist-ing approaches for semantic similarity. The present paper focuses on the study of semantic similarity between concepts (or the problem of feature based semantic similarity between Wikipedia concepts from a novel perspective by making use of
Wikipedia. Thus, the terminologies of Wikipedia concepts, concepts, and words can be used interchangeably. The author pre-fers to use concepts but some readers may interpret them to be terms or words instead. Since Wikipedia is a rich encyclo-pedia (or corpus, thesaurus, network structure) that covers almost all imaginable sources, thus, the method to similarity measures based on Wikipedia presented in this paper can process lots of terms (i.e., proper nouns, brands, acronyms, new words, conversational words, technical terms and so on) that Web users publish by social networks such as Wikis and be used in dynamic domains.

The remainder of this paper is organized as follows. In the next section, we briefly review some background on feature based similarity and Wikipedia. Section 3 presents some feature based approaches to similarity assessment using Wikipedia our approaches. Finally, in Section 5 , we draw our conclusion and present some perspectives for future research. 2. Background
For completeness of presentation and convenience of subsequent discussions, in the current section we will briefly recall
Ponzetto &amp; Strube, 2007; Rodriguez &amp; Egenhofer, 2003; Sanchez et al., 2012 ) for further details. 2.1. Feature based similarity
Feature based approaches to similarity measures assess similarity between concepts as a function of their properties. This and non-common features of compared terms, subtracting the latter from the former ones. In fact, common features tend to that yields the set of features relevant to c , Tversky proposed the following similarity function: larity is not symmetric, that is, sim tve ( a , b )  X  sim depending on the way the comparison experiment has been laid out ( Pirro, 2009 ).

The definition of the set of features is crucial in this model. Goodman (1972) argues that assessing similarity between es, containing textual descriptions of word senses) and different kinds of semantic relationships are considered ( Sanchez et al., 2012 ).

Rodriguez and Egenhofer (2003) present a kind of approach to computing semantic similarity. The similarity is computed linked via semantic pointers) of evaluated terms: ponent, which depend on the characteristics of the ontologies and S represents the overlapping between the different features, computed as follows: degree of granularity upon which the ontology is designed.

A feature based function called X -similarity relies on the matching between synsets and term description sets ( Petrakis et al., 2006 ). The term description sets contain words extracted by parsing term definitions ( X  X  X losses X  X  in WordNet larity function is expressed as follows:
The similarity for the semantic neighbors S neighborhoods where i denotes relationship type. Because not all terms in the neighborhood of a term are connected with the same rela-tionship, so each different semantic relation type (i.e., Is-A and Part-Of for WordNet and only Is-A for MeSH) is computed separately and the maximum (considering all the synsets of all concepts up to the root of each hierarchy) is taken.
Equivalently, the similarity for description sets S descriptions where A and B denote the set of synsets or term description sets for the term a and b . 2.2. Wikipedia The structure of Wikipedia is as follows ( Medelyan et al., 2009; Ponzetto &amp; Strube, 2007 ). conventional thesaurus.
 point alternative expressions for an entity to the same article, and accordingly models synonymy.
Disambiguation pages : Instead of taking readers to an article named by the term, the Wikipedia search engine some-times takes them directly to disambiguation page where they can click on the meaning they want. These pages collect links for a number of possible entities the original query could be pointed to. This models homonymy.
Hyperlinks : Articles are peppered with hyperlinks to other articles. Because the terms used as anchors are often couched in different words, Wikipedia X  X  hyperlinks are also useful as an additional source of synonyms not captured by redirects.
Hyperlinks also complement disambiguation pages by encoding polysemy. In particular, articles mentioning other encyclo-pedic entries point to them through internal hyperlinks. This models article cross-reference.
Category structure : Since May 2004 Wikipedia provides also a semantic network by means of its categories: articles can  X  X  X ree X  X  is not designed as a strict hierarchy, but allows multiple categorization schemes to coexist simultaneously. The to create cycles in the graph (which nevertheless should be avoided according to the Wikipedia categorization guidelines, tain, with a minimum of explanatory text. 3. Feature-based similarity using Wikipedia In this section we propose some feature based approaches to semantic similarity measures of concepts using Wikipedia. ly, we investigate several feature based approaches to semantic similarity measures resulting from instantiations of the framework. 3.1. Formal representation of Wikipedia concepts representation of the information drawn by one Wikipedia article). A concept is compound by an identifier name or title, of examples.
 Formally, the Wikipedia concept Artificial Intelligence can be represented as shown in Fig. 1 . Now we propose the definition of formal representation of Wikipedia concepts.
 defined as follows: is the set of categories of A .
 obtain the value of Synonyms . Fig. 3 shows part of the value of Synonyms for Artificial Intelligence . According Definition 1 and Fig. 2 , we have the following:
It should be noted that not all anchors (labels of internal hyperlinks) in Wikipedia articles are in the set Anchors . For
Anchors , because they do not represent Wikipedia concepts here. 3.2. A framework for feature-based similarity
Wikipedia concepts can be looked as the features of Wikipedia concepts. Thus, we may give some methods of semantic simi-larity measures for Wikipedia concepts by exploiting the above mentioned four kinds of features. For each feature, we have different notions for semantic similarity measures. For example, for the feature Glosses , we can use Gloss overlap method 2013 ) to measure their similarity. Of course, we also can define some novel approaches to similarity measures for Glosses
Clearly, to present these feature based approaches to similarity measures, we need a framework for these approaches firstly.

Definition 2. Let Con 1 = h Synonyms 1 , Glosses 1 , Anchors two Wikipedia concepts. The similarity of Con 1 and Con 2 WikiCon WikiCon ? [0,1], and is defined as follows: where (1) S synonyms ( Synonyms 1 , Synonyms 2 ), S glosses ( Glosses
Form Definition 2 we know that we have to give the definitions of the functions S
S concepts in order to measure the similarity of Wikipedia concepts (i.e., give the value of the function Sim
Wikipedia concepts). Concretely, given two Wikipedia concepts Con
Con 2 = h Synonyms 2 , Glosses 2 , Anchors 2 , Categories { Con 2 , Con 21 , ... , Con 2 i , ... , Con 2 m 0 }, Anchors
Categories 1 ={ Cat 11 , ... , Cat 1 i , ... , Cat 1 k }, and Categories
S
Con 2 m 0 }, Glosses 1 and Glosses 2 ,{ Anc 11 , ... , Anc and { Cat 21 , ... , Cat 2 i , ... , Cat 2 k 0 }, respectively.

Let us see a simple example. From Wikipedia we have the following where Synonyms 0 = {Machine learning, Machine learning algorithm, Learning algorithms, Statistical learning, ... }, Gloss-computer vision, Machine learning, Learning, Cybernetics}.
 similarity measures for S synonyms ( Synonyms , Synonyms 0 ), S Machine learning , Learning , Cybernetics } to define the similarity of Categories and Categories 0 . Remark 1. Definition 2 is a flexible similarity assessment framework for Wikipedia concepts. If we add a feature such as es , Anchors , Categories , Seealso i , it is easy to extend the framework as follows: games ), Never-Ending Language Learning , ... , Our Final Invention }. 3.3. Feature-based approaches to similarity assessment
Form Section 3.2 we know that we can obtain different feature based approaches to similarity assessment resulting from instantiations of the framework presented in Definition 2. For example, we can make use of the approaches presented in
S glosses (see Section 3.2 ). In this paper, we study feature based methods. Thus, we will propose some novel feature based approaches to similarity measures for Wikipedia concepts by extending the existing feature based approaches such as Tver-Egenhofer, 2003 ), or Ontology based approach ( Sanchez et al., 2012 ).
 terms (or words, concepts), i.e., they have the same abstract representation, however, Glosses is a string representation. academic , field , of , study , ... }. andcanbeapplied( Chen,Bau,&amp;Yeh,2011 ),suchasPaice/Huskalgorithm( Paice,1990 )andPorteralgorithm( Porter,1980 ).The pre-processing should find the original form of word. In this paper we will consider the pre-processing and let the formal representation of Wikipedia concept Con is defined as h Synonyms , Setglosses , Anchors , Categories i .
S following similarity computation methods for Set 1 and Set where 0 6 a 6 1 and the value of a is specified by users or experts ( a = 0.5 in default).
Given two Wikipedia concepts Con 1 = h Synonyms 1 , Setglosses Anchors 2 , Categories 2 i , according to the notions of S for Wikipedia concepts (suppose that the function S concepts 2007 ), where category structure of Wikipedia can be looked as a general (or domain-independent) ontology. Aiming at the characteristics of Synonyms and Categories , we can provide some novel similarity methods for Wikipedia concepts.
Since Synonyms is a set of synonyms, so we may define the function S Synonyms 1 and Synonyms 2 ,
Since May 2004 Wikipedia provides also a semantic network by means of its categories: articles can be assigned one or graph, thus, category structure may be considered as a huge and comprehensive ontology. Thus, we can define the function
S is, a &gt; c means that a is a parent category of c or c is a subcategory of a . a subconcepts ( a )={ c 2 CA | c &lt; a }; superconcepts ( a )={ c 2 CA | c &gt; a }; hyponyms ( a )={ c 2 CA | $ c 1 , c 2 , ... , c n 1 , c n hypernyms ( a )={ c 2 CA | $ c 1 , c 2 , ... , c n 1 , c n
It is worth noting that the condition c 1  X  c 2  X   X  c n 1
Clearly, from Definition 3 we have the following subconcepts ( a ) # hyponyms ( a ); superconcepts ( a ) # hypernyms ( a ); hyponyms ( a )={ c 2 CA | $ c 1 , c 2 , ... , c n 2 CA ^ n P 2 ^ c c hypernyms ( a )={ c 2 CA | $ c 1 , c 2 , ... , c n 2 CA ^ n P 2 ^ c c
It is worth noting that the condition c 1  X  c 2  X   X  c n 1
Clearly, from Definition 3 we have the following subconcepts ( a ) # hyponyms ( a ); superconcepts ( a ) # hypernyms ( a ); hyponyms ( a )={ c 2 CA | $ c 1 , c 2 , ... , c n 2 CA ^ n P 2 ^ c c hypernyms ( a )={ c 2 CA | $ c 1 , c 2 , ... , c n 2 CA ^ n P 2 ^ c c intelligence associations , ... , Turing tests , Artificial intelligence stubs }, and
Definition 4. The set of features describing the concept a 2 CA in Wikipedia category structure is defined in terms of the relation 6 (or P ) as: l ( a )={ c 2 CA | a 6 c } (or l ( a )={ c 2 CA | c P a }).
 Obviously, l ( a )= hypernyms ( a ) [ { a }.
 Wikipedia category structure is a huge and comprehensive  X  X  X ategory network X  X  (i.e., graph), in fact, the strength of nodesinWikipediacategorystructurerequiresanefficientalgorithmforobtainingthevalueof l ( a ).Therefore,tocompute l ( a ) efficiently, in what follows we propose an approximate method via limited neighborhood of Wikipedia category structure.
Definition 5. Let a , b 2 CA be two concepts and k P 1 be a positive integer. A walk of length k from a to b in Wikipedia category structure, denoted as walk ( a , b , k ), is a sequence of k + 1 nodes h c (1) c 1 , c 2 , ... , c k +1 2 CA , c 1 =a, c k +1 = b , (2) c 1  X  c 2  X   X  c k +1 , (3) " 1 6 i &lt; k +1, c i 2 superconcepts ( c i +1 )or c ture is a directed acyclic graph defined as follows:
Intuitively, k -neighborhood ( a ) represents the set of nodes that are hyponyms or hypernyms of a , hyponyms (resp., hypernyms) of hypernyms (resp., hyponyms) of a via at most k edges (i.e., superconcept relations or subconcept relations) Wikipedia category structure.
 According to Definition 6, we can obtain the approximate features describing the concept a 2 CA by modifying Definition 4. k -neighborhood of a . The set of approximate features of a is defined in terms of the relation
Considering the set of differential features (resp., approximate features) of a with respect to b as: l  X  a  X n l  X  b  X  X  l  X  a  X  l  X  b  X  X f c 2 CA n c 2 l  X  a  X ^ c define the dissimilarity (or approximate dissimilarity) between a and b as follows:
Definition 8. The dissimilarity (resp., approximate dissimilarity) dis : CA CA ? R (resp., dis a with respect to b and the set of differential features (resp., approximate features) of b with respect to a : dis ( a , b )=| l ( a ) n l ( b )| + | l ( b ) n l ( a )| and dis k ( a , b )=| l k ( a ) n l k ( b )| + | l k ( b ) n l
Definition 9. The normalized dissimilarity and the normalized approximate dissimilarity between categories a and b according to their features are calculated as follows, respectively: Now we can provide the notions of similarity (or approximate similarity) between categories a and b as follows:
Definition 10. The similarity(resp., approximate similarity) Sim between a and b according to their features is defined as: Sim cat ( a , b )=1 dis norm ( a , b ) and Sim kcat ( a , b )=1 dis knorm ( a , b ).

From Definition 10 we know that the similarity between two categories has been presented based on normalized { Cat 1 , ... , Cat k }, so we can give the notion of the function S
With regard to the computation of similarity for sets of categories, the approach has been inspired by the method for evaluating concept similarity defined within the Enterprise Ontology Management Tool SymOntos ( Formica &amp; Missikoff, 2002 ) and Formal Concept Analysis ( Formica, 2006 ). Essentially, it is based on the maximum weighted matching problem in bipartite graphs, that can be solved in polynomial time ( Galil, 1986 ). Informally, it is illustrated as follows.
Consider two sets of categories Categories 1 ={ Cat 11 , ... , Cat candidate set of pairs be a subset of Categories 1 Categories
CS ( Categories 1 , Categories 2 ). For each candidate set of CS ( Categories the pairs of categories. One of the candidate sets having the maximum among all the computed sums is chosen. Formally,
Definition 11. Let Categories 1 ={ Cat 11 , ... , Cat 1 i categories, and suppose that k 6 k 0 . The sets CS ( Categories of pairs are defined by all possible sets of k pairs of categories defined as follows, respectively:
Now we can present the notions of similarity (or approximate similarity) between sets of categories Categories Categories 2 as follows:
Definition 12. Given any two sets of categories Categories
Cat 2 i , ... , Cat 2 k 0 }, the similarity and approximate similarity of Categories
Categories 2 ) and S kcategories ( Categories 1 , Categories Note that S categories and S kcategories are always values between zero and one.

Let us give a simple example in order to explain Definition 12 (we only consider the computation of S the computation process, we assume that Categories 1 ={ A
Sim cat ( A 1 , B 2 )= Sim cat ( B 2 , A 1 ) = 0.6, Sim cat ( A
Sim cat ( B 2 , A 2 ) = 0.6, Sim cat ( A 2 , B 3 )= Sim cat ( B
It is easy to know that
Thus, by Definition 12 we have that S categories ( Categories Obviously, we can propose some approaches to similarity measures for Wikipedia concepts by exploiting the functions
S
Con 1 = h Syns 1 , Sglos 1 , Anch 1 , Cate 1 i and Con 2 = h Syns
S kcategories ), we have the following approaches to similarity measures for Wikipedia concepts: Remark 3. It is worth noting that a framework of similarity measures for Wikipedia concepts is presented in this paper. Twelve kinds of approaches to semantic similarity measures (see SimFir resulting from instantiations of the framework are also proposed. Clearly, we can provide more methods to similarity mea-
Islam &amp; Inkpen, 2008; Lesk, 1986; Oliva et al., 2011 ) to define the function S 4. Evaluation
To validate the effectiveness of semantic similarity measure methods proposed here and to evaluate them, in this section we will use real-world datasets to compute semantic similarity for Wikipedia concepts. Concretely, we use Wikipedia as the released on 4 June 2013, and we use JWPL 3 (Java Wikipedia Library), Java with JavaTM 2 SDK and MySQL to program and implement our approaches. At the same time, in our experiments we eliminate all special characters, punctuations, and stop we use Porter algorithm ( Porter, 1980 ).

It is well known that an objective evaluation of the accuracy of semantic similarity functions is difficult because the notion of similarity is subjective. Generally, semantic similarity measures are evaluated by means of standard benchmarks of word pairs whose similarity has been assessed by a group of human experts. However, in this paper we evaluate the proposed methods with standard benchmarks imposes some challenges and requires some modifications and adjustments in order to make such comparison meaningful. The comparative experiments have been grouped into two parts.
Firstly, we evaluate the proposed semantic similarity measures for concepts which come from three widely used bench-enough X  X  benchmark (R&amp;G) ( Rubenstein &amp; Goodenough, 1965 ), and the WordSimilarity-353 Test Collection
Wikipedia, our experiments do not consider these words since we do not investigate the approach to disambiguation of Wiki-353-TC, and Fir , Sec , ... , Twe stand for SimFir Con , SimSec
Table 2 shows the correlation coefficients of the different measures with human judgements, where we assign k =10in k -neighborhoods.

Secondly, our similarity measure methods use Wikipedia to act as information source. It is well known that Wikipedia is a rich encyclopedia that covers almost all imaginable sources, thus, the presented methods to similarity measures may deal with lots of Wikipedia concepts that Web users publish by Wikis. To evaluate the accuracy of semantic similarity of our methods for Wikipedia concepts, we will develop a benchmark and then use the benchmark to evaluate the accuracy of our proposals.

For comparison purposes, we have selected 30 pairs of real-world Wikipedia concepts as shown in Table 3 . The similarity betweeneachpairisassessed by 10studentsand10 teachers inascalebetween0 (semanticallyunrelated)and 4(highlysynony-thestudentsandtheteachers.Thus,thisbenchmarkiscreatedandcanbeusedtoevaluatetheaccuracyofourapproaches.Thatis, relation coefficients of the different measures with our benchmark, where we assign k =10in k -neighborhoods.
From the above experimental results (see Tables 1 X 4 ) we know that most results accord with our intuition, in particular, we can see that some of our similarity measures obtain higher correlation values (i.e., most of the correlation values are here are effective. On the other hand, comparing the twelve kinds of approaches to semantic similarity measures (i.e., SimFir Con , SimSec Con , ... , SimTwe Con ) presented in this paper, from Tables 2 and 4 we know that the methods SimFir SimSec Con , SimThi Con , SimFou Con , SimSev Con , SimEig SimSix Con , SimNin Con , and SimTen Con in the general case. Especially, SimThi we may select SimThi Con , SimFou Con , SimEle Con , SimTwe compute the similarity of Wikipedia concepts.
 Now we analyze and discuss the experimental results (see Tables 1 X 4 ).
 Our approaches SimFir Con , SimSec Con , SimThi Con , SimFou benchmark (best performance is 0.496 and is obtained by SimFou semantic relatedness). Our approaches are designed to quantify similarity, thus yielding a poor performance on the 353-TC benchmark. This is supported by the 353-TC annotation guidelines, which define similarity as  X  X  X elonging to the same domain or representing features of the same concept X  X . 353-TC contains also highly rated word pairs such as cell and phone no properties (i.e., features) at all.
 Regarding the approaches SimFir Con , SimSec Con , SimThi
R&amp;G benchmark, and our benchmark, with the lowest correlation being 0.520 and the highest 0.827. Tables 2 and 4 show that the measures of SimThi Con and SimFou Con obtain higher correlation values than those of SimFir on our benchmark). The reason is that the aggregation function of SimFir max such as ( Petrakis et al., 2006; Sanchez &amp; Batet, 2013 ).
 On the other hand, while both SimFir Con and SimSec Con (resp., both SimThi function average (resp., max), SimSec Con performs better than SimFir all benchmarks because SimSec Con and SimFou Con use the R&amp;E approach ( Rodriguez &amp; Egenhofer, 2003 ) and SimFir tion is repeated for the approaches SimFif Con , SimSev Con larity assessment of Wikipedia concepts.
 The most distinguishing difference between SimFif Con and SimNin between SimSev Con and SimEle Con ; between SimEig Con and SimTwe
SimEig Con ) is proposed by used the similarity of categories (i.e., S
SimEle Con , and SimTwe Con ) is presented by exploiting the approximate similarity of categories (i.e., S 12). From Tables 2 and 4 we know that SimNin Con (resp., SimTen (resp., SimSix Con , SimSev Con , SimEig Con ) on all benchmarks overall. The reason is that S
SimSev Con , and SimEig Con is defined by making use of the whole Wikipedia category structure and S which are not related to cat 1 or cat 2 . On the contrary, by the definition of k -neighborhood we know that the nodes (i.e., categories) of k -neighborhood of cat 1 (resp., cat SimNin Con , SimTen Con , SimEle Con , and SimTwe Con can obtain higher correlation values than SimFif SimEig Con , respectively.
 Comparing SimFif Con (resp., SimSix Con , SimSev Con , SimEig should note that there is a different result here. In SimFif and SimEig Con the aggregation function is the average (unless there is a common synonym). However, SimSev perform better than SimFif Con and SimSix Con . Seemingly, this conclusion contradicts that of SimFir SimFou Con (i.e., SimThi Con and SimFou Con perform better than SimFir because there are two different similarity computation methods in SimFif there is only one similarity computation method in SimFir two methods S R &amp; E and S categories in SimFif Con , however, there is only one method S
Besides, from Tables 2 and 4 we also know that SimFif Con benchmarks. Regarding similarity computation of SimFif Con is obtained by S categories . That is to say, in SimFif Con
Wikipedia category structure, such feature based similarity computation methods yield substantially inferior results. The of (Wikipedia) concepts (e.g., SimFou Con and SimEle Con ). 5. Conclusion
The final goal of computerized similarity measures is to accurately mimic human judgements about semantic similar-ity. At present similarity measures have been used for many different areas such as natural language processing, ontol-ogy mapping, and Web searching. In this paper, some limitations of the existing feature based measures are identified, non-dynamic domains). To tackle these problems, we propose some novel feature based semantic similarity measure-ment methods that are fully dependent on Wikipedia and can avoid most of the limitations and drawbacks introduced above. To implement semantic similarity measurement based on feature by making use of Wikipedia, firstly a formal representation of Wikipedia concepts is presented. We then give a framework for feature based similarity based on the formal representation of Wikipedia concepts. Lastly, we investigate several feature based approaches to semantic similarity measures resulting from instantiations of the framework. The evaluation, based on several widely used benchmarks and a benchmark developed in ourselves, sustains the intuitions with respect to human judgements. Overall, several methods presented here have good human correlation and constitute some effective ways of determining similarity between Wikipedia concepts. As future work, we are planning to apply the proposed semantic similarity esti-existing standard benchmarks for concept similarity assessment, we will pursue the design of a new benchmark specially focused on Wikipedia concepts. Acknowledgements The authors would like to thank the anonymous referees for their valuable comments as well as helpful suggestions from
Editor-in-Chief (Professor Fabio Crestani) and Associate Editor (Professor Mark Sanderson) which greatly improved the exposition of the paper. The works described in this paper are supported by The National Natural Science Foundation of
China under Grant Nos. 61272066 and 61272067; The Program for New Century Excellent Talents in University in China under Grant No.NCET-12-0644; The Natural Science Foundation of Guangdong Province of China under Grant Nos. S2012030006242 and 10151063101000031; The Project of Science and Technology in Guangzhou in China under Grant No. 2014J4100031; The Foundation of Ministry of Education and China Mobile under Grant No. MCM20130651. References
