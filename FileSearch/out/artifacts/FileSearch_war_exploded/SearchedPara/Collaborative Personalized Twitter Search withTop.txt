 The vast amount of real-time and social content in microblogs results in an information overload for users when searching microblog data. Given the user X  X  search query, delivering content that is relevant to her interests is a challenging prob-lem. Traditional methods for personalized Web search are insufficient in the microblog domain, because of the diver-sity of topics, sparseness of user data and the highly social nature. In particular, social interactions between users need to be considered, in order to accurately model user X  X  in-terests, alleviate data sparseness and tackle the cold-start problem. In this paper, we therefore propose a novel frame-work for Collaborative Personalized Twitter Search. At its core, we develop a collaborative user model, which ex-ploits the user X  X  social connections in order to obtain a com-prehensive account of her preferences. We then propose a novel user model structure to manage the topical diversity in Twitter and to enable semantic-aware query disambiguation. Our framework integrates a variety of information about the user X  X  preferences in a principled manner. A thorough eval-uation is conducted using two personalized Twitter search query logs, demonstrating a superior ranking performance of our framework compared with state-of-the-art baselines. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Collaborative personalized search; Twitter; topic modeling; language modeling
In recent years, microblogging services, such as Twitter, emerged as a popular platform for real-time information ex-change. Every day, nearly 60 million short messages ( tweets ) are published and over 2 billion search queries are issued in Twitter 1 . However, the vast amount of content in Twitter results in an information overload for users when searching microblog data. In particular, tweets cover a wide range of topics and purposes, which makes a user X  X  search for rele-vant content challenging and time-consuming. As a result, novel methods for search result personalization are needed in the microblogging domain.

Although much work has been done on personalized Web search [5, 12, 18, 19, 25] and collaborative Web search [17, 21, 23, 27], little work has been done on personalizing the search experience in the social environment of Twitter. Re-cent work related to information retrieval in Twitter does not consider individual users X  interests in the ranking [7, 8, 14, 16]. Thus, in this paper we develop an effective frame-work for collaborative personalized Twitter search.
Similarly to personalized Web search, our goal is to re-rank a set of search results based on their similarity with the user X  X  preferences and thus, improve the retrieval effective-ness. However, the microblogging environment differs from traditional Web significantly, which calls for novel methods to accurately model user X  X  preferences. The main challenges related to personalized information retrieval in microblogs canbesummarizedasfollows:
In this paper, we address the above challenges and propose a novel probabilistic framework for Collaborative Personal-ized Twitter Search (CPTS) . In the following paragraphs, we highlight the main features of our framework.

Collaborative User Modeling. The user X  X  social con-nections can provide valuable clues about her preferences. However, constructing a collaborative user model in not triv-ial, since not all information from the user X  X  social environ-ment is equally useful. In fact, the collaborative model may become noisy if all information from the user X  X   X  X riends X  is included. Therefore, we analyze the user X  X  social interactions and estimate the importance of each  X  X riend X . Furthermore, we analyze the importance of each friend X  X  topic, in order to separate potentially relevant topics from irrelevant ones. The proposed collaborative user model helps to tackle the sparseness of individual user X  X  data while avoiding the in-jection of unnecessary noise from the social neighborhood. Moreover, this method is suitable to new users who have posted few tweets, thus addressing the cold-start problem.
Topic-Specific Language Modeling. Our approach to user modeling and personalized re-ranking is based on topics . Content posted by a user may be highly diverse in terms of topics (e.g.,  X  X usiness X ,  X  X port X , but also  X  X motional comments X ). Putting all information from a user into a sin-gle user model would lead to a noisy and inaccurate model. Therefore, we distinguish the different kinds of information and propose a novel user model structure, referred to as topic-specific user language models . The proposed structure is beneficial in several ways. First, it enables effective query disambiguation by estimating the latent meaning behind a user X  X  query. Second, during personalized re-ranking, we may identify tweets from relevant topics and promote them in the ranking. Third, we consider the user X  X  topical prefer-ences when building the collaborative user model.
Integrated Posting-Search Model. Each microblog user is both a content producer and consumer. On the one hand, a user X  X  tweets indicate her preferences as a content producer. On the other hand, user X  X  search activity indicates her preferences as an information consumer. Our framework integrates both types of preferences in a principled manner.
Responsive and Dynamic Profiles. Our user models are dynamically updatable, with adjustable weights of each component. Furthermore, our framework does not require any explicit input from the users to maintain their profiles.
We summarize contributions in this paper as follows: The rest of this paper is organized as follows. We review related work in Section 2 and preliminaries in Section 3. Our proposed framework is presented in Section 4. Section 5 presents our evaluations. Section 6 concludes our findings.
Personalized Web Search. In personalized Web search [5, 12, 18, 19, 25], the principle of search result re-ranking is usually applied. Given a set of search results to a user X  X  query, we promote search results that have a higher simi-larity with the user X  X  preferences (represented as the user model ), in addition to traditional user-independent metrics used in the ranking, such as the query-document relevance and document-specific features. Building the user model in mostly relies on implicit data from user X  X  clicks. However, only considering the information from single user X  X  clicks re-sults in the problems of data sparseness and cold-start [27].
Collaborative Web Search. To alleviate the data sparse-ness problem in personalized Web search, collaborative Web search techniques were developed [17, 21, 23, 27]. In collab-orative Web search, the search preferences of a community of users are mined and utilized in a similar way to collabo-rative filtering. Search results are then re-ranked for a given user based on the pages clicked by other similar users. For example, CubeSVD [21] analyzes the correlation between users, queries and documents in a search query log. The ex-tracted click patterns among a community of users are then employed for personalizing the results of a particular user. Xue et al. [27] take a language modeling approach to build user-specific language models and cluster similar users into communities. A community-specific language model is then used for smoothing the user models to inject community knowledge. In contrast, our approach exploits the explicit social neighborhood of a user to learn about her information need. We measure the importance of each social connection and construct a topic-sensitive collaborative user model.
Microblog Search. In terms of general information re-trieval in Twitter, Massoudi et al. [14] presents a retrieval model for microblogs, which takes into account tweet-query relevance, quality features of tweets and incorporates a query expansion model. Duan et al. [7] use a learning-to-rank ap-proach for general tweet ranking. [8, 15] incorporate tempo-ral aspects of tweets to improve microblog search.
Some attempts were made to construct a user profile from microblog data for the purpose of recommendation [1, 3, 4]. For example, Chen et al. [4] take a collaborative ranking approach to tweet recommendation and employs a number of tweet-specific features to influence the importance of a tweet. However, the existing work does not address the di-versity of topics or user X  X  social connections when construct-ing the user model. Moreover, the user X  X  query has not been considered and thus the methods cannot be readily applied to microblog search personalization.

To the best of our knowledge, our work is the first to establish a collaborative Twitter-based search personaliza-tion framework and present an effective means to integrate language modeling, topic modeling and social media-specific components into a unified framework.
Statistical language modeling (LM) has been successfully applied in machine translation, speech recognition and in-formation retrieval [28]. In information retrieval, LM typi-cally adopts the Query Likelihood model [28]: Given a query Q = { q 1 ,...,q i } and a document D = { w 1 ,...,w j } ,the score for D against Q is proportional to the probability that the multinomial language model that generated D also gen-erated Q . Formally, This ranking method captures both the document X  X  rele-vance to the query and also the document X  X  prior proba-bility, P ( D ). The latter may be used to incorporate any document-specific features, such as PageRank.

Assuming the unigram model of documents, we can de-compose the query into individual words and compute the overall score as a product of individual term scores,
An important step in the estimation of the conditional probability P ( w | D ) is to account for unobserved terms. To this end, several smoothing methods were proposed. Jelinek-Mercer smoothing [28] is one of the simplest and most pop-ular smoothing methods that uses a fixed smoothing param-eter  X  to interpolate a document X  X  language model with a global (corpus) model. It is defined as where c ( w,D ) is the count of word w in document D and p ( w | C ) is the probability of w in the entire corpus.
Topic modeling (TM) has gained popularity in recent years as a tool to perform unsupervised analysis of text collections and organize documents by their latent topics. One of the most popular topic models is Latent Dirichlet Allocation (LDA) [9]. LDA can be used to discover a set of K latent topics from a document corpus, and then to represent each document D as a mixture  X  D of the latent topics. For each word w i in D ,wefirstsampleatopic z i from the document mixture  X  D . Second, we sample w i accordingtotopic z  X  X  word distribution  X  z .

Much work has been done on developing efficient inference methods for LDA. Recently, online inference for LDA has been developed in [9], which enables LDA to be trained on massive and streaming data. We use the algorithm in [9] to train LDA on a large Twitter dataset in an online fashion.
Topic modeling has previously been applied for informa-tion retrieval [26]. In topic model-driven IR, the probability of a query given document is However, this approach often resulted in decreased ranking accuracy compared with standard LM, since topics are too coarse [26]. To alleviate this problem, a linear combination of TM and document LM is usually employed,
In face of the short length and diverse topics of tweets, neither LM nor TM alone are suitable to build user mod-els for personalized microblog search. On the one hand, simply employing LM and estimating user-specific language models (e.g., in [27]) may easily promote irrelevant tweets in the ranking. Using such a model, even the match of a few words of an irrelevant tweet with the user model may boost its ranking. On the other hand, a pure topic model-ing approach to represent user X  X  preferences may yield even worse results, since the topics are too coarse [26]. Match-ing a tweet to topic  X  X T X  may not guarantee its relevance to the user X  X  preferences. Thus, we develop a novel user model structure, which enables a fine-grained and topic-aware user representation.
First, we define the scope of our personalization frame-work. Given a microblog user u , a search query Q and a set of N microblog documents returned by a base search en-gine, our goal is to re-rank the documents using query Q and ausermodel M u , such that documents matching the user X  X  interests are ranked at top positions.

To achieve this goal, we need to solve two basic problems: (1) how to construct the user model M u , and (2) how to utilize this model for document ranking. In this section, we present our personalization framework to meet the above goals. We note that throughout this paper, a document refers to a single microblog message (i.e., a tweet ).
We begin our discussion with some basic assumptions made in our framework. In particular, we recognize the impor-tance of analyzing user preferences in microblogs in terms of topics . Our observation is that microblog content is highly diverse in terms of topics and purposes. This diversity is discussed in detail in [11, 20]. For example, Java et al. [11] found that Twitter serves a wide range of purposes, such as daily chatter, conversations or news sharing. In this pa-per, we simply use the concept of topics to broadly refer to the different kinds of content. An example of such topics would be  X  X op music X ,  X  X T news X , but also  X  X ersonal feel-ings X . We note that even the interests of a single user may be very diverse. As a result, our intuition is that by treating all information with the same importance, we would obtain an inaccurate and noisy personalization model. Therefore, we propose to distinguish the different kinds of information within our framework.

State-of-the-art topic models such as LDA [9] may be em-ployed for unsupervised topic discovery and for topic assign-ment of future documents. As the first step, we therefore build a global Topic Model using a large Twitter corpus, which will be utilized throughout our framework. We will refer to this model simply as TM.
 We now define some basic operations done using the TM. To obtain a topic distribution  X  D of a new document D ,we obtain each dimension i of  X  D as follows
To assign document D to a single topic, we choose the topic that maximizes the probability of generating D ,
In contrast to previous approaches, which estimate a sin-gle language model for each user (e.g., in [27]), our approach istoconstructatwo-layerusermodel. Eachusermodelis composed of a topic layer and a word layer. The topic layer represents user X  X  high-level preferences and the word layer represents the user X  X  words used within the respective topic. We refer to this model the Individual User Model (IM).
The two-layer structure has the advantage of organizing user preferences related to different topics separately. This in turn enables semantic-aware query disambiguation and search result re-ranking. For example, Figure 1 illustrates the IM of user U. U often tweets about IT and mentions the term  X  X ndroid X . Also, U tweets about food and mentions the term  X  X pple X . Thus, if U searches for  X  X ndroid X , U X  X  IM suggests that U may be interested in IT-related tweets. Also, if U issues another query related to IT (e.g.,  X  X obile apps X ), tweets mentioning  X  X ndroid X  will be ranked high. In contrast, using a traditional single-layer user model would also falsely promote tweets mentioning  X  X pple X .
 We estimate the IM for each user u in the following way. First, we assign each microblog document D (i.e., a tweet) from u to a topic using Equation (7). Second, we build a language model for each u  X  X  topic using all u  X  X  documents assigned to the respective topic. On the word level, the max-imum likelihood (ML) estimate of the probability of word w in topic k for user u is defined as where D u is the set of documents by user u , c ( w, D )isthe count of word w in D , V is the vocabulary, z D is the topic of
D . We refer to this probability as  X  IM
On the topic level, the probability that u chooses topic k is estimated as Figure 1 illustrates an IM created from a set of user X  X  docu-ments.

Before the IM can be used by a ranking function to get the user X  X  preference for word w in topic k (i.e.,  X  IM u,k,w need to account for the case of unobserved words. To this end, we smooth the topic-word distribution in the IM using the underlying topic model, where  X  is a parameter for Jelinek-Mercer smoothing.
If we further incorporate the topic-level probabilities of user u ,weget where  X  is the prior probability of choosing a topic. In our work,wechooseaconstantvaluefor  X  .

Additionally, along with each  X  IM u,k,w , we also store the timestamp of the latest document in topic k containing w . This allows to track the recency of information in the IM. The probability of w in the IM can then be re-defined as where  X  is the forgetting coefficient. This time decay factor assumes an exponential forgetting rate and was applied to IR by Li and Croft [13].
Based on the individual user model defined above, we for-mulate a basic personalized ranking function as follows: P ( D,Q,u )  X  sonalized scores of D and Q , respectively, and P ( D )isthe document prior.

This approach essentially decomposes the ranking into two components. First, we perform query disambiguation using u  X  X  IM. That is, we predict which underlying topic the user had in mind when formulating the query. Second, the ob-tained probability given topic k is multiplied with the prob-ability that document D belongs to the respective topic in u  X  X  IM.

To obtain P ( Q |  X   X  IM u,k,w ) (and similarly, P ( D |  X   X  IM compute the product of the scores of each word,
One of the most important features of microblogs is its so-cial network structure, which enables interactions between users. Users may follow other users, such as public figures or their real-world friends, and are able to receive their tweets. If a user finds a tweet interesting, they are able to re-tweet it or add it to their favorites. Furthermore, users can have conversations or mention each other in their tweets. This so-cial environment presents rich additional information about the user X  X  interests, which can increase the completeness of the user model and tackle data sparseness of an individual user. In this work, our main focus is on the followees of a user (i.e., the users one has subscribed to), which we refer to as friends for simplicity.

However, we observe that different friends may have a different influence on a particular user u . For example, may follow hundreds of friends, but only frequently inter-acts with a small fraction of them. Furthermore, not all content posted by a friend may be of interest to u .Forex-ample, u may be interested in tweets from friend f about  X  X T news X , but may not be interested in f  X  X  comments about  X  X elationships X .

We therefore assign a weight to each friend of user u ,which is composed of four factors:
Popularity weight w P ( f ) : The popularity of user f , whichmaybeindicatedby f  X  X  number of followees, num-ber of times f is listed in public lists, PageRank score, etc. In our work, the log of the followee count is used as an in-dicator of popularity. The popularity weight is normalized by w P ( f )=log( popularity ) / log( max ), where max is the maximum popularity of a user in Twitter 2 .

Affinity w A ( u, f ) : We measure the similarity of interests of u and f as the inverse KL-divergence between their topic-level profiles, w A ( u, f )=1 /KL (  X  IM u,  X  ||  X  IM f,
Topic-interaction weight w I ( u, f, k ) : We analyze the interactions between u and f , which include the conversa-tions between u and f ,mentionsof f by u , and re-tweets of f  X  X  tweets by u . We first retrieve all tweets containing the above interactions and assign each tweet to a single topic (for conversations, we assign the entire conversation to a topic). The topic-interaction weight w I ( u, f, k ) is then based on the count of interactions between u and f that are assigned to topic k , denoted c ( u, f, k ). The weight is normalized by w ( u, f, k )=log 10 (1 + c ( u, f, k )) if c&lt; X  and w I ( if c  X   X  .Weset  X  empirically to  X  = 10.

Topic bias w T ( u, k ): Apart from the above friend-dependent weights, we also consider u  X  X  bias towards content about topic k , i.e. w T ( u, k )=  X  IM u,k .If f  X  X  IM contains topic apply the topic bias as a prior probability of u  X  X  interest.
The overall weight of friend f is then a vector  X  u,f ,where each dimension k  X  X  1 ,...,K } is defined as where 0  X   X  u,f,k  X  1and  X  is an optional weight vector to enable different influence of the weight components.
Finally, all friend weights are normalized such that friends. The total weight of friend f may then be obtained as we limit the number of friends that are considered for the collaborative user model by selecting top-n friends based on the total friend weight.

Creating the Collaborative User Model. After ob-taining the weight of each friend of u ,weconstructthe Col-laborative User Model (CM). Basically, we take the weighted average of all the individual user models of u  X  X  friends. The topic-specific language model for topic k within in the col-laborative user model is estimated as follows
The collaborative user model can now be integrated with the individual user model as follows where  X  is a parameter that controls the influence of CM on the IM. We adopt Dirichlet prior smoothing, which allows to smooth sparse individual models more aggressively than rich individual models. In this method,  X  is defined as where  X  is the Dirichlet smoothing parameter. In plain words, we smooth the individual model using the collabora-
This information can be obtained from, e.g., http://twittercounter.com/pages/100 tive model and finally smooth using the topic model. Figure 2 illustrates the collaborative user modeling method.
The collaborative personalized ranking function is then defined as P ( D,Q,u )  X 
In addition to analyzing the user X  X  individual microblog content and building the collaborative model, we also model the user X  X  search activity and construct the Search User Model (SM). As implicit evidence of the user X  X  search in-terests, we mainly consider search queries issued by the user and the user X  X  feedback on the search results. In microblogs, there are several ways a user can provide implicit relevance feedback. These include re-tweeting (re-sending) or  X  X avorit-ing X  X n interesting tweet, or clicking a URL within the tweet. We refer to these actions as clicks for convenience. Admit-tedly, a more thorough analysis of the importance of various click types in user preference modeling is an interesting fu-ture work.
 Let click ( u, Q, D ) denote a click by user u on document returned to query Q . The set of all clicked documents by is denoted S u . For each clicked document D , we first assign D to topic k using Equation (7). Second, we obtain the following implicit relevance feedback from the user X  X  click: The search user model can now be integrated with IM and CM by a weighted sum as follows Figure 3: Evaluation user interface, showing results for query  X  X ndroid X . where  X  is a parameter to control the influence of the SM. Parameter setting is discussed in more detail in Section 5.3.2.
Incorporating query-topic feedback. When  X  SM u,k,Q &gt; 0for topic k and a query phrase Q ,wemayreplace  X  SM u,k in Eq. 20 with  X  SM u,k,Q to incorporate the user X  X  query-topic feedback.
The full ranking function for collaborative personalized search is defined as
By using the ranking function in Equation 21, we incorpo-rate all three user models (IM, CM and SM) in a principled manner. Moreover, our method allows to maintain each user model separately, which has the advantage of fast updata-bility, and enables the model parameters (i.e.,  X , X , X  )tobe updated at any time.

Notably, our framework is flexible enough to incorporate additional document-specific and author-specific features in the ranking function, by means of the prior document prob-ability P ( D ). However, a comprehensive study of document and author features is not within the scope of this paper.
To train the global topic model (TM), we obtained a sam-ple of public tweets from Twitter X  X  Streaming API 3 .We crawled a total of 44.5 million tweets over the course of 6 months in 2013. After filtering non-English language tweets and removing tweets of less than 20 characters in length, our dataset contained 11.7 million tweets. This dataset is used to train the global topic model in Section 5.3.1.
To evaluate the effectiveness of different personalization approaches for Twitter search, a query log with associated information about the user (incl. user X  X  tweets and social connections) is needed. However, such information is not available in commonly used datasets (e.g., the TREC Mi-croblog Track 4 ). Therefore, we developed a web-based Twit-ter search middleware to collect user X  X  search queries and relevance judgements. Users can log in to the system us-ing their Twitter account. Given a search query, the system connects to Twitter X  X  Search API 5 and retrieves 50 recent tweets. The results consist of 3  X  X opular X  tweets as deter-mined by Twitter and up to 47  X  X eneral X  tweets matching https://dev.twitter.com https://sites.google.com/site/microblogtrack/ https://dev.twitter.com/docs/api/1.1/get/search/tweets the query. Our system presents all results to the user in a random order, in order to avoid any bias. The user may evaluate the relevance of each result by clicking on a star icon, as shown in Figure 3. The system stores all submitted queries, retrieved tweets and the user X  X  relevance ratings. Query Log 1: Controlled User Study (Log CoS).
 We obtained a search query log with relevance judgements in a user study involving 11 active Twitter users. The user study is divided into two days (referred to as Day 1 and Day 2). On Day 1, users are asked to prepare 10 queries about their topics of interest. The 10 queries are categorized into four types: recency, topical, entity-oriented and ambiguous. The first three types correspond to common search scenarios in microblogs, as reported by Teevan et al. [24]. Addition-ally, we also consider query ambiguity, which serves as an important motivation in classic personalization research [6]. We note that each query can be classified under multiple types. The query types are detailed as follows:
Users are asked to choose at least one query of each type and 10 queries in total. Users are then asked to submit each query in the evaluation system, review the 50 tweets returned by the system and mark relevant tweets.

On Day 2 of the study, users are asked to choose a new set of queries by re-submitting 5 queries from Day 1 and choos-ing 5 new queries. Similarly to Day 1, users submit each query in the evaluation system and mark relevant tweets.
We present overall statistics of the obtained query log, referred to as Log CoS , in Table 1. We further examine the type of each query, which has been indicated by the users. Table 2 shows the proportion of queries of each type and example queries from the log.

To learn about the topical diversity of queries in the dataset, we manually inspect each query and assign a topical cate-gory. We utilize the Yahoo taxonomy 6 and classify queries into the top-level categories. For ambiguous queries, we choose a category based on which tweets were marked as  X  X elevant X  by the user. The distribution of queries by their category is shown in Table 3. http://dir.yahoo.com/ Table 2: Overview of query types from Log CoS and Log IwS, including the proportion of queries ( X %Qrs X ) and example queries of each type.
 Table 3: Overview of queries by query category, in-cluding the proportions of queries ( X %Qrs X ).
 For the purpose of our evaluation, query log data from Day 1 is treated as the training dataset and data from Day 2 is treated as the testing dataset .
 In addition to the query log data, we also crawl the users X  Twitter data. Specifically, we obtain the latest 200 tweets of each user and crawl the tweets of the top-20 friends, ranked by friend weight (cf. Section 4.3).
 Query Log 2: In-the-Wild User Study (Log IwS).
 To obtain users X  search preferences in an unrestricted set-ting, we invite 24 users and conduct an open user study over a 3-month period. Users are invited to use our evalu-ation system and submit search queries of their choice. As an approximate guide, we ask users to submit at least 10 queries over the evaluation period. When a query is submit-ted, the user is asked to read through all tweet results and provide relevance ratings.
 The statistics of the obtained dataset are given in Table 1. We find that users submitted 9.42 queries on average during the study period, with a standard deviation of 3.15. We also observe that users identified 17.94 relevant results per query, which is higher than 7.19 in the controlled study.
To gain more insight into users X  choice of queries and to compare against the Log CoS dataset, we empirically ana-lyze the query log. First, we perform a post-hoc assignment of queries to the four query types utilized for the Log CoS dataset. However, we note that the importance of query recency may vary among different users, which prevents an objective decision whether a query was recency-oriented or not 7 . Therefore, we instead focus on identifying  X  X ews-related X  queries, since such queries have a strong recency focus and can be identified more objectively. Table 2 shows the proportion and examples of queries of each type.
Similarly to Log CoS, we analyze the topical diversity of queries and manually classify queries into topical categories. The distribution of queries by category is shown in Table 3. Figure 4 shows an overview of the evaluation process. First, we collect query logs (cf. Section 5.1.2) and crawl Twitter data for each user. Second, we estimate the indi-vidual and collaborative user models described in Section 4. Using the training query log, we estimate the search user model (cf. Section 4.4) and tune the global parameters of our framework. Third, we use the testing query logs (i.e., Day 2 of Log CoS and all queries in Log IwS) to measure ranking performance. The testing process involves all as-pects of our framework, which includes dynamic updating of the search model (SM). For each testing query, we pro-duce a ranking using our framework, evaluate the ranking using relevance judgements from the query log and update the SM. This cycle is repeated for each testing query. The process simulates the behavior of our framework in a real us-age scenario, in which a user submits a query, reads through the results and clicks on (e.g., re-tweet) the relevant ones.
We implement the following non-personalized and person-alized baseline models :
Proposed models. We evaluate each component of the pro-posed Collaborative Personalized Twitter Search framework:
Ranking performance is evaluated using two standard met-rics, namely Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP).
To train our global topic model, we use the Twitter cor-pus described in Section 5.1.1. We utilize an online inference algorithm for LDA [9], which is based on stochastic varia-tional inference and allows for processing of massive and streaming data. It was shown in [10] that LDA trained on grouped tweet-documents performs better than training on individual tweet-documents. In our public tweet sample, it is not practical to group tweets by their authors. In-stead, we select all tweets containing one or more hashtags and group each tweet to their respective hashtags. In this way, we obtain longer and more semantically-rich  X  X ashtag-documents X  for training. As an additional pre-processing step, we remove spam-like hashtag-documents 8 and hashtag-documents of short length.
We adopt a parameter selection approach commonly used in probabilistic frameworks (e.g., in [8]). Using the training dataset from Section 5.1.2, we optimize the global parame-ters for our framework. We proceed by optimizing one pa-rameter at a time, while keeping all other parameters fixed. The obtained parameter values are listed in Table 4.
The collaborative user model constitutes an integral and non-trivial part of our framework. Intuitively, the criteria for selecting which content to include in the CM will largely influence the CM X  X  ranking effectiveness. Therefore, we first study the importance of the friend weighting factors (pop-ularity, affinity, topic-interaction and topic bias) proposed in Section 4.3. We are interested in finding which factor or combination of factors yields the best results.

We build 15 versions of the CM for each user, based on all combinations of the 4 weight factors. We then measure
Example of a spam-like hashtag is  X #followback X , which is included in automatically generated tweets sent around the social network. Figure 5: Performance of the Collaborative Model with different weight factors: Popularity (P), Affin-ity (A), Topic-Interaction (I), Topic Bias (T). the ranking performance of each CM version in turn, using the CPTS-CM ranking model. Figure 5 shows the results of this experiment on both query logs.

When using a single weight to build the CM (i.e.,  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X  in Fig. 5), the results suggest that popularity and topic-interaction weights are the most effective. This suggests that it is beneficial to assign a higher weight to popular friends, as well as selectively promoting content in topics and by friends with whom a user often engages. The affinity weight is effective on the Log CoS dataset, but per-forms poorly on Log IwS. However, topic bias ( X  X  X ) shows the weakest performance on both datasets. This suggests that it is not beneficial to assign an  X  X priori X  weight to all content in a particular topic produced by user X  X  friends. Among all versions of CM, the best performance is achieved with  X  X AI X  on Log CoS and  X  X A X  on Log IwS. These versions of CM are therefore chosen when reporting overall ranking performance in the following sections.
In this section, we evaluate the ranking effectiveness of individual components of our framework and compare them with the baseline methods listed in Section 5.2.2. Addition-ally, we perform a paired student X  X  t-test to determine if the differences between the results of our methods and each baseline method are statistically significant (p-value &lt; Table 5 shows the overall ranking accuracy on both query logs, with indications of statistically significant differences over baseline methods.

From the results, we observe that standard language mod-eling (B-QL) is outperformed by each component of our framework. This result is somewhat expected, given that B-QL does not consider individual users or their social neigh-borhood. The topic model-based retrieval model (B-TM) shows a superior performance to B-QL, in particular at higher ranks (e.g., NDCG@5). This suggests that incorporating the latent semantics for scoring tweets provides an advan-tage over standard query likelihood. The personalized and collaborative baseline methods (B-PS, B-CS, B-CPS) fail to outperform the non-personalized baselines on the Log CoS dataset. On the Log IwS dataset, they achieve a marginal improvement at lower ranks (NDCG@10 and beyond). This shows that simply applying personalization techniques used in Web search may perform poorly in microblog search. In particular, we note that the collaborative and personalized baseline (B-CPS) does not achieve a cumulative improve-ment over its individual components (B-PS, B-CS). This further indicates that simply fusing user X  X  individual pref-erences with the group X  X  preferences may harm ranking ef-fectiveness in the microblog domain. Among the proposed models presented in this paper, the IM alone outperforms all baseline models. We note that IM significantly outperforms its baseline counterpart (B-PS) by NDCG@5 on Log CoS. This demonstrates the effectiveness of our two-level user model structure, which utilizes latent topics in microblogs to organize user preferences.
A further improvement of MAP is achieved by the col-laborative user model (CM). However, we observe that CM is not as effective as IM at top ranks (e.g., NDCG@5) on Log CoS. This indicates that IM may be more effective in promoting relevant tweets to the top positions, if the IM contains sufficient information from the user X  X  tweets. Re-garding the results of CM, we note that user X  X  own tweets are not considered within this model. This situation may arise in practice if a user produces little own content, but follows others. This may particularly benefit newly registered users.
The search user model (SM) shows the best performance among the proposed models. However, we note that the SM is based on the user X  X  implicit feedback and hence faces the cold-start problem. Our framework is designed to overcome the cold-start problem by considering the user X  X  content and social connections in the IM and CM, respectively. The over-all results show the strength of the IM and CM, even when no information about the user X  X  search behavior is available.
Finally, the full proposed framework (CPTS-All) achieves the best overall ranking performance. On the Log CoS dataset, the difference with all baselines except B-TM is statistically significant. On the Log IwS dataset, we confirm statistical significance compared with all baselines. The results demon-strate that all three information sources in our framework are complementary in improving ranking performance.
In this section, we further compare the performance of each model in our framework. Since each model uses a dif-ferent source of evidence about the user X  X  preferences, each model may be effective under different circumstances.
First, we focus on the Search Model (SM). On the one hand, this model is most prone to the cold-start problem when a user first uses the system. On the other hand, the model can be dynamically updated each time a user sub-mits a query and provides relevance feedback (referred to as a query-feedback step ). We therefore study how the effec-tiveness of SM evolves with each query-feedback step. For each user, after the i -th query is processed and relevance feedback is received, we calculate the average MAP since the first until the i -thquery. InFigures6(a)and(b),we Figure 6: Average per-user ranking performance af-ter processing i user X  X  queries. show the average per-user MAP after each query-feedback step. We observe that the effectiveness of SM increases with more queries and relevance feedback from the user.
In the next step, we compare the performance among the three proposed models. Similarly to the previous experi-ment, we calculate the average per-user MAP after each query-feedback step and show the results in Figures 6 (c) and (d). For both query logs, the results suggest that for the first few queries (first 5 queries for Log CoS and first 4 queries for Log IwS), CM gives the best results among all models. However, with more relevance feedback, the perfor-mance of SM improves, enab ling it to outperform CM.
It is important to note that the previous results are aver-aged over a number of users and queries, which blurs some details about each model X  X  performance. In particular, when inspecting the ranking effectiveness for a query Q ,wemay determine which of the proposed models achieves the best performance for Q . We therefore measure the  X  X uccess rate X  of each model, in terms of the number of queries for which the model achieved the highest MAP. On Log CoS, IM, CM andSMachievedthehighestMAPfor23 . 5%, 49% and 27 . 5% of queries, respectively. On Log IwS, IM, CM and SM achieved the highest MAP for 19 . 1%, 31 . 8% and 49 . of queries, respectively. From the results, we see that no single model produces the best performance for all queries. This again confirms that all three models are complementary
Figure 7: Effectiveness for different query types. and contribute to the overall ranking score when integrated in our framework.
In this section, we study the effectiveness of our framework when dealing with different types of queries. Intuitively, different types of queries may require personalization to a different extent. In the Web search scenario, it is reported that personalization may even harm ranking quality for some query types [6]. We focus on the four query types described in Section 5.1.2. The proportion of each query type in our datasets is given in Table 2.
 Figure 7 shows the average MAP for each query type. Among the personalized and collaborative baselines, we ob-serve that B-CS achieves the best performance for all query types on Log CoS. However for entity-oriented and ambigu-ous queries, we find that the performance of B-CS is not very stable across our datasets and fails to outperform non-personalized baselines in some cases. Moreover on Log IwS, we do not observe significant differences between the per-sonalized and collaborative baselines. These results indicate that the existing methods, which originate from Web search, do not produce satisfactory results for microblog queries.
In contrast, the proposed framework improves the rank-ing performance for all query types on both datasets. Our method is effective even in cases when the personalized base-lines perform poorly. For entity-oriented queries in Log CoS, we improve the baseline MAP of 0.194 (B-QL) to 0.245, while B-CS only achieves 0.169. For ambiguous queries in Log IwS, the baseline MAP of 0.327 (B-TM) is improved by our method to 0.341, while B-CS only achieves 0.31.
In this paper, we present a novel probabilistic framework for Collaborative Personalized Twitter Search. The frame-work integrates a variety of information about the user X  X  posting and searching preferences. At the core, we develop a topic-sensitive collaborative user model, which utilizes users X  social connections to augment the user profile and improve ranking performance. Furthermore, we propose a new two-layer user model structure, which effectively handles the di-versity of microblog users X  preferences. Our experimental evaluation has demonstrated superior performance against competitive baselines in a variety of settings.

As relevant issues for future work, we plan to categorize the query types that arise in microblogs and design query-dependent personalization strategies. In another direction, we plan to incorporate more features into our framework, such as spatial and temporal dimensions of user preferences.
