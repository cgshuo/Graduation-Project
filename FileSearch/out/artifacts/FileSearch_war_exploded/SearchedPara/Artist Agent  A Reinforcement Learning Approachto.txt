 Masashi Sugiyama SUGI @ CS . TITECH . AC . JP Among various techniques of non-photorealistic rendering, stroke-based painterly rendering simulates common prac-tices of human painters who create paintings with brush strokes. In this paper, we focus on oriental ink painting. Unlike western styles, such as water-color, pastel, and oil painting, which place overlapped strokes into multiple lay-ers (Hertzmann, 1998; Shiraishi &amp; Yamaguchi, 2000), ori-ental ink painting uses few strokes to convey significant in-formation about the scene. An artist can draw expressive strokes in various styles by soft brush tufts. The appear-ance of the stroke is therefore determined by the shape of an object to paint, the path and posture of the brush, and the distribution of pigments in the brush.
 Drawing smooth and natural strokes in arbitrary shapes is challenging since an optimal brush trajectory and the posture of a brush footprint 1 are different for each shape. Xie et al. (2011) formulated the problem of drawing brush strokes as minimization of an accumulated energy of mov-ing the brush and used the Dynamic Programming (DP) approach to obtain optimal brush strokes. It was demon-strated that smooth and natural brush strokes could be ob-tained by minimizing the accumulated energy. However, the stroke optimized by DP for a specific shape cannot be applied to other shapes even when the difference is small. Thus, it is not efficient if the target object is composed of many basic shapes, e.g., a Chinese character, since the opti-mal brush stroke for each shape has to be obtained. Further-more, ordinary DP cannot directly handle continuous ac-tions and states. Thus, smoothness of resulted brush strokes is highly dependent on the discretization of spaces. In this paper, we introduce a reinforcement learning (RL) approach to solving this problem. We model a soft-tuft brush as an RL agent that makes a sequential decision on which direction to move, and train the agent to draw grace-ful strokes in arbitrary shapes (see Figure 1). Our idea is to first learn a desired drawing policy by maximizing the sum of rewards from a number of typical training shapes. Then, the trained policy is applied to draw strokes in various new shapes.
 More specifically, the proposed approach contains two technical challenges: how to design the brush agent and how to train the agent X  X  policy. We first propose to de-sign the state space of the brush agent to be relative to its surrounding shape, e.g., boundaries and the medial axis, to learn a general drawing policy which is independent of a specific entire shape. Secondly, we propose to formu-late stroke drawing by a Markov decision process (MDP) (Sutton &amp; Barto, 1998) and apply a policy gradient method (Williams, 1992) to learn a (local) optimal drawing policy. An advantage of the policy gradient method is that it can naturally handle continuous states and actions which are important for obtaining smooth and natural brush strokes. Furthermore, since a policy is a function for selecting ac-tions given a state, a learned policy can be naturally applied to new shapes. In this section, we formulate the problem of automatic stroke generation as a reinforcement learning (RL) prob-lem. 2.1. Markov Decision Process Let us formulate the procedure of drawing a stroke as a Markov Decision Process (MDP) consisting of a tuple ( S , A ,p I ,p T ,R ) , where S is a set of continuous states, A is a set of continuous actions, p I is the probability-density density from the current state s  X  S to next state s 0  X  S when taking action a  X  A , R ( s ,a, s 0 ) is an immediate re-ward function for the transition from s to s 0 by taking ac-tion a .
 Let  X  ( a | s ;  X  ) be a stochastic policy with parameter  X  , which represents the conditional probability den-sity of taking action a given state s . Let h = ( s 1 ,a 1 ,..., s T ,a T , s T +1 ) be a trajectory of length T . Then the return (i.e., the discounted sum of future rewards) along h is expressed as where  X   X  [0 , 1) is the discount factor for the future reward. The expected return for parameter  X  is defined by where The goal of RL is to find the optimal policy parameter  X   X  that maximizes the expected return J (  X  ) : 2.2. Policy Gradient Method We use a policy gradient algorithm (Williams, 1992) to solve the above RL problem. That is, the policy parame-ter  X  is updated via gradient ascent as where  X  is a learning rate. The gradient  X   X  J (  X  ) is given by  X   X  J (  X  ) = where we used the so-called log trick :  X   X  p ( h |  X  ) = tation is approximated by the empirical average:  X   X  b J (  X  ) = where { h ( n ) } N n =1 are N episodic samples with T steps and h Let us employ the Gaussian policy function with parameter  X  = (  X  &gt; , X  ) &gt; , where  X  is the mean vector and  X  is the standard deviation: Then the derivatives of the expected return J (  X  ) with re-spect to the parameter  X  are given as Consequently, the policy gradients  X   X  b J (  X  ) are expressed as  X   X  J (  X  ) =  X   X  J (  X  ) = where b is a baseline for reducing the variance of gradient estimates. The optimal baseline that minimizes the vari-ance of the gradient estimate is given as follows (Peters &amp; Schaal, 2006): b  X  = argmin Finally, the policy parameter  X  = (  X  &gt; , X  ) &gt; is updated as In this section, we give a specific design of state space S , action space A , and immediate reward function R ( s ,a, s for our brush agent to learn smooth and natural strokes. For this purpose, we first extract the boundary of a given object and then calculate the medial axis M , as illustrated in Figure 2. 3.1. Design of Actions To generate elegant brush strokes, the brush agent should move inside the given boundaries properly. To this end, we consider four basic actions of the brush agent: movement of the brush, scaling up/down of the footprint, and rotation of the heading direction of the brush (see Figure 1(a)). Since properly covering the whole desired region is the most important issue, we treat the movement of the brush as the primary action (Action 1 ). The action a specifies the an-gle of the velocity vector of the agent relative to the medial axis. The action is determined by the Gaussian policy func-tion. In practical applications, the agent should be able to deal with arbitrary strokes in various scales. To achieve sta-ble performance in different scales, we adaptively change the velocity of the brush movement relative to the scale of the current footprint. The other actions (Actions 2 , 3 , and 4 ) are automatically optimized to satisfy the assump-tion that the tip of the agent should touch one side of the boundary; meanwhile, the bottom of the agent should tan-gent with the other side of the boundary. Otherwise, a new footprint will remain the same posture as the previous one, but just transit to a new position by Action 1 . 3.2. Design of States We use the global measurement (the pose configuration of a footprint under the global Cartesian coordinate) and the relative state (the brush agent X  X  pose and the locomotion information relative to the local surrounding environment). The relative state is calculated based on the global mea-surement values. Thus, both the global measurement and the relative state should be regarded as a state in terms of an MDP. However, for the calculating return and a policy, we use only the relative state, which allows the agent to learn a drawing policy that can be generalized to new shapes. Our relative state space design consists of two parts: Cur-rent surrounding shape and upcoming shape. More specif-ically, our state space is expressed by six features s = (  X , X ,d, X  1 , X  2 ,l ) &gt; (see Figures 2 and 3), where  X   X   X  (  X   X , X  ] : The angle of the velocity vector of the  X   X   X  (  X   X , X  ] : The heading direction of the brush agent  X  d  X  [  X  2 , 2] : The ratio of offset distance  X  (see Fig- X   X  i ( i = 1 , 2)  X  [0 , 1) :  X  1 provides the current sur- X  l  X  { 0 , 1 } : A binary label that indicates whether the 3.3. Design of Immediate Rewards We design the reward function so that the smoother the brush stroke is, the higher the reward is. For this purpose, we define the reward function as
R ( s t ,a t , s t +1 ) = That is, the immediate reward is zero when the brush is blocked by a boundary as f t = f t +1 or the brush is going backward to a region that has already been covered by pre-vious footprints f i ( i &lt; t + 1 ). |  X  1 ( t ) | + |  X  increases immediate rewards depending on the difficulty of the current shape measured by the curvature  X  i ( t ) of the medial axis.
 E location measures the quality of the location of the brush agent with respect to the medial axis, defined by E where  X  1 and  X  2 are weight parameters and W is the penalty. Since d contains information whether the agent goes over the boundary or not, as illustrated in Figure 3, the penalty W is added to E location when the agent goes over the boundary of the shape. When the brush agent is inside of the boundary, i.e., d  X  [  X  1 , 1] , E location only on the angle  X  t of the velocity vector.
 E posture measures the quality of the posture of the brush agent based on neighboring footprints, defined by where  X   X  t ,  X   X  t , and  X  d t are changes in angles  X  of the velocity vector, heading directions  X  , and ratios d of the offset distance, respectively. The notation  X  x t denotes the normalized squared changes between x t  X  1 and x t defined by  X  ,  X  2 , and  X  3 are weight parameters. We set the parameters at  X  1 =  X  2 = 0 . 5 , X  1 =  X  2 = 0 . 5 , and  X  1 =  X  2 =  X  1 / 3 . 3.4. Design of Training Sessions for Brush Agent Given an appropriately designed MDP, the final step is to design training sessions, which is also highly important to make the brush agent useful in practice.
 First of all, we propose to train the agent based on partial shapes, not the entire shapes. An advantage of this local training strategy is that various local shapes can be gen-erated from a single entire shape, which significantly in-creases the number and variation of training samples. An-other merit is that the generalization ability to new shapes can be enhanced, because even when the entire profile of a new shape is quite different from that of training data, the new shape may contain similar local shapes as illustrated in Figure 4(a).
 To provide a wide variety of local shapes to the agent as training data, we prepared an in-house stroke library. This library contains 80 digitized real single brush strokes that are commonly used in Oriental ink painting. See Fig-ure 4(c) for some examples. We extracted boundaries as the shape information and arranged them in a queue as training samples (see Figure 4(b)).
 In the training scheme, the initial position of the first episode is chosen to be the start point S of the medial axis (Chin et al., 1995), and the direction to move is cho-sen to be the goal point G as illustrated in Figure 4(b). We estimate the length of an episode, T from the se-lected shapes. In the first episode, the initial footprint is set around the start point of the shape. In the following episodes, the initial footprint is set as either the last foot-print in the previous episode or the footprint around the start point. It depends on whether the agent moves well or is blocked by the boundary. For each policy, we repeat N episodes to collect data H = [ h (1) ,h (2) , . . . ,h ( N ) h data H to calculate the gradient of the return,  X   X  J (  X  ) , and update the policy parameter M times to optimize the pol-icy.
 To ensure the continuity along the episodes, we design the initial location of the agent as shown in Figure 4(b): In the first episode, the initial location of the agent is set on the medial axis, with its tip Q pointing to the end corner. In the next episode, the initial location is set to the location of the last footprint in the previous episode.
 There are two exceptional situations where the new episode X  X  initial location goes back to the initial location of the previous episode: The first situation is that the length of the up-coming track is much less than the length of the trajectory in T steps. The other situation is that the agent is blocked by the footprints generated by the previ-ous episode. In this section, we report experimental results. 4.1. Setup We train the policy of the brush agent on the shape shown in Figure 4(c) through our training strategy introduced in Section 3.4. The parameter of the initial policy is set as  X  = (  X  &gt; , X  ) &gt; = (0 , 0 , 0 , 0 , 0 , 0 , 2) &gt; vious domain knowledge. The agent collects N = 300 episodic samples with trajectory length T = 32 . The dis-count factor is set to  X  = 0 . 99 . The learning rate  X  is set as 0 . 1 / k X   X  J  X  k . We investigate the average return over 10 trials as functions of policy-update iterations. The return at each trial is computed over 300 training episode samples. 4.2. Results The average return along the policy iteration is shown in Figure 5. The graph shows that the average return sharpy increases in an early stage and then it keeps stable after 35 iterations. Figure 6 shows examples of rendering and brush trajectory results in the policy training process. Table 1 shows the performance of policies learned by our RL method and the DP method (Xie et al., 2011) on an Intel Core i7 2.70 GHz. According to the discussion in Xie et al. (2011), the performance of the DP method depends on the setup of the parameter in the energy function and the number of candidates in the DP search space. However, it is hard to manually find the optimal parameters in practice. In Table 1, we list results obtained by the DP method with changing the number of candidates in each step of the DP search space. The results of the expected return and the execution time are significantly different depending on the number of candidates. In the DP method, the best value of the return is 26 . 27 when the number of candidates is set to 180 , but this is computationally very expensive ( 2 . 08  X  10 seconds). Our RL method outperforms the best DP setup, with much less computation time.
 We further apply our trained policy to more realistic shapes shown in Figure 7, which were not included in the train-ing samples. We can observe that the well-trained policy can produce smooth and natural brush strokes in various shapes. We therefore conclude that our RL method is use-ful in a practical environment.
 Finally, we apply our brush agent to automatic photo con-version into an oriental ink style. We manually drew con-tours on a photo and let the agent automatically fill the shapes with strokes. The results are shown in Figure 8 and Figure 9, which we think are of good quality. In this section, we briefly review current state-of-the-art in generating brush drawing, which have two approaches: Physics-based painting and stroke-based rendering . 5.1. Physics-Based Painting This approach aims at reproducing a real painting process and giving users intuitive and natural feeling when hold-ing a mouse or a pen-like device. Several previous works have dealt with modeling the brush shapes, its dynamics, and its interaction with the paper, and simulating the ink dispersion and absorption by the paper.
 Among the first stream, early representative works include the hairy brushes (Strassmann, 1986) and the physics-based models (Saito &amp; Nakajima, 1999; Chu &amp; Tai, 2004; Chu et al., 2010). For interactive use, these virtual brushes are convenient to draw various styles of strokes. Despite the extensive research literature, controlling automatically a virtual brush with six degrees of freedom X  X hree for the Cartesian coordinates and three for their angular orienta-tion (pitch, roll, and yaw) X  X n addition to the dynamics of the tufts is complex and existing physics-based models are in fact simplifications of the real process.
 On the other hand, while the digital painting tools pro-vide expert users a professional environment with a can-vas, brushes, mixing palettes, and a multitude of color options, non-expert users often prefer simplified environ-ments where paintings can be generated with minimum in-teraction and painting expertise.
 Another major problem is that the computational cost is usually very high for satisfactory visual effects to human eyes. Some of them rely on GPU acceleration for satisfac-tory speed performance (Chu et al., 2010). Also, due to over-simplification, none of these methods has been able to simulate certain special brush strokes such as those impasto ones created with paint knives. 5.2. Stroke-Based Rendering In many situations, it is desirable to automatically convert real images into ink paintings, especially when the user has no painting expertise and is interested only in the painting results rather than in the painting process.
 The skeleton stroke method (Hsu &amp; Lee, 1994) generates brush strokes from the 2D paths given by either user inter-action or automatic extraction from real images. However, the main difficulty is how to specify and vary the width of the strokes along the path as well as the texture of the strokes. One of the solutions is to specify the stroke back-bone (Guo &amp; Kunii, 2003) manually by a user. A limitation of such methods is that setting the values on each control point is time-consuming.
 Contour-driven methods (Xie et al., 2011) can successfully generate strokes in desired shapes. However, there are sev-eral strict constraints: (I) When cutting the boundary re-gion into slices at each step, the cross-sections should not intersect together. (II) A limited number of footprint candi-dates are only available for making the decision of moving to the next step. Although the second assumption ensures the same stride length of the agent at each step as well as speeding up the algorithm X  X  execution time, states are typi-cally modeled as discrete variables. This causes the result-ing brush path not to be optimized well.
 Our proposed approach belongs to the category of stroke-based rendering, with highly automatic and flexible stroke generation ability. In this paper, we applied reinforcement learning to oriental ink painting, allowing automatic generation of smooth and natural strokes in arbitrary shapes. Our contributions in-clude careful designs of actions, states, immediate rewards, and training sessions. One of the key ideas was to design the state space of the brush agent to be relative to its sur-rounding shape, which allows us to learn a general drawing policy independent of a specific entire shape. Another im-portant idea was to train the brush agent locally in the given shape. This contributed highly to enhancing the generaliza-tion ability to new shapes, because even when a new shape is quite different from training data as a whole, it contains similar local shapes.
 The experimental results demonstrated that our RL method gives better performance than the existing DP methods with much less computation time, and our RL agent can suc-cessfully draw new complex shapes with smooth and nat-ural brush strokes. Also, applications to automatic photo conversion into an oriental style was demonstrated to be promising.
 The authors would like to thank to the anonymous review-ers for their helpful comments. Ning Xie was supported by MEXT Scholarship, Hirotaka Hachiya was supported by the FIRST program, and Masashi Sugiyama was supported by MEXT KAKENHI 23120004.

