 In dealing with a usual regression task, we have a data matrix X = { x i } n i =1 in size n  X  d ,where n is the number of points (observations), and d is the dimension of each data vector. Correspondingly, we have a response vector Y = { y i } n i =1 in length n .A linear regression can be regarded as learning a coefficient vector w in length d and an offset constant b , such that
The ordinary least squares (OLS) learns the coefficient vector w by minimizing the residual squared loss where e is a vector of ones.
 However, OLS may not be accurate and robust enough. As a technique for improving OLS, ridge regression adds the L2 norm of the coefficient vector into the OLS objective function, hereby sets up an L2 constrained regression:
To obtain better prediction accuracy and int erpretation [1], Lasso (least absolute shrinkage and selection operator) has been developed, which in terms of objective func-tion substitutes the L2 norm of the coefficient vector in ridge regression for L1 norm: where the L1 norm of w can be calculated by summing up the absolute values of the resented by d i =1 | w j | p , is mentioned in [2, 1, 3, 4], Among different norms, when p  X  0 ,the || w || 0 vector w [4], and corresponds to selecting a subset of coefficients [1]. Generally speak-ing, the rationale of using a small p ,e.g p  X  1 is coefficient shrinkage and selection, but due to the computational difficulty, the cases of p&lt; 1 for regression has seldom been implemented in practice.

Other than constraining coefficients, a new class of regression algorithms aim to incorporate alternative loss functions. For instance, support vector regression [5], in-stead of employing the squared loss in OLS, lasso and ridge regression, suggests the  X  -insensitive loss (  X   X  0 ) for each point x i
The  X  -insensitive loss does not count any loss below  X  , that is, whenever the absolute by a zero value during total loss calculation.

This paper is motivated by seeking the set-up of p  X  0 in constraining regression coefficients, so as to select a good subset of coefficients and to obtain an easily in-terpretable regression model. Intrinsically the L0-constrained regression is companied with computational difficulties, but thanks to the study on L0-constrained classifica-tion [3, 6] and support vector regression, we can devise a type of regression algorithm to enforce L0-constraints upon coefficients and  X   X  insensitive loss for prediction, which can be efficiently approximated by successive linear programming.

This paper is organized as follows. In section 2, we explain our L0-constrained  X   X  insensitive regression and demonstrate how it relates to other works in literatures. And section 3 shows how to approximate the s olution through Successive Linearization Algorithm. In section 4, the properties of L0-constrained regression is studied through experimenting on the prostate cancer data. Section 6 intends to discuss the extension of L0 constraints to nonlinear regression, together with simulation. Section 7 presents summaries and concludes the paper with future works. This section shows the formulation of our L0 constrained  X   X  insensitive regression. It considers the  X   X  insensitive loss seen in support vector regression, with the L0 norm of coefficients in linear models taken into account too. These two parts can be integrated into a single objective function, which is expected to be simultaneously minimized:
In [6] the L0 constraints || w || 0 0 is approximated in the following way, where  X  is a positive tuning parameter and a larger  X  makes a closer approximation. And | w | converts the coefficients in w to their absolute values. Fig 1 shows how the || w || 0 0 is approximated, and also how the L0 norm is different from L1 or L2 criteria. approximated by
On the other hand, as a standard technique in support vector regression, the  X   X  insensitive loss can be converted into the following form through introducing nonnega-tive vectors and  X  ,.
Hereby the combination of these two parts has the following form In [6], L0 constrained linear support vector classification is proposed, and then succes-sive linearization algorithm (SLA) is used to approximate the solution.

Our L0 constrained regression adopts  X  -insensitive loss, rather than the hinge loss typical in classification, hereby leads to a different objective function. However, the converted objective form with linear constraints still makes the utilization of SLA pos-sible. We summarize the algorithm for our L0 constrained regression using SLA as follows.
 Algorithm: start with v i ,i =0 (e.g. via randomization), solve the following linear
After obtaining v i +1 , keep solving the above linear programming till the maximum number of iterations is reached or the following stopping condition is satisfied: where tol is a very small constant. We test the L0 constrained regression on the prostate data, which has been used in [1]. It has 97 samples, each is formed by eight features: log(cancer volume): lcavol ,log (prostate weight): lweight , age, log(benign prostatic hyperplasia amount): lbph ,semi-nal vesicle invasion: svi , log( capsular penetration): lcp , Gleanson score: gleason and percentage Gleason scores 4 or 5: pgg45 . The aim is to regress them into log(prostate specific antigen) (lpsa). Following the prepr ocessing procedures in [1], each feature is normalized to be with zero mean and unit standard deviation, and the responses are also set to be with zero mean.

Fig 2 shows how the coefficients change along with the tuning parameter  X  .As shown in the figure, when  X  is larger than 2, the lcp, gleanson and pgg45 starts fad-ing out of the regression model. Later the age and lbph features are dropped out too. Though our L0 regression has not been verified to possess exact piecewise solution paths as lasso does, the figure shows the shrinkage of coefficients as the tuning param-eter  X  increases. Useful features are likely ret ained for a long time, such as the lcavol, lweight and svi features. Particularly the lca vol feature presents a strong contribution to the regression model.
 Table 1 lists several related regression a pproaches together for this prostate task. Both the least square and linear support vector regression build the regression model upon all of the eight features. The L0 regression exerting a small constraint (  X  =1 ) on coefficients also leads to a model using all features. The least square regression undoubtedly minimizes the mean squared error, but support vector regression and L0 regression win with smaller absolute errors since by setup they intend to minimize absolute errors. As shown in the lower part of the table, Lasso following the parameter setting in [1] selects three features (lcavol, lweight,svi). Same features are selected by our L0 regression approach, with different coefficients and slightly better errors. It has been believed that linear regression models might not suffice for tasks character-ized with nonlinearity. Our L0 constrained regression mentioned above is developed for linear models, by obtaining an explicit coefficient vector w for data points X in the in-put space. However, it can also be extended to the case of nonlinear regression. Inspired by kernel methods, here we present a type of L0 constrained nonlinear regression.
Assuming the (implicit) data representation now is X = {  X  ( x i ) } n i =1 ,where  X  (  X  ) comprises a nonlinear mapping. Explicitly we have a kernel matrix induced from the inner product of data matrix, that is, K = XX T . Upon building a regression model Xw  X  b e , we assume the coefficient vector w comes from a linear co mbination of data points, that is, where  X  is vector in length n . Hereby the linear model is equivalent to Xw  X  b e = K  X   X  b e .

Hereby we can propose the following L0 constrained regression,
The constraints now affect  X  rather than w . The switch of consideration is because w in feature space may be too long to be explicitly emulated, hereby controllably en-forcing entries in w to be zero can hardly be done. Instead, restricting the L0 norm of  X  may lead to a small set of points that are then linearly combined into w , hereby achieves the effect of introducing a compact model. This approach is also suggested in [7, 8], though they adopt L1 restriction on  X  .

Similarly, this L0-constrained nonlinear regression can be further rewritten into the following form, which can also be approximated by SLA too.:
To verify our new nonlinear L0-constrained regression, we apply it to a synthetical data set originated from [7]. The response is a nonlinear function of the one-dimensional data point x : y i =cos( x )  X  (sin(5 x )+sin(4 x ))+1+  X  X  ,where  X  is normally distributed random variable, with standard deviation  X  =0 . 01 . The kernel used is Gaussian RBF, with  X  =5 . Figure 3 shows the results by support vector regression and nonlinear L0 regression. Both build a nonlinear regression model by only utilizing a portion of data points as highlighted by square symbols. Our nonlinear L0 regression has the advantage of leading to a smaller number of non-zero  X  coefficients (They are called support vectors in SVR).

Table 2 tests this synthetic tasks under a se ries of parameter settings for 20 repeats, which further confirms the compactness of L0 regression models. In addition, it can be noticed that the linear kernel based ridge regression performs significantly worse than all of the three RBF kernel-based nonlinear regression methods. As the  X  decreases (or equivalently increasing the C in SVR), the MSE drops gradually. When the SVR con-verges to a stable model of 6.6 support vectors on average, the L0 still keeps refining its regression models. The RBF kernel ridge regression also keeps improving performance, but it unlike the SVR or L0 regression usually cannot result in a sparse model. This paper presents a new regression algorithm, which considers both the  X   X  insensitive loss and the number of regression coefficients. The integrated objective can be approx-imated by successive linearization algorithm. Experimental investigation shows it has the ability of selecting a small set of coefficients, so as to reduce the model size and often bring accuracy gain.

To make our L0 more generally applicable to nonlinear regression, we introduce the kernel trick into the L0 set-up, with deduction it also leads to a task to be handled by SLA. Experiments show that the nonlinear L0 regression usually gives a smaller nonlinear model than support vector regression. When the parameter  X  is small, it can even achieve more accurate models.

The linear programming is the main step of SLA. Although current computers can handle a moderately large size linear programming, and the number of succession steps in our SLA are often less than ten, more efforts should be put for quicker and more stable numerical methods.

