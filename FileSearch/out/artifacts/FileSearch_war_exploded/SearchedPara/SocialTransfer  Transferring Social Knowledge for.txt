 An essential component of building a successful crowdsourc-ing market is effective task matching , which matches a given task to the right crowdworkers. In order to provide high-quality task matching, crowdsourcing systems rely on past task-solving activities of crowdworkers. However, the aver-age number of past activities of crowdworkers in most crowd-sourcing systems is very small. We call the workers who have only solved a small number of tasks cold-start crowdwork-ers . We observe that most of the workers in crowdsourc-ing systems are cold-start crowdworkers, and crowdsourcing systems actually enjoy great benefits from cold-start crowd-workers. However, the problem of task matching with the presence of many cold-start crowdworkers has not been well studied. We propose a new approach to address this issue. Our main idea, motivated by the prevalence of online social networks, is to transfer the knowledge about crowdworkers in their social networks to crowdsourcing systems for task matching. We propose a SocialTransfer model for cold-start crowdsourcing, which not only infers the expertise of warm-start crowdworkers from their past activities, but also trans-fers the expertise knowledge to cold-start crowdworkers via social connections. We evaluate the SocialTransfer model on the well-known crowdsourcing system Quora, using knowl-edge from the popular social network Twitter. Experimen-tal results show that, by transferring social knowledge, our method achieves significant improvements over the state-of-the-art methods.
 H.2.8 [ Database Management ]: Database Applications  X 
Part of the work was done when the author was an intern at Microsoft Research Asia Algorithms, Design, Experiments Crowdsourcing, Social Network
The benefits of crowdsourcing have been widely recog-nized today [26, 27, 1] and we have witnessed many success-ful systems such as Amazon Mechanical Turk [21] for generic tasks, LabelMe [11] for image annotation, and Quora [28] for question answering. As crowdsourcing systems are gaining more and more popularity, their rapid growth has also made it increasingly difficult for crowdworkers to find tasks that are suitable for them to perform in the market.

In this paper, we study the problem of task matching query in crowdsourcing systems, which is to find the right crowd-workers to solve a given task. In order to provide high-quality task matching, crowdsourcing systems rely on histo-ries of past task-solving activities of crowdworkers. Howev-er, when new crowdworkers join a system, no prior activity can be observed. In fact, a vast majority of existing crowd-workers in a real-life crowdsourcing system, including many that have joined the system for a relatively long period of time, do not have sufficient prior task-solving records for inferring high-quality task matching. To give an example, we aggregate the question-solving activities of the Quora crowdworkers in Figure 1(a), from which we can observe that the participation in the question-answering activities of most crowdworkers fall into the Long Tail part of the power-law curve, i.e., the majority of the crowdworkers have performed less than 10 tasks. These crowdworkers who have only performed a small number of tasks are called cold-start crowdworkers .

Interestingly, the crowdsourcing market also enjoys the great benefits brought by cold-start crowdworkers. We have observed the excellent performance of cold-start crowdwork-ers on an extensive number of tasks. We demonstrate the task performance of the Quora crowdworkers with respect to the number of task-solving activities in Figure 1(b), where we use the thumb-ups/downs of users in Quora to measure the quality of the task performance of crowdworkers. We can see that a significant number of cold-start crowdwork-ers attained high scores for their tasks. One explanation for this phenomenon is that cold-start workers usually par-ticipate in tasks that they feel confident to solve. Although cold-start crowdworkers can contribute greatly to the crowd-sourcing market, automatical and quality task matching for such crowdworkers is challenging in real-life crowdsourcing systems due to the lack of information to analyze them. Social Knowledge for Cold-Start Crowdsourcing. The classic algorithms [31, 39, 32, 18, 25] mainly focus on task matching for warm-start crowdworkers who have solved many tasks. However, the task matching for cold-start crowdwork-ers still remains a challenging problem.

To attain high-quality task matching, we need to find suffi-cient knowledge for cold-start crowdworkers, but knowledge about them in the crowdsourcing system is scarce. Fortu-nately, in this big data era, we may find other sources to obtain the required knowledge. In particular, we propose a novel idea of transferring knowledge from online social net-works for crowdsourcing task matching. Indeed, with the prevalence of online social networks today, it is not diffi-cult to find the activities of crowdworkers, including both warm-start and cold-start crowdworkers, as well as their con-nections, in various online social networks (e.g., Facebook, Twitter, etc.). The crowdworkers broadcast messages, post blogs, and follow the activities of other users in social net-works. Thus, a vast amount of data related to crowdworkers, such as workers X  tweet contents and worker-to-worker so-cial connection, can be obtained. For example, we observe that more than one third of the crowdworkers in Quora has a twitter account and crowdworkers are followed by other crowdworkers.

How can we transfer rich information hidden in online social networks to achieve high-quality task matching in crowdsourcing systems?
We focus on the social activities of the Quora crowdwork-ers in a popular social networking site, Twitter, and pro-pose a SocialTransfer graph in order to transfer the social knowledge of both cold-start and warm-start crowdworkers to solve task matching. We extract the Twitter accounts of the crowdworkers from their profiles in the Quora sys-tem. We mainly utilize two types of social knowledge of the crowdworkers, namely worker X  X  tweets and worker-to-worker social connections. We model the social interests of the crowdworkers from their tweets and infer the similarity of the crowdworkers by their worker-to-worker connections. We illustrate our main idea using an example of a Social-Transfer graph in Figure 2. We extracted four task match-ing queries Q = { q 1 ,q 2 ,q 3 ,q 4 } and five crowdworkers V = { v 1 ,v 2 ,...,v 5 } from Quora. First, the past task-solving activities of the crowdworkers on the three task matching queries are indicated by the edges between queries Q and
Figure 2: An Example of a SocialTransfer Graph crowdworkers V in the rectangle on the left in Figure 2. The quality scores of the answers given by the crowdwork-ers for each query in Q are shown in Table S .Ascoreof1 indicates a positive performance of the crowdworker on the task, while a score of -1 indicates a negative performance of the crowdworker on the task. The question mark means that the crowdworker did not participate in the task. The quali-ty score is based on the thumb-ups and thumb-downs of the users who posted the tasks or viewed the answers in Quora. Second, we show the social following relations (in Twitter) of the crowdworkers in the rectangle on the right in Figure 2. Each crowdworker in Twitter is associated with the work-er X  X  tweets and social relations. A crowdworker can follow other crowdworkers or other Twitter users. The process of building a SocialTransfer graph is to construct a heteroge-nous graph by integrating the social network graph of the crowdworkers into the task assignment bipartite graph.
The main idea of using a SocialTransfer graph to address the problem of task matching for cold-start crowdworkers is as follows. First, for cold-start crowdworkers with sufficient tweet information, we infer the expertise of the crowdworker-s for different types of tasks by transferring the knowledge of the tweets. Second, for cold-start crowdworkers with limit-ed tweet information, we infer their expertise by transferring the social knowledge from some warm-start crowdworkers. For example, v 1 is a cold-start crowdworker in Figure 2. A-part from inferring the expertise of v 1 from his tweets, we can also transfer the knowledge from v 3 , who is a warm-start crowdworker, if v 1 and v 3 share many common followings in Twitter.
 Task Matching for Cold-Start Crowdsourcing. There exists some work addressing the cold-start problem in user-item recommendation systems [23, 13, 24, 14, 40, 34, 10, 15]. However, most of them are not applicable in addressing the problem of task matching in cold-start crowdsourcing. Even though matching a task to the right crowdworker is analogous to recommending an item to a user, there are fun-damental differences between them. First, the existing work incorporates the social relations of users to recommend the best item among a set of items to a user. In our work, we map a question to the right crowdworker but there is no relation among the questions; thus, the existing cold-start recommendation techniques cannot be applied to solve our problem. Second, existing recommendation techniques focus on recommending existing items to users, while task match-ing in crowdsourcing aims to find the right crowdworkers to solve new tasks.

The main contributions of our work are summarized as follows:
The rest of the paper is organized as follows. Section 2 introduces the notations and formulates the problem. We propose a novel model that transfers social knowledge for cold-start crowdsourcing in Section 3. We report the exper-imental results in Section 4 and survey the related work in Section 5. We conclude the paper in Section 6.
We first define some frequently used notations and then formulate the problem of task matching in crowdsourcing systems. The summary of the notation is given in Table 1.
Definition 1. (Task Matching Queries Q )Theset Q contains a collection of existing processed task matching queries in a crowdsourcing system. We denote by Q the set { q 1 , q ... , q M } ,where q i is a task matching query and M is the number of queries. For each query q i , we denote its related task description by w q i , which is a bag of words. For example, the description of a task matching query in Quora can be a posted question, while the description of a query in Amazon Mechanical Turk can be several instruc-tions.

Definition 2. (Crowdworkers V )Theset V consists of all the crowdworkers in a crowdsourcing system. We denote by V the set { v 1 ,v 2 ,...,v N } ,where v i is a crowdworker and N is the number of crowdworkers.

Definition 3. (Task Matching Graph G Q )Thebipartite task matching graph, G Q =( Q V,A ) , stores the existing matchings of queries Q and crowdworkers V in a crowd-sourcing system. The set of vertices, Q V ,istheunionof queries and crowdworkers. The set of edges, A ,isthesetof matchings between queries in Q and crowdworkers in V .
For example, the graph G Q for the matchings between queries Q = { q 1 , q 2 , q 3 , q 4 } and crowdworkers V = ... , v 5 } isgiveninFigure2. Theedge A ( q i ,v j ) =1ifthe task in query q i is assigned to crowdworker v j , otherwise A Group Notation Notation Description Data Model
Definition 4. (Feedback Score S )Thescores S are the feedbacks given by users and are used for measuring the quality of the tasks performed by crowdworkers. For each task matching ( q i ,v j ) in graph G Q , there is a feedback score S
The feedback score S can be the satisfactory rate of the task sponsor (i.e. S ( q i ,v j )  X  [0 , 1]) in Amazon Mechanical Turk or the thumb-ups/downs for the answers by crowd-workers in Quora (i.e. S ( q i ,v j )  X  Z ).

Let F ( v i ) be the set of followings of a crowdworker v a social network, i.e., the user v i follows the set of users in F ( v i ) in the social network.

Definition 5. (Social Graph G V ) The social graph G V = ( V,B ) stores the activities of crowdworkers in a social net-work such as workers X  tweet content and worker-to-worker social connection. For each crowdworker v i  X  V ,wedenote its tweet content by w v i , which is a bag of words. For each edge ( v i ,v j )  X  B , it indicates that crowdworkers v i share some common followings in the social network, i.e., F ( v i ) F ( v j ) =  X  . We set the weight of the edge ( v between v i and v j to their total followings in the social net-work.

Note that the weight of the edges in B is within the range [0 , 1].

Now, we introduce a SocialTransfer graph that transfers the social knowledge of crowdworkers in a social network to address the cold-start crowdsourcing problem, in order to improve the quality of task matching.
 Definition 6. (SocialTransfer Graph G ST ) The Social-Transfer graph G ST =( Q V,A B ) models the activities of crowdworkers in both the task matching graph G Q and the social graph G V . The set of vertices in G ST is the union of the existing task matching queries Q and crowdworkers V . The set of edges in G ST contains both the set of exist-ing matchings modeled in A and the set of pairwise common followings of crowdworkers modeled in B .

Note that G ST is not (and need not be) a tripartite graph, though we show a tripartite graph for the example in Fig-ure 2 for simplicity of discussion there. We now define the problem as follows.

Problem 1. (Task Matching Query in Cold-Start Crowd-sourcing) Consider a crowdsourcing system with many cold-start crowdworkers and a SocialTransfer graph G ST = ( Q V , A B ) . Given a new task matching query q , find the crowd-workers with high predicted feedback score to solve the given task.
In this section, we propose our model, STC, to tackle the problem of task matching in cold-start crowdsourcing. We first introduce the background of task matching in crowd-sourcing and present the idea of transferring social knowl-edge for cold-start crowdsourcing. Then, we summarize our proposed STC model and devise an effective inference al-gorithm to build STC model. Finally, we present a task matching algorithm in crowdsourcing.
We first give the definitions of latent query factor and latent crowdworker factor for task matching queries crowdworkers el for the already processed tasks based on latent query fac-tor and latent crowdworker factor.

Definition 7. (Latent Query Factor) Given the dimen-sion of latent space K , the latent factor of the queries in crowdsourcing systems is denoted by a K-dimensional vec-tor of real values. For each task matching query q i  X  Q , we denote by  X  X  X  q =
In this paper, we consider that the prior distribution of the latent query factor is based on the latent topic of the task description w q . We employ a well-known latent Dirichlet allocation (LDA) [2] to discover the latent topic of the task description, for which we choose the normal distribution for the prior distribution of latent query factor, given by where N (  X  ) is a multivariate normal distribution and  X  the latent topic of question q i . We consider that  X  q i mean and  X   X  1 Q is standard deviation.

Definition 8. (Latent Crowdworker Factor)The latent fac-tor of the crowdworkers in crowdsourcing systems is denoted by a K-dimensional vector of real values. For each crowd-worker v j  X  V ,wedenoteby tor, which is given by
We also choose the normal distribution as the prior for latent crowdworker factor. For the crowdworkers with ac-tivities in social networks, the prior distribution of them is given by where  X  v j is the latent topic of v j  X  X  tweets and  X   X  1 dard deviation. For other crowdworkers without activities, the prior distribution of them is given by
Using the definitions of latent query factor and latent crowdworker factor, we introduce the score generative model for processed tasks below.

Definition 9. (Score Generative Model)Given latent query factor core on processed tasks is given by where N (  X  ) is a multivariate normal distribution with mean  X  X  X  Q T  X  X  X  V and standard deviation  X   X  1
For the score of each processed task ( q i ,v j ), we consider that its feedback score is generated by
We now justify the score generative model as follows: We generate the latent query factor by the latent topic of the task description and the latent crowdworker factor by the latent topic of workers X  tweets. Thus, we consider that the latent crowdworker factor is the topical expertise of the crowdworkers. Therefore, the score of the processed task is proportional to the dot-product of latent query factor and latent crowdworker factor.
We now introduce the idea of transferring social knowl-edge to improve the inference of latent crowdworker factor. We mainly utilize two types of social knowledge, workers X  tweets and worker-to-worker social connections.

We consider that the latent topic of workers X  tweets rep-resents the topical interests of the crowdworkers. For the cold-start crowdworkers, it is difficult to infer the laten-t crowdworker factor from their few task-solving activities in crowdsourcing systems. Thus, we transfer the discov-ered topical interests as an external source to improve the inference quality of the latent factor of the cold-start crowd-workers by Formula 1.

We also notice that the crowdworkers with similar topical interests have a lot of common following users or entities in social networks. Therefore, we utilize the common followings of the crowdworkers to regularize the inference of topical interests.

Consider two crowdworkers v i and v j in a social net-work. If both of them follow a number of common follow-ings (i.e., the weight of the edge ( v i ,v j ), B ( v i ,v social graph G V is large), then their topic interests are sim-propose a prior distribution for the topic interests of crowd-workers with social regularization, which is a product of the Dirichlet distribution and the Normal distributions. The prior distribution for the topic interests of crowdworker v is given by where the product of normal distributions of the topic inter-ests of crowdworker v i and other crowdworkers v j are used for regularization. Figure 3: Graphical Representation of STC Mod-el: grey nodes indicate observed variable and green nodes mean the prior; the dashed nodes are the vari-able of interest, while other nodes represent latent variable In this subsection, we present the generative process for STC model. The graphical representation of STC is given in Figure 3.

We notice that the contents of task matching queries and tweets are essentially represented by different kinds of lan-guages. The study [8] shows that the words in task matching queries are formal, while the words in tweets are short, infor-mal and abbreviated vocabularies. Particularly, tweets and task matching queries often have different words to describe the same topic. Therefore, we devise a dual language mod-el for task matching queries and tweets based on different topic-word distributions  X  q and  X  v , respectively.
Referring to Figure 3, the generative process for the words in questions and tweets, latent query factor worker factor tivities S in STC is described as follows. 1. For each task matching query q : 2. For each crowdworker v : 3. For each task matching ( q,v ) in the processed task:
In this section, we propose an inference algorithm that infers the latent query factor  X  X  X  V , topic-word distributions  X  q and  X  v in STC.
Based on the generative process of STC, the joint distri-bution over feedback score S , topic proportions  X , latent query factor factorized, which is given by where p ( S |
We solve the inference problem, by finding a maximum a posterior (MAP) configuration of the latent query factor  X  X  X  Q and latent crowdworker factor feedback score S , and the words in queries and tweets W . That is, we aim to find =argmax  X  X  X 
Maximization a posterior configuration is equivalent to maximizing the complete log likelihood
L =  X   X  V where we omit the constant terms and set the prior param-eters  X  to a vector of 1. Algorithm 1 Task Matching Query Processing Algorithm Input: A task matching query q , word distribution  X  q and crowdworkers V Output: A ranking of crowdworkers V 1: Set  X  q  X  Uniform distribution 2: for t :1  X   X  max do 3: for each word w qj  X  w q do 4: for k :1  X  K do 5: Estimate variational parameter  X  qj,k  X   X  q  X  q k,w qj 7: Estimate the latent query factor q  X  N (  X  q , X   X  1 Q ) 8: Rank crowdworkers based on the relevance 9: return A ranking of crowdworkers V
Then, we infer the latent crowdworker factor query factor queries  X  and word distributions  X  V ,  X  Q . We estimate these parameters by taking the derivative of the complete log like-lihood L in Equation 5 and set it to zero.

We first report the inference of latent query factor latent crowdworker factor  X  X  X   X  X  X  where S q i is a diagonal matrix of the feedback scores for the workers on query q i and S v j is also a diagonal matrix of the feedback scores for all tasks done by worker v j .
We now justify the estimation of latent crowdworker factor and latent query factor as follows. The inference of latent crowdworker factor from the tweets  X  v and the latent factor of matched queries  X  X  X  QS v for worker v . Similarly, the inference of latent query factor the latent factor of the crowdworkers worked for this query  X  X  X  VS q .

We then present the inference for the topical interests of crowdworkers  X  V , latent topics of the task matching queries  X  , word distributions  X  V and  X  Q , respectively. Unfortu-nately, we find that it is difficult to directly take the deriva-tive for the complete log likelihood L with respect to  X  V and  X  Q , due to the decoupling between word distributions  X 
V ,  X  Q and topic assignment Z . To tackle this problem, we introduce a new variational parameter  X  for topic assign-ment Z and derive a lower bound, denoted by L .Wethen estimate the parameters  X  Q ,  X  V , X ,  X  V and  X  Q on L .
We derive the lower bound L (  X  v i ) with respect to the topical interests of crowdworkers  X  v i by Jensen X  X  Inequality. The derivation for the lower bound L (  X  q j )withrespectto the latent topic of queries  X  q j is similar to the derivation of L (  X  v i ). The derivation of L (  X  v i )isgivenby L
We estimate the parameters  X  and  X  V by taking the deriva-tive of L with respect to them. Therefore, we report the inference of the parameters  X  v i ,j,k and  X  V k,w by We estimate the topical interests of crowdworkers  X  V and the latent topics of task matching queries  X  Q by using the root finding algorithm in numerical optimization tools 1 .
We propose a task matching query processing algorithm based on our STC model in Algorithm 1. That is, given a new task matching query q , we aim to find the right crowd-workers to solve this task. Algorithm 1 first estimates the latent topic and topic assignments of query q alternatively. After that, Algorithm 1 samples the latent query factor from its latent topic  X  q . Finally, we choose the crowdwork-ers based on the relevance between the latent crowdworker factor
The main goal of this experimental evaluation is to vali-date the effectiveness of our proposed STC model. To show its competitive performance, STC is compared with other four state-of-the-art approaches for task matching in crowd-sourcing systems, such as Vector Space Model (VSM) [31], AuthorityRank [3], Dual Role Model (DRM) [31] and Topic Sensitive Probabilistic Model (TSPM) [39]. We implement-ed our algorithm in C++ and tested on machines with Lin-ux OS Intel(R) Core(TM2) Quad CPU 2.66Hz, and 32GB RAM.
We first collect the data from a popular crowdsourcing system, Quora. Quora is a question-and-answer website where questions are posted and answered by its community of crowdworkers. Quora was launched to the public in June, 2010 and has become very successful in just a few years. We first crawled the questions posted between September 2012 and August 2013, and then crawled all the workers who answered these questions. In total, we collect 444,138 questions, 95,915 crowdworkers, 887,771 answers. We then collect the data of the social activities of all 95,915 users in http://www.gnu.org/software/gsl/ Twitter, which are users X  tweets and following relationship. In total, we collect 29 million following users and 20GB of tweets of the crowdworkers in Twitter.

We first split the already answered questions in Quora in-to a training dataset and a testing dataset. We split the answered questions into six groups: Q 1 , Q 2 , ... , Q 6 on the number of collected answers, i.e., group Q 1 contains the questions with at least one answer. For each group Q i we randomly sample 100 questions as testing dataset, denot-ed by Q i . In total, we have 600 testing questions. We then keep the remaining questions in groups Q 1 , Q 2 , ... , Q training datasets. Thus, we have generated a pair of training and testing datasets. In this experimental study, we gener-ate ten pairs of training and testing datasets to evaluate the performance of the algorithms. We take the average of the experimental results of these algorithms on the ten pairs of datasets. The summary of the datasets is given in Table 2.
We assess the performance of the task matching algo-rithms based on three measurements: Precision , Recall and Cold-Start Rate .

Precision. We employ two measurements Accu and Accu Top 1 to evaluate the performance of different algorithms to rank the crowdworkers who answered the questions. For each question, we consider that the crowdworker whose answer receives the highest thumb-ups is the best answerer. Both Accu and Accu Top 1 evaluate the ranking quality of the best answerer by different algorithms (i.e. whether the best an-swerer can be ranked on top), which are also widely used in [31, 39] for task matching.

Given a question q ,wedenoteby R q V the ranking of the crowdworkers who answered this question. We consider that |
R
V | is the number of the crowdworkers in the ranking R We denote by r q best the rank of the best answerer for question q by an algorithm, which is given by where Q is the set of testing questions. Accu illustrates the average ranking position of the best answerer by an algorith-m, where Accu = 1 (best) means the best answerer returned by the algorithm always ranks first while Accu =0means the opposite.

We also propose Accu Top 1 to validate whether the best answerer is ranked on top, which is given by
Recall. We employ the measurement Recall TopK to e-valuate the ranking quality for all crowdworkers in crowd-sourcing systems by an algorithm. Given a question q ,we denote by R q TopK the set of crowdworkers ranked on TopK by the algorithms. The measurement Recall TopK is given by
Cold-Start Rate. We also investigate the types of the crowdworkers returned by an algorithm (i.e. cold-start crowd-workers or warm-start crowdworkers). In this experimental study, we consider the crowdworker who answered less than  X  = 25 questions as cold-start crowdworkers. We propose the measurement Cold -Start Rate to illustrate the type of the crowdworkers ranked on top, which is given by Cold-Start Rate = where R q Top 1 is the set containing the crowdworker ranked the top.
We first compare the effectiveness of STC with the state-of-the-art methods on the six groups of testing questions. Next, we study the impact of various model parameters on STC model: dimension of latent space K ,socialregular-ization  X  B , and other model parameters  X  V and  X  Q ,respec-tively. Then, we illustrate the time cost of building our STC model. We compare STC with VSM, AuthorityRank, DRM and TSPM on six groups of queries and the results are presented in Figures 4(a) to 4(d).

We illustrate the Cold-Start Rate of the returned crowd-workers by all algorithms in Figure 4(a). The result shows that the rate of all the algorithms drops from Q 1 to Q 6 This is because the warm-start crowdworkers prefer solving common tasks to special tasks. We also find that the Cold-Start Rate of the crowdworkers found by STC is around 10% to 20% higher than other algorithms. The selection of the cold-start crowdworkers to solve the tasks is attributed to the effect of the knowledge transfer of STC.
 Now, we show the benefits of the knowledge transfer of STC for cold-start crowdworkers using the measurements Accu , Accu Top 1 and Recall TopK . We find that the perfor-mance of task matching can be greatly improved by STC.
Our STC has the excellent performance on both precision measurements: Accu and Accu Top 1 , for all the six groups of task matching queries. As shown in Figures 4(b) and 4(c), the performance of STC is 5% to 10% better than all the oth-er algorithms. We notice that there is a number of cold-start crowdworkers having excellent performance on an extensive number of tasks in crowdsourcing systems. Using STC, the latent factor of cold-start crowdworkers can be inferred by knowledge transfer. Unlike the other classical algorithms, the high-quality cold-start crowdworkers can also be select-ed by STC to solve the tasks. Therefore, the precision of task matching is substantially improved.

The STC also greatly improves the performance of task matching in terms of Recall TopK . Figure 4(d) shows that the recall of task matching is improved by 10% to 20% by STC. We find that the existing algorithms mostly select warm-start crowdworkers for solving the tasks. However, there is a significant number of the tasks that can be solved by cold-start crowdworkers. However, the existing algorithms do not select these cold-start crowdworkers who solve the tasks well. On the other hand, STC infers the latent factor of cold-start crowdworkers by knowledge transfer and can select both high-quality cold-start crowdworkers and warm-start crowdworkers to solve the tasks. Thus, the recall of the task matching is also improved.
We investigate the impact of various model parameters including dimension of latent space K , social regularization  X  , parameters  X  V and  X  Q for latent crowdworker factor and latent query factor, respectively.

Dimension of latent space K . We study the impact of dimension of latent space on the performance of STC by varying K from 10 to 50, which is illustrated in Figures 5(a) to 5(d). Figure 5(a) shows the Cold-Start Rate of the re-turned crowdworkers by STC increases by 5% to 10% and then becomes convergent with respect to the dimension of latent space. By increasing the dimension of latent space, more knowledge is transferred from warm-start crowdwork-ers to cold-start crowdworkers. Figures 5(b) and 5(c) illus-trate that the accuracy increases by 1% to 3% by varying the parameter K and then converges. We conclude that the setting K = 50 is good enough to represent the latent fac-tor of crowdworkers and queries. We notice that the recall increases significantly by 10% to 20% with respect to param-eter K in Figure 5(d). By transferring more knowledge to the cold-start crowdworkers, both cold-start crowdworkers and warm-start crowdworkers can be selected to solve the tasks such that the recall is greatly improved.
Social regularization  X  B . We investigate the impact of the social regularization  X  B on the performance of STC, which is illustrated in Figures 6(a) to 6(d). We vary the value of the regularization term  X  B from 0.01 to 1000. The Cold-Start Rate of the crowdworkers selected by STC first increases and then becomes convergent in Figure 6(a). The larger the parameter  X  B becomes, the more knowledge is transferred to the cold-start crowdworkers. Thus, more cold-start crowdworkers are selected for solving the tasks. The performance of STC is first improved but then declines with respect to  X  B on Accu , Accu Top 1 and Recall Top 100 in Fig-ures 6(b) to 6(d). The inference of latent crowdworker fac-tor by STC is based on both past task-solving activities in crowdsourcing systems and the activities of the crowdwork-ers in social networks. When the social regularization term  X 
B is small, the knowledge transfer improves the perfor-mance of inferring the latent crowdworker factor. When the value of social regularization term  X  B becomes large, the inference of latent crowdworker factor is dominated by the activities of the crowdworkers in social networks. Thus, the performance of STC declines. To balance the inference from both past task-solving activities and social activities, we set the value of  X  B as a new regularization term.

Impact of Parameters  X  V and  X  Q . We study the im-pact of parameters  X  V and  X  Q on the performance of STC by varying  X  V and  X  Q from 0.01 to 1000 and the results are presented in Figures 7(a) to 7(d). The Cold-Start Rate does not vary for the parameters  X  V and  X  Q and hence the result is omitted. We observe that the performance of STC declines with respect to the large value of both parameters in Figures 7(a) to 7(d), which is similar to the effect of pa-rameter  X  B . Therefore, we also set the value of  X  V and  X  as new regularization terms.

We remark that the overall performance of STC with three new regularization terms can also be improved by 5%, 5% and 10% on Accu , Accu Top 1 and Recall Top 100 , respectively.
We first study the running time of building our STC model on the convergence of model inference. We then investigate the time cost of model inference with respect to different di-mensions of latent space K . We set the convergence thresh-old to 10  X  5 for inferring STC model. Figure 8(a) shows that the inference of STC model with latent space K =10 converges after 20th iteration. Figure 8(b) illustrates that the time cost of inferring STC model scales linearly with respect to the dimensions of latent space K . Thus, it is ef-ficient to build STC model on real crowdsourcing platforms such as Quora. After inferring the STC model, the time cost for matching the right crowdworkers can be computed in real time.
In this section, we briefly review some related work on crowdsourcing, transfer learning in the literature.
Crowdsourcing. Crowdsourcing has been widely used to solve challenging problems by human intelligence in com-prehensive areas. Some successful applications that appear include CrowdDB [5] and CDAS [16].

Recently, the crowdsourcing techniques have been applied in several research areas such as database management, ma-chine learning and information retrieval. The crowdsourc-ing techniques on entity resolution were studied in [29, 30]. CrowdScreen [22] applied the crowdsourcing techniques in decision making. Guo et al. [7] studied the problem of finding maximum element in the crowdsourcing databas-es. In [4], Davidson et al proposed the top-k and group-by queries on crowdsourcing databases. Kaplan et al [9] aimed to select the right question for planing queries. Mar-cus et al [17] studied the count query with the crowd. Gao et al [6] proposed crowdsourcing based online algorithm to find the ground truth. Zhao et al [35, 36] proposed crowd-selection to find the right workers to answer the questions. The work [12, 33, 20] proposed the assignment algorithms for tagging based on crowdsourcing platform.

Transfer Learning. Transfer learning techniques tackle the problem of data sparsity in the target domain by trans-ferring supervised knowledge from other related domains, which achieve great success in a lot of applications such as classification, clustering.

Recently, Zhong et al. developed techniques [37, 38] that borrow the knowledge from the auxiliary historical activities of users in multiple networks to predict the user behavior in a target network. Mo et al. proposed cross-task crowdsourc-ing method [19] that transfers the knowledge cross different tasks. However, their approaches are difficult to be applied to task matching for cold-start crowdsourcing since the to-tal number of auxiliary crowdworker activities is limited. In this work, we enrich the profile of cold-start crowdworkers by external knowledge transfer.
We studied the problem of task matching for cold-start crowdsourcing by transferring knowledge from social net-works to crowdsourcing systems. We developed a Social-Transfer graph to capture the relations between tasks, crowd-workers and social networks. We then proposed a Social-Transfer model for cold-start Crowdsourcing called STC. The model not only infers the latent factor of warm-start crowdowrkers from their past task-solving activities, but al-so transfers the knowledge from warm-start crowdworkers to cold-start crowdworkers via social networks. We evaluated the performance of STC on the popular crowdsourcing sys-tem Quora for question-answering, by transferring knowl-edge from Twitter. Our results show that compared with four state-of-the-art methods for task matching in crowd-sourcing systems, STC selects more cold-start crowdworkers who are the best crowdworkers to solve the given tasks, and STC improves both the precision and recall of task match-ing. The results confirm that our approach effectively finds the high-quality cold-start as well as warm-start crowdwork-ers for crowdsourcing tasks.
 Acknowledgments. We thank the reviewers for giving us many constructive comments, with which we have sig-nificantly improved our paper. This research is supported in part by SHIAE Grant No. 8115048, MSRA Grant No. 6903555, and HKUST Grant No. FSGRF13EG22. [1] Y. Baba and H. Kashima. Statistical quality [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] M. Bouguessa, B. Dumoulin, and S. Wang. Identifying [4] S. B. Davidson, S. Khanna, T. Milo, and S. Roy. [5] M. J. Franklin, D. Kossmann, T. Kraska, S. Ramesh, [6] J. Gao, X. Liu, B. C. Ooi, H. Wang, and G. Chen. An [7] S. Guo, A. Parameswaran, and H. Garcia-Molina. So [8] L. Hong, G. Convertino, and E. H. Chi. Language [9] H. Kaplan, I. Lotosh, T. Milo, and S. Novgorodov. [10] Y. Koren. Factorization meets the neighborhood: a [11] Labelme. http://labelme.csail.mit.edu/ . [12] S. Lei, X. S. Yang, L. Mo, S. Maniu, and R. Cheng. [13] W.-J. Li and D.-Y. Yeung. Relation regularized matrix [14] J. Lin, K. Sugiyama, M.-Y. Kan, and T.-S. Chua. [15] N. N. Liu, X. Meng, C. Liu, and Q. Yang. Wisdom of [16] X. Liu, M. Lu, B. C. Ooi, Y. Shen, S. Wu, and [17] A. Marcus, D. Karger, S. Madden, R. Miller, and [18] D. Mimno and A. McCallum. Expertise modeling for [19] K. Mo, E. Zhong, and Q. Yang. Cross-task [20] L. Mo, R. Cheng, B. Kao, X. S. Yang, C. Ren, S. Lei, [21] Mturk. https://www.mturk.com/ . [22] A. G. Parameswaran, H. Garcia-Molina, H. Park, [23] S.-T. Park and W. Chu. Pairwise preference regression [24] S. Purushotham, Y. Liu, and C.-C. J. Kuo.
 [25] F. Riahi, Z. Zolaktaf, M. Shafiei, and E. Milios. [26] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get [27] Y. Tian and J. Zhu. Learning from crowds in the [28] G. Wang, K. Gill, M. Mohanlal, H. Zheng, and B. Y. [29] J. Wang, T. Kraska, M. J. Franklin, and J. Feng. [30] S. E. Whang, P. Lofgren, and H. Garcia-Molina. [31] F. Xu, Z. Ji, and B. Wang. Dual role model for [32] L.Yang,M.Qiu,S.Gottipati,F.Zhu,J.Jiang, [33] X. S. Yang, R. Cheng, L. Mo, B. Kao, and D. W. [34] H. Yin, B. Cui, J. Li, J. Yao, and C. Chen. [35] Z. Zhao, W. Ng, and Z. Zhang. Crowdseed: query [36] Z. Zhao, D. Yan, W. Ng, and S. Gao. A transfer [37] E. Zhong, W. Fan, J. Wang, L. Xiao, and Y. Li. [38] E. Zhong, W. Fan, Y. Zhu, and Q. Yang. Modeling the [39] G. Zhou, S. Lai, K. Liu, and J. Zhao. Topic-sensitive [40] K. Zhou, S.-H. Yang, and H. Zha. Functional matrix
