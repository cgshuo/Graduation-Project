 Sergey Kirshner skirshne@ics.uci.edu Sridevi Parise sparise@ics.uci.edu Padhraic Sm yth smyth@ics.uci.edu There are a num ber of real-w orld mac hine learning problems that can be characterized as follo ws: there are N objects of interest, where for eac h object we measure a num ber of features or attributes of the ob-jects but we do not necessarily kno w the corresp on-dence between the measuremen ts for di eren t objects. In this pap er we focus speci cally on the case where the feature values have been perm uted in a random manner. Table 1 sho ws a simple example of this type of perm utation problem. We would like to be able to learn the join t probabilit y densit y of the original data on the left given only the perm uted data and kno wl-edge of the type of perm utations that may have been applied to the data (e.g., cyclic shifts). Tw o questions naturally arise: (a) how hard is this type of learning problem in general? and (b) what kinds of algorithms can we use to solv e this problem in practice? In considering the rst problem, our intuition tells us that the \more di eren t" the features in the original (unp erm uted) table are then the \easier" the unscram-bling problem may be. For example, in Table 1, the distributions of eac h individual feature in the table on the left app ear quite di eren t from eac h other, so that one hop es that given enough data one could eventu-ally reco ver a mo del for the original data given only perm uted data. In Section 2 we mak e this notion of learnabilit y precise by introducing the notion of a Bayes-optimal perm utation error rate. In Section 3 we sho w that under certain conditions this error rate is upp er-b ounded by an appropriately de ned Bayes-optimal classi cation error rate, con rming the intu-ition that the abilit y to unmix the row-v alues should be related to the overlap of the column densities. In Section 4 we deriv e closed-form expressions for the per-mutation error rate for speci c parametric mo dels | nding for example that negativ e correlation among Gaussian columns can mak e the unmixing problem sig-ni can tly harder, while positiv e correlation can mak e it arbitrarily easy . In Section 5 we address the second question (ho w to sim ultaneously unlearn the mixing and estimate the original join t densit y) using an EM framew ork, and pro vide various exp erimen tal results to illustrate how these learning algorithms work. Con-clusions are presen ted in Section 6.
 Our interest in this \learning from perm uted data" problem is motiv ated by recen t work in applying ma-chine learning techniques to astronomical image data (Kirshner et al., 2003a). In this problem eac h im-age consists of three intensit y \blobs" that represen t a particular type of galactic structure of interest to astronomers. For eac h blob a vector of features can be extracted, suc h as mean intensit y, ellipticit y, and so forth. Astronomers can visually iden tify a cen tral \core" blob, and righ t and left \lob e" blobs in eac h image. However, in the training data the groups of fea-tures are not asso ciated with any lab els that iden tify whether they are from the cen ter, left, or righ t blob | this information is hidden and must be estimated from the data. This is a version of the perm utation prob-lem describ ed earlier. In prior work (Kirshner et al., 2003a) we dev elop ed an EM algorithm to solv e this im-age analysis problem and focused on domain-sp eci c asp ects of the astronom y application.
 Perm utation problems frequen tly occur in computer vision where, for example, features in the form of land-marks are calculated for an object of interest in an im-age (suc h as a face) but the features are not necessarily in corresp ondence across di eren t images (Gold et al., 1995). Similar problems also arise in language mo del-ing and information extraction, e.g., in learning mo dels documen ts, where di eren t text elds can occur in dif-feren t positions on the page (McCallum et al., 2000). A signi can t adv ance in recen t years is the realization that man y of these types of corresp ondence issues can be cast as mac hine learning problems, viewing the un-kno wn corresp ondences as hidden variables that can be estimated from the data using techniques suc h as Exp ectation-Maximization (EM), e.g., in vision (Frey &amp; Jojic, 2003). Muc h of this prior work tak es adv an-tage of domain-sp eci c information to help solv e the corresp ondence problem, e.g., the use of prior kno wl-edge of likely types of spatial deformations in images, or sequen tial constrain ts on text formation in informa-tion extraction. In con trast, in this pap er we focus on a more abstract theoretical characterization of learning from perm uted data. The primary novel con tribution of this pap er is the introduction and analysis of the notion of Bayes-optimal error rates for \unscram bling" the perm utations, pro viding a lower bound on the per-formance of any unsup ervised learning algorithm for this problem. 2.1. Notation vectors ~x i , with eac h vector ~x i taking values from the same d -dimensional set S. Thus, x has dimension c d and tak es values in the set S c .
 Example 1. Let eac h ~x i be a one-dimensional real-valued feature-v ector. In this case S is the real line (S = R ) with d = 1, eac h ~x i is a scalar, and S c = R c Example 2. Consider the case where A = f a ; b ; : : : ; z g is a set of letters in an alphab et, and let S = A R . In this case eac h feature-v ector tak es values as pairs of a letter and a real num ber, i.e. d = 2, and the space S c is 2 c -dimensional.
 We de ne p ( x ) as a probabilit y densit y (distribution) function over the set S c . For example, in Example 1 above p ( x ) could be a c -dimensional multiv ariate Gaussian densit y. 2.2. A Generativ e Mo del for Perm uted Data Our generativ e mo del consists of two parts: In the rst part we generate samples from p ( x ) in a standard manner | throughout this pap er we assume indep enden t and iden tically distributed random sam-ples. In this manner we can generate a data matrix of N rows and c columns, where eac h column has dimen-sion d .
 The second part of the generativ e mo del randomly ap-plies a perm utation to eac h row of the data matrix in the follo wing manner. Let P = f 1 ; : : : ; m g be a set of perm utations de ned on (1 ; : : : ; c ). For ex-ample, P could be the set of all c cyclic shifts, e.g., c = 3 ; 1 = (1 ; 2 ; 3), 2 = (2 ; 3 ; 1), and 3 = (3 ; 1 ; 2). For eac h row x of the data matrix a perm utation 2P is randomly selected according to a probabilit y distri-bution p ( ) over P . The comp onen ts of x are then perm uted according to the selected to obtain a per-muted vector taking values in the same set S c . The size of P , jPj , is denoted by m . Note that if all possible perm utations are allo wed then m = c !. Unless stated otherwise, in this pap er we will generally assume that all perm utations in P are equally likely, i.e. p ( j ) = 1 m ; j = 1 ; : : : ; m . 2.3. Probabilit y Densities for Perm uted Data It is useful to express the probabilit y densit y of a per-muted vector, call it q ( x ), as a function of (a) the densit y of the original data rows p ( x ), and (b) the distribution over perm utations p ( ). In the remainder of the pap er whenev er the sym bol q is used it is implic-itly assumed that the argumen t x has been perm uted. Note that q ( x ) can be expressed as a nite mixture over all m possible perm utations that could have led to the generation of x : where 1 j is the unique inverse perm utation for j . Example 3. Let c =3, and P = f 1 ; 2 ; 3 g be a set of cyclic shifts: 1 = (1 ; 2 ; 3), 2 = (2 ; 3 ; 1), and 3 could have been obtained from one of three possible perm utations, as re ected by the mixture mo del: q ( x ) = q ( ~x 1 ; ~x 2 ; ~x 3 j 1 ) p ( 1 ) An imp ortan t point is that p and q are not the same distribution although both are de ned over S c . Example 4. Let S = f 0 ; 1 g with c = 2. Let P = f 1 ; 2 g where 1 = (1 ; 2) and 2 = (2 ; 1). Let p ( x ) be as de ned in the Table 2. Assume p ( 1 ) = p ( 2 ) = 0 : 5. The resulting q distribution on perm uted vectors x is also listed in the Table 2. p 6 = q since, for example, 2.4. Inference and Learning There are two problems of direct interest. In the rst problem, we assume that p ( x ) is kno wn, and that the set of perm utations P and their probabilities p ( ) are also kno wn. Then, given a perm uted vector x , we can calculate using the mixture mo del in Equation 1 and Bayes rule. This allo ws us to iden tify (for example) the most likely perm utation, arg max j q ( j j x ). This decision rule is Bayes-optimal in that no other decision rule can achiev e a lower average error in terms of iden tifying the perm utations. Of interest here is the probabilit y that we mak e an error (on average) using this decision rule, i.e., what is the optimal error rate achiev able in terms of unscram bling the row values. Here an \er-ror" occurs whenev er the most likely perm utation is not the same as the true perm utation that generated x . We will refer to this error rate as the Bayes-optimal perm utation error rate, de ned as with the sup erscript ? referring to \Ba yes-optimal" and subscript P referring to \perm utation".
 In the second problem, we are given the set of per-muted vectors D = x 1 ; : : : ; x N , a set of perm uta-tions P , and an assumed functional form for p ( x ). Our task in this case is to estimate the parameters of p ( x ), the probabilities p ( ), and for eac h row x to estimate the probabilit y that perm utation j was used to generate it. This problem is discussed in Section 5. The two problems above are intimately related. The error rate of any learning algorithm in problem 2 (in terms of iden tifying perm utations) will be lower-bounded by the Bayes-optimal perm utation error rate E
P as de ned in problem 1. Thus, E tal characteristic of the dicult y of learning in the presence of perm utations and it is of direct interest to study it. In what follo ws we rst sho w in Section 3 that E ? P is it itself upp er-b ounded (under certain as-sumptions) by a well-kno wn characteristic of densit y overlap (the Bayes-optimal classi cation error rate), and we then in Section 4 deriv e closed form expres-sions for E ? P for speci c simple forms for p ( x ). Recall that x = ( ~x 1 ; : : : ; ~x c ). We can de ne a marginal distribution for eac h feature-v ector ~x i , i = 1 ; : : : ; c as p ( ~x i ) = i.e., the marginal densit y for ~x i de ned on the set S. Eac h of the c features has a similar marginal densit y on the same set S. We will use p ( ~x j C i ) = p ( ~x i ) to denote the marginal densit y of ~x i on the set S. We now have c di eren t densities de ned on S, whic h in turn de nes a nite mixture p M ( ~x ) on S: where p ( C i ) = 1 c , since all marginals have equal weigh t in the pro cess of de ning the mixture. In the space S consider a classi cation problem with c classes, where, given a measuremen t ~x 2 S, we infer the most likely feature-v ector ~x j that it originated from, j = 1 ; : : : ; c . The Bayes-optimal classi cation rate for this problem is de ned as Intuitiv ely, E ? C is the error rate obtained if we were given vectors ~x i one at a time, and ask ed to iden tify whic h of the c \columns" they originated from, based E
C is prop ortional to the overlap of the individual fea-ture densities p ( ~x i ) in the space S. For example, for the data on the left in Table 1 we would exp ect the overlap of the 4 densities, as re ected by E ? C , to be quite small. Furthermore, we would exp ect intuitiv ely that the perm utation error rate E ? P should also be low in this case, and more generally that it should be re-lated to E ? C in some manner. In what follo ws below we quan tify this intuition. Speci cally , under certain choices of P , we sho w that E ? P is upp er-b ounded by E De nition 1. For P , let k be a key index if ( 1 ( k ) ; : : : ; m ( k )) is a perm utation of (1 ; : : : ; c ). Note that P having a key implies m = jPj = c . The set of all cyclic shifts for example has a key. Example 5. For P = f 1 ; 2 g with 1 = (1 ; 2) and 2 = (2 ; 1), both indices 1 and 2 are keys.
 Example 6. A set of perm utations P = f 1 ; 2 ; 3 ; 4 g with 1 = (1 ; 2 ; 3 ; 4), 2 = (2 ; 1 ; 3 ; 4), 3 = (1 ; 2 ; 4 ; 3), and 4 = (2 ; 1 ; 4 ; 3) does not have a key.
 Theorem 1. If a set of perm utations P for a perm u-tation problem has a key, and if eac h perm utation is equally likely, then The pro of is omitted in this version of the pap er due to lack of space, but can be found online in a longer version of the pap er (Kirshner et al., 2003b). Note that if the set of perm utations P does not have a key, E
P may in fact be larger than E The theorem sho ws that under certain assumptions, the perm utation problem is easier than a corresp ond-ing version of the classi cation problem. This gener-ally agrees with our intuition since in the classi cation version of the problem we are classifying feature val-ues one at a time in terms of whic h column they are though t to have originated from, whereas in the per-mutation version of the problem we are sim ultaneously classifying c values together and have the additional information available that the c values must all be as-signed to di eren t classes. In this section we deriv e closed-form expressions for the Bayes-optimal perm utation error rate E ? P for spe-ci c functional forms for p ( x ) and we use these ex-pressions to sho w how changes in the parameters of p ( x ) can mak e learning and inference either harder or easier. 4.1. Gaussian Features We begin with the case of Gaussian features, since the Gaussian mo del is both amenable to analysis and widely used in practice. 4.1.1. Case 1: Two Independent Features Consider the case when c = 2, d = 1 and S = R . Let p ( x ) be a Gaussian with covariance matrix of the form
I where I is the iden tity matrix (the features are indep enden t Gaussians with equal variances). Thus, where, We have m = 2 with 1 = (1 ; 2) and 2 = (2 ; 1). and where It is straigh tforw ard to sho w that where Therefore, given the functional forms for q ( x j 1 ) and q ( x j 2 ), The quan tity j 1 2 j = is a measure of the overlap of the two Gaussians: as overlap increases E P decreases, and vice-v ersa. This is exactly the same qualitativ e beha vior as one gets with the Bayes-optimal classi -cation error rate E C for this problem, except that the range of integration is di eren t. Speci cally (using the results for E C in (Duda et al., 2000)) we have In the cases of both maximal overlap ( j 1 2 j = is very large) and minimal overlap (the overlap expres-sion is very small) the di erence in the two error rates is very small. The di erence between the two types of error is maximized when j 2 1 j 2 = 4.1.2. Case 2: Two Correla ted Gaussian Next consider a generalization of Case 1 where the features are no longer assumed to be indep enden t but are allo wed to have non-zero correlation : where, q ( x j 1 ) and q ( x j 2 ) are de ned as in Case 1, but now has a covariance term in the o -diagonal positions. Using Equation 2 again, we get Thus, as in the indep enden t case, E P decreases as j 1 j increases and vice-v ersa.
 As ! the lower limit of the integral approac hes 1 and E P approac hes zero. Thus, even though the two Gaussians could be hea vily overlapp ed, as the corre-lation approac hes 1 we can iden tify perm uted pairs of values with accuracy approac hing 1, in con trast to the Bayes-optimal classi cation error rate for the same problem whic h is de ned based on classifying eac h value separately and cannot tak e adv antage of the correlation information. This is a case where the perm utation error rate can approac h 0 even in cases where the classi cation error rate (prop ortional to the overlap of feature densities) can be as high as 0.5. Interestingly , negativ e correlation has the opp osite ef-fect in that as the correlation coecien t becomes more negativ e, E P increases and approac hes E C . Intu-itiv ely, negativ e correlation mak es the problem harder by e ectiv ely leading to more overlap between the two densities. To see this visually , in Figure 1 we plot sim-ulated data from q ( x ) for the case of very negativ e correlation, zero correlation, and very positiv e corre-lation. 4.1.3. Case 3: Unequal Variances Consider further the case where the Gaussian features have unequal variances 2 1 and 2 2 and covariance . Since q ( x j 1 ) and q ( x j 2 ) have unequal covariance ma-trices, there is no closed-form expression for E P or E C as before. Nev ertheless, we examine the variation of E
P as a function of 1 ; 2 and via sim ulations. Fig-ure 2 sho ws some 2-D plots for various values of , keeping 1 and 2 xed. Variance inequalit y changes the nature of the overlap between q ( x j 1 ) and q ( x j as compared to the equal variance case in Figure 1. We can also calculate empirical error rates (in terms of classifying perm utations) using the true mo del and varying the true parameters (note that these are em-pirical estimates of E P by de nition). Figure 3 sho ws the variation of these empirical error rates with for di eren t values of the variances keeping the ratio 1 2 constan t. E P dep ends on both as well as the di er-ence j 2 1 2 2 j , and the variation of the error rate with is not monotonic. 4.2. Categorical Data Consider the simple case when c = 2 and d = 1, i.e., x consists of two scalar features. Assume that the features are discrete and can tak e one of V values. Let m = 2 with 1 = (1 ; 2) and 2 = (2 ; 1), and both perm utations are assumed to be equally likely. We have, Thus, E ? P is a function of the quan tity P the perm utation distance between 1 and 2 . E ? P decreases linearly with this distance, re ecting the fact that the more dissimilar the probabilities of eac h perm uted pair of values are from the probabilities of the unp erm uted pairs, the more E ? P decreases. In this section we brie y commen t on the problem of unsup ervised learning with perm uted data. Space lim-itations do not permit a complete treatmen t of the topic: the goal here is to illustrate that learning with perm utations can be achiev ed in a practical sense, and to demonstrate that the Bayes-optimal perm utation error rate pro vides an absolute lower bound on the error rate of practical learning algorithms. We note in passing that there can be iden ti abilit y issues with mixtures of perm utations that app ear to be quite sim-ilar to those for standard nite mixtures, e.g., certain classes of distributions on categorical data cannot be iden ti ed uniquely . 5.1. EM Algorithms for Perm uted Data Consider a \matrix" data set D = x 1 ; : : : ; x N with N rows and c columns (eac h column being d -dimensional) where we assume that D was generated using the generativ e mo del describ ed in Section 3. As-sume that we kno w the set of perm utations P and the functional form (but not the parameters) of p ( x ). If we knew the perm utations that generated eac h data vector x i , then presumably the problem of estimating the parameters of p ( x ) using the \unscram bled" data would be straigh tforw ard. This suggests the use of the EM framew ork for this problem treating the m possi-ble perm utations as \hidden" information. Letting be the unkno wn parameters of p ( x ), the log-lik eliho od can be de ned as After has been initialized in some fashion, the pa-rameters are changed iterativ ely, guaran teeing a non-decreasing log-lik eliho od at the end of eac h iteration. In the E-step, the probabilit y of eac h perm utation is estimated for eac h data vector given the curren t . In the M-step, new values for are chosen to maximize the exp ected log-lik eliho od of the data with resp ect to the distribution over perm utations as estimated in the E-step. As an example, the p ( j ) terms can alw ays be updated analytically as follo ws: where here (unlik e the analysis earlier in the pap er) the probabilities of di eren t perm utations need not be equal and can be learned from the data. 5.2. Learning from Gaussian Data We sim ulated data with S = R , c = 2, using the setup in Section 4.1.1. In the rst exp erimen t, we performed an empirical analysis of how the perm u-tation error rate of mo dels learned with EM dep ends on the num ber of training examples N . For this we set 1 = 1, 2 = 1, 2 = 16 whic h yields a Bayes-optimal perm utation error rate of roughly 0.36 | thus, we kno w in adv ance that none of our learned mo dels can have a lower error rate than this. 10 di eren t training data sets were generated for eac h of the fol-lowing sizes: N = f 10 ; 20 ; 50 ; : : : ; 5000 ; 10000 g and for eac h training data set the best tting (maxim um like-liho od) mo del was chosen from 10 random restarts of EM. Eac h of these best-t mo dels were then evaluated on a large indep enden t test data set ( N = 2 10 6 data points). The plot in Figure 4 sho ws that, as exp ected, the error rate of the predicted mo del approac hes the Bayes-optimal error rate as the num ber of examples increases. For this particular problem, once the num-ber of data points is on the order of 1000 or greater, EM is performing optimally in terms of iden tifying the perm utations. 5.3. Learning with Rotated Triangles For illustrativ e purp oses we sim ulated 200 triangles from a distribution over angles (corresp onding to p ( x )), and then rotated and re ected the triangles in a manner corresp onding to a set of random perm u-tations. The learning problem is to learn bac k the distribution whic h generated the triangles and put the triangles in geometric corresp ondence. For this prob-lem, c = 3 (3 angles), S = R , and P = f 1 ; : : : ; 6 g (all six possible perm utations of (1 ; 2 ; 3)). We set p ( ) as uniform. p ( x ) is de ned as p ( x 1 ; x 2 ; x 3 N x 1 j 1 ; 2 1 N x 2 j 2 ; 2 2 if x 1 + x 2 + x 3 = , and 0 otherwise. For 1 ; 2 &gt; 0 suc h that 1 + 2 &lt; , and with small 2 1 and 2 2 , this distribution generates triangles with angles x 1 ; x 2 ; x 3 . Figure 5 demonstrates how EM learns both the underlying densit y mo del for angle generation and a distribution over rotations and re ections for eac h triangle. The rows represen t dif-feren t iterations of EM and the leftmost column is the learned densit y mo del as represen ted by the \mean tri-angle" at eac h iteration. The columns represen t the 6 possible perm utations for one of the sim ulated trian-gles in the training data, and the num bers in eac h row are the probabilit y distribution p ( j j x ) ; 1 j 6 for a speci c iteration of EM. Starting from a random tri-angle mo del (upp er left corner) and considerable un-certain ty about the likely perm utation (row 1), EM gradually learns both the correct \mean shap e" and iden ti es the most likely orien tation for this particular triangle (row 5). We analyzed the problem of unsup ervised learning in the presence of unkno wn perm utations of feature val-ues using the notion of a Bayes-optimal perm utation error rate E ? P . We deriv ed a general bound on E ? P as well as closed-form expressions for speci c learn-ing problems and found (for example) that negativ e and positiv e correlation among the feature variables can lead to very di eren t learning problems. The pa-per concluded with a brief empirical illustration of how EM can be used to perform unsup ervised learning from perm uted data. A related applications-orien ted pa-per (Kirshner et al., 2003a) sho ws how this framew ork can be usefully applied to orien ting images of galax-ies in a real-w orld classi cation application in astron-omy. There are sev eral possible extensions of this work including further analysis of the relationship between E
C and E P , computational techniques to handle large num bers of perm utations ( m = c ! for large c ), and analysis of learning algorithms for more general trans-formations than perm utations.
 Duda, R. O., Hart, P. E., &amp; Stork, D. G. (2000). Pat-tern Classi c ation . New York: John Wiley &amp; Sons. Second edition.
 Frey, B. J., &amp; Jojic, N. (2003). Transformation-invarian t clustering using the EM algorithm. IEEE
Transactions on Pattern Analysis and Machine In-telligenc e , 25 , 1{17.
 Gold, S., Lu, C. P., Rangara jan, A., Pappu, S., &amp;
Mjolsness, E. (1995). New algorithms for 2D and 3D point matc hing: Pose estimation and corresp on-dence. Advanc es in Neur al Information Processing Systems (pp. 957{964). The MIT Press.
 Kirshner, S., Cadez, I. V., Sm yth, P., &amp; Kamath, C. (2003a). Learning to classify galaxy shap es using the EM algorithm. Advanc es in Neur al Information Processing Systems 15 . MIT Press.
 Kirshner, S., Parise, S., &amp; Sm yth, P. (2003b). Unsu-pervise d learning with permute d data (Technical Re-port ICS TR-03-03). Univ ersit y of California Irvine. http://www.datalab.uci.edu.
 McCallum, A., Nigam, K., Rennie, J., &amp; Seymore,
K. (2000). Automating the construction of inter-net portals with mac hine learning. Information Re-
