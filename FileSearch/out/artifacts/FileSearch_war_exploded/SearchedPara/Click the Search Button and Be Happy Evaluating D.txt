 We define Direct Information Access as a type of information ac-cess where there is no user operation such as clicking or scrolling between the user X  X  click on the search button and the user X  X  infor-mation acquisition; we define Immediate Information Access as a type of information access where the user can locate the relevant information within the system output very quickly. Hence, a Di-rect and Immediate Information Access (DIIA) system is expected to satisfy the user X  X  information need very quickly with its very first response. We propose a nugget-based evaluation framework for DIIA, which takes nugget positions into account in order to evaluate the ability of a system t o present important nuggets first and to minimise the amount of text the user has to read. To demon-strate the integrity, usefulness and limitations of our framework, we built a Japanese DIIA test collection with 60 queries and over 2,800 nuggets as well as an offset-based nugget match evaluation inter-face, and conducted experiments with manual and automatic runs. The results suggest our proposal is a useful complement to tradi-tional ranked retrieval evaluation based on document relevance. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation evaluation, information access, nugget, test collection
Information Retrieval (IR) evaluation methods need to evolve to-gether with users X  needs and with IR systems. Classical IR evalu-ated set retrieval by means of recall and precision ; but when the ex-ponential growth of searchable information made ranked retrieval a necessity, IR evaluation turned to metrics such as Average Pre-cision and normalised Discounted Cumulative Gain (nDCG) [10]. Furthermore, as Web search studies have revealed that user X  X  queries are often ambiguous and/or underspecified , new evaluation metrics suitable for selectively diversifying search results have begun to re-ceive attention (e.g., [6, 20]). However, all of these evaluation ef-forts still focus on ranked retrieval: a list of documents is presented to the user, and the user is expected to choose from the list and then read the entire Web pages that have been chosen.

In modern Web search engines used in desktop and mobile envi-ronments, providing a simple ranked list of documents to the user is becoming increasingly insufficient. A Search Engine Result Page (SERP) typically consists of several components besides a list of URLs with snippets, depending on query type. Some of these com-ponents aim to satisfy the user needs directly: for example, weather, flight status, stock prices, currency conversion, machine translation and other types of  X  X nswers X  can be embedded in the SERP [5].
We define Direct Information Access (DIA) as a type of infor-mation access where there is no user operation such as clicking or scrolling between the user X  X  click on the search button and the user X  X  information acquisition. Chilton and Teeven X  X   X  X nswers X  [5] are examples that enable DIA; Li, Huffman and Tokuda [12] have referred to successful DIA cases as  X  X ood abandonements. X  More-over, we define Immediate Information Access (IIA) as a type of information access where the user can locate the relevant infor-mation within the system output very quickly. For example, if a system can show a query-biased summary (a search engine snippet would be a trivial example) of a retrieved Web page to the user, this system may be more effective for IIA compared to one that shows the entire contents of that page. Hence, a Direct and Immediate Information Access (DIIA) system is expected to satisfy the user X  X  information need very quickly with its very first response.
Suppose that a Japanese user needs to visit a particular Japanese hospital, and he enters the name of the hospital in the query box of a DIIA system. Figure 1 shows a possible DIIA system output (This is from our manual run which we shall discuss in Section 5.1) that the present study considers. As the English translation shows, our  X  X maginary X  DIIA system outputs the contact details and opening hours of the hospital. The Japanese text is about 140 characters and may fit a mobile phone screen size. If the DIIA system is used on a PC, then the DIIA system may be able to return more information, such as the nearest train station, car park availability and so on.
This paper proposes and validates a new methodology for evalu-ating textual DIIA systems that respond to a given query as shown in Figure 1. Our framework is closely related to multi-document summarisation and Question Answering (QA) evaluations, but dif-fers in the following aspects. First, while the summarisation task treats the set of documents to be summarised as a given input to the system, our DIIA framework considers both the phases of knowl-edge source selection (i.e., finding relevant documents by means of a Web search API etc.) and knowledge compilation (i.e., extracting relevant pieces of information or nuggets [7, 15] from documents Figure 1: An example Japanese DIIA output and its translation (query:  X  X honan Atsugi Hostpital X ). and organising them for effective presentation for a given output screen size). Second, while traditi onal summarisation and QA eval-uations are based on similarities with gold standards or nugget re-call and precision (See Section 2.2), the DIIA task we define require the following features of a system: This is a Direct IA task because the system has to return a text that contain pieces of relevant information in response to a given query (instead of returning a list of URLs); it is an Immediate IA task because the above two requirements enable the user to obtain the desired information quickly.

To evaluate the aforementioned requirements of DIIA systems, we introduce a simple evaluation metric called S-measure which is an extension of weighted nugge t recall but takes the positions of nuggets within the system output into account. To demonstrate the integrity, usefulness and limitations of our DIIA framework, we built a Japanese DIIA test collection with 60 queries and over 2,800 nuggets as well as an offset-based nugget match evaluation interface, and conducted some experiments.

Clearly, DIIA is not just about textual responses: effective DIIA systems should output visual  X  X nswers X  such as maps, images and so on. These are beyond the scope of our study. Moreover, even when we restrict ourselves to textual responses, aesthetic features such as highlighting and font sizes are important in practice. Our current framework can only handle plain text. Nevertheless, we claim that our evaluation framework is a useful departure from and complement to traditional ranked list evaluation.
 Section 2 describes previous work related to the present study. Section 3 proposes our new evaluation framework for the DIIA task. Section 4 describes the DIIA evaluation environment we con-structed, and Section 5 reports on a set of experiments using this environment. Finally, Section 6 concludes this paper.
In modern IR (particularly Web search) evaluation, nDCG [10] has become a popular evaluation metric. While essentially the same idea for evaluating ranked retrieval based on graded relevance ex-isted back in the 1960s (i.e., Pollack X  X  sliding ratio [11]), the simple ideas behind nDCG are appealing: a system is rewarded with a gain value for retrieving a relevant document, and the gain values vary according to the relevance levels; each gain is discounted according to the rank of the retrieved relevant document; and an ideal ranked list is used to normalise the overall system score.
 Our proposed metric for evaluating DIIA was inspired by nDCG. However, while nDCG evaluates a ranked list of items, we evaluate a textual output, which is much more problematic. It should also be noted that, while the original nDCG incorporated the notion of user X  X  patience as a parameter (i.e., the logarithm base), the de facto standard version of nDCG [4] lacks this parameter. In contrast, our proposed metric retains a similar parameter that represents the max-imum amount of text the user is expected to read, or equivalently, the maximum amount of time the user is expected to spend. In this respect, Dunlop X  X  work [8] that explored IR evaluation by expected search duration is also related to our work.

Recently, methods for evaluating search result diversification have been proposed (e.g., [6, 20]). Among them,  X  -nDCG and Novelty-and Rank-Biased Precision (NRBP) [6] are somewhat related to our study, as they view both information needs and documents as sets of nuggets. However, these metrics are for evaluating ranked lists, not text.

INEX (INitiative for the Evaluation of XML retrieval) 2011 has launched the Snippet Retrieval Track 1 , which is also somewhat rel-evant to this study as the task involves generation of a textual snip-pet for each retrieved document. However, the INEX task is still based on document relevance, and the snippets are regarded as a means to judge the relevance of the original documents within the traditional ranked list evaluation framework [21].

Bailey et al. [3] proposed a frawework for evaluating the whole page relevance of a SERP, which typically consists of multiple components, not all of which are textual. While their approach and our DIIA framework both attempt to go beyond ranked list evalua-tion, the former does not aim at producing a reusable test collection. In contrast, while our DIIA framework focusses on a single textual response, we aim to provide a nugget-based test collection that is reusable to some extent.
As our DIIA task is similar to summarisation and QA evaluations in that the systems return textual responses, prior art in these areas needs to be discussed.

ROUGE is a family of evaluation metrics for evaluating sum-maries automatically [13]. While several versions of ROUGE ex-ist, the key idea is to compare a system output with a set of gold-standard summaries in terms of recall (or alternatively F-measure ), where recall is defined based on automatically-extracted textual fragments such as N-grams and longest common subsequences. POURPRE , an automatic evaluation metric for complex QA, is es-sentially F-measure computed based on unigram matches between the system output and gold-standard nuggets [14]. Also, new auto-matic summarisation metrics are being explored at the recent AE-SOP (Automatically Evaluating Summaries of Peers) task at TAC (Text Analysis Conference) 2 .

The above automatic methods assume that automatic string match-ing between the system output and the gold standard works well. While these methods reduce the evaluation cost dramatically and work for extractive approaches (i.e., summaries or answers are parts of source documents), they are probably insufficient for evaluat-ing intelligent systems that can handle consolidation of information across multiple sources, paraphrasing, textual entailment and infer-ence. Hovy, Lin and Zhou [9] discuss some challenges in automatic matching in the context of evaluating summaries using Basic Ele-ments . Indeed, we believe that, in order to quickly satisfy the user X  X  information need with a small piece of text, systems will ultimately have to employ abstractive techniques. While a few approaches to incorporating paraphrase matching into automatic evaluation ex-ist [16, 25], the premise in our study is that a system output and https://inex.mmci.uni-saarland.de/tracks/ snippet/ http://www.nist.gov/tac/2011/Summarization/ a list of gold-standard nuggets need to be compared manually ,at least until we fully understand exactly what kinds of abstractive techniques are useful and reliable for the DIIA task.

The pyramid method [17], a recently-proposed method for man-ually evaluating summaries, is similar to our new proposal. The pyramid method requires multiple gold-standard summaries, from which semantic content units (SCUs) are extracted. Each SCU is weighted according to the number of gold-standard summaries it matches with. An assessor manually identifies SCUs within a system X  X  summary, and the pyramid method essentially computes SCU-based weighted precision or weighted recall.
 A similar method has been used for evaluating complex QA at TREC (and  X  X quishy-list X  QA at TAC) [7]. As was mentioned earlier, the TREC QA systems were required to output a set of documentID-answer pairs rather than a single text. In this evalu-ation methodology, gold-standard nuggets are created by a single assessor, but nugget importance is determined based on multiple assessors, who assign either vital or okay to each nugget. Nugget-based precision, weighted recall and F-measure were used for eval-uation. However, the computation of precision is problematic here, because the number of  X  X ncorrect nuggets X  present in a system output is difficult to define [7, 14]. Hence, an allowance of 100 characters per nugget match was introduced: if there are k matches and if the system output length l is smaller than then precision was defined as 1; otherwise precision was defined as 100  X  k/l . This evaluation method was also used for QA tasks with Asian languages at NTCIR 3 , with different allowance values for different question types [15].

Unlike the pyramid methods for summarisation and QA, we aim to evaluate the system X  X  ability to present important nuggets first and to minimise the amount of text the user has to read. Hence we take into account the position of each nugget match within a sys-tem output. Moreover, while the pyramid QA evaluation relies on an arbitrarily-set, fixed-length allowance for computing precision, our methodology allows nugget constructors to define vital strings which are used for approximating a length lowerbound for each gold-standard nugget (See Section 3). Note also that the pyramid methods discussed above are content selection evaluation metrics: the assumption is that linguistic quality evaluation is done sepa-rately [17]. Our evaluation framework is similar: S-measure is a content ranking evaluation metric, and does not directly consider readability aspects such as coherence and cohesiveness. The chal-lenging problem of evaluating the readability of summaries is being tackled at the aforementioned TAC AESOP task.
Another relevant line of research that lies more or less in be-tween the aforementioned document retrieval paradigm and the tex-tual output paradigm is the nugget-based evaluation effort for the DARPA GALE distillation program which was completed in 2010. Babko-Malaya [2] describes a systematic way to define nuggets in a bottom-up manner from a pool of system output texts. This is done prior to determining whether each nugget is relevant or not: thus, even nonrelevant parts of text need to be  X  X uggetised. X  In contrast, our nuggets are created in a top-down manner by means of man-ual web search. However, Babko-Malaya X  X  approach to defining the nugget granularity and handling world knowledge mayalsobe useful for our task: we leave this to future work.

White, Hunter and Goldsten [22] defined several nugget-based metrics for the distillation task, but they are set retrieval metrics. Allan, Carterette and Lewis proposed a character-based version of http://research.nii.ac.jp/ntcir/ bpref (binary preference) to evaluate a ranked list of passages [1] Yang et al. [24] and Yang and Lad [23] have also discussed nugget-based evaluation metrics that are similar in spirit to the aforemen-tined  X  -nDCG, but their distillation tasks are quite different from, and more complex than, our DIIA task: they consider multiple queries issued over a period of time, and multiple ranked lists of retrieved passages. On the other hand, their proposal to explicitly incorporate the user cost (of reading nonrelevant text at the end of system output) deserves attention and may be considered in our DIIA framework in the future.
We first define the DIIA task as follows. Given a query input by the user, return a textual response whose length is no more than characters (or words, or bytes), excluding white spaces and punctu-ation marks. We assume that some pieces of information are more important than others, and that important ones should be presented first to the user. Moreover, we expect a DIIA system to try to min-imise the amount of text that the user has to read in order to satisfy his information need. Thus, a piece of relevant information near the beginning of the output text (which we call the X -string ) will be rewarded more than the same piece of information near the end of the text. As the output length is lim ited, redundancy (i. e., repetition of the same information within the text) is penalised.

In this paper, we consider a Japanese DIIA task as an example, and consider X = 140 or 500 in Japanese characters. The for-mer is designed to represent a mobile phone screen size; the latter roughly corresponds to top five snippets in a typical SERP, which are usually visible without scro lling in a desktop environment;
We propose a simple evaluation framework for DIIA. First, let us assume that the human reading speed is constant. For example, it is known that the average reading speed of a Japanese person is 400-600 characters per minute. For convenience, we assume that the speed is 500 characters per minute. Our arguments can be extended to other languages, for example, by replacing the constant with 250 words per minute for English, and so on.

As we have discussed earlier, DIIA aims to present important nuggets first and to minimise the amount of text the user has to read in order to obtain the desired information. To evaluate this, we propose to conduct nugget-based evaluation following the practices of QA and summarisation communities, and to evaluate each sys-tem output by taking into account the positions of nugget matches found in it. This is in contrast to traditional QA and summarisa-tion evaluation, where it is only the presence of nugget matches that matters. More specifically, we use a discount factor with each nugget match based on its offset (i.e., distance from the beginning of text), so that a nugget match near the beginning of text receives more credit than one near the end. This is analogous to how nor-malised discounted cumulative gain (nDCG) [10] discounts rele-vant documents based on document ranks. However, while the log-based discounting function of nDCG is understood as a model of the user scanning a ranked list of retrieved items from top to bottom while his patience gradually runs out, we are to model a user read-ing a piece of text from beginning to end. Assuming that the read-ing speed is constant, applying a linear discount to nugget matches based on the offset values is probably one sensible approach.
It is known that there are more elegant and reliable ranked retrieval metrics than bpref, the simplest being the average precision defined over a condensed list [18].
Let us therefore assume that the value of a nugget match decays linearly with respect to the offset. Moreover, let us assume that the value of a nugget wears out completely after (say) two min-utes. This corresponds to a situation where the user cannot afford to spend more than two minutes (after pressing the search button) to gather information. For our average Japanese user, this translates to L =2  X  500 = 1 , 000 characters.

Figure 2 shows how nugget values can be discounted in the above setting. For a DESKTOP run (where X = 500 ), the value of a nugget found at the very beginning of the text is 1, but that of one found at the very end is only 0.5. For a MOBILE run (where X = 140 ), the value of a nugget found at the very end is 140 / 1000 =0.86. Ideally, the parameter L should be determined based on a practical requirement.
 How to define the offset of a nugget match deserves a discussion. We assume that an assessor uses an interface dedicated to evaluat-ing an X-string by comparing it with a list of nuggets. Moreover, unlike traditional nugget-based evaluation, we assume that when the assessor identifies a nugget match, he records which part of the X-string corresponds to a nugget. As we shall describe in Sec-tion 4.3, the nugget match area can easily be recorded by means of a mouse drag. We thus assume that for every nugget match, we can obtain the start and end positions of the nugget match area.
Figure 3 depicts how we define the offset of a nugget match in this study. As the figure shows, we propose to use the end position of a nugget match area as the offset value, because the user prob-ably has to read through the nugget match area in order to obtain the information conveyed in that nugget. Note also that, in Fig-ure 3, there are three nugget matches, and one nugget match area subsumes another. In general, nugget match areas can overlap with one another, and also multiple nugget matches may share the same offset value, which is analogous to tied documents in IR.
Following previous work in QA and summarization, we define a nugget loosely as an atomic piece of information whose pres-ence/absence in a text can be judged by an assessor. Moreover, we assume that we have a weight (i.e., importance) assigned to each nugget, just like graded relevance assessments are available in modern IR test collections. In practice, we let multiple asses-sors assign a grade to each nugget, and then take the sum of the grades to define nugget weights . An alternative approach would be to define  X  X ital X  and  X  X kay X  nuggets [7].
Figure 4: A pseudo minimal output composed of vital strings.
We now formally define S-measure. Let N be a set of gold-standard nuggets constructed for a particular query, and let N ) denote the set of matched nuggets, i.e., those manually identi-fiedinthe X -string. For each m  X  M ,let w ( m ) denote its nugget weight, and let offset ( m ) denote its offset value. Then, by apply-ing the aforementioned linear discounting scheme, we could eval-uate an output for a DIIA task by computing: As shown in Figure 2, L is the amount of text read that corre-sponds to the point where all information becomes useless to the user (set to 1000 in this paper). The max operator just means  X  X g-nore nuggets whose offsets are greater than L . X 
For evaluation with a set of queries, we should normalise the above metric so that it lies within the 0-1 range. In the case of IR evaluation based on nDCG, this is easily done by defining an ideal ranked list that exhaustively lists up all relevant documents in de-scending order of relevance levels. In our case, however, defining an ideal output is problematic, because our evaluation is based on offset-based discounting rather than rank-based one. Our proposed solution is to prepare a vital string v ( n ) for each nugget der to approximate the minimal text length required to convey the meaning of a nugget within a textual output. For example, suppose that for query  X  X aseda university, X  we have two nuggets n representing the phone and fax numbers of this university, amongst sity X  X  phone number is YY-YYYY-YYYY X  while n 2 represents the fact:  X  X aseda university X  X  fax number is ZZ-ZZZZ-ZZZZ X  (where X and Y represent any digit). Then we may define v ( n 1 ) as  X  X Y-YYYY-YYYY X  and  X  X Z-ZZZZ-ZZZZ. X  Note that if an X -string contains  X  X Z-ZZZZ-ZZZZ X  but does not mention that this is a fax number, the user may not find it useful. Thus, a vital string represents a small piece of text that probably needs to be included in the output: it does not necessarily contain sufficient information. The idea is to provide a (possibly unreachable) length lowerbound for each nugget.

For normalising our evaluation metric, we define a Pseudo Min-imal Output (PMO) by sorting all vital strings that correspond to all nuggets n  X  N for a query. The first sort key is the nugget weight w ( n ) (larger the better), and the second sort key is the vital string length | v ( n ) | (smaller the better). Figure 4 shows an exam-ple of PMO. Here, we assume that we have three different nugget weights w =3 , 2 , 1 , and the vital strings are sorted accordingly. Morever, within each set of nuggets with the same weight, the vital strings are sorted by length. For example, v 1 is the first vital string in this PMO because its nugget has the highest weight (namely 3) weight. Note that a PMO is a mere concatenation of vital strings and may completely lack cohesiveness and coherence.
For any vital string v ,let offset  X  ( v ) denote its offset position within a PMO. As Figure 4 shows, we use the end position of each vital string. Then, we define S-measure as 5 :
It is clear that as L approaches infinity and the effect of offset-based discounting is reduced, S-measure approaches the traditional weighted nugget recall . The novel features of S-measure when compared to existing QA and summarisation metrics for the pur-pose of DIIA evaluation are: (1) It takes into account the position of nugget matches; and (2) It approximates the minimal length re-quired for each nugget , while traditional nugget-based precision assumes that this length is a constant (i.e., the allowance). On the other hand, the above definition implies that S-measure is not the-oretically bounded above by 1: there are two reasons for this.
The first reason is that the PMO as defined above does not nece-sarily maximise S-measure X  X  numerator. Suppose that we have two nuggets n 1 and n 2 , such that w ( n 1 )=2 , | v ( n w ( n 2 )=1 , | v ( n 2 ) | = l 2 . Thenitiseasytoshowthatan less important nugget first) may receive an S-measure greater than one if l 1 &gt; 2 l 2 . For example, if l 1 =3 and l 2 =1 important nugget requires a much larger space than the other one), the numerator for the above X -string would be 1  X  (1000  X  1) + 2  X  (1000  X  (1 + 3)) = 2991 . (We repeat, however, that a sim-ple concatenation of vital strings such as  X  v ( n 2 ) v ( n unreadable as vital strings do not necessarily contain sufficient in-formation.) On the other hand, the denominator, which represents the PMO  X  v ( n 1 ) v ( n 2 )  X , would be 2  X  (1000  X  3) + 1  X  (1000  X  (3 + 1)) = 2990( &lt; 2991) . Despite this shortcoming, we use the nugget weight as the first sort key and the vital string length as the second sort key for defining the PMO because (a) we consider presenting important nuggets first more important than presenting short nuggets first; and (b) the above simple definition of PMO makes S-measure computationally cheap.

The second reason that S-measure can exceed one arises from the basic assumption behind all nugget-based evaluation methods, namely, that nuggets are independent of one another. For example, consider the aforementiond example with a phone number and a fax number of a university. Suppose that these two numbers happen to be identical, and that we have defined the vital strings as  X  X Y-YYYY-YYYY X  (10 characters, excluding  X - X  X ) for both nuggets. Then, a PMO will require at least 20 characters in total for these two nuggets. However, an intelligent DIIA system might express these pieces of information as  X  X hone&amp;fax:YY-YYYY-YYYY X  (19 char-acters, excluding  X - X  X  and  X : X ), which is in fact more concise than the PMO.

We later demonstrate that this lack of a theoretical upperbound for S-measure is not a serious problem in practice. One method for preventing S-measure from exceeding one is to define very short vital strings. We will provide real examples in Section 4.2. Never-theless, we can also formerly define a version of S-measure that is guaranteed to range between 0 and 1 as follows: We call this  X  X  flat measure X  because it  X  X lattens X  S-measure when-ever it exceeds one.
S stands for  X  X he user Scanning a String at a constant Speed. X 
To demonstrate the feasibility of our new evaluation framework, we constructed a Japanese DIIA test collection containing 60 queries and over 2,800 nuggets 6 . As DIIA systems are expected to per-form both knowledge source selection and knowledge compilation (See Section 1), we allow DIIA systems to utilise any existing Web pages as their knowledge sources instead of requiring them to draw information from a closed document collection. We call this the open knowledge source evaluation.

Each of our nuggets is associated with a URL: this is the support-ing document based on which the nugget was constructed. Thus, instead of making DIIA systems perform both knowledge source selection and knowledge compilation, we may optionally let the DIIA systems utilise the supporting URLs direc tly, thus isolating and evaluating the knowledge compilation component. We call this the oracle knowledge source evaluation.

In order to make our test collection as reusable as possible and to ensure successful matches between our gold-standard nuggets and system responses, we contructed queries that seek established facts rather than time-sensitive, controversial or subjective information. However, as we shall discuss in Section 4.2, even  X  X stablished X  facts may change over time, and some truth maintenance effort in the future may help prolong the life of our DIIA test collection. As we wanted our DIIA test collection to handle both DESK-TOP and MOBILE situations (See Section 3.1), we referred to the work by Li, Huffman and Tokuda [12] that analysed mobile and desktop query logs from the United States, Japan and China, and identified frequent query types for potential  X  X ood abandonment X  queries. They found that the query distributions over query types are quite different across the three countries: for Japan (which they considered to be a mature market in mobile search), the LOCAL , QA 7 , CELEBRITY and DEFINITION query types were very pop-ular in both mobile and desktop environments. We therefore de-cided to collect queries for these four query types. The original definitions of these query types by Li, Huffman and Tokuda were as follows: CELEBRITY User seeks news or images of a celebrity.
 LOCAL User seeks a local listing (address and/or phone number). DEFINITION User seeks the definition of a term.
 QA User seeks a short answer to a question.
 While we basically followed the original definitions for the DEF-INITION and QA query types, we interpreted CELEBRITY and LOCAL query types differently, as shown below: CELEBRITY User wants to gather various facts about a celebrity: LOCAL User wants to contact or vi sit a facility (school, shop,
We will make the test collection available through NTCIR.
Li, Huffman and Tokuda [12] called it the  X  X nswers X  category. Note that we had to replace the original definition of CELEBRITY completely to make the test collection as reusable as possible. These detailed specifications were formulated through repeated revisions during the nugget creation process.

After deciding on the four query types, we collected 15 queries for each type, thereby obtaining 60 queries in total. As we wanted to handle information needs that are as realistic as possible, we used two sets of real query data. For obtaining CELEBRITY and LOCAL queries, we used a proprietary Japanese mobile query log which covered two weeks in October 2009. It contained approx-imately 39,000 unique queries ranked by query frequency. From this data set, we manually selected celebrity names and local fa-cility names while conducting trial Web searches and ensuring that good information sources exist on the Web. For obtaining DEF-INITION and QA queries, we examined the Yahoo! Chiebukuro (Japanese Yahoo! Answers) data [19]. Natural language questions were manually selected from categories such as education , local and health categories, which we thought were more appropriate for factual information seeking than other categories such as love . For the DEFINITION query type, we used the look-up term as the query, while for QA, we formulated a short natural language sen-tence from each original question in the Yahoo! data.

Figure 5 shows the entire query set thus constructed, with some rough English translations 8 . For example, Query 0010 ( X  X ao Asada, X  Query 0036 is misspelt, but we did not correct it. Current Web Search APIs can actually correct the spelling automatically. Figure 6: Some nuggets from 0031  X  X samu Tezuka X  (CELEBRITY). a figure skater) is a CELEBRITY query. We classfied 0023  X  X alash-nikov X  as a DEFINITION query, as the original question in the Ya-hoo! data set was  X  X hat is Kalashnikov? X . As the second field for this query shows, it has five different interpretations in our data: a gun inventor X  X  name, his son X  X  name, the gun X  X  name, and brands called  X  X alashnikov vodka X  and  X  X alashnikov watch. X  Similarly, the LOCAL query 0037 has two interpretations, as there are two train stations named  X  X anbonmatsu X  in Japan. We intentionally in-cluded these ambiguous queries in the set, as we believe that han-dling such queries in DIIA evaluation will be important just as in traditional diversified ranked list evaluation [6, 20]. However, how to explictly encourage diversification in DIIA evaluation is beyond the scope of the present study.
Through searching and browsing Web pages (mostly official pages or Wikipedia), the first author of this paper, who is a native Japanese speaker, created a total of 2,839 nuggets across the 60 queries. Statistics on the number of nuggets are shown in Table 1. The query with the highest number of nuggets (368) is 0031  X  X samu Tezuka X  (See Figure 5), an extremely prolific cartoonist who died in 1989: about 300 nuggets correspond to comics and films that he created. A small portion of the nuggets for 0031 is shown in Figure 6: the first field is the nugget ID; the second field is the nugget grade (1, 2, or 3) assigned by the nugget creator (i.e., the first author); the third field is the nugget semantics in natural lan-guage, which represents an atomic piece of factual information and is used as a criterion for manually determining if a system output contains this nugget or not; the fourth field is the aforementioned vital string used for normalising S-measure; and the fifth field is the URL from which the nugget was extracted. Nuggets N001 and N003 say that Osamu Tezuka was born on November 3, 1928 and died on February 9, 1989, respectively; N002 says that he was born in Osaka; N004 says that he was a cartoonist; N009 and N013 say that he graduated from Osaka University in 1951 and that he got married in 1959; N014 says that his wife X  X  name is Etsuko; and N015 says that he received a medical doctoral degree in 1961. The English translations of the corresponding vital strings would be:  X  X ov 3, 1928, X   X  X saka, X   X  X eb 9, 1989, X   X  X artoonist, X   X  X raduated, X   X  X arried, X   X  X tsuko X  and  X  X edical doctor. X 
The nugget grades assigned by the first author are subjective. For example, for 0031, all the comi c book title s by Osamu Tezuka were given the grade of 1, as the first author judged that it is more impor-tant to include in the system output some basic facts about Osamu Tezuka (personal history etc.) than some of his comic titles selected from his lifetime X  X  work. Note also that in Figure 6, the fact that he got married in 1959 (N013) is considered less important than the fact that he graduated from Osaka university in 1951 (N009). To remedy this subjectivity issue, the second author of this paper (also a native Japanese speaker) assigned his nugget grades (1, 2 or 3) in-dependently, and the sum of these two grades were used as the final nugget weights in our experiments. The inter-assessor agreement was reasonable: 0.689 in terms of quadratic-weighted kappa .Note that, as in summarisation and QA evaluation, the nugget creation is still done by a single person.
Even though nugget creation depends on the view of a single person in our study, we devised a nugget creation policy document that can be shared by future nugget creators. An excerpt from the document is shown below: (a) A nugget is a short factual statement such that an assessor (b) Information available on official Web pages is considered (c) Nuggets are built based on established facts as of December Item (b) was included based on the observation that information in official pages and that in Wikipedia are occasionally contradictory. For example, for the CELEBRITY query 0022, the official date of birth is different from the one in Wikipedia (as of December 31, 2010). In such a case, we assume that the official information is correct. Item (c) tries to free us from the burden of updating our nuggets indefinitely in response to future events. However, nuggets do become obsolete, sometimes unexpectedly quickly: on Octo-ber 28, 2010, we finished constructing our preliminary nugget sets which contained a nugget representing a famous Japanese blog for the aforementioned query 0022  X  X aori Manabe X  (who was once known as  X  X ueen of Blogs X ). However, to our surprise, the blog was shut down permanently on October 31, 2010! On the other hand, for the figure skater query 0010, we later had to add nuggets representing figure skating competitions that took place in Novem-ber and December 2010. We thus re-examined all of the prelimi-nary nugget sets and made revisions where necessary, to ensure that the final nugget sets represent facts as of December 31, 2010.
How efficient was the nugget creation process? The difficult part was devising the nugget creation policy document: deciding on what kinds of information to expect from DIIA systems in gen-eral as well as for each query type. Once this was done, the actual process of searching, browsing and collecting nuggets was not dif-ficult. Although we did not measure the total time required for constructing the entire test collection (as the nugget creation pol-icy writing and most of the actual nugget creation were done in parallel), we recorded the time spent for nugget creation for some typical cases. For example, it took about 68 minutes to construct 148 nuggets for a C ELEBRITY query (0.46 m inutes per nugget), and about 40 minutes to construct 31 nuggets for a LOCAL query (1.29 minutes per nugget). This includes the time to form nugget semantics and vital strings, to record URLs, and to decide on the first set of nugget grades as shown in Figure 6. In general, queries with many nuggets were created relatively efficiently, because they usually contained large lists (CDs released, books published, etc.) that are simple to handle. For DEFINITION and QA queries which generally have a very small number of nuggets, the time spent was typically 2-20 minutes per query. Our overall experience suggests that nugget creation for the DIIA task is just as feasible as that for QA and summarization or relevance assessments for traditional IR, provided that a clear nugget policy document and training material are given to the nugget creator. Note, however, that DIIA evaluation also requires a manual nugget match evaluation for each system, as will be described in the next section.
To evaluate a system for the DIIA task, an X -string and the list of nuggets are compared manually. Figure 7 shows screenshots of the interface we have developed for this purpose. The left panel shows the X-string for a particular query. (This is an X -string from the manual DESKTOP run which will be described later). The nugget match evaluation interface can truncate each X -string into 500 (or 140) characters before evaluation. The right panel shows the list of nuggets for this query, in the PMO order (i.e., sorted first by the nugget weight and then by the vital string length). The assessor examines each nugget in turn, and decides whether it is covered by the X -string or not. If it is covered, he selects and records a nugget match area within the X -string by means of a mouse drag, as shown Figure 7 (a). In response to this user action, a Save button pops up, and the user can click on it. As a result, as shown in Figure 7 (b), the nugget match area is stored together with its start and end positions. The end position (44) is used as the offset for this nugget for computing S-measure later. The output from this interface is a list of nuggetID-offset pairs.

As was mentioned earlier, a nugget match area may overlap with or subsume another. We believe that this flexible feature is nec-essary for evaluating i ntelligent systems that go beyond extracting texts from source documents  X  X s is X . While the interface techni-cally allows the assessor to reco rd multiple matches per nugget, we use only the nugget match with the smallest offset (i.e., first nugget occurrence within the X -string) so that repetition is penalised when computing S-measure.

Given the output from the interface (i.e., a list of nuggetID-offset pairs), S-measure can easily be computed by automatically com-paring it with the gold-standard that consists of nuggetIDs, nugget weights and the lengths of vital strings (See Eq. 2) 9 . In practice, it is probably useful to hire multiple assessors to evaluate each string, and to average the S-measure values across the assessors, which should serve two purposes: (a) trivial human errors (i.e., missing an existing nugget or locating a nonexistent nugget) can be detected by automatically comparing the assessment results; and (b) since recognitio n of nugget matches and selection of nugget match areas may vary across assessors, we can obtain more re-liable results by averaging. In this study, however, only the first author performed nugget match evaluation. As the purpose of our DIIA experiments is to demonstrate the integrity, usefulness and limitations of our general approach, we argue that this is sufficient.
This section reports on a set of experiments using our DIIA test collection and S-measure. In addition to evaluating a pilot auto-matic run, we evaluated a manual run composed of X-strings con-structed by hand for each query in order to estimate a practical up-perbound for each query. This is because the maximum achievable S-measure value is often below one by design, as the denomina-tor (i.e., normalisation factor) of S-measure is often unreachable by its numerator in Eq. 2. This normalisation issue arises from the fact that the Pseudo Minimal Output (PMO) is constructed using vital strings, which are probably necessary to be included in the system output, but probably not sufficient to convey the meaning of the nugget to the user. In addition, we compare the behaviour of S-measure with the traditional weighted nugget recall, which is equivalent to S-measure without offset-based discounting.
We devised a manual run in order to approximate S-measure val-ues that are actually achievable if we have ideal information re-
A software for computing S-measure is publicly avail-able at http://research.nii.ac.jp/ntcir/tools/ ntcireval-en.html . Current nugget:  X  Automatic rifle AK 47 developed by Mikhail Kalashnikov  X  Nugget match area saved with offsets:  X  Doctor trieval and natural language processing techniques. Each output of the manual run is perfectly readable, in contrast to a PMO which may lack a lot of necessary information and may be defective as a natural language text.

Each X -string of our manual run was prepared as follows: the first author looked at the complete list of nuggets in the PMO or-der. Starting with an empty output text file, he added, for each nugget, a natural language text that concisely expresses the nugget semantics. (The text is often similar to the nugget semantics itself.) While he basically respected the original nugget order, he reordered some nuggets so that similar kinds of information are output close to each other. For example, even if a phone number is ranked at 1 and a fax number is ranked at number 5 in the PMO, he put the phone number and the fax number together in the output file (by basically respecting the rank of the first nugget). Similarly, for ex-ample, a list of book titles were output as one block even if they were spread across within the PMO, as the latter situation is not natural to the human eye. He also consolidated multiple nuggets where appropriate: recall the phone/fax number example discussed in Section 3.3. In short, he used common sense to re-arrange and merge the pieces of information conveyed in the PMO from the viewpoint of readability and conciseness. As the manual run was created by relying on the  X  X ight answers, X  it should be regarded as an oracle knowledge source run (See Section 4).

Figure 7 includes the first part of the X -string from our manual run for 0023  X  X alashnikov X  (DEFINITION). The first sentence in this X -string means  X  X r. Mikhail Kalashnikov, the Russian gun de-signer X  and this sentence covers the second nugget  X  X alashnikov is the name of a Russian gun designer X  and the fifth nugget  X  X ikhail Kalashnikov has a Ph.D. X  The second sentence in the X -string means  X  X lso refers to AK-47, the automatic rifle he designed X  and this sentence covers the first nugget,  X  X alashnikov means AK-47, an automatic rifle designed by Mikhail Kalashnikov. X 
The original manual run was created so that each X -string is around 700 characters long. Then, the nugget match evaluation interface truncated each X -string to 500 and 140 characters to pro-duce DESKTOP and MOBILE runs, respectvely.
We also generated an automatic run by assuming that the correct query types are known to the DIIA system 10 . Using the query type information and the Bing API, our automatic run adopted the sim-ple heuristics shown in Figure 8 for each query q . Hence this is an open knowledge source run (See Section 4). Just like the man-ual runs, the automatic DESKTOP and MOBILE runs were created from the same 700-character run by truncation. Note that these runs are just examples for demonstrating the integrity of our DIIA evaluation framework.
 Figure 8: The automatic run heuristics. (The URL pattern  X  X a.wikipedia.org X  represents Japanese Wikipedia pages and the four URL patterns for the QA queries represent Japanese community QA sites.)
Table 2 shows the mean S-measure (in bold) and weighted nugget recall (in italics) values for our four runs (Manual/Automatic DESKTOP/MOBILE). It can be observed that S-measure behaves similarly to weighted recall on average: the DESKTOP runs (500
Another approach would be to provide the DIIA system with some training queries (which we lacked at this point), and make it deter-mine the query type of each given query using an automatic classi-fication technique. Table 2: Manual and automatic run mean performances: S-measure (in bold) / weighted recall (in italics). D:DESKTOP; M:MOBILE.
 characters) naturally outperform the corresponding MOBILE runs (140 characters); the automatic runs do better with DEFINITION queries than with others; and the manual runs substantially outper-form the automatic runs, but are far from  X  X erfect. X 
There two reasons why the mean S-measure across all queries does not reach one even for the manual DESKTOP run (.828). The first is simply because of the output widow size of 500: for some queries, it is simply impossible to list up all nuggets within this window, and this is exactly why the mean weighted recall is also below one (.824). The second reason is that the maximum possible S-measure value is often smaller than one.

Figure 9 visualises S-measure and weighted nugget recall for the manual and automatic DESKTOP runs per query. The performance of any DIIA DESKTOP run is expected to lie below the manual DESKTOP curves as the latter represents practical upperbounds. It can be observed, for example, that the practical upperbounds are actually 1 for the QA queries as they have few nuggets per query, while those for some CELEBRITY queries are indeed quite low, for exactly the reasons discussed above.
 At the other extreme, there was exactly one query for which the S-measure only slightly exceeded 1, though this is not visible in Figure 9. The QA query 0004  X  X part from Ueno Zoo, where in Japan can I see a panda? X  has the following four nuggets: N001 Adventure World has pandas (vital string length =11 for N002 Adventure World is in Wakayama prefecture (vital string N003 Oji Zoo has pandas (vital string length =5 for  X  X ji Zoo X  in N004 Oji Zoo is in Kobe city (vital string length =2 for  X  X obe X  Therefore, its PMO ranks the nuggets as &lt; N003 N001 N004 N002 by using the weights as the first sort key and the lengths as the sec-ond sort key. Whereas, the manually composed X -string was  X  X ji Zoo (Kobe), Adventure World (Wakayama) X  [English translation]. This covers all four nuggets, and is of the form &lt; N003 N004 N001 N002 &gt; after removing punctuation marks etc., and it happens to outperform the PMO very slightly. As a result, the S-measure for this query was 1.001. Note that this is exactly one of the potential problems we discussed in Section 3.3. However, as was discussed earlier, this problem can be avoided by using the S -measure (Eq. 3) instead.

Let us now compare S-measure with weighted recall using Fig-ure 9. It can be observed that, while S-measure generally resem-bles weighted recall, there are many cases where it is less than one even though weighted recall is one. This means that S-measure is more demanding for these queries. For example, see the man-ual DESKTOP performances for the LOCAL and DEFINITION queries. Also, for the aforementioned  X  X anda X  query 0004, our au-tomatic DESKTOP run achieved a weighted recall of one but an S-measure of around 0.9: the first part of the X -string is shown in Figure 10. As the English translation shows, the red block in this figure covers all of the aforementioned four nuggets. However, because this X -string was automatically generated from search en-gine snippets, the part preceding the red block is completely irrel-evant. That is, while this output achieves perfect nugget recall, it is still not satisfactory in terms of Immediate Information Access: the automatic run fails to minimise the amount of text that the user has to read in order to obtain the desired information. There were other similar cases in our experiments, which suggest that evalua-tion with S-measure will help us design our system output strategy carefully for DIIA. Figure 10: Part of the X -string from the automatic run for Query 0004.

Finally, let us discuss a few cases where S-measure is greater than weighted recall. In Figure 9, the S-measure of the manual DESKTOP run is higher than the weighted recall for 0031  X  X s-amu Tezuka X  (CELEBRITY). Recall that this is the query with the largest number of nuggets (368). The manual DESKTOP run re-turned only 35 nugget matches for this query (unweighted recall 35 / 368 = . 095 ). However, as the X -string basically showed the nuggets in order of importance (as this was the strategy used for creating the manual runs), the S-measure was .351, while the weighted recall was .142. Similarly, for 0039  X  X anazawa Univer-sity X  (LOCAL) which had 125 nuggets, the manual DESKTOP run returned 31 nugget matches, and the S-measure was .602 while the weighted recall was .348. These examples show that S-measure can reward systems that present important nuggets first, even when their nugget recall values are low.
We defined the Direct and Immediate Information Access (DIIA) task, where the system is expected to satisfy the user X  X  need very quickly with its very first textual output, and proposed an evalu-ation framework for it. We proposed a simple evaluation metric called S-measure that takes the positions of nugget matches into account, and built a real DIIA test collection and an offset-based nugget match evaluation interface. Our experiments have demon-strated the integrity and usefulness of our framework. In particular, we have verified that S-measure rewards systems that reflect nugget importance, and those that return nuggets near the beginning of the output string. We also demonstrated that S-measure usually lies within the 0-1 range, and showed a simple variant of S-measure called S -measure which is strictly bounded above by 1.
Clearly, there are limitations to the present approach. The first is that S-measure is purely a cont ent ranking measure, and that it does not consider linguistic quality. As we discussed in Section 2.2, this is a problem inherent in all nugget-based approaches. One possible problem with S-measur e in particular is that there may be cases where presenting important information first may actu-ally hurt readability. We thus recommend that readability evalua-tion be done separately for evaluating DIIA. Also, note that while S-measure penalises presentation of nonrelevant information be-fore relevant information as in Figure 10, it does not explicitly pe-nalise presentation of nonrelevant information after relevant infor-mation [23, 24]. This is similar to IR evaluation that treats a list of nonrelevant documents and an empty list as equally useless.
The second limitation is that we can only evaluate plain text out-puts, even though real DIIA systems will probably use highlighting, different font sizes, and even non-textual presentations.
The third limitation is that, as we have observed in Section 4.2, our nuggets may become obsolete relatively quickly even though we are already restricting ourselves to retrieval of  X  X stablished X  facts. To address this problem, building a semi-automated truth maintenance system for nuggets may be useful. Such a system could periodically monitor the URLs that the current nuggets rely on (See Figure 6), and notify the test collection builder if the con-tents of the URLs have been revised. Note, however, that while some queries may require frequent updating, others do not: for ex-ample, many DEFINITION and QA queries do not require frequent updating, and the same goes even f or some CELEBRITY queries, particularly for people who have passed away.

Despite the above limitations, we believe that our framework is a useful departure from and complement to traditional ranked re-trieval evaluation. Since we explictly encourage concise presenta-tions of important information, the DIIA task may foster research in abstractive text presentation methods as opposed to simple ex-tractive ones (e.g. sentence selection). As future work, we plan to extend our DIIA experiments to other languages such as English. Our framework may also be useful to other related applications, such as search engine snippets and Bing X  X  hover previews.
