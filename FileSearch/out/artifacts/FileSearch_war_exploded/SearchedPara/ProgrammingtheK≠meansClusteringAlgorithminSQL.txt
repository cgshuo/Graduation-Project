 Using SQL has not been considered an ecien t and feasible way to implemen t data mining algorithms. Although this is true for man y data mining, mac hine learning and statistical algorithms, this work sho ws it is feasible to get an ecien t SQL implemen tation of the well-kno wn K-means clustering algorithm that can work on top of a relational DBMS. The article emphasizes both correctness and performance. From a correctness point of view the article explains how to com-pute Euclidean distance, nearest-cluster queries and updat-ing clustering results in SQL. From a performance point of view it is explained how to cluster large data sets de ning and indexing tables to store and retriev e intermediate and nal results, optimizing and avoiding joins, optimizing and simplifying clustering aggregations, and taking adv antage of sucien t statistics. Exp erimen ts evaluate scalabilit y with syn thetic data sets varying size and dimensionalit y. The prop osed K-means implemen tation can cluster large data sets and exhibits linear scalabilit y.
 H.2.8 [ Database Managemen t ]: Database Applications-Data Mining Algorithms, Languages Clustering, SQL, relational DBMS, integration
There exist man y ecien t clustering algorithms in the data mining literature. Most of them follo w the approac h prop osed in [14], minimizing disk access and doing most of the work in main memory . Unfortunately , man y of those al-gorithms are hard to implemen t inside a real DBMS where Cop yright 2004 ACM 1 X 58113 X 888 X 1/04/0008 ... $ 5.00. the programmer needs to worry about storage managemen t, concurren t access, memory leaks, fault tolerance, securit y and so on. On the other hand, SQL has been gro wing over the years to become a fairly comprehensiv e and complex query language where the asp ects men tioned above are au-tomatically handled for the most part or they can be tuned by the database application programmer. Moreo ver, nowa-days SQL is the standard way to interact with a relational DBMS. So can SQL, as it exists today, be used to get an e-cien t implemen tation of a clustering algorithm? This article sho ws the answ er is yes for the popular K-means algorithm. It is worth men tioning programming data mining algorithms in SQL has not receiv ed too much atten tion in the database literature. This is because SQL, being a database language, is constrained to work with tables and columns, and then it does not pro vide the exibilit y and speed of a high level pro-gramming language like C++ or Java. Summarizing, this article presen ts an ecien t SQL implemen tation of the K-means algorithm that can work on top of a relational DBMS to cluster large data sets.

The article is organized as follo ws. Section 2 introduces de nitions and an overview of the K-means algorithm. Sec-tion 3 introduces sev eral alternativ es and optimizations to implemen t the K-means algorithm in SQL. Section 4 con-tains exp erimen ts to evaluate performance with syn thetic data sets. Section 5 discusses related work. Section 6 pro-vides general conclusions and directions for future work.
The basic input for K-means [2, 6] is a data set Y con tain-ing n points in d dimensions, Y = f y 1 ; y 2 ; : : : ; y the desired num ber of clusters. The output are three matri-ces W; C; R , con taining k weigh ts, k means and k variances resp ectiv ely corresp onding to eac h cluster and a partition of Y into k subsets. Matrices C and R are d k and W is k 1. Throughout this work three subscripts are used to index ma-column of C or R we use the j subscript (e.g. C j ; R j ); C can be understo od as a d -dimensional vector con taining the cen troid of the j th cluster having the resp ectiv e squared ra-diuses per dimension given by R j . For transp osition we will use the T sup erscript. For instance C j refers to the j th cen-troid in column form and C T j is the j th cen troid in row form. Let X 1 ; X 2 ; : : : ; X k be the k subsets of Y induced by clusters s.t. X j \ X j 0 = ; ; j 6 = j 0 . K-means uses Euclidean distance to nd the nearest cen troid to eac h input point, de ned as d ( y i ; C j ) = ( y i C j ) T ( y i C j ) = P d l =1 ( y
K-means can be describ ed at a high level as follo ws. Cen-troids C j are generally initialized with k points randomly selected from Y for an appro ximation when there is an idea about poten tial clusters. The algorithm iterates executing the E and the M steps starting from some initial solution un-til cluster cen troids become stable. The E step determines the nearest cluster for eac h point and adds the point to it. That is, the E step determines cluster mem bership and par-titions Y into k subsets. The M step updates all cen troids C by averaging points belonging to the same cluster. Then the k cluster weigh ts W j and the k diagonal variance matrices R j are updated based on the new C j cen troids. The qualit y of a clustering solution is measured by the average quan tiza-tion error q ( C ) (also kno wn as squared assignmen t distance [6]). The goal of K-means is minimizing q ( C ), de ned as measures the average squared distance from eac h point to the cluster where it was assigned according to the parti-tion into k subsets. K-means stops when q ( C ) changes by a marginal fraction ( ) in consecutiv e iterations. K-means is theoretically guaran teed to con verge decreasing q ( C ) at eac h iteration [6], but it is common to set a maxim um num ber of iterations to avoid long runs.
This section presen ts our main con tributions. We explain how to implemen t K-means in a relational DBMS by auto-matically generating SQL code given an input table Y with d selected numerical columns and k , the desired num ber of clusters as input as de ned in Section 2. The SQL code gen-erator dynamically creates SQL statemen ts monitoring the di erence of qualit y of the solution in consecutiv e iterations to stop. There are two main schemes presen ted in here. The rst one presen ts a simple implemen tation of K-means ex-plaining how to program eac h computation in SQL. We refer to this scheme as the Standard K-means implemen tation. The second scheme presen ts a more complex K-means im-plemen tation incorp orating sev eral optimizations that dra-matically impro ve performance. We call this scheme the Optimized K-means implemen tation.

There are imp ortan t assumptions behind our prop osal from a performance point of view. Tw o tables with n rows having the same primary key can be joined in time O ( n ) using a hash-based join. So if a di eren t DBMS does not pro vide hash-based indexing, joining tables may tak e longer than O ( n ). However, the prop osed scheme should still pro-vide the most ecien t implemen tation even in suc h cases. In general it is assumed that n is large, whereas d and k are comparativ ely small. This has a direct relationship to how tables are de ned and indexed, and to how queries are form ulated in SQL. These assumptions are reasonable in a database environmen t. The basic scheme to implemen t K-means in SQL, having Y and k as input (see Section 2), follo ws these steps: 1. Setup . Create, index and populate working tables. 2. Initialization . Initialize C . 3. E step . Compute k distances per point y i . 4. E step . Find closest cen troid C j to eac h point y i 5. M step . Up date W; C and R . 6. M step . Up date table to trac k K-means progress.
Steps 3-6 are rep eated until K-means con verges. In the follo wing paragraphs we discuss table de nitions, in-dexing and sev eral guidelines to write ecien t SQL code to implemen t K-means. In general we omit Data De ni-tion Language (DDL) statemen ts and deletion statemen ts to mak e exp osition more concise. Thus most of the SQL code presen ted involves Data Manipulation Language (DML) state-men ts. The columns making up the primary key of a table are underlined. Tables are indexed on their primary key for ecien t join access. Subscripts i; j; l (see Section 2) are de-ned as integer columns and the d numerical dimensions of points of Y , distances, and matrix entries of W; C; R are de-ned as FLO AT columns in SQL. Before eac h INSER T state-men t it is assumed there is a "DELETE FR OM ... ALL;" statemen t that leaves the table empt y before insertion.
As introduced in Section 2 the input data set has d di-mensions. In database terms this means there exists a table Y with sev eral numerical columns out of whic h d columns are picked for clustering analysis. In practice the input ta-ble may have man y more than d columns but to simplify exp osition we will assume its de nition is Y ( Y 1 ; Y 2 So the SQL implemen tation needs to build its own reduced version pro jecting the desired d columns. This motiv ates de ning the follo wing "horizon tal" table with d + 1 columns: Y H ( i ; Y 1 ; Y 2 ; :::; Y d ) having i as primary key. The rst col-umn is the i subscript for eac h point and then Y H has the list of d dimensions. This table saves Input/Output access (I/O) since it may have few er columns than Y and it is scanned sev eral times during the algorithm progress. In general it is not guaran teed i (point id) exists because the primary key of Y may consist of more than one column, or it may not exist at all because Y is the result of some aggrega-tions. In an implemen tation in an imp erativ e programming language like C++ or Java the point iden ti er is immaterial since Y is accessed sequen tially , but in a relational database it is essen tial. Therefore it is necessary to automatically create i guaran teeing a unique iden ti er for eac h point y The follo wing statemen t computes a cum ulativ e sum on one scan over Y to get i 2 f 1 : : : n g and pro jects the desired d columns.
 INSER T INTO YH SELECT sum(1) over(ro ws unbounded preceding) AS i FR OM Y ; The point iden ti er i can be generated with some other SQL function than returns a unique iden ti er for eac h point. Getting a unique iden ti er using a random num ber is not a good idea because it may get rep eated, specially for very large data sets. As seen in Section 2 clustering results are stored in matrices W; C; R . This fact motiv ates having one table for eac h of them storing one matrix entry per row to allo w queries access eac h matrix entry by subscripts j and l . So the tables are as follo ws: W ( j ; w ), C ( l; j ; val ), R ( l; j ; val ), having k , dk and dk rows resp ectiv ely.
The table Y H de ned above is useful to seed K-means, but it is not adequate to compute distances using the SQL "sum()" aggregate function. So it has to be transformed into a "vertical" table having d rows for eac h input point, with one row per dimension. This leads to table Y V with de nition Y V ( i; l ; val ). Then table Y V is populated with d statemen ts as follo ws: INSER T INTO YV SELECT i,1, Y 1 FR OM YH; : : : INSER T INTO YV SELECT i,d, Y d FR OM YH;
Finally we de ne a table to store sev eral useful num bers to trac k K-means progress. Table mo del serv es this purp ose: model ( d; k; n ; iter ation; avg q; dif f avg q ). Most K-means varian ts use k points randomly selected from Y to seed C . Since W and R are output they do not re-quire initialization. In this case Y H pro ves adequate for this purp ose to seed a "horizon tal" version of C . Table CH ( j ; Y 1 ; : : : ; Y d ) is updated as follo ws. INSER T INTO CH
SELECT 1, Y 1 ; ::; Y d FR OM YH SAMPLE 1; : : : INSER T INTO CH SELECT k, Y 1 ; ::; Y d FR OM YH SAMPLE 1;
Once CH is populated it can be used to initialize C with dk statemen ts as follo ws, INSER T INTO C
SELECT 1 ; 1 ; Y 1 FR OM CH WHERE j = 1; : : : INSER T INTO C SELECT d; k; Y d FR OM CH WHERE j = k ; K-means determines cluster mem bership in the E step. This is an intensiv e computation since it requires O ( dkn ) oper-ations. Distance computation needs Y V and C as input . The output should be stored in a table having k dis-tances per point. That leads to the table Y D de ned as Y D ( i; j ; distance ). The SQL is as follo ws.
 INSER T INTO YD SELECT i; j ,sum((YV.v al-C.v al)**2) FR OM Y V; C WHERE Y V:l = C:l GR OUP BY i; j ; After the insertion Y D con tains kn rows. Before doing the GR OUP BY there is an intermediate table with dkn rows. This temp orary table constitutes the largest table required by K-means.
 The next step involves determining the nearest neigh bor (among clusters) to eac h point based on the k distances and storing the index of that cluster in table Y N N ( i ; j ). Therefore, table Y N N will store the partition of Y into k subsets being j the partition subscript. This requires two steps in SQL. The rst step involves determining the mini-mum distance. The second step involves assigning the point to the cluster with minim um distance. A deriv ed table and a join are required in this case. Table Y N N con tains the partition of Y and will be the basic ingredien t to compute cen troids. This statemen t assumes that the minim um dis-tance is unique for eac h point. In abnormal cases, where distances are rep eated (e.g. because of rep eated cen troids, or man y rep eated points) ties are brok en in favor of the cluster with the lowest subscript j ; that code is omitted. INSER T INTO Y N N SELECT Y D:i; Y D:j FR OM Y D , (SELECT i; min ( distance ) AS mindist WHERE Y D:i = Y M IN D:i The M step updates W; C; R based on the partition Y N N obtained in the E step. Giv en the tables introduced above updating clustering parameters is straigh tforw ard. The SQL generator just needs to coun t points per cluster, compute the average of points in the same cluster to get new cen troids, and compute variances based on the new cen troids. The resp ectiv e statemen ts are sho wn below.
 INSER T INTO W SELECT j ,coun t(*) FR OM Y N N GR OUP BY j ; UPD ATE W SET w = w=model :n ; INSER T INTO C SELECT l; j; avg ( Y V:val ) FR OM Y V; Y N N WHERE Y V:i = Y N N:i GR OUP BY l; j ; INSER T INTO R SELECT C:l; C:j ,avg( ( Y V:val C:val ) 2) FR OM C; Y V; Y N N WHERE Y V:i = Y N N:i GR OUP BY C:l; C:j ;
Observ e that the M step as computed in SQL has com-plexit y O ( dn ) because Y N N has n rows and Y V has dn rows. That is, the complexit y is not O ( dkn ), whic h would be the time required for a soft partition approac h like EM. This fact is key to a better performance.
 Finally , we just need to trac k K-means progress: UPD ATE mo del FR OM (SELECT sum( W R:val ) AS avg q FR OM R; W WHERE R:j = W:j )avgR SET avg q = avgR:av g q ,iteration=iteration+1;
Even though the implemen tation introduced above cor-rectly expresses K-means in SQL there are sev eral optimiza-tions that can be made. These optimizations go from physi-cal storage organization and indexing to concurren t pro cess-ing and exploiting sucien t statistics.
 We now discuss how to index tables to pro vide ecien t ac-cess and impro ve join performance. Tables Y H ( i ; Y 1 ; ::; Y and Y N N ( i ; j ) have n rows eac h, eac h has i as its primary key and both need to pro vide ecien t join pro cessing for points. Therefore, it is natural to index them on their pri-mary key i . When one row of Y H is accessed all d columns are used. Therefore, it is not necessary to individually index any of them. Table Y V ( i; l ; val ) has dn rows and needs to pro vide ecien t join pro cessing with C to compute distances and with Y N N to update W; C; R . When K-means com-putes distances squared di erences ( y li C lj ) 2 are group ed by i and j , being i the most imp ortan t factor from the per-formance point of view. To speed up pro cessing all d rows corresp onding to eac h point i are physically stored on the same disk blo ck and Y V has an extra index on i . The table blo ck size for Y V is increased to allo w storage of all rows on the same disk blo ck. The SQL to compute distances is explained below.
 For K-means the most intensiv e step is distance computa-tion, whic h has time complexit y O ( dkn ). This step requires both signi can t CPU use and I/O. We cannot reduce the num ber of arithmetic operations required since that is in-trinsic to K-means itself (although under certain constrain ts computations may be accelerated), but we can optimize dis-tance computation to decrease I/O. Recalling the SQL code given in Section 3.2 we can see distance computation re-quires joining one table with dn rows and another table with dk rows to pro duce a large intermediate table with dkn rows (call it Y kdn ). Once this table is computed the DBMS groups rows into dk groups. So a critical asp ect is being able to compute the k distances per point avoiding this huge in-termediate table Y dkn . A second asp ect is determining the nearest cluster given k distances for i 2 1 : : : n . Determining the nearest cluster requires a scan on Y D , reading kn rows, to get the minim um distance per point, and then a join to determine the subscript of the closest cluster. This requires joining kn rows with n rows.

To reduce I/O we prop ose to compute the k distances "in parallel" storing them as k columns of Y D . Then the new de nition for table Y D is Y D ( i ; d 1 ; d 2 ; : : : ; d mary key i , where d j = d ( y i ; C j ), the distance from point i to the j th cen troid. This decreases I/O since disk space is reduced (less space per row, index on n rows instead of kn rows) and the k distances per point can be obtained in one I/O instead of k I/Os. This new scheme requires chang-ing the represen tation of matrix C to have all k values per dimension in one row or equiv alen t, con taining one cluster cen troid per column, to prop erly compute distances. This leads to a join pro ducing a table with only n rows instead of kn rows, and creating an intermediate table with dn rows instead of dkn rows. Thus C is stored in a table de ned as C ( l ; C 1 ; C 2 ; : : : ; C k ), with primary key l and indexed by l . At the beginning of eac h E step column C is copied from a table W CR to table C . Table W CR is related to sucien t statistics concepts and will be introduced later. The SQL to compute the k distances is as follo ws: INSER T INTO YD SELECT i FR OM Y V; C WHERE Y V:l = C:l GR OUP BY i ;
Observ e eac h dimension of eac h point in Y V is paired with the corresp onding cen troid dimension. This join is e-cien tly handled by the query optimizer because Y V is large and C is small. An alternativ e implemen tation with UDFs, not explored in this work, would require to have a di eren t distance UDF for eac h value of d , or a function allo wing a variable num ber of argumen ts (e.g. the distance between y and C j would be distance ( y 1 i ; C 1 j ; y 2 i ; C 2 j This is because UDFs can only tak e simple data types ( oat-ing point num bers in this case) and not vectors. Eciency would be gained by storing matrix C in cac he memory and avoiding the join. But a solution based on joins is more elegan t and simpler and time complexit y is the same. The disadv antage about k distances being all in one row is that the SQL min () aggregate function is no longer use-ful. We could transform Y D into a table with kn rows and then use the same approac h introduced in Section 3.2 but that transformation and the subsequen t join would be slow. Instead we prop ose to determine the nearest cluster using a CASE statemen t instead of calling the min () aggregate function. Then the SQL to get the subscript of the closest cen troid is: INSER T INTO Y N N SELECT i , CASE END FR OM Y D ;
It becomes eviden t from this approac h there is no join needed and the searc h for the closest cen troid for one point is done in main memory . The nearest cen troid is determined in one scan on Y D . Then I/O is reduced from (2 kn + n ) I/Os to n I/Os. Observ e that the j th WHEN predicate has k j terms. That is, as the searc h for the minim um distance con tinues the num ber of inequalities to evaluate decreases. however, the CASE statemen t has time complexit y O ( k 2 ) instead of O ( k ) whic h is the usual time to determine the nearest cen troid. So we sligh tly a ect K-means performance from a theoretical point of view. But I/O is the main perfor-mance factor and this CASE statemen t works in memory . If k is more than the maxim um num ber of columns allo wed in the DBMS Y D and C can be vertically partitioned to over-come this limitation. This code could be simpli ed with a User De ned Function "argmin()" that returns the sub-script of the smallest argumen t. The problem is this function would require a variable num ber of argumen ts.
 Now we turn our atten tion to how to accelerate K-means us-ing sucien t statistics. Sucien t statistics have been sho wn to be an essen tial ingredien t to accelerate data mining al-gorithms [2, 4, 14, 7]. So we explore how to incorp orate them into a SQL-based approac h. The sucien t statistics for K-means are simple. Recall from Section 2 X j repre-sen ts the set of points in cluster j . We introduce three new matrices N; M; Q to store sucien t statistics. Matrix N is k 1, matrices M and Q are d k . Observ e their sizes are analogous to W; C; R sizes and that Q j represen ts a di-agonal matrix analogous to R j . N j stores the num ber of points, M j stores the sum of points and Q j stores the sum of squared points in cluster j resp ectiv ely. Then N j = j X M equations W; C; R are computed as W j = N j = P k J =1 W J C
To update parameters we need to join Y N N , that con-tains the partition of Y into k subsets, with Y V , that con-tains the actual dimension values. It can be observ ed that from a database point of view sucien t statistics allo w mak-ing one scan over the partition X j given by Y N N group ed by j . The imp ortan t point is that the same statemen t can be used to update N; M; Q if they are stored in the same table. That is, keeping a denormalized scheme. So instead of having three separate tables, N; M; Q are stored on the same table. But if we keep sucien t statistics in one table that leads to also keeping the clustering results in one table. So we introduce table de nitions: N M Q ( l; j ; N; M; Q ) and W CR ( l; j ; W; C; R ). Both tables have the same structure and are indexed by the primary key ( l; j ). So these table de nitions substitute the table de nitions for the Standard K-means implemen tation introduced above. Then the SQL to update sucien t statistics is as follo ws: INSER T INTO N M Q SELECT FR OM Y V; Y N N WHERE Y V:i = Y N N:i GR OUP BY l; j ;
By using table N M Q the SQL code for the M step gets simpli ed and becomes faster to update W CR .
 UPD ATE W CR SET W = 0; UPD ATE W CR SET
W = N , C =CASE WHEN N &gt; 0 THEN M=N ELSE C END , R =CASE WHEN N &gt; 0 THEN Q=N ( M=N ) 2 WHERE N M Q:l = W CR:l AND N M Q:j = W CR:j ; UPD ATE WCR SET W=W/mo del.n;
An INSER T/SELECT statemen t, although equiv alen t and more ecien t, would eliminate clusters with zero points from the output. We prefer to explicitly sho w those clusters. The main adv antages of using sucien t statistics compared Stan-dard K-means, is that M and Q do not dep end on eac h other and together with N they are enough to update C; R (elim-inating the need to scan Y V ). Therefore, the dep endence between C and R is remo ved and both can be updated at the same time. Summarizing, Standard K-means requires one scan over Y N N to get W and two joins between Y N N and Y V to get C and R requiring in total three scans over Y N N and two scans over Y V . This requires reading (3 n + 2 dn ) rows. On the other hand, Optimized K-means, based on sucien t statistics, requires only one join and one scan over Y N N and one scan over Y V . This requires reading only ( n + dn ) rows. This fact speeds up the pro cess considerably . Table W CR is initialized with dk rows having columns W; R set to zero and column C initialized with k random points tak en from CH . Table CH is initialized as describ ed in Section 3.2. Then CH is copied to column C in W CR . At the beginning of eac h E step W CR:C is copied to table C so that table C is curren t.
Exp erimen ts were conducted on a Teradata mac hine. The system was an SMP (parallel Symmetric Multi-Pro cessing) with 4 nodes, having one CPU eac h running at 800 Mhz, and 40 AMPs (Access Mo dule Pro cessors) running Teradata V2R4 DBMS. The system had 10 terab ytes of available disk space. The SQL code generator was programmed in the Java language, whic h connected to the DBMS through the JDBC interface.
Figure 1 sho ws scalabilit y graphs. We conducted our tests with syn thetic data sets having defaults d = 8 ; k = 8 ; n = 1000 k (with means in [0,10] and unitary variance) whic h rep-resen t typical problem sizes in a real database environmen t. Since the num ber of iterations K-means tak es may vary de-pending on initialization we compared the time for one itera-tion. This pro vides a fair comparison. The rst graph sho ws performance varying d , the second graph sho ws scalabilit y at di eren t k values and the third graph sho ws scalabilit y with the most demanding parameter: n . These graphs clearly sho w sev eral di erences among our implemen tations. Opti-mized K-means is alw ays the fastest. Compared to Standard K-means the di erence in performance becomes signi can t as d; k; n increase. For the largest d; k; n values Optimized K-means is ten orders of magnitude faster than Standard K-means. Extrap olating these num bers, we can see Optimized K-means is 100 times faster than Standard K-means when d = 32, k = 32 and n = 1 M (or d = 32, k = 8, n = 16 M ) and 1000 times faster when d = 32, k = 32 and n = 16 M . Researc h on implemen ting data mining algorithms using SQL includes the follo wing. Asso ciation rules mining is ex-plored in [12] and later in [5]. General data mining primi-tives are prop osed in [3]. Primitiv es to mine decision trees are introduced in [4, 13]. Programming the more powerful EM clustering algorithm in SQL is explored in [8].
Our focus was more on the side of writing ecien t SQL code to implemen t K-means rather than prop osing another "fast" clustering algorithm [1, 2, 14, 9, 7]. These algorithms require a high level programming language to access memory and perform complex mathematical operations. The way we exploit sucien t statistics is similar to [2, 14]. This is not the rst work to explore the implemen tation of a cluster-ing algorithm in SQL. Our K-means prop osal shares some similarities with the EM algorithm implemen ted in SQL [8]. This implemen tation was later adapted to cluster gene data [11], with basically the same approac h. We explain imp or-tan t di erences between the EM and K-means implemen-tations in SQL. K-means is an algorithm strictly based on distance computation, whereas EM is based on probabilit y computation. This results in a simpler SQL implemen ta-tion of clustering with wider applicabilit y. We explored the possibilit y of using sucien t statistics in SQL, whic h are crucial to impro ve performance. The clustering mo del is stored in a single table, as opp osed to three tables. Sev-eral asp ects related to table de nition, indexing and query optimization not addressed before are now studied in de-tail. A fast K-means protot ype to cluster data sets inside a relational DBMS using disk-based matrices is presen ted in [10]. The disk-based implemen tation and the SQL-based implemen tation are complemen tary solutions to implemen t K-means in a relational DBMS.
This article introduced two implemen tations of K-means in SQL. The prop osed implemen tations allo w clustering large data sets in a relational DBMS eliminating the need to ex-port or access data outside the DBMS. Only standard SQL was used; no special data mining extensions for SQL were needed. This work concen trated on de ning suitable ta-bles, indexing them and writing ecien t queries for clus-tering purp oses. The rst implemen tation is a naiv e trans-lation of K-means computations into SQL and serv es as a framew ork to introduce an optimized version with sup erior performance. The rst implemen tation is called Standard K-means and the second one is called Optimized K-means. Optimized K-means computes all Euclidean distances for one point in one I/O, exploits sucien t statistics and stores the clustering mo del in a single table. Exp erimen ts evaluate performance with large data sets focusing on elapsed time per iteration. Standard K-means presen ts scalabilit y prob-lems with increasing num ber of clusters or num ber of points. Its performance graphs exhibit nonlinear beha vior. On the other hand, Optimized K-means is signi can tly faster and exhibits linear scalabilit y. Sev eral SQL asp ects studied in this work have wide applicabilit y for other distance-based clustering algorithms found in the database literature.
There are man y issues that deserv e further researc h. Even though we prop osed an ecien t way to compute Euclidean distance there may be more optimizations. Sev eral asp ects studied here also apply to the EM algorithm. Clustering very high dimensional data where clusters exist only on pro-jections of the data set is another interesting problem. We want to cluster very large data sets in a single scan using SQL com bining the ideas prop osed here with User De ned Functions, OLAP extensions, and more ecien t indexing. Certain computations may warran t de ning SQL primitiv es to be programmed inside the DBMS. Suc h constructs would include Euclidean distance computation, piv oting a table to have one dimension value per row and another one to nd the nearest cluster given sev eral distances. The rest of com-putations are simple and ecien t in SQL. [1] C. Aggarw al and P. Yu. Finding generalized pro jected [2] P. Bradley , U. Fayyad, and C. Reina. Scaling [3] J. Clear, D. Dunn, B. Harv ey, M.L. Heytens, and [4] G. Graefe, U. Fayyad, and S. Chaudh uri. On the [5] H. Jamil. Ad hoc asso ciation rule mining as SQL3 [6] J.B. MacQueen. Some metho ds for classi cation and [7] C. Ordonez. Clustering binary data streams with [8] C. Ordonez and P. Cereghini. SQLEM: Fast clustering [9] C. Ordonez and E. Omiecinski. FREM: Fast and [10] C. Ordonez and E. Omiecinski. Ecien t disk-based [11] D. Papadop oulos, C. Domeniconi, D. Gunopulos, and [12] S. Sara wagi, S. Thomas, and R. Agra wal. Integrating [13] K. Sattler and O. Dunemann. SQL database [14] T. Zhang, R. Ramakrishnan, and M. Livn y. BIR CH :
