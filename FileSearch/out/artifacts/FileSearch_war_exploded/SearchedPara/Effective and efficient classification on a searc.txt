 Aris Anagnostopoulos  X  Andrei Broder  X  Kunal Punera Abstract Traditional document classification frameworks, which apply the learned classi-fier to each document in a corpus one by one, are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely through the inverted index of a large scale search engine. Our main goal is to build the  X  X est X  short query that characterizes a document class using operators normally available within search engines. We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. As part of our study, we enhance some of the feature-selection techniques that are found in the literature by forcing the inclusion of terms that are negatively correlated with the target class and by making use of term correlations; we show that both of those techniques can offer significant advantages. Moreover, we show that optimizing the efficiency of query execution by careful selection of terms can further reduce the query costs. More precisely, we show that on our set-up the best 10-term query can achieve 93% of the accuracy of the best SVM classifier (14,000 terms), and if we are willing to tolerate a reduction to 89% of the best SVM, we can build a 10-term query that can be executed more than twice as fast as the best 10-term query. Keywords Text classification  X  Search engine  X  Feature selection  X  Query efficiency  X  WAND  X  Term correlations 1 Introduction Automatic document classification is one of the fundamental problems of Information Retrieval and has been amply explored in the literature [ 13 , 16 , 18 , 19 , 25 ]. Applications include the classification of emails as spam or not, or into different mail folders, automatic topic classification of news articles, and so on. In most existing classification frameworks, including the ones just mentioned, the trained classifier is applied to instances being classi-fied one by one, predicting a class label for each. This method of deployment is useful when these documents are not all available at the same time and works well when only a small number of documents are to be classified. However, consider the scenario where we want to use a classifier to find a relatively small set of on-topic documents from a corpus of a billion documents. Here, the standard method of applying the classifier one by one to each document in order to determine whether it is on topic is impractical, and especially so when new classification problems appear frequently.

In this paper we show how to efficiently and effectively classify documents in a large-scale corpus that has been processed primarily for the purpose of searching. Thus, our methods access documents solely through the inverted index of a large-scale search engine. Here, our main goal is to build the  X  X est X  query that characterizes a document class (topic), which can then be used to retrieve the documents in the class from the search engine. This index query runs in time proportional to the size of the result set ( X  X utput-sensitive X ) and independent of the corpus size.

Apart from the efficiency considerations mentioned above, there are also several other reasons for taking this approach:  X  Most previous classification frameworks assume that the documents are preprocessed  X  New classification problems might appear at any moment driven by regulatory or busi- X  Once a class has been characterized by a query, one can rerun the query whenever the  X  Several search engines such as Vivisimo, Ask, Kosmix, etc. cluster the top-k search results
There are, of course, also trade-offs: in order to build the  X  X est X  query that characterizes a class, we need to balance effectiveness (what precision/recall is achievable) against efficiency (how fast can this query be executed) and implementation complexity (what capabilities are required from the underlying search engine: term weights, negated terms, etc.). Our classifi-cation approach is inherently limited to the features normally indexed by search engines and requires a modest amount of access to the internals of the underlying search engine (basically, we need to be able to explicitly set weights on query terms, and for further optimization we need to know the lengths of posting lists). However, as we show below, with respect to this feature set and these constraints, one can perform very well. 1.1 Contributions In this paper we discuss various ways to create queries to retrieve documents that belong to a concept. We investigate several classification methodologies in order to create a representa-tive query that can be easily implemented within a search-engine framework. In particular, we consider formulae-based approaches such as Naive Bayes classification and techniques from relevance feedback in information retrieval. We compare our approach with more advanced machine-learning methods (Support Vector Machines), and we show that our approach is very effective. We select the terms for inclusion in the query by applying several feature-selection techniques: we also specifically examine the effect of adding negatively-weighted terms and of the correlations between terms on performance. To further increase the effi-ciency, we investigate the factors that determine the running time and we incorporate them into our query-creation framework.

We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. Moreover, we show that optimizing the efficiency of query execution by careful selection of these terms can further reduce the query costs. More precisely, we show that on our data set, with our method, the best 10-term query can achieve 93% of the accuracy of the best SVM classifier (14,000 terms), and if we are willing to tolerate a reduction to 89% of the best SVM, we can build a 10-term query that runs more than twice as fast as the best 10-term query.

In summary, we make the following contributions:  X  We introduce a framework for performing classification by making use of the existing  X  We show that by selecting only a few terms (fewer than 10) we can achieve classification  X  We study the effect of negatively-weighted terms and of term correlations during the  X  We perform a study to estimate the execution time of a query as a function of the terms
In Sect. 2 we present some past work on similar problems and we examine how our work is related. We define the basic problem and our solution to it in Sect. 3 . In Sect. 4 we show how we apply these ideas and present an empirical evaluation. Finally, we conclude the paper in Sect. 5 . 2 Related work In this section we review some related work in the areas of machine learning for automatic text classification, and relevance feedback and query modification in information retrieval. Later in the section we provide a comparison of our work with the prior art. 2.1 Automatic text classification Automatic text classification is a heavily studied area in machine learning and numerous techniques have been proposed for the problem [ 16 , 18 ]. Of these, Support Vector Machines (SVMs) have been shown to be especially effective [ 13 , 19 ]. Multinomial Naive Bayes (NB) [ 15 ], although simple and efficient, has also been observed to be effective. Our approach in this paper relies on the use of both SVM and NB classifiers and we describe them in brief in Sect. 3.4 . Moreover, we use SVMs with linear kernels to compute the best text classification performance achievable with the full set of features (see Sect. 4.3 ). 2.2 Relevance feedback and query modification The idea of modifying a user query in order to increase precision and recall is a very old one in the field of information retrieval. One of the most popular strategies is relevance feedback , where the user identifies the relevant results returned by a query. The system then uses this information to create a new query that will potentially retrieve more documents that are related to those that the user has marked as relevant and, therefore, might be of interest to him. The main two modifications to a query are the query expansion ,inwhich terms from the relevant documents are added to the query, and term re-weighting ,inwhich the weights of the terms in the original query are modified based on the user X  X  selections. In Sect. 3 ,wereview Rocchio X  X  algorithm [ 22 ]and RTFIDF that are popular relevance feed-back mechanisms for the vector model and probabilistic model [ 21 ] respectively. Both these query weighting schemes are used in our approach in this paper. For details about model-ing in information retrieval and about relevance feedback, we refer readers to the book of Baeza-Yates and Ribeiro-Neto [ 1 ].

Chang and Hsu [ 5 ] combine document clustering and relevance feedback ideas to address the problem of providing coarse-grained feedback. They build a meta search-engine, which operates as follows. First it accepts an initial query from the user which it submits to sev-eral commercial search engines. The ensemble of the collected documents is clustered and presented to the user, who specifies the clusters that are relevant to him. Finally, the original query is expanded and re-weighted by applying a method similar to Rocchio X  X  algorithm.
 The same motivation is behind the work of Glover et al. [ 10 ]. The authors use a Support Vector Machine (SVM) classifier with a Gaussian kernel to create a query that can fetch docu-ments of a specific type (e.g., user homepages). The goal is to build a meta search-engine that creates queries for different search engines and combines the results. This work is extended in [ 8 ], where further techniques are employed on top of SVMs to increase recall. 2.3 Comparison with our work There are many differences in the motivation, problem setting, and approach of our work from prior art. The primary difference from existing work on text classification is our equal emphasis on efficiency as well as effectiveness of classification. We discuss methods for construction of queries that are not only accurate but can also be executed very fast. Another important difference is that in our problem setting the classifier does not have just black-box access to the search engine, but instead can access and make use of internal information such as sizes of posting lists. Moreover, we use the WAND primitive, described in Sect. 3.2.1 , which allows the use of weights on query terms. As shown in Sect. 4 , the availability of these two additional capabilities helps us design queries that are extremely efficient and accurate. Finally, our primary goal in this paper is to show that a search engine model can be used to perform classification on very large corpora in  X  X utput-sensitive X  time with extremely little loss in accuracy. 3 Approach In this section we formally define the problem and describe our approach towards tackling it. 3.1 Problem definition In our model a document can be viewed as a vector d  X  D of features, where D is the feature space in which every document belongs. These features could be words, bi-grams, and other information that the search engine might index about the page, such as elements from its HTML structure. For concreteness, we also assume that the feature-to-document mapping is organized as an inverted-index, which is the standard way most search engines in the web store their information. However, our approach can be applied to different models. Every document is also assigned a class label, which, without loss of generality, we call positive (  X  )or negative ( ). We are given a train set D train  X  D and a disjoint test set D test  X  D .For every document d i in the train set D train , we are given its label (class) i  X  X  X  , } . Our goal is to obtain a query Q : D  X  R to the reverse index (of documents in D test ) that maximizes the number of documents of the positive class that are ranked higher than the documents of thenegativeclass.

This problem is reminiscent of statistical text classification in which we learn a classifi-cation function F : D  X  X  X  , } that assigns a class label to each document. This function is used to classify the test set of documents D test . The objective of learning is to obtain a classification function F that maximizes the number of positive (negative) testing instances that are assigned positive (negative) labels.

In the current scenario we have to operate under three potential constraints. 1. Since we are using the search-engine model, we would like to limit the size of the que-2. We would like to optimize the selection of terms for query execution time as well as 3. In certain applications (such as query refinement via concept learning) the query has to The goal of this paper is to study these and other issues associated with the use of a search-engine model for classification. 3.2 Classification via querying As we mentioned, we want to make use of the existing technology in text search in order to classify documents that have been indexed by a search engine. In our approach we obtain the text search query by first learning a classifier for the positive concept and then converting the classifier into a search-engine query.

In this paper we consider search engines based on posting lists and document-at-a-time evaluation. To our knowledge, all popular Web search engines, for example, Google, Yahoo!, Inktomi,AltaVista,andAllTheWeb,belongtothisclass.Formostcommercialsearchengines, the query format is a boolean expression involving words as terms, but might also include additional operations, such as containment of phrases, proximity operations, and so on.
In order to obtain a query from a classifier we have to make some choices. The first deci-sion involves whether the query should have weights for the terms or whether it should be boolean. Since we are going to use the query to retrieve documents in the positive class we expect a query with term weights to better approximate the decision boundary. Later in this section we show a primitive called WAND [ 2 ], which can be used to efficiently execute a weighted query over a traditional search engine.

The second decision that we need to make is whether to limit ourselves to the use of linear ear ones. This is true especially in the context of document classification, where the features used in linear classifiers correspond well to the terms used in the queries. Moreover, in high-dimensional domains such as text classification linear features suffice to find an effective discriminating boundary and provide high-quality results [ 19 , 25 ]. For these reasons we limit this study to linear classifiers. Note that using WAND and the two-level retrieval process outlined in [ 2 ] allows us to efficiently implement nonlinear classifiers as well. However, we do not provide any more details in this paper. 3.2.1 The WAND operator Here we briefly describe the WAND operator that was introduced in [ 2 ]asameanstoopti-mize the speed of weighted search queries. WAND stands for W eak AND ,orW eighted AND .The WAND operator takes as arguments a list of Boolean variables X 1 , X 2 ,..., X k , a list of associated positive weights , w 1 ,w 2 ,...,w k , and a threshold  X  .

By definition, WAND ( X 1 ,w 1 ,..., X k ,w k , X ) is true if where x i is the indicator variable for X i ,thatis
In the original version of WAND , which appeared in [ 2 ], X i indicates the presence of query term t i in document d , and making use of it has as a result the feature space being { 0 , 1 } m ( m is the total number of features (terms)), which only exploits information of whether a word exists in a document or not. Since we want to work on the richer feature space R m , which contains frequencies of words in documents, we set A second generalization of WAND in this work is that we allow the weights of the terms to be negative; this increases the power of our query since now we can use terms that are negatively correlated with the target user concept to avoid retrieving off-topic documents. 3.3 Selecting the query terms Having presented the mechanism with which we will execute our queries, we now present the mechanism for creating them. First, we have to answer the question: How many terms should a query contain ? In general, a query with more terms retrieves more accurate results. The downside is that the cost of the query (query execution time) increases with its size. There is an inherent trade-off between query cost and retrieval accuracy, and the above question can be reframed as negotiating this trade-off.
Given that we cannot execute a query with all available terms and weights, the second question that we need to answer is: Which terms should we choose ? In this section we suggest approaches to tackle both of these questions. We describe ways to measure the quality of terms, and discuss further factors that might affect the query cost versus retrieval accuracy trade-off.

Note that the problem of selecting terms for the query is essentially the same as the problem The key difference lies in the fact that traditional feature-selection techniques emphasize the preservation of the accuracy of classification with the fewest possible features. In our work, we jointly want to optimize the processing time of a query along with its accuracy. In the rest of this section, we present our approach for achieving this goal. 3.3.1 Term selection for query accuracy In this section we present measures that score each term in the vocabulary on their impact on retrieval accuracy. Based on this scoring, the top-k terms are greedily selected to form the query. The weights of these selected terms in the WAND query are then calculated by the chosen classifier or formula. Below we present three ways to compute the term-accuracy-scores . 1. Information gain: In the past, information gain (IG) has been used as a measure of 2. Fisher index: Chakrabarti et al. [ 4 ] introduced a measure called Fisher Index (FI) score, 3. Coefficient in the linear boundary: An alternative way to construct the query is to 3.3.2 Accounting for term correlations In this section we show how we can select terms more carefully to obtain higher accuracy. The feature-selection techniques that we described examine terms independently of other terms; indeed, this is how traditionally feature-selection techniques are applied. However, the presence of a term in a query might change the effectiveness of the rest of the terms. In other words, if two terms t 1 and t 2 are positively correlated (they tend to appear in the same set of documents) then the inclusion of term t 1 in a query makes term t 2 less useful. Hence, when we select the query terms it might be beneficial to condition on the set of terms that have already been selected for the query.

To make this argument concrete we focus on one of the feature-selection methods, the information gain. In Sect. 3.3.1 we described that we select the terms t i for which the infor-mation gain IG ( t i ) is maximized. We can select terms more carefully and take into account the term frequencies with the following procedure: Initially select the term t 1 that maximizes IG ( t 1 ) . The next term that we select is the term t 2 that maximizes the conditional information gain : where H ( | t 1 ) is defined in ( 2 ), and we can define similarly H ( | t 2 , t 1 ) : More generally, the i th term is the one that maximizes the expression
Theproblem,however,isthatthetimeneededforthecomputationof H ( | t i , t i  X  1 ,..., t 1 ) increases exponentially in the number of terms. For this reason, we reduce the computational complexity and the i th term that we select is the one that maximizes the expression In words, we select the term that is the least correlated with the terms already selected and at the same time provides the maximum information. Thus, we take into account only the pairwise statistics between term occurrences; note that those contain most of the useful information [ 23 ]. We call this method Pa i r I G . 3.3.3 Term selection for query efficiency The processing time to execute a query depends on the way the search engine stores the documents and processes the queries, being roughly proportional to the number of disk accesses performed while evaluating the query. While the details are specific to each search engine, the time generally depends on the number of terms and their individual characteristics. We confirm this fact in Sect. 4.7 where we report on experiments on estimating a formula that predicts the query processing time in our search engine as a function of the terms included. Our experiments show that the query processing time is essentially proportional to the number of terms and the lengths of their posting lists.

As we mentioned earlier, there exists a trade-off between the accuracy achievable with a query and its processing time. Hence, while constructing the query we would like to include terms that have both a high term-accuracy-score and a small postings list. In order to achieve this we normalize the term-accuracy-score by the size of the postings list. Specifically, we select terms in the decreasing order of the term-selection-score value where  X  is used to trade-off between accuracy and efficiency. A higher value of  X  will prefer terms that have small postings list over terms that are deemed very important by the term accuracy measures, thus sacrificing retrieval effectiveness for efficiency. On the flip-side, using  X  = 0 results in selection terms based only on their accuracy scores. In our approach we select a fixed number of terms in this fashion, but one can easily imagine also varying the number of terms so as to fit into a budget of total sum of postings list lengths. We chose this approach, since many search engines impose limits on the number of terms they permit (e.g., as of writing, Google imposes a limit of 32 terms).

At this point we must note that the specific term-selection formula might be different for a different search engine implementation, but one can apply the same methodology that we present in this paper, and instead attempt to optimize a different function. 3.4 Weighting the query terms In the previous section we saw that a WAND query acts as a linear discriminant boundary that ranks documents based on how much towards the positive side of the boundary they lie. Hence, the weights w i in WAND queries can be computed by learning a linear classifier over the provided training instances. In this section we describe the various learning methods we use and the reasons we select them. 3.4.1 Support vector machines Support vector machines (SVMs) were introduced by Vapnik [ 24 ] as a learning approach for two-class problems. These techniques try to find a decision surface that maximizes the  X  X argin X  between the data points in different classes. Intuitively, in the linearly separable case, the SVM problem is to compute a vector w (and a threshold b ) such that while minimizing a norm of w . We will use the coefficients in this vector w to weigh the terms in our WAND query. With the use of slack variables, a similar program can be used to solve the non-separable case too. Furthermore, kernels can be used to learn nonlinear decision boundaries in the data space using SVMs.

SVMs were applied to text categorization by Joachims [ 13 ] and were found to outperform all other classification methods. Moreover, though primarily a binary classifier, SVMs have also been employed for multi-class classification. Rifkin and Klautau [ 20 ] showed that the one-vs-all strategy performed reasonably well. For these reason we use linear SVMs in a one-vs-all ensemble to indicate the  X  X est X  performance that is achievable at the classification task. While SVMs achieve very high classification accuracy, they suffer from the drawback that they are extremely computationally expensive. Hence, we consider other  X  X ormula-based X  classifiers (mentioned below), which are a function of some small number of data statistics (such as frequencies of terms). 3.4.2 Naive Bayes classifier Naive Bayes (NB) classifiers make the assumption that the occurrences of words in a doc-ument are conditionally independent given the class labels [ 15 , 16 ]. However, despite this seemingly  X  X aive X  assumption, NB has been found to be competitive for text classification problems, and even optimal under certain conditions [ 6 , 9 ]. The multinomial NB classifiers model the distribution of words in a document as a multinomial: the probability of generation of a document d that belongs to class in which the frequency of term t i is x i equals By applying Bayes X  rule, the probability that an observed document d belongs to class equals where Pr ( ) is the prior probability of a document belonging to class and Pr ( d ) is the probability of observing document d . By taking logarithms, the decision boundary ( Pr (  X  X  d ) = Pr ( | d ) ) in a two-class problem can be characterized by the following equation: or, equivalently, All the probabilities are estimated from the frequencies of the terms in the training set. As one can see, the prior probabilities of the classes do not affect the weights of the terms and only define the threshold of the linear classifier. Therefore, we set the weight for term t i in the WAND query as As we can see, learning the weights only involves a single pass over the training data to estimate the statistics and can be done very efficiently. 3.4.3 Rocchio X  X  formula Relevance feedback is one of the most successful approaches for query reformulation. One of the first, and now a standard, approach proposed to create queries is Rocchio X  X  algorithm [ 22 ], which is based on the vector-space information retrieval model (documents and queries are represented as vectors of weights of individual terms). The weights of the terms of the new query are given by where q and q m are the vectors corresponding to the old and the modified query, respectively, d is the vector corresponding to document j , C  X  ( C ) is the set of documents among the retrieved documents belonging to the positive (negative) class, and  X  ,  X  ,and  X  are constants (e.g.,  X  =  X  =  X  = 1). Often this formula is iterated several times (setting q = q m )to produce the final query.

In our work we use the Rocchio X  X  formula as a classifier to learn the weights for the WAND query. The query so generated is the difference vector of the centroid vectors of the positive class and the negative class. This query is optimal in the sense that it maximizes the difference between the average score of documents in the positive and negative classes. One should note that since we are not implementing relevance feedback, we do not have an initial query (so  X  = 0and C is the set of documents belonging to class in the training set) and we only iterate once. We also set  X  =  X  = 1. 3.4.4 RTFIDF formula A weighting mechanism similar to Rocchio X  X  that we apply is given by considering the docu-ment frequencies (number of documents that contain a given term). Specifically, we consider the rtf  X  idf formula for weighting terms: N is the total number of documents, and n i is the number of documents containing term t i . Haines and Croft [ 11 ] give an interpretation of this approach based on an inference network probabilistic model.
 There are various other term weighting approaches that we can use, for example OKAPI-BM25 [ 12 ]. However, a detailed analysis of the best term weighting scheme is beyond the scope of this paper. Our goal is to show that efficient and effective classification can be performed on a search engine model by careful construction of queries. For this purpose we experiment with the four term weighting schemes mentioned above. 3.4.5 Handling query terms with negative weights The terms that we select using the feature-selection methods can be correlated with either the positive or negative class. The formulae in Sect. 3.4 assign to those terms positive or negative weights. As we will see in the experimental section, the vast majority of the terms selected have positive weights.

The reason behind this is the one-vs-all approach that we employ for selecting terms and learning their weights. In this approach, while the positive class is a cohesive set of docu-ments, the negative class comprises documents on all other topics. Hence, potential negative terms are only present in a small fraction of documents of the negative class, thereby scoring low with the IG-based measure (which measures correlation to the class label). Similarly, as the negative class is very diverse, these terms typically have large intra-class variance and hence score low with the FI-based measure too (the FI score is inversely proportional to intra-class variance of the term).

However, including negative terms in queries is useful for distinguishing between docu-ments of closely related classes. Consider, for example, learning a query for the class soc.reli-gion.christian in the 20-Newsgroup dataset, which also contains the class alt.atheism . While these two classes share a significant common vocabulary, the term  X  X theism X  should serve as a useful negative term to separate the religious documents in the positive class from the atheist documents in the negative class. However, IG and FI scores for this term would be low because it would not be found in documents of most topics (other than alt.atheism )in the negative class. As we demonstrate in the presentation of the experimental results later in this paper, while the negative terms help in distinguishing classes that contain closely related content, they are also useful for separating classes with very different content too.
In Sect. 4.5 we examine the effect of negative query terms. We enforce the inclusion of more negative terms than the feature-selection techniques recommend. We show that in many cases this can improve the classification accuracy, even though positive terms with higher term selection scores are removed. This suggests a way to further improve feature-selection techniques that are found in the literature. 4 Experiments In this section we present an experimental study that evaluates our approach for classification with a search-engine model. Furthermore, we present an extensive exploration of the issues presented in Sect. 3 . 4.1 Datasets and experimental setup We used two datasets for our evaluation:  X  20-Newsgroup Dataset: The 20-Newsgroup dataset has been extensively used for eval- X  RCV1-v2 (Reuters) Dataset: The larger dataset that we used in our experiments is We preprocessed both datasets by removing stopwords, stemming the words using the Porter algorithm [ 17 ], and removing very frequent and very infrequent terms. (We removed terms that appear in fewer than five document and in more than 95% of the documents.)
The query-creation part of our approach, including the feature selection and the classi-fier training, was implemented and executed in the Matlab environment. Furthermore, we indexed both datasets using the Juru Search Engine developed by IBM [ 3 ]. Juru was used to execute the queries and the classification accuracy of the results returned are reported in the following sections of the paper. All reported results were averaged over five random runs of the query building and execution. This ensures that the results are robust against biases in the training set selection. Finally, for all experiments other than those in Sect. 4.8 ,wedonot use posting-list sizes for selection terms. This is done by setting  X  = 0in( 3 ). 4.1.1 Evaluation measures The query-execution step of our approach returns a ranked list of documents that match the query. One of the standard ways in information retrieval to plot the performance of such a classifier is through precision-recall graphs. Precision is the fraction of retrieved documents that are in the positive set, while recall is the fraction of documents of the positive set that were retrieved. In this paper we report the harmonic mean of these quantities, which is also known as the F X  X easure .

Receiver Operating Characteristics (ROC) graphs convey similar information as the precision-recall graphs. ROC graphs are two-dimensional graphs in which the true posi-tive rate (the fraction of the documents belonging to the positive class that were correctly classified, i.e., precision) is plotted on the y -axis and the false positive rate (the fraction of the documents belonging to the negative class that were classified as positive) is plotted on the x -axis. One of the characteristics of the ROC graphs that makes them attractive for collections of documents that are being continually modified (such as the set of documents in the World Wide Web) is the fact that the graphs are insensitive to the ratio of the sizes of the positive and negative classes. A detailed description of the ROC graphs can be found in [ 7 ].

The Juru search engine returns the documents with score higher than the threshold speci-fied by the system. Increasing the threshold results in an increase in precision and a reduction in recall. Selecting the appropriate threshold is an interesting problem and it is application specific, depending on the desired precision and recall. In our experiments, since we wanted to reproduce the entire precision-recall and ROC graphs and not a particular point, we set the threshold to 0 (since we examine only documents that appear in the query terms X  posting lists we return as a result all the documents containing at least one positive term X  X hus, formally, we set the threshold to some very small value  X  ) and then observed the scores of the documents returned (Juru provides this information in its output) to create the desired graphs.
Since we want to compare different classifications, and frequently across different param-eterizations, we need a quantitative measure of a given ROC graph. One that is often employed is the area under the curve (AUC) score of the ROC graph. A classifier that randomly clas-sifies documents has an expected AUC score of 0 . 5, while a perfect classifier has an AUC score equal to 1. 4.2 Comparing term accuracy scores In these experiments, we compared the term-accuracy-scores on their ability to create queries with good classification accuracy. In all resultant plots, the AUC of the retrieved results is plotted against the number of terms in the query for each term-accuracy measure. For com-pleteness, we present results for queries with terms weighted by all the different classifiers. Also, we performed experiments with small and large training datasets. The plots are shown in Fig. 1 .

As we can see, for all the term-weighting schemes Information Gain (IG) X  X ased term selection is slightly better than Fisher-Index (Fisher) X  X ased term selection. Both these meth-ods in turn are much better than the coefficient-based term selection. These two observations hold for all query sizes, however the difference between the performances of the measures becomes smaller as queries get larger. The only exception to the above observation is the Rocchio term-weighting scheme, where the Coefficient-based term selection performs as well as IG and Fisher. This indicates that the Rocchio formula is as useful as a term selection scheme as it is as a term weighting scheme.

A point to note is that the relative accuracies of the three term-selection measures are not impacted much by changes in the training set size. In other words, even when there is plenty of data to weight the terms in the query appropriately, careful selection of terms is extremely important. Another key observation, partially alluded to above, is that during term selection based on coefficients in the weight vector, certain classifiers perform better than others. Hence, while queries weighted by Naive Bayes using terms selected by IG and Fisher are very accurate, Naive Bayes itself doesn X  X  assign the highest absolute values to the most discriminant terms. The same is true for the RTFIDF formula. On the other hand, terms that have the highest coefficients in the linear boundaries computed by Rocchio and SVM are also very discriminative. This is further evidence to show that careful term selection is critical for creating effective queries. 4.3 Comparing term-weighting schemes In this section, term-weighting schemes are compared in terms of the accuracy of the que-ries they generate. We show results for two term-selection mechanisms (IG and Fisher) with two different training data sizes in Fig. 2 . All resultant plots have AUC of results plotted against the number of terms in the query. The  X  X est X  SVM curve plots the performance of a linear SVM classifier with the full set of terms (  X  14,000). This curve represents the best performance achievable if there were no constraints on the number of terms in the query.
The performance of all four weighting schemes is comparable with no clear winner. These resultsholdforallterm-selectionmechanisms.Asexpected,theaccuraciesofqueriesincrease as the number of terms in the queries increase, almost approaching the accuracy of the  X  X est X  possible SVM classifier. In fact, the accuracy of the query with as few as 5 X 10 terms is 90% of the  X  X est X  SVM accuracy. Hence, with careful selection and weighting of terms, we can achieve, using very few terms, classification accuracies comparable with the use of the full feature set. 4.4 Term selection by conditioning on terms In Sect. 3.3.2 we described how we can take into account second-order term interactions to construct more accurate queries. At each step, the proposed method selects the query term that is the least correlated with the terms already selected and at the same time provides the maximum information. The approach, which we call PairIG, only takes pairwise interactions into account as they capture much of the correlation information, and also for reasons of computational efficiency. We present results from our evaluation of PairIG in Fig. 3 .
In the plots, the curves with solid triangles and solid squares depict the AUC of queries with term selection performed via the PairIG method. As we can see, for all classifiers and all settings of number of training documents, the term selection by taking into account corre-lations (PairIG) results in more accurate queries than term selection with only IG. Moreover, we can see that the benefit on PairIG is higher with queries with larger number of terms. This is because these larger queries typically have chances of containing more redundant terms. Finally, there is also a clear increase in benefit of using term correlations when the number of documents in the training set are higher. This is because training on a larger number of documents results in better estimates of conditional information gain of terms. In conclusion, it is fairly clear that constructing queries by taking into account second-order interactions between terms as described in Sect. 3.3.2 results in more accurate queries. 4.5 Impact of negative terms on accuracy In Sect. 3.4.5 we saw some potential examples where negative terms that have lower weight than some positive terms can improve classification if we force them to be part of the query and replace those positive terms.

For the purposes of this paper we extended the WAND primitive to support negative weights on terms in the query. In this section we evaluate whether this additional expressive power results in more accurate queries. Here, by negative (positive) terms we mean terms that have negative (positive) weights in the WAND query.

First we want to note that both the IG and Fisher based term selection measures tend to select very few negative terms. More concretely, for the 20-Newsgroup dataset, on average only 2 out of the top-50 features as ranked by the IG term-accuracy-score are negatively correlated to the class label. The corresponding value for the Fisher measure is 7 out of the top-50. For the top-100 features, these values are 7 and 18 for the IG and Fisher based measures respectively. These observations are plotted in Fig. 4 a and b. Here we can see that for queries of all sizes very few terms selected by IG and Fisher get negative weights.
To empirically evaluate the impact of negative terms in queries, we measured the change in classification accuracy after artificially replacing a few positive terms with negative terms (which scored lower on the term-accuracy-score). We depict some results in Fig. 5 .The bottom line shows the accuracy achieved if we select only positively weighted terms, as we would have done if the search engine did not have the added functionality of WAND to handle negative terms. As expected, in this case the classification accuracy drops only marginally (recall that there were very few negative terms to begin with). In the middle line we see the accuracy when we artificially increase the fraction of negative terms in the query. Notice that in this case the accuracy is increased over the case when we do not impose any constraints (top line). Interestingly, this happens even when the positive terms being replaced are ranked higher by IG and Fisher than the negative terms replacing them. The fraction of negative terms chosen for each term-selection and term-weighting scheme combination is different and was set based on increase in the AUC score of the resulting queries. Estimating this ratio before executing a query (e.g., as a function of the corpus or the classification problem) is an interesting problem for future research.
 We have seen that inclusion of negative terms results in increases in classification accuracy. We now try to figure out which particular classes do the negative terms help discriminate. In Fig. 4 c and d we plot for each row class the fraction of its negative terms that show up as positive terms for the column classes. For instance, in the IG plot, the cell for row talk.politics.misc and column comp.graphics is labeled red, indicating that when a classi-fier is learned for talk.politics.misc , a high fraction of negative terms help distinguish it from comp.graphics . These plots show us that for many classes, a majority of negative terms are useful in separating very different classes from them. Hence, negative terms in classifiers learned for  X  X eligion-related X  classes help separate them from  X  X omputer-related X  classes, and vice versa. Negative terms also help separate the  X  X ports-related X  classes from all others.

However, for certain class pairs we can see that negative terms also help separate very fier for the comp.graphics class come from the terms of comp.os.ms-windows.misc class. We observe the same behavior in Fig. 4 d for the classifier learned for the soc.religion.christian class, where the negative terms help discriminate it from talk.religion.misc and alt.atheism . Hence,  X  X theism-related X  keywords are useful as negatively weighted terms in a query for the soc.religion.christian class. Interestingly, the converse is not true. As we can see from the figure, terms from the soc.religion.christian class do not show up as negative terms in the alt.atheism classifier. The reason is that the term  X  X theism X  which helps distinguish the two classes is prevalent in the positive set of the classifier for alt.atheism class.
In summary, we see that the ability to perform search queries with negative weights improves the effectiveness of our classification. Furthermore, there is evidence that while learning queries with very few terms in a corpus with a number of diverse classes, selecting terms solely based on IG and Fisher gives inferior results. 4.6 Accuracy on the RCV1-v2 dataset In our experiments with the RCV1-v2 dataset we created queries using the IG term-accuracy-score and the Naive Bayes term-weighting scheme. Figure 6 shows the increase in micro-averaged values of AUC and F-measure of the query results as the number of query terms is increased. To compute the F-measure, the threshold for the classifier was set using a held-out validation set of 5% of the test set documents in each class. As can be seen from the plot, the F-measure increases from 0 . 5to0 . 6 over 100 terms, and the AUC increases from 0 . 85 to 0 . 92.

In order to put these numbers in perspective we reproduce some results from [ 14 ], which though not strictly comparable (because of tuning of SVM and Rocchio parameters) are nevertheless instructive. Lewis et al. performed classification experiments using SVM and Rocchio classifiers on the RCV1-v2 dataset. As in our work, the classifiers were learned using 23,149 documents and the remaining 781,265 documents were used as the test set. However, the number of features used for learning were  X  48,000. With these settings the F-measure obtained using SVM and Rocchio was 0 . 81 and 0 . 69 respectively. We can see that using queries with very few terms (  X  0 . 1% of 48K), our approach achieves a significant fraction of the accuracy. Furthermore, our approach provides the benefit of classification of the whole corpus in  X  X utput-sensitive X  time (s). 4.7 Query execution time In this section, we report on our experiments seeking to determine the function of parameters on which query execution time depends. In Fig. 7 a, b, and c we have plotted some of our attempts, and in Fig. 7 c we see a strong indication that the query execution time is proportional to where T + ( T  X  ) is the set of terms in the query with positive (negative) weights, post ( t ) is the length of the posting list of term t ,and a , b ,and c are constants. We computed the values of these constants through regression analysis, and for the RCV1-v2 dataset their values are a = 8 . 68  X  10  X  5 , b = 3 . 57  X  10  X  6 ,and c = 4 . 15  X  10 3 .

This dependency is a result of our implementation of the WAND operator, and a different implementation will presumably demonstrate a different dependency. Despite this, the entire procedure that we have followed can be applied to other implementations to deduce results of the same type, which can later guide the term-selection mechanism.

The corresponding graph for the 20-Newsgroup dataset is depicted in Fig. 7 d. We can still see the same trend, the linear dependency of the time on the cost estimation formula presented above. However, there is more noise in this case since 20 Newsgroups is a much smaller dataset, and therefore the running time of queries is small (often a few milliseconds) making our measurements more susceptible to other factors. The constants in the cost esti-mation formula for the 20-Newsgroup dataset are a = 9 . 84  X  10  X  5 , b = 3 . 2  X  10  X  5 ,and c = 5 . 6. 4.8 Using posting-list sizes for term selection In this section we experiment with the term-selection-score in ( 3 ). Specifically, we vary the exponent  X  to observe the effect of normalizing the term-accuracy-score with the size of the term X  X  postings list. All these experiments were performed on the 20-Newsgroup dataset using RTFIDF based queries, trained with 50 documents per class.

Figure 8 a graphs the accuracy (in terms of AUC of query results) versus the number of terms in the query. The different curves represent varying amount of importance given to the size of the postings list in term selection (by varying  X  ). As expected, as  X  increases and more emphasis is laid on reducing the cost of the query at the expense of discriminative power of the terms, the accuracy decreases. Figure 8 b shows how the sum of postings list of the terms in the query increases with the number of terms in the query. Once again, the different curves represent different values of  X  . As expected, for higher  X  the rate of increase of the sum of postings list is lower. From these two plots (Fig. 8 a and b), we can conclude that  X  = 0 . 25 seems to offer the best trade-off of accuracy of query results and efficiency of query execution.

In Fig. 8 c and d we plot the ratio of AUC and  X  X uery execution time X  (cost estimate based on our investigation in Sect. 4.7 ) for the queries generated at  X  = 0 . 25 over queries generated with the highest possible values (queries created with  X  = 0). As we can see, for the IG selection measure, for as low as 10 X 15 query terms the loss in accuracy over the best possible is &lt; 7% with a  X  X ime X  savings &gt; 50%. The tradeoff is even better for the Fisher selection measure ( &lt; 3% and  X  70% respectively). For both selection measures the trade-off between accuracy and efficiency gets better as the number of terms in the query increases. Hence, we have shown that by selecting terms after normalizing the term-accuracy-scores with the size of their postings list, we can obtain very efficient queries with minimal loss in classification efficacy. 5 Conclusions In this paper, we showed how classification can be performed effectively and efficiently using a search-engine model. This model has several benefits; such as  X  X utput-sensitive X  classifica-tion time and the ability to function on a corpus processed primarily for search. We detailed a framework for construction of short queries via selection and weighting of terms. We showed that surprisingly good classification accuracy can be achieved on average by queries with as few as 10 terms. More precisely, we show that on our set-up the best 10-term query can achieve 93% of the accuracy of the best SVM classifier (which has 14,000 terms). Further, we showed that this accuracy could be boosted by judicious addition of negative terms in the query as well as by the use of term correlation in the corpus. These two techniques can be of use in other contexts where feature-selection methods are applied.

We studied the trade-offs between accuracy (effectiveness) and query processing time (efficiency). As a part of this study, we performed experiments to determine the relationship of the query execution time with number of terms and their postings list sizes in our search engine. Furthermore, we showed that by carefully selecting terms we can can further improve efficiency with minimal loss in accuracy; continuing the above example, if we are willing to tolerate an accuracy reduction to 89% of the best SVM, we can build a 10-term query that can be executed more than twice as fast as the best 10 terms query.
 References Authors Biography
 Traditional document classification frameworks, which ap-ply the learned classifier to each document in a corpus one by one, are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely through the inverted index of a large scale search engine. Our main goal is to build the  X  X est X  short query that characterizes a document class using oper-ators normally available within large engines. We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. Moreover, we show that optimizing the efficiency of query execution by careful selection of these terms can fur-ther reduce the query costs. More precisely, we show that on our set-up the best 10 terms query can achieve 90% of the accuracy of the best SVM classifier (14000 terms), and if we are willing to tolerate a reduction to 86% of the best SVM, we can build a 10 terms query that can be executed more than twice as fast as the best 10 terms query. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Measurements Text Classification, Search Engine, Feature Selection, Query Efficiency, WAND This work was done while the author was at IBM Research. Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
Automatic document classification is one of the funda-mental problems of Information Retrieval and has been am-ply explored in the literature [13, 16, 17, 18, 23]. Appli-cations include the classification of emails as spam or not, or into different mail folders, automatic topic classification of news articles, and so on. In most existing classification frameworks, including the ones just mentioned, the trained classifier is applied to instances being classified one by one, predicting a class label for each. This method of deployment is useful when these documents are not all available at the same time and works well when only a small number of doc-uments are to be classified. However, consider the scenario where we want to use a classifier to find a relatively small set of on-topic documents from a corpus of a billion docu-ments. Here, the standard method of applying the classifier one by one to each document in order to determine whether it is on topic is impractical, and especially so when new classification problems appear frequently.

In this paper we show how to efficiently and effectively classify documents in a large-scale corpus that has been pro-cessed primarily for the purpose of searching. Thus, our methods access documents solely through the inverted in-dex of a large-scale search engine. Here, our main goal is to build the  X  X est X  query that characterizes a document class (topic), which can then be used to retrieve the docu-ments in the positive set from the search engine. This index query runs in time proportional to the size of the result set ( X  X utput-sensitive X ), and independent of the corpus size.
Apart from the efficiency considerations mentioned above, there are also several other reasons for taking this approach:
There are, of course, also trade-offs: in order to build the  X  X est X  query that characterizes a class, we need to balance effectiveness (what precision/recall is achievable) against ef-ficiency (how fast can this query be executed) and imple-mentation complexity (what capabilities are required from the underlying search engine: term weights, negated terms, etc.). Our classification approach is inherently limited to the features normally indexed by search engines and requires a modest amount of access to the internals of the underly-ing search engine (basically, we need to be able to explicitly set weights on query terms, and for further optimization we need to know the length of posting lists). However, as we show below, with respect to this feature set and these constraints, one can perform very well.
In this paper we discuss various ways to create queries to retrieve documents that belong to a concept. We inves-tigate several classification methodologies in order to cre-ate a representative query that can be easily implemented within a search engine framework. In particular, we consider formulae-based approaches such as Naive Bayes classifica-tion and techniques from relevance feedback in information retrieval. We compare our approach with more advanced machine-learning methods (Support Vector Machines), and we show that our approach is very effective. We select the terms for inclusion in the query by applying several feature-selection techniques. To further increase the efficiency, we investigate the factors that determine the running time and we incorporate them into our query-creation framework.
We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. Moreover, we show that optimizing the efficiency of query execution by careful selection of these terms can further reduce the query costs. More precisely, we show that on our data set, with our method, the best 10-terms query can achieve 90% of the accuracy of the best SVM classifier (14000 terms), and if we are willing to tolerate a reduction to 86% of the best SVM, we can build a 10-terms query that runs more than twice as fast as the best 10-terms query.

In summary, we make the following contributions:
In Section 2 we present some past work on similar prob-lems and we examine how our work is related. We define the basic problem and our solution to it in Section 3. In Section 4 we show how we apply these ideas and present an empirical evaluation. Finally, we conclude the paper in Section 5.
In this section we review some related work in the areas of machine learning for automatic text classification, and relevance feedback and query modification in information retrieval. Later in the section we provide a comparison of our work with the prior art.
Automatic text classification is a heavily studied area in machine learning and numerous techniques have been pro-posed for the problem [16, 17]. Of these, Support Vector Machines (SVMs) have been shown to be especially effective [13, 18]. Multinomial Naive Bayes (NB) [15], although sim-ple and efficient, has also been observed to be effective. Our approach in this paper relies on the use of both SVM and NB classifiers and we describe them in brief in Section 3.4. Moreover, we use SVMs with linear kernels to compute the best text classification performance achievable with the full set of features (see Section 4.3).
The idea of modifying a user query in order to increase precision and recall is a very old one in the field of infor-mation retrieval. One of the most popular strategies is rele-vance feedback , where the user identifies the relevant results returned by a query. The system then uses this information to create a new query that will potentially retrieve more doc-uments that are related to those that the user has marked as relevant and, therefore, might be of interest to him. The main two modifications to a query are the query expansion , in which terms from the relevant documents are added to the query, and term re-weighting , in which the weights of the terms in the original query are modified based on the user X  X  selections. In Section 3, we review Rocchio X  X  algo-rithm [21] and RTFIDF that are popular relevance feedback mechanisms for the vector model and probabilistic model [20] respectively. Both these query weighting schemes are used in our approach in this paper. For details about modeling in information retrieval and about relevance feedback, we refer readers to the book of Baeza-Yates and Ribeiro-Neto [1].
Chang and Hsu [5] combine document clustering and rel-evance feedback ideas to address the problem of providing coarse-grained feedback. They build a meta search-engine, which operates as follows. First it accepts an initial query from the user which it submits to several commercial search engines. The ensemble of the collected documents is clus-tered and presented to the user, who specifies the clusters that are relevant to him. Finally, the original query is ex-panded and re-weighted by applying a method similar to Rocchio X  X  algorithm.
 The same motivation is behind the work of Glover et al. [10]. The authors use a Support Vector Machine (SVM) classifier with a Gaussian kernel to create a query that can fetch doc-uments of a specific type (e.g., user homepages). The goal is to build a meta search-engine that creates queries for dif-ferent search engines and combines the results. This work is extended in [8], where further techniques are employed on top of SVMs to increase recall.
There are many differences in the motivation, problem set-ting, and approach of our work from prior art. The primary difference from existing work on text classification is our equal emphasis on efficiency as well as effectiveness of clas-sification. We discuss methods for construction of queries that are not only accurate but can also be executed very fast. Another important difference is that in our problem setting the classifier does not have just black-box access to the search engine, but instead can access and make use of internal information such as sizes of posting lists. Moreover, we use the WA N D primitive, described in Section 3.2.1, which allows the use of weights on query terms. As shown in Section 4, the availability of these two additional capabil-ities helps us design queries that are extremely efficient and accurate. Finally, our primary goal in this paper is to show that a search engine model can be used to perform classifi-cation on very large corpora in  X  X utput-sensitive X  time with extremely little loss in accuracy.
In this section we formally define the problem and describe our approach towards tackling it.
In our model a document can be viewed as a vector d  X  D of features, where D is the feature space in which every docu-ment belongs. These features could be words, bi-grams, and other information that the search engine might index about the page, such as elements from its HTML structure. For concreteness, we also assume that the feature-to-document mapping is organized as an inverted-index, which is the stan-dard way most search engines in the web store their infor-mation. However, our approach can be applied to different models. Every document is also assigned a class label which, without loss of generality, we call positive (  X  )or negative ( ). We are given a train set D train  X  D and a disjoint test set D test  X  D . For every document d i in the train set D we are given its label (class) i  X  X  X  , } . Our goal is to ob-tain a query Q : D  X  R to the reverse index (of documents in D test ) which maximizes the number of documents of the positive class that are ranked higher than the documents of the negative class.

This problem is reminiscent of statistical text classifica-tion in which we learn a classification function F : D  X  { X  , } that assigns a class label to each document. This function is used to classify the test set of documents D test The objective of learning is to obtain a classification func-tion F which maximizes the number of positive (negative) testing instances that are assigned positive (negative) labels.
In the current scenario we have to operate under three potential constraints. 1. Since we are using the search-engine model, we would 2. We would like to optimize the selection of terms for 3. In certain applications (such as query refinement via The goal of this paper is to study these and other issues associated with the use of a search-engine model for classi-fication.
As we mentioned, we want to make use of the existing technology in text search in order to classify documents that have been indexed by a search engine. In our approach we obtain the text search query by first learning a classifier for the positive concept and then converting the classifier into a search-engine query.

In this paper we consider search engines based on post-ing lists and document-at-a-time evaluation. To our knowl-edge, all popular Web search engines, for example, Google, Yahoo!, Inktomi, AltaVista, and AllTheWeb, belong to this class. For most commercial search engines, the query format is a boolean expression involving words as terms, but might also include additional operations, such as containment of phrases, proximity operations, and so on.

In order to obtain a query from a classifier we have to make some choices. The first decision involves whether the query should have weights for the terms or whether it should be boolean. Since we are going to use the query to retrieve documents in the positive class we expect a query with term weights to better approximate the decision boundary. Later in this section we show a primitive called WA N D [2], which can be used to efficiently execute a weighted query over a traditional search engine.

The second decision that we need to make is whether to limit ourselves to the use of linear classifiers. In general, linear classifiers are faster to train and more interpretable than non-linear ones. This is true especially in the context of document classification, where the features used in linear classifiers correspond well to the terms used in the queries. Moreover, in high-dimensional domains such as text classi-fication linear features suffice to find an effective discrimi-nating boundary and provide high-quality results [18, 23]. For these reasons we limit this study to linear classifiers. Note that using WA N D and the two-level retrieval process outlined in [2] allows us to efficiently implement nonlinear classifiers as well. However, we do not provide any more details in this paper.
Here we briefly describe the WA N D operator that was in-troduced in [2] as a means to optimize the speed of weighted search queries. WA N D stands for W eak AND ,orW eighted AND .The WA N D operator takes as arguments a list of Boolean variables X 1 ,X 2 ,...,X k , a list of associated posi-tive weights , w 1 ,w 2 ,...,w k , and a threshold  X  . By defini-tion, WA N D ( X 1 ,w 1 ,...X k ,w k , X  ) is true iff where x i is the indicator variable for X i , that is
With X i indicating the presence of query term t i in doc-ument d ,the WA N D operator is used to return the set of documents satisfying Equation (1).

This is the original version of WA N D that appeared in [2], and making use of it has as a result the feature space being { 0 , 1 } m ( m is the total number of features (terms)), which only exploits information of whether a word exists in a document or not. Since we want to work on the richer feature space R m , which contains frequencies of words in documents, we set A second generalization of WA N D in this work is that we allow the weights of the terms to be negative; this increases the power of our query since now we can use terms that are negatively correlated with the target user concept to avoid retrieving off-topic documents.
Having presented the mechanism with which we will exe-cute our queries, we now present the mechanism for creat-ing them. First, we have to answer the question: How many terms should a query contain ? In general, a query with more terms retrieves more accurate results. The downside is that the cost of the query increases with its size. There is an in-herent trade-off between query cost and retrieval accuracy, and the above question can be reframed as negotiating this trade-off.

Given that we cannot execute a query with all available terms and weights, the second question that we need to an-swer is: Which terms should we choose ?Inthissectionwe suggest approaches to tackle both these questions. We de-scribe ways to measure the quality of terms, and discuss further factors that might affect the query cost versus re-trieval accuracy trade-off.

Note that the problem of selecting terms for the query is essentially the same as the problem of feature selection, which has been widely studied (see [4, 24]) in the context of text classification. The key difference lies in the fact that traditional feature-selection techniques emphasize the preservation of the accuracy of classification with the fewest possible features. In our work, we jointly want to optimize the processing time of a query along with its accuracy. In the rest of this section, we present our approach for achieving this goal.
In this section we present measures that score each term in the vocabulary on their impact on retrieval accuracy. Based on this scoring, the top-k terms are greedily selected to form the query. The weights of these selected terms in the WA N D query are then calculated by the chosen clas-sifier or formula. Below we present three ways to compute the term-accuracy-scores . 1. Information Gain: In the past, information gain 2. Fisher Index: Chakrabarti et al. [4] introduced a 3. Coefficient in the Linear Boundary: An alter-
The processing time to execute a query depends on the way the search engine stores the documents and processes the queries, being roughly proportional to the number of disk accesses performed while evaluating the query. While the details are specific to each search engine, the time gen-erally depends on the number of terms and their individual characteristics. We confirm this fact in Section 4.6 where we report on experiments on estimating a formula that relates the query processing time in our search engine as a function of the terms included. Our experiments show that the query processing time is essentially proportional to the number of terms and the lengths of their posting lists.

As mentioned earlier, there exists a trade-off between the accuracy achievable with a query and its processing time. Hence, while constructing the query we would like to include terms that have both a high term-accuracy-score and a small postings list. In order to achieve this we normalize the term-accuracy-score by the size of the postings list. Specifically, we select terms in the decreasing order of the term-selection-score value where  X  is used to trade-off between accuracy and efficiency. A higher value of  X  will prefer terms that have small postings list over terms that are deemed very important by the term accuracy measures, thus sacrificing retrieval effectiveness for efficiency. On the flip-side, using  X  =0resultsinselection terms based only on their accuracy scores. In our approach we select a fixed number of terms in this fashion, but one can easily imagine also varying the number of terms so as to fit into a budget of total sum of postings list lengths. We chose this approach, since many search engines impose limits on the number of terms they permit (e.g., as of writing Google imposes a limit of 32 terms).

At this point we must note that the specific term selection formula might be different for a different search engine im-plementation, but one can apply the same methodology that we present in this paper, and instead attempt to optimize a different function.
In the previous section we saw that a WA N D query acts as a linear discriminant boundary that ranks documents based on how much towards the positive side of the bound-ary they lie. Hence, the weights w i in WA N D queries can be computed by learning a linear classifier over the provided training instances. In this section we describe the various learning methods we use and the reasons we select them.
Support Vector Machines (SVMs) were introduced by Vap-nik [22] as a learning approach for two class problems. These techniques try to find a decision surface that maximizes the  X  X argin X  between the data points in different classes. Intu-itively, in the linearly separable case, the SVM problem is to compute a vector w (and a threshold b ) such that while minimizing a norm of w . Wewillusethecoefficientsin this vector w to weigh the terms in our WA N D query. With the use of slack variables, a similar program can be used to solve the non-separable case too. Furthermore, kernels can be used to learn nonlinear decision boundaries in the data space using SVMs.

SVMs were applied to text categorization by Joachims [13] and were found to outperform all other classification meth-ods. Moreover, though primarily a binary classifier, SVMs have also been employed for multi-class classification. Rifkin [19] showed that the one-vs-all strategy performed reasonably well. For these reason we use linear SVMs in an one-vs-all ensemble to indicate the  X  X est X  performance that is achiev-able at the classification task. While SVMs achieve very high classification accuracy, they suffer from the drawback that they are extremely computationally expensive. Hence, we consider other  X  X ormula-based X  classifiers (mentioned be-low) that are a function of some small number of data statis-tics (such as frequencies of terms, etc.).
Naive Bayes (NB) classifiers make the assumption that the occurrences of words in a document are conditionally independent given the class labels [15, 16]. However, de-spite this seemingly  X  X aive X  assumption, NB has been found to be competitive for text classification problems, and even optimal under certain conditions [6, 9]. The multinomial NB classifiers model the distribution of words in a docu-ment as a multinomial. Using the Bayes rule, the decision boundary in a two class problem can be characterized as the following equation: log Pr (  X  )+ log Pr ( )+ of occurrences of term t i in the document being classified, Pr ( ) is the prior probability of a document belonging to class ,and Pr ( t i | ) is the probability of term t i appear-ing in a document of class . (All these probabilities are estimated from the frequencies of the terms in the training set.) As one can see, the prior probabilities of the classes do not affect the weights of the terms and only define the threshold of the linear classifier. Therefore we set the weight for term t i in the WA N D query as As we can see, learning the weights only involves a single pass over the data to estimate the statistics X  X ven this can be avoided in some cases if the search engine stores precom-puted statistics X  X nd can be done very fast.
Relevance feedback is one of the most successful approaches for query reformulation. One of the first, and now a stan-dard, approach proposed to create queries is Rocchio X  X  algo-rithm [21], which is based on the vector information retrieval model (documents and queries are represented as vectors of weights of individual terms). The weights of the terms of the new query are given by where q and q m are the vectors corresponding to the old and the modified query, respectively, d j is the vector correspond-ing to document j , C  X  ( C ) is the set of documents among the retrieved documents belonging to the positive (negative) class, and  X  ,  X  ,and  X  are constants (e.g.,  X  =  X  =  X  =1). Often this formula is iterated several times (setting q = q to produce the final query.

In our work we use the Rocchio X  X  formula as a classifier to learn the weights for the WA N D query. The query so generated is the difference vector of the centroid vectors of the positive class and the negative class. This query is op-timal in the sense that it maximizes the difference between the average score of documents in the positive and negative classes. One should note that since we are not implement-ing relevance feedback, we do not have an initial query (so  X  =0and C is the set of documents belonging to class in the training set) and we only iterate once. We also set  X  =  X  =1.
A weighting mechanism similar to Rocchio X  X  that we apply is given by considering the document frequencies (number of documents that contain a given term). Specifically, we consider the rtf  X  idf formula for weighting terms: w where w i is the weight given to term t i , tf ( d j ,t i quency of term t i in document d j , N is the total number of documents, and n i is the number of documents containing term t i . Haines and Croft [11] give an interpretation of this approach based on an inference network probabilistic model.
There are various other term weighting approaches that we can use, for example OKAPI-BM25 [12]. However, a de-tailed analysis of the best term weighting scheme is beyond the scope of this paper. Our goal is to show that efficient and effective classification can be performed on a search engine model by careful construction of queries. For this purpose we experiment with the four term weighting schemes men-tioned above.
In this section we present an experimental study that eval-uates our approach for classification with a search-engine model. Furthermore, we present an extensive exploration of the issues presented in Section 3.
We used two datasets for our evaluation:
The query-creation part of our approach, including the feature selection and the classifier training, was implemented and executed in a Matlab environment. Furthermore, we in-dexed both datasets using the Juru Search Engine developed by IBM [3]. Juru was used to execute the queries and the classification accuracy of the results returned are reported in the following sections of the paper. All reported results were averaged over 5 random runs of the query building and execution. This ensures that the results are robust against biases in the training set selection. Finally, for all experi-ments other than those in Section 4.7, we do not use posting-list sizes for selection terms. This is done by setting  X  =0 in Equation (2).
The query execution step of our approach returns a ranked list of documents that match the query. One of the standard (a) Naive Bayes (50 docs) (b) SVM (50 docs) (c) Naive Bayes (200 docs) (d) SVM (200 docs) Figure 1: Plots depict AUC of results of queries with terms chosen by different selection measures vs. number of terms in the query, in the 20 News-groups dataset. Plots (a) and (b) correspond to training set sizes of 50 docs per class, while (c) and (d) correspond to training with larger datasets (200 docs per class). ways in information retrieval to plot the performance of such a classifier is through precision-recall graphs. Precision is the fraction of retrieved documents that are in the positive set, while recall is the fraction of documents of the positive set that were retrieved. In this paper we report the harmonic mean of these quantities, which is also known as the F X  measure .

Receiver Operating Characteristics (ROC) graphs convey similar information as the precision-recall graphs. ROC graphs are two-dimensional graphs in which the true positive rate (the fraction of the documents belonging to the positive class that were correctly classified) is plotted on the Y -axis and the false positive rate (the fraction of the documents belonging to the negative class that were classified as posi-tive) is plotted on the X -axis. One of the characteristics of the ROC graphs that makes them attractive for collections of documents that are being continually modified (such as the set of documents in the World Wide Web) is the fact that the graphs are insensitive to the ratio of the sizes of the positive and negative classes. A detailed description of the ROC graphs can be found in [7].

The Juru search engine returns the documents with score higher than the threshold specified by the user. Increasing the threshold results in an increase in precision and a re-duction in recall. Selecting the appropriate threshold is an interesting problem and it is application specific, depend-ing on the desired precision and recall. In our experiments, since we wanted to reproduce the entire precision-recall and ROC graphs, and not a particular point, we set the thresh-old to 0 (returning, as a result all the documents containing at least one positive term) and then observed the scores of the documents returned (Juru provides this information in its output) to create the desired graphs. Figure 2: AUC of queries with terms weighed by different classifiers vs. number of terms in the query for the 20 Newsgroups dataset. The x-axis on both plots is the number of terms in the query. Plot (a) corresponds to training set sizes of 50 docs per class, while plot (b) corresponds to training with larger datasets (200 docs per class).

Since we want to compare different classifications, and often across different parameterizations, we need a quanti-tative measure of a given ROC graph. One that is often employed is the area under the curve (AUC) score of the ROC graph. A classifier that randomly classifies documents has an expected AUC score of 0 . 5, while a perfect classifier has an AUC score equal to 1.
In these experiments, we compared the term-accuracy-scores on their ability to create queries with good classi-fication accuracy. In all resultant plots, the AUC of the retrieved results is plotted against the number of terms in the query for each term-accuracy measure. For robustness, we present results for queries with terms weighted by two different classifiers. Also, we performed experiments with small and large training datasets. The plots are shown in Figure 1.

As we can see, for both the term weighting schemes In-formation Gain (IG) X  X ased term selection is slightly better than Fisher-Index (FI) X  X ased term selection. Both these methods in turn are much better than the Coefficient-based term selection. These two observations hold for all query sizes, however the difference between the performances of the measures becomes smaller as queries get larger. These observations also hold for the other term weighting schemes (RTFIDF and Rocchio) as well. A point to note is that the relative accuracies of the three term selection measures are not impacted much by changes in the training set size. In other words, even when there is plenty of data to weight the terms in the query appropriately, careful selection of terms is extremely important. Another key observation is that while selecting terms based on coefficients in the weight vector, the SVM based weighting outperforms the Naive Bayes based one. Interestingly, this difference disappears with other term selection mechanisms. This is further evidence to show that careful term selection is critical for creating effective queries.
In this section, term weighting schemes are compared in terms of the accuracy of the queries they generate. For robustness, we used two term selection mechanisms with two different training data sizes. The results for the IG term selection measure are shown in Figure 2 (results for the Fisher measure are similar but not shown because of paucity of space). All resultant plots have AUC of results plotted against the number of terms in the query. The  X  X est X  SVM curve plots the performance of a linear SVM classifier with the full set of terms (  X  14 , 000). This curve represents the best performance achievable if there were no constraints on the number of terms in the query.

The performance of all four weighting schemes is compa-rable with no clear winner. These results hold for all term-selection mechanisms. As expected, the accuracies of queries increase as the number of terms in the queries increase, al-most approaching the accuracy of the  X  X est X  possible SVM classifier. In fact, the accuracy of the query with as few as 5-10 terms is 90% of the  X  X est X  SVM accuracy. Hence, with careful selection and weighting of terms, we can achieve, us-ing very few terms, classification accuracies comparable with the use of the full feature set.
For the purposes of this paper we extended the WA N D primitive to support negative weights on terms in the query. In this section we evaluate whether this additional expressive power results in more accurate queries. Here, by negative (positive) terms we mean terms that have negative (positive) weights in the WA N D query.

First we want to note that both the IG and FI based term selection measures tend to select very few negative terms. More concretely, for the 20 Newsgroups dataset, on aver-age only 2 out of the top-50 features as ranked by the IG term-accuracy-score are negatively correlated to the class label. The corresponding value for the FI measure is 7 out of the top-50. For the top-100 features, these values are 7 and 18 for the IG and FI based measures respectively. The reason behind this is the one-vs-all approach that we em-ploy for selecting terms and learning their weights. In this approach, while the positive class is a cohesive set of docu-ments, the negative class comprises documents on all other topics. Hence, potential negative terms are only present in a small fraction of documents of the negative class, thereby scoring low with the IG based measure (which measures cor-relation to the class label). Similarly, as the negative class is very diverse, these terms typically have large intra-class variance and hence score low with the FI based measure too Figure 3:  X  X o constraints X  corresponds to accu-racy of queries with terms selected by the IG mea-sure. From the terms scored by IG, we selected equal number of positive and negative weighted terms (plotted as  X  X qual Negative Weights X ). We also plotted accuracy of queries with  X  X o Negative Weights X . (FI score is inversely proportional to intra-class variance of the term).

However, including negative terms in queries is useful for distinguishing between documents of closely related classes. Consider for example, learning a query for the class talk.religion in the 20 Newsgroup dataset, which also contains the class alt.atheism . While these two classes share a significant com-mon vocabulary, the term  X  X theist X  should serve as a useful negative term. However, IG and FI scores for this term would be low because it would not be found in documents of most topics (other than alt.atheism ) in the negative class.
In order to empirically evaluate the impact of negative terms in queries, we measured the change in classification accuracy after artificially replacing a few positive terms with negative terms (that scored lower on the term-accuracy-score). We depict some results in Figure 3. The bottom line shows the accuracy achieved if we select only positively weighted terms, as we would have done if the search engine did not have the added functionality of WA N D to handle negative terms. As expected, in this case the classification accuracy drops only marginally (recall that there were very few negative terms to begin with). In the top line we see Figure 4: AUC and F-measure of the query results vs the number of query terms. the accuracy when we ensure that the number of positive and negative terms in the query is the same. Notice that in this case the accuracy is increased over the case when we do not impose any constraints (middle line). Interestingly, this happens even when the positive terms being replaced are ranked higher by IG than the negative terms replacing them. The choice of 0 . 5 as the ratio of the number of posi-tive terms over the total number of terms here is arbitrary; in general we expect the optimal ratio of positive to all the terms to be corpus and query dependent (between 0 . 5and 1). Estimating this ratio before executing a query (e.g., as a function of the corpus) is an interesting problem for future research.

In summary, we see that the ability to perform search queries with negative weights improves the effectiveness of our classification. Furthermore, there is evidence that while learning queries with very few terms in a corpus with a num-ber of diverse classes, selecting terms solely based on IG and FI gives inferior results.
In our experiments with the RCV1-v2 dataset we cre-ated queries using the IG term-accuracy-score and the Naive Bayes term weighting scheme. Figure 4 shows the increase in micro-averaged values of AUC and F-measure of the query results as the number of query terms is increased. To com-pute the F-measure, the threshold for the classifier was set using a held-out validation set of 5% of the test set doc-uments in each class. As can be seen from the plot, the F-measure increases from 0 . 5to0 . 6 over 100 terms, and the AUC increases from 0 . 85 to 0 . 92.

In order to put these numbers in perspective we reproduce some results from [14], which though not strictly compara-ble (because of tuning of SVM and Rocchio parameters) are nevertheless instructive. Lewis et al. performed classi-fication experiments using SVM and Rocchio classifiers on the RCV1-v2 dataset. As in our work, the classifiers were learned using 23,149 documents and the remaining 781,265 documents were used as the test set. However, the number of features used for learning were  X  48 , 000. With these set-tings the F-measure obtained using SVM and Rocchio was 0 . 81 and 0 . 69 respectively. We can see that using queries with very few terms (  X  0 . 1% of 48K), our approach achieves a significant fraction of the accuracy. Furthermore, our ap-proach provides the benefit of classification of the whole cor-pus in  X  X utput-sensitive X  time (seconds). Figure 5:  X  X uery execution time X  (in ms) is plotted against various parameters for the Reuters and 20 Newsgroups dataset. All these queries were created using the Naive Bayes weighting scheme and the IG term selection measure. Section 4.6 contains details about the final cost-estimation formula we obtain.
In this section, we report on our experiments seeking to determine the function of parameters on which query execu-tion time depends. In Figures 5(a), 5(b), and 5(c) we have plotted some of our attempts, and in Figure 5(c) we see a strong indication that the query execution time is propor-tional to 0 @ a  X   X   X  T +  X   X   X  where T + ( T  X  ) is the set of terms in the query with positive (negative) weights, post ( t ) is the length of the posting list of term t ,and a , b ,and c are constants. We computed the values of these constants through regression analysis, and for the RCV1-v2 dataset their values are a =8 . 68  X  10  X  5 b =3 . 57  X  10  X  6 ,and c =4 . 15  X  10 3 .
 This dependency is a result of our implementation of the WA N D operator, and a different implementation will pre-sumably demonstrate a different dependency. Despite this, the entire procedure that we have followed can be applied to other implementations to deduce results of the same type, that can later guide the term-selection mechanism.
The corresponding graph for the 20 Newsgroups dataset is depicted in Figure 5(d). We can still see the same trend, the linear dependency of the time on the cost estimation formula presented above. However, there is more noise in this case since 20 Newsgroups is a much smaller dataset, and therefore the running time of queries is small (often a few milliseconds) making our measurements more susceptible to other factors. The constants in the cost estimation formula for the 20 Newsgroups dataset are a =9 . 84  X  10  X  5 , b = 3 . 2  X  10  X  5 ,and c =5 . 6. (a) IG+RTFIDF(50 docs) (b) IG+RTFIDF(50 docs) (c) IG+RTFIDF(50 docs) (d) Fisher+RTFIDF(50 docs) Figure 6: Graphs (a) and (b) depict the AUC and sum of postings lists as a function of the number of query terms for different values of  X  (defined in Section 3.3.2). Graphs (c) and (d) depict the Ratio of AUC and the Cost Estimate for queries created by  X  =0 . 25 and  X  =0 , for different query sizes.
In this section we experiment with the term-selection-score in Equation (2). Specifically, we vary the exponent  X  to observe the effect of normalizing the term-accuracy-score with the size of the term X  X  postings list. All these experiments were performed on the 20 Newsgroups dataset using RTFIDF based queries, trained with 50 documents per class.

Figure 6(a) graphs the accuracy (in terms of AUC of query results) versus the number of terms in the query. The dif-ferent curves represent varying amount of importance given to the size of the postings list in term selection (by vary-ing  X  ). As expected, as  X  increases and more emphasis is laid on reducing the cost of the query at the expense of dis-criminative power of the terms, the accuracy decreases. Fig-ure 6(b) shows how the sum of postings list of the terms in the query increases with the number of terms in the query. Once again, the different curves represent different values of  X  . As expected, for higher  X  therateofincreaseofthe sum of postings list is lower. From these two plots (Fig-ures 6(a) and 6(b)), we can conclude that  X  =0 . 25 seems to offer the best trade-off of accuracy of query results and efficiency of query execution.

In Figures 6(c) and 6(d) we plot the ratio of AUC and  X  X uery execution time X  (cost estimate based on our inves-tigation in Section 4.6) for the queries generated at  X  = 0 . 25 over queries generated with the highest possible values (queries created with  X  = 0). As we can see, for the IG se-lection measure, for as low as 10-15 query terms the loss in accuracy over the best possible is &lt; 7% with a  X  X ime X  sav-ings &gt; 50%. The tradeoff is even better for the FI selection measure ( &lt; 3% and  X  70% respectively). For both selec-tion measures, the trade-off between accuracy and efficiency gets better as the number of terms in the query increases. Hence, we have shown that by selecting terms after normal-izing the term-accuracy-scores with the size of their postings list, we can obtain very efficient queries with minimal loss in classification efficacy.
We showed how classification can be performed effectively and efficiently using a search-engine model. This model has several benefits; such as  X  X utput-sensitive X  classification time and the ability to function on a corpus processed pri-marily for search. We detailed a framework for construction of short queries via selection and weighting of terms. We showed that surprisingly good classification accuracy can be achieved on average by queries with as few as 10 terms. Further, we showed that this accuracy could be boosted by judicious addition of negative terms in the query.
We studied the trade-offs between accuracy (effectiveness) and query-processing time (efficiency). As a part of this study, we performed experiments to determine the relation-ship of the query execution time with number of terms and their postings list sizes in our search engine. Furthermore, we showed that by carefully selecting terms we can can fur-ther improve efficiency with minimal loss in accuracy.
For our large scale experiments with the Juru search en-gine, we would like to thank David Carmel, Steve Gates, Jas-mine Novak, and Wilfried Teiken for their help and support. Ravi Kumar and Andrew Tomkins participated in the early discussions that led to the problem formulation and to the paper approach. Finally, we also want to thank Rie Ando, Roberto Bayardo, David Gondek, David Johnson, and Tong Zhang for many useful discussions and suggestions on the content of the paper. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and [3] D. Carmel, E. Amitay, M. Herscovici, Y. S. Maarek, [4] S.Chakrabarti,B.Dom,R.Agrawal,and [5] C.-H. Chang and C.-C. Hsu. Enabling concept-based [6] P. Domingos and M. J. Pazzani. On the optimality of [7] T. Fawcett. ROC graphs: Notes and practical [8] G.W.Flake,E.J.Glover,S.Lawrence,andC.L.
 [9] J. H. Friedman. On bias, variance, 01 loss, and the [10] E.J.Glover,G.W.Flake,S.Lawrence,W.P.
 [11] D. Haines and W. B. Croft. Relevance feedback and [12] M. Hancock-Beaulieu, M. Gatford, X. Huang, S. E. [13] T. Joachims. Text categorization with support vector [14] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [15] A. McCallum and K. Nigam. A comparison of event [16] T. Mitchell. Machine Learning . McGraw Hill, 1997. [17] J. R. Quinlan. C4.5: programs for machine learning . [18] J. Rennie and R. Rifkin. Improving multiclass text [19] R. Rifkin and A. Klautau. In defense of one-vs-all [20] S. E. Robertson and K. S. Jones. Relevance weighting [21] J. J. Rocchio. Relevance feedback in information [22] V. Vapnik. The Nature of Statistical Learning Theory . [23] Y. Yang and X. Liu. A re-examination of text [24] Y. Yang and J. Pedersen. A comparative study on
