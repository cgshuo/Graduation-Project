 Learning Multiple Behaviors from Unlabeled Demonstrations in a Latent Con-troller Space. The complete penalized log-likelihood of the model corresponding to the MAP estimate given the prior on B can be written as: decomposes into two different terms. The first term corresponds to the likeli-hood of the trajectory given its assignment to one of the K clusters. The model parameters  X  k of each cluster and eliminates the need to simulate trajectories for the different clusters. The second term is the sparse term that encodes the penalty and does not directly depend on the correspondences. As in the DPMM algorithm, we used a Laplacian prior that results in Lasso estimates. tively over two steps. First, the E-Step computes the expectation of the hidden portional to the likelihood of each trajectory given the component parameters . The M-step, on the other hand, minimizes the sum of the square errors with a L1 norm and weighted by the expectation of the correspondences E ( c i ) com-and the convex formulation we perform the maximization step using a general convex optimization solver. lihood model is a Multivariate Normal and the prior is a Laplacian distribution. After resolving the integral over  X  we get the following expression A . The integral requires to decompose matrix A = SDS  X  1 to compute the In the DPMM q 0 is defined as in our case f ( Y |  X  ) = N ( Y | X  X  , X  2 I T  X  T ) = Manipulating the numerator of the exponent replacing (2),(3) and (4) in (1) results q = q = and express the integral as a product of unidimensional integrals need to use the following change of variable.
 reals and that it exists an orthogonal matrix S such that A can be decomposed in the form A = SDS  X  1 = SDS T with D = S  X  1 AS a diagonal matrix whose diagonal elements are the eigenvalues of A denoted as d i . Using this property, we can apply the following change of variable an orthogonal matrix, the Jacobian determinant is | S | = 1 and d X  1  X  X  X  d X  p = dz 1  X  X  X  dz p . Now we can rewrite de equation (9) as I = I = expressed as a product of integrals greater or equal to zero we can say the in general there will be  X  p eigenvalues can rewrite
I = to eigenvalues d i &gt; 0.
 I arbitrary constants and make a new change of variable (22) to the complementary error function T B
T = C T R with R a matrix of positive values because each element is the absolute value of S and that C T = (  X   X / X ,  X  X  X  ,  X   X / X  ) being  X  and  X  positive the components e i comes from E T = J T S . In this case it holds that J T S = I
