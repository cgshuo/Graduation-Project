 Semi-automatic schema matching systems have been developed to compute mapping suggestions that can be corrected by a user. However, constructing and tuning match strategies still requires a high manual effort. We therefore propose a self-configuring schema matching system that is able to automatically adapt to the given mapping problem at hand. Our approach is based on analyzing the input schemas as well as intermediate match results. A variety of matching rules use the analysis results to automatically construct and adapt an underlying matching process for a given match task. The evaluation shows that our system is able to robustly return good quality mappings across different mapping problems and domains. D.2.12 [ Interoperability ]: Data mapping Algorithms Matching system, rewrite rules, features, matching process Finding mappings between complex metadata structures is a critical task in a number of domains such as data integration, ontology alignment or model transformation. A multitude of schema matching systems and matching algorithms were proposed (see [1] for an overview) to speed up that process. Except for some domain-specific matchers, the algorithms used in the different systems are often similar, e.g. they consider the linguistic and structural similarity of schema elements or the similarity of instance data. Many systems are constructed for a single schema type or domain and may even be tuned for specific benchmarks such as the OAEI Benchmark [5]. Moreover, schema matching publications typically report the maximally achieved quality of automatically computed mapping suggestions using some specially tuned parameter configuration. This construction and tuning is a complex, manual and time-consuming task that often requires substantial matching experience and expert knowledge as well as given perfect mappings. However, such an approach cannot be adopted for applying a schema matching system in practice onto fully unknown matching problems. Additionally, given default configurations may not be matchers to a single aggregated similarity matrix. Finally, a selection operator extracts the most probable element pairs from a similarity matrix and sets all other values to zero (see [15] for an overview to selection strategies). From the finally selected matrix a mapping between a source schema S and a target schema T can be constructed. A mapping consists of a set of correspondences (s, t, sim) referring to a source-and a target element as well as a similarity value. Matching systems not only differ in the implementation of these basic operators but also by the order in which these operators can be executed. In this paper we adopt the notion of matching processes similar to eTuner [14]. A matching process (or matching strategy) is represented by a directed acyclic graph describing the execution order of operators such as match, aggregation or selection. It contains all steps necessary to come from two input schemas to a final mapping. Operators in the graph have one or more similarity matrices as input and return a similarity matrix as output. The topology of a matching process can be very different. Simple topologies that are commonly used are parallel, sequential and iterative execution of matchers. These basic topologies may also be combined in more sophisticated match strategies. In general, tuning a matching system involves defining the underlying matching process structure, selecting the appropriate operators and parameterizing individual operators. Our system should be able to automatically choose promising matchers, aggregation and selection operations and their parameters. Also the structure of a matching process should be automatically adaptable. We took inspiration from how a matching expert would interactively develop and execute a matching process. She analyzes the source and target schema, selects appropriate matchers and constructs an initial matching process. The process is executed and the result is inspected. Depending on that result, certain parts or parameters of the matching process can be changed and extended manually. Our approach performs similarly but in an automatic way. In order to automate the analysis step, we rely on so-called features. These are computed from the input schemas but also from intermediate mapping results. Features try to give some indication about schema properties or the quality of a mapping. Computed features are used by so-called matching rules representing expert knowledge about a relation between features and operators or process patterns. Finally, an adaptive process execution system (see Figure 1) selects and applies rules and incrementally executes the constructed process. Our adaptive matching system gets two schemas as input and returns a mapping as output. Ideally, no further parameterization input should be needed when running the system. All necessary parameters should be defined automatically. The system consists of a registry that contains a number of so called feature analyzers, matching rules as well as an operator library that contains all necessary operators in particular the matchers, aggregation or selection operators. The core component of the system is the adaptive process construction that implements a staged rule application approach that we describe below. In a preprocessing step all schema features of the input schemas are computed. The adaptive process construction starts with an empty We identified different types of matching rules that are starting-, aggregation-, rewrite-, refine and selection rules. Starting rules can be applied to an empty process when no intermediate matrix was computed yet. Each application of a starting rule creates so called dangling nodes that are possible end-points of the process and do not have following nodes. Aggregation rules add aggregation operators to a process and combine a number of dangling operators from a matching process. Rewrite rules change the structure of a given process without changing the input and dangling output nodes. For instance the order of operators could be changed or additional operations can be added in between others. Selection Rules are used to add a selection operator to the last dangling node of the current matching process. The different kinds of rules are applied in a certain order to control the degree of process adaptations. Different rule classes depend on each other since some rules add operators and others combine the output of several operators as done by the aggregation rule. In order to restrict complexity we perform the application of rules only within a fixed number of stages as shown in the left part of Figure 2. This reduces the structural diversity an adaptively created process can have but simplifies the rule selection process significantly. If all rules would be able to compete in all stages of the process side-effects of rule application could not be controlled and termination could not be ensured. The process starts with importing the input schema and analyzing them to compute the schema features. An empty matching process is created. In the next stage starting rules can be selected and applied. Starting rules mostly add element-based matchers. However, also more complex starting rules can be defined that already construct an advanced matching process structure, e.g. to enforce the sequential execution of several matchers. In the next stage the dangling nodes from applying the starting rules need to be combined and the result matrices are aggregated. After the aggregation, rewrite rules can be applied. If no relevant rewrite rules can be found, a selection rule is applied. Based on the selection result the process can be finished or refine rules can be applied iteratively. Within each stage of the process a predefined rule selection process is started (see right state chart from Figure 2). The selection process begins with filtering all rules that can be executed within the current stage. This set of rules is only created once within a stage. If the remaining set of rules is empty the stage can be finished directly. If there are rules left to be applied, their relevance is computed for each rule using the rules relevance functions. The most relevant rule is selected and applied. Aggregation operator can be removed. In (4) a rewrite rule has been found relevant and applied; it adds a selection operator after the matcher operator. In (5) a refine rule that adds a path matcher after the dangling selection to refine the intermediate match result. Finally an aggregate/selection rule is applied that combines the results of both matchers. In the evaluation we want to investigate the effectiveness and robustness of our approach. For that reason we compare our adaptive schema matching system to currently known alternative approaches for diverse problems. We currently have implemented 20 feature analyzers and 18 rules. For details, please refer to the extended version of the paper. We consider a wide range of schema mapping problems from different domains. To be comparable to existing work we include the OAEI-Benchmark and Anatomy as well as the ModelCVS Benchmark [6] in our evaluation. The Purchase Order dataset and mappings were already used in the early evaluations from COMA [2] and they are publicly available. We also use them for computing a default configuration to compare against. We precomputed a best configuration for the Purchase Order dataset similar to the way the default strategy proposed in [2] was computed. We call this computed strategy our DEFAULT configuration. It consists of the matchers WeightedName, Path, Children, Leaf, Sibling and Datatype. The best selection strategy found was parameterized with delta=0.021 and a threshold=0.5 (see [2] for details on selection strategies). Secondly we implemented the Meta-level learning proposed in [4]. This approach is a valid comparison since it includes schema features in a learning process of a decision tree to increase adaptivity. The learning also takes the Purchase Order dataset as gold mappings. We include all schema features in the learning process. The learning was implemented using the weka 2 library. The computed decision trees are then translated into matching processes without loss of information. In order to reduce possible overfitting we restricted the size of the decision tree. We then executed the default configuration, the decision tree and our new adaptive matching system with all provided mapping problems. The results of our evaluations are summarized in Figure 4. For each dataset we compared the average achieved F-Measure of the individual approaches DEFAULT, DT and ADAPT. Our adaptive approach (ADAPT) returned good results across all different test cases and thus proved its viability. For the Purchase Order data set ADAPT is only closely behind the DEFAULT http://www.cs.waikato.ac.nz/~ml/weka/index.html 
