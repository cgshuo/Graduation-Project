 consist of the continuous sequence of characters (called Hanzi) without no space between words. Therefore, word segmentation is a necessary initial step to pro-cess the Chinese language. Previous research shows that word segmentation mod-els trained on labeled data are reasonably accurate.
 mostly based on sequence labeling algorithm with word-based or character-based features[16,12,14,1]. These methods use the discriminative model with millions of overlapping binary features.
 designed features to obtain the best performance. Intuitively, the complex fea-tures can give more accurate prediction than simple features, and these meth-ods often perform remarkably well. However, they still suffer from the out-of-vocabulary (OOV) words (namely, unknown words) problem. Although the ac-curacy of OOV can be improved greatly by the character-based methods [16], it is still significantly lower than the accuracy of in-vocabulary (IV) words. in paradigmatical similar way, into abstract representation.
 is subject to Zipf X  X  law. As reported in [18], though modern Chinese character sets normally include about 10,000-20,000 characters, most of them are rarely used in everyday life. Typically, 2,500 most used Chinese characters can cover 97.97% text, while 3,500 characters can cover 99.48% text. in an abstraction way, which can also bridge the gap between high and low frequency characters.
 Brown algorithm [2]. However, Brown algorithm classifies all the same charac-ters into a single cluster, but Chinese characters may have many senses, hard clustering algorithm such as the Brown algorithm may not be able to deal with multiple senses gracefully. Moreover, Brown algorithm generally tends to clus-ter the characters which significantly occur together in text. The characters in same cluster have syntagmatical similarity, not paradigmatical similarity. [10] also reports that the features with these clusters do not improve performance on CWS.
 straction on character levels. In Chinese, some characters are semantically or paradigmatically similar. We abstract characters into their semantic concept space. We propose a semi-supervised k-means clustering method to cluster the similar characters to the same class according their context. The map function is learned from large-scale raw texts, which can capture paradigmatic similarity among characters. Our approach yields a relative error reduction of 24 : 83 % and an improvement of OOV recall of 34 : 92 % in average with character abstraction over the baseline.
 works in section 2, then we describe the background of character-based word segmentation in section 3. Section 4 presents our character abstraction method. The experimental results are manifested in section 5. Finally, we conclude our work in section 6. and finds that they do not improve performance on CWS. One problem might be that Chinese characters have many more senses than English words, so a hard clustering algorithm such as the Brown algorithm may not be able to deal with multiple senses gracefully. can improve performance of CWS.
 [11] embeddings of words on other sequence labeling tasks (Named Entity Recog-nition and chunking) and find that each of the three word representations im-proves the accuracy of these baselines. tion. Each character is labeled as one of {B, I, E, S} to indicate the segmentation. {B, I, E} represent Begin , Inside , End of a multi-character segmentation respec-tively, and S represents a Single character segmentation.
 we can label x with a score function, where w is the parameter of score function S ( ) . The feature vector ( x ; y ) consists of lots of overlapping features, which is the chief benefit of discriminative model. We use online Passive-Aggressive (PA) algorithm [5,6] to train the model parameters. Following [3], the average strategy is used to avoid the overfitting problem. the characters with same semantic concepts into a single cluster. Different with English letters, Chinese characters are associated with full or partial semantic concepts. For example, the characters  X  X  X  (chickens)  X  ,  X  X  X  (ducks)  X  and  X  X  X  (geese)  X  have the same concept  X  X owl X . They are used in the same way to compose words with other characters, such as  X  X  X  (head)  X  ,  X  X  X  (feet)  X  and  X  X  X  (meat)  X  . Since characters that appear in similar contexts (especially surrounding words) tend to have similar meanings, we can use clustering technologies to find the semantic concepts from large-scale texts.
 of them partitions sets of words/characters into subsets of semantically similar words/characters.
 derives a hierarchical clustering of words from unlabeled data.
 ,  X  X  X  /  X  X  and  X  X  X  /  X  X  , the other clusters are not what we expected. The char-acters in each cluster are often collocation relations. These clusters are helpful for other NLP tasks, such as text classification, but may be misleading in CWS. 4.1 Semi-supervised K-means Cluster supervised K-means clustering method to map each character to its correspond-ing concepts based on its context. edge Database[7] as a initial guide for semantic concepts, then we use k-means algorithm to cluster on large-scale unlabeled texts.
 character. We extract all single characters and the corresponding semantic con-cepts from HowNet and categorize them by their semantic concepts. Each cate-gory represents a different semantic concept or meaning. There are 7 ; 117 char-acters and 3 ; 666 categories in total. 2 ; 979 characters belong to more than one category. Among them,  X  X  X  X  has the most meanings and belongs to 32 different categories, such as  X  X ozen X ,  X  X raw X ,  X  X eat X ,  X  X uild X ,  X  X all X  and so on. one meaning in certain context. So we need map different occurrences of a char-acter to different categories based on their different contexts. We use k-means algorithm [8] to automatically learn the map function from large scale texts. semantic categories defined in HowNet.
 where x is every occurrence of character and f ( x ) means the context feature of x ; CH i represents the i th category defined in HowNet. All the feature vectors are extracted from unlabeled data. succeeding and the union of them as features. For example: the features of the character  X  X  X  X  in sequence  X  X  X  X  X  X  X  will be {-1:  X  , 1:  X  ,  X  X  X  }.
 distance we used here is Euclidean distance. The cluster center will be updated when it changes. We make a restriction that the assigned cluster for each char-acter must be one of its categories defined in HowNet.
 learn the cluster centers. The corpus contains 1 ; 060 ; 471 ; 497 characters and has 9 ; 851 unique characters. For the characters which are not included in HowNet, we classify them into  X  X nknown X  category.
 the character with its category, we can avoid the problem of data sparsity on some level.
 5.1 Dataset 2010[17]. This dataset is well known and widely adopted. The training corpus which contains one month data of the People X  X  Daily in 1998 was provided by Peking University. There are four domains in the testing data: Literature (L), Computer (C), Medicine (M) and Finance (F). One main reason we use these corpora is that it addresses the ability of word segmentation for out-of-domain text.
 natural language processing.
 1. B : The baseline method. We use usually the commonly used features in 2. B+C : Besides the features used in baseline method, we use the character 3. B+CA : Besides the features used in baseline method, we use our proposed improve the performances. While the improvement is very limited to simply use these information B+C , our method ( B+CA ) achieves large improvements on and 26 : 88 % respectively on four datasets. The average relative error reduction is 24 : 83 %. Meanwhile, the recalls OOV words are also improved by 14 : 44 %, 81 : 44 %, 10 : 61 % and 33 : 20 % respectively. The average improvement of OOV recalls is 34 : 92 %. ent methods. We can see that our method ( B+CA ) uses fewer parameters than the baseline, which indicates the character abstraction can merge the characters used similarly and results to reduction of actually active features. 5.2 Analysis still boost the performance with less actually active features. However, we also found a number of inconsistent or irrational annotations in segmentation both in the training and the test data. For example,  X  X  X  X  X  X  X  (Construction Bank)  X  is segmented while  X  X  X  X  X  X  X  (Bank of China)  X  is used a word. These inconsistent or irrational annotations may have more impact for abstraction based method than character-based method because they can influence the process of feature abstraction. accuracy of out-of-vocabulary word. The experiments have shown that: abstract representation can improve the performance over the baseline. In future work, we would also like to investigate the other methods for character abstraction and we believed that good abstract features can boost the performance of CWS. This work was funded by NSFC (No.61003091 and No.61073069).

