 University of Edinburgh
This article considers the automatic evaluation of information ordering, a task underlying many text-based applications such as concept-to-text generation and multidocument summarization.
We propose an evaluation method based on Kendall X  X   X  , a metric of rank correlation. The method is inexpensive, robust, and representation independent. We show that Kendall X  X  reliably with human ratings and reading times. 1. Introduction
The systematic evaluation of natural language processing (NLP) systems is an impor-tant prerequisite for assessing their quality and improving their performance. Tradition-ally, human involvement is called for in evaluating systems that generate textual output. Examples include text generation, summarization, and, notably, machine translation.
Human evaluations consider many aspects of automatically generated texts ranging from grammaticality to content selection, fluency, and readability (Teufel and van Halteren 2004; Nenkova 2005; Mani 2001; White and O X  X onnell 1994).
 tions must be performed quickly and frequently, has encouraged many researchers to seek ways of evaluating system output automatically. Papineni et al. (2002) proposed B
LEU , a method for evaluating candidate translations by comparing them against ref-erence translations (using n -gram co-occurrence overlap). Along the same lines, the content of a system summary can be assessed by measuring its similarity to one or more manual summaries (Hovy and Lin 2003). Bangalore, Rambow, and Whittaker (2000) introduce a variety of quantitative measures for evaluating the accuracy of an automatically generated sentence against a reference corpus string.
 involve the following desiderata. First, they measure numeric similarity or closeness of system output to one or several gold standards. Second, they are inexpensive, robust, and ideally language independent. Third, correlation with human judgments is an important part of creating and testing an automated metric. For instance, several studies have shown that B LEU correlates with human ratings on machine translation quality (Papineni et al. 2002; Doddington 2002; Coughlin 2003). Bangalore,
Rambow, and Whittaker (2000) demonstrate that tree-based evaluation metrics for surface generation correlate significantly with human judgments on sentence quality and understandability. Given their simplicity, automatic evaluation methods cannot be considered as a direct replacement for human evaluations (see Callison-Burch, Osborne, and Koehn [2006] for discussion on some problematic aspects of B can be usefully employed during system development, for example, for quickly assessing modeling ideas or for comparing across different system configurations (Papineni et al. 2002; Bangalore, Rambow, and Whittaker 2000).
 choice (e.g., words or phrases shared between reference and system translations), con-tent selection (e.g., document units shared between reference and system summaries), and grammaticality (e.g., how many insertions, substitutions, or deletions are required to transform a generated sentence to a reference string). Another promising, but, less studied, avenue for automatic evaluation is information ordering. The task con-cerns finding an acceptable ordering for a set of preselected information-bearing items (Lapata 2003; Barzilay and Lee 2004). It is an essential step in concept-to-text genera-tion, multidocument summarization, and other text synthesis problems. Depending on the application and domain at hand, the items to be ordered may vary greatly from propositions (Karamanis 2003; Dimitromanolaki and Androutsopoulos 2003) to trees (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). It is therefore not surprising that evaluation methods have concentrated primarily on the generated orders, thus abstracting away from the items themselves.
 correlation, as a means of estimating the distance between a system-generated and a human-generated gold-standard order. Rank correlation is an appealing way of eval-uating information ordering: It is a well-understood and widely used measure of the strength of association between two variables; it is computed straightforwardly and can operate over distinct linguistic units (e.g., sentences, trees, or propositions). Indeed, several studies have adopted Kendall X  X   X  as a performance measure for evaluating the output of information-ordering components both in the context of concept-to-text generation (Karamanis and Mellish 2005; Karamanis 2003) and summarization (Lapata 2003; Barzilay and Lee 2004; Okazaki, Matsuo, and Ishizuka 2004).

Kendall X  X   X  correlates with human judgments on the information-ordering task. This is in marked contrast with other automatic evaluation methods that have been shown dertake two studies that examine whether there is indeed a relationship between  X  and behavioral data. We first briefly introduce Kendall X  X   X  and explain how it can be employed for evaluating information ordering (Section 2). Next, we present a controlled experimental study that examines whether Kendall X  X   X  is correlated with human ratings (Section 3).
 not fine-grained enough to rule out possible confounds. In the information-ordering task, for example, we cannot be certain that subjects rate a document low because it is genuinely badly organized and, therefore, difficult to comprehend or because they are unfamiliar with its content or simply disinterested or distracted. Similar confounds also arise in the evaluation of the output of MT systems, where it may be difficult to tease apart whether subjects X  ratings reflect their assessment of the quality of the translated text or its subject matter and structure. To eliminate such confounds, we follow our judgment elicitation study with an on-line reading experiment and demonstrate that  X  is also correlated with processing time (Section 4). Our second experiment provides 472 additional evidence for the validity of  X  as a measure of text well-formedness. Discus-sion of our results concludes the article. 2. Kendall X  X  Measure
In common with other automatic evaluation methods, we assume that we have access to a reference output that in most cases will be created by one or several humans. Our task is to compare a system-produced ordering of items against a reference order. For ease of exposition, let us assume that our information-ordering component is part of a generation application whose ultimate goal is to generate coherent and understandable text. It is not crucially important how the items to be ordered are represented. They can be facts in a database (Duboue and McKeown 2001), propositions (Karamanis 2003), discourse trees (Mellish et al. 1998), or sentences (Lapata 2003; Barzilay and Lee 2004).
Table 1 gives an example of a reference text containing 10 items (A X  X ) and the orders (i.e., rankings) produced by two hypothetical systems. We can then calculate how much the system orders differ from the reference order, the underlying assumption being that acceptable orders should be fairly similar to the reference. A number of metrics can be employed for this purpose, such as Spearman X  X  correlation coefficient ( r data, Cayley distance, or Kendall X  X   X  (see Lebanon and Lafferty [2002] for an overview).
Here we describe Kendall X  X   X  (Kendall 1938) and explain why it is an appropriate choice for information-ordering tasks.
 orderings of Y ,and S (  X  ,  X  ) the minimum number of adjacent transpositions needed to bring  X  to  X  . Kendall X  X   X  is defined as: where N is the number of objects (i.e., items) being ranked. As can be seen, Kendall X  X   X  is based on the number transpositions, that is, interchanges of consecutive elements, necessary to rearrange  X  into  X  . In Table 1 the number of transpositions can be calculated by counting the number of intersections of the lines. The  X  between the Reference and
System 1 is 0.82, between the Reference and System 2 is 0.24, and between the two systems is 0.15. The metric ranges from  X  1 (inverse ranks) to 1 (identical ranks). The calculation of  X  must be appropriately modified when there are tied rankings (Hays 1994; Siegel and Castellan 1988).
 sidered in this article. The metric is sensitive to the fact that some items may be always ordered next to each other even though their absolute orders might differ. It also penal-izes inverse rankings. Comparison between the Reference and System 2 gives a  X  of 0.24 even though the orders between the two models are identical modulo the beginning and the end. This seems appropriate given that flipping the introduction in a document with the conclusions seriously disrupts coherence.

Both coefficients use the same amount of information in the data, and thus both have the same sensitivity to detect the existence of association. This means that for a given data set, both measures will lead to rejection of the null hypothesis at the same level of significance. However, the two measures have different underlying scales, and, numeri-cally, they are not directly comparable to each other. Siegel and Castellan (1988) express the relationship of the two measures in terms of the inequality: can be interpreted as a simple function of the probability of observing concordant and discordant pairs (Kerridge 1975). In other words, it is the difference between the probability that in the observed data two variables are in the same order versus the probability that they are in different orders (the probability is rescaled to range from to 1 as is customary for correlation; see equation (1)). Unfortunately, no simple meaning can be attributed to r s . The latter is the same as a Pearson product X  X oment correlation coefficient ( r p ) computed for values consisting of ranks. Although r percent of variance shared by two variables in the case of r difficult to draw any meaningful conclusions with regard to information ordering based on the variance of ranks.
 situations where they diverge. For example, the statistical distribution of  X  approaches the normal distribution faster than r s (Kendall and Gibbons 1990), thus offering an advantage for small to moderate sample studies with 30 or fewer data points. This is crucial when experiments are conducted with a small number of subjects (a situation common in NLP) or test items. Another related issue concerns sample size. Spearman X  X  rank correlation coefficient is a biased statistic (Kendall and Gibbons 1990). The smaller the sample the more r s diverges from the true population value, usually underestimat-ing it. In contrast, Kendall X  X   X  does not provide a biased estimate of the true correlation.
Furthermore,  X  maintains good control of type I error rates (i.e., rejecting the null hypothesis when it is actually true). Arndt, Turvey, and Andreasen (1999) undertake an extensive empirical study and show that the number of times  X  incorrectly signals a significant correlation when there is none is close to the nominal 5% using a p &lt; 0 . 05 significance criterion. For a more detailed discussion of the advantages of  X  over r we refer the interested reader to Kendall and Gibbons (1990) and Arndt, Turvey, and
Andreasen (1999). 3. Experiment 1: Judgment Elicitation
To assess whether Kendall X  X   X  reliably correlates with human ratings, it is necessary to have access to several different orderings of the same input. In what follows we 474 describe our method for assembling a set of experimental materials and collecting human judgments. 3.1 Method 3.1.1 Design and Materials. Our goal here is to establish whether  X  correlates with human judgments on overall text understandability and coherence. A system that randomly pastes together sentences or facts from a database will ultimately produce a badly organized document lacking coherence. A good automatic evaluation method should assign low values to such documents and higher values to documents that are easy for humans to read and understand.
 ordering component. The ratings could be then correlated with  X  values representing the difference between system and reference orders. Such a comparison is, however, undesirable for a number of reasons. First, the system may be biased toward very good or very bad orders. This means that our hypothetical study would only exam-ine a restricted and potentially skewed range of  X  values. Furthermore, in concept-to-text generation applications, information ordering typically operates over symbolic representations that will be unfamiliar to naive informants and could potentially distort their judgments. A related issue arises in text-to-text generation applications where the produced documents are not necessarily grammatical, for example, when a summary is the output of an information fusion component (Barzilay 2003; Radev and McKeown grammaticality of the texts.
 tion familiar to our participants, namely, sentences. We simulated the output of an information-ordering component by randomly generating different sentence orders sentences). The texts were randomly sampled from a corpus collected by Barzilay and Lee (2004) (sampling took place over eight-sentence-length documents only). The corpus consists of Associated Press articles on the topic of natural disasters, drug-related criminal offenses, clashes between armies and rebel groups, and narratives from the U.S. National Transportation Safety Board database. 1 tively enumerated all possible orderings and calculated their  X  value against the refer-ence order found in the corpus. 2 Figure 1 shows how many different orders correspond to a given  X  value. For example, there is only one order with a  X  of 1 or there are 3,736 orders with  X  0 . 07 or  X  0 . 07.

Figure 1. Unfortunately, this would render our experimental design unwieldy. As-suming we randomly select one order for each value, our participants would have to judge 29  X  8 = 232 texts. In order to strike a balance between a manageable design and a wide range of  X  values, we split the  X  range into eight bins (see Figure 2). For each text, an order was randomly sampled from each bin. Thus, our set of materials consisted of 8  X  8 = 64 texts. Pronouns that could not be resolved intra-sententially were substituted by their referents to avoid creating coherence violation artifacts. For the same reason, we excluded from our materials texts containing discourse connectives (e.g., but , therefore ). 3.1.2 Procedure. During the elicitation study, participants were presented with texts and asked to judge how comprehensible they were on a seven-point scale. They were told that some texts would be perfectly understandable, whereas others would be fairly incoherent and the order of the sentences might seem scrambled.
 476 of instructions that explained the task and provided several examples of well-and badly organized texts, together with examples of numerical estimates. From our set of materials we generated 8 lists (each consisting of 8 texts) following a Latin square design. Each subject was randomly assigned to one list. The procedure ensured that no two texts in a given list corresponded to the same reference text. It was emphasized that there were no correct answers and that subjects should base their judgments on first impressions, not spending too much time on any one text. Example stimuli are shown in Table 2.
 structions and materials were administered via CGI scripts. A number of safeguards had to provide their e-mail address and were asked to fill in a short questionnaire including basic demographic information (name, age, sex, handedness, and language background). Subjects X  e-mail addresses were automatically checked for plausibility and subjects with fake addresses were removed. The elicited responses were also screened to identify (and eliminate) subjects taking part in the experiment more than once. 3.1.3 Subjects. The experiment was completed by 189 unpaid volunteers, all self-reported native speakers of English. Subjects were recruited by postings to local e-mail were allowed to participate. Four subjects were eliminated because they were non-native English speakers. The data of six subjects were excluded after inspection of their responses revealed anomalies in their ratings. For example, they either provided ratings outside the prespecified scale (1 X 7) or rated all documents uniformly. This left 179 subjects for analysis (approximately 22 per text). Forty-nine of our participants were female and 42 male. The age of the subjects ranged from 18 to 60 years. The mean was 28.5 years. 3.2 Results
The judgments were averaged to provide a single rating per text. We first analyzed the correspondence of human ratings and  X  values by performing an analysis of variance (ANOVA). Recall that  X  represents the degree of similarity between a synthetically gen-erated text and a reference text. In our case, the reference texts are the original human-authored documents from our corpus. Our participants judge how well a document is organized without having access to the original reference.
 to the eight bins discussed in Section 3.1 (see Figure 2). The ANOVA showed that this factor was significant in both by-subjects and by-items analyses: F statistics for each of the eight bins. Post hoc Tukey tests indicated that the ratings for texts with  X  values from Bin 1 were significantly different from the ratings assigned to all other bins (  X  = 0 . 01). Although ratings for Bins 2, 3, 4, and 5 did not significantly differ from each other, they all differed from Bins 6, 7, and 8. The results of the ANOVA show that our participants tended to give high scores to texts with high  X  values and low scores to texts with low  X  values.
 jects X  ratings and Kendall X  X   X  . The comparison yielded a Pearson correlation coefficient of r = 0 . 45 ( p &lt; 0 . 01, N = 64). Figure 3 plots the relationship between judgments and  X  values. To get a better understanding of how this automatic evaluation method compares with human judgments, we examined how well our raters agreed in their assessment. To calculate intersubject agreement we used leave-one-out resampling. The technique is a special case of n -fold cross-validation (Weiss and Kulikowski 1991) and has been previously used for measuring how well humans agree on judging seman-tic similarity (Resnik and Diab 2000; Resnik 1999), adjective plausibility (Lapata and Lascarides 2003), and text coherence (Barzilay and Lapata 2005).
 (i.e., the response data of all but one subject) and a set of size one (i.e., the response data of a single subject). We then correlated the mean ratings of the former set with the ratings of the latter. This was repeated m times. Since we had 179 subjects, we performed 478 178 correlation analyses and report their mean. 3 The average intersubject agreement was r = 0 . 56 (min = 0 . 001, max = 0 . 94, SD = 0 . 25), thus indicating that  X   X  X  agreement with the human data is not far from the average human agreement. 4. Experiment 2: Kendall X  X  Tau and Processing Effort problem with this off-line measure is that it indicates whether participants find a text easy or difficult to comprehend, without, however, isolating the causes for this difficulty.
For example, the ratings may reflect not only what subjects think about how a text is organized but also their (un)familiarity with its genre or style, their lack of attention, or disinterest in the subject matter. To ascertain that this is not the case, we conducted a follow-up experiment whose aim was to explore the relationship between Kendall X  X   X  and processing effort. Much work in psychology (McKoon and Ratcliff 1992; Britton 1991) indicates that low-coherence texts require more inferences and therefore take longer to read. If Kendall X  X   X  does indeed capture aspects of overall document organi-zation and coherence, then documents assigned a high  X  value should take less time to read than documents with low  X  values. Unlike ratings, reading times are an immediate measure of processing effort that participants cannot consciously control or modulate. 4.1 Method 4.1.1 Design and Materials. The experiment was designed to assess the relation of Kendall X  X   X  with processing effort. Our selection of materials was informed by the
ANOVA results presented in Experiment 1. We used the same eight reference texts from the previous experiment. For each text we randomly selected three synthetically generated orders, each from Bin 1 (high  X  value), Bins 2 X 4 (medium  X  value), and Bins 5 X  8(low  X  value). In other words, we collapsed Bins 2 X 4 and Bins 5 X 8, since the ANOVA revealed that ratings for these bins were not significantly different. Our set of materials consisted of 8  X  3 = 24 texts. 4.1.2 Procedure. The presentation of stimuli and collection of responses was controlled by E-Prime software 4 (version 1.1) running on a Dell Optiplex GX270 with an Intel
Pentium 4 processor and 512 MB memory. The experiment started with a practice ses-sion comprising two texts, each eight sentences long. Then eight texts were presented; the presentation followed a Latin square design, thus ensuring that no subject saw the same text twice.
 bar to proceed from one sentence to the next. Participants were instructed to read were certain that they understood it. Participants X  reading time was recorded for each sentence. After the final sentence was displayed, subjects were asked a comprehension yes/no question to make sure that they were actually reading the texts rather than pressing the space bar randomly. 4.1.3 Subjects. The experiment was completed by 32 volunteers, all self-reported native speakers of English. The experiment was administered in the laboratory and subjects were paid  X  5 for their participation. None of the subjects had previously participated in
Experiment 1. 4.2 Results
Sentence reading times were averaged to provide reading times for each text. As a first step, the reading time data were screened to remove errors and outliers. Errors consisted of items where the subjects had incorrectly answered the comprehension question. This affected 12.3% of the data. Reading times beyond 2.5 standard deviations above or below the mean for a particular participant were replaced with the mean plus this cut-off value. This adjustment of outliers affected 9.7% of the data. Mean reading times for each experimental condition (high, medium, low) are shown in Table 4.

F (2, 14) = 4 . 23, p &lt; 0 . 05]. Post hoc Tukey tests revealed that high- X  texts were read sig-nificantly faster than medium-and low- X  texts (  X  = 0 . 01). Reading times for medium- X  texts were not significantly different from low- X  texts. 480 between reading times and  X  values. We regressed  X  values and reading times following the procedure 5 recommended in Lorch and Myers (1990). The regression yielded a also significantly correlated with human ratings: Pearson X  X  r = for the use of Kendall X  X   X  as a measure of text well-formedness. It correlates not only with human ratings but also with reading times. The latter constitute much more fine-grained behavioral data, directly associated with processing effort: Less well-structured documents tend to have low  X  values and cause longer reading times, whereas documents with high  X  values tend to be better organized and cause shorter reading times. 5. Discussion
In this article, we argue that Kendall X  X   X  can be used as an automatic evaluation method for information-ordering tasks. We have undertaken a judgment elicitation study demonstrating that  X  correlates reliably with human judgments. We have also shown that  X  correlates with processing effort X  X exts with high  X  values take less time to read than texts with low  X  values. We have presented behavioral evidence collected via two distinct experimental paradigms suggesting that Kendall X  X   X  is an ecologically valid measure of document well-formedness and structure.
 pendent. It can therefore be used to evaluate both symbolic and statistical generation systems. We do not view  X  as an alternative to human evaluations; rather we consider its role complementary. It can be used during system development for tracking incremental progress or as an easy way of assessing whether an idea is promising. It can also be used to compare systems that employ comparable information-ordering strategies and operate over the same input. Furthermore, statistical generation systems (Lapata 2003;
Barzilay and Lee 2004; Karamanis and Manurung 2002; Mellish et al. 1998) could use  X  as a means of directly optimizing information ordering, much in the same way MT systems optimize model parameters using B LEU as a measure of translation quality (Och 2003).
 work (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Karamanis and Mellish 2005) has shown that there may be many acceptable orders for a set of information-bearing items, although topically related sentences seem to appear together (Barzilay,
Elhadad, and McKeown 2002). A straightforward way to incorporate multiple refer-ences in the evaluation paradigm discussed here is to compute the  X  statistic N times for every reference X  X ystem output pair and report the mean. A more interesting future direction is to weight transpositions (see Section 2) according to agreements or disagree-ments in the set of multiple references. A possible implementation of this idea would be to compute  X  against one (randomly selected) reference, but change the metric so as to give fractional counts (i.e., less than one) to transpositions that are not uniformly attested in the reference set.
 employed to assess information ordering. Barzilay and Lee (2004) and Barzilay and
Lapata (2005) measure accuracy as the percentage of test items for which the system gives preference to the gold-standard reference order. This measure allows us to com-pare the output of different systems; however, it only rewards orders identical to the gold standard, and considers all other orders deviating from it deficient. Barzilay and
Lee (2004) propose an additional evaluation measure based on ranks. Assuming that a system can exhaustively generate all possible orders for a set of items (with a certain probability), they report the rank given to the reference order when all possible orders are sorted by their probability. The best possible rank is 0 and the worst rank is N !
A system that gives a high rank to the reference order is considered worse than a system that gives it a low rank. However, not all systems are designed to exhaustively enumerate all possible permutations for a given document or have indeed a scoring mechanism that can rank alternative document renderings. Duboue and McKeown (2002) employ an alignment algorithm that allows them to compare the output of their algorithm with a gold-standard order. The alignment algorithm works by considering the similarity between system-generated and gold-standard facts. The similarity func-tion is domain dependent (Duboue and McKeown [2002] generate postcardiac surgery medical briefings) and would presumably have to be redefined for a different set of facts in another domain.
 spectively of the domain or application at hand. It requires no additional tuning and correlates reliably with behavioral data. Since it is a similarity measure, it can be used to evaluate system output that is not necessarily identical to the gold standard. Also note that  X  could be used to compare across systems operating over similar input/output even if reference texts are not available. For example,  X  could identify outlier systems with output radically different from the mean.
 Acknowledgments References 482
