 This paper proposes a new method for computing page importance, referred to as BrowseRank. The conventional approach to com-pute page importance is to exploit the link graph of the web and to build a model based on that graph. For instance, PageRank is such an algorithm, which employs a discrete-time Markov process as the model. Unfortunately, the link graph might be incomplete and inaccurate with respect to data for determining page impor-tance, because links can be easily added and deleted by web con-tent creators. In this paper, we propose computing page impor-tance by using a  X  X ser browsing graph X  created from user behav-ior data. In this graph, vertices represent pages and directed edges represent transitions between pages in the users X  web browsing his-tory. Furthermore, the lengths of staying time spent on the pages by users are also included. The user browsing graph is more re-liable than the link graph for inferring page importance. This pa-per further proposes using the continuous-time Markov process on the user browsing graph as a model and computing the stationary probability distribution of the process as page importance. An e ffi -cient algorithm for this computation has also been devised. In this way, we can leverage hundreds of millions of users X  implicit voting on page importance. Experimental results show that BrowseRank indeed outperforms the baseline methods such as PageRank and TrustRank in several tasks.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.5.4 [ Information Interfaces and Presentation ]: Hypertext / Hypermedia.
 Algorithms, Experimentation, Theory PageRank, User behavior data, User browsing graph, Continuous-time Markov process, Q-matrix.
Page importance, which represents the  X  X alue X  of an individual page on the web, is a key factor for web search, because for con-temporary search engines, the crawling, indexing, and ranking are usually guided by this measure. Because the scale of the web is extremely large and the web evolves dynamically, accurately cal-culating the importance scores of web pages becomes critical, and also poses a great challenge to search engines. In this paper, we propose a new method for computing page importance, as our an-swer to the challenge.

Currently, page importance is calculated by using the link graph of the web and such a process is called link analysis. Well known link analysis algorithms include HITS [15], PageRank[5, 18], and others [4, 8, 9, 11, 12, 16, 17, 20]. Most of the algorithms assume that if many important pages link to a page on the link graph, then the page is also likely to be important, and they calculate the im-portance of the page on the basis of a model defined on the link graph. Link analysis algorithms have been successfully applied to web search.

For example, PageRank employs a discrete-time Markov process on the web link graph to compute page importance, which in fact simulates a random walk along the hyperlinks on the web of a web surfer. Although PageRank has its advantages, it also has certain limitations as a model for representing page importance. 1. The link graph, which PageRank relies on, is not a very reli-2. PageRank only models a random walk on the link graph, but
To overcome these drawbacks, we consider using a more reliable data source and employing a more powerful mathematical model.
The first question is whether we can find a better data source than the link graph. Our answer is to utilize the user browsing graph , generated from user behavior data. 1 User behavior data can a web server. An example of the data is shown in Figure 1. Each record in the data contains the information of a visit by an anony-mous user: URL, time, and method of visiting (URL input or hy-perlink click from previous page). In our experiment, such data was recorded and collected from an extremely large group of users under legal agreements with them. Information which could be used to recognize their identities was not included. By integrating the data from hundreds of millions of web users, we can build a user browsing graph, in which vertices represent web pages and di-rected edges represent real transitions between web pages by users, and furthermore the lengths of time spent on the pages by the users are also included.

The user browsing graph can more precisely represent the web surfer X  X  random walk process, and thus is more useful for calcu-lating page importance. The more visits of the page made by the users and the longer time periods spent by the users on the page, the more likely the page is important. With this graph, we can leverage hundreds of millions of users X  implicit voting on page importance. In this regard, our approach is in accordance with the concept of
The second question is what kind of algorithm we should use to leverage the new data source. Obviously, the use of a discrete-time Markov process would not be su ffi cient. In this paper, we define a continuous-time Markov process [23] as the model on the user browsing graph. If we further assume the process to be time-homogenous (which is reasonable as discussed in Section 3), then the stationary probability distribution of the process can be used to define the importance of web pages. We employ the algorithm referred to as BrowseRank, to e ffi ciently compute the stationary probability distribution of the continuous-time Markov process. We make use of an additive noise model to represent the observations with regard to the Markov process and to conduct an unbiased and consistent estimation of the parameters in the process. In addition, we adopt an embedded Markov chain based technology to speed up the calculation of the stationary distribution. Hereafter, if there is no confusion, we will call both the algorithm and the scores output by the algorithm  X  X rowseRank X .

Experimental results show that BrowseRank can achieve bet-ter performance than existing methods, including PageRank and havior data to compute page importance. We will not discuss more work. vide client software called toolbars, which can serve the purpose. 3 http: // en.eikipedia.org / wiki / Web_2 TrustRank [8] in important page finding, spam page fighting, and relevance ranking.

The novelty of this paper lies in the following points. First, we propose using a user browsing graph, mined from user behav-ior data for computing page importance, which is more reliable and richer than a web link graph. Second, we propose using the continuous-time Markov process to model a random walk on the user browsing graph, which can more powerfully represent page importance. Third, we propose an algorithm called BrowseRank to e ffi ciently compute page importance scores.

The rest of the paper is organized as follows. Section 2 intro-duces related work. Section 3 describes the user browsing graph, the continuous-time Markov process model, and the BrowseRank algorithm. Experimental results are reported in Section 4. Conclu-sion and future work are given in Section 5.
PageRank [5, 18] and HITS [15] are popular link analysis algo-rithms in the literature. The basic idea of PageRank is as follows: the link from a webpage to another can be regarded as an endorse-ment of the linking page, the more links pointed to a page, the more likely it is important, and this importance information can be prop-agated across the vertices in the graph. A discrete-time Markov process model which simulates a web surfer X  X  random walk on the graph is defined and page importance is calculated as the stationary probability distribution of the Markov process. HITS is based on the notions of hub and authority to model the two aspects of impor-tance of a webpage. A hub page is one from which many pages are linked to, while an authority page is one to which many pages are linked from. In principle, good hubs tend to link to good authori-ties and vice versa . Previous study has shown that HITS performs comparably to PageRank [1].

Many algorithms have been developed in order to further im-prove the accuracies and e ffi ciencies of PageRank. Some work fo-cuses on speed-up of the computation [9, 17], while others focus on refinement and enrichment of the model. For example, Topic-sensitive PageRank [12] and query-dependent PageRank [20] have been proposed. The basic idea of these algorithms is to intro-duce topics and assume that the endorsement from a page that be-longs to the same topic is larger. Other variations of PageRank include those modifying the  X  X ersonalized vector X  [11], changing the  X  X amping factor X  [4], and introducing inter-domain and intra-domain link weights [16]. Besides, there is also work on theoretic issues of PageRank [3] and [10]. Langville et al [16] provide a good survey on PageRank and related work.

Link analysis algorithms that are robust against link spam have been proposed. For example, TrustRank [8] is a link analysis tech-nique which takes into consideration the reliability of web pages when calculating the importance of pages. In TrustRank, a set of reliable pages are first identified as seed pages. Then the trust of the seed pages is propagated to other pages on the web link graph. Since the propagation in TrustRank starts from the reliable pages, TrustRank can be more spam-resistant than PageRank.
Many web service applications assist users in their accesses to the web; sometimes they record user behaviors under agreements with them.

When a user surfs on the web, she usually has some information need. To browse a new page, the user may choose to click on the hyperlink on another page pointing to it, or to input the URL of it http: // aaa.bbb.com / 1.htm 2007-04-12, 21:34:11 CLICK http: // ccc.ddd.org / index.htm 2007-04-12, 21:34:52 CLICK into the web browser. The user may repeat this until she finds the information or gives up. The user behavior data can be recorded and represented in triples consisting of &lt; URL, TIME, TYPE &gt; (see Table 1 for examples). Here, URL denotes the URL of the web-page visited by the user, TIME denotes the time of the visit, and TYPE indicates whether the visit is by a URL input (INPUT) or by a hyperlink click on the previous page (CLICK). The records are sorted in chronological order.

From the data we extract transitions of users from page to page and the time spent by users on the pages as follows: 1) Session segmentation
We define a session as a logical unit of user X  X  browsing. In this paper we use the following two rules to segment sessions. First, if the time of the current record is 30 minutes behind that of the pre-vious record, then we will regard the current record as the start of a new session [24]; otherwise, if the type of the record is  X  X NPUT X , then we will regard the current record as the start of a new session. We refer to the two rules as the time rule and the type rule hereafter. 2) URL pair construction Within each session, we create URL pairs by putting together the URLs in adjacent records. A URL pair indicates that the user tran-sits from the first page to the second page by clicking a hyperlink. 3) Reset probability estimation
For each session segmented by the type rule , the first URL is di-rectly input by the user and not based on a hyperlink. Therefore, such a URL is  X  X afe X  and we call it green tra ffi c 4 . When process-ing user behavior data, we regard such URLs as the destinations of the random reset (when users do not want to surf along hyper-links). We normalize the frequencies of URLs being the first one in such sessions to get the reset probabilities of the corresponding web pages. 4) Staying time extraction
For each URL pair, we use the di ff erence between the time of the second page and that of the first page as the observed staying time on the first page. For the last page in a session, we use the follow-ing heuristics to decide its observed staying time. If the session is segmented by the time rule , we randomly sample a time from the distribution of observed staying time of pages in all the records and by the type rule , we use the di ff erence between the time of the last page in the session and that of the first page of the next session (INPUT page) as the staying time.

By aggregating the transition information and the staying time information extracted from the records by an extremely large num-ber of users, we are able to build a user browsing graph (see Figure 1). Each vertex in the graph represents a URL in the user behavior data, associated with reset probability and staying time as meta-data. Each directed edge represents the transition between two pages or selecting from bookmarks at web browsers. We call such kind of visits green tra ffi c , because the pages visited in this way are safe, interesting, and / or important for the users. please refer to Section 4.1.1. vertices, associated with the number of transitions as its weight. In other words, the user browsing graph is a weighted graph with vertices containing metadata and edges containing weights. We denoted it as G = &lt; V , W , T , X  &gt; , where V = { v edges, lengths of staying time, and reset probabilities, respectively. N denotes the number of web pages in the user browsing graph.
To better leverage the information on staying time, we propose employing a continuous-time time-homogeneous Markov process for representing a random walk on the user browsing graph.
When using the new model, we need to make the following as-sumptions. 1) Independence of users and sessions
The browsing processes of di ff erent users in di ff erent sessions are independent. In other words, we treat web browsing as a stochas-tic process, with the data observed in each session by a user as an i.i.d. sample of this process. This independence assumption is widely used when one estimates parameters from observed data in statistics. 2) Markov property
The page that a user will visit next only depends on the current page, and is independent of the pages she visited previously. This assumption is also a basic assumption in PageRank. 3) Time-homogeneity
The browsing behaviors of users (e.g. transitions and staying time) do not depend on time points. Although this assumption is not necessarily true in practice, it is mainly for technical convenience. Note that this is also a basic assumption in PageRank.

Based on these assumptions, we can build a model of continuous-time time-homogeneous Markov process to mimic a random walk on the user browsing graph. In a similar way as in PageRank, the stationary probability distribution of this process can be used to measure the importance of pages. Suppose there is a web surfer walking through all the webpages. We use X s to denote the page which the surfer is visiting at time s , s &gt; 0. Then, with the aforementioned three assumptions, the process X = { X s , s  X  0 } forms a continuous-time time-homogenous Markov process. Let p i j ( t ) denotes the transition probability from page i to page j for time interval (also referred to as time increment in statistics) t in this process. One can prove that there is a station-ary probability distribution  X  , which is unique and independent of t
The i th entry of the distribution  X  stands for the ratio of the time the surfer spends on the i th page over the time she spends on all the pages when time interval t goes to infinity. In this regard, this distribution  X  can be a measure of page importance.

In order to compute this stationary probability distribution, we need to estimate the probability in every entry of the matrix P ( t ). However, in practice, this matrix is usually di ffi cult to obtain, be-cause it is hard to get the information for all possible time intervals. To tackle this problem, we propose a novel algorithm which is in-stead based on the transition rate matrix [23]. The details of this algorithm will be given in Section 3.3.
We make use of the transition rate matrix to compute the station-ary probability distribution of P ( t ), as a measure of page impor-tance. We call the corresponding algorithm as BrowseRank.
The transition rate matrix is defined as the derivative of P ( t ) matrix Q = ( q i j ) N  X  N the Q-matrix for short. It has been proven that when the state space is finite there is a one-to-one correspondence between the Q-matrix and P ( t ), and  X  X  X  &lt; q ii &lt; 0; Due to this correspondence, one also uses Q-Process to represent the original continuous-time Markov process, that is, the browsing process X = { X s , s  X  0 } defined before is a Q-Process because of the finite state space.

There are two advantages of using the Q-matrix: 1. The parameters in the Q-matrix can be e ff ectively estimated 2. Based on the Q-matrix, there is an e ffi cient way of computing
Before giving the theorem of how to e ffi ciently compute the stationary probability distribution of Q-process (Theorem 1), we need to introduce a concept named embedded Markov chian (EMC) [22]corresponding to a Q-process. The so-called EMC is a discrete-time Markov process featured by a transition probability matrix with zero values in all its diagonal positions and  X  q i j diagonal positions, where all parameters q i j , i , j = 1 ,..., N have the same definitions as before.
 T  X  X  X  X  X  X  X  X  1. Suppose X is a Q-process, and Y is the Embedded Markov Chain derived from its Q-matrix. Let  X  = (  X  1 ,..., X  the process X and Y , then we have Refer to [22] for the proof of Theorem 1 .

Note that the process Y is a discrete-time Markov chain, so its stationary probability distribution  X   X  can be calculated by many sim-ple and e ffi cient methods such as the power method [6].
Next we will explain how to estimate the parameters in the Q-matrix, or equivalently parameter q ii and the transition probabilities  X 
According to [22], for a Q-Process, the staying time T i on the i th vertex is governed by an exponential distribution parameterized by q : This implies that we can estimate q ii from large numbers of obser-vations on the staying time in the user behavior data.

This task is, however, non-trivial because the observations in the user behavior data usually contain noise due to Internet connection speed, page size, page structure, and other factors. In other words, the observed values do not completely satisfy the exponential dis-not an exponential distribution, as we expected (see section 4.1.1). to represent the observations and to conduct an unbiased and con-sistent estimation of parameter q ii .

Suppose for page i , we have m i observations on its staying time in the user behavior data, denoted as Z 1 , Z 2 ,..., Z m i , and they have the same distribution as random variable Z . Without loss of generality, we suppose that Z is the combination of real staying time T noise U , i.e., Suppose that noise U is governed by a Chi-square distribution as Chi ( k ) 7 , then its mean and variance will be k and 2 k respectively. Further suppose that the mean and variance of Z are  X  and  X  2 . By assuming U and T i to be independent, we have [19]: Note that the sample mean  X  Z = 1 m  X  and  X  2 [19]. We then estimate q ii by solving the following opti-
Transition probabilities in the EMC describe the  X  X ure X  transi-tions of the surfer on the user browsing graph. Estimation of them can be based on the observed transitions between pages in the user behavior data. It can also be related to the green tra ffi c in the data. We use the following method to integrate these two kinds of infor-mation for the estimation.

We start with the user browsing graph G = &lt; V , W , T , X  &gt; . We then add a pseudo-vertex (the ( N + 1) th vertex ) to G , and add two types of edges: the edges from the last page in each session to the pseudo-vertex, associated with the click number of the last page as its weight; and the edges from the pseudo-vertex to the first page in each session, associated with the reset probability. We denote the new graph as  X  G = &lt;  X  V ,  X  W , T ,  X   X  &gt; , where |  X   X  ,...,  X   X  N , 0 &gt; . Then we explain the EMC model as the random walk on this new graph  X  G . Based on the law of large number [19], the transition probabilities in the EMC are estimated as below, The intuitive explanation of the above transition is as follows. When the surfer walks on the user browsing graph, she may go ahead along the edges with the probability  X  , or choose to restart from a new page with the probability (1  X   X  ). The selection of the new page is determined by the reset probability.

One advantage of using (8) for estimation is that the estima-tion will not be biased by the limited number of observed transi-tions. The other advantage is that the corresponding EMC is primi-tive, and thus has a unique stationary distribution (see Theorem 2). whose support is within [0 , +  X  ). from equation (5) and (6) respectively, and minimize the di ff erence between these two solutions. In this way, we can leverage both mean and variance for parameter estimation. Therefore, we can use the power method to calculate this stationary distribution in an e ffi cient manner.

T  X  X  X  X  X  X  X  X  2. Suppose X is a Q-process and Y is its EMC. If the entries in the transition probability matrix  X  P = (  X  p fined as in equation (8), the process Y is primitive, i.e., the transi-tion graph of process Y is strongly connected (which means there is a directed path from any node to any other node in the graph). Refer to the appendix for the proof of Theorem 2.

In summary, we get the flow chart of BrowseRank as Table 2.
We have conducted experiments to verify the e ff ectiveness of the proposed BrowseRank algorithm. We report the experimental re-sults in this section. The first experiment was conducted at the website level, to test the performance of BrowseRank on finding important websites and depressing spam sites. The second experi-ment was conducted at the web page level, to test the e ff ectiveness of BrowseRank on improving relevance ranking. We used a user behavior dataset, collected from the World Wide Web by a commercial search engine in the experiments. All possi-ble privacy information was rigorously filtered out and the data was sampled and cleaned to remove bias as much as possible. There are in total over 3-billion records, and among them there are 950-million unique URLs. The distribution of the recorded staying time on all web pages is shown in a log-log scale in Figure 2. From the figure, we can see that the curve is not straight at the beginning, indicating that it does not follow an exact exponential distribution. This validates our arguments on the noisy observations on the stay-ing time.

When running BrowseRank at website-level, we did not distin-guish web pages in the same website. That is, we ignored the tran-sitions between the pages in the same website and aggregated the transitions from (or to) the pages in the same website. As a re-sult, we created a user browsing graph at website-level, consisting of 5.6-million vertices and 53-million edges. We also obtained a link graph containing the 5.6-million websites from the commer-cial search engine. There are in total 40-million websites in this link graph. We computed PageRank and TrustRank from it as base-lines.
We listed the top-20 websites ranked by using di ff erent algo-rithms in Table 3. From this table, we can make the following observations:
First, BrowseRank tends to give high ranks to Web 2.0 web-sites (marked in bold) such as myspace.com , youtube.com , face-book.com . The reason is that web users visit the websites with high frequencies and often spend much time on them, even if the websites do not have as many inlinks as Web 1.0 websites like adobe.com and apple.com do. Note this reflects users X  real infor-mation needs.
 Second, some websites like adobe.com are ranked very high by PageRank. One reason is that adobe.com has millions of inlinks for Acrobat Reader and Flash Player downloads. However, web users do not really visit such websites very frequently and they should not be regarded more important than the websites on which users spend much more time (like myspace.com and facebook.com ).
Third, the ranking results produced by TrustRank are similar to PageRank. The di ff erence is that the well-known websites are ranked higher by TrustRank than by PageRank, mainly because these websites are likely to be included or pointed to by websites in the seed set.

In summary, the ranking results given by BrowseRank seem to better represent users X  preferences than PageRank and TrustRank.
We randomly sampled 10,000 websites from the 5.6 million web-sites and asked human experts to make spam judgments on the web-sites. 2,714 websites are labeled as spam and the rest are labeled as non-spam.

We used the spam bucket distribution to evaluate the perfor-mances of the algorithms. Given an algorithm, we sorted the 5.6-million websites in descending order of the scores that the algo-rithm produces. Then we put these sorted websites into 15 buck-ets. This experiment is similar to the experiments in [8]. The numbers of the labeled spam websites over buckets for PageRank, TrustRank, and BrowseRank are listed in Table 4.

We see that BrowseRank can successfully push many spam web-sites to the tail buckets, and the number of spam websites in the top buckets in BrowseRank is smaller than PageRank and TrustRank. That is to say, BrowseRank is more e ff ective in spam fighting than PageRank and TrustRank. The reasons that BrowseRank outper-forms the other algorithms are as follows: 1) Creating inlinks, which can hurt PageRank, cannot hurt BrowseR-ank so much, because the link information is not used in BrowseR-ank. 2) The performance of TrustRank can be a ff ected by the selection of the seed set and the determination of the seed distribution in the link graph. For BrowseRank, seed selection and seed distribution determination are not necessary.

Furthermore, the performance of TrustRank is better than PageR-ank, which is consistent with the result obtained in previous work [8].
No. PageRank TrustRank BrowseRank 1 adobe.com adobe.com myspace.com 2 passport.com yahoo.com msn.com 3 msn.com google.com yahoo.com 4 microsoft.com msn.com youtube.com 5 yahoo.com microsoft.com live.com 6 google.com passport.net facebook.com 7 mapquest.com ufindus.com google.com 8 miibeian.gov.cn sourceforge.net ebay.com 9 w3.org myspace.com hi5.com 10 godaddy.com wikipedia.org bebo.com 11 statcounter.com phpbb.com orkut.com 12 apple.com yahoo.co.jp aol.com 13 live.com ebay.com friendster.com 14 xbox.com nifty.com craigslist.org 15 passport.com mapquest.com google.co.th 16 sourceforge.net cafepress.com microsoft.com 17 amazon.com apple.com comcast.net 18 paypal.com infoseek.co.jp wikipedia.org 19 aol.com miibeian.gov.cn pogo.com 20 blogger.com youtube.com photobucket.com
In web search engines, the retrieved web pages for a given query are often ranked based on two factors: relevance rank and impor-tance rank. A linear combination of these two ranking lists is then created [2]: Here 0  X   X   X  1 is the combining parameter.
Again, we used the user behavior data and the link graph. This time we ran all the algorithms at the page level.

In addition, we also obtained a large dataset from the same search engine, containing 8000 queries and their associated webpages. For the associated webpages of each query, three researchers in web search were hired to independently score each page X  X  relevancy to the given query (1 -relevant, 0 -irrelevant). These scores were then summed, with pages having total scores of at least 2 labeled as relevant, and the others marked as irrelevant. This dataset has been preprocessed, and spam pages within it have been removed in advance. Therefore, it is not necessary to evaluate TrustRank on this dataset, and we only take PageRank as baseline. We reserve the webpages which are in this dataset and also appear in the user behavior data for the experiment.
In this experiment, we compared the performances on ranking using PageRank and BrowseRank as the page importance models for ranking. BM25 [21]was used as the relevance model for rank-ing.
 We adopted three measures to evaluate the ranking performances: MAP [2], Precision (P@n) [2], and Normalized Discount Cumula-tive Gain (NDCG@n) [13, 14]. The experimental results are pre-sented in Figures 3 to 9.

From the figures, we can see that BrowseRank consistently out-performs PageRank in all parameter settings in terms of all eval-uation measures. For example, from Figure 7, we can see that Bucket Number of PageRank TrustRank BrowseRank NDCG@5 of BM25 is 0.853 (when  X  = 1), BrowseRank hits its peak NDCG@5 value of 0.876 when  X  = 0 . 70, and the peak NDCG@5 value of PageRank is 0.862 when  X  = 0 . 80.

We also conducted t-tests at a confidence level of 95%. In terms of MAP, the improvement of BrowseRank over PageRank is sta-tistically significant with a p-value of 0.0063. In terms of P@3, P@5, NDCG@3, and NDCG@5, the improvements are also statis-tically significant with p-values are 0.00026, 0.0074, 3 . 98  X  10  X  7 , and 3 . 57  X  10  X  6 , respectively.
To further understand the user behavior data and our proposed algorithm, we consider two simple algorithms that also use user behavior data or user browsing graphs: PageRank-UBG (weighted PageRank computed on the user browsing graph), and Naive BrowseR-ank (product of the number of clicks and average observed staying time).

Based on the experimental results given in Figures 8, 9, and 10, we can make the following observations: 1. Both simple methods can also outperform PageRank, as can 2. NaiveBR performs better than PageRank-UBG. This is rea-3. BrowseRank consistently performs better than the simple al-
In this paper, we have pointed out that a web link graph is not a reliable data source for computing page importance. Furthermore, existing link analysis algorithms like PageRank are also too simple to infer page importance. To deal with these problems, we propose using user behavior data to mine a user browsing graph, building a continuous-time Markov process model on the graph, and employ-ing an e ffi cient algorithm to calculate page importance scores with the model. Figure 3: Search performance in terms of MAP for BrowseR-ank and PageRank Figure 4: Search performance in terms of P@3 for BrowseR-ank and PageRank Figure 5: Search performance in terms of P@5 for BrowseR-ank and PageRank Figure 6: Search performance in terms of NDCG@3 for BrowseRank and PageRank Figure 7: Search performance in terms of NDCG@5 for BrowseRank and PageRank Figure 8: Search performance in terms of MAP for BrowseR-ank and two simple algorithms Figure 9: Search performance in terms of P@5 for BrowseR-ank and two simple algorithms Figure 10: Search performance in terms of NDCG@5 for BrowseRank and two simple algorithms
The user browsing graph data is more reliable and richer than the conventional link graph data, and furthermore the continuous-time Markov model is more powerful than the existing models. Thus the use of them will result in more accurate results in page importance calculation. We name the new algorithm BrowseRank. Our exper-imental results show that BrowseRank outperforms PageRank and TrustRank in two web search tasks, indicating that the proposed approach really does have the stated advantages.

There are still several technical issues which need to be addressed as future work: 1) User behavior data tends to be sparse. The use of user behav-ior data can lead to reliable importance calculation for the head web pages, but not for the tail web pages, which have low frequency or even zero frequency in the user behavior data. One possibility is to use the link graph to conduct some smoothing. We need to find a principled way to deal with this problem. 2) The assumption on time homogeneity is made mainly for tech-nical convenience. We plan to investigate whether we can still ob-tain an e ffi cient algorithm if this assumption is withdrawn. 3) The content information and metadata was not used in BrowseR-ank. However, in general, a larger page often means longer staying time. We will take the metadata like page size into consideration to normalize the user staying time in the next version.
We thank Daxin Jiang, Zhi Chen, Tao Qin, Zhen Liao, and Con-gkai Sun for their help in the data preparation and code review for the work, and thank Andrew Arnold for giving many helpful sug-gestions and his intensive polishing of the paper. [1] B. Amento, L. Terveen, and W. Hill. Does authority mean [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information [3] M. Bianchini, M. Gori, and F. Scarselli. Inside pagerank. [4] P. Boldi, M. Santini, and S. Vigna. Pagerank as a function of [5] S. Brin and L. Page. The anatomy of a large-scale [6] G. H. Golub and C. F. V. Loan. Matrix computations (3rd [7] Z. Gyongyi and H. Garcia-Molina. Web spam taxonomy, [8] Z. Gyongyi, H. Garcia-Molina, and J. Pedersen. Combating [9] T. Haveliwala. E ffi cient computation of pageRank. Technical [10] T. Haveliwala and S. Kamvar. The second eigenvalue of the [11] T. Haveliwala, S. Kamvar, and G. Jeh. An analytical [12] T. H. Haveliwala. Topic-sensitive pagerank. In WWW  X  02 , [13] K. Jarvelin and J. Kekalainen. IR evaluation methods for [14] K. Jarvelin and J. Kekalainen. Cumulated gain-based [15] J. M. Kleinberg. Authoritative sources in a hyperlinked [16] A. N. Langville and C. D. Meyer. Deeper inside pagerank. [17] F. McSherry. A uniform approach to accelerated pagerank [18] L. Page, S. Brin, R. Motwani, and T. Winograd. The [19] J. A. Rice. Mathematical Statistics and Data Analysis (2nd [20] M. Richardson and P. Domingos. The Intelligent Surfer: [21] S. E. Robertson. Overview of okapi projects. Journal of [22] W. J. Stewart. Introduction to the Numerical Solution of [23] Z. K. Wang and X. Q. Yang. Birth and Death Processes and [24] R. W. White, M. Bilenko, and S. Cucerzan. Studying the use Proof of Theorem 2
We prove the theorem by showing that there is a directed path in the transition graph between any two pages. As for the user browsing graph, we have the following three observations: 1) For the reset probability, we have  X  i  X  0,for any i = 1 , 2 ,..., N . Suppose that among them there are T entries that are strictly pos-itive. Without loss of generality, we regard the first T entries as such, i.e.,  X  i &gt; 0,for any i = 1 , 2 ,..., T . In other words, all ses-sions begin with the first T pages. 2) Since all the pages in the state space E come from the user behavior data, any page will belong to a session. 3) A session corresponds to a path in the transition graph: from the first page in a session, there is a path to all the other pages in the same session.

For any i , j  X  E , based on the second observation, we assume that page i is in the session whose first page is b i , and page j is in the session whose first page is b j .

If page i is the last page in the session, then according to the reset rule and the first observation, there will be a transition path from it to page b j with the probability  X  b j &gt; 0. Then based on the third observation, a path can be found from page b j to page j . As a result, there is a directed path from page i to page j .
If page i is not the last page in its session, based on the third ob-servation, there will be a path from i to the last page in that session. Since there is a path from the last page in that session to page j , as proved above, we actually can come to the conclusion that there is a direct path from page i to page j .
