 1. Introduction
The explosive increase in information has made it difficult for the average user to easily satisfy their information needs. Information retrieval (IR) systems intend to relieve the users of these difficulties by retrieving relevant documents, while filtering out non-relevant documents ( Van Rijsbergen, 1979 ).
Although existing keyword-based retrieval systems filter every document that has the words specified in the initial query, too many or too few results are sometimes generated. Moreover, most people are not good at making effective queries straight away, leading to ill-defined initial queries, which cause poor retrieval performance. In general, this problem is somewhat lessened by the use of a ranking strategy, as people are usually interested in the top ranked documents, or by clustering the retrieved set of documents ( Leuski, 2001 ).

It is getting more difficult, however, to satisfy users with only keyword-based retrieval, and concept-based retrieval approaches are being developed. Concept-based approaches treat query words as concepts, rather than as literal strings of letters, and take into account the domain knowledge in determining an appropriate interpretation of the initial query ( Deerwester, Furnas, Landauer, &amp; Harshman, 1990; Qiu they start from the considerable interest in bridging the gap between the terminology used in defining initial queries and the terminology used in representing documents. Therefore, the concept-based retrieval ap-proaches are able to retrieve relevant documents even if they do not contain the specific words used in the initial query.

In concept-based retrieval, many researchers have tried to find some appropriate solutions for represent-ing users  X  interests correctly ( Bookman &amp; Woods, 2000; Chang &amp; Hsu, 1998; Qiu &amp; Frei, 1993; Nakata,
Voss, Juhnke, &amp; Kreifelts, 1998 ). One of these solutions is query reformulation, which consists of two basic steps: expanding the initial query with new terms and/or reweighting the terms in the expanded query. Con-cept-based retrieval approaches take into account the fact that each term in the initial query represents one concept and try to find appropriate concepts based on these terms ( Gauch &amp; Smith, 1993 ).
Baeza-Yates and Ribeiro-Neto (1999) stated that there are three types of query reformulation, depend-ing on the following used information: information from user  X  s relevance judgement, information derived from the set of documents initially retrieved, or global information derived from the document collection.
Relevance feedback is a representative method of the first type of used information, i.e. when relevance information about the retrieved set of documents is provided by the user. Klink (2001) stated that the doc-uments selected as being relevant for a given query reflect the users  X  needs properly. Nakata et al. (1998) supported collaborative query concepts extraction as a relevance feedback technique for constructing query concepts . The second query reformulation type is usually called local feedback, which analyzes documents retrieved by the initial query ( Xu &amp; Croft, 1996 ). Xu and Croft (1996) proclaimed that given a query, the terms in the top ranked documents were a decisive factor in the retrieval process, and these terms could be clustered and treated as quasi-synonyms. In fact, there have been many approaches to prove that the set of retrieved documents are useful in building concepts ( Hearst &amp; Pedersen, 1996; Leuski, 2001 ). Finally, the third type of query reformulation is called global techniques, which analyze the corpus to discover word tion from the whole corpus. The global approach constructs a global similarity thesaurus based on term-to-term relationships rather than on the matrix of co-occurrence that is usually used in the local feedback approach ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ). Furthermore, for an optimal query expansion processing, many researchers have explored the use of a thesaurus ( Mandala, Tokunaga, &amp; Tanaka, 2000; Greenberg, 2001 ). Mandala et al. (2000) proposed a method for improving the performance of query expansion by combining heterogeneous thesauri (hand-crafted thesauri and corpus-based thesauri). Greenberg (2001) de-vised query expansion with semantically encoded thesauri terminology, and examined whether that method was more effective in the automatic or interactive processing environment.
In this paper, we investigate two approaches: global techniques and local feedback approach. In our pre-vious researches ( Chang, Choi, Choi, Kim, &amp; Raghavan, 2002; Chang, Kim, &amp; Ounis, 2004 ), we defined a set of appropriate terms that denote the user  X  s information needs adequately as query concepts . These query concepts were constructed by pre-processing the concepts, which were directly extracted from a whole set of documents (global techniques). Since it was impossible to make an optimal thesaurus for every field of study, we tried to construct the domain knowledge, called the primitive (basic) concepts, not from a the-saurus (e.g. WordNet) but from a document collection considered to be not only the subject of retrieval, but also a domain, which included latent semantics (meaning). Therefore, our research focused on two is-sues; how to construct primitive concepts from a document space and how to generate the query concepts , which denote the user  X  s information need appropriately. We proposed an automatic query reformulation framework, called Query Concept Method (QCM), to construct appropriate query concepts . In this paper, we examine whether our framework is capable of using the retrieved set of documents (local feedback ap-proach) to construct the primitive concepts. Actually, we found that the construction of primitive concepts using a whole corpus had some shortcomings: it was time-consuming to deal with the whole corpus and the approach was promising only for the poorly performing queries ( Chang et al., 2004 ). This paper also dis-cusses the impact of data mining technique on the retrieval performance, through various experiments. In particular, the paper discusses how to optimally apply query reformulation, with an automatically gener-ated query concepts , using a given document space.

In Sections 2 and 3, we briefly describe how to construct primitive concepts and generate query concepts by combining these primitive concepts. The experiments and results are presented in Section 4. We compare our Query Concept Method (QCM) to a vector space model, used as a baseline, and to a pseudo (blind) relevance feedback approach ( Rocchio, 1971; Ruthven &amp; Lalmas, 2003 ). The TREC8 collection is used to compare the results of various approaches. We show that using the retrieved set of documents, QCM does improve precision, while having a reasonable computational overhead. Finally, we conclude with the main findings and point out further research directions. 2. Construction of primitive concepts from a document space
We suppose that there are primitive concepts , also called basic concepts, which are able to represent the salient and important meaning of a document or a document collection. We define these concepts as a set of terms that are strongly associated with each other in the text, but not necessarily to be a set of synonyms as used in a thesaurus. The primitive concepts represent the main topics of documents, depending on contex-tual information. Since it is possible that there may be many duplicated (repeated) topics in similar docu-ments, it is necessary to organize these topics systematically in order to make use of them in IR. One of the ideas postulated is to make the primitive concepts orthogonal to each other in a document, and simulta-neously, make them distinct in a whole set of documents. Then we try to reformulate the initial query by combining these orthogonal factors. Since a document and a user  X  s initial query generally contain multi-ple meanings (topics or concepts), we could break them into several singular meanings for representing their contents. This idea, which divides an object into several atomic units, is similar to the classical vector decomposition, which assumes that the dimensions of a vector are independent. In the vector representa-tion, we could decompose a vector to several unit-vectors. Mathematically, each unit vector has a different direction, i.e. they are orthogonal to each other. In the same manner, if the primitive concepts are ortho-gonal, we can represent the main topics of a document and/or a query by combining these primitive con-cepts, adequately. Therefore, in this paper, we use the vector space model to represent documents, queries, and concepts.

The problem is how to construct primitive concepts which have orthogonal and distinctive properties. If we assume that the document collection is a large observational dataset, which has latent meanings, then we can find those concepts through global modeling and local pattern discovery. Mannila (2002) claimed that data mining is the analysis of observational datasets to find unsuspected relations and to summarize the data in novel ways that are both understandable and helpful to the data analyst. Following this idea, in order to extract the primitive concepts from a document space, we propose to identify the distinctive fea-tures of documents by using a two-layer analysis. First, we summarize each document and extract its sig-nificant features (i.e. local analysis). Then, we cluster the resulting features of the whole document set into primitive concepts (i.e. global analysis).

In Section 2.1, we consider different sets of documents, the whole collection of documents and the set of retrieved documents, as a document space for constructing the primitive concepts. Sections 2.2 and 2.3 de-scribe the local analysis and the global analysis, respectively. In addition, to discriminate ambiguous items, we refine the generated primitive concepts by a classification method as explained in Section 2.4. 2.1. Whole set of documents vs. retrieved documents
When we construct the primitive concepts from a document space, we could consider two different sets of documents. We could construct the primitive concepts not only from the whole corpus (QCM-W), but also from the set of retrieved documents (QCM-R), given a particular query. While both are similar in that they deal with a set of documents to build concepts, they might form different primitive concepts. Indeed, since the primitive concepts represent the main content of documents, if we apply our framework to the whole corpus, we generate all the possible primitive concepts underlying the whole set of documents. In practice, this QCM-W approach was conducted in our previous work ( Chang et al., 2004 ); we constructed the pri-mitive concepts from the whole corpus (TREC8 collection). As a result, even if the obtained results were not particularly impressive, we observed that there was an improvement in 34 poorly performing queries among 50 queries. Hereby, we found that there is a possibility that our approach could be useful in query reformulation. However, we also found that as the size of the document collection increases, the generation of the primitive concepts becomes more time-consuming.

Therefore, we choose an alternative document space. We use the set of retrieved documents instead of the whole corpus, which is expected to be less computationally expensive in building the primitive concepts. The use of the set of retrieved documents makes it possible to derive the primitive concepts from the initial query. This approach is encouraged by earlier works ( Salton &amp; Buckley, 1990; Hearst &amp; Pedersen, 1996; Leuski, 2001 ), in that the reliability of the retrieved documents as a document space is quite good. In these earlier works, authors prefer to use the information extracted from the retrieved set of documents for query expansion. However, this approach has also a disadvantage. Once the
QCM-W approach builds the primitive concepts, we can use the generated primitive concepts at anytime we wish. In contrast, the QCM-R approach is troublesome in that we must construct the primitive concepts for every new query. However, it does not take that much time to build concepts from the retrieved set of documents. 2.2. Local analysis: summarization and partitioning
We propose to extract salient contents by selecting the significant sentences within a document. For this purpose, we adopt robust summarization techniques ( Lam-Adesina &amp; Jones, 2001; Tombros &amp; Sanderson, 1998; Edmundson, 1969 ) to extract features from each document. The earlier research of Lam-Adesina and
Jones (2001) stated that summary generation methods seek to identify document contents that convey the most  X  X  X mportant X  X  information within a document. Therefore, we select the significant sentences of each document by scoring; Luhn  X  s keyword cluster method (1958) , the title term frequency method ( Tombros &amp; Sanderson, 1998 ), the location method suggested by Edmundson (1969) , and the ratio of significant terms in a sentence.
First, we initially retrieve a set of documents for a given query, and then take the top ranked n docu-ments from the retrieved documents. For each document, we score the sentences with the above methods.
The final score for each sentence is calculated by summing the individual score obtained for each used method.

While previous researchers ( Tombros &amp; Sanderson, 1998; Lam-Adesina &amp; Jones, 2001 ) used the term occurrence ( TF ) information in determining significant terms, we use the TF this measure is more accurate in representing the significance of a term. We use the TF as follows ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ): where Freq i , j is the number of occurrences of term t i quency Freq in the document D j , N is the total number of documents, and n which the index term t i appears. If the weight value of a term is higher than a threshold h , we consider the word as significant and add it to the significant terms list. In our experiment, the value of threshold h is empirically set to 0.3. Lam-Adesina and Jones (2001) also limited the optimal summary length from 15% of the original document length up to various maximum summary lengths (4, 6, or 9 sentences) empir-ically. Thus, we empirically set the optimal summary length, i.e. the measure of significant sentences ( mss ), to 7, for medium sized document (i.e. 25 &lt; Number of Sentence ( NS ) &lt; 40) in our approach. For docu-ments out of this range ( NS &lt;25or NS &gt; 40), we compute the mss as follows: where NS is the number of sentences in a document and L is the limit (25 for NS &lt; 25 and 40 for NS &gt; 40).
We intended to make this mss not only proportional to the document length but also not too affected by the document length. Finally, we choose the highly ranked mss sentences as significant sentences. Then, we gen-erate the summaries with these selected sentences for each given document ( Chang et al., 2004 ). If the NS is smaller than mss , we take all sentences in the document as significant sentences ( mss = NS ).
However, summarization is not enough to generate the orthogonal features of a document. Indeed, we need to merge the selected sentences that have overlapping terms among them. Therefore, we partition the selected significant sentences into several orthogonal feature vectors that do not share common terms.
Example 1 ( partitioning ). We represent a sentence, s i , as a vector of terms with their associated TF weight values in the document (e.g. see Fig. 1 ). Thus, for each document, we can consider a set of vectors
S ={ s 1 , s 2 , s 3 , s 4 }. To build the feature vectors of the illustrated document d
First, s 1 is assigned to feature f 1 . Since s 2 has a common term  X  computer  X  with s the other hand, s 3 is assigned to the new feature f 2 in that it has no overlapping terms with the first two sentences. The sentence s 4 is merged into f 1 because of the shared term  X  information  X  . This process thus results in two feature vectors that are orthogonal to each other.

After the partitioning process, each feature vector f i will be seen as a maximally connected component as shown in Fig. 1 . We can see that the feature vectors f 1 i.e. they are orthogonal in a vector space. More formally, this idea of partitioning is devised by constructing maximally connected components in a graph representation ( Cormen, Leiserson, Rivest, &amp; Stein, 2001 ).
Each vector s i is a sub-graph, such that the vertices of the sub-graph are the terms of the vector s the edges connect the terms, which are in the same sentence s
In this process, we have two approaches to generate the feature vectors. We construct the feature vectors deed, we observed that some insignificant words, not important but which frequently occurred in sentences, led to a situation where all sentences of a given document were merged to one in the TREC8 collection.
Therefore, to remove unnecessary unification, we take only the significant terms included in the significant terms list generated in the summarization process, and then partition these sentences. However, Approach 2 may involve a loss of information, depending on the value of the threshold, as the value of the threshold is based on an empirical setting. Indeed, we cannot assert that all terms whose weights are less than the threshold h are meaningless to build the features of documents. 2.3. Global analysis: clustering the feature vectors into primitive concepts
Although the summarization approach allows us to find local patterns in a given document, we need a descriptive modeling to find orthogonal features in general. Indeed, while the extracted feature vectors of a given document have orthogonal attributes within the document, they cannot be guaranteed to be ortho-gonal with the feature vectors in other documents.

Example 2. Consider the Relation A and the Relation B in Fig. 2 . The Relation A , defined in the same document d 1 , is definitely orthogonal following our approach in Section 2.2. However, the Relation B could reference the same feature. In other words, features f 1 and f summarization methods discover only local patterns in a document.

To alleviate the above problem, we adopt a clustering method (e.g. k -means) to generate primitive con-cepts that are approximately orthogonal to each other in a set of documents. Since our feature vectors are extremely sparse, we propose another clustering algorithm, which is suitable for our objective. This algo-rithm is similar to the single pass and reallocation method used in early work on cluster analysis ( Frakes &amp;
Baeza-Yates, 1992 ). In this algorithm, we divide the clustering procedure into two phases: (1) Cluster the feature vectors ( f 1 , ... , f n ) to centroid vectors ( C to the feature vectors and generate the centroid vectors.

Algorithm (1) Assign the first feature f 1 as a representative for concept C (2) For each feature f i , compare the similarity Sim between the feature f (3) Choose the maximum value of similarity Sim called Sim (4) Repeat steps 2 and 3 until there is no more feature f All features are assigned according to the above algorithm. We obtain m initially partitioned clusters.
Then, these generated m centroid vectors are sorted according to the number of terms in each cluster. In the reallocation step, we follow the procedure described above X  X part from step 1. We reassign all features to the generated m clusters to obtain an improved partition. This reallocation process operates to select some initial partitions of the feature vectors and then to move these features from cluster to cluster, so as to obtain an improved partition. In all our experiments, the parameters u and v were set experimentally to 80% and 20%, respectively. Finally, these generated clusters are designated the primitive concepts. 2.4. Reinforcement of primitive concepts by classification
To construct more appropriate primitive concepts, we propose to apply a classification method prior to the clustering process, in order to discriminate the ambiguity of an individual word or phrase. It is expected that the classification method would improve the quality of the generated primitive concepts.
Example 3. Let us continue Example 1 of Section 2.2. We assume that a document d transportation and the feature vector f 3 in document d 2 0.2), (signal, 0.3)}. Comparing the feature vector f 3 with the feature vector f notice that those features look similar to each other, but f
 X  handle  X  ,  X  signal  X  , and  X  bus  X  in f 2 represent computer hardware process, these terms in f transportation issues. Generally, one possible solution to discriminate polysemy is to consider the other words surrounding the ambiguous terms (e.g.  X  data  X  in f
Classification is usually used for learning a function that maps an item into one of several pre-defined classes ( Mannila, 2002 ). We classify the feature vectors based on the pre-defined categories of the Yahoo!
Website 3 . Yahoo! categories have been generally applied as a well-organized classifier ( Labrou &amp; Finin, 1999 ). Firstly, we build 12 classifiers from Yahoo! categories. These categories are organized by subject;
Government, Society &amp; Culture, Education, Arts &amp; Humanities, Science, and Social Science. We take all the terms of one sub-category in each classifier and represent these classifiers in the form of a vector. We calculate the cosine similarity between all the feature vectors and the classifiers. Then, we assign the feature vectors to the most similar class. By assigning the feature vector to the correct class, we can interpret the ambiguity of terms, according to context. If one feature vector is not assigned to any class, this feature vec-tor will be allocated to a special category named  X  etc.  X  . Once the features are classified, we apply the clus-tering algorithm described in Section 2.3 to the features in each category. As a result, we get a set of primitive concepts from each category, respectively.

In this paper, we consider two different options for merging the primitive concepts of all categories. The
This approach allows us to obtain a more robust set of primitive concepts. As a second option, we merge the processes for constructing the primitive concepts. Table 1 lists the various possible data mining tech-niques. Note that we initially retrieve documents and then apply our framework to the set of retrieved doc-uments, given a particular query. PC1 and PC2 are constructed by applying local analysis and global ana-lysis without classification, respectively. However, we apply a classification to generate PC3 to PC6 before the clustering process. 3. Generation of query concepts using primitive concepts
We assume that an initial query and/or documents contain several ideas. On the other hand, the prim-itive concepts, seen to be surrogates of the domain knowledge, are extracted approximately orthogonally as discussed in Section 2. In other words, they are designed to represent single topics. Therefore, to enhance an initial query, we select its most associated primitive concepts and generate all its possible interpretations under a Disjunctive Normal Forms (DNF) form. The most probable interpretations are chosen as query concepts .

We propose to reformulate the initial queries by using the following methodology: (1) Select first top N primitive concepts (usually between 5 and 10 concepts) that are similar to the initial (2) Generate all the possible combination of primitive concepts under a Disjunctive Normal Forms (3) Choose the DNF that is most similar to the initial query q (4) Choose k high-frequency terms (usually between 5 and 10 terms) from the selected QC (5) Construct the enhanced query q 0 = a q 0 + b QC best , where 0 6 a 6 1 and b =1 a . a and b are called
Fig. 4 shows the process graphically. From all the possible DNFs, we select the one that is most similar to q 0 as the best query concept . For instance, if C 1  X  C query q 0 , C 1  X  C 3 becomes the best query concept . Among the terms in QC high-frequency terms (usually between 5 and 10 terms) to construct the enhanced query q these selected high-frequency terms to the initial query. Finally, we construct the enhanced query q = a q 0 + b ( C 1  X  C 3 ). 4. Experiments
We use the TREC8 ad hoc dataset for conducting all our experiments. It contains 521,251 documents distributed on TREC disks 4 and 5. 4 Fifty topics (topic 401 X 450) are chosen to evaluate the performance of the IR system. Each topic consists of three parts: title, description and narrative. It is thought that the different parts of the TREC topic allow the investigation of the effect of different query lengths on retrieval performance ( Lam-Adesina &amp; Jones, 2001 ). Since our hypothesis is that the user  X  s initial query is usually incomplete, we only use the title of topics. In the indexing process, we eliminate stopwords and applied the Porter  X  s stemming algorithm ( Porter, 1980 ). We evaluate the retrieval performance using the classical precision and recall measures ( Van Rijsbergen, 1979 ).
We describe our experimental results in a series of comparisons as follows. In Section 4.1, we build a baseline using the vector space model without relevance feedback, and then we perform the Rocchio  X  s rel-evance feedback method (RRF). In Sections 4.2 and 4.3, we present the results of the QCM-W ( Chang et al., 2004 ) and QCM-R approaches, respectively. 4.1. Baseline run and Rocchio X  X  relevance feedback
We conduct a baseline retrieval based on the vector space model without relevance feedback. Next, we perform the Rocchio  X  s relevance feedback method ( Rocchio, 1971 ). We use a Standard_Rocchio formula-tion to calculate the modified query ~ q 0 as follows:
Notice that ~ q 0 is an initial query vector, ~ d j is a document vector. j D documents in the sets of relevant and non-relevant documents (among the retrieved ones), respectively. a , b and c are tuning constants. In Rocchio  X  s method (RRF), since we do not use the non-relevant document information, no value is given for c . We assume that the top 10 retrieved documents are relevant to the initial query ( Harman &amp; Buckley, 2004 ). Then, we select 20 terms from those documents as relevant (ex-panded) terms. Table 2 shows the results of the baseline retrieval and the RRF runs, respectively. The
RRF results are worse than the baseline results, because this RRF is a form of simple blind feedback X  the added terms from the top ranked documents may not be suitable for query expansion. 4.2. Query concept method based on a whole set of document collection (QCM-W)
We conduct experiments using QCM-W and RRF on top of QCM-W (RRF-QCM-W). The QCM-W method reformulates the initial query with the query concepts generated by the process described in Fig. 3 . As described in Table 1 (second column), we apply various data mining techniques to the whole corpus and construct several primitive concepts using six approaches (PC1 to PC6). Due to space limitation, we have only included the best results, which were obtained using PC3 (see Tables 2 and 3 ). Recall that the set of primitive concepts PC3 is constructed by classifying and merging the twelve classes. Our weighting constants for the enhanced query q 0 were set experimentally to a = 0.8 and b = 0.2. The retrieval perfor-mance of QCM-W cannot outperform the baseline. Moreover, the performance of RRF-QCM-W ap-proach is less than that of the baseline and QCM-W. To analyze the results further, we look into the results of the poorly performing queries separately. The idea is that good queries do not need to be en-hanced by query concepts , as opposed to the poorly performing queries.

We split the 50 TREC8 topics into two groups: good queries and poor queries. We investigate whether the QCM-W approach is effective in the case of the poorly performing queries. The good/poor decision is based on the average precision achieved by our baseline. If the average precision is above 0.1, we consider the query to be good. Thirty-four queries with an average precision under 0.1 are identified to be poor. When we compare the results using the 34 poorly performing queries, we find that the RRF and our QCM-W approaches perform better than the baseline. Table 3 shows the results of the baseline and QCM-W using these 34 queries, respectively.

Generally, query reformulation consists of terms reweighting and/or query terms expansion. We re-weight the terms of the initial query if the selected primitive concepts have the same terms as the initial query (QTR in Table 3 ). On the other hand, QTE means that we expand the initial query with the selected k terms provided by the primitive concepts (QTE in Table 3 ).

Although QTR is just as good as the baseline, the retrieval performance of QTE is in general better than the baseline. It seems that the expanded terms by query concepts have a somehow positive effect on query expansion. Next, we compare the results of RRF and RRF on top of the QCM-W (RRF-QCM-W). All results of RRF approaches outperform the baseline. While the performance of RRF improves on the base-line by 5% (0.085 vs. 0.089), the performance of RRF-QCM-W performs 24% more than the baseline (0.085 vs. 0.105). QTE also performs better than QTR (0.093 vs. 0.105). Therefore, the retrieval performance of QCM-W has improved for the poorly performing queries.

As a consequence, we adopt a combined strategy, where the QCM approach is applied only for the poorly performing queries (C-QCM-W). Good queries are run with the baseline only, and the poor queries are run with the QCM-W approach (QTE based on PC3). Table 4 shows that our C-QCM-W approach overcomes the baseline. In terms of implementation, our QCM-W approach can be used in query reformu-lation when users are not satisfied with the documents produced by the first search. 4.3. Query concept method based on the set of retrieved documents (QCM-R)
Our QCM-W query enhancing approach is only useful when applied to the poorly performing queries, i.e. when it is possible to classify the queries according to their performance. It is necessary to have consis-tently better results than the baseline. Therefore, we consider another document space; the set of retrieved documents (QCM-R). For a given query, we initially retrieve all the documents that contain the query terms. In order to examine the impact of sampling a subset of the top ranked documents, we restrict the set of returned documents to only the top 10000, 1000, 500, and 100 retrieved documents, respectively.
Next, we apply our proposed six approaches of Table 1 on the set of retrieved documents, and generate the corresponding primitive concepts. We perform these processes on each query, and then calculate the aver-age precision of the 50 TREC8 queries. Table 5 shows an improvement in the retrieval performance when the retrieved set of documents is used as a document collection (compared to Table 2 ).
 In general, the performance of the QTR approaches is better than baseline. In contrast, the results of
QTE are lower than the baseline. We conjecture that the retrieved set of documents cannot provide enough information for query expansion. In other words, for efficient query expansion, expanded terms should de-note some appropriate concepts that the initial query cannot represent. In this case, since the set of retrieved documents is generated by the initial query, it is inevitably impossible to get complementary concepts.
However, when we compare the results within the 34 poorly performing queries, the QTE approach is gen-erally better than the baseline (see Table 6 ). As a result, the QTR approach compensates the weight of ini-tial query terms with query concepts . Hence, it is preferable to reweight the query terms without expanding terms in the QCM-R approach.

Now, we consider which procedure described in Fig. 3 shows good results in QCM-R. In Table 5 ,QTR is based on PC5 and QTE is based on PC2. Due to space limitations, we list only the best results for each case. In fact, the results of PC5 and PC3 show almost identical retrieval performances in QTR. Both PC5 and PC3 are generated by applying a classification method. Hence, it is necessary to apply a classification method in the QTR approach. However, it is not important whether we include the  X  etc.  X  class or not. It is probable that since many of the retrieved documents contained similar topics, a few features would have been inappropriately assigned to the  X  etc.  X  class. On the other hand, PC2 and PC1 outperform the baseline only in 34 poorly performing queries (see Table 6 ). Both of them are generated by clustering only. It seems that various data mining techniques are not useful to QTE in QCM-R. On the whole, the results of PCs generate by Approach 2 , which takes significant terms only, are not able to show good results as we ex-pected. From all these results, we may infer that if we use a set of retrieved documents to build concepts, we can consistently get better results than the baseline, provided we use QTR and apply clustering and classification.

Finally, the conducted experiments show that sampling subsets of the top ranked documents do not have a significant effect on the overall performance of the QTR and QTE approaches, respectively. The results indicate that a small number of retrieved documents (e.g. 100, 500) would reduce the overall computational overhead, while not affecting the retrieval performance. 5. Conclusion
We have discussed query reformulation using automatically generated query concepts from a document space. The query concepts , which are meant to precisely denote the user  X  s information needs, are based on the extraction of features from a document space. We have investigated the impact of the document space on the effectiveness of the proposed query reformulation mechanism. Our previous research ( Chang et al., 2004 ) merely showed that our query enhancing approach, which constructed the query concepts from the whole document collection, was only useful in the case of poorly performing queries. However, in this pa-per, we explored the construction of primitive concepts from the retrieved set of documents, and showed that we could consistently get better results than the baseline, provided that we use QTR and apply clus-tering and classification. We also showed that the QCM-R approach performs better than the QCM-W approach.

The proposed approach has many potential applications, in addition to query reformulation. Firstly, the methods, e.g. summarization, clustering, and classification were used to construct approximately orthogo-nal primitive concepts. However, if we develop more appropriate methods for constructing the primitive concepts, a greater improvement in retrieval performance would be achieved. Secondly, since our approach is able to construct primitive concepts directly from a document space, it could be used as a fundamental step in the building of an ontology for a given collection.

Another interesting application of the proposed methodology would be the prediction of the quality of the initial query ( Cronen-Townsend, Zhou, &amp; Croft, 2002 ), which is still an open research question as as-serted by the new TREC 2003 X 2004 Robust tracks. Since our approach is designed to apply to ill-defined initial queries, if we can predict the quality of the initial query, we could use it to reformulate an appropri-ately enhanced query. Similarly, we are considering other methods not only for the purpose of constructing primitive concepts from document spaces more effectively, but also for the generation of probable query concepts.
 Acknowledgement This research study was conducted in the Information Retrieval Group of the University of Glasgow.
We would like to thank our fellow Information Retrieval Group members for their assistance and cooperation.
 References
