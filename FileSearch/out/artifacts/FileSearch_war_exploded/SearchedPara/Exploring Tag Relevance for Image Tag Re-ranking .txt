 I n this paper, we propose to explore the relevance between tags for image tag re-ranking. The key component is to define a global tag-tag similarity matrix, which is achieved by analysis in both semantic and visual aspects. The text semantic relevance is explored by the Latent Semantic Indexing (LSI) model [1].For the visual information, the tag-relevance can be propagated by reconstructing exemplar images with visually and semantically consistent images. Based on our tag relevance matrix, a random-walk approach is leveraged to discover the significance of each significance values. Extensive experiments show its effectiveness on an image dataset with a large tags vocabulary. H .3.3 [ Information Storage and Retrieval ]: Search process A lgorithms, Measurement, Performance. T ag re-ranking, random walk, latent semantic indexing. Recently, millions of tagged images are available on line in social community. However, tags are labeled in a random order and cannot accurately describe the image content. Therefore, it hinders the applications in the real world. Image tag re-ranking becomes an interesting topic in research community [2] and industry. In relevance among tags from text and visual perspectives. In sum, this paper has three main contributions: (1) we propose a new scheme to measure tag relevance by integrating a subset of images sharing the same target tag into a global tag-tag similarity matrix; (2) we adopt Latent Semantic Indexing to measure tag-tag text similarity and integrate them with different schemes; and (3) we design a three-step approach to propagate the tag relevance representative exemplar image. Given an image with several tags, our goal is to re-rank those tags, ranked. We exploit the tag-tag relevance from the perspective of text similarity and visual similarity, respectively. Denote I as an image dataset with n images, and T as tag vocabulary with m tags. We explore tag-tag semantic relevance in a tag-specific manner. Each image is labeled with a set of tags, denoted as . For each tag , we find all images from I that contain . We denote such image set as ( ) , and a tag-image matrix can be constructed as follows, images in ( ) sharing the same tag and all the tags occurred in those images. ( , ) denotes the occurrence of tag on image . In Section 2.1, we study the tag-tag text similarity matrix , by Latent Semantic Indexing [1] on tag occurrence. And a tag-tag visual similarity matrix formulated by the propagated tag relevance from trustable images in Section 2.2. We further apply integration functions below to obtain global tag-tag visual similarity -/ and text similarity Then, we embed the global tag-tag similarity -into a random walk based framework for image tag re-ranking. where ' is the relevance score vector, -is a transition matrix, indicates the probability of the transition from tag to tag , describes the relevance between the image and a labeled tag. a damping factor ranging from 0 to 1. The process can be of ( 1  X  7 ) gives a score to other tags based on transition probability , . Eventually, it converges and the tags with high relevance score will be re-ranked to the top. Given images sharing the same tag in the form of tag-image matrix # $ % . We apply Latent Semantic Indexing [1] to estimate the similarity among these tags. More specifically, we do Singular Value Decomposition on # $ % , # = :;8 &lt; (5) where : and 8 are the left and right singular vector, diagonal matrix of singular values. By keeping the top k largest singular value in ; , and set the remaining as zeros, we can obtain # &gt; % .The similarity matrix between two terms is We design different integration functions in Eq. (1). row-based selection. For tag and its tag-tag similarity matrix -, we copy the row where lies to the global tag-tag similarity matrix -(?) . -(@) is the summation of weighting function (@) . It is a common way to visually represent a tag by images containing it. It unavoidably introduces noise while comparing visual similarity since images contain multiple objects. Therefore, we divide the images containing the same tag into groups so that images within the same group are more visually similar. For a propagated tags from trustable images: (1) Clustering images sharing the same tag into groups by Affinity Propagation algorithm [3]. Each group consists of an exemplar image and group member images. (2) Applying Locality-constrained Linear Coding (LLC) [4] to reconstruct exemplar image by group members. The members with high contribution should be semantically similar to the exemplar, because they lie in the same group sharing the same latent sub-concept and they all strongly agree with the exemplar. We consider those images as trustable images. (3) Propagating weighted tag information from the trustable images to form tag relevance vector for a certain group. The aggregated tag relevance vectors from groups are used to describe the similarity between and other tags. into groups. A red triangle represents the exemplar of each group. The green dots are member images with high contribution for the reconstruction and their tag information will be propagated with certain weight. Figure 1. A toy case of reconstructing exemplar by similar images We evaluate the proposed approaches on NUS-WIDE dataset [5] with 269,648 images. A challenging tag set with 4,926 unique tags is obtained by filtering the noisy tags and tags with less than 30 images in the training set. The ranking accuracy is measured by NDCG@5, which measures the consistency between the top-N ranked tags and the ground truth. To build the ground-truth dataset, 150 images are manually labeled with the relevance for each tag on a scale of 5. For each image, we extract a 712-dimension feature, including 225-dimension block-wise color moment feature and 512-dimension GIST feature. In experiment, we estimate image and tag similarity by kernel density estimation. We use all images in each group to reconstruct the exemplar image and integrate the tag relevance from all groups with norm. Baseline We implement Liu X  X  method [2] for comparison and use raw data as baseline. In terms of accuracy, Liu X  X  result is 73.36% and the baseline is 65.98%. Another baseline is to use Google distance in [2] as , in PageRank, and its accuracy is 68.37%. We conduct extensive experiments with different integration schemes and combinations for visual similarity and text similarity in Table 1. , . , , (?) and , (@) indicate the performance purely based on itself.  X  X  X  refers to the Google distance used in Liu [2].  X  , +G X  refers to Google distance combined with our visual tag-tag integration functions (?) and (@) mentioned in section 2.1 Accuracy As shown in Table 1, our best performance is 78.58%. It improves 5.22% over Liu [2], improves 12.6% over the raw data baseline, and 10.21% over the Google distance. In our approach, we apply all images to reconstruct exemplar image with H norm among groups of image for each tag, 7 = 8, , (?) and , (@) can work well individually, and their combinations also have some improvement. Efficiency Our approach is 8% faster than Liu X  X  approach [2], because in Liu X  X  work the visual similarity is calculated online by K-NN search for every tag. Ours is estimated by the aggregated vote for representative images as well as the weighted image run-time. In this paper, we propose a new approach for image tag re-ranking by exploiting tag-tag relevance using Latent Semantic Indexing on tag occurrence. We also study the tag-tag visual relevance by propagating tags based on their contribution to exemplar reconstruction. We evaluate our approach on a large scale vocabulary dataset with 4,926 tags. Extensive experiments demonstrate the superiority of our approach over others. This work was supported in part to Dr. Qi Tian by ARO grant W911BF-12-1-0057, NSF IIS 1052851, Faculty Research Awards by Google, NEC Laboratories of America and FXPAL, respectively. [1] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer ,T. K., [2] Liu , D., Hua, X.S., Yang, L.,Wang, M., Zhang, H.J., Tag ranking, [3] Frey, B. J. and Dueck, D , Clustering by Passing Messages Between [4] Wang, J., Yang, J., Yu, K., Lv, F., Huang, T. and Gong, Y., [5] Chua, T., Tang, J., Hong, R., Li,H., Luo,Z., and Zheng,Y. NUS-
