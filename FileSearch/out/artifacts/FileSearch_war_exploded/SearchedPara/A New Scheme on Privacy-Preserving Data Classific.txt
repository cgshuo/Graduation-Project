 } We address privacy-preserving classification problem in a distrib-uted system. Randomization has been the approach proposed to preserve privacy in such scenario. However, this approach is now proven to be insecure as it has been discovered that some privacy intrusion techniques can be used to reconstruct private informa-tion from the randomized data tuples. We introduce an algebraic-technique-based scheme. Compared to the randomization approach, our new scheme can build classifiers more accurately but disclose less private information. Furthermore, our new scheme can be read-ily integrated as a middleware with existing systems.
 H.2.8 [ Database Management ]: Database Applications X  Data min-ing ; H.2.7 [ Database Management ]: Database Administration X  Security, integrity, and protection Security Privacy, Privacy-preserving data mining
In this paper, we address issues related to privacy-preserving data mining. In particular, we focus on privacy-preserving data classi-fication. General classification techniques have been extensively  X 
This work was supported in part by the National Science Founda-tion under Contracts 0081761, 0324988, 0329181, by the Defense Advanced Research Projects Agency under Contract F30602-99-1-0531, and by Texas A&amp;M University under its Telecommunication and Information Task Force Program. Any opinions, findings, con-clusions, and/or recommendations expressed in this material, either expressed or implied, are those of the authors and do not necessar-ily reflect the views of the sponsors listed above.
 Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00. studied for over twenty years [15]. The main purpose of data clas-sification is to build a model (i.e., classifier) to predict the (cate-gorical) class labels of data tuples [10] based on a training data set where the class label of each data tuple is given. The classifier is usually represented by classification rules, decision trees, neural networks, or mathematical formulae that can be used for classifica-tion.

In recent years, the issue of privacy protection in classification has been raised [2, 14]. The objective of privacy-preserving data classification is to build accurate classifiers without disclosing pri-vate information in the data being mined. The performance of privacy-preserving techniques should be analyzed and compared in terms of both the privacy protection of individual data and the predictive accuracy of the constructed classifiers.

We consider a distributed environment in which training data tu-ples are stored in multiple autonomous entities. We can classify distributed privacy-preserving classification systems into two cat-egories based on their infrastructures: Server-to-Server (S2S) and Client-to-Server (C2S), respectively.

In the first category (S2S), data tuples in the training data set are distributed across several servers. Each server holds a private database, which contains part of the training data set. The servers collaborate with each other to construct a classifier over the integra-tion of all databases without letting either server know the private information of the other parties. This problem is usually formu-lated as a variation of secure multiparty computation problem [14]. Existing algorithms in this category can build decision trees [7, 14] and na  X   X ve Bayesian classifiers [12, 16] when the training data tu-ples are vertically [16] or horizontally [7, 12, 14] partitioned into multiple databases.

In the second category (C2S), a system usually consists of a data miner ( server ) and numerous data providers ( clients ). Each data provider holds only one training data tuple. As is commonly as-sumed [2], the class label attribute of each data tuple is not consid-ered as sensitive information by the data providers. All other at-tributes contains private information which needs to be preserved. The data miner builds a classifier on the aggregate data provided by the data providers. Due to privacy concern, the data miner may compromise private information in the data being mined. To pre-vent privacy from being compromised by the data miner, counter-measures must be implemented with the data providers. An on-line survey system is a typical example for C2S systems, as the system consists of one survey collector/analyzer (data miner) and thousands of survey respondents (data providers).
 Both S2S and C2S systems have a broad range of applications. Nevertheless, we focus on studying privacy-preserving data clas-sification in C2S systems. In a C2S system, the common objec-tive of the data providers and the data miner is to build a predic-tively accurate classifier. Besides, the data providers have an ob-jective to preserve their private information. As such, the goal of privacy-preserving data classification in C2S systems is to limit the information obtained by the data miner to be minimum necessary to accomplish the intended purpose of building predictively accurate classifier. This goal is also referred to as  X  X inimum necessary stan-dard X  in real-world privacy rules (e.g., Health Insurance Portability and Accountability Act (HIPAA) privacy rule [11]).

Previous studies observed that precise values of individual train-ing data are not necessary in data classification. As is shown in [2], accurate classifiers can be built upon a robust estimate of the dis-tribution of training data tuples. Randomization approach has been proposed for the data providers to add random noise to private data tuples before transmitting them to the data miner. As such, the data providers protect their privacy by using the random noise. The data miner can still reconstruct the original distribution from the ran-domized data and thereby build an accurate classifier.

Most of the current studies on C2S systems tacitly assumed that randomization was the only effective approach to preserving pri-vacy while keeping the mining results meaningful. In the random-ization approach, each attribute of a training data tuple has to be equally processed (i.e., randomized by the data providers and trans-mitted to the data miner) because a data provider cannot obtain any information from either the data miner or other data providers indi-cating which attributes are more important for building an accurate classifier. As we will illustrate in Section 2, there are several prob-lems with this kind of approach:
In this paper, we develop a new scheme based on algebraic tech-niques. In our scheme, the data providers do not just perturb their data by using random noise. Instead, a perturbation guidance is transferred from the data miner to the data providers as a refer-ence to the data perturbation. Roughly speaking, the perturbation guidance indicates which attributes of the data tuple are the mini-mum necessary ones to build an accurate classifier. After checking the validity of the perturbation guidance, the data providers perturb their data accordingly. As such, our scheme adheres to the mini-mum necessary standard by transmitting only the minimum neces-sary information to the data miner.

We will demonstrate that our new scheme has the following im-portant features to distinguish itself from previous approaches.
The algebraic-techniques-based approach was first proposed in our work for association rule mining [17]. Significant differences between our work in this paper and [17] include
The rest of the paper is organized as follows: We briefly review the randomization approach in Section 2. In Section 3 and Sec-tion 4, we introduce our new scheme and its basic components, respectively. We present a theoretical analysis on the performance of our scheme in Section 5. Theoretical bounds on the accuracy and privacy metrics are also derived in this section. An experimen-tal performance evaluation of our scheme is provided in Section 6. In this section, we make a comparison between the performance of our scheme and the randomization approach, and show the simu-lation results of our scheme on real data sets. The implementation and runtime efficiency of our scheme is discussed in Section 7, fol-lowed by final remarks in Section 8.
In this section, we review the randomization approach, which has been proposed and used to preserve privacy in data classifica-tion. We also analyze the problems associated with this approach, motivating us to propose a new scheme on privacy-preserving data classification.
Based on the randomization approach, the entire privacy-preserving classification process can be considered a two-step process. The first step is for data providers to randomize their data, and trans-mit the (randomized) data to the data miner. As in an online sur-vey system where different survey respondents come at different time, we consider this step to be iteratively carried out in a group plies a randomization operator R (  X  ) to its data tuple and transmits the randomized data tuple to the data miner. In previous studies, several randomization operators have been proposed including the random perturbation operator [2] and the random response opera-tor [8], which are shown in (1) and (2), respectively, where t is the original data tuple, r is the random noise, and  X  is a parameter predetermined by the data providers. As the result of this step, the data miner obtains perturbed training data tuples from the data providers. processes are executed in a serializable manner.
In the second step, the data miner builds a classifier on the aggre-gate data. With the randomization approach, the data miner must first employ a distribution reconstruction algorithm that intends to reconstruct the original data distribution from the randomized data tuples. Several distribution reconstruction algorithms have been proposed [1,2,8]. For example, the expectation maximization (EM) algorithm [1] reconstructs a distribution which converges to the maximum likelihood estimate of the original distribution.

Also in the second step, a malicious data miner may compromise private information using a privacy data recovery algorithm on the randomized data tuples supplied by the data providers.

Figure 1 depicts the system architecture with the randomization approach. Note that there are two kinds of data miners: an honest data miner which performs legal data mining functions without any intent to discover private data of the data providers; and a malicious data miner which is interested in the privacy of data providers and uses private data recovery method to realize this goal. Clearly, any such data classification system should be measured by its capabil-ity of both building accurate classifiers and preventing private data leakage.
While the randomization approach is intuitive, researchers have recently identified privacy breaches as one of the major problems with the randomization approach. It is shown in [13] that the spec-tral properties of randomized data could help the data miner to separate noise from private data. In particular, a filtering method is proposed based on random matrix thoery to reconstruct private data from the randomized data set [13]. The performance of this method demonstrates that randomization preserves very little pri-vacy in many cases.

Randomization approach also suffers from efficiency problems as it puts a heavy load on the data miner at run time (because of the distribution reconstruction). It is shown in [3] that the cost of mining randomized data set is  X  well within an order of magnitude  X  mining, we believe that the similarity between randomization op-erators in association rule mining and data classification makes the efficiency concern inherent in the randomization approach.
Another problem with the randomization approach is that it can-not be adapted to the diverse needs of data providers. A survey [6] on privacy concern shows that among the Internet users (potential data providers), there are 17% privacy fundamentalists, 56% pri-vacy pragmatists, and 27% marginally concerned individuals. Pri-vacy fundamentalists are extremely concerned about privacy. Pri-vacy pragmatists are concerned about privacy, but their concerns are much less than those of the fundamentalists. Marginally con-cerned individuals are generally willing to provide their private data. The randomization approach treats all the data providers in the same manner and does not address the differing need of data providers. As such, a privacy fundamentalist may not want to pro-vide its data while the accurate data from a marginally concerned individual is wasted.

We believe that the following are the reasons behind the above-mentioned problems.
The one-way communication scheme is inherent in the random-ization approach. This motivates us to propose a new scheme which allows two-way communication between the data miner and the data providers. Another possible solution to the problems of ran-domization approach is to introduce communication between data providers. We do not take this method because in our system model, different data providers come at different time and thus may not be able to communicate with each other. Our other hesitation with this approach includes 1) it introduces a problem of the trustworthiness of other data providers, and 2) it may place high computational load on data providers, which are supposed to have lower computational power than the data miner.
In this section, we introduce our scheme. Figure 2 depicts the infrastructure of our new scheme. The communication protocol of our scheme is shown in Algorithm 1. In our scheme, there is an im-portant parameter for each data provider, called maximum accept-able disclosure level , which is denoted by k i . Roughly speaking, if we consider the perturbed data tuple as a random vector, then k is the degree of freedom of the perturbed data tuple, which in most cases is much smaller than the degree of freedom of the original data tuple. With a larger k i , the data miner can make a more stable estimation on the distribution of original data tuples. Nonetheless, the data miner will also have more information about the individ-ual private data tuple. Thus, the larger k i is, the more contribution the perturbed data tuple will make to building the classifier. The smaller k i is, the more private information is preserved. As such, a privacy fundamentalist can choose a small k i to protect its pri-vacy. A privacy unconcerned individual can choose a large k help building a more accurate classifier. The relationship between k and the amount of privacy disclosure is analyzed in Section 5 and demonstrated in Section 6.

Before sending its data to the data miner, a data provider first in-quires the data miner what the current system disclosure level k  X  is. Roughly speaking, k  X  is the minimum necessary disclosure level for the data miner to construct an accurate classifier. The perturba-tion guidance component of the data miner computes k  X  and trans-mits it back to the data provider. If k  X  is not acceptable by the data provider (i.e., k  X  &gt;k i ), the data provider can keep trying. As we will show in Section 6, k  X  decreases rapidly when the num-ber of data tuples received by the data miner increases. Since the levels of privacy concerns vary among different data providers [6], the system disclosure level will be acceptable by all data providers eventually.

If k  X  is accepted by a data provider (i.e., k  X   X  k the system disclosure level k  X  and dispatches V  X  k to the data provider. Roughly speaking, V  X  k is the vector that projects the original data tuple into a k  X  -dimensional subspace where the data tuples from different classes are most different. As such, the private informa-tion divulged by the perturbed data tuple is the most valuable infor-mation for constructing accurate classifiers. This complies to our standard of disclosing only the minimum necessary information to the data miner.
 Once V  X  k is received, the data provider checks the validity of , computes the perturbed data tuple R ( t i ) from its private data tuple t i , and transmits R ( t i ) along with its class label to the data miner. After all data providers send their data to the data miner, the perturbed data tuples received by the data miner are directly used as the training data tuples to build the classifier.

As we can see, our scheme requires two rounds of message ex-change to dispatch the perturbation guidance: the round to inquire k  X  and the round to inquire V  X  k . Another possible approach is to let a data provider transmit its maximum acceptable disclosure level k to the data miner. If k i  X  k  X  , the data miner transmits V  X  back to the data provider. This approach only requires one round of message exchange. However, privacy breach may occur if this approach is used because when k i &gt;k  X  , a malicious data miner can manipulate a disclosure level  X  k such that k i  X   X  k&gt;k  X  and gen-Algorithm 1 Our new scheme For the data miner: 1: Upon receiving an inquiry message from a data provider, the 2: Upon receiving a ready message from a data provider, the PG 3: Upon receiving a perturbed data tuple R ( t i ) and its class label For a data provider: Input : k i , the maximum acceptable disclosure level of the data provider. 1: Sends an inquiry message to the data miner to obtain the cur-2: if the received k  X  is less than or equal to k i then 3: Sends a ready message to the data miner. 4: else 5: Goto 1; 6: end if erate a perturbation guidance based on  X  k . As such, the data miner may compromise private data which are unnecessary to build an accurate classifier.

Compared to the randomization approach, our scheme does not have the distribution recovery component. Instead, the classifier construction procedure is performed on the perturbed data tuples directly. Our scheme has two key components, which are the per-turbation guidance (PG) component of the data miner and the per-turbation component of the data providers. We will introduce these two components in details in the next section.
The basic components of our scheme are: a) the PG component of the data miner which computes the current system disclosure level k  X  and the perturbation guidance V  X  k , and b) the perturbation and perturbs the data tuple. Before presenting the details of these components, we first introduce some notions of the training data set.

Let there be m data providers in the system, each of which holds a private data tuple t i ( i  X  [1 ,m ]) and its class label attribute a The private data tuple consists of n attributes a 1 , ... , a label attribute is not sensitive and indicates which predefined class the data tuple belongs to. All other attributes are private informa-tion. The data miner has no external knowledge about the private information of the data providers.

In this paper, we assume there be two classes C 0 and C 1 such, the class label attribute has two distinct values 0 and 1 , cor-responding to classes C 0 and C 1 , respectively.

We first consider the case where all attributes are categorical (i.e., discrete-valued). If an attribute is continuous valued, it must be discretized first. An example of such discretion is provided in Sec-tion 6. Let the number of distinct values of a j be s j . Without loss of generality, let a j  X  X  0 , ... , s j  X  1 } . We denote a private data tuple t i by an ( s 1 + ... + s n ) -dimensional binary vector as follows. In the s j bits for a j , the h -th bit is 1 if and only if a
Although our scheme applies to all categorical attributes (with arbitrary s j ), for the simplicity of discussion, we assume that all attributes a 1 ,...,a n are binary. That is, s 1 =  X  X  X  = s 2 . As such, each private data tuple can be represented by a 2 n -dimensional vector. We represent the private part of the training data set by an m  X  2 n matrix T =[ t 1 ; ... ; t m ] . 3 We denote the transpose of T by T . We use T ij to denote the element of T with indices i and j . Let T 0 and T 1 be the matrices that represent the private data tuples in class C 0 and C 1 , respectively. We denote the number of data tuples in T i by | T i | . An example of T is shown in Table 1. As we can see from the matrix, data tuple t 1 to class C 1 and has three attributes [ a 1 ,a 2 ,a 3 the sake of completeness, we list the notions used in this paper in Appendix A as references.

As we are considering the case where data tuples are iteratively fed to the data miner, the data miner keeps a copy of all received data tuples and updates it when a new data tuple is received. Let the current matrix of received data tuples be T  X  . When a new data tuple R ( t i ) is received by the data miner, R ( t the bottom of T  X  . Without loss of generality, we assume that data tuple R ( t i ) is the i -th data tuple that is received by the data miner. As such, when the data miner receives m  X  data tuples, T  X  is an m  X   X  2 n matrix [ t 1 ; ... ; t m  X  ] . In order to compute the perturbation guidance for the first-come data provider, we assume that before the data collection process begins, the data miner already has m ( n  X  m 0 m ) data tuples in T  X  . These data tuples can either be collected from privacy unconcerned data providers, or be randomly generated.

Besides the received data tuples T  X  , the data miner also keeps track of two additional 2 n  X  2 n matrices: A  X  0 = T  X  0 1 T  X  1 where T  X  0 and T  X  1 are the matrices of received data tuples that belong to class C 0 and C 1 , respectively. Note that the update of A  X  0 and A  X  1 (after R ( t i ) is received) does not need access to any data tuple other than the recently received R ( t i ) . Thus, we do not require the matrix of received data tuples (i.e., T  X  ) to remain in main memory. If the class label attribute received with R ( t satisfies a 0 = c ( c  X  X  0 , 1 } ), A  X  c is updated as follows.
Given A  X  0 and A  X  1 , using eigen decomposition, we can decom-3 In the context of training data set, t i is a data tuple. In the context of matrix, t i is the corresponding row vector in T .  X  X  X  X   X   X  2 n ,  X   X  i is the i -th eigenvalue of A  X  , and V  X  is an 2 n  X  2 n unitary matrix composed of the eigenvectors of A  X  .

The perturbation guidance component has two objectives: to de-termine k  X  , and to compute V k based on k  X  . We address the com-should be the minimum degree of freedom of R ( t i ) that maintains an accurate estimation of the eigenstructure of A = T 0 T Based on the eigen decomposition of A  X  , we can compute k  X  as the minimum number that satisfies where  X  is a parameter predetermined by the data miner. A data miner that desires a highly accurate classifier can choose a small  X  to ensure a stable estimation of A . A data miner that can tolerate a relatively lower level of accuracy can choose a large  X  to help protecting data providers X  privacy. In order to choose a good cutoff a simple textbook heuristic is to set  X  = 15% .

Given k  X  , V  X  k is an 2 n  X  k  X  matrix that is composed of the first k  X  eigenvectors of A  X  (i.e., the first k  X  column vectors of V  X  , which correspond to the k  X  largest eigenvalues of A  X  ). In particular, if V  X  =[ v 1 ,...,v 2 n ] , then V  X  k =[ v 1 ,...,v k  X  ] . Since V  X  is a unitary matrix, we have V  X  k V  X  k = I , where I is the identity matrix.
We note that due to efficiency and privacy concern, the data miner only updates k  X  and V  X  k once several data tuples are received. The privacy concern is that if V  X  k is updated once every data tuple is received, a malicious data provider may infer the perturbed data though the victimized data provider is comfortable transmitting the perturbed data tuple to the data miner, it may not be comfortable divulging it to another data provider.
 The justification of k  X  and V  X  k will be provided in Section 5. The runtime efficiency of computing k  X  and V  X  k will be addressed also be addressed in Section 7.
The perturbation component has two objectives: to check the validity of a received V  X  k , and to perturb t i based on V  X  a data provider receives V  X  k from the data miner, the perturbation component first checks if V  X  k is a 2 n  X  k  X  matrix which satisfies k V  X  k = I , where I is the identity matrix. If so, the perturba-tion component perturbs the private data tuple t i based on V  X  result is a perturbed data tuple that will be transmitted to the data miner along with class label attribute a 0 . In our scheme, the per-turbation is a two-step process. Recall that the private data tuple t is represented as a 2 n -dimensional row vector. In the first step, t is perturbed to be another 2 n -dimensional row vector  X  Since the elements in  X  t i may be real values, we need a second step to transform  X  t i to R ( t i ) such that every element in R ( t to { 0 , 1 } . In particular, for any j  X  [1 , 2 n ] , the data provider gen-erates a real number r which is chosen uniformly at random from [0 , 1] and computes R ( t i ) as follows.
 where  X  j is the j -th element of a vector. As we can see, the prob-ability that R ( t i ) j =1 is equal to  X  t i 2 j .
The communication overhead of transmitting R ( t i ) will be ad-dressed in Section 7.
In this section, we analyze our new scheme. We will define mea-sures on 1) the error of classifiers built on the perturbed data set, and 2) the amount of privacy disclosure. We will derive bounds on them, in order to provide guidelines for the tradeoff between these two measures and hence help system managers setting parameters in practice.
Given a testing data tuple t : a 1 ,...,a n without class label, the objective of data classification is to identify C i that maximizes As P ( t ) is constant for all classes, the objective is to find C maximizes P ( C i ,t ) . Since t contains n attributes, the cost of com-puting P ( C i ,t ) is too expensive. A common compromise is to compute P ( C i ,t ) based on P ( C i ,t s ) where t s is a small subset of { a 1 ,...,a n } . For example, in na  X   X ve Bayesian classification, the product of P ( C i ,a j ) is used to approximate P ( C i ,t ) . In decision tree classification, P ( C i ,t ) is approximated by P ( C is a set of selected test attributes, which correspond to the nodes in the decision tree.

For any data tuple t , given size h  X  [1 ,n ] , let t h attributes of t . We measure the error of classifiers built on the perturbed data set in our scheme by the maximum estimation er-ror of P ( C 0 ,t s )  X  P ( C 1 ,t s ) after perturbation. Let the value of P ( C i ,t s ) estimated by the perturbed training data set be The error on P ( C 0 ,t s )  X  P ( C 1 ,t s ) is defined as Given these notions, we define the degree of error as follows.
D EFINITION 1. The degree of error l e is defined as the maxi-mum of l e ( h ) on all sizes. That is, Generally speaking, the degree of error measures the discrepancy between classifiers constructed from the original data set and the perturbed data set.

Recall that  X  is the pre-determined parameter used by the data miner to compute k  X  . Recall that A = T 0 T 0  X  T 1 T 1 derive an upper bound on the degree of error as follows.

T HEOREM 5.1. In our scheme, when m is sufficiently large, there is where  X  1 is the largest eigenvalue of A .

P ROOF . (sketch) We note that for all C i ,if t s  X  t s be P ( C i ,t s )  X  P ( C i ,t s ) . As such, we first prove that in the most difficult cases where h  X  X  1 , 2 } , there is l e ( h )  X   X  X  to do so, we first show an intuitive explanation of A . Consider an element of A with indices 2 i and 2 j ,wehave where # { C i ,a i = a j =1 } is the number of data tuples that satisfy a = a j =1 and belong to C i . Note that  X  b 1 , b 2  X  X  0 , 1 } , there is
Pr { C i ,a i = b 1 ,a j = b 2 } = # { C i ,a i = b 1 ,a j Generally, we have As we can see, l e (1) is in proportion to the maximum error on the estimate of the diagonal elements of A , l e (2) is in proportion to the maximum error on the estimate of the other elements of A . Let the matrix of the perturbed training data set be T corresponding A derived from T R be A R . We now derive an upper bound on max ij | A  X  A R ij | .

Recall that in the first step of the perturbation, a data provider computes  X  t i = t i V  X  k V  X  k . Let  X  T be an m  X  2 n matrix composed of  X  t i (i.e.,  X  T =[  X  t 1 ; ... ;  X  t m ]) .  X  T i and  X  ingly. Due to our computation of  X  t i ,wehave  X  T i = T vectors of A . In real cases, the first k  X  eigenvectors of A  X  converge to those of A fairly quickly.
 ments are the first k  X  eigenvalues of A (i.e., the diagonal of  X   X  [  X  1 ,..., X  k  X  ] ). We have That is,  X  A is the k  X  -truncation of A [9]. Thus,  X  A is the optimal rank-k  X  approximation of A in the sense that within all rank-k  X  matrices,  X  A has the minimum A  X   X  A 2 . In particular, we have As we can see from the determination on disclosure level k  X  , our scheme maintains a cutoff k  X  such that  X  k  X  +1  X   X  X  1 . Thus, we have Since the absolute value of every element of a matrix is no larger than the 2 -norm of the matrix [9], we have
As we can see from the computation of R ( t ) , for any i, j  X  [1 , 2 n ] ,wehave  X  t 2 i =Exp( R ( t ) 2 i ) , (25)
Exp( t i t j  X  R ( t ) i R ( t ) j )  X  2( t i t j  X   X  t i where Exp(  X  ) refers to the expected value. Thus, when m is suf-ficiently large, we have l e ( h )  X  2  X  X  1 /m for h  X  X  1 , 2 } . This bound can be easily extended to l e ( h ) with h  X  3 . We omit the proof here due to space limit.
In our scheme, we need to guarantee that for any private training data tuple t , the data miner cannot deduce the original t from the perturbed R ( t ) . In particular, we must consider the case when the adversary manipulates V  X  k to compromise the privacy of the data providers.

Recall that our scheme allows different data providers to choose different disclosure levels. Thus, we define privacy disclosure mea-sure on individual data providers. Formally, let the maximum ac-ceptable disclosure level selected by a data provider be k 2 n  X  k  X  matrix  X  V  X  k , let  X  R ( t,  X  V  X  k ) be the output of R ( t and V  X  k =  X  V  X  k . With these notions, we define the degree of privacy disclosure as follows.

D EFINITION 2. The degree of privacy disclosure, l p ( k i fined by the maximum fraction of private information disclosed by the perturbed data tuple when the data miner sends a arbitrary 2 n  X  k  X  matrix  X  V  X  k as the perturbation guidance. That is, where I ( t ;  X  R ( t,  X  V  X  k ) is the mutual information [5] between t and  X  R ( t,  X  V  X  k ) , H (  X  ) denotes the information entropy. In the definition, I ( t ;  X  R ( t )) measures the amount of private infor-mation about t that is disclosed by  X  R ( t,  X  V  X  k ) . H ( t ) measures the amount of information in t . Thus, the degree of privacy disclosure measures the percentage of private information that is disclosed by  X  R ( t,  X  V  X  k ) .

T HEOREM 5.2. In our scheme, we have where  X  j is the j -th singular value of T .

P ROOF . (sketch) Consider the matrix T  X  V  X  k  X  V  X  k that consists of  X  t = t i  X  V  X  k  X  V  X  k . Given any 2 n  X  k matrix  X  V  X  k , the rank of larger than k  X  . Thus, the rank of T  X  V  X  k  X  V  X  k is less than or equal to where  X  F is the Frobenius norm of a matrix (i.e., the square root of the sum of the squares of its elements). Note that we have  X  before using it to perturb the private data. As such, almost all are within [0 , 1] . Given our computation of R ( t i ) based on have if t i j =0 , Exp (( R ( t i ) j  X  t i j ) 2 ) if t i j =1 , Exp (( R ( t i ) j  X  t i j ) 2 ) where Exp (  X  ) refers to the expected value. Consider the transforma-tion of the value of an element in t i from t i j to R ( t k n in most cases, the number of transformation from 1 to 0 is much larger than that of transformation from 0 to 1 . Recall that T is the matrix of perturbed data tuples. We have That is, the number of elements in T R that are equal to 1 is less As such, for an attribute a j of a data tuple t i in T  X  probability that t i 2 j  X  1 = t i 2 j =0 is greater than 1  X  (  X  2  X  X  X  +  X  2 k  X  ) /mn . With some mathematical manipulation, we have Since k  X   X  k i , the degree of privacy disclosure satisfies
In this section, we first compare the performance of our scheme with that of the randomization approach. After that, we present the simulation results of our scheme on a real data set.

In order to make a fair comparison between the performance of our scheme and that of the randomization approach, we use the ex-actly same training and testing data sets as in [2]. Due to space limit, please refer to [2] for a detailed description of the training data set and the classification functions. The training data set con-sists of 100 , 000 data tuples. The testing data set consists of 5 , 000 data tuples. Each data tuple has nine attributes including seven continuous attributes and two categorical attributes (i.e., elevel, zip-code). Five widely varied classification functions are used to mea-sure the tradeoff between accuracy and privacy in different circum-stances. The randomization approach used is a combination of ByClass distribution reconstruction algorithm with Gaussian ran-domization operator, which performs the best in our experiment compared to other combinations proposed in [2] (i.e., combination of ByClass or Local algorithm with uniform or Gaussian distribu-tion). We use the same classification algorithm, ID3 decision tree algorithm, as in [2].

Since our scheme assumes that the data set contains only cate-gorical data, we first transform the original continuous data to cat-egorical. We split the value of each continuous attribute into four intervals based on its 1st quartile (i.e., 25% percentile), median, and 3 rd quartile (i.e., 75% percentile). As such, each continuous attribute is transformed to a categorical attribute with 4 distinct values. Since the two categorical attributes have 5 and 9 distinct values, respectively, each private data tuple is represented by a 42 -dimensional binary vector ( j s j =4  X  7+5+9 = 42 ) after preprocessing.

To demonstrate the accuracy of classification results intuitively, we compare the percentage of testing data tuples that are correctly classified by the decision trees built upon the perturbed training data set generated by our scheme and the randomization approach. The comparison of the predictive accuracy while fixing the ex-pected degree of privacy disclosure at 25% is shown in Figure 3. Since different data providers may choose different disclosure lev-els in our scheme, we compute the expected degree of privacy dis-closure of our scheme as the average for all data providers. In our scheme, the data miner updates system disclosure level k  X  and per-turbation guidance V  X  k once 100 data tuples are received. As we can see, while both approaches perform perfectly on Function 1, our scheme outperforms the randomization approach on the other four functions.

To demonstrate the transparency of our scheme to the classifi-cation algorithms, we simulate our scheme using na  X   X ve Bayesian classifier on a real data set. We use the congressional voting records database from the UCI machine learning repository [4]. The origi-nal source of data is Congressional Quarterly Almanac, 98th Congress, 2nd session, 1984. The data set was donated by Jeff Schlimmer in 1987. The data set includes 16 key votes for each of the U.S. House of Representatives congressmen. It includes 435 records with 16 attributes (all of which are binary) and a class label describing whether the congressman is a democrat or republican. There are 61 . 38% democrats and 38 . 62% republicans in the data set. The goal of classification is to determine the party affiliation based on the votes. There are 392 missing values in the data set, which we substitute with values chosen uniformly at random from { 0 , 1 } .
Since each data tuple has 16 binary private attributes, each data tuple is represented by a 32 -dimensional binary vector. We first ap-ply na  X   X ve Bayesian classification on the original data set to build a na  X   X ve Bayesian classifier. We then apply our scheme on 9 differ-ent degrees of privacy disclosure and build 9 classifiers on the per-turbed data sets. After that, we apply the same testing data set to all 10 classifiers and compare their predictive accuracy. The predictive accuracy of the classifier built on the original data set is 90 . 34% .In order to demonstrate the role of disclosure level k i in our scheme, we simulate our scheme when all data providers choose the same disclosure level k i = k . The predictive accuracy of classifiers built on perturbed data sets are shown in Figure 4. As we can see from the figure, the na  X   X ve Bayesian classifier built on the perturbed data set can predict the class label with correction rate of 85 . 99% when the degree of privacy disclosure is 9 . 56% . Thus, our classification can effectively preserve privacy while keeping the classifier predic-tively accurate.

To demonstrate that the system disclosure level k  X  decreases rapidly during the collection of data tuples, we perform another simulation while fixing the parameter  X  , which is used to compute k  X  . Recall that generally speaking, the lower  X  is, the more in-formation is retained for building a more accurate classifier. For a given  X  , we investigate the change of k  X  with the number of data tuples received by the data miner (i.e., | T  X  | ). In most cases, k  X  de-creases to be very small fairly soon. For example, when  X  = 15% , k  X  decreases to be 2 after 50 data tuples are received. Figure 5 shows the change of k  X  with | T  X  | when the degree of error is re-quired to be very small (  X  =2 . 5% ). As we can see, even when the error is strictly bounded, k  X  still decreases fairly quickly.
A prototypical system for privacy-preserving data classification
Figure 4: Na  X   X ve Bayesian Classification on Real Data Set has been implemented using our new scheme. The goal of the system is to deliver an online survey solution that preserves the privacy of survey respondents. The survey collector/analyzer and the survey respondents are modeled as the data miner and the data providers, respectively. The system consists of a perturbation guid-ance component on web servers and a data perturbation component on web browsers. Both components are implemented as custom plug-ins that one can easily install to existing systems. The archi-tecture of our system is shown in Figure 6.

As is shown in the figure, there are three separate layers in our system: user interface layer, perturbation layer, and web layer. The top layer, named user interface layer, provides interface to data providers and the data miner. The middle layer, named perturba-tion layer, realizes our privacy-preserving scheme and exploits the termined, the data perturbation component encrypts the private data and caches it on the client machine. When V  X  k is received, the data perturbation component decrypts the cached data, perturbs it, and transmits the perturbed data tuple to the data miner. The bottom layer, named web layer, consists of web servers and web browsers. As an important feature of our system, the details of data perturba-tion on the middle layer are transparent to both data providers and the data miner.
We now compare the runtime efficiency of our scheme with that of the randomization approach. As we have addressed in Section 2, it is shown in [3] that the cost of mining randomized data set is  X  well within an order of magnitude  X  in respect to that of mining the original data set. In particular, the randomization approach proposed in [2] requires the original data distribution to be recon-structed before a decision tree classifier can be built on the ran-domized data set. The distribution reconstruction is a three-step process. We use  X  X yClass X  reconstruction algorithm as an exam-ple because, as stated in [2], it is a tradeoff between accuracy and efficiency.

In the first step, split points are determined to partition the do-main of each attribute into intervals. There is an estimated num-ber of data points in each interval. The second step partitions data values into different intervals. For each attribute, the values of randomized data are sorted to be associated with an interval. In the third step, for each attribute, the original distribution is recon-structed for each class separately. The main purpose of the first two steps is to accelerate the computation of the third step. The time complexity of the algorithm is O ( mn + nv 2 ) where m is the number of training data tuples, n is the number of private attributes in a data tuple, and v is the number of intervals on each attribute. It is assumed in [2] that 10  X  v  X  100 .

Note that the overhead of the randomization approach occurs on the critical time path. Since the distribution reconstruction is not an incremental algorithm, it has to be performed after all data tuples are collected and before the classifier is constructed. Besides, the distribution reconstruction algorithm requires access to the whole training data set, some of which may not be stored in the main memory. This problem may incur even more serious overhead.
In our scheme, the perturbed data tuples are directly used to con-struct the classifier. The only overhead incurred on the data miner is to update the system disclosure level k  X  and perturbation guidance . Note that the overhead is not on the critical time path. In-stead, it occurs during the collection of data. The time complexity of the updating process is O ( n 2 ) . As we mentioned in Section 4 and demonstrated in Section 6, the data miner may only need to update k  X  and V  X  k once several data tuples are received. Since the number of attributes is much less than the number of data tuples (i.e., n m ) in data classification, the overhead of our scheme is significantly less than the overhead of the randomization approach.
Our scheme is scalable to very large training data sets. As we can see, the space complexity of computing k  X  and V  X  k is O ( n 2 ) . That is, the received data tuples need not to remain in the main memory.
Since for most data providers, the disclosure level k  X  is a small number (a heuristic average value of k  X  is an order less than n ), the communication overhead ( O ( nk  X  ) per data provider) incurred by the two-way communication in our scheme is not significant. There may be concern on the upstream traffic from the data providers to the data miner when there are many distinct values for each at-tribute of the data tuple. In this case, the sparse nature of R ( t provides an efficient way to encode R ( t i ) to a list of nonzero ele-ments such that the overhead of transmitting R ( t i ) can be substan-tially reduced.
In this paper, we propose a new scheme on privacy-preserving data classification. Compared with previous approaches, we intro-duce a two-way communication mechanism between the data miner and the data providers with little overhead. In particular, we let the data miner send perturbation guidance to the data providers. Using this intelligence, data providers perturb their data tuples to be trans-mitted to the data miner. As a result, our scheme has the benefit of a better tradeoff between accuracy and privacy.

Our work is preliminary and many extensions can be made. We are currently investigating how to apply our scheme to clustering problem. We would also like to investigate the integration of our scheme with cryptographic techniques.
 We thank the anonymous reviewers for their insightful comments that helped us improve the quality of the paper. [1] D. Agrawal and C. C. Aggarwal. On the design and [2] R. Agrawal and R. Srikant. Privacy-preserving data mining. [3] S. Agrawal, V. Krishnan, and J. R. Haritsa. On addressing [4] C. Blake and C. Merz. UCI repository of machine learning [5] T. M. Cover and J. A. Thomas. Elements of information [6] L. Cranor, J. Reagle, and M. S. Ackerman. Beyond concern: [7] W. Du and Z. Zhan. Building decision tree classifier on [8] W. Du and Z. Zhan. Using randomized response techniques [9] G. H. Golub and C. F. V. Loan. Matrix Computation . John [10] J. Han and M. Kamber. Data Mining Concepts and [11] HIPAA. Health insurance portability and accountability act, [12] M. Kantarcioglu and J. Vaidya. Privacy preserving na  X   X ve [13] H. Kargupta, S. Datta, Q. Wang, and K. Sivakumar. On the [14] Y. Lindell and B. Pinkas. Privacy preserving data mining. In [15] J. R. Quinlan. Induction of decision trees. Machine Learning , [16] J. Vaidya and C. Clifton. Privacy preserving na  X   X ve bayes [17] N. Zhang, S. Wang, and W. Zhao. A new scheme on privacy superscript  X  current version of a matrix or variable
