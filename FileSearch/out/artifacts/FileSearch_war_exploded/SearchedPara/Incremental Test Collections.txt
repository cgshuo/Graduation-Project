 Corpora and topics are readily available for information re-trieval research. Relevance judgments, which are necessary for system evaluation, are expensive; the cost of obtain-ing them prohibits in-house evaluation of retrieval systems on new corpora or new topics. We present an algorithm for cheaply constructing sets of relevance judgments. Our method intelligently selects documents to be judged and de-cides when to stop in such a way that with very little work there can be a high degree of confidence in the result of the evaluation. We demonstrate the algorithm X  X  effectiveness by showing that it produces small sets of relevance judgments that reliably discriminate between two systems. The algo-rithm can be used to incrementally design retrieval systems by simultaneously comparing sets of systems. The number of additional judgments needed after each incremental de-sign change decreases at a rate reciprocal to the number of systems being compared. To demonstrate the effective-ness of our method, we evaluate TREC ad hoc submissions, showing that with 95% fewer relevance judgments we can reach a Kendall X  X  tau rank correlation of at least 0.9. H.3 [ Information Storage and Retrieval ]; H.3.4 [ Systems and Software ]: Performance Evaluation Algorithms, Experimentation, Measurement information retrieval, evaluation, test collections, algorithms
System-based information retrieval evaluation requires a test collection : a set of items to search (a corpus ), a set of topics from which we can extract queries , and a set of rele-vance judgments that tell us which documents are relevant to which topics [12]. The test collections put together by NIST for the Text Retrieval Conferences (TREC) are well-known and widely used in the IR community [13]. Each TREC collection consists of a corpus of mostly news articles, 50 subject-based topics, and tens of thousands of relevance judgments made by NIST.

Researchers may want to put together their own test col-lections. They may have acquired a new corpus or created a new set of topics. Search engine designers in a practical set-ting have a set of documents that will be searched and can solicit topics from the people that will be using the system. Corpora are readily available; the problem is that relevance judgments are very expensive. Researchers must hire an-notators to read documents and decide whether they are relevant to each topic. Judging every document in a large corpus is impossible. Judging only a subset raises questions about how well the subset represents the document space and how well it will generalize to new systems [5, 14]. NIST has addressed this by judging a large subset of documents retrieved by actual retrieval systems, but most developers and researchers do not have the resources to judge the tens of thousands of documents that NIST judges. We need a way to minimize the cost of relevance judgments but still have confidence in the result of an evaluation.

Retrieval system design is typically an iterative process: we make design decisions based on the results of evaluations of previous systems. We can exploit the iterative nature of the design process to construct sets of judgments incremen-tally. At each step we judge only the documents that are most important to the evaluation at that step. Of course there is a tradeoff in that we will have less confidence in the result of a comparison because the judgments are not complete, but after each iteration we add more documents to the collection and thus gain confidence in the result.
In section 2 we describe an algorithm that produces a set of relevance judgments while comparing two systems. In sec-tion 3 we present results of comparisons of pairs of systems submitted to the TREC ad hoc tracks, showing that with as little as two judgments per topic we can correctly iden-tify significant differences in over 93% of cases. In section 4 we describe how our algorithm generalizes to an evaluation of multiple systems. In section 5, the main experimental re-sults are presented. We simulate the iterative design process to show how the number of necessary judgments increases as new systems are developed. We also use our algorithm to rank TREC systems, achieving a Kendall X  X  tau correla-tion greater than 0.9 with 95% fewer judgments than done by NIST. In sections 6 and 7 we analyze the algorithm and compare it to previous work. We conclude in section 8.
Given two systems, the algorithm is to identify documents that inform us most about the difference between them, and to decide when we have enough information to stop judging. We use heuristics that have intuitive appeal; we have not yet formally proved anything about this algorithm.
Intuitively, a document that appears at the same rank in two lists provides no information about the difference between the systems, while a top-ranked document in one system that is not ranked at all by the other may provide a lot of information.

Average precision (AP) of a ranked list is the sum of the precisions at each rank at which a relevant document was retrieved, divided by the total number of relevant documents known for the topic.
 AP is computed for a ranked list of a single topic. Mean average precision (MAP) is the average of the APs for each topic in the test collection. MAP is a standard evaluation measure, and is known to be stable [2].

Unjudged documents are usually assumed to be nonrele-vant for the purpose of computing AP. We could calculate AP using some incomplete set of judgments R , judge an un-judged document, and recalcuate AP with the new set of judgments R 0 . The difference in the two calculations is the effect on AP of judging that document. Applying our in-tuition, we want to find the document that maximizes the difference in two MAP scores. That document is the one that, if relevant, would have the greatest difference in its effects on the APs of the topics for which it was retrieved by both systems.

When we begin, there are no relevance judgments and average precision for each topic is 0. The first relevant doc-ument we find has an effect of 1 /r 1 ( d ) on AP 1 and 1 /r on AP 2 , where r i ( d ) is the rank of document d in ranked list i . Since we want to find the greatest difference in effects, we initially assign each document a weight w d = | 1 r Documents are presented to an assessor for judging in non-increasing order by weight, ignoring topic.

When there are some relevance judgments, the effect a document might have on AP is dependent on other rele-vant documents found for that topic. For example, suppose documents at ranks 1, 2, 4, and 5 in ranked list 1 are rel-evant, nonrelevant, nonrelevant, and relevant, respectively, and the document at rank 3 is unjudged. Then AP 1 The effect of the judgment on the numerator of AP After a similar calculation for ranked list 2, the difference in effects is | (effect on AP 1 ) -(effect on AP 2 ) | .
In general, where R j is the set of relevant documents after j documents have been judged. The effect on AP i of finding d relevant is the precision at the rank of d plus the reciprocal rank of d (to account for d being assumed relevant) plus the recip-rocal rank of every relevant document that follows d in the ranking; the weight of document d is the difference in effects.
After finding a relevant document for topic t , we may, if it seems clear that the systems are quite different, want to stop judging documents from that topic. We calculate the average precision of both systems X  ranked lists for topic t . If one is greater than the other, we want to guess how likely it is that the other can catch up given more judgments. If it is unlikely, we can stop.

We calculate a lower bound ` t on the number of judg-ments it would take to catch up. Assuming a best-case sce-nario in which every unjudged document retrieved by the worse system will be judged relevant, we count from the top-ranked unjudged document down until the hypothetical average precision of the worse system is equal to or greater than the average precision of the better system. We know it will take at least that many documents to catch up. If that number is high, it is unlikely that the lists are equal and we can stop judging documents for the topic; if it is low, we must continue. When we stop judging for a topic, we flag that topic  X  X one X .

The algorithm takes as a parameter a cutoff c . If the lower bound ` t &gt; c , we stop judging documents from that topic. The choice of cutoff reflects a tradeoff between costs of judgments and costs of accuracy errors. As we shall see, a higher cutoff results in greater accuracy but more judg-ments. The cutoff can also be thought of as the point at which you believe the probability of judging c consecutive documents relevant is nil.
If it becomes obvious in the course of judging documents that one system is far better than the other, we may want to stop judging early. We perform a one-tailed sign test using the  X  X one X  topics; if the difference is significant, we stop judging documents altogether.

Our use of the sign test is a heuristic, and the result of the test should not be taken to mean the difference actually is significant, except to the degree that we correctly predict significance. Note that our use of the sign test makes the assumption that topics that are tied or not yet  X  X one X  pro-vide no information. This is a strong assumption. We might make a weaker assumption, e.g. that any topic that is not  X  X one X  is equally likely to go to either system. Table 1: TREC corpora.  X % signif  X  is the percent of pairs with a significant difference by the sign test.
We downloaded submissions to the ad hoc tracks of TRECs 3 through 8. Table 1 shows the number of runs, topic numbers, number of relevance judgments and relevant doc-uments, and percent of pairs significantly different by the sign test for each TREC.
Because there are many pairs of systems (e.g. 40 systems submitted to TREC-3 gives 780 possible pairs), we used some approximation in the implementation to save compu-tation time. We only update the weights after every 50 rele-vant documents. We only judge documents that are ranked between 1 and 100 by either system. We only calculate the lower bound for a topic when we find a relevant document in that topic, but because a nonrelevant judgment may in-crease the lower bound, we also calculate the lower bound for every topic after every 25 judgments. For TRECs 3 and 4 we compared every pair of systems using our algorithm. For TRECs 5 through 8 we picked 25% of the pairs (or 1000, whichever is greater) at random.
The algorithm produces a set of relevance judgments. To evaluate the set, we want to know: 1. How often does it indicate a significant difference when 2. How often does it correctly identify the relative differ-3. How often does it correctly identify the relative dif-4. How often does it identify a difference as significant
The results are shown in Figure 1. Each point in the figure is the average over more than 6500 pairs of systems from all TRECs at a given cutoff. We see that as the number of judgments increases (by increasing the cutoff), precision, recall, and accuracy increase, while false alarms decrease (1-FA increases). With only 59 judgments, an average of just over one per topic, we can correctly identify a significant dif-ference when one exists 82% of the time, and we correctly identify the sign of the difference 93% of the time. Table 2 shows the numbers.  X  X cc X  is the overall accuracy; remem-ber that it includes pairs with a nonsignificant difference. Figure 1: Number of judgments vs. precision and recall as cutoff increases. Points are, from left to right, cutoffs c = 0 , 1 , 2 , 3 , 4 , 5 , 10 , 20 . Table 2: Results by cutoff averaged over pairs of systems from all TRECs.
 The false alarm rate is relatively high; this is because of the strong assumption we make about topics that are not  X  X one X .

For comparison, Table 3 shows results using mean average precision calculated using a pool of depth k , which is the NIST approach. Note that it requires many more relevance judgments to achieve the same rate of differentiation, though the traditional measures are more accurate. Compare k = 5 to our method with cutoff 10: with approximately the same number of judgments, the accuracy on significant pairs is the same, but our method finds many more significant pairs. (Recall and accuracy on a pool of depth 100 fall short of 1.0, which may be surprising considering that NIST uses pools of depth 100. In this case, the pool is only formed from the two systems being compared, not the full set of systems, so some relevant documents are not found.) Table 4 shows results at cutoff c = 4 broken out by TREC. Note that more documents are judged in earlier TRECs than in later. We believe that this is because systems from later TRECs overlap more in the documents they retrieve. As systems get more similar, the weights of documents decrease, and it becomes more clear exactly which documents need to be judged to differentiate between two systems.

The number of judgments produced is influenced by a number of factors, including the number of relevant docu-ments in the topic, the number of relevant documents re-Table 3: Evaluation by AP calculated using a pool of depth k averaged over pairs of systems from all TRECs.
 TREC-3 97.3 90.1 90.5 11.2 170 99 TREC-4 94.9 87.8 85.2 13.3 222 101 TREC-5 97.0 92.0 91.4 14.3 192 85 TREC-6 97.8 91.4 89.7 12.8 179 80 TREC-7 96.7 90.7 90.0 13.0 166 83 TREC-8 96.1 91.7 89.0 13.7 158 81 trieved by both systems, the percent difference in average precision in the two systems, and the similarity (in terms of documents retrieved) between the two systems. The algo-rithm judges documents from every topic, on average. The Pearson correlation between predicted MAP and the  X  X rue X  MAP calculated using the NIST judgments ranges from 0.44 to 0.87 (when c = 0 and c = 20 respectively). The cor-relation between the difference in predicted MAP of both systems and the difference in NIST MAP ranges from 0.75 to 0.96 ( c = 0 and 20 respectively), indicating that this al-gorithm is better at predicting the difference between two MAPs then actually predicting MAP.

These results show that the algorithm can successfully find a difference between two systems when one exists with a small number of judgments. Next we want to know how many additional judgments it would take as more systems are developed.
Information retrieval system design is typically iterative, with design decisions at step n influenced by evaluations of the previous n  X  1 systems. Our algorithm leaves uncertainty in the results of previous evaluations, so we do not want to assume that an evaluation was correct and never look at it again. We want to reevaluate systems as we accumulate more data. As we repeatedly compare any two systems, if the result is the same each time, it becomes less likely that there is an error in the evaluation. So each time we develop a new system, we want to produce a set of relevance judgments that we can use to compare all S existing systems simultaneously.

We view the comparison as O ( S 2 ) simultaneous pairwise comparisons, and generalize as follows: the document weight is the maximum of its weight in each pair. As before, we sort documents by weight. We stop judging a topic when the ranking of lists by average precision is unlikely to change, and stop judging altogether when the ranking of lists by mean average precision is unlikely to change.
The document most likely to tell us something about the ranking of systems is the one that provides the most infor-mation about the differences between each pair of systems. That suggests using the average weight of a document in all pairs. The problem with using the average is that a docu-ment that has low weight in a few pairs may have a relatively high average weight compared to one that has high weight in a few pairs but low weight in most pairs. These documents are poor discriminators, but will be judged first.
Instead of average weight, we use the maximum weight of a document in any pair of systems it appears in: w d = documents that are ranked at the top of a list and that will also discriminate at least one pair of systems. Since many documents will have the same maximum weight, we use the average weight as a tiebreaker. We update weights as in section 2.1.
We want to stop judging a topic when the ordering of ranked lists by average precision is unlikely to change. We assign to each pair of ranked lists ( t i ,t j ) a probability of swapping: P swap ( t i ,t j ) = 0 if ` t &gt; c , or P exp(  X  ` 2 t ) if ` t  X  c , where ` t is the lower bound on judgments to catch up as described in section 2.2. If additional judg-ments would not change the ranking significantly, we can accept some uncertainty because it will get averaged out in the mean average precision calculation. We calculate the ex-pected rank correlation between our current ranking and a future ranking given the probabilities of each pair swapping. If the expected rank correlation is high, we do not expect the ranking to change much, so we stop judging the topic. We set the correlation cutoff at 0.9 to reflect Voorhees X  con-jecture that that is the highest correlation that we should expect to achieve simply because of inter-annotator disagree-ment [11].
We likewise stop judging documents when the system rank-ing is unlikely to change. Each pair of systems ( s assigned a probability of swapping: P swap ( s i ,s j ) = 0 if the systems have been found to be significantly different (us-ing the sign test as in section 2.3); if not, we calculate the probability that one system can catch up to the other, with P swap ( s i ,s j ) = 0 . 5 if the number of topics favoring s the number favoring s j , and P swap ( s i ,s j ) = P ( T  X  25  X  T where T  X  Binom(50  X  ( T i  X  T j ) , 1 / 2), i.e. the probability that s j will accumulate enough topics in its favor (among the ones not yet decided) to have a majority. With those probabilities we calculate E [  X  ] and stop if it is greater than 0.9.

Voorhees previously used a definition of P swap that was calculated using mean average precision differences on mul-tiple sets of relevance judgments [10]. Since our judgments are incomplete, and we only have one set, we cannot use the same definition.
The first experiment is to verify that the algorithm can produce a set of judgments that correctly rank TREC sys-tems. We take the full sets of systems submitted to TRECs 3, 4, 5, and 6 and run the algorithm with cutoffs ranging from 0 to 20.

For the second experiment, we wish to simulate the itera-tive design process. We do not know what a typical design process looks like, and it is not obvious how to simulate the process. Options we considered are: 1. Simulate different systems by running our in-house re-2. Use TREC systems from a single site. There are only a 3. Use all TREC systems, starting with two chosen ran-The experiment we decided on is selecting S systems at ran-dom, starting at S = 2 and working up. Each evaluation of S systems is independent of any other evaluation. We expect that on average this simulates the iterative design process.
An additional advantage of selecting random subsets of systems is that we do not make the mistake of designing our algorithm so that it performs well only on full sets of TREC systems, which are comparatively easy to rank correctly.
For each TREC we ran the algorithm at least 100 times with S randomly selected systems, for more than 600 total algorithm runs for each S up to 60. We used cutoff c = 4 to keep the computation time low.
Finding the maximum weight document is computation-ally expensive: O ( S 2 D 2 ln D ), where D is the number of unique documents ranked by all S systems and assuming sorting documents by weight is in O ( D ln D ). To reduce computation time, we recalculate the weights after every 25 relevant documents are found. We only judge documents that appear above rank 50 in at least one system (meaning that we can never do better than a set of judgments formed from a pool of depth 50). If there are more than 40 systems, we find the maximum weight in a random sample of pairs.
A comparison among a set of systems produces a set of relevance judgments. We rank systems by mean average precision calculated using that set. To estimate the quality of the ranking, we calculate its correlation to the ranking of systems by MAP calculated using the full set of judgments. We use the rank correlation measure Kendall X  X  tau [6], which has become a de facto standard in this type of study.
Kendall X  X  tau is a function of the number of concordant and discordant pairs between rankings:  X  = C  X  D ( n from -1 if the two lists are inverted to 1 if they are identical. It is 0 if there is no correlation. If we know the  X  correlation, we also know that the percentage of pairs that were correctly differentiated is . 5(1 +  X  ). Figure 2: Number of judgments vs. tau correlation for complete sets of TREC systems. Points from left to right are cutoffs c = 0 , 1 , 2 , 3 , 4 , 5 , 7 , 10 , 15 , 20 .
Figure 2 shows the result of using our algorithm to pro-duce judgments for evaluation of TREC systems. High cor-relations are achieved with very small sets of judgments. A set of 2000 judgments is less than one judgment per topic per system, but that is enough to achieve  X   X  . 85 on the TREC-6 collection.

TREC-3 did not quite achieve  X   X  0 . 9 in any of our exper-iments, but with 95% fewer judgments than done by NIST, we have  X  = 0 . 87. For TREC-4, we achieve  X  &gt; 0 . 9 with 95% fewer judgments than done by NIST. TREC-5 is a bit harder, but we achieve  X  &gt; 0 . 9 with 94% fewer judgments. For TREC-6, with 95% fewer judgments we achieve  X  = 0 . 9. In general, tau correlation and judgments increase as cutoff increases.

Table 5 shows the numbers for each set of TREC sys-tems. Compare the TREC-6 numbers to Table 6, which shows numbers for different pool depths. Pooling is surpris-ingly effective, with a correlation of .82 with a depth of only 1! Although shallow pools work well with the TREC-6 cor-pora, it does not necessarily follow that a shallow pool would be sufficient for any corpora. For the TREC-3 systems, for instance, a pool of depth 1 results in a correlation of only .71 with 973 judgments.
Figure 3 shows the size of the test collection produced by the algorithm increasing as systems are incrementally de-signed and added to the set. Accuracy was about  X  = 0 . 77, which implies that 88.4% of the pair differences were cor-rectly identified X  X onsistent with the 89.5% shown in Ta-ble 2. With two systems, we make about 200 judgments, also consistent with Table 2. When we add a third system, we nearly triple the number of judgments X  X ith more than two systems, it is less likely that a single document will be retrieved at the same rank by all systems. Subsequent sys-tems add fewer and fewer judgments to the set. Figure 2, in which number of judgments increases with cutoff, suggests Figure 3: Number of systems simultaneously com-pared vs. number of judgments made when c = 4 .
 Rank correlation remains approximately constant at  X  = 0 . 77 . that higher values of c would result in approximately par-allel curves higher up. The fit curve is y = a log S , which implies that the rate of increase in the number of judgments is inversely proportional to the number of systems being compared.

Figure 3 also shows the size of the test collections pro-duced by simultaneous comparisons of the 40, 33, 61, and 74 systems from TRECs 3, 4, 5, and 6 respectively at c = 4. They are close to the fit log function. They have higher cor-relations than the random samples, probably because there is more overlap between systems.

The correlation between number of judgments made for each topic and number of true relevant documents in each topic is 0.47, indicating that more documents are judged from topics that have more relevant documents. A correla-tion of 0.76 between NIST MAP and number of documents judged from each system indicates that more documents are judged from systems that are better. The correlation be-tween predicted MAP and NIST MAP is 0.81. We expect these correlations to increase at higher cutoffs.
The results are somewhat unbelievable. With an average of just over one judgment per topic (at cutoff 0) we can dis-tinguish between 82% of the systems that have a significant difference, and we get 93% of the differences we find correct. Increasing to only eight judgments per topic (at cutoff 10) we distinguish between 93% of the systems with a significant difference, and we get 98% of comparisons correct. How is that possible?
In general, one system will be better than another if it tends to retrieve more relevant documents at higher ranks. Mean average precision is higher in that case, as are most evaluation measures. Consider cutoff c = 0 and a simpli-fied case in which the two systems do not overlap at all in retrieved documents. When we sort documents by weight, the top-ranked documents by both systems in all topics will be tied with weight w d = 1. When we begin to judge, if the better system ranked more relevant documents first, there is a higher probability that when we find a relevant document it is from the better system. If one system is better on a majority of topics, there is a high probability that we will find more relevant documents from that system, and if the majority is enough to be significant, it is likely that we will discover that quickly.

Of course we can imagine circumstances under which the algorithm would fail, but if it is correct on x out of T topics on average and on the other T  X  x it is essentially random, the expected number of topics correctly sorted is x + T x = 20 and T = 50, the algorithm will be right on a majority of topics.

In light of the results for single pairs, the results for mul-tiple comparisons make sense. If the accuracy at c = 0 is 84%, the expected  X  is .688, which is a bit higher than we observe in TREC collections due to overlap in retrieved sets. Accuracy at c = 10 is 92.5%, implying a  X  of .85, a bit lower than we observe, again due to overlap. Overlap affects pairwise comparisons as follows: at low cutoff, more over-lap means more error, because judging a single document from one pair will  X  X ecide X  any pair that document occurs in. At high cutoff, more overlap means less error, because more documents are judged from each pair before the pair is completed.

Iteratively comparing the systems gives more confidence in the results of each individual pair. It results in the same pairs being compared over and over again, so if the result of the comparison is the same each time, it becomes less and less likely that it is a mistake. Additionally, the size of the test collection keeps increasing, and more judgments always means more confidence.
The traditional means of obtaining a set of relevance judg-ments is by system pooling, as we mentioned in section 1. At TREC, NIST pools the top N documents retrieved for each topic by each system and judges the entire pool. Usu-ally N = 100; sometimes N = 50 or 200. Table 1 shows the number of relevance judgments this entails. If an assessor can make one judgment every 30 seconds, the 72,270 judg-ments collected for TREC-6 would take 25 days of around-the-clock work to produce X  X nd that is for only 0.26 percent of the 27.8 million judgments that could be obtained for 50 topics on the entire 556,000-document collection. Even this small set is infeasible for most researchers.

Although the pooling method results in a small subset of the total number of possible judgments, it is sufficient for research purposes [14, 10]. This suggests that it might not be necessary to pool 100 documents. Pools of depth 20, 10, or even 5 result in good correlation to the NIST ranking of TREC systems by MAP with a pool of depth 100 (Table 6).
Another option is to construct topics such that only a subset of the collection could be relevant. An example is restricting topics to events that happened on a certain date. Another example is known item retrieval, in which topics are defined to have only one relevant document [1]. These sorts of topics do not provide enough variance to allow us to make general statements about differences between systems [9].
Soboroff et al. investigated a way to construct test col-lections with no judgments at all [8]. Using a model of how relevant documents occur in pools, they randomly selected documents from a pool and assigned relevance to all the cho-sen documents. Ranking systems by MAP calculated using decline from full NIST judgment count. Table 6: Total number of judgments in a pool of depth k and tau correlation to the NIST ranking of TREC-6 systems. these so-called  X  X seudo-rels X , they achieved a rank correla-tion of between .4 and .6 with the NIST ranking.

Sanderson and Joho judged documents from a single sys-tem (or a subset of systems) and, when the system was a good one, achieved high rank correlations [7]. They also used successive relevance feedback runs to incrementally add documents to the pool, and achieved a high rank correlation with that method as well. These methods emulate  X  X nterac-tive Searching and Judging X , described in more detail below. They require many more judgments than our algorithm.
Results of experiments with shallow pools, no pools, or single-system pools suggest to us that TREC systems are getting more similar in terms of the sets of documents they retrieve. Further evidence is that the average number of doc-uments a system contributes to the pool has decreased. We could imagine that retrieval systems that use very different algorithms to retrieve from a very large corpus might over-lap very little in the documents they retrieve, and in that case we would not expect any of these methods (excluding a pool of reasonable depth) to provide a stable evaluation.
A different way to construct a test collection called  X  X n-teractive Searching and Judging X  (ISJ) was presented by Cormack et al. [4]. In ISJ, users submit queries to a re-trieval system and judge retrieved documents, formulating and submitting new queries as they learn about the topic and corpus. They found that with a few hours of work, an-notators could produce as many relevant documents as exist in the official set of judgments. Interestingly, there was only 33% overlap between the ISJ set and the official set, but the evaluation results were nearly the same [10]
Although Cormack et al. convincingly show that ISJ works with the TREC-6 corpus, whether it would work with a larger corpus is an open question. ISJ does not necessar-ily judge documents from any submitted run, which means it is conceivable, though perhaps not likely, that the set of judgments it produces does not intersect with any system.
Sanderson and Joho conclude that their simulation of ISJ confirms that it works, but our belief is that their results are due to TREC systems getting more similar. In TREC-7 and TREC-8, more than half the systems retrieved 60% or more of the relevant documents. This suggests not that sys-tems are getting better at finding relevant documents, but that they are finding the same relevant documents, and as a result, the judged pool is a smaller sample of the full cor-pus. Judging documents from one or several decent systems will, therefore, result in an evaluation with high rank corre-lation simply because it is very likely that the other systems retrieved the same documents, and if they did not, they probably did not retrieve very many relevant documents.  X  X ove-to-Front pooling X  (MTF) was proposed by Cor-mack et al. [4]. It imposes a priority ordering on the pool based on the assumptions that higher-ranked documents are more likely to be relevant and documents from systems that have recently discovered a relevant document are more likely to be relevant. MTF discovers relevant documents faster than traditional pooling, and achieves high positive rank correlations with much less work. Zobel proposed a similar method [14] that first judges a shallow pool, then, based on the results of the evaluation, extrapolates which systems and topics are likely to provide more relevant documents, and extends the pool using more documents from those. Zo-bel also discovers relevant documents faster than traditional pooling.

It is worth considering the performance of MTF and our algorithm on two extreme cases. 1. Consider two systems that retrieve two mutually ex-2. Consider two systems that are identical. In our algo-Figure 4: A comparison of four algorithms for pro-ducing test collections from TREC-3 systems.

Figure 4 shows a comparison of our algorithm, MTF pool-ing, TREC pooling, and Soboroff et al. X  X  pseudo-rels. The curve representing our algorithm is the same as the one for TREC-3 in Figure 2. For the other methods, we created a pool of documents of the same size as the one our al-gorithm produced, so that we could directly compare the performances on test collections of the same size. We av-eraged over trials of each method with random topic order-ings. Our method outperforms all the others, though MTF catches up at the final point in the curve. Our algorithm has the advantage over the other methods that, through the cutoff parameter, it attempts to select documents that will maximize the utility of the evaluation while minimizing the cost.
We have presented an algorithm that researchers can use in-house to build test collections incrementally. The algo-rithm selects documents that are likely to contribute a lot of information about the difference in mean average precision, and stops when it is likely that the difference is meaningful. Our algorithm performs as well as any pooling method and is more likely to generalize to non-TREC collections.
The algorithm uses heuristics that  X  X eel X  right. There may be a better weighting scheme, or better stopping conditions. If nothing else, it would be wise to incorporate the inter-topic assumption made by MTF. In the future, we intend to analyze the algorithm more formally to justify or derive its heuristics. We hope that we can quantify the amount of un-certainty in an evaluation with a set of relevance judgments. We X  X  also like to evaluate our sets of relevance judgments using the bpref measure [3] instead of mean average preci-sion, and investigate selecting documents by their effect on bpref.

The algorithm will make it possible for researchers to do in-house evaluations on new corpora and new topics. It is not, however, meant to be a replacement for NIST X  X  pro-cesses, where an overriding goal is the creation of a reusable test collection that can be adopted with some confidence by a non-participating system. Our approach may result in a broadly useful set of judgments, but it is intented to differ-entiate between a small set of systems and cannot guarantee more general value.
This work was supported in part by the Center for Intelli-gent Information Retrieval and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903. Any opinions, findings and conclusions or recommendations expressed in this ma-terial are the authors X  and do not necessarily reflect those of the sponsor. [1] S. M. Beitzel, E. C. Jensen, A. Chowdhury, [2] C. Buckley and E. M. Voorhees. Evaluating [3] C. Buckley and E. M. Voorhees. Retrieval evaluation [4] G. V. Cormack, C. R. Palmer, and C. L. Clarke. [5] S. P. Harter. Variations in relevance assessments and [6] M. Kendall. Rank Correlation Methods . Griffin, [7] M. Sanderson and H. Joho. Forming test collections [8] I. Soboroff, C. Nicholas, and P. Cahan. Ranking [9] K. Sparck Jones and C. J. van Rijsbergen.
 [10] E. Voorhees. Variations in Relevance Judgments and [11] E. M. Voorhees. Evaluation by highly relevant [12] E. M. Voorhees. The philosophy of information [13] E. M. Voorhees and D. Harman. Overview of the [14] J. Zobel. How Reliable are the Results of Large-Scale
