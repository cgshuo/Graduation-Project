 Search is at the heart of modern e-commerce. As a result, the task of ranking search results automatically (learning to rank) is a multi-billion dollar machine learning problem. Traditional models opti-mize over a few hand-constructed features based on the item X  X  text. In this paper, we introduce a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural network. In a large scale experiment using data from the online marketplace Etsy we verify that moving to a multimodal representation significantly improves ranking quality. We show how image features can cap-ture fine-grained style information not available in a text-only rep-resentation. In addition, we show concrete examples of how image information can successfully disentangle pairs of highly different items that are ranked similarly by a text-only model.
 Learning to rank; Computer vision; Deep learning;
Etsy 1 is a global marketplace where people buy and sell unique goods: handmade items, vintage goods, and craft supplies. Users come to Etsy to search for and buy listings other users offer for sale. A listing on Etsy consists of an image of the item for sale along with some text describing it. With 35 million listings for sale correctly ranking search results for a user X  X  query is Etsy X  X  most important problem. Currently Etsy treats the ranking problem as an example of supervised learning: learn a query-listing relevance function from data with listing feature representations derived from listings X  titles and tags. However, with over 90 million listing im-ages, we wonder: is there useful untapped information hiding in Etsy X  X  images that isn X  X  well captured by the descriptive text? If so, how can we integrate this new data modality into Etsy X  X  exist-ing ranking models in a principled way? In this paper we attempt www.etsy.com
The statistics reported in this paper are accurate at the time of submission of this paper.
 Figure 1: Irrelevant search results for the query  X  X edding dress": Even though it X  X  apparent in the images that these are not wedding dresses, each listing X  X  descriptive title contains the phrase  X  X edding dress", allowing it to show in search results for the query. to explore these two major questions on a real world dataset con-taining over 1 . 4 million Etsy listings with images. Specifically, we describe a multimodal learning to rank method which integrates both visual and text information. We show experimentally that this new multimodal model significantly improves the quality of search results.

We motivate this paper with a real example of a problem in search ranking. Figure 1 shows listings that appear in the results for the query  X  X edding dress". Even though it is clear from looking at the images that they aren X  X  wedding dresses, each listing X  X  title con-tains the term  X  X edding dress", causing it to appear in the results. This kind of term noise in listing descriptions is pervasive. Sell-ers write their own descriptions and are motivated to include high traffic query terms to boost the visibility of their listings in search. It is obvious that there is complementary information in the listing images that is either unavailable in the text or is even in contrast to the text. Our hypothesis is that if we can mine this complementary high-level visual information, we can incorporate it into our models to improve search ranking as suggested by multimodal embeddings literature [5, 7, 8, 13, 14, 15, 17, 20, 26, 28].

With the goal of learning high-level content directly from the image, deep convolutional neural networks (CNN) [4, 22] are an obvious model choice. CNNs are a powerful class of models in-spired by the human visual cortex that rivals human-level perfor-mance on difficult perceptual inference tasks such as object recog-nition [16]. [29] shows that image feature representations learned on large scale object recognition tasks are powerful and highly in-terpretable. The lower layers of the network learn low-level fea-tures like color blobs, lines, corners; middle layers combine lower layers into textures; higher layers combine middle layers into higher-level image content like objects. High-level visual information is made increasingly explicit along the model X  X  processing hierar-chy [6]. This high-level description formed in the deep layers of the network is what we are interested in mining as a rival source of information to the listing X  X  text description.

One consideration to make is that large modern CNNs require large amounts of training data [16, 24]. Even though Etsy X  X  pro-duction search system generates millions of training examples per day in aggregate, the amount of examples available to an individual query model can be in the low hundreds, particularly for queries in the long tail. This makes training one deep CNN per query from scratch prone to overfitting. Transfer learning is a popular method for dealing with this problem, with many examples in computer vi-sion [1, 19, 25]. We take the pre-trained 19-layer VGG net [23, 4] as a fixed extractor of general high-level image features. We chose this model due to it X  X  impressive performance on a difficult 1000-way object classification task. Our assumption is that the ac-tivations of it X  X  neurons immediately prior to the classification task contain general high-level information that may be useful to our ranking task. Figure 2 gives a high-level overview of our multi-modal feature extraction process.

Learning to rank search results has received considerable atten-tion over the past decade [2, 3, 9, 12], and it is at the core of modern information retrieval. A typical setting of learning to rank for search is to: (i) embed documents in some feature space, (ii) learn a ranking function for each query that operates in this fea-ture space over documents. Early approaches optimized over a few Algorithm 1 Multimodal Embedding of Listings 1: procedure E MBED M ULTIMODAL ( d i ) 2: d T i  X  BoW ( text ) 3: d I i  X  VGG ( image ) 5: return d MM i hand-constructed features, e.g. item title, URL, PageRank [12]. More recent approaches optimize over much larger sets of orthogo-nal features based on query and item text [2]. Our research follows this orthogonal approach and explores the value of a large set of image features. Our baseline listing representation consists of fea-tures for a listing X  X  terms, a listing X  X  id, and a listing X  X  shop id. The presence of the latter two features captures historical popularity in-formation at the listing and shop level respectively for the query.
To our knowledge, this is the first investigation of the value of transferring deep visual semantic features to the problem of learn-ing to rank for search. The results of a large-scale learning to rank experiment on Etsy data confirms that moving from a text-only rep-resentation to a multimodal representation significantly improves search ranking. We visualize how the multimodal representation provides complementary style information to the ranking models. We also show concrete examples of how pairs of highly different listings ranked similarly by a text model get disentangled with the addition of image information. We feel this, along with significant quantitative improvements in offline ranking metrics demonstrates the value of the image modality.

This paper is organized as follows: In Section 2 we describe our multimodal ranking framework. Section 2.1 gives a detailed ex-planation of how we obtain multimodal embeddings for each list-ing. Section 2.2 gives a brief introduction to learning to rank and describes how these embeddings are incorporated into a pairwise learning to rank model. Section 3 describes a large scale experi-ment where we compare multimodal models to text models. Finally in Section 4, we discuss how moving to a multimodal representa-tion affects ranking with qualitative examples and visualizations.
Here we describe how we extend our existing learning to rank models with image information. We first explain how we embed listings for the learning to rank task in both single modality and multimodal settings. Then we explain how listings embedded in both modalities are incorporated into a learning to rank framework.
Each Etsy listing contains text information such as a descriptive title and tags, as well as an image of the item for sale. To measure the value of including image information, we embed listings in a multimodal space (consisting of high-level text and image informa-tion), then compare the multimodal representation to the baseline single modality model. Let d i denote each listing document. We then use d T i  X  R | T | and d I i  X  R | I | to denote the text and image representation of d i respectively. | T | denotes the dimensionality of the traditional feature space which is a sparse representation of term occurrences in each d i . | I | denotes the dimensionality of the image feature space which in contrast to text is a dense feature space. The goal of multimodal embedding is then to represent a listing through text and image modalities in a single vector, i.e. the d MM where | M | is the dimensionality of the final embedding.
Given a listing document d i consisting of the title and tag words, a numerical listing id, a numerical shop id, and a listing image, Figure 3: Transformation to Pairwise Classification [9]: a) Shows a synthetic example of the ranking problem. There are two groups of documents (associated with different queries) em-bedded in some feature space. Documents within each group have different relevance grades: relevance ( d 1 ) &gt; relevance ( d relevance ( d 3 ) . The weight vector w corresponds to a linear rank-ing function f ( d ) =  X  w, d  X  which can score and rank documents. Ranking documents with this function is equivalent to projecting the documents onto the vector and sorting documents according to the projections. A good ranking function is one where documents in d 1 are ranked higher than documents in d 3 , and so on. Doc-uments belonging to different query groups are incomparable. b) shows how the ranking problem in (a) can be transformed into a pairwise classification task: separate well-ordered and non-well or-dered pairs. In this task, well-ordered pairs are represented as the vector difference between more relevant and less relevant document vectors, e.g., d 1  X  d 2 , d 1  X  d 3 , and d 2  X  d 3 . Non-well ordered-pairs are represented as the vector difference between less relevant and more relevant document vectors, e.g., d 3  X  d 1 , d 2 d 3  X  d 2 . We label well-ordered instances +1 , non-well-ordered  X  1 , and train a linear SVM, f q ( d ) , which separates the new feature vectors. The weight vector w of the SVM classifier corresponds to the ranking function w in (a). Algorithm 2 Generate Pairwise Classification Instances 1: procedure G ET P AIRWISE I NSTANCES ( { d + i , d  X  i } ) 2: L  X  X } 3: for i = 1 ... | P | do . | P | labeled tuples 6: Draw r uniformly at random from [0 , 1) 7: if r &gt; 0 . 5 then 9: y i  X  +1 10: else 11: x i  X  d  X  MM 12: y i  X  X  X  1 13: L = L.append (  X  x i ,y i  X  ) 14: return L . The list of classification instances. we obtain a multimodal feature vector d MM i by embedding tradi-tional terms in a text vector space, embedding the image in an im-age vector space, then concatenating the two vectors. Algorithm 1 describes this multimodal embedding. The text-based features are based on the set of title and tag unigram and bigrams, the listing id, and the shop id for each d i . These baseline features are then represented in a bag-of-words (BoW) [27] space as d T i  X  | T | = | D | + | L | + | S | , where | D | is the size of the dictionary, | L | is the cardinality of the set of listings, and | S | is the cardinality of the set of shops. Each element in d T i is 1 if d i contains the term, and is 0 otherwise.
 Each d i is also represented in the image space as d I i  X  obtain d I i , we adopt a transfer learning approach. Oquab et al. [18] shows that the internal layers of a convolutional neural network pre-trained on ImageNet can act as a generic extractor of a mid-level image representation and then re-used on other tasks. In our model, we utilize the VGG-19 network [23] and remove the last fully-connected layer. For every listing d i , we scale it X  X  image uni-formly so that its shortest spatial dimension is 256 pixels, then take the center 224  X  224 crop. We then pass the cropped image through the modified VGG network to get a 4096 dimensional feature vec-tor 3 . This vector contains the activations fed to the original object classifier. We ` 2 normalize the activations, then use the normalized vector as the final 4096 dimensional image representation d multimodal representation of the document d i can now be obtained simply as d MM i  X  R | M | = [ d T i , d I i ] where | M | = | T | + | I | is the dimensionality of the final multimodal representation. Figure 2 illustrates this process in details. E-commerce search is a task that can be described as follows: (i) Query: a user comes to the site and enters a query, q , e.g.  X  X esk". (ii) Retrieval: the search engine finds all listing documents that con-tain terms from the query in their title or tag descriptions, such as  X  X id century desk",  X  X ed desk lamp", etc. (iii) Ranking: the search engine uses a ranking function to score each listing doc-ument, where a higher score expresses that a listing document is more relevant to the query. The search engine sorts listings by that score, and presents the results to the user. The goal of learning to rank [3] is to automatically learn (iii), the ranking function, from historical search log data.
We make our scalable VGG-19 feature extractor available at: https://github.com/coreylynch/vgg-19-feature-extractor
In this paper, we restrict our attention to the pairwise preference approach to learning a ranking function [10]. That is, given a set of labeled tuples P , where each tuple contains a query q , a relevant document d + and an irrelevant document d  X  , we want to learn a is binary and determined by the user: a user is said to judge a search result relevant if she purchases the listing, adds it to her cart, or clicks on the listing and dwells on it for longer than 30 seconds.
As shown by Herbrich et al. [10], the ranking problem can be transformed into a two-class classification: learn a linear classifier that separates well-ordered pairs from non-well-ordered pairs. This is illustrated in Figure 3. To achieve this, we can transform any im-plicit relevance judgement pair ( d + , d  X  ) into either a well-ordered or non-well ordered instance. Specifically, suppose for each pref-erence pair ( q, d + , d  X  ) we flip a coin. If heads the preference pair ( q, d + , d  X  ) 7 X  ( d +  X  d  X  , +1) (a well-ordered pair), else ( q, d + , d  X  ) 7 X  ( d  X   X  d + ,  X  1) (a poorly ordered pair).
This results in an evenly balanced pairwise classification dataset for each query q . Algorithm 2 explains the process of generating classification instances for input pairwise preference tuples. The new classification task can now be solved by minimizing the regu-larized hinge classification loss: via stochastic gradient descent. A well trained pairwise classi-fier minimizes the number of pairs which are ranked out of order, i.e. the ranking loss. To rank a new set of listing documents for a query, we embed them in the feature space, then use output of the trained classifier to obtain ranking scores for each. This method, also known as RankingSVM, is used extensively in the ranking lit-erature [10, 3, 9].

Following [21], we can obtain large quantities of implicit pair-wise preference instances cheaply by mining Etsy X  X  search logs
The process for doing so is as follows: A user comes to the site, enters a query, and is presented with a page of results. If she in-teracts with listing d i and ignores the adjacent listing d sonable assumption is that she prefers d i over d j in the context of query q . We call this an implicit relevance judgement, mapping ( q, d i , d j ) 7 X  ( q, d + , d  X  ) , forming the necessary input for Al-gorithm 2. Figure 4 illustrates how we move from search logs to multimodal pairwise classification instances.
This section describes a large scale experiment to determine how a multimodal listing representation impacts ranking quality. In Sec-tion 3.1, we describe our ranking quality evaluation metric. In Sec-tion 3.2 we describe our dataset. Finally we present our findings in Section 3.3. We obtain labeled pairs for each query using the FairPairs method. A well known and significant problem in collecting training data from search is presentation bias [21]: users click on higher pre-sented results irrespective of query relevance. Ignoring this bias and training on naively collected data can lead to models that just learn the existing global ranking function. The FairPairs method modifies search results in a non-invasive manner that allows us to collect pairs of (preferred listing, ignored listing) that are un-affected by this presentation bias.
To evaluate the quality of one modality ranking model over an-other, we measure the model X  X  average Normalized Discounted Cu-mulative Gain (NDCG) [11] on holdout search sessions. NDCG is the standard measure of a model X  X  ranking quality in information retrieval. It lies in the [0 , 1] range, where a higher NDCG denotes a better holdout ranking quality. Here we give a brief background on NDCG and why it is a good choice for quantifying ranking quality.
The cumulative gain (CG) of a ranking model X  X  ordering is the sum of relevance scores over the ranked listings. The CG at a par-ticular rank position p is defined as: where rel i is the implicit relevance of the result at position i .
CG on its own isn X  X  a particularly good measure of the quality of a model X  X  ordering: moving a relevant document above an irrele-vant document does not change the summed value.

This motivates discounted cumulative gain (DCG) [3] as a rank-ing measure. DCG is the sum of each listing X  X  relevance score dis-counted by the position it was shown . DCG is therefore higher when more relevant listings are ranked higher in results, and lower when they are ranked lower. The DCG is defined as:
Finally, we arrive at Normalized Discounted Cumulative Gain (NDCG) by dividing a model X  X  DCG by the ideal DCG (IDCG) for the session:
This gives us a number between 0 and 1 for the model X  X  ranking of a holdout search result set, where a higher NDCG approaches the ideal DCG, denoting a better ranking quality.

Our holdout data (both validation and test) are in the form of a list of labeled sessions for each query q . A labeled session is a page of search results presented to a user for the query q , where the user Figure 5: Image information can help disentangle different list-ings considered similar by a text model: Here are three pairs of highly different listings that were ranked similarly by a text model, and far apart by a multimodal model. Title terms that both list-ings have in common are bolded, showing how text-only models can be confused. In every case, the multimodal model ranks the right listing higher. All three queries benefit from substantial gains in ranking quality (NDCG) by moving to a multimodal representa-tion. In a) for example, we see two listings returned for the query  X  X olf X : the left listing X  X  image shows a picture of a rhino, while the right listing X  X  image shows a t-shirt with a wolf on it. A text-only model ranked these listings only 2 positions apart, likely due to the overlapping words in their titles. A multimodal model, on the other hand, ranked the right listing 394 positions higher than the left. The query  X  X olf X  saw a 3 . 07% increase in NDCG by moving to a multimodal representation. has provided implicit relevance judgements. 5 We evaluate a trained query model f q by computing its average NDCG over all labeled holdout sessions for the query q . To achieve this, we first com-pute each session X  X  NDCG as follows: i) embed all the session X  X  listing documents in the model space, ii) score and rank embed-ded listings with the model, and iii) compute the labeled session NDCG. Then we average over session NDCG to get average query NDCG. Finally, to compare the ranking ability of models based in one modality vs. another, we report the average modality NDCG across query NDCGs.
For our experiment, we select a random 2 week period in our search logs to obtain training, validation, and test data. We mine query preference pairs from the first week as training data . We mine labeled holdout sessions from the following week, splitting it evenly into validation and test sessions. This results in 8 . 82 million training preference pairs, 1 . 9 million validation sessions, and 1 . 9 million test sessions. Across training and test there are roughly 1 . 4 million unique listing images.

We tune model learning rates,  X  1 and  X  2 regularization strengths on our validation data. We also allow queries to select the best modality based on validation NDCG from the set of multiple modal-ities : text-only , image-only , and multimodal prior to test. Of the 1394 total queries we built models for, 51 . 4% saw an increase in validation NDCG by utilizing the multimodal representation over the baseline representation. We present results in Section 3.3.
Here we present and discuss the results of incorporating image information into our ranking models. Table 1 summarizes the re-sults of our experiment for queries that moved from a text-only representation to a multimodal one. For each modality, we present the average lift in NDCG over our baseline text-only models. We find that consistent with our expectations, switching from a text-only ranking modality to one that uses image information as well (MM) yields a statistically significant 1 . 7 % gain in average rank-ing quality . We find it interesting that a purely image-based rep-resentation, while strictly worse than both other modalities, only underperforms the baseline representation by 2 . 2% .
 Table 1: Lift in Average NDCG, relative to baseline ( % ), on sam-ple dataset is compared across various modalities.  X  indicates the change over baseline method is statistically significant according to Wilcoxon signed rank test at the significance level of 0 . 0001 .
We can explore how incorporating image information changes the ranking by looking at a continuum visualization of how the two modality models rank listings for a query: The top row shows the top 90th percentile of ranked listings, the bottom row shows the 50th percentile of ranked listings, and every row in between is a continuum. Figure 6 illustrates a continuum visualization for the query bar  X  X ar necklace X . We observe that by moving from a text-only to a multimodal representation, the top ranked listings in each
We collect implicit relevance judgments in the test period the same as in training: documents are labeled 0 . 0 if ignored and 1 . 0 if the user purchased the listing, added the listing to their cart, or clicked on the listing and dwelled for more than 30 seconds. not available in the text. band show greater visual consistency. This suggests that there is relevant style information captured in the image representation that is not available, or not well described in the text representation. In-corporating this complementary side information leads to a 6 . 62 % increase in offline NDCG.

We can also see concrete examples of the image representation providing valuable complementary information to the ranking func-tion. Figure 5 shows three examples of listings that were ranked similarly under a text model and very differently under a multi-modal model. For example, Figure 5 (a) shows two listings that match the query  X  X olf". It is apparent by looking at the two listing images that only the right listing is relevant to the query: the left shows a picture of a rhino; the right is a t-shirt with a wolf on it. The text-only model ranked these two listings only two positions apart. In contrast, the multimodal model ranked the relevant right-hand listing 394 positions higher. We can see by looking at the bolded terms in common how the text model could be confused: as points embedded in text space, these listings are essentially on top of each other. It is clear that the images contain enough infor-mation to differentiate them: one contains a wolf, one contains a rhino. By embedding listings in a space that preserves this differ-ence, the multimodal model can effectively disentangle these two listings and provide a more accurate ranking. The query  X  X olf" saw an overall 3 . 07 % increase in NDCG by moving to a multi-modal representation. Similarly for Figure 5 (b), a text model for the query  X  X eather bag" ranked the two listings 1 position apart, while the multimodal model ranked the right listing 660 positions higher.  X  X eather bag" saw a 2 . 56 % increase in NDCG by moving to a multimodal representation. For Figure 5 (c) a text model for the query  X  X edding band" ranked the left and right listing 4 posi-tions apart, while the multimodal model ranked the right listing 427 positions higher.  X  X edding band" saw a 1 . 55 % increase in NDCG by moving to a multimodal representation.
Learning to rank search results is one of Etsy and other e-commerce sites X  most fundamental problems. In this paper we describe how deep visual semantic features can be transferred successfully to multimodal learning to rank framework. We verify in a large-scale experiment that there is indeed significant complementary informa-tion present in images that can be used to improve search ranking quality. We visualize concrete examples of this marginal value: (i) the ability of the image modality to capture style information not well described in text. (ii) The ability of the image modality to disentangle highly different listings considered similar under a text-only modality.
 The authors would like to thank Arjun Raj Rajanna, Christopher Kanan and Robert Hall for fruitful discussions during the course of this paper. We would also like to thank Ryan Frantz and Will Gallego for their insight and help in building the infrastructure re-quired for running our experiments. [1] A YTAR , Y., AND Z ISSERMAN , A. Tabula rasa: Model [2] B AI , B., W ESTON , J., G RANGIER , D., C OLLOBERT , R., [3] B URGES , C., S HAKED , T., R ENSHAW , E., L AZIER , A., [4] C HATFIELD , K., S IMONYAN , K., V EDALDI , A., AND [5] F ROME , A., C ORRADO , G. S., S HLENS , J., B ENGIO [6] G ATYS , L. A., E CKER , A. S., AND B ETHGE , M. Texture [7] G ONG , Y., W ANG , L., H ODOSH , M., H OCKENMAIER , J., [8] G UILLAUMIN , M., V ERBEEK , J., AND S CHMID , C.
 [9] H ANG , L. A short introduction to learning to rank. IEICE [10] H ERBRICH , R., G RAEPEL , T., AND O BERMAYER , K. Large [11] J  X RVELIN , K., AND K EK X L X INEN , J. Cumulated [12] J OACHIMS , T. Optimizing search engines using clickthrough [13] K ANNAN , A., T ALUKDAR , P. P., R ASIWASIA , N., AND [14] K ARPATHY , A., J OULIN , A., AND L I , F. F. F. Deep [15] K IROS , R., S ALAKHUTDINOV , R., AND Z EMEL , R.
 [16] K RIZHEVSKY , A., S UTSKEVER , I., AND H INTON , G. E. [17] M IKOLOV , T., S UTSKEVER , I., C HEN , K., C ORRADO [18] O QUAB , M., B OTTOU , L., L APTEV , I., AND S IVIC [19] P AN , S. J., AND Y ANG , Q. A survey on transfer learning. [20] P EREIRA , J. C., AND V ASCONCELOS , N. On the [21] R ADLINSKI , F., AND J OACHIMS , T. Minimally invasive [22] R AZAVIAN , A., A ZIZPOUR , H., S ULLIVAN , J., AND [23] S IMONYAN , K., AND Z ISSERMAN , A. Very deep [24] S RIVASTAVA , N., H INTON , G., K RIZHEVSKY , A., [25] T OMMASI , T., O RABONA , F., AND C APUTO , B. Learning [26] W ANG , G., H OIEM , D., AND F ORSYTH , D. Building text [27] W EINBERGER , K., D ASGUPTA , A., L ANGFORD , J., [28] W ESTON , J., B ENGIO , S., AND U SUNIER , N. Large scale [29] Z EILER , M. D., AND F ERGUS , R. Visualizing and
