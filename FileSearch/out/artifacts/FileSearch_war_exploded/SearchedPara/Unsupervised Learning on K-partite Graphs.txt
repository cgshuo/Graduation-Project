 Various data mining applications involve data objects of multiple types that are related to each other, which can be naturally formulated as a k-partite graph. However, the research on mining the hidden structures from a k-partite graph is still limited and preliminary. In this paper, we propose a general model, the relation summary network, to find the hidden structures (the l ocal cluster structures and the global community structures) from a k-partite graph. The model provides a principal framework for unsupervised learning on k-partite graphs of various structures. Under this model, we derive a novel algorithm to identify the hid-den structures of a k-partite graph by constructing a rela-tion summary network to approximate the original k-partite graph under a broad range of distortion measures. Experi-ments on both synthetic and real data sets demonstrate the promise and effectiveness of the proposed model and algo-rithm. We also establish the connections between existing clustering approaches and the proposed model to provide a unified view to the clustering approaches.
 Categories and Subject Descriptions: E.4 [ Coding and Information Theory] :Data compaction and compres-sion; H.3.3[ Information search and Retrieval] :Clustering; I.5.3[ Pattern Recognition] :Clustering.
 General Terms: Algorithms.
 Keywords: K-partite graph, Unsupervised learning, Clus-tering, Relation summary network, Bregman divergence.
Unsupervised learning approaches have traditionally fo-cused on the homogeneous data objects, which can be rep-resented either as a set of feature vectors or a homogeneous graph with nodes of a single type. However, many examples Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. of real-world data involve objects of multiple types that are related to each other, which na turally form k-partite graphs of heterogeneous types of nodes. For example, documents and words in a corpus, customers and items in collabora-tive filtering, transactions and items in market basket, as well as genes and conditions in micro-array data all form a bi-partite graph; documents, words, and categories in taxon-omy mining, as well as Web pages, search queries, and Web users in a Web search system all form a tri-partite graph; papers, key words, authors, and publication venues in a sci-entific publication archive form a quart-partite graph. In such scenarios, using traditional approaches to cluster each type of objects (nodes) individually may not work well due to the following reasons.

First, to apply traditional clustering approaches to each type of data objects individually, the relation information needs to be transformed into feature vectors for each type of objects. In general, this transformation results in high di-mensional and sparse feature vectors, since after the trans-formation the number of features for a type of objects is equal to the number of all the objects which are possibly related to this type of objects. For example, if we transform the links between Web pages and Web users as well as search queries into the features for the Web pages, this leads to a huge number of features with sparse values for each Web page. Second, traditional clustering approaches are unable to tackle the interactions among the cluster structures of different types of objects, since they cluster data of a single type based on static features. Note that the interactions could pass along the relations, i.e., there exists influence propagation in a k-partite graph. Third, in some data min-ing applications, users are not only interested in the local cluster structure for each type of objects, but also the global community structures involving multi-types of objects. For example, in document clustering, in addition to document clusters and word clusters, the relationship between docu-ment clusters and word clusters is also useful information. It is difficult to discover such global structures by clustering each type of objects individually.

An intuitive attempt to mine the hidden structures from k-partite graphs is applying existing graph partitioning ap-proaches to k-partite graphs. This idea may work in some special and simple situations. However, in general, it is in-feasible. First, the graph partitioning theory focuses on find-ing the best cuts of a graph under a certain criterion and it is very difficult to cut different type of relations (links) simul-taneously to identify different hidden structures for different types of nodes. Second, by partitioning the whole k-partite graph into m subgraphs, one actually assumes that all dif-ferent types of nodes have the same number of clusters m , which in general is not true. Third, by simply partitioning the whole graph into disjoint subgraphs, the resulting hidden structures are rough. For exam ple, the clusters of different types of nodes are restricted to one-to-one associations.
Therefore, mining hidden structures from k-partite graphs has presented a great challenge to traditional unsupervised learning approaches. In this study, first we propose a general model, the relation summary network, to find the hidden structures (the local cluster structures and the global com-munity structures) from a k-partite graph. The basic idea is to construct a new k-partite graph with hidden nodes, which  X  X ummarize X  the link information in the original k-partite graph and make the hidden structures explicit, to approx-imate the original graph. The model provides a principal framework for unsupervised learning on k-partite graphs of various structures. Second, under this model, based on the matrix representation of a k-partite graph we reformulate the graph approximation as an optimization problem of ma-trix approximation and derive an iterative algorithm to find the hidden structures from a k-partite graph under a broad range of distortion measures. By iteratively updating the cluster structures for each type of nodes, the algorithm takes advantage of the interactions among the cluster structures of different types of nodes and performs implicit adaptive fea-ture reduction for each type of nodes. Experiments on both synthetic and real data sets demonstrate the promise and effectiveness of the proposed model and algorithm. Third, we also establish the connections between existing cluster-ing approaches and the proposed model to provide a unified view to the clustering approaches.
Graph partitioning on homogeneous graphs has been stud-ied for decades and a number of different approaches, such as spectral approaches [6, 26, 11] and multilevel approaches [5, 15, 19], have been proposed. However, the research on mining cluster structures from k-partite graphs of heteroge-neous types of nodes is limited. Several noticeable efforts include [8, 18] and [13]. [8, 18] extends the spectral parti-tioning based on normalized cut to a bi-partite graph. After the deduction, spectral partitioning on the bi-partite graph is converted to a singular value decomposition (SVD). [13] partitions a star-structured k-partite graph based on semi-definite programming. In addition to the restriction that they are only applicable to the special cases of k-partite graphs, all these algorithms have the restriction that the numbers of clusters for different types of nodes must be equal and the clusters for different types of objects must have one-to-one associations.

The research on clustering multi-type interrelated objects is also related to this study. Clustering on bi-type interre-lated data objects, such as word-document data, is called co-clustering or bi-clustering. Recently, co-clustering has been addressed based on matrix factorization. Both [23] and [21] model the co-clustering as an optimization problem involv-ing a triple matrix factorization. [23] proposes an EM-like algorithm based on multiplicative updating rules and [21] proposes a hard clustering algorithm for binary data. [10] extends the non-negative matrix factorization to symmet-Figure 1: A bi-partite graph (a) and its relation summary network (b). ric matrices and shows that it is equivalent to the Kernel K-means and the Laplacian-based spectral clustering.
Some efforts on latent variable discovery are also related to co-clustering. PLSA [16] is a method based on a mixture decomposition derived from a latent class model. A two-sided clustering model is proposed for collaborative filtering by [17]. Information-theory based co-clustering has also at-tracted attention in the literature. [12] extends the infor-mation bottleneck (IB) framework [28] to repeatedly cluster documents and then words. [9] proposes a co-clustering al-gorithm to maximize the mutual information between the clustered random variables subject to the constraints on the number of row and column clus ters. A more generalized co-clustering framework is presented by [3] wherein any Breg-man divergence can be used in the objective function.
Comparing with co-clustering, clustering on the data con-sisting of more than two types of data objects has not been well studied in the literature. Several noticeable efforts are discussed as follows. [30] proposes a framework for cluster-ing heterogeneous web objects, under which a layered struc-ture with link information is used to iteratively project and propagate the cluster results between layers. Similarly, [29] presents an approach named ReCom to improve the cluster quality of interrelated data objects through an iterative re-inforcement clustering process. However, there is no sound objective function and theoretical proof on the effectiveness of these algorithms. [22] formulates multi-type relational data clustering as collective factorization on related matri-ces and derives a spectral algorithm to cluster multi-type interrelated data objects simultaneously. The algorithm it-eratively embeds each type of data objects into low dimen-sional spaces and benefits from the interactions among the hidden structures of different types of data objects.
To summarize, unsupervised learning on k-partite graphs has been touched from different perspectives due to its high impact in various important applications. Yet, systematic research is still limited. This paper attempts to derive a theoretically sound general model and algorithm for unsu-pervised learning on k-partite graphs of various structures.
In this section, we derive a general model based on graph approximation to mine the hidden structures from a k-partite graph.

Let us start with an illustrative example. Figure 1(a) shows a bi-partite graph G =( V 1 ,V 2 ,E )where V 1 = { v ...,v 16 } and V 2 = { v 21 ,...,v 24 } denote two types of nodes and E denotes the edges in G . Even though this graph is simple, it is non-trivial to discover its hidden structures. In Figure 1(b), we redraw the original graph by adding two sets of new nodes (called hidden nodes), S 1 = { s 11 ,s 12 and S 2 = { s 21 ,s 22 } . Based on the new graph, the cluster structures for each type of nodes are straightforward: V 1 has three clusters, { v 11 ,v 12 } , { v 13 ,a 14 } ,and { v V 2 has two clusters, { v 21 ,v 22 } and { v 23 ,b 24 } .Ifwelookat the subgraph consisting of only the hidden nodes in Figure 1(b), we see that it provides a clear skeleton for the global structure of the whole graph, from which it is clear how the clusters of different types of nodes are related to each other; for example, cluster s 11 is associated with cluster and cluster s 12 is associated with both clusters s 21 and In other words, by introducing the hidden nodes into the original k-partite graph, both the local cluster structures and the global community structures become explicit. Note that if we apply a graph partitioning approach to the bi-partite graph in Figure 1(a) to find its hidden structures, no matter how we cut the edges, it is impossible to identify all the cluster structures correctly.

Based on the above observations, we propose a model, the Relation Summary Network (RSN), to mine the hidden structures from a k-partite graph. The key idea of RSN is to add a small number of hidden nodes to the original k-partite graph to make the hidden structures of the graph explicit. However, given a k-partite graph, we are not interested in an arbitrary relation network. To ensure a relation summary network to discover the desirable hidden structures of the original graph, we must make RSN as  X  X lose X  as possible to the original graph. In other words, we aim at an optimal relation summary network, from which we can re-construct the original graph as precisely as possible. Formally, we define an RSN as follows.

Definition 1. Given a distance function D ,ak-partite graph G =( V 1 ,...,V m ,E ), and m positive integers, k 1 the relation summary network of G is a k-partite graph ing conditions: 1. each instance node in V i is adjacent to one and only 2. S i  X  S j in G s if and only if V i  X  V j in G for i = 3. G s =argmin F D ( G, F ), where S i denotes a set of hidden nodes for V i and | S i for 1  X  i  X  m ; S i  X  S j denotes that there exist edges between S i and S j , and similarly V i  X  V j ; F denotes any tion 1 and 2.

In Definition 1, the first condition implies that in an RSN, the instance nodes (the nodes in V i ) are related to each other only through the hidden nodes. Hence, a small number of hidden nodes actually summarize the complex relations (edges) in the original graph to make the hidden structures explicit. Since in this study, our focus is to find disjoint clus-ters for each type of nodes, the first condition restricts one instance node to be adjacent to only one hidden node with unit weight; however, it is easy to modify this restriction to extend the model to other cases of unsupervised learning on k-partite graphs. The second condition implies that if two types of instance nodes V i and V j are (or are not) related to each other in the original graph, then the corresponding two types of hidden nodes S i and S j in the RSN are (or are not) related to each other. For example, Figure 2 shows a tri-partite graph and its RSN. In the origina l graph Figure 2(a), V 1  X  V 2 and V 1  X  V 3 , and hence S 1  X  S 2 and S
Figure 2: A tri-partite graph (a) and its RSN (b) in its RSN. The third condition states that the RSN is an optimal approximation to the original graph under a certain distortion measure.

Next, we need to define the distance between a k-partite graph G and its RSN G s . Without loss of generality, if V i  X  V j in G , we assume that edges between V i and V j are complete (if there is no edge between v ih and v jl ,we can assume an edge with weight of zero or other special value). Similarly for S i  X  S j in G s .Let e ( v ih ,v jl weightoftheedge( v ih ,v jl )in G . Similarly let e s ( s be the weight of the edge ( s ip ,s jq )in G s .IntheRSN,a pair of instance nodes v ih and v jl are connected through a unique path ( v ih ,s ip ,s jq ,v jl ), in which e s ( v ih ( s jq ,v jl )=1accordingtoDefinition1. Theedgebetween two hidden nodes ( s ip ,s jq ) can be considered as the  X  X um-mary relation X  between two sets of instance nodes, i.e., the instance nodes connecting with s ip and the instance nodes connecting with s jq . Hence, how good G s approximates G depends on how good e s ( s ip ,s jq ) approximates e ( v v ih and v jl which satisfy e s ( v ih ,s ip )=1and e s ( s respectively. Therefore, we define the distance between a k-partite graph G and its RSN G s as follows:
D ( G, G s )= where 1  X  i, j  X  m ,1  X  h  X | V i | ,1  X  l  X | V j | ,1  X  p  X | S and 1  X  q  X | S j | .

Let us have an illustrative example. Assume that the edges of the k-partite graph in Figure 1(a) have unit weights. If there is no edge between v ih and v jl ,welet e ( v ih Similarly for its RSN in Figure 1(b). Assume that D is the Euclidean distance function. Hence, based on Eq. (1), D (
G, G s ) = 0, i.e., from the RSN in Figure 1(b), we can reconstruct the original graph in Figure 1(a) without any error. For example, the path ( v 13 ,s 12 ,s 21 ,v 22 )intheRSN implies that there is an edge between v 13 and v 22 in the original graph such that e ( v 13 ,v 22 )= e s ( s 12 ,s 21 ing this procedure, the original graph can be reconstructed completely.

Note that different definitions of the distances between two graphs lead to different algorithms. In this study, we fo-cus on the definition given in Eq.(1). One of the advantages of this definition is that it leads to a nice matrix representa-tion for the distance between two graphs, which facilitates to derive the algorithm.
 Definition 1 and Eq. (1) provide a general model, the RSN model, to mine the cluster structures for each type of nodes in a k-partite graph and the global structures for the whole graph. Compared with the traditional clustering ap-proaches, the RSN model is capable of making use of the in-teractions (direct or indirect) among the hidden structures Figure 3: The cluster structures of V 2 and V 3 affect the similarity between v 11 and v 12 through the hid-den nodes. of different types of nodes, and through the hidden nodes performing implicit and adaptive feature reduction to over-come the typical high dimensionality and sparsity. Figure 3 shows an illustrative example of how the cluster structures of two types of instance nodes affect the similarity between two instance nodes of another type. Suppose that we are to cluster nodes in V 1 (only two nodes in V 1 are shown in Fig-ure 3(a)). Traditional clustering approaches determine the similarity between v 11 and v 12 based on their link features, [1 , 0 , 1 , 0] and [0 , 1 , 0 , 1], respectively, and hence, their sim-ilarity is inappropriately considered as zero (lowest level). This is a typical situation in a large graph with sparse links. Now suppose that we have derived hidden nodes for V 2 and V 3 as in Figure 3(b); through the hidden nodes the cluster structures of V 2 change the similarity between v 11 and v 1 (highest level), since the reduced link features for both and v 12 are [1 , 1], which is a more reasonable result, since in a sparse k-partite graph we expect that two nodes are sim-ilar when they are connected to similar nodes even though they are not connected to the same nodes. If we continue this example, next, v 11 and v 12 are connected with the same hidden nodes in S 1 (not shown in the figure); then after the hidden nodes for V 1 are derived, the cluster structures of and V 3 may be affected in return. In fact, this is the idea of the iterative algorithm to construct an RSN for a k-partite graph, which we discuss in the next section.
In this section, we derive an iterative algorithm to find the RSN (local optima) for a k-partite graph. It can be shown that the RSN problem is NP-hard (the proof is omit-ted here); hence it is not realistic to expect an efficient al-gorithm to find the global optima.

First we reformulate the RSN problem based on the ma-trix representation of a k-partite graph. Given a k-partite G =( V 1 ,...,V m ,E ), the weights of edges between V i and V j can be represented as a matrix A ( ij )  X  R n i n i = | V i | , n j = | V j | ,and A edge ( v ih ,v jl ), i.e., e ( v ih ,v jl ). SimilarlyinanRSN (
V of edges between S i and S j , i.e., B ( ij ) pq denotes e s C ( i )  X  X  0 , 1 } n i  X  k i denotes the weights of edges between V i and S i , i.e., C ( i ) is an indicator matrix such that if ( partite as a set of matrices. Note that under the RSN model, we do not use one graph affinity matrix to represent the whole graph as in the graph partitioning approaches, which may cause very expensive computation on a huge matrix.
Based on the above matrix representation, the distance between two graphs in Eq. (1) can be formulated as the dis-tances between a set of matrices and a set of matrix prod-ucts. For example, for the two graphs shown in Figure 1, D (
G, G s )= D ( A (12) ,C (1) B (12) ( C (2) ) T ); for the two graphs showninFigure2, D ( G, G s )= D ( A (12) ,C (1) B (12) ( C (2) D (
A (13) ,C (1) B (13) ( C (3) ) T ). Hence, finding the RSN defined in Definition 1 is equivalent to the following optimization problem of matrix approximation (for convenience, we as-sume that there exists A ( ij ) for 1  X  i&lt;j  X  m , i.e., every pair of V i and V j are related to each other in G ).
Definition 2. Given a distance function D ,asetofmatri-G ,and m positive integers, k 1 ,...,k m ,theRSN G s repre-
In the above definition, the constraint on C ( i ) is to re-strict C ( i ) to be an indicator matrix , in which each row is an indicator vector. In the definition, the distance between two matrices D ( X, Y ) denotes the sum of the distances of each pair of elements, i.e., D ( X, Y )= h,l D ( X hl ,Y hl
For the optimization problem in Definition 1 or Definition 2, there are many choices of distance functions, which im-ply the different assumptions about the distribution of the weights of the edges in the given k-partite graph. For ex-ample, by using Euclidean distance function, we implicitly assume the normal distribution for the weights of the edges. Presumably for a specific distance function used in Defini-tion 2, we need to derive a specific algorithm. However, a large number of useful distance functions, such as Euclidean distance, generalized I-divergence, and KL divergence, can be generalized as the Bregman divergences [25, 4]. Based on the properties of Bregman di vergences, we derive a gen-eral algorithm to minimize the objective function in Eq.(2) under all the Bregman divergences. Table 1 shows a list of Bregman divergences and their corresponding Bregman convex functions. Note that Bregman divergences are non-negative. The definition of a Bregman divergence is given as follows.
 Definition 3. Given a strictly convex function,  X  : S  X  R ,definedonaconvexset S  X  R d and differentiable on the interior of S ,int( S ), the Bregman divergence D  X  : int( S )  X  [0 ,  X  ) is defined as where  X   X  is the gradient of  X  .

We prove the following theorem which is the basis of our algorithm.

Theorem 1. Assume that D in Definition 2 is a Breg-are the optimal solution to the minimization in Definition 2, then for 1  X  i&lt;j  X  m .

Proof. For convenience we use Y to denote C ( i ) B ( ij )  X  ( x )todenote  X   X  ( x ),  X  ( x )todenote  X  2  X  ( x ).
We compute the gradient  X  B ( ij ) L ,where1  X  i&lt;j  X  m and L denotes the objective function in Eq.(2). Using the given by where denotes the Hadamard product or entrywise prod-uct of two matrices. By Eq.(6), we have According to the KKT conditions, an optimal solution to Definition 2 satisfies  X  B ( ij ) L =0,whichleadsto According to Definition 3,  X  is strictly convex, hence, [ 0for1  X  p  X  k i and 1  X  q  X  k j . Therefore,  X  ( Y )canbe canceled from Eq.(8) to obtain This completes the proof of the theorem.
 The most interesting observation about Theorem 1 is that Eq.(4) does not involve the distance function D  X  . We propose an iterative algorithm to find a local optimal given k-partite graph. At each iterative step, we update one of others.

Since C ( i ) is an indicator matrix, we adopt the reassign-ment procedure such as in the k-means algorithm to update C ( i ) . To determine which element of the h th row of C ( i ) equal to 1, for l =1 ,...,k i ,welet C ( i ) hl = 1 and compute the objective function L in Eq.(2) for each l , which is denoted as L l ,then
The updating rule in Eq.(10) is equivalent to updating the edges between V i and S i in G s by connecting v ih to each hidden nodes in S i to find which hidden node gives the smallest values for D  X  ( G, G s ), i.e., Algorithm 1 Relation Summary Network with Bregman Divergences Input: A k-partite graph G =( V 1 ,...,V m ,E ), a Bregman divergence function D  X  ,and m positive integers, k 1 ,...,k Output: An RSN G s =( V 1 ,...,V m ,S 1 ,...,S m ,E s ). Method: 1: Initialize G s . 2: repeat 3: for i =1to m do 4: Update the edges between V i and S i according to 5: end for 6: for each pair of S i  X  S j where 1  X  i&lt;j  X  m do 7: Update the edges between S i and S j according to 8: end for 9: until convergence where G s l denotes the RSN with s il connecting to v ih .Note that the computation for this updating involves only edges between v ih and the related nodes, not all the edges.
Based on Eq.(4) in Theorem 1, after a little algebraic manipulation, we have the following updating rule for each This updating rule does not really involve computing in-verse matrices, since ( C ( i ) ) T C ( i ) is a special diagonal ma-of instance nodes associated with the hidden node s ip ,and similarly for ( C ( j ) ) T C ( j ) . The updating rule in Eq.(12) is equivalent to updating the edges between S i and S j in G s by re-computing the weight of the edge between a pair of hidden nodes s ip  X  S i and s jq  X  S j as follows, where U = { v ih : e s ( v ih ,s ip )=1 } , i.e., the instance nodes associated with s ip ; Z = { v jl : e s ( v jl ,s jq )=1 } instance nodes associated with s jq ,1  X  p  X  k i , 1  X  q  X  k , 1  X  h  X  n i ,and1  X  l  X  n j . This updating rule is consistent with our intuition about the edge between two hidden nodes; i.e., it is the  X  X ummary relation X  for two sets of instance nodes. It is, however, a surprising observation that the updating does not involve the distance function, i.e., this simple updating rule holds for all Bregman divergences.
The algorithm, Relation Summary Network with Breg-man Divergences (RSN-BD), is summarized in Algorithm 1. RSN-BD iteratively updates the c luster structures for differ-ent types of instance nodes and summary relations among the hidden nodes. Through the hidden nodes, the cluster structures of different types of instance nodes interact with each other directly or indirectly. The interactions lead to the implicit adaptive feature reduction for each type of instance nodes which overcomes the typical high dimensionality and sparsity. RSN-BD is applicable to a wide range of problems, since it does not have restrictions on the structures of the in-put k-partite graph. Furthermore, the graphs from different applications may have different probabilistic distributions on their edges; it is easy for RSN-BD to adapt to this situ-ation by simply using different Bregman divergences, since Bregman divergences correspond to a large family of expo-nential distributions including most common distributions, such as Normal, Multinomial and Poisson distributions [7].
Note that to avoid clutter, we do not consider weighting different types of edges during the derivation. Nevertheless, it is easy to extend the proposed model and algorithm to the weighted versions.

If we assume that the number of pairs of V i  X  V j is  X ( m which is typical in real applications, and let n = X ( and k = X ( k i ), the computational complexity of RSN-BD can be shown to be O ( tmn 2 k )for t iterations. If we apply the k-means algorithm to each type of nodes individually by transforming the relations into features for each type of nodes, the total computational complexity is also O ( tmn 2 k Hence, RSN-BD is as efficient as k-means. If the edges in the graph are very sparse, the computational complexity of RSN-BD can be reduced to O ( tmrk ) where we assume that the number of edges between each pair of V i and V j is  X (
Eq.(4) in Theorem 1 is an necessary condition for an op-timal solution, but not sufficient for the correctness of the RSN-BD algorithm. The following theorems guarantee the convergence of RSN-BD.

Lemma 1. Given a Bregman divergence D  X  : S  X  int ( S )  X  [0 ,  X  ) , A  X  R n 1  X  n 2 and two indicator matrices, C (1)  X  { 0 then for any B  X  R k 1  X  k 2 ,
Proof. For convenience we use Y to denote C (1) B ( C (2) Y  X  to denote C (1) B  X  ( C (2) ) T ,  X  ( x )todenote  X   X  denote the lefthand side of Eq.(15).
 J = During the above deduction, the second and fifth equalities follow the definition of the Bregman divergences; the fourth equality follows the fact that h,l [ A  X  ( Y  X  )] hl = h,l  X  ( ing from the special structure of the indicator matrix; the last inequality follows the non-negativity of Bregman diver-gences.

Theorem 2. The RSN-BD algorithm (Algorithm 1) monotonically decreases the objective function in Eq. (1) .
Proof. Proving the theorem is equivalent to proving that the updating rules in Eq.(10) and Eq.(12) monotonically decrease the objective function in Eq.(2). Let L ( t ) denote the objective value after the t th iteration.
 where the first inequality follows trivially the criteria used for reassignment in Eq.(10), and the second inequality fol-lows Eq.(12) and Lemman 1.
 Base on Theorem 2 and the fact that the objective function in Eq.(2) has the lower bound 0 for a Bregman divergence, the convergence of RSN-BD is proved.
In this section we discuss the connections between existing clustering approaches and the RSN model. By considering them as special cases or variations of the RSN model, we show that RSN provides a unified view to the existing clus-tering approaches.
Bipartite Spectral Graph Partitioning (BSGP) [8, 18] uses the spectral approach to partitioning a bi-partite graph to find cluster structures for two t ypes of interrelated data ob-jects, such as words and documents. The objective func-tion of BSGP is the normalized cut on the bi-partite graph, whose affinity matrix is 0 A A T 0 . After the deduction, the spectral partitioning on the bipartite graph is converted to a singular value decomposition (SVD) [8, 18].

As a graph partitioning approach, BSGP has the restric-tion that the clusters of different types of nodes have one-to-one associations. Under the RSN model, this restriction is equivalent to letting a hidden node connect with one and only one hidden node. Hence, the affinity matrix represent-ing the edges between two sets of hidden nodes is restricted to a diagonal matrix. The objective function in Eq.(2) can be formulated as where || X || denotes Frobenius norm, i.e., the Euclidean dis-tance function is adopted, and A may be normalized as de-scribed in [8]. Based on this objective function, if we relax C (1) and C (2) to any othornormal matrices as in [8, 18], it immediately follows the standard result of linear algebra [14] that the minimization of L in Eq.(16) with the diagonal constraint on B is equivalent to partial SVD. Therefore, the RSN model based on Euclidean distance function provides a simple way to understand BSGP. Comparing with BSGP, RSN-BD is more flexible to exploit the cluster structures from a bi-partite graph, since it does not have one-to-one association as a constraint and is capable of adopting differ-ent distance functions.
In [21], a model is proposed to cluster binary data by clus-tering data points and features simultaneously, i.e., cluster-ing with feature reduction. If we consider data points and features as two different types of nodes in a bi-partite graph and the binary elements of the data matrix denote whether there exists a link between a pair of nodes, then this model is equivalent to the RSN model on a bi-partite graph with unit weight edges. The objective function of this model is given in [21] as where W denotes the data matrix, A and B denote cluster memberships for data points and features, respectively, and X represents the associations between the data clusters and the feature clusters. We can see that this objective function is exactly the same as the objective function in Eq.(2) on bi-partite graph with Euclidean distance.

The immediate benefit of establishing the connection be-tween the model proposed in [21] and the RSN model is the new solution to binary data clustering with feature re-duction. In [21], the model is based on Euclidean distance. Euclidean distance function has very wide applicability, since it implies the normal distribution and most data with a large sample size tend to have a normal distribution. However, since Bernoulli distribution is a more intuitive choice for the binary data, RSN-BD directly provides a new algorithm for clustering binary data with feature reduction by using lo-gistic distance function (see Table 1), which corresponds to Bernoulli distribution. [9] proposes a novel theoretic formulation to view the contingency table as an empirical joint probability distribu-tion of two discrete random variables and developes the co-clustering algorithm, Information-Theoretic Co-Clustering (ITCC), to maximize the mutual information between the clustered random variables subject to the constraints on the number of row and column clusters. Let X and Y be discrete random variables that take values in the sets { x 1 ,...,x n 1 } and { y 1 ,...,y n 2 } , respectively, and be the cluster random variables that take values in the sets {  X  x jective function of ITCC is the loss in mutual information, I ( X ; Y )  X  I (  X  X,  X  Y ).

The joint distribution of X and Y canbeformulatedas a bi-partite graph by assigning the probability p ( x h ,y the weight of the edge between v 1 h  X  V 1 and v 2 l  X  V 2 modify the Condition 1 in Definition 1 such that an instance node v ih is connected to one and only one hidden node s ip with weight 1 # s ip where # s ip is the number of the instance nodes connected to s ip , then in the RSN of aforementioned bi-partite graph, e s ( v 1 h ,s 1 p )and e s ( v 2 l ,s 2 ered as p ( x h |  X  x p )and p ( x l |  X  x q ), respectively, be considered as p ( X  x p ,  X  x q ). Based on this formulation, it is easy to verify that the objective function of RSN with KL-divergence is equivalent to I ( X ; Y )  X  I (  X  X,  X  Y ). This con-nection between the ITCC and a variation of RSN model implies that the ITCC algorithm may be extended to more general cases of more than two random variables and with other loss functions.
Due to its simplicity, efficiency, and broad applicability, k-means algorithm has become one of the most popular clus-tering algorithms. Figure 4 explains the relation between the RSN model and k-means. If we consider data points and features as two different types of nodes, V 2 and V 1 ,inabi-partite graph, and restrict feature nodes to have one-to-one associations of their hidden nodes with unit weight, then the objective function in Eq.(2) is given as L = || A (12)  X  C (1) B (12) ( C (2) ) T || 2 where C (2) is restricted to an identity matrix. Hence, the objective function is reduced to L = || A (12)  X  C (1) B (12) || 2 , which is exactly the matrix represen-tation for the objective function of the k-means algorithm [31]. From Figure 4, we also see that since the number of feature nodes is equal to the number of their hidden nodes, k-means does not do feature reduction. Finally, we may con-sider RSN-BD as a generalization of k-means on k-partite graphs with various Bregman divergences and expect that it inherits the simplicity and efficiency of k-means and has much broader applicability.

There are more clustering approaches in the literature that may be considered as the special cases or variations of the RSN model. For example, the subspace clustering [1], which clusters the data points in a high dimensional space around a different subset of the dimensions, can be consid-ered as an extension of Figure 4 such that s 21 or s 22 only connects to a subset of S 1 . Spectral relational clustering [22] can be considered as using the spectral approach to solve the RSN model under Euclidean distance.

By examining the connections between existing cluster-ing approaches and the RSN model, we conclude that the RSN model provides a unified view to the existing cluster-ing approaches. Moreover, the idea of RSN is more gen-eral than the proposed model based on Definition 1 and Eq.(1). For example, if we change the definition of distance between graphs in Eq.(1), we may find totally different ways to mine hidden structures from a k-partite graph, and as a result, we may obtain new variations for existing clustering approaches.
This section provides empirical evidence to show the effec-tiveness of the RSN model and algorithm. In particular, we apply RSN-BD to two basic types of k-partite graphs, the bi-partite graph and the sandwich structure tri-partite graph (such as Figure 2(a)), which arise frequently in various appli-cations. Note that the application of RSN-BD is not limited to these two types of graphs and it is applicable to various k-partite graphs. Four types of RSN-BD are evaluated in the Table 2: Parameters and distributions for synthetic bi-partite graphs experiments: RSN with Euclidean Distance (RSN-ED) as-sumes the normal distribution of the data; RSN with Logis-tic Loss (RSN-LL) assumes the Bernoulli distribution of the data; RSN with Generalized I-divergence (RSN-GI) assumes the Poisson distribution of the data; RSN with Itakura-Saito distance (RSN-IS) assumes the exponential distribution of the data. Two graph partitioning approaches, BSGP [8] and Consistent Bipartite Graph Co-partitioning (CBGC) [13] (we thank the authors for providing the executable code of CBGC) , are used as the comparison on bi-partite graph and sandwich tri-partite graph, respectively. Four traditional feature-based algorithms, which cluster a type of nodes in a k-partite graph by transforming all the links into fea-tures, are also used as comparisons. They are K-Means with Euclidean Distance (KM-ED), K-Means with Logistic Loss (KM-LL), K-Means with Generalized I-divergence (KM-GI) and K-Means with Itakura-Saito (KM-IS).
The data sets used in the experiments include synthetic data sets with various distributions and real data sets based on the 20-Newsgroup data [20].

The synthetic bi-partite graphs are generated such as that both V 1 and V 2 have two clusters (to be fair for BSGP, we use equal number of clusters); each cluster has 100 nodes, hence, both V 1 and V 2 have 200 nodes. The distributions and parameters (the true means of the distributions) used to generate the links in the graphs are documented in Ta-ble 2. In the table, distribution parameters for a graph is represented as a matrix S such that S pq denotes the mean parameter of the distribution to generate the links between the p th cluster of V 1 and the q th cluster of V 2 .
The real bi-partite graphs are constructed based on var-ious subsets of the 20-Newsgroup data [20] which contains about 20 , 000 articles from 20 newsgroups. We pre-process the data by removing stop words and selecting the top 2000 words by the mutual information. The document-word ma-trix is based on tf.idf weighting scheme and each docu-ment vector is normalized to a unit L 2 norm vector. Spe-cific details of data sets used to construct bi-partite graphs are listed in Table 3. For example, to construct a BP-NG3 graph, we randomly and evenly sample 200 documents from the corresponding newsgroups; then we formulate a bi-partite graph consisting of 1600 document nodes and 2000 word nodes.

The synthetic tri-partite graphs are generated similarly to the bi-partite graphs. The distributions and parameters are documented in Table 4. Let V 1 denote the central type nodes. In Table 4, S (12) denotes the true means of distri-butions for generating the links between V 1 and V 2 ,and similarly for S (13) . The numbers of clusters for each type of nodes are given by dimensions of S (12) and S (13) and each Table 4: Parameters and distributions for synthetic tri-partite graphs Table 5: Taxonomy structures of two data sets for constructing tri-partite graphs cluster has 100 nodes. In Table 4, TP-large is a large graph with 20 clusters of V 1 ,20clustersof V 2 , and 18 clusters of (due to the space limit, the details of parameters are omit-ted). Each BP-large graph contains 5800 nodes and on an average about 3 . 25 million links.

The real tri-partite graphs are built based on the 20-newsgroups data for hierarchical taxonomy mining. In the field of text categorization, hierarchical taxonomy classifica-tion is widely used to obtain a better trade-off between ef-fectiveness and efficiency than flat taxonomy classification. To take advantage of hierarchical classification, one must mine a hierarchical taxonomy from the data set. We see that words, documents, and categories formulate a sandwich structure tri-partite graph, in which documents are central type nodes. The links between documents and categories are constructed such that if a document belongs to k cate-gories, the weights of links between this document and these k category nodes are 1 /k (please refer [13] for details).
The true taxonomy structures for two data sets, TP-TM1 and TP-TM2 , are documented in Table 5. For example, TP-TM1 data set is sampled from five categories (200 doc-uments for each category), in which two categories belong to the high level category res.sports and other three categories belong to the high level category talk.politics .
For all the algorithms on all the graphs, we fix the number of iterations to 20 (this also holds true for BSGP and CBGC, since they use classic k-means to do postprocessing) and use the same initialization, random initialization for synthetic data and classic k-means initialization for real data. The final performance score is the average of the twenty runs. At each test run, a graph is constructed by sampling from the corresponding distributions or newsgroups of the 20-newsgroup data. Hence, the variation of a final performance score includes the variance of sampling.

For the number of clusters, we use the true number of clus-ters for the synthetic graphs. For real data graphs, we use the true number of clusters for documents and categories; however, we do not know the true number of word clusters. How to determine the optimal number of word clusters is beyond the scope of this paper. We simply adopt 40 for all the RSN algorithms. For BSGP and CBGC, the number of word clusters must equal the number of document clusters. By the authors X  suggestion, the parameter setting for CBGC is  X  =0 . 5,  X  1 =1and  X  2 = 1 [13].

The performance comparison is based on the quality of the clusters of one type of nodes in each graph. In synthetic bi-partite graphs, it is based on V 1 whose clusters corre-spond to the rows of S in Table 2; in synthetic tri-partite graphs, it is based on the central type nodes V 1 ; in bi-partite graphs of documents and words, it is based on documents; in tri-partite graphs for taxonomy mining, it is based on categories whose clusters provide the taxonomy structures. For performance measure, we elect to use the Normalized Mutual Information (NMI) [27], which is a standard way to measure the cluster quality.
Table 6 shows the NMI scores of the nine algorithms on the bi-partite graphs. For the BP-b1 graph, all the algo-rithms provide perfect NMI score, since the graphs are gen-erated with very clear structures, which can be seen from the parameter matrix in Table 2. For other synthetic bi-partite graphs, the cluster structures are subtle, especially for the nodes V 1 , whose cluster structures are our objec-tive. For these graphs, the RSN algorithms perform much better than k-means algorithms, especially for the BP-b2 and BP-p graph, in which the distributions for clusters of V 1 are very close to each other and the links are relatively sparse. This comparison implies that benefiting from the in-teractions among the cluster structures of different types of nodes, the RSN algorithms are able to identify very subtle cluster structures even when the traditional clustering ap-proaches totally fail. Compared with the RSN algorithms, BSGP performs poorly for all the synthetic bi-partite graphs except BP-b1 . The possible explanation is that it assumes one-to-one associations between clusters of different types of nodes, which does not hold true for the synthetic bi-partite graphs except BP-b1 . We also observe that the RSN algo-rithm with the distance function matching the distribution to generate the graph provides the best NMI score for that graph.

For the real bi-partite graphs consisting of document and word nodes, RSN-LL always provides the best NMI score. For the difficult BP-NG1 graph based on two  X  X lose X  news-groups, RSN-LL shows about 44% improvement in compar-ison with KM-LL, which is, along with KM-GI, the best among the non-RSN algorithms. Note that since the docu-ment vector is L 2 -normalized, the KM-ED is actually based on von Mises-Fisher distribution [24], which proved efficient for document clustering [2]. We also observe that for these graphs, in general the algorithms based on logistic loss pro-vide better performance. The possible reason is that logis-tic loss corresponds to Bernoulli distribution which provides a good approximation to the distribution of the data con-sisting of a large mount of zeros, such as the sparse links between documents and words. In the meantime, it is also reasonable to assume the Poisson distribution for the fre-quencies such as the frequency in that a word appears in a document. That is why RSN-GI also shows the perfor-mance very close to RSN-LL. The above comparison verifies the assumption that under an appropriate distribution as-sumption, through the hidden nodes the RSN algorithms perform implicit adaptive feature reduction to overcome the typical high dimensionality and sparseness.

Table 7 shows the NMI scores of the nine algorithms on the tri-partite graphs. As similarly in the synthetic bi-partite graphs, the RSN algorithms perform much better than the k-means algorithms. Except for RSN-ED on the TP-p graph, the RSN algorithms perform significantly bet-ter than CBGC. The NMI scores of CBGC for some graphs are not available because the CBGC code provided by the authors only works for the case of two clusters and small size graphs. For the large dense TP-large graph, the RSN algo-rithms perform consistently better than the KM algorithms, and this demonstrates the good scalability of the RSN al-gorithms; the RSN-ED performs best on TP-large ,andthis demonstrates the advantage of the normal distribution for the very large sample size of dense links.

For the real tri-partite graphs for taxonomy mining, the k-means algorithms perform poorly since they cluster cat-egories only based on links between categories and docu-ments. From Table 7, we observe that both RSN-ED and RSN-IS provide the best NMI score for TP-TM1 .Tohave an intuition about this score, we check the details of the 20 test runs, which show that in 16 out of the 20 runs the algorithms provide the perfect taxonomy structures and in the other 4 runs one category is clustered incorrectly. We believe that if we assign different weights to different types of links, the RSN algorithms could perform more efficiently on mining the taxonomy structures. However, this is beyond the scope of this paper.
In this paper, we propose a general model RSN to find the hidden structures (the local clus ter structures and the global community structures) from a k-partite graph. The model provides a principal framework for unsupervised learning on k-partite graphs of various structures. Under this model, we derive a novel algorithm to find the hidden structures from a k-partite graph under a broad range of distortion measures. By iteratively updating the cluster structures for each type of nodes, the algorithm takes advantage of the interactions among the cluster structures of different types of nodes and performs implicit adaptive feature reduction for each type of nodes. Experiments on both synthetic and real data sets demonstrate the promise and effectiveness of the proposed model and algorithm. We also establish the connections between existing clustering approaches and the proposed model to provide a unified view to the existing clustering approaches in the literature. There are a number of interesting potential directions for future research on the RSN model and algorithms, such as extending RSN model to other cases of unsupervised learning on k-partite graphs and applying the RSN algorithms to a wide range of problems involving k-partite graphs. This work is supported in part by NSF (IIS-0535162), AFRL Information Institute (FA8750-04-1-0234, FA8750-05-2-0284), AFOSR (FA9550-06-1-0327), and a summer re-search internship at Yahoo! Research.
