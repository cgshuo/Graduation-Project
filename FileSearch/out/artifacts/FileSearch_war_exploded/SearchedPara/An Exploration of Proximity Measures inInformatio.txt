 In most existing retrieval models, documents are scored primarily based on various kinds of term statistics such as within-document frequencies, inverse document frequencies, and document lengths. Intuitively, the proximity of matched query terms in a document can also be exploited to promote scores of documents in which the matched query terms are close to each other. Such a proximity heuristic, however, has been largely under-explored in the litera-ture; it is unclear how we can model proximity and incorporate a proximity measure into an existing retrieval model. In this pa-per, we systematically explore the query term proximity heuristic. Specifically, we propose and study the effectiveness of five differ-ent proximity measures, each mode ling proximity from a different perspective. We then design two heuristic constraints and use them to guide us in incorporating the proposed proximity measures into an existing retrieval model. Experiments on five standard TREC test collections show that one of the proposed proximity measures is indeed highly correlated with document relevance, and by incor-porating it into the KL-divergence language model and the Okapi BM25 model, we can significantly improve retrieval performance. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms Proximity, retrieval heuristics
One of the most fundamental research questions in information retrieval is how to operationally define the notion of relevance so that we can score a document w.r.t. a query appropriately. A dif-ferent definition generally leads to a different retrieval model. In the past a few decades, many different retrieval models have been Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. proposed and tested, including vector space models [26, 25], clas-sic probabilistic models [23, 29, 9], and statistical language mod-els [21, 12, 16, 31, 18, 30, 32, 17, 6].

In most existing retrieval models, documents are scored primar-ily based on various kinds of term statistics such as within-document frequencies, inverse document frequencies, and document lengths [7], but the proximity of matched query terms in a document has not been exploited. Intuitively, given two documents that match the same number of query words, we would like to rank the docu-ment in which all query terms are close to each other above the one where they are apart from each other. Thus query term proximity is another potentially useful heuristic that can be incorporated into a retrieval model.

For example, consider the query  X  X earch engine X  and the follow-ing two documents, both matching the two query terms once: Example 1 Document 1:  X ... search engine ... X  Example 2 Document 2:  X ... search .... engine ... X 
Intuitively, Document 1 should be ranked higher because its two query terms are adjacent to each other. In contrast, the two query terms in Document 2 are far apart, thus their combination does not necessarily imply the meaning of  X  X earch engine X .

Interestingly, while intuitively quite appealing, such a proximity heuristic has so far been largely under-explored in the literature. In-deed, although several studies have looked into proximity [14, 15, 1, 10, 5, 22, 3, 2], the results are non-conclusive; it is still unclear how we should model proximity and how we can incorporate a proximity measure into an existing retrieval model. The proximity heuristic has also been indirectly captured in some retrieval mod-els through using larger indexing units than words that are derived based on term proximity (e.g., [20]), but these models can only ex-ploit proximity to a limited extent since they do not measure the proximity of terms. (See Section 2 for a detailed review of them.)
In this paper, we systematically study the effectiveness of the query term proximity heuristic through modeling term proximity directly and incorporating proximity measures into an existing re-trieval model. We first study how to measure query term proximity independently of other relevance factors such as Term Frequency (TF) and Inverse Document Frequency (IDF); this way, we can isolate the proximity factor and see clearly its impact on model-ing document relevance. Since it is unclear what is the best way to measure proximity, we systematically explore several different measures. They capture query term proximity from different per-spectives. For example, one such measure (called  X  X inimum cov-erage X ) is the minimum span of text in a document covering all the query terms at least once. Intuitively, the smaller the minimum coverage of a document is, the more likely it is relevant. Along similar lines, we propose four other proximity distance measures: span, minimum pairwise distance, average pairwise distance, and maximum pairwise distance.

To assess the potential of these proximity distance measures for improving a retrieval function, we compute their correlations with document relevance. The results show that minimum pairwise dis-tance is more promising than others.

Next, we study how to exploit the proposed proximity distance measures to improve a retrieval model. Since the existing retrieval models have captured other retrieval heuristics very well and have proved to be effective over many different test collections, we study how to add proximity on top of them rather than develop a com-pletely new retrieval with proximity heuristics from scratch. Specif-ically, we would add proximity to an existing retrieval function as a complementary scoring component to slightly adjust the relevance score of a document. This way, we can focus on evaluating the influence of the proximity feature on retrieval performance.
To incorporate a proximity distance measure into an existing re-trieval function, we first define two heuristic constraints, in a sim-ilar way as in [7, 8], to capture the desirable properties of the new retrieval function that we would like to develop. These con-straints suggest that the contribution of a proximity distance mea-sure should follow a function of a convex shape. Our final function therefore uses a popular logarithm function to convert a proxim-ity distance measure to a proxim ity feature value, which is then combined with two existing retrieval functions  X  the KL-divergence language model [16] and the Okapi BM25 model [23].

We evaluate our final models on five representative standard TREC data sets. The results show that the three pairwise distance mea-sures are all effective for improving retrieval accuracy while the other two span-based measures are not effective, likely due to the problem of normalization. In particular, of all the proximity dis-tance measures, we have found that the minimum pairwise distance measure is the best and when added on top of the KL-divergence re-trieval model and the Okapi BM25 retrieval model, it can improve retrieval performance significantly.

The rest of the paper is organized as follow: We review the re-lated previous work in Section 2, and report the experiment data and their statistics in Section 3. In Section 4, we propose and exam-ine five proximity distance measures. In Section 5, we study how to incorporate the proximity heuristic into two existing retrieval mod-els. We report experiment results in Section 6, and finally conclude our work in Section 7.
Keen X  X  studies [14, 15] are among the early efforts to study the effectiveness of proximity in retrieval systems, in which, a  X  X EAR X  operator was used to quantify the proximity of query terms. It has two major deficiencies: First, the experiments were conducted on very small data sets, thus the conclusions may not generalize well. Second, it was developed based on the Boolean retrieval model, which is generally regarded as less effective than the mod-ern ranking-based full text retrieval models. The work[1] is one of the follow-up studies also dealing with Boolean queries. The stud-ies [5] and [10] appear to be the first to evaluate proximity on TREC data sets. Both of them measure proximity by using a so-called  X  X pan X  measure  X  the text segment containing all query term in-stances. The evaluation results are not conclusive. We will also evaluate this span feature in this paper. In addition, we also pro-pose several other measures that are more effective than the span measure.
 Some researchers studied proximity particularly based on the BM25 retrieval model [22, 3, 2]. They heuristically added prox-Table 1: Experiment data sets. *We remove the queries without relevant documents in FR collection. Thus, there are only 21 queries left. imity to the BM25 retrieval function, but their experiments are not conclusive and they have not reached a robust effective retrieval function through explo iting proximity. In our paper, we also com-bine the proximity measures with BM25. Compared with this pre-vious work, our work is more systematic and results in an effective retrieval function in which proximity is effectively combined with other retrieval heuristics.

An indirect way to capture proximity is to use high-order n-grams as units to represent text. For example, in [27], bigram and trigram language models are shown to outperform simple unigram language models. However, query terms are not always adjacent to each other in documents. For example, if  X  X earch X  and  X  X ngine X  in the example given in Section 1 are separated by only a single word, a bigram language model would not be able to capture the proximity. We may attempt to capture such proximity by increas-ing the length of an n-gram. However, this would increase the size of the parameter space significantly, making parameter estimation inaccurate because we often have only an extremely small sample for parameter estimation, i.e., a document. A more general way to indirectly capture proximity through using appropriate  X  X atching units X  is Metzler and Croft X  X  work on term dependency [20]. In this work, term structures with different levels of proximity can be defined in a general probabilistic model. Unfortunately, one has to pre-define the levels of proximity. Moreover, parameter estimation would be more difficult as we attempt to distinguish proximity at finer granularity levels. Thus in reality, it is impossible for these indirect methods of incorporating proximity to capture proximity in its full spectrum.

Our work is also related to passage retrieval [24, 4, 13, 19, 28, 11], where documents are often pre-segmented into small passages, which are then taken as units for retrieval. Since matching a pas-sage implies imposing a proximity constraint on the matched query terms, passage retrieval can also capture proximity at a coarse gran-ularity level, though it is clear that proximity can only be captured in a limited way with this approach.
We used several representative standard TREC data sets in our study 1 : AP (Associated Press news 1988-90), DOE (Department of Energy abstracts), FR (Federal Register), TREC8 (the ad hoc data used in TREC8), WEB2g (WT2g Web data). They represent different sizes and genre of text collections. Table 1 shows the statistics of these data. Throughout this paper, we will use these five data sets to do data analysis and evaluate proximity models.
Intuitively, we hope to reward a document where the matched query terms are close to each other. However, the issue is com-http://trec.nist.gov/ plicated because a query may have more than two terms and each term may occur multiple times in a document. In this section, we propose several proximity measures to capture this notion of close-ness.

We start with assuming that we can segment a document into some units (e.g., terms or sentences). Based on a given segmenta-tion method, we can then measure the length of any text segment by the number of units in the text segment and measure the distance between two term occurrences based on the number of units in be-tween the two occurrences. In this paper, we assume that the unit for segmentation is a term. However, the proposed measures can be directly applied to other choices of the unit.

When a document matches two query terms each once, it would be natural to measure the proximity by the distance between the two matched query terms. However, in general, a document may match more than two query terms and each query term may occur multiple times in the document. A main challenge is thus to construct an overall proximity distance measure that can account for an arbitrary number of matched query terms.
 We propose two kinds of approaches: (1) Span-based approaches: We measure the proximity based on the length of a text segment covering all the query terms. (2) Distance aggregation approaches: We measure the proximity by aggregating pair-wise distances be-tween query terms. Relatively speaking, the first kind is more  X  X lobal X  because it must account for all query terms. In contrast, the second kind is more  X  X ocal X  because it may be more sensitive to the distance of an individual pair depending on how aggregation is done. Below we define five specific proximity distance measures in these two categories of approaches. We will use the following short document d as an example to explain our definitions. Definition 1 (Span) Span [10] is defined as the length of the short-est document segment that covers all query term occurrences in a document, including repeated occurrences.

For example, in the short document d ,theSpanvalueis 7 for the query { t 1 ,t 2 } .
 Definition 2 (Min coverage (MinCover)) MinCover is defined as the length of the shortest document segment that covers each query term at least once in a document.

In the above example, if the query is { t 1 ,t 2 } , its MinCover would be 2 , but if the query is { t 1 ,t 2 ,t 4 } , its MinCover would be 5 (the length of the segment from the second position to the sixth position).
Here we first define a pairwise distance between individual term occurrences, and then aggregate the pairwise distances to gener-ate an overall proximity distance value. Specifically, we first pair up all the unique matched query words and measure their closest distances in documents. For example, when a query has three dif-ferent words { t 1 , t 2 , t 3 } and a document matches all the three words, we can obtain three different pairs of query term combi-nations: { t 1 , t 2 } , { t 1 , t 3 } ,and { t 2 , t 3 } d , the closest distance for all these three pairs is 1 as they have all occurred next to each other somewhere. We use D is ( t 1 ,t denote the closest distance between the occurrences of term t term t 2 in document D .

We now consider three different aggregation operators (i.e., Min-imum, Average, and Maximum) and define the following three dis-tance measures: Definition 3 (Minimum pair distance (MinDist)) The minimum pair distance is defined as the smallest distance value of all pairs of unique matched query terms. Formally, For example, the MinDist of the example document d for query Q = { t 1 ,t 2 ,t 3 } is 1.
 Definition 4 (Average pair distance (AveDist)) The average pair distance is defined as the average distance value of all pairs of unique matched query terms. Formally, is the number of unique matched query terms in D , and in the sum, we count D is ( q 1 ,q 2 ; D ) and D is ( q 2 ,q 1 ; D ) only once. For example, the AveDist of the example document d for query Q = { t 1 ,t 4 ,t 5 } is (1 + 2 + 3) / 3=2 .
 Definition 5 (Maximum pair distance (MaxDist)) The maximum pair distance is defined as the largest distance value of all pairs of unique matched query terms. Formally,
Note that all aggregation operators are defined over the pairwise distances on the matched query terms. The pairwise distance be-tween two query terms is always based on their closest positions in a document. In the case when a document matches only one query term, MinDist, AveDist, and MaxDist are all defined as the length of the document.

All five measures can be calculated efficiently. We elaborate the calculation of MinCover briefly because it is not very straightfor-ward. Assume that a document matches K unique query terms, and the total number of occurrences of these K query terms is N .We can record the positions of these N occurrences in order in the in-verted index so that we can scan them one by one. While scanning, we maintain a list of length K , in which we store the last position of each seen query term. In other words, if a term t occurs twice, we would record the location of the first occurrence when the scan-ning hits the first one and update it when we hit the second one. In each step, we calculate the span solely based on the information in the list, and finally select the smallest span value we have ever obtained during the scanning process. Since K is often very small, the algorithm is close to linear in terms of N .
The five proximity distance measures defined above all capture proximity of matched query terms intuitively. We now look into the question whether they can potentially be exploited to improve a re-trieval model. To answer this question, we examine the correlations between these measures and the relevance status of documents.
We use the KL-divergence retrieval method [16] to retrieve top 1000 documents for each query, calculate different proximity dis-tance measures for each document, and then take the average of these values for relevant and non-relevant documents respectively. Intuitively, we expect the proximity distance measures on relevant documents to have smaller values than those on non-relevant docu-ments since the query terms are expected to be closer to each other in a relevant document than in a non-relevant document.
We first report the Span and MinCover values in Table 2. In this table, we separate non-relevance scores and relevant scores in
Table 3: Normalized global measures on different data sets two columns. For example, the first number 354 . 48 means that the average of  X  X pan X  values of all non-relevant documents is 354 . 48 .
Disappointingly, all the results are negative: the proximity dis-tance values of relevant documents are all much larger than those of non-relevant documents. This c ounter-intuition r esult indicates that we may have missed important factors in relating proximity to document relevance. We notice that not all query terms appear in every document, and also some terms appear more frequently in one document than in another. When a document has more query terms, those terms would tend to span widely. Thus, both global measures (i.e., Span and MinCover) favor documents with fewer query term occurrences. To correct this bias, we therefore intro-duce a normalization factor. Specifically, we propose to normalize Span by dividing it by the total number of occurrences of query terms in the span segment, and normalize MinCover by dividing it by the number of unique query terms. We report the results from the normalized measures in Table 3.
 We can make some interesting observations in Table 3. While Span still shows negative results, MinCover is now indeed slightly smaller on relevant documents than on non-relevant documents in most cases, suggesting the existence of weak signals. Even for Span, we also observe that the normalized version appears to be less negative than the non-normalized version.

The results about the three  X  X ocal X  measures are shown in Ta-ble 4.

We can now observe some interesting positive correlations in Ta-ble 4: While MaxDist results are still negative, both AveDist and MinDist are indeed positive. In particular, the MinDist measure has consistently smaller values for relevant documents than non-relevant ones.

The observations in this section suggest three things: First, nor-malization is an important factor for  X  X lobal X  measures. Second, a  X  X ocal X  measure can be expected to perform better than a  X  X lobal X  measures. In particular, MinDist is likely to be the best measure among all the five measures. As will be shown later in Section 6, these predictions are indeed true.
In this section, we study incorporation of the proposed proximity distance measures into an existing retrieval model. Since the raw values of proximity distances are g enerally not comparable with the values of a retrieval function, it is non-trivial to find an effective way of combining them. As we will show in Section 6, simply adding a good proximity distance measure to a retrieval function does not necessarily lead to a better retrieval function.
Our idea is to first figure out a way to transform a proximity distance measure appropriately so that it would make  X  X easonable X  contributions to retrieval scores. Specifically, given a proximity distance function  X  ( Q, D ) defined on document D and query Q , we would like to compute a retrieval score  X  X djustment factor X  (de-where f is some transformation function possibly with a parame-ter. In order to obtain some guidance on designing this transfor-mation function, we follow the axiomatic retrieval framework pro-posed in [7, 8] and define the following two constraints to help us design the transformation function:
First, we would like  X  ( Q, D ) to positively contribute to the re-trieval score of a document. We thus define the following basic proximity heuristic which simply says that a smaller  X  ( Q, D ) im-plies a larger  X  ( Q, D ) .
 Constraint (proximity heuristic) Let Q be a query and D be a document in a text collection. Let D be a document generated by switching the positions of two terms in D .If  X  ( Q, D ) &gt; X  ( Q, D ) , then  X  ( Q, D ) &lt; X  ( Q, D ) .

Second, we would like the contribution from a distance measure to drop quickly when the distance value is small and become nearly constant as the distance becomes larger. The rational of this heuris-tic is the following: small distances between terms often imply strong semantic associations, thus we should reward cases where terms are really close to each other; however, when distances are large, the terms are presumably only loosely associated, thus the score contribution should not be so sensitive to the difference in distances as when the distances are small. This heuristic is formally defined as follows: Constraint (Convex curve) Let Q be a query and D 1 , D 2 D 3 be three documents that only differ in their term orders, but would otherwise be identical. That is, they have the same bag of terms, but the order of terms is different in each document. If  X  ( Q, D 1 )=  X  ( Q, D 2 )  X  1 and  X  ( Q, D 2 )=  X  ( Q, D  X  ( Q, D 1 )  X   X  ( Q, D 2 ) &gt; X  ( Q, D 2 )  X   X  ( Q, D 3
These two constraints together suggest a convex curve for  X  as shown in Figure 1. Its first derivative should be negative and its second derivative should be positive. Figure 1: Ideal shape of the proximity transformation function
Such a curve can be obtained using the following function:
In this formula, we use exp(  X   X  ( Q, D )) to map the distance val-ues to the [0 , 1] range, and then take a logarithm transformation to force the curve to satisfy the two constraints above.  X  is a parame-ter introduced here to allow for certain variations.

To test whether  X  ( Q, D ) can indeed improve a retrieval function, we combine it with the following two representative state-of-the-art retrieval formulas (i.e., the KL-divergence language model [16] and the Okapi BM25 model [23]): and obtain the following proximity-enhanced new retrieval func-tions:
Although our extension of the two formulas is purely heuristic, we believe that exploring these modifications can shed light on how to eventually obtain a unified retrieval model with more principled incorporation of the proximity component. As will be shown in Section 6, both new formulas outperform the corresponding origi-nal formulas.
We test the proposed proximity retrieval models on the data sets listed in Section 3. In each experiment, we first use the baseline model (KL-divergence or Okapi BM25) to retrieve 2,000 docu-ments for each query, and then use the proximity retrieval model to re-rank them. The top-ranked 1,000 documents for both the base-line run and the proximity run are compared in terms of their mean average precisions (MAP), which we use as our main evaluation metric.
We first examine the effectiveness of different span-based prox-imity measures with and without normalization. For both Span and MinCover, we compare the non-normalized version with the nor-malized version at different  X  values. We show some representa-tive results (Span on DOE and MinCover on WEB2g) in Figure 2. As we expected, normalized Span and MinCover are more stable and more accurate than their corresponding non-normalized ver-sions. This suggests that normalization is important for  X  X lobal X  measures in proximity modeling.
We now turn to the question whether the proximity heuristic can improve retrieval performance. We report the best retrieval perfor-mance of all the five proximity measures for both R 1 and R ble 5. The table has two parts: the upper part is the KL-divergence model and its R 1 variations and the lower part is the Okapi model and its R 2 variations. In each part, the first row shows the retrieval performance of the original model. We use  X  = 2000 [31] in the KL-divergence language model. The Okapi BM25 has three main parameters. We set k 1 =1 . 2 and k 3 =1 , 000 as suggested in [23] and tune b to be optimal. The rest rows of the table are the best performance of all proximity models achieved by varying  X  in the range [0 , 1] .Forthe R 2 variations, we fix b to the optimal value tuned based on the original (baseline) Okapi model so that we only vary one parameter  X   X  .
 significant at 0.05 level. R 1 +MinDist 0.374 0.280 0.138 0.460 0.468 R 2 +MinDist 0.418 0.300 0.166 0.456 0.502 Table 6: Pr@10 of the MinDist method over different data sets.
The results are consistent with our previous analysis of corre-lations: the two  X  X lobal X  measures (i.e., Span and MinCover) are not effective in general. Indeed, they hurt the performance in most cases. In contrast, the three  X  X ocal X  measures perform much better. They outperform the baselines in most experiments. We highlight the best values in each column in Table 5. It is very clear that the MinDist distance measure performs the best on every data set.
We do Wilcoxon sign tests on the improvement of MinDist over the baselines. Out of ten tests on five data collections, eight of them pass the test at the significant level of 0 . 05 . In particular, the p-values of the two tests on WEB2g are smaller than 0 . 0001 .Wealso observe that the improvement on FR88-89 is insignificant for both R 1 and R 2 , even though their improvements look substantial. This may be because FR88-89 only has 21 queries (Table 1. When the number of sample points is small, a statistical test tends to support the null hypothesis, since there is insufficient evidence to support the alternative hypothesis.

We also observe from Table 5 that the improvement is not consis-tent across different data collections. The improvement appears to be most substantial on WEB2g and FR. For example, the MinDist with KL-divergence only improves the MAP value from 0.2220 to 0.2265 on AP, but it can improve MAP from 0.3008 to 0.3276 on WEB2g and from 0.2442 to 0.2718 on FR. We find that both FR88-89 and WEB2g have longer documents compared with the other data sets. Thus our results seem to suggest that proximity is more useful for collections with long documents. Indeed, because the query terms tend to spread in a wider range when documents are long, we may expect proximity measures to be more discrimina-tive, thus more effective for improving retrieval accuracy.
Since it is not easy to interpret MAP values from a user X  X  per-spective, we further report the precision at 10 documents for MinDist (the best proximity measure) in Table 6, the precision at 0.1 recall level in Table 7, and the number of retrieved relevant documents at R 1 +MinDist 0.451 0.423 0.365 0.505 0.596 R 2 +MinDist 0.496 0.439 0.449 0.520 0.621 Table 7: Pr@0.1 of the MinDist method over different data sets. Table 8: Retrieved relevant documents of the MinDist method over different data sets. the cutoff 1000 (This score is indeed equivalent to the recall at the cutoff 1000) in Table 8.

We further compare our results with those from using the Markov random field (MRF) model proposed in [20], which can indirectly capture proximity to a certain extent. We set the three parameters (  X 
T ,  X  O ,  X  U ) in the MRF model to ( 0 . 8 , 0 . 1 , 0 . 1 ), as suggested in [20]. We compare the MRF results with our R 1 + MinDist results in Table 9. Interestingly, we find both methods perform very simi-larly on all five data sets, suggesting that a major reason why MRF performs well may be because it can capture proximity. It would be interesting to further analyze the connection between these two different ways of capturing proximity.
The proposed proximity model has one parameter (  X  ). We now look into the sensitivity of performance to this parameter in all the methods, especially the MinDist method. We plot the sensitivity curves of different methods on WEB2g in Figure 3. We see that  X  X lobal X  distances are all less stable and less accurate than  X  X ocal X  distances. This suggests that  X  X ocal X  proximity distances are gener-ally more effective. Moreover, MinDist is clearly the best.
To better understand parameter sensitivity of MinDist, we further plot the performance sensitivity curves with this parameter on all +MinDist and MRF.
Figure 3: Sensitivity to parameter  X  of different methods Figure 4: Parameter (  X  ) sensitivity of MinDist over different data sets five data sets in Figure 4. All curves appear to be stable, and setting  X  =0 . 3 appears to work well.
We defined two constrains and used them to guide our design of proximity model in Section 5. Here, we want to demonstrate that this selection is non-trivial. For the purpose of comparison, we combine the proximity distance measure directly with the KL-divergence model. We name this new model as R 3 : where  X   X  (  X  X  X  ,  X  ) is also a parameter to adjust the balance between the original KL-divergence score and the proximity score. Again, we use the best proximity distance measure, MinDist, for comparison. We do a very large range of exhaustive search for optimal  X  values.

The results are shown in Table 10, where we see that R 3 can hardly improve over the baseline, indicating the proximity part does not function at all. This shows that even if a proximity value is reasonable, simply adding it to a retrieval formula does not always work. In our study, we first try to develop some constraints, and then use them to guide our design of the function. As is shown in our experiment results, this axiomatic method [7, 8] can help us find an effective retrieval function.
In this paper, we systematically explored the query term proxim-ity heuristic. We proposed five different proximity distance mea-sures, each modeling proximity from a different perspective. We evaluated their correlations with document relevance and found that the two span-based measures are generally not as well correlated with relevance as the three aggregated measures based on pairwise distances, and normalization is critical for span-based measures. In particular, the MinDist proximity distance measure is found to be highly correlated with document relevance.

We further define two heuristic constraints and use them to guide us to incorporate the proposed proximity distance measures into an existing retrieval model. Experiment results on five representative TREC test collections show that while span-based proximity mea-sures cannot improve over the baseline, the pairwise distance-based measures can improve over the baseline most of the cases. The best performing measure is MinDist, which can be effectively combined with KL-divergence language model and the Okapi BM25 model to improve their retrieval performance significantly.

Our work can be extended in several directions: First, although we have found empirically that MinDist is the best among the five proximity measures proposed, further understanding of why it is the best is needed. This may also help us find even better prox-imity measures. Second, the transformation function for incorpo-rating proximity into an existing model is only one of many pos-sible choices of functions that can satisfy the two constraints. It is thus very interesting to further explore other possibly more effec-tive transformation functions. Finally, it would be very interesting to develop a unified model to combine proximity heuristic and other retrieval heuristics such as TF-IDF weighting and document length normalization.
This material is based in part upon work supported by the Na-tional Science Foundation under award number IIS-0347933. We thank Donald Metzler for helping us with testing the Markov ran-dom field model. We also thank the anonymous SIGIR 07 review-ers for their useful comments. [1] M. Beigbeder and A. Mercier. An information retrieval [2] S. Buttcher, C. Clarke, and B. Lushman. Term proximity 1 and R 3 over MinDist [3] S. Buttcher and C. L. A. Clarke. Efficiency vs. effectiveness [4] J. P. Callan. Passage-Level Evidence in Document Retrieval. [5] C. L. A. Clarke, G. V. Cormack, and F. J. Burkowski. [6] W. B. Croft and J. Lafferty. Language Modeling for [7] H. Fang, T. Tao, and C. Zhai. A formal study of information [8] H. Fang and C. Zhai. An exploration of axiomatic approaches [9] N. Fuhr. Probabilistic models in information retrieval. The [10] D. Hawking and P. Thistlewaite. Proximity operators -so [11] M. A. Hearst. Improving full-text precision on short queries [12] D. Hiemstra and W. Kraaij. Twenty-one at trec-7: Ad-hoc [13] M. Kaszkiel and J. Zobel. Effective ranking with arbitrary [14] E. M. Keen. The use of term position devices in ranked [15] E. M. Keen. Some aspects of proximity searching in text [16] J. Lafferty and C. Zhai. Document language models, query [17] J. Lafferty and C. Zhai. Probabilistic relevance models based [18] V. Lavrenko and B. Croft. Relevance-based language [19] X. Liu and W. B. Croft. Passage retrieval based on language [20] D. Metzler and W. B. Croft. A markov random field model [21] J. Ponte and W. B. Croft. A language modeling approach to [22] Y. Rasolofo and J. Savoy. Term proximity scoring for [23] S. E. Robertson, S. Walker, S. Jones, [24] G. Salton, J. Allan, and C. Buckley. Approaches to Passage [25] G. Salton and C. Buckley. Term-weighting approaches in [26] G. Salton, C. S. Yang, and C. T. Yu. A theory of term [27] F. Song and B. Croft. A general language model for [28] S. Tellex, B. Katz, J. Lin, A. Fernandes, and G. Marton. [29] H. Turtle and W. B. Croft. Evaluation of an inference [30] C. Zhai and J. Lafferty. Model-based feedback in the [31] C. Zhai and J. Lafferty. A study of smoothing methods for [32] C. Zhai and J. Lafferty. Two-stage language models for
