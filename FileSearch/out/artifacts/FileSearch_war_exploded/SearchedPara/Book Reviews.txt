
Bing Liu (University of Illinois at Chicago)
Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by Graeme Hirst, 5(1)), 2012, 167 pp; paperbound, ISBN 978-1-60845-884-4 Reviewed by Claire Cardie Cornell University
This 2012 book is written as a comprehensive introductory and survey text for sentiment analysis and opinion mining, a field of study that investigates computational techniques for analyzing text to uncover the opinions, sentiment, emotions, and evaluations ex-pressed therein. As such, it aims to be accessible to a broad audience that includes stu-dents, researchers, and practitioners, as well as to cover all important topics in the field. concise, informative examples motivate each new topic, terminology is clearly defined, and descriptions of key algorithms are provided in the running text along with short (usually one-line) descriptions of each piece of relevant related work. The latter, in particular, makes the book an excellent platform from which to dive into the quickly expanding body of literature on sentiment and opinion analysis. In addition, I believe that the book should be easily accessible to anyone with a computer science background. gree to which the book succeeds is a matter of, well, opinion. Let me explain. Liu X  X  early research was in data mining and Web mining; not surprisingly then, the book is written from this perspective. It is very much centered around the analysis of user-generated opinions in social media. Liu X  X  particular expertise is in the area of product reviews; hence, the bulk of the book X  X  examples are from this domain. Furthermore, the book focuses on techniques that are first and foremost applicable to aspect-based sentiment analysis  X  X ine-grained analysis of opinions regarding specific aspects of products and services. For the most part, investigations in this area have been restricted to reviews of electronics products (e.g., cameras), hotels, and restaurants with their associated entity-specific aspects (e.g., weight, photo quality, and ease of use for cameras; rooms, front desk service, and cleanliness for hotels; and food, service, ambience, and cost for restaurants).

Sentiment Analysis  X  X s more even-handed in its selection of topics and techniques and is written from the point of view of natural language processing (NLP) and computational linguistics. Pang and Lee, for example, are aware of prior work in the field on fact and event-based text analysis and, within that context, focus consciously on the description of  X  new challenges 1 raised by sentiment-aware applications X  as well as the methods proposed to address them. As a result, the survey proves to be an easy, comfortable (and entertaining) read for those with an NLP-centric ancestry.
 based sentiment analysis can be at odds with many of those at the core of computational linguistics and NLP. But do not despair! This is a good thing! Precisely because of Liu X  X  different tack on opinion and sentiment analysis, for many readers the book will be a wonderful source of ideas for new problems to work on in the field. In particular, the language of product reviews is quite different from that of other opinion-oriented text (e.g., editorials, blogs, position papers, political arguments, and even movie reviews).
Product reviews tend to be quite short; they describe a single, known product; the opinion expressions themselves tend to be product-specific. Other genres of opinion-oriented text are generally longer; they can discuss virtually any topic or set of topics and, hence, are likely to exhibit a greater variety of sentence and discourse structure, including a virtually unlimited (and, out of context, ambiguous) opinion expression vocabulary, the presence of opinion holders that are not the author, and implicit opinion targets.
 experiment in determining whether the techniques it covers will perform well on opinion-oriented texts beyond product reviews; if not, how and when will they fail; and in what circumstances might more complex language understanding components like parsing, semantic interpretation, or discourse analysis be helpful in analyzing product reviews? twelfth chapter). Chapter 1 introduces the problem of sentiment analysis. It discusses the differences in terminology that exist in industry vs. academia and briefly describes approximately 20 recent 2 applications-oriented sentiment analysis research efforts pub-lished largely at venues outside of NLP. The latter is a nice entree into the applied sentiment analysis literature beyond the standard NLP conferences.
 defines an opinion in terms of its components X  X he opinion holder, the entity and as-pect of that entity that is the target of the opinion, the sentiment expressed, and the time that the opinion was expressed. The remaining chapters are organized around this definition and the sentiment-based applications that it enables. Thus, there are chapters on document-level sentiment classification and rating prediction (Chapter 3), sentence-level subjectivity and sentiment classification (Chapter 4), and aspect-based sentiment analysis (Chapter 5, roughly one-quarter of the book). Following these are shorter chapters on sentiment lexicon generation, opinion summarization, the analysis of comparative opinions, opinion retrieval (vs. Web search), and determining review quality (Chapters 6, 7, 8, 9, and 11, respectively).

Liu describes techniques to identify fake product reviews X  X ome that rely primarily on the review content and available meta-data, and others based on identifying atypical behaviors of the reviewer(s).
 there are some nice surprises in other chapters including sections on handling sarcas-tic sentences, learning a priori objective terms that imply an opinion, and analyzing 512 opinions in contexts, as well as multiple sections that address cross-language and cross-domain issues.
 the quickly expanding literature on sentiment and opinion analysis, especially tech-niques for aspect-based sentiment analysis. For NLP researchers, it can also serve as a source of new problems to tackle in the analysis of opinion-oriented text.
 Reference
 Vivi Nastase 1 , Preslav Nakov 2 , Diarmuid  X  O S  X eaghdha 3 , and Stan Szpakowicz 4 (
FBK, Trento; 2 QCRI, Qatar Foundation; 3 University of Cambridge; 4 University of Ottawa) Morgan &amp; Claypool (Synthesis Lectures on Human Language Tec hnologies, edited by Graeme Hirst, volume 19), 2013, xvii+124 pp; paperbound, IS BN 978-1-60845-979-7, $40.00; e-book, ISBN 978-1-60845-980-3, $30.00 or by subsc ription Reviewed by Lucy Vanderwende Microsoft Research Understanding noun compounds is the challenge that drew me t o study computational linguistics. Think about how just two words, side by side, ev oke a whole story: cacao seeds evokes the tree on which the cacao seeds grow, and to understa nd cacao powder we need to also imagine the seeds of the cacao tree that are cru shed to powder. What conjures up these concepts of tree and grow , and seeds and crush , which are not explicitly present in the written word but are essential for our complet e understanding of the compounds? The mechanisms by which we make sense of noun comp ounds can illuminate how we understand language more generally. And b ecause the human mind is so wily as to provide interpretations even when we do not as k it to, I have always found it useful to study these phenomena of language on the co mputer, because the computer surely does not (yet) have the type of knowledge tha t must be brought to bear on the problem. If you find these phenomena equally intriguin g and puzzling, then you will find this book by Nastase, Nakov,  X  O S  X eaghdga, and Szpakowicz a wonderful summary of past research efforts and a good introduction to t he current methods for analyzing semantic relations.
 relations that can hold between what is expressed linguisti cally as nominal. Such nomi-nals include entities (e.g., Godiva, Belgium ) as well as nominals that refer to events ( culti-vation, roasting ) and nominals with complex structure ( delicious milk chocolate ). 1 In doing so, describing the different semantic relations between chocolate in the 20th century and chocolate in Belgium is within the scope of this book. This is a wise choice as there are then some linguistic cues that will help define and narrow the type s of semantic relations (e.g., the prepositions above). Noun compounds are degener ate in the sense that there are few if any overt linguistic cues as to the semantic relati ons between the nominals. that have been used in the past to describe the nominal relati ons, an overview of meth-ods for extracting semantic relations with supervision, an d an overview of methods for extracting semantic relations with little or no supervi sion. The preface promises a very brief chapter summing up the lessons learned and makes g ood on that promise as Chapter 5 is very brief indeed.
 dational types of relations: syntagmatic relations, which hold between words present in the text, and paradigmatic relations, the associations of t he text words to the broader context (i.e., to words not in the text). The syntagmatic rel ations are then introduced to us as the predicates in logic, taking one or more nominal ar guments, or as the labeled arcs that connect concepts, making the notion of sem antic relations accessible to people with differing backgrounds in computer science. Wha t follows is  X  X  menagerie of relation schemata, X  Section 2.2, where approximately ei ght different relation sets are inventoried, an admirably comprehensive overview of the pa st decades of research in this area. 2 Whereas the authors acknowledge that from the NLP perspecti ve,  X  X the aim] is to select the most useful representation for a particular application X  (page 12), no examples are provided that demonstrate the impact of select ing one representation over another. Thus, although the overview is of great value to a re ader wishing to familiarize themselves with the topic, the advanced reader will be left w ith the impression that the authors do not know how to select from among the relation sets either.
 used. This scenario is explored in detail in Chapter 3. An ove rview of the data resources available is followed by a brief discussion of the types of fe atures often employed in learning. This, and the next chapter, are clearly intended f or readers with some famil-iarity with machine learning who may not be as familiar with t he full sweep of models available. In particular, the organization of the material provides good entry points where the reader can find plentiful references if they are cur ious about the approach.
The summary sections are very good reviews and would suffice t o get a general idea of how the task of extracting semantic relations can be appro ached. In particular, in
Section 3.5, the authors provide useful points to consider f or navigating the myriad possibilities of models and resources.
 relations needs to be explored (in Chapter 4). These range fr om manually authored pat-terns (Hearst 1992, inter alia) for extracting predetermin ed relation types, to identifying novel relations with open relation extraction. OpenIE lear ns relations as expressed by verbs or prepositions using sophisticated algorithms to de termine when the verbs or prepositions express the same or different relation type (L in and Pantel 2001, inter alia).
These paradigms are well described in this chapter and, once again, the authors provide plentiful information that invites the reader to go into gre ater depth wherever their in-terests lie, with the odd exception that the authors steer th e reader away from any ap-proach that uses parsing, claiming that  X  X eeper processing (e.g., syntactic parsing) is altogether infeasible on a Web scale X  (page 79, passim). We k now that Google is pars-ing the Web already (Petrov and McDonald 2012) and parsing is becoming orders-of-magnitude faster (Canny et al., 2013),so the reader would be well advised to stay open to the possibilities. Parsing was demonstrated to be crucial f or extracting higher-order se-mantic relations such as Location and Purpose (Montemagni a nd Vanderwende 1992). 3 516 purpose is to build resources to be used by other NLP and AI app lications X  (page 80). And the application is what will determine which set of relat ions is appropriate, which will in turn determine the best approach for extracting such semantic relations, whether through supervised or unsupervised means. This book provid es a wonderfully com-prehensive overview of the choices that a practitioner make s today. I wish there was a last chapter cataloguing NLP and AI applications that use th e semantic relations, but perhaps I can look forward to that in the next edition.
 References
 Anders S X gaard University of Copenhagen Morgan &amp; Claypool (Synthesis Lectures on Human Language Tec hnologies, edited by
Graeme Hirst, volume 21), 2013, x+93 pp; paperbound, ISBN 97 8-1-60845-985-8, $40.00; e-book, ISBN 978-1-60845-986-5, $30.00 or by subscription Reviewed by George Foster National Research Council Canada
Classical machine learning makes at least two assumptions t hat are at odds with its application to natural language. First, it implicitly assu mes there are enough data. This is rarely the case in NLP, where sparse data is the norm, espec ially for intermediate tasks like parsing that require artificial labeling. Second , it assumes that all examples are drawn from the same distribution. Language is of course n ot like this: rather than being an ordered landscape, it is a wildly varying one, rich w ith strange growths and prone to sudden monstrous blooms like micro-blogging.
 cope with data sparsity, a common strategy is semi-supervis ed learning, in which a small labeled data set is augmented by a larger amount of (typ ically more abundant) unlabeled data. To cope with domain differences between tra ining and test data, adap-tation techniques can be used to mitigate training data bias by exploiting whatever is known of the test domain. The link between these two topics is that what is known of the test domain often comes in the form of an unlabeled sample, an d hence semi-supervised techniques constitute an important class of adaptation str ategies. although it also covers semi-supervised techniques withou t considering data bias, and techniques for handling bias that are not semi-supervised.
 translation (MT) researcher toward a book that cites only tw o MT papers related to semi-supervised learning or domain adaptation (Habash 2008; Dau m  X e III and Jagarlamudi 2011), neither of which is highly representative of the fair ly substantial MT work on these topics. I have done my best to apply my neural adaptatio n faculty to the material in this book; non-MT readers might find it helpful to do the sam e with this review. and NLP that occupy half of its 80 pages (exclusive of bibliog raphy). The main topics of semi-supervised learning and adaptation are then presen ted in approximately equal-sized portions, with adaptation split into two chapters cov ering techniques for known and unknown test domains. A short final chapter deals with eva luation in the presence of domain shift. machine learning problem, then emerges to motivate the core material of the book.
Faced with a domain mismatch between training and test data, we are given the option of performing semi-supervised learning with an unlabeled t est-domain sample, using either standard algorithms or specialized ones that addres s domain shift; or, if nothing is known about the test domain, using a learning approach tha t encourages robustness.
Given the focus of the book, this catalog is entirely appropr iate, but the scenarios it contemplates are special cases of a more general one in whi ch both labeled and unlabeled training data are available for a collection of do mains that may or may not include the test domain. (Incidentally, the 20-Newsgroups data used in the book for text-classification examples falls more naturally into this hete rogeneous case than into the binary train/test split to which it is cast.) This section wo uld have benefitted from an at-tempt to situate the approaches considered here within the m ore general setting, which is barely acknowledged throughout the book. Pointers to rep resentative relevant work, both in NLP (Daum  X e III 2007; Finkel and Manning 2009) and mac hine learning (Ben-David et al. 2010; Dredze, Kulesza, and Crammer 2010), would also have been an asset. the book self-contained. This is always a tricky propositio n, as it must tread a fine line between boring the expert and baffling the beginner. The author strikes a good balance here by emphasizing practical advice over theoreti cal completeness, and pro-viding experimental results to underscore various points. Although a few passages would probably cause beginners to stumble, for instance one that invokes the concept of a generative story when explaining hidden Markov models ( HMMs), with only a reference to Brown et al. (1993) for explanation, most of the material is very accessible, and there are many links into the literature. The expert read er will also not be unduly bored, and will find this chapter a quick and probably product ive read. Although it clearly does not present all basic techniques in NLP, the chapter manages to cover a lot of ground while remaining coherent and for the most part not s uperfluous with respect to the remainder of the book.
 assumptions about data (smoothness, i.i.d., coherence), a nd related empirical tests, three basic classification techniques X  X earest neighbor, naive B ayes, and perceptron X  X re described and compared experimentally on a 20-Newsgroups t ext-classification task.
Next, weighted versions of these algorithms are given (omit ting how the weights get set). Then a section on unsupervised learning presents hier archical and k -means clus-tering, along with a somewhat elliptical account of general ized EM. Finally, structured learning is introduced through HMM-based POS tagging X  X ith a nod to conditional random fields and structured perceptrons X  X s well as transit ion-and graph-based de-pendency parsing.
 on wrapper methods traces two branches of work that refine sel f-training, in which a learner labels some of the unlabeled material and then train s on its own output. One branch is based on co-training, where two learners trained o n complementary features label data for each other. This generalizes to three learner s in tri-training, and ultimately to an arbitrary number in multi-view approaches (Ganchev et al. 2008), although these latter are not mentioned in the book. The other branch requir es a learner that can assign probabilities to outcomes, and exploits this to iterate sof t labeling and training on weighted examples in an expectation maximization (EM)-lik e (or just plain EM) pro-cedure. (The section that describes this method includes  X  X  O-EM X  in the title, but the text never seems to make it to that tantalizing destination. ) Another briefly mentioned 520 wrapper-type technique is to exploit a clustering of the unl abeled data in order to generate additional features for supervised learning; thi s is related to recent work that uses neural nets to learn embeddings from unlabeled data (Co llobert et al. 2011). classification. Label propagation (Zhu and Ghahramani 2002 ) is a well-known iterative graph-based algorithm where neighbors vote on each node X  X  l abel, with votes weighted by distance. A similar voting takes place in editing and cond ensation, which are methods for identifying a subset of prototypical data point s (similar to support vectors in support vector machines) in order to speed up nearest-nei ghbor search. Unlabeled data can be used to improve this process by essentially provi ding greater resolution. by making a standard distinction between conditional and ma rginal distributions for inputs and outputs (Jiang and Zhai 2007). Here we are clearly limited to considering biased inputs, because there is only an unlabeled sample fro m the test domain. One strategy for exploiting this is to apply the semi-supervise d methods from the previous chapter, which will work as-is if the mismatch between train ing and test domains is not too great. Otherwise, we can downweight instances in the lab eled training set whose inputs are not close to those in the test-domain data; techni ques for measuring distance include LMs and KL divergence. A similar approach can be appl ied to features, by comparing the training-data values of a feature across all e xamples (ignoring labels) to its test-domain values. A more sophisticated extension o f this idea is structured correspondence learning (Blitzer, McDonald, and Pereira 2 006), which automatically places test-domain features in correspondence with traini ng-domain features. domain shifts in the absence of any prior information about t he test domain. A common effect is the out-of-vocabulary (OOV) problem, in which fea tures do not appear in a new test domain. If these have high weights, test-domain per formance can degrade badly, to the point where it would have been better to have lef t them out of training in the first place, allowing other features to take up the slack. (Note that for some versions of the OOV problem X  X uch as in MT X  X hings aren X  X  this easy.) An interesting technique for countering feature OOVs, recently introduced to NLP by t he author (S X gaard and Johannsen 2012), is adversarial learning, in which rand om subsets of features are removed during training. The chapter ends with a discuss ion of more traditional ensemble-based methods (voting, product of experts, stack ing, meta-learning) that combine predictions from a set of diverse base classifiers in order to decrease variance. mance of systems on new domains. The main, and most interesti ng, suggestion is meta-analysis, a technique widely used in fields such as medicine a nd psychology. The idea is to extrapolate from many experimental trials (typically in previously published work) that use the same method but different data, while calibrati ng for differences among the trials and making use of their internal estimates of varianc e. I am not sure this is a perfect fit with NLP for many reasons, such as that many papers do not in clude estimates of variance, and the tendency of NLP systems to be sensitive to u ndocumented  X  X inor X  configuration changes; but it is definitely a direction that b ears further investigation. an NLP system trained on a single domain for a test domain that is either unknown or represented by an unlabeled sample. When the unlabeled sa mple comes from the training domain, the task is plain semi-supervised learnin g, a case that is dealt with extensively. The book is written in an informal style, and st icks to basic machine learn-ing concepts, which are illustrated with many examples invo lving text classification,
POS tagging, and dependency parsing. In my opinion, the most interesting chapters are the shorter ones toward the end. I recommend these to any NLP r esearcher interested in the perennial problem of making do with not enough of the ri ght kind of data. production quality. Many equations and graphs are fuzzy, an d some graphs (e.g.,
Fig 3.3) are completely indecipherable due to their having b een intended for color printing. An index would have been a boon, though its absence is partly compensated for by an extensive back-referenced bibliography. Finally , the text contains many large blocks of Python code, which are sometimes used in lieu of pse udo-code for describing algorithms. Although having real code is a bonus, it is harde r to read than pseudo-code, and would have been much more useful to readers had it been mov ed out of the book and made available separately on-line.
 References
 Philipp Cimiano, Christina Unger, and John McCrae (University of Arminia Bielefeld, Germany) Morgan &amp; Claypool, Synthesis Lectures on Human Language Technologies, March 2014, 178 pages, (doi:10.2200/S00561ED1V01Y201401HLT024) , $45.00 Reviewed by Chris Biemann TU Darmstadt, Germany A book aiming to build a bridge between two fields that share the subject of research but do not share the same views necessarily puts itself in a difficult position: The authors have either to strike a fair balance at peril of dissatisfying both sides or nail their colors to the mast and cater mainly to one of two communities. For semantic processing of natural language with either NLP methods or Semantic Web approaches, the authors clearly favor the latter and propose a strictly ontology-driven interpretation of natural language. The main contribution of the book, driving semantic processing from the ground up by a formal domain-specific ontology, is elaborated in ten well-structured chapters spanning 143 pages of content. main concepts in high machine-readable detail. The first chapter sets the scene with a motivating example: Humans have suffic ient background knowledge to interpret a description of a soccer match and can grasp a lot of meaning beyond the literal content. For example, if team A scored two goals and team B eventually wins the match, team B scored at least three goals. To enable these and other kinds of inferences in machines, these need to be equipped with a domain ontology that formalizes domain knowledge and domain-specific reasoning, as well as with a mechanism to construct formal representations of natural language text. The key idea of this book is to place the ontology at the center of such an interpretation process: The domain and all its relevant semantic distinctions are defined in the ontology, thus the natural language semantic parser must only be aware of these. As opposed to generic tools such as Boxer (Bos 2008) that turn natural language into formal representations, driving the formalization of NL directly by its target ontology ensures that it can be directly consumed by further layers of interpretation, such as reasoners. T he remainder of the first chapter gives a very short summary of the state of affairs in semantic interpretation and semantic parsing in NLP. Although this summary is sufficient to highlight what is missing for draw-ing inferences on top of natural language statements, it is X  X ith a mere two pages X  necessarily incomplete and omits even mainstream approaches such as frame semantic parsing or semantic role labeling. Further, the relation of the proposal to the Semantic Web is discussed, from which formats, inter operability, and description languages are leveraged. Primers on the RDF data format and on the soccer domain complete the chapter. nition of ontologies and various ontology description languages such as OWL 2 DL, as well as their expressivity. In Chapter 3, linguistic formalisms for representing syntax and semantics of natural language are discusse d and ways to connect these to the ontology are introduced. Although, in principle, t here are many possible formalisms to serve the  X  X inguistic side, X  the authors decide on Lexicalized Tree Adjoining Grammar (LTA,
Schabes 1990) for syntactic processing. In the LTAG lexicon, each lexical entry links to all elementary trees (subtrees that specify t he valid contexts in terms of constituents) it anchors, whereas the corresponding ontol ogy grammar links each ontological concept to the projections of all lexical items that verbalize this concept. Thus, each concept is associated with an exhaustive enumerat ion of patterns it is expressed in. On the semantic level, Discourse Representation Theory (DRT; Kamp and Reyle 1993) is chosen as a formalism for semantic operations such as coreference and quantification. DRT and LTAG are subsequently paired in a representation called DUDES (Dependency-based
Underspecified Discourse Representation structures; Cimiano 2009). Here, the DRT-inspired representations are again linked to ontology concepts. Alhough the choice of framework seems largely rooted in previous works of the first author, it is clearly stated that the connection to the ontology could als o be realized for other syntactic (e.g., LFG, HPSG) and semantic (e.g., GLUE, MRS) frameworks.
 the interpretation of lexical items with respect to the target ontology. The declarative
LEMON lexicon model for ontologies, following closely the approach to the lexicon of Ontological Semantics (Nirenburg and Raskin 2004), is laid out and exemplified in great detail. In the fifth chapter, the authors describe how the ontology grammar for DUDES X  X t least, the ontology-specific p arts X  X an be generated from the lexicon.
While this might seem merely a syntactic ma nipulation as the patterns of concept manifestation have been encoded in the lexicon, the point is to keep the lexicon separate from DUDES to allow for the generation of other types of ontology grammars. Note that different domain ontologies generate dif ferent grammars that could yield different interpretations for the same sentence. In Ch apter 6, the overall interplay of previously discussed ingredients for semantic interpretation is exemplified, and challenges with respect to structural ambiguities and unde rspecification are pointed out. These are supposed to be left underspecified until they can be resolved with ontological reason-ing, which is the subject of Chapter 7. Subsequently, the formalization of time in the framework is declared at length in Chapter 8, building on time interval calculi from the literature. In Chapter 9, the application of question answering over structured data is discussed X  X ot to be confused with question answering from unstructured sources in the flavor of TREC or IBM Watson. In such a system, the natural language query is translated into a SPARQL query that can be run against public endpoints. While the ex-amples successfully illustrate the translation from DUDES representations to SPARQL queries, no qualitative or quantitative evaluation is provided despite the existence of public benchmarks.
 computational linguistics research suppose dly concentrates on domain-independent semantic representations, which is deemed  X  X rong, X  as it would not do justice to domain distinctions and would not enable the connection to domain-specific ontologies to allow for domain-specific reasoning. Regr etting that a principled use of ontologies in NLP is not widespread, the authors finally paint a vision of an ontology-based
NLP ecosystem modeled after their approa ch. While proposing the use of statisti-cal methods to overcome brittleness, lack of coverage, and the limitations of logical 320 reasoning in the presence of uncertainty, it remains entirely unclear how this could be carried out.
 cussed concepts. Also, an array of available resources including a demo of the question answering system is announced. Some of these, however, were not yet available on the companion Web site at the time of review.
 coverage X  (p. 6), I am still missing more concrete ideas on how robustness and coverage might be addressed than  X  X y using machine learning techniques X  (p. 142) without further elaboration. Thus, it is no coincidence that the implementation remains on the level of toy examples, however illustrati ng they might be. Also, while admitting to domain drift and the necessity of adaptation, it remains unclear how this adaptation, the integration of domain-specific and domain-independent processing, as well as the integration of several domains can be tractably attained. A deeper account on related approaches to computational semantics and a more recent bibliography on NLP/CL approaches could probably have avoided the impression that the interpretation of language as advocated here provides hardly more than other, same-old classic AI approaches.
 services is more Semantic Web X  X entric t han I would have expected from a book  X  X t-tempting to provide a step towards the synergy between these two fields X  (p. xv): True, it is unfortunate that these two fields intera ct as little as they do, as both are concerned with understanding the semantics of text, but the authors X  clearly voiced credo on how NLP should finally follow suit is not very helpful for creating such synergy. To bridge the gap, the Semantic Web community should probably start leveraging results and evaluation methodologies from CL/NLP ins tead of reinventing/ ignoring them, and the NLP community probably has to understand that ontological grounding and resolving entities to URIs is in fact enabling applicatio ns that go well beyond dependency parsing and word sense disambiguation. The Semantic Web community should understand that statistical methods and unsupervised acquisition are not the enemy, but the key to scalable, domain-adaptive processing of natural language, which has been known to evade total formalization since Sapier X  X   X  X ll grammars leak. X  The NLP community, especially statistical semantics, in turn, should understand the need and the demand for linking concepts to manually curated tax onomies and controlled vocabularies, as these are pervasive in industrial knowledge management. NLP should see the benefit in standardization for interoperability, and Semantic Webbers have to finally figure out that from format does not follow function.
 tion, which is to consequently drive language processing by the ontology of a target application from the very start. The exercis es, together with the online materials, make it a useful resource for teaching. Unlike a similar proposal by Nirenburg and Raskin (2004), it is constructed and exemplified with Semantic Web technology such as RDF, OWL, SPARQL, open data, standardization, reasoning, and domain ontology. For this reason, the book will supposedly be very well received in the Semantic Web community, but could be perceived controversially by the NLP community: While the idea to radi-cally align natural language processing to a target-domain ontology is reasonable, well-motivated from the application point of view, and a worthwhile contribution, the book unfortunately does not succeed in convinci ng most modern computational linguists that this approach can overcome known limit ations of previous similar rule-based and knowledge-driven approaches, such as brittleness, poor scalability, and little adaptivity. References

Judith A. Markowitz (editor) (President of J. Markowitz, Consultants, Chicago, IL) Berlin/Boston/Munich: Walter de Gruyter, 2015, hardbound, ISBN 978-1-61451-603-3; PDF, e-ISBN 978-1-61451-440-4 EPUB; e-ISBN 978-1-61451-915-7 Reviewed by Martha Evens Illinois Institute of Technology I volunteered to review this book because I found Dr. Markowitz X  X  1995 book, Using
Speech Recognition , extremely helpful when I first be came involved in medical appli-very different, but in its own way, just as useful. In fact, it is a must-read for anyone contemplating a new speech application or enhancing a current one, as well as for anyone searching for new directions in dialogue research. hard to do justice to all the papers and auth ors in a collection like this X  X ith twelve varied papers addressing many different parts of the complex problem of enabling a robot to carry on a spoken dialogue X  X ut I will try to give at least a brief taste of each. and president of Latitude Research describes an experiment in which 348 children aged 8 X 12 from six digitally-advanced countries were asked to draw and describe in words how they would like to interact with a personal robot in the future. In the second chapter
Markowitz herself reviews the language capa bilities and behavior of powerful cultural icons, such as Frankenstein, the Hebrew Gole m, the Japanese karakuri, and Pygmalion X  X  beloved statue, Galatea. This is highly rele vant, because science fiction, whether on paper or in the movies, part of traditional mythology, or local social history, has largely shaped our expectations about what new technology promises or threatens. In the third chapter, David Dufty, author of the well-known book How to Build an Android: The True
Story of Philip K. Dick X  X  Robotic Resurrection , believes that robots are intrinsically part of art and entertainment, and that is the major reason why we should build them. part, written by Bilge Mutlu and two of his students at the University of Wisconsin,
Madison, discusses the requirements for designing a robot capable of sustaining productive dialogue. They test their framework in robot-human interactions; first using a robot as a teacher, then to assess linguistic fac tors that contribute to expertise. Director of Columbia University X  X  Japanese Langua ge Program, focuses on the knowledge that any teacher must have to be effective in teaching a foreign language. Based on her series of research studies on the teaching of Japanese, her concern is that a robot (or human) teacher must understand the bond between language and culture, which she sees as inseparable. She emphasizes the importance of using human-like facial expressions in the teaching machine, at first for teaching pronunciation, for communicating emotional reactions to the student X  X  contributions, and making the student feel comfortable with the machine. In this excellent paper, ch ock full of valuable references, Nazikian unintentionally reminds us of the huge an d inexplicable divide between the world of Computer-Aided Language Learning (CALL) and the world of AI in education. Art
Graesser and his group at the University of Memphis (Link et al., 2001) have carried out a long series of experiments studying the effect of using expressive faces on the screen coordinated with the dialogue, but the work at Memphis is never mentioned in this paper and there are no references to papers in Discourse Processes or the Journal of the Learning Sciences . The third paper in Part II, written by Manfred Tscheligi and his student, Nicole Mirnig, at the Univers ity of Salzburg, focuses on the problems of Comprehension, Coherence, and Consistency. These qualities depend, they argue, on the use and transfer of mental models, which they find essential to effective dialogue with any robot capable of learning what human beings want and giving it to them.
 tasks. In Chapter 7, Jonathan Connell of IBM X  X  T. J. Watson Research Center explains how he defined these capabilities for a rob ot named ELI (the Extensible Language
Interface). First of all, such a robot needs t obeabletoacceptandsynthesizemultimodal information. It needs to recognize speech, gestures, and object manipulations and put this information together into an algorithm for getting you a cup of coffee? It has to find your kitchen, pick up the right coffee pot, heat the water in a microwave safe measuring cup, and put in just the right amount of coffee, cream, and sugar. In the process, it will need to learn new nouns (the names that you use for objects and rooms in your house) and new verbs (for activities like opening bo ttles and boiling water). Their robot parser is a finite state machine, but it can handle new names and new activities. The emphasis is on the grounding of new language in a new environment. In Chapter 8, Alan Wagner, a Senior Research Scientist at Georgia Tech, outlines what a robot has to know before you can teach it to tell lies or recognize lie s produced by others. Unlike ELI, this robot does not yet exist and I find it hard to imagine wanting to build it or even use it myself, but the description of deceptive language in this chapter has charm. Joerg Wolf and
Guido Bugmann, at the University of Plymouth in the UK, recount their experience getting university students to teach their off-the-shelf robot how to play a simple card game. They began by collecting a corpus of sessions in which one university student taught another to play the game. They used this corpus to develop the rules for their robot, grammar rules, rules for anaphora, dialogue rules, rules for dealing cards, and then they carried out a series of experiments in which university students tried to teach the robot the game. One of many problems that they had not anticipated was that when the robot did not understand what the student wanted him to do, the student raised his or her voice and tried to simplify the explanation. The result was many out-of-grammar and out-of-vocabulary errors.
 the robot can understand what you are telling it, even when several people are talking at once or machines are clanking) and the other about how design factors, such as robot voices and gesture speed affect memory and engagement in both children and adults. The experts in audition, Franc  X ois Grondin and Franc  X ois Michaud from the Robotics
Laboratory at the Canadian Universit  X  e de Sherbrooke, have developed algorithms for localization (figuring out where a sound is c oming from), tracking (following sounds as people or robots move), and separation (extracting sounds from a given person or ma-chine when several are making noises at once). They have achieved remarkable success in a number of experiments run on a system with a bank of separate microphones and 324 parallel hardware capable of processing all this information at once. Sandra Okita, then at Stanford, now at Columbia University, and Victor Ng-Thow-Hing from the Honda
Research Institute USA in Mountain View teamed up to carry out a series of experiments with robot voices. The objective of their studies was to determine how design choices like robot voices and gesture speed effect how humans of all ages respond to a robot.
They tried out two voices with young children, one robot-like (monotone) and one more human-like. Both robots followed exactly th e same script, but the children remembered much more of what the humanoid voice said and they were much more likely to be willing to talk to that robot again. Teenagers preferred the humanoid voice, but there was much less difference between the two groups interacting with the different voices.
Adults seemed to be affected even less. Similar experiments with gesture speed showed that robots who gestured rapidly were judged to be happier and more pleasant to spend time with. Again, small children were affected the most, but teenagers also had strong preferences here. Again, adults were affected less.
 many people believed Reeves and Nass X  X  (1996) Media Equation, which is the claim that people interact with computers in the same way that they interact with people; and decided that they could substitute research on human interaction for research on interactions between people and machines. Our own experiments (Bhatt, Argamon, and Evens, 2004) have convinced us that the Media Equation is certainly not true for the students interacting with our intelligent tutoring system, who are constantly polite to human tutors, whether their professors or their peers, but often very rude to our computer tutor.
 puter Science at the University of Sheffield and the Editor-in-Chief of Computer Speech and Language, takes us back to the future where the first chapter began. He argues that we have a long way to go to meet the goal of intelligent communication with machines.
We need to build robots that understand human behavior and are capable of generating the language necessary to change it.
 opinion and approach separating the exper ts featured in this book. It will never be enough to simply tack on a multi-purpose language module to an existing robot, he argues. Spoken communication is a fundamental and integral part of ordinary human beings; and if we are to succeed in our goal of constructing  X  X ntelligent communicative machines, X  we need to make the communication function a central part of the machine design and not simply an add-on function.
 back in the third chapter (page 55). Dufty quotes Sylvia Solon, deputy editor of Wired in the UK, as saying,  X  X here is no point making robots look and act like humans. X  Dufty adds that Martin Robbins, who blogs for the Guardian ,underthename X  X heLay
Scientist, X  wrote:  X  X umanoid robots are seen as the future, but for almost any specific task you can think of, a focused simple design will work better. X  Of course, Dufty goes on to tell us about the use of robots in art and entertainment and about the workings of the Philip K. Dick android, so he may not agree entirely with Solon and Robbins. other chapters (4, 7, and 9). In Chapter 4, Mutlu X  X  group at the University of Wisconsin describe the simple semantic grammar used by their robot (a Wakamaru), along with more sophisticated dialogue and behavior models and algorithms to control gaze and gestures. Jonathan Connell begins Chapter 7 with  X  X uppose you buy a general fetch-and-carry robot from Sears and take it home X  and continues in this vein. In
Chapter 9, Wolf and Bugmann designed thei r system to operate on components made of production rules, but they still have a lot to teach us.
 thoughtful experiments with existing robots now, to help us discover the problems, but
I believe that really satisfactory solutions will require much more research. I cannot agree with Moore, however, when he argues that it is unethical to attempt to imitate human communication by stitching togethe r the limited technologies we have today.
It seems to me that much of what we now know about human communication comes from attempting to do just that.
 collections edited by Markowitz and published by Springer in 2013. Both of these were edited by Neustein &amp; Markowitz jointly. Dr. Amy Neustein is founder and CEO of Lin-guistic Technology Systems and Editor-in-Chief of the International Journal of Speech
Technology. The first is entitled Mobile Speech and Advanced Natural Language Solu-tions (hardbound ISBN 978-1-4614-6017-6; e-Book ISBN 978-1-4614-6018-3); the second,
Where Humans Meet Machines: Innovative Solutions for Knotty Natural Language Problems (ISBN 978-1-4614-6933-9, eBook ISBN 978-1-4614-6934-6).
 References

S  X  ebastien Harispe, Sylvie Ranwez, Stefan Janaqi, and Jacky Montmain (  X  Ecole des Mines d X  X l` es -LGI2P) Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 27), 2015, xv+238 pp; paperback, ISBN 978-1-62705-446-1; e-book, ISBN 978-1-62705-447-8; doi:10.2200/S00639ED1V01Y201504HLT027, $75 Reviewed by Deyi Xiong Soochow University
Learning semantic similarity for units of language or concepts is crucial not only for numerous tasks in computational linguistics, but also for language understanding and reasoning in the broad context of artificial intelligence. With growing interests and efforts in modeling and computing semantic measures in recent years, we have wit-nessed much progress in the following two strands of research that approach semantic similarity: corpus-based statistical meth ods and knowledge-enhanced methods with human knowledge defined in ontologies. This book by Harispe, Ranwez, Janaqi, and
Montmain provides a detailed introduction to state-of-the-art research in these two lines of work. the motivation, notion, and classification of semantic measures, corpus-based and knowledge-based methodologies in semantic m easure modeling, evaluations, data sets, tools, challenges, and future directions.
 highlights the importance of semantic measures from a broad perspective of artificial intelligence and provides the mathematical a nd cognitive foundation of semantic mea-sures. In order to give general definitions of semantic measures, including semantic relatedness and semantic similarity, related concepts such as semantic proxy and dif-ferences from traditional distance functions are introduced in Chapter 1. This chapter also provides a short description of applications of semantic measures in various fields (natural language processing, information retrieval, semantic web, linked data, bio-medical informatics, etc.). It is very useful f or the reader that the chapter distinguishes different types of semantic measures (e.g., semantic relatedness/unrelatedness, seman-tic similarity/dissimilarity, semantic/ta xonomic distance) with a graph showing their differences and relations (e.g., semantic s imilarity is a part of semantic relatedness).
A general profile that demonstrates the landscape of different semantic measures is offered at the end of this chapter.
 defined at the word level. In order to develop a conceptual understanding of these measures, this chapter first presents a general pipeline normally used for defining such measures, which consists of five steps: colle cting a corpus of texts as the semantic proxy, extracting the vocabulary, building a raw semantic model, transforming the raw model into a refined model, and finally computing semantic similarity based on word repre-sentations from the refined model. Subsequen tly, it discusses notions of word meaning, context, and distributional semantics that are essential elements for the definition of semantic measures, though there are some debates on these notions. A variety of distri-butional models based on the distributional hypothesis are then described with details on both the strategies (e.g., frequency we ighting and dimension reduction) used to build these models and the approaches used to learn word representations and estimate semantic similarity of word representations, which are classified into the geometric approach (e.g., Latent Semantic Analysis), the set-based approach (e.g., Dice index), and the probabilistic approach (e.g., Pointwise Mutual Information). Finally, the chapter concludes with advantages and limits of corpus-based semantic measures.
 sures are not extensively and systematically surveyed and discussed in the literature.
Chapter 3, the longest chapter in the book, is dedicated to filling this gap. This chapter starts with a general introduction to onto logies that can be built on graph representa-tions as well as graph-based ontology proce ssing technologies that can be used to calcu-late ontology-based semantic measures. It pr esents two different types of knowledge-based semantic measures, namely, semantic measures on cyclic and acyclic semantic graphs, with details on methods defining these measures, such as the shortest path and random walk approach. The chapter proceeds with the description of the core elements of semantic measures: different kinds of sema ntic evidence in semantic graph including the depth/width of a taxonomy, and conce pt specificity. It also elaborates on two particular ontology-based semantic measure s: concept-to-concept semantic similarity and its extension groupwise concept similari ty, listing papers where these measures are proposed and defined. Other knowledge-based measures, such as logic-based measures and measures defined on multiple ontologies, are also briefly introduced. Similar to
Chapter 2, this chapter discusses the advantages and limits of knowledge-based seman-tic measures. Finally, a very useful and interesting section of this chapter is Section 3.8, which introduces recent efforts in hybrid measures combining both knowledge-based and corpus-based semantic measures.
 knowledge-based semantic measures presented in the preceding two chapters. It dis-cusses two important topics, objective evalu ation and task-oriented measure selection, and presents a variety of evaluation criteri a for semantic measures (e.g., accuracy, precision, and computational complexity). In trinsic and extrinsic evaluation strategies are also introduced. Perhaps the strongest part of this chapter is the extensive treatment of data sets that are widely used to evaluate semantic measures in the literature (e.g., WordSim353 for word relatedness, TOEFL for word similarity).
 sures, and offers several suggestions for future research, for example, providing generic data sets and tools, formalizing and standardizing ontology-to-semantic-graph trans-formation, and promoting interdisciplinar y studies among cognitive sciences, logic, natural language processing, and so on. There are four appendixes in the book, which provide examples, specific introduction to i mportant algorithms, and useful tools and open-source software tools for both the computation and analysis of corpus-based and knowledge-based semantic measures.
 semantic measures from natural language where both corpus-based and knowledge-based semantic measures are clearly presented. The book will serve as a good reference for graduate students who are eager to enter this area and natural language processing 830 researchers and practitioners who want to quickly find a suitable measure for their tasks. Not only does it provide technical details of state-of-the-art semantic measure systems and algorithms, but it also offers a very useful list of tools and open-source software tools for modeling and computing various semantic measures and a list of widely used data sets for semantic measure evaluation.
 fastidious reader. First, the book title appe ars not to be fully consistent with the content covered by the book, since semantic similarity is only one particular semantic measure and the book discusses more. Second, although the introductory chapter gives an introduction to applications of semantic measures in various fields, I still think that this introduction deserves a complete chapter rather than a section, because in the current form it is not sufficient for NLP beginners who are going to use semantic measures.
Finally, I am a little disappointed that the very recent research efforts in computing semantic similarity based on word, phrase, or even sentence embeddings learned with neural networks are not covered in Chapter 2 (corpus-based semantic measures). As such, I would like to encourage and welcom e a new edition of this book to discuss these topics.


Sergei Nirenburg and Victor Raskin (University of Maryland, Baltimore County, and Purdue University)
Cambridge, MA: The MIT Press, 2004, xxi+420 pp; hardbound, ISBN 0-262-14086-1, $50.00, U 32.95 Reviewed by John F. Sowa
In this book, Nirenburg and Raskin present an important body of work in computational linguistics that they and their colleagues have been developing over the past 20 years. For a unifying perspective, they organize their assumptions, theories, and techniques around the theme of ontological semantics . Along the way, they critique many alternative views of semantics, which they distinguish from their own.
Their analyses contribute to a much-needed debate about the history and future of computational linguistics, but to preserve some balance, teachers and students should keep a few of the alternatives on their reference shelf.
 first part consists of an introductory chapter 1 and four chapters that survey important but controversial issues about linguistics, both theoretical and computational. In those chapters, the authors make a good case for their version of ontological semantics, but the alternatives are not treated in detail. In part II, the authors present their text-meaning representation (TMR) and demonstrate how it is used in language analysis. Any dis-cussion of technical material must use some notation, and TMR is sufficiently flexible to illustrate a wide range of semantic-based methods that could be adapted to many other formalisms. For most readers, part II would be the more important.
 emphasis on semantics. Students and novices, however, need examples, and none are given until chapter 6. The authors suggest that  X  X  X  well-prepared and/or uninterested reader X  X  skip the remainder of part I and go straight to chapter 6, which begins with an excellent five-page example. The authors follow that advice when they teach courses from this text.

Linguistics. X  X  Their ideas are well taken, and some are as old as Socrates: Examine the assumptions, challenge conventional wisdom, and test conclusions against experience.
The basis of their approach is what they call the four components of a scientific theory:
Under various names and with varying definitions, similar components are present in most theories about theories. The authors X  claims of novelty in proposing them  X  X  X urprisingly, for the first time in the philosophy of science X  X  are overstated. ject and the nature of the subject matter: Theories in mathematics, engineering, and the empirical sciences are very different in kind and methods of justifica-tion. Since computational linguistics is primarily an engineering discipline, it uses theories from mathematics and the sciences, and it helps test and develop them. a problem within the limits of budgets and deadlines. The authors spend too much time arguing against engineering goals that are different from their own.
For some applications, such as machine translation, an analysis of truth condi-tions may be unnecessary. For other applications, such as translating an English question into a database query, truth conditions are the focus of the task. Instead of recognizing that different engineers have different goals, they have tried to banish truth conditions from linguistics. The following passage indicates a serious misunderstanding:  X  X irst, we maintain that reference is relevant for the study of coreference and anaphora ... relations in text. Second, while we agree that truth plays no role in the speaker X  X  processing (page 109)
First, logicians do not use truth values to anchor language in reality; they use references, which are resolved to entities (objects, properties, and events) in some situation. Second, truth values are not primary, but derived from the mapping of configuration of references and relations conforms to the extralinguistic configura-tion of entities. Third, every logician from Aristotle to the present has insisted that an ontology of every general term is essential to determine the correct mapping from language to reality. Aristotle himself never used the word ontology , even though he created the subject; logicians are more likely to use the words theory , axiomatization , and conceptualization as synonyms for what Nirenburg and Raskin call an ontology.
 that the word semantics was not coined until the end of the 19th century. The subject matter, however, was established by Aristotle in the books Categories , On
Interpretation , Analytics , Rhetoric , and Poetics . Under the name of logic or theory of signs, the subject was thoroughly developed by the Hellenistic and medieval philosophers. Most books on logic before the 20th century devoted at least half their 148 text to conceptual analysis and ontology. The truncated view of history ignores 2,000 years of research: ontological semantics for MT was Margaret Masterman (1961), a former student of
Wittgenstein X  X . She organized her ontology as a lattice defined in terms of 100 primitive concepts, which Wilks adopted as a basis for preference semantics. Hutchins (1986) showed that her MT system did a better job of word selection than purely syntactic systems of that time. Appropriately, her first publication on the subject was in the Proceedings of the Aristotelian Society . Another pioneer was Silvio Ceccato (1961), who based his correlational nets on a selection of 56 relation types, which included case relations, type X  X ubtype, type X  X nstance, part X  X hole, and miscellaneous logical, numerical, causal, spatial, and temporal relations. In parsing, Ceccato built dependency trees, which he  X  X  X orrelated X  X  with predefined nets to resolve ambiguities; in generation, he used the nets to guide word selection. The single most influential collection of the early work in these two fields, edited by Minsky (1968), included classic papers by McCarthy, Quillian, Bobrow, and Raphael, among others.
 citations of authors who contributed to the field. Unfortunately, it has very few examples comparing the ways different authors would analyze similar phenomena.
Nirenburg and Raskin cite seven authors in the Russian meaning X  X ext school but don X  X  give a single example to show how a meaning X  X ext analysis would differ from their own text-meaning analysis. Throughout the chapter, they discuss Pustejovsky X  X  generative lexicon but never illustrate the arguments with examples. They consider Pustejovsky  X  X  X s a representationalist, antiformalist ally, X  X  but they never explain why they consider lexical semantics incompatible with formal semantics. That is especially odd, since the next chapter positions  X  X  X ntological semantics within the field of formal ontology. X  X  latest hope for conferring interoperability on incompatible systems. Most of that work, however, has not been adapted to natural language processing. Work on lexical resources, such as WordNet, is only loosely connected to work on formal ontology.
Section 5.3 discusses  X  X  X he difficult and underexplored part of formal ontology, namely, the relations between ontology and natural language. X  X  The most difficult problem, which the proponents of formal ontology fail to address, is the nature of ambiguities in natural languages. A good parser can enumerate syntactic ambiguities, and selectional constraints are usually sufficient to resolve most of them. The most serious ambiguities are subtle variations in word senses (sometimes called microsenses ), which change over time with variations in word usage or in the subject matter to which the words are applied. Such variations inevitably occur among independently developed systems and Web sites, and attempts to legislate a single definition will not stop the growth and shift of meaning. From their long experience with NL processing, Nirenburg and
Raskin probably have a deeper understanding of the nature of ambiguity than the proponents of the Semantic Web. Section 5.4 is a wish list of features from formal ontology that NL processors would need. Providing them is still a major research problem.
 examples that illustrate the mapping from English to TMR. Section 6.1 begins with the sample sentence Dresser Industries said it expects that major capital expenditure for expansion of U.S. manufacturing capacity will reduce imports from Japan . The next five pages carry out an informal analysis of that sentence without introducing any special notation, not even TMR. Then section 6.2 introduces TMR and shows how the results of the analysis in section 6.1 are mapped into it. The remaining sections of chapter 6 discuss the fine points of using TMR and compare them to other computational and theoretical techniques.
 binary relations that link the head to a frame, a pointer to another frame, a simple value, or a more complex combination for defaults, semantic types, relaxable types, etc. Each TMR is a set of six kinds of frames: one or more propositions , zero or more discourse relations , zero or more modalities , one style , zero or more references , and one TMR time . The kinds of frames are illustrated with numerous examples discussed throughout chapters 6, 7, 8, and 9. Unfortunately, there is no appendix or other reference section that gives a complete grammar or table of all the options for a well-formed TMR. From the examples, one can surmise that the head of each proposition frame is a concept instance that represents a state or event, which is linked by case roles to the participants. In the middle of chapter 7 is a table of nine case roles; at the end of chapter 8 is a list of five types of discourse relations, each of which may have several subtypes. The authors acknowledge that TMR has been evolving over the years, but a complete list of options for one version would be appreciated. 150 lexicon, and onomasticon. The subdivision into four sections is uneven: The fact database is described in three pages, and the onomasticon in half a page, but the ontology and lexicon sections take 36 pages and 15 pages, respectively. The discussion of inheritance (a description logic with defaults) should be in a separate section, and some material belongs in an appendix: the table of case roles, the list of 34 axioms that define constraints on the Mikrokosmos ontology, and the description of the software for browsing the knowledge sources. The question of what information to put in the onomasticon, the lexicon, or the ontology raises some troublesome issues: Toyota , for example, is in the onomasticon because it is the name of an instance of type corporation, but Toyota Corolla is in the ontology because it is a type of car, which can have many instances.
 text analysis. Section 8.1 presents the stages of tokenization, morphology, lexical lookup, and syntactic analysis. Section 8.2 covers the construction of dependency structures for propositions, which includes matching selectional restrictions and relaxing them for sentences such as The gorilla makes tools . Sections 8.3 and 8.4 cover problems of ambiguity, nonliteral language, and the inevitable exceptions. Section 8.5 treats time, aspect, and modality. Section 8.6 handles discourse: reference and coreference, discourse relations, and the temporal ordering of the propositions. This is a good chapter, but one might like to see some discussion of alternative methods of parsing and semantic interpretation. It would also be interesting to see a step-by-step processing of the sample sentence that was analyzed by hand in section 6.1.
 knowledge sources discussed in chapter 7. This is a universal problem that everybody involved with NL processing has to face, and nobody working in the field is completely satisfied with the available resources. In this chapter, the authors focus on the methods they have used in developing the Mikrokosmos ontology and associated lexicon, but they discuss issues involved in adopting and adapting resources such as WordNet and machine-readable dictionaries. They try to take an ideal scientific stance toward the subject, but most readers are likely to adopt a mixed strategy of adapting whatever resources they are given or are likely to find on the Internet. As a fact database, many readers are likely to be given, in advance, a conventional relational database, and the authors should discuss the issues of incorporating such resources.
 of detail on applications and processing. Earlier in the book they say that the kinds of applications for which TMR has been used  X  X  X nclude machine translation, information extraction (IE), question answering (QA), general human-computer dialog systems, text summarization, and specialized applications combining some or all of the above. X  X  A couple of more chapters on language generation and reasoning would have been more useful than most of the five chapters of part I. For students, a glossary would be especially welcome, since the authors frequently mention a word such as defeasible and follow it with a parenthetical list of citations instead of a definition.
 are forced to use less sophisticated tools for routine chores. The index contains five references to I. J. Good, who had not been working on computational linguistics, but only one reference leads to Good X  X  publications and four lead to capitalized occurrences of the word good . This lapse is even more embarrassing for Good

Michael F. McTear (University of Ulster)
London: Springer-Verlag, 2004, x+432 pp; paperbound, ISBN 1-85233-672-2, $59.95 Reviewed by Johannes Pittermann University of Ulm
What would you say if your refrigerator told you,  X  X ou X  X e having some friends round for hot chocolate later. Maybe you should order two cartons of milk X ? Of course, in
Spoken Dialogue Technology , Michael McTear will not give an answer to the question of whether talking to domestic appliances makes sense, but he indicates that even a normal household, for instance, may offer a wide field of application for spoken-language dialogue systems in the near future. Consequently his book primarily focuses on theory and practice of these systems.
 practitioners in human-computer interfaces, the book is subdivided into three parts which meet the readers X  needs:  X  X ackground to Spoken Dialogue Technology X  (Chap-ters 1 X  5),  X  X eveloping Spoken Dialogue Applications X  (Chapters 6  X 11), and  X  X d-vanced Applications X  (Chapters 12 X 14).

Dialogue Applications: Research Directions and Commercial Deployment, X  present recent products and aspects of dialogue technology as well as historical linguistic and artificial intelligence approaches to dialogue and simulated conversation. Aspects of present-day commercial use of spoken dialogue technology are also discussed. In
Chapter 3,  X  X nderstanding Dialogue, X  the term dialogue is defined, and four of its key characteristics X  X ialogue as discourse, dialogue as purposeful activity, dialogue as collaborative activity, and utterances in dialogue X  X nd its structures and processes are described in detail. Chapter 4 gives an overview of the components of a spoken language dialogue system: speech recognition, language understanding, language generation, and text-to-speech synthesis. The central component (i.e., dialogue man-agement) is specified in Chapter 5. Here, dialogue initiative (system initiative, user initiative, and mixed initiative), dialogue control (finite-state-based, frame-based, and agent-based control), and grounding (how to process the user X  X  input) are described.
Furthermore, knowledge sources (dialogue history, task record, world knowledge model, domain model, generic model, and user model) and problems that arise when interacting with an external knowledge source are discussed.
 into analysis and specification of requirements, design, implementation, testing, and evaluation of a dialogue system. The use-case analysis includes user profile (type input/output device type, environment, etc.). The spoken-language requirements can be analyzed with the help of existing corpora or simulations using the Wizard of Oz method. In the requirements specification, the developer defines what the system is intended to do. How these specifications are achieved is defined in the design phase.
This comprehends the dialogue flow, prompts, grammars, interaction style, navigation, help, confirmation, etc. For the implementation, McTear describes the CSLU Toolkit and its Rapid Application Developer (RAD), VoiceXML platforms, and platforms for multimodal Web-based applications like Microsoft X  X  SALT (Speech Application
Language Tags), some of which are dealt with in the subsequent tutorial chap-ters. In the testing and evaluation sections, several test and evaluation methods are outlined. Whereas testing is necessary to determine whether the system conforms to the specifications, the evaluation phase comprises the analysis of user acceptance and the analysis of the system performance which can be accomplished, for example, by using the PARADISE tool.
 and implement a dialogue system. Chapter 7,  X  X eveloping a Spoken Dialogue System
Using the CSLU Toolkit, X  deals with the development of spoken dialogue systems with the help of RAD. This chapter starts with the basic functions instancing the pizza application, the development of basic functions and subdialogues, digit recognition, tone input from telephone keypads, and alpha-digit recognition. Later on, grammars are created, and speech output, the use of TCL, and connection to a database are described. In Chapter 8, the CSLU Toolkit is used to develop a multimodal dialogue system.
This includes the design of an animated character and a login dialogue as well as the consideration of emotions. The dialogue development using VoiceXML is shown in
Chapter 9 and Chapter 10. On the basis of the tutorial, an introduction to VoiceXML is given and the reader learns how to integrate prompts, responses, verification, subdialogues, and tone/digit/alpha-digit recognition in a dialogue system. Moreover, the application of mixed initiative, the form interpretation algorithm (FIA), recognition grammars, variables, and Web server applications are dealt with. All the tutorials contain a detailed explanation of the respective topic, one or more examples for illustra-tion, and, of course, multiple exercises based on these examples and explanations. The second part concludes with a detailed overview of XHTML+Voice and SALT, both of which are designed to enable multimodal access to Web-based services. Several exam-ples of how to develop an XHTML+Voice X  X ased application using the IBM Multimodal Toolkit and how to develop SALT-based systems with the help of the Microsoft .NET Speech SDK (Version 2.0 beta) are illustrated in Chapter 11.

Chapter 12 describes advanced dialogue systems that involve more-complex tasks and provide sophisticated means to control the human-machine interaction; these are, for example, the DARPA Communicator, the TRAINS/TRIPS system, and the Con-versational Architecture Project at Microsoft. Research topics in spoken-dialogue tech-nology are discussed in Chapter 13. These topics include the information-state theory, error handling, adaptive dialogue systems, and the optimization of dialogue strate-gies. Chapter 14 shows future prospects of dialogue technology, especially multimodal systems.
 students who want to become familiar with spoken dialogue systems and a good ref-erence manual for practitioners and researchers. The book is attractively presented and structured in a well-thought-out way. It is clearly written and easily understandable.
The large number of examples given in the text introduce some variety to the topics. In addition to the standard references at the end of each chapter, Chapters 1 X  6 conclude 404 with an exercises section, in which the interested reader can delve into the topic. These exercises either contain questions about the content of the chapter or involve Web sites that demonstrate state-of-the-art technology. Although all the Web sites mentioned in these sections exist at the moment, it would have been good if the author or the publish-ing house had set up a central Web site for the book that would be updated constantly, taking into account changes to the referenced Web sites. Chapters 7 X 11 provide many tutorials, exercises, and examples that offer a big playing field for exploring the features and functionality of the respective toolkits and architectures. Instructions on how to obtain and install the necessary toolkits and software are given in the appendices. Some readers may miss a detailed list of abbreviations; however, most of the terms appear in the index.
 say:  X  X ou possess more than $59.95. Wouldn X  X  you like to buy Spoken Dialogue Technology by Michael F. McTear? X 

John Coleman (University of Oxford)
Cambridge University Press (Cambridge introductions to language and linguistics), 2005, xi+301 pp; hardbound, ISBN 0-521-82365-X, $90.00; paperbound, ISBN 0-521-53069-5, $39.99 Reviewed by Mary Harper Purdue University
In October 2003, a group of multidisciplinary researchers convened at the Symposium on Next Generation Automatic Speech Recognition (ASR) to consider new directions in building ASR systems (Lee 2003). Although the workshop X  X  goal of  X  X ntegrating multi-disciplinary sources of knowledge, from acoustics, speech, linguistics, cognitive science, signal processing, human computer interaction, and computer science, into every stage of ASR component and system design X  is an important goal, there remains a divide among these communities that can only be addressed through the educational process.
The book Introducing Speech and Language Processing by John Coleman represents a bold effort to educate students in speech science about some of the important methods used in speech and natural language processing (NLP). This book represents an important first step for forging effective collaborations with the speech and language processing communities.
 short textbook in speech and natural language processing for beginners with little or no previous experience of computer programming X  (page 2). Coleman targets the book at students in a variety of disciplines, including arts, humanities, linguistics, psychology, and speech science, as well as early science and engineering students who want a glimpse into natural language and speech processing. However, since it assumes prior knowledge of basic linguistics, the text is likely to be less accessible to traditional science and engineering students. Coleman X  X  motivation for writing this book is that the currently available textbooks in NLP and speech require knowledge that students from more of a humanities background would not have (e.g., programming, signal processing). The author also astutely points out that there tends to be a divide between the areas of signal processing and computational linguistics, although in recent years with ubiquity of statistical modeling and machine learning techniques in both areas, this divide is becoming much smaller. The author X  X  motivation for this book is excellent:  X  X  refusal to let the old sociological divide between arts and sciences stand in the way of a new wave of spoken language researchers with a foot in both camps X  (page 4). ing, along with computer programs implementing many of them in either C or Prolog, and it capitalizes on Coleman X  X  insights from courses offered to graduate linguistics students. It comes with a companion CD containing software needed to compile and/or execute the programs in the book, as well as source code for all of the described implementations. The readme file on the CD contains helpful installation notes, while the text describes how to compile and use each of the programs. Chapter 1 contains a comprehensive list of topics that are covered from  X  X irst principles, X  provides de-tails about the computational environment that is needed to compile and execute the programs provided on the CD, and a listing of computer skills one would need to get started. Coleman encourages the reader/student (I will use student henceforth) not just to run the programs but to also to  X  X inker X  with them in order to gain a deeper understanding of the way they work. Chapter 1 also lays out the structure of the text graphically in order to depict the dependencies among the chapters. In addition to the book chapters, there is an appendix on ASCII characters, a helpful glossary, a list of references, and a comprehensive index. Importantly, there is also a companion website with errata, solutions to selected exercises, bug reports, software updates, additional programs, links to third-party software, and some nice bibliography links. Presumably, this page will be updated over time.
 a preview and a list of key terms (allowing the student an opportunity to look up the definitions prior to beginning to read the chapter content) and ends with a chapter summary, a set of exercises that are helpful for developing a deeper understanding of the materials discussed in the chapter, suggestions for further reading, and suggestions for readings to prepare for the next chapter. I will discuss chapters 2 through 9 in turn. focus on the composition of a sound file and how such a file can be loaded into a sound-editing program for audio display. The chapter starts off by guiding the student through the process of listening to a cosine waveform and then viewing the same file using a sound editing program such as Cool Edit 2000. The student is asked to fill in a worksheet with values for a cosine function and then plot the values. Coleman then presents important information on the digital representation of sound and on sampling theory.
Given this knowledge, the student is walked through the process of generating and playing a cosine wave. The chapter contains a just-in-time introduction to C sufficient for a student to read and comprehend the cosine wave generation program coswave.c .
Various computing terms (e.g., bit, compilation, machine code) are defined, followed by a discussion of C numeric data types and differences in representation across ar-chitecture. The C code presented in this chapter makes concrete Coleman X  X  discussion of loops, arrays, calculation of mathematical expressions, overall program layout, and file output. The chapter ends with several helpful exercises. The first provides a very detailed set of instructions for compiling and executing the coswave program and then playing the generated output signal in Cool Edit 2000. It should be noted that Cool Edit 2000 is not a public-domain package and is no longer available through the original developers. Alternatives mentioned on the text X  X  Web site (e.g., wavesurfer, Praat) can be used instead, although no details are offered about using them for the exercises.
Students may face some challenges in opening and playing raw data files with these alternatives.
 particular, the concept of filtering is introduced, followed by a very brief discussion of how filters are employed in a Klatt formant synthesizer. The chapter first discusses how operations can be applied to number sequences in C to set the stage for discussion of several speech-processing applications. RMS energy is then defined and a correspond-ing C program is discussed in detail. Next, a moving-average program is presented as an example of a low-pass filter. The concept of recursion is next introduced in order to pave the way for a discussion of IIR (Infinite Impulse Response) filters. High-, low-, and band-pass filters are defined and tables of coefficients for various filters are provided.
An implementation of an IIR filter is discussed quite briefly; here the author relies on the 138 fact that there is similarity to the earlier moving-average program. Finally, after the basic introduction to filters, the Klatt synthesizer is discussed and a schematic diagram for the system is presented together with a brief discussion of the control parameters that are used to synthesize sound. IIR filters are tied in because they are used for modeling the glottal wave and filter-specific frequency components in order to obtain the resonant frequencies of the vocal tract required for the sound to be synthesized. A consonant X  vowel example is used to demonstrate the synthesizer in action. There is a series of three exercises at the end of the chapter that should help the student get a better sense of filters and the type of sound generated by the Klatt synthesizer. The synthesizer exercises have a cookbook feel to them and give only a glimpse of what is needed to actually synthesize speech. At the end of the chapter, no further readings on filters are provided, although readings are recommended for the Klatt synthesizer and methods for estimating its parameters.
 signal. First up is the fast Fourier transform (FFT), for which a C implementation is presented and described in detail. The student is asked to apply the compiled code to an example speech file in order to generate its spectrum, which is then plotted in
Excel or Matlab for comparison to the spectral analysis obtained using Cool Edit. Given this example, there is a discussion of the types of peaks found in the spectrum, the resonances of the vocal tract, and the harmonics, as a prelude to the discussion of cepstral analysis. Coleman first provides a high-level discussion of cepstral analysis, which employs an inverse FFT, followed by the discussion of its C implementation and an example using the executable. Cepstral analysis is then used to build a rudimentary pitch tracker, which is applied to a speech example. This leads to the discussion of a voicing detection algorithm. Next, the autocorrelation method for pitch tracking is presented together with its C implementation. Finally, the chapter discusses linear predictive coding (LPC) and various applications. The chapter ends with exercises to compare the cepstral and autocorrelation pitch trackers, to modify the output of the LPC program, and to analyze the vowels in a speech sample and use the LPC spectrum to estimate their formants. Additional readings are provided on the algorithms presented in this chapter.
 (FSMs) with a focus initially on symbolic language applications. There is a shift from
C to Prolog, although it would have been perhaps more coherent to stick with C. The discussion of the peculiarities of Prolog could be distracting to a novice programmer.
Furthermore, the representation of an FSM in Prolog is tedious to read, and it may be difficult for the uninitiated to observe correspondences between the Prolog code and depictions of corresponding models. Simple examples are used to introduce the concept of, rather than a formal definition of, FSMs. Issues of coverage, over-generation, determinism, and nondeterminism of an FSM are discussed briefly. Although Coleman makes clear that backtracking is an issue for a nondeterministic FSM and notes that there are methods for converting such an FSM to a representationally equivalent deter-ministic form, existing tools that could be used for carrying out this conversion (e.g., the
AT&amp;T FSM library) are not discussed. Coleman next presents a Prolog implementation of an interesting English-like monosyllable FSM. A box is used to introduce a collection of facts about Prolog, and then there is a walk-through of the code. A nice set of exercises follows in which the student loads the FSM program and executes it in various ways, followed by a discussion of some examples of using the FSM to generate strings with particular constraints. A more formal presentation of FSMs is then provided togeth-er with a discussion of the state-transition-table representation. The chapter ends by introducing the concept of finite-state transducers and providing several examples from various levels of processing, including phonetics, phonology, orthography, and syntax. Exercises at the end of the chapter build nicely upon the Prolog code already discussed.
The suggested additional readings are appropriate, but perhaps too broad, as many are textbooks.
 a general discussion of knowledge-based and pattern-recognition approaches to ASR without a historical perspective. The knowledge-based method with its focus on fea-ture extraction and knowledge integration is described at a very high level without the benefit of any hands-on exercises. Coleman uses dynamic time warping (DTW) to exemplify the pattern-matching approach, as it is a fairly straightforward dynamic programming algorithm of which the student can gain some understanding by filling in tables of distances. The chapter also contains a nice discussion on the sources of variability in speech, although no insights are offered on how they would be addressed by the two approaches to ASR. Only two exercises are found in this chapter, one to fill in matrices used by the dynamic time-warping algorithm and one asking the student to think about pitfalls of the pattern-matching approach. The chapter does not discuss the implementation of any of the methods discussed, although I believe a C implementation of DTW could have been added to good effect. There are some helpful recommended readings on ASR techniques, many of which are textbooks or edited books of papers. tic analysis with finite-state models. The chapter begins with a discussion of Markov models and the use of probabilistic methods for coping with uncertainty. Part-of-speech n -gram models are introduced together with a very brief discussion of probabilities and Markov models (along with a few simple exercises). Coleman then provides an informal discussion of the hidden Markov model (HMM), followed by a discussion of trigram models, data incompleteness, and backoff. Finally, the three basic problems for
HMMs (Rabiner 1989; Rabiner and Juang 1993) are discussed, providing the student with a clearer understanding of the kinds of questions that can be addressed with them.
There are two very short sections on using HMMs for part-of-speech tagging and speech recognition, but there are no code or exercises associated with them. The chapter ends with a discussion of Chomsky X  X  objections to Markov models and a response to each. The only exercises appearing in this chapter concern probability and Markov models.
The chapter does not discuss implementations of any of the approaches discussed, and yet it would seem that the student would gain a deeper understanding of many of the topics presented in this chapter by playing, for example, with a simple part-of-speech tagger. There are many publicly available resources that could be used to fill in this hole.
 in chapter 5. A simple definite-clause grammar is introduced, followed by an intuitive discussion of parsing and recursive descent parsers. A second grammar, dog grammar.pl , is then discussed together with difference lists in Prolog so that the grammar can be updated to produce a tree structure. Coleman then provides an example grammar that breaks phoneme sequences into syllables. The chapter ends with a very brief introduction to various parsing algorithms, chart parsing, issues of search, deterministic parsing, and parallel parsing. The chapter would have been improved by the addition of exercises; however, the student could load the grammars discussed in the chapter into Prolog and play with them. Several textbooks are recommended for additional reading, although the novice might gain a richer perspective by consulting the chapters on parsing of Allen (1994). 140 into a context-free parsing algorithm. Coleman begins with a discussion about why a probabilistic approach is useful in computational linguistics, ranging from the fact that human judgments of grammaticality are gradient and at times uncertain of providing a good mechanism to account for collocations and the learnability of grammars. A simple probabilistic context-free grammar (CFG) is then presented, along with a discussion of how to obtain the grammar rules and estimate their probabilities. The chapter ends by discussing limitations of probabilistic CFGs and briefly introducing two alternative ap-proaches, tree-adjoining grammars and data-oriented parsing. This chapter contains no exercises for the student. However, it does provide a list of materials to assist the student in learning more about C programming, digital signal processing, the Klatt synthesizer, speech recognition, Prolog, computational linguistics, and probabilistic grammars. of introducing the uninitiated to a variety of techniques in speech and language process-ing. Due to its broad coverage, the text is unable to delve deeply into many of the details, although this is mitigated in part by the fact that he provides additional readings for students with an interest in a particular topic. The reading list on the companion website would be improved by including more modern sources, pointers to current conferences and journals in speech and natural language processing (e.g., Bird 2005), and links to helpful resources available on the Internet (e.g., DISC 1999; Hunt 1997; Jamieson 2002;
Kita 2000; Krauwer 2005; Manning 2005; Picone 2005). Additionally, although the book is not aimed at students with a strong background in mathematics or computer science, they would benefit from additional readings in these areas. The book would benefit from additional editing, as it contains errors that could easily confuse a novice, as well as from the addition of more hands-on exercises, particularly in Chapters 6 through 9.
Quibbles aside, if the book builds bridges between the communities Coleman desires, it will have a broad impact that could be felt for years to come. I believe education is an important first step to building multidisciplinary solutions to some of the most pressing problems in speech and natural language processing. It would be wonderful to see more books with Coleman X  X  vision.
 References

Patrick Blackburn and Johan Bos (INRIA, France, and University of Edinburgh, Scotland) Stanford: CSLI Publications (CSLI Studies in Computational Linguistics, edited by
Ann Copestake), distributed by the University of Chicago Press, 2005, xxv+350 pp; paperbound, ISBN 1-58576-496-7, $30.00,  X 21.00 Reviewed by Francis Jeffry Pelletier Simon Fraser University
Computational semantics is the study of how to represent meaning in a way that comput-ers can use. For the authors of this textbook, this study includes the representation of the meaning of natural language in logic formalisms, the recognition of certain relations that hold within this formalization (such as synonymy, consistency, and implication), and the computational implementation of all this. I think that, while there probably are not many courses devoted to computational semantics, this book could profitably be incorporated into more traditional computational linguistics courses, especially when two courses are offered serially. The material here could be spread out and integrated into parts of a more standard pair of these courses, and it would result in a substantial widening of the knowledge that students come away with from these courses.
 a goal of justifying the enterprise in the face of the modern emphasis on statistical natural language processing. Besides this introduction, the book contains six substantial chapters and four short appendices. There is also a very extensive suite of material on-line at a Web site maintained by the authors.
 the particular outlook taken in this book. There is a very short introduction to the notion of a model of a set of quantifier-free sentences, followed by an equally short discussion of the interpretation of quantifiers in a model. With respect to quantifier-free sentences, students are referred to the three-page Appendix B on propositional logic, where truth-tables are covered. The innovations start with the introduction of three  X  X nference tasks X : querying, consistency checking, and informativity checking. These correspond to the logical concepts of satisfiability (of a given formula in a given model), consistency (whether there is a model that satisfies a given formula), and validity (whether a formula is true in all models, or, equivalently, whether a given argument is valid). Although the notions of querying, etc., are thus merely renamings of standard logical notions, it seems to me that the renaming is particularly apposite in the setting of a textbook for non-logicians, especially since much is to be done computationally with these notions in the enterprise of computational semantics. Much of the remainder of chapter 1 consists of a gentle introduction to Prolog and using it to check formulas for well-formedness and to build model checkers with the goal of implementing the querying task.
 a representational language, which serves to motivate chapter 2,  X  X ambda calculus. X 
This chapter starts with the question of whether we can automate associating semantic representations with sentences of natural language. A  X  X irst pass X  is made in terms of definite-clause grammars, and some  X  X xperiments X  are carried out to show students the necessity of having some representational scheme similar to the lambda calculus.
This seems to me a very nice motivation for lambda calculus, and the remainder of this chapter is developed with an eye to simultaneously doing  X  X anguage engineering, X   X  X exical development, X  and  X  X emantic rule manipulation. X  Some interesting exercises guide the student along this path.
 scope ambiguity. Four methods are considered in turn: Montague X  X  original method, Cooper storage, Keller storage (i.e., nested Cooper storage), and hole semantics.
Montague X  X  method of generating different scopes by generating their analysis trees differently is described in some detail and found  X  X nelegant. X  This leads to the other three methods, for which implementations in Prolog are built up, starting with Cooper storage. The authors remark that Cooper storage is extremely easy to implement, but runs into trouble with  X  X ested NPs, X  such as Every owner of a hash bar , where the Cooper method inappropriately generates unbound variables for some retrievals of the NPs in the store. This motivates the Keller method, which is essentially just Cooper storage with a method for ensuring that the embedded NPs retain appropriate values for their variables that are contributed from the embedding NPs. The authors remark that Keller storage is  X  X  very simple modification of our earlier code for Cooper storage. X  to say that one NP must definitely outscope another, while at the same time saying that they have no scoping relation to a third NP. Further, there are other constructs (e.g., negation and intensional verbs such as knows that  X  X lthough the intensional construc-tions are not considered in this book) that introduce scope ambiguities, and the NP storage method seems unable to generalize to these cases. The authors thus motivate the more general notion of underspecified semantic representation ( version of USR they adopt is (not surprisingly) Johan Bos X  X  hole semantics .Thisisthe longest (and most intricate) part of the chapter, and no doubt the place where students will most easily stumble. The transition to hole semantics is aided by Prolog macros for building USR trees with the appropriate labeling, and this is followed by a number of plugging predicates, also written in Prolog. By the end of the chapter, and by using the on-line materials, students can come to manipulate moderately complicated tional linguistic courses: inference. Recall the authors X  desire to have (a) querying, (b) consistency checking, and (c) informativity checking as a part of their computational semantics. (Indeed, they seem to claim that this is a primary source of data against which one can evaluate the worth of (computational) semantic theories.) These are to be implemented as (a ) model checking, (b ) model building, and (c ) validity of arguments. They therefore wish to involve automated methods of performing these tasks. To these ends, they introduce automated theorem proving at length. (Automated model checking is an easier task that was discussed in chapter 1.) With respect to the propositional logic (chapter 4), signed tableau and resolution methods are introduced, first in theory and then with respect to some Prolog macros. Students are encouraged to experiment with some provers that the authors have on-line. The section concludes with a discussion of some meta-theoretic properties of propositional logic and remarks about issues of complexity. Chapter 5 expands both tableau and resolution to the first-order 284 case. This chapter contains a lot of information about technical issues in automated theorem proving. After developing all this machinery, the authors surprise the student by tossing away all the theorem provers that have been built, on the twin grounds that  X  X hey are too na  X   X ve X  and  X  X hey don X  X  handle equality. X  Instead, the authors move to off-the-shelf theorem provers, in particular to Otter and Bliksem, which are both resolution-based. The authors have written interfaces that allow students to enter formulas in the notation employed in the book and call up one or another of these provers (which are on the book X  X  Web site). Also on the Web site are the model-building programs
Mace and Paradox, and although there was no discussion in the text about how model building is automated, students can use these programs to build models for sets of formulas.
 calculus, the USR s, the theorem-proving programs, the model-checking programs, and the model-building programs are employed together to construct a system,  X  X lever Use of Reasoning Tools X  (Curt). Curt is developed in seven stages, each one adding some aspect that was discussed earlier in the text to the preceding version. The final version,
Knowledgeable Curt, has a small fund of lexical knowledge, world knowledge, and situational knowledge at its disposal. And like some of the earlier Curts, it can maintain a model of the Discourse-So-Far. On the basis of all this, Knowledgeable Curt can accept new information if it does not contradict what is already known or is not implied by what is already in its store of information. And for simple dialogues, it does a credible job. This leads into the last part of the book, which is a discussion of lexical knowledge, world knowledge, ontologies, and the like. Despite its simplicity, Curt is a  X  X roof of concept X  for the entire enterprise being undertaken in this textbook, and students who actually get this far and can work with Curt will be interested in extending it. And isn X  X  that what we would want out of any course? are always interesting and will give the instructor who uses this book a much broader understanding of the topics under discussion in the chapters. They are also quite nice for historical and bibliographic information.
 ties, and so it will always lead others to complain that certain things were omitted that should not have been and that too much time was spent on some other things.
This book, with its unusual emphasis on (a) semantics without having any theoretical discussions to motivate a firm syntactic base, (b) first-order logic without considering any other framework, (c) underspecified semantics over other alternatives, (d) hole semantics as the favored version of underspecified semantics, and (e) theorem-proving technology in the service of semantics, is bound to generate complaints. I would have liked to see some firm syntactic theory or other that would form the basis for generating semantic representations, with thoughts on how to arrive at such a grammar. (This book and its Web site present a small context-free grammar.) I would have liked to see at least a mention of alternatives to underspecified semantics, and possibly some general criticisms of underspecified forms (perhaps in the notes). And even though some of my own work is in automated theorem proving and its use in semantics, it is not so clear to me that the discussion of the inner workings of tableau and resolution methods is really appropriate here, especially since the systems that are developed are discarded in favor of off-the-shelf systems. Furthermore, the authors also employ automated model-building programs, and these are not explained at all, in contrast to the theorem-proving methods. In the model-building case, students are told from the beginning to just use the off-the-shelf software; why not do the same with the theorem provers? Much could have been omitted from these chapters on theorem proving (and replaced with more straightforward semantic information) if theorem proving weren X  X  covered so deeply. exploration of computational semantics aimed at the senior undergraduate student or beginning graduate student who has not taken any computational semantics. It nicely introduces a current influential direction in the research field and might even convince students that there is more to semantics than corpus-based studies of what words are n -grams of some given word. It might also have some positive effect on automated theorem proving, moving its current emphasis on mathematical issues to include language-oriented topics.

 Harry Bunt, John Carroll, and Giorgio Satta (editors) (Tilburg University, University of Sussex, and University of Padua) Dordrecht: Kluwer Academic Publishers (Text, speech, and language technology, volume 23), 2004, xi+403 pp; hardbound, ISBN 1-4020-2293-x, $209.00; paperbound, ISBN 1-4020-2294-8, $64.95 Reviewed by Stefan Riezler Palo Alto Research Center New Developments in Parsing Technology is a collection of papers based on contributions to the International Workshop on Parsing Technology in the years 2000 and 2001. The publication format of a collection might raise the following questions: Is the whole of the collection more than the sum of its previously published parts by virtue of an inspired selection of the most seminal papers in the area? Or does the collection go beyond a mere reprint of revised versions of workshop papers by including insightful overview articles or other previously unpublished material? In the case of New Developments in Parsing Technology , the answers to these questions are yes and no. Yes, concerning added value by the inclusion of a previously unpublished invited talk by Michael Collins. No, concerning exceeding the sum of its previously published parts.
 troductory chapter written by the editors. In this article, the editors motivate an in-terest in parsing technology by listing 12 application areas that make crucial use of parsing techniques. Given the limited pool of candidate papers from two workshops, unfortunately, the only application area that is addressed in the collection concerns the processing of spoken language (chapters 15 X 17). Topical clusters for the remaining papers can be induced loosely from the grammar frameworks they refer to. The large majority of papers can be tagged as extensions of context-free grammars, with some work on head-driven phrase structure grammar (HPSG) and tree-adjoining grammar (TAG) interspersed. Again, the limited candidate pool prevented a broader spectrum of grammar frameworks, thus excluding work in the areas of lexical functional grammar (LFG), combinatory categorial grammar (CCG), and other linguistically deep parsing frameworks. Statistical parsing techniques, which arguably are the focus of most recent developments in parsing technology, are addressed only in chapters 2 X 4. Again, this misrepresents the state-of-the-art in parsing research. Without diving into short sum-maries of the papers contained in the collection X  X he editors do an excellent job on this task X  X t seems unfortunate that it was not possible to extend the paper selection beyond the pool of contributions to two workshops, for example, by accompanying the selected papers by related work, or follow-up papers that apply or criticize the presented work. sented in the invited paper by Michael Collins (chapter 2). This chapter truly lives up to the promise of the book title and it also serves many purposes: First, it is an excel-lent, self-contained introduction to large-margin methods for machine learning. Collins gently leads the reader from the well-known territory of statistical parameter estima-tion for probabilistic phrase structure grammars (PCFGs) to generalization theory and algorithms for large-margin classifiers. Generalization theory asks the question of how well a learner classifies unseen data given only a limited amount of training data, instead of referring to the law of large numbers for guarantees in parameter estimation.
In other words, it asks how much training data is needed for an estimator to converge to a point where it has minimal error on unseen data, that is, where it is probably approximately correct. Collins manages to provide the novice reader with an intuitive explanation of the most important convergence bounds in the framework of probably approximately correct (PAC) learning theory. Furthermore, the paper serves the ad-vanced reader by showing how large-margin classifiers can be applied to multiclass classification problems such as parsing, together with previously unpublished proofs for bounds on the generalization error of large-margin X  X ased parse selection methods.
The algorithms presented in the paper include support vector machines, boosting, and the voted perceptron, all of which have been shown to provide significant im-provements in parse selection in previously published experiments. The paper con-cludes with a discussion of the relation of large-margin methods to Markov random fields (MRFs). Collins points out that these methods are closely related, for example, to boosting techniques, and refers to publications that show the similarity of both methods in a maximum-likelihood framework (Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002). The missing link of a PAC interpretation of regularized
MRFs was presented in the same year as this collection in a paper by Andrew Ng. Ng (2004) presents impressive generalization bounds for l 1 -regularized logistic regression, showing that the sample complexity of such learners grows only logarithmically in the 440 number of irrelevant features instead of linearly as for learners without feature selection techniques. This result fills a gap in the literature and shows that both  X  X arametric X  and  X  X istribution-free X  methods can be justified in a framework of maximum-likelihood es-timation as well as in a PAC learning setting. Unfortunately, the three-year gap between the workshops and the publication of the collection prevented a reference to this result in Collins X  X  article. On the same note, it should be mentioned that polynomial bounds for multiclass classifiers outperforming Collins X  X  Theorems 8 and 9 that require an exponential number of constraints were already presented in 2003 by Taskar, Guestrin, and Koller (2003).
 ogy must be as follows: Collins X  X  invited contribution is so outstanding that it alone makes it worthwhile to get hold of a copy of the book. Each of the selected workshop papers is a worthwhile read in itself; however, beyond the fact that the papers appeared in two consecutive years of IWPT, there is no added value in having them brought together in this particular collection. Considering that most workshop papers are avail-able on-line as well as in the workshop proceedings, it is really Collins X  X  chapter alone that justifies a purchase of the book.
 References

Margaret Masterman (1910 X 1986) (Edited by Yorick Wilks, University of Sheffield) Cambridge University Press (Studies in natural language processing, edited by Steven
Bird and Branimir Boguraev), 2005, x+312 pp; hardbound, ISBN 0-521-45489-1, $95.00,  X 55.00; eBook, ISBN 0-511-13318-9, $76.00 Reviewed by John F. Sowa VivoMind Intelligence, Inc.

Margaret Masterman was one of six students in Wittgenstein X  X  course of 1933 X 1934 whose notes were compiled as The Blue Book (Wittgenstein 1958, which is the first publication after the mimeographed copies that were circulated informally). In the late 1950s, she founded the Cambridge Language Research Unit (CLRU) as a discussion group, which evolved into one of the pioneering centers of research in computational linguistics. In her will, she requested that Yorick Wilks edit a collection of her papers for publication. The result is this book, which is important for its historical perspective on the development of computational linguistics. It consists of 11 of her reports and articles from the late 1950s to 1980 and includes a 17-page introduction and commentary by Wilks. Karen Sp  X  arck Jones also wrote a commentary on one of the papers she had coauthored.
 dations of theoretical linguistics. Around the same time that Chomsky was developing his syntactic theories and Montague was advocating a logic-based alternative, she was proposing a  X  X eo-Wittgensteinian X  view, whose organizing principle was a thesaurus of words classified according to the  X  X anguage games X  in which they are used. Although no single paper in the book formulates a succinct summary that could be called a theory, the following principles are discussed throughout:
Unlike the a priori formalisms of Chomsky or Montague, this approach is based on data about actual language use. In the commentary, Wilks noted that Masterman X  X  work contained  X  X he germ of what was later to be called EBMT or example-based translation (Nagao 1989), which is now perhaps the most productive current approach to MT world-wide, and I have heard Professor Nagao refer to [her] in this connection in a lecture X  (page 279).
 ilarities to the Cognitive Linguistics of Croft and Cruse (2004). Croft X  X  radical construc-tion grammar ,Cruse X  X  dynamic construal of meaning , and Lakoff and Johnson X  X  (1980) work on metaphor are compatible with and to some extent anticipated in Master-man X  X  papers. The multiplicities of context-dependent word senses discussed in the first paper of the book could be aptly characterized by the term microsense , which was coined by Cruse (2000). Although most of the papers are forty years old or more, the goal of implementing the ideas in a computable form has forced a greater attention to detail and precision than is found in some of the more recent work on cognitive linguistics.
 their rather disorganized structure, but the book contains many intriguing insights that still seem fresh today. Among them are her penetrating criticisms of Chomsky X  X  fixation on syntax:
My quarrel with [the Chomsky school] is not that they are abstracting from the facts. How could it be? For I myself in this paper am proposing a far more drastic abstraction from the facts. It is that they are abstracting from the wrong facts because they are abstracting from the syntactic facts, that is, from that very superficial and highly redundant part of language that children, aphasics, people in a hurry, and colloquial speakers, quite rightly, drop. (page 266)
As an alternative, she discussed the writings of the phoneticist Peter Guberina (1954), who had worked in a school for the deaf: any piece of discourse, and the linguistic elements that carry it  X  is vastly more important than he clearly hears some one thing that is clearly said about some basic subject of discourse, while the actual subject of discourse remains unknown to him, very little of the deaf man X  X  problem is solved; he has only heard one thing. (page 228)
In summary, she said that  X  X uman communication consists of patterns of semantic interactions between ascertainably cognate subjects of discourse. X  By cognate subjects , she meant ones that originate from the same or similar language games and are grouped 552 in the same area of a thesaurus. The semantic patterns led to the templates of Wilks X  X  own theory of preference semantics , and they are closely related to the chunks, frames, scripts, and schemata of other systems.
 but the cost of the book and the limited selection underscores the difficulty of getting access to research from the years before the World Wide Web. The eleven papers are a fraction of Masterman X  X  more than 80 published and unpublished reports, memos, and manuscripts, not to mention the more than 200 CLRU memos by other authors. As Wilks noted, there was a great deal of duplication among the manuscripts, but even for those that were selected, much of the lengthy appendices, quotations, and diagrams had to be omitted. One of the papers that was not included (Masterman 1961) contained the first recorded use of the term semantic network . The old material is valuable for clarifying the historical record and for making available important, but long-forgotten insights. References

Sophia Ananiadou and John McNaught (editors) (University of Manchester and UK National Centre for Text Mining)
Boston and London: Artech House, 2006, xi+286 pp; hardbound, ISBN 1-58053-984-X,  X 53.00 Reviewed by Nikiforos Karamanis University of Cambridge
Text mining is defined by Hearst (1999) as the automatic discovery of new, previously unknown information from unstructured textual data. This is often seen as comprising three major tasks: information retrieval (gathering relevant documents), information extraction (extracting information of interest from these documents), and data mining (discovering new associations among the extracted pieces of information).
 with work on information extraction and its subtasks such as noun phrase chunking, named entity recognition, and anaphora resolution, typically applied to newswire arti-cles. The explosive growth of biomedical literature has prompted increasing interest in applying such techniques to biomedical text in order to address the information overload faced by domain experts. This is reflected by the proliferation of articles reviewing this work (Reviews 2006), which typically appear in bioinformatics journals and target experts in biosciences as their primary audience.
 approaches to biomedical NLP in more depth than is typically offered in a review article.
The book consists of an introductory chapter written by the editors and nine chapters that each discuss a different sub-area of biomedical NLP. Each chapter is authored by researcher(s) with significant contributions to the overviewed sub-area.
 definition of text mining by Hearst (p. 1), although their interpretation lays more em-phasis on the unstructured nature of the input textual data than on the potential novelty of the output information. Those  X  X nterested in organizing, searching, discovering, or communicating biological knowledge X  (p. 2) are the targeted readers of the book. The book aims to provide them with an  X  X xtensive summarization and discussion of the research literature in text mining and reported systems, geared towards informing and educating rather than oriented towards other text mining experts X  (p. 3). Information retrieval is placed outside the scope of the book, which focuses on performing fa-miliar tasks such as named entity recognition (NER) and information extraction (IE) on biomedical text, but also extensively discusses problems that are less studied in general NLP but are of particular importance in this area, such as the exploitation of domain-specific knowledge sources, the construction of terminologies, how to deal with abbreviations, and so on. An outline of the main aims and challenges in biomedical NLP is followed by an overview of how these issues are discussed in each chapter. and Joachim Wermter, first explains how each level of linguistic analysis (i.e., mor-phology, syntax, and semantics) is associated with distinct NLP components. Then a reference architecture for text mining that combines these components with each other and with domain resources is presented and compared with the organization of two extant systems, GeneWays (Rzhetsky et al. 2004) and PASTA (Gaizauskas et al. 2003).
The comparison leads to the conclusion that the reference architecture represents a somewhat  X  X dealized view of system building . . . which has to be supplemented by many heuristic solutions X  (p. 37), which might explain why this architecture is not re-ferred to extensively in subsequent chapters. The archetypal text-mining system should ideally strive to produce  X  X ome form of proposition X  (p. 33) as its output, which will be subject to subsequent processing, for example, to discover new knowledge. However, the authors acknowledge that they are not aware of any system with such an advanced reasoning functionality (p. 34). I did not spot such a system being reviewed anywhere else in the book so I may rather safely claim that the book is mainly about conduct-ing NLP in the biomedical domain rather than discussing the text mining process as a whole.

NLP . . . necessary to fully appreciate discussions in following chapters X  (p. 7). The authors cite and discuss the seminal literature for each NLP component in their hypo-thetical architecture quite comprehensively, although they do not include any references to other introductory readings to NLP. These could have been helpful, because one often comes across terminology that might be unknown to the non-specialist (e.g.,
Section 2.3.1 on part-of-speech tagging contains terms such as seed tagging, second order n-gram Markov models, probabilistic suffix analysis, and smoothing by linear interpolation without explanation). Quite a bit of jargon is used in other chapters as well, so my feeling is that the book will be more accessible to readers with some familiarity with NLP than to the non-initiated.
 mining X  by Olivier Bodenreider, discusses how the major publicly available knowledge sources may support biomedical NER and IE, with particular emphasis on the three components of the Unified Medical Language System (UMLS), namely, the Specialist
Lexicon, the Metathesaurus, and the Semantic Network (Bodenreider 2004). The chapter exemplifies the utility that these resources may provide, although Bodenreider also points out that they might often need to be extended or re-engineered to better serve
NER and IE. This is one of the main insights of the book that will be repeated in subsequent chapters.
 complementarity of the existing resources, which are classified into three types: lexical, terminological, and ontological. Not distinguishing between the three types of resources has been claimed earlier in the book to be likely to  X  X ead to confusion and hamper attempts at exploitation for text mining X  (p. 7) although, in Bodenreider X  X  own words, this distinction often ends up being  X  X omewhat arbitrary X  (p. 55). Hence he appears to concentrate more on some of the limitations of the resources, such as the restricted coverage of the genomic and the molecular biology subdomain. An overview of sug-gested solutions to this problem is provided, although these seem to be more applicable to domain-specific resources (e.g., model organism databases) than to the more general
UMLS resources that the chapter focuses on. It has also been argued earlier that the lack of an explicit link between a lemma in the Specialist Lexicon and the corresponding concept in the Metathesaurus might also limit their utility for NER and IE (p. 27), but this issue remains unaddressed in Chapter 3.

Ananiadou and Goran Nenadic, focuses on automatic term recognition (ATR) and 136 automatic term structuring (ATS). ATR identifies lexical units that correspond to do-main concepts, and ATS organizes the recognized terms into knowledge structures (terminologies). A brief introduction to terminology construction is followed by a presentation of terminological resources in biomedicine (somewhat overlapping with material in the previous chapter). A detailed review of the main approaches to ATM and ATS constitutes the core of the chapter. Equally interesting is the discussion of the challenges posed to ATR by the pervasive phenomena of term variation and ambiguity.
The chapter concludes with an overview of the ATRACT system (Mima, Ananiadou, and Nenadic 2001), a terminology management workbench incorporating modules for
ATR and ATS. There is much valuable material in this chapter, although I felt that it would have been more appropriate to discuss the difference between ATR and NER here rather than having to wait until Chapter 6 (or go back to page 8). Some discussion of the differences between ATS and IE (overviewed in Chapter 7) would have been useful as well.
 deals with the problem of linking an abbreviation to its expanded form(s). This is important because of the very frequent use of abbreviations in the biomedical genre and the continuous introduction of many new abbreviations. An introductory discussion of the problems of defining and identifying abbreviations is followed by a detailed review of the methods used to construct and evaluate the Stanford Biomedical Abbreviation
Database (Chang, Sch  X  utze, and Altman 2002). Different types of abbreviation variations (already introduced in the previous chapter, although no cross-reference is provided) and the methods used for their normalization are also overviewed. The chapter also touches upon the problem of identifying long forms that do not appear in the same document as the abbreviation. Several directions for future work are proposed, the most interesting of which, in my view, are the need for a comprehensive study to compare the coverage and accuracy of different abbreviation databases and the more ex-tended investigation of algorithms that can automatically generate abbreviations from long forms.
 on applying NER to biomedical text. The nature of candidate named entities (NEs) and issues related to their ambiguity, variation, and growth rate (also mentioned in previous chapters) are discussed in detail to exemplify how biomedical NER differs from traditional NER in the newswire domain. The main approaches to biomedical
NER are reviewed in depth with particular emphasis on the reported evaluation results (although the authors also point out that these cannot always be used to directly compare the approaches because of important methodological differences between the evaluation studies). Grounding the recognized NEs in an ontology and dealing with
NEs other than gene and protein names are identified as the main challenges to address in forthcoming research.
 lapping or related sections often requires some effort on behalf of the reader. In par-ticular, I often found it hard to keep track of literature reviewed in different chapters.
Each chapter contains its own list of references (enumerated in the order in which they appear in the text, which seems to be the norm in biomedical publications). The number in the list is typically used to point to a reference in the text. For instance, the ABGENE system (Tanabe and Wilbur 2002) is reviewed both in Chapter 4 (p. 77) and in Chapter 6 (p. 134). In Chapter 4 the system is mentioned by name followed by its citation number (i.e.,  X  X BGENE [36] X ), whereas in Chapter 6 the authors X  names appear together with the citation number (i.e.,  X  X anabe and Wilbur [26] X ). (Note that the term ABGENE is not included in the book X  X  index.) Had the book come with a single reference section and a citation index, the reader X  X  attempt to identify and extract related information would have been facilitated greatly.
 devoted to rule-based methods for the extraction of simple facts and more complex events. An overview of IE as shaped by the MUC evaluation efforts is followed by a comprehensive critical assessment of the various approaches adopted for IE in the biomedical domain. Sublanguage-driven systems that simultaneously consider syntax and semantics, such as GENIES (Friedman et al. 2001), and systems that take advantage of ontological information (Gaizauskas et al. 2003; Cimiano, Saric, and Reyle 2005) are proclaimed to be the most successful. As in Chapter 2, being able to deliver abstract representations of facts and events that can be subjected to subsequent data mining or integrated in a knowledge base to enable reasoning (instead of simply returning textual strings or their transforms) is regarded as a bonus for a system. Given that these representations are likely to be heavily reliant on the requirements of the mining or rea-soning process, I would welcome more discussion on how systems developed to deliver material suitable for different knowledge bases may be compared with each other. used to train and evaluate more advanced IE systems, echoing other similar statements throughout the book (most notably in Chapters 2 and 3). Given that the preparation somehow surprising that semi-supervised or unsupervised machine learning methods, for example those discussed by McCallum (2005), are not mentioned as an alternative research avenue. The problem of resolving anaphoric references is mentioned in sev-eral chapters as another essential NLP task that awaits in-depth investigation in the biomedical domain. Chapter 7 is meant to provide an overview of existing approaches to anaphora resolution in biomedical text (p. 148), but this takes place only in passing. Devoting some more space to this issue would have been worthwhile as well.
 discusses issues related to the collection and annotation of corpora. From their own ex-perience in the development of the GENIA corpus (Kim et al. 2003), the authors provide practical advice on how to compile a representative corpus, prepare annotation schemes and guidelines, perform the actual annotation and, ultimately, assess the reliability of the produced data. There is a section on annotation format that lays emphasis on XML-based schemes but does not mention the B-I-O notation that is used in Chapters 2 and 4.
An informative discussion on available annotation tools concludes the chapter, which is written very clearly, although I found some material too low level (particularly the script to retrieve MEDLINE abstracts in Figure 8.1) or even subpar (Section 8.3.3 on the comparison of corpora).

Christian Blaschke, begins by explaining how the MUC and TREC evaluation challenges and similar efforts in molecular biology inspired community attempts to build shared assessment resources and agree on evaluation methodologies to appraise the state of the art in biomedical NLP. The authors address the key issues of why, how, and what to evaluate and then detail the design, organization, and main results of four recent evaluation challenges and how these should motivate additional efforts in the years to come.
 focused on resolving problems of practical relevance to biologists in order for them to become involved in the development effort and continue to participate in challenging 138 evaluations. This issue is of particular interest to me because of my current involvement in a project aiming to integrate an NLP system into an existing curation workflow (Kara-manis et al. 2007). In Section 9.3.1, the authors distinguish between the tasks performed by different types of users, namely database curators and research scientists, and then go on to explain how different evaluation tasks were designed with a different type of user in mind. Meeting users X  requirements seems to be relevant to other chapters of the book as well. For instance, it is not clear whether delivering abstract representations of facts (as suggested in Chapters 2 and 7) will assist curators more than pointing them to actual textual strings. Developing and evaluating integrated systems that address the users X  real-world needs is one of the greatest challenges in biomedical NLP (Cohen and Hersh 2005), but is not substantially covered in the book.
 strates how certain NLP techniques may be incorporated into extant algorithms for analyzing nontext biological data such as genomic sequences and expression profiles.
Many of the overviewed techniques have been shown to improve solutions to problems that are of particular importance to research scientists, such as homology search and sequence-based functional classification. This is a very interesting chapter, which comes closer to the Hearstian notion of text mining than previous chapters, although most of the reviewed techniques treat the text as a bag of words, thus deviating significantly from the NLP technology discussed previously.
 summary of the fundamental approaches with the authors X  precious insights. The book is recommended to anyone interested in a more detailed overview of biomedical NLP than what is typically presented in a review article although, unavoidably, the most recent of these reviews may include more up-to-date information. Readers with some
NLP background will probably find the book more easily accessible than biomedical scientists and might benefit even more from it if they apply some additional effort to synthesize opinions across chapters. This review partially reflects my attempt to do that, indicating a few ways in which I think that the book could be further enhanced. In any case, the book will almost certainly fertilize ongoing research in the rapidly expanding area of biomedical NLP, so I felt that studying it was time well spent.
 References

Eneko Agirre and Philip Edmonds (editors) (University of the Basque Country and Sharp Laboratories of Europe) Dordrecht: Springer (Text, speech, and language technology series, edited by Nancy
Ide and Jean V  X  eronis, volume 33), 2006, xxii+364 pp; hardbound, ISBN 1-4020-4804-4, $169.00,  X  129.95 Reviewed by Diana McCarthy University of Sussex
Word sense disambiguation ( WSD ), the tagging of words in context with labels indi-cating the sense in which the words are used, has become an increasingly popular area of computational linguistics research. This is particularly due to the S evaluation exercises which created standard data sets for the task. This book gives a thorough overview of current WSD techniques and performance of systems on these data sets, as well as a brief history of the field and some truly insightful discussions on potential developments for the future.
 by leading experts in the field rather than an anthology of authors X  own work, though because these are experts there is naturally a good representation of their work. As well as surveys of existing material, there are analyses which will be of interest even to those familiar with the WSD literature. The book is an extremely useful resource for information that has not been collated elsewhere and additionally contains material that has not been published elsewhere, such as personal communication references and findings from project reports. There are some excellent discussions on the hot topics of the field. One primary focus for discussion is the choice of sense inventory for the tagging. This issue is so fundamental to the WSD community (Hanks 2000; Ide and
Fellbaum 2006) that it permeates many of the chapters and relates also to another hot topic: the relevance of the task for applications. In the Introduction, the co-editors ob-serve that although most work strives to provide useful technology from an engineering perspective, there is also scope in pursuing WSD for gains in theoretical computational semantics.
 troduction. Chapter 2, by Kilgarriff, looks at the notion of word sense using evidence collected from his work on the match (or mismatches) between defined senses and corpus evidence. He argues that although sense inventories might be a useful place to start, they are constructed under the pragmatic considerations of lexicographers. It should therefore not be assumed that they have cognitive validity or meet the require-ments of an NLP system. Kilgarriff looks at the concept of word sense in the context of
Fregean versus Gricean semantics. He argues that the Gricean framework focusing on the intended usage fits better with the spectrum of contexts of a word than trying to isolate the truth values of these contexts in a Fregean setting.
 cographer sense distinctions from a practical viewpoint. They argue that we would be better off abandoning the fine-grained test material that we have been focusing on and starting from a coarser-grained level that humans and systems can more reliably dis-criminate. Once we are closer to 100% accuracy, we can then see if this improves a given
NLP application, rather than the somewhat futile situation we have at present where system performance is too poor to be sure if WSD is beneficial to an end application.
The issue remains of which word senses to use. Ide and Wilks discuss using evidence for distinctions from cross-linguistic and psycholinguistic studies or the etymology distinctions captured by lexicographers as homonyms.
 get to grips with the standard evaluation data sets, particularly those created for the S
ENSEVAL exercises. They provide thorough descriptions of the procedures involved in producing the data sets for S ENSEVAL -1 and -2 and a brief overview of S (the book was written around the time of the latter). There are details on the range of tasks in these evaluation exercises, and the core methodology is described specifically with respect to English. There is a bias towards description of the English all-words and verbal lexical sample tasks due to the role of the first and third authors in the con-struction of these resources. The methods for producing the coarse-grained groups that were used for scoring the S ENSEVAL -2 verbs are clearly and succinctly described. The involvement of Palmer and Dang in the construction of these resources (Palmer, Dang, and Fellbaum 2007) makes them well qualified to provide a very useful overview of this valuable work on verb classes. The methods for producing coarse senses for other parts of speech (PoS) are unfortunately not available in the S ENSEVAL point out, there is inevitably some overlap. Classification of approaches and division of the material is not straightforward because topics are very much interleaved and the field abounds with hybrid systems. Some duplication could perhaps have been avoided, for example the repetition of factual material on data sets and evaluations, but the overlap is not significant and some is beneficial because it makes each chapter self-contained, reinforces important issues, and provides different perspectives on the same material.
 based, unsupervised, and supervised. In Chapter 5, Mihalcea gives a clear exposition of methods that use  X  X nowledge X  for disambiguation. This knowledge is typically in-formation coded manually in a given inventory (often WordNet), though the term  X  X nowledge-based X  is also used here and elsewhere for hybrid approaches that use the predefined information to structure knowledge acquired from corpus data. Heuristics are included in this chapter, some of which rely on sense-tagged data, so they might well have been placed with the supervised methods in Chapter 7.
 captures the difference between systems that distinguish senses according to evidence from raw data and those that make distinctions according to a predefined inventory.
This is a very important distinction considering the issues with predefined inventories raised by Kilgarriff. In the WSD community, however, there is ambiguity in the term unsupervised , which is widely used in the WSD literature, and particularly the S proceedings, for systems that do not use hand-tagged data (even though they use predefined senses). The reader is warned of the ambiguity in many places but because the ambiguity is present in the book from the Introduction, it might have been worth making this ambiguity explicit from the start. In Chapter 6, Pedersen reserves the terminology for systems that discover senses automatically from data. So far such work applied induced senses to the task of disambiguation and information retrieval. The 256 full potential of these unsupervised systems has yet to be realized because evaluation on standard data sets necessitates mapping from the induced classes to whichever pre-defined inventory was used in the creation of the data set (Agirre et al. 2006). When evaluation can be performed on a task that doesn X  X  prescribe a particular set of labels, these methods should benefit from renewed interest.
 work on supervised methods that use both predefined senses and hand-tagged data.
The availability of standard data sets has made systematic comparison possible; how-ever, such comparison is difficult because so many factors are involved and typically only a few parameters are considered by any one study. In this respect, Chapters 7 and 8 contain some very useful analysis. In Chapter 7, M  X  arquez, Escudero, Mart  X   X nez, and Rigau provide an experimental comparison of some of the supervised learning algorithms on the DSO corpus. The results are dependent on the combination of algo-rithm, features, and evaluation data set and because the differences are small it is hard to determine definitive winners. The comparison is followed by an excellent section explaining why systems are hitting ceilings on performance and expanding on some possibilities that have been advocated for tackling the problems.
 feed into WSD systems, identify these sources in existing ings on the benefits of the various categories from a number of comparative evaluations in the literature. They provide a useful set of observations on the contribution of the knowledge sources described in the chapter. Again it is apparent that although there are general trends to be found X  X or example that verbs may do better with specific knowledge sources, such as subcategorization, and discriminative approaches X  X here is no  X  X ne size fits all X  even across a given PoS and the interaction between features and algorithms makes the exploration difficult. There is plenty of motivation for com-bining knowledge sources to get optimum results, as no one knowledge source or algorithm is a panacea; however, work is clearly needed to isolate components and to determine what works well when. The discussion in Chapter 8 is a useful step in that direction.

WSD can be acquired automatically. They look at automatic acquisition of topical knowledge about word senses and also pick up on the thread from the end of Chap-ter 7 on trying to provide supervised systems with sufficient sense-tagged examples using cross-lingual resources and information gleaned from the Web. They highlight research (Agirre and Mart  X   X nez 2004) demonstrating the importance of determining the right sampling bias when using Web data. Another approach has been to exploit the
Web community for voluntary labor in annotation tasks. Web directories also show potential for finding valuable domain-specific information, which is championed in Chapter 10 as a crucial input for WSD .
 topics. Buitelaar, Magnini, Strapparava, and Vossen ask whether domains are neces-sary for WSD and whether they are the whole story. The evidence suggests that the answer is somewhere between these two viewpoints. There is no doubt that for many words, domain information is important, though the high percentage of  X  X actotum X  (i.e., domain-independent) words implies it cannot be the whole answer. The importance of the domain issue depends on the purpose of WSD . If one were using for semantic analysis of generic text, then domain issues would not be as significant compared to a cross-lingual task operating on domain-specific text. The chapter ends with a series of results demonstrating the potential for domain-specific information retrieval and cross-lingual information retrieval tasks.
 cations. If one is making claims about the potential utility of an explicit which most WSD research does, one needs to be aware of the lack of proof for this as-sumption. Resnik presents the evidence in a very readable summary along with reasons why the benefits of WSD have yet to be proved. He does, however, argue that we are right to endeavor to validate our techniques by demonstrating that they have practical utility, and he speculates on some emerging applications where the might find a niche.
 a community looking to broaden our horizons as we look forward to S successor of the S ENSEVAL exercises. This collection serves as a thorough record of where we are now and provides some nice pointers for where we need to go. It is a great resource containing valuable reference material, helpful summaries of findings, further-reading sections, and a useful appendix on resources. There is also an index to many of the authors and algorithms cited in the book, though not all cited systems actually appear in the index. Even though the book is tailored for those new to the field, veteran WSD researchers will find the collection makes good reading with plenty of material and discussions that do not appear elsewhere. I will certainly be dipping into the book for many years to come.
 References
 Florian Wolf and Edward Gibson (Massachusetts Institute of Technology) The MIT Press, 2006, vii+137 pp; hardbound, ISBN 0-262-23251-0, $36.00 Reviewed by Alistair Knott University of Otago In his blurb on the back cover, Mark Liberman calls this book  X  X he biggest step forward [in research on discourse structure] since Aristotle. X  Given this eminent recommenda-tion, I read the book with great interest and some anticipation.
 models of text structure. Following a tradition originating with Mann and Thompson (1988) and Hobbs (1985), Wolf and Gibson assume a  X  X elational X  account of coherence: the coherence of a text is attributed to the presence of relations holding at various levels of hierarchy between its constituent elements. Traditionally, relations hold be-tween pairs of propositions; canonical examples are CAUSE , TEMPORAL SEQUENCE ,and concerns how to characterize the set of special relations whose presence can confer coherence on a text. How many such relations are there, and how should each relation be defined? A second question concerns where in a text we expect these special relations to be found. Answering this question involves using relations as building blocks in a theory of text structure. Wolf and Gibson X  X  book addresses this latter question: their main aim is to propose and argue for a particular theory about the structure of relations in coherent text.
 relations in coherent text. Both are expressed using the terminology of graph theory: the sequence of atomic discourse segments in a text is interpreted as a set of nodes in a graph, ordered by adjacency, and relations introduce arcs (and sometimes additional nodes) into this graph. One theory proposes that the relations in a coherent text must form a tree structure. In this model, each relation holds between two adjacent nodes, and creates a new node representing a unit of text spanning both nodes which can itself be linked to other units by other relations; see Figure 1 for an example. The other theory is less restrictive, requiring only that the relations in a coherent text form a connected graph. In this model, arcs directly denote relations; see Figure 2 for an example. Note that in the graph model, relations can hold between discontiguous text segments and participate in crossing dependencies between segments, and that a single text segment can be involved in several relations.
 represent the important relations in coherent text, and that a looser graph structure is preferable. In fact, their model allows some hierarchical structure; however, this is introduced to represent topic continuity between adjacent segments rather than text units created by regular discourse relations. An example of a Wolf-and-Gibson-style text structure is given in Figure 3.
 an exercise in manual annotation. Two annotators hand-analyzed a corpus of 135 texts in the newspaper/newswire genre. In the first part of Chapter 2, Wolf and Gibson describe their model of text structure in detail, and give an admirably detailed pro-tocol for manual analysis of texts, with copious examples. The remainder of the book outlines a number of separate arguments for their conception of text structure. I will first summarize these arguments, and then assess them.
 structures which make up the corpus of hand-annotated texts. In the second part of
Chapter 2, they argue that there are many phenomena captured in these analyses that would have been missed by a model imposing a strict tree structure. In particular, their analyses reveal a large number of crossed dependencies, and a large number of nodes participating in more than one relation, neither of which can be accounted for with a tree-based model.
 nouns in a text. These arguments address the issue of the  X  X sychological reality X  of the relations hypothesized in the model: the authors reason that if discourse relations are 592 shown to influence the way pronouns are resolved or generated, this is evidence that they reflect real psychological representations in the minds of readers and writers. The first part of Chapter 3 reports on a psycholinguistic study, in which subjects read a series of two-clause sentences. The second clause of each sentence contained a pronoun whose antecedent was unambiguously found in the first clause (e.g., Fiona defeated Craig, and so James congratulated him ). The connective in between the clauses was manipulated; it was found that this had an influence on the time taken by subjects to read the pronoun. The second part of Chapter 3 reports on a study of the pronouns in the annotated newspaper corpus. Again it was found that pronominalization preferences are different for different discourse relations.
 model of text structure in a practical text-processing application: text summarization. Wolf and Gibson develop a number of algorithms for text summarization that use their graph-based text structures. They then compare the quality of the summaries generated by these algorithms with summaries produced using other techniques, including some techniques using hand-annotated tree structures (from Carlson, Marcu, and Okurowski 2003). The graph-based summarization techniques outperform the other techniques, which they take as another piece of evidence in favor of a graph-based conception of text structure.
 have a rather different aim from those presented in Chapters 2 and 4. The arguments given in Chapter 3 are only tangentially relevant to the issue of the structure of relations in coherent text. They bear on the psychological reality of individual relations, rather than on the general question of how relations organize text into coherent structures. The experiments in Chapter 3 certainly provide good evidence about the relevance of individual relations to pronoun generation and interpretation. In fact there is already quite a lot of experimental work showing the effect of coherence relations on pronoun interpretation; see, for example, the references cited by Stevenson et al. (2000) for a review. Much of this work uses a sentence completion paradigm, in which subjects are asked to complete a sentence like Ken impressed Geoff because he. . . ; the connective used in a sentence has a strong influence on subjects X  interpretation of the pronoun. Wolf and Gibson X  X  experiments come to a similar conclusion about the influence of coherence relations on pronominal reference, using different experimental paradigms (self-paced reading and corpus analysis), and therefore extend the earlier findings. However, they do not bear on the authors X  central hypothesis about text structure.
 texts should be analyzed using tree or graph structures. The argument in Chapter 2 is the most direct: the authors analyze a large number of texts, and identify many phe-nomena that a tree-based account would overlook. Although there have been several other analyses in this vein in the past, these have typically involved discussion of a small set of problematic example texts. Wolf and Gibson are the first to provide a large-scale quantitative evaluation of the coverage of a tree-based theory; they enumerate the frequency of crossed dependencies and of nodes with multiple parents in their corpus, to emphasize that these phenomena are widespread.
 I do not believe so. My main concern is that the analyses assume the truth of the very theory they are being used to test. Wolf and Gibson X  X  analysts follow a detailed protocol when analyzing the texts in the corpus. This protocol allows analysts to create crossing dependencies and re-entrant structures. It is unsurprising that the resulting analyses display these phenomena. To take a fanciful analogy: imagine proposing a theory that holds that each discourse segment in a text is related to each other segment.
We can certainly write a protocol to tell analysts how to annotate texts in line with this theory. No doubt analysts following this protocol will also achieve excellent inter-annotator agreement. However, we obviously can X  X  use the set of analyses they produce as empirical evidence for the theory. Rather, the theory must be assessed in relation to its predictions about independently observable phenomena in discourse. 1 dependencies, they do not oblige an analyst to find such phenomena in a text, and there-fore that the presence of these phenomena still constitutes an empirical result. However, there are several somewhat nonstandard relations in Wolf and Gibson X  X  model that in-troduce relations between discontinuous spans of text, and therefore heavily bias analy-ses towards crossing dependencies. For instance, one relation, called SAME -SEGMENT , is used to link two portions of a single sentence separated by a sentential modifier; for example, [The economy,] according to some analysts, [is expected to improve] . This relation is in fact defined to hold across discontinuous text segments. Another relation is used to analyze cases of explicit attribution of an utterance to a speaker; for example, [ X  X ure
I X  X l be polite, X  X [promised one driver] . Such attribution statements frequently appear within an extended speaker utterance, which again makes relations between discontinuous segments almost inevitable. Both SAME -SEGMENT and ATTRIBUTION are contentious as discourse relations because they apply between portions of propositions rather than whole propositions. And by creating relations between discontinuous text segments, both bias analyses towards crossing dependencies.
 structure are due to a specific subset of relations. Indeed, they perform an analysis which apparently rules out this possibility. Surprisingly, this analysis seems to indicate that
SAME -SEGMENT and ATTRIBUTION appear considerably less frequently in crossed de-pendencies than in analyses generally. This is very hard to believe, given that nearly all of their own examples of crossed dependencies involve one or other of these relations, and often involve both. In conclusion, Wolf and Gibson X  X  collection of text analyses do not provide sufficient evidence to prefer the graph-based structural theory over the tree-based theory. These analyses are perhaps better thought of as embodying a statement of their theory than as an empirical test of it.
 support for a graph-based theory of text structure. To recap, the theory is used as the basis for an automatic text summarization algorithm, the results of which are compared to those of other algorithms, including algorithms based on the alternative tree-structure model of text. In this case, the theory (supplemented by its associated summarization al-gorithm) makes predictions about a quite distinct empirical phenomenon: the intuitions of naive readers about the relative importance of the segments of a text. The alternative tree-structure theory (supplemented by its own summarization algorithm) also makes such predictions. We can evaluate these two sets of predictions against data gathered from actual readers. And it turns out that the graph-based model of text structure outperforms the tree-based model.
 no reference to the concept of segment importance. In fact their definition of directed relations is similar to the definition of  X  X uclearity X  in Mann and Thompson X  X  (1988) 594 theory of coherence relations, and this makes explicit reference to the analyst X  X  intu-itions of segment importance. However, these decisions are all local to single relations between pairs of text segments. It is only if the relations in a text are combined in an appropriate way that these individual judgments will result in the right ranking of the segment importances over a whole text. (In fact, the alternative tree-based model of text also uses Mann and Thompson X  X  notion of nuclearity, which effectively controls for the influence of these local judgments across the two types of analysis.) So the study on text summarization appears to provide quite solid evidence in favor of a graph-based model of text structure.
 Gibson X  X  book in his blurb, it nonetheless represents a significant advance in discourse theory.
 References

Ronen Feldman and James Sanger (Bar-Ilan University and ABS Ventures) Cambridge, England: Cambridge University Press, 2007, xii+410 pp; hardbound, ISBN 0-521-83657-3, $70.00 Reviewedby RadaMihalcea UniversityofNorthTexas
Text mining is the process of discovering information in large text collections, and automatically identifying interesting patterns and relationships in textual data. It is a relatively new research area, which has recently raised much interest among the research and industry communities, mainly due to the continuously increasing amount of information available on the Web and elsewhere. Text mining is a highly interdisci-plinary research area, bringing together research insights from the fields of data mining, natural language processing, machine learning, and information retrieval. In particular, text mining is closely related to the older area of data mining, which targets the extrac-tion of interesting information from data records, although text mining is allegedly more difficult, as the source data consists of unstructured collections of documents rather than structured databases.
 ering the general architecture of text mining systems, along with the main techniques used by such systems. It addresses both the theory and practice of text mining, and it illustrates the different techniques with real-world scenarios and practical applications.
It is particularly relevant for students and professional practitioners, being structured as a self-contained handbook that does not require previous experience in any of the research fields involved.
 text mining and related topics, starting with an introduction to the task of text mining, and ending with examples of practical applications from three different domains. problem of text mining and the key elements in text mining: the document collections, the document features (words, terms, and concepts), and the role of background knowl-edge in text mining. It then briefly touches upon the possible applications of text mining, such as pattern discovery and trend analysis, and shortly discusses the interface layer of text mining systems. The second part of the chapter lays down the general architecture of a text mining system, which also serves as a rough guide for the rest of the book, as it describes the main components of a text mining system that are described in detail in subsequent chapters.
 dense in terms of newly introduced concepts. Despite being a more difficult read compared to the other chapters in the book, I found it to be the most informative with respect to operations specific to text mining. The chapter starts by defining the core text mining operations, including distributions, sets, and associations, and introduces the main techniques for isolating interesting patterns and analyzing trends over time.
The second part of the chapter overviews the role of background knowledge in text mining. The authors describe several ontologies and lexicons, and show with evidence from a real-world example (the FACT system they developed in the late 1990s) how background knowledge can be effectively integrated into text mining systems. A shortcoming of this section is the interchangeable use of  X  X omain ontol-ogy X  and  X  X ackground knowledge, X  which can be confusing for computational lin-guists, who typically make a distinction between these terms. Finally, the third part of the chapter briefly describes query languages, which are later addressed in detail in Chapter 9.
 ing tokenization, tagging, and parsing. This chapter, as well as several of the following chapters addressing text classification and information extraction, were most likely included in the book because of the authors X  intention to make the book self-contained and appealing even for those with no background in computational linguistics. are relevant to the selection of documents addressed by a text mining system. Chapter 4 describes the representation of documents for the purpose of text categorization, and introduces several machine-learning algorithms, including decision trees, naive Bayes, and SVMs, as well as committees of classifiers through bagging and boosting.
Chapter 5 introduces several clustering algorithms, including agglomerative clustering, expectation maximization, and K -means, as well as techniques specific to clustering textual data such as latent semantic analysis.
 is a key element in any text mining system. Chapter 6 begins by defining the problem of information extraction, the architecture of a typical information extraction system, and the main knowledge-based and structural approaches to information extraction. Chap-ter 7is dedicated to probabilistic models, including maximum entropy, hidden Markov models, and conditional random fields. Chapter 8 then describes text preprocessing using these probabilistic models, including part-of-speech tagging, shallow parsing, and named-entity tagging. The chapter also addresses bootstrapping techniques for information extraction. One drawback of this section of the book is the fact that the division of the material among these three chapters is not always very clear, and some parts could have been organized differently. For instance, the probabilistic models and their applications are split among Chapters 7and 8, although they could have been combined under one chapter. Chapter 8 includes material on bootstrapping approaches for information extraction, which seems unrelated to the rest of this chapter and instead could have been included in Chapter 6.
 ing browsing and displaying of distributions, associations, and hierarchies, as well as query languages and query refinement. Visualization techniques are then addressed in
Chapter 10, which describes visual interfaces to text mining systems, such as association graphs, histograms, and self-organizing maps. The chapter is rich in illustrations, which contribute to a better understanding of the differences between the various visualization methods.
 using techniques for link analysis, which are described in Chapter 11. The chapter begins with a brief introduction to graph theory, followed by a description of several graph centrality methods and algorithms for network partitioning. The chapter also provides pointers to software packages for link analysis. 126 in the book to three practical text mining problems: industry literature mining, patent analysis, and protein interactions. By showing examples of concrete applications, the authors demonstrate the applicability of the text mining theory introduced in the book to practical real-world scenarios.
 leading experts in the field. The book is well written and addresses both the theory and practice of text mining, which makes it appealing for researchers and practitioners alike. relevant to text mining, the book is highly recommended to those who would like to start delving into the area of text mining without having any previous background in computational linguistics. Although experts in computational linguistics will most likely find that they can safely skip over several of the text processing chapters (e.g., the introductory chapters on text preprocessing, or the chapters on text classification and information extraction), they will certainly find a lot of value in the chapters addressing the specific task of text mining (mainly Chapters 2, 10, 11, and 12).


Gian Piero Zarri (Politecnico di Milano) Springer Verlag (Advanced Information and Knowledge Processing series, edited by
Lakhmi Jain and Xindong Wu), 2009, x+301 pp; hardbound, ISBN 978-1-84800-077-3, $99.00; e-book, ISBN 978-1-84800-078-0; DOI 10.1007/978-1-84800-078-0 Reviewed by Frank Schilder Thomson Reuters
Gian Piero Zarri X  X  book summarizes more than a decade of his research on knowledge representation for narrative text. The centerpiece of Zarri X  X  work is the Narrative Knowl-edge Representation Language (NKRL), which he describes and compares to other competing theories. In addition, he discusses how to model the meaning of narrative text by giving many real-world examples. NKRL provides three different components or capabilities: (a) a representation system, (b) inferencing, and (c) an implementation.
It is implemented via a Java-based system that shows how a representational theory can be applied to narrative texts.
 basic principles of NKRL. The chapter first defines the focus on nonfiction narratives by contrasting the domain with fictional narratives, for example, a novel. Zarri chooses n -ary predicates in order to represent events formally. He argues for a neo-Davidsonian knowledge representation following Schank (1980), Schubert (1976), and others, and at the same time he sets his approach apart from the knowledge representation proposals one can find in Semantic Web representation languages suc has RDF and OWL. How-ever, Zarri emphasizes that NKRL, despite its similarity to conceptual graphs (Sowa 1999), is more focused on practical applications. The chapter concludes by introducing so-called templates in an attempt to demonstrate the practical usefulness of NKRL. are introduced:
This chapter also contains a comparison of NKRL and other formalisms that deal with the representation of temporal information, such as TimeML (Pustejovsky et al. 2003) or
Discourse Representation Theory (DRT; Kamp and Reyle 1993). A detailed description is given of how NKRL represents temporal information based on Allen X  X  (1984) interval calculus and how NKRL approaches the problem of underspecified or coarse temporal information suc has in around December 25, 2005 .

A set of predefined conceptual structures is introduced and numerous examples taken from real-world narratives are provided. The definitional component introduced in
Chapter 2 is fleshed out and the distinction between sortal and non-sortal concepts (e.g., CHAIR versus GOLD ) is described. This hierarchy is quite similar to other so-called upper-level ontologies suc has CYC or SUMO (Gu ha and Lenat 1991; Pease, Niles, and
Li 2002), which are introduced to the reader in more detail in the beginning of this chapter. In addition to the conceptual hierarchy HClass, the descriptive component
HTemp holds a set of often-used templates. Each template is described with a specific example that shows how different slots of the templates may be filled.
 mented system. The NKRL system provides several query tools for retrieving infor-mation from the knowledge base encoded in NKRL annotations, as described by the previous chapters. The query tools comprise querying by search patterns, unification / filtering operations, and indexing temporal information. The indexing of temporal information considers different levels of temporal information including a temporal perspective. The temporal perspective is used to represent information about when an event starts or ends in addition to whether it is observed by somebody.
 ical enhancement to the current version of NKRL. Appendix A contains a detailed de-scription of the NKRL software and Appendix B discusses the treatment of a particular linguistic phenomenon within NKRL: plural entities.
 information. It contains valuable discussions of important questions such as whether n -ary predicates should be used. However, some of these discussions would have benefited from a more in-dept htreatment. T he comparison wit hTimeML (Pustejovsky et al. 2003), for example, only partly covers recent developments and does not men-tion software that utilizes TimeML. For example, the TARSQI toolset, by Zarri, allows the user to extract events and temporal expressions while temporal links are derived and consistency checks can be run via a constraint propagation component.
 for the Semantic Web, such as RDF and OWL. He rightly points out similarities while addressing shortcomings of the Semantic Web technology (e.g., restriction to ternary predicates). But he overlooks an important point: RDF and OWL were not created for the semantic representation of non-fictional narratives X  X he focus of this book. Halevy,
Norvig, and Pereira (2009), for instance, point out that one needs to distinguish between 152 approaches to the semantic interpretation problem of natural language and the repre-sentation of the Semantic Web. The former tries to address the question of how language can be formally represented while the latter focuses on the interoperability of semantic information expressed by Web pages (e.g., flight information provided by a travel agency). Because Zarri proposes an approac hfor solving t he semantic interpretation problem, a clear distinction as to what problem is being addressed would be helpful in order to avoid giving the wrong impression that the proposed methods are suitable for the Semantic Web.
 not cite work by Krifka (1992) and Dowty (1991). These authors presented theories on sortal event hierarchies similar to the one discussed by Zarri, and both theories discuss how the sortal quality of an object influences the event type (e.g., eating an apple vs. eating apple pur  X  ee). Zarri X  X  introduction to his theoretical framework could have been improved by a more in-dept htreatment of event semantics, suc has t he t heories introduced by Krifka, Dowty, and others.
 overuse of italics and other fonts. Less would have been definitely more here. In ad-dition, a list of abbreviations and a glossary of important terms would have been useful so that the reader could use the book as a quick reference, for example.
 provides many narrative texts exemplifying the expressiveness and capability of NKRL, an unaddressed issue is whether the implementation can be scaled. Unfortunately, there is no way for the reader to decide, because the book does not include a CD with a demo of the system, let alone the source code.
 narrative information, but not to beginners in this field. Students, for example, who are interested in this area would need a more guided approach to the topic. Nevertheless, the book provides a solid theoretical foundation for representing information extracted from narrative text such as news messages; and I am pleased to see that the author undertook the effort of implementing his theory in an actual system that has the poten-tial for many different and exciting practical applications.
 References
Sandra K  X  ubler, Ryan McDonald, and Joakim Nivre (Indiana University, Google Research, and Uppsala and V  X  axj  X  o Universities) Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 2), 2009, xii+115 pp; paperbound, ISBN 978-1-59829-596-2, $40.00; e-book, ISBN 978-1-59829-597-9, $30.00 or by subscription Reviewed by John Carroll University of Sussex
This book is a survey of the latest research in dependency parsing, describing some of the approaches currently being investigated by the research community, assessing their strengths and weaknesses, and exploring relationships between them. The book is one of the first in a series of short monographs entitled Synthesis Lectures on Human
Language Technologies ; other related series include speech and audio processing, and artificial intelligence and machine learning. A PDF version of the book can be purchased on-line for $30.00.
 timely. Until recently, most researc hinto inducing statistical parsers from syntactically annotated text has worked with phrase structure tree representations. However, over the last ten years or so a number of researchers have argued (Lin 1995; Briscoe and
Carroll 2006) that dependency analyses X  X n which syntactic structure is represented by linking pairs of words by labeled dependency relations X  X ave a number of important advantages over phrase structure trees. In particular: 154
In addition, as annotated corpora for languages other than English have become more numerous, it has become evident that:
The CoNLL shared tasks from 2006 onwards have played an important role in raising interest in dependency parsing and supporting work in this area, by organizing the annotation and distribution of dependency corpora in several languages, providing the stimulus of friendly competition, and facilitating comparisons between the various techniques used by the systems that have been entered.
 terms of a common framework consisting of three elements: a set of constraints defining the space of permissible dependency structures for a given sentence, a set of param-eters (possibly empty), and a parsing algorithm. Two main classes of models are dis-tinguished: data-driven, in which the parser is learned from a corpus of dependency structures; and grammar-based, in which the parser is directed by a formal grammar (and also possibly by weights controlling how alternative solutions are explored). such as how dependency grammar can be used to describe the linguistic facts about a given language, how particular syntactic constructions could be analyzed, or how hand-crafted dependency grammars are developed. The focus is on parsing algorithms, and the two main data-driven approaches of transition-based and graph-based parsing are described in some detail. The focus on the process of parsing itself also means that the book does not refer (even in a further-reading section) to some widely distrib-uted parsing systems which use other types of grammar internally but which output dependencies, for example the C&amp;C parser (Clark and Curran 2007). And although the CoNLL data sets have been very influential, there is no mention of the PARC 700
Dependency Bank (King et al. 2003), which has been used in comparative dependency-based evaluations of a wide range of parsers.
 in computer science, linguistics, and computational linguistics. The reader is assumed to have some background knowledge of linguistics and computer science, and although not necessary for an understanding of most of the material, at some points the reader is expected to be comfortable wit hproof by induction, analysis of t he complexity of algorithms, and algorithms for directed graphs. Two of the chapters begin with a few pages of formal definitions, propositions, and proofs, wit hlittle in t he way of motivating or illustrative examples, and here the less mathematically confident reader will have to persevere.
 is only one month after the date at the end of the Preface. However, a side effect of this is a lack of polish: There are a number of typographical errors (and at least one incom-plete sentence), occasional lapses into unidiomatic English, some instances of incorrect referencing style, and there is no index. Also, decisions on when references are given within the body of a chapter or postponed to end-of-chapter notes are inconsistent, some technical terms (e.g.,  X  X purious ambiguity X ) are not explained at the first point at which they are mentioned, and others (e.g.,  X  X ap degree X ) are not explained at all. impede the reader X  X  understanding, this book serves as a very useful and up to date survey of the burgeoning research area of dependency parsing. References This book review was edited by Pierre Isabelle.


Gian Piero Zarri (Politecnico di Milano) Springer Verlag (Advanced Information and Knowledge Processing series, edited by
Lakhmi Jain and Xindong Wu), 2009, x+301 pp; hardbound, ISBN 978-1-84800-077-3, $99.00; e-book, ISBN 978-1-84800-078-0; DOI 10.1007/978-1-84800-078-0 Reviewed by Frank Schilder Thomson Reuters
Gian Piero Zarri X  X  book summarizes more than a decade of his research on knowledge representation for narrative text. The centerpiece of Zarri X  X  work is the Narrative Knowl-edge Representation Language (NKRL), which he describes and compares to other competing theories. In addition, he discusses how to model the meaning of narrative text by giving many real-world examples. NKRL provides three different components or capabilities: (a) a representation system, (b) inferencing, and (c) an implementation.
It is implemented via a Java-based system that shows how a representational theory can be applied to narrative texts.
 basic principles of NKRL. The chapter first defines the focus on nonfiction narratives by contrasting the domain with fictional narratives, for example, a novel. Zarri chooses n -ary predicates in order to represent events formally. He argues for a neo-Davidsonian knowledge representation following Schank (1980), Schubert (1976), and others, and at the same time he sets his approach apart from the knowledge representation proposals one can find in Semantic Web representation languages suc has RDF and OWL. How-ever, Zarri emphasizes that NKRL, despite its similarity to conceptual graphs (Sowa 1999), is more focused on practical applications. The chapter concludes by introducing so-called templates in an attempt to demonstrate the practical usefulness of NKRL. are introduced:
This chapter also contains a comparison of NKRL and other formalisms that deal with the representation of temporal information, such as TimeML (Pustejovsky et al. 2003) or
Discourse Representation Theory (DRT; Kamp and Reyle 1993). A detailed description is given of how NKRL represents temporal information based on Allen X  X  (1984) interval calculus and how NKRL approaches the problem of underspecified or coarse temporal information suc has in around December 25, 2005 .

A set of predefined conceptual structures is introduced and numerous examples taken from real-world narratives are provided. The definitional component introduced in
Chapter 2 is fleshed out and the distinction between sortal and non-sortal concepts (e.g., CHAIR versus GOLD ) is described. This hierarchy is quite similar to other so-called upper-level ontologies suc has CYC or SUMO (Gu ha and Lenat 1991; Pease, Niles, and
Li 2002), which are introduced to the reader in more detail in the beginning of this chapter. In addition to the conceptual hierarchy HClass, the descriptive component
HTemp holds a set of often-used templates. Each template is described with a specific example that shows how different slots of the templates may be filled.
 mented system. The NKRL system provides several query tools for retrieving infor-mation from the knowledge base encoded in NKRL annotations, as described by the previous chapters. The query tools comprise querying by search patterns, unification / filtering operations, and indexing temporal information. The indexing of temporal information considers different levels of temporal information including a temporal perspective. The temporal perspective is used to represent information about when an event starts or ends in addition to whether it is observed by somebody.
 ical enhancement to the current version of NKRL. Appendix A contains a detailed de-scription of the NKRL software and Appendix B discusses the treatment of a particular linguistic phenomenon within NKRL: plural entities.
 information. It contains valuable discussions of important questions such as whether n -ary predicates should be used. However, some of these discussions would have benefited from a more in-dept htreatment. T he comparison wit hTimeML (Pustejovsky et al. 2003), for example, only partly covers recent developments and does not men-tion software that utilizes TimeML. For example, the TARSQI toolset, by Zarri, allows the user to extract events and temporal expressions while temporal links are derived and consistency checks can be run via a constraint propagation component.
 for the Semantic Web, such as RDF and OWL. He rightly points out similarities while addressing shortcomings of the Semantic Web technology (e.g., restriction to ternary predicates). But he overlooks an important point: RDF and OWL were not created for the semantic representation of non-fictional narratives X  X he focus of this book. Halevy,
Norvig, and Pereira (2009), for instance, point out that one needs to distinguish between 152 approaches to the semantic interpretation problem of natural language and the repre-sentation of the Semantic Web. The former tries to address the question of how language can be formally represented while the latter focuses on the interoperability of semantic information expressed by Web pages (e.g., flight information provided by a travel agency). Because Zarri proposes an approac hfor solving t he semantic interpretation problem, a clear distinction as to what problem is being addressed would be helpful in order to avoid giving the wrong impression that the proposed methods are suitable for the Semantic Web.
 not cite work by Krifka (1992) and Dowty (1991). These authors presented theories on sortal event hierarchies similar to the one discussed by Zarri, and both theories discuss how the sortal quality of an object influences the event type (e.g., eating an apple vs. eating apple pur  X  ee). Zarri X  X  introduction to his theoretical framework could have been improved by a more in-dept htreatment of event semantics, suc has t he t heories introduced by Krifka, Dowty, and others.
 overuse of italics and other fonts. Less would have been definitely more here. In ad-dition, a list of abbreviations and a glossary of important terms would have been useful so that the reader could use the book as a quick reference, for example.
 provides many narrative texts exemplifying the expressiveness and capability of NKRL, an unaddressed issue is whether the implementation can be scaled. Unfortunately, there is no way for the reader to decide, because the book does not include a CD with a demo of the system, let alone the source code.
 narrative information, but not to beginners in this field. Students, for example, who are interested in this area would need a more guided approach to the topic. Nevertheless, the book provides a solid theoretical foundation for representing information extracted from narrative text such as news messages; and I am pleased to see that the author undertook the effort of implementing his theory in an actual system that has the poten-tial for many different and exciting practical applications.
 References
Sandra K  X  ubler, Ryan McDonald, and Joakim Nivre (Indiana University, Google Research, and Uppsala and V  X  axj  X  o Universities) Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 2), 2009, xii+115 pp; paperbound, ISBN 978-1-59829-596-2, $40.00; e-book, ISBN 978-1-59829-597-9, $30.00 or by subscription Reviewed by John Carroll University of Sussex
This book is a survey of the latest research in dependency parsing, describing some of the approaches currently being investigated by the research community, assessing their strengths and weaknesses, and exploring relationships between them. The book is one of the first in a series of short monographs entitled Synthesis Lectures on Human
Language Technologies ; other related series include speech and audio processing, and artificial intelligence and machine learning. A PDF version of the book can be purchased on-line for $30.00.
 timely. Until recently, most researc hinto inducing statistical parsers from syntactically annotated text has worked with phrase structure tree representations. However, over the last ten years or so a number of researchers have argued (Lin 1995; Briscoe and
Carroll 2006) that dependency analyses X  X n which syntactic structure is represented by linking pairs of words by labeled dependency relations X  X ave a number of important advantages over phrase structure trees. In particular: 154
In addition, as annotated corpora for languages other than English have become more numerous, it has become evident that:
The CoNLL shared tasks from 2006 onwards have played an important role in raising interest in dependency parsing and supporting work in this area, by organizing the annotation and distribution of dependency corpora in several languages, providing the stimulus of friendly competition, and facilitating comparisons between the various techniques used by the systems that have been entered.
 terms of a common framework consisting of three elements: a set of constraints defining the space of permissible dependency structures for a given sentence, a set of param-eters (possibly empty), and a parsing algorithm. Two main classes of models are dis-tinguished: data-driven, in which the parser is learned from a corpus of dependency structures; and grammar-based, in which the parser is directed by a formal grammar (and also possibly by weights controlling how alternative solutions are explored). such as how dependency grammar can be used to describe the linguistic facts about a given language, how particular syntactic constructions could be analyzed, or how hand-crafted dependency grammars are developed. The focus is on parsing algorithms, and the two main data-driven approaches of transition-based and graph-based parsing are described in some detail. The focus on the process of parsing itself also means that the book does not refer (even in a further-reading section) to some widely distrib-uted parsing systems which use other types of grammar internally but which output dependencies, for example the C&amp;C parser (Clark and Curran 2007). And although the CoNLL data sets have been very influential, there is no mention of the PARC 700
Dependency Bank (King et al. 2003), which has been used in comparative dependency-based evaluations of a wide range of parsers.
 in computer science, linguistics, and computational linguistics. The reader is assumed to have some background knowledge of linguistics and computer science, and although not necessary for an understanding of most of the material, at some points the reader is expected to be comfortable wit hproof by induction, analysis of t he complexity of algorithms, and algorithms for directed graphs. Two of the chapters begin with a few pages of formal definitions, propositions, and proofs, wit hlittle in t he way of motivating or illustrative examples, and here the less mathematically confident reader will have to persevere.
 is only one month after the date at the end of the Preface. However, a side effect of this is a lack of polish: There are a number of typographical errors (and at least one incom-plete sentence), occasional lapses into unidiomatic English, some instances of incorrect referencing style, and there is no index. Also, decisions on when references are given within the body of a chapter or postponed to end-of-chapter notes are inconsistent, some technical terms (e.g.,  X  X purious ambiguity X ) are not explained at the first point at which they are mentioned, and others (e.g.,  X  X ap degree X ) are not explained at all. impede the reader X  X  understanding, this book serves as a very useful and up to date survey of the burgeoning research area of dependency parsing. References This book review was edited by Pierre Isabelle.


Kees van Deemter (University of Aberdeen) Oxford: Oxford University Press, 2010, xvi+341 pp; hardbound, ISBN 978-0-19-954590-2, $29.95 Reviewed by Ewan Klein University of Edinburgh
Let X  X  start off by setting the scene for this book. A term is generally regarded as vague if it admits borderline cases, that is, cases where speakers are reluctant to say either that the term definitely applies or definitely does not apply. An example is the expression bright , say applied to lights. Although some lights are definitely bright, and others are definitely dim, there are in-between cases which seem to be neither bright nor dim.
Borderline vagueness is often associated with the notion of tolerance , in the sense that small changes in objects don X  X  affect the applicability of a vague term. Thus, suppose my sitting room light is definitely bright and I turn down the dimmer a very tiny fraction; then the light will still be regarded as bright. This principle of tolerance gives rise to a puzzling situation: If I keep on turning down the dimmer, small step by small step, the diminution in brightness will be imperceptible at each step. Thus, if the light is bright at step n , then tolerance dictates that it will also be bright at step n + 1. Yet inevitably at some stage in this process the light will no longer be bright X  X ndeed, it might be entirely extinguished. Situations like this are often discussed under the heading of the sorites paradox.
 vagueness, and he would have been well placed to write a technical monograph on the subject. Instead, he has taken the brave step of producing a book for the general public, following in the  X  X opular science X  footsteps of such luminaries as Richard Dawkins. In general, he has succeeded admirably. I X  X  not sure whether I would recommend giving this book to your grandmother, but van Deemter X  X  informal style, vivid examples, and lucid exposition make it a pleasure to read.
 in communication and why is it so prevalent? Are there situations in which vague terms are preferable to precise ones? And what is the relation between vagueness and context-dependence? Framed by an introduction and epilogue, the twelve chapters of the book that explore these themes are divided into three parts. Part I makes a case for the pervasiveness of vagueness, even  X  X here one least expects it, X  such as in scientific measurement. Chapter 2 contains an interesting excursus on how apparently clearcut notions such as  X  X pecies X  become more indeterminate on closer inspection, and
Chapter 3 points out that attempts to turn vague terms such as obesity , poverty ,and intelligence into precisely defined notions suitable for scientific investigation are subject to arbitrariness and residual vagueness. Examples of gradual change in the identity of individuals are addressed in Chapter 4, and issues of numerical approximation, rounding, round numbers, and levels of statistical significance appear in Chapter 5. ophy, and logic. The linguistic frameworks X  X nd approaches to vagueness X  X iscussed in Chapter 6 are likely to be familiar to readers of this journal. (One small surprise is the omission of Lakoff [1973] from the discussion of hedges and also from the presentation of fuzzy logic later in the book.) Probably more interesting to most readers is Chapter 7, devoted to  X  X easoning with vague information. X  The chapter starts with a presentation of three principles of reasoning with vague concepts. Admissibility is concerned with respecting rank ordering along a dimension, as in this example: If x is tall, and y is taller than x , then y too is tall. According to the second principle, the relation indistinguishability (with respect to some vague property) is non-transitive: if x and y  X  z , it need not hold that x  X  z . Van Deemter motivates this principle with the example of a weighing balance. Given friction between the moving parts, the balance will have limited sensitivity, so that for some y that is fractionally heavier than x ,the difference in weight will fail to register, and similarly for a z that is fractionally heavier than y . However, it may well be that there is sufficient difference between x and z for the balance to distinguish between the weight of the two. The third principle, tolerance, is similar to the notion mentioned in the first paragraph of this review, though couched in slightly different terms. Van Deemter uses these principles to present the sorites paradox, and then proceeds to review and critique the so-called epistemic solution, taking experimental results on color perception as one of the crucial sources of counter-evidence. Consideration of the sorites paradox continues into Chapters 8 and 9. Among the approaches considered are supervaluations, context-dependence, and fuzzy logic.
Van Deemter finds the last of these sufficiently compelling that he devotes some time to pointing out its flaws. Fuzzy logic, he suggests, belongs to the class of degree theories , which he favors as the most promising overall approach to the semantics of vagueness.
It is worth noting that his nomenclature is potentially confusing: Whereas linguists would expect a degree-theoretic approach to vagueness to involve quantification over degrees (e.g., the degree to which John is tall) in a classical logic, van Deemter is thinking here in terms of degrees of truth. From fuzzy logic, he segues into a brief but intriguing exploration of probabilistic logic, drawing particularly on little-known work by Max Black (1937).
 guistics. Artificial intelligence is exemplified by decision support systems using fuzzy logic. This is a point in the book where I would have liked to see mention of a topic that is missing, namely the pervasiveness of vagueness in geographical information systems (see, e.g., Bennett, Mallenby, and Third [2008] and references therein). Next, van Deemter turns to the challenge of getting a natural language generation (NLG) system to use vague terms appropriately, focusing on gradable adjectives in definite descriptions. One of the novel contributions at this point is to couch the NLG system X  X  choices in terms of game theory. Although this direction of inquiry seems extremely promising, I am not persuaded that the account of vagueness in strategic communica-tion developed by Aragones and Neeman (2000) deserves the amount of space that it receives here. By contrast, van Deemter presents a valuable and detailed response to
Lipman X  X  (2006) argument that in a cooperative game, using vague terms can never be optimal. In considering a range of potential counter-arguments, van Deemter creates an opportunity to restate and review many of the themes running through the book.
Although no conclusive answers are given, I certainly had a sense that this closing discussion tied together much of the earlier material as well as indicating promising new research directions. 250 work of others in an even-handed manner, he has his own views on the right way to model vagueness and is not afraid to express them. The range of material covered is impressive. Even if you are well-versed in the literature on vagueness, you are likely to learn something new. And if you X  X e never really thought about vagueness, this is a great starting point. In fact, the book would serve as an excellent jumping off point for a graduate seminar. Finally, I X  X  like to endorse a remark found in the Preface:
It has been a delight to discover how much complex material can be reduced to simple ideas. On a good day, it even seems to me that some themes are best explored in this informal way, free from the constraints of an academic straitjacket.
 References

Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault (Butler Hill Group, Hunter College, Microsoft Research, Educational Testing Service) Morgan &amp; Claypool (Synthesis lectures on human language technologies, edited by
Graeme Hirst, volume 9), 2010, ix+122 pp; paperbound, ISBN 978-1-60845-470-9, $40; ebook, ISBN 978-1-60845-471-6, $30 or by subscription Reviewed by Stephen Pulman University of Oxford
This book is a useful survey of the current state of the art in automated grammatical error detection in (mostly) non-native English, by some of the leading researchers in the field, aimed at audiences in computational linguistics and computer-aided language learning. The book begins with a working definition of grammatical error, distinguish-ing these from typos and some kinds of spelling and punctuation errors, and points out that whereas (only) 400 million people have some kind of English as their first language, another billion speak it as their second (or are trying to learn it as such). statistical methods in the 1990s, most systems used some kind of rule-based parsing supplemented with mechanisms for dealing with ill-formed input. These mechanisms included pattern matching, the addition of special  X  X al-rules X  for spotting frequently occurring forms of error, or different types of  X  X arse fitting X  and  X  X elaxation X  techiques (e.g., in the case of unification grammars, allowing for some types of unification fail-ure). Probably the most successful of these, both in commercial terms and in terms of performance, was the Epistle system originating from IBM and subsequently included in Microsoft Word, although its primary purpose was first-language rather than second-language correction.

On the basis of evidence from the Cambridge University Press corpus of learner English, it seems that the most frequent error type is simply an incorrect choice of content word, followed in turn by incorrect preposition, and incorrect determiner choice. Like many collocation errors, it is difficult to use the techniques described in the book to address incorrect content word choice, because this usually does not result in ungrammaticality, and the state of the art is not such that semantic anomaly can be reliably detected. Many systems therefore have concentrated on preposition and determiner errors.  X  2011 Association for Computational Linguistics different languages. The field of automated error detection suffers from a lack of large-scale annotated corpora: They exist, at least for English, but for commercial reasons are not easily or cheaply available. There is a real need in this area for good corpus resources, and some shared benchmark testing data. At present, it is literally impossible to compare the performance of different systems, because they are all tested on different data sets, often using different metrics; these issues are explored further in Chapter 5. Chapter 6 focuses in more detail on article and preposition errors, which are those where the techniques discussed X  X ypically various types of classifier, often trained on grammatically  X  X orrect X  data like the British National Corpus (BNC) or equivalents X  X re able to achieve reasonable performance on error detection (80 X 90%) although error correction is typically less accurate. Two particular complete systems, ETS X  X  Criterion and Microsoft X  X  ESL Assistant, are described here.
 challenging. Current methods for measuring strength of association for detecting collo-cations are described. There are few systems which aim to detect collocation errors, but one described here, aimed specifically at Chinese speakers learning English, achieves good performance: Noting that many of their collocation errors are the result of in-appropriate direct translation from Chinese, the system first checks whether verb+noun combinations occur in the BNC, and, if not, using bilingual dictionaries and an aligned
English-Chinese parallel corpus, suggests alternative translations for one of the words, and then checks whether the resulting combination occurs in the BNC.
 forms or agreement, and Chapter 9 describes annotation schemes for error corpora and discusses the various issues that arise in the context of trying to develop such resources.
In Chapter 10 some alternative error detection techniques using Google n -grams, on-line translation systems, and Web counts are briefly discussed, and Chapter 11 summarizes and concludes.
 the current state of the art, and would be a good starting place for those new to the field. Of course, grammatical errors and typos are not unique to second-language learners.
One might expect the same technology to improve the quality of some apparently hurriedly written text: There is an amusingly large collection of grammatical errors and typos in this book:  X  X hey are likely do some additional verification X ,  X ... errors in the use of auxiliary verbs, gerunds and infinitives in. X   X  X rammatical error typess X , etc., etc. Quis custodiet ipsos custodes? This book review was edited by Pierre Isabelle.


Martha Palmer,  X  Daniel Gildea,  X  and Nianwen Xue  X  (
University of Colorado, Boulder;  X  University of Rochester; Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 6), 2010, ix+91 pp; paperbound, ISBN 978-1-59829-831-4, $30.00 or by subscription Reviewed by Alessandro Moschitti University of Trento
A basic aim of computational linguistics (CL) is the study and design of computational models of natural language semantics. Although frequency-based approaches X  X or ex-ample, distributional semantics X  X rovide effective and concrete solutions for natural language applications, they still fail to fully reconcile the field with the theoretical-linguistic soul. In contrast, semantic role labeling (SRL), a recent new area of CL, aims to automatically provide (shallow) semantic layers using modern linguistic theories of semantic roles, also exploitable by language applications. The centrality and importance of such theories in CL has promoted the development of a rather large body of work on SRL; its many aspects and research directions make it difficult to survey the field. detailing all its main concepts and practical aspects. The authors accurately illustrate all important ingredients to acquire a global and precise view of the field, namely, (i) the theoretical framework, ranging from linking theory to theta roles, Levin X  X  classes and frame semantics; (ii) computational models based on syntactic representations derived from diverse parsing paradigms; (iii) several resources in different languages; (iv) many machine learning approaches and strategies; and (v) portability to other languages and domains.
 want to simply use its technology in natural language applications. As an  X  X riadne X  X  ball of thread, X  this book will guide the reader through the conceptual SRL labyrinth, saving months of work needed to understand theory and practice of this exciting research field. The book is divided into four content chapters.

Chapter 1. A sound and natural way to approach SRL, independently of one X  X  own background and interest, is to start from an overall understanding of the theory of semantic roles. This phase is not painless for the practitioner as there is no general agreement on the theory of semantic roles. Several concepts must be acquired, including theta roles, proto-roles, Levin X  X  classes, and frame semantics. As a consequence, to understand the potential of the different formalisms, entire books (e.g., Jackendoff 1990;
Levin 1993) should be examined. This step, although interesting, probably would slow down the work of the practitioners and may frustrate those not having the required linguistic background. By contrast, this chapter is an optimal compromise as it simply summarizes all the important aspects of semantic role theory, also providing concrete examples. This allows for saving much of the time needed to recombine and interpret the different results obtained from different theories.  X  2011 Association for Computational Linguistics
Chapter 2. One interesting aspect of SRL is the practical instantiation of linguistic the-ories in some corpora. A nice feature of this chapter is the ability to quickly focus on practical aspects by presenting the most useful data sets. The authors X  technical description is minimal but precise and highlights the differences between resources and their annotation from theoretical and practical viewpoints: The chapter provides what is needed to understand the format and meaning of the data so that writing code for using it becomes straightforward. Advanced topics such as linking different resources to boost the accuracy of automatic SRL systems are also presented. One drawback of the chapter, which is a consequence of the limited available space, is the lack of a description of other important resources (e.g., NomBank [Meyers et al. 2004], and resources in languages other than English.

Chapter 3. For a computational linguist, this is the most fascinating chapter. It presents the typical computational models used to design automatic SRL systems by illustrating the most effective pipeline architectures. These are typically composed of different mod-ules performing different tasks; the filtering, identification, classification, and joint infer-ence stages are described in detail by proposing features and models that have proved to be effective during several years of research. Additionally, the chapter discusses im-portant aspects of SRL systems such as (i) the impact of syntactic information along with different parsing paradigms, (ii) models combining different types of syntactic represen-tation, and (iii) models integrating syntactic and semantic parsers in a single approach.
These descriptions constitute key information, which allows for enhancing accurate basic systems to state-of-the-art shallow semantic parsers. The chapter concludes with valuable information for testing the quality of an SRL system, that is, a description of the most-used accuracy measures for different kinds of parsing paradigms. Typical aspects that impact system performance such as domain variability, combinations of different resources, and the use of unsupervised approaches are also illustrated. However, this chapter may still be considered incomplete as some architectures exploiting advanced machine learning techniques, for example, kernel methods (Moschitti 2004), are not reported.

Chapter 4. After all the important topics of English SRL have been presented, this chapter is dedicated to discussing the problem of extending semantic parser models to other languages (e.g., Chinese). After a brief presentation of resources for other languages, which serves only the purpose of showing their availability, the chapter focuses on interesting topics such as semantic role projection and alignment. The former allows for automatically generating labeled data using annotation on one language and parallel corpora, whereas the latter aligns both annotated corpora and can be exploited for machine translation. The second main topic of the chapter is an interesting discussion of the adaptation of system architectures when they are applied to languages different than English. Although the focus is only on the English X  X hinese pair, the derived guidelines are generally applicable to other languages. Finally, the presence of nominal predicates in the Chinese PropBank is used for introducing the important topic of nominal SRL. Although this explanation along with other related resources (e.g., NomBank) could have received a larger space, it is enough to complete all the most relevant topics of the field.

What is missing? This is a sound and complete book on SRL. It can also work as an initial manual of SRL systems as it provides indications for designing state-of-the-art parsers. 620
However, its small size suggests that it could not have been comprehensive, given the very large body of work in shallow semantic parsing. Consequently, there are some missing or not completely described topics. One of them is the practical use of semantic parses for concrete tasks or real-world applications. That is, the question  X  X ow that we have such a nice shallow semantic representation, how do we use it for a concrete (e.g., commercial) task? X  remains unanswered. It is not easy to respond to this question as, at the moment, no industrial company is using SRL (or getting from it a resounding suc-cess). However, a discussion of previous work that has successfully exploited SRL X  X or example, for question answering (Moschitti et al. 2007; Shen and Lapata 2007; Surdeanu,
Ciaramita, and Zaragoza 2008), for sentiment analysis (Johansson and Moschitti 2010), and for cross-document coreference resolution (Ponzetto and Strube 2006) X  X ould have been attempted.
 ing approaches. Although the book wisely presents a well-assessed and restricted set of techniques, the prolific SRL research has developed many other interesting methods, for example, in CoNLL (Carreras and M ` arquez 2005; Surdeanu et al. 2008). Additionally, more evidence on the accuracy and speed achievable by the different SRL models on different corpora and tasks would have been useful for practitioners to estimate the expected performance of new systems in new application domains.
 tic role theory, the language, and the genre would have been very nice. It would have given a clear picture of the spread of SRL in the natural language processing or related fields, for example, semantic Web or data mining.
 edge and years of research in linguistics, computational linguistics, and machine learn-ing for SRL into a small number of pages. It allows the reader to save time in getting acquainted with SRL research. It is also useful in helping the design of one X  X  own system and can serve as a starting point for conducting advanced research in SRL. In conclusion, this book is indispensable for researchers who are approaching SRL. References This book review was edited by Pierre Isabelle.

 Nizar Y. Habash (Columbia University) Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by Graeme Hirst, volume 10), 2010, xvii+167 pp; paperbound, ISBN 978-1-59829-795-9, $40.00; ebook, ISBN 978-1-59829-796-6, $30.00 or by subscription Reviewed by Imed Zitouni IBM T. J. Watson Research Center The Arabic language, which is the mother tongue of more than 300 million people, presents significant challenges to many natural language processing (NLP) applications (Farghaly and Shaalan 2009). Arabic is a highly inflected and derived language. The scale of Arabic computational linguistic research work is now orders of magnitude beyond what was available a decade ago. Hence, it becomes important to find a book that introduces necessary background information for working with Arabic in research or development on NLP and computational linguistics. Habash X  X  book Introduction to Arabic Natural Language Processing is an important step toward that goal. It introduces Arabic linguistic phenomena, shows how the Arabic language can be handled by com-puters, and presents resources and tools available for Arabic.
 build or conduct research in a specific NLP research area. It is not meant to be an in-depth description of how to build specific NLP applications. However, it highlights necessary issues to be aware of in NLP research and application building. As an example, readers will find a solid overview of core tasks including morphological analysis, generation, tokenization, part-of-speech (POS) tagging, and even parsing. But it does not contain detailed instructions on how to build any of these technologies. The author X  X  approach is to highlight Arabic-specific issues (e.g., how Arabic morphology interacts with different approaches to POS tagging) and to leave language-independent technologies such as core POS tagging or parsing to other resources. Readers will find a nice note on machine translation from and to Arabic (Chapter 8) that addresses the multilingual aspects of working with Arabic.
 working in Arabic computational linguistics and NLP. No prior knowledge of the Ara-bic language is required. The book mainly discusses Modern Standard Arabic (MSA), but the author doesn X  X  hesitate to refer to Arabic dialect occasionally, especially in the early chapters. Readers not familiar with Arabic get a good introduction to the language and how it can be handled by computers. Readers with prior knowledge of Arabic are reminded of the basics needed for computational research, some of which may not be intuitive to a person familiar with the language only linguistically as opposed to computationally.  X  2011 Association for Computational Linguistics briefly explains the difference between Arabic the language and Arabic dialects. It then gives a better understanding of what the reader will learn from the following seven chapters. In the subsequent chapters, the author carefully discusses resources available in the field for each specific topic. This book also has five appendixes that are of great interest to readers.

Arabic Script. This chapter discusses the Arabic scripts as used in MSA. It is not only a linguistic description of the Arabic script but also a discussion of computer encoding and text input and display for Arabic. Another aspect discussed in this chapter that is important in any NLP application is orthographic transliteration and orthographic nor-malization. Several transliteration approaches (including the well-known Buckwalter transliteration method) are also presented here. These approaches are simply methods to produce a one-to-one mapping between Arabic and Latin characters, sometimes needed for non-Arabic readers. One can work directly with the Arabic script. One application directly related to Arabic scripts is handwriting recognition, which the author gives a short note about in the last section, with pointers to important starting references.

Arabic Phonology and Orthography. This chapter nicely introduces MSA phonology and shows how the Arabic spelling rules can be used to map between Arabic phonology and script. This knowledge is important for applications such as proper name translit-eration, spelling correction, automatic speech recognition, and speech synthesis. These applications are also briefly discussed with references in the last section of this chapter.
Arabic Morphology. This chapter has the lion X  X  share of this book in terms of size and content. This is expected because Arabic morphology is challenging and it is central when working on any Arabic NLP application. This chapter has the advantage of helping new scientists and engineers remove some confusion about the large amount of terminology and disambiguate terms about Arabic morphology frequently used in the community. This chapter is not meant to be a complete reference but rather a detailed introduction to understanding challenges in Arabic morphology.

Computational Morphology Tasks. This chapter aims at introducing a set of common morphologically oriented tasks needed for several NLP applications. Examples of these tasks include morphological analysis, generation, tokenization, and POS tagging. It is building NLP applications. For example, readers will find an introduction to tools such as BAMA, MADA (Roth et al. 2008), and AMIRA (Diab 2009, among others) that are widely used for processing Arabic morphology.

Arabic Syntax. This chapter gives a general survey of Arabic syntax and its specific con-structions such as Idafa, Tamyiz, and so on. This chapter also discusses and compares three Arabic treebanking projects that are widely used in the community: Arabic Penn
Treebank (Maamouri et al. 2004), Prague Dependency Treebank (B  X  ohmov  X  a et al. 2000) and Columbia Treebank (Habash, Faraj, and Roth 2009). The author then summarizes a few research efforts on Arabic syntactic parsing.

A Note on Arabic Semantics. Because of the smaller amount of research in this area, this chapter is meant only to give a few remarks about Arabic semantics. It discusses the set of resources developed for Arabic computational semantic modeling but leaves out discussions of various theories and representations of semantics. 624 A Note on Arabic Machine Translation. Compared to previous chapters, this one addresses the multilingual aspects of working with Arabic. After briefly explaining the field of machine translation (MT), the author discusses Arabic linguistic features with MT in mind. The chapter also discusses available resources and presents recent advances in MT from and to Arabic.
 Appendices. The book contains five interesting appendices. They are of great value to anyone who wants to conduct research or development in Arabic computational linguistics and NLP. As an example, Appendix A will help the reader become familiar with the available repositories and networking resources for Arabic NLP. Appendices C and D are particularly important because they discuss available resources: lexicons and tools, respectively.
 up-to-date, and concisely written introductory reference. The book does an excellent job of introducing the Arabic language to people who have an interest in working in the field of Arabic NLP. It is also a good source of information for accessing more detailed work on Arabic NLP applications through its bibliography.
 References

Afra Alishahi (University of the Saarland) Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 11), 2010, xiv+93 pp; paperbound, ISBN 978-1-60845-339-9, $40.00; ebook, ISBN 978-1-60845-340-5, $30.00 or by subscription Reviewed by Sharon Goldwater University of Edinburgh
For much of the last 25 years or more, researchers in natural language processing (NLP) and those interested in human language acquisition have had little to say to one another. NLP researchers were increasingly focusing on data-intensive supervised learning methods, mostly using structured representations, while models of language acquisition were typically based either on symbolic nativist accounts (Dresher and
Kaye 1990; Gibson and Wexler 1994) or on the unstructured distributed representations of the connectionist approach (Rumelhart and McClelland 1986; Elman et al. 1996).
Moreover, language acquisition researchers have understandably been more interested in unsupervised than supervised learning, and (perhaps due to the much more difficult nature of this problem) have often focused on learning from toy data sets rather than large naturalistic corpora.
 however. NLP researchers have become increasingly interested in unsupervised and minimally supervised methods, and the rise of probabilistic models of cognition (Chater and Oaksford 1998; Griffiths, Kemp, and Tenenbaum 2008) means there is now a grow-ing number of cognitive scientists who are well-versed in many of the same statistical methods that are used in NLP. Thus, a short introductory text on computational mod-eling of human language acquisition seems particularly apt at this time. Alishahi X  X  slim volume is not intended to be comprehensive, but rather to provide a brief overview of the goals and methods of the field for researchers in related areas X  X ither language acquisition researchers with little computational experience or NLP researchers with-out much knowledge of cognitive science. It aims for intuitive explanations rather than highly technical ones, and includes a number of figures and diagrams, but no equations.  X  2011 Association for Computational Linguistics chapter. The first part (Chapters 1 and 2) provides an overview of the major research questions and methodologies in the field. Chapter 1 begins by introducing some of the main theoretical debates in the field of language acquisition X  X uestions of modularity and learnability. NLP researchers with some background in linguistics will probably already be familiar with these debates at the level of detail presented here; these sections will be more useful to those with a straight computer science background. Also included in Chapter 1 is a section motivating the use of computational models as an alternative to behavioral studies for studying language acquisition. This section will be useful to anyone who is new to the idea of computational modeling of cognition, as will Chapter 2 of the book. Chapter 2 discusses Marr X  X  (1982) influential analysis of the different kinds of explanations that models can provide, as well as the criteria for cognitive plausibility against which models are judged, and the main frameworks for model development (symbolic, connectionist, probabilistic). Finally, it describes the various ways models can be evaluated, along with a list of corpus resources.
 language acquisition in particular: learning the meanings of words (Chapter 3), learning morphology and syntax (Chapter 4), and learning relationships between syntax and semantics, such as verb X  X rgument structure and semantic roles (Chapter 5). Each chap-ter is divided into sections focusing on more specific topics X  X or example, Chapter 4 includes sections on morphology, syntactic categories, and syntactic structure. Each section begins by reviewing the most salient empirical facts about children X  X  acquisition in that domain, along with relevant linguistic concepts and theories that have influenced the modeling community (e.g., Mutual Exclusivity in word learning; Construction
Grammar and the Principles and Parameters theory in syntactic acquisition; theories of selectional restrictions in the acquisition of verb X  X rgument structure). The empirical and theoretical background is followed by an overview of many of the models that have been proposed in the area, and finally one or more  X  X ase studies X  (more on these in a moment).
 neighboring fields (including NLP) who want to get a quick taste of what computational modeling of human language acquisition is all about. They can read the first couple of chapters to get a general idea of the questions and methodologies, and then pick and choose any topics from the remaining chapters that might be of interest. There are very few dependencies between sections in the second part of the book, because most of the relevant background for each section is provided in that section itself. Someone who is interested in pursuing the field further (either a beginning graduate student or a more advanced researcher moving into the field) will also find many useful references, at least in the three areas that Alishahi focuses on. Many other active areas of modeling are not covered at all (for example phonetic and phonological acquisition), although this choice is understandable given the length of the book.
 study (with some exceptions, as noted subsequently) details a single model, with a half-page to two-page description of the model X  X  primary methods and assumptions, as well as the input and results. It is an excellent idea to provide concrete examples showing how models can be used to address important questions in language acquisition. But in practice, most of the case studies fall short of this goal, as they are too light on motivation and analysis. Alishahi is not always clear about why certain models, rather than others, were chosen for case studies (are they ground-breaking in some way, or merely a simple example of a particular theoretical idea put into practice?), nor how each model X  X  assumptions and results relate to the broader goals of modelling set out in the first two chapters. In addition, three out of the four  X  X ase studies X  in Chapter 4 are really just additional review sections, covering models of the English past tense, learning algorithms based on Principles and Parameters, and distributional models of syntactic structure (all worthy topics, but not case studies). This leaves only one true case study in Chapter 4, on the MOSAIC model of grammar induction (Jones, Gobet, and Pine 2000). Finally, although there are case studies of symbolic, connectionist, and probabilistic models, no Bayesian models are given a detailed look (though they are included in the review sections). Bayesian models are, of course, a subset of probabilistic models, but take a very different philosophical approach to most other models, includ-ing the type of incremental probabilistic model that Alishahi discusses in more detail. 628
Bayesian modeling is now an important force in cognitive science generally and has begun to make an impact in language acquisition specifically (Xu and Tenenbaum 2007;
Foraker et al. 2009; Goldwater, Griffiths, and Johnson 2009); as such it is worth taking a bit more space to explain the ideas behind at least one of these models.
 this book to make reading it worthwhile to any researcher who wants to get a quick overview of the main goals and approaches in the field, along with some of the many models that have been developed over the years. It will also be helpful to those who are looking for a starting point for a more in-depth study of models in one of the three areas of acquisition that Alishahi focuses on. Overall, it is a very accessible, if necessarily selective, brief introduction to the field.
 References
 Violeta Seretan (University of Geneva) Berlin: Springer (Text, speech and language technology series, volume 44), 2011, xi+217 pp; hardbound, ISBN 978-94-007-0133-5, $139.00 Reviewed by Pavel Pecina Charles University in Prague and Dublin City University Collocation is a common language phenomenon which has attracted the interest of re-searchers in many subfields of both theoretical and computational linguistics. Although there is no commonly accepted and precise definition of this phenomenon, collocations are generally understood as complex lexical items, often characterized as unpredictable, idiosyncratic, holistic, mutually selective, and so forth. Together with other types of multiword expressions (or phraseological units, such as compound nouns, phrasal verbs, idioms, etc.), collocations form a borderline phenomenon positioned between lexis and grammar: On one hand, they are unpredictable and must be learned in the same way as single words are (as whole units); on the other hand, they often also have inter-nal syntactic structure and their components must then adhere to grammatical rules. Collocations play an important role in applications involving text production (e.g., machine translation and language generation), text analysis (e.g., parsing and word sense disambiguation), and also in other related tasks (such as information extraction, text classification, etc.).
 doctoral dissertation defended in 2008 at the Department of Linguistics, University of Geneva, under the supervision of Eric Wehrli, and refers to a number of their previous publications. The main text is divided into six chapters (amounting to 128 pages) and six appendices (70 pages).
 the notion of collocation, explains its relevance (and importance) for natural language processing, specifies the aims of the work, and most importantly, it presents arguments for syntax-based collocation extraction as a more appropriate alternative to the tradi-tional syntax-free n -gram and window-based techniques.  X  2011 Association for Computational Linguistics theoretical point of view. It provides a very comprehensive (and readable) survey of numerous and heterogeneous definitions, descriptions, and discussions of this topic appearing in the literature in the past 90 years. The author (correctly) points out that although this phenomenon has an implicit linguistic aspect, the majority of definitions of collocation rather adopt a statistical view. But other perspectives X  X exicographic, pedagogical, contextualist, and lexical-semantic X  X re very well addressed, too. Special attention is paid to the role of collocations in text cohesion, colligation, semantics, metaphoricity, and a lexis-grammar interface. The author also reviews various semantic and morpho-syntactic properties of collocations considered in the literature and at-tempts to define the  X  X ore X  of the concept of collocation.
 their automatic extraction from text corpora. The author first reviews a general ex-traction procedure, which consists of two steps: collocation candidate identification using specific criteria and candidate ranking with a given association measure. Both the extraction steps are discussed in detail, including linguistic pre-processing, construction of contingency tables, and application of association measures. A significant part of the chapter is devoted to the survey of the state of the art in this task. An exhaustive compendium of relevant works is practically organized by the languages that the ex-periments were carried out on.
 contribution X  X  method of collocation extraction based on syntactic parsing. The author again advocates the need for syntax-based extraction methods (supported by several examples and citations from the literature) and reviews other relevant papers tackling this issue. The remainder of the chapter is devoted to the description of the proposed method (which identifies collocation candidates using the Fips parser and ranks them according to their log-likelihood ratio) and its evaluation in two scenarios. In the monolingual experiments, the extraction procedure is applied to a part of the Hansard corpus containing 1.2 million words in French. In the crosslingual case, the extraction procedure is simultaneously applied to French, English, Spanish, and Italian parts of the
Europarl corpus X  X ach containing about 3.8 million words. The proposed syntax-based method is compared against a standard window-based extraction technique (with a sliding window of five consecutive words). The evaluation is performed using manually judged candidates in terms of precision measured for the 500 top-ranked candidates (in
Experiment 1) and 5 test sets each containing 50 contiguous items situated at different levels in the ranked list spanning the top 10% for each language (in Experiment 2). In all cases, the evaluation experiments showed that by applying a syntax-based candi-date identification  X  X  considerable improvement is obtained over the standard sliding window method X  (p. 97). The results are presented in a very thorough way, including qualitative analysis and error analysis with lots of concrete examples.
 sented in the previous chapter. First, the author applies a method based on the simple concept of  X  X ollocation of collocations X  for inferring longer collocations from the binary combinations extracted. The second extension is a technique for data-driven induction of syntactic patterns which are adequate for identifying collocations. The candidate patterns (part-of-speech combinations) are ranked using the log-likelihood ratio and subsequently undergo a process of manual analysis. The third extension addressed is the automatic acquisition of collocation equivalents from parallel corpora. However, no thorough evaluation (or comparison with alternative approaches) of these methods is presented (e.g., a comparison of the translation equivalents extraction method with a simple look-up of equivalents in a translation phrase table of a phrase-based statistical machine translation system).
 book and sketches directions for the future work. The appendices contain an overview of published collocation dictionaries (Appendix A), a list of various definitions of a collocation (Appendix B), some mathematical notes on association measures, and de-tailed results of the experiments presented in Chapter 4: the monolingual evaluation experiment (Appendix D) and the crosslingual evaluation experiment (Appendix E). and applied work on collocations and their extraction from text corpora. It does not require any special previous experience in the field. Most of the technical passages 632 (e.g., in Chapter 3) are presented in an intuitive and self-contained way, suitable even for non-expert users. However, certain parts (mostly in the experimental chapters) lack sufficient detail and/or are somewhat arguable. For example, the author does not provide (or refer to) any evaluation of the parser X  X  accuracy and (more importantly) it is not clear how the accuracy can affect the quality of the collocation extraction procedure. From the figures in the book, one can guess that the parser provides a complete parse for about 50% of the sentences but the accuracy on syntactic pairs is unknown. Further, it seems that the parser features an internal collocation dictionary and collocations detected by this dictionary automatically receive (by convention) maximum association score by default. It is not very clear whether those cases are also used in the evaluation and comparison with the baseline (window-based) method or not. If they are, the comparison might be unfair; if they are not, we would lose information on how these cases would be ranked. In any case, the evaluation is probably biased by this dictionary, although the author does not report on its size and coverage of the corpora and how the dictionaries differ for the different languages used.
 arbitrary. This hyper-parameter was set to five, but this choice was not supported by any discussion or evidence (e.g,. distribution of the surface distance of collocation compo-nents) although it has a substantial effect on performance of the method. The window-based technique is known to pollute the list of candidates with noise (words without any syntactic relation), which decreases precision. The author reports that the syntax-based method outperforms the baseline quite substantially. The difference, however, can be probably increased by extending the window to six or even more words. On the other hand, the window can also be shortened and the difference in precision might diminish or even be reversed. The composition of the evaluation data raises a concern too: The items which were not agreed upon by the judges were excluded from the evaluation. There were not that many such cases, but this approach (ignoring borderline cases) is not very rigorous.
 For example, it is not mentioned which test of statistical significance was used in the experiments, whether sampling of the evaluation data was really stratified, or what is meant by  X  X artitioning the candidate data into syntactically homogeneous classes X  and how it affected evaluation.
 be appreciated by researchers interested in collocations and related phenomena. There is no doubt that collocation extraction should be based on syntactic preprocessing of the source corpora (simply because collocations often have syntactic structure), but the evaluation presented in the book is not very convincing. However, the parts surveying the theoretical aspects of collocations and describing the current state of the art and related work can easily serve well as a handbook for students entering the field of collocation extraction.

 Jimmy Lin and Chris Dyer (University of Maryland) Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by Graeme Hirst, volume 7), 2010, xi+165 pp; paperbound, ISBN 978-1-60845-342-9, $40.00; ebook, ISBN 978-1-60845-343-6, $30.00 or by subscription Reviewed by Peng Xu Google Inc.
 The world has been blessed by the ever-growing World Wide Web and the asso-ciated vast amount of information available through commercial search engines. Many subfields of computational linguistics, such as speech recognition, machine translation, summarization, coreference resolution, question answering, word sense disambiguation, and so on, are playing increasingly important roles in information extraction from the Web. At the same time, the Web itself is also providing more and more data for computational linguists to study. For example, in the past decade, the amount of text available for speech recognition and machine translation research has increased by several orders of magnitude. Corpora consisting of hundreds of millions of words are not uncommon anymore. This clearly poses serious challenges in terms of computation for traditional text processing approaches using a single computer. As a result, efficient distributed computing has become more crucial than ever.
 Chris Dyer, is an excellent source for computational linguists to start the paradigm shift. The book focuses on algorithm design with MapReduce X  X  programming model for distributed computations on massive data sets on clusters of commodity servers. With text processing algorithms at the core, the book provides easy-to-follow MapReduce design patterns with reusable solutions to common problems in natural language processing, information retrieval, and machine learning. The entire book is based on the authors X  excellent experiences in using MapReduce on Hadoop, the most well-known open source implementation of MapReduce and a proven commodity easily available to academia. It is a must-read for computational linguists who might be interested in taking advantage of billion-word level corpora in their research. Given the increasing importance of large data processing both in research and in industry, this book can also serve as an excellent supplementary textbook in modern computer science education.  X  2011 Association for Computational Linguistics Chapter 1: Introduction The first chapter of the book is really fun to read. The authors first give convincing facts about the importance and availability of large data corpora. It follows naturally that large-scale data processing using computer clusters is inevitable. With the authors X  interesting view of MapReduce as a service in the cloud (cloud computing), it becomes clear that the MapReduce programming model is a powerful abstraction that separates the what from the how of data-intensive processing. What X  X  more revealing and interest-ing are the  X  X ig Ideas X  that the authors summarize for MapReduce:
These Big Ideas behind MapReduce are excellent motivations for readers to move on to the rest of the book. They are the secret sauce that makes large-scale data processing a commodity to regular users, including computational linguists.
 Chapter 2: MapReduce Basics
Starting from the functional programming roots of MapReduce, the authors present a precise picture of how MapReduce works. They not only show the fundamental concepts in programming with MapReduce, but also describe the execution framework and the highly optimized distributed file systems backing the computation and storage needs of MapReduce. Fortunately, as the authors point out, these details in execution framework and distributed file systems are integrated components of MapReduce, but they are not part of the programming model. Undoubtedly, MapReduce would not work well without the execution framework and the supporting distributed file system, but the fact that users of MapReduce do not need to address them directly makes it even more appealing. The most important concepts from this chapter are mappers, reducers, partitioners, and combiners, all of which are concisely and precisely described with a simple example. A smart reader could already start designing MapReduce programs! Chapter 3: MapReduce Algorithm Design
Diving deeper into the MapReduce world, the authors explain several important and useful design patterns in this chapter. The focus is on controlling code execution and data flow X  X o design algorithms that are both scalable and efficient. On the surface, it may seem that users have to express algorithms in terms of a small set of rigidly defined components in the MapReduce environment, but as the authors show with concrete examples, there exist many controls at the users X  disposal to shape the flow of computation.
 of tasks that are simple yet challenging when the amount of data is large. These tasks and example algorithms guide the readers to gain a better understanding of the basics from the previous chapter. Each design pattern is presented to solve or alleviate a particular performance issue in real examples. Advantages and disadvantages of each design pattern are explained at length. Through the examples and the design patterns, it becomes increasingly clear that scalable and efficient algorithms can be achieved by controlling synchronization in the MapReduce programming model. 636 Chapters 4 X 6: Inverted Web Indexing, Graph Algorithms, EM Algorithms In these chapters, three classes of important real-world problems are studied in detail within the MapReduce world. The first problem is to build a Web index X  X ne of the core components of Web search. The design patterns from the previous chapter are reinforced here, either to solve a scalability bottleneck or to properly set parameters for compressing the Web index.
 can be applied to many real-world problems, such as ranking in Web search, friend suggestion in social networks, and product promotion in advertising, to name a few. Two such problems, breadth-first search and PageRank, are discussed in detail with an iterative design which employs a driver program to chain multiple MapReduce tasks. expectation maximization (EM) algorithms or gradient-based optimization techniques. The authors show that the EM algorithms can be expressed naturally in the MapReduce programming model. Many EM algorithms make independence assumptions that permit a high degree of parallelism in both the E-and M-steps. Furthermore, EM algorithms are unsupervised learning algorithms, which makes them much more often exposed to large data sets. A generic EM algorithm in the MapReduce programming model is discussed in Chapter 6, together with a case study for the word alignment problem in machine translation. The authors show that the MapReduce implementation of the EM algorithm can achieve significant speed-up compared to a highly optimized single-core implementation. When the computation is expensive, such as in the HMM word alignment model, near perfect speed-up can be achieved because the overhead associated with distributing and aggregating data in MapReduce is almost negligible compared to the real computation. The discussion demonstrates that not only does MapReduce provide a means for coping with ever-increasing amount of data, but it is also useful for parallelizing expensive computations. The ability to leverage clusters of commodity hardware to parallelize computationally expensive algorithms is an important use case of MapReduce.
 Conclusion Jimmy Lin and Chris Dyer have done a fantastic job in demonstrating the beauty and simplicity of the MapReduce programming model in this well-written book. With solid data, simple algorithms, and convincing reasoning, the authors show us how low the barriers are to adopting MapReduce. The capabilities necessary to tackle large-data problems are already within reach of many and will continue to become accessible over time. For our fellow computational linguists, it is easy to imagine the possibility of Web-scale unsupervised natural language grammar induction, Web-scale machine reading and text understanding, and machine translation systems using parallel text from the whole world. As the authors say,  X  X he golden age of massively distributed computing is finally upon us. X  This reviewer believes this book could potentially help computational linguistics to reach the  X  X olden age of unsupervised natural language processing. X  This review was edited by Pierre Isabelle.


Carlos G  X  omez Rodr  X   X guez (University of A Coru  X  na)
London: Imperial College Press (Mathematics, computing, language, and life series, edited by Carlos Martin-Vide, volume 1), 2010, xiv+275 pp; hardbound, ISBN 978-1-84816-560-1, $89.00 Reviewed by Anoop Sarkar Simon Fraser University
Deductive systems are a widely used exposition technique in contemporary computa-tional linguistics to explain novel parsing algorithms (e.g., Huang and Sagae 2010) and decoders in machine translation (e.g., Huang and Mi 2010). Deductive parsing provides a succinct formal syntax that abstracts away the implementation details yet directly reflects the time and space complexity of the underlying algorithm.
 specifies a parser at an abstract level, focusing on the semantics of the parser actions rather than implementation. The schema itself can be compiled into an executable parser allowing implementation optimizations to be shared across different parsing algo-rithms. Schemas thus provide quick prototyping of parsing algorithms. In the notation used by the compiler described in this book we would describe the familiar CYK parsing algorithm (Kasami 1965; Younger 1967) as the following declarative specification: completeness) of a parsing algorithm. Well-formed transformations of deductive sys-tems would permit the addition of new capabilities such as weighted rules in the grammar. Deductive systems could also provide a means to compare different parsing algorithms. This book provides several examples of how such properties can be useful in parsing theory and parsing implementation, in particular for converting a parser into an error-correcting parser and explicitly showing the relationship between several dependency parsing algorithms. by formally defining the semantics of items and related concepts used in deductive systems. In particular, items are sets of partial constituency trees that are licensed by rules of the grammar. As a result, parsing schemas allow compilation of schemas to executable parsers and also permit formal reasoning about properties of the parser directly. Unlike many deductive systems used to define parsers, there is no one-to-one relationship between a parsing schema and algorithm. In later work, Alonso Pardo et al. (1999) showed parsing schemas can be used to define parsers for other grammar formalisms such as tree-adjoining grammars.
 work, extends the theory and practice of parsing schemas in several directions. There are three major parts to this book:
Compiling and executing parsing schemas. The first part of the book provides a language syntax that can be used to precisely specify parsing schemas and a compiler for this domain-specific language. The syntax is shown in the CYK example above. The full specification also includes interpretations for indices such as i and i + 1 as word posi-tions, A , B , C as grammar symbols, and how deduction steps and goals are converted into parser code. The implementation details, including static analysis of the schema and the Java code generation, are described well and in sufficient detail. The source code of the compiler for parsing schemas is available for download at www.grupocole. org/software/COMPAS . (The code is typical research software X  X t takes some effort to use it, but once you do, you can play with compiling and running most of the schemas in the book.) This part of the book contains experiments on comparing many different parsing schemas for each of these formalisms. The comparison is done using hand-written grammars with feature structures (the parsers include feature unification) and evaluated on test-suite data rather than on modern Treebank grammars and data from newswire and other  X  X eal-world X  data. Another issue is that the comparison does not include the GHR parser (Graham, Harrison, and Ruzzo 1980), which may impact the comparison between CYK and Earley parsers. Also, interesting synthetic-data exper-iments are presented that compare tree-adjoining parsers with context-free parsers; it is not clear whether these results extend to natural language corpora. With regard to implementation of schemas, the focus is mainly on agenda-based implementation of de-ductive steps rather than, say, the use of (pushdown) transducers to produce parse trees.
Error-repair parsers. The second part of the book focuses on error-repair in parsing (using parsing schemas, of course). Such an approach tries to deal with limited coverage of the grammar by performing insertions, deletions, or substitutions on the input string. This makes a lot of sense in programming language parsers, but for natural languages it makes little sense to transform the input because the grammar has poor coverage. It is trivial to add (weighted) glue rules that accept any input string, or a finite-state acceptor of strings can be used as a back-off grammar to improve coverage. Speech repair and other such cases are typically handled using appropriate augmentations of the under-lying grammar combined with grammar-driven edits (Charniak and Johnson 2001).
Despite this, error-repair is a good use-case for parsing schemas. G  X  omez Rodr  X   X guez can show that some existing error-repair parsers are in fact provably correct, and also a generic recipe can be given that converts any given parser schema into an error-repair parser schema. This is an instructive use of parsing schema transformations, because it is easy to show that the changes preserve correctness.

Parsing schemas for dependency parsers. This third part of the book has the potential to be the most popular. There is increased interest in multilingual dependency parsing, and there are a large number of different dependency parsing algorithms. Parsing schemas 882 allow a concise description of many different parsing algorithms, and G  X  omez Rodr  X   X guez provides many parsing schemas corresponding to popular dependency parsing algo-rithms; there are too many to list here, but he provides schemas for no less than ten dependency parsers, including some that recover non-projective dependencies. He also provides explicit relationships between schemas for these varied parsers, such as item refinement (an item deduced in one parser is broken up as multiple items in another parser) or step refinement (a deduction step in one parser can be emulated by a sequence of steps in another parser). It is also useful that these relationships are transitive and reflexive. However, it is in describing dependency parsing that the biggest weakness of parsing schemas is exposed and its potential role as an universal language for the parsing community runs into trouble. Non-constructive aspects of parsing cannot be represented with a schema, because that violates the semantics of deductive steps.
For instance, in a parser that computes the dependency tree by using the minimum spanning tree (MST) algorithm (McDonald et al. 2005), there is a step that eliminates cycles in the graph. This step is not constructive and therefore the MST parser cannot be represented as a schema. Parsing schemas are generally grammar-driven and often parsers are written without any finite underlying grammar, which makes tree building harder to describe concisely.
 (Shieber, Schabes, and Pereira 1995), Datalog for specifying parsers (McAllester 2002;
Liu and Stoller 2003), the DyALog system (Villemonte de la Clergerie 2005), and Dyna (Eisner, Goldlust, and Smith 2005). It is true that Dyna is quite powerful because it is a full general-purpose declarative programming language, but for that reason it offers an attractive alternative to parsing schemas. On the other hand, schemas do allow formal reasoning about parsers that may be more fine-grained than is possible in Dyna.
Surprisingly, work on semiring parsing (Goodman 1998, 1999) is not mentioned. The use of probabilities or weights is generally ignored in this book, even though it enables interesting methods for speeding up parsers such as coarse to fine parsing (Goodman 1997) or generalized A  X  search for parsing (Pauls and Klein 2009). While there is more than enough content in this book, it does not cover the use of parsing schemas in machine translation. In particular, formal properties of schemas might make it easier to describe and implement the integration of language models into parsing algorithms for synchronous context-free grammars (Chiang 2007). Schemas might have much to offer with respect to proving correctness in machine translation decoders. the power of schemas to represent parsing algorithms succinctly and to prove them correct. They might also be interested in showing relationships between their novel parsing schemas and other well-known parsers, or showing how extensions to existing parsers are well justified. Dependency parsing enthusiasts who want to wrap their head around the many different parsing algorithms out there might also be interested in the concise description of such parsers.
 References

Rada Mihalcea and Dragomir Radev (University of North Texas and University of Michigan) Cambridge, UK: Cambridge University Press, 2011, viii+192 pp; hardbound, ISBN 978-0-521-89613-9, $65.00 Reviewed by Chris Biemann Technische Universit  X  at Darmstadt
Graphs are ubiquitous. There is hardly any domain in which objects and their relations cannot be intuitively represented as nodes and edges in a graph. Graph theory is a well-studied sub-discipline of mathematics, with a large body of results and a large number of efficient algorithms that operate on graphs. Like many other disciplines, the fields of natural language processing (NLP) and information retrieval (IR) also deal with data that can be represented as a graph. In this light, it is somewhat surprising that only in recent years the applicability of graph-theoretical frameworks to language technology became apparent and increasingly found its way into publications in the field of computational linguistics. Using algorithms that take the overall graph structure of a problem into account, rather than characteristics of single objects or (unstructured) sets of objects, graph-based methods have been shown to improve a wide range of NLP tasks. In a short but comprehensive overview of the field of graph-based methods for
NLP and IR, Rada Mihalcea and Dragomir Radev list an extensive number of techniques and examples from a wide range of research papers by a large number of authors.
This book provides an excellent review of this research area, and serves both as an introduction and as a survey of current graph-based techniques in NLP and IR. Because the few existing surveys in this field concentrate on particular aspects, such as graph clustering (Lancichinetti and Fortunato 2009) or IR (Liu 2006), a textbook on the topic was very much needed and this book surely fills this gap.
 first part gives an introduction to notions of graph theory, and the second part covers natural and random networks. The third part is devoted to graph-based IR, and part
IV covers graph-based NLP. Chapter 1 lays the groundwork for the remainder of the book by introducing all necessary concepts in graph theory, including the notation, graph properties, and graph representations. In the second chapter, a glimpse is offered into the plethora of graph-based algorithms that have been developed independently of applications in NLP and IR. Sacrificing depth for breadth, this chapter does a great job in touching on a wide variety of methods, including minimum spanning trees, shortest-path algorithms, cuts and flows, subgraph matching, dimensionality reduction, random walks, spreading activation, and more. Algorithms are explained concisely, using examples, pseudo-code, and/or illustrations, some of which are very well suited for classroom examples. Network theory is presented in Chapter 3. The term network is here used to refer to naturally occurring relations, as opposed to graphs being generated by an automated process. After presenting the classical Erd  X  os-R  X  enyi random graph model and showing its inadequacy to model power-law degree distri-butions following Zipf X  X  law, scale-free small-world networks are introduced. Further, several centrality measures, as well as other topics in network theory, are defined and exemplified.
 from natural language. Co-occurrence networks and syntactic dependency networks are examined quantitatively. Results on the structure of semantic networks such as WordNet are presented, as well as a range of similarity networks between lexical units.
This chapter will surely inspire the reader to watch out for networks in his/her own data. Chapter 5 turns to link analysis for the Web. The PageRank algorithm is de-scribed at length, variants for undirected and weighted graphs are introduced, and the algorithm X  X  application to topic-sensitive analysis and query-dependent link analysis is discussed. This chapter is the only one that touches on core IR, and this is also the only chapter with content that can be found in other textbooks (e.g., Liu 2011). Still, this chapter is an important prerequisite for the chapter on applications. It would have been possible to move the description of the algorithms to Chapter 2, however, omitting this part.
 the Fiedler method, the Kernighan X  X in method, min-cut clustering, betweenness, and random walk clustering. After defining measures on cluster quality for graphs, spectral and non-spectral graph clustering methods are briefly introduced. Most of the chapter is to be understood as a presentation of general graph clustering methods rather than their application to language. For this, some representative methods for different core ideas were selected. Part IV on graph-based NLP contains the chapters probably most interesting to readers working in computational linguistics. In Chapter 7, graph-based methods for lexical semantics are presented, including detection of semantic classes, synonym detection using random walks on semantic networks, semantic distance on
WordNet, and textual entailment using graph matching. Methods for word sense and name disambiguation with graph clustering and random walks are described. The chap-ter closes with graph-based methods for sentiment lexicon construction and subjectivity classification.
 unsupervised part-of-speech tagging algorithm based on graph clustering, minimum spanning trees for dependency parsing, PP-attachment with random walks over syn-tactic co-occurrence graphs, and coreference resolution with graph cuts. In the final chapter, many of the algorithms introduced in the previous chapters are applied to
NLP applications as diverse as summarization, passage retrieval, keyword extraction, topic identification and segmentation, discourse, machine translation, cross-language IR, term weighting, and question answering.

The writing style is concise and clear, and the authors succeed in conveying the most important points from an incredibly large number of works, viewed from the graph-based perspective. I also liked the extensive use of examples X  X hroughout, almost half of the space is used for figures and tables illustrating the methods, which some readers might perceive as unbalanced, however. With just under 200 pages and a topic as broad as this, it necessarily follows that many of the presented methods are exemplified and touched upon rather than discussed in great detail. Although this sometimes leads to the situation that some passages can only be understood with background knowledge, it is noteworthy that every chapter includes a section on further reading. In this way, the book serves as an entry point to a deeper engagement with graph-based methods for NLP and IR, and it encourages readers to see their NLP problem from a graph-based view. 220 examples were less detached from the text and explained more thoroughly. At times, it would be helpful to present deeper insights and to connect the methodologies, rather than just presenting them next to each other. Also, some of the definitions in Chapter 2 could be less confusing and structured better.
 than aiming at exhaustively treating the numerous tasks that benefit from graph-based methods, it cannot replace a general introduction to NLP or IR: For students without prior knowledge in NLP and IR, a more guided and focused approach to the topic would be required. The target audience is, rather, NLP researchers and professionals who want to add the graph-based view to their arsenal of methods, and to become inspired by this rapidly growing research area. It is equally suited for people working in graph algorithms to learn about graphs in language as a field of application for their work. I will surely consult this volume in the future to supplement the preparation of lectures because of its comprehensive references and its richness in examples. References
 O.S. le Si (edited by Aur  X  elie Herbelot) University of Cambridge
Berlin: Peer Press, 2011, 55 pp; paperbound, ISB N978-3-00-33516-7, on-line at peerpress.de/discoursecpp.pdf Reviewed by Lori Emerson
University of Colorado at Boulder discourse.cpp isashortcollectionofcomputer-generatedpoetryeditedbycomputational linguist Aur  X  elie Herbelot. The poetry was produced by a program, named O.S. le Si, that was derived from Herbelot X  X  research on context-based ontology extraction from text (Herbelot and Copestake 2006; Herbelot 2009). In this case, Herbelot provided 200,000 pages from Wikipedia for the program toparse and output lists of items whose contextissimilartowordssuchas gender , love , family ,and illness .Forexample,Herbelot explains that content in the opening piece titled  X  X he Creation X  was  X  X elected out of a list of 10,000 entries. Each entry was produced by automatically looking for taxonomic relationships in Wikipedia X ; and, for the piece titled  X  X ender, X  she chose the  X  X wenty-five best contexts for man and woman in original order. No further changes X  (page 47).
The collection is, then, as we are told on the back cover,  X  X bout things that people say about things. It was written by a computer. X  intentionally delivers well-crafted, expressive writing,  X  X o-called poets X  X  X ave been experimentingwithproducingwritingwiththeaidofdigitalcomputeralgorithmssince
Max Bense and Theo Lutz first experimented with computer-generated writing in 1959 (Funkhouser 2007). The best-known English-language example is the 1984 collection of poems The Policeman X  X  Beard is Half-Constructed by the artificial intelligence program Racter(acollectionwhich,itwaslaterdiscovered,washeavilyeditedbyRactercreators
William Chamberlain and Thomas Etter). discourse.cpp is yet another experiment in testingthecapabilitiesofthecomputerandcomputerprogrammertocreatenotsomuch carefully and intentionally crafted, rhythmically or musically pleasing verse as broadly revealing poetry X  X oetry that is not meant to be close-read (most often to discover un-derlyingauthorialintent)butratherreadasacollectionofakindoflinguisticevidence.
Inthiscase,thecollectionprovidesevidenceoftrendsinon-linehumanlanguageusage which in turn, not surprisingly, provides evidence of certain prevailing cultural norms; for example, we can see quite clearly English-speaking, western culture X  X  continued attachment to heteronormative gender roles in  X  X ender X  (page 18): writing becomes more about programming and editing, and if the reader or critic does nothaveaccesstothecodeortoalloftheeditorialdecisionsthatcreatedthiscollection, how, then, do we judge whether the computer-generated writing is successful or not?
There X  X nodoubtthatthepiece X  X ender X  X sprovocativeandrevealing,butisita X  X ood X  poem? Perhaps the collection teaches us that, with the ever-increasing intertwinement of human and digital computer that results in the displacement of the human as sole reader X  X riter now that the computer is also a reader X  X riter alongside (and often in collaboration with) the human, these sorts of judgments of  X  X ood X  and  X  X ad X  are no longer appropriate. Once the human is no longer the sole creator and no longer in control of the final artistic or literary artifact, then we are left to judge only the process in place of judging the product; with regard to discourse.cpp , it is a process we do not have direct access to.
 extent this experimentation with the computer as reader X  X riter also comes out of early 20th century, avant-garde writing that similarly sought to undermine, if not displace, the individual intending author. Dadaist Tristan Tzara, for instance, infamously wrote  X  X O MAKE A DADAIST POEM X  in 1920 (see Tzara 1924) in which he advocates writing poetry by cutting out words from a newspaper article, randomly choosing these words from a bag, and then appropriating these randomly chosen words to create a poem by  X  X n infinitely original author of charming sensibility. X  Tzara was, of course, being typically Dadaist in his tongue-in-cheek attitude; but he was also, I believe, serious in his belief that the combination of appropriation and chance-based methods of producing text could produce original writing that simultaneously un-dermined the egotism of the author. Insofar as discourse.cpp comes out of a lineage of experimental writing invested in chance-generated writing and, later, in exploiting computer technology as the latest means by which to produce such writing, it also comesoutofacertaintraditionofdisingenuousnessthatcomesalongwiththislineage.
No matter how much Tzara and later authors of computer-generated writing sought to remove the human-as-author, there was and still is no getting around the fact that humans are deeply involved in the creation process X  X hether as cutters-and-pasters, programmers, or editors. The collection, then, is a much more complex amalgam than even Herbelot seems willing to acknowledge, as discourse.cpp is evidence of the evenly distributed reading and writing that took place between Herbelot and the program itself.
 References 924
