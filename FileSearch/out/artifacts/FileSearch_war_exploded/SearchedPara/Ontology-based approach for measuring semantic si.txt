  X  is a  X  taxonomy. Our proposal combines, in a complementary way, the fi cient between human judgement and computational measures. The 1. Introduction
The measurement of semantic similarity is exploited in several research fi elds, including arti fi cial intelligence, knowledge man-agement, information retrieval and mining, and several other biomedical applications. This notion is critical in determining the relatedness between a pair of concepts or words.

Several efforts have been made to de fi ne Semantic Similarity (SS) measures using a lexical database such as WordNet. The semantic similarity estimation between words/concepts, under-stood as their degree of taxonomical resemblance ( Goldstone, 1991 ), has many direct and relevant applications.

Some basic natural language processing tasks, such as word sense disambiguation ( Patwardhan et al., 2003 ), synonym detection ( Lin, 1998 ) or automatic spelling error detection and correction ( Budanitsky and Hirst, 2001 ), rely on the assessment of words' semantic resemblance. Other direct applications can be found in the knowledge management fi eld, including thesauri generation ( Curran, 2002 ), information extraction ( Atkinson et al., 2009;
Stevenson and Greenwood, 2005 ), semantic annotation ( S X nchez et al., 2011 ), biomedical domain ( S X nchez and Batet, 2011 )and ontology merging ( Formica, 2008 ; Gaeta et al., 2009 ) and learning ( S X nchez, 2010; S X nchez and Moreno, 2008 ), in which new concepts related to already existing ones should be discovered or acquired from texts.

The most popular way for people to compare two objects and acquire knowledge is the similarity between those two objects. For humans, it is easy to say if one word is more similar to a given word than another. For example, we can easily say that car is more similar to automobile than car is to journey . The semantic quanti-fi cation is based on semantic resources by exploiting the knowl-edge existing inside these resources. Some of the most popular semantic similarity measures are implemented and evaluated using WordNet 1 as the underlying reference ontology. Some works try to use other resources like the Wikipedia Category Graph in works of Hadj Taieb et al. (2013a, 2013b), (2012) and Zesch (2010) . In fact, there exists a great difference between WordNet and WCG.
WordNet is designed and realized by experts and includes several types of semantic relations that are expressed explicitly. As for
WCG, the categories are proposed by volunteers who often rely on them without specifying the type of semantic relation. It, there-fore, needs a pre-treatment step that would be subsequently used in this topic. The proposed measures exploited the textual (gloss) and structural (taxonomic parameters) aspects of WordNet and mainly the taxonomy  X  is a  X  . Textual measures are based on weighting the word overlapping within the glosses. However, they are not very well developed in WordNet because the glosses are very short to be used in such measures. Several measures judged as structural approaches exploited the taxonomic parameters extracted from the  X  is a  X  taxonomy. Among these approaches we can cite:
Edge-counting measures assess similarity based on the number of taxonomic links and the minimum path length between two concepts present in a given ontology ( Wu and Palmer, 1994; Leacock and Chodorow, 1998; Li et al., 2003; Rada et al., 1989 ).
Information content-based approach quanti fi es the similarity between concepts as a function of the information content (IC) that both concepts have in common in a given ontology. The basic idea is that general and abstract entities found in a discourse present less IC than more concrete and specialized ones ( Hadj Taieb et al., 2013a; 2013b); Zhou et al., 2008b; Meng et al., 2012; Tversky, 1977; S X nchez et al., 2011; Sebti and Barfroush, 2008; Seco et al., 2004 ).

Feature-based measures estimate similarity according to the weighted sum of the number of common and non-common features ( S X nchez et al., 2012 ). By features, authors usually consider taxonomic and non-taxonomic information modeled in ontology, in addition to concept descriptions (e.g., glosses) retrieved from dictionaries ( Tversky, 1977; Petrakis et al., 2006;
Rodr X guez and Egenhofer, 2003 ). However, they usually rely on non-taxonomic features that are rarely found in ontologies ( Ding et al., 2004 ) and that require the fi ne-tuning of weighting parameters to integrate heterogeneous semantic evidences ( Petrakis et al., 2006 ).
 Gloss-based measures exploit the short de fi nitions provided by
WordNet in order to quantify the overlaps between the glosses of two concepts with their semantic neighbors ( Banerjee and Pedersen, 2003; Lesk, 1986; Patwardhan and Pedersen, 2006 ).
Hybrid measures combine between measures conceived for different methods in order to merge the advantages of mea-sures ( Zhou et al, 2008a ).

In this paper, we propose a new structural measure based on taxonomical parameters which are extracted from WordNet  X  taxonomy. Mainly, our proposal used a new method for quantify-ing the subgraph formed by the hyponyms and the depth ratio between the two concepts concerned by the semantic similarity task and the lowest common subsumer. Furthermore, we treat the problem of fi ne-grained representation of synsets inside the WordNet that can affect negatively the computed similarity degree.

The rest of the paper is organized as follows. Section 2 provides an overview about the set of measure s highlighted in structure-based approaches exploiting the WordNet. Section 3 presents the different taxonomical parameters extracted from the WordNet  X  is a  X  omy and used in previous works to determine the most signi parameters in the semantic similarity topic. Section 4 provides a detailed description of our proposed structural measure. Section 5 presents the used datasets for se mantic similari ty assessment and the metrics exploited to express their performance. Section 6 reports on the evaluation and comparison of our measure against currently available ones using known benchmarks. The fi nal section is devoted to presenting our conclusions an drecommendationsforfuture research. 2. Ontology-based semantic similarity measures: WordNet as case study
WordNet is a lexical database for the English language ( Fellbaum, 1998 ). It was created and is being maintained at the Cognitive Science Laboratory of Princeton University under the direction of psychology professor George A. Miller. It groups words into sets of synonyms called synsets , provides short, general de fi nitions, and records the various semantic relations between these synonym sets. WordNet is particularly well suited for similarity measures, since it organizes nouns and verbs into hierarchies of is-a relations. Fig. 1 illustrates a fragment of the WordNet 3.0  X  is a  X  hierarchy. Several measures for determining semantic similarity between words and concepts have been proposed in the literature and most of them have been tested on WordNet. Similarity measures are applied only for nouns and verbs in WordNet (taxonomic properties for adverbs and adjec-tives do not exist). The measures can be grouped into fi ve classes: path based measures, information content (IC) based measures, gloss based measures, feature based measures, and hybrid measures. 2.1. Path-only based measures
The main idea of path-based measures is the fact that the similarity between two concepts is a function of the length of the path linking the concepts and their positions in the taxonomy.
Rada (Ra) states that ontologies can be seen as a directed graph in which concepts are interrelated mainly by means of taxonomic (is-a) and, in some cases, non-taxonomic links. A simple measure to calculate their similarity is to compute the minimum path length linking their corresponding ontological nodes via is-a links. The longer the path, the more semantically far the concepts are. concepts c 1 and c 2 in a taxonomy. Let | path( c 1 , c 2 of this path. Then, considering all the possible paths from c their semantic distance as de fi ned by ( Rada et al., 1989 ) is: dis rad  X  c 1 ; c 2  X  X  min 8 i j path i  X  c 1 ; c 2  X j X  1  X 
Hirst and St-Onge (1997) (HSO) de fi ne the similarity between concepts as a path distance between two concepts. Hirst and St-Onge categorizes the semantic relations in the WordNet into three types of relations namely extra strong relations, strong relations and medium strong relations. sim HS  X  c 1 ; c 2  X  X  C path length k d  X  2  X  Where d is the number of changes of direction in the path, and C and k are the constant parameters ( C  X  8 and k  X  1 are used by the authors); if no such path exists, sim HS ( c 1 , c 2 ) is zero and the concepts are unrelated. The following path directions are considered: upward (such as hypernymy and meronymy), down-ward (such as hyponymy and holonymy) and horizontal (such as antonymy). Due to the non-taxonomic nature of some of the relations considered during the assessment, Hirst and St-Onge 's measure captures a more general sense of relatedness than of taxonomical similarity.

Despite their simplicity, the edge counting approaches suffer from the problem of relying on the edges of the taxonomy to represent uniform distances. Sussna states that the distance represented by an edge should decrease with increasing depth ( Sussna, 1993 ). The authors consider the depth of nodes as the longest path from the root of the taxonomy to the target concept. 2.2. Path and depth based measures
Wu and Palmer (1994) (WP) proposed a new measure on semantic representation of verbs and analyzed the impact on lexical selection problems in machine translation. Wu and Palmer de fi ne semantic similarity measure between concepts c 1 and c follows: Sim where N 1 and N 2 refer to the number of  X  is a  X  links from c respectively, to the lowest common subsumer c , and H to the number of  X  is a  X  links from c to the root of the taxonomy.
Sussna (1993) (Su) uses the depth-relative scaling and several types of relations to de fi ne his semantic relatedness measure. [min ; max r ]. This weight is calculated with the local density which corresponds to the number of relations of the type r which go from c 1  X  n r  X  c 1  X  X  : w  X  c -r c 2  X  X  max r max r min r
The point in the range for a relation r linking concept c depends on the number n r of edges of the same type, leaving c which is denoted as the type speci fi c fanout factor. The fanout factor denotes the dilution of the strength of the connotation between the source and the target concepts and takes into account the possible asymmetry between the two nodes where the strength of connotation differs from one direction to that in the other direction. dist  X  c 1 ; c 2  X  X 
For example, hypernymy, hyponymy, holonymy and meronymy relations have weights between min r  X  1 and max r  X  2. As for synonymy and antonymy, he assigns the values 0 and 2.5, respectively. Finally, the semantic distance between two arbitrary nodes c 1 and c 2 is the sum of the distances between the pairs of adjacent nodes along the shortest path connecting them.
Leacock and Chodoro (1998) (LC) proposed an approach for measuring semantic similarity as the shortest path using is-a hierarchy for concepts in WordNet. The different noun hierarchies are combined into a single hierarchy so that all nodes will have a path. The taxonomic path length between concepts has been considered and it is mapped into a similarity value using a logarithmic function. In this measure, the similarity between two concepts is determined by the length of the shortest path that connects them in the WordNet taxonomy. The length of the path that is found is scaled to a value between 0 and 1 and similarity is then calculated as the negative logarithm of this value. However, they exploit only the  X  is-a  X  links and scale the path length by the depth D of the taxonomy. The Leacock and Chodorow measure is expressed as follows: sim LC  X  c 1 ; c 2  X  X  log length  X  c 1
Li et al. (2003) (Li) have proposed similarity measure that overcomes the weakness of Rada edge counting method. Rada measure quanti fi es similarity based on the path length alone which yielded good results for much constrained medical seman-tic networks. When tested for more general semantic nets like
WordNet the results were not good. Li et al. consider the shortest length L between two concepts and the depth of their lowest common subsumer H to compute similarity. The similarity between concepts c 1 and c 2 is de fi ned as non-linear function: sim Li  X  c 1 ; c 2  X  X  e  X  L e where  X  Z 0 and  X  Z 0 refer to parameters scaling the contribu-tion of the shortest path length and depth, respectively. Based on empirical study, the optimal parameters are  X   X  0.2 and  X 
Al-Mubaid and Nguyen (2006) (AMN) proposed a cluster-based measure that combines the minimum path length and the tax-onomical depth for measuring the distance between two given concepts. They de fi ne clusters for each of the branches in the hierarchy with respect to the root node. They measure the common speci fi city of two concepts by subtracting the depth of their lowest common subsumer (LCS) from the depth D , which is the depth of the taxonomy: CSpec  X  c 1 ; c 2  X  X  D depth  X  LCS  X  c 1 ; c 2  X  X  X  8  X 
The common speci fi city expresses the fact that lower level pairs of concept nodes are more similar than higher level pairs, as in Wu and Palmer approach. So, the proposed distance measure is de fi ned as follows: dis
AN  X  c 1 ; c 2  X  X  log  X  X  min 8 i j path i  X  c 1 ; c 2  X j 1  X  where  X  4 0 and  X  4 0 are the contribution factors of path length and common specify features and k is a constant ( k  X  1). Moreover, they performed their experiments by giving the same weight to two components (path length and common specify) by using  X   X   X   X  1.

Hao et al. (2011) (Ha) used the semantic distance between two
LCS in the lexical hierarchical tree based on WordNet to signify the different points and calculate the similarity of the words. They proposed the following formula for calculating the similarity between two words where  X  and  X  are the smoothing factors.
Sim Hao  X  c 1 ; c 2  X  X  1 j path  X  c 1
In the above formula, when Depth(LCS  X  c 1 ; c 2  X  )  X  0, the two words have the least common attributes and the similarity is 0.
The interval of  X  is [0,1] and increasing step is 0.1. The interval of is ( Hliaoutakis, 2005; Devitt and Vogel, 2004 ) and the increasing step is 1. The experiment demonstrates that when  X   X  0,  X  correlations reach the maximum value.

Liu et al. (2007) (Liu) presented a different measure to estimate semantic similarity between concepts in WordNet, using edge-counting techniques. The fundamental idea of this measure is based on the assumption that human judgment process for semantic similarity can be simulated by the ratio of common features to the total features between words. To achieve this consideration, they proposed two following formulae: S S the subsumer between c 1 and c 2 in hierarchical semantic nets and, and  X  are smoothing factors  X  0 o  X  ;  X  r 1  X  . The experiments showed that the best correlations are reached for S Liu 1 with (  X  respectively, r  X  0.91 and r  X  0.92 for the dataset MC30. But, these results hide the fact that their measure provides for 8 couples among 30 the 0-value, i.e., coverage of 73%. 2.3. Information content-based measures
The IC-based similarity measure was fi rst introduced by Resnik (1998) . The concept has then been modi fi ed and extended by several authors to include other methods. Although they com-monly rely on IC values assigned to the concepts in the ontology. IC-based measures are based on couples (IC computing method, IC measure). Concerning the computing IC methods, they follow two strategies: statistical corpora analysis and exploiting only the topological parameters of  X  is a  X  taxonomy known as intrinsic computing method. A complete survey of IC-measures is pre-sented in the next paragraph. 2.3.1. Similarity measures exploiting the IC
Several proposals have been discussed in research papers, in order to express as better as possible the semantic similarity between two concepts pertaining to an ontological structure such as WordNet  X  is a  X  taxonomy.
 Resnik. The Resnik measure Resnik (1995) (Re) was the fi rst to merge ontology and corpus. Guided by the intuition that the similarity between a pair of concepts may be judged by  X  the amount of shared information  X  , Resnik de fi ned the similarity between two concepts as the IC of their lowest common subsumer, LCS( c 1 , c 2 ): Sim Res  X  c 1 ; c 2  X  X  IC  X  LCS  X  c 1 ; c 2  X  X  X  13  X  Jiang-Conrath. The notion of IC is also used in the approach of Jiang and Conrath ( Jiang and Conrath, 1997 ) (JC). This approach subtracts the IC of the LCS from the sum of the IC of the individual concepts. It is worth noting that this is a dissimilarity measure because the more different the terms are, the higher the difference between their IC and the IC of their LCS will be.
 Dis
JC  X  c 1 ; c 2  X  X  X  IC  X  c 1  X  X  IC  X  c 2  X  X  2IC  X  LCS  X  c 1 Lin. The similarity measure described by Lin (1998) (Lin) uses the same elements as Dis JC , but in a different way: Sim Lin  X  c 1 ; c 2  X  X  2IC  X  LCS  X  c 1 Pirro. He proposes a new similarity measure ( Pirr X , 2009 ) (Pi) that is conceptually similar to the previous ones but is based on the feature-based theory of similarity posed by Tversky (1977) . His conceptualization is based on previous studies ( Pirr X  and Seco, 2008; Seco, 2004 ) where the concept was fi rst worked out.
According to Tversky, the similarity of a concept c 1 to a concept c is a function of the features common to c 1 and c 2 , those in c similarity between concepts can be computed as follows:
Sim tvr  X  c 1 ; c 2  X  X  3IC  X  LCS  X  c 1 ; c 2  X  X  IC  X  c 1  X   X  IC  X  c Finally, he de fi nes his own measure as follows: Sim
Meng. This measure ( Meng et al., 2012 ) (Me) is based on Lin's measure. From Eq. (15) we can see that it will monotonically increase with Sim Lin . It is expressed by the following equation: Sim
In these measures IC is computed in two principal ways, namely Corpora-based using external resources and intrinsic structure computing based on knowledge structures, which are explained below. 2.3.2. Corpora based IC calculus methods
The earliest approach to calculate IC requires a corpus of text documents related to the domain of the ontology to determine the
IC of a concept. Resnik (1995) was the fi rst to consider the use of this idea, which is inspired from the work of Shannon (1948) , for the purpose of semantic similarity judgments. His work estimates the frequencies of concepts in taxonomy using noun frequencies from the Brown Corpus of American English ( Francis and Ku 1982 ). The IC value is then calculated by negative log likelihood equation as follows: IC  X  c  X  X  log  X  p  X  c  X  X  X  19  X 
Where c refers to a concept and p to the probability of encounter-ing c in a given corpus.

The basic idea behind the use of the negative likelihood is that the more probable a concept appears, the less information it conveys. More succinctly, speci fi c words are more informative than general ones. Each noun that occurs in the corpus is counted as an occurrence of each taxonomic class that contains it as follows:
Freq  X  c  X  X   X  where Word( c ) refers to the set of words subsumed by the concept c and Count( w ) to the frequency of the word w in the corpus. Then, p ( c ) is computed as follows: p  X  c  X  X  Freq  X  c  X  N  X  21  X  where N refers to the total number of observed nouns, except those which are not subsumed by any class in the ontology.
This approach has several inadequacies, particularly those pertaining to the determination of a suitable corpus, the effort required to calculate probabilities, the need for a correct disambi-guation of each noun, and the necessity of updating such prob-abilities due to changes in the corpus.

For these reasons, several works have proposed the use of only the taxonomic structure without making recourse to external corpora. These approaches are called intrinsic information content . 2.3.3. Intrinsic IC computation methods
Several studies have indicated that knowledge resources can also be used with no need for external ones. The following paragraphs present these IC measures following their chronologi-cal appearance in the literature.

Seco et al. (2004) (Seco) present a comprehensive IC intrinsic measurement which is connected only to the hierarchical struc-ture of an ontology. The IC of a concept c depends on the concept which it subsumes. The computation equation is as follows:
IC  X  c  X  X  1 log  X j hypo  X  c  X j X  1  X  log  X  max _ nodes  X   X  22  X  where hypo( c ) refers to a function that returns the hyponyms of a given concept and max_nodes to a constant that represents the maximum number of concepts in the knowledge resource (in WordNet 3.0, max_nodes  X  82115 for Noun POS (Part Of Speech)).
Sebti and Barfroush (2008) (Sebti) use the hierarchical struc-ture of the resource and implicitly includes the depth of a target concept. This method is based on the number of direct hyponyms of each concept pertaining to the initial path of the root till reaching the target concept.

In Fig. 1 , the numbers on the left represent the number of direct hyponyms for each concept. For a better understanding of this method, Eq. (23) computes the IC of the concept vehicle # 1 # 4:
IC  X  vehicle # 1 # 4  X  X  Log 1 4 1 7 1 38 1 8 1 46 1 15 1 15
Zhou et al. (2008b) (Zhou1) present a new approach to over-come the limitations in the Seco method which considers only hyponyms of a given concept. In brief, concepts with the same number of hyponyms but different degrees of generality will be considered equally similar. In fact, Zhou et al. proposed to enhance hyponym-based IC computation with the relative depth of the concept in the taxonomy, which is integrated in a formula with tuning factor:
IC  X  c  X  X  k 1 log  X j hypo  X  c  X j X  1  X  log  X  max _ nodes  X 
In addition to hypo( c ) and max_nodes, which have the same meaning as in Eq. (22) , depth( c ) refers to the depth of the concept c in the taxonomy and max_depth to the maximum depth of the taxonomy. The parameter k is a tuning factor that adjusts the weight of the two features used in the IC formula. The authors performed experiments using WordNet with k being set at 0.5.
S X nchez et al. (2011) (Sanch) followed another strategy and did not include the depth notion. They used the hyponyms through the leaves of the hyponym tree of a concept and integrated a novel parameter, subsumers ( c ). In fact, they consider that the leaves are enough to describe and differentiate the concept from any other one. Formally, they de fi ne the leaves and subsumers of a concept c as: of concepts of the taxonomy.
 hierarchical specialization of a .

Following a similar principle in related works, they consider that concepts with many leaves in their hyponym tree are general (i.e., they have low IC) because they subsume the meaning of many important terms. The IC formula is as follows:
IC  X  c  X  X  log  X j leaves  X  c  X j
Meng et al. (2012) (Meng) present a formula that merges the principles used by Seco and Zhou. They have also changed the term j hypo  X  c  X j by another term to better express the hyponyms contribution in the IC of a concept. The novel term integrates the depth notion in the Seco formula.
 Hence, the method is expressed as follows:
IC  X  c  X  X  log  X  depth  X  c  X  X  log  X  max _ depth  X  1 log  X 
For a given concept c , depth( c ) refers to the depth of concept c in the taxonomy, max_depth to the maximum depth in the taxonomy, and max_nodes to the maximum number of concepts that exists in the ontology.

Hadj Taieb et al. (2013a , 2013b) (Hadj) proposed a multi-strategic approach for measuring semantic relatedness purpose that exploits the noun and verbal  X  is a  X  taxonomies and a weighting mechanism for overlapped words in glosses. A novel
IC computing method was used in each strategy. This method quanti fi es the subgraph formed by the ancestors of a target concept c as described in Fig. 2 . They calculate the contribution of each ancestor pertaining to the subgraph modeling the IC. The IC of a given concept Con is computed as follows:
IC  X  Con  X  X   X  c A Hyper  X  Con  X  Score  X  c  X  AverageDepth  X  Con  X  X  27  X  where Score(c) refers the contribution of each ancestor (hyper-nym) pertaining to the set Hyper( c ). This score is computed using taxonomical parameters hyponyms number (direct and indirect descendants) and the depth (the longest path between the con-cept c and the root of the noun  X  is a  X  taxonomy entity# 1 # 1) as follows: where c and c 0 are the concepts (which are represented by synsets in WordNet). DirectHyper( c ) is the set containing the direct parents of the concept c and Hypo( c ) is the set of direct and indirect descendants including the concerning concept.
As for the term AverageDepth( Con ) is used to give an informa-tion about the vertical distribution of the subsumers subgraph.
The set Hyper( Con ) contains the ancestors of the concept Con including itself.
 AverageDepth  X  Con  X  X  1 j Hyper  X  Con  X j  X  c A Hyper  X  Con  X  2.4. Feature-based measures
Different from all the above presented measures, feature-based measure attempts to exploit the properties of the ontology to obtain the similarity values. It is based on the assumption that each concept is described by a set of words indicating its proper-ties or features, such as their  X  glosses  X  in WordNet. When two concepts have more common characteristics and less non-common characteristics, they are more similar.

Tversky (1977) (Tver) proposes a measure, which argues that similarity is not symmetric. Features between a subclass and its superclass have a larger contribution to the similarity evaluation than those in the inverse direction. For example, for the words second than the inverse sense. It is de fi ned as follows: and c 2 respectively, k is adjustable and k A [0,1].

From formula (30) it is noted that, the values of Sim tversky vary from 0 to 1 and it increases with commonality j  X   X  c and decreases with the difference between the two concepts j  X   X  c 1  X  =  X   X  c 2  X j and j  X   X  c 2  X  =  X   X  c 1  X j .

The de fi nition of the features set is important in this measure. It of synonyms (called synsets in WordNet), de fi nitions (i.e., glosses, containing textual descriptions of word senses) and different kinds tion of this measure, we consider features of a given concept c the set of synsets which is formed by its ancestors in the taxonomy a  X  ; the meronyms, holonyms and attributes of each ancestor and the hyponyms. The experiments are performed with k  X  0.5.
Rodr X guez and Egenhofer (2003) (RE) computed the similarity as the weighted sum of similarities between synsets, features (e.g., meronyms, attributes, etc.) and neighbor concepts (those linked via semantic pointers) of evaluated concepts: where  X  ,  X  and  X  weight the contribution of each component, which depend on the characteristics of the ontology and S represents the overlapping between the different features, com-puted as follows: S  X  c 1 ; c 2  X  X  where A and B are the terms evaluated for concepts corresponding of terms in B but not in A . Finally,  X  ( c 1 , c 2 ) is computed as a function of the depth of c 1 and c 2 in the taxonomy as follows:  X  c  X  X  X 
In the implementation of this measure, we consider S synsets the set of words that compose the synset of c . For S features include all synsets which are related to the ancestors of c in the taxonomy  X  is a  X  by the relations  X  part of  X  ,  X  attribute  X  similar to  X  . As for S neighbors  X  c  X  , we take in consideration the neighborhood of the concept c which is formed by synsets linked are performed with  X   X   X   X   X   X  1.

Petrakis et al. (2006) (Pe) proposed a feature-based function called X-similarity based on the overlapping between synsets and the concept's glosses extracted from WordNet (i.e., words extracted by parsing term de fi nitions). They consider that two concepts are similar if their synsets and glosses and those of the concepts in their neighborhood (using Semantic Relations (SR)) are lexically similar. The function is expressed as follows:  X 
The similarity for the semantic neighbors S neighborhoods lated as follows:
S where each different semantic relation type (i.e., is-a and part-of in WordNet) is computed separately and the maximum (consider-ing all the synsets of all concepts up to the root of each hierarchy) is taken. Equivalently, the similarity for both glosses S synonyms S synsets are computed as follows:
S  X  c ; c 2  X  X 
Where A and B denote the set of synsets or glosses for the concepts c and c 2 .

In our implementation of this measure, S synsets  X  c  X  is the set of words included into the synset c . As for S glosses  X  c  X  stems that compose the concept c ancestors' glosses. Moreover,
S neighbors  X  c  X  includes the synsets which are linked via  X  part of  X  relation until the deep 5.

Feature-based measures exploit more semantic knowledge than edge-counting approaches. They evaluate both commonal-ities and differences of compared concepts. However, by relying on features like glosses or synsets (in addition to taxonomic and non-taxonomic relationships), those measures limit their applicability to ontologies in which this information is available. Another problem is their dependency on the weighting parameters that balance the contribution of each feature. In all cases, those parameters should be tuned according to the nature of the ontology and even to the evaluated concepts. This hampers their applicability as a general purpose solution. Only the de fi
Petrakis et al. (2006) does not depend on weighting parameters, as the maximum similarity provided by each feature alone is taken. Pirr X  (2009) proposes a measure based on Tversky approach.
The common and different features were de fi ned in terms of information theoretic domain. The rede fi ned Tversky formulation of similarity is given by Eq. (16) . 2.5. Gloss-based measures
This kind of measures is used to compute semantic relatedness and not only the semantic similarity. It exploits the hypothesis that words are similar if their contexts are similar ( Harris, 1954 ).
Lesk (1986) (Lesk) proposes an idea that exploits the already cited principle. In fact, he supposes that related word senses are (often) de fi ned using the same words. Therefore, he uses the
WordNet glosses of synsets which are considered as accurate and short de fi nitions. For example the word bank has two synsets which are: bank#1#2:  X  a fi nancial institution  X  bank#2#2:  X  sloping land beside a body of water  X  and the word lake has one synset: lake#1#1:  X  a body of water surrounded by land  X 
So, the semantic relatedness is quanti fi ed as the gloss overlaps which is equal to the content words common to two glosses. Thus, lake# 1 # 1)  X  0.

This measure includes some inconvenient due to the shortness of the most of glosses which reduce the number of overlaps that can be found. So, the solution has been proposed by Banerjee and Pedersen (2003) to extend the Lesk measure.

Banerjee and Pedersen (2003) (BP) proposed a measure named extended gloss overlap (EGO) derived from Lesk measure ( Lesk, 1986 ). This measure is based on the number of shared words (overlaps) in their de fi nitions (glosses). This measure extends the glosses of the concerned concepts to include the glosses of other concepts to which they are related according to a given concept hierarchy. In fact, Lesk simply summed up overlapped words, but in Banerjee and Pedersen (2003) , the authors assign to a group of n overlap words the score n 2 . For example, let's suppose that the expression  X  court of law  X  is shared by the glosses of the target concepts. So, the score is equal to 3 2  X  9. This measure exploits the glosses of neighbors to the target synsets which are related by some relations such as hypernyms (  X  car  X  - X  vehicle  X  ), hyponyms (  X  car  X  - X  convertible  X  ), meronyms (  X  car  X  - X  accelerator (  X  car  X  - X  train  X  ), also-see (  X  enter  X  - X  move in  X  ), attribute ( sure  X  - X  standard  X  ) and pertainym (  X  centennial  X  - X 
Some others gloss-based measures construct co-occurrence vectors that represent the contextual pro fi le of concepts (context vectors). To build the context vectors, they extract contextual words (using a fi xed window of context) from the glosses assigned to concepts. These vectors capture a more general sense of concept likeness, not necessarily reduced to taxonomical similarity but also to inter-concept relatedness. The semantic relatedness of two concepts c 1 and c 2 is computed as the cosine of the angle between their context vectors.

Patwardhan and Pedersen (2006) 's (PP) measure created vec-tors from term glosses extracted from WordNet, calling them gloss vectors (GV). Glosses are brief notes about the meaning of a particular word sense. As a result, the gloss vector measure was able to obtain good correlation with regards to human judgments in several domain independent benchmarks.

The gloss vector (pairwise) measure (GVP) is very similar to the  X  regular  X  gloss vector measure, except in the way it augments the glosses of concepts with adjacent glosses. The regular gloss vector measure fi rst combines the adjacent glosses to form one large  X  super-gloss  X  and creates a single vector corresponding to each of the two concepts from the two  X  super-glosses  X  . The pairwise gloss vector measure, on the other hand, forms separate vectors corre-sponding to each of the adjacent glosses (does not form a single super gloss). For example separate vectors will be created for the hyponyms, the holonyms, the meronyms, etc. of the two concepts.
The measure then takes the sum of the individual cosines of the corresponding gloss vectors, i.e., the cosine of the angle between the hyponym vectors is added to the cosine of the angle between the hlonym vectors, and so on.

Pesaranghader et al. (2013) (Ah) attempt to improve the gloss vector semantic relatedness measure for a more accurate estimation of relatedness between two input concepts. Their measure criticizes the low/high frequency cut-off phase applied on the matrix of the fi rst order co-occurrence. In fact, this step of cutting causes loss of signi fi cant information that can be useful in semantic relatedness computing. So, they propose a measure based on Pointwise Mutual Information (PMI) to overcome the forgoing problems. Their experiments have been performed on resources of biomedical domain. Moreover, Pesaranghader and Muthaiyah (2013) propose a semantic similarity measure for ontologies aligning purpose. Their measure is based on concepts' de fi nitions such as glosses in WordNet. In fact, these de fi are used to compute the information content vector for each concept based on the probability vector extracted from its de tions. They, then apply the cosine measure to estimate the degree between two given concepts. Experiments are performed in the biomedical domain. 2.6. Hybrid measures
Hybrid measures combine the structural characteristics described above (such as path length, depth and local density) and some of the above presented approaches.

Zhou et al (2008a) (Zhou2) proposed a measure that takes information content measures and path based measures as para-meters. They used a tuning factor to control the contribution of each point (in their experiment k  X  0.5). Their measure is expressed by the following equation: Sim zhou  X  c 1 ; c 2  X  X  1 k log  X  len  X  c 1
The already cited different methods pertaining to a variety of approaches exploited the topological parameters extracted for the WordNet  X  is a  X  taxonomy. Authors in their works suppose that the taxonomical structure is organized in meaningful way. So, they try to express better the semantic content of concepts (synsets in WordNet) to simulate the intellectual human process and to ensure an expressive semantic similarity comparison between concepts or words. Following the same way but with a different vision, we treat the different topological parameters composing the  X  is a  X  taxonomy to deduce the more semantic expressive ones. These parameters are combined to conceive a new taxonomical-based approach that will be detailed in next section.
Table 1 summarizes the related works according to the differ-ent components of WordNet. We also specify the nature of each measure: semantic similarity (SS) or semantic relatedness (SR). In general, the taxonomy  X  is a  X  is exploited for SS computing because it models the characteristics sharing. However, the use of whole semantic network including several relations kinds (  X  as the SR measure. Although the main focus of this paper is on the semantic similarity task, we cite other measures designed for the semantic relatedness purpose because they exploit other WordNet components. They are indicated in this paper to study the contribution of WordNet features other than  X  is a  X  taxonomy into semantic computing.

Table 2 shows the values of the different parameters needed for computing the semantic similarity between two concepts  X  car# 1 # 5  X  and  X  room# 1 # 4  X  (see Fig. 1 ). Those parameters are then exploited to quantify similarity using the various measures cited in the table. This table shows that the estimated values pertain to a variety of scales. Therefore, for speci fi c application based on SS or SR semantic measure, the interpretation of provided similarity estimation for a word couple differs from a method to another.
 3. WordNet  X  is a  X  taxonomical parameters studying
In WordNet each word is expressed by a set of synsets (considered as concepts) that represents the possible meanings of the concerned word. The taxonomy  X  is-a  X  is mainly used to measure the degree of similarity between concepts or words. In order to compute the semantic similarity degree, authors exploited the taxonomical parameters to express the likeness between two concepts. Table 3 contains statistics concerning
WordNet 3.0 where each index word is represented by a fi xed number of synsets (concepts) referring to its different meanings.
The proposed measure in this paper uses the nominal and verbal  X  is a  X  taxonomies of WordNet which are more signi semantics computing. In this section, we study the taxonomical parameters of the WordNet  X  is a  X  taxonomy which are exploited to design based hierarchies measures (see Table 1 ). According to the related works presented in the previous section, the used para-meters are mainly: descendants (hyponyms), depth, leafs and ancestors (hypernyms). This study has as purpose the semantic interpretation, detection of dependencies between these para-meters and determining their probability distribution.
Fig. 3 shows the number of nouns (a) and verbs (b) represented by n synsets in y -axis. The shape of the two parts (a) and (b) are very similar despite the fact that the maximum number of noun synsets is 33 and for verb is 59.
 3.1. The depth taxonomical signi fi cance
The depth of a concept is a parameter used in several measures for SS computing (see Table 1 ). In fact, the concepts existing at the top of the taxonomy refers to a general concept. But another concept, which exists at the bottom, represents a speci fi which is semantically richer.

The depth is signi fi cant to determine the speci fi city of a concept because going up on the taxonomy from a level to another generates the data propagation towards the descendants with addition of certain speci fi cities. The transition from concept c towards concept c 2 using the  X  is a  X  relation does not mean the passage from depth i to depth i  X  1. Thus, two concepts, directly connected, do not have necessarily successive depths in  X  taxonomy (for example in Fig. 1 , the synsets instrumentality# 3 # 3 and container# 1 # 1 are directly connected but the depth is equal to 5 for the fi rst synset and for the second synset, depth  X  7). So, while going down from the root towards any target concept, it includes data enrichment. Indeed, a subordinate concept inherits the basic features from the superordinate concept and adds its own speci fi c features. Fig. 4 shows that the depth distribution is
Gaussian for the noun  X  is a  X  taxonomy (4 a ), with parameters  X  5.4143 and  X   X  8.5484, and also for the verbal one (4 b ), with  X  2.5403 and  X   X  2.5292.

Fig. 4 shows that the majority of nominal synsets have a depth between 6 and 11. As for verbal synsets (4b) the depth is mainly between 0 and 4. This fact illustrates the marked difference between the nominal and verbal  X  is a  X  taxonomy. 3.2. The hyponyms: signi fi cance and quanti fi cation methods
The set of hyponyms subsumed by a concept c , noted as Hypo depth, to determine the generality/speci fi city feature. The hyponyms distribution presented in Fig. 5 shows that the majority of synsets are considered as speci fi c concepts. Indeed, a concept that subsumes a great number of hyponyms (direct and indirect descendents) is considered as a general concept and the inverse leads to a concept more speci fi c. Thus, it coincides with the notion of information content (IC). Indeed, a general concept containing an important set of hyponyms is considered as more probable. So, it is represented by low information content. But, there is not the case for leafs concepts which have an important information content. Therefore, this parameter has been used by Seco et al. (2004) as another alternative for IC computing independent from corpus processing task to compute the probability of each concept pertaining to the  X  is a  X  taxonomy.

In literature, there are two methods, cited in previous section, for quantifying the hyponyms subgraph ( HypoValue ) formed by direct and indirect descendants of a given concept into the second is proposed by Meng et al. (2012) . The fi rst method takes the cardinality of the hyponyms set (Eq. (38) ) and the second exploits the depth in order to take in consideration the speci of each concept pertaining to the set of hyponyms (Eq. (39) ). Hypo Value  X j hypo  X  c  X j X  38  X 
Hypo Value  X   X 
The parameters depth and hyponyms subgraph can be used in complementary way. This is explained by the fact that two leafs have the same information content if we consider only the hyponyms number. So, it is important to invoke the depth parameter that can be exploited to have a more signi fi cant semantic quanti fi cation (for example: the hyponyms number can be multiplied by the depth).
 3.3. Taxonomical ancestors meaning
The ontologies modeling multiple inheritances may incorpo-rate several direct subsumers per concept. In WordNet 3.0, 2.28% nodes of the taxonomy are with multiple inheritances ( Devitt and
Vogel, 2004 ). However, as they are normally distributed through the depth of the WordNet taxonomy, the effect of multiple inheritances is invoked to specializations at a deeper-level ( Devitt and Vogel, 2004 ). Ontology-based related works ( Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Li et al., 2003;
Lin, 1998; Rada et al., 1989; Pirr X , 2009; Wu and Palmer, 1994 ) consider, in case of multiple inheritance, only the subsumer that de fi nes the maximum of shared characteristics between the target concepts pair.

However, when a concept inherits from several subsumers it becomes more speci fi c than another one inheriting from a unique subsumer. Due to this reason, in order to better differentiate concepts, the de fi nition of taxonomical subsumers recursively considers the whole set of generalizations of a concept by explor-ing all the taxonomical branches to which it belongs. With this strategy, it represents a broader and more realistic notion of concept's concreteness than other works based solely on the taxonomical depth ( Zhou et al., 2008b ). Hadj Taieb et al. take in consideration the multiple inheritances in their information content computing method ( Hadj Taieb et al., 2013a , 2013b ). They assign to each ancestor a score according to its depth in the taxonomy  X  is a  X  .

The number of subsumers can be considered as the depth parameter because the multiple inheritance is not frequent within WordNet  X  is a  X  taxonomy. In fact, the parameters of the normal distribution in Fig. 6 are very close to the parameters already cited for the depth distribution of the nouns (6a) (  X  2  X  8.2442 and  X   X  9.0512) and verbs (6b) (  X  2  X  2.5954 and  X   X  3.5393) 3.4. Leaves taxonomical interpretation
Exploiting leaf concepts is relied to the fact that some inner nodes of the hyponym subgraph of a concept present multiple taxonomical inheritances may cause that several paths exist from that concept to a leaf. Following a similar principle as in related works, we consider that concepts with many leaves in their hyponym subgraph are general as they subsume the meaning of many salient concepts. Leaves, on the other hand, will present equally specialized concepts as they are completely differentiated from any other concept in the taxonomy. The leaves number can replace the hyponyms number because the leaves accounts 79% of the whole nodes in  X  is a  X  WordNet taxonomy.
 3.5. Discussion
The previous paragraphs go some way to outlining the topology of WordNet. We have looked at the distributions of depth and hyponyms, and the notion of multiple inheritance and its sig-ni fi cance within nouns and verbs  X  is a  X  taxonomy. The parameters distributions of these two taxonomies are similar, with a slight difference that characterizes the verbs  X  is a  X  taxonomy which is less deep and multi-roots (560 in WordNet 3.0). In fact, concepts at higher levels of a hierarchy tend to represent more general and abstract meanings while those lower down tend to represent more speci fi c meanings. So, the same path length at different levels represents different distances.

To overcome these issues, the notion of depth has been incorporated to account for speci fi city. The motivation is that two nodes are semantically closer if they reside deeper in the hierarchy. Thus the measure of speci fi city should re fl monotonic nature. Given a node c in a taxonomy T , the depth of c denoted by depth ( c ) is the number of nodes along the longest path between c and root , and the depth of the taxonomy is the depth of the deepest node. In contrast, the hyponyms or leaves nodes number are used to refer to a general concept having a general meaning. Indeed, when the hyponyms number decreases, i.e. the concept is less general. Therefore, in our semantic similar-ity measure we choose to exploit the hyponyms number and the depth to compute as better as possible the semantic similarity between two concepts or two words. 4. The proposed taxonomical semantic similarity measure
The estimation of semantic similarity between two concepts is based on the quanti fi cation of the common properties which are well modeled into the  X  is a  X  taxonomy representing the inheri-tance paradigm between concepts. In the  X  is a  X  hierarchical structure, a subordinate concept inherits the basic features from the superordinate concept and adds its own speci fi c features to form its meaning. Thus, a concept is an accumulation of the propagated information from a distant ancestor to another less distant by adding speci fi cities to each descendant. Therefore, the common semantic amount of two concepts is represented by the lowest common subsumer (LCS). A concept depends strongly on its direct parents and ancestors.

For example, in relation with the Fig. 7 , the LCS( communica-tor# 1 # 1, golf club# 2 # 2) is whole# 2 # 2 which re fl which re fl ects a high similarity degree.

In our proposal, the speci fi city degree of a concept is quanti as a product of the more signi fi cant parameters deduced from the previous study: the hyponyms subgraph (TermHypo) and the depth (TermDepth). 4.1. The hyponyms subgraph quanti fi cation
The goal of hyponyms subgraph quanti fi cation is to measure the speci fi city of a concept pertaining to a WordNet  X  is a taxonomy. Differently to Seco (Eq. (38) ) and Meng (Eq. (39) ) processes followed to quantify the hyponyms (descendents) sub-graph of a concept c , our proposed method HypoValue( c ) is based on the depth probability distribution over the WordNet  X  is a taxonomy. In fact, we proceed as follows: HypoValue  X  c  X  X   X  where Hypo ( c ) is the hyponyms set of the concept c and depth ( c ) represents the length of the longest path between a given concept as follows: P  X  depth  X  c  X  X  X  jf c 0 With C is the set of concepts pertaining to the WordNet  X  taxonomy and N the cardinality of the set C .

The speci fi city of a general concept is low because it subsumes as follows: Spec
Hypo  X  c  X  X  1
Where max  X  HypoValue  X  root  X  X  43  X 
Then, the semantic similarity degree based on hyponyms parameter TermHypo ( c 1 , c 2 ) is calculated as Dice coef ing the LCS that represents the shared information between target concepts:
TermHypo c 1 ; c 2  X  X  X  Where Spec Hypo ( c ) is based mainly on the quanti fi cation method
HypoValue ( c ) (Eq. (40) ) of the hyponyms subgraph of the concept c . 4.2. The depth parameter
As we have already stated, our proposal is based on two parameters the hyponyms and the depth. So, for the second term TermDepth , it applies the Dice coef fi cient on the depth parameter:
TermDepth c 1 ; c 2  X  X  X  2Depth LCS c 1
Polysemous words in WordNet are represented by a number of synsets that details their different meanings. Some words are referred by a great number of synsets that is known as fi granularity problem (view Table 4 ). Therefore, high similarity values can be provided for a non-similar words couple due to a meaning rarely exploited. So, the Spec Depth is corrected by a factor that tries to resolve the fi ne granularity of word senses in
WordNet. For instance, Table 4 contains some examples in differ-ent parts of speech (POS) of words that are represented by a great number of synsets.

The important number of synsets assigned to a word can affect negatively the semantic similarity estimation between two words.
In fact, the measure provides an important degree for two words non-semantically related. So, we have proposed an adjustment factor that exploits the synsets number of each word. This factor is expressed as follows:
 X   X  w ; w 2  X  X  max  X j Syn  X  w 1  X j ; j Syn  X  w 2  X j X  =  X   X  46  X 
With  X  is equal to the maximum number of synsets assigned to a word into  X  is a  X  taxonomy. For example  X   X  33 for the WordNet nominal taxonomy and  X   X  59 for the verbal one.

Then, the semantic similarity between two words w 1 and w the maximum value provided using the different synsets of target words:
SemSim w 1 ; w 2  X  X  X 
Where Syn ( w 1 ) and Syn ( w 2 ) are the sets of concepts (synsets) pertaining to the ontological hierarchy and represent the words w and w 2 , respectively. The semantic similarity between two con-cepts c 1 and c 2 is computed as follows:
SemSim  X  c 1 ; c 2  X  X j TermDepth  X  c 1 ; c 2  X   X   X  w 1 ; w
The proposed method is computed using two terms, the fi rst is based on hyponyms and the second exploits the depth parameter.
As example, Fig. 8 illustrates the  X  is a  X  taxonomy fragment containing whole synsets of the verbs pair  X  recognize  X  and
If, we consider the equation without adjustment factor (  X  in consideration the senses number of each word, we found: SemSim  X   X  welcome  X  ;  X  recognize  X   X  X  0 : 7203 Now, the use of  X  brings a remarkable improvement. In fact, (  X  recognize  X  ,  X  welcome  X  )  X  max(3,9)/59  X  0.1525. So, the similarity degree became 0.6098, which is considered closer to human judgment (0.50).

Fig. 7 shows the  X is a X  fragment including all senses of the noun pair  X  forest  X  and  X  wood  X  . The SemSim (  X  forest  X  ,  X  ignore the factor  X  . Then, including the adjustment factor
 X  degree 0.76, which is closer to human judgment 0.773. 5. Semantic similarity measures evaluations The experiments exploit the WordNet 3.0 and the package JWNL (Java WordNet Library) as an interface to query the
WordNet base. 5.1. Datasets
Table 5 contains the benchmarks formed by human judgments and used to assess different measures for semantic similarity (SS) purposes.

In this study, we have experimentally evaluated machine gener-ated measurements of semantic similarity between words and compared them against human ratings performed in the same settings. For the SS task, Rubenstein and Goodenough (1965) (RG65) obtained  X  synonymy judgments  X  of 51 human subjects on 65 pairs of words. The pairs ranged from  X  highly synonymous  X  semantically unrelated  X  . The participants were asked to rate them and Charles (1991) (MC30) extracted 30 pairs from the original 65 and then obtained similarity judgments from 38 participants. For the same purpose, Agirre et al. (2009) created a semantic similarity dataset based on Fin353 (AG203). This contains 203 pairs of terms from Fin353, each re-scored for their similarity rather than related-ness. All of the datasets mentioned above were composed of word pairs pertaining to different grammatical categories such as Noun (N), Verb (V), Adjective (A) and Adverb (A).

Yang and Powers (2006) created a dataset that contains 130 verb pairs 2 (YP130). The evaluation studies the performance of the proposed measure with the verbal taxonomy  X  is a  X  (in WordNet 3.0, 11529 verbs grouped in 13767 synsets with 560 roots).
Also,wehaveusedadatasetformedby38medicaltermpairs (MED38: Appendix B ) extracted from two different biomedical data-sets ( Pedersen et al., 2007; Hliaoutakis, 2005 ) and existing within
WordNet. In fact, the fi rst one ( Pedersen et al., 2007 )iscreatedin collaboration with Mayo Clinic experts and it is a set of word pairs referring to general medical disorders. The similarity of each concept pair was assessed by a group of 9 medical coders who were aware a fi nal set of 30 word pairs with the averaged similarity measures provided by experts in a scale between 1 and 4 were obtained. The second biomedical benchmark, proposed by ( Hliaoutakis, 2005 ), is composed by a set of 36 word pairs extracted from the MeSH repository. The similarity between word pairs was also assessed by 8 medical experts from 0 (non-similar) to 1 (synonyms). Often, these datasets are tested with biomedical ontologies (SNOMED CT and MeSH) because some medical terms do not exist in WordNet. 5.2. Evaluation metrics
Semantic similarity measures can be evaluated using correla-tion coef fi cients to correlate the scores computed by a measure with the judgments provided by humans in different datasets. The Pearson product-moment correlation coef fi cient r can be employed as an evaluation metric. It indicates how well the results of a measure resemble human judgments, where a value of 0 means no correlation and 1 means perfect correlation.
Pearson's r is calculated as follows: r  X  n  X  where x i refers to the i th element in the list of human judgments and y i to the corresponding i th element in the list of SS values computed by a measure and n is the number of word pairs.
The standard method that statisticians use to measure the  X  al., 1996 ). The p -value is a number between 0 and 1 representing the probability that this data would have arisen if the null hypothesis were true. The  X  null hypothesis  X  in this case is the statement  X  judgments and computed values are unrelated  X  The results are signi fi cant due to the very low value of p -value (i.e., p -value o 0.001). The p -value according to the Pearson corre-lation coef fi cient is computed using a web interface 3 . 6. Experiments and interpretations 6.1. Comparing the hyponyms subgraph quanti fi cation
In this paper, we propose a new method for quantifying the subgraph of hyponyms (direct and indirect descendants of a concept within  X  is a  X  taxonomy). In order to compare the different methods:
Seco et al. (2004) (Eq. (38) ), Meng et al. (2012) (Eq. (39) )andour method (Eq. (40) ), we chose to measure the semantic similarity using only the hyponyms parameter ( hypoValue )withEq. (50) and human judgments presented in already cited datasets. Eq. (50) is inspired from the work of Seco et al. that proposed an intrinsic information content computing method based only on the hyponyms parameter (Eq. (22) ). This equation exploits the lowest common subsumer of the concepts couple and the hypoValue ( root )suchas hypoValue ( Entity# 1 # 1) in the WordNet  X  is a  X  noun taxonomy.
Sim  X  c 1 ; c 2  X  X  1
Table 6 shows that our quanti fi cation method performed better than other methods for all datasets except MED38. Moreover, the good correlation values validate the importance of the hyponyms parameter into the estimation process of semantic similarity degree between two concepts or two words. 6.2. Datasets ' experiments results
In order to make a comparison between all IC-based approaches (IC computing method, IC based measure), we have implemented and performed the experiments that their results are cited in Appendix A .
In general, the IC-based approache s with the majority of IC-computing methods give good correlations ( r Z 0.80) and mainly for the datasets RG65 and MC30. The negative values provided using JC (Jiang and Conrath) measure are related to dis similarity nature of this measure.
The Meng IC computing method outperforms all others methods with the different IC-based measures (for example, r  X  0.87 with Res measure for the dataset RG65). But, with the largest dataset AG203, the performed experiments sho w a decreasing performance ( r  X  0.68 with Meng IC computing method) in relation with the correlations obtained for the datasets RG65 and MC30. Moreover, the method proposed by Seco et al. fails with the Pirro measure. But, the same measure provides good results competitive to other known IC measures (Lin, Res and JC).

The excellent correlations found with YP130 using IC-based measures (for example r  X  0.80, Hadj Taieb et al. and Lin formula) hides the coverage capacity. In fact, only 82 verb couples among 130 (63%) are treated. This is due to the great number of verbal taxonomies (560 roots) which are not well-developed having minor depth. Concerning the dataset MED38, the results re the nature of WordNet that is conceived to be exploited in general domain purpose. Indeed, the best value is the r  X  0.65.
Table 7 shows the results obtained with other taxonomical approaches in comparison with those obtained with our proposed approach. The length approach provides dissimilarity values which prove their negative correlations. In comparison with the results presented in Appendix A , we remark that IC-based approaches express better the semantic similarity than those in Table 7 . The correlations for IC based-corpora methods (Resnik, Lin, Jiang and Conrath (JC)) are computed using an external corpus.
The WordNet::Similarity 4 authors have pre-computed information content values from the British National Corpus (World Edition), the Penn Treebank (version 2), the Brown Corpus, the complete works of Shakespeare and SemCor (with and without sense tags)
The values found for MED38 are very low due to the general nature of the already cited general resources. It is arguable that IC values obtained from very general corpora may be different from those obtained with more specialized ones. However, if we were calculating similarity between concepts of very specialized ontol-ogies, such as MeSH, such corpus would presumably not contain many of the terms included in that ontology, which can affect the
IC values and corresponding similarity assessments. For these reasons, the methods based on only the taxonomic structure without making recourse to external corpora, are very important because speci fi c ontologies for the speci fi c concepts are rare.
Our measure outperforms all other measures for the largest
As for the datasets RG65 and MC30, we obtain respectively competitive results which are r  X  0.88 ( Appendix C ) and r  X  0.85 ( Appendix D ). Also, we found a good correlation r  X  0.70 that exceeds all the other results for the dataset MED38 ( Appendix F ) despite of the general nature of the WordNet resource. Our taxonomic measure obtained a gain equal to 5% in relation with the higher correlation found with the couple (Seco et al. comput-ing IC method, Meng IC-based measure: see Appendix A ). Con-cerning the second column for the dataset YP130, it indicates the number of verb couples treated among the whole 130 verbs. The coverage percentage is indicated only for the dataset YP130 because the verbs in WordNet are divided on 560 independent components representing the verbal  X  is a  X  taxonomy. So, these components are not very deeper like the noun  X  is a  X  taxonomy and the LCS does not exist for a great number of verb couples.
Therefore, the percentage coverage is considered only for the verbal dataset which is not the same case for the nominal datasets (MC30, RG65, AG203 and MED38). Moreover, our proposal pro-vides the higher correlation r  X  0.66 for YP130 ( Appendix G ) with a coverage percentage 100% which leads to a gain equal to 37% in relation with the higher correlation r  X  0.80 found with coverage equal to 63% for the couple (Hadj Taieb et al., IC computing method, Lin measure: see Appendix A ).

In order to show the impact of adjustment factor  X  ,we performed the experiments using our measure (Eq. (47) ) without
 X  . The results support its positive impact due to the improvements which reached 6% for datasets AG203 and YP130.

Table 8 shows the p -values of datasets used in the experiments with our measure according to the correlation coef fi cients in
Table 7 . As indicated in the table below, all p -value o indicate that the results obtained are signi fi cant. 7. Conclusion
The results obtained by our tax onomical-based approach out-performed the ones attained by their homologs, which suggest that the initial assumption about the taxonomic structure of WordNet is correct. This approach combines the most signi fi cant taxonomical parameters, depth and hyponym subgraph of a concept referring to speci fi city notion. The study performed through the semantic interpretation of taxonomical parameters showed that hyponyms and the depth are the most semantically expressed parameters within the
WordNet  X  is a  X  taxonomy. We, therefore, proposed a new method for hyponym quanti fi cation based on the depth probability distribution.
Moreover, we treated the fi ne grained problem with a correctness factor based on the number of synsets for the concepts involved, c and c 2 . The results demonstrated that our taxonomical measure outperforms path-based, IC-based, hybrid, and features approaches. the largest dataset AG203 treating the semantic similarity task, we reach the best correlation r  X  0.76. Also, our ontological measure shows very good correlations in relation with two speci fi verbal benchmark YP130 ( r  X  0.66) with coverage 100% and a dataset composed of medical terms MED38 ( r  X  0.70).

Our ontological-based similarity measure can be studied with other ontologies for speci fi c domains such as MeSH or SNOMED for biomedicine or GO (Gene Ontology) for biomedical domain that contains  X  is a  X  taxonomies. Moreover, we can extend the measure by including other kind of relations in its computation such as  X  part of  X  .

IC-intrinsic computing method
Reference Seco et al. IC-based measures Lin Resnik Jiang and Conrath Pirro Meng RG65 Seco et al. 0.82 0.85 0.85 0.43 0.85 Sebti and Barfroush 0.69 0.82 0.79 0.80 0.85 Zhou et al. 0.81 0.83 0.83 0.83 0.86 Sanchez et al. 0.85 0.86 0.87 0.86 0.87 Meng et al. 0.84 0.87 0.87 0.87 0.87 Hadj Taieb et al. 0.80 0.58 0.56 0.64 0.82 MC30 Seco et al. 0.80 0.81 0.84 0.35 0.83 Sebti and Barfroush 0.71 0.79 0.73 0.76 0.82 Zhou et al. 0.81 0.82 0.82 0.81 0.84 Sanchez et al. 0.82 0.84 0.85 0.86 0.85 Meng et al. 0.84 0.85 0.86 0.86 0.86 Hadj Taieb et al. 0.76 0.55 0.54 0.67 0.77 AG203 Seco et al. 0.66 0.64 0.64 0.30 0.68 Sebti and Barfroush 0.62 0.55 0.54 0.63 0.65 Zhou et al. 0.58 0.59 0.60 0.63 0.62 Sanchez et al. 0.63 0.63 0.63 0.66 0.66 Meng et al. 0.66 0.65 0.63 0.68 0.68 Hadj Taieb et al. 0.64 0.50 0.45 0.54 0.66 MED38 Seco et al. 0.64 0.62 0.65 0.32 0.65 Sebti and Barfroush 0.57 0.52 0.57 0.58 0.62 Zhou et al. 0.52 0.52 0.53 0.52 0.62 Sanchez et al. 0.56 0.54 0.57 0.56 0.61 Meng et al. 0.59 0.64 0.60 0.60 0.64 Hadj Taieb et al. 0.52 0.50 0.43 0.52 0.52 YP130 Seco et al. 0.18 0.48 0.21 0.11 0.15 Sebti and Barfroush 0.76 0.62 0.52 0.70 0.78 Zhou et al. 0.64 0.72 0.60 0.72 0.64 Sanchez et al. 0.42 0.52 0.44 0.59 0.40 Meng et al. 0.79 0.70 0.60 0.73 0.52 Hadj Taieb et al. 0.80 0.40 0.22 0.25 0.80
Word 2 Human ratings 1 Anemia Appendicitis 0.031 2 Dementia Atopic Dermatitis 0.06 3 Osteoporosis Patent Ductus Arteriosus 0.156 4 Sinusitis Mental Retardation 0.031 5 Hypertension Kidney Failure 0.5 6 Hyperlipidemia Hyperkalemia 0.156 7 Hypothyroidism Hyperthyroidism 0.406 8 Sarcoidosis Tuberculosis 0.406 9 Asthma Pneumonia 0.375 10 Lactose Intolerance Irritable Bowel Syndrome 0.468 11 Urinary Tract Infection Pyelonephritis 0.656 12 Psychology Cognitive Science 0.593 13 Adenovirus Rotavirus 0.437 14 Migraine Headache 0.718 15 Hepatitis B Hepatitis C 0.562 16 Carcinoma Neoplasm 0.75 17 Pulmonary stenosis Aortic stenosis 0.531 18 Breast feeding Lactation 0.843 19 Pain Ache 0.875 20 Measles Rubeola 0.906 21 Down Syndrome Trisomy 21 0.875 22 Renal failure Kidney failure 1 23 Abortion Miscarriage 0.825 24 Delusion Schizophrenia 0.55 25 Metastasis Adenocarcinoma 0.45 26 Calci fi cation Stenosis 0.5 27 Mitral stenosis Atrial fi brillation 0.325 28 Rheumatoid arthritis Lupus 0.275 29 Carpal tunnel syndrome Osteoarthritis 0.275 30 Diabetes mellitus Hypertension 0.25 31 Acne Syringe 0.25 32 Antibiotic Allergy 0.3 33 Multiple sclerosis Psychosis 0.25 34 Appendicitis Osteoporosis 0.25 35 Depression Cellulitis 0.25 36 Hyperlipidemia Metastasis 0.25 37 Heart Myocardium 0.75 38 Stroke Infarct 0.7 Appendix C. RG65 Cord Smile 0.03 Magician Oracle 0.18 Shore Woodland 0.04 Rooster Voyage 0.08 Crane Implement 0.31 Monk Oracle 0.17 Noon String 0.03 Brother Lad 0.18 Boy Sage 0.20 Fruit Furnace 0.15 Sage Wizard 0.19 Automobile Cushion 0.20 Autograph Shore 0.18 Oracle Sage 0.46 Mound Shore 0.38 Automobile Wizard 0.03 Bird Crane 0.46 Lad Wizard 0.18 Mound Stove 0.19 Bird Cock 0.53 Forest Graveyard 0.02 Grin Implement 0.05 Food Fruit 0.09 Food Rooster 0.01 Asylum Fruit 0.15 Brother Monk 0.87 Cemetery Woodland 0.02 Asylum Monk 0.03 Asylum Madhouse 0.93 Shore Voyage 0.13 Graveyard Madhouse 0.02 Furnace Stove 0.17 Bird Woodland 0.04 Glass Magician 0.10 Magician Wizard 0.94 Coast Hill 0.41 Boy Rooster 0.05 Hill Mound 0.85 Furnace Implement 0.23 Cushion Jewel 0.18 Cord String 0.80 Crane Rooster 0.27 Monk Slave 0.20 Glass Tumbler 0.82 Hill Woodland 0.03 Asylum Cemetery 0.02 Grin Smile 0.97 Autograph Signature 0.85 Coast Forest 0.05 Serf Slave 0.78 Coast Shore 0.88 Grin Lad 0.13 Journey Voyage 0.80 Forest Woodland 0.94 Implement Tool 0.89 Cushion Pillow 0.88 Midday Noon 0.97 Cock Rooster 0.85 Cemetery Graveyard 0.97 Gem Jewel 0.85 Boy Lad 0.80 Automobile Car 0.85 Car Journey 0.10 Cemetery Mound 0.02 Glass Jewel 0.17 Appendix D. MC30 Car Automobile 0.85 Cemetery Woodland 0.02 Gem Jewel 0.85 Glass Magician 0.06 Food Rooster 0.01 Shore Woodland 0.03 Coast Hill 0.32 Journey Car 0.05 Forest Graveyard 0.02 Noon String 0.00 Journey Voyage 0.75 Furnace Stove 0.15 Boy Lad 0.70 Rooster Voyage 0.02 Coast Shore 0.76 Coast Forest 0.04 Asylum Madhouse 0.87 Chord Smile 0.09 Magician Wizard 0.94 Lad Brother 0.15 Midday Noon 0.97 Lad Wizard 0.17 Bird Cock 0.44 Food Fruit 0.07 Bird Crane 0.38 Crane Implement 0.25 Tool Implement 0.78 Monk Oracle 0.15 Brother Monk 0.73 Monk Slave 0.18 Appendix E. AG203 Tiger Cat 0.65 Football Basketball 0.60 Drink Car 0.02 Tiger Tiger 1.0 Football Tennis 0.53 President Medal 0.02 Plane Car 0.40 Arafat Jackson 0.25 Prejudice Recognition 0.27 Train Car 0.33 Physics Chemistry 0.54 Viewer Serial 0.11 Television Radio 0.70 Vodka Gin 0.57 Peace Insurance 0.57 Media Radio 0 Vodka Brandy 0.66 Mars Water 0.03 Bread Butter 0.46 Drink Eat 0 Media Gain 0 Cucumber Potato 0.48 Car Automobile 0.85 Precedent Cognition 0.28 Doctor Nurse 0.58 Gem Jewel 0.85 Announcement Effort 0.04 Professor Doctor 0.45 Journey Voyage 0.75 Line Insurance 0.06 Student Professor 0.20 Boy Lad 0.70 Crane Implement 0.25 Smart Stupid 0.11 Coast Shore 0.76 Drink Mother 0.18 Wood Forest 0.76 Asylum Madhouse 0.87 Opera Industry 0.03 Money Cash 0.62 Magician Wizard 0.94 Volunteer Motto 0.13 King Queen 0.70 Midday Noon 0.97 Listing Proximity 0.07 King Rook 0.51 Furnace Stove 0.15 Street Place 0.04 Bishop Rabbi 0.50 Food Fruit 0.09 Monk Slave 0.18 Fuck Sex 0.58 Bird Cock 0.44 Lad Wizard 0.17 Football Soccer 0.82 Bird Crane 0.38 Sugar Approach 0.04 Food Rooster 0.01 Seafood Food 0.66 Rooster Voyage 0.08 Money Dollar 0.51 Seafood Lobster 0.66 Noon String 0.03 Money Currency 0.76 Lobster Food 0.44 Chord Smile 0.11 Tiger Jaguar 0.64 Lobster Wine 0.11 Professor Cucumber 0.04 Tiger Feline 0.69 Championship Tournament 0.52 King Cabbage 0.07 Tiger Carnivore 0.47 Man Woman 0.37 Cup Entity 0.29 Tiger Mammal 0.31 Man Governor 0.16 Jaguar Car 0.03 Tiger Animal 0.19 Murder Manslaughter 0.73 Skin Eye 0.23 Tiger Organism 0.15 Opera Performance 0.04 Century Year 0.39 Tiger Fauna 0.19 Mexico Brazil 0.42 Doctor Personnel 0.03 Psychology Science 0.64 Aluminum Metal 0.71 Travel Activity 0.29 Psychology Discipline 0.50 Rock Jazz 0.56 Five Month 0.17 Planet Moon 0.46 Shower Thunderstorm 0.32 Morality Importance 0.33 Planet Sun 0.46 Monk Oracle 0.15 Money Operation 0.05 Precedent Example 0.66 Cup Food 0.22 Delay News 0.11 Cup Tableware 0.57 Street Children 0 Practice Institution 0.60 Cup Artifact 0.24 Car Flight 0.08 Century Nation 0.04 Cup Object 0.09 Space Chemistry 0.06 Coast Forest 0.04 Jaguar Cat 0.58 Situation Conclusion 0.16 Shore Woodland 0.03 Mile Kilometer 0.36 Word Similarity 0.17 Energy Secretary 0.01 Japanese American 0.47 Peace Plan 0.04 Precedent Group 0.23 Announcement News 0.27 Consumer Energy 0.02 Production Hike 0.13 Harvard Yale 0.67 Ministry Culture 0.15 Stock Phone 0.07 Life Death 0.33 Smart Student 0.11 Holy Sex 0.01 Type Kind 0.66 Investigation Effort 0.54 Stock Cd 0.08 Street Avenue 0.72 Image Surface 0.16 Drink Ear 0.15 Street Block 0.10 Life Term 0.38 Delay Racism 0.17 Cell Phone 0.63 Start Match 0.11 Stock Life 0.03 Dividend Payment 0.59 Computer News 0.13 Stock Jaguar 0.06 Pro fi t Loss 0.36 Lad Brother 0.15 Direction Combination 0.15 Dollar Yen 0.39 Observation Architecture 0.19 Wednesday News 0.03 Dollar Buck 0.85 Coast Hill 0.32 Glass Magician 0.10 Liquid Water 0.72 Benchmark Index 0.40 Possibility Girl 0.13 Marathon Sprint 0.12 Attempt Peace 0.04 Cup Substance 0.21
Theater History 0.09 Consumer Con fi dence 0.13 Forest Graveyard 0.02 Situation Isolation 0.23 Start Year 0.12 Stock Egg 0.05 Pro fi t Warning 0.04 Focus Life 0.03 Month Hotel 0.15 Media Trading 0 Development Issue 0.20 Stock Live 0 Precedent Information 0.45 Cup Article 0.44 Morality Marriage 0.14 Architecture Century 0.03 Sign Recess 0.20 Minority Peace 0.13 Seven Series 0.10 Experience Music 0.18 Report Gain 0.05 School Center 0.21 Music Project 0.34 Appendix F. MED38 Anemia Appendicitis 0.37 Measles Rubeola 0.97 Dementia Atopic Dermatitis 0.15 Down Syndrome Trisomy 21 0.97 Sinusitis Mental Retardation 0.15 Abortion Miscarriage 0.94 Hypertension Kidney Failure 0.39 Delusion Schizophrenia 0.46 Hyperlipidemia Hyperkalemia 0.42 Metastasis Adenocarcinoma 0.06 Hypothyroidism Hyperthyroidism 0.74 Calci fi cation Stenosis 0.13 Asthma Pneumonia 0.70 Rheumatoid arthritis Lupus 0.35 Psychology Cognitive Science 0.42 Acne Syringe 0.06 Adenovirus Rotavirus 0.57 Antibiotic Allergy 0.07 Migraine Headache 0.82 Multiple sclerosis Psychosis 0.16 Hepatitis B Hepatitis C 0.79 Appendicitis Osteoporosis 0.02 Carcinoma Neoplasm 0.78 Depression Cellulitis 0.20 Pulmonary Stenosis Aortic Stenosis 0.72 Hyperlipidemia Metastasis 0.11 Breast Feeding Lactation 0.60 Heart Myocardium 0.20 Pain Ache 0.72 Stroke Infarct 0.27 Appendix G. YP130 Brag Boast 0.97 Seize Refer 0.56 Scrape Lean 0.29 Divide Split 0.91 Levy Believe 0.40 Re fi ne Sustain 0.49 Build Construct 0.84 Refer Carry 0.41 Advise Furnish 0.36 End Terminate 0.95 Remember Hail 0.42 Arrange Study 0.62 Swear Think 0.55 Alter Highlight 0.37 Swear Vow 0.73 Split Crush 0.75 Imitate Highlight 0.19 Solve Figure out 0.95 Depict Recognize 0.33 Correlate Levy 0.25 Enlarge Swell 0.33 Consume Eat 0.91 Refer Lean 0.46 Drain Tap 0.31 Demonstrate Show 0.81 Ache Spin 0.30 Lean Rest 0.39 Furnish Supply 0.94 Request Concoct 0.44 Weave Intertwine 0.58 Scorn Yield 0.32 Situate Isolate 0.33 Twist Curl 0.67 Bruise Split 0.39 Discard Arrange 0.44 Accentuate Highlight 0.60 Swear Explain 0.29 Hasten Permit 0.49 Position Situate 0.50 Merit Deserve 0.98 Supervise Concoct 0.26 Concoct Devise 0.41 Build Organize 0.35 Relieve Hinder 0.47 Furnish Impress 0.52 Hail Judge 0.44 Divide Figure out 0.51 Clean Concoct 0.38 Spin Twirl 0.88 Want Deserve 0.49 Postpone Show 0.39 Swing Sway 0.80 Rotate Situate 0.34 Empty Situate 0.38 Circulate Distribute 0.84 Expect Deserve 0.45 Sweat Spin 0.22 Yell Boast 0.35 Need Deserve 0.49 Lean Grate 0.29 Recognize Acknowledge 0.86 Swing Crash 0.32 Hinder Yield 0.44 Resolve Settle 0.66 Shake Swell 0.36 Sustain Lower 0.50 Prolong Sustain 0.89 Terminate Postpone 0.38 Arrange Explain 0.42 Tap Knock 0.81 Forget Resolve 0.46 Request Levy 0.46 Flush Spin 0.32 Hasten Accelerate 0.32 Complain Boast 0.49 Refer Direct 0.71 Challenge Yield 0.30 Boil Tap 0.31 Highlight Restore 0.23 Yield Seize 0.47 Approve Boast 0.27 Block Hinder 0.75 Arrange Plan 0.46 Recognize Succeed 0.79 Show Publish 0.42 Evaluate Terminate 0.87 Move Swell 0.71 Hail Acclaim 0.92 Seize Request 0.69 Anger Approve 0.29 Dissipate Disperse 0.91 Hinder Assist 0.48 Twist Fasten 0.41 Impose Levy 0.95 Finance Build 0.30 Swing Bounce 0.30 Stamp Weave 0.27 Seize Take 0.44 Dissipate Isolate 0.31 Rap Tap 0.81 Resolve Examine 0.53 Resolve Publicize 0.28 Make Earn 0.39 Swell Curl 0.26 Refer Explain 0.43 Approve Scorn 0.23 Build Propose 0.32 List Figure out 0.31 Acknowledge Distribute 0.42 Boast Yield 0.35 Weave Print 0.31 Hail Address 0.48 Market Sweeten 0.31 Express Figure out 0.68 Welcome Recognize 0.61 Catch Consume 0.54 Dilute Market 0.29 Supply Consume 0.57 Make Trade 0.53 Twist Intertwine 0.65 Call Refer 0.48 Swear Describe 0.43 Submit Yield 0.67 Sell Market 0.67 Clip Twist 0.28 Approve Support 0.74 Explain Boast 0.29 References
