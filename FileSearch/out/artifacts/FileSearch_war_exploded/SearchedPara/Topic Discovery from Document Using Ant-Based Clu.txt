 With the explosive growth of the World Wide Web, it has become more and more describe the topics from document in order to meet users X  information needs. 
Clustering is an un-supervised learning t echnique used in the process of topic dis-classic clustering approaches include hierarchical algorithms, partitioning methods such as K-means, Fuzzy C-means, graph theoretic clustering, neural networks cluster-rithm is inspired by the behavior of ant co lonies in clustering their corpses and sorting their larvae. One of the first studies related to this domain is the work of Deneubourg et al. [2], who have proposed a basic model that allowed ants to randomly move, pick cluster them. Lumer and Faieta [3] have developed this model from Deneubourg et al. X  X  robotic implementation to exploratory data analysis (LF algorithm). The work of textual document clustering. Several combinations of the ant algorithm and K-means clustering algorithm, such as the AntClass algorithm by Monmarche [5], the CSIM by posed. Strehl and Ghosh studied three effective cluster ensemble techniques based on a hypergraph model [8]. based on aggregation of clustering generated by different clustering techniques [9]. K. documents, which combines a method of set construction, a clustering algorithm and an iterative principal eigenvector computation method [10]. approach based on multi-ant colonies cluste ring combination. The algorithm consists of three parts. First, each document is analyzed and represented as a vector of features ings produced by three kinds of ant-based algorithms with different moving speed such as constant, random, and randomly decreasing. Finally, the topic of each cluster is extracted by re-computing the term weights. Test evaluation shows that the number system performance. The remaining of this paper is organized as follows: Section 2 describes the sys-tem architecture and principles in each step. Section 3 reports the test results evalu-ating the performance of the proposed algorithm. Finally, Section 4 offers a sum-mary of the paper. 2.1 Overview Fig. 1 shows the system architecture for topic discovery from documents. The first step is the document indexing analysis that cleans and represents the textual content of the documents using the vector space model. The second step is called document cluster-proximate description of the topic of each cluster using the highest weighted attributes. 2.2 Document Indexing Analysis algorithm. The document indexing analysis usually is of the following step: Cleaning a document is to get rid of unwanted elements of the document. The procedure for document cleaning in this algorithm includes removing tags, removal of stop-words, and stemming of words. 
After removing tags, the textual contents are extracted ignoring the textual structure  X  X he X ,  X  X nd X ,  X  X f X , etc. It is often useful to eliminate these words. Finally, word stem-ming is the process of converting different forms of a word into one canonical form, called terms. Words like  X  X alk X ,  X  X alker X ,  X  X alked X ,  X  X alking X  are all converted to a single word  X  X alk X . The Porter stemming [11] is a popular algorithm for this task. Indexing. The most commonly used document representation is the so-called vector space model introduced by Salton in 1975 [12]. In the vector space model, each document is represented by a vector of words d. Each element of the vector reflects a d = , where n j w rarely in the remainder of the collection. A well-known approach for computing term weights is the TF-IDF-weighting. The weight of a term j in a document i is given by: frequency, that is the number of documents in which the term j occurs at least once. The inverse document frequency ( idf ) factor of this type is given by 
The TF-IDF-weighting does not consider that documents may be of different lengths. For our algorithm, the TFC-weighting with length normalization is used as a term weighting equation for term j in a document i [13]: Reducing Dimensionality. When documents are represented as vectors, as described dimension for each unique term in the collection of documents. In order to reduce the dimension of the feature vector, the Document Frequency Thresholding is performed. Some terms whose document frequency are less than the predetermined threshold or appear in over 90% of the documents are removed. Further, only a small number of n terms with the highest weights in each document are chosen as indexing terms. 2.3 Document Clustering In this module, there are two phases for clustering as shown in Fig. 1. The first phase consists of three clustering components using ant-based algorithm with different moving speed such as constant, random, and randomly decreasing, each of which generates a clustering. The second phase is the combination component that aggre-gates three clusterings produced from the previous phase and generates what is called a combined clustering using the hypergraph model. Ant-Based Clustering Algorithm. The ant-based clustering algorithm is based on the basic model proposed by Deneubourg et al. [2] and some improvements [3][4][7]. First, the collection of document vectors is randomly projected onto a plane. Second, each ant chooses a document vector at random, and picks up or moves or drops down the vector according to picking-up or dropping probability with respect to the collected from the plane. 
Let us assume that an ant is located at site r at time t , and finds a document vector d other vector d j present in its neighborhood is given by: denotes the speed of the ants, and v max is the maximum speed.  X  defines a parameter to angle between the vectors d i and d j (their dot product divided by their magnitudes): wise it approaches 0. document vectors. Larger values of  X  will result in making the similarity between the vectors larger and forces vectors to lay the same clusters. When  X  is small, the simi-parameter  X  also determines the cluster number and the speed of convergence. The bigger  X  is, the smaller the cluster number, and the faster the algorithm converges. 
Fast moving ants form clusters roughly on large scales, while slow ants group document vectors at smaller scales by placing vectors with more accuracy. So we with different speed: v is a constant . All ants move with the same speed at any time; v is random . The speed of each ant is distributed randomly in [1, v max ], where v max is the maximum speed; v is randomly decreasing . The speed term starts with large value (forming clus-ters), and then the value of the speed gradually decreases in a random manner (helping ants to cluster more accurately). 
The picking-up and dropping proba bilities both are a function of f ( d i ) that converts the average similarity of a document vector into the probability of picking-up or drop-ping for an ant. The converted approaches are based on: the smaller the similarity of a document vector is (i.e. there aren X  X  many documents that belong to the same cluster picking-up probability is (i.e. documents are unlikely to be removed from dense clus-ters) and the higher the dropping probability is. The sigmoid function is used as prob-ability conversion function in our algorithm. Only one parameter needs to be adjusted in the calculation. 
The picking-up probability P p for a randomly moving ant that is currently not car-rying a document vector to pick up a vector is given by: where has a natural exponential form. Parameter  X  is a slope constant and can speed up the algorithm convergence if it is increased. ment vector is given by: transforming the given clusterings into a suitable hypergraph representation as in [8]. membership indicator matrix ) ( q  X  are 1, if the row corresponds to a document vector with known label. Rows for document vectors with unknown label are all zero. A concatenated block matrix vertex corresponding to the row is part of that hyperedge and 0 indicates that it is not. hypergraph. 
Usually, two objects are considered to be full y similar if they are in the same clus-viewed as the fraction of clusterings in which two objects are in the same cluster. The next step of clustering combination is to compute the similarity matrix Z by: where matrix H T is the transposition of matrix H , Z is N  X  N sparse matrix. ments again. The new similarity matrix Z is performed and the clustering that has the lowest outlier number, is used as an initial data set. 2.4 Topic Discovery When clusterings are generated, the topic can be extracted by re-computing the terms weights of the revealed cluster structure as in [9]. It is assumed that the best terms are TF-IDF-weighting approach is used to re-compute the terms weights as follows: quency, or the number of clusters in which the term j occurs at least once. 
For each cluster, the term weights obtained by Equation (10) give a way of finding pairs with the highest weighted attributes as below: The proposed algorithm was implemented in VC++6.0. Each document is represented as a vector of features in a vector space model using the TFC-weighting. The similar-clusterings are generated using the ant-based clustering algorithm and combined. The topic of each cluster is represented using th e highest weighted attributes. The goal of the test is to compare the quality of the results before and after the combination. 3.1 Test Data Tests draw on data from the Reuters-21578 collection 1 . The documents in the Reuters-21578 collection were collected from the Reuters newswire in 1987. We sample only news documents that have TOPICS labels. The data set contains 10 dif-ferent document collections each of size 300 documents that belong to a single-topic. 3.2 Performance Evaluation the combination in terms of metrics: F-meas ure. The F-measure combines the ideas of respect to a cluster X are defined as: number of documents in cluster X , and n T is the number of documents judged to be of topic T in the whole collection. 
The F-measure of a topic T is then given by With respect to topic T we consider the cluster with the highest F-measure to be X *. That is F-measure becomes the score for topic T . The overall F-measure is computed given by topic T , and F ( T ) is the F-measure for topic T . 3.3 Test Results ant-based clustering algorithms and their combination separately. Only 25 terms with vs. the number of manual topics 12.8. We noted that the performance of the combina-tion is better than the average performance of the single ant-based algorithm. multi-ant colonies clustering combination. Th e topic of each cluster is extracted by re-computing the term weights. The overall F-measure was used for evaluating the per-formance of the proposed method. Test results show that the performance of the com-single ant-based clustering algorithm. This work was partially funded by the Key Basic Application Founding of Sichuan Province (04JY029-001-4) and the Science Development Founding of Southwest Jiaotong University (2004A15). 
