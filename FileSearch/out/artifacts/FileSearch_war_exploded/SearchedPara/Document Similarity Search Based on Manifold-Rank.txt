 Document similarity search is to find documents similar to a query document in a text similarity search is K nearest neighbor search, namely K-NN search, which is to find K documents most similar to the query document. Similarity search is widely used in recommender systems in library or web applications. For example, Google form an advanced search with  X  X elated X  option to find similar web pages with a user-specified web page and CiteSeer.IST rently browsed paper. 
Document similarity search differs from tr aditional keyword-based text retrieval in that similarity search systems take a full document as query, while current keyword-need, while the document-based long query may contains more redundant and am-large number of words unrelated to the overall topic in the document. Cosine function, the Jaccard function, the Dice function [2, 14], the BM25 function in the Okapi system [10, 11] and the vector space model with document length normali-considered as the best model for document similarity search because of its good abil-ity to measure the similarity between two documents. To our knowledge, almost all comparison between the document and the query, thus neglecting the intrinsic global manifold structure of the documents. In order to make up for this limitation, we em-whole document. TextTile is validated to be a better text unit than the whole document in the manifold-ranking process. 
The rest of this paper is organized as follows: Popular retrieval functions are intro-conclusion in Section 5. 2.1 The Cosine Function The Cosine measure is the most popular measure for document similarity based on the calculated by the tf document d and idf total number of documents in the collection and n ment d , can be defined as the normalized inner product of the two vectors q G and d w 2.2 The Jaccard Function The Jaccard function is similar to the Cosine function as follows: 2.3 The Dice Function The Dice function is defined similarly to the Cosine function as follows: 2.4 The BM25 Function framework and is widely used in the Okapi system. Given the query document q , the similarity score for the document d is defined as follows: where t represents a unique term; N is the number of documents in the collection; n the is the frequency of term t in d ; dlf average of dlf 2.5 The Vector Space Model with Docu ment Length Normalization The vector space model with document length normalization (NVSM) is also a popu-lar retrieval model and is used in the Smart system. Given the query document q , the similarity score for the document d is defined as follows: where idf terms in d ; avef average of dlb 3.1 Overview based only on pairwise comparison; the othe r is to evaluate document similarity at a limitation of present similarity metrics based on the whole document. 
The proposed approach first segments the query and the documents into TextTiles using the TextTiling algorithm [4, 5], and then applies a manifold-ranking process on TextTiles. 
Note that it is of high computational cost to apply the manifold-ranking process to all the documents in the collection, so the above manifold-ranking process is taken as ranking process. consists of the following four steps: 1. Initial Ranking: The initial ranking process uses a popular retrieval function to 2. Text Segmentation: By using the TextTiling algorithm, the query document q 3. Manifold-Ranking: The manifold-ranking process in applied on the whole set 4. Score Fusion: The final score FinalScore(d
The steps 2-4 are key steps in the re-ranking process and they will be illustrated in detail in next sections, respectively. 3.2 The Text Segmentation Process There have been several methods for division of documents according to units such as sections, paragraphs, or fixed length sequences of words, or semantic passages given sequence of subtopical discussions that occur in the context of a few main topic dis-cussions. For example, a news text about China-US relationship, whose main topic is the good bilateral relationship between China and the United States, can be described as consisting of the following subdiscussions (numbers indicate paragraph numbers): 1 Intro-the establishment of China-US relationships 2-3 The officers exchange visits 4-5 The culture exchange between the two countries 6-7 The booming trade between the two countries 8 Outlook and summary
We expect to acquire the above subtopics in a document and use them in the mani-resent subtopics. lexical connectivity and word distribution. The main idea is that terms that describe a subtopic will co-occur locally, and a switch to a new subtopic will be signaled by the ending of co-occurrence of one set of terms and the beginning of the co-occurrence of a different set of terms. The algorithm has the following three steps: 1) Tokenization: The input text is divided into individual lexical units, i.e. pseudo-3) Boundary identification: The resulting sequence of similarity values is graphed 
For TextTiling, subtopic discussions are assumed to occur within the scope of one ments are adjacent and non-overlapping, they are called TextTiles. JTextTile [3]. 3.3 The Manifold-Ranking Process Manifold-ranking [15, 16] is a universal ranking algorithm initially used to rank data ranking is: (1) nearby points are likely to have the same ranking scores; (2) points on the same ranking scores. An intuitive description of manifold-ranking is as follows: A weighted network is formed on the data, and a positive rank score is assigned to each points then spread their ranking score to their nearby neighbors via the weighted net-points obtain their final ranking scores. 
In our context, the data points are denoted by the TextTiles in the query document q and the top documents in D formalized as follows: the TextTiles in the documents in D which assigns to each point x = . We also define a vector [] T for the TextTiles in q and y ment d The manifold-ranking algorithm goes as follows: 
In the above iterative algorithm, the normalization in the third step is necessary to work. The parameter of manifold-ranking weight  X  specifies the relative contributions reinforcement is avoided since the diagonal elements of the affinity matrix are set to zero. 
The theorem in [16] guarantees that the sequence { f ( t )} converges to where  X  =1- X  . Although f the iteration algorithm is preferable due to computational efficiency. Usually the con-scores computed at two successive iterations for any point falls below a given thresh-old (0.0001 in this study). 
Using Taylor expansion, we have From the above equation, if we omit the constant coefficient  X  , f second term is to spread the ranking scores of the TextTiles to their nearby TextTiles, TextTiles in the documents is gradually incorporated into the ranking score. 3.4 The Score Fusion Process The final retrieval score of a document d scores of its TextTiles as follows: associated document d document d malization avoids favoring long documents. Finally, the documents in D re-ranked list is returned 4.1 Experimental Setup In the experiments, the manifold-ranking based approach ( X  X R+TextTile X ) is com-pared with two baseline approaches:  X  X osine X  and  X  X R+Document X . The  X  X osine X  ments by their Cosine similarity with the query document, which is the popular way which uses the whole document instead of TextTile in the manifold-ranking process, functions. the task of topic detection and tracking [1] in 1999 and 2000. TDT-3 corpus is anno-sources for the period of October through December 1998. 120 topics are defined and about 9000 stories are annotated over these topics with an  X  X n-topic X  table presenting tion of TDT, the on-topic stories within the same topic are similar and relevant. After while the others were used as a training set. with default setting was employed to segment each document into TextTiles. The total stories are considered as the document collection for search, the first document within the topic is considered as the query document and all the other documents within the same topic are the relevant (similar) documents, while all the documents within other with the query document. For the proposed manifold-ranking process, the number of initially retrieved documents is typically set to 50, i.e. | D As in TREC lated as follows: defined above for a given query document. The precision is calculated for each query and then the values are averaged across all queries. 
Note that the number of documents within each topic is different and some topics P@10 may be low. 4.2 Experimental Results approaches (i.e.  X  X osine X  and  X  X R+Document X ) are compared in Table 1, when the manifold-ranking weight  X  is set to 0.3, which is tuned on the training set. The upper bounds are the ideal values under the assumption that all the relevant (similar) docu-ments are retrieved and ranked higher than those irrelevant (dissimilar) documents in the two baseline systems. We can also see that the  X  X R+Document X  baseline achieves almost the same P@5 value with the  X  X osine X  baseline and the higher P@10 can benefit document ranking. 
The performances of two MR-based approaches (i.e.  X  X R+TextTile X  &amp;  X  X R+Document X ) with different manifold-ranking weight  X  are shown and compared TextTile is a more appropriate unit than the whole document for the manifold-ranking process. This result can be explained by that a document is usually characterized as a sequence of subtopical discussions that occur in the context of a few main topic dis-manifold-ranking process can work at a finer granularity. 
Figure 4 explores the influence of the number of initially retrieved documents (i.e. k ) on the performance of the proposed approach (i.e.  X  X R+TextTile X ). Seen from the the re-ranking process and it will not improve the retrieval performance by increasing the number of initially retrieved documents. and the VSM with document length normalization (NVSM). We use these functions for ofthe systems using the re-ranking process are compared in Table 2. For example,  X  X ac-documents and then applying the re-ranking process to get the re-ranked document list. demonstrates the robustness of the proposed manifold-ranking process. The proposed approach re-ranks a small number of initially retrieved documents based on manifold-ranking of TextTiles. The manifold-ranking process can make full use of mental results demonstrate the favorable performance of the proposed approach. 
In future work, we will explore the influence of different text segmentation meth-ods on the retrieval performance. We will also adapt the proposed retrieval approach to search of semi-structured documents, such as XML documents and web pages. 
