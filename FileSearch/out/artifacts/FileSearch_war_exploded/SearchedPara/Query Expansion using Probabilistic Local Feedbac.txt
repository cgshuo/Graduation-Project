 As one of the most effective query expansion approaches, local feedback is able to automatically discover new query terms and improve retrieval accuracy for different retrieval models. However, the performance of local feedback is heav-ily dependent on the assumption that most top-ranked doc-uments are relevant to the query topic. Although this as-sumption might be sensible for ad-hoc text retrieval, it is usually violated in many other retrieval tasks such as mul-timedia retrieval. In this paper, we develop a robust local analysis approach called probabilistic local feedback(PLF) based on a discriminative probabilistic retrieval framework. The proposed model is effective for improving retrieval ac-curacy without assuming the most top-ranked documents are relevant. It also provides a sound probabilistic interpre-tation and a convergence guarantee on the iterative result updating process. Although derived from variational tech-niques, this approach only involves an iterative process of simple operations on ranking features and thus can be com-puted efficiently in practice. Our multimedia retrieval exper-iments on TRECVID X 03- X 05 collections have demonstrated the advantage of the proposed PLF approaches which can achieve noticeable gains in terms of mean average precision over various baseline methods and PRF-augmented results. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Performance, Theory Multimedia Retrieval, Query Expansion, Probabilistic Local Feedback Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
Most users begin their retrieval process without knowing the detailed collection information and the retrieval envi-ronment. Therefore their query terms can be significantly different from the terms used to describe the same concepts in the documents [20]. As one of the most effective ways to mitigate this  X  X ord mismatch X  issue and improve the re-trieval accuracy, automatic query expansion helps users to formulate a better query by adding new terms/phrases based on initial retrieval results or entire collection information. Numerous query expansion techniques have been developed before and most of them fall into two categories, i.e., global analysis and local analysis. The basic idea of global analy-sis is to expand the query description using global collection statistics based on the concurrence analysis of the entire col-lection. For example, a representative global technique is to automatically create a domain-specific thesaurus based on global term-to-term similarities [13] and use it to expand ad-ditional query terms based on their similarities to the query keywords. Other well-known global techniques include La-tent Semantic Indexing [3], PhraseFinder [7] and so forth. However, global analysis has to obtain statistics for each pair of terms in the entire collection, which is a computationally demanding task especially for large text collections.
As an alternative, we can resort to a local analysis strategy that explores the information from initially retrieved docu-ments in real time in order to determine which terms to ex-pand. The essence of local analysis, also known as local feed-back or pseudo-relevance feedback(PRF), is to utilize the top retrieved documents as positive examples to select discrimi-native query terms to improve the retrieval performance. In these approaches, a small number of top-ranked documents are assumed to be relevant, from which the expansion terms are discovered and used to formulate a second-cycle query. In practice, the idea of local analysis has been implemented in various forms for different retrieval models. In the vector space model, new query vectors are constructed by mov-ing the initial query vectors towards the center of the feed-back documents [15]. The classical probabilistic model takes feedback documents as positive examples and estimates the model parameters using Bayesian rules [14]. Language mod-eling approaches explore the local analysis idea by estimat-ing additional query language models [9, 17] or relevance models [10] from a set of feedback documents. By combining global analysis and local analysis, Xu et al. [20] proposed an effective local analysis algorithm called local context anal-ysis(LCA). In this work, several noun groups are selected from the top ranked documents based on the passage co-occurrence of query terms and introduced into the original query. Recent TREC evaluation results have demonstrated that local analysis approaches are usually effective in im-proving retrieval accuracy and they outperform the global analysis approaches on average. However, these approaches suffer from a drawback that their performance is heavily de-pendent on the quality of the initial retrieval outputs. If the retrieval quality of the first set of results is poor, local analysis tends to fail with too many noisy terms introduced in the second round. Another concern is that although it is intuitive to execute multiple local analysis runs to obtain better results, these methods are not always guaranteed to converge to a fixed set of retrieval results.

From another perspective, while query expansion enjoyed great success over the last decade, it is usually discussed within the domain of ad-hoc text retrieval. But the idea of automatically enriching query semantics is largely appli-cable to many other retrieval scenarios. In this work, we investigate the query expansion methods in a broader con-text, which aims to augment retrieval results by expand-ing various types of ranking features beyond word features. For instance, queries in multimedia retrieval can be refor-mulated to incorporate additional  X  X isual term X  features from high-level semantic concepts, which are learned from the visual modality using manually annotated development data. However, broader applications can bring additional challenges to the local analysis approaches, such as more heterogenous document features and poorer initial perfor-mance. For example, the state-of-the-art automatic video retrieval systems can only achieve a mean average precision of 12% in the TRECVID X 05 evaluation [16], which is obvi-ously insufficient to support the PRF approaches.
In this paper, we develop a robust local analysis approach called probabilistic local feedback(PLF) based on a discrim-inative probabilistic retrieval framework. Because PLF only requires the top-ranked documents contain more relevant documents than the bottom-ranked documents, it can au-tomatically expand ranking features to improve the initial retrieval accuracy even if mo st top-ranked documents are irrelevant. Formally, this model can be described as an undirected graphical model that treats document relevance and weights of ranking features as a set of latent variables. Therefore, it provides a sound probabilistic interpretation and a convergence guarantee on the iterative result updat-ing process. Although derived from variational techniques, the final form of this approach only involves an iterative pro-cess of simple operations on ranking features, and thus it can be computed efficiently in practice 1 . To investigate whether the proposed approach can perform robustly on poor initial retrieval results, we evaluate it on several multimedia re-trieval collections, i.e., the TREC video track(TRECVID)  X 03- X 05 collections. Our retrieval experiments have demon-strated the effectiveness of the proposed approaches, which can achieve noticeable performance gains over various base-line methods and their PRF-augmented results.
In the following discussions, we present a discriminative retrieval framework as a basic platform for the proposed Please find more discussions on the issue of efficiency in Section 3.2 approach. Let us begin by introducing the basic notations and terminologies used in this work. The term document is used to refer to the basic unit of retrieval throughout this paper. A search collection D contains a set of documents { d 1 , ..., d j , ..., d M D } .Givenaquery Q provided by users, let y  X  X  1 ,  X  1 } indicate if the document D is relevant or irrele-vant to Q .Forthequery Q and each document D ,wecan generate a bag of ranking features from N text keywords or information sources, denoted as f i ( D ). For instance, in ad-hoc text retrieval, f i ( D ) can be defined as the tf.idf weight of the i th term. In multimedia retrieval, f i ( D )canbethe outputs of uni-modal retrieval experts on text/visual modal-ities or the detection results of predefined semantic concepts. Generally speaking, the goal of information retrieval is to combine these ranking features f i ( D ) and generate a ranked list of documents that satisfies users X  information need.
Conventional relevance-based probabilistic models [5] rank documents by sorting the conditional probability that each document would be judged relevant to the given query, i.e., P ( y =1 | D, Q ). Most well-known text retrieval models, such as the BIR model [14], proceed by inverting the position of y and D based on the Bayes rule and estimating the gener-ative probabilities of document D in the relevant and irrel-evant documents. However, the underlying model assump-tions of these approaches such as term independency, could be invalid in practice. In contrast, discriminative models can directly model the classification boundary and typically make fewer model assumptions. They have been applied in many domains of text processing such as text classifica-tion and information extraction. Moreover, it is possible for retrieval systems to provide different types of ranking features from completely irrelevant sources including both query-dependent features and query-independent features. Generative models might have difficulties manually postu-lating different model distributions for these outputs. Nal-lapati [11] has shown that with presence of heterogenous features, discriminative models are superior to generative models in a home-page finding task.

Taking these factors into account, we decided to adopt discriminative models as the bas ic retrieval framework. For-mally, we model the posterior probability of the relevance as a logistic function on a linear combination of ranking fea-tures, i.e., where Z j is the normalization factor to construct a correct probability distribution and  X  i is the combination param-eter for the output of i th ranking features f i ( D j ). This logistic regression model presented in Eqn(1), a.k.a. the maximum entropy model, summarizes our basic retrieval framework. It naturally provides a probabilistic interpre-tation for the retrieval outputs. The weights  X  i can either be learned from existing training data [14, 11] or assigned by predefined rules (e.g., if they are set to the tf.idf weights of query terms, this probabilistic model will reduce to a vec-tor space model [2]). Once the parameters are estimated, documents can be presented to users in a descending or-der of P ( y j =1 |  X , D j ), or equivalently the weighted sum of retrieval outputs N i =0  X  i f i ( D j ). Note that, this lin-ear combination model can be recovered to a large num-ber of standard retrieval models with appropriate choices Figure 1: (a) The graphical model representation for the discriminative probabilistic retrieval model, where the document relevance is only determined by the initial weights  X  and the ranking features, (b) The graphical model representation for PLF, which assumes the weights  X  of unweighted ranking fea-tures to be latent variables. The nodes with known values are shaded, while other nodes are unshaded. of query/document features, such as the tf.idf-based vector space model and the Okapi-family retrieval models.
To concisely represent the retrieval model, we can summa-rize document relevance variables into a vector and remove the normalization factor by using a proportional sign, where all the bold letters represent a vector of the corre-sponding scalar variables, e.g., y means { y 1 , ..., y M the following discussions, we assume  X  and f i ( D j )areal-ready known, and only the distribution of variable y needs to be estimated. The graphical model representation of the discriminative retrieval model is shown in Figure 1(a), where the document relevance is determined by the initial combi-nation weights  X  together with the corresponding ranking features f ( D j ), and thus the document relevance variables Y j are conditionally independent to each other. Therefore, Eqn(1) is essentially equivalent to Eqn(2) with a normaliza-tion factor that is only relevant to {  X , D } .
In this section, we propose a new retrieval model called probabilistic local feedback(PLF), followed by describing its inference approach as well as its connections to other meth-ods. Finally we conclude with an illustrative example on a two-dimensional synthetic dataset.
Although the basic retrieval model defined in Eqn(2) pro-vides a principled probabilistic framework for retrieval, it may suffer from the fact that only a very small proportion of the ranking features are utilized (i.e., associated with a non-zero weight) and the other features remain unweighted in the retrieval process. Simply stated, the reason is twofold: 1) general users tend to provide short queries [20], and 2) external training data are usually sparse or they do not ex-hibit common patterns on most of the features. However, it does not imply these unweighted ranking features are useless for every query. Instead, since general users only have lim-ited knowledge on underlying data collections, they might not be able to discover the most relevant features to express their own information need. Such a  X  X ismatch X  issue can greatly impede the accuracy of retrieval results.
In order to address this problem, we aim to automatically expand additional ranking features that are closely related to the original query description and can better capture the content of underlying data collections. This expansion is achieved by treating the weights of unweighted ranking fea-tures as latent variables rather than simply setting them to be 0. In more detail, let us assume that the initial retrieval results are generated from some ranking features f i ( D j a set of initial weights  X  i . Based on this setting, we further introduce a new latent weight  X  for each unweighted rank-ing feature, i.e., the feature of which the associated  X  is 0, where  X  is a random variable ranged from [  X  X  X  ,  X  ]. For example, if we give a query  X  X inding a building X  to a mul-timedia retrieval system with 10 different semantic ranking features available (such as  X  X utdoors X  and  X  X uilding X  which are learned from pre-collected visual examples), and initially the retrieval system decides only the weight  X  for  X  X uilding X  is 1 based on word spotting, then we can introduce the la-tent weights  X  for all the other 9 semantic features instead of simply setting them to be zero.

To enable b oth  X  and  X  to exert a joint effect on the document relevance variables Y j , we follow the definition of conditional random fields [8] to derive the conditional prob-ability of relevance y , latent weights  X  given initial weights  X  and documents D as, where W = { i :  X  i =0 } contains the indices of initially weighted ranking features, U = { i :  X  i =0 } contains the in-dices of unweighted ranking features,  X  l is a latent combina-tion weight for the l th unweighted concepts. 2 For the sake of efficiency, we select only the top M T documents in the initial ranking to update in Eqn(3), where M T is a smaller number than the number of documents in the entire collection M D The settings and experiments w.r.t. M T are described in the experiment section.

A comparison between the models described in Eqn(2) and Eqn(3) shows that both retrieval models share the same components exp( y j  X  i f i ( D j )) to capture the effect of the ini-tial retrieval weights. But the proposed model further in-troduces an additional set of components exp( y j  X  l f l as to model the connections between unweighted concepts and document relevance. The prior distribution p 0 (  X  l fers another level of modeling flexibility, which represents how likely an unweighted ranking feature is relevant to in-formation needs based on human prior knowledge or external knowledge sources. For example, we can model the prior as a normal distribution N (  X  0 l , X  2 )where  X  0 l reflects our prior belief on the weight for l th ranking feature and  X  2 is a pre-defined constant variance. In case when no prior knowledge is available, we can set all the  X  0 l to be 0. But if we have
We use the proportional sign to indicate the intractability to compute the normalization factor on the right hand side. a semantic lexicon at our disposal (e.g., WordNet), we can define  X  0 l &gt; 0ifthe l th term is closely connected to a query term in the semantic lexicon, otherwise  X  0 l =0.
In the following discussions, we refer the model presented in Eqn(4) to as the probabilistic local feedback model. Its graphical model representation is shown in Figure 1(b). In fact, the construction under undirected graphical model se-mantics is of crucial importance for the correct functionality of PLF. The conditional dependence between  X  and  X  allows the posterior probability of p (  X  |  X , D ) to be updated accord-ing to the initial weights and the ranking features in the search collection. Therefore, the proposed PLF model is able to estimate the effectiveness for each unweighted fea-ture and discover useful features from the query context. In contrast, if we switch the model representation to be a di-rected graph, the latent combination weight  X  will be then independent to the initial ranking results  X  given y j is un-known. Since no additional information from  X  can flow to the nodes of  X  , a directed graphical model will trivially produce the same retrieval outputs as the original ones.
Note that because both  X  i and f i ( D j ) are already known, we can pre-compute the initial retrieval results f  X  = i  X  and simplify the Eqn(3) to be, This simplification gives us an option to handle initial out-puts provided by arbitrary retrieval systems, even if their outputs are not in form of i  X  i f i . However, in order to mitigate the effect of the heterogeneity of retrieval output distributions and deal with the case when the retrieval scores are not available in practice, it could be beneficial to re-normalize f  X  based on document ranks before running the following inference step. Given that only M T documents are selected, we can linearly normalize the conditional probabil-ity P ( y j |  X , D j )ofthe j th ranked document to be M T or equivalently, f  X  ( D j )= 1 2 log( M T +1  X  j j ). Similarly, we also shift the mean of each ranking feature f l to be 0. These nor-malization schemes are applied in the rest of this paper. We will develop and evaluate the other possible normalization schemes in the future.
According to the probabilistic ranking principle, docu-ments need to be ranked in a descending order of the condi-tional probability of relevance, i.e., p ( y |  X , D ). Bymarginal-izing out the latent variables  X  , we can compute the condi-tional probability of document relevance y as follows, p ( y |  X , D )  X  However, because of the presence of the normalization con-stant on the right hand side, it is usually intractable to com-pute the posterior probability in Eqn(5) with an exact infer-ence approach. Therefore, we resort to variational methods to provide an approximate inference for the intractable pos-terior distributions.

Specifically, we adopt the mean field approximation [12] in our derivation, which takes a factorized form of all singleton marginals over the variables. The first step of the mean field approximation is to construct the following family of variational distributions, as a surrogate to approximate the posterior distribution p ( y , X  |  X , D ), where q (  X  l |  X  l ) is a Gaussian distribution with mean  X  l and the same variance  X  as the prior p 0 (  X  ), q ( y is a Bernoulli distribution where y i = 1 with a sample prob-ability of  X  j and otherwise y i =  X  1. After some mathemat-ical manipulations (see details in Appendix), we can find that the variational distribution closest to p ( y , X  |  X , D )must satisfy the following fix point equations,  X  j = 1+exp 2 f  X  j +2 These equations are invoked iteratively until the change of KL-divergence is small enough. To understand how this ap-proach is expected to work, it helps to run the updates for only one step. In our implementation, we start with the first update rule and initialize  X  l to 0. The first equation attempts to adjust the relevance score for each of the M T document being selected, where top-ranked documents have positive scores and bottom-ranked documents have negative scores. Given the previous judgments, the second equation aims to estimate the weights of each ranking feature. The weight is larger if the feature can better distinguish the pos-itive and negative documents. To explain why PLF can identify the correct features to expand, let us suppose the l feature is a discriminative feature that should be expanded, such as f jl &gt;f kl when d j is a relevant document and d an irrelevant document. Since most of the documents are irrelevant and the mean of f l is set to 0, the expected sum j (2  X  j  X  1) f jl for irrelevant documents is close to 0. There-fore, as long as top-ranked documents contain more relevant documents than bottom-ranked documents, PLF can auto-matically expand f l with a positive weight.

A nice property of these two update rules is that their con-vergence is almost always guaranteed. Upon convergence, we use the final q ( y j |  X  j ) as a surrogate to approximate the posterior probability p ( y j |  X , D ) without explicitly comput-ing the integral. Since q ( y j |  X  j ) is a Bernoulli distribution, we can simply rank the documents in a descending order of the parameter  X  j as the retrieval outputs.

Note that, this iterative update process only involves sim-ple mathematical operations on the ranking features, such as sum, product and exponential function. Moreover, it typ-ically converges in a small number of iterations. Thus the proposed PLF approach can be implemented efficiently in a real retrieval system. For example, with a single 2.66GHz Intel CPU, it only takes less than 1 second to generate the final retrieval outputs for the default settings described in our experimental section.

Remark: The update process of PLF shares some char-acteristics similar to the traditional pseudo-relevance feed-back(PRF) techniques in the sense that both of them aim to refine the retrieval outputs based on initial rankings. How-ever unlike PRF, PLF does not require the assumption that most of top-ranked documents have to be relevant. Instead, The relevant documents are ranked at the top after expansion. it can work reasonably well as long as the top-ranked doc-uments contain more relevant documents than the bottom-ranked documents. PLF also provides a sound probabilistic interpretation and a convergence guarantee on the iterative parameter updating process, which is usually missing in the PRF approaches.

PLF is also related to previous work that applied statis-tical learning algorithms to automatically improve existing ranking functions or retrieval models. For example, Collins et al. [1] considered a discriminative reranking approach us-ing additional features of the trees to improve upon the ini-tial ranking for natural language parsing. Tieu et al. [19] used boosting algorithms to choose a small number of fea-tures from millions of highly selective features for image re-trieval. In the task of collaborative filtering, Freund et al. [4] proposed the RankBoost algorithm which learns to rank a set of objects by combining multiple  X  X eak X  classifiers to build up a more accurate composite classifiers. Taskar et al. [18] developed a learning method based on undirected probabilistic model s to induce  X  X nseen X  features in the test-ing set. Their approach introduced a continuous hidden vari-able for each unseen feature to describe its influence on the class. The probabilistic inference over test data can estimate the distribution of hidden variables and thus outperform the learning method using training data alone.
To show the ability of PLF to automatically expand use-ful ranking features and improve initial search results, we prepared a synthetic dataset shown in Figure 2(a) where X-axis shows the initial retrieval outputs f  X  ( D j )andY-axis shows the value of an unweighted ranking feature f ( D j ). In this figure,  X   X   X  X nd X   X   X  represent the relevant and irrele-vant documents. Blue and red colors represent the positive and negative predictions from PLF. There are a total of 20 relevant documents and 380 irrelevant documents. The doc-uments are ranked in a descending order of their correspond-ing variational parameters  X  j ,orequivalently, f  X  j + l Figure 2(a) also uses a solid line to indicate the initial de-cision boundary, which shows the documents ranked at the medium positions. It is purely determined by f  X  ( D j )at Table 1: Labels of video collections and their statis-tics. t  X  X  X  indicate the search collection labels. the starting point. From this graph, we can observe that neither the initial prediction f  X  ( D j ) or the ranking feature f ( D j ) is perfect in predicting relevant documents, but both of them provide informative evidence to do so. More im-portantly, since the initial ret rieval results provide a better-than-random performance, it can serve as a good initial set-ting to decide whether the unweighted feature should be expanded.

Figure 2(b) plots the decision boundary after running one step of the fix point equations in Eqn(8). It can be found that the variational parameter  X  becomes a positive num-ber and the decision boundary is shifted to a more accurate position. Figure 2(c) shows the final decision boundary af-ter the fixed point equations converge. It produces a much better retrieval results than the initial setting. This demon-strate the effectiveness of PLF even when the initial retrieval results are not so accurate.
We evaluate the proposed approach following the guide-lines of the manual retrieval task in the TREC video re-trieval evaluation(TRECVID) [16], which requires an auto-matic video retrieval system to search relevant documents without any human feedback. The main motivation for us to choose the multimedia collections as the testbed is to in-vestigate whether PLF can perform robustly for the noisy initial retrieval outputs. Although the proposed approach can also be applied to the traditional text collections with-out any technical difficulties, we will leave it to the future work due to the time limits on the data preparation process. Hu Jintao Leader:1.0, Crowd:0.39, Airplane:0.07 Tony Blair Leader:1.0, Commercial:-0.37, Crowd:0.93 Helicopter Airplane:0.23, Sky:1.0 Fire/flame Car:0.51, Building:0.64, Urban:0.93 Basketball Crowd:0.53, Commercial:-0.35 Map of Iraq Maps:1.00, Computer Screen:0.13 Table 2: Examples of useful ranking features (out of 75 visual concepts) and associated combination weights  X  found by PLF for six TRECVID X 05 query topic. These features are provided by their corre-sponding semantic concept detectors built on devel-opment data.
In the following experiments, the retrieval units were video shots defined by a common shot boundary reference. The query topics contain multimodal information including text descriptions, image examples and video examples. We used the query topics and video collections from TREC X 03- X 05 to evaluate the proposed learning algorithms. Each of these video collections is split into a development set and a search set chronologically by source. The development sets are used as the training pool to develop automatic multime-dia retrieval algorithms and the search sets mainly serve as the testbeds for evaluating the performances of retrieval sys-tems. All of our experiments are evaluated on the search sets where for each query topic, the relevance judgment on search sets was provided officially by NIST. The development sets are only used to build the models for semantic concepts and learn the combination function in baseline methods. The computation of PLF has no relations to the development sets. Table 1 lists the labels of each search collection and their statistics of query/document numbers.

As the building blocks of multimedia retrieval, we gener-ated a number of ranking features on each video document including 75 high-level semantic concepts learned from de-velopment data (including face, anchor, commercial, studio, graphics, weather, sports, outdoor, person, crowd, road, car, building, motion and so forth), and 5 uni-modal retrieval ex-perts (text retrieval, face recognition, image-based retrieval based on color, texture and edge histograms). For each re-trieval expert, we transformed their raw scores into ranks and normalized the mean value to 0 in order to avoid the problems brought by inconsistent scales of various retrieval outputs,. Each semantic concept is associated with a short text description, such as  X  X ar: segment contains video of an automobile X  and ground truth annotation on the devel-opment data. The detailed descriptions on the feature gen-eration can be found in [6].

In order to improve the robustness of the PLF model, we apply a  X  2 test [23] to filter out some irrelevant ranking features before the inference process. The  X  2 statistics are generally computed to measure the dependence between two random variables. In our work, we use it to measure the in-dependence between each feature and document relevance. If a feature tends to be independent of the relevance labels, this feature will be eliminated from the inference process. Only those features with a strong indication of their depen-dence are kept in the model. Under the assumption that irrelevant features are less likely to be strongly correlated with the relevance labels, the  X  2 test is able to eliminate most of the irrelevant features and improve the learning ro-bustness, although a small proportion of relevant features might also be mistakenly discarded. In our experiments, 2 in the prior distribution. we set the cutoff threshold to be 5.02, which corresponds to a confidence interval of 2.5% in the  X  2 distribution. Be-cause the choice of threshold is relatively insensitive to the retrieval results, we do not show any experiments in this paper varying the  X  2 threshold.
To illustrate the ability of PLF to automatically expand useful ranking features, Table 2 lists six TRECVID X 05 query topics (in the first column) together with their expanded ranking features and combination weights  X  found by PLF (in the second column). The query-class based combina-tion method [22] is used to provide the initial search results for PLF. For each query, we normalized the highest com-bination weight to be 1 and discarded the ranking features when the absolute values of their combination weights are less than 0.05. It can be observed that most of the rank-ing features suggested by PLF are reasonable and closely related to the query topics. For example, for the query of  X  X u Jintao X , it is ea sy to understand that the results can be augmented by using the visual concepts of  X  X overnment Leader X  and  X  X rowd X . The appearance of  X  X irplane X  can be explained by the fact that the truth video clips often contain arrival/departure scenes of Hu Jintao in the airport. These kinds of concepts are very difficult to find by merely analyz-ing the query description. Note that, the learned combina-tion weights can be either positive or negative. For instance, the concept of  X  X ommercial X  is assigned a negative weight for the query of  X  X asketball X , which means that the basket-ball scenes usually do not contain any commercials.
Next, we present the retrieval performance of PLF as well as three baseline approaches, i.e., text retrieval (where only one ranking feature from text retrieval expert is used), query-class based combination [22] and adaptive probabilis-tic latent query analysis (ApLQA) [21]. The initial retrieval outputs of PLF is provided by the baseline method that we are comparing with. We also compare PLF with a pseudo-relevance feedback algorithm, which assumes a subset of the top-rank examples to be positive and updates the combi-nation parameters via the second equation in Eqn(3). For PRF, the number of feedback documents is chosen as the best configuration ranged from 50 to 500 at a step of 50 in the search collection (so PRF has an unfair advantage over PLF). To determine the parameter  X  0 l in Eqn(3), we directly match query terms with the text description of se-mantic concepts. If there is a match between them, we set the corresponding  X  0 l to be 1 in our experiments. This infor-mation is incorporated in both the baseline methods and the PRF method in order to provide a fair comparison. Among all 75 ranking features, we only consider expanding the un-weighted ranking features that do not appear in the baseline retrieval functions. Unless stated otherwise, the prior vari-ance  X  2 is set to 1 and M T is set to 300, which means only the top 300 documents from the baseline outputs are up-dated.

Table 3 provides a detailed comparison between three baseline approaches and their PLF-augmented outputs on TRECVID X 03- X 05. All the retrieval results are reported in terms of the mean average precision(MAP) up to 1000 doc-uments and precision at top 30, 100 documents. From this table, we can observe that the performance of PRF is incon-sistent across multiple collections. This is partially because the initial results of multimedia retrieval are not sufficiently accurate to meet the requirement of PRF in general. In contrast, PLF can produce robust improvement for all the collections, because its assumption that requires more rel-evant documents on top is much easier to satisfy in prac-tice. It is almost always superior to the baseline methods no matter which initial retrieval output and data collection are used. On average, it provides a roughly 1-2% absolute Figure 3: Learning curves vs. the number of up-dated documents M T . improvement in terms of mean average precision (or equiv-alently 10% relative improvement) over the baseline per-formance. The major performance growth factor for PLF can be traced to a higher precision/recall on the top-ranked documents. To analyze the results in more detail, we also grouped the queries in each collection and reported their MAP in five different categories, i.e., named person, special object, general object, sports and general queries. By com-paring the retrieval performance with respect to each query type, we find that PLF benefits most from the person-type and special-object-type queries, as well as the sport-type queries in t 05. This is because these query types can pro-vide initial retrieval outputs of better quality, and thus they are able to be improved by making better use of additional ranking features.

To evaluate the sensitivity of PLF with respect to its pa-rameters, we designed a series of experiments with text re-trieval as initial outputs. Table 4 compares PLF with the prior variance  X  2 varied from 0.1 to 10. As we can observe, the setting of  X  2 =0 . 1 is on par with the setting of  X  However, if we modify  X  2 to be a large value, e.g., 10 in our case, it might occasionally result in a large loss in terms of mean average precision. This suggests that PLF becomes more stable with a smaller variance in its prior potential, because large variances might dilute the useful information encoded in the prior distribution. Figure 3 depicts the learn-ing curve of PLF with the number of updated documents M
T grown from 0 to 500 at a step of 50. In these three collections, although the highest mean average precision are achieved at different parameter settings, the fluctuation of mean average precision is typically less than 1% especially when M T is larger than 200. It shows that PLF is not too sensitive to the variation on the number of updated docu-ments.
In this paper, we propose a robust local analysis approach called probabilistic local feedback(PLF) based on a discrim-inative probabilistic retrieval framework. The proposed ap-proach is effective to improve retrieval accuracy without as-suming most top-ranked documents are relevant. This ap-proach can be represented as an undirected graphical model by treating document relevance and weights of unweighted features as latent variables. Thus, it allows the information from initial weights and collection statistics to jointly influ-ence the expansion of ranking features. It also provides a sound probabilistic interpretation and a convergence guar-antee on the iterative result updating process. Although derived from variational techniques, this approach can be computed efficiently in practice. Our multimedia retrieval experiments on three TRECVID collections have demon-strated the advantage of the proposed PLF approach, which achieves noticeable gains in terms of mean average precision over various baseline methods and PRF-augmented results. We expect that introducing external semantic knowledge sources in PLF could result in a further improvement on the retrieval performance. We also plan to investigate the effectiveness of PLF on the ad-hoc text retrieval task. To approximate the distribution p ( y , X  |  X , D )usingmean field methods, the first step is to construct the following family of variational distributions, as a surrogate to approximate the posterior distribution p ( y , X  | a , D ), where q (  X  l |  X  l ) is a Gaussian distribution with mean  X  l and the same variance  X  as the prior potential p q ( y j |  X  j ) is a Bernoulli distribution where y i =1withasam-ple probability of  X  j and otherwise y i =  X  1. The inde-pendence between variables in the variational distributions results in the following efficient inference algorithm, which aims to optimize the KL divergence between q ( y , X  )and p ( y , X  |  X , D ). This optimization can alternatively be cast into the maximization of the following lower bound, 0  X  KL ( q ( y , X  ) || p ( y , X  |  X , D )) where f jl denotes f l ( D j ), f  X  j denotes f  X  ( D j ), E to the expectation of f ( x ) with respect to the distribution of q ( x ), H ( q ) refers to the entropy of the distribution q .It could be found that the gap between the inequality is exactly the K-L divergence between the variational posterior distri-bution q ( y , X  ) and true posterior distribution p ( y , X  Therefore, we can alternatively solve a simpler optimization problem, i.e., to maximize the variational lower bound in or-der to find the best variational distributions to approximate the true posterior distribution.

By taking the derivative of the variational low bound with respect to the variational parameters  X ,  X  to be zero, we can derive the following fixed point equations,  X  j = 1+exp 2 f  X  j +2 This completes the derivation of the fix point equations in Eqn(8). After running these update rules to convergence, we can use the resulted variational distributions to approximate the true distribution p ( y , X  |  X , D ). Therefore, the marginal probability p ( y j |  X , D ) can be approximated by, p ( y j |  X , D )  X  [1] M. Collins. Discriminative reranking for natural [2] W. B. Croft and D. J. Harper. Using probabilistic [3] S.Deerwester,S.T.Dumais,G.W.Furnas,T.K.
 [4] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. [5] N. Fuhr. Probabilistic models in information retrieval. [6] A. Hauptmann, M.-Y. Chen, M. Christel, C. Huang, [7] Y. Jing and W. B. Croft. An association thesaurus for [8] J. Lafferty, A. McCallum, and F. Pereira. Conditional [9] J. Lafferty and C. Zhai. Probabilistic relevance models [10] V. Lavrenko and W. B. Croft. Relevance based [11] R. Nallapati. Discriminative models for information [12] C. Peterson and J. Anderson. A mean field theory [13] Y. Qiu and H.-P. Frei. Concept based query [14] S. E. Robertson and K. S. Jones. Relevance weighting [15] J. J. Rocchio. Relevance feedback in information [16] A. Smeaton and P. Over. TRECVID: Benchmarking [17] T. Tao and C. Zhai. Regularized estimation of mixture [18] B. Taskar, M. F. Wong, and D. Koller. Learning on [19] K. Tieu and P. Viola. Boosting image retrieval. In [20] J. Xu and W. B. Croft. Improving the effectiveness of [21] R. Yan and A. G. Hauptmann. Probabilistic latent [22] R. Yan, J. Yang, and A. G. Hauptmann. Learning [23] Y. Yang and J. O. Pedersen. A comparative study on
