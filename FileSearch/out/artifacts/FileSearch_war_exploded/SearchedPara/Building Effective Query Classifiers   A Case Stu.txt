 Query-based triggers play a crucial role in modern search systems, e.g., in deciding when to display direct answers on result pages. We address a common scenario in designi ng such triggers for real-world settings where positives are rare and search providers possess only a small seed set of positive examples to learn query classification models. We choose the critical domain of self-harm intent detection to demonstrate how such small seed sets can be expanded to create meaningful training data with a sizable fraction of positive exam-ples. Our results show that with our method, substantially more pos-itive queries can be found compared to plain random sampling. Ad-ditionally, we explored the effectiveness of traditional active learn-ing approaches on classification performance and found that maxi-mum uncertainty performs the best among several other techniques that we considered. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  query formulation, search process .
 Query classification; Self-harm; Active learning. Query-based triggers play an important role in modern information retrieval (IR) systems. Such triggers can be used to decide when to display rich direct answers (weather or stocks) on search engine re-sult pages (SERPs), target display advertisements to particular que-ries or query classes, or issue other specific notifications to search-ers. While a high-accuracy query classifier can help such systems reach all interested searchers, misclassification has a cost of show-ing notifications or advertisements in inappropriate contexts which may annoy or frustrate searchers. In the construction of query clas-sifiers, search providers often only possess a small seed set of target queries, which can be insufficient for training. Positive queries (ex-amples of queries that should be targeted) are rare in many query classification tasks. A common need is to expand the seed set to identify more positives without adding many negatives. In this work, we choose one partic ularly important query classifica-tion problem, self-harm intent detect ion, as a case study to highlight the challenges and approaches to building an effective query classi-fier for a targeted domain. Currently, the major Web search engines respond to queries such as [how to kill yourself] with an answer-like treatment that provides a telephone number for the National Suicide Prevention Lifeline. The research challenge in this domain is to build classification models that can automatically and accurately de-tect when a query indicates an intent of the searcher for self-harm. To address that challenge, we make the simplifying assumption that the classifier solely relies on the text of query statements for training and prediction, and does not use additional information such as re-sult clicks, cursor movements, and part of speech tagging X  X ll of which have been used for query classification purposes [6][8][9]. This has the practical benefit that classifying solely based on query features reduces latency which is crucial for Web-scale use. We make the following contributi ons with our research. We exam-ine different notions of relatedn ess in query classification, and whether they provide different benefits when expanding a small set of seed queries to a much larger training and evaluation set. Addi-tionally, this is the first work in IR on self-harm, a critically im-portant topic. Finally, we believe that our work will stimulate dis-cussion in the research community on this important, albeit sensi-tive, issue with specific questions such as when any intervention should be triggered, how intervention can be best provided, how specific intervention assistance s hould be, whether query re-writing should occur on such queries and the general question of how search engines should operate in such sensitive areas. Query intent classification has been an active research area for many years. Several methods have been proposed to automatically iden-tify three broad classes of intent: navigational, informational and transactional [2][11]. Most of th ese methods rely on additional in-formation such as result clicks or an chor text [9], part of speech tag-ging [8] or mouse cursor moveme nts on SERPs [6]. For example, Lee et al. [10] used the observation that the click distribution of nav-igational queries are usually highly skewed toward a few domains to distinguish navigational and informational search queries. Jansen et al. [7] proposed a rule-based method that solely relied on queries. This supervised method is similar to ours in that it did not utilize additional information beyond queries. However, general rules such as the presence of terms like  X  X ownload X  indicating a transactional query are not applicab le in our context since many que-ries of opposite labels share terms. Broder et al. [4] proposed meth-ods to find relevant advertisements for tail queries. Our two studies were motivated by a similar goal of building effective classifiers for domains where positives are rare. However, our techniques are dif-ferent. We propose methods to find positives to build more balanced training sets for classification. In contrast, Broder et al. target offline computation on head and torso queries. While our focus is not on automatic query expansion (AQE), work in that area is still relevant to the research described herein. A survey of the significant body of literature on AQE can be found in [5]. Broder et al. [3] also targeted ra re query classification, proposing a pseudo relevance feedback mechanism for classifying a query by first classifying its search results and then using a voting mechanism to determine the label of the query. Their method requires down-loading and processing the content of top-ranked results (and of course assumes that these top-ra nked results are relevant to the query). Such a method is better suited to query topic classification or cases where a document likely satis fies a singular query intent. Topic may help inform query intent classification (our task). How-ever, a document (e.g., a page on safe dosing limits for a medicine) can satisfy multiple intents (e.g., a query clearly indicating a user intends to overdose versus checking on how to safely take the med-icine), and the query text plays a pivotal role in indicating searcher intentions that is not fulfilled by considering document topic. In con-trast to Broder et al. [3], we rely solely on the query text. In these respects, the two methods are quite complementary. We now describe how we collecte d and manually labeled our query classification data. In particular given a small seed set of positive queries, we broaden this set in a tw o-step process. First, we auto-matically expand the query set (denoted as ThreeHop ) using a three-step graph walk on the set of query suggestions obtained from Bing. In the second step, we further expand this to a set of related queries (denoted as Related ) that are similar to the queries in ThreeHop in some way (specifically in appl ying bigram matching, trigram matching, or session matching). To follow a common paradigm for c onstructing query classifiers, we assume that we have a small set of positive trigger queries and seek to broaden these to other relevant queries using commonly available resources. We started with a small seed set of queries that unambig-uously expressed self-harm intent and expanded it by including the top-ten related query suggestions re turned for those queries by Bing. Query suggestions are a convenient way of expanding an initial query set that is also reproducible publicly through search engine APIs. Focusing on query classificati on, since query suggestions are often largely influenced by session co-occurrence, this step can be viewed as sampling from the head of session co-occurrence. Our in-itial seed set was constructed manually and contained the following four queries: [how to commit suicide], [how to kill yourself], [I want to kill myself] , and [I want to commit suicide]. Our choice of the seed set queries was quite arbitr ary; since we lacked knowledge about the query-space for this domain , we selected four queries that we believed unambiguously express self-harm intent. This may also be quite typical: it is unlikely that search engine designers will have domain knowledge in each domain be ing targeted for specialized query support. With subsequent expansion, we found several posi-tives which were more specific than the four queries that we started with (e.g., [how to od on xanax], [what is the best combination of pills and carbon monoxide in suicide]). Note that mere synonym substitution can help us find queries such as [how to shoot yourself] from [how to kill yourself], but in our process we also find queries which are related in intent but lack common synonyms with other positives (e.g., [final exit], which is a book on assisted suicide). One iteration of expanding our set invol ves adding all distinct sugges-tions for each query. We used a standard publicly available API for such expansion with a user account that had no previous search his-tory (to avoid biasing the query suggestions with personalization signals). We repeat to expand our seed set to 662 queries which in-cludes all neighbors within three hops of the seed set in the query suggestion graph. We wanted our set to be diverse enough while keeping the number of queries unrelated to self-harm intent (e.g., queries related to celebrities who attempted self-harm) restricted. For this goal, we found three hops to be a reasona ble heuristic. Given the rarity of positive examples, one challenge in building query classifiers is that if we simply sampled randomly, we would likely find no positives. Conversely, building an evaluation set by only leveraging query suggestions woul d be overly sensitive to head (popular) queries and our initial seed set. To this end, we expanded the set of queries for labeling in a way that would likely either find positive queries or queries that we would likely erroneously trigger on (false positives) without overly -strong assumptions. We sampled queries in one of three ways. First, we sampled from all search ses-sions with a co-occurring query in ThreeHop (denoted Session ). This expands the set of queries likely covered by the query sugges-tions to include more tail sessions . We had access to search logs from Microsoft X  X  Bing search engine and employed the commonly-used practice of demarcating sess ion boundaries via 30 minutes of inactivity. All queries were sampled from the logs of the aforemen-tioned search engine from June 1  X  December 31, 2013 in the Eng-lish-speaking U.S. locale. We also sampled queries from the logs that had at least one bigram or trigram (denoted Bigram and Trigram respectively) overlapping with the queries in ThreeHop . We did this since queries with bigram/trigram overlap with this initial set would be more likely to either be predicted positive/negative. To further ensure diversity and balance across the set of queries we sought a mechanism to help ensure the related query types were dis-tributed across both query frequency and the likely ambiguity of the query. To achieve this, we sampled 600 queries of each related query type. Within each related query ty pe, we sampled evenly between head-queries (20 or more occurren ces in a six-month query log) and tail queries (less than 20 occurren ces in a six-month query log). Fi-nally, to help ensure that the set covered a likely range from unam-biguously negative to unambiguously positive, we trained an initial classifier (see Section 5) on the labeled ThreeHop set, and used that classifier X  X  predictions to further stratify the space. From each decile of the classifier X  X  predicted confidence, we sampled 30 head and 30 tail queries for use in our analysis. We also sampled 600 queries at ra ndom stratified similarly (60 from each decile). Sampling yielded 2,400 queries for the related set (in addition to the 662 queries in ThreeHop ) After manually labeling these 2400 queries, the Related query set consists of 169 positive queries (divided between relatedness categories as shown in Table 2), 2212 negative queries, and 19 que ries were labeled as indetermi-nate. Indeterminate queries were discarded from Related . Following the selection of subsets of queries as defined in the pre-vious section, the queries were manually labeled by three labelers. As with any query classification ta sk, deciding how to label a par-ticular query has to be balanced be tween the specific intent and the likely intent on average across all searchers issuing that query. For example, the query [suicide] is qu ite general and may express many research intents related to philosoph y or ethics courses, medical re-search, and similar topics. That is, it is not clear that the searcher has a likely intent for self-harm. This is in contrast to the much more specific intentions demonstrated in the seed queries, some of which include first-person language, e.g., [I want to kill myself]. To help provide consistency in judgments, labeling guide lines instructed la-belers to look for a likely clear in tent of self-harm which includes, among other things, focused questi ons on suicide methods and their effectiveness, but does not includ e queries on suicide demographics, celebrity suicide, and various death-related obsessions. Example queries representing each class were also provided to assist the la-belers. Labelers were allowed to inspect the search results for a query but instructed to assume that the searcher may not find any results relevant. Labelers reported that examining results was useful to identify special cases of queries such as lyrics or song titles that were not obvious from the query text alone. Three annotators labeled al l of the queries in the ThreeHop set. The Fleiss X  kappa measure of the inter-rater agreement between the three labelers was 0.73, indicates substantial agreement between the an-notators at this task. Disagreements were examined to help clarify the labeling guidelines with additional examples. After resolving disagreements, the ThreeHop set comprised 390 positives, 259 neg-atives and 13 queries labeled as indeterminate. The fact that some labels could not be resolved, ev en after discussion among the la-belers, reflects the difficulty of this task. These uncertain queries were discarded from the ThreeHop set during training. We now briefly consider the type s of classification model and rep-resentation that are necessary to accurately model both the self-harm domain and more generally the typi cal interactions seen in query classification problems. Given our focus on classification using only query text, the primary question that we ask is whether a linear clas-sifier employing a bag-of-words representation or even a bag of n-grams is sufficient? Inspection of the labeled data quickly reveals that there are many interactions which are not likely to be additive. For example, there are subtleties of word ordering. The query [pain-less suicide] is a likely self-harm query but [suicide is painless] is a song for a very popular television seri es. This latter query is an ex-ample of the types of  X  X xceptions X  that often occur where a head query has a very specific intent not obvious from the query text alone. A similar example is [kill yourself] which is also a likely self-harm query versus [kill yourself in 5 minutes] which is a popular online game. Finally, while often a single word or phrase can be pivotal as in [painless easy ways to kill yourself] vs. [painless easy ways to kill mold]. The phrase  X  X il l yourself X  occurs in both positive and negative examples as seen in the previous example. Alterna-tively,  X  X ill mold X  covers this negative but misses the more general pattern as this same phrase is seen for many related items, e.g.,  X ....kill bugs X ,  X ...kill roaches X , et c. Thus, more general pattern matching may help with generali zation, but a bag-of-words ap-proach is clearly not sufficient given the complexity of the space. As the representation is extended to increasingly longer n-grams, a classifier can learn the general structure while memorizing excep-tions. As a compromise between these extremes we target a repre-sentation that employs features of unigrams, bigrams, and trigrams. To predict self-harm intent, we first trained an off-the-shelf linear support vector machine (SVM) on the labeled ThreeHop set using unigram, bigram, and trigram features. This particular SVM was specifically designed to handle large-scale query classification. This classifier was used to provide the stratified sampling over query class probability for each of the related query methods in Table 1. Since the samples are stratified by posterior, one would expect the number of positives to be similar w ithin each bin across the different methods and the total number of positives to also be the same across the different methods. If the classifier were well-calibrated we would expect the number of positives to match the expected poste-rior, but even if it did not, a co nsistent miscalibration would skew each related query method in the same way. If they skew differently, it suggests that building a query cl assifier by simply sampling sug-gested queries is subject to bias that can be partially overcome by broadening sampling as described herein. Table 1 (on the previous page) shows that we obtained far more pos-itives using Bigram , Trigram and Session approaches than via the plain random sampling approach. By using the frequency of bins for stratification, we can estimate the overall probability of positive ac-cording to each method. We use the random positive ra te as the ex-pected rate of positives for the proportion not matching a related fil-ter. Interestingly, the variance across these estimates demonstrates the difficulty in sampling for rare items. The final column expresses these as a ratio to random to emphasize the magnitude of variation and the greater rate of positive discovery. This result underscores that by plain random sampling we would get very few positives, meaning that the classifier would have limited data for further im-provement. Among the three types of relatedness we considered, Session obtained the maximum number of positives which follows the intuition that a person with a specific intent will make similar searches within a given session. We split Related for train and test (90:10, Related train and trained the SVM on Related train + ThreeHop ). As a baseline, we compared the performance with a classifier only trained on Three-Hop. Table 2 shows that with add itional training data, we obtained improvement over all measures of classification performance. Since we have widened the net of different types of se lf-harm queries through Related , our recall performance s ubstantially improves. A classifier that always predicts negative (the marginal) only achieved an accuracy of 89.38%. While our queries are sampled in a manner to help discover positives and likely errors, we should also consider operative performance. Such perfor mance is weighted by query fre-quency and the most frequent queries are often easily classified. Even in our challenging set, we see better performance when re-weighting by query frequency with precision, recall, and F1 of 92.64%, 83.31%, and 91.83% respectively. On analyzing the misclassified queries, we found that some of them happened as a result of possible typos (e.g., [hbest ways to commit suicide], [how to commit suciside]) and can be accurately classified if the typo is corrected (e.g., by applying automated spelling correc-tion). Some misclassificat ions can be handled with appropriate do-main knowledge. For example, a ty pical pattern for self-harm que-ries was found to cont ain medicines that ar e used for overdosing (e.g., [lorazepam suicide]). Adding such lists of medicines in the do-main knowledge of the classifier and treating them as a wild-card can reduce the number of misclassifications. Some of the misclas-sified queries were long queries (e .g., [what is the best combination of pills and carbon monoxide in suicide]). Adding higher order n-grams may address these type of queries. In various learning scenarios, active learning is found to be useful in reducing the number of training examples required to learn a con-cept and thus being particularly useful when labeling resources are scarce. Also, in a practical setting, requesting labels in batches is more cost-effective than requesting one at a time. For these reasons, next we analyzed the performance of different sampling strategies for active learning in a batch setting. In doing so, we were interested in whether traditional active learning approaches can succeed in this setting or whether it is primarily the discovery of extreme posi-tive/negative examples that drives classifier improvements. We con-sidered strategies listed in Table 3. We chose random sampling as our baseline and maximum uncertain ty since it is a known sampling technique that is useful across many different active learning tasks. We observed that several queries classified as positives with high confidence were actually a negative query with a high degree of n-gram matching with self-harm queries (e.g., [painless easy ways to kill mold]). In order to examine if labeling queries predicted with very high or low confidence can improve performance, we explored two more strategies: one-sided extreme and two-sided extreme. We choose one-sided extreme since it was found to outperform maxi-mum uncertainty sampling in the context of short document classi-fication [1][12]. Figure 1 plots the area under the receiver operating characteristic (AUROC) curve comparison of different sampling strategies. We initially train our SVM on ThreeHop and at each step, we add 10 samples from Related train . We restrict ourselves to 350 additional samples since the earlier part of the curve is mainly inter-esting as the performance of each strategy will eventually converge. Since ties are broken randomly, which can create some amount of variability, we repeated the experiment five times for each sampling strategy and report the average performance. Figure 1 shows that maximum uncertainty outperforms other the techniques (in many cases with statistical significance at p &lt; 0.05 using t -tests), and there is no clear winner among batch tw o-sided extreme and random sam-pling. So we conclude that active learning does help in improving performance, but only identifying one -sided or two-sided extremes do not outperform maximum uncertainty sampling. We proposed a structured approach to building a classifier when positives are rare and we have a small initial set of positive exam-ples. We showed that our approach finds many more positives than random sampling. We focused on se lf-harm, an important domain underexplored in IR despite the use of search systems by people in-teresting in learning about self-harm, and we share valuable insights about our key challenges. We compar ed the performance in a batch active learning setting and found that maximum uncertainty is the best sampling strategy give n limited labeling resources. Although we focused on self-harm, most of the challenges faced in our work apply to any query classification task with rare positives. We do not address the broader que stion of when given self-harm queries or the high probability of a self-harm intent, search engines should show an answer or offer ot her interventions. We hope this research prompts a broader discussi on of self-harm intentions within relevant communities, including IR, mental health, and ethics, about if/when/how such interventions should be triggered. [1] Attenberg, J., Melvil le, P., and Provost, F. (2010). A unified [2] Broder, A. (2002). A taxonomy of web search. SIGIR Forum , [3] Broder, A., Fontoura, M., Gabrilovich, E., Joshi, A., Josi-[4] Broder, A., Ciccolo, P., Gabrilovi ch, E., Josifovski, V., Metz-[5] Carpineto. C. and Romano, G. (2012). A survey of automatic [6] Guo, Q. and Agichtein, E. (2008). Exploring mouse move-[7] Jansen, B.J., Booth, D.L., and Spink, A. (2008). Determining [8] Kang, I.H. and Kim, G.C. (2003). Query type classification [9] Li, Xiao, Wang, Y.-Y., and Acero, A. (2008). Learning query [10] Lee, U., Liu, Z., and Cho, J. (2005). Automatic identification [11] Rose, D.E. and Levinson, D. (2004). Understanding user [12] Sindhwani, V., Melville, P., and Lawrence, R.D. (2009). Un-Figure 1: Performance of different query sampling strategies. Lines denote five-point moving average over five runs ( X SEM).
