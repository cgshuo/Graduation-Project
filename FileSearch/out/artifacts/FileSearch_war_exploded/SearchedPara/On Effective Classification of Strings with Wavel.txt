
In recent years, the technological advances in mapping genes have made it increasingly easy to store and use a wide va-riety of biological data. Such data are usually in the form of very long strings for which it is difficult to determine the most relevant features for a classification task. For exam-ple, a typical DNA string may be millions of characters long, and there may be thousands of such strings in a database. 
In many cases, the classification behavior of the data may be hidden in the compositional behavior of certain segments of the string which cannot be easily determined apriori. An-other problem which complicates the classification task is that in some cases the classification behavior is reflected in global behavior of the string, whereas in others it is reflected in local patterns. Given the enormous variation in the be-havior of the strings over different data sets, it is useful to develop an approach which is sensitive to both the global and local behavior of the strings for the purpose of classification. 
For this purpose, we will exploit the multi-resolution prop-erty of wavelet decomposition in order to create a scheme which can mine classification characteristics at different lev-els of granularity. The resulting scheme turns out to be very effective in practice on a wide range of problems. and record a wide variety of string data for a number of applications. Examples of such data include proteins which often contain long sequences of amino acids. Another class of data which are closely related to strings axe time series or sequential data in which sequences of events axe stored in strings [11]. A number of approaches for traditional prob-lems such as clustering, indexing and subpattern identifi-cation have also been developed for this domain [3, 9, 10, 13]. tion. The classification problem has been widely studied in the data mining, artificial intelligence and machine learn-permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ... $5.00. ing communities and is defined as follows: we have a set of records called the training data, in which each record is labeled with a class. This training data is used to construct a model which relates the features in the data records to the class label. For a given record for which the class label is unknown, this model may be used to predict its class la-bel. This problem often arises in the context of customer profiling, target marketing, medical diagnosis, and speech recognition. Examples of techniques which are often used for classification in the data mining domain include decision trees, rule based classifiers, nearest neighbor techniques and neural networks [5, 6, 7, 8]. A detailed survey of classifica-tion methods may be found in [8]. of the classification problem. An important example is the biological domain in which large amounts of data have be-come available in the last few years. Applications of DNA matching and identification include the fields of archeology, forensics and medicine [5]. Other examples include sequen-tial data for event classification, and classification of cus-tomer data for user profiling. In many of these cases, the resulting strings are quite long and vary from a few hundred symbols to the thousands. For applications in which the strings are very long, the classification problem turns out to be very perplexing in pratice. In most cases, both the global and local composition of the proteins may influence its clas-sification behavior. For example, a typical protein sequence may contain thousands of amino acids, which are drawn from the fixed alphabet E = {al ...at} = {A,C,T,G}. 
In most cases, the compositional behavior of the sequence in certain subportions may significantly affect the physical characteristics of the corresponding protein. In other cases, certain kinds of proteins may show local or periodic presence of different kinds of amino acids. These characteristics may be hard to distinguish at the global level and significantly complicate the classification process. Since each sequence may contain thousands of characters, it is also a difficult and complex problem of finding the most discriminatory compo-sitions at the correct level of detail and granularity. This is true for a large number of applications in which the classifi-cation behavior of the relevant strings can only be accurately determined by taking both the compositional and positional behavior of the constituents into account. nearest neighbor technique based on the edit distance [4]. In this method, the class label of the nearest neighbor to the test instance is reported as the relevant label. This technique has several drawbacks: (1) The edit distance turns out to be edit distance is not effectively indexable. This restricts the applicability of the method for very large scale applications. able length is rarely a good measurement of locality in class behavior. For example, two strings of very different lengths may have similar classification characteristics because of cer-tain important local combinations of symbols. However, the edit distance between such strings is likely to be at least as large as the the difference in their length. Therefore, it is useful to create a technique which can mine such local combinations of patterns effectively. Also, in many biologi-cal applications, only sample string fragments axe available rather than the entire strings. In such cases, global char-acteristics tend to be an especially poor indicator of the classification behavior. (3) The distance based classifier has an overwhelming dependence on the exact positioning of the alphabets in the strings. This dependence results in an in-ability to capture important compositional characteristics of the strings. For example, in a biological application, certain broad classes of proteins may have similar composition but may often vary considerably in the exact ordering of the characters depending upon the particular subclass that the protein may belong to. Such behavior cannot be captured well by distance based classifiers. recognized as a useful tool for a number of database ap-plicatious. A comprehensive overview of the wavelet tech-nique may be found in [17]. An important property of the wavelet technique is that it creates a hierarchical decompo-the creation of ;x system which provides multi-resolution in-sight into the data characteristics, we combine it with a rule based technique in order to make the classifier sensitive to particular local characteristics of the strings. this section, we will discuss the contributions of this pa-per, notations and background of the wavelet decomposition technique. In section 2, we will discuss how to use this tech-nique in order to build an effective rule based classifier for the data. In section 3, we will discuss the empirical results. 
Section 4 contains the conclusions and summary. for string classification. This wavelet based approach pro-vides the ability to isolate the most discriminatory composi-tions of the strings at varying levels of analysis. In addition, a rule based approach to the problem ensures that the local compositional characteristics of the strings are used during the classification process. The result is a system which is not only much more effective than the currently used near-est neighbor classifier, but is also significantly more efficient during the classification process. 164 this paper, we will introduce some notations and definitions. 
We assume that the training data set :D contains N strings, such that the length of the ith string is denoted by di. We assume that each of the N strings is drawn from the alpha-bet E = {al...al}. We also assume that associated with each record in the data set :D, we have one of a set of k class labels drawn from C1 ... Ck. The classification model is constructed using the records in the database :D along with their corresponding class labels. method which we refer to as the Haar Wavelet. We will also discuss a brief algorithmic overview of the method used in order to generate the Haar coefficients from a given record and the modifications necessaxy for their use in a string do-main containing sequences which are drawn from a fixed alphabet. composition of the data characteristics into a set of wavelet functions and basis functions. The property of the wavelet method is that the higher order coefficients of the decom-position illustrate the broad trends in the data, whereas the more localized trends are captured by the lower order coef-ficients. the series is a power of 2. The Haar Wavelet decomposi-coefficients corresponds to a contiguous portion of the time series of length q/2 ~-1. The ith of these 2 k-1 coefficients corresponds to the segment in the series starting from posi-this coefficient by  X ~ and the corresponding time series seg-ment by S~. At the same time, let us define the average value of the first half of the S~ by a~ and the second half by bL Then, the value of  X ~ is given by (a~ -b~)/2. More formally, if ~ denote the average value of the S~, then the value of  X ~ can be defined recursively as follows: 
The set of Haar coefficients is defined by the @~ coefficients of order 1 to log2(q). In addition, the global average ~ is required for the purpose of perfect reconstruction. We note that the coefficients of different order provide an understand-ing of the major trends in the data at a particular level of granularity. For example, the coefficient  X ~ is half the quan-tity by which the first half of the segment S~ is larger than the second half of the same segment. Since larger values of k correspond to geometrically reducing segment sizes, one can obtain an understanding of the basic trends at different levels of granularity. 
Algorithm WavRules( Training Database:9; 
Maximum Gap: maxgap); begin 
W' = Create WavRepresentation( D ) ; 7E = CreateRules(W', s, c, maxgap); (T~',defaultclass) = RearderRules(T~, I,V'); end Figure 2: The Training Phase of the Wavelet Clas-sifier 
Algorithm Create WavRepresentation(Database: 9); begin w' = {}; for each string in DF determine wavelet coefficients as described in subsection 1.3; 
Create a set of  X  ranges covering values from -0.5 to -4-0.5; { Thus, the ith range corresponds to values between -0.5 + (i -1)/ X  and -0.5 + i/ X  for i  X  {1,...  X } }; for each coefficient in each string in D, replace return'0N'); end Figure 3: Determining the Wavelet Representation of class behavior. In this section, we will discuss a classi-fier which builds on these representational advantages of the wavelet technique. 
The overall training phase is illustrated in Figure 2. We assume that the input to the algorithm is the set of N strings in the database D, a minimum support s, a minimum con-fidence c, and a parameter called maxgap which quantifies non-relevant lengths of strings in the classification process. We will discuss these parameters in more detail slightly later. 
The training process of the classifier works in three phases. (1) In the first phase, the wavelet representation of the data is constructed. This is discretized in order to create a binary representation which is necessary for an effective rule based classifier. This process is denoted by the subroutine Cre-ateWavRepresentation in Figure 2. (2) in the second phase we determine the set of ordered compositional rules which are most indicative of class behavior. The beauty of com-positional rules is that they leverage the flexibility of the wavelet decomposition process in order to identify combina-tions of both composition as well as the ordering behavior of the segments in order to model the class discrimination in the data. Such ordered compositional behavior identifies a local region of the string in which the corresponding trends occur. The length of this local region is heavily dependent on the detail level of the corresponding wavelet coefficients. This essentially defines the level of granularity of the cor-responding classification rules. This process is denoted by the procedure CreateRules in Figure 2. (3) Once these com-positional rules have been constructed, we prune them in order to reduce overfitting and improve the effectiveness of the classification model. This procedure is denoted by Re-orderRules in Figure 2. These rules axe then used in order to classify individual test instances. In the next subsections, Minimum Support: s Minimum Confidence: c, 
Maximum Gap: maxgap); { We denote this set by  X 2(gap)}; candidate in Ck+l; greater than s; minimum confidence c; precedence rule covered by it; Algorithm Classify(Testlnstanee: T, Rules: R', 
Default Class: defaultclass); begin Transform test instance T into wavelet representation Tw; 
Find highest precedence rule for which Tw is a subpattern of the antecedent of the rule R; if no such class exists return(defaultclass); end Figure 6: The classification procedure for the wavelet method we will describe the details of each of these phases. 
This procedure is denoted by Create WavRepresentation and is described in Figure 3. In the first step, we deter-mine the wavelet coefficients of the strings in D. We note that wavelet coefficients of different orders correspond to in-tervals which may possibly subsume one another. Each of these coefficients is then discretized into  X  intervals. Specif-ically, we'create a set of  X  ranges covering the values from -0.5 to +0.5. Thus, the ith range corresponds to values between -0.5 + (i -1)/ X  and -0.5 + i/ X . The reason for the use of this range is that each wavelet coefficient is half the difference of the fractional composition of the current interval with that of an adjacent interval. Thus, all coeffi-cients lie in the range (-0.5, 0.5). The only exception are the l wavelet coefficients corresponding to  X ~ (j) which are the global averages across the decomposition. These coeffi-cients are separately discretized into  X  intervals between 0 and 1. We shall refer to the resulting string as the diseretized wavelet decomposition. 
In this section, we will discuss the process of rule con-struction for the wavelet representation. While the wavelet representation provides an overview of the variations in com-positional characteristics at different levels of granularity, it is useful to isolate localized regions in the data where dis-criminatory variations in compositional behavior occur. For this purpose, we will define the concept of a compositional pattern: 
DEFINITION 2.1. A compositional pattern 0 &lt; vl,idl &gt; ing properties: (1) We assume that each wavelet coefficient in the string belongs to detail order O. Thus, this reflects the level of detail or granularity of each wavelet coefficient in the pattern. (~} The value vi is an interval number from 1 through  X . (3) The value idl is a number from 1 through l corresponding to the particular alphabet 0"~ from E which the wavelet coefficient belongs to. (3) The value 91 is the beginning of the interval for &lt; vi+l,idi+l &gt; in terms of the number of unit interval lengths of detail order O. We note that the above definition ensures that each element of the compositional pattern is derived from the same level 
The compositional patterns are generated using a two phase iterative process. In the first phase all the compo-sitional patterns of length two are generated. The remain-ing compositional patterns axe then generated iteratively in a level wise fashion. The overall process for generation of compositional patterns is illustrated in Figuxe 4. In order to generate patterns of length two, we use an iterative process in which we find patterns which have gaps staxting from 0 to maxgap. For each particular value of the gap, this pro-cess is similax to that of finding 2-itemsets in databases. In this case, we however ensure that both the elements of the pattern are wavelet coefficients of the same order. Once such patterns of length two have been found, we use them in order to generate k-patterns by using an iterative method-ology. In each iteration, we use joins in order to generate (k + 1)-candidates from pairs of k-patterns. Let us assume that the set of all patterns of length k which have support at least s axe denoted by  X k. In order to decide whether a join, they must satisfy the following properties: We shall denote this common segment in the two patterns as P = "P~ = P~. Then, upon performing the join operation on the two patterns, we obtain the new pattern &lt; v~, id~ &gt; generated. We denote these candidate patterns by Ck+l. We prune these patterns by using an analogous trick to that which is used by the Apriori method [2]. Specifically, all k-subset patterns of each member of C~+1 must be frequent and present in  X k. Otherwise, it cannot have the required support and must be pruned from C~+1. All those patterns in C~+~ which have support greater than s are retained. This set of patterns  X k+1 axe the frequent (k + 1)-patterns. This process continues in bottom up fashion until at some level, the set Ek is empty. Once all the frequent patterns axe generated, we use them to generate the rules at the user-specified level of confidence. For each frequent pattern P E  X ~ and class C~, the rule P =~ Ci is generated is it has the desired minimum confidence c. We denote the final set of rules generated by 7~. The formal pseudocode for the creation of rules is illustrated in Figure 4. 
Once these rules have been generated, we need to use them in order to actually classify the records. In order to do so, we find which rules axe fired by a given test instance. In some cases, these rules may be conflicting, as a result of which it becomes necessary to develop precedence crite-ria. The criteria and algorithms developed for rule prece-dence and pruning share some common characteristics with those developed in [21] for the multidimensional classifica-tion problem. Given two rules R: and R2, the rule R1 has higher precedence than R2 if: 
We assume that the rules in 7~ axe sorted in the order of their precedence. In order to prune the rule set, we will analyze the coverage behavior of this rule set in conjunction with the precedence. A data point x is said to be covered by a rule, when the antecedent of the rule is a subpattern of the wavelet transformed representation of x. A data point is said to be consistent with a rule, when the class label in the consequent of that rule is the same as the label of the data point x. 
We initialize the final rule set .T to the null set (}. For each record x in the data set :P, we examine the rules in 7~ in the order of precedence, starting at the highest. We find the first rule R' from 7~ which covers x. Then, we check whether R' is consistent with x. If so, then it is marked. Otherwise, the data point x is removed from T~. We repeat this process for each of the data points. At the end of this procedure, a trnncated database remains, along with the rules in T~ which have now been marked. We retain only the marked rules from T~ in the final rule set, while maintain-ing their original order of precedence. The majority class in the truncated data set is denoted as defaultclass. The ordered set of rules together with the default class form the model which is used for the classification of individual test instances. The procedure for the reordering and pruning of rules is illustrated in Figure 5. 
Once these rules have been generated, the classification of test instances is relatively straightforwaxd. The order of precedence of the rule set along with the defaultclass pro-vides a classification algorithm. For each test instance T, we first determine its wavelet transformed representation. We used the transformed representation to determine the high-est precedence rule which covers the test instance T. The class label in the consequent of this rule is reported as the class of T. If no rule covers the test instance T, then default-class is reported as the class label. The formal pseudocode for the classification procedure is illustrated in Figure 6. 
We tested the system on an AIX 4.1.4 system with 200 MB of main memory. We generated two different kinds of data sets in order to demonstrate the effectiveness of the method over a wide range of problems. We tested the algorithm with two different types of data sets: 2We note that this is a small subset of the total number of such patterns present in the Entrez database. Figure 7: Effects of increasing fragmentation 
Mouse Genome Data Set Figure 8: Effects of increasing fragmentation on 
Web Access Data Set (WAS1) base pattern with equal probability of 0.5, and the or-der was maintained. by sampling web pages from a proxy traces. Each se-quence was drawn from the alphabet ~ = {ibm, other} depending upon the whether the page accessed be-longed to the ibm.corn domain or not. The sequences were labeled morning or evening, depending upon the time of access of the first web page in the sequence. We had two traces from which we generated two data sets which we will henceforth refer to as WAS1 and WAS2 respectively. These data sets contained 521 and 644 sequences respectively. As in the case of the mouse data set, we created fragmented version of the data sets, which we refer to as WAS1-F and WAS2-F re-spectively. 
As a baseline for effectiveness, we tested the nearest neigh-bor classifier using global alignment [4]. In addition, in or-der to test the effectiveness of the wavelet decomposition itself, we used a classifier which used exactly the same algo-rithm as discussed above except that it used the raw strings the other hand, the DirectRule method did not use the ad-vantages of the Wavelet representation in identifying useful compositional characteristics of the data. As a result, in each case it could not match the effectiveness of the WavRule classifier in spite of its similarities to the latter in all other respects. In Table 4, we have illustrated the statistics of the average pattern length and order in the antecedent of the rules which was used to classify the test instances. While the pattern lengths are relatively short, they are often of varying orders. This tends to indicate two facts: (1) The classification behavior was hidden in small local regions of the string. (2) The classification behavior is often created by compositional behavior at different levels of granularity, since the most discriminatory patterns vary considerably in order. We note that neither the nearest neighbor classi-fier nor the DirectRule method is capable of providing such insight. In particular, the nearest neighbor classifier was unable to achieve either of the above two goals. 
In order to test the robustness of the WavRule classifier further, we tested the data at varying levels of fragmen-tation. We define f as the fraction of the symbols which are sampled from the string in order to create the subse-quences of these strings. In Figure 7, we have illustrated the accuracy of all classifiers with increasing fragmentation rate of the strings for the mouse data set. It is clear that with increasing fragmentation rate, the WavRule classifier outperforms the nearest neighbor classifier by an increasing margin. The differences are particularly pronounced in the case of the web data sets as illustrated in Figures 8 and 9. In all cases, even the DirectRule classifier performed better than the nearest neighbor classifier at higher fragmentation rates. The reason for the poor performance of the nearest neighbor classifier was that the lengths of the sequences were quite small in the case of the web data set. Small amounts of fragmentation were able to change the order of the distances sufficiently so that the methodology was no longer effective. This confirms the fact that the WavRule method is signifi-cantly more robust than the nearest neighbor classifier. The reason for this is its combination of a compositional and po-sitional technique for classification which continues to retain its robustness over different kinds of data sets. 
We note that the nearest neighbor classifier is somewhat cumbersome because it requires the calculation of the edit distance between the strings from scratch. Since this dis-tance calculation requires a dynamic programming algorithm, the time complexity of the most efficient algorithm is worse than quadratic in the average pattern length. Furthermore, because of the lack of suitable nearest neighbor indexes for complex distance functions such as the edit distance, the nearest neighbor algorithm needs to compute this value over all strings in the database sequentially. This leads to a database size dependent time for testing. The tradeoffs in the case of the WavRule classifier are somewhat different. While the WavRule classifier is. extremely efficient in clas-sification of test instances by several orders of magnitude, it requires an additional time for training which is not re-quired by the nearest neighbor classifier. However, we note that this training time cost is incurred only once, after which the model can be efficiently used for any number of test in-stances. In Table 3, we have illustrated the running time for classification of individual test instances for each of the two [10] V. Guralnik, J. Srivastava. Event detection from time [11] D. Gusfield. Algorithms on Strings, Trees and [12] J. Hart, G. Dong, Y. Yin. Efficient Mining of partial periodic patterns in time series databases. ICDE [13] H. Jagadish, N. Koudas, S. Muthukrishnan. Mining Deviants in a Time Series Database. VLDB Conference, 1999. [14] H. Jagarlish, N. Koudas, S. Muthukrishnan. On Effective Multidimensional Indexing of Strings. [15] M. James. Classification Algorithms, Wiley, 1985. [16] J. R. Quinlan. C4.5: Progrttrns for Machine Learning. Morgan Kaufmann, 1993. [17] D. A. Keim, M. Heczko. Wavelets and their Applications in Databases. ICDE Conference, 2001. [18] E. J. Keogh, M. J. Pazzini. An enhanced representation of time series data which allows fast and accurate classification, clustering and relevance feedback. [19] E. Keogh, P. Smyth. A probabilistic approach to pattern matching in time-series databases. KDD [20] E. Keogh, K. Chakrabarti, S. Mehrotra, M. Pazzini. Locally Adaptive Dimensionality Reduction for Indexing Large Time Series Databases. SIGMOD Conference, 2001. [21] B. Liu, W. Hsu, Y. Ma. Integrating Classification and Association P~ule Mining. KDD Conference, 1998. [22] S. Manganaris. Learning to Classify Sensor Data. TR-CS-95-10, Vanderbilt University, March 1995. [23] T. Oates. Identifying distinctive subsequences in multivariate time series by clustering. KDD Conference, 1999. [24] C. Perng, H. Wang, S. Zhang, S. Parker. Landmarks: A new model for similarity-based pattern querying in 
