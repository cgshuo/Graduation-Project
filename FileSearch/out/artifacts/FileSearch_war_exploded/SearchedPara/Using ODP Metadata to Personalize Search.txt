 The Open Directory Project is clearly one of the largest collabora-tive efforts to manually annotate web pages. This effort involves over 65,000 editors and resulted in metadata specifying topic and importance for more than 4 million web pages. Still, given that this number is just about 0.05 percent of the Web pages indexed by Google, is this effort enough to make a difference? In this paper we discuss how these metadata can be exploited to achieve high quality personalized web search. First, we address this by intro-ducing an additional criterion for web page ranking, namely the distance between a user profile defined using ODP topics and the sets of ODP topics covered by each URL returned in regular web search. We empirically show that this enhancement yields better results than current web search using Google. Then, in the second part of the paper, we investigate the boundaries of biasing Page-Rank on subtopics of the ODP in order to automatically extend these metadata to the whole web.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.5 [ Information Storage and Retrieval ]: On-line Information Services X  Web-based services ; F.2.1 [ Analysis of Algorithms and Problem Complexity ]: Numerical Algorithms and Problems Algorithms, Experimentation, Analysis Personalized Search, Metadata, Open Directory, Biased PageRank Copyright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00.
Everyone working in the context of the semantic web is con-vinced of the utility of metadata describing the content and various other interesting properties of web pages and relationships between them. Probably almost everybody is equally convinced that we will not be able to manually annotate all web pages. But do we really need to?
This paper focuses on manually entered metadata expressing top-ical categorizations of web pages, as well as on the importance of these pages. This kind of metadata was one of the first metadata available on the web in significant quantities, because it is useful to provide hierarchically structured access to high-quality (recom-mendable) content on the web, starting with efforts like the Yahoo! Directory, collected and put together by a group of human editors. By inserting a web page into one or more categories, basically a content classification is annotated to the document.

Most notable is the annotation / categorization done in the con-text of the Open Directory Project (ODP). This is one of the largest efforts to manually annotate web pages, exporting all this metadata information in RDF format. Over 65,000 editors are busy keeping the directory reasonably up-to-date, and the ODP now provides ac-cess to over 4 million web pages in the ODP catalog. Still, given the fact that Google now indexes more than 8 billion pages, the ODP effort still only covers about 0.05 percent of the Web pages indexed by Google. So does search using these metadata stand any chance against Google?
One good use these metadata can be put to is to personalize search, i.e., returning search results which are both relevant to the user profile, as well as of good quality. This paper investigates the possibilities we have for building such a personalized search en-gine based on ODP or similar directory metadata and investigates the quality and effectiveness of such personalization. Specifically, this paper investigates two ways to personalize search and makes the following contributions:
First, using ODP entries directly, we show how to generalize per-sonalized search in catalogs such as ODP and Google Directory be-yond the currently available search restricted to specific categories. The precision of this personalized search significantly surpassed the precision offered by Google in a set of experiments on topic related searches.

Second, extending the manual ODP classifications from its cur-rent 4 million entries to a 8 billion Web in an automated way is fea-sible, based on an analysis of how topic classifications for a small but important subset of a large page collection can be extended to this large collection via topic-sensitive biasing of PageRank values [21]. This generalizes earlier approaches which already investi-gated topic-sensitive page ranks, but relied on very simple classifi-cations using only 16 topics.

The paper is organized as follows: In Section 2 we will give a short overview of the Open Directory Project, as well as of Page-Rank and Personalized PageRank as relevant algorithms for this paper. In Section 3 we discuss how we can directly use ODP and Google directory entries to implement personalized search based on user profiles corresponding to topic vectors from the ODP hier-archy, and discuss a user study comparing Google and ODP search with these personalized versions. Section 4 builds on the idea that sets of ODP or other directory entries can be used to bias PageRank appropriately, and thus to implicitly extend such annotations to the rest of the Web. We specifically investigate when biasing on such a set actually makes a difference to non-biased PageRank, presenting experiments with various kinds of biasing sets (i.e., including dif-ferent kinds of entries). We then use these results to analyze biasing sets from the ODP 2001 crawl used in [11] and show that all bias-ing sets we investigated (up to four levels deep) can be successfully used for biasing. Finally, we sketch future work and conclude.
Description . The  X  X MOZ X  Open Directory Project (ODP) [20] is the largest, most comprehensive human-edited web page cata-log currently available. It covers 4 million sites filed into more than 590,000 categories (16 wide-spread top-categories, such as Arts, Computers, News, Sports, etc.) Currently, there are more than 65,000 volunteering editors maintaining it.

ODP X  X  data structure is organized as a tree, where the cate-gories are internal nodes and pages are leaf nodes. By using sym-bolic links, nodes can appear to have several parent nodes. Since ODP truly is free and open, everybody can contribute or re-use the dataset, which is available in RDF (structure and content are avail-able separately). Google for example uses ODP as basis for its Google Directory service.

Applications . Besides its re-use in other directory services, the ODP taxonomy is used as a basis for various other research projects. In Persona [23], ODP is applied to enhance HITS [13] with dynamic user profiles using a tree  X  X oloring X  technique (by keeping track of the number of times a user has visited pages of a specific category). Users can rate a page as being  X  X ood X  or  X  X n-related X  regarding their interest. This data is then used to rank and omit interesting/unwanted results. While [23] asks users for feed-back, we only rely on user profiles, i.e., a one-time user interaction. More, we do not develop our search algorithm on top of HITS, but on top of any search algorithm, as a refinement. In [17], a similar approach using the ODP taxonomy is applied onto a recommender system of research papers.

The Open Directory can also be used as a reference source con-taining  X  X ood X  pages, to fight web spam containing uninteresting URLs through whitelisting [14, 24], as a web corpus for compar-isons of rank algorithms [5], as well as for focused crawling to-wards special-interest pages [7, 3]. Unfortunately, the free avail-ability of ODP also has its downside. A clone of the directory modified to contain some spam pages could trap people to link to this fake directory, which results in an increased ranking not only for this directory clone, but also for the injected spam pages [10].
PageRank [21] computes Web page scores based on the graph inferred from the link structure of the Web. It is based on the idea that  X  X  page has high rank if the sum of the ranks of its backlinks is high X . Given a page p , its input I ( p ) and output O ( p ) the PageRank formula is:
The dampening factor c &lt; 1 (usually 0.15) is necessary to guar-antee convergence and to limit the effect of rank sinks [2]. Intu-itively, a random surfer will follow an outgoing link from the cur-rent page with probability (1  X  c ) and will get bored and select a random page with probability c (i.e., the E vector has all entries equal to 1 /N , where N is the number of pages in the Web graph).
Initial steps towards personalized page ranking are already de-scribed by [21] who proposed a slight modification of the above presented algorithm to redirect the random surfer towards preferred pages using the E vector. Several distributions for this vector have been proposed since.

Topic-sensitive PageRank . Haveliwala [11] builds a topic-oriented PageRank, starting by computing off-line a set of 16 Page-Rank vectors biased on each of the 16 main topics of the Open Di-rectory Project [20]. Then, the similarity between a user query and each of these topics is computed, and the 16 vectors are combined using appropriate weights.

Personalized PageRank . A more recent investigation, [12], uses a different approach: it focuses on user profiles. One Per-sonalized PageRank Vector (PPV) is computed for each user. The personalization aspect of this algorithm stems from a set of hubs (H) 1 , each user having to select her preferred pages from it. PPVs can be expressed as a linear combination of PPVs for preference vectors with a single non-zero entry corresponding to each of the pages from the preference set (called basis vectors). The advantage of this approach is that for a hub set of N pages, one can compute 2 gorithm again, unlike [21], where the whole computation must be performed for each biasing set. The disadvantages are forcing the users to select their preference set only from within a given group of pages (common to all users), as well as the relatively high com-putation time for large scale graphs.
Motivation . We presented in Section 2.2 the most popular ap-proaches to personalizing Web search. Even though they are the best so far, they all have some important drawbacks. In [21], we need to run the entire algorithm for each preference set (or biasing set), which is practically impossible in a large-scale system. At the other end, [11] computes biased PageRank vectors limited only to the broad 16 top-level categories of the ODP, because of the same problem. [12] improves this somewhat, allowing the algorithm to bias on any subset of a given set of pages (H). Although work has been done in the direction of improving the quality of this latter set [4], one limitation is still that the preference set is restricted to a subset of this given set H (if H = { CN N, F OXN ews } we can-not bias on MSNBC for example). More importantly, the bigger H is, the more time is needed to run the algorithm. Thus finding differently from the more popular definition from [13]. a simpler and faster algorithm with at least similar personalization granularity is still a worthy goal to pursue. In the following we make another step towards this goal.

Introduction . Our first step was to evaluate how ODP search compares with Google search, specifically exploiting the fact that all ODP entries are categorized into the ODP topic hierarchy. We started with the following two observations: 1. Given the fact that ODP  X  X ust X  includes 4 million entries, 2. ODP advanced search offers a rudimentary  X  X ersonalized
Most people would probably answer (1)  X  X o, not yet X  , and (2)  X  X es X  . In the following Section we will prove the correctness of the second answer by introducing a new personalized search algorithm, and then we will concentrate on the first answer in the experiments Section.
Our algorithm is exploiting the annotations accumulated in generic large-scale annotations such as the Open Directory [20]. Even though we concentrate our forthcoming discussion on ODP, practically any similar taxonomy can be used. These annotations can be easily used to achieve personalization, and can also be com-bined with the initial PageRank algorithm [21].

We define user profiles using a simple approach: each user has to select several topics from the ODP, which best fit her interests. For example, a user profile could look like this: Then, at run-time, the output given by a search service (from Google, ODP Search, etc.) is re-sorted using a calculated distance from the user profile to each output URL. The execution is also depicted in Algorithm 3.1.
 Algorithm 3.1 . Personalized Search.
 Input : P rof u : Profile for user u , given as a vector of topics Output : Res u : Vector of URLs, sorted after user u  X  X  preferences 1 : Send Q to a search engine S (e.g., Google) 2 :
Res u = Vector of URLs, as returned by S 3 : For i = 1 to Size ( Res u ) 4 : Sort Res u using Dist as comparator
We additionally need a function to estimate the distance between a URL and a user profile. Let us inspect this issue in the following discussion.

When performing search on Open Directory, each resulting URL comes with an associated ODP topic. Similarly, a good amount of the URLs output by Google [9] is connected to one or more topics within the Google Directory (almost 50%, as discussed in Section 3.2). Therefore, in both cases, for each output URL we are dealing with two sets of nodes from the topic tree: (1) Those representing the user profile (set A ), and (2) those associated with the URL (set B ). The distance between these sets can then be defined as the minimum distance between all pairs of nodes given by the Carte-sian product A  X  B . Finally, there are quite a few possibilities to define the distance between two nodes. Even though, as we will see from the experiments, the simplest approaches already provide determine which metric best fits this kind of search. In the follow-ing, we will present our best solutions so far.

Na  X   X ve Distances . The simplest solution is the minimum tree-distance, which, given two nodes a and b , returns the sum of the minimum number of tree edges between a and the sub-sumer (the deepest node common to both a and b ) plus the minimum number of tree edges between b and the subsumer (i.e., the shortest path between a and b ). On the example from Figure 1, the distance between / Arts / Architecture and /
Arts / Design / Interior Design / Events / Competitions is 5, and the subsumer is / Arts .

If we also consider the inter-topic links from the Open Directory, the simplest distance becomes the graph shortest path between and b . For example, if there is a link between Interior Design and Architecture in Figure 1, then the distance between Competitions and Architecture is 3. This solution implies to load either the entire topic graph or all the inter-topic links into memory. Furthermore, its utility is subjective from user to user: the existence of a link between Architecture and Interior Design does not always imply that a famous architect (one level below in the tree) is very close to the area of interior design. We can consider these links in our metric in three ways: 1. Consider the graph containing all intra-topic links and output 2. Consider graph containing only the intra-topic links directly we took in this study. 3. If there is an intra-topic link between a and b
Complex Distances . The main drawback of the above metrics comes from the fact that they ignore the depth of the subsumer. The bigger this depth is, the more related are the nodes (i.e., the con-cepts represented by them). This problem is solved by [16], which investigates ten intuitive strategies for measuring semantic simi-larity between words using hierarchical semantic knowledge bases such as WordNet [18]. Each of them was evaluated experimentally on a group of testers, the best one having a 0.9015 correlation be-tween the human judgment and the following formula:
The parameters are as follows:  X  and  X  were defined as 0.2 and 0.6 respectively, h is the tree-depth of the subsumer, and semantic path length between the two words. Considering we have several words attached to each concept and sub-concept, then if the two words are in the same concept, 1 if they are in different concepts, but the two concepts have at least one common word, or the tree shortest path if the words are in different concepts which do not contain common words.

Although this measure is very good for words, it is not per-fect when we apply it to the Open Directory topical tree because it does not make a difference between the distance from profile node) to the subsumer, and the distance from put URL) to the subsumer. Consider node a to be /Top/Games and b to be /Top/Computers/Hardware/Components/Processors/x86 . A teenager interested in computer games (level 2 in the ODP tree) could be very satisfied receiving a page about new processors (level 6 in the tree) which might increase his gaming quality. On the other hand, the opposite scenario (profile on level 6 and output URL on level 2) does not hold any more, at least not to the same extent: a processor manufacturer will generally be less interested in the games existing on the market. This leads to our following exten-sion of the above formula: with l 1 being the shortest path from the profile to the subsumer, the shortest path from the URL to the subsumer, and  X  a parameter in [0 , 1] .
 Combining the Distance Function with Google PageRank . And yet something is still missing. If we use Google to do the search and then sort the URLs according to the Google Directory taxonomy, some high quality pages might be missed (i.e., those which are top ranked, but which are not in the directory). In or-der to integrate that, the above formula could be combined with the Google PageRank. We propose the following approach:  X  is another parameter in [0 , 1] which allows us to keep the final score S 00 ( a, b ) also inside [0 , 1] (for normalized PageRank scores). Finally, if a page is not in the directory, we take S 0 ( a, b )
Conclusion . Human judgment is a non-linear process over in-formation sources [16], and therefore it is very difficult (if not im-possible) to propose a metric which is in perfect correlation to it. A thorough experimental analysis of all these metrics (which we are currently performing, but which is outside the scope of this paper) could give us a good enough approximation. In the next Section we will present some experiments using the simple met-ric presented first, and show that it already yields quite reasonable improvements.
To evaluate the benefits of our personalization algorithm, we in-terviewed 17 of our colleagues (researchers in different computer science areas, psychologists, pedagogues and designers), asking each of them to define a user profile according to the Open Di-rectory topics (see Section 3.1 for an example profile), as well as to choose three queries of the following types: We then compared test results using the following four types of Web search: 1.  X  X lain X  Open Directory Search 2. Personalized Open Directory Search , using our algorithm 3. Google Search, as returned by the Google API [8] 4. Personalized Google Search , using our algorithm from Sec-
For each algorithm, each tester received the top 5 URLs with respect to each type of query, 15 URLs in total. All test data was shuffled, such that testers were neither aware of the algorithm, nor of the ranking of each assesed URL. We then asked the subjects to rate each URL from 1 to 5, 1 defining a very poor result with respect to their profile and expectations (e.g., topic of the result, content, we took the average grade as a measure of importance attributed to that &lt; algorithm, querytype &gt; pair. The average values for all users and for each of these pairs can be found in table 1, together with the averages over all types of queries for each algorithm.
We of course expected the  X  X lain X  ODP search to be significantly worse than the Google search, and that was the case: an average of 2.41 points for ODP versus the 2.76 average received by Google. Also predictable was the dependence of the grading on the query type. If we average the values on the three columns representing each query type, we get 2.54 points for ambiguous queries, 2.91 for semi-ambiguous ones and 3.25 for clear ones -thus, the clearer was the query, the better rated were the URLs returned.

Personalized Search using ODP . But the same table 1 also pro-vides us with a more surprising result: The personalized search al-gorithm is clearly better than Google search , regardless whether we use Open Directory or Google Directory as taxonomy. Therefore, a personalized search on a well-selected set of 4 million pages often provides better results than a non-personalized one over a 8 billion set. This a clear indicator that taxonomy-based result sorting is in-deed very useful. For the ODP experiments, only our clear queries did not receive a big improvement, mainly because for some of other meaning. itations imposed by the Google API, as well as the limited number of Google API licenses we had available. ODP Search 2.09 2.29 2.87 2.41 Personalized ODP Search 3.11 3.41 3.13 3.22 Google Search 2.24 2.79 3.27 2.76 Personalized Google Directory Search 2.73 3.15 3.74 3.20 these queries ODP contains less than 5 URLs matching both the query and the topics expressed in the user profile.

Personalized Search using Google . Similarly, personalized search using Google Directory was far better than the usual Google search. We would have expected it to be even better than the ODP-based personalized search, but results were probably negatively in-fluenced by the fact that the ODP experiments were run on 1000 results, whereas the Google Directory ones only on 100, due to the limited number of Google API licenses we had.

The grading results are summarized in Figure 2. Generally, we can conclude that personalization significantly increases out-put quality for ambiguous and semi-ambiguous queries. For clear queries, one should prefer Google to Open Directory search, but also Google Directory search to the plain Google search. Also, the answers we sketched in the beginning of this Section proved to be true: Google search is still better than Open Directory search, but we provided a personalized search algorithm which outperforms the existing Google and Open Directory search capabilities.
Another interesting result is that 40.98% of the top 100 Google pages were also contained in ODP. More specifically, for the am-biguous queries 48.35% of the top pages were in the directory, for on our experiments [1], obtaining the following results: were covering multiple topics. Src. of variance QS Deg. of Free. F-value [25] Query Type 17.092 2 F(2,32,75%) = 2.114 Algorithm 22.813 3 F(3,48,99%) = 6.812 Inter-Relation 7.125 6 F(6,96,95%) = 2.512 Table 2: Survey results for the analyzed web search approaches
For a more in-depth view, the statistical analysis data is collected in table 2.
In the last Section we have shown that using ODP entries and their categorization directly for personalized search turns out to be amazingly good. Can this huge annotation effort invested in the ODP project (with 65,000 volunteers participating in building and maintaining the ODP database) be extended to the rest of the Web? This would be useful if we want to find less highly rated pages not contained in the directory. Just extending the ODP effort does not scale, because first, significantly increasing the number of vol-unteers seems improbable, and second, extending the selection of ODP entries to a larger percentage obviously becomes harder and less rewarding once we try to include more than just the  X  X ost im-portant X  pages for a specific topic.

We start with the following questions:
One of the most important work investigating PageRank biasing Rank on and then provides a method to combine these 16 result-ing vectors into a more query-dependant ranking. But what if we would like to use one or several ODP (sub-)topics to compute a Personalized PageRank vector? More general, what if we would like to achieve such a personalization by biasing PageRank towards some generic subset of pages from the current Web crawl we have? Many authors have used such biasings in their algorithms. Yet none have studied the boundaries of this personalization, the character-istics the biasing set has to exhibit in order to obtain relevant re-sults (i.e., rankings which are different enough from the non-biased PageRank). We will investigate this in the current Section. Once these boundaries are defined, we will use them to evaluate (some of) the biasing sets available from ODP in Section 4.2.

First, let us establish a characteristic function for biasing sets, which we will use as parameter determining the effectiveness of bi-asing. Pages in the World Wide Web can be characterized in quite a few ways. The simplest of them is the out-degree (i.e., total num-ber of out-going links), based on the observation that if biasing is targeted to such a page, the newly achieved increase in PageRank score will be passed forward to all its out-neighbors (pages to which it points). A more sophisticated version of this measure is the hub value of pages. Hubs were initially defined in [13] and are pages pointing to many other high quality pages. Reciprocally, high qual-ity pages pointed to by many hubs are called authorities . There are several algorithms for calculating this measure, the most common ones being HITS [13] and its more stable improvements SALSA [15] and Randomized HITS [19]. Yet biasing on better hub pages will have less influence on the rankings because the  X  X ote X  a page gives is propagated to its out-neighbors divided by its out-degree. Moreover, there is also an intuitive reason against this measure: PageRank biasing is usually performed to achieve some degree of personalization and people tend to prefer highly valued authorities to highly valued hubs. Therefore, a more natural measure is an authority-based one, such as the non-biased PageRank score of a page.

Even though most of the biasing sets consist of high PageRank pages, in order to make this analysis complete we have run our experiments on different choices for these sets, each of which must be tested with different sizes. For comparison to PageRank, we used two degrees of similarity between the non-biased PageRank and each resulting biased vector of ranks. They are defined in [11] as follows: 1. OSim indicates the degree of overlap between the top n 2. KSim is a variant of Kendall X  X   X  distance measure. Unlike
Even though [11] used n = 20 , we chose n to be 100 , after ex-perimenting with both values and obtaining more stable results with the latter value. A general study of different similarity measures for ranked lists can be found in [6].

Let us start by analyzing the biasing on high quality pages (i.e., with a high PageRank). We consider the most common set to contain pages in the range [0  X  10]% of the sorted list of Page-Rank scores. We varied the sum of scores within this set between 0.00005% and 10% of the total sum over all pages (for simplicity, we will call this value T OT hereafter). For very small sets, the biasing produced an output only somewhat different: about 38% Kendall similarity (see Figure 3). The same happened for large sets, especially those above 1% of T OT . Finally, the graph makes also clear where we would get the most different rankings from the line, then we consider the biased ranks to be relevant, i.e., different enough from the non-biased ones.
 Figure 3: Biasing behavior for top 0 -10% PageRank pages Figure 4: Biasing behavior for top 0 -2% PageRank pages
Someone could wish to bias only on the best pages (the top 2]% , as in Figure 4). In this case, the above results would only be shifted a little bit to the right on the x-axis of the graph, i.e., the highest differences would be achieved for a set size from 0.02% to 0.75%. This was expectable, as all the pages in the biasing set were already top ranked, and it would therefore take a little bit more effort to produce a different output with such a set.

Another possible input set consists of randomly selected pages (Figure 5). Such a set most probably contains many low PageRank pages. This is why, although the biased ranks are very different for low T OT values, they start to become extremely similar (up to almost the same) after T OT exceeds 0.01% (because it would take a lot of low PageRank pages to accumulate a T OT value of 1% of the overall sum of scores, for example).

The extreme case is to bias only on low PageRank pages (Figure 6). In this case, the biasing set will contain too many pages even sooner, around T OT = 0 . 001% .

The last experiment is mostly theoretical. One would expect to obtain the smallest similarities to the non-biased rankings when us-ing a biasing set from [2  X  5]% (because these pages are already close to the top, and biasing on them would have best chances to overturn the list). Experimental results support this intuition (Fig-
Figure 6: Biasing behavior for random low PageRank pages ure 7), generating very different rankings for very small biasing sets and up to sets of T OT = 0 . 1% , that is for a large scale of sizes for the biasing set.

The graphs above were initially generated based on a crawl of 3 million pages. Once all of them had been finalized, we se-lectively ran similar experiments on the Stanford WebBase crawl [22], obtaining similar results. For example, a biasing set of size T OT = 1% containing randomly selected pages produced rank-ings with a 0.622% Kendall similarity to the non-biased ones, whereas a set of T OT = 0 . 0005% produced a similarity of only 0.137%. This was necessary in order to prove that the above dis-cussed graphs are not influenced by the crawl size. Even so, the limits they establish are not totally accurate, because of the random or targeted random selection (e.g., towards top [0  X  2]% pages) of our experimental biasing sets. The URLs collected in the Open Directory are manually added Web pages supposed to (1) cover the specific topic of the ODP tree leaf they belong to and (2) be of high quality. Both requirements are not fully satisfied. Sometimes (rarely though) the pages are not really representing the topic in which they were added. More Figure 7: Biasing behavior for top 2 -5% PageRank pages Table 4: Low-level ODP biasing analysis for the Stanford ODP crawl important for PageRank biasing, they usually cover a large interval of page ranks, which made us decide for the random biasing model. However, we are aware that in this case, the human editors chose much more high quality pages than low quality ones, and thus the decisions of the analysis are susceptible to errors.

Generally, according to the random model of biasing, every set with T OT below 0.015% is good for biasing. According to this, all possible biasing sets analyzed in tables 4, 5 and 3 would generate a
We can therefore conclude that biasing is (most probably) possi-ble on all subsets of the Stanford Open Directory crawl.
Given that directories like ODP represent some of the largest manual metadata collections today (ODP contains topic classifi-cations for about 0.05% of all Google indexed pages), this paper investigated the impact these efforts have and specifically their fea-sibility to implement personalized search based on these metadata. We investigated two possibilities to do that, and made the following contributions:
First, we showed how to generalize personalized search in cata-logs such as ODP and Google Directory beyond the currently avail-able search restricted to specific categories. The precision of this personalized search significantly surpassed the precision offered by unpersonalized search in a set of experiments. ceed this limit a little bit, but running the biased PageRank with it produced a good enough similarity -most probably because of the special structure of the ODP topic sets, as we discussed above in this Section. Table 5: Low-level ODP biasing analysis for the Stanford ODP crawl
Second, extending the manual ODP classifications from its cur-rent 4 million entries to a 8 billion Web is feasible, based on an analysis of how topic classifications for subsets of large page col-lection can be extended to this large collection via topic-sensitive biasing of PageRank values.

While the theoretical framework we presented in this Section is generally applicable, so far we were only able to apply it on an ex-isting ODP crawl from 2001 (the one used in [11]). Our crawler is currently collecting a new crawl based on the current ODP di-rectory, which we will then use to evaluate the current status and quality of the ODP entries and their suitability for biasing. [1] J. Bortz. Statistics for Social Scientists . Springer Verlag, [2] S. Brin, R. Motwani, L. Page, and T. Winograd. What can [3] S. Chakrabarti, M. van den Berg, and B. Dom. Focused [4] P.-A. Chirita, D. Olmedilla, and W. Nejdl. Pros: A [5] C. Ding, X. He, P. Husbands, H. Zha, and H. D. Simon. [6] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank [7] M. Ester, H.-P. Kriegel, and M. Schubert. Accurate and [8] Google search api. http://api.google.com. [9] Google search engine. http://www.google.com. [10] Z. Gy  X   X  X ngyi, H. Garcia-Molina, and J. Pendersen. [11] T. Haveliwala. Topic-sensitive pagerank. In Proceedings of [12] G. Jeh and J. Widom. Scaling personalized web search. In [13] J. M. Kleinberg. Authoritative sources in a hyperlinked [14] O. Kolesnikov, W. Lee, and R. Lipton. Filtering spam using [15] R. Lempel and S. Moran. The stochastic approach for [16] Y. Li, Z. A. Bandar, and D. McLean. An approach for [17] S. E. Middleton, D. C. D. Roure, and N. R. Shadbolt. [18] G. Miller. Wordnet: An electronic lexical database. [19] A. Y. Ng, A. X. Zheng, and M. I. Jordan. Stable algorithms [20] Open directory project. http://dmoz.org/. [21] L. Page, S. Brin, R. Motwani, and T. Winograd. The [22] Stanford webbase project. http://webbase.stanford.edu. [23] F. Tanudjaja and L. Mui. Persona: A contextualized and [24] M. Williamson. Using dmoz open directory project lists with [25] J. B. Winer. Statistical principles in experimental design .
