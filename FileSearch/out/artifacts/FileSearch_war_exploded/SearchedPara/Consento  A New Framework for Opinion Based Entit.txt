 Search engines have become an important decision making tool today. Decision making queries are often subjective, such as  X  X  good birthday present for my girlfriend, X   X  X est action movies in 2010, X  to name a few. Unfortunately, such queries may not be answered properly by conventional search systems. In order to address this problem, we introduce Consento , a consensus search engine designed to answer subjective queries. Consento performs segment indexing, as opposed to document indexing, to capture semantics from user opinions more precisely. In particular, we define a new indexing unit, Maximal Coherent Semantic Unit (MCSU). An MCSU represents a segment of a document, which cap-tures a single coherent semantic. We also introduce a new ranking method, called ConsensusRank that counts online comments referring to an entity as a weighted vote. In order to validate the efficacy of the proposed framework, we com-pare Consento with standard retrieval models and their recent extensions for opinion based entity ranking. Experi-ments using movie and hotel data show the effectiveness of our framework.
 H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods, Linguistic processing; H.3.3 [ Information Storage and Retrieval ]: Retrieval models entity search, consensus search, ConsensusRank, sentiment analysis, maximal coherent semantic unit
Web search has become ubiquitous today. Commercial search engines have been highly effective for factual queries like  X  X Phone 4S release date. X  Users can find the answers to such queries in one of the top ranked documents. How-ever, the current search engines fall short of giving relevant  X  Corresponding author.
 answers to subjective queries. For example, queries like  X  X est action movies in 2010 X  and  X  X hrillers with a plot twist X  are not properly answered by the current engines. There, the top ranked documents may contain a list of best action movies or plot-twisting thrillers. That list, however, reflects only the opinions of document authors, rather than public sentiment.

Vertical search engines, such as Google Product Search 1 and Bing Video 2 , provide domain specific query results. How-ever, they just produce the entities whose descriptions match the query terms. Apart from the conventional search sys-tems, there exist entity search and question-answering(QA) systems [2, 5, 4]. These systems produce direct answers to queries such as  X  X ho is the president of the House of Chanel? X  and  X  X irlines flying Boeing 747. X  Here, the most popular approach is to query a search engine to retrieve passages that are likely to contain candidate answers, and then extract the answers from those passages using natural language processing and machine learning techniques. Still, they just focus on factual queries that contain finite sets of true answers. Thus, these systems are not appropriate, either, for our problem context.

Opinion QA (OQA), as opposed to  X  X actual X  QA, focuses on answering opinion queries, such as  X  X hat negative opin-ions do people have on Hilary Benn? X  and X  X hat are the rea-sons for the success of the Kyoto Protocol? X  3 [1]. However, such systems still follow the standard QA processes, retriev-ing relevant information pieces and extracting answers from them. With only a small number of top ranked documents, it would be difficult to ensure reliable performance for the consensus search problem.

Opinion finding tasks have been run in the TREC blog track (2006-2009) in the form of retrieving blog posts that contain the most relevant opinions to the query [8, 9]. A typical query in this task is  X  X hat do people think about X ? X  This task focuses on finding opinion-bearing blog posts and not opinions themselves. Most participants approach this task as a re-ranking problem. In the first stage, top-k blog posts are retrieved independent of their opinionated na-ture using a conventional retrieval system, and in the second stage, the top-k list is reordered using an opinion detection technique. This re-ranking approach suffers from the same shortcomings as the OQA systems. It is difficult to ensure http://www.google.com/shopping http://www.bing.com/videos
TAC 2008 Opinion Question Answering. http://www.nist.gov/tac/ that the top-k blog posts reflect the natural distribution of opinions on the query in the blogosphere.

As explained so far, most of the previous approaches in the related problem domains are inappropriate for our problem context. For example, given a query,  X  X ood value hotels, X  we need to quantify positive and negative opinions on the value aspect of each hotel and rank the hotels based on a combination of them. The key challenges here are how to quantitatively summarize the opinions on an aspect of a tar-get entity such that the quantitative summary reflects the consensus in the user opinions as closely as possible, and more importantly, how to do it all online in query time. The previous approaches fall short of answering this kind of query mainly because they postpone semantic analysis of in-dexed documents until query time. Relevant documents are retrieved first and then they are further processed to extract entities, opinions and so on.

Let us now turn to the formal definition of the search problem that we are to address.

Definition 1. (Consensus Search Problem) Given an entity set E = { e 1 ,e 2 ,  X  X  X  ,e u } and a query Q , suppose there exists an ideal ranking function CR ( Q, e i ) that would return arankof e i reflecting the amount of votes that e i would have received on the web with respect to Q . The consensus search problem is then defined as the problem of locating the ranked list of entities L =[ e k 1 ,e k 2 ,  X  X  X  ,e k u CR ( Q, e k i )  X  CR ( Q, e k j ) for all 1  X  i  X  j  X  u .
In order to address this problem, we introduce Consento , a consensus search engine designed to answer subjective queries. Unlike standard text retrieval systems, Consento indexes Maximal Coherent Semantic Unit (MCSU), which is a maximal subsequence of words containing a single co-herent meaning within a document. For example,  X  X xcellent performance, but plot was hard to follow X  implies two dif-ferent sentiments, or one positive sentiment (performance) and the other negative (plot). However, for a query  X  X xcel-lent plot, X  conventional text retrieval systems would find the above review relevant to the query, since their indexing unit is a document and the review document contains both of the two query terms in proximity. In order to capture the user X  X  sentiment correctly, Consento splits the review comment into two MCSU segments, and indexes them separately.
Among others, our search subsystem significantly differs, in terms of ranking, from conventional standard text re-trieval models. Conventional models would produce seg-ments that best match query terms. On the other hand, Consento is designed to return entities that are most agreed upon by users with respect to the query context. In order to implement this, we introduce a new ranking model, Con-sensusRank, which takes into account a user opinion, or an MCSU segment, matching a particular query as a weighted vote to the  X  X eferred to X  entity.
 Figure 1 shows the conceptual diagram illustrating how ConsensusRank works. When given a query, all matching segments are retrieved from the index. The retrieved seg-ments are then grouped by their referencing entities. Fi-nally, the scores of the segments are aggregated to compute the scores of the corresponding entities. The score of each segment is determined by multiple factors including similar-ity to the query terms, sentiment orientation, strength, and review quality.
The working prototype of Consento is available at http:// Consento .korea.ac.kr .
In order to process consensus queries online, Consento performs most of the semantic analysis early in the indexing stage. The MCSU has been introduced for this purpose 4 .
An MCSU segment captures a user X  X  opinion on an entity X  X  aspect. Hence, Consento  X  X  query processing is closely re-lated, in essence, to aspect-based opinion mining [7].
Aspect-based opinion mining is typically carried out through four phases: aspect extraction [11], search for associated opinions [10], classification of sentiments on the aspect [12], and summarization of sentiments on the aspect [14].
However, virtually all previous works were developed with emphasis on off-line processing. Contrastingly, Consento is designed to perform all of the four sentiment analysis phases online. MCSU is formally defined as follows: Definition 2. (Maximal Coherent Semantic Unit) Given a review r i =[ w 1 ,w 2 ,  X  X  X  ,w n ], an MCSU segment in r is s j =[ w k 1 ,w k 2 ,  X  X  X  ,w k r ]where1  X  k 1 &lt;k 2 k r  X  n , a subsequence of r i containing a single coherent semantic.

The task of extracting MCSU is non-trivial as it requires sophisticated natural language processing. We start with a dependency parser to analyze the sentence structure. Fig-ure 2 shows resulting parsing examples produced with the FANSE parser [13]. We explain the segmentation processes using the examples below.

Example 1.  X  X he performance is excellent but the camera work is terrible. X 
A brief sketch of Consento was presented in [3]. Figure 2: Segmentation Examples: Sentence (top) and Phrases (bottom).

This comment contains two distinct opinions on two dif-ferent aspects of the target movie, one for performance (pos-itive) and the other for camera work (negative). Our MCSU extraction algorithm works in two steps as follows: 1. Locating root-level verbs. We identify the root 2. MCSU expansion. The first phase produces two
Example 2.  X  X reat music, fantastic action, believ-able characters. X 
Not all user posts are well-formed sentences. Users may express their opinions freely in any format they like. The most popular form, other than sentences, is similar to the above examples. Figure 2 (bottom) shows the dependency relations obtained from the parser. The segmentation pro-cess is almost identical to the previous example. The dif-ference is that we locate nouns instead of verbs at the root level in the first phase. The second step is identical. The resulting three MCSUs are illustrated in Figure 2 (bottom).
The details of our MCSU segmentation algorithm is given in Algorithm 1.

Once MCSUs are extracted, Consento indexes them with a standard inverted index. The only change is the additional information included in each posting , or an entry in an in-verted list. Given a document corpus, D = { d 1 ,d 2 ,  X  X  X  a standard inverted index consists of two parts: the lex-icon L containing all the distinct terms in the documents in D and the inverted lists of postings, with each associ-ated to a term t i in L . Normally, a posting consists of &lt;id j ,f ij , [ o 1 ,o 2 ,  X  X  X  ,o | f ij | ] &gt; ,where id document d j that contains the term t i , while f ij represents the frequency count of t i in d j ,and o k  X  X  indicate the offsets of t i in d j . On the other hand, for the purpose of MCSU indexing, we store additional information in each posting Algorithm 1 MCSU Segmentation Algorithm such as review ID and sentiment words (hereinafter,  X  X enti-words X ), in order to use it in the ranking stage. We formally define the structure of MCSU postings below.
 Definition 3. (MCSU Posting)
Given a data source set W = { w 1 ,w 2 ,  X  X  X  ,w y } ,are-view set R = { r 1 ,r 2 ,  X  X  X  ,r x } ,anMCSUsegmentset S = { s 1 ,s 2 ,  X  X  X  ,s r } , an aspect set A = { a 1 ,a 2 ,  X  X  X  ,a word set M = { m 1 ,m 2 ,  X  X  X  ,m t } , and a target entity set E = { e 1 ,e 2 ,  X  X  X  ,e u } , an MCSU posting consists of &lt;sid [ o ,  X  X  X  ,o | f ij | ] , ( aid l , [ mid 1 ,  X  X  X  ,mid q ]) ,rid sid j is the ID of the MCSU segment s j that contains the term t i ; e k is the target entity that s i refers to; f sents the frequency count of t i in s j ; o k  X  X  indicate the offsets of t i in s j ; aid l is the ID of the aspect a l appearing in s mid p refers to the ID of the sentiword m p modifying a l means the ID of the review r v containing s j ;and wid z is the ID of the data source w z containing the review r v .
For example, suppose IMDB ( w 1 )hastworeviews( r 1 , r 2 on Titanic ( e 1 ). The second review ( r 2 ) is split into three segments ( s 4 -s 6 ). The MCSU posting on the first segment ( s 4 ) that says  X  X ouching soundtrack, X  (e.g., m 1 a 2 ,respec-tively) is constructed as &lt;s 4 ,e 1 , ( a 2 , [ m 1 ]) ,r dexed to X  X ouching X  X nd X  X oundtrack X  X osting lists. Negation is also handled in our model. Our segmentation algorithm picks up negations and encodes them by inverting the polar-ity of the sentiword in the corresponding aspect-sentiment pair.
Given a query, once the matching segments are retrieved, we group them by entity and aggregate the segment scores per entity to compute the entity scores. The segment score and the entity score (i.e., ConsensusRank) are defined for-mally as follows: Definition 4. (Segment Score) Given the sets W , R , S , A , M in Definition 3 , and a query Q , the score of the segment s is defined by: where n represents the total number of the segments in the corpus; n t indicates the number of the segments containing the query term t ; q ( r ) means the review quality of the review r containing s ; p ( s ) is the mean polarity of the sentiwords in typically set between 0 and 5.

Definition 5. (ConsensusRank) Given a query Q ,an entity set E , a segment set S , S Q = { s i | s i  X  S that match Q } , S e = { s i | s i  X  S that refer to entity e  X  E } ,and S { s i | s i  X  S Q  X  S e } ,thescoreofentity e is defined as: where | S e | represents the size of S e and K is the parameter typically set between 0 and 1.

The term | S e | K is a factor intended to normalize the im-balance in the review volume among entities. As K de-creases, the scoring function prefers the popular entities that tend to acquire more reviews.
We validate the framework using the review data sets from two different domains, movies and hotels.

Data Sets. Forthemoviedataset,wecrawledfromsix major sources including Amazon, IMDB, Metacritic, Flixster, Rotten Tomatoes and Yahoo Movies. For experiments, we used the reviews for the movies from 2008 to 2010, which accumulate to 130MB.

We obtained the hotel data set from Ganesan and Zhai, which they used for evaluating their work in [6]. The data set contains the reviews for the hotels in 10 major cities, which are crawled from TripAdvisor 5 . The authors also kindly provided us the corrected judgement set for our test because the original one they used in [6] has some missing hotels. Following them, we removed the hotels with less than 10 reviews from the evaluation. For more details about the data set, please refer to [6].

Relevance Judgment Generation. As no gold stan-dard is available for consensus movie ranking, we constructed one for validation purposes. Although it would be ideal to use human judgments to construct the gold standards, it would be a very time consuming and labor intensive process to have human judges read the reviews and score how accu-rately the review descriptions match the returned entities. To alleviate this problem, we opted to use some authorita-tive lists of movies that may reflect the cinematic quality and popularity of each film. http://www.tripadvisor.com
As to cinematic quality, we resorted to the last three years X  movie awards and festivals. We looked up the award histo-ries of top 12 award ceremonies and festivals such as the Academy Awards and the Cannes Film Festival. The win-ner in an event receives 2 points and a nominee receives 1 point. By aggregating the points, we generated the movie rankings for five award categories for each year from 2008 to 2010. The five categories include  X  X est movies X  (e.g., best picture award),  X  X est performance, X  X  X est cinematography, X   X  X est music, X  and  X  X est screenplay. X  For constructing multi-aspect query judgement sets, we normalized the scores in each ranking by dividing by max score.

With regard to the popularity list, we used the statistics on box office revenues. We generated the box office rank-ings for three categories, including  X  X ction, X  X  X omedy, X  and  X  X rama, X  for each year from 2008 to 2010.

Query Generation. Following the Ganesan and Zhai X  X  approach, we asked three average users to provide three short seed queries per aspect [6]. We then generated all possible combinations of the seed queries across the five cin-ematic quality aspects. A query  X  X est performance, good direction X  is one of the example two-aspect queries overar-ching performance and direction aspects. The shortest query contains only one aspect while the longest touches all five as-pects. A total of 6298 queries are generated in this process. We do not construct a multi-aspect query from the genre aspects because it is not natural. For example, there are not many movies that span over all three genres. Hence, the popularity queries are all single aspect queries. All datasets, the judgment sets, and the seed query sets are available for download at http://infos.korea.ac.kr/consento .
 Baselines. As the baseline, we used Ganesan and Zhai X  X  OE and QAM methods, which are the current state-of-the-art opinion-based entity ranking methods [6]. They con-catenate all reviews on an entity in a single document, and index the document using a standard text retrieval system. As to query time, the OE expands the user query with a predefined set of synonyms of opinion words, and processes the expanded query as usual. The QAM is an additional improvement. It splits a query based on aspects, processes each subquery separately, and aggregates the scores from the subqueries for a final computation of rankings. Finally, the ranks of the returned documents represent the ranks of the corresponding entities. We implemented the OE and the QAM on Lucene 3.1 for evaluation.
 Two standard text retrieval models are also compared: BM25 and VSM+BM (default Lucene scorer). Since they are all based on the conventional text retrieval model, we had to concatenate all reviews for a movie in one document, and index the documents, in order to test them for the pur-pose of entity search. The current version of Consento is implemented on Lucene 3.1 and runs on an 8-node HP Pro-Liant DL320 cluster running Ubuntu server v10. Each node is equipped with a 4-core 2.13GHz Intel Xeon CPU, 12 GB memory and 1TB of disks.
Table 1 shows the nDCG@10 scores for the queries con-cerning cinematic quality averaged over the 2008-2010 re-sults. The  X  X ingle X  and  X  X ouble X  means single-aspect and double-aspect queries, respectively. The X  X ong X  X eans queries span over 3 to 5 aspects. As illustrated in the table, Con-sento (CR) significantly outperformed the baselines includ-Table 1: nDCG@10 Scores of Cinematic Quality Queries.
 Table 2: nDCG@10 Scores of Popularity Queries.
 ing VSM+BM (Lucene default scorer), BM25, and their OE and QAM extensions. The improvements range from 39% to 70% in the three test cases. We used the default model parameters for CR ( k 1 = k 2 =1and K = 0) for all of our evaluation.

Table 2 shows the nDCG@10 scores for the popularity queries averaged over the 2008-2010 results. In this test, we only evaluate the single aspect queries due to the afore-mentioned reason. Thus, no QAM results are reported in this test. Similar to the previous test, CR outperformed all baselines with substantial margins, achieving 33% to 203% gains in the three test cases.

Table 3 shows the nDCG@10 scores for the hotel queries averaged over all 10 cities. In this case, CR underperformed the OE and QAM baselines for the long queries by 1.4%. In order to comprehend the result, we investigated what facilitates this drastic performance improvement of the OE and QAM in the hotel data set.

This happens because the OE expands an opinion word in the query such as  X  X ood X  and  X  X ice X  to a predefined set of 35 positive sentiment words and an intensifier such as  X  X ery X  to a collection of 21 similar adverbs. It appears that the expanded words completely dominate the aspect (or any context) words in the matching process, which leads to a production of a generic ranked result where hotels that are generally good in all aspects are always put on the top. It implies that OE may not work in domains where the vari-ances in per-aspect preferences are high. It is certainly not a desirable property for consensus search.

In fact, the Spearman X  X  rank correlation coefficients of ho-tel rankings in the judgment set range from 0.44 to 0.93 while those of cinematic quality range from -0.28 to 0.37. The rank correlations of the popularity rankings cannot be computed as they do not have much overlap. In this study, we introduced a new search problem, which we have termed consensus search. In order to address the problem, we proposed a new search framework, and attested to its validity by implementing a prototype search engine, Consento . Consento is unique in the sense that it is the first consensus search engine that ranks entities based on the implicit votes extracted from users X  online posts. In future work, we plan to expand Consento to other domains including, for example, products, events, organizations, and social issues, upon which users express their opinions. We also plan to explore broader options for ranking parameters to further enhance the performance of our system. This work was supported by the National Research Foun-dation of Korea(NRF) grant funded by the Korea govern-ment(MEST)(No.2012R1A2A2A01014729). [1] A. Balahur, E. Boldrini, A. Montoyo, and [2] K. Balog, P. Serdyukov, and A. de Vries. Overview of [3] J. Choi, D. Kim, S. Kim, J. Lee, S. Lim, S. Lee, and [4] H. T. Dang, D. Kelly, and J. Lin. Overview of the trec [5] G. Demartini, T. Iofciu, and A. P. de Vries. Overview [6] K. Ganesan and C. Zhai. Opinion-based entity [7] B. Liu. Sentiment analysis and subjectivity. Handbook [8] C. Macdonald, R. L. Santos, I. Ounis, and I. Soboroff. [9] I. Ounis, C. Macdonald, and I. Soboroff. Overview of [10] A.-M. Popescu and O. Etzioni. Extracting product [11] G. Qiu, B. Liu, J. Bu, and C. Chen. Opinion word [12] T. T. Thet, J.-C. Na, and C. S. Khoo. Aspect-based [13] S. Tratz and E. Hovy. A fast, accurate, non-projective, [14] J. Zhu, M. Zhu, H. Wang, and B. K. Tsou.

