 Tail queries, which occur with low frequency, make up a large fraction of unique queries and often affect a user X  X  expe-rience during Web searching. Because of the data sparseness problem, information that can be leveraged for tail queries is not sufficient. Hence, it is important and difficult to improve the tail query performance. According to our observation, 26% of the tail queries are not essentially scarce: they are expressed in an unusual way, but the information require-ments are not rare. In this study, we improve the tail query performance by fusing the results from original query and the query reformulation candidates. Other than results re-ranking, new results can be introduced by the fusion model. We emphasize that queries that can be improved are not only bad queries, and we propose to extract features that predict whether the performance can be improved. Then, we utilize a learning-to-rank method, which is trained to di-rectly optimize a retrieval metric, to fuse the documents and obtain a final results list. We conducted experiments using data from two popular Chinese search engines. The results indicate that our fusion method significantly improves the performance of the tail queries and outperforms the state-of-the-art approaches on the same reformulations. Exper-iments show that our method is effective for the non-tail queries as well.
 H.3.3 [ Information Search and Retrieval ]: [Retrieval models, Query formulation] Algorithms, Experimentation, Measurements Fusion model; Tail query; Query reformulation; Results re-finement; Learning to rank
The frequencies of the queries that are issued to search engines vary over a wide range: some are submitted far more than ten thousand times per day, while others, usu-ally called tail queries 1 , can be submitted less than once a day. Although the frequencies of the tail queries are rather low, they make up more than 70% of all of the unique queries; they involve most of Web users; furthermore, they bring about considerable commercial value and long-term revenue for the search engines[16]. Because most of today X  X  commercial search engines can generally respond to non-tail queries with excellent results, the performances of the tail queries, to a large extent, determine the users X  experiences. While current search engines do not perform well on them in many cases, users who issue tail queries tend to reformulate their previous query more frequently[12]. We analyzed one month X  X  search log of a Chinese commercial search engine and found that in approximately 28% of the tail queries, the users do not click on any of the results before they change to other queries or quit, and most of those queries have no relevant documents in the first search engine results page (abbreviated as SERP below), while in non-tail queries, the rate is only 3%. The average time for user scanning of the first SERP is 2.8 times longer in tail queries than that in non-tail queries. These observations show that many tail queries mismatch good results in the first SERP, making users have to try navigating to later pages or reorganizing their queries in the search. The poor performance of tail queries is also one of the reasons that users switch search engines[37].

There are three main causes of the mismatch problem of tail queries. The first cause is that the user X  X  information need is very rare. The second cause is the lack of correspond-ing Web resources. The last cause is the unusual or improper expression of the query. We study the third case, which is the easiest case, in this work. Different people sometimes ex-press the same information need in different ways, and some of the representations are unusual. Thus, these unusual rep-resentations can lead to a mismatch between the query and the target documents. In fact, it is often difficult for a user to compose appropriate and usual queries in Web searches because he might not know how others make similar state-ments. From real search logs, we find that 26% of the tail queries describe information requirements that are stated in an unusual way, but their requirements are not rare. So it
As in [34, 30], we define tail queries as queries whose fre-quencies are below 15 in one month in this paper is reasonable to introduce good results for a tail query from its reformulations.

Because there are fewer user clicks on the results of the tail queries, it is difficult to optimize the tail query search results when utilizing clickthrough data[21]. Thus, it is still difficult to handle an adaptive retrieval model, especially for the tail queries at present. In our work, we have proposed a fusion method that merges the reformulation results to generate a better results list for the queries that are issued, especially tail queries.

The contributions of our work are threefold. First, we in-troduce new results to improve the tail queries instead of re-ranking the documents. Second, we deem that queries that can be improved are not equivalent to bad queries, and we incorporate features in our fusion model that can pre-dict whether a query can be improved. Finally, our exper-iments on search engine data show that our fusion method can improve the performance of tail queries as well as non-tail queries significantly.

The remainder of this paper is organized as follows. Sec-tion 2 discusses related work. In section 3, we describe our method, including its features and the model. Section 4 shows the experiment, and we discuss the results in section 5. Section 6 provides the conclusions and raises some direc-tions for future work.
Our research is related to three areas of study: (I)studies about tail queries, (II)merging of results, and (III)query per-formance prediction. We cover these three areas next.
In recent years, tail query study has attracted a substan-tial amount of attention because the long-tail phenomenon has significantly influenced sponsored searches[25].
Some work has been performed on user behavior analy-sis of tail queries. Doug Downey et al. observed that users often click less following a tail query, which indicates that the results that are returned by search engines are not as valuable for the tail queries[13]. Reference [12] showed that similar frequencies of the query and destination URL are more likely to lead to search success and that the infor-mation that is provided by search engines depends on the frequency of the information goal. Ting Yao et al. proposed a practical categorization framework and gave a comparison of different types of tail queries[34]. This analysis indicates that the search performance of tail queries is not as good as for non-tail queries and that user behaviors on different frequencies of queries vary. Thus, the performance of tail queries must be improved and methods of optimizing non-tail query results might be not as effective on tail queries.
There are studies that address improving tail query perfor-mances. One approach to help achieve search success is that of using query recommendations. Query recommendations modify user queries to improve the retrieval performance. Some methods aiming at tail query reformulation have been proposed. Andrei Broder et al. [5] described an online tail query expansion approach for sponsored search. Song et al. [30] incorporated implicit feedback from users(i.e., skipped URLs) into random walks, and Szpektor et al. [31] pro-posed query-template flow graphs in which the ODP was used to transfer queries to templates. Reference [4] de-fined a new approach to query recommendations for long tail queries, which is based on random walks on a Term-Query Graph . These methods performed well on tail query recommendations. To ensure high precision, Liu et al. used a learning-to-rank approach to select query suggestions [23]. Query recommendations can give users a hint to change their query when they are not satisfied with the results, and this approach is an indirect method for improving the perfor-mance of tail queries. Some work has also been performed to improve the results list itself. Zhou et al.[38] utilize the similarities between different queries to overcome the data sparsity problem for tail queries and to re-rank the results with information from different but related queries. In our work, new results of reformulations are introduced, and we are focused on not only the similarity of reformulations but also whether a tail query must be improved and whether a reformulation performs better. On the other hand, we train our model to optimize a retrieval metric with a method that is based on LambdaRank.
The concept of merging results originates from metasearch and federated search. Metasearch is the well-studied process of fusing the ranked lists of documents that are returned by a collection of search systems in order to obtain a combined list whose quality exceeds that of any of the underlying lists. In a results merging task, the core issue is aggregating the ranking scores of the results. Two benchmark techniques are CombSUM [14] and Condorcet [24]. Both can also be used on a federated search, which merges the results of a sin-gle query on multiple collections or the results of multiple reformulations on a collection. In the two techniques, each source(reformulation or collection) is given the same impor-tance, but in fact, that is not always the case in reality.
There is related work on supervised merging, in which the merging can be weighted according to the quality of the reformulation or collection. Reference [36] first predicted the query difficulty from using the IR metrics and then merged the results based on the predicted quality. In [28], the two steps above are fused in a learning-to-rank framework. The authors used quality-indicating gating features as part of a joint training process to optimize a retrieval metric (e.g., NDCG [20]) of the final merged list.

In our work, we extract more specific features of search results to measure the relevance of a document. And we utilize features that indicate not only the performance of the original queries and the similarity of reformulations but also how much better a reformulation is than the original query. What X  X  more, we use both the document level features and query level features when training our fusion model.
Recently, many query performance predictors have been proposed. Pre-retrieval query-performance prediction meth-ods [18] analyze the query expression. The most effective prediction approaches employ post-retrieval analysis of the results list, and we show three types of prominent examples.
The clarity prediction method [10] computes the relative entropy between a query language model and the corre-sponding collection language model. The clarity scores mea-sure the ambiguity of a query with respect to its documents by computing different forms of their distances [1, 7].
Additionally, there is related work on analyzing retrieval score distributions to predict query performances [11, 2].
Different ideas of the robustness of the results list were proposed to indicate the query performance [35, 29]. Refer-ence [17] used the features of the query, search results and user interaction with the search results to predict the query performance.

In our work, we use more specific features of search results to measure query performance. And we evaluate the perfor-mance of part of the query X  X  results list (e.g., the top two results and the top 10 results). Moreover, we would like to also predict by how much a reformulation X  X  results are bet-ter than the original results. Additionally, we incorporate these features to fuse the results of the original query and its reformulations.
In this section, we describe FusRank , which is the pro-posed fusion ranking framework, in detail. To provide a better results list for the tail query, we expect to enrich rele-vant documents via reformulations, remove bad results that are ranked in front in the original list and place results that have higher relevance forward. For this purpose, we submit the original tail query and reformulation candidates to the search engine and extract the documents in the first SERP of the original query and each candidate query (typically, there are ten documents). The fusing process is performed on all of those documents.
The power of learning-to-rank is that it can employ many different features into one scoring function. Next, we list all the features used by our model and give a brief motivation for their employment. In general, to improve the tail query performance by merging multiple results lists, it is neces-sary to solve three problems: 1) to determine whether a tail query must enrich its results from reformulations, 2) if so, to estimate whether the reformulation candidates are rele-vant to the original query and whether they can bring better results in case the results get worse, and 3) if this circum-stance is the case, to find out which results in the lists are relevant to the original query X  X  information need and rank them in the front. We extract features for all of the three problems(see Table 1). We also report the coverage of each feature because it is important to understand the quality of sparse features, such as click-through-based statistics.
Here, we define some symbols. For a tail query q ,whichis also called the original query, let C = { c 1 ,c 2 , ..., c set of reformulation candidates, and let D = { d 1 ,d 2 , ..., d be the document set that contains the top 10 results of each candidate as well as q . All of the employed classes of features are covered in the following.
When users issue a query, search engines score the doc-uments in the collection to obtain a reasonable results list. Most of the ranking functions define a term weighting func-tion such as BM25[27], which exploits the term frequency as well as other factors, such as the document X  X  length, the collection statistics and the document X  X  structure[26]. Then, the results list will be optimized by some other factors, such as the clickthrough data[21]. Because there are few clicks on tail query results, we utilize more specific matching charac-teristics. Each of the retrieved documents for a query has a title and a snippet to respond to the user, and here we obtain those that are from the SERPs. The title and the snippet of a document show how the document matches the query, and we define the matched terms as the terms in the title or snippet that match the original query. It is worthwhile mentioning that the matched terms in a reformulation result are the terms that match the original query instead of the reformulation. We calculate some of the features along with the length of all of the matched terms in the title and snip-pet. Additionally, we consider the positions of the matched terms and introduce features such as the position of the first matched term in the title.

In addition, while there are few clicks in the tail queries, we expect these clicks to be a measure of the relevance. Thus, we introduce the CTR ( click through rate ) and CRa-tio( the ratio of clicks ) of the document in both the original query and its own query(If a document appears in more than one results list, the maximum value is selected), and these terms are defined as follows:
Because we attempt to import reformulations that enrich the results list and rank the results, it is unsafe to introduce irrelevant reformulation results. Thus, we introduce some features that can show the similarities between q and c i
The reformulation candidates are generated with scores by random walks on three graphs, which are mentioned in the experiments. Certainly, the scores will change along with the method of reformulation mining. Candidates who have higher scores can be closer to the original query. We use the three scores as three features, and if a candidate is not generated by a graph, the value of the corresponding feature is 0.

The pseudo-relevance feedback method assumes that a certain number of top-ranked documents are relevant [33]. Based on this idea, we assume that if there are some docu-ments in two queries X  top results and they are clicked more than once, the two queries might have similar information needs. Thus, we extract click information from search logs and center on the co-click documents in the first SERP of the original query and candidate query. We count the clicks of each document and calculate the cosine value of the two queries X  click vector in which each dimensionality corresponds with clicks of a document in SERP. Further-more, we estimate the relevance of each co-click document according to its click-through rate and then compute the DCG ( discounted cumulative gain )andMAP( mean aver-age precision ) metrics.

It is reasonable that if the results lists of two queries are similar, then the two queries are probably similar. For q and c i , we count the overlap results of q and c i  X  X  results lists at the page level and site level. In addition, we calculate the cosine similarity between the vectors of q and c i  X  X  top 10 results. The vectors are formed by terms that appear in the results after the stopword removal, and each term is weighted by TF-IDF. Reference [23] has used similar fea-tures. Ind. Feature name Feature description Cov.

Document Matching Characteristic Features 10 UTSMatchQ ( q,d i ) Proportion of the union of matched terms in d 11 ITSMatchQ ( q,d i ) Proportion of intersection of matched terms in d 12 CTR ( q,d i ) Click-through rate of d i in q 6.3% 14 CTR own ( c i ,d j ) Click-through rate of d j in d  X  X  own query c i 20.1%
Query Similarity Features 16 GScore 1( c i ) c i  X  X  generating score on the Query  X  17 GScore 2( c i ) c i  X  X  generating score on the Query  X  19 Click NUM ( q,c i ) Total clicks of co-click documents in q and c 20 Click COS ( q,c i ) Cosine value of q and c i  X  X  results click vectors 72.0% 21 Click DCG ( q,c i ) DCG value of co-click documents in q and c 22 Click MAP ( q,c i ) MAP value of co-click documents in q and c 23 Edit ( q,c i ) Edit distance between q and c i 100.0% 24 PageOverlap ( q,c i ) Number of overlap results between q and c 25 SiteOverlap ( q,c i ) Number of overlap results between q and c 26 PageSim ( q,c i ) Cosine similarity of q and c  X  X  vectors constructed by their result pages [23] 100.0%
Query Satisfaction Features 27 ListTMatchQ ( q,N ) Mean of TMatchQ of q  X  X  top N results 100.0% 28 ListTMatchT ( q,N ) Mean of TMatchT of q  X  X  top N results 100.0% 29 ListSMatchQ ( q,N ) Mean of SMatchQ of q  X  X  top N results 100.0% 30 ListSMatchS ( q,N ) Mean of SMatchS of q  X  X  top N results 100.0% 31 ListTMatchQL ( q,N ) Mean of TMatchQL of q  X  X  top N results 100.0% 34 ListTMathcFP ( q,N ) Mean of TMathcFP of q  X  X  top N results 100.0% 35 ListSMatchFP ( q,N ) Mean of SMatchFP of q  X  X  top N results 100.0% 36 ListUTSMatchQ ( q,N ) Mean of UTSMatchQ of q  X  X  top N results 100.0% 37 ListITSMatchQ ( q,N ) Mean of ITSMatchQ of q  X  X  top N results 100.0% 38 ListCTR ( q,N ) Mean of CTR of q  X  X  top N results 100.0% 39 ListCRadio ( q,N ) Mean of CRadio of q  X  X  top N results 100.0%
Satisfaction Improvement Features 40 TMatchQImp ( c i ,q,N ) ListTMatchQ ( c i ,N )  X 
ListTMatchQ ( q,N ) 100.0% 41 TMatchTImp ( c i ,q,N ) ListTMatchT ( c i ,N )  X 
ListTMatchT ( q,N ) 100.0% 42 SMatchQImp ( c i ,q,N ) ListSMatchQ ( c i ,N )  X 
ListSMatchQ ( q,N ) 100.0% 44 TMatchQLImp ( c i ,q,N ) ListTMatchQL ( c i ,N )  X 
ListTMatchQL ( q,N ) 100.0% 45 TMatchQBegImp ( c i ,q,N ) ListTMatchQBeg ( c i ,N )  X 
ListTMatchQBeg ( q,N ) 100.0% 46 TMatchQIntImp ( c i ,q,N ) ListTMatchQInt ( c i ,N )  X 
ListTMatchQInt ( q,N ) 100.0% 47 TMathcFPImp ( c i ,q,N ) ListTMathcFP ( c i ,N )  X 
ListTMathcFP ( q,N ) 100.0% 48 SMatchFPImp ( c i ,q,N ) ListSMatchFP ( c i ,N )  X 
ListSMatchFP ( q,N ) 100.0% 49 UTSMatchQImp ( c i ,q,N ) ListUTSMatchQ ( c i ,N )  X 
ListUTSMatchQ ( q,N ) 100.0% 50 ITSMatchQImp ( c i ,q,N ) ListITSMatchQ ( c i ,N )  X  ListITSMatchQ ( q,N ) 100.0% 51 CTRImp ( c i ,q,N ) ListCTR ( c i ,N )  X  ListCTR ( q,N ) 100.0%
Original Reciprocal Rank Feature 53 OriRank ( d i ) Reciprocal of d i  X  X  original rank 100.0%
Before improving the performance of a tail query, it is best to know whether the original results are good enough. If the performance of the original query is good, then the system should be more careful when introducing the reformulation results. Previous work has shown that the performance of the search engines can be evaluated by users X  click behav-iors [22]. This automatic evaluation method required a sub-stantial number of user clicks, while there are only a small number of user clicks for the tail queries. Thus, we still use the matching characteristics to measure the satisfaction of a query. For each of the matching characteristic features that are proposed above, we calculate the mean value of the top N results as a feature. Take TMatchQ as an exam-ple; we will obtain a feature called ListTMathcQ ,whichis calculated as follows: Here we introduce these features for which N is 10 and 2. As we attempt to fuse the top 10 results, we can evaluate the satisfaction of the entire list as well as the top results.
Satisfaction improvement features aim to measure whether the candidates X  results are better than the original results, predicting whether the query can be improved by fusing multiple results lists. The intuition is that a candidate that has better results is more likely to bring about an im-provement. For a candidate c i , its satisfaction improvement features are the difference values between its query satis-faction features and the original query q  X  X . For example, ListTMatchQ ( c i ,N ) represents one of c i  X  X  satisfaction fea-tures, and the corresponding satisfaction improvement fea-ture TMatchQImp can be computed as follows:
TMatchQImp ( c i ,q,N )
In addition, we introduce those features for which N is 10 and 2 to evaluate the satisfaction improvement of the entire listaswellasthetopresults.
Although we introduce some candidate results to enrich the original results and rank them to obtain a better list, we must be careful when disrupting the original order to-tally. It is better to retain some of the information that is in the original order and, thus, the reciprocal of a document X  X  original rank becomes one of its features.
The features that we extracted above can divide into two classes: document-level features and query-level features. We can place them into a mixture of experts architecture[19], in which the query-level features can be used to train a gat-ing network to determine the contribution of each reformu-lation candidate. Daniel Sheldon et al. have utilized such an architecture to merge the results of different reformula-tions successfully[28]. However, this architecture might not be suitable for our features because there are cases in which the documents in a candidate X  X  results list should not share the same weight. For example, the satisfaction improvement features could indicate that a candidate c i is worse than the original query q in the top 10 results but better in the top 2 results; then, we should consider importing c i  X  X  top 2 re-sults and rejecting the others. Thus, we aggregate all of the features as a vector for training a learning-to-rank model.
Let x d be the vector of features for document d of a tail query q . If some of the features are missing in d , i.e., the features that are related to the clicks, then the default values must be assigned, and we typically set the default value to be zero.

Let f ( x ;  X  ) be the scoring function, with the parameters  X  . A given document d thus receives a score f ( x d ;  X  ). The scoring function f can be implemented by any differential function, such as a linear function, a neural network, or a set of boosted decision trees. We choose f to be MART(multiple additive regression trees)[15]. Then, the scoring function f can be written as where each f i ( x )  X  R is a function that is modeled by a single regression tree, and  X  i  X  R is the weight that is associated with the i th regression tree. Both f i and  X  i learned during training. A given tree f i maps a given x to a real value by passing x down the tree, where the path (left or right) at a node in the tree is determined by the value of a specific feature x j , j =1 , ..., d , and where the output value of the tree is a fixed value that is associated with each leaf,  X  kn , k =1 , ...L , n =1 , ..., N ,where L is the number of leaves and N is the number of trees. The parameters L and N and learning rate  X  , which multiplies every  X  kn for every tree, are selected before training. The  X  kn are learned during the training. MART uses gradient descent to decrease the loss: specifically, the next regression tree models the derivatives of the cost with respect to the current model score that is evaluated at each training point.

Our model is based on LambdaMART[32]. The param-eters are trained to optimize a retrieval metric, e.g., Pre-cision@k, NDCG, ERR or MAP, utilizing a method that is based on LambdaRank[6], and we choose NDCG as the met-ric to optimize. It is difficult to optimize the retrieval metrics directly because they depend on the sorted order. Thus, the derivatives of the cost with respect to the model parameters are either zero, or are undefined. LambdaRank works with a smoothed version of the objective to avoid these difficulties. The smoothed objective C is defined as where  X  determines the shape of the sigmoid function, s i the current score of vector x i , X  | Z ij | is the absolute change in the metric if documents i and j were swapped in the current ranking, and { i,j } I g ( i, j ) can be expressed as possible rank pairs of each two documents. Then, we can easily calculate its gradients as When we are going to train the k th tree, we change the label of each training data instance; for example, x i  X  X  label is changed to  X  X   X  X  i where s i is determined by the previous k  X  1 trees. Then, we create a tree { R lk } L with these labels. The leaf values are calculated based on a Newton step, as follows: and the weight of the tree is equal to  X  . In our experiments, we train 1000 trees with the learning rate 0.09, and each tree has 10 leaves.
Here, we present the experiments that were used to im-plement and evaluate our fusion method. The experiments were conducted using both search logs and crawled SERPs from a widely-used Chinese commercial search engine.
We use randomly sampled data from one month X  X  search logs from the search engine in March, 2013. Each log entry records an interactive action by the user, such as a query submission or any post-query behavior, including clicking on search results, turning pages, reformulating queries and other actions. There are, in total, 4,190,638 unique queries in the logs. We define tail queries as queries whose frequency in a month is less than 15, and then, there are 2,933,451 tail queries, which make up approximately 70% of all of the unique queries. A total of 300 tail queries are sampled as original queries at random for our experiments.

Experiments are conducted on the 300 randomly sampled tail queries and the reformulation candidates generated be-low. Here, for every tail query, we fuse its top 10 results with each of its candidates X  top five results. Thus, for every tail query, we will obtain a document pool that contains at most 25 documents. All of the documents in the pool are anno-tated with three-level relevance judgements (2:good; 1:fair; 0:bad) in response to the corresponding query by three vol-unteers. We calculate the kappa coefficient of the annota-tion from every two annotators, and the minimum is 0.6823, which means that there is substantial agreement.

We utilize 5-fold cross validation in our experiments. Two metrics, NDCG[20] and ERR[9], are chosen to evaluate the retrieval effectiveness. NDCG is a popular metric for rel-evance judgments, which assigns exponentially high weight to highly relevant documents. ERR is a novel metric that is defined as the expected reciprocal rank at which the user will stop his search. Both metrics are also used in Yahoo! Learn-ing to Rank Challenge [8]. Here we calculate NDCG @ N and ERR @ N where N is from one to ten.
Our method can use any source of reformulation candi-dates. As described in Section 2, there is related work on query recommendations for tail queries. Here, we select three approaches to generating reformulation candidates for the given tail queries; each of the approaches conducts a ran-dom walk on a bipartite graph. The three bipartite graphs are described as follows: 1. Query-flow graph[3]. In this graph if query q j appears 2. Query-URL graph[30]. Because tail queries have much 3. Term-query graph[4]. There are two sets of nodes in We construct the graphs using the month X  X  logs, and we con-duct a random walk on each of them. For the 300 sample tail queries, we obtain 496 reformulations on query-flow graphs that involve 118 queries, 477 reformulations on query-URL graphs that involve 115 queries and 773 reformulations on term-query graphs that involve 284 queries. In total, we have generated 1471 reformulation candidates, which in-volve 287 queries. In our experiments, we conduct the fusion method for each query with 3 reformulation candidates. For the queries that have more than three candidates generated, we rank them by the sum of the three normalized gener-ating scores and choose the top three. All of the contrast experiments also use the same reformulation candidates.
We compare the results of FusRank , our fusion model, against the traditional merging method and the state-of-the-art tail query improvement techniques. All of the methods are listed as follows: 1. ORG: The results of the original query given by the 2. CombSUM: CombSUM is an unsupervised merging 3.  X  -Merge: Sheldon et al. [28] have proposed a super-4. CollRank: Zhou et al. [38] have proposed a method
As it shows in Table 2, CombSUM does not consider the relevance of the reformulation candidates, and it is on aver-age much worse than the original results(ORG). In  X  -Merge, a gating network is used to evaluate the contribution of each candidate, and the results are better than CombSUM; how-ever, it still cannot bring more effective results and even performs slightly worse on average with some metrics. Coll-Rank uses information about related queries to alleviate the problem of sparsity and weighs each related query according to its similarity. The performance is improved, especially in NDCG@3 and NDCG@5. In our FusRank, we introduce specific features that indicate how a document matches the query and features that compare the results of the candi-dates with the original results. It shows that our fusion method performs best, especially in the top results.
We have shown that merging the results of different query reformulations with our fusion model can improve the per-formance significantly relative to the baseline methods. In this section, we provide further insights into our fusion method.
We have observed in the data set that approximately one third of the queries have the original NDCG @10 larger than 0.8; this finding indicates that the tail queries are not all difficult queries. Here, we would like to see how our model performs on queries that have different original quality. We define improved queries as queries that have higher retrieval metric values than their original values. Figure 1 shows the ratio of improved queries in each NDCG @3 bin and NDCG @10 bin. Our model has more improved queries in each bin than the baselines and the improvements are signif-icant with p &lt; 0.05. A query is more likely to be improved when its original performance is neither too good nor too bad. In the bin [0,0.2], for almost half of the queries, the original NDCG value was 0, and the results that were pro-vided by reformulation candidates are often irrelevant. The reason could be that there are few web resources for these queries. The queries in the bin (0.8,1] already have good results lists, and our model can also improve some of these well qualified queries; Figure 2 shows that there are more queries in the bin (0.8,1] after our fusion step. This finding shows that the improved queries are not all bad queries.
Figure 3 shows the cumulative number of results that have different relevance at different ranks. From the figures, we can see that in our method, the cumulative number of good results is larger than the original result at any rank, and the cumulative number of bad or fair results is always smaller. It is worthwhile mentioning that the number of good results in the top three changes from 521 to 558, increasing by 7.1%, while the number of bad results in the top three changes from 234 to 209, decreasing by 10.7%. By introducing new results, our model ranks more good documents in the front. Figure 1: Ratio of improved queries in different bins of NDCG@3 and NDCG@10
In the experiments above, we choose the top 5 results of each candidate to fuse. Here, we want to see how the Figure 2: Number of queries in different bins of NDCG@3 and NDCG@10 improvement changes when different numbers of results are introduced. Figure 4 shows the increase of NDCG metrics when a different number of each candidate X  X  top results are chosen to fuse. The improvements are significant with p &lt; 0.05. When the number is 0, there are no new results in-troduced, and our model will rank only the original results. From the figure, we can see that the increase in NDCG is also impressive, which indicates that the specific matching features that we have extracted are effective. As more re-sults of candidates are chosen, the performance tends to be better. While there is an exception, when the number is 2, the performance become worse than the performance at 1. The reason could be that the improvement is too large when only the first result of each candidate is introduced, which is also shown in the figure. This finding is reasonable because the first result is usually good in SERP. Because the refor-mulation candidates are usually non-tail queries, we think that it is safe to introduce their top results based on the trust in the search engines. Certainly, we should not in-troduce too many results because the document that has a higher rank could become less relevant.
 Figure 4: Increase of NDCG when different numbers of each candidate X  X  top results are fused
We have extracted the five classes of features that are de-scribed in Section 3.1, of which three classes are at the query level, to judge whether a candidate is a good candidate. In this subsection, we analyze the effect of the three classes of query-level features: query similarity features ( QSimF ), query satisfaction features ( QSF ) and satisfaction improve-ment features ( SIF ). For QSF and SIF , we extract the fea-tures for the top 2 results as well as the top 10 results, and we define QSF @ n as the QSF on the top n results. Here, we train our model with different combinations of query-level features. To be specific, we remove one class of query-level features every time. Then, we conduct experiments on the different combinations. Figure 5 shows the performance of each combination of features as well as the performance of the original results and all of the features for contrast.
Query similarity features express the topic drift of the candidates. We can see from the figure that when we leave the query similarity out, the performance becomes worse than ORG . The reason is that a reformulation candidate could be irrelevant to the original query and introducing its results could lead to worse performance.

The query satisfaction features express the satisfaction of the original query; the queries that have poor performance must be improved, while the well-qualified queries should be improved more carefully. QSF @2 could be more important than QSF @10 because leaving QSF @2 out causes a larger reduction in performance. QSF @2 protects the top 2 results because the top results are more important for user expe-rience and we can also see in the figure that NDCG @1 is even smaller than ORG when QSF @2 is removed. It might be feasible to extract the query satisfaction features at more ranks, but more features will make our model more com-plex and QSF @2 and QSF @10 are sufficient to evaluate the performance of the whole list as well as the top results.
The purpose of introducing satisfaction improvement fea-tures is to evaluate the relative performance of the candi-dates with the original query. While we want to improve the queries that have poor performances, we should be care-ful to fuse a candidate X  X  results if its performance is worse than the original performance. Likewise, the well qualified queries can also be improved if their candidates have better performance. Removing SIF @2 or SIF @10 degrades the performance, and SIF @10 is relatively more important.
In summary, our model incorporates various features at the query level and fuses the results of the proper candidates with the proper original queries.
Each of the 300 sampled queries in our experiments is issued less than 15 times in a month. Here, we split the queries into two sets. One set (set1) contains 143 queries that appear only once in a month X  X  search log and the other set (set2) contains the left 157 queries. We test our model on the two sets and Figure 6 shows the results. The original Figure 5: The retrieval metric NDCG for various combinations of features at the query level performance is poorer in set1 than the performance in set2, especially for the top results. Our model is effective on both sets and the range of performance improvement is larger in the top results(i.e., NDCG @1 and NDCG @3). All the improvements are significant with p &lt; 0.05.
 Figure 6: The retrieval metric NDCG for the two sets of queries divided by the frequencies
For comparison purposes, we also experimented on query sets of different frequencies. We randomly sample queries from search logs of another Chinese commercial search en-gine, and we divide the queries into different sets by their issued frequency in March, 2014. Each set contains 500 queries, and the reformulation candidates are provided by the search engine. The top three results of every query and its candidates are annotated with five-level relevance judge-ments by the search engine X  X  annotators. Thus, the top three results of each query and three of its reformulation candi-dates are fused. We conduct the experiments of FusRank on different sets with a five-fold cross validation. Figure 7 shows the NDCG@1 and NDCG@3 metrics on different sets. We find that our method can improve the performance of the queries with a different level of frequency, and the scope of improvement on tail queries is the largest. All the improvements are significant with p &lt; 0.05.
 Figure 7: FusRank on queries with different levels of frequencies
This work was supported by Natural Science Foundation (61472206, 61073071), National High Technology Research and Development (863) Program (2011AA01A207) and Na-tional Key Basic Research Program (2015CB358700) of China.
In this work, we propose FusRank , which is a fusion model to improve the tail queries by introducing new re-sults from their reformulations. We extract specific features at both the document level and the query level, and our model learns to optimize a retrieval metric. We introduce features measuring how much a reformulation is better than the original query and indicate that tail queries whose orig-inal performance is good enough are also improved by our model. Experimental results show that our methods can ef-fectively improve the search performance of the tail queries and improve the performance of the non-tail queries as well. We have analyzed the contribution of each class of query-level features and have shown that all of the features are necessary to help to evaluate the quality of the reformula-tion candidates.

For future work, we plan to obtain more insights into tail queries, including identifying tail queries that lack Web resources and predicting user satisfaction on tail queries, which will provide more guidance in improving the bad per-formance of tail queries. [1] G. Amati, C. Carpineto, and G. Romano. Query [2] N. Balasubramanian, G. Kumaran, and V. R.
 [3] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, [4] F. Bonchi, R. Perego, F. Silvestri, H. Vahabi, and [5] A. Broder, P. Ciccolo, E. Gabrilovich, V. Josifovski, [6] C. J. Burges, R. Ragno, and Q. V. Le. Learning to [7] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. [8] O. Chapelle and Y. Chang. Yahoo! learning to rank [9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [10] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [11] F. Diaz. Performance prediction using spatial [12] D. Downey, S. Dumais, and E. Horvitz. Heads and [13] D. Downey, S. Dumais, D. Liebling, and E. Horvitz. [14] E. A. Fox and J. A. Shaw. Combination of multiple [15] J. H. Friedman. Greedy function approximation: a [16] S. Goel, A. Broder, E. Gabrilovich, and B. Pang. [17] Q. Guo, R. W. White, S. T. Dumais, J. Wang, and [18] C. Hauff, D. Hiemstra, and F. de Jong. A survey of [19] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. [20] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [21] T. Joachims. Optimizing search engines using [22] T. Joachims et al. Evaluating retrieval performance [23] Y. Liu, R. Song, Y. Chen, J.-Y. Nie, and J.-R. Wen. [24] M. Montague and J. A. Aslam. Condorcet fusion for [25] S. Pandey, K. Punera, M. Fontoura, and V. Josifovski. [26] S. Robertson, H. Zaragoza, and M. Taylor. Simple [27] S. E. Robertson and S. Walker. Some simple effective [28] D. Sheldon, M. Shokouhi, M. Szummer, and [29] A. Shtok, O. Kurland, and D. Carmel. Predicting [30] Y. Song and L.-w. He. Optimal rare query suggestion [31] I. Szpektor, A. Gionis, and Y. Maarek. Improving [32] Q. Wu, C. J. Burges, K. M. Svore, and J. Gao. [33] J. Xu and W. B. Croft. Query expansion using local [34] T. Yao, M. Zhang, Y. Liu, S. Ma, and L. Ru.
 [35] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. [36] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. [37] H. Zaragoza, B. B. Cambazoglu, and R. Baeza-Yates. [38] K. Zhou, X. Li, and H. Zha. Collaborative ranking:
