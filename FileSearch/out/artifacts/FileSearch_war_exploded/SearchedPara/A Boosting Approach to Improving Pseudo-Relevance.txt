 Pseudo-relevance feedback has proven effective for improv-ing the average retrieval performance. Unfortunately, many experiments have shown that although pseudo-relevance feed-back helps many queries, it also often hurts many other queries, limiting its usefulness in real retrieval applications. Thus an important, yet difficult challenge is to improve the overall effectiveness of pseudo-relevance feedback with-out sacrificing the performance of individual queries too much. In this paper, we propose a novel learning algo-rithm, FeedbackBoost, based on the boosting framework to improve pseudo-relevance feedback through optimizing the combination of a set of basis feedback algorithms us-ing a loss function defined to directly measure both robust-ness and effectiveness . FeedbackBoost can potentially ac-commodate many basis feedback methods as features in the model, making the proposed method a general optimization framework for pseudo-relevance feedback. As an applica-tion, we apply FeedbackBoost to improve pseudo feedback based on language models through combining different doc-ument weighting strategies. The experiment results demon-strate that FeedbackBoost can achieve better average pre-cision and meanwhile dramatically reduce the number and magnitude of feedback failures as compared to three repre-sentative pseudo feedback methods and a standard learning to rank approach for pseudo feedback.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models, query formulation, relevance feedback Algorithms Pseudo-relevance feedback, FeedbackBoost, loss function, robustness, learning, optimization, boosting  X 
This work was done when Wan Chen was with University of Illinois at Urbana-Champaign.

Pseudo-relevance feedback has proven effective for auto-matically improving the overall retrieval accuracy and aver-age precision for informational queries in many experiments [24, 22, 25, 3, 23, 15, 31, 18]. The basic idea of pseudo feed-back is to assume a certain number of top-ranked documents from an initial retrieval run to be relevant and extract useful information from these feedback documents to improve the original query.

However, although traditional pseudo feedback techniques generally improve retrieval performance (e.g., AP) on aver-age, they are not robust in the sense that they tend to help some queries, but hurt other queries [10, 7], limiting its use-fulness in real retrieval applications. Thus an important, yet difficult challenge is to improve the overall effectiveness of pseudo-relevance feedback without sacrificing the perfor-mance of individual queries too much . Although there has been a lot of work on pseudo feedback, little work has been devoted to address this issue, with only a few exceptions, e.g., [7]. In [7], the authors tried to reduce feedback failures in a constrained optimization approach. However their work was only able to optimize an objective function loosely re-lated to the effectiveness and robustness of pseudo feedback.
In this paper, we propose a novel learning algorithm, Feed-backBoost, based on the boosting framework to improve pseudo-relevance feedback through combining a set of basis feedback algorithms optimally using a loss function defined to directly measure both robustness and effectiveness ,which has not been achieved in any previous work on pseudo feed-back. Specifically, like all other boosting algorithms [9, 26, 8, 30], FeedbackBoost iteratively selects and combines ba-sis feedback methods. In each iteration, a basis feedback method is selected to improve those queries on which the already selected basis feedback methods perform poorly in terms of both effectiveness and robustness. At last, Feed-backBoost uses a linear combination of these basis feedback methods as its final feedback model.

There are several important differences between our work and previous work on improving pseudo feedback: (1) we cast the pseudo feedback problem as an optimization prob-lem that can be solved in a supervised way; (2) we propose a novel objective function that directly measures the effective-ness and the number of failure cases of pseudo feedback; (3) FeedbackBoost can incorporate potentially many different basis feedback methods as features in the model, making it a general optimization framework for pseudo feedback; (4) FeedbackBoost does not introduce any extra parameter that needs to be manually tuned.
As an application, we apply FeedbackBoost to improve pseudo feedback based on language models through com-bining different document weighting strategies. One cause of the low robustness and effectiveness in pseudo feedback is that the feedback documents a re simply assumed to be rel-evant whereas in reality, not all of them are relevant. Thus one way to improve pseudo feedback would be to assign ap-propriate weights to these documents. Indeed, our previous work [18] has already shown that with relevance-based doc-ument weighting, the relevance model [15] tends to be more robust and effective than alternative models for feedback with language models. In the existing work, however, such weighting is generally based on one heuristic or another, and is not optimized directly to improve feedback. Our main idea is to combine a variety of feedback methods each with a dif-ferent strategy for document weighting under the framework of FeedbackBoost. Although we only try to leverage feed-back methods with different document weighting methods in this work, the proposed boosting framework can potentially accommodate many other basis feedback methods.

We evaluate our method using two representative large test sets and compare FeedbackBoost with multiple baseline methods. The experiment results demonstrate that the pro-posed FeedbackBoost algorithm can improve average pre-cision significantly and meanwhile reduce the number and magnitude of feedback failures dramatically as compared to two representative pseudo feedback methods based on lan-guage models, the mixture model and the relevance model. We also compare our algorithm with a recently proposed constrained optimization approach to robust feedback, and the results show that our method is more robust. In addi-tion, we compare FeedbackBoost with a traditional learning to rank approach applied for pseudo feedback and observe that FeedbackBoost works clearly better.
Pseudo-relevance feedback has been shown to be effective for various retrieval models [24, 22, 25, 3, 23, 15, 31, 18]. In the vector space model, feedback is usually done by using the Rocchio algorithm [24]. The feedback method in classical probabilistic models is to select expansion terms primarily based on the Robertson/Sparck-Jones weight [22]. A num-ber of model-based pseudo feedback techniques have been developed for the language modeling framework, including the relevance model [15] and the mixture-model feedback [31], which will be reviewed in Section 5.1.1 and 5.1.2 re-spectively. Most existing pseudo feedback algorithms aim at improving average precision alone but rarely address the robustness issue. In contrast, our work attempts to improve both average precision and robustness at the same time.
A few previous studies also attempted to improve the ro-bustness of pseudo feedback [28, 27, 7]. Tao and Zhai [28] used a regularized EM algorithm to reduce the parameter sensitivity of the mixture-model feedback but did not mini-mize the feedback failures. Soskin et al. [27] leveraged mul-tiple relevance models [15] in a heuristic unsupervised way to improve feedback performance. However, their method is not guaranteed to optimize the combination of feedback algorithms. Collins-Thompson [7] also tried to reduce feed-back failures in an optimization framework. However this work was only able to optimize an objective function loosely related to the robustness of pseudo feedback. In contrast, we propose a general machine learning framework to directly optimize both the robustness and effectiveness of pseudo feedback, which can incorporate existing methods, such as [28], [27], and [7], as features. In this sense, our work offers a unified framework that can be used to potentially combine all the existing pseudo feedback methods.

Selective feedback [2] and adaptive feedback [17] are an-other stream of work to improve the robustness of pseudo feedback, where the idea is to disable query expansion if query expansion is predicted to be detrimental to retrieval [2] or to adaptively set the amount of query expansion in a per-query way [17]. However, these methods are not as general as our proposed framework. Besides, our work and the se-lective/adaptive feedback method are complementary in the following sense: our work can construct a strong ensemble feedback method, which can be used by selective/adaptive feedback to further improve its performance, while selec-tive/adaptive feedback methods can be incorporated into our framework as features for boosting.

Recently, learning to rank [13, 8, 4, 30, 6, 33, 16] has attracted much attention in IR. Our work can also be re-garded a novel use of machine learning to leverage multiple feedback-related features to improve ranking. However, a main difference of our work from traditional work on learn-ing to rank is that we design a novel learning algorithm to di-rectly optimize both robustness and effectiveness of pseudo feedback (novel objective function). Another difference is that most learning to rank work learns optimal ways to combine retrieval functions but fails to improve the query representation. Our work, however, uses machine learning to improve a content-based query representation. Therefore, our study is orthogonal to the existing learning to rank work, and existing learning to rank algorithms can be used to learn a retrieval function on the basis of our improved query rep-resentation to further improve retrieval performance.
Machine learning was also introduced to improve pseudo feedback through selecting good expansion terms [5] or good feedback documents [12]. However, neither work attempted to directly optimize robustness of pseudo feedback.
Boosting is a general method for improving the accuracy of supervised learning. The basic idea of boosting is to re-peatedly construct  X  X eak learners X  by re-weighting training data and form an ensemble of weak learners so that the to-tal performance of the ensemble is  X  X oosted X . Freund and Schapire have proposed the first and most popular boosting algorithm called AdaBoost for binary classification [9]. Ex-tensions of boosting have been made to deal with the prob-lems of multi-class classification [26], ranking [8, 30, 33], etc. Our work can be viewed as a novel extension of the boosting framework to improve pseudo-relevance feedback.
Given a query q i and a document collection C ,are-trieval function F returns a ranked list of m documents d = { d 1 ,  X  X  X  ,d m } ,where d j denotes the j -th ranked doc-ument in the ranked list. In pseudo feedback, we assume the top-n documents are  X  X elevant X , and construct a feed-back model  X  t ( q i , d ,n,C ), or  X  t ( q i ) for short, for query q by exploiting those assumed  X  X elevant X  documents. Here  X  t can be any pseudo feedback method that is able to output an improved query representation, and we call it a weak or basis feedback method;  X  t ( q i ) is essentially an expanded rep-resentation of the original query q i , and we call it a weak or basis feedback model for q i . In general, the format of  X  Figure 1: Plot of each query w.r.t. different weak feedback methods, where  X   X   X  indicates that the cor-responding weak feedback improves performance. would depend on the retrieval function F ; for example, in the vector space model,  X  t ( q i ) is represented as a vector of weighted terms, while in language modeling approaches, it is represented as a word distribution. We can use  X  t ( q the new query and apply F to retrieve another ranked list of documents d .

Given a performance measure E and the relevance judg-ments set J ( q i )forquery q i , we can compute the perfor-mance scores for the original query q i and for the expanded query  X  t ( q i ), which will be denoted as E ( F ( q i ) ,J ( q E ( q i )and E (  X  t ( q i )) in the rest of the paper for conciseness. We choose the widely accepted average precision (AP) as the performance measure E in this paper, though the proposed algorithm can in principle also work with other measures.
Although many pseudo feedback methods have been shown to improve the performance of a retrieval system on average, they all share a common deficiency, i.e., the average perfor-mance gain always comes inevitably at the cost of (some-times significantly) degraded performance of some queries. it is almost always the case that for some queries, E (  X  ( q E ( q j ) . Indeed, it has been a long-standing difficult challenge to improve the robustness of pseudo feedback so that we can improve average performance without sacrificing the perfor-mance of individual queries too much.

In this paper, we propose to use a learning method to ad-dress this problem. Specifically, given a query q i , we assume there are a variety of basis feedback models  X  k ( q i )based on different feedback methods  X  = {  X  1 ,  X  X  X  , X  m } ,andour main idea is to combine a set of such basis feedback models  X  ( q i ) into a single feedback model H ( q i ) called the final or combined feedback model to reduce feedback failures: A linear combination is chosen because the final feedback model H ( q i ) should be in the same format as that of each terms, so is H ( q i ), while if  X  k ( q i ) is a language model, H ( q should also be a language model by normalizing the learned  X  k ( k =1 ,  X  X  X  ,t ) to make them sum to 1.

Our main motivation of combining multiple feedback meth-ods is that different basis feedback methods often have rela-tive strengths for different query topics and thus are comple-mentary to each other. To illustrate it, we examine 46 ba-sis feedback methods constructed using methods described in Section 5. The results are presented in Figure 1. We can see that many feedback methods indeed have relative strengths on different queries (e.g., topic 708 and 709). It seems there are only 3 very hard queries that no feedback method can help, while for other queries, there are always some successful feedback methods. Thus, constructing an ensemble feedback method H wouldbeaneffectivestrat-egy to reduce feedback loss. For example, suppose we have two weak feedback methods  X  1 and  X  2 ;  X  1 improves 0 . 2and  X  0 . 1 (i.e., decreases by 0 . 1) on q 1 and q 2 respectively, while  X  2 improves  X  0 . 1and0 . 2on q 1 and q 2 respectively. So the fbloss scores are 0 . 5forboth  X  1 and  X  2 on these two queries. However, an ensemble method  X  1  X  1 +  X  2  X  2 would probably have 0 fbloss while still achieving better or comparable av-erage improvement if the combination coefficients  X  1 and  X  are chosen appropriately.

Our goal is to minimize the number of query instances for which the retrieval performance is decreased by the final feedback model H ( q i ) as compared to the original query q The learning algorithm that we study attempts to find an H with a small number of query failures, a quality called the feedback loss and denoted by fbloss D ( H ). Formally, Here and throughout this paper, the indicator function I { is defined to be 1 if the predicate  X  holds and 0 otherwise. D ( q i )istheweightof q i , and we set it initially to uniform, i.e., D ( q i )=1 / | Q | so that all queries are equally important; later the query weights would be updated iteratively in a boosting framework [9] so that queries that do not perform well would contribute more to the loss function more. We will also show later in Section 4.1 that the feedback loss can be bounded by the degradation of retrieval precision, which means that the feedback loss can actually measure both feed-back failures and retrieval effectiveness, which is necessary in order to ensure both robustness and effectiveness.
In the following section, we will discuss how to optimize the combination of weak feedback methods so as to minimize fbloss on some training data, formally,
With only a few basis feedback methods, it is possible to optimize their combination through manual parameter tuning. However, manual tuning is infeasible if there are many basis feedback methods as is often the case. Inspired by the AdaBoost [9], we devise a boosting approach, referred to as  X  X eedbackBoost X , to solve this problem.
Ideally we want to minimize the feedback loss on the train-ing data as shown in Equation 3. However, it is difficult to solve this optimization problem because the combination is inside a retrieval function and not differentiable. To simplify this problem, we make an assumption that the feedback loss of the combined feedback H is less than or equal to that of a linear performance combination of the corresponding basis feedback methods. Formally,
Although we have not found a theoretical proof for the above inequality, we can empirically guarantee this assump-tion to be true in our algorithms since we can automatically adjust the algorithms to make sure this assumption holds (as shown in the step 5 of the pseudo code of the algorithm). In practice, this assumption turns out to work well. One possible explanation is that different feedback models often complement to each other, so the performance of a mixture model with reasonable coefficients is often better than the performance of any single model and thus also better than the weighted average performance of these single models. An example is that an appropriate interpolation of a pseudo feedback model and the original query model usually works better than either single model [31, 1, 18]. Thus we will minimize the following upper bound of the feedback loss. However the above optimization problem is still hard to han-dle, because the indicator function I is non-continuous. For-tunately, we know that I { x&lt;y } X  e y  X  x for all real x and y , so, instead of solving Equation 5 directly, we can alterna-tively minimize its upper bound below, which is essentially a measure of retrieval performance degradation.
Now we can address this problem using forward stage-wise additive modeling, which is an effective strategy to find an approximate solution to an optimization problem through sequentially adding new basis method without ad-justing those already selected methods [11]. By forward stagewise additive modeling, the above formula can be ex-pressed as follows where we would iteratively choose  X  t and  X  ,aswellasadjust D t ( q i ): where Here, Z t is a normalization factor to make D t a distribution; such a normalization operation does not affect the choosing of  X  t and  X  t ,since D t depends neither on  X  t nor  X  t in the forward stagewise additive modeling [11]. D t is defined in a recursive way, where D 1 ( q i )= D ( q i )=1 / | Q | is the initial weight for query q i .After T iterations, we can obtain the Algorithm 1 The FeedbackBoost algorithm 1: for t =1 , 2 ,  X  X  X  ,T do 2: Select basis feedback method  X  t with weighted distribution 3: Compute Eloss t (  X  t ) using Formula 9. 5: While the inequality in Formula 4 does not hold, goto step 7: end for
FeedbackBoost is designed to find a solution to the opti-mization problem in Formula 7 using a boosting approach. Specifically, like all other boosting algorithms, Feedback-Boost operates in rounds. At each round, we calculate a distribution of weights over training queries. In fact, D can be regarded as a weight that is applied to each query after t  X  1 iterations. From Equation 8, we can see that D t will put more weights on queries that are hurt more by previously selected basis feedback methods.

We next select a basis feedback method  X  t that works well on those highly-weighted queries, and the selection of this basis feedback method is to optimize an objective function that is defined to directly measure the feedback loss. Specif-ically, we define the weighted performance degradation of  X  at iteration t as And the feedback method  X  t with the minimum Eloss t will be selected. Theoretical analysis is provided as follows:
The performance measure E , e.g., AP in this paper, is usually within range [0 , 1]. Even a measure is beyond this range, we can still normalize it to [0 , 1]. Therefore, for any basis feedback method  X  t , we have the performance degra-dation E ( q i )  X  E (  X  t ( q i ))  X  [  X  1 , 1] .Bytheconvexityof e a function of x when x  X  [  X  1 , 1] [8], we thus have So Equation 7 can be approximated by We can easily minimize this equation by setting which indeed makes sense since it suggests that a  X  t with less performance degradation will receive a larger weight  X  Then, by plugging Equation 12 into 11, we obtain the fol-lowing optimal basis feedback. We can see that Equation 13 is minimized when Eloss t (  X  is close to 1 or  X  1. With respect to the former case, we would choose a  X  t with the largest weighted performance degradation, and  X  t will be negative, which, however, does not make sense: if a pseudo feedback  X  t leads to large per-formance degradation, a negative  X  t may not make  X  t work well. Therefore, Eloss t (  X  t ) should be negative to keep the coefficient  X  t positive. So we choose a basis pseudo feedback  X  with the smallest negative Eloss t so far.
To summarize, FeedbackBoost works as follows. The in-put to the algorithm includes a set of queries and relevance  X = {  X  1 ,  X  X  X  , X  m } , a document collection C , a retrieval func-tion F , a retrieval performance measure E , and the iteration number T . FeedbackBoost works in an iterative way: dur-ing each iteration t , a basis feedback method  X  t is chosen based on its performance on training data with weight dis-tribution D t , i.e., Eloss t (  X  t ). Also the coefficient  X  calculated based on Eloss t (  X  t ). After that, the query weight distribution D t is updated by increasing weights on queries for which  X  t performs poorly, leading to a new distribution D +1 ; D t +1 will be used in the next iteration to select  X  and  X  t +1 . At last, the final feedback model H is created by linearly combining all the selected basis feedback methods. A sketch of the algorithm flow is shown in Algorithm 1.
In order to apply FeedbackBoost, the main task is to de-sign appropriate basis feedback methods {  X  } . A basis feed-back method  X  k generally consists of three components: a weighting function h k to assign weights to feedback docu-ments, a weighting function g k to calculate the importance of different expansion terms, and the retrieval model F to decide the representation format of the feedback model. For-mally,  X  k = f ( h k ,g k ,F ) . Given a retrieval model F , one feed-back method differs from others often because it uses differ-ent document and/or term weighting functions [18]. We can thus naturally construct many basis feedback methods by varying the document and/or term weighting functions. As a specific application, here we discuss how we can apply FeedbackBoost to improve pseudo feedback under the lan-guage modeling framework through combining different doc-ument weighting strategies. This application is especially interesting because (1) language models deliver state of the art retrieval performance [21, 14]; (2) feedback document weighting has been shown to be a critical factor affecting ro-bustness and effectiveness of pseudo feedback methods [18]. However, our methodology could be applicable to other re-trieval models and to optimizing term weighting methods as well, which we leave as future work.

We use the KL-divergence retrieval method [14] as our retrieval model (i.e., F ), which scores a document d with re-spect to a query q by computing the negative KL divergence between the query and the document language model: Two important instantiations of basis pseudo feedback meth-ods in language models are the relevance model [15] and the mixture model [31], which are among the most effective and robust feedback techniques based on language models [18].
The relevance model  X  r essentially uses the query likeli-hood as the weight for document d and takes an average of the probability of word w given by each document lan-guage model. Formally, let  X  represent the set of smoothed document models in the pseudo feedback collection  X  = { d 1 ,  X  X  X  ,d n } . The formula of the relevance model is:
Let X  X  denote the original query model as P ( w | q ). The relevance model P ( w |  X  r ( q )) can be interpolated with the original query model P ( w | q ) to improve performance using a interpolation coefficient  X  . In this paper, we will use the fol-lowing interpolated model P ( w |  X  q ) as the new query model, which is often called RM3 [1]:
In Equation 16, h r ( d )= w  X  q P ( w |  X  d ) is the query likeli-hood score of document d , serving for document weighting, and g r ( w,d )= P ( w |  X  d ) works for term weighting. If we in-stantiate the document weighting strategy in a different way, e.g., h r , whereas the term weighting strategy is fixed to g it will lead to a different  X  X elevance model X   X  r . Formally which can also be used for feedback after a similar interpola-tion. Following this way, we can construct a set of relevance-model style basis feedback methods by varying their docu-ment weighting strategies.
In the simple two-component mixture model (SMM)  X  m , the words in  X  are assumed to be drawn from two mod-els: (1) background model P ( w | C ) and (2) topic model P ( w |  X  m ( q )). Thus the log-likelihood for the entire set of feedback documents is: where V is the word vocabulary, c ( w,  X ) is the count of word w in feedback document set  X , and  X   X  [0 , 1] is the mixture parameter. The estimate of  X  m ( q ) can be computed using the EM algorithm to maximize the log-likelihood. Finally,  X  m ( q ) is also interpolated with the original query model P ( w | q ) to update the query model with a coefficient  X  .We notice in Formula 19 that It means that the document length | d | is used as a weight of document d (i.e., h m ( d )= | d | )tosumovertheterm evidence from each feedback document. As in the case of the relevance model, we can also use any other document weighting method h m to replace h m while keeping g m the same, which will lead to a new family of mixture-model style basis feedback method  X  m .
We next introduce a set of document weighting strategies { h t } . Aswehavediscussed,eachofthemcanbeplugged into the relevance model (Section 5.1.1) or the mixture model (Section 5.1.2), leading to a new basis feedback method  X 
Relevance Score: Relevance score is shown to be a criti-cal factor for feedback document weighting in a recent work [18]. We explore the use of query likelihood [21] h 1 and BM25 score [23] h 2 for document weighting: w.r.t. the for-mer, the Dirichlet smoothing method [32] with  X  =1 , 000 is used to smooth the document language model; w.r.t. the latter, we fix k 1 =1 . 2, b =0 . 5, and k 3=1 , 000.
Document Novelty: We estimate a novelty score for each document to reward novel information. Three different methods are proposed: (1) The distance between the cen-where  X  X osim X  stands for cosine similarity; (2) The distance between the centroid of all feedback documents ranked be-distance between the most similar document ranked before
Query Term Proximity: Query term proximity has been largely ignored in traditional retrieval models [23, 21]. We use the recently proposed positional language model [19] and the minimum pair distance [29] to capture term proxim-ity for improving document weighting: (1) we compute a po-sitional query likelihood score based on the  X  X est-matching X  c ( w, q ) is the count of w in q , and we follow work [17] to estimate the positional language model P ( w | d, j ) ;(2)weuse the normalized minimum pair-wise distance proposed in [29] by setting  X  = 1 which prevents negative document weights,
Document Length: Though the heuristic of document length normalization has been incorporated into the rele-vance scores [23, 32], we still list it here because it was explicitly used in some existing pseudo feedback methods [18]: (1) raw document length: h 8 ( d )= | d | ; (2) reciprocal of the raw document length: h 9 ( d )=1 /h 8 ( d ) ;(3)Dirich-let document length: h 10 ( d )= | d | / ( | d | +  X  ) ,whereweset  X  =1 , 000; (4) reciprocal of the Dirichlet document length:
Besides the above basic document weighting methods, for each h t , we also introduce some of their variations as our document weighting, including exp( h t ), ( h t ) 2 ,and we include log( h 2 )andlog( h 8 ), since the values of h are usually larger than 1 . 0 for top-ranked documents. Over-all, there are 46 methods in total, all of which are normalized to sum to 1 . 0 for each query.
We next apply the FeedbackBoost algorithm to learn an ensemble feedback method. We denote E d (  X  ( q )) as the score of document d with respect to a basis feedback method  X  on query q . In fact, with the KL-divergence retrieval method (Equation 15), we only need to retrieve and score documents once for each basis feedback method. Then during the train-ing process, if we need to score a document d using any combined feedback H = t k =1  X  k  X  k ,wecandoitefficiently by linearly combining the scores of the corresponding basis feedback methods.
 So training FeedbackBoost would be as efficient as training general AdaBoost algorithm [9].

Besides, during the testing phase, H ( q ) can also be esti-mated efficiently. For example, for the relevance model, we can plug the ensemble document weighting method used in H ( q ) into Formula 18 to replace h r ( d ) and estimate H ( q ) directly; for the mixture model style H ( q ), we can also re-place | d | with the combined document weighting methods in Formula 20, which does not affect feedback performance in the experiments. Hence, the efficiency of H ( q ) for pseudo feedback would be comparable to that of basis feedback al-gorithms, which is also confirmed empirically.
We evaluate our method using the Terabyte Web dataset and a large news dataset Robust04. Only title portions of the topics are taken as queries. For each dataset, we split the available topics into training, validate and test sets, where the training set is used solely for training the algorithms, the validate set is used for tuning the number of iterations T , and the test set is used for evaluation purposes. Table 1 shows some document set statistics. The preprocessing of the collections includes stemming using the Porter algorithm and stopword removal using a standard InQuery stoplist.
We train two FeedbackBoost models: in the first one, each basis feedback method implements a different docu-ment weighting strategy proposed in Section 5.2, while all of them use the same mixture-model style term weighting, and thus we refer to it as BoostMM ; in the second one, each basis feedback method also implements a different docu-ment weighting strategy but all of them share the relevance-model style term weighting, thus the name BoostRM .That is, we parameterize  X  in different ways for BoostRM and BoostMM. This design allows us to meaningfully compare BoostMM and BoostRM with the corresponding two base-line feedback methods (i.e., SMM and RM3) and the basic query language model without feedback (labeled as X  X oFB X ). This set of experiments are mainly to compare Feedback-Boost with traditional pseudo feedback methods. A main hypothesis we would like to test is whether BoostMM and BoostRM are indeed more robust than the corresponding baseline SMM and RM3.

Furthermore, we also compare FeedbackBoost with two other lines of baseline methods. In the first line, we compare FeedbackBoost with another strong baseline representing a recent work on improving robustness of pseudo feedback, i.e., the REXP-FB method [7]. In the second line, we are in-terested in knowing if an existing learning to rank approach can also improve both robustness and effectiveness as much as FeedbackBoost does. So we introduce yet another base-line AdaRank [30], which performed well [16]. AdaRank attempts to learn a ranking function through directly opti-mizing retrieval measures. One variation of AdaRank used in our comparison is AdaRank.MAP , which tries to op-timize MAP. Since it is non-trivial to directly optimize the proposed fbloss using AdaRank, we further extend AdaRank to optimize a loss function that is similar to fbloss: a novel robustness-related measure E that measures the performance According to the Formula 6 in [30], AdaRank turns out to minimize an exponential loss function | Q | i =1 exp  X  leading to a new run AdaRank.FB . Note that AdaRank.FB is no longer the traditional AdaRank algorithm because the loss function is novel, thus it is a very strong baseline.
We use the Dirichlet smoothing method [32] for all docu-ment language models, where we set the  X  =1 , 000. Besides, as suggested in our previous study [18], we set the mixture noise parameter  X  to 0 . 9 for SMM and all the mixture-model style basis feedback methods. We also set the number of ex-pansion terms to 40. These parameter settings work well and are used in our experiments unless otherwise stated.
It does not make sense to talk about fbloss alone, since we can always get 0 loss for any feedback method by setting the feedback coefficient  X  to 0. In the traditional evaluation strategy [31, 18], people often tune  X  to optimize one re-trieval precision measure. We thus follow such a strategy to first optimize  X  in terms of MAP, on the basis of which we then try to reduce fbloss. Specifically, we give a priority to SMM and RM3 to optimize their feedback coefficient  X  on the training, validate and test sets respectively. However, for all the mixture-model style basis feedback methods, we simply set the feedback coefficients to those optimized for SMM, while for all the relevance-model style basis feedback methods, we use RM3 X  X  setting directly.

We are interested in both effectiveness and robustness of pseudo feedback methods, so besides MAP (on top-ranked 1 , 000 documents) and Pr@20, we also compare all runs in robustness-related loss function.
 terms of two robustness measures, the robustness index (RI) and the accumulative loss of retrieval performance (APloss). RI = 1  X  2  X  fbloss , is essentially a transformation of the fbloss proposed in our paper; we show RI instead of fbloss mainly because RI was often used in previous studies, e.g., [7]. APloss is to measure the feedback loss in a finer de-gree, which is the accumulative AP degradation in failure cases (since AP is used as E in our paper), defined as: Table 2 compares MAP, Pr@20, RI, and APloss for NoFB, SMM, RMM, BoostMM, and BoostRM on the validate and test sets. Note that each time all runs use the same set of feedback documents to make the comparison fair; that is, we fix the base ranking for all runs. Besides, the itera-tion number T in FeedbackBoost is chosen to minimize the corresponding fbloss on the validate sets. We also vary the number of feedback documents from 20 to 50.

For all cases, we can see that BoostMM and BoostRM significantly improve the robustness over SMM and RM3 respectively. For example, BoostMM reduces APloss as com-pared with SMM by amounts ranging from 50 . 9% to 77 . 8% when using 20 feedback documents and from 65 . 1% to 79 . 5% when using 50 feedback documents. Moreover, BoostMM and BoostRM also significantly improve MAP over SMM and RM3 in almost all cases. The results demonstrate that the FeedbackBoost algorithm does a good job to improve robustness while still achieving better effectiveness. More-over, FeedbackBoost works consistently well when we use different number of feedback documents. The performance of FeedbackBoost shows that it is effective to combine mul-tiple feedback methods using our FeedbackBoost to improve both robustness and effectiveness of pseudo feedback. We next compare FeedbackBoost with AdaRank.MAP and AdaRank.FB. These three algorithms are all trained on the same set of relevance-model style basis feedback methods. The iteration number for all the algorithms are chosen to minimize fbloss on the validate set. We present the experi-ment results in Table 3. One interesting observation is that AdaRank.FB is more effective than AdaRank.MAP, suggest-ing that the proposed robustness-related measure is better than MAP as an objective function to improve pseudo feed-back: one possible explanation is that the average precision does not work well to indicate the room for improvement of a query, so focusing on queries with lower average precision may not be a good strategy to fully exploit the potential of different queries; however, our new measure would be able to capture more precisely the potential room of a query through comparing its performance with the baseline perfor-mance. Moreover, we also see that FeedbackBoost is clearly better than AdaRank.FB, though they use similar objective functions, sugggesting that our optimization framework is more effective for pseudo feedback.

We finally compare FeedbackBoost with a state-of-the-art feedback method, REXP-FB [7 ], which attempted to reduce failures of pseudo feedback but was only able to optimize an indirect objective function. They also used Terabyte and Robust04 as their test collections. So we used a 3-fold (701-750, 751-800, and 801-850) and a 2-fold (301-450 and 601-700) cross validation method to evaluate FeedbackBoost on the whole Terabyte and Robust04 topics respectively in or-der to compare with their reported numbers. In this com-parison, we use the same collection and parameter settings as were used in [7]. The comparison is reported in Table 4.
There are clear performance improvements of our baseline runs (NoFB-2) over theirs (NoFB-1), probably because we use a latest version of Indri search engine (2.10) 1 . We still can see that, in terms of relative improvements, BoostMM performs similarly to REXP-FB, although REXP-FB used a lot of constraints to improve the selection of expansion terms while we only use the default term weighting. However, the most interesting observation is from the comparison of RI, which may be more comparable across systems. We can see that our algorithms achieve a significantly higher RI in all cases, even though our baseline run is even harder to beat. This finding confirms the conclusion in [18] that doc-ument weighting plays a key role in affecting the robustness of feedback. Furthermore, it suggests that our way of di-rectly optimizing robustness indeed works more effectively.
To examine in details how badly queries are hurt by a pseudo feedback algorithm, we show in Figure 2 the robust-ness histograms by combining two test sets (query 651-701 and query 801-850). The x-axis represents the individual http://www.lemurproject.org/ Figure 2: Histograms that compare robustness of SMM vs BoostMM (first and third) and RM3 vs BoostRM (second and fourth) on the combination of two test sets. 20 ( 50 ) feedback documents are used in the left (right) two histograms respectively. Figure 5: Sensitivity of BoostMM and BoostRM to the iteration number on Robust04 validate set. the corresponding feedback method) for individual failure queries, and the y-axis stands for the number of queries with the corresponding percentage APloss. We can see that for the worst cases, where a query X  X  AP is decreased by more than 40%, both BoostMM and BoostRM perform much bet-ter than SMM and RM3 respectively. It suggests that the proposed FeedbackBoost algorithm indeed concentrates more on difficult queries that are hurt seriously by baseline feed-back methods, i.e., SMM and RM3, and the significant re-duction of APloss and improvement of RI shown in Table 2 could be mainly due to the elimination of those worst cases.
Usually there is a tradeoff between robustness and effec-tiveness: if we use a smaller feedback coefficient  X  ,there would be fewer feedback failures, but we may not fully im-prove the effectiveness; if we increase this  X  to some ap-propriate value, the overall retrieval precision could be opti-mized, which, however, may lead to more feedback failures. Although we have shown that FeedbackBoost improves both robustness and effectiveness at the same time, we are still interested in how the performance of FeedbackBoost inter-acts with  X  (where  X  in FeedbackBoost is used to control the feedback interpolation of each basis feedback method in-volved.) We thus draw the sensitivity curves for Feedback-Boost in terms of the percentage MAP degradation and the overall MAP improvement in Figure 3 and 4 respectively. The percentage MAP degradation is essentially the relative use 50 feedback documents in all curves. The curves clearly show that FeedbackBoost consistently improves the robust-ness and effectiveness over two baseline algorithms. Addi-tionally, our algorithm is also less sensitive to  X  in both curves, and setting  X  around 0 . 8 often leads to a large im-provement in precision with only a small amount of failures.
Finally, we show in Figure 5 the curves of performance changes as we increase the iteration number T .Weseethat the APloss decreases steadily and quickly as the training goes on, until it reaches its plateau. In our experiments, we can usually find the best parameter T within 100 rounds.
In this paper, we propose a novel learning algorithm, Feed-backBoost, based on the boosting framework to improve pseudo feedback. A major contribution of our work is to op-timize pseudo feedback based on a novel loss function that directly measures both robustness and effectiveness ,which has not been achieved in any previous work.
 The experiment results show that the proposed Feedback-Boost algorithm can improve average precision effectively and meanwhile reduce the number and magnitude of feed-back failures dramatically as compared to two representa-tive pseudo feedback methods based on language models, the mixture model and the relevance model. We also com-pare our algorithm with a recently proposed robust feedback method, and the results show that our method is more ro-bust. In addition, we compare FeedbackBoost with a well-performing learning to rank approach applied for pseudo feedback and observe that FeedbackBoost works clearly bet-ter. These results show that the proposed FeedbackBoost is more effective and robust than any of the existing method for pseudo feedback, including both traditional pseudo feed-back methods and new learning-based approaches.

Our work can be extended in several ways. First, in our current work, we only use basis feedback methods with dif-ferent document weighting strategies, so a straightforward future work is to also introduce term weighting methods to construct a larger set of basis feedback methods. Second, we are also interested in combining even more recently proposed pseudo feedback algorithms, e.g., [2, 28, 7, 20], and other families of feedback methods, e.g., [24, 22], to diversify our basis feedback methods. Third, personalized search is an-other scenario which shares similar effectiveness-robustness tradeoff issues; thus it is also interesting to improve person-alized search by exploring the FeedbackBoost framework.
We thank the anonymous reviewers for their useful com-ments. This material is based upon work supported by the National Science Foundation under Grant Numbers IIS-0713581 and CNS-1028381. The first author is also sup-ported by a Yahoo! Key Scientific Challenge Award.
