 The automatic assessment of free-text responses of students is a relatively newer task in both computational linguistics and educa-tional technology. The goal of the task is to produce an assessment of student answers to explanation and definition questions typically asked in problems seen in practice exercises or tests. Unlike some conventional methods which assess the student responses based on only information about their corresponding questions, this paper exploits idea of collaborative filtering to analyze student responses and used an effective collaborative filtering model  X  feature-based matrix factorization model to deal with this challenge. The ex-perimental results show that our feature-based matrix factorization model outperforms the baseline models and the model with a re-ranking phase can achieve a better and competitive performance  X  63.6% overall accuracy on the Beetle dataset.
 I.2.7 [ Computing Methodologies ]: Artificial Intelligence X  Natu-ral Language Processing Algorithms, Experimentation Assessment of student response, collaborative filtering, feature-based matrix factorization
As the Internet technology develops at a staggering rate, an in-creasing number of tests such as TOEFL-IBT are taken online in-This author is the corresponding author.
 stead of in the traditional paper-based pattern. Compared with paper-based test (PBT), computer-based test (CBT) and Internet-based test (IBT) can save much labor of human raters and resources. Although computers are capable of assessing answers of students to some types of questions like multiple choice questions, human raters are still indispensable at present because it is a challenge for computers to accurately assess free-text responses of students to some questions such as English writing tasks. Also, current e-learning systems have limited capability for giving students feed-back and providing automatic assessment since there is no estab-lished technology for assessing natural language responses to ques-tions. Therefore, automatic assessment of student answers is wor-thy of investigation.

The task of automatic assessment of student responses proposed in semeval-2013 [1] tries to deal with the challenge of automatic assessment of student responses. The goal of the task is to grade student answers for enabling well-targeted and flexible feedback in a tutorial dialogue setting.

Previous work on student answer assessment for intelligent tu-toring systems used LSA [2], classifiers based on "bag-of-words" features [3] to determine if a student answer corresponds to one of the expected correct or incorrect answers anticipated by a system designer. More recently, [4, 5] formulated the problem of assessing student input in terms of recognizing textual entailment.
However, previous methods for this task predict the grading level of a given response based on only information about its correspond-ing question such as reference answers and grading level of re-sponses to this question. In other words, they do not take into con-sideration the grading information about responses to other ques-tions. Unlike these conventional approaches, this paper exploits the idea of collaborative filtering to analyze the student responses, which predicts the grading level of a student response based on both the grading records of its corresponding question and the global in-formation of gradings across the dataset. It is not difficult to un-derstand the fact that the global grading information is useful for accurately assessing the responses to a specific question because this information can tell us which grading level the most responses get and what kind of responses tend to get high or poor grades. For instance, a response whose text is  X  X  don X  X  know X  is always a poor response to whatever questions and this fact can help accurately predict such responses to a unseen question in the test set. Further-more, in a recommendation perspective, if two users have similar shopping records, then they may have the similar preferences for items; likewise, some questions may have similar  X  X reference X  for semantic information of student responses. Assuming that good responses to given two questions always share many features, if a response to one of the questions is similar to a good response to the other question, then it is likely to be a good response to its corre-sponding question. Based on this intuition, we consider this task as a rating prediction problem where we try to predict the  X  X ating X  of questions (users) to student responses (items) by using a popular collaborative filtering model  X  feature-based matrix factorization model.
Since it is a relatively newer task in both knowledge manage-ment and educational technology community, we briefly describe the task of automatic assessment of student responses. The goal of this task is to assess student answers to exercise questions that can be useful in tutorial dialogue and/or e-learning systems. Specifi-cally, given a question, a known correct  X  X eference answer X , a set of student answers with manually annotated grading levels and a 1-or 2-sentence student answer, the goal is to determine the student X  X  answer accuracy.

The task can be performed at different levels of granularity. In this paper, we mainly address the 5-way task, where the system is required to classify the student answer according to one of the following judgments:
In this paper, we used the Beetle dataset [6] for training and eval-uation, which is a set of transcripts of students interacting with an intelligent tutorial dialogue system for teaching conceptual knowl-edge in the basic electricity and electronics domain.
In this section, we discuss how to use collaborative filtering tech-niques to deal with the challenge. We first explain why collabora-tive filtering techniques can be exploited for this task in Section 3.1. Then, a popular collaborative filtering model  X  feature-based ma-trix factorization model is to be discussed in detail in Section 3.2. Finally, Section 3.3 presents a re-ranking method for re-ranking the marginal predictions.
Collaborative filtering is one of the most promising technologies for recommender systems. In the newer, narrower sense, collabo-rative filtering is a method of making automatic predictions (filter-ing) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying as-sumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B X  X  opinion on a different issue x than to have the opinion on x of a person chosen randomly. This intuition is very popular in the online shopping recommendation.

Likewise, the task of automatic assessment of student responses can also be addressed using the idea of collaborative filtering. For a given question, different student responses may be graded with different grading levels just as different items might be rated with different ratings by a given user under the online shopping scenario.
For a specific example, there are two questions whose ids are q1 and q2 respectively. Question of q1 is  X  X hy are wires made of copper? X  and question of q2 is  X  X an silver conduct? If so, why can we hardly see wires made of silver? X . By analyzing the student responses, we can find responses involving the issue of conductiv-ity and price to both q1 and q2 are graded with a high grade such as Correct . In contrast, responses which do not involve these two aspects are graded with a poor grade like Irrelevant for the two questions. Based on the facts mentioned above, assuming that r1 and r2 are a response to q1 and q2 respectively and they are very similar (e.g. similar unigram features), if r1 is a good response to q1, then a response r2 which is similar to r1 is very likely to be a good response to q2 and vice-versa because q1 and q2 have the similar  X  X references X  for the semantic information concerning  X  X onductivity X  and  X  X rice X , which is exactly the idea of collabora-tive filtering.

Also, there are some responses such as  X  X  don X  X  know X  which are always graded with a poor grade for whatever questions. For a col-laborative filtering model, a negative bias will be assigned to such responses. As a result, even for a new question without any prior in-formation or reference answers, such responses will be graded with a poor grading level by the collaborative filtering model, which can hardly be handled by other previous models.

For the above-mentioned reasons, we used a typical and effective collaborative filtering model  X  feature-based matrix factorization model to address the challenge. The feature-based matrix factor-ization model can predict the grade of a response based on both the grading records of its corresponding question and the global infor-mation of gradings across the dataset. Furthermore, this model can capture semantic information with a latent factor space, which also contributes to a good performance.
One of collaborative filtering models is matrix factorization mod-els [7]. Matrix factorization models transform both items and users to the same latent factor space which tries to explain ratings by characterizing both products and users on factors automatically in-ferred from user feedback, as shown in Figure 1.

Previous research has shown that the matrix factorization model can model recommendation problems well. However, it is not dif-ficult to find that the task of predicting rating a user rates to an item is very similar to the task of assessing student responses given a question. W e consider the assessing task as a rating prediction problem. Specifically, we regard questions as users, responses as well as ref-erence answers as items and grading levels as ratings. For the 5-way task, the grading levels Correct , Partially_correct_incomplete , Contradictory , Irrelevant and Non_domain are mapped to the rat-ing space R which is defined as follows:
In our task, each row of the rating matrix R in Figure 1 represents a question, each column represents a response and each element of the matrix is the grade level of a response(column) to a ques-tion(row). For the factor matrice P and Q , the dimension f latent semantic dimension. For the example mentioned by Section 3.1, f 1 might represent the semantic information about conductiv-ity and f 2 might represent the information about price. If the value of f 1 of a question is large, then it means that the desirable answer to this question should contain sufficient information about conduc-tivity. On the other hand, if the value of f 1 of a response is large, then it means that the response provides much information about conductivity. In this way, some complex information can be cap-tured by the latent factor spaces and also  X  X reference X  of a question for a response is modeled.

However, basic matrix factorization models such as basic SVD cannot address this challenge well because these models do not use any features about questions and responses except their IDs. Since each response corresponds to only one question, features are ex-tremely sparse and responses to different questions do not have any connection even if their text is very similar. For example, assuming that there are two responses to two different questions and text of these two responses is the same, e.g. their text is  X  X  don X  X  know X , but the model cannot be aware of that the responses are the similar because their features i.e. their IDs are totally different and they are never  X  X ated X  by the same questions. Likewise, similar ques-tions also cannot be identified by this model. As a result, the basic matrix factorization model cannot work at all.

For solving the problem mentioned above, we have to introduce more features for profiling questions and answers in order to make features less sparse. For leveraging more features of questions and responses, feature-based matrix factorization model which was proposed by [8] is used. This model generalizes the basic factor-ization models, in which new types of information can be utilized by simply defining new features. The framework of feature-based matrix factorization is shown by Equation 1 where is a constant indicating global mean value of rating, b ( g ) , b ( u ) of global features, user features and item features respectively, p and q represent factors of features of users and items respectively and , and are weights of user features, item features and global features respectively. ^ r = + )
To model profiles of questions and responses with features, we select questionid as the feature of questions and bag-of-words of responses as the response features. The reason why we do not se-lect text of questions as features is that the question text is very confusing. For example, text of many questions is a word such as  X  X hy X . The specific feature-based matrix factorization model for our task is shown in Equation 2, in which S ( i ) is the set of features of the response i .
Feature-based matrix factorization is naturally a regression model so its prediction of each test example is a numeric instead of a nom-inal class. Although we can use rounding-off method to remap the numeric to class label, it is not effective for some cases. For a test example predicted with a marginal score e.g. 4.5, it is difficult to tell whether it is should be graded as Correct or Partially_correct for the matrix factorization model. Therefore, we used a maximum entropy classifier to re-rank such marginal predictions for a better performance.
 We select bag-of-word features and train the MaxEnt classifier. For the test example t 2 T , MaxEnt classifier serves re-predicting their classes. T is defined as follows and pred ( t ) is the predicting score by the matrix factorization model.

In this section, we first introduce the experimental settings in detail. Then we discuss the results and give an analysis. Dataset The dataset we used for evaluation is the Beetle Dataset, which contains 47 questions. Each question is associated with 1 to 10 different reference answers provided by experienced tutors and dozens of student responses.
 Pre-processing In the pre-processing step, we perform lemmatiza-tion for each token and filter out stop words.
 Evaluation Since the semeval-2013 organizer has not released the test set with golden standard, we alternatively perform cross vali-dation to evaluate the performance of our model. To simulate the scenario of unseen answers [1], we randomly divided the student responses into 20 groups and perform 20-fold cross validation. The test set in each fold contains 197 or 198 test examples of which there are on average 4.2 student responses to each question.
We set the following models which only use lexical features as baselines for evaluations: Baseline1: Majority Class The majority class baseline is a model which assigns Correct (the most frequent class) to each test in-stance.
 Baseline2: Lexical Similarity The lexical similarity baseline is a simple decision tree classifier with features such as lexical similar-ity and overlap by using an implementation toolkit  X  Weka [9]. Baseline3: MaxEnt Classifier This baseline is a maximum en-tropy classifier using bag-of-word features. The classifier also serves re-ranking the marginal predictions, as discussed in Section 3.3.
Note that Baseline1 and Baseline2 are baselines officially used in the semeval-2013 competition. We compare the performance of following models with the baselines: Model1: Feature-based matrix factorization model which predicts grading levels of student responses. We used the rounding-off method to map the numeric prediction to one of the five given classes. Model2: Feature-based matrix factorization model with re-ranking. The classifier for re-ranking is the model described by Baseline3.
Table 1 shows the performance of different models for the task of student response analysis. For saving space, we use integers to represent the grade levels in Table 1 in which  X  X acro X  and  X  X icro X  mean macro-average and micro-average respectively and  X  X verall X  represents the overall accuracy of models.

The majority baseline achieves 42.3% overall accuracy. How-ever, this is obviously at the expense of serious errors. For instance,
Grade B1 B2 B3 M1 M2 P R F P R F P R F P R F P R F o verall 0.423 0.549 0.611 0.613 0.636 such a system would tell the students that they are correct even if they are saying something contradictory. This is reflected in a much lower macro-averaged F score. Compared with the majority base-line, Baseline2 and Baseline3 can assess student responses more accurately and achieve the overall accuracy 54.9% and 61.1% re-spectively but their abilities to capture semantic of responses seem not so good as the feature-based matrix factorization model which tries to represent the semantic information in a latent factor space and achieved a performance of 61.3% overall accuracy.

Furthermore, it is clear that combining a re-ranking phase can improve the performance of feature-based matrix factorization mod-els for the reason that errors due to marginal predictions made by the matrix factorization model are corrected. The matrix factor-ization model with re-ranking can achieve 63.6% overall accuracy, which is a competitive performance for a model which only ex-ploits the bag-of-word features without too much pre-processing such as spelling correction for the task.

It is also notable that responses which are Correct or Non_domain can be identified most easily by all models except the majority baseline since the responses of these two grades have more dis-tinct features than ones of other grades. In contrast, it seems quite difficult for models to identify irrelevant responses since such re-sponses may contain some important words mentioned in either reference answers or good responses. As a result, they are likely to be graded with a high grade. Therefore, identifying such responses requires deeper semantic analysis. In addition, it can be found that the matrix factorization model is very awkward in identifying the irrelevant responses though it is good at dealing with other levels of responses. In contrast, the maximum entropy classifier has a more stable performance, which is reflected in a higher macro-average F score. Therefore, when the matrix factorization model is combined with the MaxEnt classifier, its weakness in handling the irrelevant responses can be addressed to some extent and that is probably one of reasons why combining a MaxEnt classifier helps improve the performance of the matrix factorization model.
This paper addresses the task of automatic assessment of student responses in a novel perspective. We model the problem as a rat-ing prediction problem and used a feature-based matrix factoriza-tion model to predict the grading levels of responses to their corre-sponding questions based on the idea of collaborative filtering. The experiments show that the feature-based matrix factorization model can deal with the assessment task well. Furthermore, the feature-based matrix factorization model combined with a re-ranking phase achieves a higher performance  X  63.6% overall accuracy  X  a com-petitive performance for a model only using bag-of-word features for this task, which proves the effectiveness of our model. Ad-ditionally, our model is so flexible that we can also exploit more promising features such as n-gram features, categories of questions, syntactic features in this model for better performance, which is to be explored in future work.
 This paper is supported by NSFC Project 61075067 and National Key Technology R&amp;D Program (No: 2011BAH10B04-03). [1] Myroslava O Dzikovska, Rodney D Nielsen, and Chris Brew. [2] Art Graesser, Phanni Penumatsa, Matthew Ventura, Zhiqiang [3] Pamela W Jordan, Maxim Makatchev, and Kurt VanLehn.
 [4] R Nielsen, Wayne Ward, James H Martin, and Martha Palmer. [5] Rodney D Nielsen, Wayne Ward, and James H Martin.
 [6] Myroslava O Dzikovska, Diana Bental, Johanna D Moore, [7] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix [8] Tianqi Chen, Zhao Zheng, Qiuxia Lu, Weinan Zhang, and [9] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
