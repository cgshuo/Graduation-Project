 REGULAR PAPER Li Wei  X  Eamonn Keogh  X  Helga Van Herle  X  Agenor Mafra-Neto  X  Russell J. Abbott Abstract In this paper, we define time series query filtering , the problem of mon-itoring the streaming time series for a set of predefined patterns. This problem is of great practical importance given the massive volume of streaming time se-ries available through sensors, medical patient records, financial indices and space telemetry. Since the data may arrive at a high rate and the number of predefined patterns can be relatively large, it may be impossible for the comparison algorithm to keep up. We propose a novel technique that exploits the commonality among the predefined patterns to allow monitoring at higher bandwidths, while maintain-ing a guarantee of no false dismissals. Our approach is based on the widely used envelope-based lower-bounding technique. As we will demonstrate on extensive experiments in diverse domains, our approach achieves tremendous improvements in performance in the offline case, and significant improvements in the fastest pos-sible arrival rate of the data stream that can be processed with guaranteed no false dismissals. As a further demonstration of the utility of our approach, we demon-strate that it can make semisupervised learning of time series classifiers tractable. Keywords Time series  X  Data mining  X  Streams  X  Monitoring  X  Semisupervised learning 1 Introduction With the increasingly massive volume of time series available through sensors, medical patient records, financial indices and space telemetry, the need to monitor streaming time series for a set of predefined patterns has increased correspond-ingly. For more concrete motivation, we will briefly consider application exam-ples in several domains in which we have direct experience, and have conducted experiments. 1.1 Electrocardiogram monitoring Cardiologists often encounter new interesting ECG patterns. These patterns may be unannotated and unexplained, or explicitly/implicitly annotated, for example, the pattern seems to show up older patients that were given the drug Terbutaline [ 4 ], or the pattern shows up when the Holter electrodes have gotten wet. In ei-ther case, once a cardiologist has seen an interesting pattern, she will attempt to remember it so that future encounters with similar patterns can benefit from her experience. In our framework, all new interesting patterns are simply saved in the cardiologists  X  X rofile X  and any future occurrences of similar patterns will be automatically flagged. 1.2 Audio sensor monitoring The damage done by agricultural insect pests costs more than US$ 300 billion populations and target harmful species before they can become a major prob-lem. Monitoring insect pests in the field is usually done manually, and is there-fore limited by the amount of time and expertise available from farm workers. Technological advances and falling prices for hardware have created an explo-sion of interest in continuous, real-time monitoring of critical pest data by auto-mated ( X  X mart X ) traps in recent years [ 22 ]. While it has been shown in the lab [ 20 ], these successes are hard to reproduce in the field because field stations typi-cally have low-powered CPUs and greater variety of possible insects (i.e. patterns) encountered. 1.3 Space telemetry monitoring Engineering discipline specialists must make the critical go/no-go decision mo-ments before the launch of every unmanned space vehicle launched by the DoD [ 18 ]. The cost of a false positive, allowing a launch in spite of a fault, or a false negative, stopping a potentially successful launch, can be measured in the tens of millions of dollars, not including the cost in morale and other more intangi-ble detriments to the U.S. defense program. The technician making the go/no-go decision has access to data from previous launches and must constantly monitor streaming telemetry from the current mission. Ideally, they would like to be able to encode their (often vast) previous experiences by creating a query filter that alerts them if a time series pattern occurs which is similar to one from a library of patterns from previous problematic launches.
 ied) time series indexing problem [ 13 , 19 ]. It is however a very close analogue to the problem of Query Filtering for discrete valued data (e.g. XML) [ 6 ]. As noted in [ 6 ],  X  filtering is the inverse problem of querying a database: In a traditional database system, a large set of data is stored persistently. Queries, coming one at a time, search the data for results. In a filtering system, a large set of queries is persistently stored. (new data), coming one at a time, drive the matching of the queries.  X  While the need for filtering is well established in discrete domains (XML, Bioinformatics, etc.), to the best of our knowledge it has not been ad-dressed for time series before.
 intensive care monitoring and insect population monitoring. With continuously arriving data and large number of patterns representing the interests of the user, it may be impossible for the comparison algorithms to keep up. However, in real world systems, there is likely to be significant commonality among the predefined patterns. Based on this (empirically validated) assumption, we propose a hierar-chical wedge-based comparison approach, which merges large number of patterns into a small set of wedges (with similar patterns being merged together) and then compares this set of wedges against the subsequence in the coming data stream. The experimental results show that our approach provides tremendous improve-ments in performance. Furthermore, we will demonstrate that it can make semisu-pervised learning of time series classifiers tractable. 1.3.1 Related work To the best of our knowledge, this problem has not been addressed in the liter-ature before. The most similar work is by Gao and Wang [ 7 ]. Here the authors consider the problem of continuously finding the nearest neighbor to a streaming time series. They assume that the database of predefined patterns is in secondary memory. Thus, the problem in question is disk-bound. In contrast, our problem is CPU-bound. We can easily fit our relativity small database of predefined pat-terns in main memory (indeed, in the insect monitoring problem on which we are field testing our algorithm, there is no secondary storage on the sensors). Our problem is to allow the fastest possible arrival rates for new data, while guaran-teeing that no qualifying sequences are missed. Furthermore, in Gao and Wang X  X  problem definition, there is always some nearest neighbor to any streaming query. In contrast, we are only interested in finding when a streaming query is within r of anything in our database and we generally expect this to very rarely be the case.
 with errors and don X  X  cares [ 5 ]. This problem is defined in [ 6 ]as X  preprocess(ing) a text or collection of strings, so that given a query string p, all matches of p with the text can be reported quickly . X  The crucial difference is that this problem deals with discrete data, and researchers are therefore able to tackle it with an arsenal of tools that are defined only for discrete data, such as suffix trees and lexicographic sorting.
 material. We introduce our algorithms and representations in Sect. 3. Section 4 sees a comprehensive empirical evaluation and we offer some conclusions in Sect. 5. 2 Background material In order to frame our contribution in the proper context, we begin with a review of the necessary background material. 2.1 Notation and definitions We begin with a definition of our data type of interest, time series.
 Definition 1 Time series: A time series T = t 1 ,..., t m is an ordered set of m real-valued variables.
 rather, data miners confine their interest to subsections of the time series, called subsequences.
 Definition 2 Subsequence: Given a time series T of length m, a subsequence C p ofTisasamplingoflength w&lt; m of contiguous positions from T, that is, C = t ,..., t tracted from a streaming time series, or a long batch sequence, and then compared to one or more target time series. This extraction is achieved by use of a sliding window.
 Definition 3 Sliding window: Given a time series T of length m, and a user-defined subsequence length of w, all possible subsequences can be extracted by  X  X liding a window X  across T and extracting subsequence C p .
 Definition 4 Euclidean distance: Given two time series (or time series subse-quences) both of length n, the Euclidean distance between them is the square root of the sum of the squared differences between each pair of corresponding data points: D ( Q , C )  X  n i = 1 ( q i  X  c i ) 2 . Figure 1 gives a visual intuition behind the Euclidean distance.
 a given distance r from each other, we can potentially speed up the calculation by doing early abandoning .
 Definition 5 Early abandon: During the computation of the Euclidean distance, if we note that the current sum of the squared differences between each pair of corresponding data points exceeds r 2 , then we can stop the calculation, secure in the knowledge that the exact Euclidean distance, had we calculated it, would exceed r.
 Table 1. The optimized version of the Euclidean distance takes in r as a pa-rameter and uses it to test if an early abandonment is possible. If an early abandonment was performed we can tell by noting that an infinite distance was returned (or by noting num steps was less than the length of the two in-put time series). We call the distance computation of each pair of correspond-ing data points a step , and we use num steps to measure the utility of early abandonment.
 ries indexing, clustering, and classification [13 X 16, 19]. However, in applications such as speech recognition and word image matching, it is desirable to allow elas-tic shifting of the time axis to accommodate sequences that are similar but out of phase. Dynamic Time Warping (DTW) [ 17 ] is such a technique which calculates distance by discovering optimal alignments between points in the two time series. Figure 3 gives a visual intuition of the difference between Euclidean distance and DTW.
 time series.
 where to the alignment between the points q i and c j , which is illustrated in Fig. 4. Definition 6 Warping path: A warping path P is a contiguous (in the tween Q and C. The tth element of P is defined as p t = (i, j) t so we have warping path must start and finish in diagonally opposite corner cells of the matrix; the steps in the warping path are restricted to adjacent cells (including diagonally adjacent cells); the points in the warping path must be monotonically spaced in time. In addition to these constraints, virtually all practitioners using DTW also constrain the warping path in a global sense by limiting how far it may stray from the diagonal [1 X 3, 12, 29 X 31].
 Definition 7 Warping window: The subset of matrix that the warping path is al-lowed to visit is called the warping window.
 Sakoe X  X hiba Band and the Itakura Parallelogram [ 23 , 25 ].
 such that j  X  R  X  i  X  j + R ,where R is a term defining the reach ,oral-lowed range of warping, for a given point in a sequence. In the case of the Sakoe X  Chiba Band R is independent of i . For the Itakura Parallelogram R is a function of i .
 tions, however we are only interested in the path that minimizes the warping cost.
 Definition 8 DTW distance: Given two time series (or time series subsequences), the DTW distance between them is the minimal cost of all warping paths: This review of DTW is necessarily brief. We refer the interested reader to [ 17 ]and [ 23 ] for a more detailed treatment. 3 Time series filtering We are now in a position to give a formal statement of the problem we wish to solve. As we shall see, the problem can come in one of two flavors.
 w and a range r by a user. We want to either  X  search a long batch time series for any subsequences that are within r of any  X  msonitor a (possibly infinite) time series stream for any subsequences that are we only have a small O( C ) memory buffer. The second assumption is that once we are given C and r , we have some reasonable amount of time (say O( C 2 )) to prepare for the coming task.
 reporting how efficient we are when examining large datasets. For the second problem, the quality of solution is the fastest possible arrival rate we can guarantee to handle.
 sequence, both for Euclidean distance and DTW distance. Then we will describe our time series filtering algorithm, H-Merge , and analyze its performance in the streaming case. 3.1 Euclidean distance lower bound Imagine that we have several candidate sequences, C 1 ,..., C k , we can use these sequences to form two new sequences U and L : They form the smallest possible bounding envelope that encloses all members of the set C 1 ,..., C k from above and below. More formally, and denote a wedge as W : an arbitrary query Q and the entire set of candidate sequences contained in a wedge W : of Proposition 1 below is a more general proof). A proof (using different nota-tion, and for a different application) appears in [ 19 ], where the authors use this representation for an indexing problem. 3.2 DTW distance lower bound To allow our approach to handle DTW we will use the envelope-based approach introduced in [ 12 ]. Since its introduction this approach has seen widespread adop-tion. For example, it has been used for indexing handwriting [ 24 ], music [ 10 ], shapes [ 1 ], trajectories [ 29 ], in addition to dozens of applications to time series [ 2 , 21 , 26 , 27 ].
 sequences, DTW U and DTW L : in Fig. 7.
 arbitrary query Q and the entire set of candidate sequences contained in a wedge W : Proposition 1 For any sequence Q of length w and a wedge W containing a set of time series C 1 ,..., C k of the same length w, for any global constraint on the warping path of the form j  X  R  X  i  X  j + R, the following inequality holds: Proof Suppose we know that among the k time series C 1 ,..., C k , C s has the minimal DTW distance to query Q . And we wish to prove every term in the left summation can be matched with some greater or equal term in the right summation. case when q i &gt; DT W U i .Wewanttoshow ( q ( q the same length and j  X  R  X  i  X  j + R , we know i  X  R  X  j  X  i + R . So we can rewrite the right side and the inequality becomes max ( U j ) which is obviously true since U j = max( C 1 j ,..., C kj ). to show, since clearly 0  X  ( q i  X  C sj ) 2 because ( q i  X  C sj ) 2 must be nonnegative. or larger term on the right side. Our inequality holds.
 ing function LB Keogh is the same, only the definitions of the envelope are different.
 cial case where W is created from a single candidate sequence, it degenerates to the Euclidean distance. Second, not only does LB Keogh lower bound all the candidate sequences C 1 ,..., C k , but we can also do early abandon with LB Keogh. While the latter fact might be obvious, for clarity we make it explicit in Ta b l e 2 . 3.3 H-Merge algorithm Suppose we have two candidate sequences C 1 and C 2 of length n , and we know that in future we will be given a query sequence Q and asked if one (or both) of the candidate sequences are within r of the query, we naturally wish to minimize the number of steps we must perform ( X  X tep X  was defined in Sect. 2.1) or the number of times DTW being called. We are now in a position to outline two possible approaches to this problem (note that from now on we only consider the case of Euclidean distance, DTW distance leads to a similar analysis):  X  We can simply compare the two candidate sequences, C  X  We can combine the two candidate sequences into a wedge, and compare the worst case is if both candidate sequences are within r of the query, which will require 2 n steps. In the best case, the first point in the query may be radically different to the first point in either of the candidates, allowing immediate early abandonment and giving a total cost of two steps.
 the query, because we will waste n steps in the lower bounding test between the query and the wedge, and then 1 n steps for each individual candidate, for a total of 3 n . However, the best case, also if the first point in the query is radically different, would allow us to abandon with a total cost of one step.
  X  The shape of the candidate sequences. If they are similar, this greatly favors  X  The shape of the query. If the query is truly similar to one (or both) of the  X  The matching distance r . Here the effect is non-monotonic and dependent on begin by augmenting the notation of a wedge to include information about the sequences used to form it. For example, if a wedge is built from C 1 and C 2 ,we for example the sequence C 1 can also be denoted as W 1 . We can combine W ( 1 , 2 ) and W 3 into a single wedge by finding maximum and minimum values for each i th location, from either wedge. More concretely, C the merge approach. Suppose we have a query Q and a wedge W (( 1 , 2 ), 3 ) .Wecan compare the query to the wedge using LB Keogh. If the LB Keogh function early abandons, we are done. We know with certainty that none of the three candidate sequences is within r of the query. If we cannot early abandon on the wedge, we need to compare the two child wedges, W ( 1 , 2 ) and W 3 to the query. Again, if we two candidate sequences, C 1 and C 2 (in either order) to the query. We call this algorithm H-Merge (Hierarchally Merge).
 three time series shown in Fig. 8. We measured the utility by the number of steps needed by each approach. We found that for reasonable values of r , the type of data we compared it to made little difference: H-Merge was almost always three times faster on average.
 for all possible collections of candidate sequences. This is because the utility of a wedge is strongly correlated with its area. We can get some intuition as to why by this reduces the value returned by the lower bound function and thus the possibility to early abandon.
 formance. For example, suppose we are looking for regions of dramatic level change in a stream. Since we are interested in either positive or negative changes, we query the stream with the two-step function sequences, C 1 and C 2 as shown in Fig. 10. Note that the wedge W ( 1 , 2 ) , created from C 1 and C 2 has an exceptional large area, and is very unlikely to be able to prune off any steps.
 date sequences and the data stream itself. In general, merging similar sequences into a hierarchal wedge is a good idea, but merging dissimilar sequences is a bad idea. Since the meaning of similar / dissimilar is relative to a data stream that by definition we cannot see in advance, it is difficult to predict in advance if H-Merge will be useful.
 set of k sequences, we can merge them into K hierarchal wedges, where 1  X  K  X  k . This merging forms a partitioning of the data, with each sequence belonging to exactly one wedge. We will use W to denote a set of hierarchal wedges: where W set ( i ) is a (hierarchally nested) subset of the k candidate sequences. Note that we have pare this set of wedges against our query. Table 3 formalizes the algorithm. pressive speedup if we make judicious choices in the set of hierarchal wedges that make up W . However, the number of possible ways to arrange the hierar-chal wedges is greater than K K , and the vast majority of these arrangements will generally be worse than classic , so specifying a good arrangement of W is critical. good arrangement of W . Note that hierarchal clustering algorithms have very sim-ilar goals to an ideal wedge-producing algorithm. In particular, hierarchal cluster-ing algorithms can be seen as attempting to minimize the distances between ob-jects in each subtree. For our purposes we are considering the Euclidean distance. A wedge-producing algorithm should attempt to minimize the area of each wedge. However, the area of a wedge is simply the maximum Euclidean distance between any sequences contained therein (i.e. Newton X  X otes rule from elementary calcu-lus). This motivates us to derive wedge sets based on the result of a hierarchal clustering algorithm. Figure 12 shows wedge sets W , of every size from 1 to 5, derived from the dendrogram shown in Fig. 11.
 is to choose the best one. We could attempt to do this by eye, for example in Fig. 12, it is clear that any sequence that early abandons on W 3 will almost certainly also early abandon on both W 2 and W 5 ; similar remarks apply to W 1 and W 4 .At the other extreme, the wedge at K = 1 is so  X  X at X  that it is very likely to have poor However, because the set of time series might be very large, visual inspection is not scalable. More generally, we choose the wedge set based on empirical tests. We t e s t a l l k wedge sets on a sample of data that we believe to be representative of future data and choose the most efficient one.
 3.4 A bound on H-Merge As it stands, H-Merge is an efficient tool for comparing a set of time series to a large batch dataset. However, it does not make any contribution to the problem of streaming time series. The reason is that while it is efficient on average , streaming algorithms are limited by their worst case. The worst case is easy to see. Imagine that we might have chosen W with size of K = 1, and that we examine a new time series Q which is within r of all k candidates. This would mean that we would be forced to do EA LB Keogh 2 k  X  1 times, without ever doing early abandoning. This is actually worse than classic , which only requires k complete invocations of EA LB Keogh in the worst case.
 for H-Merge . The intuition is based on the observation that for realistic values of r and realistic sets of time series, no query sequence Q will be within r of all members of the pattern set. For example, consider the five time series in Fig. 12. Clearly, any sequence Q that is close to C 1 or C 4 cannot also be close to C 3 , C 5 or C . A more formal explanation is given below.
 between them as distance between them is at least d ( W 1 , W 2 ) . This is easy to see. For unoverlapped portion, we sum up the distance between the closest edges of the two wedges. Recall that wedges form the smallest possible bounding envelope that encloses all its members, which means any pair of the time series from W 1 and W 2 cannot be closer than the closest edge pair. For overlapped area, we count the distance as zero, which obviously lower bounds the distance.
 d ( According to triangular inequality, wedges recursively. We illustrate the computation of the cost upper bound in Ta b l e 4 .
 and interesting patterns, so usually r is a relatively small value, which in-creases the possibility for two wedges having distance larger than 2 r .Asare-sult, the H-Merge algorithm can skip a lot of computations based on the proof we gave above. As we shall see in Sect. 4, with reasonable value of r ,in approach. 3.5 A final optimization There is one simple additional optimization that we can do to speed up H-Merge . Recall that in both Tables 1 and 2 that when we explained early aban-doning we assumed that the distance calculation proceeded from left to right (cf. Fig. 2). When comparing individual sequences we have no reason to suppose we accumulate the error will allow an earlier abandonment. However, this or-der can make a huge difference. It is simply that we cannot know this order in advance.
 wedge. In this case, we do have an a priori reason to suspect that some order-ings are better than others. Consider the wedge shown in Fig. 6. The left side of this wedge is so  X  X at X , that most query sequences will pass through this part of the wedge, thus contributing nothing to the accumulated error. In contrast, consider the section of the wedge from 10 to 50. Here the wedge is very thin, and there is a much greater chance that we can accumulate error here.
 able is sorted in ascending order by the value of U i  X  L i (the local thick-ness of the wedge). This sorting takes O (w log (w)) for each wedge ( w is the length of the wedge), but it only needs to be done once. As we shall see in the next section, this simple optimization speeds up the calculations by an order of magnitude. 4 Experimental results In this section, we test our proposed approach with a comprehensive set of ex-periments. Section 4.1 shows some experiments using Euclidean distance mea-surement. Section 4.2 contains experiments using DTW distance measurement. In Sect. 4.3 we conduct a simple experiment which shows the speedup power of the minor optimization we mentioned in Sect. 3.5. An application of our approach to semisupervised learning of time series classifiers is shown in Sect. 4.4. reader may already appreciate, the value of r can make a huge difference to the utility of our work. If r is arbitrarily large, we can be sure that the performance of H-Merge will be worse than brute force .Incontrast,bymaking r arbitrarily small we can guarantee that H-Merge is approximately k times faster than classic .We want to know the performance of H-Merge at the values of r that we are likely to encounter in the real world. Two of the domain experts (cardiology and ento-mology) that are co-authors of this work independently suggested the following policy.
 interest, for example several examples of atrial premature beat and several exam-ples of ventricular escape beat . A logical value for r would be the average distance from a pattern to its nearest neighbor. The intuition is that if the patterns we have previously seen tended to be about r apart, then a future query Q that is actually a member of this class will probably also be within r of one (or more) pattern(s) of our dataset. Setting a smaller value will increase the risk of a false negative, and setting a larger value will increase the risk of a false positive. We therefore have adopted this policy for all the experiments that follow.
 tioned in [ 11 ]. 4.1 Experiments using Euclidean distance This set of experiment uses Euclidean distance as the distance measurement. We try all four approaches, H-Merge , brute force , classic ,and H-Merge-R ,andcom-pare the efficiency of them. Among these four approaches, H-Merge is the ap-proach we proposed in this paper. Brute force is the straightforward approach that compares each pattern to the query without using the early abandoning algorithm. Classic is better than brute force in the sense that it computes the Euclidean dis-tance between two time series with early abandon. H-Merge-R is similar to H-Merge , except that instead of using the wedge sets resulted from the hierarchical clustering algorithm (in this paper we use complete linkage clustering algorithm), we randomly merge time series to form the wedge set. This modification is essen-tially a lesion study which helps us separate the effectiveness of H-Merge from our particular wedge merging strategy. 4.1.1 ECG dataset The ECG dataset comes from the MIT-BIH Arrhythmia Database [ 8 ]. It contains half an hour X  X  excerpts of two-channel ambulatory ECG recordings. The record-ings were digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range. We use signals from one channel as our batch time series, which has 650,000 data points in total. Our pattern set consists of 200 time series, each of length 40. According to the cardiologists X  annotation, they are represen-tative patterns of left bundle branch block beat , right bundle branch block beat , atrial premature beat ,and ventricular escape beat .For H-Merge and H-Merge-R , we tested all 200 wedge sets on first 2,000 data points, and chose the most efficient one as the wedge set to use.
 steps needed by each approach in Fig. 14 (the precise numbers are recorded in the appendix). The result shows that our approach is faster than brute force by three orders of magnitude, and faster than classic by two orders of magnitude. Note that H-Merge-R does not achieve the same speedup as H-Merge , suggesting that our wedge building algorithm is effective. We also computed the upper bound of the cost of H-Merge for ECG dataset. In the worst case, 2,120 steps will be needed for H-Merge to compare a subsequence to the wedge set, about four times faster than that of the brute force approach, which in the worst case will need to compare the subsequence to all patterns, resulting in 200  X  40 = 8,000 steps. 4.1.2 Stock dataset Our second experiment considered the problem of finding interesting patterns in a stock dataset. We tested on a stock time series with 2,119,415 data points. There are 337 time series of length 128 in the pattern set. They represent three types of patterns which were annotated by a technical analyst, with 140 for head and shoulders , 127 for reverse head and shoulders , and 70 for cup and handle .Again, for H-Merge and H-Merge-R , we tested all 337 wedge sets on first 2,000 data points, and used the most efficient one for the rest of the data.
 illustrated in Fig. 15. The result again indicates impressive speedup of H-Merge . H-Merge is faster than brute force by two orders of magnitude, and faster than classic by one order of magnitude. For stock dataset, the cost upper bound of H-Merge is 18,048, which is about one third to that of the brute force approach (337  X  128 = 43,136). 4.1.3 Insect audio dataset As mentioned in Sect. 1, the availability of advanced technologies and cheap hard-ware has made possible automatic insect monitoring. In this experiment, we tested a 1-h wave file to monitor the occurrences of some harmful mosquito species. The wave file, at sample rate 11,025 Hz, was converted to a 46,143,488 data points X  time series. Here we used a sliding window of size 11,025 data points (1 s sound) and slid it by 5,512 points (0.5 s) each time. Because insect detection is based on the frequency of wing beat, we applied Fourier transformation on each sub-sequence and then resampled the time series we got (note that the FFT was per-formed by specialized hardware directly on the sensor [ 18 ] and that the time taken for this is inconsequential compared to the algorithms considered here). The pur-pose of resampling is to keep the feature (shape) of the time series while reducing the number of data points in it. We have 68 candidate time series of length 101, which are obtained through the same procedure (FFT plus resampling) from three different species of harmful mosquitoes, Culex quinquefasciatus , Aedes aegypti , and Culiseta spp. For H-Merge and H-Merge-R , we used first 3 min sound to de-cide which wedge set to use.
 parameter r equals to 4.14. H-Merge is faster than brute force by two orders of magnitude. Note that in this experiment, H-Merge-R is worse than classic .For audio dataset, the cost upper bound of H-Merge is 2,929, which is about one third to that of the brute force approach (68  X  101 = 6 , 868).
 4.2 Experiments using DTW This set of experiment uses DTW distance as the distance measurement. To evalu-ate the efficiency of our approach, we compare the number of times DTW function being called by H-Merge andbythe brute force approach. Note that we do not test on classic and H-Merge-R approaches here because from previous experiments we are pretty sure that the computational cost for classic and H-Merge-R approaches will be unacceptably high. While for brute force approach, the number of DTW calls depends only on the length of the batch time series, the length of the pattern, and the number of patterns. So it can simply be computed . In addition to the effi-ciency test, we also compare the effectiveness of our approach when using DTW distance and using Euclidean distance. 4.2.1 Gun dataset This dataset contains a 2D time series, which were extracted from videos of actors performing various actions with and without a replica gun. The two time series, each of length 18,750, measure the X and Y coordinates of the actors X  right hand. For simplicity we only consider Y -axes here. There are two different actors (a male and a female) and two typical sequences ( Gun-Draw and Point ). In Gun-Draw sequence, the actor draws a replica gun from a hip mounted holster, aims it at a target, and returns it to the holster. In Point sequence, the actor points his/her index finger to a target, and then returns his/her hand to the side. Combining the actors and the sequences, we have four classes of interests: female actor drawing the gun (Female-Gun), female actor pointing (Female-Point), male actor drawing the gun (Male-Gun), and male actor pointing (Male-Point). We have collected an 80-instance pattern set, 20 for each class. Each instance has the same length of 150 data points. ing with Sakoe X  X hiba Band. We then compare the accuracy achieved by each distance measure. The long time series is fully annotated, that is, we know the starting point of every interesting segment. For each interesting segment, if a seg-ment of the same class is filtered out within a reasonable offset of its starting point, we count it as a hit . The accuracy is simply the number of hits over the number of interesting segments. Recall that at the beginning of the paper, we claimed that our approach guarantees no false dismissals, which should not be confused with the accuracy. No false dismissals means that if a subsequence is within r of any of the pattern, it is guaranteed to be filtered out by our approach. While ac-curacy measures how effective our approach is in finding objectively annotated patterns.
 offset = 15, and warping window size = 3%. As we expect, DTW achieves much higher accuracy than does the Euclidean distance. Note that during the en-tire process, only 28,198 DTW calls are made, which is two orders of magni-tude less than those made in the brute force approach. In contrast, brute force approach needs to compute the DTW distance between each sliding window and all the patterns, resulting in (18,750  X  150 + 1 )  X  80 = 1,488,080 DTW calls. 4.2.2 ECG dataset This dataset is a subset of the one used in Sect. 4.1.1, but much shorter. It contains 200,000 data points. Note that to allow the use of the DTW distance, which is expensive to compute, we deliberately choose a shorter dataset. Again, our pattern bundle branch block beat (L), right bundle branch block beat (R), atrial premature beat (A), and ventricular escape beat (E).
 is defined in Sect. 4.2.1). And the parameters we use are as follows: r = 0 . 5, offset = 20, and warping window size = 3%. As shown in Table 6 , DTW again achieves much higher accuracy than does Euclidean distance. DTW is only called 8,038 times during the filtering process, which is thousands times less than that required by the brute force approach: (200,000  X  40 + 1 )  X  200 = 39,992,200. This once again shows the pruning power of our wedgies.
 4.3 Speedup by sorting In Sect. 3.5, we described an optimization on H-Merge , where the distance cal-culation proceeds in the ascending order of the local thickness of the wedge. To demonstrate the advantage of this optimization, we compare the wedge shown in Fig. 6 to 64,000 random walk time series, and recorded the total number of steps required with and without sorting. The result, shown in Fig. 17, demonstrates that the simple optimization can speed up the calculations by an order of magnitude. 4.4 Application to time series classification So far, we have demonstrated that our approach could efficiently filter out the sub-sequences from a long time series that are similar to a set of predefined patterns. Now we will take some time to show that how this can help other time series data mining tasks. We give an example of the application of the H-Merge approach in semisupervised time series classification.
 est in the last decade. However, all these research assumes the existence of large amounts of labeled training data. In reality, such data may be very difficult or ex-pensive to collect. For example, it may require the time and energy of the domain experts. As in many other domains, there is often copious amount of unlabeled time series available. For example, the PhysioBank archive [ 8 ] contains more than 40 GB of ECG data freely available over the web, and hospitals often archive even larger amounts of ECG data for legal reasons.
 and the difficulty of obtaining labeled data, we propose a semisupervised algo-rithm which builds time series classifiers using unlabeled data. Note that the full explanation of the semisupervised approach has consequences for other problems and perhaps deserves a separate paper. Therefore, we content ourselves with a brief explanation here. For simplicity, we consider the binary classification prob-lem (any N -classification problem can always be converted to multiple applica-tions of a binary classification algorithm). The idea is as follows. Given a small set of labeled positive examples P and a set of unlabeled examples U , the algo-rithm first uses P to train a 1-nearest neighbor classifier. Second, let the 1-nearest neighbor classifier examine the unlabeled set U and select one example it most confidently labels as positive. Then the example selected in this way is added to P , and removed from U . The algorithm iterates the above procedure until some stopping criterion is reached. The framework of the algorithm is shown in Table 7. probably positive, and the stopping criterion is highly related to the strategy. We adopt a distance-based strategy here, with the intuition that the closer an example is to the positive set, the more likely it is positive. The distance between an exam-ple and the positive set is computed as the distance from the example to its nearest neighbor in the positive set. In each iteration, we need to find an unlabeled time series which is closest to any example in the positive set. This can be regarded as a time series filtering problem with a dynamic r (every time a closer match is found r will be updated accordingly). The H-Merge approach could be applied to improve the efficiency of the training process. We do not present a solution to the stopping criterion problem here, since this work is merely presented as a proof of concept for tractable semisupervised time series classification.
 distribution as the training set. Once again we would like to point out that our training set only contains a small set of labeled data, which is different from the traditional training set used in supervised training. At the first glance, using the classifier would be easy. For each data object to be classified, just compare it to each example in the training set and place it in the class which its nearest neigh-bor belongs to. However, in reality it is typical that we have a huge training set (because unlabeled data is abundant). To compare an object to each example in the training set will be quite expensive. Another observation is that the training set is usually extremely imbalanced, with relatively small number of time series in positive class and a huge number of time series in negative class. For example, the document of the MIT-BIH Arrhythmia Dataset pointed out that those clinically significant arrhythmias are rare and appear very infrequently in the ambulatory ECG recordings they collected [ 8 ].
 fier, using only the positive examples in the training set. If an object to be classified is within r distance to any of the positive examples in the training set, it will be classified as positive. Otherwise it is negative. Note that this is exactly the time series filtering problem we are trying to solve in this paper! It is very natural for us to adopt the H-Merge approach to do the classification.
 algorithm could use the unlabeled data to outperform the standard supervised classifier. Performance of the classifier is reported as precision X  X ecall breakeven point, a standard information retrieval measure for binary classification. Accuracy is not a good performance metric here because the classifier can simply classify everything as negative while still getting very high accuracy. The precision X  X ecall breakeven point is the value at which precision and recall are equal [ 9 ]. ECG dataset is obtained from the MIT-BIH Arrhythmia Database [ 8 ]. Each data record in the ECG dataset contains the measurements recorded by one electrode during one heartbeat. The data has been analyzed by cardiologists and a label of normal or abnormal is assigned to each data record. Of the 2,026 data records in the dataset, 520 were identified as abnormal and 1,506 were identified as normal. We use about half of the data as the training set and the other half as the testing set, as summarized in Table 8.
 the training set as the initial labeled positive set P . In each iteration, the semisuper-vised algorithm adds one example to the positive set P , uses the 1-nearest neighbor with the new positive set P to classify the testing set, and reports the precision X  recall breakeven point. We run the experiments 208 times and the average per-formance is shown as the bold line in Fig. 18. The gray lines bounding it from above and below are one standard deviation away. Note that the precision X  X ecall increases dramatically at the beginning and stabilizes after about 10 iterations. On average, the maximal precision X  X ecall achieved by the semisupervised approach is 94.97%. We then run another 208 experiments for the k -nearest neighbor classifier (with k = 208) without semisupervised training, each time using the same 10 pos-itive examples as we used in the semisupervised approach. The k -nearest neighbor classifier achieves much lower precision X  X ecall (the first point of the bold line rep-resents the average precision X  X ecall achieved by the k -nearest neighbor classifier, which is 81.29%). This indicates that using unlabeled data increases the perfor-mance of the classifier. Note that after increasing the number of objects in the positive set to about 170, the accuracy rapidly decreases. This reiterates the need of finding a good stopping criterion, an issue we sidestep here.
 W on the initial 10 objects in P , use the mean distance between objects in this set as a tentative value for r , and begin searching. All objects within r are flagged for inspection and the closest of these is added to P, and the search is repeated. Each time we add a new object to P we must rebuild the wedge set W from scratch, however the time taken to do this is inconsequential compared to the time taken to search a large time series database. 5 Conclusions In this paper, we introduce the problem of time series filtering: fast, on-the-fly subsequence matching of streaming time series to a set of predefined patterns. Given the continuously arriving data and the large number of patterns we wish to support, a brute force strategy of comparing each pattern with every time series subsequence does not scale well. We propose a novel filtering approach, which exploits commonality among patterns by merging similar patterns into a wedge such that they can be compared to the time subsequence together. The resulting shared processing provides tremendous improvements in performance. We also show an example of the utilization of our approach to the semisupervised time series classification problem.
 some representative sample data. More generally, data may change over time (i.e. concept drift) and we may not be able to find such a sample of data. Under this condition, dynamically choosing the wedge set will be more useful. We leave such considerations for future work.
 Appendix A Appendix B References Author Biographies
