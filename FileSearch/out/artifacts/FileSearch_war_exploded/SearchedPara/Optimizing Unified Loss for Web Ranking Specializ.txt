 In this paper, we proposed a novel divide-and-conquer ap-proach to optimize the overall relevance in an unified frame-work for query clustering and q uery-based ranking. Latent topics and specialized ranking models are learned iteratively so that an unified objective function, which lower-bounds the conditional probability of observed grades annotated by human editors on training data, is maximized. We con-ducted experiments comparing the proposed method with several baseline approaches on two data-sets. Experimental results illustrate that our method can significantly improve the ranking relevance over these baselines.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Retrieval functions ; H.4.m [ Information Systems ]: Miscellaneous X  Machine learning Algorithms, Experimentation, Theory Ranking specialization, Ranking-based Clustering, Unified Loss
In the general web search ranking scenario, training and testing data usually consists of different types of queries that vary from each other significantly in semantics, user inten-sions, etc. Different queries, like navigational queries, per-sonal queries, product queries, or local queries, may have very different behaviors in the ranking process. To over-come the problem caused by the heterogeneous nature of web-search queries, IR community has proposed several pos-sible solutions based on the topical ranking idea. The basic  X  Human defined categories have been used as query par-titions for topical ranking in many previous works ( [2], [3], [10], [12]). However, in most cases, the cate-gories are defined for their semantic meanings, instead of for maximizing the overall ranking performance. In fact, semantically similar queries may have very dif-ferent result-set feature values and are not coherent in feature space. Thus these approaches may not be the best choice to solve the problem of heterogeneous queries from the ranking point of view.  X  Another choice is to automatically learn the latent top-ics from traning data using clustering algorithms. In the training phase, training data is partitioned into K clusters based on the query-level-similarity, which is calculated from the result-set features of given queries. Example of recent works following this line include Topical RankSVM proposed in [4] and query-dependent-ranking models (off-line version) proposed in [7] . The limitation of such methods is that, its clustering pro-cedure is still a separate step from the ranking model training procedure. The clustering procedure only re-lies on query result-set features, and does not exploit the information from labels of query-URL pairs anno-tated by human editors in the training data, thus it is not optimized for the final ranking purpose. This could lead to unexpected results. One possible example is that, result-set features that play dominant roles in the clustering step may be simply irrelevant for rank-ing tasks. In such cases, final ranking results will not get benefit from the clustering procedure.
 Specialization and Ranking-based Clustering. The first step (Ranking Specialization) trains a specialized ranking model for each latent topic, and the second step (Ranking-based Clustering) maps each query to latent topics whose special-ized ranking models most fit the query. Both these two steps are designed to decrease the value of an unified loss function on training data, and this process is repeated until conver-gence occurs. This makes our method significantly different from previous works in this line, and we also believe it is an important advantage that enables our approach to serve the ranking purpose in a better way.

The rest of the paper is organized as follows. In section 2, we describe our model and algorithm to solve it. In section 3, we show our experimental settings and results. In section 4, we summarize the conclusions.
In ranking problem, we a re given training set { q i ,U i ,G i query-URL feature vectors associated with q i ,and G i means to represent the j th URL in U i , and use G ij to represents Let X  X  also assume there is a set of query-dependent features, denoted as F i , for each query q i .
 In this paper, we propose the loss function as: where m k represents the specialized ranking model for the to latent topics.  X  k are the parameters of the mapping func-tion.

In the training procedure, the loss function in formula 1 can be solved by optimizing  X  k and m k iteratively. The learning of m k corresponds to the Ranking Specialization step and the learning of  X  k corresponds to the Ranking-based clustering step.

In the testing procedure, given a test query q i and a list of associated URLs U i , The predicted score of the ij th query-URL pair is calculated as
In this paper, we assumed the mapping function z ( F i ,  X  has the following linear form: The main reason we take the assumptions of linearity is to simplify our finial loss function in formula 1 so that the pa-rameters  X  k can be solved efficiently using linear program-ming.
 {  X  k | k =1 , ..., K  X  1 } and { m k | k =1 , ..., K } iteratively 1. Initialize values for {  X  k | k =1 , ..., K  X  1 } 2. Iterate until convergence (a) Fix the current values of {  X  k | k =1 , .., K  X  1 } ,and (b) Fix the current m k , and use linear programming 3. Return { m k | k =1 , ..., K } and {  X  k | k =1 , ..., K  X  Step 2(a) is easy to solve since it is no more than learn-ing regular ranking functions with additional weights associated with training examples. We can use stan-dard GBDT learning algorithms, with sample weights set as z k ( F i ,  X  k )), to learn ranking models { m k | 1 , ..., K } .  X  Step 2(b) can be solved by standard linear program-ming. When m k is fixed, becomes a fixed number and the objective function can be reduced to a standard linear programming form.  X  LETOR 3.0: LETOR 3.0 [11] is a benchmark dataset for research on ranking [1]. We use TREC2003 and and TREC2004 datasets in LETOR 3.0 to evaluate our approach. TREC2003 contains 350 queries and TREC2004 contains 225 ones. For each query, there are about 1,000 associated documents. Each query-document pair is given a binary judgment: relevant Figure 1: The values of metric NDCG@K with K =1 , 5 , 10 for our method, Single-RM, Semantic-RM, Hard-Clustering-RM, Soft-Clustering-RM and Offline-KNN-Topical-RM on SE-Dataset, using GBDT.
In this paper, we generate a set of query dependent fea-tures by taking advantage of the ranking features of top pseudo feedbacks of the query. For each training query q  X  Train , we first retrieve a set of pseudo feedbacks, D ( q )= { d 1 ,d 2 ,  X  X  X  ,d T } , consisting of the top T documents ranked by a reference model (we use BM25 in this paper). Then we take the mean and variance of the ranking feature values from these T documents to generate our query dependent features.
We compared our method with the following baseline ap-proaches in experiments.  X  Single Ranking Model (Single-RM): This baseline approach trains a single model using all the training data, and apply this model on all the testing queries.  X  Ranking Model with pre-defined topics (Semantic-Topical-RM): This baseline approach trains a model for each pre-defined semantic topic on the training data. Given a testing query, the corresponding model on this query X  X  topic will be invoked to generate the ranking results.  X  Ranking Model with topics generated by tradi-tional clustering (Hard-Clustering-Topical-RM): In this method, we implement the Hard-Clustering-based Topical Ranking approach. After identifying the topics using traditional clustering approaches, we assign each training query into the closest query cluster. Based on this hard partition of training queries, we train a sep-arate ranking model for each query cluster using its own fraction of training queries. At testing time, ac-cording to the correlation between the test query and query categories, the ranking model of the most corre-lated query cluster is selected to generate the ranking results.  X  Ranking Model with topics generated by tra-ditional clustering (Soft-Clustering-Topical-RM): In this method, we implement the Soft-Clustering-based Topical Ranking approach. We first simulate the idea in [4] and generate the topics and membership proba-bilities for training queries. Then we train a separate ranking model for each query cluster using the mem-bership probabilities as query weights. At testing time, we also follow [4] and set the final predictive score as the weighted sum of predictive scores of ranking mod-els in different clusters.  X  Ranking Model with topics generated by KNN-based clusters (Offline-KNN-Topical-RM): We sim-ulate the idea of the KNN offline-2 model proposed in [7] and construct topics based on K nearest neighbors. respectively respectively
In figure 1, we plot the bar graph that gives the values of metric NDCG@K with K =1 , 5 , 10 for our method and the baselines on SE-dataset. The result shows that our method is consistently better.
In this paper, we explored improving the overall rank-ing performance by a divide-and-conquer approach to learn multiple specialized ranking functions for different types of queries. Compared with previous works that treat cluster-ing and ranking as separate steps, our approach generates query partitions and specialized ranking models within a consistent framework, and the human annotated relevance grades are exploited to supervise the implicit clustering pro-cedure in our model. Thus we expect our method to achieve better overall ranking performance compared with previous works. Experiments are conducted with several state-of-art baseline on two data-sets. The empirical results show that our method can significantly outperform these baselines on both datasets. [1] Letor dataset website. [2] S. M. Beitzel, E. C. Jensen, A. Chowdhury, and [3] S. M. Beitzel, E. C. Jensen, O. Frieder, D. Grossman, [4] J. Bian, X. Li, F. li, H. Zha, and Z. Zheng. Ranking comparison of normalization methods for high density oligonucleotide array data based on variance and bias.
Bioinformatics , 19:185 X 193, 2003. gradient boosting machine. In Annals of Statistics 29 , page 1189  X  lC1232, 2001.
H.-Y. Shum. Query dependent ranking using k-nearest neighbor. In SIGIR , page 3, 2007. vector learning for ordinal regression. In Proc. of
ICANN , 1999. evaluation of ir techniques. In ACM Transactions on
Information Systems , 2002. user goals in web search. In WWW , pages 391 X 400, 2005.
Benchmark dataset for research on learning to rank for information retrieval. In Proc.ofSIGIR , 2007.
W.-Y. Ma. Support vector machines classification with a very large-scale taxonomy. In SIGKDD , pages 36 X 43, 2005. and G. Sun. A general boosting method and its application to learning ranking functions for web search. In NIPS , 2007. and G. Sun. A regression framework for learning ranking functions using relative relevance judgments.
In SIGIR , pages 287 X 294, 2007.
