 Risi Kondor risi@cs.columbia.edu Tony Jebara jebara@cs.columbia.edu Kernel metho ds, such as Supp ort Vector Machines, Gaussian Processes, etc., have proved to be extremely successful at a wide variety of supervised and unsu-pervised Machine Learning tasks. Whilst the core al-gorithms in this  X eld are now fairly well crystallized (Sch X olkopf &amp; Smola, 2001) and their theoretical prop-erties have been thoroughly investigated,  X nding op-timal ways of represen ting real life data as input to these algorithms is still a largely open issue. Instead of operating on training and testing exam-ples x 1 ; x 2 ;:::; x m 2X directly (where X is the input space), kernel based algorithms only make recourse to the value of the kernel function K ( x ; x 0 ) evaluated for each pair of examples. The kernel K can be any sym-metric similarit y measure satisfying positiv e (semi-) de X niteness, for any m 2 N , any selection of examples x 1 x 2 ::: x m X , and any set of coe X cien ts c 1 ;c 2 ;:::;c m 2 R . These conditions ensure the existence of a mapping  X  K : X 7! F to some Hilbert space F (called feature space), in which K turns into the inner product: Traditionally , examples have been represen ted as vec-tors, x i 2 R n , and the kernel was de X ned as a closed form positiv e de X nite function on R n , such as the Gaus-sian Radial Basis Function (RBF) kernel More recen tly, it has been realized that one of the strengths of the kernel based learning paradigm is its abilit y to supp ort much more general represen ta-tions of the data. Indeed, the input space can be almost anything, as long as we can de X ne a func-tion on it that is both positiv e de X nite, and a plau-sible similarit y measure between examples for the task at hand. This idea has given rise to a whole host of novel kernels, such as string kernels (Watkins, 2000)(Lo dhi et al., 2002)(Leslie et al., 2003)(Vish-wanathan &amp; Smola, 2003), kernels on graphs (Kondor &amp; La X ert y, 2002), kernels on automata (Cortes et al., 2003), kernels on the statistical manifold (Jaakk ola &amp; Haussler, 1999)(La X ert y &amp; Lebanon, 2003), and kernels on more general discrete objects (Haussler, 1999)(Collins &amp; Du X y , 2002).
 In this paper we focus on represen ting examples as sets of vectors x = f x 1 ; x 2 ;:::; x k g , x i 2 R n . Such a \bag of tuples" approac h (Figure 1) can suit diverse domains in a natural way. For images, each tuple may corresp ond to a single pixel, encoding its ( x;y ) co-ordinates and the corresp onding intensit y value. In time series analysis, tuples may encode (value ; time) pairs. A video sequence can be seen as a collection of ( x;y; intensit y ; time) 4-tuples.
 In all of the above cases, the emphasis is not on the represen tation for its own sake, but rather the behavior it confers on the kernel. For instance, our kernel be-tween sets of vectors is automatically invarian t under permutations of vectors within the set. More to the point, we are interested in kernels that are relativ ely insensitiv e to transformations x i 7! x i +  X  , especially when  X  is a smoothly varying function of x . Such \soft invariances" matc h intuitiv e notions of similarit y, and are a key elemen t in the design of high performance kernels. For example, for images, when  X  is a slowly varying function of ( x;y ), transformations of this kind corresp ond to translations, rotations and warpings. To achieve this soft invariance prop erty, we  X t dis-tributions p and p 0 to the sets x = f x 1 ; x 2 ;:::; x k Bhattac haryy a overlap measure between p and p 0 . The intermediate step of  X tting the distributions ensures explicit invariance in permutation and a X ords a de-gree of smoothing in R n . In the following we concern ourselv es with  X nding the right parametric family of distributions to choose p and p 0 from, being su X cien tly general to capture the structure of the objects we wish to represen t, a X ord controllable smoothing, and still allow us to compute the kernel in closed form. A similar vector set represen tation for images in con-junction with using the kernel trick has been prop osed by Wolf and Shash ua (2003). Other approac hes to handling sets of vectors, collections of tuples or bags of pixels include applying PCA to the data (i.e. for several images) while main taining each image's built-in permutational invariance (Jebara, 2003). In this paper we investigate the case when examples are presen ted as collections x = f x 1 ; x 2 ;:::; x k g of n dimensional vectors ( n -tuples) x i 2 R n or x i 2  X   X  R n Mono chrome bitmap images can naturally be repre-sented in this form by letting x = ( x;y ) &gt; encode the x and y coordinates of each pixel, and letting x be the set of all foreground pixels. For gray-scale or color images, represen tations of the form x = ( x;y; X  ) &gt; or x = ( x;y; X  r ; X  g ; X  b ) &gt; may be used to encode informa-tion about brigh tness or the intensit y of the RGB color comp onen ts. The set x will then contain one such tu-ple for every pixel, or a random subset of pixels. If desired, images can be describ ed in terms of more com-plex features (G X  X bor wavelets, edge features, etc.), and each tuple can code one occurrence of such a feature. In this paper, instead of de X ning a kernel directly be-tween sets of tuples, we regard x and x 0 as i.i.d. sam-ples from unkno wn distributions p and p 0 from some parametric family P . We proceed by de X ning a ker-nel between mem bers of P , and a statistical proce-dure for estimating p from x and p 0 from x 0 . The vector set kernel between x and x 0 is then de X ned as the kernel between the corresp onding distributions: K ( x ; x 0 ) = K ( p;p 0 ). To simplify the notation, in the following we omit the use of bold face font for vector quan tities, in the understanding that x , x 0 , x i , etc. will always denote mem bers of R n . 2.1. Bhattac haryy a Kernels There are several well-kno wn de X nitions of similar-ity or distance between distributions, such as the Kullbac k-Leibler divergence, Fisher kernel,  X  2 dis-tance, and so on. To de X ne our kernel, in this paper we use Bhattac haryy a's a X nit y (Bhattac haryy a, 1943) trivially related to the better known Hellinger's dis-tance by H = nal basis of functions shows that K is automatically positiv e de X nite. In addition, it also satis X es the nor-malization prop erty for any p 2P . 2.2. The Multiv ariate Normal Model In the following we shall restrict our atten tion to the case where P is the family of multivariate normal dis-tributions N (  X ;  X ) with probabilit y densit y function where j  X  j denotes the determinan t. For more general applications of kernels of the form (1) see (Jebara &amp; Kondor, 2003).
 We set  X  and  X  to their Maxim um Likeliho od esti-mates, given by the sample mean and empirical covariance matrix A short computation shows that the Bhattac haryy a kernel (1) between p = N (  X ;  X ) and p 0 = N (  X  0 ;  X  0 ) is exp  X  0  X  1  X  0 . Hence, by plugging (2) and (3) into (4), the kernel K ( x ; x 0 ) can be computed in closed form. At this point, the represen tational power of our ker-nels migh t seem rather limited. Certainly , for images we cannot truly hope that two dimensional Gaussians will capture su X cien t detail for successful learning. To overcome this di X cult y, we introduce a second positiv e de X nite kernel,  X  : R n  X  R n 7! R , this time de X ned be-tween the elemen tary vectors x .
 Recall that for any such kernel, we can construct a Hilbert space H and a mapping  X : R n 7!H such that We can now repeat the construction of Section 2, this time letting P be a family of distributions over H ,  X tting p to the Hilbert space points  X ( x 1 ) ;:::;  X ( x k and de X ning the kernel as For typical kernels, the  X ( x i ) will span a subspace in H of dimensionalit y much greater than n , allowing simple parametric families of distributions over H to capture complex structures in the original sample.
 Our choice of  X  in our experimen ts on images in Sec-tion 5 will be the familiar Gaussian RBF kernel but our metho d is not in any way limited to this par-ticular kernel.
 Indep enden t of our work, the same idea of de X ning a kernel between a swarm of Hilbert space vectors in-duced by another kernel has recen tly been prop osed by Wolf and Shash ua (2003), also in the context of rep-resen ting images as sets of vectors. In contrast to our distribution-based approac h, Wolf and Shash ua con-centrate on the subspaces spanned by the  X ( x i ) and de X ne their kernel via principal angles between such subspaces.
 We now discuss how, in the special case of the multi-variate normal model, (5) can be computed in closed form without the need to explicitly construct the im-ages  X ( x i ). 3.1. Normal Distributions on H To facilitate the following discussion, we adopt Dirac's bra-k et notation (Dirac, 1930) for Hilbert space ob-jects. The \ket" j x i will denote  X ( x ) and the \bra" h x j will denote its dual, the analog of  X ( x ) &gt; for  X nite dimensional vector spaces. Bras and kets labeled with letters other than x will denote general elemen ts of H , which migh t or migh t not be the images of some x 2 R n under  X .
 The inner product between j  X  i and j  X  0 i is simply writ-ten h  X  j  X  0 i , while expressions of the form j  X  ih  X  0 j weighted sums of such expressions,  X  = P i j  X  i i a i h  X  are symmetric bilinear forms on H , corresp onding to symmetric matrices in the  X nite dimensional case. The power of Dirac's notation begins to show when consid-ering the corresp onding linear mapping  X  : H 7! H : where, of course, each h  X  i j  X  i is just a number. Let V denote the orthogonal complemen t to the nullspace of  X , V  X  = f j z i 2 H : h z j z 0 i = 0 8j z 0 i 2 H such that  X  j z 0 i = j 0 i g . Note that for invertible  X  ( V  X  = H ), provided the j  X  i i form an orthonormal set ( h  X  i j  X  j i =  X  ij ), the inverse of the  X  will simply be  X  A  X nite dimensional Normal distribution N ( j  X  i ;  X ) on H is of the form where  X  is a symmetric, positiv e de X nite bilinear form of rank d . Note that this is a prop er distribution only on V  X  , not on the whole of H , since p ( j z i ) is uniform in all directions orthogonal to V  X  . Plugging in the empirical mean and covariance as before is unlik ely to lead to good results, since in the Bhattac haryy a kernel this will not penalize for the lack of alignmen t between the spaces V ^  X  and V ^  X  0 . A related problem is that of over X tting. In particular, for the Gaussian RBF kernel it can be shown that the j x i will span a subspace of dimension exactly k in H . Fitting a k dimensional Normal distribution to k data points is not robust in the directions of low covariance, nor are these directions informativ e. Generally , the  X rst few eigen vectors of the covariance matrix give a good description of the data: carrying around all the eigen vectors is wasteful and potentially misleading. To address both problems at the same time, instead of ^  X , we take  X  to be the regularized covariance form where j v 1 i ;:::; j v r i are the r largest eigen vectors of  X  ;:::; X  r are the corresp onding eigen values,  X  is a reg-ularization constan t, and the j  X  i i form an orthonormal basis for H .
 Note that in the case that H is in X nite dimensional, the denominator in (6) becomes divergen t. Strictly speaking, p is not a normal distribution anymore but a Gaussian Process, as we shall discuss in Section 4. However, in the form ula for the Bhattac haryy a kernel (4) these diverging normalization factors will cancel, conforming to our intuition that all action is limited to the  X nite dimensional subset of H spanned by the data.
 The technique of computing eigen vectors in feature space is known as Kernel Principal Comp onen t Anal-ysis (Kernel PCA), and was developed in (Sch X olkopf et al., 1998) in the context of unsup ervised learning. We now review this technique and show how to con-struct the eigen vectors j v j i without any explicit calcu-lations in H . 3.2. Kernel PCA The key observ ation in Kernel PCA is that the eigen-alently, the centered images j x  X  i i = j x i i X j ^  X  i : Plugging (9) in the eigen vector equation ^  X  j v i =  X  j v i , and multiplying on the left by any h x  X  l j gives Observ e that these sums are but regular matrix mul-tiplications in disguise, so equiv alently, where K  X  is the centered Gram matrix, K  X  i;j =  X  x i j x  X  j  X  =  X  ( x  X nding the principal comp onen ts of the typically very high, possibly in X nite, dimensional vectors j x i i reduces to the k -dimensional eigen vector problem Figure 2 shows the  X rst three kernel principal comp o-nents of ^  X  for a handwritten letter R , mapp ed back to the original image plane by v induced l ( x ) = h x j v The principal comp onen ts capture visually recogniz-able features of the  X gure.
 Figure 3 shows the reconstruction of the same letter from 1 ; 2 ; 3 and 4 dimensional Normal distributions in H with no regularization term in  X . Note that thanks to the nonlinearit y of  X , four comp onen ts can already capture the appearance of the original letter quite well. Finally , Figure 4 shows the reconstruction based on regularized Gaussian model with three principal com-ponen ts. Note that the recovered images are closer to the original than in Figure 3 and that the e X ect of tuning  X  is similar to smoothing in the image plane. 3.3. Computing the Bhattac haryy a Kernel It remains to put all the pieces together and compute the Bhattac haryy a kernel between p = N ( j  X  i ;  X ) and p = N ( j  X  0 i ;  X  0 ), where now  X = ^  X  and  X  0 = ^  X  0 . Recall that V  X  is the orthogonal complemen t of the nullspace of  X . It is easy to see that in (5) dimensions orthogonal to W = V  X   X  V  X  0 integrate out to 1, relieving us of the need to take determinan ts, etc., of in X nite dimensional forms: as in (4), the kernel is given by where j  X  y i =  X  1 2 j  X   X  1 i + 1 2 j  X  0 X  1 i  X   X  1 and j  X   X   X  1 j  X  i + 1 2  X  0 X  1 j  X  i . The subscript W denotes the matrix corresp onding to the restriction of the given form to the subspace W .
 The term h  X  j  X   X  1 j  X  i and its dashed coun terpart can be evaluated by expansion into linear combinations of centered and then uncen tered bras and kets, ulti-mately reducing it to a weighted sum of kernel eval-uations h x i j x j i =  X  ( x i ;x j ). The determinan t j  X  j is and similarly for j  X  0 j .
 The mixed determinan t and the mixed term in the exponen t require explicit construction of the matri-ces  X  W = [ h  X  i j  X  j  X  j i ] i;j and  X  0 W , where fj  X  orthonormal basis for W . It is easiest to construct this basis by starting with the basis of V  X  given by the eigen vectors j v l i and extending it one vector at a time by adding the eigen vectors of  X  0 and performing Gram-Sc hmidt orthogonalization. So far, we have not said anything about what the ele-ments of H actually are. We now show that the nat-ural interpretation is that they are functions over our original space, R n .
 Let us identify j x i =  X ( x ) with the function f x =  X  ( x;  X  ) and extend this linearly , j  X  i = P i c i j x x ;x 2 ;::: 2 R n and c 1 ;c 2 ;::: 2 R corresp onding to f P i c i f x i . In the continuous limit j  X  i = R c ( x ) j x i dx is identi X ed with f  X  = R c ( x ) f x dx . In the following, j  X  i and j f  X  i will be used interchangeably . The curious-looking prop erty h f  X  j f x i = construction the name of Repro ducing Kernel Hilbert Space, commonly abbreviated RKHS.
 For images, the interpretation of the above is par-ticularly clear. Supp ose that we are dealing with mono chrome images over the unit square, i.e. x 2  X = [0 ; 1] 2 . Each j  X  i is now a function f  X  : [0 ; 1] 2 7! R f ( x )= h  X  j f x i i.e., it is itself an image. The analog of the normal distribution for function spaces is the Gaussian Process. More precisely , a set of real valued random variables f y z : z 2 Z g for some index set Z is said to form a Gaussian Process G if for any z 1 ;z 2 ;:::;z k 2 Z , the marginal distribution p ( y z When Z  X  R n , it is natural to regard G as a distribu-tion over functions g : Z 7! R , g ( z ) = y z . An impor-tant prop erty is that by de X ning the mean E[ g ( z )] and covariance function Cov( g ( z ) ;g ( z 0 )), all the marginals, and hence G itself, is uniquely de X ned.
 The Gaussian Process concept meshes in naturally with the above RHKS point of view. Setting Z = X , re-placing the z 's with x 's and letting g ( x )= h  X  j x i = f makes G into a distribution on H . We see that the previously laboriously  X tted \generalized normal dis-tribution" p over H is nothing but a Gaussian Pro-cess with mean E[ f ( x )] = 1 k P i  X  ( x i ;x ) and covariance Cov( f ( x ) ;f ( x 0 )) = h f x j  X  reg j f x 0 i . Our kernel PCA-based procedure can then be inter-preted as  X tting a Gaussian Process to the sample of functions f f x tribution over functions, p ( f ) really encodes our beliefs of how similar each f is to the image whose pixels are x . The Bhattac haryy a kernel (5) then de X nes similar-ity between x and x 0 as the integral over all f of (the square root of) how similar x is to f and how similar f is to x 0 .
 In the Machine Learning literature, there is a long history of using Gaussian Processes as a compact Bayesian function learning tool by itself, without the need to invoke any outside estimation procedure (Zhu et al., 1997)(Mac kay, 1997). This metho d is based on the fact that a Gaussian Process prior updated with observ ations t ( x i ) = f ( x i ) +  X  (where  X  is ad-ditive Gaussian noise of known variance) gives rise to a posterior that is also a Gaussian Process.
 The question arises as to why we do not estimate p using this Bayesian approac h. The answ er is that al-though both metho ds yield GP estimators, they are fundamen tally di X eren t: whereas the \classical" GP procedure is a regression tool, our Kernel PCA-based procedure is a densit y estimator.
 The justi X cation for our estimation procedure is essen-tially the same as that for the traditional MLE estima-tor for Normal distributions. Given a sample of func-tions f f x Gaussian Process to generate these functions is that with mean E[ f ( x )] = 1 k P k i =1  X  ( x i ;x ) and covariance Cov[ f ( x ) ;f ( x 0 )] = 1 k P k i =1  X  ( x;x i )  X  ( x mator p is a regularized appro ximation to this GP, us-ing only the  X rst r comp onen ts of the covariance form. A potentially more satisfying Bayesian approac h to es-timating  X  and  X  that would also involve estimating r and  X  along the lines of (Zhu et al., 1997) remains the subject of further researc h. 5.1. Crosses and Squares To explore the robustness of the vector sets kernel to spatial variation, in a preliminary classi X cation ex-perimen t we generated 100 mono chromatic images of crosses and squares at various positions and scales in a 40  X  40 pixel  X eld (Figure 5).
 We trained a supp ort vector machine to separate the cross images from the square images using half the dataset for training and the other half for testing. As a baseline, we compare against the standard metho d of treating each image as a single vector in R 1600 to which we apply a conventional Gaussian RBF kernel. Figure 6 depicts the classi X cation accuracy as a func-tion of the SVM regularization parameter C . Multiple curves are shown for the various settings of  X  for the conventional RBF and for various settings of the anal-ogous  X   X  parameter in the Gaussian base kernel of our novel point set kernel. For regularization we keep the  X rst r =4 principal comp onen ts and use  X  =0 : 01, which were empirically found to be reasonable values. Clearly , provided that  X   X  is appropriate, the point set kernel can easily outp erform the traditional RBF. The latter is severely handicapp ed by the crosses and squares appearing in di X eren t parts of the  X gure be-cause it is only sensitiv e to coincidence of pixels and is unaware of the relativ e position of pixels. In contrast, the point set kernel can abstract shap e from position to some degree. 5.2. Handwritten Digits Towards comparing our kernel with common bench-marks in a familiar setting, we conducted experimen ts on an intentionally small dataset of handwritten dig-its, consisting of just the  X rst twenty examples of each of the digits 0 ; 1 ;:::; 9 from the NIST dataset. To test how well we can learn visual patterns from sparse, noisy examples, instead of the original images, we sam-pled 30 pixels from the foreground region of each im-age (intensit y greater than 191 on a 0 to 255 scale) and only presen ted the coordinates of these pixels to the algorithm.
 Experimen ts were performed by training on 120 im-ages and testing on the remaining 80, averaging the performance over 10 such random splits, thereb y on average giving 12 positiv e training examples for each class. Pursuing a simple one-v ersus-all strategy , sepa-rate Supp ort Vector Machines were built for each class, and in testing the predicted class was chosen to be the one with highest P i  X  i K ( x i ; x ) + b , where  X  i are the usual supp ort vector coe X cien ts and b is the bias term. Results are compared to the baseline of using a conven-tional RBF or a dot product kernel on the sparsi X ed images (Figure 7). Clearly , the performance of the point set kernel is very sensitiv e to the choice of  X   X  , but has the potential to far outp erform the baseline. As in the previous experimen t, no attempt has been made to optimize performance over r and  X  (10 and 0 : 1, respectiv ely): a more systematic study would set these parameters by cross-v alidation or by identifying a drop-o X  point (eigen-gap) in the spectrum of  X . We have prop osed a novel kernel that applies to a wide class of learning problems where instances can be rep-resen ted as sets of vectors. The kernel is de X ned as Bhattac haryy a's a X nit y between Gaussian models  X t-ted to the set. The resulting kernel becomes powerful when the whole procedure is \kernelized" by the in-troduction of a second kernel  X  , de X ned between ele-mentary vectors.
 The \bag of tuples" represen tation of instances is itself worthy of further exploration. In addition to explicit invariance to permutation, by treating all variables on the same footing, the base kernel can extend its favor-able smoothness prop erties to all variables. Contrast this with a conventional Gaussian RBF kernel between images, where ( x;y ) pixel coordinates are treated as indices and the kernel only behaves gracefully in in-tensit y. Such a traditional kernel has no concept of the metric structure of ( x;y ) and consequen tly behaves poorly under translation, rotation, etc..
 The choice of parametric model is essen tially con-strained to Gaussians by the dual requiremen ts of ker-nelizabilit y and the existence of a closed form form ula for Bahattac haryy a's a X nit y. On the other hand, the base kernel  X  can be chosen freely , making our metho d quite  X  X xible. Indeed, the restriction to sets of vectors in the title is unnecessary: the x i could come from any continuous or discrete set X on which a meaning-ful kernel can be de X ned. Another possible extension of this work is to apply our metho d recursiv ely to sets of sets. Finally , it migh t be possible to integrate each step of our procedure, including estimating r and  X  , into a single consisten t Bayesian operation. We would like to thank Lior Wolf, Patrick Ha X ner and the anon ymous referees for corrections and several im-portan t commen ts which have been integrated into the paper.
 Bhattac haryy a, A. (1943). On a measure of divergence between two statistical populations de X ned by their probabilit y distributions. Bull. Calcutta Math Soc. , 35 , 99{110.
 Collins, M., &amp; Du X y , N. (2002). Convolution kernels for natural language. Advanc es in Neur al Informa-tion Processing Systems 14 (pp. 625{632). Cam-bridge, MA: MIT Press.
 Cortes, C., Ha X ner, P., &amp; Mohri, M. (2003). Rational kernels. Advanc es in Neur al Information Processing Systems 15 . Cam bridge, MA: MIT Press.
 Dirac, P. A. M. (1930). The principles of quantum mechanics . Oxford Univ ersity Press.
 Haussler, D. (1999). Convolution kernels on dis-crete structur es (Technical Report UCSC-CRL-99-10). Departmen t of Computer Science, Univ ersity of California at Santa Cruz.
 Jaakk ola, T., &amp; Haussler, D. (1999). Exploiting gener-ative models in discriminativ e classi X ers. Advanc es in Neur al Information Processing Systems 11 . Cam-bridge, MA: MIT Press.
 Jebara, T. (2003). Convex invariance learning. Ninth
International Workshop on Arti X cial Intelligenc e and Statistics .
 Jebara, T., &amp; Kondor, R. (2003). Bhattac haryy a and expected likeliho od kernels. Proceedings of the Six-teenth Annual Confer ence on Learning Theory and Seventh Kernel Workshop . In press.
 Kondor, R., &amp; La X ert y, J. (2002). Di X usion kernels on graphs and other discrete input spaces. Ma-chine Learning: Proceedings of the Ninete enth In-ternational Confer ence (ICML '02) .
 La X ert y, J., &amp; Lebanon, G. (2003). Information di X u-sion kernels. Advanc es in Neur al Information Pro-cessing Systems 15 . Cam bridge, MA: MIT Press. Leslie, C., Eskin, E., Weston, J., &amp; Noble, W. S. (2003). Mismatc h string kernels for SVM protein classi X action. Advanc es in Neur al Information Pro-cessing Systems 15 . Cam bridge, MA: MIT Press. Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristian-ini, N., &amp; Watkins, C. (2002). Text classi X cation using string kernels. Journal of Machine Learning Research , 2 , 419{444.
 Mackay, D. J. C. (1997). Gaussian processes:
A replacemen t for neural networks? Tutorial at the Tenth Annual Confer ence on Neur al In-formation Processing Systems . Available from http://wol.ra.phy.cam.ac.uk/pub/mackay/ .
 Sch X olkopf, B., &amp; Smola, A. J. (2001). Learning with kernels: Supp ort vector machines, regularization, optimization and beyond . Cam bridge, MA: MIT Press.
 Sch X olkopf, B., Smola, A. J., &amp; M X  uller, K.-R. (1998).
Nonlinear principal comp onen t analysis as a kernel eigen value problem. Neur al Computation , 10 , 1299{ 1319.
 Vishwanathan, S. V. N., &amp; Smola, A. J. (2003). Fast kernels for string and tree matc hing. Advanc es in
Neur al Information Processing Systems 15 . Cam-bridge, MA: MIT Press.
 Watkins, C. (2000). Dynamic alignmen t kernels. In
A. J. Smola, B. Sch X olkopf, P. Bartlett, and D. Schu-urmans (Eds.), Advanc es in kernel metho ds . Cam-bridge, MA: MIT Press.
 Wolf, L., &amp; Shash ua, A. (2003). Kernel principal an-gles for classi X cation machines with applications to image sequence interpretation. IEEE Conf. on Com-puter Vision and Pattern Recognition (CVPR) . Zhu, H., Williams, C. K. I., Rohwer, R., &amp; Morciniec,
M. (1997). Gaussian regression and optimal  X -nite dimensional linear models (Technical Report
NCR G/97/011). Aston Univ ersity, Neural Comput-
