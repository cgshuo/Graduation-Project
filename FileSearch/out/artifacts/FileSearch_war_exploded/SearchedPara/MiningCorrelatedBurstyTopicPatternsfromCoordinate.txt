 Previous work on text mining has almost exclusiv ely focused on a single stream. However, we often have available mul-tiple text streams indexed by the same set of time points (called coordinate d text streams ), whic h o er new opp ortu-nities for text mining. For example, when a ma jor event happ ens, all the news articles published by di eren t agen-cies in di eren t languages tend to cover the same event for a certain perio d, exhibiting a correlate d bursty topic pattern in all the news article streams. In general, mining corre-lated burst y topic patterns from coordinated text streams can rev eal interesting laten t asso ciations or events behind these streams. In this pap er, we de ne and study this novel text mining problem. We prop ose a general probabilistic algorithm whic h can e ectiv ely disco ver correlated burst y patterns and their burst y perio ds across text streams even if the streams have completely di er ent vocabularies (e.g., English vs Chinese). Evaluation of the prop osed metho d on a news data set and a literature data set sho ws that it can e ectiv ely disco ver quite meaningful topic patterns from both data sets: the patterns disco vered from the news data set accurately rev eal the ma jor common events cov-ered in the two streams of news articles (in English and Chinese, resp ectiv ely), while the patterns disco vered from two database publication streams matc h well with the ma-jor researc h paradigm shifts in database researc h. Since the prop osed metho d is general and does not require the streams to share vocabulary , it can be applied to any coordinated text streams to disco ver correlated topic patterns that burst in multiple streams in the same perio d.
 Categories and Sub ject Descriptors: H.3.3 [Informa-tion Searc h and Retriev al]: Clustering, Text Mining General Terms: Algorithms Keyw ords: Correlated burst y patterns, coordinated streams, clustering, reinforcemen t.
 Cop yright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
Text streams are ubiquitous and are often naturally formed as new information is incremen tally created and accum u-lated. For example, newswires publish news articles every-day on the Web to rep ort new events to users, generating news streams in di eren t languages suc h as English, Chi-nese, and Spanish. Searc h engines accept and answ er end users' queries from all over the world con tinuously , creat-ing streams of queries. Researc hers publish scien ti c pap ers year by year, forming literature streams. Blog authors regu-larly publish blog articles, forming a dynamic stream of blog articles.

One interesting characteristic of a text stream is that there is often an intensiv e coverage of some topic within a certain perio d, whic h we refer to as a bursty topic pattern . For ex-ample, when a ma jor event happ ens in the world, all news articles tend to have intensiv e coverage of the event; as a re-sult, there would be a coverage burst of the topic lasting for a certain perio d. Similarly , when a new researc h direction is opened up in a researc h eld, man y publications in the new direction tend to be generated, again forming a burst y pattern about the researc h topic. Mining suc h burst y topic patterns can help rev eal the underlying events and has po-ten tially man y applications, suc h as monitoring opinions, analyzing trends, and summarizing the ma jor topics in a text stream.

So far, text mining researc h has almost exclusiv ely focused on mining one single text stream. For example, the Topic Detection and Tracking (TDT) work [5, 24, 23, 4] has fo-cused on detecting new events and trac king kno wn events in a single news article stream. Other work on extracting burst y patterns has also focused on only one single stream (see, e.g., [18, 19, 12, 8, 15, 14]). However, we often have available multiple related text streams indexed by the same set of time points (called coordinate d text streams ), whic h o er new opp ortunities for text mining. In particular, we may disco ver correlate d bursty topic patterns from multiple coordinated text streams. A correlated burst y topic pattern refers to sim ultaneous bursting of some related topics in all the text streams; it is often asso ciated with some underly-ing event that has in uenced the generation of all the text streams involved.

For example, when a ma jor event happ ens, all the news articles published by di eren t agencies in di eren t languages tend to cover the same event for a certain perio d, exhibit-ing a correlated burst y topic pattern in all the news article streams. Exploiting multiple streams to detect the laten t events would be more accurate than using only one single stream as the latter may not be able to distinguish a global event from a local event, leading to mixed mining results. Also, when there is a ma jor shift in researc h paradigm in a eld, all the journals or conferences in the eld will likely have a coverage burst of the new researc h paradigm; again, mining multiple journals or conferences can reco ver the re-searc h paradigm shift more accurately than using only one single publication source.

In general, mining correlated burst y topic patterns from coordinated text streams is quite interesting for sev eral rea-sons: (1) It can help disco ver interesting common (causal) events that have in uenced all the streams. (2) It can re-veal interesting asso ciations and link ages between the in-volved streams. (3) It can help disco ver \local" (i.e., stream-speci c) patterns more accurately by factoring out the \global noise." For example, iden tifying correlated burst y topic pat-terns from news streams in di eren t natural languages suc h as English and Chinese can not only rev eal the same ma jor events covered by both streams, but also create asso ciations of English terms and Chinese terms; suc h asso ciations would be very useful for cross-lingual information retriev al, inte-gration, and summarization [22, 20]. The disco vered com-mon events in multiple news streams can also facilitate dis-covery of local events speci c to one stream (e.g., Chinese news) whic h would otherwise have be mixed with the com-mon/global events.

In this pap er, we de ne and study the novel problem of mining correlated burst y topic patterns from multiple co-ordinated text streams. We prop ose probabilistic mixture mo dels whic h can iden tify the burst y patterns and their burst y perio ds from coordinated streams sim ultaneously even if the streams have completely di eren t vocabularies (e.g., English and Chinese). The basic idea of our approac h is to introduce a laten t cause variable to mo del the underlying events to be disco vered and mo del the text data in mul-tiple streams with a mixture mo del involving multinomial comp onen t topic mo dels. Eac h topic mo del is a word distri-bution with the high probabilit y words indicating the topic con ten t. We do not require the multiple streams to share the same vocabulary; instead, we rely on the correlation be-tween the time distributions of topics to \align" topics from multiple streams. Thus the prop osed metho ds can actually be applied to any discrete data streams, though we have only evaluated it using text streams in this pap er. By t-ting suc h a mo del to the available text streams using the Exp ectation-Maximization (EM) algorithm, we can obtain the topic mo dels asso ciated with eac h value of the laten t cause variable; these topic mo dels together with their peak-ing time perio ds are tak en as the correlated burst y topic patterns that we want to disco ver.

We further prop ose two extensions to this basic mining approac h: (1) We incorp orate local dep endency (along the time line) into the mixture mo del to further favor a topic mo del that can explain well all the documen ts in a con-secutiv e time perio d. This allo ws us to disco ver consecu-tive burst y perio ds. (2) We prop ose a mutual reinforce-men t metho d whic h allo ws multiple streams to work to-gether to further impro ve the qualit y of the iden ti ed cor-related burst y patterns by selecting terms that truly have strong global correlations across all the streams.
We test the prop osed metho ds on two data sets { news streams and literature streams. Exp erimen t results sho w that the prop osed metho ds can e ectiv ely disco ver quite meaningful topic patterns from both data sets: the patterns disco vered from the news data set can accurately rev eal the ma jor common events covered in the two streams of news articles (in English and Chinese, resp ectiv ely), while the patterns disco vered from two database publication streams matc h well with the ma jor researc h paradigm shifts in database researc h. The prop osed two extensions (i.e., local dep en-dency and mutual reinforcemen t) are also both e ectiv e for further impro ving the qualit y of disco vered patterns.
The rest of the pap er is organized as follo ws. We rst review the related work in Section 2. Then we de ne the problem of mining coordinated streams in Section 3 and presen t the prop osed mining metho ds in Section 4. We re-port the exp erimen tal results in Section 5 and conclude in Section 6.
Our work is related to sev eral lines of work in text min-ing, stream data mining, and multilingual natural language pro cessing.

First, the work on Topic Detection and Tracking (TDT) [4, 5, 24, 23] all aims to detect and trac k events from a stream of news stories, thus is related to our work. However, this body of work all considers a single news stream and does not address the issue of multiple subtopics within a news arti-cle. Our work is more related to the retrospective version of TDT [24] where the whole stream is analyzed. A main di erence between our work and the TDT work is that we consider multiple coordinated text streams and mine corre-lated burst y topic patterns.

Second, burst y patterns or events are recen tly studied[18, 19, 12, 8]. In [12], an in nite automaton was prop osed to iden tify burst y features and their burst y structures; it has been used in [13] to iden tify the burst y evolution of blogspace. However, the work is restricted in only iden ti-fying burst y features one by one and does not group the features to nd interesting topic patterns. In [18, 19] and [8], burst y features are iden ti ed heuristically with multi-ple steps. For example, in [18, 19], for eac h named entity and noun phrase in the stream, 2 tests are performed to iden tify the days in whic h the test scores are higher than a threshold. In [8], binomial distributions are calculated to iden tify the burst y features. All these metho ds pro cess the features one by one and only a single stream is analyzed. One problem of suc h metho ds is that the results are often quite sensitiv e to some noisy features whic h may be inciden-tally burst y and the burst y pattern is not meaningful. Our metho d is more robust since it iden ti es burst y patterns by pooling together man y words whic h share similar patterns. Furthermore, in [18, 19] and [8] it is sho wn to be dicult for their metho ds to nd long consecutiv e time perio ds. In con-trast, our metho ds (esp ecially the local dep endency mo del) can help nd long consecutiv e perio ds of burst y patterns.
Data streams and time-series data are extensiv ely studied in the database and data mining comm unities [9, 1]. Muc h of the emphasis there is on similarit y searc h, whic h is to nd similar time-series sequences given a time-series query (e.g., [3, 21]), and on classi cation or incremen tal clustering of data streams (e.g.,[11, 2]).

Temp oral information has also been used to iden tify se-man tically similar searc h engine queries [7], to integrate mul-tilingual information [20], and to acquire lexical asso ciations for transliteration and translation [17]. We prop ose a mix-ture mo del for coordinated text streams with temp oral in-formation. It is an extension of Probabilistic Laten t Seman-tic Analysis (PLSA) [10] and is also related to some other recen t extensions suc h as [25, 16].
In this section, we formally de ne the problem of mining correlated burst y topic patterns from multiple coordinated text streams. We rst de ne text stream.

Definition 1 (Text Stream) . A text stream S of length n and with vocabulary V is an ordered sequenc e of text sam-ples ( S 1 ; S 2 ; :::; S n ) indexe d by time, wher e S i of wor ds from the vocabulary set V at time point i .
For example, in a news article stream S , S i could be a concatenation of all the news articles published on date i , while in a searc h engine query log stream S 0 , S 0 i could be all the queries sen t to the searc h engine on date i .
Definition 2 (Coordina ted Text Streams) . A set of text streams is called coordinate d text streams if all the streams shar e the same time index and have the same length. Formal ly, a set of m coordinate d text streams with length n is with vocabulary V i . S ij is the text sample at time point j in the i -th stream, thus it consists of a sequenc e of wor ds from V .

Note that we allo w eac h stream S i to have a poten tially distinct vocabulary set V i ; this allo ws us to con venien tly mo del text streams in di eren t natural languages suc h as English, Spanish, and Chinese.

In order to de ne the concept correlated burst y topic pat-tern, we rst de ne topic.

Definition 3 (Topic) . A topic in stream S i is de ne d as a probability distribution of wor ds in vocabulary set V We also call such a wor d distribution a topic model.
Using a word distribution (i.e., unigram language mo del) to represen t a topic has been quite common in text mining (see e.g. [10, 6, 25, 16]). Intuitiv ely, a topic mo del would assign high probabilities to those words that can charac-terize the topic well. For example, the topic mo del about the 9{11 terrorist attac k may have high probabilities for words suc h as \attac k", \terror", \terrorist", \Afghan", and \Bin Laden", but very small probabilities for words suc h as \Olympic", \game", \sp ort", and \swimming", whereas the topic mo del about an Olympic swimming event would likely be the opp osite.

While any topics that we can disco ver from a text stream would be interesting, we are particularly interested in a spe-cial kind of topics whic h we refer to as \burst y topics." These are topics that are covered intensiv ely within a rel-ativ ely long consecutiv e time perio d in a stream. Burst y topics are interesting because they tend to be asso ciated with some ma jor events, and disco vering suc h events is our goal. We now formally de ne a burst y topic.

Definition 4 (Bursty Topic) . Let be a topic (mo del) in stream S i . Let t 2 [1 ; n ] be a time index variable and p ( j t; S i ) be the relative cover age of the topic at time t in stream S i . is a bursty topic in stream S i if 9 t 1 ; t such that t 2 t 1 and 8 t 2 [ t 1 ; t 2 ] , p ( j t; S i ) wher e is a span threshold and is a cover age threshold. Intu-itively, the rst condition ensur es that the topic is cover ed in the stream for a relatively long conse cutive perio d; the second ensur es that the cover age of the topic is relatively intensive.
Finally , we de ne a correlated burst y topic pattern, whic h is a set of topics (eac h from a di eren t stream) that are burst y during the same time perio d. Formally , Definition 5 (Correla ted Bursty Topic Patten) .
 A correlate d bursty topic pattern in a set of coordinate d text streams S = fS 1 ; :::; S m g is de ne d as a set of top-ics f 1 ; :::; m g such that i is a bursty topic in stream S and 9 t 1 ; t 2 2 [1 ; n ] such that t 2 t 1 and 8 t 2 [ t 8 i 2 [1 ; m ] , p ( i j t; S i ) wher e is a span threshold and is a cover age threshold.

According to these de nitions, the problem of mining cor-related burst y topic patterns from a set of coordinated text streams mainly involves three challenges: (1) We need to disco ver burst y topics from eac h stream. As we will sho w later, a straigh tforw ard application of existing approac hes to topic disco very cannot e ectiv ely detect topics that are burst y. (2) We need to locate the burst y perio d of a burst y topic. The coverage of a burst y topic may be unev en and unsmo oth even during the burst y perio d, making it a chal-lenge to accurately detect the burst y perio d boundaries. (3) Since the topics forming a correlated burst y topic pattern must be burst y during the same perio d in multiple streams, we need to coordinate the disco very of burst y topics in all the streams to more e ectiv ely focus on the truly correlated topics. In particular, the disco very of topics in one stream should pay atten tion to whic h perio d other streams suggest to be promising for nding a correlated burst y topic pattern.
In this section, we describ e our coordinated mixture mo del to disco ver correlated burst y topic patterns from coordi-nated text streams. Figure 1: Examples of burst y words related to 9{ 11 event in the news data. x -axis denotes the time points and y -axis is the relativ e frequency .

The basic idea of our approac h is to align the text samples from di eren t streams based on the shared time stamps and disco ver topics from multiple streams sim ultaneously with a single probabilistic mixture mo del. Recen t work suc h as [10, 6, 16] has sho wn that probabilistic mixture mo dels are quite e ectiv e for disco vering topics from text. Their basic idea is to represen t a topic by a word distribution and assume that a text collection is \generated" by rep eatedly sampling words from a mixture of multiple topic distributions. By tting the mixture mo del to the text data, we can then obtain an estimate of eac h word distribution, whic h we can tak e as a disco vered topic. Di eren t metho ds di er in the way of mixing the word distributions and estimating the parameters.

A straigh tforw ard way of applying suc h a metho d to our problem would be to use a mixture mo del to disco ver topics from eac h stream and then try to matc h the topics across streams in hop e of detecting some topics that happ en to burst during the same perio d. However, there are two prob-lems with this simple approac h: (1) We will need to matc h topics across di eren t streams, whic h is dicult because the vocabularies of di eren t streams do not necessarily overlap. (2) The topics disco vered in eac h stream may explain the corresp onding stream well but not necessarily matc h the common topics shared by multiple streams. Indeed, a shared topic may not t a speci c stream so well as some varian t of the topic.

This analysis suggests that we should someho w mak e these mixture mo dels designed for di eren t streams \comm uni-cate" with eac h other so that they would all focus more on disco vering the common topics shared by all streams. We achiev e this goal by aligning the text samples from di eren t streams based on their common time stamps and t all the streams with a single mixture mo del. Speci cally , we would merge the text samples on the same time point (keeping their stream iden tities) to form a uni ed text sample on the time point. In order to matc h topic mo dels across di er-ent streams, we also align the topic mo dels from di eren t streams; again, we keep their stream iden tities. Since we have kept the stream iden tities of both the text sample and the topic mo del, we can t the righ t mo del to the righ t data when tting the whole mixture mo del to all the streams.
We call suc h a mixture mo del a coordinate d mixtur e model because the mixture mo dels for all streams \co ordinate" with eac h other so that eac h would focus more on topics that have strong correlations with topics in other streams. After we t suc h a coordinated mixture mo del to all the streams, we will obtain all the topic mo dels. Since the topic mo dels from di eren t streams are already aligned with eac h other in adv ance, we naturally obtain a correlated burst y topic pattern if all the involved topics are burst y in a simi-lar perio d.

Intuitiv ely, eac h topic mo del (i.e., word distribution) de-nes a soft cluster in the sense that it speci es the probabil-ity of mem bership of eac h word in the cluster, and our mix-ture mo del would group words based on their co-o ccurrences in samples of the same stream to form word clusters and matc h clusters across streams based on the correlations be-tween the temp oral distributions of the clusters from di er-ent streams.

Note that our mo del does not require di eren t streams to share any vocabulary; instead it exploits the fact that topics involved in a correlated burst y topic pattern tend to have similar temp oral distribution to matc h the topics from di eren t streams. In Figure 1, based on the news data set used in our exp erimen t (see Section 5), we sho w strong cor-relations of the relativ e frequency (i.e., frequency of a word in eac h time point normalized by its total frequency) distri-butions over time between two English words \terror" and \attac k" as well as between the English word \attac k" in an English news stream and its Chinese translation in a Chinese stream. In general, when the multiple streams share some common causal factors (e.g., a ected by the same event), we will observ e suc h correlations.
We now give the formal de nition of the prop osed coor-dinated mixture mo del.
Let S = fS 1 ; :::; S m g be m coordinated text streams with vocabularies V 1 ; :::; V m . Without loss of generalit y, we as-sume there are k correlated burst y topic patterns in our streams and asso ciate a laten t cause variable z 2 [1 ; k ] with them; a di eren t value of z would indicate a di eren t pat-tern. Giv en a value j of z , we have a set of \aligned" topic mo dels, eac h corresp onding to a single stream. The topic mo del for stream S i is given by P ( w j z = j; i ) where w 2 V That is, the f P ( w j z = j; i ) g i 2 [1 ;m ] de nes a poten tial corre-lated burst y topic pattern.

In order to explain all the words in our streams, including those that are not involved in any correlated burst y topic pattern, we further introduce a bac kground topic mo del P ( w j B ; i ) for eac h stream S i .

In general, we assume that a word w app earing at time t in stream S i with probabilit y P ( w j t; i ) (i.e., w is a word in text sample S it ) can either be a bac kground word (thus should be generated using the bac kground mo del) or poten tially cover any of the k patterns (thus should be generated from a mixture of the k pattern mo dels). In other words, w would be regarded as a sample dra wn from the follo wing mixture mo del:
P ( w j t; i ) = B P ( w j B ; i )+(1 B ) where B is the mixture weigh t of the bac kground mo del, and P ( z j t ) is the probabilit y of choosing pattern z at time point t .

The log-lik eliho od of generating text sample S it is thus where c ( w; S it ) is the coun t of word w in S it . Therefore, the log-lik eliho od of generating all the m coordinated streams is:
Our coordinated mixture mo del can be regarded as an ex-tension of Probabilistic Laten t Seman tic Analysis (PLSA) [10] to mo del coordinated streams. Note that the P ( w j t; i )'s for di eren t streams are coordinated because of the common variable t , and P ( z j t ), whic h is indep enden t of any stream, forcing all the streams to have the same preferences for the burst y topic patterns. On the other hand, P ( w j t; i ) is dif-feren t for a di eren t stream, allo wing us to mo del di eren t vocabularies.
We estimate the parameters of the coordinated mixture mo del by tting the mo del to our coordinated text stream data. To mo del the bac kground words in our streams (from some prior kno wledge) and regularize our mo del, we x our B to a constan t and set where c ( w; S i ) is the coun t of word w in stream S i . The remaining parameters to estimate are P ( w j z; i ) and P ( z j t ). Without assuming any prior kno wledge, we may use the maxim um likeliho od estimator and use the exp ectation-maximization (EM) algorithm to compute an estimate iter-ativ ely. The exp ectation step is to calculate: The maximization step is to update the probabilities:
One de ciency of the basic coordinated mixture mo del is not capturing the dep endency among the consecutiv e time points in covering topics. Speci cally , eac h text sample mak es its own, indep enden t choice among the possible top-ics to cover. Intuitiv ely, however, the text samples within a consecutiv e time perio d tend to be in uenced by the same event(s). So it would be desirable to someho w force all of them to mak e similar choices of topics.
 To implemen t this intuition, we prop ose to mo dify the EM algorithm to imp ose a temp oral dep endency constrain t on P ( z j t ) so that during eac h iteration P ( z j t ) would be smo othed (constrained) by both its neigh bors P ( z j t 1) and P ( z j t + 1): Here Q ( l +1) ( j t ) is the original form ula in EM iteration. The parameter is to con trol the amoun t of smo othing. A larger would imp ose a stronger dep endency constrain t among adjacen t time points. When = 0, we imp ose no constrain t and have P ( l +1) ( j t ) = Q ( l +1) ( j t ). We found in our exp erimen ts that introducing a non-zero can smo oth a burst y pattern and help disco ver consecutiv e burst y perio ds.
Although the disco very of correlated burst y patterns is coordinated across streams through the shared time points, the disco vered topic mo dels in eac h stream (i.e., P ( w j z; i )) can be biased by some stream-sp eci c local themes, whic h may peak at about the same time as the true correlated burst y topic patterns. As a result, P ( w j z; i ) may have mixed subtopics. To \clean up" P ( w j z; i ) and mak e it more focused on the correlated topics across all other streams, we prop ose to use a reinforcemen t metho d to rew ard words from di er-ent streams that are strongly correlated with eac h other over the entire time span.

The basic idea of this approac h is to exploit the fact that those words in di eren t streams about the same common causal factor can be exp ected to have strong correlations between their frequency distributions over time, whereas a noisy \local word" in a stream is unlik ely to have strong correlations with them. Thus we can use the corresp onding topic mo dels in other streams (i.e., f P ( w j z; j ) g j 6 = i lter out \local noise" in P ( w j z; i ). Speci cally , we would adjust P ( w j z; i ) to promote words that are highly correlated with high probabilit y words in all other streams. We itera-tively do suc h adjustmen t for all the topic mo dels.
To implemen t this idea, we rst compute the global corre-lations between all the high probabilit y words in one stream and those in another, where the probabilit y of a word is given by the corresp onding topic mo del (i.e., P ( w j z; i )). Follo wing [20], we represen t eac h word from stream S i by the normal-ized empirical frequency distribution vector over the entire time span of the stream: We can then compute the Pearson correlation coecien t r ( x; y ) between two words x and y as follo ws: where x i and y i are i -th entries in x and y 's frequency dis-tribution vectors de ned above.

With these correlation values, we then use the follo wing iterativ e pro cedure to adjust eac h P ( w j z; i ):
Intuitiv ely, suc h a mutual reinforcemen t pro cedure re-wards the high probabilit y words in stream i (according to P ( w j z; i )) whic h have high correlations with the high prob-abilit y words in its correlated topic mo del of other streams P ( w 0 j z; j )'s.

To increase the eciency , we can perform mutual rein-forcemen t updating on only the words with high probabili-ties according to P ( w j z; i ) and let the iteration stop after a few iterations. (In our exp erimen ts, the ranking of words ac-cording to the updated probabilities usually becomes stable after 10 iterations.)
After we get all the parameters P ( w j z; i ) and P ( z j t ) esti-mated, we can disco ver the correlated burst y topic patterns directly . Since P ( z j t ) is stream indep enden t, given a span threshold and a coverage threshold , an iden ti ed pat-tern is a correlated burst y topic pattern if P ( z j t ) satis es the constrain ts in De nition 4 of burst y topic pattern.
Intuitiv ely, P ( z j t ) represen ts the strength of pattern z at time t and the high probabilit y words according to P ( w j z; i ) characterize the con ten t of burst y pattern z in stream i . In the exp erimen t, we use P ( z j t ) and P ( w j z; i ) to represen t the iden ti ed correlated burst y topic patterns. For exam-ple, in the correlated news streams in di eren t languages where a correlated burst y topic corresp onds to a real world event, P ( z j t )'s are the intensiv eness of the corresp onding event over time, whic h can be used to iden tify the burst y perio d, and P ( w j z; i )'s indicate what the event is about in di eren t languages.
The prop osed metho d can work with quite large text col-lections. Eac h iteration in the EM algorithm has complexit y O ( m i =1 jS i j k + m i =1 j V i j k + n k ). For the mutual reinforcemen t, we only need to calculate the top N words of eac h stream given a pattern. Eac h iteration of reinforce-men t has complexit y O ( N N k ). In practice, N = 100 is enough since the words outside of top 100 have very low probabilities.

Although presen ted as a mo del on text streams with words as units, our mo del is quite general and can be applied to any discrete data streams with any interesting features as units.
To evaluate our metho ds of iden tifying ma jor correlated burst y topic patterns, we conduct exp erimen ts on a news data set and a literature data set. The correlated burst y topic patterns in news streams are highly related to ma-jor real world events and the patterns in literature streams can indicate researc h paradigm shifts. In this section, we sho w that our prop osed metho ds can iden tify meaningful correlated burst y topic patterns from these two types of co-ordinated streams to rev eal the ma jor real world events and researc h paradigm shifts with appropriate time line. Our news streams consist of six mon ths' news articles of Xinh ua English and Chinese newswires dated from June 8th, 2001 through November 7th, 2001. There are altogether 43,488 documen ts in Chinese and 34,751 documen ts in En-glish distributed in the 148 days. Eac h day is used as a time point in these two streams thus the length of the coordinated streams is 148.

In eac h stream, a text sample with time stamp i is the concatenation of all news articles app earing on date i . For Chinese news articles, there is no \space" between two Chi-nese words and the meaning of eac h single Chinese char-acter can be interpreted quite di eren tly dep ending on its con text. The ambiguit y of Chinese characters can be allevi-ated much by grouping them into bi-grams. Thus, without using sophisticated Chinese segmen tation tools, we use bi-grams of Chinese characters as words in the Chinese news stream. Therefore, at eac h time point (i.e., day), eac h news stream has a sequence of the aforemen tioned words whic h app ear in the corresp onding stream at that time point. As we will sho w, even with suc h crude segmen tation, the min-ing results are already quite interesting. Naturally , with a better segmen tation tool, our mining results can be further impro ved.
The data set in literature domain we use is similar to the one used in [12]. Speci cally , we use all the pap er titles of SIGMOD and VLDB from the years 1975{2005 in our exp er-imen t. SIGMOD and VDLB are two ma jor conferences in database researc h comm unit y, whic h have existed for more than 30 years since 1975. Publications accum ulated in the two conferences over years naturally form coordinated text streams. In this data set, eac h year is treated as a time point and thus the length of the coordinated streams is 31.
The statistics of the news and literature data sets are sho wn in Table 1. Besides in di eren t genres, the two data sets are clearly di eren t in statistic measures. We use these two data sets to test the generalit y of our prop osed metho ds.
In the coordinated mixture mo dels, there are sev eral user-input parameters whic h pro vide exibilit y for burst y topic pattern analysis. These parameters are set empirically , as in principle, it is imp ossible to optimize these parameters without relying on domain kno wledge. We discuss the e ect of di eren t parameters as follo ws.

Parameter B is to con trol the strength of the bac kground mo del. The bac kground mo del captures global common words in eac h stream. A larger B will force disco vered burst y patterns to be more discriminativ e from eac h other but an extremely large B will attract too much useful in-formation to the bac kground mo del thus weak en the inter-pretabilit y of the disco vered patterns. Empirically , a suitable for text documen ts is between 0.9 and 0.95 and we use B = 0 : 95 in our exp erimen ts.

Parameter is to con trol the dep endency strength among adjacen t time points in the streams. A high forces a high dep endency , i.e., assumes that adjacen t time points have very similar burst y topic patterns. A extreme lower value suc h as 0 can not fully utilize the consecutiv e prop erty of data streams. In principle, can be set based on the overall similarities among adjacen t time points. Empirically , we set = 0 : 1 in the follo wing exp erimen ts.

Parameter k is the num ber of burst y patterns shared by all coordinated streams in a data set. When no domain kno wledge is available as in our exp erimen ts, we use a similar strategy as [15] to determine the num ber of burst y patterns by enumerating multiple possible values of k and drop the patterns whic h do not satisfy our burst y pattern de nition.
On the news streams, we rst apply our coordinated mix-ture mo del to iden tify the ma jor correlated burst y topic pat-terns and then use our mutual reinforcemen t metho d across streams to further re ne the iden ti ed patterns. Burst y pat-terns whic h satisfy = 5 for = 0 : 01 are kept. We nally obtain 7 ma jor burst y topic patterns, whic h are sho wn in Figure 3. Recall that eac h pattern has its burst y perio d and a set of high probabilit y words. We sho w both results in Figure 3(a) and Figure 3(b) resp ectiv ely. In Figure 3(a), we plot the strength of these 7 patterns over time points indicated by probabilit y P ( z j t ) whic h is estimated from our mixture mo del. In Figure 3(b), we list the top 10 words with highest probabilities in eac h pattern mined from both En-glish and Chinese news streams. For eac h Chinese bi-gram, we include its English translation in the paren thesis after itself. Note that Chinese bi-grams are not alw ays complete Chinese words. We use \*" to indicate that a bi-gram is only a fragmen t of our translation.

Figure 3 gives us a good, uni ed summary of the two streams by the 7 iden ti ed burst y topic patterns. The 1st pattern is about Comm unist Party of China (CPC) and has a sharp peak around July 1st, 2001, whic h is the 80th an-niv ersary of CPC. The 2nd pattern is on the bid of Olympic 2008 and bursts from July 13th, 2001 when Beijing, the Cap-ital of China, won the bid of Olympic 2008. The 3rd pattern is the 9th swimming championship held in Fukuok a in 2001. The 4th pattern is the shrine event in Japan and the 5th pattern is the 21st Univ ersiade held in Beijing during Aug. 22nd to Sep. 1st, 2001. 9{11 event is the 6th pattern in the gure. From Figure 3(a), we can see this pattern accurately bursts at Septem ber 11th, 2001. The set of high probabilit y words suc h as \terror" and \attac k" are very informativ e in both English and Chinese. The last pattern iden ti ed by our algorithm is Afghanistan war whic h happ ened consequen tly after the 9{11 event. Though both 9{11 and Afghanistan war are related to terrorists, they are two di eren t events and our metho ds are able to distinguish them correctly .
The results above sho w that our prop osed metho ds can successfully iden tify correlated burst y topic patterns in co-ordinated news streams. It could have man y applications. For example, most of the iden ti ed English and Chinese words, although in completely di eren t vocabularies, matc h very well. This can poten tially help cross-lingual informa-tion retriev al and integration [22, 20]. The burst y perio d of a pattern can be used to analyze the trend of the cause event. Furthermore, com bining the iden ti ed burst y words and perio ds together is informativ e enough to give a uni ed summary of the coordinated text streams.
Figure 4: Result of documen t-based clustering
We apply our metho ds to the SIGMOD and VLDB stream data, and the ma jor burst y patterns whic h satisfy = 2 for = 0 : 01 are sho wn in Figure 2. The iden ti ed patterns summarize the researc h paradigm shifts interestingly: at the beginning, the database comm unit y focused on relational data mo del (1st pattern: 1975-1985); then the researc h focus changed to deductiv e and object-orien ted databases (2nd pattern: 1986-1991); after that, man y researc hers began to study parallel databases while the object-orien ted database researc h stayed as a ma jor topic (3rd pattern: 1992-1995); data mining and web related topics became popular after 1996 (4th pattern: 1996-2001); in recen t years, XML and stream data managemen t became dominan t together with Web related topics (5th pattern: 2002-2005).

In a whole, we can see that all the iden ti ed paradigm shifts can re ect well the real progress in the database com-munit y. This result can not only pro vide a big picture of the database eld to a novice, but also assist an exp ert in writing overviews about the whole eld.
In this section, based on the news streams, we compare our prop osed metho ds to directly applying PLSA-based doc-umen t clustering metho d [10, 25] and analyze di eren t vari-ants of our metho ds.
To disco ver burst y patterns, a straigh tforw ard metho d would be to apply documen t based clustering metho ds whic h have been widely used to disco ver topics in a documen t col-lection. However, as will be sho wn, these metho ds are not e ectiv e in detecting \burst y" patterns. In this exp erimen t, we use the English stream and treat eac h English article as a documen t. We use PLSA based clustering metho d prop osed in [10, 25] to group articles into clusters. The strength of a cluster in eac h day is calculated as in [15] and is prop or-tional to the num ber of documen ts in the cluster whic h ap-pears on that day. In Figure 4, we sho w the strength of two biggest clusters iden ti ed by PLSA. The rst cluster is about \sto ck" and it has high probabilit y words suc h as \sto ck", \dollar", and \millions". The second cluster is about the pollution in China and it has high probabilit y words suc h as \pollute", \water", and \protect". From their strength over time plotted in Figure 4, we can see that neither cluster is a meaningful burst y pattern as they occur rather evenly in the whole time perio d. This sho ws that documen t-based cluster-ing metho ds are biased by these \common" topics and are ine ectiv e for detecting burst y patterns. One reason why this is so is because suc h a metho d does not utilize the time information in the stream. In con trast, our metho d utilizes time information and can detect burst y patterns as sho wn in Figure 3.
Another possible metho d for nding correlated burst y topic patterns is to rst nd bust y patterns in eac h single stream and then align patterns across streams according to their temp oral overlaps. However, this ad-ho c metho d does not work well and we sho w an example in Figure 5. In this g-ure, one burst y pattern from English news stream and two Figure 6: Topic mo dels before (top-half ) and after (bottom-half ) mutual reinforcemen t. Distinct words are bold-faced. burst y patterns from the Chinese news stream are sho wn. The pattern in English stream is the 9{11 event. The rst one in Chinese stream is about \western dev elopmen t" and the second is a mix of 9{11 and Chinese \national day holi-day" (Octob er 1st) events. From Figure 5, we can see that the burst y perio ds of both Chinese patterns overlap with that of the 9-11 pattern from English news. Thus both of the Chinese patterns can be aligned with the English 9{11 pattern but neither of them is a good matc h. This example sho ws that patterns iden ti ed from single streams can be bi-ased by their local patterns and the alignmen t of suc h biased patterns across streams could be imprecise and misleading.
Since correlated burst y patterns presumably share simi-lar perio ds, the coordinated mixture mo del considers all the streams sim ultaneously and calculates the same burst y pe-riods for their correlated burst y patterns. As sho wn in Fig-ure 3(a), correlated burst y patterns have the same burst y perio d thus we do not need to align them. By consider-ing coordinated stream sim ultaneously , our metho d is more robust for iden tifying imp ortan t correlated patterns. We demonstrate the adv antage of mutual reinforcemen t in Figure 6. Figure 6 (a) and Figure 6 (b) sho w the words of the iden ti ed patterns before and after mutual reinforcemen t, where the di erences are bold-faced. In Figure 6 (a), the words of Afghanistan war are the ma jorit y but still mixed with some noisy words suc h as \APEC" (The Asia-P aci c Economic Cooperation) and \economic." With mutual rein-forcemen t across Chinese and English streams, those noisy words are suppressed and the pattern becomes more coher-ent. This is because those ma jor words in both streams are not only the high probabilit y words, but also have high glob al temp oral correlations with their coun terparts in the other stream. Our mutual reinforcemen t pro cedure can ef-fectiv ely utilize these prop erties and thus impro ve the qual-ity of the burst y patterns.
The adv antage of the constraining EM by local dep en-dency is that it can utilize the consecutiv e prop erty of a stream when parameter &gt; 0. To see the e ect of apply-ing local dep endency , we compare = 0 : 1 and = 0 in Figure 7 using the example of 9{11. It is clear that the peri-ods are inconsecutiv e and rugged for = 0, making it hard to interpret. On the con trary , using dep endency gives us consecutiv e burst y perio ds whic h are natural in realit y.
In this pap er, we de ned and studied a novel problem of mining correlated burst y patterns from coordinated text streams. We prop osed coordinated mixture mo dels whic h can iden tify burst y words and their burst y perio ds sim ulta-neously . Furthermore, we enhanced our mo del by incorp o-rating local dep endency along the time line and prop osed a mutual reinforcemen t approac h across streams to further re-ne the correlated burst y patterns. We evaluated our meth-ods on two data sets: coordinated news streams and coordi-nated literature streams. On the news streams, the iden ti-ed burst y patterns re ect the real world events and on the literature streams, the iden ti ed burst y patterns well sum-marize the researc h paradigm shifts in the database comm u-nity.

Besides the correlated patterns, there are local burst y pat-terns in eac h stream. In the future, we will further study the problem of iden tifying both global correlated and local patterns. We also plan to apply our metho ds to other co-ordinated non-text streams to study their correlated burst y patterns.
We thank the anon ymous review ers for their valuable com-men ts. This work is in part supp orted by a Microsoft Liv e Labs Researc h Gran t, a Google Researc h Gran t, and an NSF CAREER gran t IIS-0347933. [1] C. Aggarw al. Data Streams: Models and Algorithms . [2] C. C. Aggarw al, J. Han, J. Wang, and P. S. Yu. On [3] R. Agra wal, K.-I. Lin, H. S. Sawhney , and K. Shim. [4] J. Allan, J. Carb onell, G. Doddington, J. Yamron, [5] J. Allan, R. Papk a, and V. Lavrenk o. On-line new [6] D. Blei, A. Ng, and M. Jordan. Laten t diric hlet [7] S. Chien and N. Immorlica. Seman tic similarit y [8] G. P. C. Fung, J. X. Yu, P. S. Yu, and H. Lu.
 [9] J. Han and M. Kam ber. Data Mining: Conc epts and [10] T. Hofmann. Probabilistic laten t seman tic indexing. [11] G. Hulten, L. Spencer, and P. Domingos. Mining [12] J. Klein berg. Burst y and hierarc hical structure in [13] R. Kumar, J. Novak, P. Ragha van, and A. Tomkins. [14] Q. Mei, C. Liu, H. Su, and C. Zhai. A probabilistic [15] Q. Mei and C. Zhai. Disco vering evolutionary theme [16] Q. Mei and C. Zhai. A mixture mo del for con textual [17] R. Sproat, T. Tao, and C. Zhai. Named entity [18] R. Swan and J. Allan. Extracting signi can t time [19] R. Swan and J. Allan. Automatic generation of [20] T. Tao and C. Zhai. Mining comparable bilingual text [21] M. Vlac hos, C. Meek, Z. Vagena, and D. Gunopulos. [22] J. Xu, R. Weisc hedel, and C. Nguy en. Evaluating a [23] Y. Yang, T. Ault, T. Pierce, and C. W. Lattimer. [24] Y. Yang, T. Pierce, and J. Carb onell. A study of [25] C. Zhai, A. Veliv elli, and B. Yu. A cross-collection
