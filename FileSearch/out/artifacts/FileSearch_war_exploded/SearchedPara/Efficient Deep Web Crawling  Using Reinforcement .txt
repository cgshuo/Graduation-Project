 surface Web, which is directly indexed by search engines. Studies [1] show deep web information. However, to obtain such conten t of deep web is challenging and has been acknowledged as a significant gap in the coverage of search engines [2]. Surfacing is crawler pre-computes the submissions for deep web forms and exhaustively indexes the response results off-line as other static HTML pages. The approach enables lever-such as HiWE (Hidden Web Exposer) [3], Hidden Web crawler [4] and Google X  X  Deep Web crawler [2]. 
One critical challenge in surfacing approach is how a crawler can automatically generate promising queries so that it can carry out efficient surfacing. The challenge has been studied by several researches such as [2], [4], [5], [6], [7]. In these methods, candidate query keywords are generated from the obtained records, and then their harvest rates, i.e. the promise to obtain ne w records, are calcula ted according to their local statistics, such as DF (Document Frequency) and TF (Term Frequency). The one derive the estimated harvest rate of each query candidate. 
However, to the best of our knowledge, existing methods suffer from the following known as  X  X yopia problem X  [8]. The next query is selected according to the harvest rate defined as the immediate reward at current step. This inherent deficiency makes the existing methods fail to look ahead to future steps thus cannot make a decision of quired data records while ignoring the ex perience gained from previous queries, which usually results in reducing the efficiency of crawling. For example when a crawler issues an unpromising keyword which brings few or even no response re-remain the same. Therefore, it is likely that the crawler will make the same mistakes cases of databases providing non full-text search interfaces, in which some words databases, e.g. Zipf X  Law [4], [9] can hardly be applied to non full-text databases. E.g. Fig. 1 illustrates the keywords distribution on AbeBooks (www.abebooks.com), in which x-axis represents document frequency ranks of keywords in the data corpus and y-axis denotes the percent of response records brought by the keywords. As one can see, the Zipf curve fails to simulate the distribution of keyword response. In this paper, we present a formal framework based on the RL (Reinforcement Learning) [10] for deep web crawling. In the framework, a crawler is regarded as an agent and deep web database as the environment. The agent perceives its current state reward. The environment responds by giving the agent some reward (new records) and changing it into the next state. Each action is encoded as a tuple using its linguis-tic, statistic and HTML features. The rewards of unexecuted actions are evaluated by unpromising queries, as long as some of them have been issued. The experimental results on 5 real world deep web sites show that the reinforcement learning crawling To sum up, the main contributions of our work are: 1) We introduce a formal framework for the deep web surfacing problem. To the 2) We formalize the problem in the framework and propose an efficient and ap-3) We develop a Q-value approximation algorithm allows for a crawler selecting The rest of this paper is organized as follows: Section 2 gives a brief introduction of related work. Section 3 presents the formal reinforcement learning framework. Sec-tion 4 discusses the crawling algorithm and key issues in it. The experimental results final section. The sate-of-the-art deep web crawling approaches generate new keywords by analyz-ing statistic of current acquired records returned from previous queries. Barbosa L. et al. first introduced the ideas, and presented a query selection method which generated records are returned from the deep web database. Ntoulas A. et al. proposed a greedy query selection method based on the expected harvest rate [4]. In the method, the one with the maximum expected harvest rate will be selected for the next query. Ping W. et al. modeled each web database as a distinct attribute-value graph and a greedy link-based query selection method was proposed to approximate the optimal solution [8]. Lu J. et al. resolved the problem using set-covering sampling method [7]. Liu J. et al. extended the Ntoulas X  X  method to entire form by introducing a novel concept MEP (Minimum Executable Pattern). In the method, a MEP set is build and then promising keywords are selected by joint harvest rate of a keyword and its pattern. By selecting among multiple MEPs, the crawler achieves better results [6]. Jayant M. et al. improved the keyword selection algorithm by ranking keywords by their TFIDF (Term Frequency Inverse Document Frequency) [2]. Jiang L. et al. present a method to evaluate a keyword by its HTML features besides TF and DF using supervised learning [11]. RL and formalize the crawling problem under the framework. First of all we give an overview of the RL framework. From the figure, one can conclude that at any given step, an agent (crawler) perceives its state and selects an action (query). The environment responds by giving the agent some (possibly zero) reward (new records) and changing the agent into the successor state. More formally we have Definition 1. Suppose and are two sets of states and actions respectively. A state An action ( for short) denotes a query to the deep web database with the keyword , which causes a transition from state to some successor state with the probability . Process consisting of a set of states , a set of actions and transition probabilities distribution . A crawling process follows a specific issue policy 
In the paper, we assume that the decision process is a deterministic process i.e. is subjected to a uniform distribution. During the process, after execution of an action the agent is responded by giving a collection of data records by the environment. The response record can be defined as: database. After execution of action at state , the response record set represents the collection of data records responded by the environment. Likewise the portion of the new records in the response record set retrieved by action at state is denoted as ( ). 
Suppose a crawling process follows an issue policy , the portion of new records in the response records of action at state can be formulated as tion is the reward received at the transition from state to state by exe-cuting action , i.e. the portion of new records brought by executing , computed from equation Though in some cases is either unknown or cannot be obtained beforehand, the absence of the value does not influence the calculation of the reward as they are rela-tive values to rank actions in the same baseline. 
The transition of actions causes a cost. In the paper, the cost is measured in terms of time consumed, i.e. . is the cost of issuing an action and is proportional to the average time of handling a response record. 
The expectation conditioned on the current state and the policy is called state-value function of state , computed from lices, there must exist an optimal policy, noted defined as ( , 
Based on the presentations above, the formal definition of deep web crawling prob-lem can be defined as: Problem. Under the constraint , find such policy is the maximum cost constraint. Section 3. There are two crucial factors in solving the problem i.e. the reward of each action r and the reward of an issue policy Q-value. Section 4.1 and 4.2 introduces the methods for the action reward calculation and Q-value approximation respectively. Section 4.2. 4.1 Reward Calculation Before specifying the method for the action reward calculation, we need the definition of the document frequency. frequency of action denoted by ( for short) is the number of documents containing keyword in acquired record set .

Note that the document frequency of each ac tion is a known statistic. Since records of having been retrieved at the step , the number of documents containing keyword can be counted up in the acquired record set. Relying on Def. 4, the following theorem can be established. Theorem 1. At state , the reward of each action in can be calculated from Proof: By incorporating Eq. (1) into Eq. (2) we have Eq. (5) can be further rewritten as keyword of action a in the data set . According to the Def. 4 the value equals to the document frequency of the action, i.e. Consequently the Eq. (4) could be proved by incorporating Eq. (7) into Eq. (6). 
The absence of in Eq. (4) does not affect the final result for the same reason de-scribed in Section 3. According to Eq. (4) for an executed action, as response record set is acquired, the reward can be calculated. In contrast, the reward calcula-tion for an unexecuted action directly through Eq. (4) is infeasible. Nevertheless, the those executed. Before proceeding any further, we define the action training and can-didate set. Definition 5. Suppose at state , training set is a set of executed actions, . Similarly, candidate set is a set of available action candidates for submission in the current state. Each action in either or is encoded in the same vector space. 
Based on Def. 4, for an action in , its reward can be estimated as: in which is a kernel function used to evaluate the distance between the given two actions. Since the response record set of an action is irrelevant to the executed action are at the current state, the si ze of response record set of an action in applied in Eq. (4) to calculate its reward. 
Now the action rewards for both executed and unexecuted actions can be calcu-kernel function in Eq. (8). Calculating the similarity of actions requires encoding them in a feature space. We incorporate three types of features i.e. linguistic features, statistical features and HTML features to establish the feature space [11]. 
Linguistic features consist of POS (Part of Speech), length and language of a keyword (action). Length is the number of characters in the keyword. Language web database. Statistical features include TF (Term Frequency), DF (Document Frequency) and RIDF (Residual Inverse Document Frequency) of a keyword in the acquired records. The value of RIDF is computed as: RIDF tends to highlight technical terminology, names, and good keywords and to exhibit nonrandom distributions over documents [12]. 
The HTML format usually plays an important role in indicating the semantics of the presented data. This brings us to consider the HTML information of keywords. We propose two HTML features tag-attribut e and location. Tag-attribute feature en-depth of the keyword X  X  node in the DOM tree derived from the HTML document. The guishing unpromising keywords. 
For linguistic and HTML features whose values are discrete, the liner kernel is as-over documents, the Gaussian kernel is adopted to evaluate similarity upon the statis-tical features, which is formulated as 
The final kernel function is hybrid of these kernels. Suppose , and ( ) are weights for linguistic, HTML and statistical kernel respectively, the kernel function to evaluate similarity of two actions is In experiments the weight of statistical features usually accounts for a larger part. 4.2 Q-Value Approximation and Surfacing Algorithm Once the reward of each action is obtained, given the problem definition in Section 3, the agent can find an optimal policy if the of each state can be calculated. The calculation of could be well solved when the agent uses Q-function [13], [14]: Here Q-function represents the reward received immediately upon executing action from state , plus the value discounted by thereafter. Using Eq. (3), we can rewrite Q-function as garded as important as those for the present. is a critical parameter denoting the step Q-value equals to the immediate reward i.e. . When , Q-value represents the long-term reward. However, as the action reward at state is un-available at state , the Q-value has to be approximated. To estimate the Q-value, we make the following assumption: assume at the current state, the action set will not enlarge in the next steps ( ). When is not very large the assumption is reasonable. Under the assumptions, Theorem 2 could be established. Theorem 2. At state when the Q-value of an action ( , ) can be estimated as: Proof: To simplify the notion, let , , value can be searched in the action set at th e current state. Accord ing to the Eq. (13), when h =1 the Q-value can be formulated as Following the method described in Section 4.1, can be calculated; whereas Because the response records are independent with each others, the capture-mark-recapture [15] method can be applied to estimate the overlaps records: Further Eq. (17) can be transformed into By incorporating Eq. (18) into Eq. (16) we have Note that according to the characteristics of response record set in Def. 3 and Eq. (5): Following Eq. (5) and Eq. (20), Eq. (19) can be reformulated as Then the Theorem 2 can be derived by incorporating Eq. (7) into Eq. (21) . 
As all the factors in the Eq. (14) are either calculated or can be statistically numer-ated, the Q-value of each action can be approximated based on the acquired data set. Now, we can approximate Q-value with a given step length by iteratively applying Eq. (14). Due to the lack of space, we cannot present the details here. Note if h goes experienced state rendering the approximation for future reward imprecise. 
We develop an adaptive algorithm for deep web surfacing based on the framework, action as input and outputs the next optimal action. 
Specifically, the surfacing algorithm first calculates the reward of the last executed action and then updates the action set through Step 2 to Step 7, which causes the agent estimates the reward and Q-value for each action in candidate set in Step 10 and Step 11 respectively. The action that maximizes Q-value will be returned as the next to be executed action. To demonstrate the efficiency of our proposed approach for deep web crawling, we execute our algorithm on five real world d eep web databases with different scales and domains. The detailed information about these databases is listed in Tab.1. In the case textbox  X  X eywords X . Regarding Wikicfp, Yahoo movie, which are the typical medium Deep Web databases, we utilize the only generic search textboxes as their query inter-faces. As for Baidu Baike and Google Music, the sites are multilingual sites consist-ing of both English and Chinese. On the sites we select the rewarding textbox  X  X eyword Tag X  and  X  X inger X  as their query interfaces. 
To compare our RL method with existing ones, we choose following three methods as baseline methods: (Suppose at state , the agent is to evaluate an action )  X  Random [4], [7], [8]: the reward of an action is assigned to a random float i.e.  X  GF (Generic Frequency) [5], [7], [8]: the reward of an action is evaluated by All methods above (including RL) share the same candidate set generation policy which selects the top 500 actions with highest RIDF. It is interesting to note that RL is ignored i.e. and the reward of an action is determined by a presumed distribu-tion, the RL degenerates to Zipf, i.e. . Further if the acquired portion of an action is ignored too i.e. , the RL degenerates to the GF, i.e. . 5.1 Effectiveness of RL Method Our interest is to discover their records as many as possible with affordable cost. To number of actual retrieved records and number of queries (Sixth column) to evaluate cases the agent achieves more than 80% coverage by issuing around 500 hundred queries. 5.2 Performance Comparison with Baseline Method space we only present some results here). In experiments step length was set to 1. As can be seen, the result shows that RL method is more efficient than baseline methods on the experiment websites. We analyzed the queries logs and summarized two rea-sons accounting for excellence of RL method. Firstly because the RL method selects a keyword according to the long-term rather than immediate reward, it is able to acquire a better awareness of the environment leading to more accurate estimation for suc-ceeding rewards. As we found in experiments the rewarding keywords are issued earlier in RL than other methods. Secondly the keywords issued in RL are more rele-agent using RL learns the experience from its previous queries and hence sticks on the keywords matching against more records; whereas the agent using other methods do not make any adjustment when the presumed assumption is not applied. In this paper we tackle the problem of deep web surfacing. The paper first presents a formal reinforcement learning framework to study the problem and then introduce an adaptive surfacing algorithm based on the framework and its related methods for reward calculation and Q-value approximatio n. The framework enables a crawler to learn an optimal crawling strategy from its experienced queries and allows for it mak-ing decisions on long-term rewards. Experimental evaluation on 5 real deep web sites suggests that the method is efficient and applicable. It excels the baseline method and works well on both full-text and non full-text databases. In general, it retrieves more than 80% of the total records by issuing a few hundreds of queries. 
We are studying the issues of deep web crawling in practical and developing an open source platform for Deep Web crawling: DWIM (Deep Web Intelligent Miner). grates many crawling policies and keywords selection criteria which can be used as an experimental platform for researches and a crawler engine for developers. The research was supported by the National High-Tech R&amp;D Program of China under Grant No.2008AA01Z131, the National Science Foundation of China under Grant Nos.60825202, 60803079, 60633020, the National Key Technologies R&amp;D Program of China under Grant Nos.2008BAH26B02, 2009BAH51B00, the Open Project Pro-gram of the Key Laboratory of Complex Systems and Intelligence Science, Institute of Automation, Chinese Academy of Sciences under Grant No. 20080101, Cheung Kong Scholar X  X  Program. 
