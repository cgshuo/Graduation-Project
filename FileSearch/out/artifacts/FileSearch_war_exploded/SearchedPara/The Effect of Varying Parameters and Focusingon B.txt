 In the last 2 or 3 decades, mass transit companies made important investments in Advanced Public Transportation Sy stems, allowing them to collect massive amounts of data. The problem we address in this paper is the prediction of travel time 3 days ahead in order to improve planning of driver duties [4] and, consequently, to reduce costs with extra time. One of the challenges of this application is the dynamic nature of the data: different bus lines are affected by different factors (e.g., type of road) and the factors affecting the travel time for a single line may also change (e.g., creation of a bus lane). We empirically evaluate how the accuracy of several algorithms i s affected by varying their parameters (Sect. 3) and by using different methods for the focusing tasks [5] (Sect. 4). The experimental setup is given in Sect . 2 and we close with conclusions. We simulate the real planning process by inducing models to predict travel times for day d using the data from days d  X  32 ,  X  X  X  ,d  X  3 for training. The experiments use data from a single bus line for the period 1/Jan/04 to 31/Mar/04. This cor-responds to 90 days of data and, given that each training set has 30 days, we obtain 90  X  30  X  2 = 58 different days for testing. The average number of trips in each training set is around 900. The set of variables used were { departure time, week day, day of the year and day type } , except where indicated otherwise. Three algorithms, support vector regression (SVM), random forests (RF) and project pursuit regression (PPR), were selected based on [3]. The generalization error is assessed using the variation index function, varIndex = where f and  X  f represent, respectively, the unknown true function and the pre-dictor. All the experimen ts use the R-project [7]. SVM. We use the  X   X  SVM implementation from the R package e1071 [7], and we tested the linear, radial and sigmoid ke rnels. The parameters are presented in Table 1 and the results for each kernel are presented in Figs. 1 and 2 (left-hand side). Each parameter set is represented by c[ C index]n[  X  index]g[  X  index]f[ coef0 index]. The parameter sets are ordered firstly by c then by n for the linear kernel, by c, n and g for the radial kernel and by f, g, n and c for the sigmoid kernel. The most striking result for SVM linear is the low sensitivity to different parameter settings, except for the largest values of C and the lowest values of  X  . For SVM radial it is possible to observe that the higher the value of C is, the more sensitive the algorithm is to the remaining parameters. The sigmoid kernel has four parameters and consequently it is more difficult to analyze results. However, the performance of the algorit hm decreases significantly with higher values coef0 , except when the value of  X  are low.
 RF. Random Forest (RF) has two main parameters with impact on the accuracy of the predictions: the number of generated trees (the ntree parameter); and the number of variables randomly s elected at each iteration (the mtry parameter). Given that result converge with increasing value of ntree , it is set to 1000, as suggested in [1], and given that we have four variables, mtry  X  X  1 , 2 , 3 , 4 } .The results (right-hand side of Fig. 2) show that RF is not very sensitive to mtry . PPR. The PPR implementation in the stats package of R [7] was used. The settings tested for the general parameters were nterms  X  X  1 , 2 , 3 , 4 } and optlevel  X  X  0 , 1 , 2 , 3 } . Additionally, the three ridge functions, called smoothers, avail-able in the implementation were tested: super smoother, spline and gcvspline, each one with its own specific parameters (Table 2). Each parameter set is rep-resented by n[ nterms index]o[ optlevel index]b[ bass index]s[ span index] for the super smoother and by n[ nterms index]o[ optlevel index]d[ df index] for spline. The parameter sets are ordered by b, s, n and o for the super smoother and by d, n and o for the spline.

Results for PPR supsmu vary significantly (Fig. 3, left), depending on whether bass is equal to zero or not. Other than th at, the effect of the other parameters is not very significant. When bass is equal to zero, the best results are obtained with lower values of span . For values different from zero, the results tend to degrade as the value of bass increases. For PPR spline (Fig. 3, right), the value of df strongly affects the impact of the remaining parameters. Indeed, for small values of df the algorithm has low sensitivity to the remaining parameters while for larger values of df , the best results are obtained for smaller values of nterm . Results for gcvspline lead to similar conclusions to the ones presented here and, are thus omitted in interest of space. Example selection. The aim of example selectio n is to increase accuracy by selecting from the training set just a subset of the examples for the training task [5] Two approaches were tested. The first, referred to as equivalent days (ed), uses the examples from identical past days according to several groups, defined by visual inspection and using domain knowledge (e.g., Day Type = Normal and working days (from Monday to Friday); Day Type = Normal and Saturdays). If there is not a minimum of examples from equivalent days, according to the input parameter min members , all the examples from the training set are used. We have used min members = 10. The second, leaf node (ln), uses the examples from the same leaf of a CART [2] as the current test example: the CART model is induced on the full training set. In the following,  X  X ll X  presents the results using all the examples from the training set, i.e., without example selection.
The most important observation is that the two approaches behave differently for each one of the three methods. On the other hand, RF (results not plotted due to space limitations) are not very sensitive to example selection. This is not surprising because the CART algorithm already embeds example selection. For PPR (one variant is shown in Fig. 4, right), it is clear that both approaches for example selection can incr ease accuracy. For SVM (Fig. 4, left), the leaf node approach is the best one.
 Domain values selection. Domain values selection can include: 1) the choice of the data type for each variable; 2) the discretization of continuous variables; or 3) the choice of appropriate values for a symbolic variable [5]. Here we have only tested changing the data type of the week day variable (WD), which was originally treated as symbolic.

The use of the numeric data type for the variable week day when using SVM is not promising (one variant is shown in Fig. 5, left). The change in data type of WD has small effect on the performance of SVM. PPR is more sensitive to the change in the type of WD, independently of the smoother (e.g., Fig. 5, right).
Table 3 presents the results of the bes t parameters settings after example selection and the choice of the data type for the week day variable. Focusing with example selection achieves larger gains than the change in type of WD. Anyway, as observed earlier, the PPR benefits from the change in type of WD. Feature selection. The four variables used so fa r were selected because of good results that were previously obtained [ 4]. However, to assess the effects of fea-ture selection [5], more variables are n ecessary, so we included other variables describing the bus trips and also a few meteorological variables (Fig. 6, left). The first four variables capture the different seasonalities of the data, namely, daily (departure time), weekly (week day) and yearly (day of the year and week of the year) seasonality. The next three variables capture the possible impact fac-tors, namely, the existence of holidays (da y type), school holidays (school break) and number of Sundays before the pay day (Sundays unpd). The entrance and exit flow variables try to depict the o ccurrence of unusual flow at the entrances and exits of the town. The bus type can be short or long and the driver is a code identifying the driver of the bus. The service is an internal concept to the companies that represent the offer of tri ps. The three meteoro logical variables we use are the wind speed, temperature an d precipitation. Feature selection was carried out using the RReliefF method [6]. For each variable, we compute the RReliefF weights for all training sets. Given that we have 58 different training sets (Sect. 2), we compute the statistics presented in Fig. 6 (left) for the set of 15 variables considered here. Based o n these results, we have eliminated the variables week of the year, school break, Sundays until next pay, entrance flow and exit flow. A few observations can be made: (1) The low values for day type, entrance flow and exit flow can be explained by the small size of the training set (30 days) and the previous knowledge that these values are rarely different from the standard ones. (2) The low value for Sundays until next pay day can be due to the use of just one cycle on the seasonality of this event. Although we have eliminated it here, we note that this variable could be useful if using a larger training set. (3) Of the two variables used to capture the seasonality of the year: day of the year and week of the year, the first one is the most relevant. Due to the dependence between these two variables, the respective weights are expected to be lower than they should be [6].

Besides the set of all variables (All15) and the set of 10 variables selected with RReliefF (AllRRF), we have tested a variant without the meteorological variables (AllRRF-Meteo) and the original set of 4 variables, with and without the meteorological variables (RedSet+Meteo and RedSet). We only used the RF algorithm because its performance was quite stable in the previous experiments and, thus, it was not necessary to try diff erent parameter settings and methods for the other focusing tasks. We have also tested the feature selection method embedded in RF, which limits the number of variables that are randomly se-lected in each split, given by the value of the mtry parameter. We note that, while the RReliefF method is global, because it selects a set of variables before running the algorithm, the method embedded in RF can be regarded as local, be-cause the selection process is repeated sev eral times during the learning process and may result in different sets of variables being selected. All possible values of the mtry parameter were tested for each se t of variables (right-hand side of Fig. 6).

The best subset is All15, as expected, b ecause of the featur e selection mecha-nism which is characteristic of tree-based models (i.e., if a variable is not relevant, it will not be used in any of the nodes). However, the set obtained with RReli-efF (AllRRF) obtains a similar accuracy. The results of the three sets containing meteorological values are generally better than the others. However, we note that we used the real values rather than 3-day ahead predictions. This means that, these are optimistic estimates of their accuracy. Finally, although the accu-racy obtained with the original set of variables, RedSet, is generally the lowest, we observe that the range of values obtained by varying the mtry parameter is smaller in comparison to the other sets. This indicates that All15 contains informative variables which are not in the original set but also some which are misleading. The prediction of travel time is an important tool to improve planning of driver duties, as well as other planning and customer information tasks. In this work, we evaluated empirically the impact of varying parameters on the performance of different regression algorithms (SVM, RF and PPR). We also evaluated the impact of the focusing tasks (example selection, domain value definition and feature selection) in the accu racy of those algorithms.

The results can be summarized as follows . (1) From the three regression meth-ods we tested, RF is the most attractive as an off-the-shelf method. It has just one input parameter that is very easy to choose and the results are competitive. (2) The sensitivity of the results to the variation in input parameters displayed by SVM does not change meaningfully by varying the focusing methods. This means that the choice of parameter values and focusing methods can be made independently, thus reducing computational costs of modelling. Additionally, our results indicate that the leaf node appr oach for example selection is the most worthy of the focusing methods. (3) Conv ersely, the effect of varying parameters of PPR changes with the focusing method used. Therefore, it is advisable to set parameters and choose focusing methods simultaneously. (4) RF are able to obtain the best results from the extended set of variables. However, the results obtained with the smaller, original set are not much worse, and seem to be more robust, which indicates that the latter set may be more suitable with less robust algorithms, such as SVM and PPR.
 This work was partially supported by FCT -Funda  X  c  X  ao para a Ci X  encia e a Tecnolo-gia, FEDER and Programa de Financiamento Plurianual de Unidades de I&amp;D.
