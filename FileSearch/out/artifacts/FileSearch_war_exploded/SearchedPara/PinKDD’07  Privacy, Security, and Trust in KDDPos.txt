 In this report, we summarize the events of the First In-ternational Workshop on Privacy, Security, and Trust in KDD (PinKDD), which was held in conjunction with the 13 th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. The workshop convened on Au-gust 12, 2007 in San Jose, California and brought together researchers, as well as practitioners, working on how privacy, security, and trust can be resolved or modeled within a data mining framework. Vast amounts of data are collected by service providers, sys-tem administrators, and are available in public information systems. Data mining technologies provide an ideal frame-work to assist in the analysis of such collections for computer security and surveillance-related endeavors. For instance, system administrators can apply data mining to summarize activity patterns in access logs so that potential malicious incidents can be further investigated. Beyond computer se-curity, data mining technology supports intelligence gath-ering and reporting for homeland security. For years, and most-recently fueled by events such as September 11, 2001, government agencies have focused on developing and apply-ing data mining technologies to monitor terroristic behaviors in public and private data collections.
 The application of data mining to person-specific data raises serious concerns regarding data confidentiality and citizens privacy rights. These concerns have promulgated the adop-tion of various legislation and policy controls. In 2006, the European Union passed a data-retention directive that re-quires all telephone and Internet service providers to store data on their consumers for up to two years to assist in the prevention of terrorism and organized crime. [4] Similar data-retention regulation proposals are under heated debate in the United States Congress. However, the debate often formal models of the system, Dr. Dwork, proceeded to sum-marize research that characterizes the theoretical limits of privacy protection that can be achieved in databases. This talk concluded by highlighting recent work conducted by Dr. Dwork and colleagues on the ability, or lack thereof, to protect privacy in highly complex datasets, such as social networks. The first research session of the workshop was dedicated to papers that concentrated on algorithms and data structures to achieve privacy in distributed datasets. Privacy preserv-ing data mining in distributed settings has become a crucial topic, due in part to the rapid growth of ubiquitous com-puting environments. When data is distributed across a set of parties, the involved parties have the opportunity to per-form data mining on a larger quantity of information and thus learn more robust and accurate rules and models. A common goal in privacy preserving distributed data mining is to merge models learned from local datasets to construct a global model without revealing sensitive local information. In the first presentation, Sharkey et al. [8] tackled this prob-lem and proposed a method for knowledge sharing among multiple parties. Specifically, they focused on how to learn decision tree models. The main contribution of their ap-proach was that it reduced the computational and commu-nication complexity that is inherent to earlier data exchange protocols for distributed privacy preserving data mining. Model sharing needs to be supported by a mechanism to tune the model or to select the best model among the pos-sible alternatives. The work presented by Yang et al. [10] proposed a method for multiple parties to perform model selection in a privacy preserving fashion, without revealing any data to each other. They consider the popular cross-validation method for model selection and demonstrate how to mine vertically partitioned data in a secure setting for two parties.
 When data owners lack expertise or computational power, a central authority is needed to perform data mining tasks. One way this architecture can be leveraged is for each data owners to perturb their records before submitting data to the central repository. The goal of perturbation is to pre-vent the revelation of the original values of any particular record, while retaining enough information in the data so that it remains useful for data mining purposes. Tan and Ng [9] provided the third presentation of this session and described a new privacy-preserving distributed data saniti-zation algorithm. In this algorithm, they build a classifier by independently randomizing the private data at each site before the data is fed into the central store.
 Most prior distributed privacy preserving data mining al-gorithms assume a  X  X emi-honest X  model, which is a very strong assumption that is inappropriate for certain settings. Ahmad and Khokhar [1] addressed this issue in the third presentation in which they described an efficient privacy pre-serving clustering algorithm on horizontally partitioned data under the malicious adversary model. In this setting, the authors presented a protocol that does not require a central authority. The continuing growth of online search engines, such as Google, Yahoo, and Microsoft, generates incredible quan-tities of detailed personal information. Every day, people looking for information on the World Wide Web submit mil-lions of search queries. This information is stored and, in many instances, is mapped to an individual X  X  prior searches. Search queries are only one type of information collected by websites through user interaction and as the amount, and diversity, of person-specific data stored at websites grows, so too do the data mining opportunities for service per-sonalization. However, the events of the AOL search log re-identifications, and the work presented by Poblete et al. [7], provided clear illustrations of how weak protections can lead to significant breaches of user X  X  expected, as well as promised, privacy. The goal of this panel was to gather ex-perts on data mining, weblogs, and privacy to discuss the current issues and potential directions for research in the area. The panel consisted of Dr. Ricardo Baeza-Yates (Ya-hoo Research), Dr. Cynthia Dwork (Microsoft Research), Dr. Lise Getoor (University of Maryland, College Park), and Dr. David Jensen (University of Massachusetts Amherst). Privacy, security, and trust in data mining are related is-sues that have captured the attention of many researchers, administrators, and legislators. Consequently, data mining for improved security, and the study of data mining side-effects on privacy, has rapidly become a hot and lively re-search area. The issues are rooted in the real-world and concern academia, industry, government, as well as society in general. The issues are global and many governments are struggling to set national, and international, policies on privacy and security for data mining endeavors. The PinKDD workshop demonstrated that privacy-aware data mining technologies can be developed, but also that there are limitations to existing computational models and privacy policies. The analysis of the security and privacy aspects of data mining have begun, and the research and debate on display at PinKDD illustrated that the topics is moving to-wards maturity and main stream acceptance. We commend all of participants and look forward to continuing advances in the field! We would like to thank the authors of all submitted papers, the invited speaker, the panelists, and all attendees for con-tributing to the success of the workshop. We would also like to express our gratitude to the members of the Program Committee for their vigilant and timely reviews, namely: Maurizio Atzori, Roberto Bayardo, Barbara Carminati, Pe-ter Christen, Josep Domingo-Ferrer, Wenliang (Kevin) Du, Tyrone Grandison, Satoshi Hada, Dawn Jutla, Murat Kantar-cioglu, Hillol Kargupta, Stan Matwin, Ilya Mironov, Taneli Mielikinen, David Skillicorn, Kian-Lee Tan, Bhavani Thu-raisingham, Vicen  X c Torra, Vassilios Verykios, Ke Wang, and Jeffrey Yu.
 Last, but not least, we acknowledge the fundamental sup-port of our sponsors: The UNESCO Chair in Data Privacy,
