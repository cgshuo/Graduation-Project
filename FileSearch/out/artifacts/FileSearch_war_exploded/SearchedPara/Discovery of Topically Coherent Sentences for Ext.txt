 A query-focused multi-document summarization model produces a short-summary text of a set of documents, which are retrieved based on a user X  X  query. An ideal generated summary text should con-tain the shared relevant content among set of doc-uments only once , plus other unique information from individual documents that are directly related to the user X  X  query addressing different levels of de-tail . Recent approaches to the summarization task has somewhat focused on the redundancy and co-herence issues. In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences.
Prior research has demonstrated the usefulness of sentence extraction for generating summary text taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Be-cause documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summariza-tion work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daum  X  e -III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010) build hierarchical topic models to iden-tify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redun-dancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task.

Topical coherence in text involves identifying key concepts, the relationships between these concepts, and linking these relationships into a hierarchy. In this paper, we present a novel, fully generative Bayesian model of document corpus, which can dis-cover topically coherent sentences that contain key shared information with as little detail and redun-dancy as possible. Our model can discover hierar-chical latent structure of multi-documents, in which some words are governed by low-level topics (T) and others by high-level topics (H). The main con-tributions of this work are:  X  construction of a novel bayesian framework to capture higher level topics (concepts) related to sum-mary text discussed in  X  3,  X  representation of a linguistic system as a sequence of increasingly enriched models, which use posterior topic correlation probabilities in sentences to design a novel sentence ranking method in  X  4 and 5,  X  application of the new hierarchical learning method for generation of less redundant summaries discussed in  X  6. Our models achieve compara-ble qualitative results on summarization of multiple newswire documents. Human evaluations of gener-ated summaries confirm that our model can generate non-redundant and topically coherent summaries. Prior research has demonstrated the usefulness of sentence extraction for summarization based on lex-ical, semantic, and discourse constraints. Such models often rely on different approaches includ-ing: identifying important keywords (Nenkova et al., 2006); topic signatures based on user queries (Lin and Hovy, 2002; Conroy et al., 2006; Harabagiu et al., 2007); high frequency content word feature based learning (Nenkova and Vanderwende, 2005a; Nenkova and Vanderwende, 2005b), to name a few.
Recent research focusing on the extraction of la-tent concepts from document clusters are close in spirit to our work (Barzilay and Lee, 2004; Daum  X  e -III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Wang et al., 2009). Some of these work (Haghighi and Vanderwende, 2009; Celikyil-maz and Hakkani-Tur, 2010) focus on the discov-ery of hierarchical concepts from documents (from abstract to specific) using extensions of hierarchal topic models (Blei et al., 2004) and reflect this hier-archy on the sentences. Hierarchical concept learn-ing models help to discover, for instance, that  X  X ase-ball X  and  X  X ootball X  are both contained in a general class  X  X ports X , so that the summaries reference terms related to more abstract concepts like  X  X ports X .
Although successful, the issue with concept learn-ing methods for summarization is that the extracted sentences usually contain correlated concepts. We need a model that can identify salient sentences re-ferring to general concepts of documents and there should be minimum correlation between them.

Our approach differs from the early work, in that, we utilize the advantages of previous topic models and build an unsupervised generative model that can associate each word in each document with three random variables: a sentence S , a higher-level topic H , and a lower-level topic T , in an analogical way to PAM models (Li and McCallum, 2006), i.e., a di-rected acyclic graph (DAG) representing mixtures of hierarchical structure, where super-topics are multi-nomials over sub-topics at lower levels in the DAG. We define a tiered-topic clustering in which the up-per nodes in the DAG are higher-level topics H , rep-resenting common co-occurence patterns (correla-tions) between lower-level topics T in documents. This has not been the focus in prior work on genera-tive approaches for summarization task. Mainly, our model can discover correlated topics to eliminate re-dundant sentences in summary text.

Rather than representing sentences as a layer in hierarchical models, e.g., (Haghighi and Vander-wende, 2009; Celikyilmaz and Hakkani-Tur, 2010), we model sentences as meta-variables . This is sim-ilar to author-topic models (Rosen-Zvi et al., 2004), in which words are generated by first selecting an author uniformly from an observed author list and then selecting a topic from a distribution over topics that is specific to that author. In our model, words are generated from different topics of documents by first selecting a sentence containing the word and then topics that are specific to that sentence. This way we can directly extract from documents the summary related sentences that contain high-level topics. In addition in (Celikyilmaz and Hakkani-Tur, 2010), the sentences can only share topics if the sen-tences are represented on the same path of captured topic hierarchy, restricting topic sharing across sen-tences on different paths. Our DAG identifies tiered topics distributed over document clusters that can be shared by each sentence. In this section we discuss the main contribution, our two hierarchical mixture models, which improve summary generation performance through the use of tiered topic models. Our models can identify lower-level topics T (concepts) defined as distributions over words or higher-level topics H , which represent correlations between these lower level topics given sentences. We present our synthetic experiment for model development to evaluate extracted summaries on redundancy measure. In  X  6, we demonstrate the performance of our models on coherence and infor-mativeness of generated summaries by qualitative and intrinsic evaluations.

For model development we use the DUC 2005 dataset 1 , which consists of 45 document clusters, each of which include 1-4 set of human gener-ated summaries (10-15 sentences each). Each doc-ument cluster consists  X  25 documents (25-30 sen-tences/document) retrieved based on a user query. We consider each document cluster as a corpus and build 45 separate models.

For the synthetic experiments, we include the pro-vided human generated summaries of each corpus as additional documents. The sentences in human summaries include general concepts mentioned in the corpus, the salient sentences of documents. Con-trary to usual qualitative evaluations of summariza-tion tasks, our aim during development is to measure the percentage of sentences in a human summary that our model can identify as salient among all other document cluster sentences. Because human pro-duced summaries generally contain non-redundant sentences, we use total number of top-ranked hu-man summary sentences as a qualitative redundancy measure in our synthetic experiments.

In each model, a document d is a vector of N d words w d , where each w id is chosen from a vocabu-lary of size V , and a vector of sentences S , represent-ing all sentences in a corpus of size S D . We identify sentences as meta-variables of document clusters, which the generative process models both sentences and documents using tiered topics. A sentence X  X  re-latedness to summary text is tied to the document cluster X  X  user query. The idea is that a lexical word present or related to a query should increase its sen-tence X  X  probability of relatedness. Our base model, the two-tiered topic model ( TTM ), is inspired by the hierarchical topic model, PAM , proposed by Li and McCallum (2006). PAM struc-tures documents to represent and learn arbitrary, nested, and possibly sparse topic correlations using a directed acyclic graph. Our goals are not so dif-ferent: we aim to discover concepts from documents that would attribute for the general topics related to a user query, however, we want to relate this informa-tion to sentences. We represent sentences S by dis-covery of general (more general) to specific topics (Fig.1). Similarly, we represent summary unrelated (document specific) sentences as corpus specific dis-tributions  X  over background words w B , (functional words like prepositions, etc.).

Our two-tiered topic model for salient sentence discovery can be generated for each word in the doc-ument (Algorithm 1) as follows: For a word w id in document d , a random variable x id is drawn, which determines if w id is query related, i.e., w id either ex-ists in the query or is related to the query 2 . Oth-erwise, w id is unrelated to the user query. Then sentence s i is chosen uniformly at random ( y s Uniform ( s i ) ) from sentences in the document con-taining w id (deterministic if there is only one sen-tence containing w id ). We assume that if a word is related to a query, it is likely to be summary-related (so as the sampled sentence s i ). We keep track of the frequency of s i  X  X  in a vector, DS  X  Z S D . Ev-ery time an s i is sampled for a query related w id , we increment its count, a degree of sentence saliency.
Given that w id is related to a query, it is as-sociated with two-tiered multinomial distributions: high-level H topics and low-level T topics. A high-level topic H k over low-level topics T specific to that s i and one low-level topic T k over words, and w id is generated from the sampled low-level topic. If w id is not query-related, it is gen-erated as a background word w B .

The resulting tiered model is shown as a graph and plate diagrams in Fig.1 &amp; 2. A sentence sampled from a query related word is associated with a dis-tribution over K 1 number of high-level topics H k each of which are also associated with K 2 number of low-level topics T k words of a corpus. In Fig.2 the most confident words of four low-level topics is shown. The bolded links between H k Algorithm 1 Two-Tiered Topic Model Generation relation between T k is found to be more correlated with  X  X etail X  than the  X  X etwork X  topic given H 1 . This information is used to rank sentences based on the correlated topics. 4.1 Learning and Inference for TTM Our learning procedure involves finding parame-ters, which likely integrates out model X  X  posterior distribution P ( H , T | W d , S ) , d  X  D . EM algorithms might face problems with local maxima in topic models (Blei et al., 2003) suggesting implementa-tion of approximate methods in which some of the parameters, e.g.,  X  H ,  X  T ,  X  , and  X  , can be integrated out, resulting in standard Dirichlet-multinomial as well as binomial distributions. We use Gibbs sam-pling which allows a combination of estimates from several local maxima of the posterior distribution.
For each word, x id is sampled from a sentence specific binomial  X  which in turn has a smooth-ing prior  X  to determine if the sampled word w id is (query) summary-related or document-specific. De-pending on x id , we either sample a sentence along with a high/low-level topic pair or just sample back-ground words w B . The probability distribution over sentence assignments, P ( y s sumed to be uniform over the elements of S , and de-terministic if there is only one sentence in the docu-ment containing the corresponding word. The opti-mum hyper-parameters are set based on the training dataset model performance via cross-validation 3 .
For each word we sample a high-level H k a low-level T k ( x id = 1 ). The sampling distribution for TTM for a word given the remaining topics and hyper-parameters  X  H , X  T , X , X , X  is: and when x = 0 (a corpus specific word), The n k 1 d is the number of occurrences of high-level topic k 1 in document d , and n k 1 k 2 d is the number of times the low-level topic k 2 is sampled together with high-level topic k 1 in d , n w k currences of word w sampled from path H-T given that the word is query related. Note that the number of tiered topics in the model is fixed to K 1 and K 2 , which is optimized with validation experiments. It is also possible to construct extended models of TTM using non-parametric priors, e.g., hierarchal Dirich-let processes (Li et al., 2007) (left for future work). 4.2 Summary Generation with TTM We can observe the frequency of draws of every sen-tence in a document cluster S , given it X  X  words are related, through DS  X  Z S D . We obtain DS during Gibbs sampling (in  X  4.1), which indicates a saliency score of each sentence s j  X  S , j = 1 ..S D : where w id indicates a word in a document d that ex-ists in s j and is sampled as summary related based on random indicator variable x id . nw j is the num-ber of words in s j and normalizes the score favoring sentences with many related words. We rank sen-tences based on (1). We compare TTM results on synthetic experiments against PAM (Li and McCal-lum, 2006) a similar topic model that clusters topics in a hierarchical structure, where super-topics are distributions over sub-topics. We obtain sentence scores for PAM models by calculating the sub-topic significance (TS) based on super-topic correlations, and discover topic correlations over the entire docu-ment space (corpus wide). Hence; we calculate the TS of a given sub-topic, k = 1 ,..,K 2 by: where z k sub is a sub-topic k = 1 ..K 2 and z k 1 sup is a super-topic k 1 . The conditional probability of a sub-topic k given a super-topic k 1 , p ( z k sub | z k 1 sup the variation of that sub-topic in relation to other sub-topics. The higher the variation over the entire corpus, the better it represents the general theme of the documents. So, sentences including such topics will have higher saliency scores, which we quantify by imposing topic X  X  significance on vocabulary: score PAM ( s i ) =
Fig. 4 illustrates the average salience sentence se-lection performance of TTM and PAM models (for 45 models). The x-axis represents the percentage of sentences selected by the model among all sentences in the DUC2005 corpus. 100% means all sentences in the corpus included in the summary text. The y-axis is the % of selected human sentences over all sentences. The higher the human summary sen-tences are ranked, the better the model is in select-ing the salient sentences. Hence, the system which peaks sooner indicates a better model.

In Fig.4 TTM is significantly better in identifying human sentences as salient in comparison to PAM . The statistical significance is measured based on the area under the curve averaged over 45 models. Our model can discover words that are related to summary text using posteriors  X  P (  X  H ) and  X  P (  X  T
T T as well as words w B specific to documents (via  X  P (  X  ) ) (Fig.1). TTM can discover topic correlations, but cannot differentiate if a word in a sentence is more general or specific given a query. Sentences with general words would be more suitable to in-clude in summary text compared to sentences con-taining specific words. For instance for a given sen-tence:  X  X tarbucks Coffee has attempted to expand and diversify through joint ventures, and acquisi-tions. X  ,  X  X tarbucks X  and  X  X offee X  are more gen-eral words given the document clusters compared to  X  X oint X  and  X  X entures X  (see Fig.2), because they appear more frequently in document clusters. How-ever, TTM has no way of knowing that  X  X tarbucks X  and  X  X offee X  are common terms given the context. We would like to associate general words with high-level topics, and context specific words with low-level topics. Sentence containing words that are sampled from high-level topics would be a bet-ter candidate for summary text. Thus; we present enriched TTM ( ETTM ) generative process (Fig.3), which samples words not only from low-level top-ics but also from high-level topics as well.
ETTM discovers three separate distributions over words: ( i ) high-level topics H as distributions over corpus general words W H , ( ii ) low-level topics T as distributions over corpus specific words W L , and ( iii ) background word distributions, i.e,. document specific W B (less confidence for summary text). Similar to TTM  X  X  generative process, if w id is re-lated to a given query, then x = 1 is determin-istic, otherwise x  X  { 0 , 1 } is stochastically deter-mined if w id should be sampled as a background word ( w B ) or through hierarchical path, i.e., H-T pairs. We first sample a sentence s i for w id uni-formly at random from the sentences containing the level L w high-level word, e.g., more general to context like  X  X tarbucks X  or  X  X offee X  or more specific to related context such as  X  X ubsidiary X  ,  X  X rappucino X  . Each path through the DAG, defined by a H-T pair (total of K 1 K 2 pairs), has a binomial  X  K level of the path outputs sampled word. If the word is a specific type, x = 0 , then it is sampled from the background word distribution  X  , a document specific multinomial. Once the level and conditional path is drawn (see level generation for ETTM above) the rest of the generative model is same as TTM . 5.1 Learning and Inference for ETTM For each word, x is sampled from a sentence spe-cific binomial  X  , just like TTM . If the word is related to the query x = 1 , we sample a high and low-level topic pair H  X  T as well as an additional level L is sampled to determine which level of topics the word should be sampled from. L is a corpus specific bi-nomial one for all H  X  T pairs. If L = 1 , the word is one of corpus general words and sampled from the high-level topic, otherwise ( L = 2 ) the word is corpus specific and sampled from a the low-level topic. The optimum hyper-parameters are set based on training performance via cross validation.
The conditional probabilities are similar to TTM , but with additional random variables, which deter-mine the level of generality of words as follows: 5.2 Summary Generation with ETTM For ETTM models, we extend the TTM sentence score to be able to include the effect of the general words in sentences (as word sequences in language models) using probabilities of K 1 high-level topic distributions,  X  w H score ETTM ( s i )  X  # [ w id  X  s j ,x id = 1] /nw j  X  where p ( w | T k ) is the probability of a word in s being generated from high-level topic H k . Using this score, we re-rank the sentences in documents of the synthetic experiment. We compare the re-sults of ETTM to a structurally similar probabilis-tic model, entitled hierarchical PAM (Mimno et al., 2007), which is designed to capture topics on a hi-erarchy of two layers, i.e., super topics and sub-topics, where super-topics are distributions over ab-stract words. In Fig. 4 out of 45 models ETTM has the best performance in ranking the human gener-ated sentences at the top, better than the TTM model. Thus; ETTM is capable of capturing focused sen-tences with general words related to the main con-cepts of the documents and much less redundant sentences containing concepts specific to user query. In this section, we qualitatively compare our models against state-of-the art models and later apply an in-trinsic evaluation of generated summaries on topical coherence and informativeness.

For a qualitative comparison with the previous state-of-the models, we use the standard summariza-tion datasets on this task. We train our models on the datasets provided by DUC2005 task and validate the results on DUC 2006 task, which consist of a total of 100 document clusters. We evaluate the perfor-mance of our models on DUC2007 datasets, which comprise of 45 document clusters, each containing 25 news articles. The task is to create max. 250 word long summary for each document cluster. 6.1. ROUGE Evaluations: We train each docu-ment cluster as a separate corpus to find the optimum parameters of each model and evaluate on test docu-ment clusters. ROUGE is a commonly used measure, a standard DUC evaluation metric, which computes recall over various n-grams statistics from a model generated summary against a set of human generated summaries. We report results in R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams) ROUGE scores w/ and w/o stop words included.

For our models, we ran Gibbs samplers for 2000 iterations for each configuration throwing out first 500 samples as burn-in. We iterated different values for hyperparameters and measured the performance on validation dataset to capture the optimum values.
The following models are used as benchmark: ( i ) PYTHY (Toutanova et al., 2007): Utilizes hu-man generated summaries to train a sentence rank-ing system using a classifier model; ( ii ) HIERSUM (Haghighi and Vanderwende, 2009): Based on hier-archical topic models. Using an approximation for inference, sentences are greedily added to a sum-mary so long as they decrease KL-divergence of the generated summary concept distributions from doc-ument word-frequency distributions. ( iii ) HybHSum (Celikyilmaz and Hakkani-Tur, 2010): A semi-supervised model, which builds a hierarchial LDA to probabilistically score sentences in training dataset as summary or non-summary sentences. Using these probabilities as output variables, it learns a discrim-inative classifier model to infer the scores of new sentences in testing dataset. ( iv ) PAM (Li and Mc-Callum, 2006) and hPAM (Mimno et al., 2007): Two hierarchical topic models to discover high and low-level concepts from documents, baselines for syn-thetic experiments in  X  4 &amp;  X  5.

Results of our experiments are illustrated in Table 6. Our unsupervised TTM and ETTM systems yield a 44.1 R-1 (w/ stop-words) outperforming the rest of the models, except HybHSum . Because HybHSum uses the human generated summaries as supervision during model development and our systems do not, our performance is quite promising considering the generation is completely unsupervised without see-ing any human generated summaries during train-ing. However, the R-2 evaluation (as well as R-4) w/ stop-words does not outperform other models. This is because R-2 is a measure of bi-gram recall and neither of our models represent bi-grams whereas, for instance, PHTHY includes several bi-gram and higher order n-gram statistics. For topic models bi-grams tend to degenerate due to generating inconsis-tent bag of bi-grams (Wallach, 2006). 6.2. Manual Evaluations: A common DUC task is to manually evaluate models on the qual-ity of generated summaries. We compare our best model ETTM to the results of PAM , our benchmark model in synthetic experiments, as well as hybrid hierarchical summarization model, hLDA (Celiky-ilmaz and Hakkani-Tur, 2010). Human annotators are given two sets of summary text for each docu-ment set, generated from either one of the two ap-proaches: best ETTM and PAM or best ETTM and HybHSum models. The annotators are asked to mark the better summary according to five criteria: non-redundancy (which summary is less redundant), coherence (which summary is more coherent), fo-cus and readability (content and no unnecessary de-tails), responsiveness and overall performance.
We asked 3 annotators to rate DUC2007 predicted summaries (45 summary pairs per annotator). A to-tal of 42 pairs are judged for ETTM vs. PAM mod-els and 49 pairs for ETTM vs. HybHSum models. The evaluation results in frequencies are shown in Table 6. The participants rated ETTM generated summaries more coherent and focused compared to PAM , where the results are statistically significant (based on t-test on 95% confidence level) indicat-ing that ETTM summaries are rated significantly bet-ter. The results of ETTM are slightly better than HybHSum . We consider our results promising be-cause, being unsupervised, ETTM does not utilize human summaries for model development. We introduce two new models for extracting topi-cally coherent sentences from documents, an impor-tant property in extractive multi-document summa-rization systems. Our models combine approaches from the hierarchical topic models. We empha-size capturing correlated semantic concepts in docu-ments as well as characterizing general and specific words, in order to identify topically coherent sen-tences in documents. We showed empirically that a fully unsupervised model for extracting general sen-tences performs well at summarization task using datasets that were originally used in building auto-matic summarization system challenges. The suc-cess of our model can be traced to its capability of directly capturing coherent topics in documents, which makes it able to identify salient sentences. The authors would like to thank Dr. Zhaleh Feizol-lahi for her useful comments and suggestions.
