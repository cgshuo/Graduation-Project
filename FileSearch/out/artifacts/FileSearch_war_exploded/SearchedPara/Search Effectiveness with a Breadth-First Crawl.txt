 Previous scalability experiments found that early precision improves as collection size increases. However, that was under the assumption that a collection X  X  documents are all sampled with uniform probability from the same population. We contrast this to a large breadth-first web crawl, an im-portant scenario in real-world Web search, where the early documents have quite different characteristics from the later documents.
 H.3.3 [ Information Storage and Retrieval ]: Information Storage and Retrieval Experimentation, Measurement
The Web is a very large collection of pages and search engines serve as the primary access mechanism to the con-tent. To be able to provide the search functionality, search engines use crawlers that automatically follow links to web pages and extract the content over which indexes are built.
Crawling is usually described as a process that begins with a set of seeds, gathering new pages based on a pre-defined link exploration policy. When the crawler visits a page for the first time, it extracts all out-links on this page and adds them to the list of candidate links yet to be visited.
Given the infinite size of the web, there are constraints that impose the need for the crawler to stop downloading new pages at a pre-defined point (for example, a limit on the number of pages that can be indexed). It is therefore important to ensure that good pages get visited early on in the process. Past work has differed in terms of how they interpret the phrase  X  X ood page X .
 For example, [1] uses link-based popularity metrics (like PageRank) to reflect the importance of a page. It is a rea-sonable expectation that in the presence of the early stop-ping criterion, greedily following links into popular URLs will lead to a good collection of pages. The RankMass of a crawler [2] formalises this notion by defining an index quality metric that is the sum of the PageRank of its constitutent pages. It should however be remembered any link-based metrics that the crawler uses to make its decisions are go-ing to be inaccurate because they are based on incomplete information and quantities like PageRank are expensive to compute. Given these considerations, the natural breadth-first search policy [5] is a good alternative.

By definition, a good crawl ordering policy is one that is able to stop potentially relevant search results from being crowded out by useless and redundant pages. With the crawl showing such diminishing returns, our goal is to evaluate retrieval effectiveness. The trajectory of effectiveness over the progress of the crawl helps us understand when to stop the crawl.

Previous studies on the relationship between collection size and retrieval effectiveness [4] found that early precision improves as collection size increases. Hawking and Robert-son X  X  approach was to take a collection of 100 gigabytes con-taining 18 million documents, and measure early precision for corpus sizes of 100, 10 and 1 gigabytes. They found that the highest precision was achieved with the largest collec-tion.

It is noted that the crawling scenario is in contrast to sub-sampling a large collection because pages encountered early in the crawl are fundamentally different from those reached later on. Taking this into consideration, we wish to esti-mate the potential benefit of continuing the expansion of the crawl. We measure this benefit by tracking the following two quantities at periodic checkpoints of a breadth-first crawl: (a) A link-based index quality metric calculated as the cu-mulative sum of PageRank calculated on the complete crawl (b) Retrieval effectiveness, represented by NDCG@100, cal-culated on a reference set of queries. For the experiment described here, we crawled 696,168,028 URLs between October 25, 2007 and November 28, 2007. Our crawl was started using the URL of the homepage of the Open Directory Project as the single seed. The crawl expanded out in breadth-first order. Once the crawl was completed, we constructed the link-graph of the entire col-lection of pages and calculated the PageRank [6] of each URL.

At chosen instances during the crawl, referred to here as checkpoints , we calculated the values of two crawl quality metrics (we had 29 inspection instances). The first one of these is the cumulative sum of global PageRank values of all pages crawled up until each checkpoint. The word global is used to indicate that this quantity was calculated on the final completed crawl. This metric was chosen to represent Sum of global PageRank a variation of RankMass [2], a measure used to compare the quality of search-engine indexes.

A set of reference queries was constructed by sampling uniformly from the workload of the Live Search engine. These were matched with URLs judged on a 5 point scale for rele-vance: Bad, Fair, Good, Excellent and Perfect. Navigational results for a query (if any) were assigned the Perfect rating. We constructed a retrieval function that combines the well-known BM25 scoring method with an inlink prior using the method described in [3]. While a ranking function so pro-duced might not reflect state-of-the-art, we believe that the trends of effectiveness results so produced will be indicative of more sophisticated rankers.

Our ranker was used to generate result sets of size 100 for each query in our reference set at each checkpoint. Using the relevance judgments available for each query, we calculated the NDCG for these result sets, representing search effective-ness achieved on a collection comprising of URLs crawled up until this checkpoint. By tracing the value of NDCG through the checkpoints, we can estimate the retrieval-effectiveness-based utility of continuing the crawl. The next section pro-vides the results.
As can be seen in Figures 1 and 2, the trajectories taken by the two measures is different. The cumulative PageRank, which is a link-based measure of crawl corpus quality, has a steep rise at the start and this confirms previous work that a breadth-first search strategy obtains good pages at the very beginning [5]. The linear dependence between crawl size and the value of the metric is also perhaps expected given published research. We might expect that this curve begins to flatten at some point, this saturation point can be guessed to be much larger than 700 million (the size of our crawl).

The search-based measure of crawl quality behaves dif-ferently. The curve for NDCG also has a spike at the start, most likely because of results for navigational queries. There-after, the curve increases steadily, suggesting that the breadth-first crawl continues to improve user satisfaction for some time. Around the 225 million mark, this curve plateaus out indicating that there are diminishing returns, with respect to retrieval effectiveness. We also calculated the average Jaccard similarity between result sets on the current set of URLs and the final complete crawl, and observed almost perfect linear correlation between increasing crawl size and ! DCG @100 the average Jaccard coefficient between result sets produced on the final crawl and the intermediate checkpoints.
We acknowledge that the retrieval-based metrics reported here are dependant on the specific set of queries used, but given the scale of the experiment (  X  700 million URLs and  X  7300 queries), the observed trends are expected to be re-liable. In particular, we believe that tracking the behavior of alternate quality metrics through stages of the crawl is a novel experiment.
In this paper, we have described our experiments that performed a large, breadth-first crawl of  X  700 million URLs and used a set of reference queries and manually judged re-sults to calculate retrieval effectiveness at 29 different points in the crawl. We also compute the fraction of global PageR-ank at each of these points in the crawl and confirm previous results. We find that the link-based measure of crawl quality behaves differently than the search-based measure. A com-prehensive investigation of this difference is future work. [1] J. Cho, H. Garcia-Molina, and L. Page. Efficient [2] J. Cho and U. Schonfeld. RankMass crawler: a crawler [3] N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor. [4] D. Hawking and S. Robertson. On collection size and [5] M. Najork and J. L. Wiener. Breadth-first crawling [6] L. Page, S. Brin, R. Motwani, and T. Winograd. The
