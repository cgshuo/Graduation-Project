 This work studies the problem of distributed classification in peer-to-peer (P2P) networks. While there has been a significant amount of work in distributed classification, most of existing algorithms are not designed for P2P networks. Indeed, as server-less and router-less systems, P2P networks impose several challenges for distributed classification: (1) it is not practical to have global synchronization in large-scale P2P networks; (2) there are frequent topology changes caused by frequent failure and recovery of peers; and (3) there are frequent on-the-fly data updates on each peer.
In this paper, we propose an ensemble paradigm for dis-tributed classification in P2P networks. Under this paradigm, each peer builds its local classifiers on the local data and the results from all local classifiers are then combined by plural-ity voting. To build local classifiers, we adopt the learning algorithm of pasting bites to generate multiple local classi-fiers on each peer based on the local data. To combine local results, we propose a general form of Distributed Plurality Voting ( DPV ) protocol in dynamic P2P networks. This protocol keeps the single-site validity for dynamic networks, and supports the computing modes of both one-shot query and continuous monitoring. We theoretically prove that the condition C 0 for sending messages used in DPV 0 is locally communication-optimal to achieve the above properties. Fi-nally, experimental results on real-world P2P networks show that: (1) the proposed ensemble paradigm is effective even if there are thousands of local classifiers; (2) in most cases, the DPV 0 algorithm is local in the sense that voting is pro-cessed using information gathered from a very small vicinity, whose size is independent of the network size; (3) DPV 0 is significantly more communication-efficient than existing al-gorithms for distributed plurality voting.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining; C.2.4 [ Computer-communication Networks ]: Distributed Systems X  Distributed applications Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Algorithms, Experimentation Distributed classification, P2P networks, Distributed plural-ity voting
Peer-to-peer (P2P) networks [9] are an emerging technol-ogy for sharing content files containing audio, video, and realtime data, such as telephony traffic. A P2P network relies primarily on the computing power and bandwidth of the participants in the network and is typically used for connecting nodes via largely ad hoc connections. Detection and classification of objects moving in P2P networks is an important task in many applications.

Motivating Examples. In a P2P anti-spam network, millions of users form a community and spam emails are defined by a consensus, particularly in a collaborative en-deavor. Essentially, the P2P network allows users to share their anti-spam experiences without exposing their email content. Another example is to automatically organize Web documents in a P2P environment [20]. Initially, the focused crawler on each peer starts to gather the Web pages, which are related to the user-specified topic taxonomy. Based on the training data, a local predictive model can be derived, which allows the system to automatically classify these Web pages into specific topics. Once we link together a large number of users with shared topics of interest in P2P net-works, it is natural to aggregate their knowledge to construct a global classifer that can be used by all members.
However, P2P networks are highly decentralized, dynamic and normally includes thousands of nodes. Also, P2P net-work usually does not have routers and the notion of clients or servers. This imposes several challenges for distributed classification in P2P networks. First, it is not practical to have global synchronization in large-scale P2P networks. Also, there are frequent topology changes caused by frequent failure and recovery of peers. Finally, there are frequent on-the-fly data updates on each peer.

To meet the above challenges, in this paper, we propose to build an ensemble classifier for distributed classification in P2P networks by plurality voting on all the local classi-fiers, which are generated on the local data. To this end, we first adapt the training paradigm of pasting bites [6, 8] for building local classifiers. Since data are not uniformly distributed on each node, the number of local classifiers re-quired to generate can be different for different local regions. As a result, we provide a modified version of pasting bites to meet this requirement in P2P networks.

Next, to combine the decisions by local classifiers, we for-malize the problem of Distributed Plurality Voting (DPV) in dynamic P2P networks. Specifically, we propose a general form of DPV protocol which keeps the single-site validity [3] for dynamic networks, and supports the computing modes of both one-shot query and continuous monitoring. Fur-thermore, we theoretically prove that the condition C 0 for sending messages used in DPV 0 is locally communication-optimal to achieve the above properties. While C 0 used in DPV 0 is only locally (not globally) communication-optimal, the experimental results show that DPV 0 is significantly more communication-efficient than existing algorithms for distributed plurality voting. Thus, this ensemble paradigm is communication-efficient in P2P networks, since neighbor-to-neighbor communication is mainly concerned in combin-ing local outputs by DPV without any model propagations.
Finally, we have shown that our DPV algorithm can be extended to solve the problem of Restrictive Distributed Plurality Voting (RDPV) in dynamic P2P networks. This problem requires that the proportion of the maximally voted option to all the votes be above a user-specified threshold. RDPV can be used in a classification ensemble in a restric-tive manner, i.e. by leaving out some uncertain instances.
Related literature can be grouped into two categories: en-semble classifiers and P2P data mining.

Ensemble classifiers. The main idea is to learn an en-semble of classifiers from subsets of data, and combine pre-dictions from all these classifiers in order to achieve high classification accuracies. There are various methods for com-bining the results from different classifiers, such as voting and meta-learning [7]. In (weighted) voting, a final decision is made by a plurality of (weighted) votes. Lin et al. [16] theoretically analyzed the rationale behind plurality voting, and Demrekler et al. [12] investigated how to select an op-timal set of classifiers. In addition, meta-learning is used to learn a combining method based on the meta-level attributes obtained from predictions of base classifiers. The meta-learning method includes arbiter [7], combiner [7], multi-response model trees [11] using an extended set of meta-level features, meta decision trees [21] and so on.

Ensemble classifiers have been used for classification in both centralized and distributed data. For centralized data, base-level classifiers are generated by applying different learn-ing algorithms with heterogeneous models [18], or a single learning algorithm to different versions of the given data. To manipulate the data set, three methods can be used: ran-dom sampling with replacement in bagging [5], re-weighting of the mis-classified training examples in boosting [14], and generating small bites of the data by importance sampling based on the quality of classifiers built so far [6].
For naturally distributed data, several techniques have been proposed as follows. Lazarevic et al. [15] give a dis-tributed version of boosting algorithm, which efficiently in-tegrates local classifiers learned over distributed homoge-neous databases. Tsoumakas et al. [22] present a framework for constructing a global predictive model by stacking local classifiers propagated among distributed sites. Chawla et al. [8] present a distributed approach to pasting small bites, which uniformly votes hundreds or thousands of classifiers built on all distributed data sites. Chawla X  X  algorithm is fast, accurate and scalable, and illuminates the classifica-tion framework proposed in the following section.
P2P data mining. With the rapid growth of P2P net-works, P2P data mining is emerging as a very important research topic in distributed data mining. Approaches to P2P data mining have focused on developing some primi-tive operations as well as more complicated data mining al-gorithms [9]. Researchers have developed some algorithms for primitive aggregates, such as average [19, 17], count [3], sum [3], max , min , Distributed Majority Vote (DMV) [25] and so on. The aggregate count , sum and DMV are in-herently duplicate sensitive, which requires that the value on each peer be computed only once. To clear this hurdle Paper [3] processes count and sum by probabilistic count-ing in an approximate manner, while the DMV algorithm in [25] performs on a spanning tree over the P2P network. A main goal of these studies is to lay a foundation for more sophisticated DM algorithms in P2P systems. Pioneer works for P2P DM algorithms include P2P association rule min-ing [25], P2P K-means clustering [10], P2P L2 threshold monitor [24] and outlier detection in wireless sensor net-works [4]. They all compute their results in a totally de-centralized manner, using information from their immedi-ate neighbors. A recent paper [20] presents a classification framework for automatic document organization in a P2P environment. However, this approach propagates local mod-els between neighboring peers, which heavily burdens the communication overhead. It only focuses on the accuracy issue, and does not involve any dynamism of P2P networks. The experiments of this method were performed only with up to 16 peers. Thus, it is just a minor extension of small-scale distributed classification.
 To the best of our knowledge, the most similar work to our DPV problem is the work on DMV [25]. To illustrate the dif-ference, let us consider the case that a group of peers would agree on one of d options. Each peer u conveys its preference by initializing a voting vector P  X  u  X  N d (where N is the set of integers), and P u [ i ] is the number of votes on the i -th op-tion. DPV is to decide which option has the largest support over all peers, while DMV is to check whether the voting proportion of a specified option to all the votes is above a given threshold. Thus, DPV is a multi-valued function while DMV is a binary predicate. In addition, the advantage of DPV over DMV will be further illustrated in Section 4.
In this section, we introduce the method for building local classifiers on the local data in P2P networks. Specifically, the approach we are exploring is built on top of the pasting bites method, which is also known as Ivote [6]. In the Ivote method, multiple local classifiers are built on small train-ing sets (bites) of a specific local data and the bite for each subsequent classifier relies on the plurality voting of the clas-sifiers built so far. In other words, bites are generated from a specific local data by sampling with replacement based on the out-of-bag error . Also, a local classifier is only tested on the instances not belonging to its training set. This out-of-bag error estimation provides a good approximation on the generalization error, and is used as the stop condition for the training process. Indeed, Ivote is very similar to boost-ing, but the sizes of  X  X ites X  are much smaller than that of the original data set. Algorithm 1 A Pasting Bites Approach for Building Local Classifier Input: the size of each bite, N ; the minimal difference of Output: multiple local classifiers 1: if the size of local data on the peer is less than N then 2: Learn a classifier on the whole data by  X  3: else 4: Build the first bite of size N by sampling with re-5: Compute the smoothed error, e ( k ); and the probabil-6: For the subsequent bite, an instance is drawn at ran-7: Learn the ( k + 1)-th classifier on the bite created by 8: Repeat steps 5 to 7, until | e ( k )  X  e ( k  X  1) | &lt;  X  9: end if
The key of the pasting bites method is to compute the out-of-bag error rate r ( k ) of the k current aggregated clas-sifiers on the local data. To compute r ( k ), we add two data structures  X  and ~ V on each instance of the local data.  X  records the index of the last classifier, whose training set includes this instance. ~ V records the vote values of the in-stance on each class label by the out-of-bag classifiers so far. The details of this algorithm are shown in Algorithm 1. In k -th round of Steps 5 through 7, for each instance ~ V is up-dated by the last built classifier only when  X  6 = k  X  1. Then, by plurality voting, r ( k ) can be computed easily from the ~ V s on all the instances. The stop criteria in Step 8 satis-fies if the difference of error rates between two successively generated classifiers is below  X  .

Although the local classifier is only trained on a small portion of the raw data, there is still a large communication burden for model propagations when thousand or even more local classifiers participate into the classification process. In addition, local models are frequently updated caused by fre-quent updates of distributed data. Therefore, for distributed classification as described in the next section, each peer is responsible for maintaining its own local classifiers, which are never propagated in the rest of the network.
In this section, we present a Distributed Plurality Voting (DPV) protocol for classification in P2P networks. Specif-ically, every peer will participate in the distributed voting. The final prediction is made by the local votes from all the local classifiers. The distributed plurality voting protocol is used to make sure that the results from all nodes that are reachable from one another will converge.
We first give the problem definition. Following the nota-tions in [25], we assume that a group U of peers in a P2P network, denoted by a connected graph G ( U, E ), would like to agree on one of d options. Each peer u  X  U conveys its preference by initializing a voting vector P  X  u  X  N d , where N is the set of integers and P  X  u [ i ] is the number of votes on the i -th option. DPV is to decide which option has the largest number of votes over all peers (it may contain multiple max-imally voted options), formalized as arg max i
For distributed majority voting (DMV) [25], each peer u  X  U initializes a 2-tuple  X  sum  X  u , count  X  u  X  , where sum stands for the number of the votes for an option on peer u and count  X  u stands for the number of the total vote on peer u . This majority voting is to check whether the voting pro-portion of the specified option is above a given majority ratio  X  . This is formalized as sgn
From the above, we know that DPV is a multi-valued function while DMV is a binary predicate. Also, we can see that DMV can be converted to DPV by replacing the 2-tuple  X  sum  X  u , count  X  u  X  on each peer with the voting vec-tor  X  sum  X  u ,  X   X  count  X  u  X  . However, DMV can only be used for 2-option DPV as a pairwise comparison. For a d -option DPV problem, pairwise comparisons among all the d options must be performed by DMV for d  X  ( d  X  1) 2 times, as suggested by Multiple Choice Voting [25], whose function is actually to rank the d options by their votes (in the rest of this paper, we refer this algorithm as RANK ). The proposed DPV algorithm in this paper do not need to perform pairwise comparisons many times. Instead, it finds the maximally supported option directly, and thus saves a lot of commu-nication overhead and the time for convergence. Therefore, DPV is a more general form of DMV.
 Algorithm 2 C 0 ( P vu , P uv ,  X  uv ) 1: for all i uv such that i uv  X  ~ i uv do 2: for all j := 1 ,  X  X  X  , d such that j 6 = i uv do 3: if ( X  uv [ i uv ]  X   X  uv [ j ]) &lt; ( P uv [ i uv ]  X  P 4: return true 5: end if 6: end for 7: end for 8: return false
The purpose of DPV is to make sure that every node in the network converges toward the correct plurality through neighbor-to-neighbor communication. The DPV protocol includes a mechanism to maintain an un-directional span-ning tree [26] for the dynamic P2P network, and a node is informed of changes in the status of adjacent nodes.
Because there is only one path between two nodes in a tree, the voting vector on each peer is only added once through the edges in the tree. This ensures that the DPV protocol is duplicate insensitive . Algorithm 3 DPV Protocol
Input for node u : The set E u of edges that collide with it, the local voting vector P  X  u . C is the condition for sending messages.

Output : The algorithm can be used for both one-shot query and continuous monitoring. It outputs ~ i u := arg max i Definitions : See notation in Table 1.

Initialization : For each vu  X  E u , set P vu to null, and send P  X  u over uv to v .
 On failure of edge vu  X  E u : Remove vu from E u . On recovery of edge vu  X  E u : Add vu to E u , and send P  X  u over uv to v .
 On message P received over edge vu : Set P vu to P .
On any change in  X  uv ( uv  X  E u ), resulting from a change in the input, edge failure or recovery, or the receiving of a message : for each vu  X  E u do end for
The DPV protocol specifies how nodes react when the data changes, a message is received, or a neighboring node is reported to have detached or joined. The nodes commu-nicate by sending messages that contain the voting vector. Each node u will record, for every neighbor v , the last mes-sage it sent to v , P uv , and the last message it received from v , P vu . For conciseness, we extend the group of edges collid-ing with u , denoted by E u , to include the virtual edge  X  u , and the extended edge set is denoted by N u . These related notions are listed in Table 1. Node u calculates the following functions of its messages and its own voting vector: When the local voting vector changes, a message is received, or a node connects to u or disconnects from u , The above functions, including  X  uv ,  X  uv , ~ i uv and ~ i u , will be recalcu-lated.  X  uv is the message when the protocol asks u to send to v when necessary.  X  uv and ~ i uv are the voting vectors and the corresponding set of all maximally voted option(s) on v fromtheviewpointof u ,respectively.Notethatifnomes-sage is yet received from any neighbor, then ~ i u converges to the right result, the option(s) with the largest number of votes over all the peers.

The enforcement of the voting proposal on each node is independent from that of the immediate neighbors of this node. Node u coordinates its plurality decision with node v by making sure that P uv will not lead v to a wrong result. When  X  uv changes, the protocol dictates that u send v a message,  X  uv , if a certain condition C 0 satisfies. This condi-tion is actually a boolean function of three inputs P vu , P and  X  uv . In the following, we detail this condition, which is the key of the DPV protocol.
Algorithm 2 shows the pseudo-code of C 0 for sending mes-sages. The inequality in Algorithm 2 says that if  X  uv can result in a decrease of the voting difference between the max-imally voted option i uv and any other option j ,  X  uv must be sent. Actually, C 0 is the more generic and extended form of the condition in DMV [25] for generic d -option DPV prob-lems. These two conditions are equivalent only when solving 2-option voting problems. Therefore, DPV and DMV are actually equivalent for 2-option voting problems. However, DMV cannot solve d -option DPV problems ( d &gt; 2) directly. In addition, C 0 saves much communication overhead for d -option DPV problems ( d &gt; 2), because it avoids multiple pairwise comparisons in RANK .
Algorithm 3 shows the pseudo-code of the general form of the DPV protocol with a condition C as an input. DPV 0 is an instance of this algorithm, in which C 0 is used.
To transparently deal with the frequent changes of the net-work topology and the inputs of voting vectors, this DPV protocol is designed in a way such that every peer regards itself as the root of the spanning tree. During the execution of this algorithm, each node maintains an ad-hoc solution. If the system remains static long enough, this solution will quickly converge to the exact (not approximate) answer. For the dynamic system, where nodes dynamically join or depart and the votes on each peer change over time, the ad-hoc so-lutions are adjusted quickly and locally to the current right answer. Therefore, this DPV protocol keeps the single-site validity , and supports the computing modes of both one-shot query and continuous monitoring for distributed plurality voting in P2P networks. Additionally, in Section 5, our ex-perimental results also show that this protocol demonstrates a local effect: in the most of cases, each node computes the plurality voting based on information arriving from a very small surrounding environment. Locality implies that this algorithm should be scalable to very large networks.
It can be proved that DPV 0 keeps the correctness of dis-tributed plurality voting. When this protocol specifies that no node needs to send any message, ~ i u converges to the right result for all nodes. If there is a disagreement, there must be a disagreement between two immediate neighbors, in which case at least one node satisfies the inequality and sends a message. This will cause the total number of votes received to increase. This total number is bounded by the number of classifiers in the system. Hence, this protocol always reaches the right consensus in a static state. The details of this proof are omitted due to the space limit and can be found in the Author X  X  web site 1 .
 Algorithm 4 C e ( P vu , P uv ,  X  uv ), 0  X  e  X  1 1: if C 0 ( P vu , P uv ,  X  uv )= true then 2: return true 3: end if 4: e 0 :=a random value in [0 , 1) 5: if e 0 &lt; e then 6: return true 7: else 8: return false 9: end if
In the following, we provide a formal description of the local communication optimality for the DPV 0 protocol, and then theoretically prove it. It is shown that C 0 is the most restrictive condition for sending messages to support the computing modes of both one-shot query and continuous monitoring for distributed plurality voting in P2P networks.
Definition 1. Condition C 1 is more restrictive than con-dition C 2 denoted by C 1  X  C 2 , if and only if, for any input case if C 1 satisfies then C 2 satisfies.

Definition 2. Condition C 1 is strictly more restrictive than condition C 2 denoted by C 1  X  C 2 , if and only if, C more restrictive than C 2 and there at least exists an input case such that C 2 satisfies and C 1 does not.

Theorem 1. For the problem of distributed plurality vot-ing there does not exist a condition C which is strictly more restrictive than C 0 , such that the Algorithm 3 with C as the condition for sending messages still reaches the right answer for every input case.

Proof. We first assume that such a condition C exists, then construct the cases for which it reaches a wrong result. Without loss of generality, the following analysis is under the assumption of a 2-option DPV problem.

Because C  X  C 0 , at least the following case  X  exists: u and v are two immediate neighbors, C 0 ( P uv , P vu ,  X  uv true , C ( P uv , P vu ,  X  uv ) is false . We assume that P ( u [1] , u [2]), P vu = ( v [1] , v [2]),  X  uv = ( u [1] http://cimic.rutgers.edu/  X  hui/publication/publication.html construct the error case in Figure 1, in which v is surrounded by k nodes of u 1 ,  X  X  X  , u k where
In this figure the vector on the double-arrowhead side is the corresponding vector P which has been sent, the vector on the double-arrowtail side is the corresponding vector  X  which will be sent if the condition satisfies.

Initially, P  X  v is set as a vector ( x, y ) such that initial setting, all the peers converge to the state that the second vote is the maximally voted option.

After this convergence, P  X  v changes to ( v [1]  X  ( k  X  1)  X  u [1] , v [2]  X  ( k  X  1)  X  u [2]). Because u [1] + v [1] &gt; u [2] + v [2], v changes to the state that the first vote is the maxi-mally voted option. Then, it sends ( v [1] , v [2]) to u i
At this time,  X  u i v on each node u i ( i = 1 ,  X  X  X  , k ) changes is not sent from u i to v . Thus, v remains to the state that the first vote is the maximally voted option. However, the right state for the whole network is the second one, because by the initial setting the following inequality holds: It generates an error case, and follows the conclusion.
The above proof of Theorem 1 shows that to keep the cor-rectness of Algrithm 3,  X  uv must be sent from u to v for any case, which satisfies the following conditions: u and v are two immediate neighbors, C 0 ( P uv , P vu ,  X  uv ) is true ; other-wise, it will result in the error case in Figure 1. Therefore, the conditions in Algorithm 4 for 0  X  e  X  1 includes all the conditions for sending messages, which keep the correctness of Algrithm 3. In this family of conditions, C 0 is the most restrictive one. We should mention that C 0 is a local opti-mal condition, not a global one. This indicates that, for a certain input case, the total communication overhead used in C e (0 &lt; e  X  1) may be smaller than that used in C 0 However, the careful experiments conducted in the follow-ing will show that the case mentioned above is very rare, and C 0 is significantly more communication-efficient than C (0 &lt; e  X  1).
Here, we show that our DPV protocol can be extended to solve the problem of Restrictive Distributed Plurality Vot-ing (RDPV) in dynamic P2P networks, which outputs the maximally voted option whose proportion to all the votes is above a user-specified threshold  X  . Using the notation in Section 4.1 this output is formalized as follows: k  X  arg max
This can be used in P2P classification in a restrictive man-ner. In this manner the classification decision can only be made when the proportion of the maximally supported op-tion to all the votes is not below the threshold  X  , and the uncertain instance is discarded.

This problem can be solved by Algorithm 3 with the fol-lowing modifications: (1) the threshold  X  is added to the inputs of this algorithm; (2) the condition M for sending messages in Algorithm 5 is used; (3) the output changes to
The lines through 7 to 12 in Algorithm 5 are responsible for the additional check of the additional constrain for RDPV. And this check is only applied to the current maximally voted option i uv . The correctness of M is directly based on the correctness of C 0 , and thus the proof is omitted. Algorithm 5 M ( P vu , P uv ,  X  uv ,  X  ) 1: for all i uv such that i uv  X  ~ i uv do 2: for all j := 1 ,  X  X  X  , d such that j 6 = i uv do 3: if ( X  uv [ i uv ]  X   X  uv [ j ]) &lt; ( P uv [ i uv ]  X  P 4: return true 5: end if 6: end for 7: Q vu := ( P vu [ i uv ] ,  X   X  8: Q uv := ( P uv [ i uv ] ,  X   X  9:  X  uv := ( X  uv [ i uv ] ,  X   X  10: if C 0 ( Q vu , Q uv ,  X  uv ) = true then 11: return true 12: end if 13: end for 14: return false
In this section, we present the experimental evaluation of our approach for P2P classification. Specifically, we demon-strate: (1) the performance of P2P classification in terms of accuracy, (2) the performance comparison of our DPV algo-rithms and RANK [25] in terms of communication overhead and the converge time. Three types of networks, including power-law graph [2], random graph [23] and grid, are used in our experiments. The power-law and random graphs are generated by BRITE 2 with the default parameters. For the n -node grid, the n 1  X  n 2 grid is generated where n 1 is the the largest common divisor of n and n 1  X  n 2 = n .
Due to the lack of labeled P2P data, we used the covtype (581012  X  54, 7 classes) data set, which is the largest data set in the UCI repository 3 . We first randomly divided this data set into hundreds of disjoint partitions in a way such that the size of each partition is not too small. Then, each partition was assigned to a peer in the P2P network. The P2P network used in this experiment is a power-law graph with 500 nodes. For this distributed classification, the data size s i of peer i is set to be proportional to its number of immediate neighbors k i , so that s i = s  X  k i P the size of the original dataset. The rationale behind this is that if k i of peer i is big, it indicates that peer i stays in this network for a long time, and thus accumulate more http://www.cs.bu.edu/brite/ http://www.ics.uci.edu/  X  mlearn/MLRepository.html data. Also, we make the following initial settings to Algo-rithm 1: N = 800 ,  X  = 0 . 002. Finally, the J48 (a variant of C4.5) from the Weka Package 4 is used as the base training algorithm and a 10-fold cross validation is performed.
First, we compare the performance of the P2P classifica-tion and classification on the centralized data. The classi-fication algorithm for the centralized data is the same with the local training algorithm on each peer. Next, we also evaluate the performance of this P2P classification by re-strictive plurality voting, where the instance is discarded if the proportion of the maximally supported option to all the votes is below a threshold  X  . By changing the  X  values, we study the relationship between the accuracy (the fraction of the correctly classified instances) and the loss (the frac-tion of the discarded instances). The results of these two experiments are shown in Table 2.

As can be seen in Table 2, the accuracy of distributed learning is pretty close to that of centralized learning. In-deed, the difference is less than 6% for the most cases. This difference is mainly due to the fact that some classification patterns may only exist in the centralized data. In addition, Table 2 also demonstrates that the accuracy is increased as the increase of  X  values. However, this improvement is at the cost of the increase of data loss. Thus, there is a tradeoff between accuracy and loss.
To evaluate the performance of our DPV algorithms, we have implemented a discrete event simulator which has the capabilities in processing networks with different sizes and types. For each time unit (1 ms), this simulator checks each peer to see whether the condition for sending messages satis-fies. It is assumed that each peer can only process one mes-sage in a time unit, and the delays on the immediate links are uniformly distributed between 40 ms and 1000 ms. Since the algorithms for one-shot query and continuous monitor-ing are performed in the same manner, we only measure the one-shot query in this experiment. For the one-shot query of plurality voting, a network topology and the voting vectors on all peers are firstly initialized and will not change during protocol execution.

We use two performance metrics: communication over-head and convergence time. In the experiment, we record the following two time points for an one-shot query: t c (the time of convergence), which is the time when each peer reaches a right and stable result (sometimes all peers may reach the right results, however, the result on certain peer may not be stable); t n , which is the time when no message is propagated in the network. t c is always before t n . In addition, we define the communication overhead of peer u as the volume of the messages it has sent. This overhead is equal to the number of received messages times the num-http://www.cs.waikato.ac.nz/ml/weka ber of entries in each message 5 . The locality of a protocol is measured mainly according to the average communica-tion overhead of all the nodes, denoted by  X  a . The two time points t c and t n corresponds to two average communication overheads :  X  a c and  X  a n .  X  a c is the  X  a consumed before t  X  a n is the  X  a consumed before t n .

In the following experiments, we evaluate the performance of DPV from three aspects. First, we present a performance comparison of DPV 0 and RANK algorithms. Next, we show the scalability of DPV 0 . Finally, we illustrate the local op-timality of DPV 0 .
For both DPV 0 and RANK 6 , the average metrics over 2000 instances of 7-option plurality voting over 500-node networks are computed. In the following experimental data, the unit of communication overhead is the volume of each message consumed by this 7-option distributed plurality vot-ing. Thus, each message consumed by RANK has 2 7 units.
We use a statistical method [13] in comparing these algo-rithms. This statistical method allows to give a rank to both algorithms for each instance; that is, a rank 1 is assigned to a winning algorithm and a rank 2 is assigned to the losing one. If there is a tie, the average value 1 . 5 is assigned to both algorithms. Let r j i be the rank of the j -th algorithm on the i -th of N instances, this method compares the aver-age ranks of algorithms, R j = 1 N test [13].

Tables 3, 4, and 5 show the comparison results on three network topologies: power-law graph, random graph and grid. As can be seen in these tables, DPV 0 significantly outperforms RANK in terms of both  X  a n and t n .
Our experimental results also indicate that the locality of DPV relates to the parameter r of the input votes: r = P the second largest number of votes. Figures 2 (a), (b), and (c) show the values of the  X  a n for DPV 0 at different r val-ues, for three network topologies: power-law graph, random graph and grid, respectively. In these figures, we can see that  X  a n decreases as the increase of r . In addition, these fig-ures show that in most cases ( r &gt; 300)  X  a n is always smaller than 5. This indicates that DPV 0 has a local effect in the sense that voting is processed using information gathered from a very small vicinity.

Finally, we observe that the average rank values of  X  a n
Note that each message communicated by DPV contains d entries; however, every message communicated by RANK contains only 2 entries. This has been considered when com-paring the communication overheads of two algorithms.
For RANK, we assume that the d  X  ( d  X  1) 2 pairwise compar-isons are performed one after another; thus, the metric con-sumed by RANK is the sum of the metrics of the d  X  ( d  X  1) pairwise comparisons.
 DPV 0 in Tables 3 and 5 are a little greater than 1. This indicates that there exists such a rare input case, where the  X  a n for RANK is smaller than that for DPV 0 . We also find that for both DPV 0 and RANK the performance in terms of both  X  a n and t n for the grid is worse than that for the power-law and random graph. A possible reason for this finding is that the clustering coefficient [23] of the grid is much smaller than that of the power-law graph and random graph.
In this experiment, we use the synthetic data to study the scalability of DPV 0 . Specifically, we develop synthetic data models to simulate different problem environments for distributed plurality voting by changing model parameters. In general, there are two design issues for the generation of synthetic data models: the voting vector on each peer and the network topology.

We propose Algorithm 6 for voting vector modeling. Algo-rithm 6 has 3 parameters: n (the number of peers), cov (de-scribing the degree of difficulty for the voting vectors), and ~ P (the basic data vector). We first introduce this method, and then describe the meaning of the model parameters. Algorithm 6 The Voting Vector Modeling Input: n, cov, ~ P .
 Output: E n  X  d , where E [ i, j ] is the j -th vote value on the i -th peer and d = | ~ P | . ~
D :=generateDistribution( cov ) for all i := 1 ,  X  X  X  , n do end for return E
The function generateDistribution( cov ) generates a distri-bution vector ~ D by the gamma distribution [1], such that the bigger the value of cov is, the more different the values of all the entries ( ~ D [ i ] &gt; 0 for i = 1 ,  X  X  X  , d ) in function generateOneVoteVector( ~ D, ~ P ) randomly generates a voting vector ~ P 0 , such that the bigger the value of is, the more possible that ~ P 0 [ i ] is set by a bigger entry of ~ P , and each entry of ~ P is used only once (the pseudo-codes for these two functions are omitted). Hence, the bigger the value of cov is, the more possible that the entry ~ P 0 [ i ] with a much bigger distribution value ~ D [ i ] is set by the much bigger entry of ~ P . As a result, the more possible that the magnitude sequences of the voting vector entries on each peer are the same; and thus, the smaller the degree of diffi-culty for the voting vectors is. This is the reason why cov is used to describe the degree of difficulty for the input vectors. at different r values.
 All the parameters and their values used in the second part experiment are listed in Table 6.

The main goal of this experiment is to show that the per-formance of  X  a n for the DPV 0 algorithm is independent of the network size. These experiments are performed under the assumption that the degree of difficulty for the whole prob-lem remains unchanged with the increase of the network size. The vote modeling method in Algorithm 6 provides a great control over the degree of difficulty for the voting vectors; however, the networks with different topologies generated by BRITE may change the degree of difficulty for the problems with the increase of the network sizes, and it is hard to con-trol the factor of network topology. Due to this reason, not every experimental case supports the conclusion that the  X  a of the DPV 0 algorithm is independent of the size of the net-work. However, we did observe some experimental cases as shown in Figure 3. Additionally, the results on the synthetic data also show that DPV 0 significantly outperforms RANK in terms of both  X  a n and t n . Figure 3: The  X  a n of DPV 0 with respect to networks with different sizes.
Finally, in this experiment, we investigate the local opti-mality of DPV 0 by comparing it with other protocols DPV e ( e = 0 . 05 , 0 . 1 , 0 . 15 , 0 . 2, DPV e is the DPV protocol with the C e as its condition for sending messages). Tables 7 through 10 record these comparison results for the power-law graph. Tables 7 and 8 show that DPV 0 is significantly more communication-efficient than the other protocols in terms of both  X  a n and  X  a c , and the increase of e results in the increase of communication overhead. Table 9 shows that t n increases as the increase of e , while Table 10 shows that t decreases as the increase of e . The above results demon-strate that:
In general, although C 0 is local optimal, DPV 0 is the most communication-efficient protocol in most cases. For some practical needs, we can select a bigger value of e to decrease t (the convergence time) at the cost of the increase of com-munication overhead.
In this paper, we proposed an ensemble paradigm for dis-tributed classification in P2P networks. Specifically, we for-malized a generalized Distributed Plurality Voting (DPV) protocol for P2P networks. The proposed protocol DPV 0 imposes little communication overhead, keeps the single-site validity for dynamic networks, and supports the computing modes of both one-shot query and continuous monitoring. Furthermore, we theoretically prove the local optimality of the protocol DPV 0 in terms of communication overhead. Finally, the experimental results showed that DPV 0 out-performs alternative approaches in terms of both average communication overhead and convergence time. Also, the locality of DPV 0 is independent of the network size. As a result, our algorithm can be scaled up to large networks.
This work is supported by the National Science Foun-dation of China (No. 60435010, 90604017, 60675010), the 863 Project (No.2006AA01Z128), National Basic Research Priorities Programme (No. 2003CB317004) and the Nature Science Foundation of Beijing (No. 4052025). Also, this re-search was supported in part by a Faculty Research Grant from Rutgers Business School-Newark and New Brunswick. [1] S. Ali, H. J. Siegel, M. Maheswaran, S. Ali, and [2] A.-L. Barabsi and R. Albert. Emergence of scaling in [3] M. Bawa, A. Gionis, H. Garcia-Molina, and [4] J. Branch, B. Szymanski, C. Gionnella, R. Wolff, and [5] L. Breiman. Bagging predictors. Machine Learning , [6] L. Breiman. Pasting bites together for prediction in [7] P. Chan and S. Stolfo. A comparative evaluation of [8] N. V. Chawla, L. O. Hall, K. W. Bowyer, and W. P. [9] S. Datta, K. Bhaduri, C. Giannella, R. Wolff, and [10] S. Datta, C. Giannella, and H. Kargupta. K-means [11] S. D  X  z eroski and B.  X  Z enko. Is combining classifiers [12] M. Demrekler and H. Altincay. Plurality voting-based [13] J. Demsar. Statistical comparisons of classifiers over [14] Y. Freund and R. E. Schapire. Experiments with a [15] A. Lazarevic and Z. Obradovic. The distributed [16] X. Lin, S. Yacoub, J. Burns, and S. Simske.
 [17] M. Mehyar, D. Spanos, J. Pongsajapan, S. H. Low, [18] C. J. Merz. Using correspondence analysis to combine [19] A. Montresor, M. Jelasity, and O. Babaoglu.
 [20] S. Siersdorfer and S. Sizov. Automatic document [21] L. Todorovski and S. D  X  z eroski. Combining classifiers [22] G. Tsoumakas and I. Vlahavas. Effective stacking of [23] D. Watts and S. Strogatz. Collective dynamics of [24] R. Wolff, K. Bhaduri, and H. Kargupta. Local l2 [25] R. Wolff and A. Schuster. Association rule mining in [26] J. Zhao, R. Govindan, and D. Estrin. Computing
