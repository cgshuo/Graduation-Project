 Animashree Anandkumar a.anandkumar@uci.edu Department of EECS, University of California, Irvine Daniel Hsu dahsu@microsoft.com Microsoft Research New England Adel Javanmard adelj@stanford.edu Department of Electrical Engineering, Stanford University Sham M. Kakade skakade@microsoft.com Microsoft Research New England It is widely recognized that incorporating latent or hid-den variables is a crucial aspect of modeling. Latent variables can provide a succinct representation of the observed data through dimensionality reduction; the possibly many observed variables are summarized by fewer hidden effects. Further, they are central to pre-dicting causal relationships and interpreting the hid-den effects as unobservable concepts. For instance in sociology, human behavior is affected by abstract no-tions such as social attitudes, beliefs, goals and plans. As another example, medical knowledge is organized into casual hierarchies of invading organisms, physical disorders, pathological states and symptoms, and only the symptoms are observed.
 In addition to incorporating latent variables, it is also important to model the complex dependencies among the variables. A popular class of models for incorpo-rating such dependencies are the Bayesian networks, also known as belief networks. They incorporate a set of causal and conditional independence relationships through directed acyclic graphs (DAG) (Pearl, 1988). These models are widely applicable to a number of fields such as artificial intelligence, computational bi-ology, and economics, to name a few.
 An important statistical task is to learn such latent Bayesian networks from observed data. This involves discovery of the hidden variables, structure estima-tion (of the DAG) and estimation of the model pa-rameters. Typically, in the presence of hidden vari-ables, the learning task suffers from identifiability is-sues since there may be many models which can ex-plain the observed data. In order to overcome inde-terminacy issues, one must restrict the set of possi-ble models. We establish novel criteria for identifi-ability of latent DAG models using only low order observed moments (second/third moments). We in-troduce a graphical constraint which we refer to as the expansion property . Roughly speaking, expansion property states that every subset of hidden nodes has  X  X nough X  number of outgoing edges, so they have a noticeable influence on the observed nodes, and thus on the samples drawn from the joint distribution of the observed nodes. This notion implies new identifiability and learning results for DAG structures. More specif-ically, we show that under this constraint, some broad families of DAG models with hidden variables, includ-ing multi-level DAGs and DAGs with effective depth Figure 1: A multi-level DAG and a DAG with effective depth one (observed nodes are shaded). one, which includes (a subset of) trees and polytrees 1 satisfy this constraint and are thus, identifiable from only second and third observed moments. In addition, we propose novel and efficient algorithms for the learn-ing task which leverage on the ideas from sparse re-covery and dictionary learning (Spielman et al., 2012) as well as from spectral methods for inverse moment problems (Anandkumar et al., 2012a). 2.1. Notation We write k v k p for the standard ` p norm of a vector v . Specifically, k v k 0 denotes the number of non-zero entries in v . Also, k M k p refers to the induced opera-tor norm on a matrix M . For a matrix M and set of indices I,J , we let M I denote the submatrix contain-ing just the rows in I and M I,J denote the submatrix formed by the rows in I and columns in J . For a vector v , supp( v ) represents the positions of non-zero entries of v . We use e i to refer to the i -th standard basis element, e.g. , e 1 = (1 , 0 ,..., 0). For a matrix M we let Row( M ) (similarly Col( M )) denote the span of its rows (columns). For a set S , | S | is its cardinality. We use the notation [ n ] to denote the set { 1 ,...,n } . For a vector v , diag( v ) is a diagonal matrix with the ele-ments of v on the diagonal. For a matrix M , diag( M ) is a diagonal matrix with the same diagonal as M . 2.2. Model We define a DAG model as a pair ( G , P  X  ), where P  X  is a joint probability distribution, parameterized by  X  , on n variables x := ( x 1 ,...,x n ) that is Markov with respect to a DAG G = ( V , E ) with V = { 1 ,...,n } (Lauritzen, 1996). More specifically, the joint probability P  X  factors as where PA i := { j  X  V : ( j,i )  X  E} denotes the set of parents of node i in G .
 The learning task involving DAG models can be de-scribed as: Given i.i.d. samples generated from the joint distribution P  X  over x S for some S  X  X  , recover (some part of ) the graph structure G and estimate the model parameter  X  .
 We consider DAG G = ( V obs  X  X  hid , E ) with observed nodes V obs = { x 1 ,...,x n } and hidden nodes V hid = { h 1 ,...,h k } . Let  X  i be the noise variable associated with x i , for i  X  [ n ] and denote the variance of  X  ( h 1 ,...,h k ), x := ( x 1 ,...,x n ) and  X  := (  X  1 ,..., X  The noise terms  X  are assumed to be uncorrelated. The class of models considered are specified by the following assumptions.
 Condition 1 (Linear model) . The observed and hid-den variables obey the model 2 where {  X  i } are uncorrelated and are independent from { h j } . Furthermore, the hidden variables are linearly independent, i.e. , with probability one, if P i  X  [ k ]  X  0 , then  X  i = 0 , for all i  X  [ k ] .
 We note that without a non-degeneracy assumption on the hidden variables there is no hope of distinguishing different hidden nodes.
 Notice that the structure of G is defined by the non-zero coefficients in Eq. (2). Therefore, there is no edge among the observed nodes. We define A  X  R n  X  k by letting the ( i,j ) entry be a ij if j  X  PA i and zero oth-erwise. We refer to matrix A as the coefficient matrix . Remark 2.1. The linear relationships described above can be thought of as linear structural equation models (SEM). In general, an SEM is defined by a collection of equations with z i be the variables associated to the nodes. Re-cently, there has been some progress on the identi-fiability problem of SEMs in the fully observed set-ting (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011; Peters &amp; B  X uhlmann, 2012). This paper can be viewed as a contribution to the problem of iden-tifiability and learning SEMs with latent variables. We now describe sufficient conditions under which the linear DAG model with hidden variables becomes iden-tifiable. Given observations x , note that we can only hope to identify the columns of matrix A up to per-mutation because the model is unchanged if one per-mutes the hidden variables h and the columns of A correspondingly. Moreover, the scale of each column of A is also not identifiable. To see this, observe that Eq. (2) is unaltered if we both rescale all the coeffi-cients { a ij } j  X  [ k ] and appropriately rescale the variable h . Without further assumptions, we can only hope to recover a certain canonical form of A , defined as follows: Definition 2.2. We say A is in a canonical form if for each j  X  [ k ] ,  X  2 h transformation A  X  A diag(  X  h 1 , X  h 2 ,..., X  h corresponding rescaling of h place A in canonical form and the distribution over x i , i  X  [ n ] , is unchanged. Furthermore, observe that the canonical A is only specified up to sign of each column since any sign change of column i does not alter the variance of h i . We now discuss a rank condition on the coefficient ma-trix A .
 Condition 2 (Rank condition) . There exists a fixed partition P of [ n ] such that |P| = 3 , and A I has full column rank for all I  X  X  .
 Since rank( A I ) = k , for I  X  P , we have as a conse-quence n  X |P| k = 3 k . Therefore, it essentially states that the number of hidden nodes should be at most one third of the observed ones. In most applications, we are looking for a few number of hidden effects that can represent the statistical dependence relationships among the observed nodes. Thus the rank condition is reasonable in these cases. As we will see later, due to this assumption we can extract the noise term from the observed moments.
 We proceed by defining the expansion property of a graph which plays a key role in establishing our iden-tifiability results.
 Definition 2.3. Let H ( V 1 , V 2 ) be a bipartite DAG with parts V 1 and V 2 , and edges directed from V 1 to V We say that H ( V 1 , V 2 ) satisfies the expansion prop-erty if for any subset S  X  V 1 , with | S |  X  2 , we have | N( S ) |  X  | S | + d max , where N( S ) := { i  X  V 2 : ( j,i )  X  E for some j  X  S } is the set of the neighbors of S and d max is the maximum degree of nodes in V 1 .
 Condition 3 (Graph expansion) . Let H ( V hid , V obs ) denote the graph formed by the edges between V hid and V obs . Then, H ( V hid , V obs ) has the expansion property. The last condition is a generic assumption on the en-tries of matrix A . We first define the parameter gener-icity property for a matrix.
 Definition 2.4. We say that matrix M  X  R n  X  k has the parameter genericity property if for any v  X  R k with k v k 0  X  2 , the following holds true. where for a set S  X  [ k ] , N M ( S ) := { i  X  [ n ] : M 0 for some j  X  S } .
 Condition 4 (Parameter genericity) . The coefficient matrix A has the parameter genericity property. This is a mild generic condition. More specifically if the entries of an arbitrary fixed matrix M are perturbed independently, then it satisfies the above generic property with probability one.
 Remark 2.5. Fix any matrix M  X  R n  X  k . Let Z  X  R n  X  k be a random matrix such that { Z ij : M ij 6 = 0 } are independent random variables, and Z ij  X  0 when-ever M ij = 0 . Assume each variable is drawn from a distribution with uncountable support. Then P ( M + Z does not satisfy Condition 4 ) = 0 . (5) The proof of Remark 2.5 is available in the long version of this paper (Anandkumar et al., 2012b). 2.3. Summary of contributions We establish identifiability of different classes of linear DAG models from the observed data, and also pro-pose efficient algorithms for the learning task. In the following, we summarize our identifiability results and the proposed algorithms.
 Identifiability. Our core result is the following. Core result. Under the model assumptions in Sec-tion 2.2, one can identify the coefficient matrix A from the second order moment E [ xx &gt; ], without additional assumptions on the dependency relationships among the hidden nodes.
 This result shows how the graph expansion property enables the identifiability of connectivity structure be-tween the set of hidden nodes and the set of observed nodes for a general DAG. It is worth noting that the re-sult is obtained using only the second order moments. If the hidden nodes obey a Gaussian joint distribution, then so do the observed nodes and the second moment completely characterizes their joint distribution. But in general, the second moment provides strictly smaller amount of information than the entire joint distribu-tion. This makes our result robust to the noises in the observations as it relies on them only through the second moment.
 We next consider two ensembles of DAG models, namely multi-level DAGs and DAGs with effective depth one. Building upon our core result, we show that for these ensembles the induced model among the hidden nodes is also identifiable.
 Multi-level DAGs. This ensemble contains graphs with a hierarchal structure. The nodes of a multi-level DAG can be partitioned into levels L 1 ,...,L m , such that there is no edge within a level and all the edges are between nodes in level L i and the nodes in the adjacent levels L i  X  1 and L i +1 (see Fig. 1(a) for an illustration). Assuming that the induced model between levels L i and L i +1 obeys the conditions in Section 2.2 for i  X  [ m  X  1], we show that the entire model can be learned in a sequential manner.
 DAGs with effective depth one. A DAG has effective depth one if any hidden node has at least one observed neighbor (See Fig. 1(b) for an illustration). Now sup-pose that the dependence relationships among the hid-den nodes are also linear and are described as follows: where {  X  j } j  X  [ k ] denote the noise terms. For mod-els in this class, we use Excess Correlation Analysis (ECA) (Anandkumar et al., 2012a) to learn the model from the third order moment of the observed variables. Here, we assume that the noise variables at the hidden nodes are non-Gaussian ( e.g. , they have non-zero third moment or excess kurtosis).
 Our presentation focuses on using exact (population) observed moments to emphasize the correctness of the methodology. However,  X  X lug-in X  moment estimates can be used with sampled data.
 Learning algorithm. The above results already im-ply identifiability of the aforementioned DAG models through exhaustive search. We also present some con-ditions on the coefficient matrix A , under which we can efficiently learn the columns of A from the second order moment, by solving a set of convex optimization problems. This leads to efficient algorithms for learn-(a) Full ternary tree (b) Caterpillar tree Figure 2: Examples of graphs from the ensembles of multi-level DAGs and DAGs with effective depth one. ing multi-level DAGs and DAGs with effective depth one ( Algorithm 1 and Algorithm 2 ).
 Examples. It is useful to consider some concrete ex-amples of multi-level DAGs and DAGs with effective depth one, which satisfy the expansion property. Us-ing the results of this paper, under the rank condition and the parameter genericity property for matrix A , these models are identifiable.
 Full d -regular trees. These are tree structures in which every node other than the leaves has d children. These are included in the ensemble of multi-level DAGs and it is immediate to see that for d  X  3, the model can be identified under the described model in Sec-tion 2.2. (Note that d  X  2 suffices for expansion prop-erty but d  X  3 is necessary for the rank condition.) See Fig. 2(a) for an illustration of a full ternary tree with latent variables.
 Caterpillar trees. These are tree structures in which all the leaves are within distance one of a central path. See Fig. 2(b) for an illustration. These structures have effective depth one. Let d max and d min respectively de-note the maximum and the minimum number of leaves connected to a fixed node on the central path. It is im-mediate to see that if d min  X  d max / 2 + 1, the structure has the expansion property.
 Random bipartite graphs. Consider bipartite graphs with hidden nodes in one part and observed nodes in the other part. Each edge (between the two parts) is included in the graph with probability  X  , independent from every other edge. It is easy to see that, for any set S  X  [ k ], the expected number of its neighbors is E | N( S ) | = n (1  X  (1  X   X  ) | S | ). Also, the expected degree of the hidden nodes is  X n . Now, by applying a Cher-noff bound, one can show that these graphs have the expansion property with high probability, if k  X   X n/ 2, i.e. , with probability converging to one as n  X  X  X  . Application to correlated topic models. An im-portant application of the results of this paper is in estimating topic models with correlated topics. Topic models are a popular family of mixture models that incorporate latent variables, the topics, to explain the observed co-occurrences of words in documents. Each document has a mixture of active topics and each ac-tive topic determines the occurrence of words in the document. A topic model can be viewed as a bipartite DAG with topics in one part and the observed nodes in the other part. See Fig. 2(c) for an illustration. (As an example, one may think of the i -th observed variable as the word counts in the i -th sentence of a document.) Using this representation, estimating the topics from the document is exactly the learning prob-lem of the corresponding DAG. Existing work on esti-mating topic models provide results for certain distri-butions over the topics. For instance, in independent component analysis (ICA), the topics are assumed to be independent, while in Latent Dirichlet Allocation (LDA), a Dirichlet prior is assigned to the distribution of topics in documents. However, it has been observed empirically that correlated topic models provide bet-ter fit for document modeling (Blei &amp; Lafferty, 2007; Li &amp; McCallum, 2006). A popular correlated topic model, termed as Pachinko allocation involves multi-level DAGs for modeling word dependencies. We can now efficiently learn a rich class of similar correlated topic models. 2.4. Our techniques Our proof techniques rely on ideas and tools devel-oped in dictionary learning, matrix decomposition, and method of moments. We briefly explain our tech-niques and their relations to these areas.
 Matrix decomposition into diagonal and low-rank parts. To prove our core result, we first observe that under the linear model, E [ xx &gt; ] is the sum of a low-rank matrix and a diagonal one: We prove that under the rank condition (Condition 2), E This means that we can remove the noise contri-bution from the second order moment. Moreover, nodes. We propose a simple algorithm ( Subroutine ) for this decomposition. 3 Dictionary learning. We proceed by showing that using the graph expansion property (Condition 3), one can recover A from the low-rank part A E [ hh &gt; ] A &gt; , obtained from the decomposition of the observed co-variance matrix, as described above. To prove this claim, we leverage the ideas developed by Spielman et al. (2012) for the dictionary learning problem. Spiel-man et al. consider the problem of learning sparsely used dictionaries with an invertible dictionary and a random, sparse coefficient matrix, Bernoulli-Gaussian and Bernoulli-Radamacher models. They establish that the dictionary and the coefficient matrix can be learned from exact measurements. The gist of the idea is that under the above conditions, the row space of the coefficient matrix is the same as that of the measure-ments matrix. The rows of the coefficient matrix are then the sparsest vectors in the corresponding space. Notice that here we are in the same situation. Since E [ hh &gt; ] and A have full column rank, we have Col( A ) = tonary learning setting of Spielman et al. (2012), the coefficient matrix A is not generated from a probabilis-tic model. We introduce the graph expansion property as the underlying notion which makes the recovery of A possible. In fact, it can be shown that the proba-bilistic models considered by Spielman et al. possess this property almost surely. Our core result (identifi-ability of A ) is established by showing that, under the expansion property for the model, the columns of A are the sparsest vectors in Col( A E [ hh &gt; ] A &gt; ). Method of moments.
 For DAGs with effective depth one, observe that the hidden variables are related to each other and to the noise terms {  X  j } j  X  [ k ] via linear equations (6). Define  X   X  R k  X  k by letting the ( i,j ) entry be  X  ij if j  X  PA and zero otherwise. Solving for the hidden variables h , we have h = ( I  X   X )  X  1  X  , with  X  := (  X  1 ,..., X  The observed variables are also related to the hidden ones via the coefficient matrix A . The idea is to con-sider an equivalent DAG model obtained by suppress-ing the hidden nodes h j and treating the noise terms  X  j as the new uncorrelated hidden variables. The ob-served variables x i are then related to the new hid-den variables through the matrix A ( I  X   X )  X  1 . Next, we apply ECA method of Anandkumar et al. (2012a) to learn A ( I  X   X )  X  1 from the second and third or-der moments of the observed variables. ECA is based on two singular value decompositions: the first SVD whitens the data (using second moment) and the sec-ond SVD uses the third moment to find directions which exhibit information that is not captured by the second moment. Finally, in order to identify the de-pendence structure among the hidden nodes (matrix  X ), we use the expansion property to extract A and  X  Fig. 3. 2.5. Related work The problem of identifiability and learning graphical models from distributions has been the object of in-tensive investigation in the past years and has been studied in different research communities. This prob-lem has proved important in a vast number of appli-cations, such as computational biology, economics, so-ciology, and computer vision (see, e.g. , Durbin et al., 1998; Zellner, 1971; Bollen, 1989; Choi et al., 2010). The learning task has two main ingredients: structure learning and parameter estimation.
 Structure estimation has been extensively studied in the recent years. It is well known that maximum likelihood estimation in fully observed tree models is tractable (Chow &amp; Liu, 1968). However, for gen-eral models, structure learning is NP-hard even when there are no hidden variables. The main approaches for structure estimation are score-based methods, lo-cal tests, and convex relaxation methods. Score-based methods ( e.g. , Chickering, 2003) find the graph struc-ture by optimizing a score, like Bayesian Indepen-dence criterion (BIC), in a greedy manner. Local test approaches attempt to build the graph based on lo-cal statistical tests on the samples, both for directed and undirected graphical models ( e.g. , Spirtes et al., 2000; Bresler et al., 2008). Convex relaxation ap-proaches have also been considered for structure es-timation ( e.g. , Ravikumar et al., 2010).
 In the presence of latent variables, structure learn-ing becomes more challenging. A popular class of latent variable models are latent trees, for which ef-ficient algorithms have been developed ( e.g. , Erd  X os et al., 1999; Anandkumar et al., 2011). Recently, ap-proaches have been proposed for learning (undirected) latent graphical models with long cycles in certain parameter regimes (Anandkumar &amp; Valluvan, 2012). Chandrasekaran et al. (2012) estimate latent Gaussian graphical models using convex relaxation approaches via sparse + low rank matrix decomposition. Silva et al. (2006) study linear latent DAG models and pro-pose methods to (1) find clusters of observed nodes that are separated by a single latent common cause; and (2) find features of the Markov Equivalence class of causal models for the latent variables. Their model al-lows for undirected edges between the observed nodes. Ali et al. (2005) characterizes equivalence classes of DAG models when there are latent variables. How-ever, the focus is on constructing an equivalence class of DAG models, given a member of the class. In con-trast, we focus on developing efficient learning meth-ods for latent DAGs.
 For parameter estimation with hidden variable mod-els, the traditional approach is expectation maximiza-tion (EM) algorithm, which finds a local maximizer of the likelihood. Unfortunately, optimality and re-covery guarantees are generally lacking for EM, even when the model is correct. Another approach is to constrain the dependency structure among the hidden nodes. For instance, in independent component analy-sis (ICA) (Hyv  X arinen et al., 2001), it is assumed that the latent variables obey a product distribution and hence in the corresponding graph model there is no edge between the latent variables (there are only di-rected edges from latent nodes to the observed nodes). Several generalizations of ICA have also been devel-oped that allow some dependent components ( e.g. , Bach &amp; Jordan, 2003; Theis, 2007). Anandkumar et al. (2012a) considers latent variables to be drawn from a Dirichlet distribution, relevant in topic modeling (Blei et al., 2003), and obtains parameter estimates via the method of moments. In this work, we also use the method of moments to establish identifiability and ef-ficient recovery for DAG models. In this section, we state our identifiability results and algorithms for learning the DAG models with latent variables. Due to space limitations, we omit proofs and most technical details, and refer the interested reader to (Anandkumar et al., 2012b). 3.1. Learning the coefficient matrix A Our core identifiability result is the following theorem. Theorem 3.1. Let  X  := E [ xx &gt; ] be the second or-der moment of the observed variables. For the model described in Section 2.2 (Conditions 1, 2, 3, 4), all columns of A are identifiable from  X  .
 As shown in the proof, columns of A are in fact the result already implies identifiability of A via an ex-haustive search, which is an interesting result in its own right. The following theorem provides some con-ditions under which the columns of A can be identified by solving a set of convex optimization problems. Be-fore stating the theorem, we need to establish some notations.
 For i  X  [ n ], we define N i := { j  X  [ k ] : A ij 6 = 0 } and N i := { l  X  [ n ] : A lj 6 = 0 for some j  X  N i } . Similarly, for j  X  [ k ], define N j := { i  X  [ n ] : A ij 6 = 0 } and N j := { l  X  [ k ] : A il 6 = 0 for some i  X  N j } . We use superscript c to denote the set complement.
 Theorem 3.2. Suppose that in each row of A , there is a gap between the maximum and the second maximum absolute values. For i  X  [ n ] , let  X  i be a permutation such that | a i, X  | a ther suppose that [ k ]  X  {  X  1 (1) ,..., X  n (1) } . In words, each column contains at least one entry that has the maximum absolute value in its row. If the following conditions hold true for i  X  [ n ] , then Algorithm 1 returns the rows of A in canonical form. ( ii ) k A (N k v k 1 for all j  X  N i and all non-zero vectors v  X  R Subroutine : Decomposition of a matrix into its low-rank and diagonal parts.
 Input: Matrix C = AB &gt; + D , with A,B  X  R n  X  k , Output: Diagonal D and low-rank L = AB &gt; parts. 1: for each I  X  X  do 2: Choose distinct J,K  X  X \{ I } . 3: Let U I  X  R | I | X  k be the matrix of left singular 4: Let V J  X  R | J | X  k be the matrix of right singular 5: Let U K  X  R | K | X  k be the matrix of left singular 8: return D and L = C  X  D .
 Algorithm 1 : Recovering columns of coefficient ma-trix A from the second order moment  X .
 Input: Second order moment of observed variables  X . Output: Columns of A up to permutation. 1: Find a partition P of [ n ] such that |P| = 3 and 2: Let L be the low-rank part returned by 3: for each i  X  [ n ] do 4: Solve the optimization problem 5: Set s i = L 1 / 2 w , and let S = { s 1 ,...,s n } . 6: for each j = 1 ,...,k do 7: repeat 8: Let v j be an arbitrary element in S . 9: Set S = S\{ v j } . 10: until rank([ v 1 | X  X  X | v j ]) = j 11: Set  X  A = [ v 1 | X  X  X | v k ]. 12: Let  X  B be a left inverse for  X  A , i.e. ,  X  B  X  A = I 13: return Columns of  X  A (diag(  X  BL  X  B &gt; )) 1 / 2 Algorithm 1 is essentially the ER-SpUD algorithm of Spielman et al. (2012) for exact recovery of sparsely-used dictionaries, but the technical result and applica-tion in Theorem 3.2 are novel.
 According to Theorem 3.1, we can learn the coefficient matrix A of the model without any assumption on the dependence relationships among the hidden nodes. (We only need the non-degeneracy assumption in Con-dition 1, which requires the hidden variables to be lin-early independent with probability one.) Note that the coefficient matrix A does not completely specify the distribution, as the h i  X  X  are not necessar-ily statistically independent, and we can hope to learn the correlation structure among the h i  X  X . We next con-sider two families of DAG models, namely multi-level DAGs and DAGs with effective depth one. For these families, we proceed further and prove identifiability of the entire model. 3.2. Multi-level DAGs We first formally define multi-level DAGs.
 Definition 3.3. A multi-level DAG model is a model with the following graph structure. The nodes of the graph can be partitioned into levels L 1 ,...,L m such that there is no edge between the nodes within one level and all the edges are between nodes in adjacent levels, ( L i ,L i +1 ) for i  X  [ m  X  1] . Furthermore, the edges are directed from L i to L i +1 . The nodes in level L m respond to the observed nodes and other levels contain the hidden nodes.
 The next theorem concerns identifiability of linear multi-level DAGs. More specifically, consider a multi-level DAG model and let G i be the induced graph with nodes L i  X  L i +1 and suppose that the induced model between levels L i and L i +1 satisfies the model condi-tions described in Section 2.2 with coefficient matrix A , for i  X  [ m  X  1]: A i has the rank condition (Con-dition 2) and parameter genericity property (Condi-tion 4), and (bipartite) graph G i has the expansion property (Condition 3).
 Theorem 3.4. Consider a multi-level DAG with lev-els L 1 ,...,L m and suppose that the induced model be-tween levels L i and L i +1 satisfies the model conditions described in Section 2.2 with coefficient matrix A i , for i  X  [ m  X  1] . Then all columns of A i are identifiable for i  X  [ m  X  1] from the second order moment of the ob-served variables  X  . Therefore, the entire DAG is iden-tifiable up to permuting the nodes within each level. Remark 3.5. By the definition of a multi-level DAG, the hidden nodes in level L 1 are independent. Now consider the case that the nodes in L 1 have arbitrary dependence relationships. By using the same argument as in the proof of Theorem 3.4, we can still learn all the coefficient matrices A i and the second order moment of the nodes in L 1 . 3.3. DAGs with effective depth one Another important subclass of DAGs are those with effective depth one.
 Definition 3.6. The effective depth of a DAG model with hidden nodes is the maximum graph distance be-tween a hidden node and its closest observed node. In particular, in a DAG with effective depth one every hidden node has at least one observed neighbor. Recall that the observed and the hidden nodes obey the linear model in Eq. (2), which in vector form reads x = Ah +  X  . Let  X  = (  X  ij )  X  R k  X  k be the matrix with  X  ij = 0 if j /  X  PA i . We assume further that the hidden variables obey the linear model in Eq. (6), i.e. , h =  X  h +  X  .
 As described in Section 2.2, without loss of generality, we assume that hidden variables h j , the observed vari-ables x i and the noise terms  X  i , X  j are all zero mean. We also denote the variances of  X  i and  X  j by  X  2  X   X  the third moment of  X  i and  X  j , i.e. ,  X   X  i := E [  X  3  X  j := E [  X  Finally, denote the second and third order correla-tions of the observed variables by  X  := E [ xx &gt; ] and  X  := E [ x  X  x  X  x ], where  X  denotes the tensor prod-uct.
 Theorem 3.7. Consider a DAG model with effec-tive depth one, which satisfies the model conditions described in Section 2.2 and the hidden variables are related through linear equations (6) . If the noise vari-ables  X  j have non-zero skewness for j  X  [ k ] , then the DAG model is identifiable from  X  and  X  .
 Furthermore, under the assumptions of Theorem 3.2, there is an efficient algorithm that returns matrices A and  X  up to a permutation of hidden nodes. The algorithm basically combines the Excess Correlation Analysis method of (Anandkumar et al., 2012a) and Algorithm 1 . The details of the algorithm are given in the full version of the paper under the name of Algorithm 2 . 3.4. Remark on finding the partition P In the full version of the paper, we show that under a weak incoherence assumption about A , a random par-titioning of its rows into three groups satisfies Condi-tion 2, with fixed positive probability.
 The authors thank anonymous reviewers for their useful comments. A. Anandkumar acknowledges the support of NSF Award CCF 1219234, AFOSR Award FA9550-10-1-0310, and ARO Award W911NF-12-1-0404. Part of this work was completed while A. Anandkumar and A. Javanmard were visiting MSR New England.

