 A problem of recognizing textual entailments (RTE) X  X iven two text fragments T (for a text) and H (for a hypothesis), determine whether T entails, contradicts or is neutral to H  X  X s consid-ered as a complex and, at the same time, funda-mental problem for several NLP tasks (Dagan et al., 2005). For more than a decade, RTE chal-lenges have been held, where systems are compet-ing to each other with respect to human annotated RTE test data; but there are few systems that try to solve RTE problems by computing meanings of linguistic expressions and employing inference engines similar to proof procedures of formal log-ics. Moreover, those few systems are usually used in combination with shallow classifiers since the systems X  performances alone are poor.

The current paper advocates that purely deduc-tive inference engines over linguistic representa-tions backed up with a simple lexical knowledge base could be solely and successfully used for the RTE task. Our work builds on the theory of an analytic tableau system for Natural Logic (Natural Tableau) introduced by Muskens (2010). The the-ory offers to employ a tableau method X  X  proof procedure used for many formal logics X  X or the version of Natural Logic that employs Lambda Logical Forms (LLFs) X  X ertain terms of simply typed  X  -calculus X  X s Logical Forms (LFs) of lin-guistic expressions. The merits of the current ap-proach are several and they can be grouped in two categories: virtues attributed to the tableau prover are (i) the high precision for the RTE task charac-teristic to proof procedures, (ii) the transparency of the reasoning process, and (iii) ability for solv-ing problems with several premises; and those concerning LLFs are (iv) an evidence for LFs that are reminiscent of Surface Forms but still retaining complex semantics, and (v) an automatized way of obtaining LLFs from wide-coverage texts.
 The rest of the paper is organized as follows. First, Natural Tableau is introduced, and then a method of obtaining LLFs from raw text is de-scribed. We outline the architecture of an imple-mented theorem prover that is based on the the-ory of Natural Tableau. The power of the prover is evaluated against the SICK data; the results are an-alyzed and compared to related RTE systems. The paper concludes with future work. Natural Logic is a vague notion and refers to log-ics that account for valid inferences of natural languages, where reasoning and the grammar are strongly related to each other and LFs resemble surface forms (Lakoff, 1972). On the other hand, a tableau method (Beth, 1955) is a popular proof procedure and nowadays many formal logics have their own version of it (D X  X gostino et al., 1999). A combination of these two devices is offered by Muskens (2010), where the language of Natural Logic is considered to be a part of simply typed Figure 1: Tableau rules for quantifiers (  X  F and  X  F ), Boolean operators ( NOT ), formatting ( PUSH and PULL ) and inconsistency (  X   X  -terms that are built up from variables and lexi-cal constant terms with the help of application and lambda abstraction. The terms of the language are called LLFs and resemble linguistic surface Note that common nouns and intransitive verbs are typed as properties (i.e. functions from enti-ties to truth values) and quantifiers as binary rela-tions over properties; the latter typing treats quan-tified noun phrases (QNPs) as generalized quanti-fiers (GQs) X  X  term of type properties over prop-erties ( et ) t .

A Natural Tableau entry is a tuple containing a term, a sequence of terms representing an argu-ment list, and a truth sign. The entries are such that when a term is applied to all arguments from an ar-gument list in the order of the list, the resulted term is of type truth value. For example, A eet c e :[ d e ]: T is a valid tableau entry (i.e. a node) since it con-sists of a term A eet c e , an argument list [ d e ] and a truth sign T standing for true , and additionally, A eet c e d e is a term of type t .

A tableau method is a refutation method and it proves an argument by searching a counterexam-ple. The search process is guided by applications of certain set of rules. A tableau rule is a schema with a set of antecedent nodes above a line and a set of precedent branches below a line, where each Figure 2: The closed tableau serves as a proof for: not all birds fly  X  some bird does not fly branch consists of (precedent) nodes. A rule is ap-plicable if all its antecedent nodes match to some nodes in a tableau, and after the rule is applied, precedent nodes of the rule are introduced in the tableau. A tableau consists of branches where each branch models a situation and is either closed (i.e., inconsistent) or open (i.e., consistent) depending whether it contains a closure  X  sign (i.e., an ob-vious contradiction). A tableau is closed if all its branches are closed, otherwise it is open.

In Figure 2, a tableau proof, which employs the rules of (Muskens, 2010) from Figure 1, is pre-sented. In order to show a way the tableau is de-veloped, the nodes are enumerated and annotated with a source rule and IDs of nodes from which a current node is obtained. For example, 3 is ob-tained from 1 by the PUSH rule. In order to prove an argument, the tableau starts with a counterex-ample of the argument, i.e. a premise being true and a conclusion false. After several rule applica-tions, all the branches of the tableau close meaning that none of the situations for the counterexample were consistent.

An advantage of Natural Tableau is that it treats both single and multi-premised arguments in the same fashion and represents a deductive procedure in an intuitive and transparent way. 3.1 CCG and the C&amp;C Parser Combinatory Categorial Grammar (CCG) is a lex-icalized grammar formalism that assigns a syntac-tic category and a semantic interpretation to lexi-cal items, where the items are combined via com-binatory rules (Steedman, 2000; Steedman and Baldridge, 2011). The CCG category A/B (or A \ B ) is a category of an item that becomes of cat-egory A when it is combined with an item of cat-egory B on its right (or left, respectively) side. In the example below, the sentence every man walks is analyzed in the CCG formalism, where lexical items are combined via the forward application rule and unspecified semantic interpretations are written in a boldface:
The CCG derivation trees are suitable structures for obtaining LLFs for at least two reasons. First, the CCG framework is characterized by a trans-parent interface between syntactic categories and semantic types; second, there exist efficient and robust CCG parsers for wide-coverage texts. During obtaining LLFs, we employ the C&amp;C CCG parser of Clark and Curran (2007) and Easy-CCG of Lewis and Steedman (2014). While the C&amp;C parser is a pipeline of several NLP sys-tems: POS-tagger, chunker, named entity recog-nizer (NER), lemmatizer (Minnen et al., 2001) supertagger and sub-parser, EasyCCG is an ex-tremely simple but still comparably accurate CCG use different settings for supertagging and parsing; therefore, it is interesting to test both parsers for our application.

In Figure 3, there is a CCG derivation by the Figure 3: The CCG tree by the C&amp;C parser for there is no one cutting a tomato ( SICK -2404), where thr, dcl, ng category features stand for an expletive there , declarative and present participle, respectively.
 C&amp;C parser displayed in a tree style: terminal nodes are annotated with tokens, syntactic cate-gories, lemmas and POS-tags while non-terminal nodes are marked with combinatory rules and re-sulted categories; some basic categories are sub-categorized by features. 3.2 From CCG Trees to LLFs Initially, it may seem easy to obtain fine-grained LLFs from CCG trees of the parsers, but careful observation on the trees reveals several compli-cations. The transparency between the categories and types is violated by the parsers as they employ lexical (i.e. type-changing) rules X  X ombinatory rules, non-native ones for CCG, which changes categories. Lexical rules were initially introduced in CCGbank (Hockenmaier and Steedman, 2007) to decrease the total number of categories and rules. In the tree of Figure 3, a lexical rule changes a category S ng \ NP of a phrase cutting a tomato with N \ N . In addition to this problem, the trees contain mistakes from supertaggers (and from the other tools, in case of the C&amp;C parser).
The first step in processing CCG trees is to re-move directionality from the categories. This step is the same as obtaining unspecified semantic in-terpretation of a phrase in the CCG framework. While converting categories A \ B and A/B into a non-directional type ( b , a ) , the arrangement of nodes must be changed in a corresponding way. For instance, in case of the top backward appli-cation rule ( ba[ S dcl ] in Figure 3), the order of Figure 4: A CCG term obtained from the CCG tree of Figure 3. Categories are converted into types. nodes is reversed to guarantee that the function category ( s dcl , np thr ) precedes its argument cate-gory np thr . There are about 20 combinatory rules used by the parsers and for each of them we de-sign a way of reordering subtrees. In the end, the order of nodes coincides with the order according to which semantic interpretations are combined. The reordering recipes for each combinatory rule is quite intuitive and can be found in (Steedman, 2000) and (Bos, 2009), where the latter work also uses the C&amp;C parser to obtain semantic interpre-tations. Trees obtained after removing the direc-tionality from the categories are called CCG terms since they resemble syntactic trees of typed  X  -terms (see Figure 4).
Lexical rules are the third most commonly used combinatory rules (7% of all rules) by the parsers on the SICK data (Marelli et al., 2014b), and there-fore, they deserve special attention. In order to compositionally explain several category changes made by lexical rules (represented with ( . )  X  oper-ator in terms), either types of constant terms are set to proper types or lexical entries are inserted in CCG terms. For explaining a lexical rule n ; np , mainly used for bare nouns, an indefinite deter-miner is inserted for singular nouns (1) and a plu-Figure 5: A fixed CCG term that is obtained from the CCG term of Figure 4. A node with a dashed (solid) frame is inserted (substituted, re-spectively). A type vp a,b abbreviates ( np a , s b ) . ral morpheme s is used as a quantifier for plurals (2). Also identifying proper names with the fea-ture assigned by the C&amp;C NER tool helps to elim-inate n ; np change (3). Correcting the type of a quantifier that is treated as a noun modifier is an-other way of eliminating this lexical rule (4). In case of ( s , np ) ; ( n , n ) change, which is phrase is inserted and salvages the category-type trans-parency of CCG (see Figure 5). As a whole, the designed procedures explain around 99% of lex-ical rules used in CCG terms of the SICK sen-tences. Note that explaining lexical rules guaran-tees a well-formed CCG term in the end.

Apart from the elimination of lexical rules, we also manually design several procedures that fix a CCG term: make it more semantically adequate or simplify it. For example, the C&amp;C parser assigns a category N/PP of relational nouns to nouns that are preceded by possessives. In these cases, a type n is assigned to a noun and a type of possessive is changed accordingly (5). To make a term seman-tically more adequate, a relative clause is attached to a noun instead of a noun phrase (6), where a type w  X  ( vp , np , s ) of a wh-word is changed with w 0  X  ( vp , n , s ) . CCG terms are simplified by sub-stituting terms for no one, nobody, everyone , etc. with their synonymous terms (see (7) and Figure 5). These substitutions decrease a vocabulary size, and hence, decrease the number of tableau rules.
The final operation is to convert a fixed CCG term into an LLF, meaning to convert QNPs into GQs of (Montague, 1974; Barwise and Cooper, 1981). In this procedure, a type ( n , np ) of a quan-sulted new NP is applied to the smallest clause it occurs in; but if there are other QNPs too, then it also applies to the clauses where other QNPs are situated. This operation is not deterministic and can return several terms due to multi-options in quantifier scope ordering. As an example, two  X  -terms, (9) and (10), are obtained from the CCG b ( no ( w ( b ( c ( a t ))) p )) th (8) no w ( b (  X x. a t (  X y. c yx ))) p a t (  X x. no w ( b ( c x )) p
Eventually the final  X  -terms, analogous to (9) and (10), obtained from CCG trees will be con-sidered as LLFs that will be used in the wide-coverage theorem prover. It has to be stressed that generated LLFs are theory-independent abstract semantic representation. Any work obtaining se-mantic representations from CCG derivations can combine its lexicon with (already corrected) LLFs and produce more adequate semantics in this way. 3.3 Extending the Type System An obvious and simple way to integrate the LLFs, obtained in Subsection 3.2, in Natural Tableau is to translate their types into semantic types built up from e and t . 4 We will not do so, because this means the information loss since the information about syntactic types are erased; for example, usu-ally syntactic types pp , n and ( np , s ) are trans-lated as et type. Retaining syntactic types also contributes to fine-grained matching of nodes dur-ing rule application in the prover. For instance, without syntactic types it is more complex to de-termine the context in which a term game occurs and find an appropriate tableau rule when consid-ering the following LLFs, game n , n theory and game pp , n ( of X ) , as both ( n , n ) and ( pp , n ) are usually translated into ( et ) et type, like it is done by (Bos, 2009).

In order to accommodate the LLFs with syntac-tic types in LLFs of (Muskens, 2010), we extend the semantic type system with np , n , s , pp basic syntactic types corresponding to basic CCG cate-gories. Thus complex types are now built up from the set { e,t, np , n , s , pp } of types. The extension automatically licenses LLFs with syntactic types as terms of the extended language.

We go further and establish interaction between semantic and syntactic types in terms of a subtyp-ing v relation. The relation is defined as a partial order over types and satisfies the following condi-tions for any  X  1 , X  2 , X  1 , and  X  2 types: ( a ) e v np , s v t, n v et, pp v et ; ( b ) (  X  1 , X  2 ) v (  X  1 , X  2 ) iff  X  1 v  X  1 and  X  2 v  X  Moreover, we add an additional typing rule to the calculus: a term is of type  X  if it is already of type  X  and  X  v  X  . According to this typing rule, now a term can be of multiple types. For example, both walk np , s and man n terms are also of type et , and all terms of type s are of type t too. From this point on we will use a boldface style for lexical constants of syntactic types.

Initially it may seem that the lexicon of con-stant terms is doubled in size, but this is not the case as several syntactic constants can mir-ror their semantic counterparts. This is achieved by multiple typing which enables to put seman-tic and syntactic terms in the same term. For in-well-formed LLFs of type t that combine terms of syntactic and semantic types, and there is no need of introducing semantic terms (e.g., at eet or love eet ) in order to have a well-formed term. In the end, the extension of the language is conserva-tive in the sense that LLFs and the tableau proof of Section 2 are preserved. The latter is the case since the tableau rules are naturally extensible to match new LLFs. In order to further develop and evaluate Natural Tableau, we implement the prover, LangPro, based on the extended theory. Its general architecture is based on the first-order logic (FOL) prover of Fitting (1990). The prover also contains a mod-ule for  X  -calculus that roughly follows (Blackburn and Bos, 2005).

Setup of the inventory of rules is a crucial for ef-ficiency of the prover. There is a priority order for the categories of rules according to their computa-tional efficiency. The prover most prefers to em-ploy non-branching rules that introduce no fresh terms and antecedents of which can be ignored af-ter the application (e.g., NOT ). Less preferred and inefficient rules are the ones that branch, produce new terms or antecedents of which are kept after the application (e.g.,  X  F and  X  F ). In order to en-courage finding short proofs, admissible rules rep-resenting shortcuts of several rule applications are also introduced (e.g., FUN  X  and ARG in Figure 9). The inventory consists of about 50 rules, where most of them are manually designed based on RTE problems (see Section 5.1) and the rest represents the essential part of the rules found in (Muskens, 2010).

The LLF generator (LLFgen) is a procedure that generates LLFs from a CCG derivation in the way described in Subsection 3.2. We also implement an LLF-aligner that serves as an optional prepro-cessor between LLFgen and the prover itself; it aligns identical chunks of LLFs and treats them as a constant (i.e. having no internal structure). This treatment often leads to smaller tableau proofs. The example of aligned LLFs is given in Figure 8.
LangPro uses only the antonymy relation and a transitive closure of the hyponymy/hypernymy relations from WordNet 3.0 (Fellbaum, 1998) as its knowledge base (KB). The entailment  X  (con-tradiction  X  ) relation between lexical constants of the same type A  X  B ( A  X  B ) holds if there ex-ists a WordNet sense of A that is a transitive hy-ponym (an antonym) of some WordNet sense of B . Note that there is no word sense disambigua-tion (WSD) used by the prover; therefore, adopt-ing these interpretations of entailment and contra-diction amounts to considering all senses of the words. For example, a man is crying entails a man is screaming as there are senses of cry and scream that are in the entailment relation.
 All in all, chaining a CCG parser, LLFgen, the LLF-aligner, the prover and KB results in an au-tomatized tableau prover LangPro which operates directly over natural language text. 5.1 Learning For learning and evaluation purposes, we use the SICK data (Marelli et al., 2014b). The data con-sists of problems that are rich in the lexical, syn-tactic and semantic phenomena that compositional distributional semantic models (Mitchell and La-SICK data contains around 10K text-hypothesis pairs that are classified in three categories: entail-ment, contradiction, and neutral.

During learning we used only the trial portion of the data, SICK -trial, including 500 problems. The learning process consists of improving the components of the prover while solving the RTE problems: designing fixing procedures of LLFgen, adding new sound rules to the inventory, and intro-ducing valid relations in KB that were not found in WordNet (e.g., woman  X  lady , note  X  paper and food  X  meal ). During learning, each RTE problem is processed as follows: A function llf denotes the combination of LLFgen and a CCG parser; for learning we use only the C&amp;C parser. A function one of the tableaux initiated with aligned or non-aligned set S of nodes closes; otherwise it re-turns OPEN . For instance, while checking a prob-lem ( T,H ) on entailment (contradiction), tableau starts with a counterexample: T being true and H false (true, respectively). Note that 5-7 procedures are carried out manually while the phase is signifi-cantly facilitated by graphical proofs produced by
As a result, there were collected around 30 new rules where about a third of them are admissible ones; the new rules cover phenomena like noun and adverbial modifiers, prepositional phrases, passive constructions, expletive sentences, verb-particle constructions, auxiliaries, light verb con-structions, etc. Most of the new rules are discussed in more details in (Abzianidze, 2015).
 -train with gold and LangPro judgments. Figure 6: Performance of LangPro on SICK -train (4500) using CCG derivations of the C&amp;C parser.
LangPro was unable to prove several problems requiring complex background knowledge (e.g., SICK -3670 in Table 1) or having wrong CCG derivations from the C&amp;C parser (e.g., in SICK -219, white dancing is a noun constituent). 5.2 Development The part of the SICK data, SICK -train, issued for training at RTE14 was used for development. Af-ter running LangPro on SICK -train, we only an-alyzed false positives, i.e. neutral problems that were identified either as entailment or contradic-tion by the prover. The analysis reveals that the parsers and WordNet are responsible for almost all these errors. For example, in Table 1, SICK -5248 is classified as entailment since toad and frog might have synonymous senses; this problem shows the advantage of not using WSD, where a proof search also searches for word senses that might give rise to a logical relation. SICK -7402 was falsely identified as contradiction because of the wrong analyses of the premise by both CCG parsers: no man and child... are in coordination, which implies there is no man , and hence, contra-dicts the conclusion. SICK -8490 is proved as con-tradiction since the prover considers LLFs where shirt takes a wide scope. With the help of Lang-Pro, we also identified inconsistency in the annota-tions of problems, e.g., SICK -1431, 8913 are sim-ilar problems but classified differently; it is also
Table 2: Evaluation of the versions of LangPro surprising that SICK -5248 is classified as neutral.
During this phase, also the effective (800) and efficient (50) upper bounds for the rule application number were determined (see Figure 6). More-over, 97.4% of proofs found in 1600 rule appli-cations are actually attainable in at most 50 rule applications; this shows that the rule application strategy of LangPro is quite efficient. 5.3 Evaluation We evaluate LangPro on the unseen portion of the SICK data, SICK -test, which was used as a bench-mark at RTE14; the data was also held out from the process of designing LLFgen. The prover clas-sifies each SICK problem as follows: The results, in Table 2, show evaluation of LangPro on SICK -test using both parsers sepa-rately with the efficient and effective rule applica-tion upper bounds. Slightly better results with the C&amp;C parser is explained by employing the parser in the learning phase. The difference of .5% in Table 3: Comparing LangPro to the top or related accuracy between the C&amp;C-based and EasyCCG-based provers show that LLFgen was not fitted to the C&amp;C parser X  X  output during the learning phase.
In order to eliminate at some extent errors com-ing from the parsers, hybrid provers are designed that simply combine answers of two systems X  X f one of the systems proves a relation then it is an answer. Both hybrid versions of LangPro show more than 80% of accuracy while only 5 systems were able to do so at RTE14, where 77.1% was a median accuracy. The prover turns out to be ex-tremely reliable with its state-of-the-art precision being almost 98%. A high precision is conditioned by the formal deductive proof nature of LangPro and by the sound rules it employs.

In Table 3, we compare the best version of hy-brid LangPro to the top 5 systems of RTE14 on SICK -test and show the improvement it gives to each system when blindly adopting its positive an-swers (i.e. entailment and contradiction).

The decision procedure of the prover is com-pletely rule-based and easy to comprehend since it follows the intuitive deductive rules. Tableaux proofs by LangPro for SICK -247 (in Figure 7) and SICK -2895 (in Figure 8) show step by step how T contradicts and entails, respectively, H . 8 Several new rules employed in these tableaux are given in Figure 9. Note that the both problems, SICK -247, 2895, were wrongly classified by all the top 7 sys-tems of the RTE14. Taking into account that solv-ing SICK -247 requires a sort of De Morgan X  X  law for negation and disjunction, this demonstrates where LangPro, a purely logic-based system, out-problem, SICK -2895, is an evidence how unreli-able the state-of-the-art and non-logic-based RTE systems might be since solving the problem only requires a lexical knowledge barbell  X  weight , which is available in WordNet. Using formal logic tools for a wide-coverage RTE task goes back to the Nutcracker system (Bos and Markert, 2005), where a wide-coverage semantic processing tool Boxer (Bos, 2008), in combination with the C&amp;C tools, first produces discourse rep-resentation structures of (Kamp and Reyle, 1993) and then FOL semantic representations (Curran et al., 2007). Reasoning over FOL formulas is car-ried by off-the-shelf theorem provers and model the latter in several main aspects: (i) the under-ling logic of LLFs (i.e. higher-order logic) is more expressive than FOL (e.g., it can properly model GQs and subsective adjectives), (ii) LLFs are cheap to get as they are easily obtained from CCG derivations, and (iii) we develop a com-pletely new proof procedure and a prover for a ver-sion of Natural Logic.
 The other related works are (MacCartney and Manning, 2008) and (Angeli and Manning, 2014). Both works contribute to Natural Logic and are has two main shortcomings compared to Natural Tableau; namely, it is unable to process multi-premised problems, and its underling logic is weaker (e.g., according to (MacCartney, 2009), it cannot capture the entailment in Figure 2). Figure 7: A closed tableau for SICK -247: The woman is not wearing glasses or a headdress  X  A woman is wearing an Egyptian headdress We made Natural Tableau of Muskens (2010) suit-able for the wide-coverage RTE task by extending it both in terms of rules and language. Based on the extended Natural Tableau, the prover LangPro was implemented, which has a modular architec-ture consisting of the inventory of rules, KB and the LLF generator. As a whole, the prover repre-sents a deductive model of natural reasoning with the transparent and naturally interpretable decision procedure. While learning only from the SICK -trial data, LangPro showed the comparable accu-racy and the state-of-the-art precision on the un-seen SICK data.

For future work, we plan to explore the FraCaS (Consortium et al., 1996) and newswire RTE (Da-gan et al., 2005) data sets to further improve the LLF generator and enrich the inventory of tableau rules. These tasks are also interesting for two rea-sons: to find out how much effort is required for Figure 8: A closed tableau for SICK -2895: The man isn X  X  lifting weights  X  The man is lifting barbells , where M abbreviates a shared term the man aligned by the LLF-aligner.

Figure 9: Several rules learned from SICK -trial adapting the LLF generator to different data, and which rules are to be added to the inventory for tackling the new RTE problems. Incorporating more WordNet relations (e.g., similarity, deriva-tion and verb-group) and the paraphrase database (Ganitkevitch et al., 2013) in KB is also a part of our future plans.
 I thank Reinhard Muskens for helpful discussions on this work, Jan Sprenger for his feedback on an early version of this paper, the authors of (Honni-bal et al., 2010) for sharing a recent parser model, and anonymous reviewers for useful comments. This work is a part of the project  X  X owards Logics that Model Natural Reasoning X  and supported by the NWO grant (project number 360-80-050).
