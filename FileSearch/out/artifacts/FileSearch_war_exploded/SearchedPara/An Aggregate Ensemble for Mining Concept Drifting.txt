 Recent years have seen a large body of work on pattern discovery and data mining from data streams. From the supervised learning perspective, all existing solutions build stream data mining models under two major assumptions: Stationary Assump-tion [1, 2, 3, 4, 5, 6, 8,11] and Learnable Assumption [9, 10]. 
The Stationary Assumption assumes that training data chunks have a similar or identical distribution as the yet-to-come data chunk, so classifiers built from the train-ing data chunks will perform well in classifying data from the yet-to-come data chunk. However, this stationary assumption violates the concept drifting reality in data streams where prior knowledge on when and where the concepts may change is not capable of capturing all realistic situations of data streams. a  X  X earnable Assumption X  which assumes that the training chunks have different distributions p(x,y) (where x denotes the feature vector and y denotes the class label) from that of the yet-to-come data chunk, and classifiers built on the training set may perform only slightly better than random guessing or simply predicting all examples to a single class. Under the learnable assumption, building classifiers on the up-to-date data chunk to predict the yet-to-come data chunk is better than building classifi-ers on a lot of buffered chunks because the buffered chunks will degenerate the en-semble performance. However, the disadvantage of the learnable assumption is also obvious, in the sense that it doesn X  X  discriminate concept drifting from data errors. If the up-to-date data chunk is a noisy chunk, building classifiers on this noisy data chunk to predict the yet-to-come data chunk may cause more errors than using a clas-sifier ensemble built on previously buffered data chunks. Consequently, although the learnable assumption is more reasonable than the stationary assumption for data streams, in practice, it is still not capable of describing the stream data. 
Considering a toy data stream consisting of five data chunks as shown in Figure 1, the stationary assumption can only cover the process from D 1 to D 2 , where the distri-bution p 1 (x,y) remains unchanged. The learnable assumption covers the process from D to D 3 , where the concept drifts from p 1 (x,y) to p 2 (x,y) without interrupted by noisy data chunks. However, the most common situation of a realistic data stream is de-p (x,y) ) is mixed with noise (a noisy data chunk D 4 is observed). To explicitly de-scribe this type of data streams, we define a realist assumption as follows: Realistic Assumption: In realistic data streams, the streams may experience concept drifting and data errors simultaneously. 
The realistic assumption addresses both concept drifting and data errors in data streams. It is much more general than the stationary and learnable assumptions. So it can be widely adopted to help solve various stream data mining challenges. In this paper, we employ this realistic assumption to formulate data streams, and we further propose an aggregate ensemble framework to mine data streams with both concept drifting and data errors. tional methods adopted in data streams and then propose a new aggregate ensemble (AE) framework. We analyze the performance of the Aggregate Ensemble method in Section 3 and report experimental results on both synthetic and real-life datasets in Section 4. The conclusions and remarks are given in Section 5. The nature of continuous volumes of the stream data raises the needs of designing effective classifiers with high accuracy in predicting the future testing data chunk as well as good efficiency to learn from the massive volumes of training in-stances[12,13]. To overcome these challenges, a number of ensemble methods have been proposed. Horizontal Ensemble Framework. Consider a data stream S containing an infinite number of data chunks D i ( i= - X  ,... ,+ X  ). Due to the space limitations, we can only buffer at most n blocks each of which contains a certain number of instances. Assume chunk D n+1 , one can [2,3,5,6] choose a learning algorithm L to build a base classifier f i from each of the buffered data chunks D i , say f i = L ( D i ), and then predict each instance classifier ensemble through the model averaging mechanism as follow: tage of the horizontal ensemble is twofold: (1) they can reuse information of the buff-ered data chunks, which may be useful for the testing data chunk; and (2) they are robust to noisy streams because the final decisions are based on the classifiers trained from different chunks. Even if noisy data chunks may degenerate some base classifi-ers, the ensemble can still maintain relatively stable prediction accuracy. The disad-vantage of such an ensemble framework, however, lies on the fact that if the concepts of the stream continuously change, information contained in previously buffered clas-sifiers may be invalid to the current data chunk. Consequently, combining old-fashioned classifiers may not improve the overall prediction accuracy. Vertical Ensemble Framework. Assume we have m learning algorithms L j ( j=1,2,...,m ), a vertical ensemble [9,10] builds base classifiers using each algorithm on the up-to-date data chunk D n as, f j = L j ( D n ) and then combines all the base classifi-ers through model averaging. Figure 2(2) gives an illustration of the vertical ensemble framework. In the case that prior knowledge of the yet-to-come data chunk is un-known, model averaging on the most recent chunk can achieve a minimal expectation error on the test set. In other words, building classifiers using different learning algo-rithms can decrease the expected bias error compared to any single classifiers. For example, assuming a data stream with its joint probability p(x,y) evolves continu-ously, if we only use a stable learner such as SVM, then SVM may perform better than an unstable classifier when p(x) changes while p(y|x) remains unchanged. On the other hand, if we only use an unstable learner such as decision trees, then decision trees may perform better than SVM when p(x) does not evolve much but p(y|x) changes dramatically. Although the vertical ensemble has a much looser condition The vertical ensemble builds classifiers only on a single up-to-date data chunk. If the up-to-date data chunk is a noisy data chunk, the results may suffer from severe per-formance deterioration. Without realizing the noise problems, the vertical ensemble limits itself merely to the concept drifting scenarios, but not to the realistic data streams. Aggregate Ensemble Framework. The disadvantages of the above two ensemble frameworks motivate our proposed Aggregate Ensemble framework as illustrated in Figure 2(3). We first use m learning algorithms L i ( i = 1,2,...,m ) to build classifiers on ( D j ), where i denotes the i bine these base classifiers to form an aggregate ensemble through model averaging defined in Eq. (2)  X  which indicates that the aggregate ensemble is a mixture of the horizontal ensemble and vertical ensemble, and its base classifiers constitute a Classi-fier Matrix ( CM ) in Eq.(3). data chunk j . As we have mentioned in the vertical ensemble, classifiers on each col-umn of CM ( i.e. , classifiers built on the same data chunk by different algorithms) are used to reduce the expected classifier bias error on an unknown test data chunk. Clas-sifiers on each row of CM ( i.e. , classifiers built on different data chunks by using the same algorithm) are used to eliminate the impact of noisy data chunks. By building a classifier matrix CM , the aggregate ensemble is capable of solving a realistic data stream which contains both concept drifting and data errors. As we described above, on each data chunk, the aggregate ensemble builds m classifi-ers by using m different learning algorithms. To a specific test instance x in the yet-to-come data chunk, the horizontal ensemble uses classifiers on a row in matrix CM to semble can be denoted by Eq. (4) The vertical ensemble can be denoted by model averaging on the last column (column n ) of Matrix CM , which is given in Eq. (5) An aggregate ensemble combines all classifiers in CM as base classifiers, through the averaging rule defined by Eq. (2). From this point of view, the horizontal ensemble and vertical ensemble are, in fact, two sp ecial cases of the aggregate ensemble. Gao [9] has proven that in data stream scenarios, the performance of a single classifier within a classifier ensemble is expected to be inferior to the performance of the entire classifier ensemble. The horizontal ensemble and vertical ensemble, as special cases of the aggregate ensemble, are not expected as good as the aggregate ensemble. For example, when combining each column in CM , we get a variant of CM as CM g g g = , where each [] 12 , ,..., T iiimi gff f = is independent and shares the same distribution, say p(g) . Then the mean squared error of the horizontal ensemble (with the i th algorithm) on a test instance x (with class label y ) can be denoted as: 
For the aggregate ensemble, the mean squared error on x can be calculated by 
So, the difference between Eqs. (7) and (6) can be calculated by 
Thus, we assert that the error rate of the aggregate ensemble is expected to be less than or equal to the horizontal ensemble. Similarly, if we regard CM as a column vector where each element is a combination of different rows in CM , we can show that the mean squared error of the aggregate ensemble is also expected to be less than or equal to that of the vertical ensemble. Another important issue with the aggregate ensemble is to determine the number of data chunks we should use to build the ensemble. If we buffer too many data chunks, the ensemble may contain lots of inva-lid patterns that deteriorate the prediction performance, while too few data chunks may not be able to eliminate the noise impact. In our experiments below, we set the number of buffered data chunks to be 3. Our testing system is implemented in Java with an integration of the WEKA [14] tool. Synthetic data streams: We create synthetic data streams as follows. Firstly, we generate instances x t at time stamp t by a Gaussian distribution x t ~N (  X  t ,  X  t ), where Then we define the potential pattern p(y|x) at time stamp t as: nonlinear part ( a sin x ) of Eq. (9) generates discriminative groups, the second nonlinear part ( bx 2 ) of Eq. (9) generates a nonlinear classification boundary, and the third part is a random noise variable which follows a Gaussian distribution  X  ~N(0,0.3 2 ) . To simu-late concept drifting, we let p(x,y) change randomly. To evolve p(x) , we let x  X  X  distri-direction of  X  t , which has a 10% chance to reverse, and d denotes the step length of evolution. To evolve p(y|x) , we let b t have a 50% chance to become b t+1 = b t +1 . Fi-chunk at time stamp t has a 20% chance to be a noisy chunk. We can assign class labels by equally dividing the first part of Eq. (9). KDDCUP X 99 Data. Since many research efforts have reported that the concepts underlying this dataset [15] appear to be linearly separable, we complicate the learn-ing task by using the following four approaches to build different types of data streams. (1) Random Selection: we randomly select 100 data chunks, each of which contains 1000 instances with an equal class distribution; (2) Random Noisy Selection: we randomly select 20% data chunks from (1), and then arbitrarily assign each in-stance a class label which does not equal its original label; (3) Rearranged Selection: we first find the most informative attribute by using the information gain, then we sort all of the instances by this attribute, and the sorted instances are finally put into 100 data chunks each of which contains 1000 instances. (4) Rearranged Noisy Selection: we add 20% noisy data chunks in (3) in a similar way to the procedure in (2). 4.1 Experimental Results According to each classifier X  X  accuracy, we ra nk all of the algorithms in an order of 1 (which is denoted as #W) to 5 (which is denoted as #L). We measure the performances of all the algorithms by using the average accuracy (Aacc), average ranking (AR), standard deviation of the ranks (SR), and the number of #W and #L. Results on Synthetic Streams. From the results in Tables 1 and 2, we can observe that among the five algorithms, AE performs the best, with the highest average accu-racy and ranking; VE performs the second best, with the second highest accuracy and ranking; HE and WE are considered the th ird with a tie; and the single tree is the least accurate method for stream data. Additionally, we can draw several other con-clusions: (1) With the same base learners, HE and WE appear to perform similarly, and adding a weight to each base classifier does not seem to be very helpful; (2) HE and WE mostly have the least AR, and they are consistently ranked inferior to AE and VE, but superior to the single tree; (3) VE always has the most winning chance, whereas AE always has the least chance to lo se; and (4) compared to other four meth-ods, the single decision tree is the least accurate method for stream data mining, which has the lowest accuracy and ranking, and minimal winning chance.
 Results on KDD Cup X 99 Data. The results reported in Table 3 validate our conclu-sion that the weighted ensemble does not have any difference from the horizontal ensemble, and AE always performs the best. Since random selection selects data chunks from the raw data set without any revisions, we can regard it as a realistic data stream. It is safe to say that AE performs the best on this realistic data stream. Table 4 reports a random selection with 20% noise and we can observe VE and the single tree are more vulnerable to noise. Compared with Table 4, the accuracy of VE and the single tree significantly drops, while AE, HE and WE marginally drop. This tells us that buffering a small number of data chunks can prevent a significant drop of accu-racy caused by noise. Table 5 lists the results of a rearranged method, and we can observe that VE performs the best on all of the five measurements. The rearrangement procedure, in fact, generates a special data stream according to the learnable assump-tion. So the classifier ensemble built on the most recent data chunk is better than the classifier ensemble built with several buffered data chunks. That is why VE and the single tree perform better than AE, HE and WE. Table 6 reports the results of a rear-ranged noise selection method, with 20% noise in addition to the change of p(x,y) . Among them, AE achieves the best. This is because the rearranged noise selection method generates a data stream that experiences concept drifting and data errors si-multaneously, and AE performs the best under this circumstance. Accurately formulating real-world data stream is the first step in developing effective stream data mining algorithms. In this paper, we argued that the existing stationary assumption, which assumes that the training and test data chunks share similar or identical distributions, is unrealistic for characterizing real-world data streams mainly because it violates the concept drifting nature. On the other hand, the learnable assumption which was tailored to solve the concept drifting problem takes no consid-eration on data errors. In this paper, we have formulated a Realistic Assumption by taking both concept drifting and data errors into consideration to characterize realistic data streams. In our assumption, we claimed that real data streams may experience concept drifting and data errors simultaneously. Under this assumption, we have pro-posed an Aggregate Ensemble (AE) method to mine data streams with both concept drifting and data errors. Experimental results on both synthetic and real-life data have demonstrated that AE outperforms all ot her methods including horizontal ensemble and vertical ensemble methods. Our empirical studies have also investigated the overall performance of the popular stream mining methods on the data streams char-acterized by the proposed realistic assumption. Acknowledgements. This research has been supported by the National Science Foundation of China (NSFC) under Grants No. 60674109 and No. 70621001. 
