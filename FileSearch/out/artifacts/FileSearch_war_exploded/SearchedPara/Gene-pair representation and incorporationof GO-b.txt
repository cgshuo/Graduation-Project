
Hochschule Weihenstephan-Triesdorf, Freising, Germany Corporate Technology Division, Siemens AG, Erlangen, Germany 1. Introduction
Since the human genome has been sequenced in 2003 [18], biological research is becoming more interested in the genetic cause of disease. Microbiologists try to fi nd responsible genes for the disease under study by analyzing the expression values of genes. To extract useful information from the ge-netic experiments and to get a better understanding of the disease, usually a computational analysis is performed [29].

One of the commonly used learning algorithms with genetic data is called k -Nearest-Neighbour clas-si fi cation ( k -NN). The Euclidean distance of an unseen patient record to the patients X  records used for training the model is normally calculated and the k nearest neighbours are determined [35]. Usually, the new data instance is then labeled with the most frequent class of the k neighbouring instances. The k -NN algorithm is not the only approach that uses a distance function for classi fi cation. During the last three decades, the importance of the distance function in machine learning has been gradually ac-knowledged [6]. However, for many years only canonical distance functions or hand-crafted distance functions were used. Recently, a growing body of work has addressed the problem of supervised or semi-supervised learning of customized distance functions [17]. In particular, two different approaches, learning from equivalence constraints and the intrinsic Random Forest similarity have been introduced and shown to perform well on image data [6,19,38]. Many characteristics of gene expression data are similar to those of image data. Both data types usually have a big number of features where many of them are redundant, irrelevant or noisy. For this reason, we assume that learning distance functions can be of bene fi t also for the classi fi cation of genetic data. Some evidence for this is considered in [40].
Another often used possibility to improve the analysis of genetic data is to exploit available external biological knowledge [10,22,28,43]. The Gene Ontology (GO) [3] is a valuable source of biological knowledge that can be incorporated into the process of classi fi cation or clustering of genetic data. GO offers a controlled vocabulary of gene products containing their annotations and characteristics. The main effort of the GO project was to build a controlled structure describing the interactions and relations between gene products. Therefore three species independent ontologies (structures) have been devel-oped describing each gene product in terms of their associated biological process, cellular component and molecular function. A gene product can be associated with or located in one or more cellular com-ponents, be active in one or more biological processes and can perform several molecular functions. The GO is a set of words and phrases used for indexing and retrieving information and also provides the relationships between two terms, making it a structured vocabulary. The structure of the GO vocabulary is represented with an acyclic graph, where each node represents a GO term and each arc describes a directed relationship between these two terms. Each term can have more than one parent. Section 2.2 will give a brief overview of how knowledge can be derived from the GO to be incorporated into classi-fi cation. The content of GO is periodically re fi ned and constantly growing, becoming a more complete and reliable source of knowledge. The number of entries in the GO, for example, has increased from 27,867 in 2009 to 33,217 in 2010, which is a more than 10% increase.

Genetic datasets normally contain gene expression values, where each feature is the expression of a single gene. In biology, however, the in fl uence of a gene on a certain disease often depends not only on its own expression, but also on the expression of some other genes, interacting with it. Thus, at a certain disease, the higher expression of gene A might only be in fl uencing the etiopathology if another gene B is over-or under-expressed, too. By training a classi fi er with microarray data with the usual single gene representation, these dependencies are often neglected or are at least more dif fi cult to grasp and thus require strong adaptive learners. To better consider these co-operations, the data can be transformed into another representation. A normalization of each gene X  X  expression with respect to the other genes is needed.

Another motivation for the new representation is the incorporation of semantic similarity into classi fi -cation. The GO provides similarity measures between two genes A and B while most learning algorithms when the usual representation is used, consider differences between patients with the given gene expres-sion values. The GO provides information about gene-gene interactions and the classi fi ers normally use patient-patient relations for classi fi cation. There is no obvious way how to incorporate the semantic similarity between two genes to improve the classi fi cation of patients, when the plain representation is used.

Thus, in this paper, a novel data representation for learning from gene expression data is introduced, which is aimed at emphasizing gene-gene interactions in learning. With this representation, the data simply comprise differences in the expression values of gene pairs and not the expression values them-selves. An important bene fi t of this representation, except the better sensitivity to gene interactions, is the opportunity to incorporate external knowledge in the form of semantic similarity corresponding to the pairs, which is also studied.

This paper is organized as follows. In Section 2 we review related work and the datasets used; in Sec-tion 3 we introduce the methods; the novel gene-pair representation and the application of the semantic inter-gene similarity for guiding feature selection and feature weighting. Section 4 presents our empir-ical study aimed at the evaluation of the gene-pair representation and of the use of semantic similarity in order to guide the process of feature selection. Section 5 concludes the paper with a summary, and a discussion of limitations of the present study and directions for future work. 2. Materials: Related work and data sets 2.1. Machine learning in bioinformatics
Much work has been done in the area of machine learning over the last few decades and as bioin-formatics is gaining more attention, different techniques have been applied to process genetic data. The most crucial problems in classifying genetic data presently are missing data imputation, discrimina-tive feature selection, classi fi cation, and clustering. A single Microarray experiment may contain many thousands of genes (rows) under different conditions (columns) and is scanned automatically by a robot. The scanning procedure can sometimes result in data leaks caused by scanning problems, insuf fi cient resolution, image corruption, scratches, dust or defective spots. However, most data mining algorithms require a complete data matrix for a proper processing. In a related paper, Troyanskaya et al. [37] im-plemented and evaluated different methods of missing data imputation for gene expression data. Three different methods where compared: Singular Value Decomposition (SVD), row average and k -NN re-gression, where the last one was shown to perform best. In addition to a usually small sample of instances (conditions), the large amount of gene expression values complicates classi fi cation. For that reason, a feature selection step is normally conducted before classi fi cation to determine discriminative genes and eliminate redundancy in the dataset. For an exhaustive review of feature selection techniques for gene expression data, an interested reader is referred to  X  X  review of feature selection techniques in bioinfor-matics X  [33]. After the imputation of missing values and selection of the most meaningful features, the data can either be used for unsupervised or supervised learning. The main task in unsupervised learning is to cluster unlabeled data into groups such that an instance is more similar to all members of the same group than to any instance of all the other groups. Ben-Dor et al. [8] introduced a data mining algorithm called CAST (Cluster Anity Search Technique) which has been further improved in 2002 by Bellaachia et al. [7] and was shown to perform well on clustering gene expression data. In contrast to unsupervised learning, supervised learning uses labeled data to train a classi fi cation system which is able to predict the labels of unseen (unlabeled) data. A few empirical comparisons of classi fi cation algorithms including k -NN, Linear Discriminant Analysis (LDA), classi fi cation trees (like C4.5 [31] or CART [30]), Support Vector Machines (SVM), Arti fi cial Neural Networks ( ANN) and their combinati ons using Bagging and Boosting for several cancer gene expression studies have been published [11,23,36,44]. 2.1.1. Learning distance functions
Historically, research on distance functions in machine learning has started from supervised learning of distance functions for k -NN classi fi cation [35]. Since that time canonical distance functions like the Euclidean distance or Mahalanobis distances have been used. Even today the Euclidean distance function is used in many applications although it is well known that its use is justi fi ed only when the feature data distribution is Gaussian. Mahalanobis metric learning has received big research attention but it is often inferior to many non-linear and non-metric distance techniques and usually fails for learning distances from image data [17], too.

Discriminative distance functions have been used in various classi fi cation domains and in image pro-cessing and pattern recognition. Hertz [17] performed extensive research in this area and presented three novel distance learning algorithms: Relevant Component Analysis (RCA), DistBoost and Kernel Boost. In RCA, a Mahalanobis distance metric is learned which is optimal under several conditions, using generative learning with positive equivalence constraints. Hertz described the application of these novel algorithms to various data domains including clustering and classi fi cation. It was shown that us-ing the presented improved distance functions instead of off-the-shelf distance functions a signi fi cant improvement was reached in all of these applicatio n domains. Bar-H illel [6] discussed learning from equivalence constraints where distance function learning is considered as learning a classi fi er de fi ned over instance pairs. A new method, named coding similarity, has been introduced and shown to hold an empirical advantage over the common Mahalanobis distance. Based on this algorithm a two-step method for subordinate class recognitio n in images was developed.

It was shown that the most popular Euclidean and Manhattan distance metrics are not suitable for many data distributions. Yu et al. [45] proposed a novel boosting-based algorithm that fi nds the best distance metric dynamically for a given dataset. This boosted distance metric was shown to give robust results with fi fteen benchmark datasets from the UCI repository [9] and with two image retrieval applications. Later, Yu et al. [46] further improved this metric and presented a general guideline to fi nd a more optimal distance function for similarity estimation with a speci fi c dataset. Tsymbal et al. [38 X 40] performed a detailed comparison of learning discriminative distance functions for case retrieval and decision support. Two types of discriminative distance learning methods, learning from equivalence constraints and the intrinsic Random Forest similarity have been compared. They have demonstrated that both distance learning techniques are competitive to plain induc tive learning (includi ng boosting and Random Forest itself) while the Random Forest similarity exhibits a slightly more robust behavior and is more stable with missing data and noise. 2.1.2. Learning from equivalence constraints
Today, the most commonly used representation of distance learning is the one based on equivalence x 2 are data points in the original space and y is a label indicating whether x 1 and x 2 correspond to the same class or not. This is also called learning in the product space [6]. The product space is out of scope for this paper and will not be considered in more detail. Another possible approach is to learn in the space of vector differences, called difference space, and is often used with homogeneous high-dimensional data such as pixel intensities in imaging. For learning from equivalence constraints in the difference space, the dataset needs to be transformed. For two instances A and B from the original set with attributes a 1 ,...,a n and b 1 ,...,b n , the transformation into the difference space is performed as shown below Eq. (1), where d indicates an elementary distance function: Each difference example is provided with a label, which is 1, when the original examples belong to the same class (are  X  X quivalent X ), and 0 otherwise. A certain learning algorithm, e.g. Random Forest or AdaBoost, is then used to learn a distance function from the equivalence constraints. The tests presented in this paper all have been performed using the simple L 1 distance to obtain the difference space. Our extensive comparative analysis has demonstrated it to be the best candidate in comparison with the possible alternatives.

The availability of equivalence constrains in most learning contexts and the fact that they are a natural input for optimal distance function learning [6] are two crucial reasons that motivate their use. It was class, the optimal distance function under the i.i.d. assumption can be expressed in terms of generative models p ( x | y ) [25] as shown in Eq. (2). The function (2) was shown to approach the Bayesian optimal accuracy [25] and was analytically proven to be at least as good as any distance metric, which satis fi es the metric properties. 2.2. Incorporation of GO-based semantic similarity
Based on the controlled vocabulary of the GO, two genes can be semantically associated and further a similarity can be calculated based on their annotations in the GO. For example, Sevilla et al. [34] have successfully applied semantic similarity which is well known in the fi eld of lexical taxonomies, AI and psychology, to the GO, by calculating the information content of each term in the ontology based on several methods including [20,24,32]. Sevilla et al. [34] computed correlation coef fi cients to compare the physical inter-gene similarity with the GO semantic similarity. The experiments demonstrated a bene fi t for the similarity measure of Resnik [32], resulting in higher correlations.

Later Wang and Azuaje [42] have integrated the similarity information from the GO into the clustering of gene expression data and introduced a novel way to use the GO for knowledge-driven genetic data procession. They have shown that this method not only ensures competitive results in terms of clustering accuracy, but also has the ability to det ect new biological dependenc ies for the given problem.
Another approach to incorporate the knowledge available in the GO into machine learning is to use it for feature selection. Thus, Qi and Tang [28] have introduced a novel method to select genes not only by their individual discriminative power, but also by integrating the GO annotations. The algorithm cor-rects invalid information by ranking the genes based on their GO annotations and was shown to boost accuracy in all four tested public datasets. Chen and Tang [10] further investigated this idea, suggested a novel approach to aggregate semantic similarities a nd integrated it into traditional redundancy evalua-tion for feature selection. This resulted in higher or comparable classi fi cation accuracies on benchmark datasets with using less features compared to the common feature selection techniques. The approach we introduce here is inspired by these attempts but is different in that the semantic similarity is applied directly to the corresponding gene pair in the novel gene-pair representation. 2.3. Benchmark datasets
For evaluation, ten benchmark datasets have been used, see Table 1. The datasets are associated with different clinical problems and have been received from various sources. Colon [2], Embryonal Tu-mours [27], Leukemia [12] and Lymphoma [1] datasets have been obtained from the Bioinformatics Research Group, Spain ( http://www.upo.es/eps/aguilar/datasets.html ), Arcene [14] is available at the UCI repository [9], Lupus [5] originates from The Human-Computer Interaction Lab at the University of Maryland, USA, Breast Cancer [41] is obtained from the University of Minnesota, and Lung Can-cer [13] from the Division of Thoracic Surgery at Brigham and Women X  X  Hospital, USA. The Mesh dataset represents 3D shapes and was generated from cardiac aortic valve images, see [19]. The last dataset, HeC Brain Tumours is obtained from the hospitals participating in the EU FP6 Health-e-Child consortium, see www.health-e-child.org . The datasets are public and often used in research studies, ex-cept the last two which are not publicly available. 3. Methods 3.1. The gene-pair representation and experimental setting
Genetic datasets similar to those considered in Section 2.3 normally contain gene expression values, where each feature is the expression of a single gene. In biology, however, the in fl uence of a gene on a certain disease often depends not only on its own expression, but also on the expression of some other genes, interacting with it. Thus, at a certain disease, the higher expression of gene A might only be in fl uencing the etiopathology if another gene B is over-or under-expressed, too. By training a classi fi er with microarray data with the usual single gene representation, these dependencies are often neglected or are at least more dif fi cult to grasp and thus require strong adaptive learners. To better consider these co-operations, the data can be transformed into another representation. A normalization of each gene X  X  expression with respect to the other genes is needed.

Another motivation for the new representation is the incorporation of the semantic similarity into classi fi cation. The GO provides similarity measures between two genes A and B while most learning algorithms when the usual representation is used, consider differences between patients with the given gene expression values. The GO provides information about gene-gene interactions and the classi fi ers normally use patient-patient relations for classi fi cation. There is no obvious way how to incorporate the semantic similarity between two genes to improve the classi fi cation of patients, when the plain representation is used.

For these reasons, the original datasets can be transformed into a new representation of gene-pairs instead of single genes. First, all possible pairs of single genes available in the dataset are generated. As the information of the two pairs of a gene A and a gene B , Pair (A,B) and Pair (B,A) is redundant, only one of these pairs is used. If n is the number of genes (features) in the dataset, then ( n 2  X  n ) 2 pairs are generated by subtracting the gene expression for one gene from another. The Pair (A,B), for example, is constructed by subtraction A  X  B. Note that compared to the common usage of a distance, where the absolute value is often used, the sign is kept in these pair calculations in order not to lose important information.
 We call this new representation Gene-Pair Representation . On the scheme above, three genes are trans-formed into three gene pairs. But the number of pairs increases with the number of genes. Most datasets in our experiments have been reduced to 200 features, so that 19,900 gene pairs are constructed. Train-ing a classi fi cation model may not fi nish in a limited period of time with this large number of features. Moreover, most of these pairs will not be useful for classi fi cation.

Therefore, a fi lter feature selection, ReliefF was used fi rst to pre-select 400 pairs in advance. The fi lter selection is used to pre-select a certain amount of pairs, because the wrapper feature selection approach is normally prohibitively computationally expensive. After the pre-selection, a fi ner feature subset eval-uation and selection method together with a greedy stepwise search (forward sequential inclusion) is applied to select a set of 100 most discriminative pairs out of the 400 pre-selected pairs, see Fig. 1. In the experiments done for this work, Correlation-based Feature Subset Selection (CFS) [15] has been used as the second wrapper-like feature subset selection technique, in order to re fi ne the initial set of 400 features. With this new gene-pair representation, the genes are normalized with respect to the other genes and it is possible to include the GO semantic similarity as long as the transformed datasets are represented with gene-gene relations. Therefore, an instance I in the new gene-pair representation is de fi ned by a certain amount n of pairs of genes p i = { g i 1 ,g i 2 } and a class label c . Our preliminary experiments with this representation have demonstrated that the classi fi cation models usually perform well already with 100 pair-features, if a proper feature selector is used. Selecting more gene pairs, in general, does not increase accuracy, and requires considerable time as long as feature selection is usually a computationally expensive task.
Arcene and Mesh datasets are non-genetic, but they contain enough features to get comparable results and were used as non-genetic reference datasets. As the base for our experiments, reduced datasets have been used, containing not more than 200 single gene features (if necessary, pre-selection was conducted with the GainRatio feature fi lter). The fi nal number of gene pairs to select ( CFS , Correlation-based Feature Selection for subset evaluation, together with the greedy stepwise search with forward inclusion available in Weka [16] were used) was set to 100 with 400 gene-pairs preselected by ReliefF . For datasets containing more than 90 cases, 30 iterations of 10-fold cross validation has always been used in our experiments and leave-one-out cross validation for the others. Each representation was evaluated on all datasets with four classi fi ers, plain k-Nearest-Neighbour (kNN) classi fi cation, Random Forest (RF) , kNN with learning from equivalence constraints (EC) ,and kNN with intrinsic Random Forest Similarity (iRF) . To measure the robustness of the gene-pair representation to noise, the same tests have been conducted also with noisy data. 10% and 20% class noise has been injected arti fi cially into the training set. 3.2. Integration of GO semantic similarity and experimental setting
With the novel representation of features as the difference in the expression values of a pair of two genes introduced in Section 3.1, it becomes possible to incorporate the external biological knowledge into classi fi cation. There are several reasons that motivate us to use semantic similarity to guide the classi fi cation of gene expression data. First, the selected pairs may itself contain valuable biological knowledge that can be used to guide the classi fi cation. A set of two genes that are known to interact (that is the genes whose semantic similarity is expected to be relatively high), might be more useful for clas-si fi cation than two genes that are not associated with each other. The usually big number of features in gene expression data makes it dif fi cult for common feature selection methods not to ignore some impor-tant features. The support by semantic similarity might guide the selection process and help to consider pairs that would otherwise be neglected by the feature selection algorithm but might be representative for the studied task. Next, the importance of a pair can be weighted by the semantic similarity between the two genes. Gene-Pair-features with a high semantic similarity might be more useful for classi fi ca-tion than the other. In both approaches described above, the background biological knowledge is used to support classi fi cation decisions and is assumed to increase the classi fi cation accuracy. Compared to all other reports of using semantic similarity for classi fi cation support, the approach used for this paper is different as the gene-pair representation was developed offering the opportunity to apply the semantic similarity directly to the expression values.

In the experiments conducted for this work, the GO semantic similarity can be applied to the learning algorithms generally in two different ways, by feature weighting or feature selection. 3.2.1. GO-based feature weighting
As feature weighting has already been successfully applied with the k -NN classi fi er before [35], the experiments with GO-based feature weighting have been conducted with the k -NN classi fi cation. The weighting can be accomplished in two different ways, either directly incorporated at calculating the feature values for the pairs or be integrated within the work fl ow of the k -NN algorithm.
In the fi rst solution, weights are simply incorporated for each gene-pair immediately while construct-ing the feature pairs as follows. Each feature pair, or the difference between the two corresponding values, is multiplied by the corresponding seman tic similarity value co rresponding to the pair: The second possibility is to use the semantic similarity values as weights within the k -NN classi fi er, using the weighted Euclidean distance, where each feature is assigned a certain weight [21]. We used only one learning algorithm in these experiments ( k -NN), as long as the main purpose was only to test the hypothesis whether the predictive performance can be improved by the incorporation of GO-based feature weighting. Moreover, k -NN is known as a robust classi fi er often used in the context of classi fi cation of microarray data. 3.2.2. GO-based feature selection
The second approach to incorporate semantic similarity to support classi fi cation is to select pair-features based on their GO similarity values. A related feature selection framework has been imple-mented and evaluated. Five different ways to select pairs have been implemented in the framework and studied, as follows: 1. A selected number of pairs with the biggest semantic similarities. 2. A selected number of pairs with the lowest semantic similarities. 3. All pairs with a similarity bigger than a given threshold. 4. All pairs with a similarity smaller than a given threshold. 5. Thepairswithade fi ned value x and all pairs with a value y that does not differ from x in more For experiments conducted for this paper, only the fi rst method is used, to select pairs with the biggest similarity, which has proven to be the most competitive and robust approach in a series of preliminary experiments. This feature selection has been combined with the CFS feature selection to exclude noisy and redundant pairs. First, 400 pairs with the biggest GO similarity have been selected, followed by CFS to select the 100 most discriminative gene-pairs out of 400.

To identify the best matching semantic similarity calculation technique for genetic data, different combinations of similarity calculation methods have been tested. There are three parameters that can in fl uence the classi fi cation result. First, the method used to calculate the similarity between single GO terms. Next, the algorithm to combi ne these similarities for two gen es (each gene is nor mally annotated with more than one term), and last, the way to apply the similarity to the learning algorithms. Except accuracy evaluation, the average semantic similarity of all calculated pairs and of the selected pairs have been reported. It was assumed, that pairs selected by the common feature selection have a higher average semantic similarity than all the pairs. This means that pairs with a high similarity have more chances to be discriminative for classi fi cation and therefore it is more probable that GO-based feature selection will improve classi fi cation accuracy. 3.3. Test con fi gurations
For the GO semantic similarity, only six datasets could be used as it is necessary to have genetic datasets where the attribute gene-names are known. Only Breast, Colon, Embryonal Tumours, HeC Brain Tumours, Leukemia and Lupus include attribute names where the corresponding gene can be identi fi ed. As Leukemia includes only 38 attributes that can be matched to genes, this dataset has thus been also excluded from the experiments. As the GO is not complete and some attribute names cannot be matched to genes, the datasets have been reduced in their number of features, to be able to retrieve at least one GO term for each attribute remaining in the dataset. A schematic description of translating the datasets into GO ID based datasets can be seen in Fig. 2. First, NCBI EUtils is used to match the gene symbols from the datasets. Further, their of fi cial gene symbols are extracted, and the corresponding GO terms are determined.
First, the fi ve datasets have been tested with different combinations of similarity calculation methods and the two possibilities to i nclude feature weighting. The followi ng similarity calculation methods have been compared:  X  Max-Resnik : Maximum value of Resnik [32] similarities;  X  Max-Lin : Maximum value of Lin [24] similarities;  X  Max-Wang : Maximum value of Wang [43] similarities;  X  Azuaje-Resnik : Azuaje [4] combination of Resnik [32] similarities;  X  Azuaje-Lin : Azuaje [4] combination of Lin [24] similarities;  X  Azuaje-Wang : Azuaje [4] combination of Wang [43] similarities;  X  Schoen-Lin : Schoen combination of Lin [24] similarities;  X  GIC : Graph information content [26];  X  Random : Random values in the range [0,1] are used as similarity; The Random method was used as a reference method, to see whether the change in accuracy is caused by the methods or by chance. The method Schoen is a combination of the Max and Average methods. First, all combinations of GO similarities of the two terms from the two genes are calculated. The upper third of the values with the highest semantic similarity are considered and the average over this group is used as the fi nal similarity value. The Schoen method was tested only in combination with the Lin similarity. The intention was only to test the comparability in classi fi cation accuracy of the Schoen similarity calculation to the Max and the Azuaje approach.

The described methods have been compared by performing tests on the datasets Breast, Colon, Em-bryonal Tumours and Lupus. The two described methods to incorporate GO-based feature weighting have been compared under the same classi fi cation conditions and the average semantic similarities of the pairs and the selected pairs have been reported.

Next, the GO-based feature selection technique was tested without feature weighting. The 400 gene-pairs with the highest semantic similarity have been preselected followed by a CFS based reduction to 100 pairs. This method has been evaluated on the fi ve datasets, including the HeC Brain Tumours dataset and compared with respect to the classi fi cation accuracy.
 4. Experimental results 4.1. Gene-pair representation
The gene-pair representation was compared with the plain representation and evaluated with four classi fi ers. The following con fi gurations have been used for the experiments.  X  400 preselected pairs by ReliefF .  X  100 pairs selected by CFS and used for training the models.  X  Four classi fi ers as follows:
It was shown that k =7 and case weighting inversely proportional to distance is the most robust pa-rameter choice for kNN in our preliminary tests, which was thus used in all nearest neighbour classi fi ers in our experiments. The experiments have been implemented and conducted based on the Weka machine learning library in Java [16] and default parameter values were always used for classi fi ers and feature selectors unless otherwise stated here.

Main experimental results are presented in Table 2, where the fi rst rows of each dataset correspond to the original representation and the second row includes results for the gene-pair representation. Each column includes results for one learning algorithm and the average over all four classi fi ers is presented in the last column. First, results for the genetic datasets and averages over them and then results for the non-genetic datasets and the averages are presented.

For six out of eight genetic datasets the novel representation outperforms the original one (according to the average performance and most particular accuracies). Only for Breast and Lupus the original representation reached better results on average, but with less than 0.56% difference each. The gene-pair representation could increase the average accuracy over all classi fi ers and datasets by 1.67%. Moreover, all the four classi fi ers demonstrate a better average performance with the new representation, where distance learning from equivalence constraints appears to have the biggest accuracy increase.
The experimental results show a clear bene fi t of the novel representation, which is presumably mo-tivated by the fact that genes often depend on each other. To validate the assumption that this is the main reason for its better performance, two non-genetic datasets have also been tested. Table 2 shows how the gene-pair representation may fail for non-genetic data. The results indicate that the bene fi tof the gene-pair representation presumably relies on the interactions of genes indeed. Conducting a more extensive analysis on a wider set of genetic and non-genetic data to validate and explore in detail this curious trend is an interesting direction of future work.

From the results of our experiments with noisy data (these results are not included in this paper for the sake of brevity) it can bee seen that the novel representation is unfortunately less robust to noise with respect to the original representation. For no noise the pairs outperform the single genes by 1.67% while with 10% class noise they perform almost equal and with more noise the gene-pair representation performed worse. With the gene-pair representation it is easier to over fi t noise; one thus needs to be cautious and take this into consideration when using it.

The gene-pair datasets have been reduced to 100 pairs for classi fi cation while the original represen-tation uses 200 features for training the models. The gene-pair representation was shown to improve the accuracy with only half of the original number of features. Note that this does not mean that only the information of 100 genes is present here, as one pair represents two different genes. The statistics of selected pairs show a tendency to select a big number of pairs where the same feature is included. In some cases this feature was highly ranked in the old representation as well. However some features being part of many pairs have not been ranked high for the plain representation. A deeper and more thorough analysis of these statistics is a direction f or future work. In addition, it is also important to note that selected gene pairs will not only be useful as input for learning algorithms but can also provide precious information about gene interactions having important in fl uence on the disease under study. 4.2. Incorporation of the GO semantic similarity
The datasets have been reduced to contain only features that can be matched exactly to a gene and that can be associated with a GO identi fi er. Therefore the number of features for the fi ve test sets in this section decreased. Some discriminative features may thus have been deleted, and therefore the accuracies reported for these experiments are not comparable with the results of the pair representation evaluation. 4.2.1. GO-based feature weighting
Four datasets have been tested with different similarity calculation methods and two different ap-proaches of semantic similarity based feature weighting. All tests have been run under the same con-ditions as described for tests with the gene-pair representation. The average accuracies over the four datasets have been calculated and are shown in Table 3. Each row includes one used method while the last one is called Random where no similarity values have been used but random values in the range between 0 and 1. The fi rst column shows the results for feature weighting within the k -NN classi fi er (with the weighted Euclidean distance) and the second one demonstrates the corresponding results for the direct weighting of the pairs, incorporated in the pair features themselves. The third column shows the average similarity value of the 100 selected pairs while the last column shows the similarity value over all calculated pairs. The fi eld No FW shows the results of the reduced pair representation without feature weighting.

The results demonstrate that for each dataset, at least one combination of methods could reach or even beat the non-weighted gene-pair representation. But this seems to be not the effect of semantic similarity. The random similarity was able to reach the accuracy for the experiments without weighting on two datasets and increased it for the other two datasets. This result is somehow counter-intuitive and was not expected. Preliminary results for experiments with the gene-pair representation showed that the new representation is over fi tting noise. We assume th at the reason for the increase in accuracy here is not caused by the weighting with semantic similarity, but is due to the injection of noise into the pairs, making the system more robust to over fi tting. More experiments to test this hypothesis is a direction for future work. Table 3 shows the average values for each combination. For weighting the pairs incorporated into the values, the Max-Lin combination performed best while for feature weighting within k -NN, the GIC method reached the best accuracy. However, the Random reference technique got a comparable result. In summary, from the experiments it was observed that weighting the pair features as introduced in this paper can slightly improve the classi fi cation accuracy in some contexts, but a reference test with random similarity values has demonstrated surprisingly robust accuracy, too. Overall, from our experiments with feature weighting, we could not deduce an approach that could be recommended for future use with other datasets. The main reason for this is believed to be the problem of data over fi tting, which can be solved with the use of less fi ner feature characterization, and in particular with the use of feature selection instead of feature weighting, addressed in the next section. 4.2.2. GO-based feature selection
Next, experiments with the GO semantic similarity based feature selection have been performed. The results of the feature weighting tests could not show a clear bene fi t for a semantic similarity method to increase the classi fi cation accuracy. Therefore it was not obvious which similarity method should be used for GO-based feature selection. Because all combinations including the Max algorithm are not sensitive to the difference in the problem, in that a big amount of semantic similarities is equal to 1, these methods could not be used. GO-based feature selection is de fi ned to pre-select pairs with a higher similarity; therefore the most discriminative pairs selected with a common feature fi lter (Gain Ratio) have been fi rst analyzed with the goal to fi nd a semantic similarity calculation method where the average value of selected pairs is higher than the average over all the pairs. The percentage of the increase of the average selected pair similarity for most discriminative pairs ( s all the pairs ( s
The difference of the average semantic similarity between the most discriminative pairs and all the pairs is negligible for most datasets. For the Schoen method, the similarity of the selected pairs is 4% higher. But the GIC, Graph Information Content [26], semantic similarity was shown to demonstrate the highest correlation with the physical gene expression similarity (Pearson X  X  correlation in gene expression values), and be the best guide for feature selection (in particular, the GIC values in the group of  X  X est X  most discriminative pairs was shown to be 71% higher than for all the pairs, while this increase was much lower or even absent for the other techniques). This is in line with the results presented in [26]. Thus, GIC was used in our experiments with the GO similarity. Therefore, GIC similarity was selected further for testing the GO-guided feature selection. It was assumed that if the pairs selected by common feature selection algorithms have a higher average GIC similarity then a pre-selection of pairs with high GIC similarity can bene fi t the feature selection too.

In more detail, the GIC similarity [26] is de fi ned as where A and B are term-sets annotating the two gene products in the GO for which similarity is sought, and IC ( t ) is information context for term t (  X  log p ( t )) .

In the main experiments, 400 pairs with the highest GIC semantic similarity have been pre-selected for each dataset followed by a CFS feature selection to reduce the number of pairs to 100. The combination with a common feature selection method (CFS) is needed to eliminate redundant and irrelevant features. It must be noted that for some of highly semantically similar gene pairs expression values are also iden-tical or strongly correlated without any deviations. This makes these pairs useless for classi fi cation and thus necessitates their removal. The tests have been performed under the same conditions as described in the previous section. The main results can be found in Table 5.

As can be seen from the Table 5, the GIC guidance usually results in an increase in accuracy. The biggest increase was reached with the Embryonal Tumours dataset, 6.67%. This is the best accuracy ever reached in all experiments conducted for this study. The overall average accuracy with no GIC guidance is 80.23%. The GIC guidance improves this number by 2.08% to 82.31%. It could be noted that for datasets where the average semantic similarity of selected pairs was much higher than for all the pairs, the GIC-based pre-selection could also improve accuracy.

The bad performance for Lupus is not surprising as the gene-pair representation failed for this dataset too. One reason for this might be that for this dataset gene interactions are not so important or not re fl ected well in the data. The other dataset failed, Colon, performed well with the gene-pair representa-tion, and its worse performance with the GIC guidance is rather unclear. The affecting factor here could be the fact that for many genes which might be discriminative information is still absent in the GO and they were thus excluded from the experiments. However, this trend is expected to change when the GO will become more complete. Validating this hypothesis forms an important direction of future work. 5. Conclusions
A new representation for genetic datasets has been proposed, which emphasizes interaction between genes. The new representation was shown to increase accuracy on genetic data by 1.67%. The assump-tion that this increase is caused by the re fl ection of gene-gene interactions could be validated by testing the gene-pair representation on non-genetic datasets where worse results were obtained. The gene-pair representation increased the accuracy for six out of eight genetic datasets. Tests with arti fi cial noise showed that the gene-pair representation is prone to over fi tting noise by the classi fi ers using it.
GO-based feature selection was shown to perform well in combination with a common data-driven feature selection method (CFS) and the use of GIC similarity and could improve the classi fi cation accu-racy by 2.08% over the tested datasets in comparison with the use of the plain feature selection only.
More thoroughly analyzing dependencies between the data driven feature importance measure and the semantic similarity of the pairs can provide a better understanding of how to better use the GO similarity for feature selection. A histogram of dependencies between the semantic similarity of the pairs for Colon and the GainRatio feature merit of the gene-pairs is shown in Fig. 3. The pairs are divided into 100 bins based on their corresponding semantic similarity values, where each bin contains the same number of pairs. This histogram shows that the fi rst groups, the groups with the highest semantic similarity, are ranked low on average (this trend holds true for all datasets, at least for all those included in our study). Notice that the low value is an average value over the GainRatio of the pairs included in the bin. There are also gene-pairs of high importance within the fi rst bins. To better analyze these trends and to use the knowledge that can be derived from these correlations for better feature selection is a promising direction for future work. Another important direction for future work is the validation of statistical signi fi cance of these results. With the present datasets including a relatively small number of instances it is rather impossible; however the microarray technology is ge tting cheaper and more affordable, and more and more gene expression data is being collected nowadays for different reasons, making a more thorough investigation a reality.

The main objective of this study was not the creation of a competitive gene expression data classi fi -cation framework, but instead the validation of the new gene pair representation and the techniques for the incorporation of external knowledge in the form of the GO. Clearly, the predictive performance of the framework may still be improved by reconsidering its components and re fi ning their parameters, and in particular it concerns the feature fi ltering and feature subset selection techniques used. This forms an important direction for future work.
 Acknowledgments
This work has been partially funded by the EU project Health-e-Child (IST 2004-027749). The authors wish to acknowledge support provided by all the members of the Health-e-Child consortium in the preparation of this paper.
 References
