 The performance of kernel metho ds, suc h as Supp ort Vector Mac hines, Gaussian Pro cesses, etc. dep ends critically on the choice of kernel. Conceptually , the kernel captures our prior kno wledge of the data domain. There is a small num ber of popular kernels expressible in closed form, suc h as the Gaussian RBF kernel k ( x;x 0 ) = exp( k x x 0 k 2 = (2 2 )), whic h boasts attractiv e and unique prop erties from an abstract function appro ximation point of view. In real world problems, however, and esp ecially when the data is heterogenous or discrete, engineering an appropriate kernel is a ma jor part of the mo delling pro cess. It is natural to ask whether instead it migh t be possible to learn the kernel itself from the data. Recen t years have seen the dev elopmen t of sev eral approac hes to kernel learning [5][1]. Arguably the most principled metho d prop osed to date is the hyperk ernels idea introduced by Ong, Smola and Williamson [8][7][9]. The curren t pap er is a con tinuation of this work, introducing a new family of hyperk ernels with attractiv e prop erties.
 Most work on kernel learning has focused on nding a kernel whic h is subsequen tly to be used in a con ventional kernel mac hine, turning learning into an essen tially two-stage pro cess: rst learn the kernel, then use it in a con ventional algorithm suc h as an SVM to the kernel in its own righ t to answ er relational questions about the dataset. Instead of same, or related. Kernel learning can be used to infer the net work structure underlying data. A di eren t application is to use the learn t kernel to pro duce a low dimensional embedding via kernel PCA. In this sense, kernel learning can be also be regarded as a dimensionalit y reduction or metric learning algorithm. We begin with a brief review of the kernel and hyperk ernel formalism. Let X be the input we mean a symmetric function k : X X ! R that is positiv e de nite on X . Whenev er we refer to a function being positiv e de nite, we assume that it is also symmetric. Positiv e de niteness guaran tees that k induces a Repro ducing Kernel Hilb ert Space (RKHS) F , hypothesis ^ f 2F by solving some varian t of the Regularized Risk Minimzation problem where L is a loss function of our choice. By the Represen ter Theorem [2], ^ f is expressible in the form ^ ( x ) = The idea exp ounded in [8] is to set up an analogous optimization problem for nding k itself in the RKHS of a hyperk ernel K : X X ! R , where X = X 2 . We will sometimes view K regularized risk minimization problem is itself a kernel. To this end, we require that the functions K x be symmetric and positiv e de nite kernel in the remaining two argumen ts.
 De nition 1. Let X be a nonempty set, X = X X and K : X X ! R with K x ( ) = K ( x ; ) = K ( ;x ) . Then K is called a hyp erkernel on X if and only if Denoting the RKHS of K by K , poten tial kernels lie in the cone K pd = f k 2Kj k is pos.def. g . Unfortunately , there is no simple way of restricting kernel learn-k 2Kj k;K x 0 8 x 2X , whic h is a sub cone of K pd .
 The actual learning pro cedure involved in nding k is very similar to con ventional kernel metho ds, except that now regularized risk minimization is to be performed over all pairs of data points: where Q is a qualit y functional describing how well k ts the training data and K = K + . Sev eral candidates for Q are describ ed in [8].
 If K has the prop erty that for any S X the orthogonal pro jection of any k 2 K to the subspace spanned by K x j x 2X remains in K , then b k is expressible as variables ( ij ) m i;j =1 , introducing the additional constrain ts ij 0 to enforce b k 2K + . Finding functions that satisfy De nition 1 and also mak e sense in terms of regularization theory or practical problem domains in not trivial. Some poten tial choices are presen ted in [8]. In this pap er we prop ose some new families of hyperk ernels. The key tool we use is the follo wing simple lemma.
 Lemma 1. Let f g z : X ! R g be a family of functions indexe d by z 2Z and let h : ZZ ! R be a kernel. Then is a family of pointwise positive kernels, then points x and x 0 . For the Gaussian RBF kernel, and heat kernels in general, this similarit y can be regarded as induced by a di usion pro cess in the ambien t space [4]. Just as physical substances di use in space, the similarit y between x and x 0 is mediated by intermediate similarit y. Speci cally , the normalized Gaussian kernel on R n of variance 2 t = 2 , satis es the well kno wn con volution prop erty Suc h kernels are by de nition homogenous and isotropic in the ambien t space. What we hop e for from the hyperk ernels formalism is to be able to adapt to the inhomoge-neous and anisotropic nature of training data, while retaining the transitivit y idea in some form. Hyp erk ernels achiev e this by weigh ting the integrand of (5) in relation to what is \on the other side" of the hyperk ernel. Speci cally , we de ne con volution hyperk ernels by setting in (4) for some r : X X ! R . By (3), the resulting hyperk ernel alw ays satis es the conditions of De nition 1.
 De nition 2. Given functions r : XX ! R and h : XX ! R wher e h is positive de nite, the convolution hyp erkernel induc ed by r and h is A good way to visualize the structure of con volution hyperk ernels is to note that (6) is prop ortional to the likeliho od of the graphical mo del in the gure to the righ t. The only requiremen ts on the graphical mo del are to have the same poten tial function 1 at eac h of the extremities and to have a positiv e de nite poten tial function 2 at the core. 3.1 The Gaussian hyperk ernel are Gaussians. To simplify the notation we use the shorthand The Gaussian hyperk ernel on X = R n is then de ned as Fixing x and completing the square we have where x i = ( x i + x 0 i ) = 2. By the con volution prop erty of Gaussians it follo ws that
K (( x 1 ;x 0 1 ) ; ( x 2 ;x 0 2 )) = It is an imp ortan t prop erty of the Gaussian hyperk ernel that it can be evaluated in closed the opp osite extreme, in the limit 2 h ! 1 , the hyperk ernel decouples into the pro duct of two RBF kernels.
 Since the hyperk ernel expansion (2) is a sum over hyperk ernel evaluations with one pair of argumen ts xed, it is worth examining what these functions look like: with 0 = by a spatially varying Gaussian intensit y factor dep ending on how close the mean of x 2 and x 2 is to the mean of the training pair. This can be regarded as a localized Gaussian , and around in X , whic hev er localized Gaussians are cen tered close to their mean will dominate the sum. By changing the ( ij ) weigh ts, the kernel learning algorithm can choose k from a highly exible class of poten tial kernels.
 The close relationship of K to the ordinary Gaussian RBF kernel is further borne out by changing coordinates to ^ x = ( x + x 0 ) = hyperk ernel in the form Omitting details for brevit y, the consequences of this include that K = ^ K ~ K , where ^ K is the RKHS of a Gaussian kernel over X , while ~ K is the one-dimensional space gener-regularization operator (de ned by h k;k 0 i K = h k; k 0 i L RBF kernels. In summary , K beha ves in (^ x 1 ; ^ x 2 ) like a Gaussian kernel with variance 2( 2 + 2 h ), but in ~ x it just e ects a one-dimensional feature mapping. With the hyperk ernels so far far we can only learn kernels that are a sum of rotationally rescaling of the axes and anisotropic dilations are one of the most common forms of variation in naturally occurring data that we would hop e to accomo date by learning the kernel. 4.1 The Wishart hyperk ernel We de ne the Wishart hyperk ernel as K (( x 1 ;x 0 1 ) ; ( x 2 ;x 0 2 )) = where and IW (; C;r ) is the inverse Wishart distribution over positiv e de nite matrices (denoted 0) [6]. Here r is an integer parameter, C is an a normalizing factor. The Wishart hyperk ernel can be seen as the anisotropic analog of (7) in the limit 2 h ! 0, h z;z 0 i 2 analogy with (8), By using the iden tity v &gt; Av = tr( A ( vv &gt; )), h x;x 0 i IW (; C;r ) = (11) and noting that the integral of a Wishart densit y is unit y, we conclude that largest eigen vector of C . It is not so easy to immediately see the dep endence of K on the relativ e distances between x 1 ;x 0 1 ;x 2 and x 0 2 .
 that C = cI for some c 2 R and use the iden tity cI + vv &gt; = c n 1 c + k v k 2 to write where Q c ( A;B ) is the anit y matrices, as we can see from the fact that it is the overlap integral (Bhattac haryy a kernel) between two zero-cen tered Gaussian distributions with inverse covariances cI +2 A and cI + 2 B , resp ectiv ely [3]. Figure 1: The rst two panes sho w the separation of '3's and '8's in the training and testing its rst two eigen vectors according to the learned kernel k ). The righ t hand pane sho ws a similar KernelPCA plot but based on a xed RBF kernel. We conducted preliminary exp erimen ts with the hyperk ernels in relation learning between pairs of datap oints. The idea here is that the learned kernel k naturally induces a distance metric d ( x;x 0 ) = the data in suc h a way that data points with the same lab el are close to eac h other, while those with di eren t lab els are far apart.
 similar to the hinge loss is Q ( X;Y;k ) = 1 m 2 problem learns k ( x;x 0 ) = sub ject to the classi cation constrain ts x are of the same class and k ( x;x 0 ) 0 to mean that they are of di eren t classes. As an illustrativ e example we learned a kernel (and hence, a metric) between a subset of the NIST handwritten digits 1 . The training data consisted of 20 '3's and 20 '8's randomly rotated by 45 degrees to mak e the problem sligh tly harder. Figure 1 sho ws that a kernel learned by the above strategy with a Gaussian hyperk ernel with parameters set by cross validation is extremely good at separating the two classes in training as well as testing. In comparison, in a similar plot for a xed RBF kernel the '3's and '8's are totally intermixed. Interpreting this as an information retriev al problem, we can imagine in ating a ball around eac h data point in the test set and asking how man y other data points in this ball are of the same class. The corresp onding area under the curv e (AUC) in the original space is just 0.5575, while in the hyperk ernel space it is 0.7341. Figure 2: Test area under the curv e (AUC) for Oliv etti face recognition under varying and h .
 We ran a similar exp erimen t but with multiple classes on the Oliv etti faces dataset, whic h consists of 92 112 pixel normalized gra y-scale images of 30 individuals in 10 di eren t poses. Here we also exp erimen ted with dropping the ij 0 constrain ts, whic h breaks the positiv e de niteness of k , but migh t still give a reasonable similarit y measure. The rst case we call \conic hyperk ernels", whereas the second are just \linear hyperk ernels". Both feature vector [ x i ;x j ] and using a Gaussian RBF between these concatenations. The results on the Oliv etti dataset are summarized in Figure 2. We trained the system with a kernel that predicted the lab eling matrix. When speed becomes an issue it often suces to work with a subsample of the binary entries in the m m lab el matrix and thus avoid having m 2 constrain ts. Also, we only need to consider half the entries due to symmetry . Using the learned kernel, we then test on 100 unseen faces and predict all their pairwise kernel evaluations, in other words, 10 4 predicted pair-wise lab elings. Test error rates are averaged over 10 folds of the data. For both the baseline Gaussian RBF and the Gaussian hyperk ernels we varied the parameter from 0 : 1 to 0 : 6. For the Gaussian hyperk ernel we also varied h from 0 to 10 . We used a value of C = 10 for all exp erimen ts and for all algorithms. The value of C had very little e ect on the testing accuracy .
 Using a conic hyperk ernel com bination did best in lab eling new faces. The adv antage over SVMs is dramatic. The supp ort vector mac hine can only achiev e an AUC of less than 0 : 75 while the Gaussian hyperk ernel metho ds achiev e an AUC of almost 0 : 9 with only T = 20 training examples. While the di erence between the conic and linear hyperk ernel metho ds is harder to see, across all settings of and h , the conic com bination outp erformed the linear com bination over 92% of the time. The conic hyperk ernel com bination is also the only metho d of the three that guaran tees a true Mercer kernel as an output whic h can then be con verted into a valid metric. The average run time for the three metho ds was implemen ted quadratic programming using the MOSEK optimization pac kage on a single CPU workstation. The main barrier to hyperk ernels becoming more popular is their high computational de-ing). In certain metric learning and on-line settings however this need not be forbidding, and is comp ensated for by the elegance and generalit y of the framew ork.
 intuitiv ely app ealing interpretations. In the case of the Gaussian hyperk ernel we even have a natural regularization scheme. Preliminary exp erimen ts sho w that these new hyperk ernels can capture the inheren t structure of some input spaces. We hop e that their introduction will give a boost to the whole hyperk ernels eld.
 Ackno wledgemen ts The authors wish to thank Zoubin Ghahramani, Alex Smola and Cheng Soon Ong for discussions related to this work. This work was supp orted in part by National Science Foundation gran ts IIS-0347499, CCR-0312690 and IIS-0093302.

