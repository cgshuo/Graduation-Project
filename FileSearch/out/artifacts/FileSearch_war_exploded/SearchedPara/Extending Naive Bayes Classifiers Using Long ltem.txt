 Itemsets provide local descriptions of the data. This work proposes to use itemsets as basic means for classification purposes is introduced, i.e., how many times an itemset occurs when a specific class Ci is present. Class supports of frequent itemsets are computed in the training phase. Upon arrival of a new case to be classified, some of the generated itemsets are selected and their class supports SUpi are used to compute the probability that the probability. We show that selecting and combining many and long itemsets providing new evidence (interesting) is an effective strategy for computing the class probabilities. The proposed classification technique is called Large Bayes as it happens to reduce to Naive Bayes classifier when all itemsets selected are of size one only. Experimental results on a large number of benchmark data sets show that Large Bayes consistently outperforms the widely used Ndive Bayes classifier. In many cases, Large Bayes is also superior to other state of the art classification methods such as C4.5, CBA (a recently proposed rule classifier built from association rules), and TAN (a Bayesian network extension of Ndive Bayes). Classification, Association Mining, Bayesian learning, Lazy Learning. Condensing data to make it more manageable and comprehensible has been recognized as a key topic in data mining [GFC98,MT96]. One popular way to condense data is to employ any of the successful itemset generation algorithms [AS94]. An itemset with its support provides information on the frequency of data. Since training a classifier can be thought of as finding a condensed model of the data [W94], the question arises whether itemsets can also be used for classification purposes. This paper KDD-99 San Diego CA USA provides a positive answer by showing: (i) how itemsets can be resulting classifier leads to superior accuracy in many cases. In the training phase, an Apriori-like frequent pattern mining algorithm is employed to discover frequent itemsets of arbitrary size together with their class supports. The class support is a variation of the usual support notion and estimates the probability that the pattern occurs under a certain class label. Upon arrival of a new focussing on the context of this particular case. This approach is in between lazy learning (no work done during training) and eager learning (full construction of a global model while training). The actual classification model for a new case with attribute values A={aaba2,..., aJ consists of a formula computing the conditional probability P(cilA), i.e. the probability that the case highest such conditional probability. As discussed in [L59], the probability P(c;lA) can be estimated using different product approximations. Each product approximation implies different independence assumptions about the attributes. For example, P(a,azalci)P(apsla,ci) and P(ala~~;)P(asla2ci)P(a41alasci) are both product approximations of P(a,azag@sci). We show that selecting a certain product approximation is equivalent to selecting certain itemsets generated while training. The selection strategy for the itemsets uses four underlying principles: (i) iternsets should be as many itemsets as possible should be used, i.e. the product approximations should contain as many factors as possible; (iii) the and (iv) the itemsets chosen should be interesting in the sense that they indeed provide more information than their subsets. Consequently, the proposed classifier is called Large Bayes (LB). This name seems justified as in the extreme case, when the selected itemsets are all of size one, Large Bayes collapses to Ndive Bayes. Already Naive Bayes (NB) is a surprisingly successful classification method that has outperformed much more complicated methods in maw application domains [FGG97,DP97]. NB assigns new cases, represented as a vector of attribute-values A={al,aa.. ., aJ, to the class ci with the highest conditional probability P(cilA). From the definition of conditional probability follows P(ci!A)=P(A,cil/P(A). Since the denominator is be in class ci with the highest value P(A,ci)=P(Alci)P(Ci). TO compute P(A,c/, NB makes the assumption that all attributes are conditionally independent given the class ci, hence:  X  We use the notation P(a,.. .ancJ and P(A,ci) interchangeably. Eq. 1 P(a ,...a,c;)= P(,,)P(a, I c,)-P(a, I c,)=  X =I Essentially, this means that the classification solely depends on the values of P(aj,cJ and P(cJ LB can also be seen as a powerful extension of NB that uses itemsets of arbitrary size when estimating P(u,...a+$ This implicitly relaxes the strong independence assumptions implied by NB but retains its strengths: superior performance in many cases over state of the art classification methods, and the important ability to handle missing attribute values. Our approach fundamentally differs from existing classification methods in the following aspects: 
LB fits in between the eager and lazy learning paradigms [A97]. Eager methods, like Decision Trees [Q93] and Bayesian 
Networks [P91], infer a global model of the data during training. Classification requests are then answered by querying the learned model. Lazy learners such as nearest-neighbor classifiers, defer processing of the data until classification time, and classify cases by directly using the unprocessed data. In contrast to both, the learning phase of LB creates a condensed representation of the database in the form of itemsets with their class supports. Classification queries are then satisfied by constructing a query-specific local model using the evidence provided by the itemsets. 
We shed light on the relationship between descriptive (association) and classification mining. A well-known and successful method, Apriori [AS94], is modified to discover frequent and interesting patterns from data and we show that this condensed representation can be used for effective classification. Such an approach is particularly useful in domains where greedy search methods (such as decision tree induction algorithms) are unable to take into account all aspects of the underlying model. 
In contrast to probabilistic inference methods, LB bypasses the conditional independence testing methodology as investigated in Bayesian Network literature [P91]. Most real data sets contain only a small fraction of the possible attribute-value pairs. Instead of trying to accurately estimate the probability of any possible set of attribute-value pairs (also called items) we rather focus on estimating the probabilities of those sets of items that occur frequently in the data. This leads to accurate and stable results. The rest of the paper is structured as follows. Section 2 reviews related work and motivates our approach. Section 3 provides an overview and the intuition behind LB. Section 4 discusses the algorithm in detail while pinpointing various fine-tuning opportunities that can lead to even further improved classification accuracy. Section 5 presents an extensive experimental evaluation of LB on popular benchmark datasets and compares its performance with C4.5, the standard decision-tree classifier, Ndive Bayes, a restricted Bayesian network extension called TAN, and CBA, a recently proposed rule based classification method based on association rule mining. Section 6 draws conclusions and highlights future work. The surprising success of NB has triggered the development of several extensions, most of which aim at relaxing its strong independence assumptions. Tree Augmented Naive Bayesian classifier (TAN) [FGG97] extends NB taking into account additional dependencies among non-class attributes. TAN employs a modified version of a method proposed by [CL681 to learn a restricted tree-structured Bayesian network [P91] considering only the most important correlations between pairs of attributes. Several other methods follow the same direction. KDB [S96] constructs a Bayesian classifier where each attribute may depend on at most k other attributes, where k is a given parameter. Selective Bayesian Classifier [LS94] preprocesses data using a form of feature subset selection to delete strongly inter-related (and therefore redundant) attributes. Semi-Naiire Bayesian Classifier [K91] iteratively joins pairs of attributes to relax the strongest independence assumptions. Following a different approach, Adjusted Probability NB [WP98] infers a weight for each class, which is applied to derive an adjusted probability estimation used for the classification. Finally, NBTree [K96] is a hybrid approach combining Naive Bayes and decision-trees. A decision tree partitions the instance space into regions and a separate NB classifies cases within each region. NBTree is shown to frequently outperform both NB and decision-trees in terms of classification accuracy. All these methods aim at relaxing the strong independence assumptions implied by NB. Variables x and y are said to be conditionally independent given variable z, denoted I(nlzly), if for all values of x,y and z: P(xly,z)=P(nlz), whenever P(y,z)&gt;O. Consider now the extreme case where the condition P(xly,z)=P(xlz) is satisfied by all possible instantiations of x,y and z except by a few instantiations only, which badly violate it. Clearly, x and y are conditionally dependent given z. Algorithms like the ones above would consider to store all elements of the joint probability distribution P(x,y,z) and use it during classification to obtain more accurate estimations. Such an approach is not only redundant but also error prone because there might not exist enough data to provide reliable probability estimations for each possible variable assignment. To alleviate this representational weakness, [B+96] introduces the notion of context-specific independence, which describes independence relations among variables that hold in certain contexts only. Similarly, Buyesian Multinets [H91] model relationships among variables within each class separately. Large Bayes goes even further. Only relationships among instantiations of all variables are considered. These relationships are expressed using itemsets. As illustrated before, this representation accounts for additional flexibility, precision and increased representational ability. Liu et al [LHM98] follow an approach similar to ours introducing a method for the integration of association and classification mining. They use association rules to generate a complete classifier. All association rules with only the target class in the heuristics to prune this large rule set, they simplify it to finally obtain a smaller classifier. It is shown that this classifier, called CBA, outperforms C4.5 [Q93] in many cases. Earlier work on using association rules for classification includes [B97] that discusses the problem of enhancing an association rule miner with additional pruning strategies to extract high-confidence (&gt;90%) classification rules from data sets. [AMS97] discusses the use of association rule mining for the discovery of models of the data that may not cover all classes or all examples. 
LB approximates long marginals (support of itemsets with many items) using itemsets of arbitrary length as long as they are frequent and, therefore, reliable. All attributes are assumed to be discrete. Continuous attributes are discretized into intervals using standard discretization algorithms [EI93]. The resulting intervals are considered as distinct attribute values. Each possible attribute-value pair is called an item. Thus, in a domain described by n attributes, each training example is represented by an itemset [AS941 {a ,,...,a,/ containing the items which are true, and is labeled with a class ci. Note that the labeled itemsets have size n class attribute) and this size is reduced by one for each missing attribute value. 
Let D denote the set of training examples. An itemset 1 has support s for class ci in D (denoted by LSUpi = Note that LSUpi is the observed probability P(Lcij. Similarly, Lsup is the observed probability of P(l). An itemset is calledfrequent if its support matches at least a given minimum support minsup. 
The learning phase of LB employs an extension of Apriori [AS941 to extract from D a set F of frequent and interesting itemsets. An itemset A is interesting in our context, if P(A,ci) cannot be accurately approximated by its direct subsets, i.e. subsets of A with one item missing. A quantitative measure of interestingness is presented in Section 4.1. No complete model of the domain is condensed representation of the database tailored for the purpose of classification. Classification of new cases A=/a,,...,a J is done by combining the evidence provided by the subsets of A that are present in F (see procedure classify, Section 4.2). Long itemsets (i.e. with many items) are obviously preferred for classification as they provide more information about the higher order interactions between attributes. This is realized by first selecting the border of F with solution. The following example illustrates this and will be used in the sequel as the running example. Suppose a request to classify A={a,,...,as/ arrives. In the learning phase the set of all interesting and frequent itemsets F has been computed. Figure 1 shows all subsets of A in F. The non frequent or uninteresting ones are marked gray and cannot be used as their support can either not be counted reliably (non frequent) or can be approximated using smaller itemsets (uninteresting). We first select the border of F with respect to (a,,...,as}. These itemsets, marked in bold in Figure 1, can be used to compute the desired 1, i.e. all single attributes, and TAN employs some itemsets of the second level, i.e. pairs of attributes. LB uses the itemsets of B to derive a product approximation of P(A,cJ for all classes ci. [L59] describes the concept of product approximation in the context of approximation of higher order probability distributions by their lower order components. We only consider the approximation of individual elements of a probability distribution. Following [L59], the product approximation of the probability of an n-itemset A for a class ci contains a sequence of at most n subsets of A such that each itemset contains at least one item not contained (or covered) in the previous itemsets. To derive the approximated value of P(A,cJ the itemsets are combined using the chain rule of probability while assuming that all necessary independence assumptions are true. 
The following are some valid product approximations of la1 ,...,as} using the border itemsets shown in Figure 1: 
Note that (ala2a3),(ala4a5},(a2as) (which contains the same itemsets as ii. but in a different order) is not a product approximation since all items of (a2as} are already covered by the first two itemsets. 
Clearly, more than one product approximation of a given itemset exists and not all border itemsets can be used in the solution. The product approximation of the query itemset A is created incrementally adding one itemset at a time until no more itemsets can be added. All items already included in the product approximation are said to be covered. An itemset 1 inserted in the solution should satisfy the following condition: Condition 1) II-covered I 2 1 
Condition 1 guarantees that the solution is indeed a product approximation. Selection among alternatives is done as follows. 
Itemset lk is selected instead of lj if the following conditions are satisfied in the given order of importance: Condition 2) Ilk-coveredl 5 Ilj-coveredl Condition 3) IlkI 2 ujl Condition 4) lk is more interesting than 4. 
Condition 2 assures that each itemset in the sequence contains the smallest number of not covered items. This is equivalent to maximizing the number of itemsets used. This bias essentially ensures that as many higher order interactions as possible are taken into account (minimizes the conditional independence assumptions). Condition 3 ensures that long itemsets are considered if there are alternatives with the same minimal number of uncovered items (implicitly again minimizing the independence assumptions). Finally, condition 4 gives priority to interesting itemsets. 
Figure 2: Incremental construction of a product approximation Figure 2 displays the incremental construction of the product approximation of P(al...asci), in our running example. Initially, the solution and the set of covered items are empty. While all itemsets satisfy condition 1, condition 2 is satisfied by {azas],{a3a4} and {a&amp; and all three have the same size. Therefore, the most interesting one is selected. Assume this is /azas). It is inserted in the solution and items az and a5 are covered now. Among the remaining itemsets, {a3as/ introduces the fewest non-covered items, namely a3 only. It is inserted in the solution and a3 is marked as covered. In the third step, itemsets {a3aJ and (alaza3} both introduce one non-covered item (a4 and (alaps] preferred over (ajar} in the fourth step. Itemset {a3aJ remains unused as all its items are already covered. As a result, P(a,. , . aJcJ is computed using the product approximation This solution is a local model describing only the relationships among the specific attribute values of the query. All implied independence assumptions are only true in the context defined by their corresponding variables. In other words, a2, a3 and a5 are instantiations of the variables v2, v, and v,-. Our local model assumes that P(a~lazasci)=P(a~lasci). This is much weaker than assuming that P(v31v2vsci)=P(v31v~C~) for all possible instantiations of v2, v, and v5; a global model would make this stronger assumption. In the sequel we describe the algorithm in detail and explain the factor selection strategy. Section 4.1 presents the training phase which consists of discovering the set F of all interesting (according to a strict definition) and frequent (satisfying a minimum support threshold) itemsets with their class supports. Section 4.2 shows how these itemsets and the class supports are used to classify new cases, while Section 4.3 studies statistical fine-tunings. Algorithm genltemsets (Figure 3) generates the interesting and frequent itemsets using a bottom-up approach based on Apriori [AS94]. The input of the algorithm is the database D and the output is the set of interesting and frequent itemsets F with their class counts. To facilitate the class counting, each itemset has an associated counter COUnti for each class ci. Dividing a class counter by the number of tuples IDI provides the class support, that is, LCOUntillDI=P(Z,ci). First, all I-itemsets l=/aj/ are included in F1 (aj is a non-class attribute). Then the class count LCoUnti for each class ci is determined by scanning the database D once (line 2). This assures that LB keeps at least as much information as Nai X  X e-Bayes does. In general, the set Fk contains the set of frequent and interesting itemsets of size k. genItemsets(D) input: the database D of training examples output: the set F of itemsets 1 and their class counts Z.counti 1. F, =( /ai/ ai is non class attribute} 2. determine l.counti for all 1~ F, and all classes i 3. for (k=2; Fk., f 0; k++) { 4. C, = genCandidates(F,.,) 5. for all tuples t E D { 6. C, = subsets(&amp;, t); 7. i = class of 1; 8. for all candidates 1~ C,( 10. 1 11. 1 12. Fk = selectF(C&amp; 13. } 14. return F = uk Fk and l.counti for all 1~ F and all I Procedure gencandidates generates the candidate itemsets C,, a superset of Fk.,. In lines 5-9, the database is scanned to calculate subsets of t that belong to C, are selected (line 6) and their This determines the class counts LCOUnti of all candidate itemsets 1. After the counting is completed, itemsets which are interesting (see below) and frequent are selected by procedure seZectF (line 12). Procedure gencandidates is an extension of the candidate generation procedure suggested in Apriori [AS94]. It generates k-l is frequent and interesting. Note that when calling genCandidates(Fk.,) for k&gt;2, all itemsets in Fk., satisfy the two requirements as this is ensured in line 12 by procedure selectF. Therefore, there is no need to explicitly check these two properties. When k=2, however, we explicitly check if the itemsets are frequent (since F1 contains both frequent and infrequent itemsets), but we assume that all I-itemsets are interesting. 
Procedure selectF selects from C, those itemsets that are frequent and interesting. An itemset is frequent, if its support is above the user defined threshold minsup, We define the interestingness of an itemset 1 in terms of the error when estimating P(l,cJ using subsets of 1. Let 1 be an itemset of size 111 and lj , lk be two (Ill-l)-itemsets obtained from 1 by omitting the j X * and k X * item respectively. We can use lj , lk to produce an estimate Pj,, (I, ci) of P(Z, ci) : measure of the accuracy of this estimate: I(1 I lj,l,) becomes zero if Pj3k (Z,ci) = P(l,ci) ; its value increases with the difference between the estimated an the actual can be greater or less than P(l,c,) , thus yielding positive or negative values for the logarithm. Taking the absolute value prevents a positive and a negative diversion from canceling each other. The interestingness of 1 is defined as the average over all its subsets li and lk with size M-1. Since there are 2 2 pairs of different lj and lk, we have: Eq. 4. 2 Cz(z I zj*Lk&gt; The proposed interestingness measure I(l) quantifies how accurately P(l,cJ can be approximated using any two subsets of size Ill-l. A high value of Z(1) means that 1 is interesting, since P(l,ci) cannot be accurately estimated using smaller itemsets. On safely ignore 1 since it does not provide much additional information. Our interestingness measure is a variation of cross-entropy I,.,* CL.591, a measure used in probability theory to estimate the distance between two probability distributions P and P X : Eq. 5. I p-p = c P(x)log+g). The difference in our case is that we are estimate the difference between specific elements of P and P X  rather that between the whole distributions. Once the set F of interesting and frequent itemsets is available, new unlabeled cases A are classified by procedure classify, in 
Figure 4. The class supports P(l,c~)=l.counti/lDI computed during the learning phase are used. As described earlier, the goal is to incrementally build the product approximation of A by adding one itemset at a time until no more F. Procedure pi&amp;Next (Figure 5) then repeatedly inserts itemsets from the border in the solution marking their items as covered. 
The algorithm stops once all items in A have been covered. classifi(F,A) 
Input: The set F of discovered itemsets a new instance A output: the classification ci of A 1. cov = 0 \\ the subset of A already covered 2. nom= 0 \\ set of itemsets in nominator 3. den=0 \\ set of itemsets in denominator 4. B = &amp;F I 1 _cA and 41 X  X F: 1 X  _cA and 1~1 X ). 5. for (k=l; COVC A; k++){ 6. lk = pickNext( COY, B ) 7. nom = nomu [ld 8. den = denu {I, n COV} 9. cov = covu lk 10. ) 11. output that class ci with maximal P(A,cJ computed as: Each itemset lk selected in line 6 contributes one factor to the product approximation, namely, the conditional probability of the non-covered items of lk given the covered ones and the class: Eq. 6 P(I, -cov I I, n cov, c, ) = POk 9 ci 1 In line 7 and 8 the nominator and denominator itemsets of Eq. 6, II, and 1, n cov respectively, are stored in two separate sets nom and den which are used in line 11 to derive the value of the product approximation of P(A,ci) for each class cti The most likely pickNext( cov, B ) a) I&amp;coveredl &lt; Ilj-coveredl, or b) Ilk-coveredl = I&amp;coveredl and Ukl &gt; l/J , or c) Ilk-covered1 = lb-coveredl and IlkI = llj and U(l,) &gt; I(lj) class is then returned. Procedure pi&amp;Next selects from B the next itemset of the product approximation. This is uniquely determined from the selection conditions 1-4 of Section 3. In the rare case where more than one itemsets qualify one is randomly chosen. The set of conditional probabilities used in line 11 of classify can to incorporate a small-sample correction into the observed probabilities [N87]. This is called smoothing in [FGG97] and helps eliminating unreliable estimates and zero-counts. Let P(X,yy,ci) and P(y,cJ be the observed frequencies in D of {X,y,Ci} and {y,cJ respectively (x and y are itemsets). Instead of P(XIy,ci)=P(X,y,ci)/P(y,ci) we use the smoothed conditional probability: Eq. 7. where n,, is a small smoothing factor indicating the confidence in the observed probabilities. This estimate is based on the assumption that the conditional probabilities Pfxly) are usually close to the marginal probabilities P(X). Intuitively, it states that the fewer examples support the observed conditional probability P(rly,Ci), the closer the actual conditional probability is to P(X). In the special case where both P(x,y,cJ and P(y,CJ are zero, Eq.7 We use 21 data sets from UC1 ML Repository [MM961 to evaluate the performance and the behavior of Large Bayes (LB) algorithm. We compare LB with Ndive Bayes [DH73] and three other state-of-the-art classifiers: the widely known and used decision tree classifier C4.5 [Q93]; TAN, a state of the art extension of NB shown to outperform many Bayesian Network approaches and also NB [FGG97]; and CBA, a recently proposed classifier similar to ours in the sense that it employs a frequent pattern miner to discover association rules [LHM98]. In all experiments, accuracy is measured using the holdout testing set), and IO-fold cross-validation (CV-10) for the small data sets. For all classification methods, the same train/test set split and the same partitioning for cross-validation is employed. Since all methods, except C4.5, deal with discrete attributes, all attributes are discretized using entropy discretization [FI93] as implemented in the MLC++ system [K+94]. No discretization was applied for C4.5. Table 1 provides detailed information on the data sets used and summarizes the accuracies achieved by the four algorithms on the 21 data sets. Keeping in mind that no classification method can outperform all others in all possible domains [W94], we can draw several conclusions from Table 1. Firstly, it is clear that TAN and LB are in general ahead of the others in terms of prediction accuracy. This is also supported by the fact that they win in 6 (TAN) and 8 (LB) cases respectively. On the other hand, if the domain concept can naturally be expressed as a set of if-then rules (e.g. data sets Chess, Voting Records), then rule-based classifiers such as C4.5 and CBA are still producing better results. TAN and LB are comparable in terms of accuracy, LB, however, has the additional advantage of handling missing values. For TAN and CBA the parameters are set to the standard values as suggested in the literature. For example, CBA is used with minimum support 1% and minimum confidence 50% plus the pruning option. Since TAN does not handle missing attribute 0.8511 0.8499 0.8499 0.8496 0.8496 values, its accuracy for data sets containing missing attribute values can not be reported. Its smoothing parameter is set to 5. 
For LB, we fix the minimum support threshold to 1% or 5 occurrences (needed for the very small data sets), whichever is larger. The interestingness threshold z, was set to 0.04 and the smoothing parameter n,, is set to 5. This is further discussed and justified in the sequel. Varying the minimum support threshold helps somewhat pruning the number of itemsets generated. But as classification data sets often contain a huge number of associations, minsup is not very effective in preventing the combinatorial explosion in the number of generated itemsets, otherwise many useful ones are omitted. 
Indeed, the prediction accuracy of LB is very stable with respect to the minimum support threshold. In most datasets we observed that the accuracy slowly decreases with increasing minimum support, As far as the minimum support is low enough to enable the presence of important local patterns, the interestingness threshold is the key factor influencing the number of itemsets 110 89 76 68 63 60 50 generated and the classification accuracy. Tables 2 and 3 provide more insights into the behavior of LB. 
Table 2 shows the accuracy and Table 3 the number of itemsets used by LB for values of the interestingness threshold q varying from 0.02 to 0.06. The last column of Table 3 also provides the number of itemsets used by NB; this is the minimum number of itemsets LB can possibly use. It is apparent that LB is relatively stable with respect to 7,. Nevertheless, its performance can be boosted even further if the specific aspects of each data set are taken into account. Figure 6 illustrates the effect of varying the interestingness the number of itemsets used by LB as a function of r, for the two most extreme cases, Lymph and Adult data set. Varying r, has clearly the opposite effect for these two data sets. The increased number of itemsets produced by lowering r,, causes a 7.5% reduction of the error rate in Adult (from 15.05% to 13.92%), whereas in Lymph the error rate increases by almost 47% (from 12.81% to 18.86%). The reason is the size of the data sets. Adult is the largest and Lymph the smallest data set. five out of the seven largest data sets used in our experiments, whereas in 12 out of the 14 smallest data sets the error rate increases. This is expected since lower values of r, allow more and longer itemsets to be discovered, which tend to overlitting of the training data and unreliable estimates if the data set is small. Figure 7 further supports this claim. It shows the difference in the accuracy of LB when lowering z, from 0.06 to 0.02 as a function of the data set size. Figure 7 indicates that lowering q in order to discover a large number of itemsets is beneficial only if the data set is large enough to provide reliable estimates. This is further supported by experimental results in [DP97] where NB is found to outperform more complex models (like C4.5) if the training sample size is small. However, the performance of NB cannot scale up if more data are available. In contrast to NB, LB can take advantage of the largest samples by 
Figure 6. Accuracy and the number of itemsets used by LB for adjusting z, and allowing more complex models to be built. In order to directly compare LB with the other methods (see Table 1) we simply fixed r1 to 0.04, the middle of the range of values we used in our experiments so as to cope with large and small data sets. However, making rI a function of the size of the training set would boost the accuracy of LB even more. For example, from Table 2 follows that taking q as 0.02 instead of 0.04 for the two largest data sets, would result in LB being also the best classification method for Adult and being second for Letter. 
The classification time of LB mainly depends on the number of discovered itemsets. As shown in Table 3, for most data sets a fairly small number of itemsets is generated and is sufficient to produce accurate classifications. Table 4 shows the CPU time in seconds required for training and testing of LB with 2,=0.02 and 2,=0.04 respectively. The experiments were carried out on a 400 MHz Pentium II 
Windows NT workstation. For the 14 small data sets, the average time over the 10 CV-folds is reported. It is apparent from Table 4 that LB is fast, though our implementation has not focused on optimizing speed. Our itemset mining routine, genltemsets, could for instance be extended by using the additional pruning strategies proposed in [B97] and classification speed can be further improved by indexing the set F of itemsets. 
A new classification algorithm, called Large Bayes (LB), has been introduced. In the training phase, LB employs an Apriori-like frequent pattern mining algorithm to discover frequent and interesting itemsets of arbitrary size together with their class supports. The class support is an estimate of the probability that case to classify, a local classification model is built on the fly depending on the evidence of the new case. In the extreme case where all the discovered itemsets are of size one only, LB reduces to Nafve Bayes. Large Bayes is especially suitable for large and complex data sets and is shown to consistently outperform Nai X  X e 
Bayes. Ndive Bayes (NB) is more accurate on only three of twenty one data sets; and NB X  X  accuracy is on average only 0.28% higher than LB X  X  in these three cases. LB is also shown to be very competitive compared with other state of the art classification while still being very efficient. Moreover, it has been shown that the accuracy of LB can be easily improved by exploring data specific properties such as the size of the data. The authors were partially supported by Sino Software Research 
Institute Project No SSRI97/98.EGO3 and HKUST DAG 98/99.EG06. We would like to thank Bing Liu and Yiming Ma for providing the CBA algorithm and helping us during the experiments. LA971 D.W.Aha, Lazy Learning, (Reprinted from Artificial [AS941 R. Agrawal, R. Srikant,  X  X ast algorithms for mining [AMS97]K.Ali, S. Manganaris, R. Srikant,  X  X artial Classification 
U3971 [B+96] [CL681 [DP97] [DH73] [FGG97] N.Friedman, D. Geiger, M. Goldszmidt,  X  X ayesian [FI93] U.M.Fayyad, K.B.Irani,  X  X ulti-Interval discretization of [GFC98] G.Graefe, U.Fayyad, S.Chaudhuri,  X  X n the Efficient W911 
K961 [ K+94] LK911 Wll J.Pearl, Probabilistic Reasoning in Intelligent Systems, [QW J.R.Quinlan, C4.5: Programs for Machine Learning, 
