 For non-nati ve speak ers writing in a foreign lan-guage, feedback from nati ve speak ers is indispens-able. While humans are lik ely to pro vide higher -quality feedback, a computer system can offer bet-ter availability and pri vacy. A system that can dis-tinguish non-native ( X ill-formed X ) English sentences from native ( X well-formed X ) ones would pro vide valuable assistance in impro ving their writing.
Classifying a sentence into discrete cate gories can be dif cult: a sentence that seems uent to one judge might not be good enough to another . An alternati ve is to rank sentences by their relati ve uenc y. This would be useful when a non-nati ve speak er is un-sure which one of several possible ways of writing a sentence is the best.
 We therefore formulate two tasks on this problem. The classication task gives one sentence to the sys-tem, and asks whether it is nati ve or non-nati ve. The ranking task submits sentences with the same in-tended meaning, and asks which one is best.
To tackle these tasks, hand-crafting formal rules would be daunting. Statistical methods, howe ver, require a lar ge corpus of non-nati ve writing sam-ples, which can be dif cult to compile. Since machine-translated (MT) sentences are readily avail-able in abundance, we wish to address the question of whether the y can substitute as training data.
The next section pro vides background on related research. Sections 3 and 4 describe our experiments, follo wed by conclusions and future directions. Pre vious research has paid little attention to rank-ing sentences by uenc y. As for classication, one line of research in MT evaluation is to evaluate the uenc y of an output sentence without its reference translations, such as in (Corston-Oli ver et al., 2001) and (Gamon et al., 2005). Our task here is simi-lar , but is applied on non-nati ve sentences, arguably more challenging than MT output.

Ev aluation of non-nati ve writing has encom-passed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can pro vide holistic scores that correlate well with those of human judges.

At the sentence level, which is the focus of this paper , pre vious work follo ws two trends. Some re-searchers explicitly focus on indi vidual classes of er-rors, e.g., mass vs count nouns in (Brock ett et al., 2006) and (Nag ata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in conte xt-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004).
Typically , howe ver, non-nati ve writing exhibits a wide variety of errors, in grammar , style and word collocations. In this research, we allo w unrestricted classes of errors 1 , and in this regard our goal is clos-est to that of (Tomokiyo and Jones, 2001). Ho w-ever, the y focus on non-nati ve speech, and assume the availability of non-nati ve training data. 3.1 Data Our data consists of pairs of English sentences, one nati ve and the other non-nati ve, with the same  X in-tended meaning X . In our MT data ( MT ), both sen-tences are translated, by machine or human, from the same sentence in a foreign language. In our non-nati ve data ( JLE ), the non-nati ve sentence has been edited by a nati ve speak er 2 . Table 1 gives some ex-amples, and Table 2 presents some statistics. MT (Multiple-T ranslation Chinese and Multiple-JLE (Japanese Learners of English Corpus) Tran-3.2 Machine Lear ning Framew ork SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classication task.

For the ranking task, we utilize the ranking mode of SVM-Light. In this mode, the SVM algorithm is adapted for learning ranking functions, origi-nally used for ranking web pages with respect to a query (Joachims, 2002). In our conte xt, given a set of English sentences with similar semantic content, say s the learning algorithm estimates the weights ~ w to satisfy the inequalities: where s a sentence to a feature vector . This is in contrast to standard SVMs, which learn a hyperplane boundary between nati ve and non-nati ve sentences from the inequalities: where y used in our experiments, and the regularization pa-rameter is tuned on the development sets. 3.3 Featur es The follo wing features are extracted from each sen-tence. The rst two are real numbers; the rest are indicator functions of the presence of the lexical and/or syntactic properties in question.
 Ent Entrop y 3 from a trigram language model Parse Parse score from Model 2 of the statisti-Deriv Parse tree deri vations, i.e., from each parent DtNoun Head word of a base noun phrase, and its native speak er, and by a mac hine translation system. Colloc An in-house dependenc y parser extracts 4.1 An Upper Bound To gauge the performance upper bound, we rst at-tempt to classify and rank the MT test data, which should be less challenging than non-nati ve data. Af-ter training the SVM on MT train , classication accurac y on MT test impro ves with the addition of each feature, culminating at 89.24% with all ve features. This result compares favorably with the state-of-the-art 5 . Ranking performance reaches 96.73% with all ve features.

We now turn our attention to non-nati ve test data, and contrast the performance on JLE test using models trained by MT data ( MT train ), and by non-nati ve data ( JLE train ). Table 3: Classication accur acy on JLE test . (-) indicates accur acy on non-native sentences, and (+) indicates accur acy on native sentences. The over all accur acy is their aver age. 4.2 Classication As sho wn in Table 3, classication accurac y on JLE test is higher with the JLE train set (66.4%) than with the lar ger MT train set (59.0%). The SVM trained on MT train consistently misclas-sies more nati ve sentences than non-nati ve ones. One reason might be that speech transcripts have a less formal style than written news sentences. Tran-scripts of even good con versational English do not always resemble sentences in the news domain. 4.3 Ranking In the ranking task, the relati ve performance be-tween MT and non-nati ve training data is reversed. As sho wn in Table 4, models trained on MT train yield higher ranking accurac y (76.2%) than those trained on JLE train (74.6%). This indicates that MT training data can generalize well enough to per -form better than a non-nati ve training corpus of size up to 10000.

The contrast between the classication and rank-ing results suggests that train/test data mismatch is less harmful for the latter task. Weights trained on the classication inequalities in (2) and on the rank-ing inequalities in (1) both try to separate nati ve and MT sentences maximally . The absolute boundary learned in (2) is inherently specic to the nature of the training sentences, as we have seen in x 4.2. In comparison, the relati ve scores learned from (1) have a better chance to carry over to other domains, as long as some gap still exists between the scores of the nati ve and non-nati ve sentences. We explored two tasks in sentence-le vel uenc y evaluation: ranking and classifying nati ve vs. non-nati ve sentences. In an SVM frame work, we exam-ined how well MT data can replace non-nati ve data in training.

For the classication task, training with MT data is less effecti ve than with non-nati ve data. Ho w-ever, for the ranking task, models trained on pub-licly available MT data generalize well, performing as well as those trained with a non-nati ve corpus of size 10000.

In the future, we would lik e to search for more salient features through a careful study of non-nati ve errors, using error -tagged corpora such as (Izumi et al., 2003). We also plan to explore techniques for combining lar ge MT training corpora and smaller non-nati ve training corpora. Our ultimate goal is to identify the errors in the non-nati ve sentences and propose corrections.

