 REGULAR PAPER Sheila Garfield  X  Stefan Wermter Abstract Our objective is spoken-language classification for helpdesk call rout-ing using a scanning understanding and intelligent-system techniques. In partic-ular, we examine simple recurrent networks, support-vector machines and finite-state transducers for their potential in this spoken-language-classification task and we describe an approach to classification of recorded operator-assistance tele-phone utterances. The main contribution of the paper is a comparison of a variety of techniques in the domain of call routing. Support-vector machines and trans-ducers are shown to have some potential for spoken-language classification, but the performance of the neural networks indicates that a simple recurrent network performs best for helpdesk call routing.
 Keywords Classification  X  Finite-state automata  X  Recurrent neural networks  X  Spontaneous language  X  Support-vector machines 1 Introduction Since the late 1980s and the early 1990s, research in the domain of spoken on existing speech technology, topic-spotting techniques, and machine-learning approaches [ 8 , 12 , 15 , 35 , 41 , 57 ]. This is because the task of call routing can be seen to be similar to that of topic identification [ 22 , 35 ] and document routing [ 27 ] because the goal is to identify which one of n topics or, in this case, destinations, most closely match the caller X  X  request. In the context of this article, we are focus-ing on helpdesk scenarios. Processing spontaneous spoken language in helpdesk scenarios is important for automatic telephone interactions for telecommunication companies or banks, etc., to increase efficiency. Consequently, there is a need to develop systems that can handle spoken-language processing robustly with respect to irregularities in the input.
 ysis [ 51 , 52 , 54 ] in a robust manner, and this is our motivation to explore neural networks. In particular, in this paper, we want to explore networks based on simple recurrent networks (SRN) [ 16 ]. These recurrent networks promise to be particu-larly useful because they can encode sequences of arbitrary length. In this paper, simple recurrent networks are developed and compared in order to provide perfor-mance indicators in the context of a larger hybrid system for helpdesk automation. Furthermore, we explore alternative techniques based on transducers and support-vector machines because these techniques can be seen as two very well-known and successful representatives of symbolic and statistical techniques for classification. 2 The helpdesk corpus 2.1 Objective The objective of this work is to examine alternative techniques for use in helpdesk automation and call routing using recorded natural telephone requests. In this work, textual transcriptions of recorded spoken language input from callers is used. The task is to classify the first utterance of the caller into one of a num-ber of predefined call classes or categories. Consider the following example of an actual caller utterance: would be referred to the customer-care service. Although the caller has not stated explicitly which service is required, the operator determines what he believes is the caller X  X  intention from what the caller has said. This allows the speaker to ut-ter sentences appropriate to an infinite number of situations [ 7 ]. As a result, the problem of automatically classifying fluent speech is difficult for a machine. In order to do this, it is necessary to design a system that processes the caller X  X  utterance to be able to determine the appropriate action. Usual approaches are to use formal grammars, for example, context-free grammars, but this has the disad-vantage of constraining the system to a very limited robustness [ 23 ]. This limits the language a caller can use to communicate with the machine but alleviates the problems of complexity in recognising human language. However, this does limit the scope of the machine and becomes an obstacle to developing more powerful, user friendly and flexible systems.
 is to develop a system that is able to function in response to what is said by the caller and not what we would like the caller to say [ 24 ]. A flat, learned-language representation can support the types of problem associated with the processing of spoken language and is more robust because it has only minimal expectations about a sequential structure [ 52 ]. Neural networks, support-vector machines and inducted transducers are good candidates for robust classification based on their learning and generalisation capabilities because the learning capabilities allow regularities to be induced from the training examples presented. 2.2 Description of the helpdesk corpus For our task of call routing, our telecommunications collaborator provided a corpus of textual transcriptions of the spoken word chain of recorded operator-assistance telephone calls [ 14 ]. The calls were recorded in different phases at all times of the day and divided into nine separate call sets. Four call sets were randomly selected for this task. Each call set contains 1,000 utterances with calls from each of the phases. The corpus provides a representative selection of call traffic to the operator-assistance service. The utterances range from simple direct requests for services to more descriptive narrative requests for help as shown by the examples below: 1.  X  Could I um the er international directory enquiries please?  X  2.  X  Can I have a early morning call please?  X  3.  X  Could you possibly give me er a ring back please I just moved my phone I 4.  X  Well, hello I wonder if you could help me er my son is in the Isle of Wight and matically interpreting spoken language. The utterances are interspersed with filled pauses such as er and um , which add little to the semantic content of the utterance. In the first example, the caller does not state directly whether he wants the num-ber for, or wants to be connected to, the international directory enquiries service. The operator has to deduce what the intention of the caller is X  X hat he wants to be connected to this service, before redirecting the call. In the last example, example 4, the request for help is preceded by a lengthy description of the problem situa-tion. The operator has to listen to the problem description and then propose some solution to the problem. In the above examples, the length of the utterances ranges from eight words in the shortest utterance to 82 words in the longest utterance; although exceptional utterances in excess of 100 words have been recorded. The average length of an utterance is 25.76 words. 2.3 Call transcription The corpus only contains transcriptions of the first utterances of callers to the operator service stored in individual call sets. These transcriptions are a repre-sentation, in text, of the actual spoken word chain of the caller to the operator, including any hesitations, filled pauses or corrections, for example. This assumes 100% correct word recognition and 100% correct segmentation. The call tran-scription process is carried out by the telecommunications collaborator [ 14 ]. The calls are transcribed using turn-based annotation; if the first two conversational turns demonstrate greeting behaviour, further turns are annotated until the caller has explained his or her problem to the operator. The focus of our investigation was the first utterance of the caller. The first utterance is the spoken word chain uttered by the caller in response to the operator greeting that explains his or her problem. This utterance was transcribed unless it was a greeting or phrase such as  X  Yes er I X  X  wondering if you could help me?  X  X r X  Is that the operator?  X . In this case, the second utterance of the caller was also transcribed. Therefore, in the case of example 3 in Table 1 , the second utterance of the caller would also be transcribed beginning at  X  X  wonder if er you can tell me ...  X .
 class ,a primary move and a request type , as indicated by the example in Table 2 . greeting. The call class is related to the service to which the caller request is routed. The number of call classes in the task is 17 and a breakdown of the spread of each call class within each call set is provided in Fig. 1 [ 14 ]. in boundary markers, @@, and assigned a primary-move type. The primary-move type is associated with the identified intention of the caller. The boundary markers @@ are not presented to the classifier. The markers are used during pre-processing to tag the transcription with the labels (CALL , )CALL , (CLASS , and )CLASS . As the task is classifying a caller utterance to a call class, the primary-move type is not used in the experimental work reported here.
 with each primary-move type. As a result, it is unlikely that certain service requests would be classified to a particular call class. Consequently, if we were to classify initially based on the primary-move type, this could reduce the number of call classes available for selection. This could perhaps provide some good indicators for the future about either the types of dialogue or speech acts used by the callers or the vocabulary contained within the utterances to aid the classification process. The call class and the primary-move type differ in their focus. The call class is more concrete and is associated directly with the service the caller requires. The primary move is more abstract and reflects what the caller is trying to achieve or what the intention of the caller is believed to be. the billing service and the primary move is connect because the caller has indicated a requirement for a connection to another service. The request type identifies the style of language used by the caller to talk to the operator. Is the request for service expressed in explicit terms or is it an implicit request expressed by the use of free language [ 14 ]. The above utterance is an example of the latter.
 dergo some preprocessing. The individual segments of each utterance are manu-ally tagged, as shown in the example tagged utterance, Table 3 ; call class becomes CLASS, primary move becomes MOVE and request type becomes TYPE.
 the training and test sequences. The tagged utterances in each call set are processed and a lexicon in the form of a vector representation, which includes the frequency of occurrence of each word in a particular call class, is generated, as illustrated in Ta b l e 4 . The labels are used to identify which words are added to the lexicon, that is, only those words that occur between the (CALL and )CALL labels. The labels are also used to identify with which call class the words are associated between the (CLASS and )CLASS labels. All text outside of these labels is ignored. The labels appended to each utterance are not used in the input to the classifiers. The labels are only used in the processing of the call set for the purpose of generating the lexicon and the training and test-vector representations that are used to create the inputs for each of the classifier components. 2.4 Calls and semantic vectors In our experiments, we use a semantic vector representation of the words in a lexicon [ 51 ]. These vectors represent the frequency of a particular word occurring in a call class and are independent of the number of examples observed in each call class. The number of calls in a call class can vary substantially. Therefore, we normalize the frequency of a word w in call class c i according to the number of calls in c i (2).
 normalized frequency of occurrences of word w in semantic call class c i ,divided by the normalized frequency of occurrences of word w in all call classes. That is, where Each call class is represented in the semantic vector. An illustrative example is given in Table 4 . As can be seen in the illustrative example, domain-independent words like  X  X an X  and  X  X o X  have fairly even distributions. which is to be expected as these are function words, while domain-dependent words like  X  X heck X  and  X  X rder X  have more specific preferences. 2.5 Training and test call sets Four separate call sets, each containing 1,000 utterances, were used in this study. The four call sets were used individually as a baseline to establish how each of the classifiers performed with a view to scaling up the investigation using the call sets in a cross-validated approach and selecting the best classifiers. Testing each classifier on only one call set may not have provided a true indication of performance, whereas testing on four call sets allows for variation in the call sets. of utterances used for testing. Only the part of the utterance identified as indicating the primary move was used for training and testing. As part of the preprocessing stage, this part of the utterance was assigned a classification using the call class associated with the full transcription of the utterance. At this stage, some utter-ances were excluded from the training and test sets because they did not contain a primary-move utterance, which effectively means that utterances that contained phrases like  X  oh sorry wrong number  X  were grouped into an other call class. A breakdown of the number of training and test examples per call set is provided in Fig. 2 . The average length of an utterance in the training set is 16.05 words, and in the test set, the average length of an utterance is 15.52 words. An illustrative example is given in Table 5 ; however, not all call classes are shown.
 selecting one or more items from a set of items. Thus, entropy is a measure of how many bits of information we are trying to extract in a task and it gives an indication of how certain or uncertain we are about the outcome of the selection [ 10 ]. In addition, the concept of entropy provides an indication of problem diffi-culty and both high and low entropy values can indicate difficult problems. A low entropy value indicates diversity; an uneven class distribution with the classifiers being unable to learn the small classes well. A high value indicates more similar-ity and therefore greater difficulty in differentiating between items. To normalize the entropy value of a class to the range [0, 1], the base of the logarithm is the number of utterances. In this task, the four call sets under investigation each have an entropy of 0.31.

Entropy = X  2.6 Input representations of each technique The focus of this work is the classification of a caller X  X  first utterance to one of a predefined number of call classes. To make any comparison as fair as possible, the same corpus and basic underlying representation was used as the basis for the input representations for each of the techniques. This means that the initial lex-icon values generated from processing the corpus were converted into a feature-representation format applicable to each technique. Although the techniques use different feature representations, no changes were made to the lexicon values; therefore, each representation was derived from the same basic representation. Consequently, each technique is starting with the same underlying information from the corpus. An example of the respective representations used by each tech-nique, for the same example, is provided in Tables 6 X 8.
 number, a category output representation and a word input representation, as il-lustrated in Table 6 . The category-output representation represents the call class associated with the utterance and the word-input representation represents the fre-quency of occurrence of each word.
 colon, as shown in Table 7 . For example, 1:0.16, the feature, in this, case 1, is the position of the word in the utterance, and the value, 0.16, is the frequency of occurrence of that word in any position for the particular call class under investi-gation.
 sions, each utterance in the call set is symbolically tagged. These symbolic tags enable identification of possible patterns or strings and substrings in the utterance. This means that each word in the utterance is tagged with a symbolic representa-tion of the call class associated with that word based on the highest frequency of occurrence, as shown in Table 8 .
 call class based on the frequency of occurrence. These symbolic tags represent the sequence for a specific utterance. This clearly shows the higher frequency of the ln call class would indicate that this utterance is classified as an example of the ln call class. There is no removal of function words such as a, the and  X  X nd X , etc. because it has been indicated that they may serve important roles. 3 Architecture Figure 3 shows the general structure of our architecture, which combines different components. The preprocessing and generation of the lexicon were described in Sect. 2.3 and the call sets were described in Sect. 2.5 .
 put, that is, the training and test sequences for the classification component of our architecture. The aim of the experimentation was to identify effective techniques for use in the classification component of our architecture. 3.1 Experiments using neural networks, support vector machines and transducers We have considered using language models for this task but, then focused on identifying alternative techniques that may have potential for use in the task of classifying a caller utterance to a call class. A drawback to language models is that they require large amounts of transcribed and labelled training data from which to calculate the probabilities of the most likely word sequences. Additionally, lan-guage models are unable to model long-distance dependencies [ 47 ] because they comparison, our approach uses the continuous word sequence of the caller X  X  utter-ance from the corpus as supplied by the telecommunications collaborator rather than keywords or phrases. Our approach uses a shallow surface rather than a deep interpretation to produce a classification result because this may be all that is re-quired if the next stage were to be the handling of the call by an operator. The caller request is routed to a service and the operator extracts the details of the caller X  X  problem or enquiry. Details of the techniques used are provided in the subsequent sections.
 rics recall and precision [ 43 ]andthe F -score [ 48 ]. Recall and precision [ 43 ]are standard measures of information retrieval performance. Recall and precision can be calculated using the following equations: The F -score [ 48 ] is a combination of precision and recall and is a method for calculating a value without bias, in other words, without favouring either recall or precision, and is calculated using the recall and precision values, For each call set, at the end of both the training and the testing Phases, three values were obtained for each call class for the following categories: (1) the number of utterances retrieved that were relevant, (2) the number of utterances retrieved that were not relevant and (3) the number of relevant utterances not retrieved. For a call set, the values for each call class were added together to produce a total value for each of the three categories. These totals were used to calculate the results for recall and precision that are used to calculate the F -score. A small illustrative example calculation for recall is provided below.
 3.2 Neural networks Several neural-network architectures were used for the experiments, ranging from a feedforward network with only one input and one output layer to a simple re-current network with input, output, hidden and context layers. The feedforward network uses a system of error correction to adjust the network while it produces the required output. The actual output result is compared with the desired result and the weights are either strengthened or weakened by an amount proportional to the size of the error. This involves examining the sum of all the differences between the desired outputs and the actual outputs and reducing this error by a process of gradient descent [ 34 ]. A recurrent neural network extends a feedfor-ward network by introducing previous states and short-term incremental memory and can be either fully or partially recurrent. A partially recurrent network, such as a simple recurrent network, has recurrent connections between the hidden and context layers [ 18 ], whilst a Jordan network has connections between the output and context layers [ 31 ]. A key aspect of this network is that it allows the process-ing of sequences in a robust manner. 3.2.1 Training environment Supervised learning techniques were used for training [ 16 , 50 ], which requires presentation of both the required output and the input stimulus. The weights of an untrained network are initialised to small random numbers. In one epoch, or cycle, of training through all training samples, the network is presented with all utterances from the training set and the weights are adjusted at the end of each utterance. The training algorithm used compares the actual output to the desired output and calculates the difference so that the output produced moves closer to the desired output [ 34 ]. testing, utterances are presented sequentially to the network one word at a time. Each input unit receives the value of v(w, c i ) ,where c i denotes the particular call class of a word, w , with which the input is associated. Utterances are presented to the network as a sequence of word input and category output representations, one pair for each word. At the beginning of each new utterance, the context layers are cleared and initialised with 0 values. Each unit in the output layer corresponds to a particular call class, and the output unit that represents the desired call class is set to 1 and all other output units are set to 0.
 mance of the simple recurrent network (SRN) is reported because this architecture was more successful than either the feedforward or the multimulti-layer percep-tron (MLP) due to its additional memory. The results produced by the feedforward network suggested that this network did not have sufficient processing capacity. Likewise, even the MLP, with one hidden layer, did not have sufficient context-processing capacity to handle the sequential inputs. 3.2.2 Simple recurrent neural network The SRN [ 16 , 50 , 53 ], depicted in Fig. 4 , uses a set of context units that copy their activations (values) from the hidden units. The hidden units also receive input from the context units. The hidden units are updated first before the context units copy the hidden units X  values and provide input to the hidden units. Therefore, the context layer enables the output values of the network X  X  hidden units to be stored and then reused in the network by providing the hidden layer with the pattern of the previous activation state [ 17 , 55 ].
 works have been examined in [ 50 ]. An SRN with input, output, hidden and context layers was used for our experiments. Training was carried out using supervised-learning techniques [ 16 , 50 ]. In general, the input to a hidden layer, L n , is con-strained by the underlying layer, L n  X  1 , as well as the incremental context layer, C weighted activation of the units in the previous layer, L ( n  X  1 ) i ( t ) , and the units in the current context of this layer, C ni ( t ) , limited by the logistic function f . to perform sequential tasks over time, which means the output of the network not only depends on the input but also on the state of the network at the previous time step. Events from the past can be retained and used in current computations, that is, the output at time t is reused as the input at time t + 1. This allows the network to produce complex time-varying outputs in response to simple static input, which is important when generating complex behaviour. As a result, the addition of recurrent connections can improve the performance of a network and provide the facility for temporal processing [ 55 ].
 lation to the number of layers in the network, the strategy for training, that is to say, the learning rate (0.01, 0.006 and 0.001), momentum term (0.8) and number of epochs of training (1,000) remained constant. The initial learning rate of 0.01 changed at 600 epochs to 0.006 and again at 800 epochs to 0.001. The network was trained on the four call sets using this training strategy. The addition of a second hidden layer, a context layer, which was used to add sequential memory to the network, produced a significant improvement in network performance; see Fig. 5 . The recall rate after testing on the test set for call set one increased to 79%. This was a difference in improvement of over 51% on the previous best recall rate achieved by the feedforward network.
 was over 45% when compared with that of the MLP network. Overall, when tested, the difference in improvement in recall rate achieved by the SRN ranged from 49.5 to 57.9% when compared with the recall rate for the MLP network trained using the same strategy of 1,000 epochs and a changing learning rate. Then the SRN was trained again on the four call sets using a different learning strategy. In this strategy, the learning rate was fixed at 0.01, the number of epochs of training remained constant at 1,000, and the momentum term also remained unchanged at 0.8. The recall rate of 76.6% on the test set for call set one is slightly lower, by 2.4%, than the recall rate of the network trained with a changing learn-ing rate, whilst the precision rate is only lower by 0.8%. The performance of the network varied slightly across the call sets and the use of a fixed learning rate did produce some improvement in performance but only on two of the call sets compared with the network trained with a changing learning rate. Overall, there was a significant increase in performance when compared with the feedforward and the MLP networks trained with the same fixed learning rate. The F -score [ 48 ] performance of the trained SRN on the four call sets, which is a combination of the precision and recall rates, is shown in Table 9 . The recall and precision per-formance is shown in Fig. 5 . There is a difference of 3.5 and 5.1% between the highest and the lowest test recall and precision rates, respectively. In conclusion, simple recurrent networks performed substantially better than simple perceptrons or feedforward networks. 3.3 Support-vector machines Because support-vector machines have been shown to be good statistical Clas-sifiers, we wanted to compare our results with them. Neural networks use the empirical risk minimisation (ERM) principle, which minimises the error on the training data, while the support vector machine (SVM) developed by Vapnik [ 49 ] uses the structural risk minimisation (SRM) principle, which minimises an upper bound on the generalisation error [ 6 , 26 , 29 , 44 ]. An SVM is a binary classifier and the SVM approach divides the problem space into two regions via a dividing line or hyperplane , with a classifier referred to as the optimal separating hyperplane , Fig. 6 .
 maximum margin [ 9 , 19 , 39 , 40 ] between it and the nearest data point of each class. SVMs can deal with nonlinearly separable cases, i.e. where a straight line cannot be found, and can handle noisy data [ 40 ]. SVMs make use of kernels [ 38 , 39 ]; therefore, all computations are performed directly in input space and, depending on the kernel function used, for example, polynomial, radial basis function (RBF), and sigmoid, different types of classifiers can be constructed [ 38 , 45 , 46 ]. One potentially problematic area is the optimisation of the kernel parameters and choosing the right combination of parameters.
 3.3.1 Support-vector machine experiments In order to compare the performance of the SRN to other classification approaches, a series of experiments was conducted using an SVM with different kernel func-tions: polynomial, RBF, and sigmoid. The value for the tuning or trade-off pa-rameter C , the error penalty [ 28 , 30 ] was chosen based on empirical experience. This parameter allows a trade-off between the margin width and classification er-ror [ 28 , 30 ]. Several values for C were investigated in order to determine a value that reduced the test error to a minimum, while at the same time constructing a classifier that was able to perform successfully on the individual categories. The value for C used in the experiments was 64. Other parameters that could be set for each kernel and were the same across the kernels were kept constant, which means the classifiers were constructed using the same values.
 sets were generated from the same utterance sequences from the same four call sets. The training approach adopted was one-versus-all , that is to say, the training and test sequences used positive examples for the particular call class in question and negative examples of all the other call classes. The classifier in each case was trained and then tested on these sequences.
 in the testing phase. In their basic form, the decision function that determines whether a test example is classified can be described by a linear hyperplane, h ; attribute vector, x ; weight vector, w ; and a threshold, b , and takes the form of where the output is is shown in Table 10 . in Fig. 7 . These results are for the individual classifiers trained and tested using constant parameter values.
 performance figures for recall on each of the four call sets when compared with both the sigmoid and polynomial kernels. The classifier constructed with a poly-nomial kernel achieved greater test performance figures for precision on all four call sets than the other two kernels. Across the four call sets, there is a difference of some 5% between the highest and the lowest test recall and test precision rates for the RBF kernel classifier. For the sigmoid kernel classifier, the difference is some 6% for test recall and some 3% for test precision. Therefore, for both of these kernel types, the difference is smaller for test precision. In the case of the polynomial kernel classifier, the difference between the highest and the lowest rates is smaller for test recall than test precision; this is opposite to the results for the RBF and sigmoid kernel classifiers. 3.4 Finite-state automata and transducers Finite-state automata and transducers or finite-state machines could prove useful in this task for several reasons. First, finite-state machines are well understood and clearly defined and can be small in size. Second, their representation and their output is easy both to understand and interpret: their representation is symbolic in nature and they use as their basic unit a string. Therefore, it is relatively easy to input data. Finally, finite-state machines can handle strings of any length. This is of particular significance in this task because the length of the caller X  X  utterance is variable.
 represent transitions, one start state, any number of final states and an alphabet [ 32 , 42 ]. A finite-state machine operates on strings. A string is a sequence of symbols composed from an alphabet, and an alphabet is a set of symbols or characters. Computation in a finite-state machine begins at the start state with an input string and the transitions change from one state to other states. Finite-state machines can either be simple automata that encode regular languages or transducers that represent a regular relation that is a set of pairs of strings, as illustrated in Fig. 8 . or generate pairs of strings. As a result, one view of a transducer is a machine that reads one string and generates another. For example, the transducer in Fig. 8 encodes the relation denoted by the regular expression that is, the transducer would recognise sequences such as (aaabbbc) and generate a d as output. 3.4.1 Construction of transducers from regular expressions Finite-state machines can be specified using regular expressions. A regular expres-sion specifies how patterns or strings and substrings can be created. The strings and substrings are created by combining symbols, in other words, characters or words that have been extracted from a restricted alphabet or vocabulary. The order in which symbols in the strings and substrings can be joined together is indicated by the regular expression. The expression also indicates whether, for each position, there are other possible alternatives for symbols and whether or not it is possible to repeat these strings and substrings. In order to create the regular expressions, the utterances in the call set are symbolically tagged to enable identification of possi-ble patterns or strings and substrings in the utterances, as illustrated in Sect. 2.6 . 3.4.2 Transducer experiments To enable a comparison between transducers and the other classification tech-niques outlined, a series of experiments was conducted. The same four call sets of utterances were used as in previous experiments and transducers were con-structed for each call class. Each utterance in the training sets was converted into a symbolic representation of the call class associated with each word based on the highest frequency of occurrence. Then transducers were constructed for each call class from regular expressions, which were hand crafted. The converted train-ing sets were used as reference sets whilst creating the regular expressions based on sequences of call classes identified in the symbolic representation of the utter-ances.
 the other call classes. The transducer produces an output class for each sequence. A successful output is the semantic label of the call class for which the trans-ducer has been constructed while an unsuccessful output is 0. For the call class for which the transducer was constructed, the number of successful outputs gener-ated was counted as utterances that were retrieved and relevant and the number of unsuccessful outputs was counted as relevant utterances not retrieved. Successful outputs on call classes that were not the call class for which the transducer was constructed were counted as utterances retrieved that were not relevant. Using these values, the recall, precision,] and F -score results were calculated for each call set; see Sect. 3.1 .Thetest F -score results for each of the four call sets are provided in Table 11 . The test recall and precision results for each of the four call sets are shown in Fig. 9 .
 the transducer achieved 100% performance on its own individual sequences and the error rate on the other sets was low. This performance is useful if the trans-ducers are to be used as a means of exception handling or identifying those se-quences that a recurrent neural network cannot classify. On the other hand, for one or two call classes, the regular expressions identified were more general be-cause the transducers also generated output for the other call classes. Based on the performance of these transducers, it has been demonstrated that it can be difficult to create a specific regular expression for some call classes because the semantic content of the utterances in some call classes does at times closely represent that of other call classes, as in the examples shown in Table 12 ; and therefore, it is problematic to correctly identify the appropriate call class. Given this, for these call classes, the recurrent neural network has proven more successful as it is better able to handle more difficult semantic sequences due to its generalisation ability. 3.5 Classifier processing time The processing time for the SRN and the SVM with respect to training varies; FSTs do not require training, but creating regular expressions is a time-consuming process. The time taken to create the regular expressions varied depending on the call class and the number of examples in the training sets. This is because the training sets, which were used as reference sets, had to be examined manually to identify sequences of call classes and the sequences were used to create the regular expressions. This process may take less than 1 hour or several hours for a human, depending on the call class. The shortest computer-training time for the SRN was 3 hours 30 minutes and the longest training time was 5 hours 30 minutes. The training time for the SVM not only varied depending on the call class but also on the kernel being used and the kernel parameter values selected. The SVM training time was considerably faster than the SRN, at under 1 minute, although performance of the SRN was much better. The SVM training used a one-versus-all method (see Sect. 3.3.1 ), that is, each SVM was trained on one call class against all the other call classes, whereas the SRN was trained on 17 call classes at the same time. Optimisation of the SVM is an integral and automatic part of the training process. The SVM software [ 28 ] employs a fast optimisation algorithm, where the number of members of the working set of examples are selected according to a steepest feasible descent strategy.
 4 Analysis and discussion To establish what effect factors such as number of network layers and learn-ing rate have on network performance, these elements were changed and several experiments conducted to determine the effects of these changes. Overall changes to the learning rate had little effect on the performance of the feedforward net-work. The results show that, with or without a constant learning rate, the network was only able to correctly classify on average 21.91% of the utterances. There was, however, a slight negative effect on performance of the MLP network by changes to the learning rate. The network trained using a constant learning rate of 0.01 was able to correctly classify on average 23.83% of all utterances, whilst, with changes to the learning rate, the network was only able to correctly classify on average 22.53% of all utterances, a drop in performance of 1.3%. On average, the SRN was able to correctly classify over 75% of all utterances with or without changes to the learning rate. Therefore, changes to the learning rate on the SRN had only a slight effect on performance. This suggests that the biggest factor in improving the network performance was the addition of the context layer. utterances identified several factors, one of which is the trade-off parameter, C . If the parameter value is low, the SVM produces a simple decision surface and subsequently lots of misclassification errors, while a high value produces a com-plex decision surface and good classification. Another key factor is the type of kernel function used and how much the individual parameters are tuned. As with the trade-off parameter, C , altering other parameters also has an effect on the re-call and precision performance of the SVM. Changes, for example, to the value of the reciprocal of the Gaussian function in the RBF kernel in conjunction with the trade-off parameter, C , cause the classifier to improve performance in one area (in this case, recall), yet decrease it in the other (in this case, precision). The aim therefore is to find a balance that provides an acceptable level of recall with an ac-ceptable level of precision. In these experiments, the parameter values were kept constant for each kernel type. However, it is likely that using parameter values tuned specifically for each kernel will produce better results on some call classes than others.
 hand crafted. This process, as described in Sect. 3.4.1 , is both time consuming and labour intensive. In most instances, the transducer is able to correctly clas-sify utterances to the appropriate call class and reject those utterances that do not belong to the call class under investigation. Overall, the FST was able to classify examples belonging to all call classes, but the performance was lower than that of the SRN. This means that the FST was able to produce a classification result for utterances associated with each call class, while the SRN and SVM, in some cases, were unable to produce a classification result for utterances associated with six of the call classes in total across all four test sets. For the SVM, the inability to classify some of the call classes could be related to the type of kernel function chosen. All kernel types (RBF, sigmoid and polynomial) had problems with two or more call classes.
 utterances associated with certain call classes could be that these particular call classes have few examples in both the training and test sets in comparison with other call classes; both of these techniques learn from examples, while the FST relies on hand-crafted regular expressions. One call class that both the SVM and the SRN were able to classify constitutes 36.66% of the training set and 28.78% of the test set, and one call class that both techniques had problems classifying constitutes only 0.42% of the training set and only 0.49% of the test set. However, only a small increase in the overall representation of a call class can be sufficient to enable both techniques to classify that call class, e.g. 1.54% of the training set and 1.46% of the test set. The improvement in the classifiers X  ability to clas-sify utterances resulting from a small increase in the number of examples raises the issue of whether to manipulate the number of examples to produce a more bal-anced distribution. This issue was not considered for this work because we wanted to establish how well the classifiers could handle the task using the corpus as supplied by our telecommunications collaborator, where the distribution of exam-ples is representative of the call traffic to the operator-assistance service. Other possible reasons could be related to the vocabulary used in the utterances associ-ated with these problematic call classes or the length of the utterances. quently, that is, those classes they see more of before learning those that are less frequent. SVMs rely on labelled training data as well as patterns in the data to char-acterise the classification function that maximises the margin between the classes and at the same time minimises the number of support vectors. If there are few samples of the class to be learned, it is likely therefore that it will be difficult for the classifier to identify the patterns or support vectors needed.
 selected because the task was approached as a classification task, and recall and precision are standard measures of classification performance. When comparing classifiers, using only one evaluation measure may be preferable because using only one measure makes comparison simpler. Consequently, the F -score is used as the comparison metric. The F -score performance of the SRN, SVM and the FST on the test sets for the four call sets is summarised in Table 13 .
 formance varies across each technique. Generally, the performance of the SRN is better than that of the SVM and the FST. While the results of the SRN would seem to indicate that the use of transcripts of the spoken word chain were beneficial to performance, this would not seem to be true based on the results of the SVM and the FST. Their performance overall is considerably lower than that of the SRN and therefore for these classifiers does not seem to provide additional benefit. This is a possible indication that the method of representation of the semantic content of the utterances for the SVM and the FST is not optimal and does not allow either technique to consistently classify the utterances. Based on the very low perfor-mance of the FST, it is evident that the hand-crafted regular expressions do not capture a fine enough level of granularity in the semantics of the utterance for the FST to accurately classify the utterances.
 classes. The number of examples in the ln call class is over five times greater than that of the ac call class. As a result, if one or two examples are misclassi-fied or not classified in a small number of examples, the impact on the result, in percentage terms, is much greater and produces a greater difference in the result, but the same is not true for a larger set of examples. In general, the F -score re-sults for the SRN are quite high given the number of call classes available against which each utterance can be classified. The SRN achieved an average F -score per-formance of 81.59. This result is calculated based on the performance figures for the SRN shown in Fig. 5 .
 rejection rate: a call is rejected or classified as other and the call is routed to the operator when it could have been automatically routed, and probability of cor-rect classification, that is, how often the classifier is correct. On transcribed calls, Gorin et al [24] achieved a probability of correct classification rate of 84% with a false rejection rate of 10%, while in the OASIS project, the accuracy rate of the classifier was approximately 75% on transcribed data [ 11 ]. The vector-based approach of Chu-Carroll and Carpenter [12] achieved an overall classification per-formance in the range of 77 X 97% and, on transcribed calls, a classification perfor-mance of approximately 94% with a rejection rate of 10%. A key factor in the ap-proach of Chu-Carroll and Carpenter is the use of a disambiguation module; Chu-Carroll and Carpenter acknowledge that the disambiguation module does govern performance.
 spontaneous language, the corpus and evaluation metrics used differ. The focus of the evaluation measures used by those working on the task of call routing appears to be the ability of the classifier to either reject a call or label the call as other, whereas the focus of our approach is how successfully the classifier can classify all call classes. Our classification task used an unbalanced real data set, and using accuracy as the evaluation metric may have produced false or confusing results [ 21 ] because accuracy results can be affected if one class is scarce; in this case, recall and precision may provide a better alternative. The F -score can be used to evaluate performance on unbalanced data sets because this gives the harmonic mean of recall and precision and is often a preferable measure for this type of data set [ 21 ]. The F -score results reported show the potential of these techniques and the focus is a comparison of the performance among these techniques on the task of call classification and is not intended as a direct comparison with the work of Gorin et al or Chu-Carroll and Carpenter. It is acknowledged that there are other methods available for comparing performance. For future work, the methods of other researchers investigating spoken dialogues could be investigated [ 25 , 56 ]. 5 Conclusions The performance of SRNs, SVMs and FSTs has been discussed and a compari-son made to the work of other researchers in the field of call routing. The main result from this work is that the performance of the SRN was best, in particular, when factors such as irregularities in the utterance and the number of call classes are taken into consideration. However, a combination of techniques might yield even better performance. The SVM allows a trade-off between recall and preci-sion and, as a result for call classes that are problematic to the SRN, the SVM might be a better alternative. Whilst the FST does not allow a trade-off between recall and precision, again it could be used for those call classes that are prob-lematic because it can achieve almost perfect performance, that is 100%, on both recall and precision. The drawback to this approach is that it requires hand-crafted regular expressions, which are time-consuming to construct. One additional fac-tor that must be considered is the number of examples available for training and testing, as this does influence performance.
 unique corpus of spontaneous spoken language. Based on transcribed, segmented utterances, it has been demonstrated that an SRN performed best for spoken-language classification. From the perspective of the other techniques identified, it has been demonstrated that there is potential in SVMs and FSTs, for example, for exception handling, for spoken-language classification, but overall, the SRN achieved a better performance on this classification task and therefore may be a useful main component for a hybrid architecture.
 semantic analysis (LSA) to create a feature representation that provides a deeper meaning for the words in the utterances based on relationships identified between the words [ 33 ]. This means that performance is not dependent on the choice of words and classification may still be achieved even if utterances have no words in common because LSA considers semantic similarity. Additionally, stochastic finite-state transducers, which can be learned efficiently from data using pairs of source and target utterances, could be adopted, as they have been used successfully in research on the call-routing task [ 3 ].
 References Author Biographies
