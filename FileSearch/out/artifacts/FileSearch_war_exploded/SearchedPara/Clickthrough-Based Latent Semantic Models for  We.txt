 This paper presents two new do cument ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR). Assuming that a query is parallel to the titles of the documents clicked on for that query, large amounts of query-title pairs are constructed from clickthrough data; two latent semantic models the language modeling framework. It ranks documents for a query by the likelihood of the query being a semantics-based translation of the documents. The semantic representation is language inde-pendent and learned from query-title pairs, with the assumption semantic topics. The other is a discriminative projection model within the vector space modeling framework. Unlike Latent Se-mantic Analysis and its variants, the projection matrix in our model, which is used to map from term vectors into sematic space, is learned discriminatively such that the distance between a query and its paired title, both represented as vectors in the projected semantic space, is smaller than that between the query and the titles of other documents which have no clicks for that query. These models are evaluated on the Web search task using a real world data set. Results show that they significantly outperform their corresponding baseline models , which are state-of-the-art. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Clickthrough Data, Latent Seman tic Analysis, To pic Model, Li-near Projection, Transla tion Model, Web Search Most modern search engines retr ieve Web documents by literally matching terms in documents with those in a search query. How-ever, lexical matching methods can be inaccurate due to the lan-guage discrepancy between Web documents and search queries [20, 31] i.e., a concept is often expressed using different vocabula-ries and language styles in documents and queries. been proposed to address the issue [e.g., 9, 19, 4]. Different terms that occur in a similar context are grouped into the same semantic cluster. Thus, a query and a docu ment, represented as vectors in the lower-dimensional semantic space, can still have a high simi-larity even if they do not share any term. translation of any word in a document which may be different from, but semantically related to the query term; and the relevance of a document given a query is assumed proportional to the trans-lation probability from the document to the query. models for Web search by combining, in a principled way, the methods of semantic re presentation and statistical translation. Our assumption is that the translation between a query and a document can be modeled more effectively by mapping them into some semantic representations that ar e language independent than by mapping them at the word level. is a set of clickthrough-based translation models for Web search presented and evaluated in [14], which is a significant extension of the original approach [2], mo tivated by the increasingly large amount of clickthrough data. Following [14], in this study we consider documents and queries as two different  X  X anguages X  (i.e., the query language and the document language), and construct parallel training data from clic kthrough data by pairing a query with the titles of the documents that have clicks for that query Different from [14], our models are trained to capture the map-ping relationships between terms in a document and terms in a query at the sematic level rather than at the word level. this study is the research on cross-lingual and multi-lingual latent semantic models [11, 25, 27]. In these earlier works, various ex-tensions of Latent Semantic Anal ysis (LSA) or topic models are developed for applications such as cross-lingual IR [11] and re-variety of bilingual latent semantic models, which are the exten-sions and variants of these prev ious models. In particular, we focus the research on three aspects: (1) the methods of learning semantic representations from clickthrough data, (2) the ap-proaches to incorporating the late nt semantic models into proper IR frameworks, and (3) the evaluation of their effectiveness for Web search on a real world data set. for Web search, a bilingual topic model and a discriminative pro-jection model. Both models are learned on clickthrough data. The topic model represents a document as a distribution of semantic topics, and is incorporated into the language modeling framework for IR by assuming that a query term is generated from a mixture of these topics. The bilingual topi c model learns a semantic repre-sentation in such a way that each query and its paired titles, ex-tracted from clickthrough data, share as much as possible the same topic fractions. Unlike the topic model, the projection model, similar to LSA, represents a document as a point in the semantic space. It fits naturally the IR framework based on vector space model (VSM). The relevance of a query and a document is com-puted as the (cosine) similarity between their vectors in the se-mantic space. Different from LS A and its variants, our model learns a projection matrix, which maps the term-vector of a doc-ument onto a lower-dimensional se mantic space, using a super-vised learning method. Inspired by the learning-to-rank frame-work [6], the projection matrix is learned discriminatively in such represented as vectors in a projected semantic space, is smaller than that between the query and the titles of other documents which have no clicks for that query. We evaluated the two new ranking models on the Web search task using a real world data set. Results show that they significantly outperform their corres-ponding baseline models, which are considered state-of-the-art. discriminative projecti on model, respectively. Section 5 presents the experiments. Section 6 concludes the paper. This section reviews previous a pproaches to bridging the lexical gap between queries an d documents for IR. The statistical translation based a pproach is an extension of the language modeling approach to IR [2, 28]. Each document is scored by the likelihood of it translating into a query. Let  X  X   X  translation model [2] assumes that both  X  and  X  are bags of words, and that the translation probability of  X  given  X  is computed as  X   X   X  |  X   X  is the probability of translating  X  into a query term  X  . into itself, Eq. (1) is reduced to the simple exact term matching that is used by traditional unigram language modeling approaches. To bridge the lexical gap between queries and documents, the translation based approach allows a document to translate any one with a nonzero probability. the translation-based approach does not map different terms into latent semantic clusters but learns translation relationships directly between a term in a document and a term in a query. A major challenge of this approach is how to estimate the translation prob-abilities. The ideal training data would be a large amount of query-document pairs, in each of which the document is judged as resorts to some synthetic query-document pairs, and [22] simply uses the title-document pairs as substitutes for training data. Since recently, with the growing availabilit y of search logs, it is possible to mine implicit relevance j udgments from clickthrough data, and to generate a large amount of real query-document pairs for trans-lation model training [14]. Given enough training data, more so-phisticated translation models such as phrase models and factored models have also been in vestigated [14, 23, 26]. Latent Semantic Analysis (PLSA) [19]. Although Hofmann ap-plied PLSA to IR in the VSM framework in the original paper, PLSA is in nature a generative model, and can be more straightforwardly incorporated into the language modeling framework, under which documents are ranked by their probabili-ties of generating a query. In PLSA, a query, viewed as a short document, is generated from a document using the following word distribution. Assuming that both a document and a query are bag-of-words, the probabilit y of generating the query  X  from the document  X  is nerative model where a query term is generated from a mixture of factors. While in translation models the factors are simply words in a document, in PLSA the factors are hidden topics. proper generative model and places Dirichlet priors over the pa-likely topic vector  X  for a document, a posterior distribution over which is the same for all documents. So, in theory LDA over-comes some problems of PLSA su ch as overfitting and the issues regarding generating queries fr om unseen documents [4, 32]. However, whether the theoretical superiority of LDA can be trans-lated into significant empirical improvement over PLSA on realis-tic applications, such as Web search, remains to be demonstrated. The effectiveness of LDA for IR is demonstrated in [32] without directly comparing it to PLSA. [ 17] clarifies the relationship be-tween LDA and PLSA in the context of IR, and concludes that PLSA is a maximum a posteriori (MAP) estimated LDA model. [1] shows that MAP inference performs comparably to the best Bayesian inference methods for LDA. Therefore, in our experi-ments all the topic models are imp lemented as PLSA, or equiva-lently, LDA with MAP inference. cross-lingual or multi-lingual cases, where there are pairs or tuples of corresponding documents in di fferent languages. For example, the Poly-Lingual Topic Model (PLTM ) [25] is an extension to LDA that views documents in a tuple as having a shared topic distribution  X  . Each of the documents in the tuple uses  X  to select topics z , but could use a different language-specific, word-topic-models, Joint PLSA (JPLSA) a nd Coupled PLSA (CPLSA) are introduced in [27]. JPLSA is a va riant of PLTM when documents of different languages share the same word-topic distribution, and MAP inference, instead of Gibbs sampling, is performed. CPLSA extends JPLSA by constraining corresponding documents to have similar fractions of words assigned to each topic according to the posterior distribution of topic assignments, instead of sharing the prior topic distributions. The bili ngual topic models discussed in Section 3 are the variants and exte nsions to these previous models when we strive to effectively learn model parameters on click-through data for the application of Web search. One of the most well-known linear projection models for IR is LSA [9]. LSA models the whol e document collection using a  X  X  X  document-term matrix C , where n is the number of docu-ments and d is the number of word types, and performs singular value decomposition (SVD) on C . The k biggest singular values are then used to find the  X  X  X  projection matrix. Thus, a docu-ment represented by a d -dimensional term vect or can be mapped to a k -dimensional concept vector. pairs or tuples of parallel or comparable documents. For example, Cross-Language LSA (CL-LSA) [11] applies LSA to concate-nated comparable documents from different langua ges. Oriented Principal Component Analysis (OPCA) [12, 27] solves a genera-lized Eigen problem by introducing a noise covariance matrix to ensure that comparable documents can be projected closely. Ca-nonical Correlation Analysis (CCA) [30] finds projections that maximize the cross-covariance between the projected vectors. In all these methods, the linear proj ection is learned without explicit-ly taking into account how docu ments are ranked. We will show that learning the projection matrix discriminatively on query-title pairs, using a cost function closel y related to the measure of eva-luating document ranking, leads to a much more effective model for Web search. The bilingual topic model (BLTM) can be viewed as a special case of PLTM [25], where search queries and web documents are assumed to be written in two different languages and MAP infe-rence is used instead of Bayesian inference. BLTM is also a close variant of JPLSA [27].  X  X  X   X   X ... |  X  | share a common distribution of topics, but use different (probably overlapping) vocabularies to express these topics. The graphical model is shown in Figure 1. Formally, BLTM assumes the following pro cess of generating a query-title pair.  X  Then, for each query and its paired title, a topic distribution  X  Each term in the query is then generated by first selecting a  X  Similarly, each term in the paired title is generated by select-gether with the paired docume nt-topic vectors and word-topic vectors, is  X log X   X   X   X  |  X   X   X   X  X  X   X   X  X   X   X  X  X   X   X  |  X   X   X  X  X  X  X , X  X  X  X   X , X  , X   X  , X   X   X  where Note that since we use MAP estimation, as will be described in rather than hidden variables, as in the Bayesian inference methods. We use the standard EM algorithm [10] to estimate the parameters (  X   X , X  , X   X  , X   X   X  of BLTM by maximizing the joint log-likelihood of the parallel corpus and the parame ters, as shown in Eq. (3). The the E-step, the posterior probabilities for each term  X  in query  X  variables  X  according to:  X , X  X  X  X  X   X , X   X  X   X , X  X  X  X  X   X , X   X  X  Figure 1 : Graphical model for BLTM.  X  In the M-step, parameters are up dated for given posterior proba-bilities computed in the previous E-step. We treat  X  ,  X  as hyperparameters, each corresponding to one Dirichlet prior, of title vocabulary. To simplify notation, letting  X  X , X  X  X  be the frequency of term  X  in query  X  , and  X  X , X  X  X  the frequency of term  X  in title  X  , we define  X   X , X   X , X   X   X   X   X , X   X   X , X | X  X  X   X , X   X  and  X   X , X   X , X   X  X   X   X , X   X   X , X | X  X  X   X , X   X  . Then, the updates can be written as  X  X  X  X  X  X   X   X   X  X   X  X  X  X  X  X   X   X   X  X   X  X  X  X  X   X , X   X  X  X  share the same prior distribution over topics but also contain simi-lar fractions of words assigned each topic. Since MAP estimation of the shared topic vector is concerned with explaining the union of tokens in the query and document and can be easily dominated by the longer one of the two, it does not guarantee that each topic  X  occurs with similar frequency in the query and title. Thus, fol-lowing [27], we extend BTLM by constraining the paired query and title to have similar fractions of tokens assigned to each topic, and the constraint is enforced on expectation using posterior regu-larization [13]. of CPLSA [27] with two import ant modifications. First, while BTLM-PR assumes a pair of query and title to share the same topic distribution  X  as shown in Figure 1, in CPLSA the topic distributions of a pair of docum ents in two languages are com-pletely independent. Second, CP LSA uses inequality constraints with slack variables to form the constrained space whereas BTLM-PR uses only equality cons traints. These modifications not only lead to a mathematically simpler model, thus making the model training faster (e.g., it requires fewer EM iterations), but also significantly improve the retrieval results. where in the E-step the posterior distributions of topics computed on a query-title pair  X  X , X  X  are projected onto a constrained set of are assigned topic  X  is the same as the expected fraction of tokens in  X  that are assigned the same topic. The expected counts with respect to this projected posterior distribution are then used in the M-step, which remains the same as Eq. (6) to (8). performed. Let  X  X , X  X  be a pair of sequences of tokens and their topic assignments, where The projection minimizes the Kullback-Leibler divergence be-note the set of posterior distribu tions over hidden topic assign-ments for the tokens in  X  and  X  , computed by the standard E-step set.  X  X  X  is an ideal set of distributions that has the desired property: the expected fraction of each topic is equal in  X  and  X  such that for each topic where Now, the projection can be formulated as a constrained optimiza-tion problem, where we seek an ideal set of distributions  X  X  X  that is closest to  X  : min where  X  is a space of distribution sets  X   X  satisfying the con-straint: In our case, the valid ideal distribution space  X  is non-empty. The the sake of clarity, we leave the derivation to Appendix A, and terms of the dual solution  X   X  by where  X   X , X   X   X   X , X  X  X  X  X  X  X  X   X , X   X  X xp X  X   X   X  where  X   X , X   X   X   X , X  X  X  X  X  X  X  X   X , X   X  X xp X   X   X  The optimal value of  X   X  can be obtained using the gradient ascent algorithm with the update where  X  is the learning rate, and the gradient  X  is computed as  X   X   X   X   X   X  X   X   X  X , X  X  in training data as follows. First, parameters  X  are optimized using gradient ascent according to Eq. (12) and (13). Then, the projected probabilities are computed according to Eq. (10) and (11). The modified EM algorithm uses the projected posterior probabilities  X  X  to update the model parameters in the M-step. In our Web search experiments, we mixed BLTM with the smoothed unigram language model, and used the following doc-ument ranking function: smoothed background model and doc ument model, respectively.  X  and  X   X  are tuning parameters with their values between 0 and guage model with Jelinek-Mercer smoothing [34], which is used as baseline in our experiments. Letting  X   X   X 0 , the document model depends solely on BLTM. Also notice that BLTM in Eq. (17) differs from a topic model of Eq. (2). Although in both mod-query term  X  is generated from topic-specific word distributions in query language in BLTM. Thus, BLTM can be considered as our experiments, we used folding-in with 20 EM iterations to map a document in test data to its corresponding topic vector  X  The discriminative projection mode l (DPM) maps a sparse, high-dimensional term vector onto a dense, low-dimensional space through a simple matrix multiplication. DPM differs from other linear projection models in the way the projection matrix is learned. In this study we compare DPM to LSA and its variants. Since in our implementation DPM and various LSA models take the same clickthrough data as in put and output the projection ma-trix in the same model form, we start the description of DPM with a brief review of LSA and its variants. LSA models the whole document collection using a  X  X  X  docu-ment-term matrix  X  , where n is the number of documents and d is three matrices using SVD where the orthogonal matrices  X  and  X  are called term and docu-ment vectors, respectively, and the diagonal elements of  X  are singular values in descending order. Then, a low-rank matrix ap-gular values in  X  . Now, a document (or a query) represented by a term vector  X  can be mapped to a low-dimensional concept vector  X   X  as where the  X  X  X  matrix  X  X  X   X   X   X   X  X  is called the projection matrix. In document search, the relevan ce score between a query and a document, represented resp ectively by term vectors  X  and  X  , is assumed to be proportional to their cosine similarity score of the tion matrix  X  analysis (PCA), and can be so lved via Eigen-decomposition in-stead. Let  X  be the correlation matrix between terms  X  X  X  Compared to LSA, the only difference is that PCA solves the Eigen problem on the covariance matrix instead. Because the term vectors are very sparse in practice and the column means are close experiments we used PCA to de rive the LSA projection matrix. riance of the projected vectors. Wh en applied directly to the click-through data, queries an d title are treated as separate documents and the click information is no t used. CL-LSA [11] and OPCA [12, 27] are two variants to LSA, striving to reduce the projected distance between a query and its paired title. Unfortunately, al-though both variants use the pair information to bias the deriva-only a coarse approximation to the final ranking measure. We hypothesize that with a large set of training data, the projection matrix can be learned discrimina tively by minimizing a loss func-tion that targets the ranking scenario , and thus yields better results. We describe this new projection learning framework in the next section. The projection matrix in DPM is learned from query-title pairs using S2Net, a newly proposed le arning framework that learns discriminatively the projection ma trix from pairs of related and unrelated documents. We briefly introduce the model below and interested readers can refer to [33] for more detail. mapped concept vector as the output layer. The value of each where the weights are associated with the edges. In other words, the network structure is a complete bipartite graph between the input and output layers, and the edge weights are equivalent to the describe the loss function and training process. learning-to-rank paradigm outlined in [6]. Consider a query  X  and two documents  X   X  and  X   X  , where  X   X  has clicks for  X  but  X  not. Let  X , X   X  and  X   X  be the term vectors of  X , X   X  and  X  tively. We then construct two pairs of term vectors  X , X  X   X , X  X   X   X  , where the former is preferred and should be ranked high-difference of the cosine similarity scores of their projected con-cept vectors, following Eq. (20). Namely,  X  X  X  X   X   X   X  X  X   X   X , X  X   X   X  . Intuitively, we want to learn a model to increase  X  . We use the following logistic loss over  X  , which can be shown to upper bound the pairwise accuracy The loss function in Eq. (21) has a shape similar to the hinge loss used in SVMs. Because of the use of the cosine similarity function, we add a scaling factor  X  that magnifies  X  from [-2, 2] to a larger it is large enough. In th e experiments, we set  X 10 X  . Because the loss function is differentiable, optimizing the model parameters  X  can be done using the gradient-b ased methods, such as L-BFGS. For the sake of a clean presentation, we leave the gradient deriva-tion to Appendix B. the model from a good projection matrix often helps reduce the training time and may converge to a better local minimum. In our experiments, we always started the model parameters from the LSA matrix. In principle, Eq. (21) can further be regularized by adding a term  X  deviate too much from the initial model  X   X  . However, we did not find clear empirical advantage over the simpler early stop ap-proach in a preliminary study, which is adopted in the experi-ments in this paper. This section evaluates the effectiveness of the models described in Sections 3 and 4 on the Web search application. Instead of pre-senting a direct comparison between topic modeling and linear projection modeling 2 , we focus our experiments on demonstrating that for each type of models, cl ickthrough data can lead to signifi-cant improvements when modeled pr operly. Thus, we will report the results of the topic models an d the linear projection models in separate sections. For each type, we compare our models to their baseline methods, which are consid ered state-of-the-art in the research community. We evaluated the retrieval models on a large-scale real world data set, called the evaluation data set henceforth. The data set contains 16,510 English queries sampled fro m one-year query log files of the Microsoft Bing search engine. On average, each query is asso-ciated with 15 Web documents (URLs). Each query-title pair has a relevance label. The label is human generated and is on a 5-level relevance scale, 0 to 4, with 4 meaning document  X  is the most relevant to query  X  and 0 meaning  X  is not relevant to  X  . All the queries and documents are preprocessed as follows. The text is Strictly speaking, the results of topic models and linear projec-tion models, reported in Tables 1 and 2 respectively, cannot be compared directly due to three reasons. First, the baseline mod-els, with which the proposed m odels are combined to achieve the best results, are different. Se cond, in order to perform S2Net training efficiently for fast experimentation, the vocabulary used in building linear projection models is much smaller than that for topic models. Third, the input term weighting functions for them are different: the topic models use the raw term frequency counts, while the projection mode ls take TFIDF vectors. We leave to future work a direct comparison of topic modeling and linear projection modeling in a more consistent setting, and how to best combine them for Web document ranking. white-space tokenized and lowercas ed, numbers are retained, and no stemming/inflection is performed. els, topic models, VSM and linear projection models ) contain free parameters that must be estimated empirically by trial and error. Therefore, we used 2-fold cross validation: A set of results on one half of the data is obtained using the parameter settings optimized on the other half, and the global retrieval results are combined from those of the two sets. mean Normalized Discounted Cumulative Gain (NDCG) [21]. We report NDCG scores at truncation levels 1, 3, and 10. We also performed a significance test using the paired t -test. Differences are considered statistically significant when the p -value is less than 0.05. ing, are extracted from one year query log files using a procedure similar to [16]. First of all, a set of query sessions were extracted query and a ranked list of documents, each of which may or may pairs  X  X  X  X , X  X  X  X  X  X  X , X  X  , where  X  is a unique query string for which  X  , as defined in Eq. (1) in [16]. Only those pairs whose scores are larger than a threshold were retained.  X  X , X  X  X  X  X  X  X  could be the number of times the document was cl icked on for that query, but been shown to the user and the pos ition in the ranked list at which the page was shown. Finally, we formed a set of query-title pairs by aligning the title of the document to each unique query string in the query click field of the same document. field, when it is valid, is the most effective for Web search. How-ever, click information is unavail able for many URLs, especially new URLs and tail URLs, leaving their click fields invalid (i.e., this study, we assume that each document contained in the evalua-tion data set is either a new URL or a tail URL, thus has no click information (i.e., its click field is invalid). Our research goal is to investigate how to learn the latent semantic models from the popular URLs that have rich click information, and apply the models to improve the retrieval of those tail or new URLs. To this end, in our experiments only the title fields of the Web documents are used for ranking. amounts of query-title pairs using the procedure described above. For training latent semantic models in this study, we used a ran-domly sampled subset of 82,834,648 pairs whose documents are popular and h ave rich click info rmation. We then tested the trained models in ranking the documents in the evaluation data set, which do not have click informa tion. The query-title pairs were pre-processed in the same way as the evaluation data to ensure uniformity. Table 1 shows the main Web docu ment ranking results using var-ious topic models, tested on the human-labeled evaluation data set via 2-fold cross validation, as described in Section 5.1. with Jelinek-Mercer smoothing, parameterized by Eq. (14) to (16) with  X   X   X 1 . Eq. (14) to (17). To improve the efficiency of model training, we pruned the query-title training da ta by retaining only top 500K high-frequency words. We used 100 topics  X 100 X  X  X  X  for all the the unigram model and the latent semantic model to ranking, we report for each topic model the resu lts using two different settings. One is letting the document mode l solely depend on the latent semantic model by setting  X   X   X 0 in Eq. (16). These results are shown in the shaded rows in Table 1. The other is defining the document model as a mixture of the unigram model and the latent semantic model by using a nonzero  X   X  in Eq. (16), tuned via cross-validation. We used folding-in with 20 EM iterations to map each document in the evaluation data set to its corresponding topic vector. In what follows, we describe the four topic models in turn. proposed in [19], and was trained on documents only (i.e., the title PLSA was learned using MAP estimation, with  X 1.1 X  and  X 1.01  X  . The model can also be view ed as an approximation to the LDA document model described in [32], which is learned on the TREC document collection via Gibbs sampling. where the model parameters were learned on query-title pairs using MAP estimation, as describe d in 3.1. We found that the model performance is not very sensitive to the values of the hyperparameters, which were set in our experiments as  X 1.1 X  and  X   X   X  X   X   X 1.01 . We also found after around 20 EM itera-tions, the likelihood of the model barely increases. ified EM algorithm that uses po sterior regularization (PR), de-only to share the same prior topic distribution  X  , but to also have similar fractions of tokens assign ed to each topic. We found that with PR, the EM algorithm takes fewer iterations to converge. In our experiments, the likelihood s eems to saturate after 16 itera-tions. where we merge the vocabularies in query and title languages and learn topic-specific word distributions over these merged vocabu-only simplifies the implementation but also sometimes leads to better results. In our experiments, we found that using the merged vocabularies does not bring any significant difference for BLTM, but does lead to some small but significant improvement for BLTM-PR. One possible interpretation is that the same word in a query and document very often has the same topic, which is not rare words that may be harmful since there might not be enough data to estimate their topic distributions in queries and titles com-pletely independently. Therefore, we speculate that a hierarchical Bayesian model that encourages matching words in queries and titles to have the same topics, but also allows them to diverge, would be superior to both the single vocabulary and two-vocabulary models. We leave it to future work. word translation model described in [14], listed here for compari-son. The corresponding ranking function is similar to Eq. (14) to the word translation model  X   X  X  X  X  defined as where  X   X   X  |  X   X  is the word translation probability assigned by IBM-Model-1 [5], trained on query-title pairs using EM. PLSA alone as a document model hurts the ranking performance (Row 2 vs. Row 1). But a linear combination of PLSA and the original document model signif icantly outperforms the baseline model (Row 3 vs. Row 1). The resu lts are consistent with those previously reported on the TREC collections [32]. Second, using clickthrough data for model training by extending PLSA to BLTM, leads to a significant improvement (Rows 4 and 5 vs. Rows 2 and 3). Third, the performance of BLTM can be further improved by introducing constraints in the EM training to force the paired query and title to share the same proportion of topics (Rows 6 to 9 vs. Rows 4 and 5). The differences among BLTM, BLTM-PR, and BLTM-PR-V1 are statistically significant. Finally, we confirmed the effectiveness of the word translation model. The model performs as well as BLTM, i.e., their results are not signif-icantly different (Rows 10 and 11 vs. Rows 4 and 5). However, both BLTM-PR and BLTM-PR-V1 beat the translation model with a statistically significant margin (Rows 6 to 9 vs. Rows 10 and 11). We also tried combining WTM_M1 with BLTM-PR, but the result is not significantly better than that of BLTM-PR. # Models NDCG@1 NDCG@3 NDCG@10 1 UM 0.308 0.373 0.454 2 PLSA (  X   X   X 0 ) 0.295 0.371 0.456 3 PLSA 0.325 0.391 0.470 4 BLTM (  X   X   X 0 ) 0.330 0.399 0.476 5 BLTM 0.338 0.404 0.479 6 BLTM-PR (  X   X   X 0 ) 0.334 0.403 0.479 7 BLTM-PR 0.342 0.406 0.482 8 BLTM-PR-1V (  X   X   X 0 ) 0.337 0.403 0.480 9 BLTM-PR-1V 0.344 0.407 0.483 10 WTM_M1 (  X   X   X 0 ) 0.332 0.400 0.478 11 WTM_M1 0.338 0.404 0.480 Table 1 : Web document ranking results using different topic models, tested on the evaluation data set, where only the title field of each document is used. Figure 2: Average number of function evaluations and gradient computations per EM iteration, as a function of the number of the EM iterations, in the projecti on step for training BLTM-PR. same as the EM training for PLSA, which has been well-studied. BLTM-PR uses a modified EM algorithm. Although BLTM-PR needs fewer EM iterations to converge, each iteration is more expensive due to the projection step. The runtime of the projection step is dominated by function evaluations (Eq. (28)), and other-wise the most expensive step is the computation of the gradients for each query-title pair. Initializing BLTM-PR with a uniform distribution for  X  and  X  , Figure 2 plots the average number of function evaluations and gradient computations per EM iteration, as a function of the number of the EM iterations. Both curves show that after 10 EM iterations, the training becomes much slower due to the dramatically increased cost of the projection step, indicating that from this moment EM starts to lead the distri-bution set far away from the ideal one in terms of KL distance. The cost of projection reduces slightly after 16 iterations when the EM training saturates. In our experiments, we found that training BLTM takes around 30 hours on a commodity 8-core server with 64-GB memory, and training BLTM-P R takes twice as much time. In practice, since the EM algorithm can be easily parallelized, topic model training could be perfo rmed much more efficiently on a cluster of computers. Table 2 shows the main Web docu ment ranking results using var-ious linear projection models, tested on the human-labeled evalua-tion data set. and queries are represented as term vectors, with the TF-IDF term weighting, and the documents are ranked by the cosine similarity between the query and document vectors. them have the same model form as that of LSA [9]. To improve the efficiency of model training, we truncated the term vectors based on a vocabulary consisting of only the top 40K high docu-ment-frequency (DF) words, where the DF values are calculated based on the clickthrough data. We used 100 dimensions ( k =100) for the vectors in semantic spa ce. Similar to topic models, we report a pair of results for each projection model using two differ-ent settings. One is ranking docume nts using the cosine similarity scores in the semantic space, as in Eq. (20). These results are shown in the shaded rows in Table 2. The other is ranking docu-ments based a weighted linear comb ination of two cosine similari-ty scores, computed in the original term space and in the projected semantic space, respectively. The linear combination weight is tuned via cross-validation. In what follows, we describe each of these models in turn. scribed in [9]. As described in Section 4.1, we used PCA instead of SVD to compute the matrix. Queries and titles are treated as separate documents; the pair information from the clickthrough data was not used in this model. information so that the projected distance between a query and its paired title is reduced [11]. In our implementation, each query and its paired title were concatenated first to form a new document. Then, the projection matrix was learned by applying LSA to this new corpus. more principled way [12, 27] than CL-LSA does. Noticing that solving the Eigen-decomposition problem in PCA is the same as finding the vectors  X  that maximize the Rayleigh quotient: OPCA improves PCA by replacing Eq. (22) with the generalized Rayleigh quotient: where  X  is the noise covariance matrix. The role of  X  is to ensure from the same pair can be minimized. Let  X   X  and  X  ument-term matrices of queries and titles, respectively. In addition, column vectors in  X   X  and  X   X  correspond respectively to the query and title in the pair when they have the same column index. The noise covariance matrix is constructed as where  X  is the number of query-title pairs. Section 4.2, where the projection matrix is discriminatively learned using relevant and irreleva nt pairs of queries and titles. We first randomly split the clickt hrough corpus into two subsets, training (99.5%) and validation (0.5%). For each query, the paired title is treated relevant (positive) and we randomly selected 4 oth-wise training setting encourages the model to lead to higher simi-larity scores of positive pairs compared to negative ones of the same query. We stop the training process based on the model performance on the validation set. shown in Table 2. First, when comparing different linear projec-tion models with the VSM baseline (Rows 2, 4, 6 and 9 vs. Row 1), we found that all models except S2Net perform worse than VSM. This is consistent with the observation made by other re-searchers, which is that using LSA alone can hurt the ranking performance, especially for a very low dimensional concept vec-tor space [24]. This result also justifies the scheme of combining the projection models with VSM. As presented in Table 2, the NDCG scores of the combined models are all better than both VSM and the corresponding projection models. Second, unlike the case of topic models, simply extending LSA to its bilingual version CL-LSA does not lead to any significant improvement (Rows 4 and 5 vs. Rows 2 and 3). Third, by simultaneously mini-mizing the distance between projec ted vectors of queries and their # Models NDCG@1 NDCG@3 NDCG@10 1 VSM 0.313 0.379 0.460 2 LSA 0.298 0.372 0.455 3 LSA + VSM 0.330 0.396 0.474 4 CL-LSA 0.298 0.370 0.454 5 CL-LSA + VSM 0.330 0.396 0.474 6 OPCA 0.306 0.373 0.454 7 OPCA + VSM 0.328 0.395 0.473 8 S2Net 0.329 0.401 0.479 9 S2Net + VSM 0.340 0.407 0.483 
Table 2 : Web document ranking results using different linear projection models, tested on the evaluation data set, where only the title field of each document is used. paired titles, OPCA does outpe rform LSA and CL-LSA with a small but statistically significant margin (Row 6 vs. Rows 2 and 4). However, after combining with the term vector model, the differences among these methods are not significant (Row 7 vs. Rows 3 and 5). Finally, the S2Net-trained DPM, when either used alone or combined with the term vector model, outperforms sig-nificantly other competing models (Rows 8 and 9 vs. Rows 1 to 7). Its superior results demonstrate th at with an objective tightly re-lated to the measure of evalua ting document ranking, the discri-minative learning approach can be very effective. its training process is, unfortuna tely, more computationally ex-pensive. Unlike LSA, CL-LSA and OPCA, which can all be solved by Eigen-decomposition, there is no analytic solution that minimizes the loss function in S2Net. In our current implementa-tion, using a cluster of 60 ~ 80 nodes, each training iteration takes 1 to 1.5 hours and the model converges in approximately 40 itera-tions. The training time scales roughly linearly in terms of the number of dimensions and the num ber of examples. In contrast, using a commodity 8-core server with 64-GB memory, it typically takes 8 hours or less to derive an LSA, CL-LSA or OPCA model. This paper presents two new document ranking models by com-bining the methods of latent se mantic representation and the sta-tistical translation-based approach to IR. We explore various me-thods of learning the semantic re presentation that is shared by a query and its paired titles fro m clickthrough data. Our evaluation on Web search shows that the proposed clickthrough-based latent semantic models significantly out perform both the standard IR models that do not use clickthrou gh data and those previous click-through-based translation models that do not use semantic repre-sentation. combining latent semantic models and translation models for IR. For example, we can form query-title corpora, where both the queries and titles are labeled by topi cs or concepts (e.g., generated using LSA). Then we can align the corpora using word-alignment models and readily compute translation probabilities based on words and topics. Another research area is the modeling of the correlations between topics in document ranking, as suggested in [3]. This is motivated by the observation that a search user may click a document on a topic that is related to, but not the same as, the topic in her query. [1] Asuncion, A., Welling, M, Smyth, P., and Teh, Y W. 2009. On [3] Blei, D., and Lafferty, J. 2007. A correlated topic model of [4] Blei, D. M., Ng, A. Y., and Jord an, M. J. 2003. Latent Dirichlet [10] Dempster, A., Laird, N., and Ru bin, D. 1977. Maximum likelih-[12] Diamantaras, K. I., and Kung, S. Y. 1996. Principle Component [18] Griffiths, T. L., Tenenbaum, J. B., and Steyvers, M. 2007. Top-[24] Manning, C. D., and Schutze, H. 1999. Foundations of [26] Och, F. 2002. Statistical machine translation: from single-word [27] Platt, J., Toutanov a, K., and Yih, W. 2010. Translingual [29] Svore, K., and Burges, C. 2009. A machine learning approach We derive the forms of the projected posterior probabilities in Eq. (10) and (11), and the gradient of Eq. (13). The derivation follows closely the one presented in [13], and uses the standard Lagran-gian duality results. problem in Eq. (9) is max where  X   X   X   X   X , X , X ,  X   X  X  X  X  X  X   X  | |  X   X  The Lagrangian includes the equality constraints to ensure that we are in the desired constrained space and that we have valid distri-this is not necessary as it fall s out from the other conditions. The form of  X  X  X  X  X  X  X  X  X  X  can be obtained by setting the derivative of  X . X  X  with respect to  X  X  to zero as  X  X , X , X , X  X  X  X  X  X   X  X   X   X   X  X  X  |  X   X   X 0  X  X  X og  X   X   X  X  X  |  X   X   X , X  X  X  X  X  X  X  X  X 1 X  X og  X , X   X  X  Thus, we have Since  X   X   X   X   X  X  X  |  X   X   X   X 1 , we get We end up with the following form Similarly, we can derive the form of  X   X   X   X  X  X  |  X   X  as where respectively. Now, we show how to estimate  X   X  . Plugging Eq. (26) and (27) into Eq. (25), we have  X   X   X   X   X ,  X   X  X  X  X   X   X   X  X  X  |  X   X   X  X  We then use the gradient ascent algorithm to get the optimal  X  where  X  is the learning rate, and the gradient  X  is computed as  X   X   X   X   X   X  which is identical to Eq. (13) We derive the gradient of the loss function in Eq. (21) as follows. Breaking it into three parts, we have respectively. Eq. (33) becomes  X   X  X 
