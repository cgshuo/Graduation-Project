 Bernardo  X  Avila Pires bpires@ualberta.ca Csaba Szepesv  X ari szepesva@cs.ualberta.ca Let A be a real-valued m  X  d matrix, b be a real-valued m -dimensional vector, M be an m  X  m positive semi-definite matrix, and consider the loss function L M R d  X  R defined by where k X k M denotes the M matrix-weighted two-norm. We consider the problem of finding a minimizer of this loss when instead of A,b , one has access only to their respective  X  X oisy X  versions,  X  A ,  X  b . We call this problem a statistical linear inverse problem.
 Our main motivation to study this problem is to bet-ter understand the so-called least-squares approach to value function estimation in reinforcement learning, whose goal is to estimate the value function that cor-responds to a Markov reward process. 1 The least-squares approach originates from the work of Bradtke and Barto (1996), who proposed to find the parameter-vector  X   X  of a linear-in-the-parameters value function by solving  X  A X  =  X  b where the  X  X oisy X  matrix-vector pair, (  X  A,  X  b ), is computed based on a finite sample. They have proven the almost sure convergence of  X   X  to  X   X  , the solution of A X  = b , under appropriate con-ditions on the sample as the sample-size converges to infinity. In particular, they assumed that the sam-ple is generated from either an absorbing or an er-godic Markov chain. More recently, several studies ap-peared where the finite-sample performance of LSTD-like procedures were investigated (see, e.g. , (Antos et al., 2008; Ghavamzadeh et al., 2010; Lazaric et al., 2010; Ghavamzadeh et al., 2011)). The nonparametric variant has also received some attention (Farahmand et al., 2009; Maillard, 2011).
 One of the difficulties in the analysis of these proce-dures is that in these problems the sample is corre-lated, so the standard techniques of supervised learn-ing that assume independence cannot be used. The approach followed by the above-mentioned papers is to extend the existing techniques on an individual ba-sis to deal with correlated samples. However, this might be quite laborious, even only considering the rel-atively easier case of regression 2 ( e.g. , Farahmand and Szepesv  X ari 2011). Thus, a more appealing approach might be to first derive error bounds as a function of the errors  X  A  X  A ,  X  b  X  b . The advantage of this ap-proach is that it allows one to decouple the technical issue of studying the concentration of the errors  X  A  X  A ,  X  b  X  b from the error (or stability) analysis of the es-timation procedures. This is the approach that we advocate and follow in this paper. Consequently, our results will always be applicable when one can prove the concentration of the errors  X  A  X  A ,  X  b  X  b , leading to an overall elegant, modular approach to deriving finite-sample bounds. In some way, our approach par-allels the recent trend in learning theory where sharp finite-sample bounds are obtained by first proving de-terministic  X  X egret bounds X  ( e.g. , Cesa-Bianchi et al., 2004).
 A second unique feature of our approach is that we de-rive our results in the above-introduced framework of general statistical linear inverse problems. This allows us to concentrate on the high-level structure of the problem and yields cleaner proofs and results. Fur-thermore, we think that the problem of linear estima-tion is interesting on its own due to its mathemati-cal elegance and its applicability beyond value func-tion estimation (a number of specific linear inverse problems, ranging from computer tomography to time series analysis, are discussed in the books by Kirsch (2011) and Alquier et al. (2011)).
 We will also place special emphasis in statistical lin-ear inverse problems whose underlying system is in-consistent ( i.e. , when there is no solution to A X  = b ). In value function estimation, such inconsistency may arise in the so-called off-policy version of the problem. Understanding the inconsistent case is important be-cause results that apply to it may shed light on issues arising when learning in badly conditioned systems. 1.1. Goals In this paper, our goal will be to derive exact, uni-form, fast, high-probability oracle inequalities for the estimation procedures we study. That is, our goal is to prove that for our choice of an estimator  X   X  , for any 0 &lt;  X  &lt; 1, with probability 1  X   X  , where, for fixed values of  X , X  , for some appropriate norm k X k . The above is called an oracle inequality since the performance of  X   X  (as mea-sured with the loss) is compared to that of an  X  X racle X  that has access to the true loss function. The term c
A,  X  b (  X , X  ) expresses the  X  X egret X  permitted due to the lack of knowledge of the true loss function. The scal-ing of this term with  X  (or a norm of it) and  X  will also be of interest.
 Let us now explain the special attributes of the above inequality. We call the  X  X ate X  in the above inequality  X  X ast X  when (2) holds. Such a  X  X ast rate X  is possible in simple settings ( e.g. , when d = 1, A =  X  A = 1), hence it is natural to ask whether such rates are still possible in more general settings. The oracle inequal-ity above is called exact because the leading constant (the constant multiplying L M (  X  )) equals to 1. When L  X  = inf  X  L M (  X  ) is positive (implying that the system is inconsistent ), then only a leading constant of one can guarantee the convergence of the loss to the mini-mal loss, i.e. , the consistency of the estimator . We call the above inequality uniform because it holds for any value of  X  . This should be contrasted with inequali-ties where the range of  X  is lower-bounded and/or the estimator uses its value as input, which may be use-ful in some cases but falls short of fully characterizing the tail behavior of the loss of the resulting estimator. With some abuse of terminology, an inequality of the above form that holds for all small values of  X  shall be also called uniform. Uniform bounds seem to be harder to prove than their non-uniform counterparts, and we do not know of any uniform, high-probability exact oracle inequality with fast rates, not even in the case of linear regression. Unfortunately, we were also unable to derive such results.
 When deriving the estimators, we shall see that a ma-jor challenge is to control the magnitude of  X   X  . Indeed, it follows from our objective function that the size of A  X   X  must be controlled, and when A is unknown the magnitude of  X   X  must be controlled. This might be diffi-cult when following a naive approach of solving  X  A X  =  X  b to get  X   X  , e.g. , when  X  A is singular, or near-singular (as might be the case frequently in practice). To cope with this issue, in this paper we study procedures built around penalized estimators where a penalty Pen(  X  ) is combined with the empirical loss  X  L M (  X  ) = k  X  A X   X   X  The penalty is assumed to be some norm of  X  . We study two procedures. In the first one, the loss is com-bined directly with the penalty, in an additive way to get the objective function  X  L M (  X  ) +  X  k  X  k , while in the second one the square of the empirical loss is combined with the penalty:  X  L 2 M (  X  ) +  X  k  X  k . Note that both ob-jective functions are convex. We note in passing that the second objective function when k  X  k is the ` 1 -norm gives a Lasso-like procedure, but we postpone further discussion of these choices to later sections of the pa-per.
 In the case of both objective functions the main issue becomes selecting the regularization coefficient  X  &gt; 0. In this paper we give novel procedures to this end and show that these procedures have advantageous prop-erties: we are able to derive oracle inequalities with fast rates for our procedures, although the inequalities will be either exact or uniform (but not both). To the best of our knowledge our general approach, our procedures, analytic tools and results are novel. The organization of the paper is as follows: in the next section, to motivate the general framework, we briefly describe value function estimation and how it can be put into our general framework. This is fol-lowed by a brief section that gives some necessary def-initions. Section 3 contains our main results for the two approaches mentioned above. Section 4 discusses the results in the context of value function estimation. The paper is concluded and future work is discussed in Section 5. The purpose of this section is to show how our results can be applied in the context of value-estimation in Markov Reward Processes. Consider a Markov Reward Process (MRP) ( X 0 ,R 1 , X 1 ,R 2 , ... ) over a (topological) state space X . By this we mean that ( X 0 ,R 1 , X 1 ,R 2 , ... ) is a stochastic process, ( X t ,R t +1 )  X  X  X  R for t  X  0 and given the history H t = ( X 0 ,R 1 , X 1 ,R 2 , ...,X t ) up to time t , the dis-tribution of state X t +1 is completely determined by X , while the distribution of the reward R t +1 is com-pletely determined by X t and X t +1 given the history H t +1 . Denote by P M the distribution of ( R t +1 ,X t +1 given X t . We shall call P M a transition kernel . As-sume that support of the distribution of X 0 covers the whole state space X . Define the value of a state x  X  X  is the so-called discount factor .One central problem in reinforcement learning is to estimate the value func-tion V given the trajectory ( X 0 ,R 1 , X 1 ,R 2 , ... ) (Sut-ton and Barto, 1998). One popular method is to exploit that the value function is the unique solu-tion to the so-called Bellman equation , which takes the form TW  X  W = 0, where W : X  X  R and T : R X  X  R X is the so-called Bellman operator de-fined using ( TW )( x ) = E [ R t +1 +  X W ( X t +1 ) | X t Note that T is affine linear.
 Given a finite sample ( X 0 ,R 1 , X 1 ,R 2 ,...,X n +1 ), the LSTD algorithm of Bradtke and Barto (1996) finds an approximate solution to the Bellman equation by solving the linear system functions,  X  i : X  X  R , 1  X  i  X  d , and W  X  : X  X  R is defined using W  X  ( x ) =  X   X , X  ( x )  X  . Denoting by solution to (3), W  X   X  is the approximate value function computed by LSTD. This method can be derived as an instrumental variable method to find an approximate fixed point of T (Bradtke and Barto, 1996) or as a Bubnov-Galerkin method (Yu and Bertsekas, 2010). In any case, the method can be viewed as solving a  X  X oisy X  version of the linear system Here, A = E (  X  ( X st t )  X   X  X  ( X st t +1 ))  X  ( X st t a steady-state MRP with transition kernel P M . 3 The linear system (4) can be shown to be consistent (Bert-sekas and Tsitsiklis, 1996). 4 Note that (3) can also be written in the compact form  X  A X  =  X  b , where  X  A = 1 /n P n t =1 (  X  ( X t )  X   X  X  ( X t +1 ))  X  ( X t ) versions of A , b and observing that for any M 0 so-lutions to (4) coincide with the minimizers of L M (  X  ) = k A X   X  b k M we see that the least-squares approach to value function estimation can be cast as an instance of statistical linear inverse problems. When M = C  X  1 , C = E  X  ( X t )  X  ( X t ) &gt; , L M (  X  ) becomes identical to the so-called projected Bellman error loss which can also be written as L M (  X  ) = k  X   X , X  ( TW  X   X  W  X  ) k where  X  is the steady-state distribution underlying P M , k X k  X , 2 is the weighted L 2 (  X  )-norm over X and  X  : L 2 ( X , X  )  X  L 2 ( X , X  ) is the projection on the lin-ear space spanned by  X  with respect to the k X k  X , 2 -norm (Antos et al., 2008).
 Note that under mild technical assumptions (to be discussed later) one can show that (  X  A n ,  X  b n ) = ( gets concentrated around ( A,b ) at the usual paramet-ric rate as the sample size n diverges. Thus, we can indeed view  X  A,  X  b as  X  X oisy X  approximations to ( A,b ). One variation of this problem, the so-called off-policy problem, gives further motivation to recast the prob-lem in terms of a loss function L M (  X  ) to be minimized. In the off-policy problem the data comes in the form of triplets, (( X 0 ,  X  R 1 ,  X  X 1 ) , ( X 1 ,  X  R 2 , H t = (( X 0 , given X t and is equal to the transition kernel P M . Fur-ther, it is assumed that ( X t ) t  X  0 is a Markov process. The previous setting (also called the on-policy case) is replicated when  X  X t = X t , thus this new setting is more general than the previous one. The straightfor-ward generalization of the least-squares approach is to define A = E h (  X  ( X st t )  X   X  X  (  X  X st t +1 ))  X  ( X tem A X  = b is not necessarily consistent but one can still aim for minimizing (for example) the pro-jected Bellman error. Using  X  A = 1 /n P n t =1 (  X  ( X t  X  X  (  X  X t +1 ))  X  ( X t ) &gt; and  X  b = 1 /n P n t =1 can again cast the problem as a statistical linear in-verse problem. In this section we give our main results for statistical linear inverse problems. We start with a few defini-tions. For real numbers a,b , we use a  X  b to denote max( a,b ). The operator norm of a matrix S with re-spect to the Euclidean norm k X k 2 is known to satisfy k S k 2 =  X  max ( S ). In what follows, we fix a vector norm k X k . Define the errors of  X  A and  X  b with the following respective equations: let where k X k 2 ,  X  denotes the operator norm of matrix X with respect to the norms k X k 2 and k X k , meaning that Although our main results are oracle inequalities, it will also be interesting to name a minimizer of L M (  X  ) to explain the structure of some bounds. For this, we introduce  X   X   X  R d as a vector such that  X   X   X  arg min  X   X  R d L M (  X  ) where if multiple minimizers exist we choose one with the minimal norm k X k . 5 In general,  X  A ,  X  b are unknown. As it will turn out, in order to properly tune the penalized estimation meth-ods we consider, we need at least upper bounds on these quantities (in particular, on  X  A ). To stay in-dependent of sampling assumptions, we assume that suitable high-probability bounds on  X  A and  X  b are available: Assumption 3.1. There exist known scaling con-stants s A ,s b &gt; 0 and known  X  X ail X  functions z A, X  ,z simultaneously with probability (w.p.) at least 1  X   X  : To fix the scales of these bounds, we restrict z A, X  , z b, X  so that z A, 1 logarithm.
 The reason to have two terms on the right-hand side in the above inequalities as opposed to having a single term only is because we wish to separate the terms attributable to  X  and the sample size. The intended meaning of s a (and s b ) is to capture how the errors behave as a function of the sample size n (typically, we expect s A ,s b = O ( n  X  1 / 2 )), while the terms z A, X  capture how the errors behave as a function  X  ( e.g. , they are typically of size O ( p ln(1 / X  ))). In particular, s ,s b should be independent of  X  and z A, X  ,z b, X  should be independent of the sample size. This separation will allow us to distinguish between uniform and non-uniform versions of our oracle inequalities. 3.1. Minimizing the unsquared penalized loss In this section, we present the results for the unsquared penalized loss. Choose k X k to be some norm of the d -dimensional Euclidean space. For  X  &gt; 0, define where  X  L M (  X  ) = k  X  A X   X   X  b k M . Our first result gives an oracle inequality for  X   X   X  as a function of  X  A and  X  b . Lemma 3.2. Consider  X   X   X  as defined in (6) . Then, The proof, which is attractively simple and thus ele-gant, is given in the appendix. The result suggests that the ideal choice for  X  is  X  A . Since  X  A is unknown, we use its upper bound to choose  X  . Depending on whether we allow  X  to depend on  X  or not, we get a non-uniform or uniform oracle inequality. In all cases, the rate in the oracle inequality will be fast. We start with the uniform version, non-exact version.
 Theorem 3.3. Let Assumption 3.1 hold and consider  X   X   X  as defined in (6) where  X  = s A . Then, for any 0 &lt;  X  &lt; 1 , w.p. at least 1  X   X  it holds that By allowing  X  to depend on  X  , we get an exact, non-uniform oracle inequality with a fast rate: Theorem 3.4. Let Assumption 3.1 hold. Fix 0 &lt;  X  &lt; 1 arbitrarily and choose  X   X   X  as defined in (6) with  X  = s A z A, X  . Then, w.p. at least 1  X   X  it holds that L Note that this bound is as tight as if we had first chosen  X  =  X  A and then applied the stochastic assumptions to obtain a high probability (h.p.) bound.
 When the linear system defined by ( A,b ) is consistent, L
M (  X   X  ) = 0. In this case one may prefer Theorem 3.3 to Theorem 3.4. Indeed, focusing on the behavior at  X   X  we get from Theorem 3.3 the bound s A z A, X  (1 + z
A, X  ) k  X   X  k + s b (1 + z A, X  ) z b, X  that holds w.p. 1  X   X  for any value of  X  , while from Theorem 3.4 we conclude the only for  X  0  X   X  . 3.2. Minimizing the squared penalized loss A more  X  X raditional X  estimator uses the square of the empirical loss function: To be able to handle Lasso-like procedures, we decided to avoid squaring the norm of  X  . Moreover, not squar-ing this term is convenient for the proof techniques we used. The extension of our results for other types of penalties, in particular k  X  k 2 , is left for future work. Unlike the previous case where the loss function and the norm were both unsquared, in this case the se-lection of the regularization parameter  X  will be more involved. In practice, one often uses a hold-out esti-mate to choose the best value of  X  amongst a finite number of candidates on an exponential grid. Here, we propose a procedure that avoids splitting the data, but uses the unsquared penalized loss with the same data. The new procedure is defined as follows. For some  X ,c &gt; 0 to be chosen later, let where  X (  X ,c ) . = 2 k  X  2 c X  : k  X  N and define We now have two parameters that need tuning. How-ever, as we will see, the tuning of these parameters is very similar to what we have seen in the previous sec-tion. The reason for this is that  X  is rich enough to contain a value  X  that makes  X  L M (  X   X   X  )+  X  k  X   X   X  k compara-ble to (not much larger than)  X  L M (  X  ) +  X  k  X  k no matter what  X  one selects. This is in fact the key to the proof of the following lemma, which gives a deterministic oracle inequality for  X   X   X ,c : Lemma 3.5. Let  X   X   X ,c be as in (9) . Then, L With the (unattainable) choice  X  =  X  A ,c =  X  b we get These choices are impractical but, as it happened with in the previous section, we can obtain uniform non-exact or non-uniform exact oracle inequalities with fast rates. The non-exact uniform oracle inequality is for-malized as follows: Theorem 3.6. Let Assumption 3.1 hold and choose  X   X   X ,c be as in (9) with  X  = s A and c = s b . Then, for any 0 &lt;  X  &lt; 1 w.p. at least 1  X   X  it holds that L The next theorem gives a non-uniform, exact oracle inequality with fast rates.
 Theorem 3.7. Let Assumption 3.1 hold. Fix 0 &lt;  X  &lt; 1 and choose  X   X   X ,c be as in (9) with  X  = s A z A, X  and c = s b z b, X  . Then, w.p. at least 1  X   X  it holds that
L M (  X   X   X ,c )  X  inf The relative merits of the uniform and non-uniform oracle inequalities are unchanged compared to what we have seen in the previous section. Let us now return to value-estimation in Markov Re-ward Processes. We consider the projected Bellman error objective, L M (  X  ) = k A X   X  b k M , where M = C  X  1 (for the definitions see Section 2). Assume that  X  A ,  X  b are concentrated as in Assumption 3.1, with known bounds. This can be arranged for example if the fea-tures  X  i ( X t ) and rewards R t +1 are a.s. bounded, and if we assume appropriate mixing, such as exponen-tial  X  -mixing (Yu, 1994), or when the Markov chain ( X t ) t  X  0 forgets its past sufficiently rapidly (Samson, 2000). Note that in these cases (  X  A,  X  b ) gets concen-trated around ( A,b ) at the usual parametric rate, i.e. , s ,s b = O ( p 1 /n ) and z A, X  ,z b, X  = O ( p ln(1 / X  )). For simplicity, assume first that C is given and con-sider the on-policy case . As mentioned previously, in this case the system A X  = b is guaranteed to have a solution and therefore L M (  X   X  ) = 0. Consider the es-timator that minimizes the unsquared penalized loss. Then, Theorem 3.3 shows a uniform fast rate when using  X  = s A : We get a similar inequality for the squared penalized loss using the result Theorem 3.6 with a slightly larger bound.
 In the off-policy case , the linear system A X  = b may not have a solution. When it does, the previous bound applies. However, when this linear system does not have a solution, to get an exact oracle inequality we are forced to choose  X  (in the case of minimizing the unsquared penalized loss) based on  X  . In particular, with the choice  X  = s A z A, X  , Theorem 3.4 gives L Again, this inequality gives fast, O ( p 1 /n ) rates when s ,s b = O ( p 1 /n ). Similar results hold for the proce-dure defined for the squared penalized loss where the bound is given by the inequality of Theorem 3.7. When C is unknown, one may resort replacing it by M 0. Then, a non-exact oracle inequality can be a matrix S , we denote by  X  max ( S ) , X  max ( S ) its largest and smallest singular values, respectively.) Consider first the unsquared penalized loss. In this case, k A X   X  b k that for an estimator  X   X  it holds that k A X   X  b k M inf  X  h k A X   X  b k M + c  X  A,  X  b (  X  ) i . Then, from k A X   X  b k  X  the  X  X onditioning number X  of M 1 / 2 CM 1 / 2 and  X  =  X  min ( M 1 / 2 CM 1 / 2 ). In the on-policy case, for exam-ple, this gives bounds of the form The bound for the off-policy case derived from (10) takes the form L Similar inequalities can be derived for our procedures that minimize the squared penalized loss.
 Finally, let us discuss the dependence of our bounds on the choice of the basis functions. This dependence comes through Assumption 3.1. As an example, as-sume that  X  i : X  X  [  X  1 , 1] and k X k = k X k p with 1  X  p  X  2. In this case, the bound on  X  A is expected to scale linearly with d , while  X  b is expected to scale linearly with linearly with d note that  X  A  X  k M 1 / 2 (  X  A  X  A ) k 2 , 2 k M 1 / 2 (  X  A  X  A ) k F , where k X k F denotes the Frobenius norm. Now, the Frobenius norm is the norm underly-ing the Hilbert-space of square matrices with the in-ner product  X  P,Q  X  = trace( P &gt; Q ) and thus an ap-plication of any concentration inequality for Hilbert-space valued random variables ( e.g. , (Steinwart and Christmann, 2008)) gives a bound that scales with the  X  X ange X  of N = k M 1 / 2 (  X  ( X t )  X   X  X  (  X  X t +1 ))  X  ( X Using the rotation property of trace, we get that can be bounded using the triangle inequality as a ( e.g. ,) that M is the identity matrix, we get that O (  X  The above bound on  X  A is naive; we believe using  X  d . E.g. , for d  X  d -matrices with i.i.d standard normal entries, the maximum eigenvalue is O ( 2010). Furthermore, note that if the basis functions are correlated, or if they are sparse, the dimension will not necessarily appear linearly in the bound ei-ther. For a discussion of when to expect a milder de-pendence of the norm of  X  on d , the interested reader may consult the paper by Maillard and Munos (2009). 4.1. Related work Antos et al. (2008) proved a uniform high-probability inequality both for the on-policy and the off-policy cases for LSTD. Their bound takes the form L M (  X   X  )  X  L
M (  X   X  ) = O d ln( d ) 1 n than the rate we are able to obtain. Further, with our bounding method the ln d factor can be removed from this bound.
 There are more results available for the on-policy case. As mentioned earlier, in this case the system A X  = b is consistent and thus our bound, under appropriate mixing conditions, takes the form where  X  . =  X  min ( M 1 2 CM 1 2 ), L is the worst-case norm of features in the dual norm ( L . = sup x  X  X  k  X  ( x ) k as discussed previously, L may be O ( worst-case bound on the norm of the parameter vector (i.e., k  X   X  k X  R ). In the next two results, the norm k X k is the 2-norm. Lazaric et al. (2010) for their (unregu-larized) path-wise LSTD method obtain (cf. Theorem 3 in their work). Although this is a fast rate, it also shares the undesirable dependence on 1  X  . Non-uniform, slow rates can be extracted from the paper by Ghavamzadeh et al. (2010) for LSTD with random projections. The result with our notation would look like (cf. Theorem 2) More recently, for the so-called Lasso-TD method, Ghavamzadeh et al. (2011) showed non-uniform error, i.e. , the empirical norm at the states used by the algorithm. These rates depend on the ` 1 -norm of  X   X  and have no dependence on the minimum eigenvalue, but they are slow in n . At the expense of additional assumptions on the Gram matrix  X  C (a sample estimate of C ), they have also derived fast rates. We have shown performance bounds for two estima-tors in linear inverse problems. Each of these mini-mizes one of L M (  X  ) and L 2 M (  X  ), plus a penalty  X  k  X  k . The penalty weight  X  can be chosen a priori without the need for a separate validation data set, and the bounds were presented in a general form that apply to many different instances of statistical linear inverse problems, requiring only that  X  A and  X  b concentrate around zero. Our split analysis, into a deterministic step and a stochastic step, allows us to decouple the behavior of  X  A ,  X  b from that of the estimators. We have recovered ` 1 -penalized variations of LSTD (Bradtke and Barto, 1996) for value function estima-tion in MRPs. We have shown fast, uniform rates, which, in the on-policy case, are exact and competitive with those existing in the literature. In the off-policy case, the rates are non-exact, and the non-uniform bound is also competitive with existing results. Finally, we would like to point out interesting ways to further develop our work. ` -penalties. The choice when the norm used in the penalty is the ` 1 -norm has been extensively stud-ied in the supervised learning literature (see, e.g. , (Bickel et al., 2009; Koltchinskii, 2011; B  X uhlmann and Geer, 2011) and the references therein), as well as in the reinforcement learning setting (Kolter and Ng, 2009; Ghavamzadeh et al., 2010; 2011; Maillard, 2011), mainly because it allows for non-trivial performance bounds even when the dimension d of the parameter vector is comparable to the sample size n (or even larger than n ) provided that the true parameter vec-tor is sparse ( i.e. , there are many zeroes in it). In this paper we decided not to specialize to this case but rather to focus on the problem of proving fast, exact and (possibly uniform) oracle inequalities. Our results, when applied to the case of an ` 1 -penalty show that in a way adding an ` 1 -penalty does not hurt performance (as we expect that the oracle inequalities with the said properties should hold for a decent method) even if the conditions ideal for the ` 1 -penalty do not hold. We do not know of performance bounds (ours included) for ` -penalized estimation have all of the characteristics we are after in a bound ( viz. bounds that are exact, fast and uniform).
 Linear regression. Our results are also worth inves-tigating in the context of linear regression. It is easy to cast regression as a statistical linear estimation prob-lem whose underlying system is always consistent. If we use k X k as the ` 1 -norm, we recover procedures sim-ilar to the square-root Lasso (Belloni et al., 2010) and the Lasso (Tibshirani, 1996) for the estimators studied in Sections 3.1 and 3.2, respectively. We believe that confronting the bounds that can be derived from our results with bounds for linear regression in the litera-ture can be very instructive.
 Connection to Inverse Problems. The theory of Inverse Problems is very pertinent to this work, and it is important to study our results under the light of those shown in Chapter 2 of Kirsch (2011); Alquier et al. (2011). The existing knowledge of inverse prob-lems may help us better understand which choices of k X k allow  X  A to concentrate around zero, and how fast this concentration occurs. The idea of having learn-ing problems as inverse problems is not new; Rosasco (2006); Vito et al. (2006) study regression in Hilbert spaces as an inverse problem.

