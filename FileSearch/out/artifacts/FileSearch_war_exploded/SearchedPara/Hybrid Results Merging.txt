 The problem of results merging in distributed information retrieval environments has been approached by two different directions in research. Estimation approaches attempt to calculate the relevance of the returned documents through ad-hoc methodologies (weighted score merging, regression etc) while download approaches, download all the docu-ments locally, partially or completely, in order to estimate  X  X irst hand X  their relevance. Both have their advantages and disadvantages. It is assumed that download algorithms are more effective but they are very expensive in terms of time and bandwidth. Estimation approaches on the other hand, usually rely on document relevance scores being returned by the remote collections in order to achieve maximum perfor-mance. In addition to that, regression algorithms, which have proved to be more effective than weighted scores merg-ing, rely on a significant number of overlap documents in order to function effectively, practically requiring multiple interactions with the remote collections. The new algorithm that is introduced reconciles the above two approaches, com-bining their strengths, while minimizing their weaknesses. It is based on downloading a limited, selected number of documents from the remote collections and estimating the relevance of the rest through regression methodologies. The proposed algorithm is tested in a variety of settings and its performance is found to be better than estimation ap-proaches, while approximating that of download.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Results Merging, Distributed Information Retrieval
Distributed Information Retrieval (DIR) [5] (also known as collection fusion [22] or federated search [16]) offers users the capability of simultaneously searching multiple remote document collections (i.e. search engines or specialized web sites) through a single interface. This process can be per-ceived as four separate but often interleaved sub-processes: Source representation [6, 15], in which surrogates of the available remote collections are created. Source selection [8, 15, 19], in which a subset of the available information collections is chosen to process the query. Query submis-sion, in which the query is submitted to the selected sources and results merging [10, 16, 18], in which the separate re-sults are combined into a single merged result list which is returned to the user.

The importance of DIR has particularly augmented in re-cent years as the prohibitive size and rate of growth of the web make it impossible to be indexed completely. More im-portantly, a large number of web sites, collectively known as invisible web [4, 20] are either not reachable by search en-gines or do not allow their content to be indexed by them, offering their own search capabilities. Studies [4] have indi-cated that the size of the invisible web may be 2-50 times the size of the web reachable by search engines.

The focus of this paper is on the last part of the dis-tributed information retrieval process, results merging. Pre-vious research [5, 10] has shown that the results merging phase is vital to the overall effectiveness of the retrieval pro-cess. Even if the most appropriate information sources have been chosen in the previous stages, if the merging isn X  X  effec-tive the overall quality of the retrieval process will deterio-rate. This importance is augmented particularly in the web environment where users rarely look past the top 20 results [12].

Algorithms that make use of regression have sustainably proved to be more effective than other estimation approaches [16, 18], such as weighted score merging. Their drawback is that they rely on a significant number of overlap documents in order to function effectively and make use of document relevance scores returned from the remote collections. The above facts make the utilization of regression methodologies at least problematic in realistic web environments.
On the other hand, download approaches download all the returned documents, completely or partially, in order to es-timate  X  X irst hand X  their relevance. In the context that they were tested [10], they proved to be more effective than esti-mation approaches but, to our knowledge, there hasn X  X  been a comparison between them and state-of-the-art estimation approaches. Their main disadvantages is the increased time and bandwidth overhead that they pose on the retrieval pro-cess.

The suggested algorithm presents an approach that com-bines the advantages of the above methodologies, while try-ing to minimize their drawbacks. It is based on a selective and limited download of documents, with the help of which it estimates the relevance of the rest. In addition to that, the algorithm doesn X  X  require document relevance scores from remote collections, making it practicable in realistic environ-ments where remote collections return only ranked lists of documents. Also, for the first time, a comparison of perfor-mance is conducted between download and state-of-the-art estimation approaches.

The rest of the paper is divided as follows. Section 2 re-ports on prior work. Section 3 describes the new methodol-ogy proposed in this paper. Section 4 describes the setup of the experiments conducted. Section 5 reports and discusses the results obtained and section 6 concludes the paper, sum-marizing the findings.
Significant research has been made in distributed infor-mation retrieval in recent years. Although most of the focus has been on source selection, significant progress has also been made in results merging.

The STARTS [11] initiative is an attempt to facilitate the task of querying multiple document sources through a com-monly agreed protocol. It provides a solution for acquiring resource descriptions in a cooperative environment and thus facilitates source selection and results merging. It requires that all remote collections transmit predefined information regarding their content both regularly and upon query time. Unfortunately, the protocol requires that all available infor-mation sources cooperate and does not guarantee that the information being transmitted is accurate, thus it is suscep-tible to manipulation. It is therefore most suited to envi-ronments where all available sources are administered by a single authority, such as small to medium sized corporate networks.

When cooperation from collections is not available (i.e. isolated environments ), techniques have been developed that allow for the estimation of their contents. Query-based sam-pling [6] creates samples of the collections through multiple one-term queries. Through them, statistics concerning the contents of the collections (such as terms, term frequencies, document frequencies etc) can be inferred. Estimation of the size of the collections is also possible through various techniques (i.e. sample-resample [15]).

Merging the result lists from individual collections is a complex problem not only because of the variety of retrieval algorithms that may be utilized at the remote collections, but also because of the diversity of individual corpus statis-tics. Voorhees in [22] was one of the first to conduct ex-periments in results merging. In that work two approaches where tested: one simple interleaving algorithm and a prob-abilistic biased c-faced die algorithm. The interleaving ap-proach is based on the assumption that all chosen collections have the same number of relevant documents and works by simply interleaving their results one by one. It was found to be highly ineffective since the above assumption is rather improbable in most environments. The biased c-faced die approach produced better results and was considered the most sophisticated technique that could be adopted in iso-lated environments in the absence of both sample collections and relevance scores.

In environments where the remote collections return not only ranked lists of documents but also document relevance scores, a variety of approaches have been proposed. Raw score merging merges the results as they are returned from the remote collections, but it was found to be ineffective [8] since it required that the scores are within a common range (i.e. between 0 and 1). The problem of incomparable scores was overcome by normalizing the returned scores at a common range. This approach produced better results, but the problem of different corpus statistics, eventually re-sulted in incomparable scores. For example, in a collection that is mainly about sports a document containing the term  X  X omputer X  will rank high if that term appears in the query, while the same document would rank lower in a computer science related collection.

Weighted scores merging overcomes the above issue by as-signing each document a score which is based both on the relevance of the document itself and the collection where it belongs. This way, high scoring documents from low scoring collections (as in the above example) rank lower than highly relevant scores from highly relevant collections. The CORI [8] results merging algorithm is such a weighted scores merg-ing algorithm and is considered state-of-the-art. The final score of each document is calculated as shown below: Equations (1) and (2) normalize the collection and document scores respectively to a range of 0 to 1 while equation (3) assigns the final relevance score to each document. Specif-ically, C i is the relevance of the collection according to the source selection algorithm, C min and C max are the minimum and maximum scores respectively that any collection can be assigned by the source selection algorithm and C  X  i is the nor-malized collection score. Similarly, D is the relevance score given to a document at the remote collection, D max and D are the maximum and minimum document scores that could be assigned by the collection and D  X  is the document nor-malized score. Note that D max and D min require cooperation from the remote collection to be set. In case when this co-operation is not available, they are set to the relevance score achieved by the most and least relevant document respec-tively. Finally, D  X  X  is the final document score, which again is normalized (thus the division by 1.4) between 0 and 1.
An attempt to estimate the probability of relevance of documents returned from remote collections by making use only of their ranking has been made in the past in [9], but it required an extensive training phase before it could be utilized successfully. In addition to that, the approach fol-lowed in that work created a single model for each collec-tion, regardless of the query being posed. The approach presented here builds a model on-the-fly for each collection in respect to the particular query, without requiring any training phase, thus making it practicable in volatile and rapidly evolving environments, such as the web.
Semi-supervised learning (SSL) [16] is a results merging algorithm that is based on linear regression. It takes advan-tage of a centralized sample index, comprised of all the sam-pled documents from the remote collections. By locating the matching documents between the centralized index and the remote collections, it estimates a linear regression model per collection between the two corresponding relevance scores. Utilizing the estimated model, it assigns scores to the rest unmatched documents. It was found to perform sustainably better than CORI in the majority of settings.

The disadvantages of the algorithm is that it relies heavily on the discovery of a significant number of matching docu-ments in order to estimate an accurate model and makes use of document relevance scores returned from remote col-lections in order to function effectively. Usually, 300-1000 documents are requested from each collection, in order for the algorithm to locate the needed documents. However, in most modern information retrieval environments, there is a limit of 100 results per page so obtaining enough train-ing data would require multiple interactions with the re-mote collection, increasing the time and bandwidth require-ments of the algorithm considerably. In addition to that, studies have indicated that users rarely go past the first 10-20 results [12], so under this context, requesting an ex-cessive number of documents can be regarded as a signifi-cant  X  X verkill X . On the other hand, the usage of document relevance scores from the remote collections limits the prac-ticality of the algorithm in semi-cooperative environments, where they are provided. In uncooperative environments, where only ranked lists of documents are returned without scores, the algorithm is forced to make assumptions about the mapping of rankings to scores that do not necessarily hold [2], thus having deteriorated effectiveness.
Last but not least, a number of approaches download on-the-fly, partially or completely, the returned documents in order to produce a final ranking [10]. The advantage of these methods is that they can locally estimate the rele-vance of documents and do not have to rely on the ranking or the scores that they received at the remote collections. In addition to that, they can function quite effectively with a limited amount of returned documents from collections (10-100). Their disadvantages are that they are very  X  X x-pensive X  in terms of time and bandwidth in order to down-load, index and score all the returned documents, even if the download is only partial.

The results merging problem is often confused with the metasearch problem, where a number of information re-trieval algorithms pose a query to a single document collec-tion or multiple similar collections and subsequently merge the results of the individual algorithms in one final list [13]. Most of the approaches under that context make use of the matching documents returned from the various sources in order to find the most relevant. They are based on the con-cept that the more a document appears at the returned lists of the individual algorithms, the more relevant it is. Tech-niques like COMBSUM or COMBMNZ [14] estimate the final score of a document as the sum of the scores obtained by individual document collections, or by multiplying this sum by the number of collections which had non-zero scores. The work presented in this paper differs from that work in that it assumes that the remote collections have no docu-ments in common (i.e. are non-intersecting), thus making the utilization of the above algorithms inappropriate.
The motivation behind the new results merging algorithm is to function effectively and efficiently in realistic informa-tion seeking web environments. It combines regression and download methodologies, taking advantage of the benefits of both, while trying to minimize their disadvantages. Instead of requesting an excessive number of documents from the remote collections or alternatively downloading every sin-gle document, it selectively downloads a limited number of documents that are used as training data for the regression model.

The goal of algorithm is to estimate on-the-fly a model for each collection, on a per-query basis, in order to map collection-specific rankings (the ranking that documents at-tain at the remote collection in respect to a particular query) to collection-independent relevance scores (relevance scores that documents would attain if the query had been posed on a single global index). It functions by considering the cen-tralized sample index, which is comprised of all the sampled documents indexed together, as a representative of the sin-gle global index that would be created if all the documents were available for indexing. The centralized sample index is therefore utilized as a  X  X eference statistics database X  [10], that provides estimates of global collection statistics (i.e. global idf), necessary for the estimation of document rele-vance scores. To our knowledge, this is the first time that the centralized sample index has been used in this fashion.
Additionally, the algorithm actively tries to maximize its effectiveness while at the same time minimizing any occur-ring efficiency issues. Considering that the most useful doc-uments from each collection are the ones usually returned at the top 10-20 ranks, a hypothesis that is later proved by the conducted experiments, it requests a limited amount of documents from the remote collections. Under this con-text, it is considered as granted that there won X  X  be enough training data discovered at the returned list in order to ef-ficiently train the regression model. Thus, it incorporates the decision of downloading documents into the algorithm itself, explicitly minimizing the produced overhead, while maximizing the gained accuracy.

One should also note that in the web environment, a list of 100 results is probably much more  X  X xpensive X  to view in terms of bandwidth than one with 10 or 20 documents. Likewise, in most cases it can be considered equally, or at least not overwhelmingly more, expensive to download two or three documents from the results list of the first 10 doc-uments than viewing 300 or more results from a remote col-lection, that would require multiple interactions with the remote collection. Therefore, the downloading of a limited number of documents can be considered as an acceptable so-lution that doesn X  X  pose an excessive overhead to the process in comparison to other existing approaches.
One of the main issues concerning the proposed algorithm is the fact that an appropriate model would have to be de-termined that provides an accurate mapping of ranking to relevance score. In previous approaches [2] it was assumed that this mapping is linear. Specifically, artificial scores were assigned to returned documents in an heuristic manner, giv-ing a score of 0.6 to the 1st ranked document and decreasing at equal intervals until assigning a score of 0.4 to the last.
In [9] a logistic function was indicated to provide an ac-curate mapping between rank and relevance. According to that work, the probability that document D i is relevant given its rank x i , is given by the equation: Figure 1 demonstrates the general expected correlation be-tween probability of relevance and ranking using a logistic function with various parameters.
 Figure 1: Graph demonstrating the expected corre-lation between rank (x) and probability of relevance (y) for various values of parameters a and b.

Two are the important properties of the logistic function that make it attractive to this model [17]. First of all, the results are always in the [0,1] area, as a probability of rel-evance should be. Secondly, it can produce a large variety of curves, by simply modifying a and b. This is particularly important since given a specific query and the number of relevant documents, the actual curve for each collection can vary significantly.
Influenced by the work in [9], we hypothesize that the correlation between the rank X of a document and relevance score Y is given by a logistic function: Applying the following transformations, we are able to mod-ify the above equation into a liner one: We will need to estimate the parameters a, b of the above model in order to estimate the S-curve for each collection, that will map collection-dependent rank X to collection-independent relevance score Y. Since equation (8) is a linear one, the estimation can be accomplished through linear re-gression.
Linear regression models are used when the relationship between two variables, dependent (y) and independent (x) or their transformation can be expressed with a linear function. The model can be formally stated as: where a and b are the parameters of the model and e is the observed error.

The aim of the model is to estimate the parameters a and b that minimize the error e which represents the dif-ference between the observed values of y and the ones es-timated through the model. The best way to accomplish this is through least-squares regression analysis. In partic-ular, the algorithm aims at minimizing the sum of squared residuals S: The problem can be formalized using matrix terminology: where The optimal solution for parameters a and b is the one that minimizes S in equation (10) and is given by:
The observations that are used for the above estimation remote collection of the i th downloaded document (see below on which documents are utilized for regression), y i is the relevance score of the document using the centralized sample index as a reference statistics database and n is the number of downloaded documents.
One of the most important aspects of the proposed algo-rithm is the integration of the download process as an indis-pensable part of the estimation of an accurate model. The aim of the process is to selectively download a limited num-ber of documents from the remote collections, maximizing the effectiveness of the model. Excessive download would negate the efficiency advantages of the algorithm, while a very limited one would create an inaccurate model.
Two were the main issues concerning the download deci-sion. First of all, the rankings of the documents that would be downloaded and therefore utilized for regression had to be determined. Since most of the relevant documents are expected to appear at the top ranks it was evident that the download process would have to start from the top ranks. A decision had to be made so that appropriate sample data would be produced. Two approaches were possible. We could either download documents at a steady rate r (at ranks n  X  r , where n  X  N  X  ) or at unequal intervals (i.e. at ranks n = 1 , 2 ,... ). The former choice produced a more uniform sampling space, so it was adopted.
In order to further minimize the download overhead, we also took advantage of any matching documents between the returned lists from the remote collections and the centralized sample index. Specifically, should any matching document be found at ranks: we didn X  X  download the document at rank n  X  r from the remote collection, but instead used the matching document (and its corresponding rank) for regression. In effect, we divided the rank space into segments and requested that at least one document represent each segment. This step helped in reducing the download overhead into a minimum. In the experiments that were conducted r was set to 3.
Secondly, a stopping rule had to be determined that would end the download process, when the model was deemed suf-ficiently accurate. An adaptive download process was de-signed that estimated how accurate was the produced model after each document was downloaded and decided whether to continue or cease the download. Since we were inter-ested at estimating the  X  X oodness of fit X  of the regression, the coefficient of determination R 2 provided the appropriate solution: where S S
E is the sum of squared errors (i.e. the squared sum of the differences between the observed values y i and the predicted values  X  y i ) and S S T is the total sum of squares (i.e. the squared sum of the differences between the observed values y i and their mean  X  y ). Equation (13) calculates the coefficient of determination of the fit. The closer the value is to 1, the better the model fits the data.

In order to avoid overfiting the curve to the top 1 or 2 doc-uments, the algorithm by default utilizes at least three top ranked documents (downloaded or pre-sampled) at the ranks determined by the above process and inserts an artificial document at rank 4  X  X  N | with relevance score 0.001, where | is the number of requested documents, effectively creating an extra tuple for regression at coordinates ( 4  X  X  N | , The artificial document additionally aims at better simulat-ing the decline at the end of the graph (fig. 1).
The documents are added to the centralized sample in-dex and their relevance scores are calculated. We used the inquery retrieval algorithm [7], but any effective algorithm would do. Next, the algorithm estimates the parameters of model (equation 5) through regression using the obtained data and calculates the R 2 value. Should that value exceed a threshold, the model is deemed good enough and no further download occurs. If not, the algorithm proceeds at down-loading the next document and re-estimates the R 2 value. The process continues until either the set threshold is ex-ceeded or a maximum of 5 documents is downloaded. The threshold was heuristically set to 0 . 95 , which produced a  X  X ood enough X  model, without being too demanding on the process.

Note that the downloaded documents can afterward either be kept at the centralized sample index, practically updating it, or be deleted. In the line of experiments that were con-ducted, the second approach was adopted so that previous queries would not effect the results of subsequent queries.
The above process is repeated for each collection chosen by the source selection algorithm. Applying equation (5) with the estimated parameters for each remote collection, the algorithm assigns relevance scores to all the documents returned based on their ranking.
We used a variety of testbeds to evaluate the proposed algorithm. The TREC123 and TREC4 testbeds [19] have been used extensively in distributed information retrieval experiments, so we briefly present them below: The advantages of the above testbeds is that they offer qualitative content in a way similar to authoritative web resources, such as hidden web or enterprise resources. In addition to that, each one of the collections in the testbeds contains a significant number of documents, effectively cre-ating nontrivial content-oriented clusters. Their drawback is that they are artificially made, not web-based and they both offer the same limited degree of distribution (100 col-lections). In order to better evaluate the proposed algorithm a more natural testbed had to be manufactured, ideally one that is both web-based and presents a more natural sep-aration of collections. We therefore introduce a new test collection, based on the WT10g [3]: The advantages of the last test collection are numerous. It is web-based, naturally divided into collections as they were created by their authors and offers a much greater distribu-tion than the standard trec collections. More details on the collections are provided in Table 1.
 Name Number of Collections Size in GB Num of Documents Min Max Avg Trec123 100 3.2 752 39.713 10.782 Trec4 100 2.0 301 82.727 5.675 WT10g 1.000 7.5 278 26.505 1.206
Details about the queries used for each test collection are provided in Table 2. An added advantage of the newly pro-posed test collection is that it has an average of 2 words per query, which is common for actual web queries [12].
In order to create representatives for the remote collec-tions, we used query-based sampling [6], sending 75 one-word queries and downloading the first 4 documents. We
Name Number Trec123 100 51-150 Title 3 Trec4 50 201-250 Description 7 WT10g 100 451-550 Title 2 used the CORI algorithm at the source selection stage, which has been used extensively in research and is one of the best known and better performing algorithms: d f is the number of docs in collection C i that contain term r , c f is the number of collections that contain term r k , is the number of terms in C i , avg cw is the average cw , | is the number of available collections and b is the default belief, set to the default value of 0.4.

An important factor that had to be considered was the in-formation retrieval algorithm that would be used at the re-mote collections. Two strategies were possible; we could ei-ther assume that all the remote collections employ the same algorithm, or that those differ. In order to make the ex-periments more realistic, the second approach was adopted. Three retrieval algorithms were implemented: inquery [7], kl divergence [23] and okapi [21] and were assigned to the remote collections in a round robin fashion. All the al-gorithms, including our own, were implemented using the Lemur Toolkit [1].

Although the environment that we are particularly inter-ested in, is a completely uncooperative one (i.e. the remote collections do not return documents relevance scores), in or-der to present the performance of CORI and SSL under the best possible light, we allowed them to make use of scores. It is generally expected that the performance of those al-gorithms in score-lacking environments, where estimates of documents relevance scores need to be made based on their ranking [2], will be lower than the performance reported here.

The Download approach downloads every returned docu-ment (except for those that were already sampled, in which case the sampled document was used) and scores them lo-cally, using the inquiry retrieval algorithm. The Central-ized Sample Index was again used as a  X  X eference statistics database X  in order to estimate global statistics. Neither the Download approach nor the Hybrid algorithm made any use of document relevance scores from the remote collections.
In distributed information retrieval environments it is usu-ally inefficient to retrieve all the relevant documents scat-tered in the remote collections. Especially, when the focus of the retrieval is on the results merging part of the process, the focus is on precision.
 Tables 3 to 5 report the results of the initial experiments. Each table refers to one testbed. The left column indicates the number of documents that are requested from each re-mote collection (10, 100 or 1000 documents per collection). We tested the Download and the new Hybrid approaches only at the first two settings. This decision was based on the assumption that for the Download approach it is highly inefficient to download 1000 documents from each collection while the new algorithm was explicitly designed to function effectively with a limited number of returned documents. We provide the last setting of 1000 documents mainly for comparison reasons with the estimation approaches, espe-cially SSL that cumulatively builds a more accurate model when there are more documents available.

One of the first things that can be noted, applicable to all the test collections, is that the performance of most al-gorithms remains ineffected by the gradual increase of re-turned documents from the remote collections. The find-ings support our initial hypothesis that most of the rele-vant documents are returned on the top 10 results and very few relevant documents are added to the final merged list thereafter. In addition to that, the excess documents may introduce noise to the final merged lists, effectively deterio-rating performance. In a realistic web environment it would potentially be much more beneficial to the retrieval process if more collections are added, potentially increasing the di-versity and completeness of the final merged list, instead of requesting an increasing number of documents from a same limited number of collections. The only exception is, as ex-pected, SSL whose performance generally increases as more documents are returned.

A second conclusion that is also common in all the test col-lections is that both of the approaches that incorporate some sort of downloading, Download or Hybrid, achieve perfor-mance that is persistently better than that of any estimation approach at most settings, with few exceptions. The above observation may imply that collection-dependent document relevance scores provide insufficient evidence to collection-independent document scores. Therefore, approaches that do not rely on such scores but take a more direct approach, such as downloading all or some, completely or partially (as it will be shown below) of the returned documents, thus not being susceptible to such estimation errors, are able to per-form more adequately regardless of the performance of the underlying remote collections.

The intent behind the new Hybrid algorithm is to ap-proximate the effectiveness of the Download approach, with as little overhead as possible. Clearly, the algorithm cannot avoid downloading some documents, which means that some overhead, even a limited, will be introduced to the retrieval process. What the Hybrid algorithm suggests is a limited compromise of efficiency, in exchange for a significant benefit in performance. The amount of introduced overhead will be studied subsequently, and a comparison with the Download approach will be presented.

In the first setting of the trec123 testbed (Table 3), where each collection returns 10 documents, the difference between the CORI -SSL and the Download -Hybrid groups is rather distinct and statistically significant, while the intra group differences aren X  X  (i.e. the difference between Download and Hybrid isn X  X  statistically significant). In lack of sufficient training data SSL performs worse than CORI, although not in a statistically significant manner. The performance of Docs/ Collection 10 Docs 100 Docs Table 3: Precision when 10 collections are selected at the trec123 testbed.
 CORI, Hybrid and Download remains mostly unchanged at the other settings, noting only small fluctuations, thus re-taining their statistical significance. SSL X  X  performance is surprisingly unstable, noting a decline in the second setting despite the additional documents and reaching a peak at the last. It is only at that setting, when the collections re-turn 1000 documents each, that the its performance comes near that of the Hybrid -Download group at the previous settings, but still doesn X  X  manage to surpass it despite the excessive amount of documents and training data.
 Docs/ Collection 10 Docs 100 Docs Table 4: Precision when 10 collections are selected in the trec4 testbed.

The trec4 testbed (Table 4) has been characterized as more difficult than the trec123, because of the heterogene-ity of the individual collections and the skewness of the word distributions. It is possibly because of this fact that the Download approach attains such a notable performance gain, especially at P@5, while it retreats closer to the per-formance of the Hybrid in the following ranks. The perfor-mance of the Hybrid at the first setting is again above the performance of the CORI -SSL group, in a statistically sig-nificant manner. Again, CORI performs better than SSL, a behavior that doesn X  X  persist in the second setting, where SSL X  X  performance increases (in comparison to the trec123 testbed). Again, the Hybrid algorithm outperforms the best estimation approach, although in a statistically significant manner only at P@5. At the last setting, the performance of SSL approximates very closely that of the Hybrid -Down-load approaches.
 Docs/ Collection 10 Docs 100 Docs Table 5: Precision when 100 collections are selected in the wt10g testbed.

The newly introduced WT10g-1000col-byUrl testbed (Ta-ble 5) presents very interesting challenges because of the level of distribution that it offers and the diversity of the sizes and topicality of the underlying collections. In order to have meaningful measurements, we decided to select 100 collections per query, which even though may seem excessive is at the same percentile scale as the previous testbeds (10% of the available collections). Early experiments showed that only few relevant documents were returned at the first 10 collections, making the comparison between the algorithms trivial. In this testbed, the performance differences between the CORI -SSL and the Hybrid -Download group are more profound in comparison to the previous testbeds, giving a possible hindsight about the actual performance of the ap-proaches in realistic web environments. In the first setting, where each collection returns 10 documents, the difference between the Hybrid algorithm and the best performing esti-mation algorithm varies around + 85% at any presicion mea-surement, a statistically signficant difference. In contrast, the difference between the Hybrid and the Download ap-proach are not statistically significant. At the second set-ting, the difference between the two groups is reduced, but still remains at significant levels. SSL X  X  performance at the last setting surprisingly remains at the same level as the sec-ond, possibly because the collections do not return as much as 1000 documents, thus not approaching the performance of the Hybrid algorithm at any setting.

Overall, the performance of the algorithms that incor-porate some sort of downloading of documents, Hybrid or Download, almost always exceeds that of estimation ap-proaches and in most cases with a statistically significant difference. It is only in the, unrealistic, case that each col-lection returns 1000 documents per query that SSL approx-imates that performance, but only rarely does it surpass it.
In this section, we will study the overhead that the Hybrid and the Download approaches introduce to the retrieval pro-cess. Thus, we counted the number of documents that each approach downloads at the above settings. As noted above, the Hybrid algorithm incorporates a progressive download criterion, while the Download approach downloads every re-turned document, unless it has already been sampled in the query-sampling phase. Results are presented on table 6.
As expected, when remote collections are requested to re-Docs/ Coll.
 10 Docs 100 Docs Table 6: Number of downloaded documents per col-lection per query. The left column indicates the number of requested documents from each collec-tion. turn 100 documents, the Download approach downloads an excessive number of documents, while the Hybrid algorithm utilizing the progressive download methodology described above, limits the number of downloads to a minimum. Tak-ing into consideration that the effectiveness gains in this setting are trivial when compared to the first setting where collections return 10 documents we will focus our attention to that setting, which is also more fair for comparison to the Download algorithm. The results are presented here only for completeness reasons.

When the remote collections return 10 documents each, the Download algorithm downloads on average 9.3 docu-ments in the trec123 testbed and 8.8 documents in the trec4 testbed. In comparison, the Hybrid algorithm downloads a maximum of 2.5 documents in either setting, noting a ef-ficiency gain of more than 70% in both cases. A question may arise here since as it was mentioned above, the Hybrid algorithm by default utlizes 3 documents from each collec-tion to avoid overfitting the curve to the top 1-2 documents. The answer lies in the usage of the already sampled docu-ments, which provide some of the necessary representatives of the divided rank space. In addition to that, in most cases the initially produced model was above the set R 2 thresh-old and therefore the algorithm didn X  X  resort to additional downloading.
 The new wt10g testbed presented some surprising results. The query-based sampling process in this testbed proved to be unexpectedly efficient, since the Download algorithm had to download only 5.4 documents on average per collec-tion. Again, the Hybrid algorithm proves to be much more efficient, resorting to downloading only 1 document per col-lection. The skewness of the size distribution of the col-lections may provide the explanation for the above results, potentially making the most the useful documents from the collections already available at the query-sampling phase, thus minimizing the needed downloading on query time.
The above results demonstrate the efficiency gains of the algorithm in comparison to the Download approach. Let it be noted, that in realistic web environments both the algo-rithms could download the cached version of the returned document which most search engines provide, instead of the actual one, thus reducing any delay in producing the final results. Still even in this case, the Hybrid approach remains a much more efficient solution, downloading on average only 2-3 documents per collection.
For the next line of experiments, we compared the perfor-mance of the Hybrid algorithm with partial download ap-proaches. Under this context, instead of downloading every returned document completely, only a part is downloaded, based on a size threshold. The above approach aims at laxing the efficiency constraints of the original Download approach, in which every returned document is completely downloaded. In parallel, we also tested the Hybrid algo-rithm under the partial download approach, in which the selected documents are also downloaded partially, in order to test its robustness.

Since the initial experiments showed that requesting more than 10 documents from the remote collections did not at-tribute to a significant increase in precision, we tested the al-gorithms only in this setting. Taking into consideration that the average size of a single trec document is roughly 6kbytes, two size thresholds were tested: 2kbytes and 3kbytes, which approximately translate into downloading one third and half of the returned documents respectively. Results for all the test collections are reported at Tables 7 to 9.
 Size Threshold Table 7: Precision when 10 collections are selected to return 10 documents each at the trec123 testbed. The left column indicates the size threshold for the downloaded documents.

Generally, the performance of both algorithms using par-tial downloading is decreased, but not in a significant man-ner. The performance of the Hybrid algorithm is always at par with the Download approach, as in the previous settings where documents are completely downloaded. We will fo-cus our attention with the comparison of the results of this setting to the ones produced by the estimation approaches (Tables 3 to 5) to examine whether the partial download-ing has a significant effect on the performance of the Hybrid algorithm or whether the conclusions drawn at the initial experiments are still valid in this setting.

In the trec123 testbed, when only 2kbytes from the se-lected documents are downloaded, the performance of the Hybrid remains above the CORI -SSL group (Table 3). It is only at the setting where each collection returns 1000 documents that the SSL manages to overpass the Hybrid approach, a result that doesn X  X  persist when the threshold is increased to 3kbytes.

It is worth making a point here concerning the bandwidth requirements of the algorithms. In this setting for example, the Hybrid algorithm roughly downloads 10 results lists of 10 documents each plus 75kbytes of documents (3kbytes per document  X  2.5 documents (on average, see Table 6)  X  10 col-lections). In comparison, the SSL algorithm needs to view 1000 results, which translate into 10 interactions with each remote collection, each interaction returning 100 results. Al-though the actual sizes of the results lists of search engines may vary from one to another, one can assume that the lat-ter method would be at least problematic in realistic web environments.

In the trec4 testbed, the conclusions are generally similar as the ones observed in the trec123 collection. Hybrid man-Size Threshold Table 8: Precision when 10 collections are selected to return 10 documents each at the trec4 testbed. The left column indicates the size threshold for the downloaded documents. ages to outperform the CORI -SSL group in all the settings (Table 4), with the exception of the last where collections return 1000 documents in which SSL performs better, but not in a statistically significant manner.
 Size Threshold Table 9: Precision when 100 collections are selected to return 10 documents each at the wt10g testbed. The left column indicates the size threshold for the downloaded documents.

Finally, in the wt10g testbed, the performance of the Hy-brid algorithm, even at the 2kbyte setting steadily remains well above that of any estimation algorithm. The differences increase when the size threshold is set to 3kbytes, but only marginally.

In general, the results demonstrate the robustness of the algorithms that incorporate a downloading process, even if documents are downloaded only partially. Additionally, the performance of the Hybrid algorithm steadily remains above that of estimation approaches in most settings, making it a very efficient and robust approach.
In this work, a new results merging algorithm for dis-tributed information retrieval environments was presented. The algorithm attempts to offer a hybrid solution to the two directions from which the problem has been approached in research, estimation and download , by combining their strengths and minimizing their drawbacks. It is based on downloading a limited number of selected documents from the remote collections with the help of which it creates a model for the estimation of the relevance of the rest through regression methodologies. In addition to that, the algorithm doesn X  X  make use of document relevance scores from remote collections, making it practicable in completely uncoopera-tive environments where they aren X  X  provided.

In the experiments that were conducted it was shown that the effectiveness of the proposed algorithm is almost always above the best of the estimation algorithms, while approximating that of download approaches. Even if the se-lected documents are partially downloaded, the impact on the performance of the algorithm isn X  X  significant, making it a robust solution in realistic web environments. It was also demonstrated that the new approach is much more efficient in terms of bandwidth overhead that download approaches, downloading on average 78% less documents.

Last, but not least, a new test collection for experiments in distributed information retrieval environments was pre-sented. The new testbed supplements the already existing testbeds by offering a new array of interesting challenges, based on the fact that it is web-based, naturally divided into collections, very skewed in size and topicality and offers a much greater level of distribution.
This paper is part of the 03ED404 research project, im-plemented within the framework of the  X  X einforcement Pro-gramme of Human Research Manpower X  (PENED) and co-financed by National and Community Funds (25% from the Greek Ministry of Development-General Secretariat of Re-search and Technology and 75% from E.U.-European Social Fund). [1] The lemur toolkit. www.lemurproject.org . [2] T. Avrahami, L. Yau, S. Luo, and J. Callan. The [3] P. Bailey, N. Craswell, and D. Hawking. Engineering a [4] M. Bergman. The deep web: Surfacing hidden value. [5] J. Callan. Distributed information retrieval. 2000. [6] J. Callan and M. Connell. Query-based sampling of [7] J. Callan, W. Croft, and S. Harding. Inquery retrieval [8] J. Callan, L. Zhihong, and W. Croft. Searching [9] Calve, A.L., and J. Savoy. Database merging strategy [10] N. Craswell, D. Hawking, and P. Thistlewaite. [11] L. Gravano, K. Chang, H. Garcia-Molina, and [12] B. Jansen, A. Spink, and T. Saracevic. Real life, real [13] M. Javed A. Aslam. Models for metasearch. Technical [14] J. Lee. Analyses of multiple evidence combination. [15] S. Luo and J. Callan. Relevant document distribution [16] S. Luo and J. Callan. A semisupervised learning [17] H. Nottelmann and N. Fuhr. From uncertain inference [18] G. Paltoglou, M. Salampasis, and M. Satratzemi. [19] A. Powell, J. French, J. Callan, M. Connell, and [20] S. Raghavan and H. Garcia-Molina. Crawling the [21] S. Robertson, S. Walker, H.-B. M., and G. M. Okapi [22] E. Voorhees, N. Gupta, and B. Johnson-Laird. The [23] C. Zhai and J. Lafferty. A study of smoothing
