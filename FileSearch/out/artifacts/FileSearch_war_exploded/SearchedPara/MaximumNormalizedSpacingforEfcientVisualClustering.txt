 In this paper , for efcient clustering of visual image data that have arbitrary mixture distrib utions, we propose a simple distance met-ric learning method called Maximum Normalized Spacing (MNS) which is a generalized principle based on Maximum Spacing [12] and Minimum Spanning Tree (MST). The proposed Normalized Spacing (NS) can be vie wed as a kind of adapti ve distance metric for conte xtual dissimilarity measure which tak es into account the local distrib ution of the data vectors. Image clustering is a dif cult task because there are multiple nonlinear manifolds embedded in the data space. Man y of the existing clustering methods often fail to learn the whole structure of the multiple manifolds and the y are usually not very effecti ve. Combining both the internal and exter -nal statistics of clusters to capture the density structure of mani-folds, MNS is capable of efcient and effecti ve solving the cluster -ing problem for the comple x multi-manifold datasets in arbitrary metric spaces. We apply this MNS method into the practical prob-lem of multi-vie w image clustering and obtain good results which are helpful for image bro wsing systems. Using the COIL-20 [19] and COIL-100 [18] multi-vie w image databases, our experimental results demonstrate the effecti veness of the proposed MNS cluster -ing method and this clustering method is more efcient than the traditional clustering methods.
 I.5.3 [ Patter n Recognition ]: Clustering X  Algorithms, Similarity Measur es ; E.1 [ Data Structur es ]: Graphs and Netw orks Algorithms Data Clustering, Distance Metric Learning, Data Mining
Clustering is the unsupervised learning of pattern grouping. It identies groups of data, such that data in the same group are simi-lar to each other , while data in dif ferent groups are dissimilar . Data clustering is an important kind of data analysis tasks. As a kind of data clustering, image clustering is a technique that associates each image in dataset with a class label such that the image associ-ated with the same label are similar to each other . Image clustering is practically very useful for image management systems because digital image datasets are gro wing explosi vely in both number and size due to the rapid popularization of digital cameras and mobile phone cameras in the last decade. These lar ge image collections re-quire automatic clustering to facilitate bro wsing, manipulation and sharing of images with image management systems.

The problem of data clustering has been studied for decades [11, 28] and it is an acti ve research eld in machine learning and data mining. As a kind of data types considered in the clustering re-search, image is very dif cult to be handled due to lack of under -standing on their intrinsic properties. Usually , there are multiple nonlinear manifolds embedded in the image data space. So image clustering is a kind of multi-manifold data clustering. In Figure 1, there are some toy data points which have embedded multiple manifolds. According to the density distrib ution, three nonlinear manifolds can be identied on the data points in Figure 1. Image data are often sho wn to reside on such nonlinear embedding. Tradi-tional clustering methods, such as K -means and Gaussian mixture model, often get poor results for multi-manifold data clustering. The reason wh y traditional methods failed is that the typical mix-ture of Gaussian distrib utions is dened on the Euclidean space, and hence it can not always describe the data points sampled from nonlinear manifolds.
 Figur e 1: A case wher e some toy data points ha ve multiple non-linear manif olds.
Some feature extraction methods, such as ISOMAP [25], LLE [22] and semidenite embedding [26], have been proposed for ex-tracting the nonlinear manifold structure for data analysis and vi-sualization. But it's dif cult to directly apply these feature extrac-tion methods to data clustering. A manifold clustering method [24] used MDS (multi-dimensional scaling) for data clustering. Another manifold clustering method [30] used ISOMAP for shape cluster -ing. Currently the kernel clustering [9] and spectral clustering [23, 27, 20, 5, 32] methods have attracted increasing attention due to their promising performance for comple x datasets. Ho we ver, ker-nel and spectral clustering are still not good enough for image clus-tering. In our opinion, the reason wh y spectral clustering is not very good is that it is not specially adapted to multi-manifold data clustering. Self-tuning spectral clustering [32] used local scaling to impro ve clustering accurac y. Instead of selecting a single scaling parameter  X  , local scaling parameter  X  i for each data point calculated in [32]. In [13], for spectral clustering, afnity func-tion with multiple local scaling parameter  X  has been proposed for impro ving the clustering performances. Spectral clustering was used for content-based image retrie val in [3]. Spectral clustering was used for hierarchical clustering of WWW image search results with visual, textual and link information in [1]. A method [16] used lossy data compression for segmenting data that have Gaus-sian mixture distrib utions. A method called afnity propag ation [10] has been proposed for data clustering. Af nity propag ation has some attracti ve properties but it is still not very good for multi-manifold data clustering because it can not capture the whole struc-ture of nonlinear manifolds. A method with metric and local linear structure [15] has been proposed for image clustering and it ob-tained good results on COIL-20 [19] and COIL-100 [18] image databases. But this method [15] focused on computing-intensi ve distance metric learning and it is not an efcient clustering method. A global geometric method [33] with geodesic distance learning has been proposed for image clustering and it has an experimental result on COIL-20 database.

For the multi-manifold data clustering, we propose a clustering method using a simple new clustering criterion called Maximum Normalized Spacing (MNS) which is a generalized principle based on Maximum Spacing [12]. Combining both the internal and ex-ternal statistics of clusters to capture the density structure of mani-folds, MNS can effecti ve and efcient solv e the clustering problem for the comple x multi-manifold datasets. With maximum normal-ized spacing, boundaries among manifolds can be identied and clustering is obtained by dividing the data points at the boundary of manifolds. So MNS is specially suitable for multi-manifold data clustering. MNS is dif ferent from the methods in [24, 30, 15, 33] because these methods mainly focused on computing-intensi ve dis-tance metric learning with nonlinear dimensionality reduction and geodesic distance while MNS is a simple and efcient clustering method for arbitrary metric spaces with very simple distance met-ric learning which isn' t computing-intensi ve. MNS method is ap-plied into the practical problem of image clustering and good re-sults which are helpful for image bro wsing systems are obtained. Our experimental results on the COIL-20 [19] and COIL-100 [18] image databases sho w that MNS method is consistently accurate, efcient and it has some adv antages over some of the state-of-the-art clustering methods.
According to [34], there are internal, external and combining cri-terion functions to be optimized by clustering methods. The inter -nal criterion functions focus on producing a clustering solution that optimizes a function dened only over the data within each cluster and does not tak e into account the data assigned to dif ferent clus-ters. The external criterion functions deri ve the clustering solution by focusing on optimizing a function that is based on how the vari-ous clusters are dif ferent from each other . The combining criterion functions focus on combining both internal and external charac-teristics of the clusters. The popular K -means algorithm uses an internal criterion function: where k is the number of clusters, S r is the dataset of the cluster , d i is the i -th data point, O r is the centroid vectors of cluster , and D ( d i ;d j ) is the distance between data points This internal criterion function does not tak e into account the dif-ferences among the data assigned to dif ferent clusters. The greedy nature of the K -means algorithm does not guarantee that it will con verge to a global minima, and the local minima solution it ob-tains depends on the particular set of seed data points that were selected during the initial clustering. In order to impro ve clustering accurac y, researchers try to nd some new clustering methods for real-w orld applications. The new clustering method afnity propa-gation [10] also has an internal criterion function: where c i is the exemplar (representati ve data point) of the current cluster in which data point i lies, s ( i;c i ) is the similarity of data point i to its exemplar c i and N is the number of data points of the dataset. s ( i;c i ) =  X  D ( i;c i ) in [10]. The afnity propag ation [10] is dif ferent from K -means because it tries to nd clusters' exem-plars instead of means and optimizes the internal criterion function (2) based on exemplars.

For meaningful clustering solutions, external criterion functions are hard to be dened [34]. Actually with an external criterion function, an old clustering method [31] used minimum spanning tree (MST) for data clustering. Using MST representation of the dataset, this MST clustering method [31] generates clusters by delet-ing the MST edges with the lar gest lengths. Gene expression data clustering [29] and image segmentation [8] were studied based on MST . Clustering of maximum spacing was dened in [12]. In [12], the  X spacing" is dened as the minimum distance between any pair of data points in dif ferent clusters. A similar concept was also de-ned as dif ference between two components in [8]. Clustering of maximum spacing [12] can be produced using this MST clustering method [31].
Combining both the internal and external statistics of clusters, maximum normalized spacing is a kind of partitional clustering methods. Partitional clustering and agglomerati ve clustering are two kinds of methods for hierarchical clustering. Contrary to the common belief, the experimental evaluation for document cluster -ing in [34] sho wn that partitional clustering methods always lead to better solutions than agglomerati ve clustering methods. We pro-pose maximum normalized spacing (MNS) for partitioning data points to form clusters. This clustering criterion is a generalized principle based on maximum spacing [12] and MST . For comple x multi-manifold datasets that have arbitrary mixture distrib utions, the clustering can be obtained through maximum normalized spac-ing with respect to local data density distrib utions.
According to [12], spacing is the minimum distance between any pair of data points in dif ferent clusters. The MST clustering method [31] has the follo wing criterion for maximizing spacing [12] maximize S = min where S denotes the spacing of a k -clustering problem (the clustering is to divide data points into k non-empty groups), and S r are two dif ferent clusters. D ( d i ;d j ) is the distance be-tween data points d i and d j . This is an external criterion function. As a result after optimizing the criterion function (3), the generated k clusters have maximum spacing which is denoted as SP ( k ) = max where T is the number of all possible solutions for a k -clustering problem on a dataset and S t is the spacing in the t -th clustering solution. S ( t ) q and S ( t ) r are two dif ferent clusters in the tering solution. On a dataset with N data points, the k clusters ( k = 2 ; 3 ;:::;N ) generated by the MST clustering method has maximum spacing SP ( k ) which is uniquely determined by the MST of the dataset because k  X  1 lar gest MST edges have been deleted. Therefore, for various values of k (the number of clusters to be generated), there are N  X  1 candidate spacings for a dataset with N data points. These N  X  1 candidate spac-ings ( SP (2) ;SP (3) ;:::;SP ( N )) can be identied by construct-ing MST of the dataset and each spacing SP ( k ) is equi valent to the length of the ( k  X  1) -th lar gest edge e k of the MST where L ( e k ) denotes the length of the edge e k of the MST , and d v are the two verte xes of the edge e k . So the N  X  1 ings ( SP (2) ;SP (3) ;:::;SP ( N ) ) are associated with the MST edges ( e 2 ;e 3 ;:::;e N ) one-to-one respecti vely through their relationships according to equation (6). For example, SP (2) sociated with the edge e 2 which is the MST edge with the lar gest length. The edge set f e k g has the non-increasing order according to the edge lengths The set f SP ( k ) g also has the non-increasing order
The MST clustering method is not very rob ust for some comple x datasets because its external criterion function (3) neglects the re-lationships among the data within each cluster . Simply breaking lar ge MST edges is inadequac y and it is sho wn in Figure 2 where some toy data points have been partitioned. In Figure 2, the MST clustering method just produces the bad partitioning and the good partitioning is be neglected because the internal statistics of clus-ters can not be considered by the maximum spacing. A modica-tion was made for this method in [6]. According to [6], inconsistent edges of MST should be deleted for generating clusters. In [6], an edge is inconsistent if its length is signicantly lar ger than the av-erage length of all other edges incident on its nodes. This modied method has impro ved the MST clustering method in some condi-tions. Ho we ver, as discussed in [6], this modied method is still Figur e 2: A case wher e some toy data points ha ve been parti-tioned. sensiti ve to local conditions and not accurate for some comple x sit-uations.

Combining both the internal and external vie ws of the cluster -ing process should be made for impro ving the clustering accurac y. Follo wing this direction, we dene normalized spacing NSP ( k ) as
NSP ( k ) = SP ( k ) where C k;a and C k;b ( k = 2 ; 3 ;:::;N ) are two connected com-ponents of MST and the y are both connected with the edge d a and d b are two verte xes of the edge e k . These two connected components are respecti vely constructed by greedily selecting smallest edges through the sequential forw ard selection [21] from the two sides of the edge e k . In the sequential forw ard selection [21], C k;a is gro wing from one side d a of the edge e k is gro wing from another side d b of the edge e k . In this gro wing process, the edges which are directly connected to the connected component are sorted and the smallest one is selected into the con-nected component. For example, the rst edge selected into is the smallest one among all the edges (except edge e k ) incident on verte x d a . This gro wing step is repeated until M edges are se-lected. The follo wing is the algorithm for constructing connected component C k;a ( C k;b is constructed in the same way):
C k;a and C k;b do not include the edge e k but the y both have at least one edge which is directly connected to the edge e k and C k;b are not connected with each other and the y both have edges: where Num ( C k;a ) denotes the number of edges in C k;a and is the number of data points of the whole dataset. C k;a and represent two denser neighborhoods beside the edge e k and the y are used for measuring local density information. The connected component C k;a tries to optimize the follo wing criterion function For equation (11), C k;a is not the exact solution and it's an approx-imate solution which is obtained through the sequential forw ard selection [21]. This problem is similar to the K-minimum span-ning tree (K-MST) problem and the K-MST problem is kno wn to be NP-complete.

As sho wn in equation (9), the normalized spacing NSP ( k ) computed as the sum of two fractions which are ratios of the spac-ing SP ( k ) to the maximum spacing among its neighborhood spac-ings. The normalized spacing is relati ve to its neighborhood den-sity and dif ferent from the original spacing [12] which is absolute on the whole. Therefore, both the internal and external statistics of clusters can be measured by normalized spacing which is adapted to multi-manifold data.

The normalized spacing is inspired by a pre vious work called min-max cut [5]. For k -clustering problem, min-max cut [5] is dened as where S r is the set of data in a cluster and S  X  S r is the set of the rest of data in the dataset. The style of min-max cut for combining both the internal and external vie ws of the clustering process is sim-ilar to that of normalized spacing but normalized spacing is simpler and easier to be computed than min-max cut because normalized spacing is computed on MST instead of complete graph. The nor -malized cut [23] also used the similar combining style. Computed on complete graph, min-max cut problem is NP-complete because of its combinatory nature. Computed on MST , efcient greedy al-gorithm can be used for normalized spacing.

Simple new clustering criterion, maximum normalized spacing, is dened as follo ws This is a combining criterion function. This new criterion tries to maximize the external spacing while simultaneously minimize in-ternal sparsity of clusters. Both the internal and external vie ws of the clustering process are considered by this new criterion func-tion. This new clustering criterion is a generalized principle based on maximum spacing [12]. There are N  X  1 normalized spac-ing ( NSP (2) ;NSP (3) ;:::;NSP ( N ) ) for a dataset with points. We can associate the N  X  1 normalized spacing with the N  X  1 MST edges ( e 2 ;e 3 ;:::;e N ) one-to-one respec-tively . Therefore, it can be seen that an edge e k of MST has an as-sociated normalized spacing NSP ( k ) . According to the criterion function (13), MNS clustering method generates clusters by delet-ing the MST edge e i which is associated with the maximum nor -malized spacing NSP ( i ) . MNS method can be used to compute hierarchical clustering solutions using repeated cluster bisection-ing. In MNS, all the data points are initially partitioned into two clusters with maximum normalized spacing. Then, one of these clusters containing more data points than others is selected and is further bisected according to the maximum normalized spacing. This process continues k  X  1 times and produces k clusters with maximum normalized spacing. In addition, being dif ferent from [12] in which Kruskal' s algorithm was used, Prim' s algorithm is used for constructing MST of the dataset for MNS because MNS is a graph-breaking method and not a graph-gro wing method for which Kruskal' s algorithm is used.
For determining the number of clusters, the method via coding length [16] should be considered. In [16], the coding length func-tion subject to the squared error " 2 for encoding the m vectors in W  X  R n from a Gaussian distrib ution is For nonlinear manifold data that have arbitrary mixture distrib u-tions, we replace matrix WW T with the kernel Gram matrix and modify the coding length function (14) as where K is the kernel Gram matrix with element K ij For a given distance metric D ( w i ;w j ) , such as geodesic distance [25] or the  X  2 distance: kernel k ( w i ;w j ) can be computed as: k ( w i ;w j ) = 1 where w o is the centroid vector which can be easily found. Cen-tering matrix [25] also can be used to con vert distances to inner products for Gram matrix. The descent of coding length [16] for bisectioning S 1 [ S 2 into S 1 and S 2 is where
L s ( S 1 ;S 2 ;:::;S k ) : = For nishing the clustering, the cluster bisectioning step can be au-tomatically stopped when H  X  0 according to the equation (19) . For efcienc y, this computing on equation (19) can only be made when the bisectioning steps are near nishing clustering because this computing is expensi ve.
The main computationally expensi ve step in MNS is the com-putation of MST . It tak es O ( N 2 ) time for constructing MST us-ing Prim' s algorithm for MNS. While some new algorithms have been proposed for constructing MST , such as Bernard Chazelle' s soft heap [2], Prim' s algorithm is still suitable for us because our problem is based on complete graph. For lar ge databases, MNS is still computationally expensi ve. To speed up clustering process for lar ge clustering problems, we use the canopies [17] as divide-and-conquer strate gy for constructing MST for MNS. Using overlap-ping canopies for very lar ge databases, we can construct approx-imate MST (AMST) for MNS through modular hierarchies and parallel computing. Lar ge clustering problems that were formerly impossible become practical through AMST and MNS. This reduc-tion in computational cost comes without much loss in clustering accurac y.
 Figur e 3: In four overlapping canopies, four MSTs are con-structed respecti vely using Prim' s algorithm on some toy data points.

AMST is constructed based on overlapping canopies. According to [17], start creating canopies with a list of the data points in any order , and with two distance thresholds, T 1 and T 2 , where Pick a data point randomly from the list and measure its distance to all other data. Put all data that are within distance threshold into a canop y. Remo ve from the list all data that are within distance threshold T 2 . Repeat until the list is empty and canopies are created on the dataset. In each canop y, MST is constructed respecti vely us-ing Prim' s algorithm. At last, all MSTs in dif ferent canopies are mer ged to form a single lar ge AMST . Dif ferent MSTs are con-nected by the smallest edges using Kruskal' s algorithm based on overlapping data points between dif ferent canopies. For example, the overlapping data points between two canopies have to be con-nected by the smallest edge using Kruskal' s algorithm to mer ge the two MSTs inside these two canopies. It's interesting that both Prim' s and Kruskal' s algorithms are used for constructing AMST . An example is sho wn in Figure 3 and four small MSTs are con-structed respecti vely in four overlapping canopies using Prim' s al-gorithm. In Figure 4, these four small MSTs are mer ged to form a single lar ge AMST . The red edges have connected the four MSTs to form a single spanning tree using Kruskal' s algorithms in Figure 4. AMST can be constructed by parallel computing and this comput-ing can be based on the efcient systems of Google' s MapReduce [4]. MNS can speed up for lar ge databases based on the efcient computing of AMST through the parallel computing of MapRe-duce.

We use the AMST as data graph which is a data structure for data inde xing and organization. we use the graph shrinking method to inde x the data points. As sho wn in Figure 5 and Figure 6, the end points of the data graph have been found and remo ved from the graph. This remo ving action is called graph shrinking. For the shrink graph, some former normal points become end points and these new end points can also be remo ved. So the graph shrinking process can be repeated man y times and all the points in the data graph can be remo ved. As a result, all the data points can be rank ed based on the remo ving sequence. So hierarchical data structure can Figur e 4: Four MSTs are connected to form a single lar ge ap-proximate MST (AMST) using Kruskal' s algorithm. be constructed in this way. This hierarchical data structure mak es the clustering process very efcient.
 Figur e 5: The end points (red points) of the data graph ha ve been found.
In this section, we report our experimental results. To evaluate the performance of MNS, we compare MNS with some state-of-the-art clustering methods. We use the COIL multi-vie w image databases of Columbia which are popular databases for 3D object recognition problems. There are two COIL databases, COIL-20 [19] and COIL-100 [18]. The y contain 20 and 100 objects, respec-tively . For both databases, the images of each object were tak en 5 degrees apart as the object is rotated on a turntable and each object has 72 images. There are 1440 images in COIL-20 and 7200 im-ages in COIL-100. In Figure 7, some example images of 20 objects in COIL-20 database are sho wn. In Figure 8, some example multi-vie w images of a object in COIL-20 database are sho wn. In Figure 9, some example images of 100 objects in COIL-100 database are sho wn. We sub-sampled the image collections because the sam-pling in COIL databases is very dense. We let COIL20.2 denote Figur e 6: After graph shrinking, the new end points (yello w points) of the data graph ha ve been found. the collection of images obtained from COIL-20 by sub-sampling it with a factor of 2. So COIL20.2 contains the same number of objects as the original COIL-20 but with half as man y images per object. Similarly , COIL20.4 denotes the collection obtained from COIL-20 by sub-sampling it with a factor of 4 and so on. All ex-periments were made on PC of Intel Core Duo 2 GHz CPU with 2 GB RAM.
 Figur e 7: Example images of 20 objects in COIL-20 database.
We use an image texture feature called LPCIH (local patterns constrained image histograms) [7] as the image feature for image clustering in our experiments. As in [7], we use the follo wing im-age distance metric This distance metric is similar to the distance of Chi square statistic (  X 
We use clustering accurac y to measure clustering performance as follo ws: Figur e 8: Some example multi-view images of a object in COIL-20 database.
 Figur e 9: Some example images of 100 objects in COIL-100 database. where N is the number of data points in the dataset,  X  ( u;v ) delta function that equals one if u = v and otherwise equals zero, y and r i are the groundtruch label and obtained cluster label re-specti vely , and map ( r i ) is a function to map the cluster label to the ground truth label. In our experiments, the map function map ( r is chosen to map the cluster label to the majority groundtruth label of the each estimated cluster .
 Figur e 10: Our demo softwar e loads the image set from COIL-20 database.

On COIL-20 and COIL-100 databases, the results of clustering accuracies of our algorithm and other three existing algorithms are reported in Table 1. We compare our algorithm with the methods in [15] and in [33]. In Table 1, the MST method denotes the MST clustering method [31]. We cite Lim' s experimental results in [15] and Zhang' s experimental results in [33] for comparing in Table 1. According the experimental results sho wn in Table 1, we can Figur e 11: Our demo softwar e sho ws the visual clustering re-sult for COIL-20 database. see that MNS algorithm is more accurate comparing with the other three existing methods. In Figure 10 and Figure 11, our demo soft-ware sho ws the visual results of image clustering based on COIL-20 database. Figur e 12: Clustering accuracy on COIL-20 database using MNS method.
 In Figure 12, clustering accurac y on COIL-20 database using MNS method is sho wn. In Figure 12 , the horizontal coordinate M denotes the Num ( C k;a ) in equation 10. In Figure 13, cluster -ing accurac y on COIL20.2 database using MNS method is sho wn. From the results sho wn in Figure 12 and 13 , we can see that the performances of MNS are rob ust with respect to the changes of the parameter M . Figur e 13: Clustering accuracy on COIL20.2 database using MNS method.
 In Figure 14, clustering accuracies on COIL-20 database using MNS method and other three existing methods are sho wn. In Fig-ure 15, CPU times of the clustering on COIL-20 database using MNS method and other three existing methods are sho wn. In Fig-ure 14 and 15, the horizontal coordinate K denotes the number of the clusters, X SC" denotes the spectral clustering [23], X  AP" denotes the afnity propag ation [10], and  X KM" denotes K -means. In Figure 16, clustering accuracies on COIL20.2 database using MNS method and other three existing methods are sho wn. In Fig-ure 17, CPU times of the clustering on COIL20.2 database using MNS method and other three existing methods are sho wn. In Fig-ure 14, 15, 16, and 17, we can see that the performances of MNS are rob ust comparing with the three existing methods (spectral clus-tering, afnity propag ation and K -means).

In Figure 18, clustering results produced by MNS on a small subset of the COREL [14] database are sho wn. These clustering results are used by our demo system of image bro wsing. We can see that such image clustering is helpful for bro wsing a lar ge amount of images in photo alb ums and photo databases. We propose a simple new clustering criterion called Maximum Normalized Spacing (MNS). This new clustering criterion is a gen-eralized principle based on Maximum Spacing [12] for cluster -ing. Combining both the internal and external statistics of clusters, MNS method is effecti ve and efcient for the multi-manifold data clustering. MNS can speed up for lar ge databases using divide-and-conquer strate gy. MNS is dif ferent from the methods in [24, 30, 15, 33] because these methods mainly focused on distance met-ric learning with nonlinear dimensionality reduction and geodesic distance while MNS is a direct and simple clustering method for arbitrary metric spaces without distance metric learning. So MNS Figur e 14: Clustering accuracies on COIL-20 database using MNS method and other thr ee existing methods. Figur e 15: CPU times of the clustering on COIL-20 database using MNS method and other thr ee existing methods. is con venient to use for real-w orld applications. MNS method is applied into the practical problem of image clustering in this paper and good results which are helpful for our demo system of image bro wsing are obtained. MNS can be used for man y elds of real-world. For example, MNS is useful for image retrie val and image sharing on the social netw orking websites. Our experimental re-sults sho w that MNS method is consistently accurate, efcient and it has some adv antages over some of the state-of-the-art clustering methods. [1] D. Cai, X. He, Z. Li, W. Y. Ma, and J. R. Wen. Hierarchical [2] B. Chazelle. A minimum spanning tree algorithm with [3] Y. Chen, J. Z. Wang, and R. Kro vetz. Clue: cluster -based [4] J. Dean and S. Ghema wat. Mapreduce: simplied data Figur e 16: Clustering accuracies on COIL20.2 database using MNS method and other thr ee existing methods. Figur e 17: CPU times of the clustering on COIL20.2 database using MNS method and other thr ee existing methods. [5] C. Ding, X. He, H. Zha, M. Gu, and H. D. Simon. A [6] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern [7] Z. G. Fan, J. Li, B. Wu, and Y. Wu. Local patterns [8] P. F. Felzenszw alb and D. P. Huttenlocher . Efcient [9] M. Filippone, F. Camastra, F. Masulli, and S. Ro vetta. A [10] B. J. Fre y and D. Dueck. Clustering by passing messages [11] A. K. Jain, M. N. Murty , and P. J. Flynn. Data clustering: a [12] J. Kleinber g and E. Tardos. Algorithm design . Addison [13] A. Le vin, D. Lischinski, and Y. Weiss. A closed form [14] J. Li and J. Z. Wang. Automatic linguistic inde xing of [15] J. Lim, J. Ho, M. Yang, K. Lee, and D. Krie gman. Image [16] Y. Ma, H. Derksen, W. Hong, and J. Wright. Segmentation of [17] A. McCallum, K. Nig am, and L. Ung ar. Efcient clustering [18] S. A. Nene, S. K. Nayar , and H. Murase. Columbia object [19] S. A. Nene, S. K. Nayar , and H. Murase. Columbia object [20] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: [21] P. Pudil, J. No vovico va, and J. Kittler . Floating search [22] S. Ro weis and L. K. Saul. Nonlinear dimensionality [23] J. Shi and J. Malik. Normalized cuts and image [24] R. Souv enir and R. Pless. Manifold clustering. ICCV 2005 , [25] J. Tenenbaum, V. de Silv a, and J. Langford. A global [26] K. Q. Weinber ger and L. K. Saul. Unsupervised learning of [27] Y. Weiss. Segmentation using eigen vectors: a unifying vie w. [28] R. Xu and D. Wunsch. Surv ey of clustering algorithms. IEEE [29] Y. Xu, V. Olman, and D. Xu. Clustering gene expression data [30] D. Yank ov and E. Keogh. Manifold clustering of shapes. [31] C. T. Zahn. Graph-theoretical methods for detecting and [32] L. Zelnik-Manor and P. Perona. Self-tuning spectral [33] S. Zhang, C. Shi, Z. Zhang, and Z. Shi. A global geometric [34] Y. Zhao and G. Karypis. Ev aluation of hierarchical clustering Figur e 18: Clustering results produced by MNS on a small sub-set of the COREL [14] image database are used in our demo system.
