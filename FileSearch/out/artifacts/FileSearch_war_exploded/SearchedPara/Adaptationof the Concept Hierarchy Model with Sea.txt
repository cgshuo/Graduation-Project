 A concept hierarchy created from a document collection can be used for query recommendation on Intranets by ranking terms according to the strength of their links to the query within the hierarchy. A major limitation is that this model produces the same recommendations for identical queries and rebuilding it from scratch periodically can be extremely inefficient due to the high computational costs. We pro-pose to adapt the model by incorporating query refinements from search logs. Our intuition is that the concept hierarchy built from the collection and the search logs provide com-plementary conceptual views on the same search domain, and their integration should continually improve the effec-tiveness of recommended terms. Two adaptation approaches using query logs with and without click information are com-pared. We evaluate the concept hierarchy models (static and adapted versions) built from the Intranet collections of two academic institutions and compare them with a state-of-the-art log-based query recommender, the Query Flow Graph, built from the same logs. Our adaptive model significantly outperforms its static version and the query flow graph when tested over a period of time on data (documents and search logs) from two institutions X  Intranets.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval Log Analysis, Query Suggestions, Concept Hierarchy
The need to assist a user in finding relevant information has led to continuous changes in the functionalities of search engines. Prominent internet search engines now provide user-friendly features such as query auto-completion, query refinement lists and visual representation of related concepts to the query in addition to the typical ranked list of retrieved documents (e.g. Google, Bing and Yahoo! Web search en-gines). These extra features are expected to make it easier and quicker for a user X  X  information need to be satisfied. We focus on the techniques used for generating suitable re-finements to a current (or an initial) query, known as query recommendation , which is one of the interactive features in modern search engines.

Although lots of models have been proposed for recom-mending query refinement terms, most have exploited the document collection or previous search logs but not both. Our intuition is that these two approaches to query recom-mendation provide complementary conceptual views on the same search domain. Recommendations from the collection typically discover relationships between terms based on sta-tistical analysis giving a conceptual view of the domain. On the other hand, the search logs provide a conceptual view of the domain based on collective user intelligence. Each approach also has its inherent weakness. For instance, the search logs approach might not be able to give any sug-gestions for new queries that were never used in the past. On the other hand, the document collection approach might give too many unsuitable suggestions for current queries that have a very high document frequency leading to a statisti-cal relationship with many of the terms in the collection. Therefore, our hypothesis is that the integration of the two approaches will improve the query recommendation quality and in turn the retrieval performance. A conceptual view of the search domain would be a natural bridge and common foundation for the two approaches.

Specifically, we propose the integration of these two ap-proaches by continuously adapting the concept hierarchy model built from a document collection with user interac-tions derived from query logs. The concept subsumption h ierarchy [24] is a good choice as a simple but effective ap-proach to query recommendation. Our research focuses on Intranet search which (similar to Enterprise search) is dif-ferent from Web search [15]. The document collection on an Intranet is relatively small and changes less frequently. It is thus feasible to build a domain model to reflect the con-ceptual structure within the collection. In this scenario, the topic structure generated by the concept hierarchy might map onto a pre-defined structure in the organisation. For example, a concept hierarchy model generated from docu-ments in an academic institution X  X  Intranet might show the relationship between lecturers and the courses they teach, research students and their supervisors, or academic staff and their research interests. On an Intranet, it can also be expected that identical queries are more likely to be related to the same user information need since the context of search is more limited than in Internet search. Therefore, previous user interactions can be used to improve a new user search by suggesting query refinement terms from a previous ses-sion with a similar query. Query terms used in the same session also implies that they are related and can be used to improve query recommendation models.

In this paper, we propose a novel method to adapt the model with search logs and compare with its static equiva-lent as a baseline which does not incorporate any previous user interactions. We also compare these hierarchy mod-els (static and adaptive) with a state-of-the-art adaptive query recommendation approach called the query flow graph model [6] which is built solely from search logs. We test the effectiveness of these models using an automated evaluation based on query log data by experimenting with search logs obtained from two academic institutions X  search engines.
Section 2 discusses related work on query recommenda-tion and utilization of search logs to improve information seeking. We then provide in-depth details of the concept hi-erarchy model, how we utilise it for generating query refine-ment terms and our proposed method for adaptation with search logs in Sections 3, 4 and 5. Section 6 explains our ex-perimental set-up while evaluation results are discussed in Section 7. We conclude the paper in Section 8 by outlining our main contributions and plans for future work.
Recommendation of refinements to queries is a common feature of modern search engines, both general and spe-cialised ones. Determining the best terms to recommend remains a challenge as this depends on the combination of several factors. However, studies have indicated that users prefer having suggestions regardless of their usefulness [23, 26, 27]. Knowledge of the query X  X  context, which can be a function of the user X  X  knowledge about the domain, previ-ous search queries, etc., is a crucial factor when suggesting terms that help a user find specific information faster. The two main resources available for query recommendation are the document collection (including anchor logs) [28, 24, 10, 18, 20, 13] and search logs [3, 19, 12, 4, 17, 6, 5, 7, 8], which can also be used as forms of implicit or explicit feedback to re-rank retrieved documents.

The approaches that utilise the collection discover rela-tionships between terms using all documents (global) [9, 13] or those retrieved as relevant to a query (local) [24, 10]. Terms considered as related to the query are then recom-mended for query refinement or automatically used for query expansion. These techniques are therefore less dependent on any user feedback (implicit or explicit) but require updates to their recommendation models when there is a significant change in the underlying collection if their suggestions are mined from all documents. This method is perhaps most useful for new search engines with little or no query logs.
Query search logs, on the other hand, provide real user ex-perience which can be mined to discover useful local or global patterns. Query recommendations can be said to be local if they are based only on each user X  X  previous searches [19], while global recommendations are derived from a group of users or all users [6, 8]. The main drawback is that logs have to be collected over a period of time before such techniques can be applied since data from one search engine might not be easily applicable to others, especially for Intranets.
Mining post-query click behaviour has been studied and applied in information retrieval tasks. For example, using landing page information to derive query suggestions [12] and mining user search trails for search result ranking [26], where the presence of a page on a trail increases its query rel-evance. Click graphs were also used to derive labels to short-cut search trails to help users reach target pages efficiently [25]. Since users are typically reluctant to provide explicit feedback on the usefulness of ranked results returned for a search query, automatic extraction of implicit feedback has continued to be exploited by researchers. Click-through data is one form of implicit feedback left by users, which can be used to learn the retrieval ranking function [17]. Queries and clicks can be interpreted as  X  X oft relevance judgements X  [11] to find out the user X  X  actual intention and interests. Query recommendations can then be derived, for example, by look-ing at submitted queries and building query flow graphs [6, 7], query-click graphs [11], cover graphs [4] or association rules [14]. Mining query logs has also been combined with query similarity measures to derive query modifications [19].
The Query flow graph model (QFG) [6] has been success-fully applied to mine query suggestions from query logs. It is a directed graph, which contains a set of nodes consisting of all distinct queries submitted to a search engine, while the edges between two nodes represent a user query refinement. Edges are weighted primarily based on the frequency of such refinements, though the actual weighting function depends on the application. Recommendations can be made for a query if it is found in any of the QFG nodes by obtaining terms at the end of all edges that emerge from the query node. These terms can then be ranked based on the weights of the edges leading to them from the query node. In this paper, we compare the concept hierarchy used in our work to QFG since they are both directed graph structures but one is typically built from a document collection while the other is built from query logs.
The concept hierarchy model [24] was initially introduced to assist users to browse the top documents returned by a search engine by showing the topic structure in the re-trieved documents. A hierarchical tree, where specialised concepts or terms are subsumed by more generic ones, is gen-erated automatically using an unsupervised algorithm based on a statistical co-occurrence measure. Hence, we adopt the acronym SHReC ( S ubsumption H ierarchy for Re sult C l ustering) of an existing implementation 1 and use this to refer to the concept hierarchy model in this paper. Although SHReC was introduced for clustering retrieval results, it can easily be applied to an Intranet collection. This model can be utilised for suggesting terms for query refinements (addi-tion or replacement) by showing a sub-tree from the initial hierarchy encapsulating the given query as used in [18]. A more intuitive way to utilise the model for query refinement is to transform terms in the sub-tree encapsulating the query into a ranked list based on the strength/closeness of their links to the query in the SHReC graph.
The SHReC model is automatically derived as a hierarchi-cal organization of concepts from a set of documents without using training data or standard clustering techniques [24]. The basic idea is to use term co-occurrence to create a sub-sumption hierarchical tree. The generality or specificity of a term (or concept) in the model is determined mainly by its document frequency. The more documents a term appears in, the more general it is assumed to be. The conditions for subsumption are therefore dependent on document frequen-cies. It should be noted that a term as used in this paper for the SHReC model is the equivalent of a concept and does not necessarily mean a single keyword. A term could be an entire query which can be formulated as a word, keyword, phrase or n-grams. A term  X  X  X  is said to subsume  X  X  X  when Equation 1 is met; where is the strength of subsumption.
The relationship between  X  X  X  and  X  X  X  are derived from statistical probabilities of their co-occurrence in the collec-tion. Operationally, if df is a function that returns the document frequency of a given term, df ( x ) &gt; df ( y ) and df ( x ^ y ) =df ( y ) . Although = 1 is ideal for a complete subsumption, a slightly smaller value can be chosen in order to allow few occurrences of term  X  X  X  with other terms other than  X  X  X . = 0 : 8 was chosen in [24] based on empirical studies and we used the same value in our work.
Although the original SHReC model was built from top documents returned by a search engine given a query, we extended this to build the model from a substantial crawl of an Intranet collection. SHReC can be viewed as a directed graph G SHReC = ( V; E; w ) where:
Generating a list of suitable candidate terms or concepts ( V ) to occur in the model is not trivial in our case since unlike in the original subsumption hierarchy approach, we do not have a current query to guide the selection of re-lated terms. We must therefore ensure that our candidate terms cover a reasonable proportion of the vocabulary that h ttp://shrec.sourceforge.net/ users might employ to formulate their queries. Meanwhile, the computational complexity associated with building a SHReC model must also be considered. This is because each candidate term should be compared with all other terms to obtain their co-occurrence document frequency. For exam-ple, a list of 50,000 candidate terms will require about 2.5 billion (50000 2 ) co-occurrence checks between terms while building the model.

A generic approach will be to extract all unique keywords found in the collection as candidate terms. Additional candi-dates can then be generated by extracting n -grams (phrases) from documents in the collection that contain these unique keywords to cater for queries that contain more than one keyword. The maximum size of n in each n-gram should be limited to a small number ( 5) as users are less likely to use longer phrases from an analysis of the query logs avail-able to us. The extracted n-grams can also be restricted to those that match a particular syntactic pattern (e.g. Noun Preposition Noun for a 3-gram) [21].
We employ the SHReC model for suggesting useful terms for query refinement in order to satisfy a user X  X  informa-tion need. We propose to do so by finding a given query in the model and suggesting more generalised (immediate ancestor) and specialised (immediate descendants) terms re-lated to the query in the hierarchy. These terms are ranked based on the weighting function shown in Equation 2. The function implicitly combines the subsumption ratio ( df ( x y ) =df ( y )) and document frequency ratio ( df ( y ) =df ( x )) be-tween any two terms ( X  X  X  subsumes  X  X  X ) in the model. The subsumption ratio is the quotient of the co-occurrence doc-ument frequency ( df ( x ^ y )) and the child X  X  document fre-quency, and is used to determine subsumption between terms in the hierarchy (see Equation 1). The document frequency ratio is the ratio of the document frequencies of two con-nected terms in SHReC. It ensures that the closer the doc-ument frequencies of two directly linked terms, the more useful they are as a refinement for one another.
Figure 1 shows a sample SHReC model to illustrate its use for query recommendation. We only show the computation of weights for the link between terms  X  X  X  and others to avoid cluttering the diagram. Given a query such as term  X  X  X  in the figure, our technique will suggest terms  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X  and  X  X  X , ranked in order of their computed weights ( w ).
The example above assumes that term  X  X  X  would be found in the model irrespective of whether it is a keyword or phrase. This is not intuitive as it means that a lot of queries might not be matched in the model. Since many queries are likely to be formulated as phrases and might contain more than one word, one or more of the query words might occur sep-arately in the model. Therefore when a query is not found in the model, its keywords are extracted and suggestions are provided by aggregating the recommendations from each of this keyword. The combined recommendations are re-ranked based on magnitude of their weights with respect to the key-word that recommend them. This should reduce the number of times a suggestion is not made, since previous works [27, 26] have found that users prefer to be provided suggestions regardless of their usefulness. df=30
In order to improve the effectiveness of query refinement recommendations from the SHReC model, we propose its adaptation with users X  search logs. This ensures that query terms not captured during the creation of the SHReC model are incorporated into the model. Adaptation with query logs can be seen as an amalgamation of the query flow graph model [6] (see Section 2) and SHReC. The adaptation pro-cess is carried out for pairs of query refinements found at periodic intervals (e.g. every week) to minimise its com-putational cost. In a live system, such adaptation will be programmed to take place at a time when the search en-gine is least busy such as past midnight on Sunday morning in preparation for the new week. We start with an initial SHReC model built as described in Section 4 and then con-tinuously update the model with the refinement terms found in the search logs. The weight of each refinement in the log is computed as shown in Equation 3. Such refinements might be restricted to only those in which there was at least a click after refining the current query or all refinements found in the logs for the adaptation period. The total frequency is cumulative so that collective user intelligence is taken into account rather than those for just that periodic interval. for x ! y, logW eight ( x; y ) = totalF requency ( x where * is any term apart from  X  X  X  found as a refinement of  X  X  X  in the logs from inception of SHReC adaptation. Func-tion totalF requency is also computed cumulatively from the beginning of the adaptation process.

The adaptation process is formalised with the pseudo-code listed in Algorithm 1. It starts with the normalisation (as listed in Algorithm 2) of weights w for each edge in the initial SHReC built from the document collection only (Lines 1-3 of Algorithm 1), so that the total weight of all edges originating from a term node sums up to 1. The weights of new refine-Al gorithm 1 Adapting SHReC with query logs Re quire: G SHReC = ( V; E; w ), a concept hierarchy graph Require: G normSHReC , normalised SHReC Require: R = f r 1 ; : : : ; r n g , reformulations in test period Require: r i = x ! y , query  X  X  X  reformulated as query  X  X  X  Require: lw ( r i ) = logW eight ( x; y ), log weights (Eqn 3) 1: if (Inception of original SHReC X  X  adaptation) then 3: end if 4: for each r i 2 R do 5: if x 2 V AND y 2 V then 6: G normSHReC .updateweights( x; y; lw ( r i )) 7: else if x 2 V AND y = 2 V then 8: G normSHReC .addDescendant( x; y; lw ( r i )) 9: else if x = 2 SHReC AND y 2 SHReC then 10: G normSHReC .addAscendant( y; x; lw ( r i )) 11: else 12: G normSHReC .add( x; y; lw ( r i )) 13: end if 14: end for Al gorithm 2 Function norm ( G SHReC ) Inpu t: G SHReC = ( V; E; w ), a concept hierarchy graph 1: for each v 2 V do 2: sum 0 3: OUT getAllOutNodes(v), where OU T V 4: for each o 2 OU T do 5: sum + = w ( v; o ) 6: end for 7: for each o 2 OU T do 8: w  X  ( v; o ) w ( v; o ) =sum 9: end for 10: end for men t terms added to the model from the logs will also have values between 0 and 1. The function norm ( G SHReC ) there-fore ensures that the weights of newly added terms are com-parable to those already in the SHReC graph. The weights of edges from the initial SHReC are kept in w  X  for updates with the log weights. This ensures the original SHReC weights remain influential and comparable to the log weights.
Each unique query refinement (reformulation) pair found in the query logs between the periodic interval used for adap-tation is then incorporated into the model. We cater for all possible scenarios as shown by the conditional statements listed on Lines 5, 7, 9 and 11 of the main adaptation algo-rithm. We update the weights between refinement terms in the model if both already exist by adding the weight calcu-lated from the query logs to the current normalised SHReC weights (Line 6). When only one of the terms exist in the current SHReC, the other term is added (as parent or child) with the weights of the edge between them initialised to the computed weights from the logs (Line 8 &amp; 10). Both te rms are added into the graph if they do not currently exist using weights computed from logs (Line 12). We do not re-normalise after updates because w  X  is always from the original SHReC and the weights from the logs are com-puted from cumulative frequency. Normalising and adding cumulative log weights will mean that refinements in previ-ous adaptation periods contribute more than to the weight updates thereby giving them undue advantage.

Figure 2(i) illustrates the normalisation step of the adap-tation process using a sub-tree from Figure 1. The sum of w  X  from node  X  X  X  equals to 1. We then exemplify other steps of the adaptation process in Figure 2(ii). Here, the refinements from the query logs during the adaptation pe-riod from inception are assumed to be the three shown in the rectangle at the top-right corner of the figure with the log weights calculated using Equation 3. The effect of the adaptation is that the link to  X  X  X  is given more weights than others because this refinement is found in both the logs and SHReC model: w adapted ( A; F ) w  X  ( A; F ) + lw ( A; F ) from Line 6 of Algorithm 1. The ranking of the recommendation also changes since some of the log weights are higher than those from the original SHReC. For instance, term  X  X  X  will be ranked higher than  X  X  X  during query recommendation.
Our aim is to improve a user X  X  search experience by sug-gesting refinement terms related to a current query. Recom-mended terms can be useful as they can assist users in choos-ing the right query terminology that leads them to most rel-evant documents. The expectation is that the refined query will result in more relevant results than the current query, thereby making it easier and quicker for a user to find re-quired information. We evaluate the effectiveness of query refinement terms recommended by the following three mod-els. 1. S SH ReC, static SHReC built from only the Intranet 2. A SH ReC, our adaptive SHReC discussed in Section 5 3. QFG, a Query Flow Graph[6] discussed in Section 2
These models are evaluated extensively with an automated evaluation methodology which utilises real user logs. The datasets (web crawl and search logs for two academic in-stitutions X  Intranets) used in our experiments are described in Section 6.1. Our evaluation methodology is described in Section 6.2, followed by specific details of how we built the SHReC model used in our experiments in Section 6.3.
The data (documents collection and search logs) used in our experiments were obtained from the Intranets of two UK academic institutions namely University of Essex and The Open University, referred to as Essex and OU respectively. The document collections were created by crawling the main websites of the institutions with Nutch 2 . We limited each crawl to a maximum depth of fifteen and simulated the In-tranet search engines of the institutions on our machines us-ing Apache Solr 3 . This allows us to obtain the document fre-quency of any term and co-occurrence document frequency of any two terms required for SHReC creation. The crawls for Essex and OU were done in July 2011 and October 2011 containing 77,841 and 220,059 documents respectively.
The search log data used in our experiments are obtained from the Intranet search engines of Essex and OU . Each search record contains the user query, a transaction time stamp, a session identifier and URLs visited by the user. Query refinements are extracted from queries in each ses-sion based on their time stamp. We used logs of 12 weeks between February and May 2011 for our evaluation. The to-tal number of queries entered during this period were 32,212 and 117,358 for Essex and OU respectively.
We use an automated evaluation framework, AutoEval , which measures the effectiveness of query recommendations over a period of time based on actual query logs [2]. The evaluation framework was validated with a user study which showed that scores from the automated evaluation correlated highly with user evaluation of the recommendations. The query recommender models are evaluated automatically by comparing the actual refinements observed in the log files, for which there is at least one user click on results after refinement, to those proposed by a model. In other words, we interpret a user click after a reformulation as presence of a relevant suggestion.
 Each model is evaluated continuously at periodic intervals. In our experiments, this is done on a weekly basis. The logs h ttp://nutch.apache.org/ http://lucene.apache.org/solr/ E F D df=30 for each successive week are used for training and testing respectively. In other words, we adapt the models with week i  X  X  log refinements (training) and use the logs from week i + 1 as test data; where 1 i n , the number of weeks in the test period. The training and test data are therefore different for each week. The model is first evaluated on a particular weekly batch and then updated from the logs of the same batch. The evaluation methodology is not circular as the data is first used for evaluation then for learning.
For all Q , i.e. query refinements with user clicks found in the query logs for a particular week, we compute each model X  X  Mean Reciprocal Rank (MRR) score as shown in Equation 4. The evaluation process results in a score for each logged week. Overall, the process produces a series of scores for each query recommendation model being evalu-ated. These scores allow the comparison between different models. One query recommender model can therefore be considered superior over another if a statistically significant improvement can be achieved over the specific period. where r i is the rank of an actual query refinement in the list of refinements recommended by the model. Note that when the actual query refinement is not included in a model X  X  list of recommended terms, then 1 =r is set to zero.
Creation of a SHReC model for an Intranet requires two main resources: a collection of documents and a list of can-didate terms. The main challenge is in generating the list of suitable candidates that should occur in the model. As discussed earlier in Section 3.1, the computational complex-ity of the creation process necessitates that the size of the list is manageable. In order to determine the reasonable size of candidate terms based on the trade-off between computa-tional cost and effectiveness of recommendations, we tested different sets of candidates extracted from Essex  X  X  collec-tion. Firstly, we extracted all keywords from the collection and up to two other words around each keyword to form bi-grams and tri-grams leading to 54,830 unique candidates. This set is labelled as CS all (candidate set for all terms). CS 15 k consists of 15,086 terms from the list of keywords with document frequency between 20 and 13,226 (the maximum document frequency), and terms not in this set occurred in less than 0.026% (20/77,841) of the crawled Intranet collec-tion. We also created CS 10 k , CS 5 k and CS 1 k consisting of 10,148, 5,005 and 1,002 terms respectively. The terms with the least document frequency were 35, 89 and 631 for CS 10 k
Table 1 shows a comparison of the candidate sets across some important features. The number of subsumptions in-creases with the size of candidate set although there is no direct proportionality. For example, whilst the number of candidates in CS all is more than triple of those in CS 15 k proportion of subsumptions is just about twice. This might mean that the most important terms for subsumption are those with medium document frequencies. The computa-tional cost on the other hand is like a geometric progression of the number of terms. The cost here refers to the time it takes to completely build a SHReC model on an Intel Duo core CPU (3GHz each), 4GB RAM with Windows 7 enter-prise edition OS running 64-bit Java codes and using 4GB virtual memory. A candidate set of a thousand terms ( CS 1 k takes less than 30mins to complete, while a candidate set ten times bigger ( CS 10 k ) completes its SHReC creation in over 36hrs, which is about seventy-two times the amount of time. This is due to the amount of co-occurrence document frequency checks whose time complexity is O ( n 2 n ).
The average MRR results from the automated evaluation of the SHReC built from these candidate sets over a 12 weeks test period are also shown in Table 1. The query recom-mendations from the static SHReC built from CS all are sig-nificantly better than those from the other candidate sets (paired t-test at 95% confidence). However, there is only a marginal difference among the adaptive versions of the mod-els after 12 weeks of adaptation with all observed refinements from search logs. This implies that though the size of can-didate set is important for the original static SHReC, its a daptation with search logs minimises the effect of the size of candidates when utilised for query recommendation.
We obtained identical patterns for similar candidate sets extracted from OU as shown in Table 2. The candidate set for all terms which had over 11,8820 terms was not tested as this would not be practical. Based on the above analysis of different candidate sets across the two datasets, especially with respect to the trade-off between time complexity and effectiveness, we selected CS 10 k whose SHReC is used for further analysis and comparison to the QFG in the next section.
Evaluation results from our experiments are analysed in this section. Firstly, we discuss results related to a compara-tive analysis of Static SHReC (S SH ReC), QFG and Adap-tive SHReC (A SH ReC) which uses all query refinements found in the search logs for the test in Section 7.1. We then analyse results from experiments when SHReC is adapted with only refinements where there is a user click immedi-ately after the refinement in Section 7.2. We also compare this adaptive SHReC to a query flow graph (equivalent to the query click graph [11]) which also uses only refinements with user clicks.
Experimental results of the query recommendation mod-els for the two Intranets are plotted on graphs shown in Fig-ures 3 and 4. The average weekly MRR evaluation across the 12 weeks test period are given in Table 3. The values in bold font are significantly better than others across the same eval-uation metric at 95% confidence, while the underlined val-ues are significantly worse. We employed a non-parametric measure (Kruskal-Wallis) since a plot of our results deviated from the normal distribution. The z-values are also shown in the Tables to indicate other levels of significance. For example, a z -value above 2 : 58 indicates a significantly bet-ter or worse result at 99% confidence depending on polarity (positive or negative). It should be noted that QFG evalu-ation results are always one week less than those from the SHReC models. This is because QFG is built cumulatively on a weekly basis from the search logs and therefore has no evaluation in the first week. SHReC is first built from a col-lection and can be evaluated with the query logs from the first week as test data before its subsequent adaptation for other weeks. Fi gure 3: Essex evaluation results for adaptation with all query log refinements
Static SHReC (S SH ReC) was significantly worse than adaptive SHReC and QFG at 99% confidence ( z &lt; 2 : 58) as shown in Table 3. Its performance in proposing suitable query refinement terms is about the same with very little fluctuations every week throughout the evaluation for both Essex and OU (Figures 3 and 4). A possible reason for the low performance of S SH ReC might be that the statisti-cal co-occurrence led to lots of noisy subsumptions between terms. The evaluation method might also have adversely affected S SH ReC since queries from search logs are used for the automated testing. Popular queries are likely to be S SHR eC 0.0 0044 (z =-4.45) 0 .00042 0 .00078 ( z=-4.45) 0 .00066 A SHR eC 0 .07750 (z=3.23) 0 .12717 0. 11038 (z=2.99) 0 .16196
QF G 0 .06400 (z=1.24) 0 .10776 0. 09764 (z=1.49) 0 .14694 rep eated often by different users thereby giving advantage to models that utilise the logs for their recommendation. Fi gure 4: Evaluation results for OU for adaptation with all query log refinements
Our adaptive SHReC (A SH ReC) is significantly better than QFG from the evaluation results across both datasets as can also be seen in Table 3. This is mainly due to the fact that it is able to effectively integrate suggestions mined from both document collection and search logs. Users are there-fore able to benefit from other previous users with similar information needs as reflected by their queries. The supe-rior performance of A S HReC over the other two models verifies our hypothesis that the collection and search logs are complementary sources for query recommendation. The continuous increase in MRR on every week is an indication that our adaptation method supports gradual learning and the increase is likely to continue for a long period after-wards. Although, the average MRR score was about 8% for Essex , a MRR score of about 13% in the 12th week of adaptation (Table 3) means that the average value might not give a true picture of the model X  X  effectiveness. This is because the MRR score in the first week was very low, same as S S HReC X  X . The same analysis applies to OU where the difference between the average score and score in the 12th week is over 5%. The MRR score from the second week onwards varied between 7-16%. The slightly higher scores obtained from OU compared to Essex is likely to be due to the availability of a bigger search log which enable more of the model X  X  suggestions to be matched during evaluation.
The behaviour of the A SH ReC as compared to S S HReC is similar to the decision stump [16] and its boosted ver-sion in machine learning. The decision stump on its own has a very low accuracy but a far better accuracy with boosting, performing better than or equivalent to some of the then state-of-the-art machine learning models. There is generally an increase in evaluation score every week for both A S HReC and QFG with occasional decreases across the two Intranets. However, the graph of A SH ReC is gen-erally above QFG for the two datasets throughout the test period (see Figures 3 and 4). This supports the fact that A SH ReC X  X  performance is not solely due to its adaptation with search logs since QFG also learns from the same logs. The suggestions from the S SH ReC are therefore useful but their performance is boosted after adaptation with the logs.
The generally low MRR scores might be due to the harsh gold standard (a user X  X  immediate refinement with click) em-ployed in the automated evaluation framework compared to what might be obtained in a user study where users are asked to rate the suggestions as relevant or not. The context of the ground truth refinements are also not fully captured by the evaluation framework and this context might have changed based on the resulting documents shown to the user by the search engine.

T able 4 shows the average number of weekly queries used in our evaluation and how many of these queries for which the models were able to recommend at least one term for re-formulation. A S HReC is able to provide suggestions about 88% of the time on the average compared to 46% and 74% for QFG and S SH ReC respectively across both datasets. We observe that quite a few of the terms used in creating our initial SHReC model were found directly in the query logs. This is because the difference in the number of queries for which a suggestion was made between A S HReC and S S HReC is far smaller than that of QFG. For instance, only an average of 104 (1181 1067) new refinements were added into the SHReC model for Essex though an average of 581 new refinements were added to QFG every week. This means that the weights on the links between 477 terms al-ready existing in the SHReC and other nodes were strength-ened by the adaptation on a weekly basis. An identical trend is observed for OU . Nevertheless, users will still not get a suggestion 12% of the time from A SH ReC. This can be re-duced further extracting similar terms to those in the graph from dictionaries or domain glossaries.
Figures 5 and 6 show results from our evaluation of the query recommendation models for the Essex and OU where only log refinements with at least one user click are used for SHReC adaptation as well as creation of QFG. The average values are also given in Table 5.

The results trends are very similar to those from the pre-vious section, with A S HReC significantly better than both S S HReC and QFG. However, the evaluation scores for both S SHR eC 0. 00044 (z =-4.54) 0. 0004 0. 00078 (z =-4.54) 0.0 0066 A SH ReC 0. 06453 (z=3.11) 0. 1204 0.1 0078 (z=3.15) 0.1 5474
QF G 0. 05146 (z=1.46) 0. 09697 0 .08709 (z=1.42) 0.1 3904 Fi gure 5: Evaluation results for Essex for adaptation with log refinements having user clicks Fi gure 6: Evaluation results for OU for adaptation with log refinements having user clicks A S HReC and QFG when only log refinements with user clicks are used for adaptation are smaller than those for which all refinements in the logs are used (see Tables 3 and 5). For instance, the average MRR for Essex when adapta-tion is carried out with all logs is 0 : 0775 compared to 0 : 06453 for using only those with clicks. The same applies to OU (0 : 11038 versus 0 : 10078). This implies that users will benefit more from the models when all log refinements are used for adaptation rather than only those with clicks. Although this initially seems counter intuitive, a good explanation is that users do not necessarily have to click on the resulting doc-uments before modify their queries even though they might have read the result snippets or titles which give hints on how to better formulate their queries.
A recent study showed that a better retrieval performance is obtained when users are able to reformulate their queries to more successful queries [1]. This was estimated by metrics which rely on click through data such as those introduced in [22]. Furthermore, we also found out that increase or de-crease in the retrieval performance of the original query can be a good indication of the quality of query recommenda-tion. This suggests we can automatically assess the quality of query recommendation through these quality metrics be-fore involving real users. bu raries, midwifery courses, masters courses, pa yday, foundation courses, copy shop, cred its, academic reference, study skills, su mmer term, human rights centre, phd courses, sh ort courses, language lingistics,graduate diploma, exa m papers, pre sessional course, performing arts, un dergraduate admissions, extenuating cercumstances Fi gure 7: Sample twenty (20) queries used for click based quality evaluation
Therefore, we conducted a further evaluation based on that finding to assess the recommendation coming from both A SH ReC and QFG models. The models were first trained (adapted) on all refinements in the log data for Essex be-tween October and December 2010. For testing, fifty queries of medium frequency between 5 and 15 are sampled from the logs of Essex between February and May 2011. This is similar to the sampling approach in [6] where frequent and rare queries are avoided. We then selected the worst performing 20 queries (see Figure 7 4 ) out of these queries using the M RR C 5 quality metric. Here, M RR C refers to the mean value of of the specific query in the logs; r i is the rank of docu-ment clicked within the result list displayed to the user. The intuition behind selecting the least performing queries is that they present a more challenging scenario for the query recommender models as they are the ones where the user is most likely to reformulate. For each of the 20 queries, we selected up to three recommendations from each recom-mender model, i.e. A SH ReC and QFG. For each model, query q and recommendation r , we calculate the difference  X ( M RR C ) = M RR C ( r ) M RR C ( q ).

The averages are shown in Table 6. We observe that over-all both models are capable of recommending queries that can increase the observed retrieval quality estimated by click through data. We also see that the adaptive concept hier-archy model outperformed the query flow graph model rein-forcing our previous evaluation.
S ome of these queries are actually mis-spellings.
M RR C should not be confused with the M RR scores used in Equation 4. T able 6: Retrieval performance differences achieved by recommendations from both models
Re mark: Although we discussed only MRR results, other evaluation metrics such as MAP &amp; Recall as well as using only the top ten recommendations gave results with identical trends to the ones discussed in this paper.
We have proposed a novel method for adaptation of the concept hierarchy model with query refinements from search logs on Intranets for query refinement recommendation. Ex-perimental results indicate that our adaptive model improves in its query recommendation performance over a period of time and is significantly better than its static version as well as the query flow graph which is a state-of-the-art adaptive query recommender model. We also discovered that it was better to adapt the concept hierarchy model with all log re-finements irrespective of whether there were user clicks or not. We intend to further validate our automated evalua-tion results with user studies, incorporate our model in a live system and compare its effectiveness to other query rec-ommendation models.
 This work is part of the AutoAdapt research project funded by EPSRC grants EP/F035357/1 and EP/F035705/1. [1] M.-D. Albakour, U. Kruschwitz, N. Nanas, [2] M.-D. Albakour, N. Nanas, U. Kruschwitz, M. Fasli, [3] P. Anick. Using terminological feedback for web search [4] R. Baeza-Yates and A. Tiberi. Extracting semantic [5] R. Baraglia, F. Cacheda, V. Carneiro, D. Fernandez, [6] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, [7] I. Bordino, C. Castillo, D. Donato, and A. Gionis. [8] D. Broccolo, L. Marcon, F. M. Nardini, R. Perego, [9] J. Callan, B. W. Croft, and S. M. Harding. The [10] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. [11] N. Craswell and M. Szummer. Random walks on the [12] S. Cucerzan and R. W. White. Query suggestion [13] V. Dang and W. B. Croft. Query reformulation using [14] B. M. Fonseca, P. B. Golgher, E. S. de Moura, and [15] D. Hawking. Enterprise search. In R. Baeza-Yates and [16] W. Iba and P. Langley. Induction of one-level decision [17] T. Joachims and F. Radlinski. Search engines that [18] H. Joho, M. Sanderson, and M. Beaulieu. Hierarchical [19] R. Jones, B. Rey, and O. Madani. Generating query [20] R. Kraft and J. Zien. Mining anchor text for query [21] U. Kruschwitz. Intelligent Document Retrieval: [22] F. Radlinski, M. Kurup, and T. Joachims. How does [23] I. Ruthven. Re-examining the potential effectiveness of [24] M. Sanderson and B. Croft. Deriving concept [25] R. White and R. Chandrasekar. Exploring the use of [26] R. W. White, M. Bilenko, and S. Cucerzan. Studying [27] R. W. White and I. Ruthven. A study of interface [28] J. Xu and W. B. Croft. Query expansion using local
