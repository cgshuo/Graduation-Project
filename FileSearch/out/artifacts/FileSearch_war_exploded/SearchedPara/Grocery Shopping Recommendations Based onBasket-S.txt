 We describe a recommender system in the domain of grocery shopping. While recommender systems have been widely studied, this is mostly in relation to leisure products (e.g. movies, books and music) with non-repeated purchases. In grocery shopping, however, consumers will make multiple purchases of the same or very similar products more fre-quently than buying entirely new items. The proposed rec-ommendation scheme offers several advantages in addressing the grocery shopping problem, namely: 1) a product simi-larity measure that suits a domain where no rating informa-tion is available; 2) a basket sensitive random walk model to approximate product similarities by exploiting incomplete neighborhood information; 3) online adaptation of the rec-ommendation based on the current basket and 4) a new performance measure focusing on products that customers have not purchased before or purchase infrequently. Em-pirical results benchmarking on three real-world data sets demonstrate a performance improvement of the proposed method over other existing collaborative filtering models. H.2 [ DATABASE MANAGEMENT ]: Database Appli-cation X  Data mining ;H.3[ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval Algorithms, Experimentation, Performance.  X  corresponding author.
 grocery shopping recommendation, basket sensitive random walk, popularity-based performance evaluation.
Grocery shopping is most often considered a real drudgery, especially by busy families. In order to eliminate this neg-ative feeling, many brick-and-mortar grocery stores have set up their online shopping websites and employed recom-mender systems to support consumers during their shopping processes. For example, these recommender systems some-times display a list of forgotten items or a list of new but relevant products.

At the heart of the recommender system is a personalisa-tion algorithm. These algorithms model consumer shopping behaviour and are used to automatically identify items 1 that are new to the individual consumer, but are likely of interests to them. This is a valuable function to many E-Commerce websites, such as book sales on amazon.com , DVD rental ser-vice on netflix.com and online grocery shopping on leshop.ch . Generally, a rank-ordered list of products not currently in the basket is generated from the items present in the indi-vidual basket, and it is expected that the shopper is more likely to accept the recommendations ranked at the top of the list.

Collaborative Filtering (CF) has been demonstrated to be an effective framework to generate recommendations [2, 15, 11, 13, 19]. It identifies the potential preference of a consumer for a new product solely based on the informa-tion collected from other consumers with similar items in the basket. Thus, compared to the content-based filtering framework [1], there is no need to apply more complicated (and sometime less reliable) content analysis techniques. In general, the development of the CF based recommender sys-tem involves three components: 1) amethodtorepresent user-user or product-product similarities, 2) amethodto
In recommender system, items often refer to products bought by consumers. In this paper, we use the terms items and products interchangeably. combine the similarities in order to generate a list of recom-mendations (i.e. items not yet purchased by the consumers, but of potential interest to them), and 3) an evaluation strategy to regulate the model via retrospective data for optimal performance. In general, the relative performance of the recommendations depends on the sparsity of the data, which is a major issue challenging the usefulness of the CF-based techniques. By sparsity, we refer to the prevalence of null entries in transactional or feedback data, which renders them insufficient for deriving reliable product affinities.
While recommender systems has become a popular re-search topic over recent years, most publications are origi-nally developed for those one-off leisure products (e.g. movies, books and music) with non-repeated purchases. Further-more, many existing recommendation techniques are based on user-ratings, which requires the degree of preference to be explicitly represented on a discrete numerical scale ranging from the lowest (most disliked) to the highest (most favored) value. In grocery shopping, however, people tend to make repeated purchases, e.g. during weekly shopping with dairy, vegetables, etc and buy novel items less frequently. In addi-tion, product preference is conveyed implicitly in the trans-action data instead of being expressed explicitly as ratings. This paper is motivated by the application of recommender systems to the grocery shopping domain. We propose a new algorithm under the collaborative filtering framework, which is better suited to the characteristics of grocery shopping data.
 There are four novel aspects in the proposed algorithm. Firstly we propose a new product similarity measure based on implicit user preference, which uses a bipartite network to represent the shopping data. Secondly, we propose over-coming with the data sparsity problem by using a basket-sensitive random walk model that can derive the product similarities by exploiting partial or incomplete preference information from their neighborhood area (i.e. the other products bought together with the two in consideration). Thirdly, by making the random walk model sensitive to the current basket contents, we define a framework for mak-ing personalised recommendations. Finally, we propose a new performance measure of the recommendations, which weights each correct prediction in inverse proportion to over-all prevalence of the items. We refer to this new performance measure as the weighted hit rate, which extends the prin-cipal performance measure that we have employed thus far (i.e. the popularity based binary hit rate described in [17]).
The outline of this paper is as follows. Section 2 reviews the collaborative filtering and random walk algorithms, in the context of recommendations. Sections 3 and 4 describe our proposed method and evaluation measures respectively. The experimental results are described in Section 5. Finally, we present our conclusions and future work in Section 6.
Item-based Collaborative Filtering (CF) is one of the most popular recommendation algorithms due to its simplicity and relatively good performance. As proposed in [15], the item-based CF is a similarity-based algorithm that assumes the customers are likely to accept product recommendations that are similar to what they have bought before. To alle-viate the  X  X old start X  problem and scalability issues arising in user-based CF [16, 11], item-based CF performs the sim-ilarity calculation in item space, which is usually lower in dimension and relatively more static than the user space. It has been shown that item-based CF can achieve prediction accuracies that are comparable to or even better than the user-based approach [15].

The effectiveness of item-based CF depends on the quality of the similarity measure. In general, two types of similari-ties have been widely used, i.e. cosine-based and conditional probability based similarities. The cosine based measure is symmetric and calculates the similarity as the normalized in-ner product of two feature vectors. The x th element of the feature vector indicates either the adjusted (via the user X  X  average rating) product rating from the x th user, or the number of times the user bought the product: where R is the user-item matrix and R  X  ,i is the vector no-tation of its i th column.

The conditional probability based similarity is calculated as the ratio between the number of users who bought both items and the number of users who have bought just one of them, which is asymmetric. As mentioned in [5], one of the limitations of an asymmetric measure is that each item tends to have high conditional probabilities with respect to the most popular items. To alleviate this problem, the following variant of the conditional probability is proposed in [5]: where  X   X  [0 , 1], Freq ( i ) is the number of users that have purchased item i in the training data, and R ( i, j )isthe( i, j ) element in the normalized n  X  m user-item matrix.
While item-based CF has shown good performance in com-parison with its user-based counterpart [5, 15], its perfor-mance is still limited. This is due to its inability to explore transitive associations between the products that have never been co-purchased but share the same neighborhoods. This is an important limitation of all pairwise distance or similar-ity metrics, which includes the vast majority of the published literature on recommender systems. One way to overcome this problem is to use random walk based recommendation algorithms. The random walk model is closely related to Google X  X  PageRank algorithm [4], which is a link analysis algorithm initially designed to calculate the importance scores for all the pages on the Web. It defines a random walk over the entire Web based on the hyperlink structure, with each ran-dom step being either a jump to an arbitrary page on the Web or a walk through one of the outgoing hyperlinks of the current page. The importance score for a given page is essentially the probability of visiting that page following the random walk process in the long run and it is calculated iteratively as: where P is a transition matrix (with P ( i, j )representingthe transition probability from the page j to the page i ), R the ranking vector of all the pages (which converges as n goes to infinity), d  X  (0 , 1) is a damping factor (which con-trols how often the random walker jumps to an arbitrary page), and U is a unit vector used to avoid the reducibil-ity of the transition matrix. Several personalized PageRank methods have been proposed to take individual preferences into account [6, 9]. This is achieved by adjusting the vector U so that it reflects the individual X  X  personal interests.
Many researchers have adapted the random walk model for the domain of movie recommendations [8, 13, 18, 19]. Here the transition probability P is obtained by normaliz-ing the similarity matrix calculated from the movie ratings (or binary scores [0 , 1] if the ratings are not available). The top N movies with the highest importance scores (i.e. R in Eq.(3)) are then provided as the recommendations. While this work has shown positive results in the domain of movie recommendations, it is not immediately clear how it can be applied to the grocery shopping domain due to several issues. Firstly, as mentioned earlier, the product preference is not available as explicit ratings in the shopping basket data. One way to derive the implicity preference is by normalizing the user purchase frequency as described in Eq.(2). However, this may result in inferior recommendations as shown in our experiments, e.g. cf ( cp ) in Section 5. Secondly, the methods described in [8, 13] are user-based approaches, which require the user-user similarity matrix. Therefore, these approaches are less favorable for large-scale applications in which the number of the users are typically much larger, and less sta-ble than the number of items. Furthermore, the models in [13, 19] generate the transition matrix P by normalizing the similarity matrix calculated from arbitrary measures. A more rational approach would need to calculate P specif-ically from the graph representing the data, which might become more effective with respect to the application of the random walk model.
In order to address the concerns with the existing meth-ods described previously, we propose an new recommender algorithm for the grocery shopping domain, which we call the basket-based random walk algorithm. In the proposed approach, the similarity is defined as the transition proba-bility between products instead of users and it is calculated directly from the bipartite network used to represent the basket data. Furthermore, the transitive associations be-tween the products are further explored by a modified ran-dom walk model. This new model calculates the product importance score by taking into account the current shop-ping context (i.e. the items already in the basket), such that relevant products are ranked higher in the list of recommen-dations, with less bias towards the most prevalent products.
The shopping basket data can be described as a bipartite network in which there are two types of nodes: consumers and products. The edges of this bipartite network only lie between nodes of different type. Each edge in the network represents a consumer X  X  purchase frequency of a product. If is P = p 1 ,p 2 , ..., p | P | and the set of purchase frequencies is basket data as BN = { C, P, F } in Figure 1. Figure 1: Bipartite network for shopping basket data
From the random walk perspective, the conditional prob-ability of p j given p i can be interpreted as the transition probability P ( p j | p i ) for a random surfer to jump once from the product node i to product node j via all the connected consumer nodes c k , which can be expressed as the equation below: where P ( c k | p i ) is the probability that a random surfer jumps from the product node p i to the consumer node c k , P ( p is the probability that this surfer then jumps from c k to the product node p j ,and P ( p j | p i ) is the marginal probability distribution over all the consumers. Intuitively, P ( p j be understood as the preference voting for target product p , from all the consumers in C who have bought p i ,where every vote P ( p j | c k )fromthe k th consumer is weighted pro-portionally to his share of total purchase frequency of p P ( c k | p i ).

P ( p j | p i ) defined in Eq.(4) can be considered as a first-order similarity between products i and j , since the calcu-lation is made by hopping from the original product node to the target product node only via the shoppers who bought both products. Note that, two products never bought to-gether does not necessarily imply that they are not similar. This could be, for example, a result of insufficient observa-tions of a newly launched products. Therefore, first-order similarity measures such as this, tend to suffer when the data is sparse, and as a consequence result in less effective recommendation.

In general, the conditional probability P ( p j | c k )inEq.(4) can be empirically estimated as the fraction of the num-ber of baskets belonging to consumer c k containing product p , divided either by the total number of baskets belonging to consumer c k or by the overall number of products pur-chased by consumer c k , counting both the baskets with and without p j . The former is referred as the basket-based es-timation, while the latter is referred to as the item-based estimation [12]. The basket-based estimation is the most commonly used in recommender systems. However, previ-ous research with Naive Bayes recommenders has shown an improvement in performance by using the item-based esti-mation [12]. Therefore, in this work, we choose to implement the item-based estimation, and the conditional probabilities used in Eq.(4), are computed as: where  X  1 , X  2  X  [0 , 1] are used to control the penalization for consumers (or products) that have a large number of pur-chases (or transactions). The introduction of  X  results from the intuition that the consumers who have bought a large number of different products are less informative than those who have bought just a few. The same also applies to the products, where the more popular products are less infor-mative in terms of the personal preferences of the shoppers who buy them. While  X  1 and  X  2 could be of different values, in practise, we set alpha 1 =  X  2 for convenience reason.
The limitation with first-order similarity measures can be alleviated by aggregating the similarities of all the orders, as follows: where P is the stochastic transition matrix calculated via Eq.(4) and d  X  (0 , 1) is the damping factor to adjust down-wards, the contribution from the higher order similarities. As discussed in [19], the higher order similarities tend to be influenced more by product popularity than by relevance, and P  X  converges to dP ( I  X  dP )  X  1 as n tends to infinity.
P  X  defined above in Eq.(7) is computed independently from the current shopping behaviour. Therefore, in order to generate more accurate resu lts, we need to bias the model towards the items already bought in the basket. This can be achieved by introducing an additional rank to the items currently in the shopper X  X  basket, during each iteration of the random walk computation. More specifically, we propose adding a non-uniform personalization vector U basket during each iteration of the random walk computation, as follows: where R basket is the basket-based importance scores used for ranking all the products, and the i th entry of the per-sonalization vector U i basket = 1 m ,ifthe i th product is in the basket and 0 otherwise, and m is the number of products in the current basket. Note that, the non-empty entries in U basket can be considered as introducing artificial links from all the product nodes in the bipartite network (see Figure 1) to the nodes representing the products currently in the bas-ket.

A straightforward implementation of Eq.(8) would need to re-compute the product ranking scores whenever the con-tents of the basket changes. However, this is unfeasible in practice due to the large number of product permutations, and will also increase the online response time. In this work, we propose a quick approximation R basket of R basket as fol-lows: where R item is the item-based importance score and U item i is the personalization vector with all elements set to zero except for the i th entry, which is set to one. In practice, R item can be pre-computed off-line for each product, and R basket can be quickly obtained online by summing up all the R item vectors relating to the items currently in the bas-ket. This computational simplification is based on the ob-servation that R basket in Eq.(8) can be re-written as where m is the total number of product currently in the bas-ket, and U basket is expanded as a weighted combination of the elements of U item .As R basket is proportional to R basket they will both lead to the same rank-ordered list of recom-mendations.
While the approach proposed in this paper was developed independently, some of our ideas are similar to those pre-sented in a few previous studies. For example, our calcula-tion of the importance scores in Eq.(10) is referred to as a random walk with restart in [14], where they use it to find correlations between a query image and images in an archive. In our work, this idea is generalized to handle multiple query objects (i.e. basket items). Furthermore, in [14] Pan et. al use a binary adjacency matrix (with a weighting of 1 for existing links and 0 for non-existent links) to compute the transition probabilities. In contrast, we use the richer infor-mation contained in the bipartite network (where the links between nodes are weighted by the purchase frequencies) to calculate the transition probability matrix ( P in Eq.(8)).
The bipartite representation of user-object data has also been suggested previously by Zhang [8] and Zhou [20]. In Zhou X  X  work [20], a weighting scheme for the bipartite net-work is proposed but not fully explored under the random walk framework. Zhang X  X  work [8] is an adaptation of the HITS algorithm [10] and it is too expensive to run with our data, as it requires computation of the user-user ma-trix. As mentioned earlier in Section 2.1, the user space is usually very high in dimension, and therefore user-based approaches are less favourable when using large scale data sets such as ours. For example, the user-user matrix for the TaFeng data [7] would require 54GB of memory, and even when employing a sparse representation it still requires around 24G of memory. However, using the same data set, the item-item matrix only requires 9MB of memory. Thus the items-based approach requires substantially less memory than the user-based approach.
Performance evaluation via retrospective data is essential to the development of recommender systems. However, it is necessary to ensure that the evaluation results are repre-sentative of live, interactive behavior. In our previous case study [17], we compared our live results against three eval-uation strategies, which differed mainly in terms of how a retrospective test basket is split into the evidence and tar-get components. The three approaches we tested are to split randomly, based on popularity and via the leave-n -out ap-proach [2]. As the results showed [17], the popularity-based strategy was the only one that ranked the recommender al-gorithms consistently with their live performance. The con-sistency sought was a correct ranking of the expected rel-ative take-up of recommendations between four groups of consumers along two dimensions: active vs. non-active, and test vs. control. The former characterizes consumers ac-cording to their frequency of shopping episodes during the preceding 6 month block period, while the latter separates the consumers receiv ing personalized recommendations from those receiving generic recommendations based purely on global popularity [17].

In our work presented here, we continue to use the popu-larity based binary hit rate, bHR(pop) , as the principal per-formance measure. In addition, we also propose a new per-formance measure based on weighted hit rates in order to provide complementary information.
In this evaluation protocol, the items in each test basket are split into two segments: targets and evidence. The least three (globally) popular items (based on the training data) make up the targets , while the remainder are used as the evidence .The evidence items are then used during testing, to try to predict the targets [17]. The binary hit rate is cal-culated as the proportion of test baskets having atleast one (out of three) correctly predicted target item. This evalua-tion setting reflects a real scenario of marketing promotion: increasing visibility of less popular (or new) products.
In keeping with the performance evaluation of the rec-ommender systems in related work [2, 7], we also report the binary hit rates obtained by using a random leave-3-out pro-tocol ( bHR(rnd) ), which withholds three randomly selected items as the targets .Notethat, bHR(rnd) weights all the items equally and thus tends to over-estimate model perfor-mance by favoring the hits for those more frequently occur-ring items. This may explain the results for the Belgium data (underlined in Table 2(c)) where the simplest model that recommends the most popular item is ranked above many other more advanced models.
With the popularity based protocol, the model perfor-mance is evaluated by only a subset of the items in each test basket. It is desirable to have a performance measure which makes use of all the items available. In this work, we adopt the idea of leave-one-out cross-validation, which involves iteratively using one basket item as the target and the remaining items as the evidences until every item is used once as the target to be predicted.

Similar to bHR(rnd) , the standard hit rate based on the leave-one-out protocol over-emphasizes the performance on popular items. This has been observed in our previous stud-ies [12, 17]. Therefore, we propose a new performance met-ric, which weights the  X  X it X  on a particular item inversely proportional to its popularity. We call this the weighted hit rate based on leave-one-out protocol, wHR(loo) : where x i is the item in the test basket, p ( x i ) is its prior prob-ability, and HIT ( x i )=1if x i is predicted correctly, and 0 otherwise. A basket will score one if all its items are pre-dicted correctly. The final score is calculated by averaging the score over all test baskets.

With leave-one-out cross validation, we also calculate the macro-averaged hit rate, macroHR(loo) , which is the hit rate calculated for each product and then averaged over all products. This procedure tends to bias the results towards performance on small classes (i.e. less popular products). Note that, while both macroHR(loo) and wHR(loo) are de-signed to favor the performance of less popular products, macroHR(loo) is averaged across the products and wHR(loo) is averaged across the baskets. As the experimental results show later in Figure2, the two measures do not always agree.
We run the experiments using three basket data sets col-lected from the following grocery stores: LeShop 2 ,TaFeng and an anonymous Belgium retailer 4 . The LeShop and TaFeng data sets are represented at category level, and hence have fewer unique items compared to the Belgium data set.
The baskets are represented as vectors of purchase fre-quencies indexed by item/category index. With the LeShop and TaFeng data, we aggregate the training data for each consumer into a single basket in order to maximize the in-formation content. Since the Belgium data does not contain user ID information, we use non-aggregated baskets in all the experiments relating to this data. All baskets with less than four items are removed from the test baskets 5 and the test baskets, in all cases, remain non-aggregated in order to simulate the live shopping scenario. Furthermore, due to the temporal nature of the basket data, we form the train-ing/testing data split longitudinally over time. This ensures that we are always training on earlier baskets and testing on later ones. It also avoids possible artificial dependen-cies earlier baskets may have on later baskets, which may for example be created by a random split of the data. Fi-nally, the raw purchase frequency is converted to a log scale, log ( frequency + 1), which reduces the problem of overesti-mating the product relevance caused by extreme high values. Summary information about the data is shown in Table 1.
The performance of the proposed model is affected by two parameters: the penalty factor  X  (see Eq.(5&amp;6)) and the damping factor d (see Eq.(8)). As discussed in Sec-tion 3,  X  penalizes the products (or consumers) with too many transactions (or shopping items), while the damping factor controls the extent to which the product importance www.leshop.ch aiia.iis.sinica. edu.tw/index. php?option=com docman&amp; task=cat view&amp;gid=34&amp;Itemid=41 [7] fimi.cs.helsinki.fi/data/retail.dat [3] required by leave-three-out evaluation protocol as de-scribed in section 4.1. Table 1: Experimental datasets: # items indicate the number of unique items used in the experiment. sparsity represents the proportion of empty entries in the co-occurrence matrix is influenced by the current shopping behavior. Here, we set  X  1 =  X  2 for convenience reason (see Eq.(5&amp;6)).
Figure 2 (a) displays the per formance curves in terms of bHR(pop) and macroHR(loo) on the Leshop data. As can be seen, both performance measures improve with increasing parameter values. This observation is consistent with our expectations. In other words, a higher value for  X  tends to decrease the similarity of more popular items, and a lower d (i.e. a higher (1  X  d )) tends to increase the importance scores for those similar but less frequent products. As a consequence, the less frequent items appear more often on top of the recommendation list, and thus we see better re-sults in bHR(pop) and macroHR(loo) .

The performance on the TaFeng and Belgium data sets, however, show inconsistencies between the two measures (see Figure 2 (b) &amp; (c)). This is because the macro-averaged measure has a stronger bias towards the performance in small classes in comparison with the basket-averaged bHR(pop) . In general, the model only works well for either small or big classes, and not both. Therefore, when more infrequent items are correctly predicted by the models, we see that more popular items tend to be missed. As a result, the macro-averaged measure macroHR(loo) increases, while the basket-averaged measure bHR(pop) decreases. This is be-cause of the lower basket coverage from those infrequent items.

It is desirable for a recommendation algorithm to achieve high performance on both bHR(pop) and marcroHR(loo) . However, this usually imposes extra computational cost in terms of online-processing time and memory storage capac-ity, which may not be feasible for every E-commerce website. In practise, a performance trade-off between the popular and un-popular products needs to be determined via an appro-priate business rule on a case-by-case basis .
In this section we benchmark our model against several item-based CF models that are popular in the recommender systems literature. Here we include two types of models: the standard CF models and the random work based CF models. The standard CF models are implemented using the cosine (Eq.1), conditional probability (Eq.2) and the bi-partite network (Eq.4) based similarities. We refer to these models later, as cf ( cos )and cf ( cp )and cf ( bn ) respectively. With the random walk based CF model, we use both the bas-ket sensitive ( bsrw ) and non-basket-sensitive ( rw ) schemes. The latter is similar to the model described in [19], while the former is the new model presented in this paper. A base-line method, pop , that simply recommends the most popular items not present in the basket is also implemented for com-parison. Figure 2: Impact of model parameters on bHR(pop) and macroHR(loo) . bHR(pop) represents the popularity based binary hit rate and macroHR(loo) represents the macro-averaged hit rate obtained via leave-one-out cross validation.
The best performance of all the models (i.e. using the best parameter settings) in terms of the popularity-based metric bHR(pop) are reported in Table 2, together with the cor-responding bHR(rnd) and wHR(loo) described in Section 4. Here we see that the two proposed BN-based methods are always among the top two performers in terms of bHR(pop) . In particular, with the standard CF model, the performance of cf ( cos )and cf ( cp ) is data dependant as the latter outper-forms the former in only two out of three data sets. The bi-partite network based model ( cf ( bn )), however, consistently generates better bHR(pop) in all experiments. The same re-sults are also observed in the experiment with the basket sensitive random walk based CF models ( cf ( . )+ bsrw )where the bipartite network based model ( cf ( bn )+ bsrw )always generate the best results across all performance measures.
The comparison between the standard and random walk based CF models show that, the standard CF using the bi-partite network based similarity ( cf ( bn )) is always improved by the bsrw scheme. With the other two similarities ( cf ( cos ) and cf ( cp ), the performance is improved only in the exper-iments with sparser data such as the TaFeng and Belgium data sets. With the Leshop data, the performance of the cf ( cos )and cf ( cp ) models become slightly worse when us-ing the bsrw scheme. As mentioned earlier, the proposed method ( cf ( bn )) defines the transition probability directly from the graph (i.e. bipartite network) representing the data. The other methods ( cf ( cos )and cf ( cp )), however, obtain the transition probability indirectly by normalizing the similarity matrix calculated via arbitrary measures. This observations supports our hypothesis that, with the random walk model, the transition probability defined directly from the graph structure is more effective than the one indirectly derived from arbitrary similarity measures.

In all our experiments, the basic random walk scheme showed no improvement to the standard CF models. This result could be explained by its insensitivity to the items currently in the basket during the computation of the rank-ing scores.
In this experiment, we vary the number of training bas-kets from 20% to 100% and draw the performance curve of each model in Figure 3. In order to make the figures more concise, we only display the graphs relating to the top three performers selected from Table2. As can be seen in Figure 3, in all experiments our proposed methods ( cf ( bn ) and cf ( bn )+ bsrw ) consistently outperform the best baseline model.

We see that cf ( bn )and cf ( bn )+ bsrw models show quite similar performance in terms of the bHR(pop) . However, there is a noticeable difference between them in terns of the wHR(loo) , which is more pronounced when the data is sparser (e.g. when the sparsity =0.2 in Figure 3(c)). We attribute this to the benefit of high order similarities intro-duced by the basket-sensitive random walk scheme.
In the previous experiments, we used a non-personalized approach in which the recommendations are solely based on the product similarities, regardless of personal purchase his-tory. The non-personalized approach is computationally ef-ficient and produces reasonably effective recommendations. However, it is still desirable to have personalized recom-mendations in order to better support the consumers during their shopping session.

A straightforward implementation toward the personal-ization is to assign different weights to the items in the non-personalized recommendation list. In this work, we choose the consumer preference vector, as calculated in Eq.(5), to be the weighting vector. To avoid having a zero probability for the items never bought before, we smooth the weighings by: where R is a item-item matrix in which the i th column rep-resents the ranking score R item i calculated from Eq.(10), and weight is the normalized consumer-product matrix con-structed from the raw basket data.

The results from the LeShop and TaFeng data sets are displayed in Table 3, where the best performance is shown in bold font and the second best is underlined. The Belgium data is not used in this experiment because the consumer IDs are not available for the data. As can been seen, the proposed method cf ( bn )+ bsrw has the best performance in all the experiments. Once again we see that the performance of the standard CF model is consistently improved by bsrw approach, only when the bipartite-network-based similarity is used. This may again indicate the effectiveness of the proposed method with respect to the construction of the transition probabilities.
This paper proposes a basket-sensitive random walk model for personalized recommendation in the grocery shopping domain. The proposed model is better suited than tradi-tional rating-oriented algorithms to a domain where product preference are implicit and repeat purchases are frequent. The proposed method extends the basic random walk model by calculating the product similarities (i.e. transition prob-abilities) through a weighted bipartite network and allow-ing the current shopping behaviors to influence the prod-uct ranking (i.e. importance) scores. Furthermore, the pro-posed method computes the similarity matrix in item space, and thus saves enormous computational resource compared with working in the user space. The experimental results based on three real-world data sets demonstrated a perfor-mance advantage of the proposed method over other existing CF models. Having established that network-based similar-ity measures outperform those defined on metric spaces, we plan now to investigate further approaches to obtain latent variable representations of the adjacency matrix, e.g. with matrix factorization methods and will explore their appli-cability to the domain of online grocery shopping. It is intended also to deploy the network-based approach in a Table 3: Performance comparison with personalized recommendation live recommender system and track its actual performance against other candidate methods.
The authors wish to thank Dominique Locher and his team at LeShop ( www.LeShop.ch -the No.1 e-grocer of Switzer-land since 1998), and other data providers, for making trans-action data available for this work. Without those data, the work presented in this paper would not have been possible. We would also like to thank the anonymous reviewers for their comments which helped to improve this paper. [1] M. Balabanovic and Y. Shoham. Fab: Content-based, [2] J. Breese, D. Heckerman, and C. Kadie. Empirical [3] T. Brijs, G. Swinnen, K. Vanhoof, and G. Wets. Using [4] S. Brin and L. Page. The anatomy of a large-scale [5] M. Deshpande and G. Karypis. Item-based top-n [6] T. H. Haveliwala. Topic-sensitive pagerank. In WWW [7] C.-N. Hsu, H.-H. Chung, and H.-S. Huang. Mining [8] D. Z. Huang, Z. and H. Chen. A link analysis [9] G. Jeh and J. Widom. Scaling personalized web [10] J. M. Kleinberg. Authoritative sources in a [11] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, [12] M.Li,B.Dias,W.El-Deredy,andP.J.G.Lisboa.A [13] N. N. Liu and Q. Yang. Eigenrank: a ranking-oriented [14] J.-Y. Pan, H.-J. Yang, C. Faloutsos, and P. Duygulu. [15] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [16] U. Shardanand and P. Maes. Social information [17] C. M. Sordo-Garcia, M. B. Dias, M. Li, W. El-Deredy, [18] D. T. Wijaya and S. Bressan. A random walk on the [19] H. Yildirim and M. S. Krishnamoorthy. A random [20] T. Zhou, J. Ren, M. Medo, and Y. C. Zhang. Bipartite
