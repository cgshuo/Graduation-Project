 Using a low-dimensional embedding to summarize a high-dimensional data set has been widely used for exploring the structure in the data. The methods for discovering such low-dimensional embedding are often referred to as dimensionality reduction (DR) methods. Principal component As a more recent development, probabilistic PCA (PPCA) [21] provides a probabilistic formula-tion of PCA [13] based on a Gaussian latent variable model [1]. Compared with the original non-probabilistic derivation of PCA in [12], PPCA possesses a number of practical advantages. For ex-ample, PPCA can naturally deal with missing values in the data; the expectation-maximization (EM) it is easy to generalize the single model in PPCA to the mixture model case; furthermore, PPCA as a probabilistic model can naturally exploit Bayesian methods [2].
 Like many existing DR methods, both PCA and PPCA are based on some assumptions about the and identically distributed (i.i.d.).
 However, the data in many real-world applications, such as web pages and research papers, contain though they may bear low similarity in their content due to the sparse nature of the bag-of-words representation, but the relational information is not exploited at all when applying PCA or PPCA. action. Therefore, PCA and PPCA, or more generally most existing DR methods based on the i.i.d. assumption, are not suitable for relational data analysis.
 on a closed-form solution and the other based on an EM algorithm [9], are proposed to learn the parameters of PRPCA. Although the i.i.d. assumption is no longer adopted in PRPCA, the learning algorithms for PRPCA can still be devised easily like those for PPCA which makes explicit use of the i.i.d. assumption. Extensive experiments on real-world data sets show that PRPCA can effec-performance. such as z , to denote vectors. The i th row and the j th column of a matrix K are denoted by K and K i th element of z . K T is the transpose of K , and K  X  1 is the inverse of K . K 0 means that K cov (  X  ) denotes the covariance operation.
 Note that in relational data, there exist both content and link observations. As in [21], { t denotes a set of observed d -dimensional data (content) vectors, the d  X  q matrix W denotes the q principal axes (or called factor loadings),  X  denotes the data sample mean, and x denotes the corresponding q principal components (or called latent variables) of t the d  X  N matrix T to denote the content matrix with T the latent variables of T with X Section 4.1.1, and an example will be given in Section 5. Hence, A between instances i and j , and otherwise A self-links, i.e., A To set the stage for the next section which introduces our PRPCA model, we first briefly present the derivation for PPCA [21], which was originally based on (vector-based) multivariate normal distributions, from the perspective of matrix variate normal distributions [11]. If we use  X  to denote the Gaussian noise process and assume that  X  and the latent variable matrix X follow these distributions: we can express a generative model as follows: T = WX +  X  e T +  X  .
 T | X  X  N d,N ( WX +  X  e T ,  X  2 I d  X  I N ) , T  X  N d,N  X  e T , ( WW T +  X  2 I d )  X  I N . (2) Let C = WW T +  X  2 I same as that in [21]. Using matrix notations, the graphical model of PPCA based on matrix variate normal distributions is shown in Figure 1(a).
 i.i.d. assumption can make the modeling process much simpler and has achieved great success in In relational data, the attributes of connected (linked) instances are often correlated . presented in the previous section, we can obtain PRPCA just by introducing some relatively simple (but very effective) modifications. A promising property is that the computation needed for PRPCA is as simple as that for PPCA even though we have eliminated the restrictive i.i.d. assumption. 4.1 Model Formulation Assume that the latent variable matrix X has the following distribution: According to Corollary 2.3.3.1 in [11], we can get cov ( X that  X  actually reflects the covariance between the instances. From (1), we can see that cov ( X I covariance matrix  X  for the distribution of X in (4). This  X  should reflect the physical meaning can also change the I 4.1.1 Relational Covariance Construction Because the covariance matrix  X  in PRPCA is constructed from the relational information in the data, we refer to it as relational covariance here.
 The goal of PCA and PPCA is to find those principal axes onto which the retained variance under p ( X ) in (1) as p ( X ) = exp Observation 1 For PPCA, the larger the retained variance of X , i.e., the more X approaches the destination point, the lower is the probability density at X given by the prior. they approach each other.
 Because the design principle of PRPCA is similar to that of PPCA, our working hypothesis here is will be empirically verified in Section 5.
 this assumption, the ideal goal of PRPCA should be to make the latent representations of two in-of the Euclidean distances between the linked instances. Based on Observation 1, the more X ap-be the probability density at X given by the prior. We will prove that if we set  X  =  X   X  1 where  X  ,  X  I N + ( I N + A ) T ( I N + A ) with  X  being typically a very small positive number to make  X  0 , we can get an appropriate prior for PRPCA. Note that A ij = 1 if there exists a relation between instances i and j , and otherwise A  X  =  X  I N + ( I N + A )( I N + A ) .
 Let  X  D denote a diagonal matrix whose diagonal elements  X  D get  X  = (1+  X  ) I j , we can see that B correlated, B paper j are about the same topic will increase. Hence, the larger B between instance i and instance j . Because B B is also consistent with the physical meaning underlying A .
 Letting G = 2 A + B , 3 we can find that G actually combines the original graph reflected by A and the derived graph reflected by B to get a new graph, and puts a weight 2 A between instance i and instance j in the new graph. The new weight graph reflected by G is also consistent with the physical meaning underlying A . Letting L , D  X  G , where D is a diagonal matrix whose diagonal elements D can get  X  = (1+  X  ) I we can get  X  =  X  D  X  L . Then we have The first term P N instances in the latent space. We can see that the larger  X  D instance i , which is reasonable because  X  D P 4.1.2 Model where  X  =  X   X  1 .
 We can further obtain the following results: T | X  X  N d,N ( WX +  X  e T ,  X  2 I d  X   X  ) , T  X  N d,N  X  e T , ( WW T +  X  2 I d )  X   X  . (6) ence between PRPCA and PPCA lies solely in the difference between  X  and I (2), we can find that the observations of PPCA are sampled independently while those of PRPCA are sampled with correlation. In fact, PPCA may be seen as a degenerate case of PRPCA as detailed below in Remark 1: Remark 1 When the i.i.d. assumption holds, i.e., all A because  X  does not have to be pd. When  X  0 , we say T follows a singular matrix variate we find that the performance under  X  = 0 is almost the same as that under  X  = 10  X  6 . Further deliberation is out of the scope of this paper.
 As in PPCA, we set C = WW T +  X  2 I PRPCA is where c =  X  d only difference between PRPCA and PPCA lies in the difference between H and S . Hence, all the learning techniques derived previously for PPCA are also potentially applicable to PRPCA simply by substituting S with H . 4.2 Learning By setting the gradient of L (MLE) for  X  as follows:  X  = T  X  e As in PPCA [21], we devise two methods to learn W and  X  2 in PRPCA, one based on a closed-form solution and the other based on EM. 4.2.1 Closed-Form Solution Theorem 1 The log-likelihood in (7) is maximized when where  X  the first q largest eigenvalues, U eigenvectors of H corresponding to  X  The proof of Theorem 1 makes use of techniques similar to those in Appendix A of [21] and is omitted here. 4.2.2 EM Algorithm complete data. The EM algorithm operates by alternating between the E-step and M-step. Here we can be downloaded from http://www.cse.ust.hk/  X  liwujun .
 the missing data X is computed. To compute the expectation of the complete-data log-likelihood, we only need to compute the following sufficient statistics : where M = W T W +  X  2 I values obtained from the previous iteration.
 { W ,  X  2 } are updated as follows: Note that we use W here to denote the old value and 4.3 Complexity Analysis solution. Although PPCA possesses additional advantages when compared with the original non-probabilistic embedding results of PCA and PPCA are expected to achieve comparable results. Hence, in this paper, we only adopt PCA as the baseline to study the performance of PRPCA. For the EM algorithm algorithm of PRPCA in the following experiments. 5.1 Data Sets and Evaluation Scheme information about them can be found in the longer version. We use three data sets to evaluate PRPCA. The first two data sets are Cora [16] and WebKB [8]. We pages are co-linked by or link to another common web page, we add a link between these two pages. undirected links.
 The Cora data set contains four subsets: DS, HA, ML and PL. The WebKB data set also contains to evaluate PRPCA on the Cora and WebKB data sets. For the PoliticalBook data set, we use the testing procedure of the latent Wishart process (LWP) model [15] for evaluation. 5.2 Convergence Speed of EM of PRPCA. The performance on other data sets has similar characteristics, which is omitted here. of EM iterations T is shown in Figure 2. We can see that PRPCA achieves very promising and stable 5.3 Visualization We use the PoliticalBook data set to visualize the DR results of PCA and PRPCA. For the sake of can be expected when the examples are clustered or classified in the latent space of PRPCA. Figure 2: Convergence speed 5.4 Performance The dimensionality of Cora and WebKB is moderately high, but the dimensionality of PoliticalBook general settings.
 Performance on Cora and WebKB The average classification accuracy with its standard deviation We can find that PRPCA can dramatically outperform PCA on all the data sets under any dimen-it very effectively.
 We also perform comparison between PRPCA and those methods evaluated in [26]. The methods include: SVM on content , which ignores the link structure in the data and applies SVM only on the content information in the original bag-of-words representation; SVM on links , which ignores the content information and treats the links as features, i.e, the i th feature is link-to-page PLSI+PHITS , which is described in [7]; link-content MF , which is the joint link-content matrix for comparison. Because during the DR procedure link-content sup. MF employs additional label information which is not employed by other DR methods, it is unfair to directly compare it with other methods. As in the link-content MF method, we set q = 50 for PRPCA. The results are shown in Figure 5. We can see that PRPCA and link-content MF achieve the best performance among all the evaluated methods. Compared with link-content MF, PRPCA performs slightly better on DS and HA while performing slightly worse on ML and Texas, and achieves comparable performance on the other data sets. We can conclude that the overall performance of PRPCA is comparable with that of link-content MF. Unlike link-content MF which is transductive in nature, PRPCA naturally supports inductive inference. More specifically, we can apply the learned transformation matrix of PRPCA to perform DR for the unseen test data, while link-content MF can only perform DR for those data regularized matrix factorization (RRMF) [14], has achieved better performance than PRPCA on the Cora data set. However, similar to link-content MF, RRMF cannot be used for inductive inference either.
 Performance on PoliticalBook As in mixed graph Gaussian process (XGP) [19] and LWP [15], we cess is repeated for 100 rounds and the average area under the ROC curve (AUC) with its standard the low-dimensional representation. Here, we set q = 5 for both PCA and PRPCA. We can see that on this data set, PRPCA also dramatically outperforms PCA and achieves performance comparable with the state of the art. Note that RGP and XGP cannot learn a low-dimensional embedding for the instances. Although LWP can also learn a low-dimensional embedding for the instances, the to invert the kernel matrix defined on the training data.
 Acknowledgments Li and Yeung are supported by General Research Fund 621407 from the Research Grants Council of Hong Kong. Zhang is supported in part by 973 Program (Project No. 2010CB327903). We thank Yu Zhang for some useful comments. [1] D. J. Bartholomew and M. Knott. Latent Variable Models and Factor Analysis . Kendall X  X  [2] C. M. Bishop. Bayesian PCA. In NIPS 11 , 1998. [3] J. Chang and D. M. Blei. Relational topic models for document networks. In AISTATS , 2009. [4] J. Chang, J. L. Boyd-Graber, and D. M. Blei. Connections between the lines: augmenting [5] W. Chu, V. Sindhwani, Z. Ghahramani, and S. S. Keerthi. Relational learning with Gaussian [6] F. Chung. Spectral Graph Theory . Number 92 in Regional Conference Series in Mathematics. [7] D. A. Cohn and T. Hofmann. The missing link -a probabilistic model of document content [8] M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. M. Mitchell, K. Nigam, and S. Slattery. [9] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM [11] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions . Chapman &amp; Hall/CRC, 2000. [13] I. T. Jolliffe. Principal Component Analysis . Springer, second edition, 2002. [14] W.-J. Li and D.-Y. Yeung. Relation regularized matrix factorization. In IJCAI , 2009. [16] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. Automating the construction of internet [18] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning . The MIT [19] R. Silva, W. Chu, and Z. Ghahramani. Hidden common cause relations in relational learning. [21] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal Of The
