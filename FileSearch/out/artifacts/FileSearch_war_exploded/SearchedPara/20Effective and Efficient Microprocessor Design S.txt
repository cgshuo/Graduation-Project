 When designing a microprocessor, the first and probably the most important step is to decide appropriate design configurations satisfying different performance/power/ temperature/reliability constraints, which is called as Design Space Exploration (DSE). It has become a great challenge to computer architects, since the size of design space grows exponentially with the number of interactive design parameters (e.g., cache size, queue size, issue width and so on), and the simulation required by evaluating the qual-ity of each configuration of design parameters is quite time-consuming. Moreover, the difficulty of DSE task is further exacerbated by the increasing amount and complexity of computer workloads with significantly different characteristics.

Traditionally, computer architects employed large-scale cycle-accurate architectural simulations on representative benchmarks to explore the design space. However, time-consuming simulations make it intractable to explore the entire design space. For instance, during the design of Godson-3, which is a 16-core chip-multiprocessor (CMP) with a reconfigurable architecture [Hu et al. 2009], it takes several weeks to simulate only one design configuration on SPEC CPU2000 benchmark suite. To reduce the sim-ulation costs, several fast simulation approaches were proposed to reduce the number of simulated instructions with respect to each design configuration [Hamerly et al. 2006; Genbrugge and Eeckhout 2009; Joshi et al. 2006]. However, the number of de-sign configurations to simulate is still quite large. To reduce the number of simulated configurations and thus reduce the overall simulation costs, predictive modeling was proposed [Joseph et al. 2006;  X  Ipek et al. 2006; Lee et al. 2008]. As illustrated in Figure 1, a predictive modeling approach contains two phases, that is, training phase and pre-dicting phase. In the training phase, some design configurations are simulated. Along with the corresponding responses (e.g., performance or energy response) obtained by simulations, these labeled design configurations are utilized to train a regression model that characterizes the relationship between the design parameters and processor re-sponses. In the predicting phase, such a regression model is employed to predict the responses of new design configurations that are not involved in the training set. Since simulations are only required in the training phase, predictive modeling is relatively efficient in comparison with traditional approaches. However, considerable simulation costs are required to attain the labeled design configurations for supervised learning, which encumbers the models from achieving high prediction accuracies given limited computational resources and stringent design-to-market pressure. In fact, the design configurations that have not been simulated may also be effective in enhancing the pre-diction accuracy of a regression model, which are overlooked by previous investigations on DSE.
 To circumvent these deficiencies of previous techniques, in this article, we propose the COAL (CO-training regression tree with Active Learning) approach for the challeng-ing DSE problem. The key intuition is that similar architectural configurations would behave similarly, and thus, some unlabeled design configurations can be labeled for enhancing the prediction accuracy. Generally speaking, COAL works in the cotraining style [Blum and Mitchell 1998], which is an algorithm falling into the disagreement-based learning paradigm [Zhou and Li 2010], where two learners label unlabeled in-stances for each other. At each iteration, COAL also simulates the most informative unlabeled instance on which the two learners exhibit the largest disagreement, which is inspired by active learning. To enable the learned model to be comprehensible for computer architects and designers, COAL employs M5P regression trees [Wang and Witten 1997] as the base learner. At the beginning of a typical DSE procedure, COAL initializes two regression trees by using an initial data set containing several labeled design configurations. After that, COAL starts the semisupervised active learning process consisting of a number of iterations. In every iteration, each regression tree is refined by not only the design configuration labeled by the other regression tree, but also the configuration newly simulated by the processor simulator. The key issue here is how to select appropriate unlabeled configurations to label (semisupervised learning) or simulate (active learning), which will be elaborated in Section 3.

To demonstrate the effectiveness of COAL, we conduct comprehensive experiments to explore a typical superscalar microprocessor design space. Experiments show that, given the same simulation budget to attain the labels of training design configura-tions, COAL can reduce by 35 X 95% mean squared error of the state-of-the-art ANN DSE technique. Furthermore, we also conduct experiments to demonstrate that COAL can achieve better prediction accuracy compared with pure cotraining semisupervised learning and pure active learning approaches, which further demonstrates the superi-ority of COAL approach.
 The rest of the article proceeds as follows. Section 2 introduces some related work. Section 3 presents our COAL approach. Section 4 reports our empirical results. Section 5 concludes this article. In many real-world applications, the overhead of collecting a labeled example is very expensive, while a large number of unlabeled instances are very easy to obtain. To ex-ploit the unlabeled instances and improve the accuracies of learners, Semi-Supervised Learning (SSL) and Active Learning (AL) have been intensively investigated by the machine learning community.

SSL is a mainstream methodology for exploiting unlabeled data to improve the pre-diction accuracy. Generally, SSL can be classified into four categories [Zhou and Li 2010], that is, generative methods [Fujino et al. 2005; Miller and Uyar 1997; Nigam et al. 2000], S3VMs (Semi-Supervised Support Vector Machines) [Xu and Schuurmans 2005; Joachims 1999; Chapelle and Zien 2005], graph-based methods [Zhu et al. 2003; Zhou et al. 2004], and disagreement-based methods [Blum and Mitchell 1998; Zhou and Li 2010]. Generative methods conduct maximum likelihood estimation to determine the parameters of models, where the labels of unlabeled data are treated as missing values. S3VMs usually utilize unlabeled data to adjust the decision boundary built from labeled examples. In graph-based methods, the SSL problem can be addressed by propagating the label information in a graph constructed from labeled and unlabeled data where each node corresponds to one instance. The key of disagreement-based methods is to generate multiple learners, let them collaborate to exploit unlabeled data, and maintain a disagreement among the base learners. This line of research started by Blum and Mitchell [1998] X  X  seminal work on cotraining, which is a multi-view learning algorithm. Zhou and Li [2005a] proposed a Semi-Supervised Regression (SSR) approach, C OREG , which employs two k NN regressors to conduct the data labeling and the predictive confidence estimation. C OREG does not require multiviews, it utilizes k NN as the base regressor since it is easy to update and smoothly consistent with the manifold assumption of SSL. In C OREG , the most confidently labeled example is deter-mined as the one which makes the regressor most consistent with labeled data. Though studies of disagreement-based SSL approaches started from multiview setting [Blum and Mitchell 1998], there are many successful algorithms that do not require mul-tiviews [Zhou and Li 2005a, 2010]. Recently, theoretical studies showed that multi-view is not really needed for disagreement-based algorithms [Wang and Zhou 2007, 2010b].

As another well-known methodology of leveraging unlabeled data, AL improves the prediction accuracy by actively querying the oracle (in the context of DSE, the oracle refers to the simulator) the labels of some unlabeled instances. According to the con-crete way of selecting the instance-to-query, existing approaches of AL can roughly be categorized into three types. The first type of approaches query the label of the most informative instance which is expected to reduce the uncertainty of a statistical model to the most. For example, in cotesting [Muslea et al. 2000], a multiview active learn-ing algorithm, the most informative instance is the unlabeled instance on which the two learners trained from different views exhibit the largest disagreement. Query-by-committee [Seung et al. 1992; Freund et al. 1997], uncertainty sampling [Lewis and Catlett 1994] and committee-based sampling [Dagan and Engelson 1995] also fall into this type. The second type of approaches query the label of the most representative instance which is expected to characterize the pattern in the data to the best, where the most representative instance is often selected based on the cluster structure of unlabeled data [Nguyen and Smeulders 2004; Dasgupta and Hsu 2008]. The third type of approaches query instances that are both informative and representative [Donmez et al. 2007; Huang et al. 2010]. Theoretical aspects of active learning have been stud-ied by many researchers [Balcan et al. 2006; Hanneke 2007; Dasgupta et al. 2007], and recently it has been shown that multiview active learning is able to achieve an exponential improvement of sample complexity in setting close to real tasks [Wang and Zhou 2010a].

Zhou et al. [2006] showed that it is possible to combine disagreement-based SSL and AL to get a promising performance in content-based image retrieval. Wang and Zhou [2008] theoretically verified that such a combination is really able to get an improved sample complexity than pure SSL or pure AL. Many investigations reduce the simulation costs for DSE by analyzing program char-acteristics. Hamerly et al. [2006] simulated only some representative program phases rather than the whole program. From the perspective of program, Joshi et al. [2006] found a reduced representative subset of programs by cluster analysis based on inher-ent microarchitecture-independent characteristics. Moreover, statistical simulation was employed to construct a synthesized shorter program to emulate the execution characteristics of the original program [Genbrugge and Eeckhout 2009]. Unlike the above approaches, predictive modeling techniques reduce simulated design configura-tions by learning the relationship between design parameters and processor responses. Following the supervised learning framework, the preceding task was accomplished by linear regression model [Joseph et al. 2006] or Artificial Neural Networks (ANNs) [  X  Ipek et al. 2006; Khan et al. 2007; Cho et al. 2007; Dubach et al. 2011], where ANNs are most widely used. Inspired by active learning,  X  Ipek et al. [2006] proposed the intelligent sampling technique to enhance the accuracy of the supervised ANN approach. This technique repeatedly updates an ensemble of 10 ANNs trained by 10-fold cross validation over the labeled design configurations, and iteratively labels (simulates) the unlabeled configurations on which the ANNs present largest disagreements. Benefited from these techniques, architects no longer need to simulate an excessively large number of design configurations. However, since the usefulness of unlabeled design configurations is ignored, the above approaches still suffer from either high simulation costs (for achieving high accuracies) or low prediction accuracy (given limited computational resources). configuration with d interested design parameters, and y i is the corresponding proces-sor response, for instance, performance metric like Instruction-per-Cycle (IPC). Let U be the unlabeled data set, that is, the set of design configurations without simulation.
The most critical issue in semisupervised regression is how to estimate the labeling confidence such that the most confident unlabeled instance can be selected to label. Following Zhou and Li [2005a] X  X  approach, we consider that the error of the regressor on the labeled data set should decrease the most if the most confident unlabeled instance is labeled. Formally, for each unlabeled instance x s , the quality of x s can be measured using a criterion as shown in Equation (1): where h is the original regressor, and h is the regressor refined with instance x s and its label. In other words, the instance with the best mean squared error (MSE) reduction on the labeled set will be selected to label.

Another challenge during the design of COAL is that, repeatedly measuring the MSE of regression tree on the entire labeled data set in each iteration is time-consuming. To address this problem, COAL utilizes the local information of constructed regression tree to improve the update efficiency. Specifically, given an unlabeled instance which would be labeled by the original regression tree, it should fit into a linear model that lies in a leaf of the tree. Thus, we approximately measure the labeling confidence of each unlabeled instance by computing only the MSE of its siblings locating in the same leaf of the tree, where siblings refer to those labeled examples. In this case, the Promising Labeled Example (PLE) of an unlabeled data set, denoted by x , is determined by maximizing the local error reduction ( ( x s )) defined in Equation (2): where s is sibling set of x s in the original tree, m is the original regressor, and m is the regressor refined by ( x s , y s ), y s = m ( x s ). Following the cotraining paradigm, COAL uses two base regression trees, each of which will label the PLE for the other tree during the learning process. Notice that, COAL does not require two views. Similar to C OREG [Zhou and Li 2005a] and other single-view disagreement-based SSL approaches, the validity of COAL can be justified by the recent theoretical results [Wang and Zhou 2007, 2010b]. More information on cotraining and other disagreement-based SSL techniques can be found in the recent survey [Zhou and Li 2010].

On the other hand, at each iteration COAL selects and simulates an unlabeled configuration. To be specific, the unlabeled configuration on which two regression trees present the largest disagreement, denoted by x a , will be labeled by simulation: where U is a large set of unlabeled instances.
 Algorithm 1 presents the pseudo-code of COAL algorithm. As the first step, the M5P algorithm is used to construct two diverse regression trees (say, m 1 and m 2 )via employing distinct parameters M 1 and M 2 respectively, where M 1 and M 2 determine the minimal number of examples in each leaf of m 1 and m 2 , respectively [Wang and Witten 1997]. Let L 1 and L 2 be the current labeled data sets of m 1 and m 2 respectively, and L 1 and L 2 are initialized by the same labeled data set L . In each iteration, COAL uses the PLE determined by m 1 to augment the labeled set L 2 , and vice versa. After that, we should replenish the pool U with unlabeled instances to size p for next iteration. After using the latest labeled sets to update two regression trees, COAL selects the unlabeled configuration with the largest disagreement to simulate, and updates the regression trees using the newly labeled configuration. It is notable that the unlabeled data selected for simulation is chosen from the entire unlabeled data set U instead of the pool U . Finally, we average the prediction on each updated regression tree as the final prediction.

It is possible to employ more base learners for disagreement-based semisupervised learning as tritraining [Zhou and Li 2005b] and coforest [Li and Zhou 2007] did, and for active learning as query-by-committee [Seung et al. 1992] did. However, the com-bination of such kind of semisupervised learning and active learning techniques will encounter a big challenge, that is, how to maintain the diversity among the learners in the learning process, as it is well known that the diversity is crucial for disagreement-based approaches to succeed [Wang and Zhou 2007]. Thus, our approach only uses two base learners, and such a process has a sound theoretical support [Wang and Zhou 2008]. Figure 2 illustrates how COAL deploys a regression model for DSE. As the first phase, COAL labels a number of randomly generated design configurations by the simulator (simulation cluster), which constructs the initial labeled data set. At each learning phase of COAL, the semisupervised learning is utilized to select and label two un-labeled design configurations. After that, COAL updates each regression tree by the configuration labeled by the other tree. Using the updated trees, COAL invokes the active learning engine, via which COAL finds and simulates the unlabeled design configurations on which the trees show the largest disagreement. This newly labeled design configuration is then used to further update both trees. Such a procedure is repeated for several times until the predefined iteration is reached. Benefited from the collaboration of cotraining semisupervised learning and active learning, COAL provides us accurate predictions of processer responses at the cost of simulating only a few design configurations. In this section, we carry out empirical study to evaluate the performance of COAL. The performance of COAL is first compared against that of a state-of-the-art DSE approach. After that, we evaluate the impacts of active learning and semisupervised learning on the performance of COAL, respectively.
 4.1.1. Processor Simulator and Design Space. The most common way to evaluate the performance of a processor during DSE is to measure the execution time of many popular benchmarks on cycle-accurate simulators. Here we employ renowned Sim-plescalar [Austin et al. 2002] as the prototype simulator of a modern superscalar microprocessor, which is run on a cluster with Intel Xeon processors. Each design configuration considered in DSE, as shown in Table I, consists of 10 different design parameters, and the total number of design configurations in the design space exceeds 70 million. 4.1.2. Benchmarks. According to Hennessy and Patterson [2003], the performance of a general-purpose processor should be evaluated over a number of representative bench-mark programs. The most famous benchmark suite covering various application fields was created by SPEC (Standard Performance Evaluation Corporation) [Hennessy and Patterson 2003]. SPEC benchmarks consist of real programs with slight modifica-tion for portability, and are widely utilized by architects, researchers, and computer vendors for performance evaluation. For example, vendors of desktop computers and servers periodically submit the performance results measured by SPEC benchmarks to www.spec.org to provide fair comparisons with other machine products.

SPEC CPU2000 is the fourth generation benchmark suite of SPEC series, which con-sists of a set of 11 integer benchmarks (CINT2000) and 14 floating-point benchmarks (CFP2000). SPEC CPU2000 aims at providing fair evaluations of general-purpose pro-cessors. To validate the effectiveness of COAL on different programs, we consider 14 representative programs with distinct behaviors from SPEC CPU2000 as applu , apsi , SPEC CPU2000 offers 3 inputs with different sizes for each benchmark, that is, test , train ,and ref , and the corresponding size increases in order. Since the size of ref input is the largest but the closest to the size of real-world input, it can offer a promising approximation to the behavior of a real application. Here it is adopted as the input in our experiments. Meanwhile, due to the extremely slow simulation speed, we adopt a statistical program sampling technique called SimPoint [Sherwood et al. 2002] to only simulate 100 million representative instructions (which is typically about 1% of all dynamic instructions) selected from the entire execution of each benchmark to obtain the approximate responses (e.g., performance or power) 1 . 4.1.3. Metric. In our study, we use a conventional metric, called the Mean Square Error (MSE), to measure the prediction accuracy of each DSE approach: where y k represents the predicted response (e.g., IPC) of the k -th configuration ( k = 1 ,..., N ), and  X  y obtained by cycle-by-cycle simulation. To compare the prediction accuracy of COAL with that of the state-of-the-art (ANN), the MSE of COAL on the testing data will be normalized to that of the ANN DSE approach introduced by  X  Ipek et al. [2006]. 4.1.4. Parameter Setting. In our experiments, the initial training set of COAL consists of 300 randomly generated design configurations which have been labeled by cycle-accurate simulations on the processor simulator. The number of iterations ( t in Algo-rithm 1) in which SSL+AL is carried out is set to 100, which implies that 100 extra unlabeled design configurations, selected in the AL process, will be labeled by cycle-accurate processor simulations; 200 unlabeled design configurations will be labeled by the regression trees in the cotraining SSL process. The entire unlabeled data set ( U in Algorithm 1) consists of 100K randomly generated unlabeled design configurations, which are prepared for both SSL and AL processes. For the SSL, the size of pool con-taining unlabeled design configurations ( p in Algorithm 1) is set to 100 as in [Guo et al. 2011]. Moreover, M 1 and M 2 (the minimal numbers of examples in each leaf of two trees, respectively) are set to 4 and 10, respectively, to obtain two diverse M5P regres-sion trees. In order to test the performance of COAL, additional 100 different design configurations are simulated as the testing data. We did not finely tune the parameters because our task is quite different from common machine learning problems owing to the great simulation cost. For example, simulating a design configuration on a SPEC benchmark taking the ref input may consume hundreds of hours.

To demonstrate the effectiveness of COAL, we compare it with the ANN DSE ap-proach proposed by  X  Ipek et al. [2006], and a supervised M5P regression tree. Following the setting utilized by  X  Ipek et al. [2006], the ANN adopts one 16-unit hidden layer, a learning rate of 0.001, and a momentum value of 0.5. The minimal number of examples in each leaf of the M5P tree is set to 4. In our experiments, both ANN and M5P mod-els are constructed by a training set consisting of 400 labeled design configurations. Among the 400 labeled configurations, 300 labeled design configurations are the same to the configurations in the initial training set of COAL, and other 100 configurations are generated randomly and labeled by cycle-accurate processor simulations. In addi-tion, we also provide the performance data of the intelligent-sampling-based ANN DSE approach (ANN-IS for short), a variant of the ANN DSE approach proposed by  X  Ipek et al. [2006], as a reference. Following the setting suggested by  X  Ipek et al., ANN-IS repeatedly updates an ensemble of 10 ANNs trained by 10-fold cross validation over the labeled design configurations, and iteratively labels (simulates) the unlabeled con-figurations on which the ANNs present largest coefficients of variance (ratio of the standard deviation to mean). ANN-IS shares the same initial training set with COAL, and all approaches share the same testing data with COAL. Figure 3 compares the normalized MSEs of ANN, ANN-IS, the supervised M5P tree and COAL. We can clearly see that COAL outperforms other approaches over all 14 benchmarks. In average, COAL reduces the MSE by 58% of ANN. Most notably, COAL reduces the MSE by 95% of ANN on the benchmark swim . Even on the benchmark with the least MSE reduction, that is, perlbmk , the MSE reduction can still achieve 35%. Comparing with ANN-IS, COAL reduces the MSE by 51% in average. Hence, we can conclude that COAL is much more practical than state-of-the-art DSE techniques due to its high accuracy.
 Figure 3 also presents the comparison between supervised M5P regression tree and COAL. It can be observed that COAL reduces the MSEs of M5P over all 14 benchmarks. In average, COAL reduces the MSE by 42% of M5P. Hence, it can be concluded that the exploitation of unlabeled configurations in COAL can significantly improve the prediction accuracy. 4.3. Sensitivity to Training Iteration t Training iteration t determines the number of unlabeled design configurations that are used for improving prediction accuracy. The experiment conducted here follows the same parameter setting (except t ) as the previous experiments.

Figure 4 shows the relationship between the MSE of COAL and t , where 8 bench-marks, applu , apsi , bzip2 , crafty , eon , lucas , mgrid and vortex , are considered. In gen-eral, the MSE (red curves in Figure 4) roughly decreases as the number of iterations increases. For example, on benchmark applu , the MSE of the COAL trained with 100 iterations is 63.5% of that of the COAL trained with only 10 iterations. However, when the number of iterations has become large enough, on several benchmarks the MSE may decrease with a relatively slower speed. This is not difficult to understand. On one hand, according to theoretical studies [Wang and Zhou 2007], disagreement-based approaches require a large difference between the learners, while when a large num-ber of iterations has been executed, the learners will become too similar to enable a further performance improvement. On the other hand, after a large number of labeled examples are obtained, the gains from exploiting unlabeled data become smaller. Figure 4 also illustrates the training cost of COAL given different training iterations. We can clearly see that the training cost of COAL grows almost linearly with respect to t (black curves in Figure 4). Hence, if the computational resources are limited, it would be beneficial if t is determined by trading off the gained prediction accuracy and training costs, which can be achieved by setting more sophisticated stopping criterion for COAL. The related investigation will be conducted in our future work.
 The computational cost of COAL for solving a DSE problem contains two parts, the simulation cost spent on obtaining the actual labels of design configurations, and the training cost spent on selecting unlabeled data and updating the regression trees. Figure 5 compares the simulation cost and training cost consumed by COAL on different benchmarks. We can see that the simulation cost always overwhelmingly dominates the training cost. In average, the simulations consume 90% of the computational cost. On benchmark mesa , the simulation cost even exceeds 97% of the entire computational cost. It is worth noting that for each benchmark (benefited from SimPoint [Sherwood et al. 2002]) we only simulate 100 million representative dynamic instructions, while in industry all dynamic instructions (more than 10 billion for most benchmarks) should be simulated. Under this circumstance, the training cost of COAL can be completely neglected in practice. So far we have validated that the graceful combination of SSL and AL contributes significantly to the high accuracy of COAL, and found promising solution to the DSE problem. However, it is still not clear whether AL and SSL are both necessary to COAL. In order to gain more insight on this issue, we design additional experiments to study the impact of AL and impact of SSL, respectively. To be specific, we turn off the AL phase in Algorithm 1 and obtain a DSE approach called noAL (as shown in Algorithm 2, noAL is identical to C OMT proposed in our preliminary study [Guo et al. 2011]); We turn off the SSL phase in Algorithm 1 and obtain a DSE approach called noSSL (whose flow is shown in Algorithm 3).

In our experiments, noAL follows the same parameter setting as COAL, except that noAL uses the size-400 training set as supervised ANN and M5P while COAL uses an initial training set with 300 labeled configurations; noSSL follows the same parameter setting as COAL (but does not require the parameter p anymore). A summary of performance of COAL, noAL, noSSL, supervised ANN, ANN-IS and M5P is presented in Table II. According to this summary, COAL performs the best among the 5 DSE approaches on 8 out of 14 benchmarks, noAL and noSSL perform the best on 3 benchmarks, respectively. In the average sense, COAL significantly outperforms all other approaches, that is, it reduces the average MSE by 58%, 42%, 51%, 21% and 28% compared with ANN, M5P, ANN-IS, noAL and noSSL, respectively. Moreover, it is worth noting that approaches leveraging unlabeled configurations (COAL, noAL and noSSL) significantly outperform approaches do not exploit any unlabeled configuration (ANN and M5P). The above observations again validate the usefulness of unlabeled configurations, and strongly support to exploit unlabeled design configurations in DSE.
Compared with the noAL approach, COAL outperforms it on 11 out of 14 bench-marks, for instance, on benchmarks swim and vortex COAL can reduce the MSE by about 58% of that of noAL; Compared with noSSL approach, COAL also outperforms it on 11 out of 14 benchmarks, for instance, on the benchmark vortex COAL can reduce the MSE by about 76% of that of noSSL. One reason that COAL does not always improve the performance might owe to the fact that there are some similar architec-tural configurations with different functional behaviors. Such issues can be address by adding some additional features that are able to distinguish these architectural configurations. This will be one of our future work. Moreover, it is well known that sometimes semisupervised learning might degenerate performance; however, recently there are some proposals on safe semisupervised learning [Li and Zhou 2011], and it is possible to introduce these techniques for further improvement.
 Fortunately, adopting both AL and SSL achieves the best performance at most time. More specifically, when COAL is beaten by noAL on benchmarks eon , parser ,and perlbmk , it can still achieve significantly better performance than that of noSSL on these benchmarks. When COAL is beaten by noSSL on gzip , lucas ,and sixtrack , it still significantly outperforms noAL on these benchmarks. The above observation reveals that the prediction accuracy may vary from one benchmark to another when exploiting unlabeled configurations by SSL or AL, but such performance variance can be signifi-cantly reduced if we combine SSL and AL together (cf. Table II). In other words, COAL exhibits more stable performance in comparison with noAL and noSSL, which will be favored by computer architects and engineers. Ideally, to validate the effectiveness of COAL, we have to simulate the entire design space consisting of 70 million design configurations, then compare the configuration de-duced by COAL and the actual optimal configuration. However, this is infeasible due to intractably large simulation costs. A compromise is made by comparing the promising configuration deduced by COAL with the configuration deduced by the state-of-the-art DSE approach (ANN). To be specific, we simulate the two architectural configurations on two illustrative benchmarks, and compare the corresponding processor responses (performance and power) directly.

As an example, we employ the benchmarks bzip2 and crafty in the experiments. Sup-pose the design specification is that  X  X aximizing the performance with the constraint that the power consumption must be less than 100 watt X , we can attain the promising design configurations as shown in Table III, where the promising configurations found by its opponent (state-of-the-art DSE approach using ANN) are also presented. We can clearly see that ANN and COAL suggest different design configurations for these two benchmarks given the above design specification.

Table IV further presents the corresponding performance and power of these two configurations (namely,  X  X NN-conf X  and  X  X OAL-conf X ). On both benchmarks, the actual performance (obtained by cycle-accurate simulations) of COAL-confs is better than that of ANN-confs given the 100 watt power constraints. It is notable that on benchmark crafty , the predicted and actual performance/power of COAL-conf are very close while those of ANN-conf are quite different. Quantitatively, the relative error of the predicted IPC of ANN-conf over the actual IPC is 21.5%. Hence, we can conclude that COAL is more effective than the state-of-the-art ANN-based approach. In contrast to traditional DSE approaches that only consider labeled design configura-tions, this article proposes to leverage unlabeled design configurations to help improve the performance, which extends our preliminary research [Guo et al. 2011]. By ex-ploiting unlabeled design configurations via both semisupervised learning and active learning, our proposed COAL approach significantly improves prediction accuracies and reduces excessive simulation costs of DSE. Experimental results on the design space of a modern superscalar microprocessor show that the MSE reduction of COAL over the state-of-the-art DSE approach (ANN) ranges from 35% to 95% over 14 eval-uated benchmarks, and the average MSE reduction of COAL is 58%. COAL enables effective and efficient deduction of the optimal architecture for every given benchmark program, which greatly benefits the DSE of microprocessor design. In practice, COAL has been utilized in the design of the next-generation Godson processor core.
Currently in our proposed method, the semisupervised learning and active learning components are executed with almost equal importance. It is possible to introduce a tradeoff parameter to enable them to have different amount of contributions, and this will be one of our future work. Besides, it is interesting to study how much minimum the labeled data would be required. Zhou et al. [2007] disclosed that when there are two sufficient views, semisupervised learning with even a single labeled example is possible. However, for our current proposed method, and particularly for DSE application, this also remains an interesting future issue.

It is worth noting that the current version of COAL is proposed for pre-silicon design phase of microprocessor, that is, deciding an appropriate design configuration before manufacturing the chip. In the future, we will continue to develop new versions of COAL to conduct power-efficient post-silicon microprocessor reconfiguration which can adapt to various programs or even some given program features. This task would be a crucial step towards the development of an elastic processor (by which we call a proces-sor whose architecture parameters can be dynamically reconfigured to suit different programs) and a computer tribe (by which we call a series of downward compatible elastic processors). Unlike the Field Programmable Gate Array (FPGA) whose recon-figuration may have to modify millions of controlling parameters, an elastic processor only employs a moderate number of reconfigurable parameters, which alleviates the problem of dimension explosion when building performance/power regression models for guiding architecture reconfiguration.

