 Answer typing is commonly thought of as finding appropri-ate responses to given questions. We extend the notion of answer typing to information retrieval to ensure results con-tain plausible answers to queries. Identification of a large class of applicable queries is performed using a discrimina-tive classifier, and discriminative preference ranking meth-ods are employed for the selection of type-appropriate terms. Experimental results show that type-appropriate terms iden-tified by the model are superior to terms most commonly as-sociated with the query, providing strong evidence that an-swer typing techniques can find meaningful and appropriate terms. Further experiments show that snippets containing correct answers are ranked higher by our model than by the baseline Google search engine in those instances in which a query does indeed seek a short answer.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Experimentation, Measurement, Performance
The tasks of question answering (QA) and information re-trieval (IR) both provide a means of locating information rel-evant to a request, albeit in slightly different ways. QA pro-vides a natural question and answer interface that reduces the amount of information returned, but requires additional work on the part of the system to find correct answers. This additional work has led to QA systems being deployed on a much smaller scale than IR systems. IR systems are often only aware of questions in a limited sense, such as answer-ing only questions matching a certain pattern, and exhibit reduced performance on other questions.

In this paper, we seek to apply the QA notion of answer typing to generate better responses to queries. Specifically, we wish to identify type-appropriate terms for these queries such that standard IR responses (such as snippets) include terms that satisfy the user X  X  information needs. To take advantage of typeability we introduce methods to 1) iden-tify queries that can benefit from typing, 2) identify type-appropriate terms for these queries, and 3) use these terms to promote results.

Experimental results show that snippet terms scored by our model are preferred nearly twice as often as the most frequent terms shared by the top 100 snippets. Further ex-periments show that snippets containing answers are ranked higher by our model than by the Google baseline for those cases in which queries have a complete or partial short an-swer. An increase of 0.05 in mean reciprocal rank (MRR) is observed, resulting in a MRR value of 0.567. This MRR value is quite high, especially considering the fact that mul-tiple results are displayed to a user.

Information retrieval is a central component of many QA systems [5, 14]. Although IR techniques have been investi-gated with the specific goal of QA in mind [13], the applica-tion of QA techniques specifically with IR in mind has been largely unexplored. However, because document retrieval (the IR component of QA) can benefit from increased aware-ness of the question [22, 2], certain aspects of the QA process have been transplanted into document retrieval. Although the application of Natural Language Processing (NLP) tech-niques to the IR task is not a new concept [21, 8], ours is one of the few works that seeks to transfer the notion of an automatically-detected answer type into IR or document retrieval. However, work on templated queries [9] allows for the incorporation of type information specified by the user.
Answer typing is a central component for many QA sys-tems (e.g., [3, 5]). In the context of QA, answer typing seeks to identify those candidate answers that are plausible or appropriate as responses to the question. Answer typing is most often performed by assigning one or more answer type classes to the question, either via rules or via question classification such as that performed by the system of Li and Roth [11]. The work we follow for answer typing in this paper is that of Pinchak et al. [19] in which answer typing is performed without the use of answer type classes.
Before we can apply answer typing to queries, we must first identify those queries that are naturally amenable to typing. A query stream, such as the one found in the AOL query logs [16], contains a wide variety of queries searching for different kinds of information. Queries can be roughly divided into short-answer informational , long-answer infor-mational , navigational , and resource . Informational queries are seeking some piece of information about a subject, and the response may be short-answer (e.g.,  X  X ities in Canada X ) or long-answer (e.g., X  X auses of WWII X ). In contrast to infor-mational queries, navigational queries are seeking some spe-cific page (e.g.,  X  X oogle image search X ) and resource queries are looking for some comprehensive source of information on a subject (e.g., the Wikipedia page for Britney Spears).
Lapata and Keller [10] describe a task of compound noun interpretation in which some compound noun phrases, such as  X  X ar stories, X  can be rearranged or rewritten as preposi-tional queries, such as X  X tories about war. X  Many queries en-countered in a query stream are compound noun phrases and so we wish to develop techniques that cover these queries. Therefore, we propose that compound noun queries can be normalized by transforming them into their most likely prepo-sitional rewriting thus simplifying our task to considering only those queries that already contain prepositions.
Although the set of prepositional queries is homogeneous in structure, we must further filter the queries to remove those unlikely to benefit from a notion of answer type. For this task, we make use of large-margin discriminative learn-ing as implemented by a Support Vector Machine (SVM) classifier [6] for which we identify the set of useful features summarized in Table 1. Because we only consider prepo-sitional queries, we can break each query into a left-hand side (LHS) and right-hand side (RHS). The LHS contains only the root noun of the query. The RHS includes both the preposition and the root of the prepositional attachment. Cluster information is used to increase the amount of over-lap between queries that are not identical.

The purpose of our SVM is to identify queries that benefit from our notion of typing. To provide training data, the first author of this paper labeled a set of 2000 queries according to whether or not the answer typing methods described next are likely to be of benefit to the query. A single annotator was used because of the high degree of subjectivity of the task; it is not always clear what exactly defines a typeable query and some familiarity with the answer typing method is required to identify cases where typing is likely to help. Ex-periments on a small test set of 200 queries show an accuracy of 87.5% for the query identification model (25 errors), with errors being divided according to the proportion of positive and negative examples (one-third and two-thirds, respec-tively). Our experiments in Section 5 show that this level of performance produces results comparable to our baseline approach, and assuming perfect accuracy leads to clear im-provements over the baseline.
Answer typing for QA typically takes the form of assign-ing one or more pre-defined type categories to a given ques-tion [11]. Pinchak and Lin [19] approach the problem of answer typing for QA by directly modeling the probability of a candidate answer being appropriate to a given question without the use of an intermediary type category. Following this work, we make use of discriminative preference ranking to order a given list of candidate answers according to how appropriate they are to a given question. Because Pinchak et al. [19] use support vector machines (SVMs), we use the SVM light package [7] to perform discriminative preference ranking.
We begin by considering only those prepositional queries identified as typeable by the model discussed above in Sec-tion 2. From the queries, we generate query contexts from the LHS and from the combination of LHS with RHS. For example, the query  X  X ities in Canada X  generates the context  X  X is a city X  from the LHS and  X  X is a city in Canada X  from the combination of LHS and RHS. Here X is a placeholder for the expected appropriate term.

Given that these queries were originally intended for infor-mation retrieval, we submit them as-is to the popular Google search engine and obtain a ranked list of results along with their snippets. The top 100 snippets are tagged [18] and chunked [17] to extract a set of candidates that are then ranked by the model. These ranked candidates form an or-dered candidate list .

A discriminative preference ranking model grants us the flexibility of using many diverse features. The feature tem-plates we use are in Table 2. As the basis of the model, we rely on an expected value of candidate answer t appearing in context c . This value is calculated using a list of similar words along with their similarity values. To balance these expected counts, we include the actual observed counts of candidate t in context c , C ( t, c ), along with the individual counts of the candidate t and context c in our corpus [1].
To these basic counts, we add five additional kinds of fea-tures that do not rely on query contexts. The first, denoted by W ( t ), is the estimated depth of the candidate t in the WordNet hierarchy [4]. Should the candidate not appear in WordNet, we estimate the depth of the candidate by aver-aging the depth of words with high similarity ( t 0 ) accord-ing to the clusters of Pantel and Lin [15]. We include a feature for the number of times t occurs on the candidate list, S ( t ). Through our extraction of candidates from snip-pets, appropriate candidates are often repeated a number of times. LHS ( t, q ) is simply a flag that fires whenever the LHS of a query (such as X  X ity X  X n X  X ities in Canada X ) appears as part of the candidate. We also include a flag that fires when a candidate contains one or more capitalized letters, U ( t ), and the integer number of space-delineated words in the candidate, T ( t ).

For training data, two annotators identified appropriate candidates in the top 20 snippets returned for each query from a total of 200 queries randomly selected from the posi-tive training examples used for query identification (Section 2). We observed an inter-annotator agreement (kappa) of 0.68, which is relatively low. Because of this relatively low level of agreement, we chose to train on the intersection of the labels.

Once the candidates have been ordered according to our preference ranking model, we select the top 20 candidates for scoring snippets. The score of a snippet s is calculated as the average number of candidates per fragment : where in ( t, s ) = 1 if snippet s contains candidate t (0 oth-erwise) and f rag ( s ) is the number of fragments delineated by  X . . .  X  in the snippet. Fewer fragments indicates a more cohesive snippet; highly-fragmented snippets require more appropriate candidates to receive a high score.

Given that Google often performs well at returning rel-evant results, we do not wish to venture too far from the Google ordering without good reason. To this end, we use an interpolated model most often employed for smoothing [12]. Our final score for a snippet is therefore: The primary goal of applying answer typing techniques to IR queries is to improve the relevance of results returned to the user. Given that results will be rescored based in part on the terms they contain, we propose two different experiments to evaluate the quality of this reordering. These experiments make use of an  X  parameter of 0 . 4, determined using a held-out development set. When looking at snippets, we examine only the top five snippets.
Our candidate ranking experiment is meant to measure the quality of terms selected to rerank snippets in compari-son with terms that are frequent. For this task, we introduce a side-by-side (SxS) experiment in which results from two alternatives are compared next to one another. Annotators are then asked to choose which side provides a better result. Displaying a list of appropriate terms does not conform to the task of IR, but allows us to determine whether or not our model can find relevant results.
 Annotations for this experiment are collected from the Amazon Mechanical Turk (AMT) system. 1 AMT results, when averaged, have been shown to have high agreement with expert annotators [20] even though Turkers are not experts. We take advantage of this by requiring a minimum of five judgements on any one set of query responses. One side is preferred over the other if and only if we observe a majority of votes (i.e.,  X  3) for that side.

Table 3 shows the results of comparing our terms with the most frequent terms for a set of 996 queries judged as typeable by the query identification model of Section 2. This set of queries includes those that are erroneously identified as answerable because useful terms can exist for queries that are not answerable by short answers alone. The results in Table 3 show the clear advantage of our model over the most frequent terms indicating that our model is able to identify terms appropriate to this particular subset of queries.
The encouraging results of the previous section lead to a further experiment in which snippets are reranked according to our model and compared with the original Google order-ing. This experiment deals only with those queries for which there exists some short answer. Of the 996 queries used in the prior SxS experiment, 331 are strictly determined to be short-answerable. Snippets provide additional information and context for answers and are the expected response to IR queries. As a result, snippets that include a short an-swer to the query allow a user to find desired information along with some context without having to visit additional external Web pages.

Annotators were asked to identify the position of the first snippet containing a short answer to the query. Two anno-tators were used instead of Amazon Mechanical Turk due to http://www.mturk.com/ the attention to detail required to identify short answers in snippet text. The answer was allowed to be partial for cases in which a list of answers is sought or more than one answer is correct. Only the top 5 snippets of our two systems were presented to the annotators to produce a measure of top-5 reciprocal rank. The results for both systems are presented in Table 4. The two annotators agree on over 80% of the queries, indicating high confidence in these values.
The results of Table 4 show a slight improvement over the high performance of Google-ranked snippets. Given the al-ready high performance of Google, we conclude that Google offers relatively few opportunities for which we can improve results. The fact that we show a significant improvement for this set of queries means that we are able to capitalize on those rare situations in which Google provides an overly general response.

Given the fact that Google performs well at finding cor-rect answers for our set of queries, it is worthwhile to ex-amine how often our model has an opportunity to improve performance. We observed that 176 (53%) of queries are answered by the first or second snippet provided by Google. This subset of queries offers only a slight opportunity for improvement. In spite of this fact, we observe a significant improvement in the number of queries correctly answered by the first two snippets provided by our interpolated model, up from 176 (53%) to 217 (66%). This means that a cor-rect answer exists in the first two snippets for 2/3 of the queries that are identified as strictly answerable. Placing such snippets high in a ranked list is very important for an IR system, and our model successfully increases the number of queries that can be answered by considering only the first two snippets provided by our model.
Incorporating a notion of type-awareness into information retrieval is desirable for those queries that can benefit from typing. We have presented here a simple method by which such queries can be identified along with a means of identi-fying terms that are type-appropriate for a query. On their own, these terms are much better than common terms found in results; if a user is searching for one or a few specific in-stances then these terms may well satisfy their information needs. When these terms are used to rerank snippets, we ob-serve a slight but significant improvement in finding correct answers to queries.
We would like to thank the Natural Sciences and Engineer-ing Research Council of Canada and the Alberta Informatics Circle of Research Excellence for their support of this work. [1] The AQUAINT Corpus of English News Text.
 [2] M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. [3] E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. [4] C. Fellbaum. WordNet: An Electronic Lexical [5] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, [6] T. Joachims. Making Large-Scale SVM Learning [7] T. Joachims. Optimizing Search Engines Using [8] B. Katz and J. Lin. Selectively Using Relations to [9] G. Kumaran and J. Allan. Information Retrieval [10] M. Lapata and F. Keller. Web-based Models for [11] X. Li and D. Roth. Learning Question Classifiers. In [12] C. Manning and H. Sch  X  utze. Foundations of Statistical [13] C. Monz. From Document Retrieval to Question [14] E. Nyberg, R. Frederking, T. Mitamura, M. Bilotti, [15] P. Pantel and D. Lin. Document Clustering with [16] G. Pass, A. Chowdhury, and C. Torgeson. A Picture of [17] X. Phan. CRFChunker: CRF English Phrase [18] X. Phan. CRFTagger: CRF English POS Tagger. [19] C. Pinchak, D. Lin, and D. Rafiei. Flexible Answer [20] R. Snow, B. O X  X onnor, D. Jurafsky, and A. Ng. Cheap [21] T. Strzalkowski, L. Guthrie, J. Karlgren, [22] J. Tiedemann. Improving Passage Retrieval in
