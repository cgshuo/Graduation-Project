 1. Introduction
The actual European energy context reveals that the building sector is one of the largest sectors of energy consumption. In France, about 25% of greenhouse gases (GHG) emissions and 45% of energy consumption are due to buildings ( Agence De  X  parte-mentale, 2007 ). Consequently, the adopted  X  X  X nergy Performance of Buildings Directive X  X  ( Official Journal of the European Commu-nities, 2002 ), focusing on energy use in buildings, requires all the European Union (EU) members to enhance their building regula-tions and improve energy efficiency. With the aim of surmounting the actual energy crisis (mainly caused by both the rarefaction of fossil fuels and excessive energy consumption), managing energy demand, promoting renewable energy and finding ways for energy savings are worldwide concerns ( Europe X  X  energy position, 2008 ).

The present paper, the first of a series of three dealing with the modeling and the optimization of a district boiler as part of the
OptiEnR project, focuses on the development of a forecasting tool using a wavelet-based multi-resolution analysis and artificial neural networks. The OptiEnR project began in late 2008 and will proceed, at least, until late 2010. It includes researchers from the
ELIAUS laboratory of the University of Perpignan Via Domitia (south of France) and engineers from two French companies, Cofely GDF-SUEZ ( http://www.cofely-gdfsuez.fr/en/homepage/ );
Weiss France ( http://www.weiss-france.fr/index.php?_lngwe b=en ). Cofely GDF-SUEZ is currently the leading European brand for environmental and energy efficiency services while Weiss
France designs, manufactures and installs automatic boiler rooms for all types of wood, biomass, wastes and fluids. The considered district boiler, which will be described later in the paper, is situated at La Rochelle, west coast of France, and managed by
Cofely GDF-SUEZ. It supplies domestic hot water and heats to residential and public buildings, using mainly wood and some-times fuel or gas if necessary. The OptiEnR project focuses on optimizing the performance of the boiler, adding to the plant a thermal storage unit and using a model-based predictive optimal controller. Its main objective is to minimize the use of fossil energy, stocking renewable energy during low-demand periods and using it during high-demand periods. The project consists of the following successive tasks ( Fig. 1 ): the first task is to forecast outdoor temperature and thermal power consumption (of the hot water distribution network); the second task focuses on modeling the district boiler; the third task deals with studying the feasibility of integrating to the plant a thermal storage unit and finding the most adequate storage material; the fourth task is to model the thermal storage, while the fifth task focuses on optimizing the boiler functioning using both a model predictive controller (MPC) ( Garcia and Prett, 1989; Qin and Badgwell, 2003 ) and the forecasted parameters (first task). As above-mentioned, the present paper deals with the first task, the next papers of the series dealing with the tasks 2 (paper 2) and 3 X 5 (paper 3).
Schemes for hourly temperature forecasting have been mainly developed in the context of short-or long-term load forecasting and power utilities management. The complex and nonlinear nature of temperature variations and the abundance of historical data suggest that computational intelligence data-based modeling techniques would be good candidates to forecast temperature. Liu et al. (1996) evaluated fuzzy logic, artificial neural networks and auto-regressive models for very short-term load forecasting. They concluded that the just-mentioned tools can be good candidates for this application. Khotanzad et al. (1996, 1997) also used backpropagation neural networks to forecast hourly temperature for the next seven days, using daily high and low temperatures and an adaptive daily update of the weights. The main drawback of this approach is that the large size of the neural networks involved leads to a large number of weights to be optimized.
Hwang et al. (1996) also used artificial neural networks and proposed a new and effective learning algorithm to allow solving both the problems of underfitting and overfitting. Yoo and
Pimmel (1999) developed in 1999 a self-supervised adaptive neural network to perform short-term load forecasts. They used this kind of neural networks to extract correlational features from temperature and load data. The levels of error compared favorably with those given by other techniques. Sharif and Taylor (2000) used separate artificial neural networks for load forecasting of 1 X 4 h ahead. We can also highlight that Gonz alez and Zamarren  X  o (2001) used in 2001 a radial basis functions neural network to develop a short-term temperature forecaster, without using the daily high and low temperature values. They obtained good results. One year later (i.e. 2002), the same authors, Gonz alez and
Zamarren  X  o (2002) , proposed a short-term hourly environmental temperature forecaster based on a state space neural network. Its training was based on a random optimization method. Because of the non-stationary characteristic of temperature, training was executed daily with the aim of updating the network weights. The outdoor temperature dynamics was satisfactorily captured. How-ever, and, as it was highlighted by Hippert et al. (2000) , a large input dimensionality compared with the number of training records can lead to unstable networks and/or overfitting. As a consequence, one can remark that Tassadduq et al. (2002) used a backpropagation neural network to forecast the temperature at a given hour of the next day, only using the temperature at the same hour of the present day. Chaabene and Ben Ammar (2008) introduced a new methodology for dynamic forecasting of meteorological parameters. An ANFIS ( Jang, 1993; Jang and Sun, 1995; Lin and Lee, 1996 ) neuro-fuzzy system offered daily time distribution of irradiance and ambient temperature relying on the meteorological behavior during the days before. First, the obtained medium term forecasting was modeled using an ARMA model ( Makridakis et al., 1998; Pandit and Wu, 1983 ). Next, a
Kalman filter ( Kalman, 1960; Cathin, 1988; Hasltine and Rawlings, 2005 ) provided short-term forecasting. The developed models were tested and validated, leading again to very good results.
Because artificial neural networks also suffer from the difficulty of finding both an optimal topology and adequate training parameters ( Grieu et al., 2005b ), some different approaches were proposed. Allen (1957) tested a simple para-metric model for minimum temperature prediction. Scheider et al. (1985) fitted a two-harmonics Fourier model to the temperature data of the past 21 days to produce a temperature day profile. Hourly forecasts were then obtained by stretching or contracting this profile. Fan and McDonald (1994) proposed a load forecasting model consisting of time series, nonlinear load-weather functions and a residual function represented by an ARMA model ( Makridakis et al., 1998; Pandit and Wu, 1983 ).
Chow and Leung (1996) presented a novel technique for electric load forecasting based on neural weather compensation. They proposed a nonlinear generalization of the Box and Jenkins approach ( Box and Jenkins, 1970; Pankratz, 1983 ) for nonsta-tionary time series forecasting and obtained very good results. Chen and Hwang (2000) used fuzzy time series ( Song and
Chissom, 1993a,1993b,1994a,1994b ) to deal with temperature forecasting problems and overcome the drawback related to historical data represented by linguistic values. They proposed a new fuzzy time series model, called two-factor time-variant fuzzy time series model. Based on the proposed model, they developed two algorithms for temperature prediction and obtained good forecasting results. Using the same model, Lee et al. (2006) established two-factor high-order fuzzy logical relationships based on historical data. Again, the proposed method got a higher forecasting accuracy rate than the existing methods. Abdel-Aal (2004) used abductive networks to forecast hourly temperature.
Abductory inductive mechanism is a supervised inductive machine-learning tool for automatically synthesizing abductive network models from a database of inputs and outputs represent-ing a training set of solved examples. This tool can automatically synthesize adequate models that embody the inherent structure of complex and highly nonlinear systems. Abdel-Aal developed next-day and next-hour models. Performance compares favorably with neural networks models developed using the same data, and with more complex neural networks that require daily training. Performance is significantly superior to na X   X  ve forecasts based on persistence and climatology. Dong and Pedrycz (2008) introduced the concept of granular time series. It can be mainly applied to long-term forecasting and trend forecasting to overcome the curse of dimensionality (which plagues most predictors when carrying out long-term forecasts) and cope with uncertainly present in many times series. Dong and Pedrycz used a technique of fuzzy clustering to construct information granules on a basis of available numeric data present in the original time series. In the sequel, they developed a temperature forecasting model, which captures the essential relationships between such information granulates and in this manner constructs a fundamental forecast-ing mechanism. Dong and Pedrycz demonstrated that the model they proposed comes with a number of advantages that manifest when processing with a large number of data. The same year Lee et al. (2008) presented a new method for temperature prediction, based on high-order fuzzy logical relationships and genetic simulated annealing techniques ( Kirkpatrick et al., 1983; Gen and Cheng, 1997; Goldberg, 1989; Goldberg et al., 1989; Holland, 1975 ). Simulated annealing techniques were used to adjust the length of each interval in the universes of discourse with the aim of increasing the forecasting accuracy rate. The proposed method provided very good accuracy.

One can also highlight some interesting and recent works, although not directly related to temperature forecasting but having many similarities with the proposed methodology. Gonza  X  lez-Romera et al. (2007) used artificial neural networks to extract the trend component from monthly electric demand data and then performed separate predictions of both tendency and fluctuation, which were summed up to obtain the series forecasting. A mean absolute percentage error of about 2% was obtained. The same authors, Gonza  X  lez-Romera et al., proposed a novel hybrid approach to investigate the periodic behavior of the Spanish monthly electric demand series ( Gonza  X  lez-Romera et al., 2008 ): this behavior is forecasted with a Fourier series ( Mitra and Kaiser, 1993 ) while the trend is predicted using an artificial neural network. Satisfactory results were obtained. These results im-prove those reached when only neural networks or ARIMA models ( Abdel-Aal and Al-Garni, 1997; Saab et al., 2001 ) were used for the same purpose. As highlighted by the authors, a correct separation of trend and fluctuation and the optimization of the forecasting tools used to carry out the predictions are the key issues in the success of this technique. Furthermore, the combined use of a neural network and a Fourier series provides a simpler structure than that with only one neural network to carry out the whole prediction.

The proposed methodology deals with the concept of time series, even if not only past values are considered for estimating future values, and uses a wavelet-based multi-resolution analysis and multi-layer artificial neural networks. One could speak of  X  X  X RA-ANN X  X  methodology. The discrete wavelet transform allows decomposing sequences of past data in subsequences (named coefficients) according to different frequency domains ( Gao and Tsoukalas, 2001 ). From these coefficients, multi-layer Perceptrons (MLP) are used to estimate future subsequences of 4 h and 30 min. Future values of outdoor temperature and thermal power consumption are then obtained by simply summing up the estimated coefficients. Substituting the prediction task of an original time series of high variability by the prediction, using MLP neural networks (many other neural networks have been tried but no significant improvement of the accuracy was observed), of its wavelet coefficients on different levels of lower variability, is the main idea of the present work. Then, the reconstruction of future values is performed by simply summing up the estimated coefficients. In addition, the sequences of past data are completed, for each of their components, by both the minute of the day and the day of the year to place the developed model in time. First, the paper focuses (Section 2) on the considered district boiler and its working, and highlights the required upgrades. Section 3 describes both the data used to develop and validate the forecasting tool and the proposed
MRA-ANN methodology. The wavelet-based multi-resolution analysis and the multi-layer Perceptron are also described.
Sections 4 (outdoor temperature) and 5 (thermal power con-sumption of the hot water distribution network) present the forecasting results. Section 4 deals also with the impact on forecast accuracy of parameters such as both wavelet order and decomposition level, and with the topology of the ANN used. The final part of the paper concludes the present work and considers the aim of the OptiEnR project. 2. The district boiler of La Rochelle The district boiler of La Rochelle, whose synopsis is shown in
Fig. 2 , is composed of a breaking pressure bottle, a cogeneration plant and two thermal boilers. The first one, a large 4.5 MW wood boiler, uses renewable energy (it is fed with woodchip). The second one, a 7 MW gas-fuel oil boiler, uses fossil energy. Both boilers have smokes and air-to-water heat exchangers. They supply hot water to the collecting hydraulic circuit according to a temperature set-point that is defined from outdoor temperature.
During the cold season (from October to May), the wood boiler is continuously running while its heating power is adapted to the demand. The gas-fuel oil boiler functions during very cold periods only, when the wood boiler fails to respond to the demand. The primary hydraulic circuit (3000 m 3 ), or  X  X  X istribution network X  X , supplies hot water to heat residential and public buildings (for example, schools), for a total of 2700 accommodations. Domestic hot water is also produced, for a total of 3500 accommodations. The cogeneration plant, connected to the  X  X  X eturn X  X  part of the primary hydraulic circuit, produces electricity using gas and warms up the cold water before it goes back to the collecting hydraulic circuit. The breaking pressure bottle pulls apart the two hydraulic circuits, because of the difference between their respective flows.
Table 1 presents the coverage rate of each of the three heat generators. Let us remember that the main goal of the OptiEnR project deals with the minimization of the gas-fuel oil boiler coverage rate, leading to the reduction of the fossil energy consumption. As a consequence, the wood boiler coverage rate will be maximized. To reach this objective, we proposed to add a thermal storage unit to the plant ( Fig. 2 ) with the aim of stocking hot water when the demand is low and using it when the demand cannot be met by the wood boiler. This is a classical solution to optimize the functioning of boilers used to heat buildings ( Tanton et al., 1987; Stritih and Butala, 2004; Cao and Cao, 2006 ). Thus, the gas-fuel oil boiler will be only used when both the hot water demand is very high and the thermal storage unit is empty. The flow of the water passing through this unit ( Q TS ) will be adjusted, thanks to the control of the thermal storage feed pump ( Fp
With the aim of optimizing the use of this thermal storage unit and reducing the fossil boiler coverage rate, a model predictive controller will define over the next 4 h and 30 min the optimal sequence of Q TS and the wood boiler set-point temperature ( T taking into consideration some parameters measured at the district boiler as well as both the forecasted outdoor temperature ( T out ) and thermal power consumption (of the distribution network) ( W DN ). The wood boiler ( Fp WB ) and gas-fuel oil boiler ( Fp
GFB ) feed pumps are controlled using a standard on/off controller. Fp DN is the distribution network feed pump allowing controlling the water differential pressure according to outdoor temperature variations ( Fig. 2 ). 3. Materials and methods 3.1. Database: outdoor temperature and thermal power consumption
Outdoor temperature ( T out ) is measured at the district boiler and used for computing both the boilers and the primary circuit temperature set-points and the differential pressure set-point.
That is why forecasting accurately this parameter, also used by the model predictive controller (MPC) for managing the thermal storage unit, is of paramount interest. When taking a look at the data, one can highlight some interesting characteristics: first, outdoor temperature increases during the considered period (one speaks of  X  X  X ncrease trend X  X ); secondly, a 24 h pseudo-period has been detected; finally, climatic phenomena impact on the parameter variability throughout the day. Of course, we want the proposed forecast methodology, based on a multi-resolution analysis and artificial neural networks, to be able of detecting these characteristics and taking advantage of them to make accurate forecasts. Let us also note that a meteorological station located in La Rochelle measures outdoor temperature and provides to the district boiler operators forecasted temperatures for five future days (from D +3 to D +7), with a sampling time set to 3 h. With the aim of comparing these measured and forecasted temperatures with the temperatures measured at the district boiler, the considered vectors are upsampled from 3 h to 5 min.
Indeed, the sampling time of all the parameters measured and used to develop the boiler model is set to 5 min. Table 2 allows comparing both the forecasted and measured outdoor tempera-tures with the measurements done at the district boiler of La Rochelle.

Table 2 highlights first that both the outdoor temperatures measured at the meteorological station and at the district boiler are quite similar. As a result, one can suppose that the temperature forecasted at the meteorological station is usable for managing the thermal storage unit using a MPC. Indeed, both the temperatures forecasted at the station and measured at the district boiler are also quite similar ( Fig. 3 ), with a FIT and a MRE ranging between 17.1% ( T D  X  7 out ) and 43.1% ( T D  X  3 highlights that the peaks of temperature are significantly under-estimated. As a consequence, the MPC could give the order to start the gas-fuel oil boiler unnecessary. So, instead of improving the district boiler performance and decreasing the use of fossil energy, more gas and fuel will be consumed. That is why, finally, the outdoor temperature forecasted at the meteorological station will not be used by the MPC, a forecast module being developed and presented in this paper.

Fig. 4 depicts the calculated thermal power consumption of the hot water distribution network ( W exp DN ) during the first months of 2009 (from early January to late March). This parameter is not measured in situ and has an exogenous influence on the district boiler functioning because it impacts on the temperature of the water back from the distribution network. W exp DN is calculated using the difference between T E DN  X  T L BPB (the temperature of the hot water leaving the breaking pressure bottle and entering the distribution network) and T L DN (the temperature of the cold water coming back from the distribution network) as well as the flow of the water passing through this network ( Q DN ). Q DN is estimated using the opening percentage of the distribution network feed pump valve (% fp ), related to the water differential pressure ( P its set-point ( P SDN ) and outdoor temperature ( Fig. 2 ). As depicted by Fig. 4 , the thermal power consumption and the outdoor temperature are inversely proportional. 3.2. Wavelet-based multi-resolution analysis 3.2.1. Continuous wavelet transform (CWT)
The continuous wavelet transform is used with the aim of decomposing a signal into wavelets, i.e. into highly localized small oscillations ( Daubechies, 1992; Heil, 1989 ). Whereas the Fourier transform ( Cody, 1992; Perrier et al., 1995 ) decomposes a given signal into infinite length sines and cosines, losing all time-localization information; the CWT X  X  basis functions are scaled and shifted versions of the time-localized mother wavelet. So, this mother wavelet is continuous in both time and frequency function that can be considered as a source function from which scaled and translated basis functions are generated. The CWT is used to obtain a time X  X requency representation of a signal that offers very good time and frequency localization. Considering a mother wavelet C ( t ), one can obtain the following wavelet series
C a , b ( t ), where a is the scale factor and b is the translation factor according to the following expression ( Gubner and Chang, 1995 ):
C
For any function x ( t ) A L 2 ( R ), its continuous wavelet transform can be expressed as follows:
C where C * is the complex conjugate of C and C a , b is called the wavelet coefficient. The continuous wavelet coefficients can be used with the aim of reconstructing the function x ( t ) in the following way: x  X  t  X  X  1 C 3.2.2. Discrete wavelet transform (DWT) and bank of filters
Because translating the mother wavelet leads to redundant information, one can use, instead of the continuous wavelet transform, the discrete one. In this case, both scale ( a )and translation ( b ) factors are restricted to only discrete values such that a  X  a m 0 and b  X  nb 0 a m 0 ,where a 0 4 1, b 0 a 0, and m , n  X  X 
Basically, the discrete wavelet mul ti-resolution analysis, commonly based on Daubechies orthogonal wavelet basis ( Daubechies, 1992 ), allows decomposing a signal into approximations (i.e. low frequency coefficients) and details (i.e. high frequency coefficients) using a filter bank composed of both low-pass (LP) and high-pass (HP) filters ( Mallat, 1999 ). This process can be repeated n times, producing n levels of decomposition, but decomposing (downsampling) the approximations (the low frequency coefficients) only. Indeed, the high frequency coefficients are neglected. So, a signal x can be first decomposed into an approximation A 1 and a detail D 1 (that is the level 1 of the decomposition). Then A 1 can be decomposed into an approximation A 2 and a detail D 2 (that is the level 2 of the decomposition) and so on. Considering n levels of decomposition, the reconstruction process allows recovering the initial signal, summing the n details D 1 , D 2 , y , D n and the approximation A level n .Asanexample, Fig. 5 depicts the 3-level decomposition of a signal x and its reconstruction ( Mallat, 1999 ). 3.2.3. The Daubechies wavelets family
Different families of wavelets whose qualities vary according to several criteria can be used for analyzing sequences of data points ( Percival and Walden, 2000 ). The main criteria are: (1) the speed of convergence to 0 of these functions when the time t or the frequency w reaches infinity, which quantifies both time and frequency localizations, (2) the symmetry, (3) the number of vanishing moments of C and (4) the regularity, which is useful for obtaining nice features, like smoothness of the reconstructed signal. The most commonly used wavelets are the orthogonal ones (Daubechies, Symlet or Coiflet wavelets) ( Addison, 2002 ).
Because the Daubechies wavelets ( Daubechies, 1992 )( Fig. 6 ) have the highest number of vanishing moments, this family has been chosen for carrying out the wavelet-based multi-resolution analysis of the considered sequences of data points. 3.3. The multi-layer Perceptron neural network 3.3.1. Network topology
The Perceptron, the simplest neural network, is only able to classify data into two classes ( Rosenblatt, 1957 ). Basically it consists of a single neuron with a number of adjustable weights ( McCullogh and Pitts, 1943 ). It uses an adaptative learning rule. Given a problem that calls for more than two classes, several Perceptrons can be combined: the simplest form of a layered network just has an input layer of source nodes that connect to an output layer of neurons. The single-layer Perceptron can only classify linearly separable pro-blems. For non-separable problems it is necessary to use more layers. A multi-layer network has one or more hidden layers whose neurons are called hidden neurons. The network is fully connected, every node in a layer is connected to all nodes in the next layer.
According to the previous remarks, the network used for the present work is a multi-layer Perceptron. It consists of one layer of linear output neurons and one hidden layer of nonlinear neurons ( Hornik et al., 1989 )( Fig. 7 ). According to previous tests, more than one hidden layer proved to cause slower convergence during the learning phase because intermediate neurons not directly connected to output neurons learn very slowly. Based on the principle of generalization versus convergence, both number of hidden neurons and iterations completed during the training phase were optimized ( Grieu et al., 2005a ). The multi-layer Perceptron neural network learns using an algorithm called backpropagation. During this iterative process, input data a re repeatedly presented to the network. With each presentation, the network output is compared to the desired output and an error is computed. This error is then fed back to the network and used to adjust the weights such that it decreases with each iteration and the model gets closer and closer to produce the desired output ( Charalambous, 1992 ). 3.3.2. The Levenberg X  X arquardt algorithm Several training methods were used, but the Levenberg X 
Marquardt algorithm ( Hagan and Menhaj, 1994 ) proved to be the fastest and the most robust. It is particularly adapted for networks of moderate size and has memory reduction feature for use when the training set is large. Like the quasi-Newton methods, the Levenberg X  X arquardt algorithm was designed to approach second-order training speed without having to compute the Hessian matrix. When the performance function has the form of a sum of squares, the Hessian matrix can be approximated as H  X  J
The gradient can be computed as g  X  J T d e  X  5  X  where J is the Jacobian matrix that contains first derivatives of the network errors with respect to the weights and biases, and e is a vector of network errors. The Jacobian matrix can be computed through a standard backpropagation technique that is much less complex than computing the Hessian matrix. The Levenberg X  Marquardt algorithm uses this approximation to the Hessian matrix in the following Newton-like update: x k  X  1  X  x k  X  J T d J  X  m d I 1 d J T d e  X  6  X 
When the scalar m is zero, this is just Newton X  X  method, using the approximate Hessian matrix. When m is large, this becomes gradient descent with a small step size. Newton X  X  method is faster and more accurate near an error minimum, so the aim is to shift towards Newton X  X  method as quickly as possible. Thus, m is decreased after each successful step and is increased only when a tentative step would increase the performance function. In this way, the performance function will always be reduced at each iteration of the algorithm. The main drawback of the Levenberg X  Marquardt algorithm is that it requires the storage of some matrices that can be quite large for certain problems. The size of the Jacobian matrix is Q n , where Q is the number of training sets and n is the number of weights and biases in the network. It turns out that this matrix does not have to be computed and stored as a whole. For example, if we were to divide the Jacobian into two equal submatrices, we could compute the approximate Hessian matrix as follows: H  X  J T d J  X  J T 1 J T 2 Therefore, the full Jacobian does not have to exist at one time. The approximate Hessian can be computed by summing a series of subterms. Once one subterm has been computed, the corresponding submatrix of the Jacobian can be cleared. 3.4. Forecast methodology
This section deals with the description of the proposed forecast methodology, based on both the concept of time series and the use of a wavelet-based multi-resolution analysis and multi-layer artificial neural networks. One then speaks of MRA-ANN methodology. 3.4.1. Overall approach
A time series is a sequence of data points, measured typically at successive times and spaced at, often uniform, time intervals ( Brockwell and Davis, 1991 ). Time series forecasting is the use of a model to forecast future events based on known past events ( Brockwell and Davis, 1997 ). The aim of the proposed MRA-ANN methodology is forecasting, thanks to artificial neural networks, a sequence of l data points, using M  X  M Z 1  X  past sequences, also of length l . According to the respective values of M , l and of the sampling time T (set initially to 30 min; forecasted sequences are then upsampled to be in agreement with the sampling time of all the parameters measured at the district boiler), the results can be more or less accurate. Indeed, the further the forecasting horizon (set to 4 h 30 min, as requested by the boiler operators and taking into account the main objective of the OptiEnR research project, this leads to l  X  9), the more the inaccurate it can be. Inaccuracy can also be the result of a lack of usable information provided by past sequences or the result of a too low number M of considered past sequences. Past sequences are examples to be learned and used to forecast future values of the considered parameters (in this case, the outdoor temperature and the thermal power consumption of the hot water distribution network). To help the model to place itself in time, past sequences are completed, for each of their components, by the minute of the day and the day of the year. The proposed neuronal model has been developed (artificial neural networks have been trained) using half of the available sequences and validated, thanks to the remaining sequences ( Fig. 8 ). 3.4.2. Wavelet-based multi-resolution analysis contribution
Since the data usually consist of low and high frequencies components including noise, obtaining the essential frequency-domain features from these data becomes an important aspect of data analysis, visualization and forecasting. As a consequence, understanding and being able to learn from data, using artificial neural networks, is not as easy as one may think. Usually, tools such as artificial neural networks learn very well, optimizing their topologies and all the training parameters, from limited frequency domain data. When the data consist of low and high frequencies components, a useful approach can be to develop one forecast model for each of the frequency domains and then to combine the information provided by all of the models to build the desired signal. That is why one needs a tool allowing, on one hand, decomposing a signal according to different scales of frequency, while preserving its temporal characteristics (which is funda-mental when proposing a forecast methodology based on time series), and, on the other hand, rebuilding this signal. This tool is the wavelet-based multi-resolution analysis. It allows decompos-ing, according to both a decomposition level ( N ) and a wavelet order ( R ), a signal into approximations and details, using a filter bank composed of low-pass and high-pass filters. Keeping in mind the previously mentioned characteristics of both the outdoor temperature and the thermal power consumption of the hot water distribution network, one can easily understand that the wavelet-based multi-resolution analysis allows isolating the overall trend and the 24 h pseudo period characterizing the considered time series (approximation coefficients) from the variability caused by climatic phenomena (detail coefficients). As previously mentioned, the wavelet coefficients are of lower variability than the original and to-be-forecasted time series, which favors the estimation of future values. All the series of M past sequences (of length l ) considered to develop the model are so decomposed into N details and an approximation of level N . For each series, the target sequence P is also decomposed in the same way ( Fig. 9 ). 3.4.3. Multi-layer Perceptrons for forecasting details and approximations of future sequences
Substituting the prediction task of an original time series of high variability by the prediction of its detail and approximation coefficients on different levels of lower variability is the key-point of the proposed MRA-ANN methodology. In this sense, a multi-layer Perceptron neural network is needed, and it has of course to be trained using the coefficients of the considered past sequences, for estimating each of the N details and the approximation of level
N of the target sequence (the sequence to be forecasted). Let us remember that a sequence is composed of data points and temporal indicators (for each observation, the minute of the day and the day of the year). So, for a decomposition of level N , the overall forecast model is composed of N +1 multi-layer Perceptrons. As previously mentioned, summing up the estimated coefficients allows rebuilding the to-be-forecasted sequence ( Fig. 10 ). Similar approaches have already been applied to the forecasting of parameters such as electric consumption ( Tran et al., 2008a; Nengling et al., 2006 ), solar rradiation ( Grieu et al., 2009; Tran et al., 2008b ) or wind speed ( Tran et al., 2009 ). The particularities of the proposed methodology lie in the use of sequences (to train the artificial networks or the to-be-fore-casted), in the addition of temporal information allowing a better understanding of how the considered parameters ( T out and W ) evolve in time and in the use of a specific artificial neural network for estimating one of the wavelet coefficients of the future values we want to forecast. Of course, and this being the main weakness of such an approach, we have to train and find the right topology for each of the neural networks used. 3.4.4. Selection of the most accurate models
As previously mentioned, the respective values of M (the number of considered past sequences), l (the length of the sequences), T (the sampling time), N (the wavelet decomposition level) and R (the wavelet order) impact the forecast accuracy. The networks topology, basically the number of hidden neurons, as well as training parameters, such as the number of iterations, the learning rate and the error goal, also impact the networks generalization capability. That is why the present paper deals with the study and the quantification of all of these influences. It focuses also on the impact on the model performance of the random initialization of all the networks synaptic weights and on the relevance of developing  X  X  X pecialized X  X  models (each model is developed to forecast outdoor temperature during only a specific period of the year). As it will be highlighted in the results section and because the mean relative error (MRE) distribution (con-sidering the networks validation phase) can be considered as a Gaussian function, with more or less skewness, one can easily select the most accurate models. According to statistical con-siderations, a model is selected, for a given configuration related with all the above-mentioned parameters and with a random initialization of all the network synaptic weights, when the validation error is inferior to the mean error (considering all the tested configurations) minus twice the deviation standard. The N +1 selected models define the overall forecast model. 4. Outdoor temperature forecasting
This section of the paper presents experimental results in forecasting outdoor temperature. All the parameters mentioned in
Section 3.4.4 have been optimized to obtain the most accurate models. With the aim of reducing the complexity of this optimization problem, all the considered parameters have not been studied at the same time. As a consequence, the models X  configurations obtained are probably not optimal configurations but, most certainly, sub-optimal configurations. According to the common topology (the number of hidden neurons) of the multi-layer Perceptrons used, the influence of both the wavelet order and the wavelet decomposition level is, first, studied (Section 4.1).
Next, the impact of the number of both the considered past sequences and the specialized models (Section 4.2) and of the sampling time (Section 4.3), considering optimal values of R , N and F , is studied. As mentioned in Section 3.4.1, the forecasting horizon has been set to 4 h 30 min. It is a good compromise between accuracy and computation time, keeping in mind that forecasted sequences will be used by a model predictive controller to manage a thermal storage unit. 4.1. Influence of both the wavelet order and the wavelet decomposition level
As just above-mentioned, the two first parameters studied are both the wavelet order and the wavelet decomposition level. Only when the number of hidden neurons varies, the sampling time is set to 30 min while only one past sequence of 9 points is considered ( M  X  1 and l  X  9). Figs. 11 and 12 present the validation
MRE according to R , N and F (the networks X  common number of hidden neurons), ranging, respectively, between 2 and 10, between 1 and 9 and between 2 and 50. Let us note that the wavelet decomposition level has been averaged over its variation interval to obtain Fig. 10 . It is interesting to note that the best results (the most accurate forecasts) are achieved for a wavelet order equal to 4, using 5 hidden neurons ( Fig. 11 ). This can be explained by both the specific shape and frequency spectrum of a wavelet. This spectrum can be found, or not, in the analyzed signal. Considering, for example, a two-order wavelet, its shape is too different from the signal form and this leads to bad results.
Having a quick look at Fig. 12 (the wavelet order has been averaged over its variation interval), it can be noticed that when the wavelet decomposition level is high, only few hidden neurons are needed for accurately forecasting outdoor temperature. More hidden neurons are needed when the decomposition level is low.
One can consider, and this is a very interesting result, that the number of hidden neurons to be used for accurately forecasting outdoor temperature is inversely proportional to the decomposi-tion level. Indeed, for a too low decomposition level, each coefficient contains an important part of the initial information and, as a consequence, many hidden neurons are required to correctly model these coefficients using artificial neural networks.
For a high decomposition level, the detail and approximation coefficients contain a small amount of information; that is why both their modeling and forecasting are easier and require for each artificial neural network used fewer hidden neurons. On the other side, let us remember that the higher the decomposition level, the more the networks needed (one for each coefficient to be estimated). The most accurate forecasts are achieved for a 5-level decomposition and 5 hidden neurons.

Fig. 13 depicts the linked influence of the wavelet order and the wavelet decomposition level (the common number of hidden neurons for all of the neural networks used has been averaged over its variation interval). It highlights specific domains of low MRE, one can speak of  X  X  X alleys X  X , and specific domains of higher
MRE. Taking a look at Fig. 13 , it becomes apparent that, beyond an adequate wavelet decomposition level, the wavelet order is more influential than the decomposition level itself. Two valleys are clearly visible when the wavelet order is set to 4 or when it is set to 7 (the best choice seems to be 4). These valleys are very slightly dependent on the decomposition level.

As a result of this first part of the study, one can conclude that the most accurate forecasts are achieved when decomposing the considered sequences into 5 levels, using Daubechies wavelets of order 4 and multi-layer Perceptrons with respective hidden layers composed of 5 neurons. These results were taken into account to study the influence of the number of considered past sequences.
Let us note that similar results were obtained with a sampling time set to 10 min. 4.2. Influence of both the number of considered past sequences and the number of specialized models
In this section, the influence of both the number M of considered past sequences and the number S of specialized models is analyzed. Several simulations have been carried out, considering a number of past sequences, a number of specialized models and a number of hidden neurons ranging, respectively, between 1 and 8, 1 and 3, and 3 and 21. Fig. 14 depicts the influence of the number of considered past sequences, consider-ing only one overall model, according to the common topology of the neural networks used to model all of the wavelet decomposi-tion coefficients. The results highlight the necessity of considering at least 4 past sequences (the optimal number) to obtain good forecasts. This clearly reduces the validation MRE observed.
However, for a too high number of considered past sequences, the forecasts accuracy deteriorates gradually. Again, the common optimal number of hidden neurons for all of the multi-layer Perceptrons used is equal to 5. With this topology, considering 4 past sequences instead of 1 for developing and validating the overall model, the MRE is reduced of about 12.5% (from 4.8% to 4.2%).

Fig. 15 depicts the influence of the number of considered past sequences, but now considering three specialized models (each model is developed to forecast outdoor temperature during a specific period of the year only), according to the common topology of the neural networks used to model all of the wavelet decomposition coefficients. Each of the three specialized models being trained using less (three times less) information than when developing and validating an overall model, less hidden neurons are needed for accurately estimating, using multi-layer neural networks, approximation and detail coefficients. As a conse-quence, the influence of a bad networks X  sizing (too many hidden neurons favors overfitting) is more visible.

Considering the right number of hidden neurons for all of the neural networks used to model the wavelet decomposition coefficients, Fig. 16 depicts the influence of the number of considered past sequences according to the number of specialized models. Analyzing this figure, one can easily conclude that using several specialized models, instead of a unique overall model, does not improve the forecasts accuracy, but quite the opposite
Whereas the main objective of developing specialized models was to dissociate and better understand (learn) the various behaviors outdoor temperature might have; one can notice that both the too restrictive historical data considered (from mid January to early
April) and the networks training using less examples do not allow reducing the MRE. The networks generalization capability seems to be negatively impacted. So, one can suppose that considering, for example, one year or historical data, developing specialized models according to the various seasons of the year (temperature evolution is quite different from a season to another) would be very useful. 4.3. Influence of the sampling time
Finally, the influence of the sampling time on the models performance has also been studied, considering a wavelet order and a wavelet decomposition level equal to 4 and 5, respectively.
One can conclude that reducing the sampling time from 30 to 10 min does not impact significantly the forecasts X  accuracy.
Figs. 17 and 18 depict, with a sampling time set to 10 min, the influence of the number of considered past sequences according to the topology of the neural networks used and the number of specialized models (in this case, the right number of hidden neurons for all of the neural networks used to model the wavelet decomposition coefficients are used), respectively. Both figures highlight that four past sequences and a unique overall model lead to the best results (in this case, the MRE observed is about 4.3%). A reduced sampling time impacts significantly on the common topology of the neural networks used to model all of the wavelet decomposition coefficients only. The common number of hidden neurons is about 15 (it was only of 5 with a sampling time set to 30 min). One can suppose that, because a reduced sampling time increases the length of the sequences (in this case, up to 27 points), more information has to be learned and, as a conse-quence, requires more hidden neurons for all of the neural networks used. 4.4. Optimal configuration and experimental results
As a conclusion of the study about outdoor temperature forecasting, Table 3 depicts the optimal configuration to meet performance needs when applying the MRA-ANN methodology, according to the wavelet order ( R ), the wavelet decomposition level ( N ), the topology (the common number of hidden neurons F ) of the artificial neural networks used for estimating the detail and approximation coefficients, the number of considered past sequences ( M ), the number of specialized models ( S ) and, finally, the sampling time ( T ).

Fig. 19 allows comparing, from February 15, 2009 to March 02, 2009, the outdoor temperatures measured at the district boiler ( T ), forecasted at the meteorological station ( T D  X  3 out forecasted, thanks to the MRA-ANN methodology, based on a multi-resolution analysis and the use of artificial neural networks out ). Clearly, the proposed methodology provides better forecasts than the meteorological station. The temperature peaks are far better modeled (as previously mentioned, it was one of the major objectives of the work, the forecasted temperature being used by a MPC for optimizing the boiler functioning). Indeed, one can note that frequent differences of about 5 1 C between the measured and forecasted (at the meteorological station) outdoor temperatures are reduced, using the developed model, to only 1 1 C.

Table 4 specifies, for the considered period (from February 15, 2009 to March 02, 2009) and the two forecasted outdoor temperatures (by the meteorological station and using the MRA-
ANN methodology), the mean relative error (MRE), the mean absolute error (MAE) as well as the curve fitting (FIT) obtained.
First, one can remark, considering the temperature forecasted at the meteorological station as a reference, that the developed model allows reducing both the MRE and the MAE by 33% (from 6.28% to 4.14%) and 29.4% (from 1.75 to 1.15 1 C), respectively. The curve fitting is improved of about 40% (from 42.6% to 60.6%). As a conclusion, one can highlight the model performance and the validity of the proposed MRA-ANN methodology. 5. Thermal power consumption forecasting
This section of the paper presents experimental results in forecasting thermal power consumption. Two different ways of performing were considered: first, using the previously forecasted outdoor temperature (both parameters are strongly correlated) and, secondly, as we did for outdoor temperature, applying the
MRA-ANN methodology. Finally, a hybrid approach is proposed. 5.1. Use of the forecasted outdoor temperature
As previously mentioned (Section 3.1), outdoor temperature and thermal power consumption are inversely proportional. That is why the first of the two proposed approaches deals with a simple linear curve fitting, optimizing both the parameters a and b of Eq. (8), thanks to the minimization of the difference between
W DN (the calculated thermal power consumption) and W  X  W T D  X  3 out DN (the thermal power consumed by the hot water distribution network and estimated using the outdoor tempera-ture forecasted at the meteorological station ( T out  X  T W  X  W T 4 h 30 DN DN (the thermal power consumed by the hot water distribution network and estimated using the outdoor tempera-ture forecasted thanks to the MRA-ANN methodology ( T  X  T 4 h 30 out )). Let us note that a  X  381.4 and b  X  7607 if T is used while a  X  329.6 and b  X  7472.7 with T 4 h 30 out . min W 8 &lt; :  X  8  X 
Table 5 (lines 1 X 2) and Fig. 20 present the results of the thermal power consumption ( W DN ) forecasting. These results validate the first proposed approach: one can easily predict the thermal power consumption of the hot water distribution network using outdoor temperature. Whatever be the forecasted temperature used, the results are quite similar. For the previously mentioned period (from February 15, 2009 to March 02, 2009), the MRE, the MAE and the FIT are about 6.97%, 761.9 kW and 37.6%, respectively, when using T D  X  3 out and about 6.94%, 758.3 kW and 38.4%, respectively, with T 4 h 30 out . However, having a quick look at Fig. 20, one can remark, first, that the amplitude is not always well estimated and, secondly, that the curves are not always completely in phase. So, it could be interesting to use the proposed MRA-ANN methodology for directly forecasting the thermal power consumption of the hot water distribution network. 5.2. Use of the MRA-ANN methodology
The MRA-ANN methodology proposed to forecast outdoor temperature, based on a multi-resolution analysis and artificial neural networks, has also been used for directly estimating the thermal power consumption of the hot water distribution network ( W DN ). Due to the high correlation between the just-mentioned parameters, the optimal configuration found for outdoor temperature forecasting ( Table 3 ) has been re-used to develop and validate the new model. Table 5 (line 3) and Fig. 21 present the results of the thermal power consumption forecasting, using the MRA-ANN methodology. The MRE, the MAE and the FIT are about 6.07%, 663.6 kW and 44.7%, respectively. These results are better than when dealing with a linear curve fitting, using the the model allows reducing both the MRE and the MAE by 12.5%, from 6.94% to 6.07% and from 758.3 to 663.6 kW, respectively.
The curve fitting is improved by about 16.4% (from 38.4% to 44.7%). However, taking a look at Fig. 21, one can remark that the peaks of consumption are again not always well estimated. That is why a last approach is proposed, combining time series forecast-ing (the MRA-ANN methodology) and linear curve fitting. 5.3. Hybrid approach As just mentioned, a last approach is proposed, combining the
MRA-ANN methodology and linear curve fitting, for predicting the thermal power consumption of the hot water distribution network. One can highlight that time series forecasting (Section 5.2) provides better results than linear curve fitting (using the forecasted outdoor temperature, Section 5.1), but sometimes underestimates peaks of consumption. Both approaches are valid but have different characteristics. That is why it appears that they can be used in tandem to improve the forecasts accuracy. The idea is so to combine the forecasted sequences provided by each of the mean whose coefficients g and d are optimized in the following way: 8 &gt; &lt; &gt; :
The optimal values of g and d are about 0.28 and 0.71, respectively. As it was expected, the weight of the most accurate forecasted sequence, provided by the MRA-ANN methodology, is the highest weight. However, one can note that the influence (weight) of the sequence estimated by the  X  X  X inear curve fitting X  X  method, using the forecasted outdoor temperature, is regardless significant. Table 5 (line 4) and Fig. 21 present the results of the thermal power consumption forecasting, using the just-described hybrid approach. The MRE, the MAE and the FIT are about 5.85%, 639.1 kW and 46.9%, respectively. Taking as a reference the performance of the MRA-ANN methodology, one can observe that the proposed hybrid approach allows reducing both the MRE and the MAE by 3.7%, from 6.07% to 5.85% and from 663.6 to 639.1 kW, respectively. The curve fitting is also improved by about 5% (from 44.7% to 46.9%). Although these improvements do not appear to be significant, they validate the last proposed hybrid approach for estimating the thermal power consumption of the hot water distribution network ( W DN ).
 6. Conclusion and perspectives
As part of the OptiEnR project, the present paper, the first of a series of the three dealing with the modeling and the optimization of a district boiler, situated at La Rochelle (west coast of France), focuses on forecasting both the outdoor temperature and the thermal power consumption of the hot water distribution network. This district boiler is composed of a wood boiler, a gas-fuel oil boiler, a breaking pressure bottle and a cogeneration plant. With the aim of optimizing the use of a hot water thermal storage unit and reducing the gas-fuel oil boiler coverage rate (i.e. the fossil energy consumption), a model predictive controller will be developed and implemented. It requires forecasting, over the next 4 h 30 min, the two above-mentioned parameters.

The proposed MRA-ANN methodology deals with the concept of time series and uses a wavelet-based multi-resolution analysis and artificial neural networks. Multi-layer Perceptrons were trained and validated, using the wavelet decomposition coeffi-cients of past sequences, for estimating both the detail and approximation coefficients of the considered target sequences.
Substituting the prediction task of an original time series of high variability by the estimation, using artificial neural networks, of its wavelet coefficients on different levels of lower variability (according to different frequency domains), while preserving the temporal characteristics of the series, is the main idea of the present work. Then, the reconstruction of the target sequences is performed by simply summing up the estimated coefficients. One can note that the wavelet-based multi-resolution analysis allows isolating both the overall trend and the 24 h pseudo period characterizing the considered time series (approximation coeffi-cients) from the variability caused by climatic phenomena (detail coefficients). Because these coefficients are of lower variability than the original and to-be-forecasted time series, the MRA-ANN methodology makes the estimation of future values easier.
The number of considered past sequences ( M ), the sampling time ( T ), the wavelet decomposition level ( N ) and the wavelet order ( R ) impact the forecast accuracy. The networks topology, basically the number of hidden neurons ( F ), and training parameters, such as the number of iterations, the learning rate and the error goal, also impact the networks X  generalization capability. That is why the present paper deals with the study and the quantification of all of these influences. It focuses also on the impact of the random initialization of all the networks synaptic weights on the performance of the models and on the suitability of developing specialized models ( S ). As a result of the study about outdoor temperature forecasting, the optimal configuration, leading to the most accurate forecasts (the MRE, the MAE and the
FIT are about 4.2%, 1.2 1 C and 60%, respectively), is defined as follows: M  X  4, T  X  30, N  X  5, R  X  4, F  X  5 and S  X  1. These results validate the proposed MRA-ANN methodology and highlight both the impact of working with wavelet decomposed sequences and the generalization capability of multi-layer neural networks.
Next, two different ways of performing have been considered for forecasting the thermal power consumption of the hot water distribution network: first, using the previously forecasted out-door temperature (both parameters are strongly correlated) and, secondly, applying the MRA-ANN methodology used to forecast outdoor temperature again (in this case, the optimal configuration highlighted during the first part of the work has been re-used to develop and validate the new model). Finally, a hybrid approach is proposed, combining time series forecasting and linear curve fitting. Whatever be the considered approach, the results are satisfactory. The MRE, the MAE and the FIT are ranging between 5.85% and 6.97%, 639.1 and 761.9 kW, and 37.6% and 46.9%, respectively. The hybrid approach provides the best results. However, one can note that, comparing the two forecasted parameters, forecasts accuracy is not as good as that achieved when predicting outdoor temperature. It can be explained by hardly quantifiable influences leading to amplitude variations. Some peaks of consumption may be due to an unusual function-ing of big buildings X  heating systems (for example, an exhibition hall connected to the hot water distribution network). Some other peaks may be the consequence of the way the thermal power consumption is specified, using both the temperatures of the water entering the distribution network and coming back from this network. So, if the gas-fuel oil boiler starts working, the temperature of the water entering the network will increase quickly, but no significant impact will be noticed on the temperature of the cold water coming back to the plant until a few minutes. However, this leads to variations of the thermal power consumption, induced by both the functioning of the district boiler and the thermal inertia of the hot water distribution network. One can suppose that adding to the plant a hot water thermal storage unit (one of the next steps listed in the OptiEnR research project) will limit the presence of hardly predictable peaks of consumption.

As just mentioned, future work will now focus on optimizing the performance of the district boiler of La Rochelle, adding to the plant a thermal storage unit which will be managed, thanks to a model-based predictive controller. Using forecasted outdoor temperature and thermal power consumption sequences, such a controller calculates an optimal command sequence to be applied to the thermal storage feed pump. This allows adjusting the flow of the water passing through this unit. Let us remember that the main objective of the work deals with the minimization of the fossil energy consumption, stocking renewable energy during low-demand periods and using it during high-demand periods. Future papers will describe both the modeling and the way the district boiler functioning will be optimized.
 References
