 String data has recently become important because of its use in a number of applications such as computational and molecular biology, protein analysis, and market basket dat a. In many cases, these strings contain a wide variety of sub-structures which may have physical significance for that ap-plication. For example, such substructures could represen t important fragments of a DNA string or an interesting por-tion of a fraudulent transaction. In such a case, it is de-sirable to determine the identity, location, and extent of that substructure in the data. This is a much more difficult generalization of the classification problem, since the lat ter problem labels entire strings rather than deal with the more complex task of determining string fragments with a par-ticular kind of behavior. The problem becomes even more complicated when different kinds of substrings show com-plicated nesting patterns. Therefore, we define a somewhat different problem which we refer to as the generalized clas-sification problem . We propose a scalable approach based on hidden markov models for this problem. We show how to implement the generalized string classification procedu re for very large data bases and data streams. We present ex-perimental results over a number of large data sets and data streams.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms String, Classification, Hidden Markov Models Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
In recent years, a number of applications such as market basket analysis, customer tracking, DNA analysis and text use data which are in string format. In many cases, it is desirable to classify particular kinds of fragments of thes e strings. This is a generalization of the traditional classi fica-tion problem for strings in which we associate a class label with an unmarked string. This does not address the sig-nificantly more complex problem of finding particular kinds of substructures in strings. In many applications, the prob -lem of finding substructures in strings is significantly more important than that of classifying the string itself. Some examples of such domains are as follows:  X  In a genome application, we would like to find differ-ent kinds of protein sequences which are embedded in very long patterns. A given pattern may contain one or multi-ple occurrences of such a substructure. Such a problem is especially difficult because of the considerable length of th e strings in a typical biological application.  X  In a credit card application, a particular subset of trans-actions may correspond to fraudulent behavior of a set of customers. While the problem of finding fraudulent trans-actions has been well studied in prior work, our framework provides a more general technique to find particular portion s of the transactions which are fraudulent in nature.
The standard classification problem for strings was been studied in the context of a number of domains [1, 4, 5, 6, 9, 10, 12, 13, 14, 15, 17]. The nature of substructure min-ing is inherently more difficult than the standard version of the classification problem because of the added difficulty of determining the substructure extent. Therefore, standard classification methods cannot be easily extended to the gen-eralized classification problem. In many applications, an additional level of complexity arises from the need to ap-ply the procedure to very large databases and data streams. The data stream problem imposes additional one-pass con-straints on the classification process. Many classification algorithms for a variety of domains have also been extended to the data stream problem [2, 7, 11]. The generalized string classification problem is a particularly difficult one for the stream domain because of the complexity of the Hidden Markov Model construction process. Most natural training approaches for Hidden Markov Models are inherently multi-scan methods. We will develop an approach in which is I/O efficient and requires only one pass over the data stream.
In order to solve the generalized classification problem, we will utilize a stochastic modeling approach, which is refer red to as hidden markov models . We will first discuss how to find simple kinds of substructures in a string which does not have any complicated nesting patterns. Then, we will discuss the extension of this technique to substructures with different kinds of nesting patterns. It is a challenging problem to use the hidden markov model approach for classification of very large databases. This is because the parameter estima-tion procedure of a hidden markov model usually requires an iterative expectation maximization procedure which is computationally expensive and may require a large number of passes on disk resident data. In this paper, we are able to avoid this problem by designing the topology of the Markov Models effectively. Specifically, we construct the models in such a way that the entire training procedure requires only a single database scan with simple additive operations.
This paper is organized as follows. In the remainder of this section, we will define the generalized classification prob-lem. In section 2, we will discuss the basic hidden markov model (HMM) approach to the generalized string classifica-tion problem. Section 3 discusses extensions to more com-plex variants. The empirical results are discussed in secti on 4. Section 5 contains the conclusions and summary.
The generalized classification problem for strings is de-fined as follows. We given a database D containing N strings s . . . s N . We also have a set k class labels denoted by C 1 . . . C k . Each string s i may have one or more substrings which satisfy the following properties:  X  The substring has a begin marker which indicates its first position in s i .  X  The substring has an end marker which indicates its last position in s i .  X  The substring has a label drawn from C 1 . . . C k which in-dicates its class.

The database D is used for training a model to identify the substructures in the strings. We note that the database D may also take on the form of a data stream in which new records are received continuously. For a given test instanc e T , the model is used to determine the following: (1) The identity of the classes present in T as substructures. (2) The location and extent of these substructures. The clas-sification algorithm automatically inserts the appropriat ely labeled markers at various positions in the string in order t o identify the corresponding substructures to the user.
The above definition can be generalized to data streams by assuming that the data contains separate training and test streams. These separate training and test streams contain records which are differentiated only by the fact that the markers are present in the training data. In practice, the process of training and testing is a separate and continuous process by which the training model is continuously updated with additional data, whereas the testing process uses this continuously constructed training model.
In this section, we will first provide a brief description of hidden markov models, and then discuss how the concepts can be suitably leveraged in order to solve the generalized classification problem. A hidden markov model describes a series of observations by a  X  X idden X  stochastic process cal led a Markov chain. This model has a number of underlying states, which have transitions into one another with pre-defined probabilities. A probability distribution of symbo ls is associated with each state. For each transition, a symbol is generated by using the probability distribution of sym-bols at the state that the system transitioned from. While the actual sequence of states is not observable (and is only a conceptual model), the sequence of symbols generated by the hidden markov model are observed explicitly. The par-ticular topology of the model, state transition probabilit ies and symbol generation probabilities create a class of strin gs which are generated by that markov model.
 For example, consider the markov model illustrated in Figure 1. Other than the START and END states, the model contains 4 states, which we have labelled A (1), A (2+), B (1), and B (2+) respectively. This model represents sequences drawn from the alphabet { A, B } . We assume that a tran-sition from either of the states A (1) or A (2+) result in the generation of the symbol A , whereas a transition from ei-ther of the states B (1) or B (2+) result in the generation of the symbol B . The topology of the model ensures that all sequences generated from the model contain alternating sub -sequences of two or more symbols of the same type. Thus, a valid sequence generated from this model would be AABB-BAA, but not AABAA. For any given string, the maximum probability path (product of corresponding transition pro b-abilities and symbol generation probabilities) through th e model reflects the level of fit of the string to the model. It is clear that in the case illustrated in Figure 1, the proba-bility of any path generating the string AABAA is 0. By changing the symbol distribution and transition probabili -ties at the states, it is possible to change the nature of the sequences generated. We note that it is often possible to model a given class of strings with different topologies of the markov model. However, it is often a matter of skill and experience to design the topology in such a way that the parameter estimation for a particular topology reflects the class behavior well. In this paper, we will develop a topolog -ical class of models which are suited to string classificatio n both in terms of accuracy and efficiency.

In order to develop further descriptions of the algorith-mic techniques used in this paper, we will introduce some notations and definitions. We assume that the states of the markov model are denoted by S = { q i . . . q N } . The initial state probabilities are denoted by {  X  (1) . . .  X  ( N ) } . There-fore  X  ( i ) is the probability that the system is in state i at the beginning of the stochastic process. It is clear that P i =1  X  ( i ) = 1.

The set of symbols from which the strings are constructed are denoted by  X  = {  X  1 . . .  X  l } . For each state q j probability distribution characterizes the symbol being g en-erated at the corresponding transition. We shall denote thi s probability of the symbol  X  i being generated in state q j b (  X  i ). Thus, for a given state q j , We have P l i =1 b The probability of a transition from state q j to state q denoted by a jk . Since the probability of transition from any state is one unit, we have P N j =1 a ij = 1.

Each sequence of transitions in the markov model creates a sequence of symbols generated at the various states. Let the corresponding path P for the sequence of transitions be denoted by i 0 . . . i T +1 , and let the set of symbols generated at those states be denoted by SS =  X  1 . . .  X  T +1 . Then, the probability of the transitions following path P is given by the following expression: The above expression is simply the product of the probabil-ities of state transitions and corresponding symbol genera -tions. A higher value of this probability indicates a greate r fit of the particular string  X  1 . . .  X  T +1 to the class of strings generated by this model. Therefore, hidden markov mod-els provide the ability to model families of strings by using a particular layout of the model, symbol generation proba-bilities, and state transition probabilities. Conversely , for a given training set of strings, the parameters such as a ij b (  X  ) need to be estimated from the data.
In this section, we will discuss the hidden markov model for the most simple case when there is no nesting of the class labels within one another. In later sections, we will show how to generalize it to the case when the underlying structures may be complex variations on the basic model.
In order to model the simple case, we divide the string into three portions: (1) The portion before the begin marker outside the class substring. (2) The portion within the clas s substring itself. (3) The portion outside the class substri ng. As illustrated in Figure 3, there are three kinds of states in this model, corresponding to symbols encountered before, inside or after the class substring. In order to transition f rom one class of states to another, either a begin or end marker must be encountered. We denote these markers by beg str and end str respectively. This is achieved by ensuring that the probability distribution of the symbols generated by th e first and last states of the second segment of states are the deterministic distributions corresponding to the  X  X egin X  and  X  X nd X  markers. Therefore, the state marked as begin struct in Figure 3 always generates the beg str symbol, whereas the state marked end struct always generates the end str sym-bol. Thus, the symbol set for the markov model is given by  X   X  =  X   X  X  beg str, end str } . In addition, we impose two constraints on matching a string with the markov model: (1) The initial set of state probabilities  X  are be chosen so that the beginning state is the START state. This is achieved by setting  X  ( i ) = 0 for each state other than the START state. (2) After generation of all the symbols of the hidden markov model, the system finishes in the END state. Therefore, only those sequences in the Markov Model which begin at the START state and end at the END state are of interest. We note that the constraint on the model finishing in the END state can also be imposed by appending a special ter-mination marker at the end of each string which is generated only by the END state.

It is assumed that the portion inside the structure con-tains k states. The portions before and after the structure contain k  X  and k  X  X  states respectively. The last symbol of each segment contains a self-looping state which is used to model the case when the length of the string in that segment is larger than the number of available states. The values of k , k  X  and k  X  X  are estimated using the training data. We will discuss this issue in more detail in a later section.
While other Markov model designs are certainly possi-ble, we choose this particular design for several reasons: ( 1) The symbol generation and transition probabilities are sen -sitive to the distance of the state from the beginning of the first state of each segment. This models the fact that in most real data sets, the distribution of symbols and posi-tion of the class begin and end markers is sensitive to its position. (2) This class of Markov models can be easily extended to more complex structures containing nested se-quences. These methods will be introduced at a later stage in this paper. (3) This class of Markov models creates a formulation which allows an efficient method for the param-eter estimation problem for very large databases. We will discuss this issue in the next section.
The hidden markov model discussed in the previous sec-tion has a number of parameters which need to be esti-mated from the training data. The effectiveness of the model depends considerably upon how these parameters are esti-mated. Furthermore, since the method is to be used for massive databases and data streams, it is important to be able to perform these estimations efficiently.

The first parameter which needs to be estimated is the number of states for each segment of the hidden markov model. This is because these parameters are needed to ac-tually set up the physical layout of the markov model. In Figure 3, we have denoted these parameters by k , k  X  and k In order to perform this computation, we find the mean and standard deviation of the number of symbols inside, before, and after the class substring. We denote the corresponding means by  X  ,  X   X  and  X   X  X  respectively. The standard deviations are denoted by  X  ,  X   X  and  X   X  X  respectively. Then the param-eters estimations for k , k  X  and k  X  X  are  X  + r  X   X  ,  X   X  and  X   X  X  + r  X   X   X  X  respectively. Here r is a user-defined pa-use a sample of the data or the entire database to perform this computation. In the case of a data stream, this compu-tation is done only once for an initial segment of the data stream. At that point the markov model is constructed and its parameters are estimated for the training process.
Once the values of k  X  , k  X  X  and k  X  X  X  have been determined, the physical layout of the states of the markov model can be finalized. We use this physical layout to estimate the transi -tion and symbol generation probabilities. We note that this part of the parameter estimation is often a non-trivial task for a given hidden markov model. It is not practical to use an approach such as the Baum-Welch algorithm, because it requires multiple passes over the data. Such a process is inefficient for most large databases because of the consider-able disk I/O required for multiple data passes. In the case of a data stream, such a procedure is essentially infeasible .
In the family of models discussed in this paper, the pa-rameter estimation problem is simplified because of the par-ticular topology of the models we have constructed. We note that the begin struct and end struct states generate a par-ticular (marker) symbol with probability one. All markov models discussed in this paper are designed in such a way that the state to which each transition occurs belongs to one of two kinds of states: (1) The begin struct or end struct states which generate the begin markers and end markers deterministically. (2) Any of the other states which gener-ate one of the symbols from the base alphabet.

Since the training data is provided with the begin markers and end markers specified in the strings, the exact sequence of transitions for each training data point can be known exactly irrespective of the transition and symbol generati on probabilities. This is not the case for most markov models in which the paths are only known probabilistically, and the parameter estimation method tries to use iterative methods 1 For real applications, a choice of r = 3 suffices, since most of the data points are located within 3 standard deviations from the mean under the normal distribution assumption. in order to maximize the accuracy of estimation. In the family of models discussed in this paper, the parameters can be easily estimated by finding the unique path through the model for each training example. We note that such a process requires one examination of the string, after which it never needs to be used again. This is important from a computational point of view for the data stream problem. Once these paths have been determined, we compute the following set of aggregate statistics from the training dat a: A ij : Number of times that a transition from state q i to state q j occurs over all unique paths for the different training sequences.
 B (  X  k ) : Number of times that symbol  X  k is observed in state q over all unique paths for the different training sequences.
One way of estimating the transition probability a  X  ij is de-fined as a  X  ij = A ij / P N j =1 A ij . This is because the statistic in the numerator represents the number of transitions from state i to state j , whereas the statistic in the denominator represents the total number of transitions from state i . We note that while a  X  ij is the maximum likelihood value of the transition probability without any prior knowledge of the data, this may often lead to unstable estimates when the amount of training data is small. Therefore, some amount of smoothing of the estimates using the default initial prob -abilities is useful.

If no data is available for a particular state, then we as-sume by default that each possible transition out of that state has the same probability. Similarly, for a given trans i-tion, since l possible symbols can be generated, we assume that the probability of each possible symbol being gener-ated by default is given by 1 /l . When the amount of data available is small, we would like these default probabiliti es to be incorporated in the final estimation. Therefore, we make the implicit assumption that the observed transitions correspond to only a fraction  X  ( i ) of the entire set of obser-vations. The remaining fraction of 1  X   X  ( i ) observations for state i correspond to the default case in which the transition probability to every other state is equally likely. Then, th e transition probabilities for the markov model are estimate d as a ij =  X  ( i )  X  a  X  ij + (1  X   X  ( i ))  X  (1 /N ).
The value of  X  ( i ) heavily depends upon the number of data points which result in a transition from state i . When there are a larger number of transitions from state i , the available data provides an accurate estimation of the state transition probabilities, and the value of the fraction  X  ( i ) should be close to 1. Let us assume that the total number of transitions out of state i is denoted by M ( i ) = P N If p ij be the true transition probability from state i to state j , then the standard deviation of the observed value a  X  A ij /M ( i ) is approximately p p ij  X  (1  X  p ij ) / p M ( i ) by the central limit theorem. Correspondingly, we estimated the value of  X  ( i ) to 1  X  1 / p M ( i ). This ensures that the maxi-mum error contributed by the smoothing term is no larger than that of estimating the parameters using purely the training data. The symbol generation probabilities can be estimated in a similar way to the state transition probabili -ties using the following expression: b
In this case  X  ( i ) represents the fraction of the symbol generations at state i which are reflected by the available data. The value of  X  ( i ) is estimated in a similar way as  X  ( i ). Specifically,  X  ( i ) is estimated using the number of times that a transition into state i occurs over the entire training data set. Let P ( i ) = P l k =1 b i (  X  k ) be the corresponding number of transitions. As in the case of  X  ( i ), the value of  X  ( i ) was chosen 2 to be 1  X  1 / p P ( i ).

We note that the training process results in the estimation of the parameters A ij , and B i (  X  ). These parameters are con-tinuously updated over time and are stored in a dynamically updated database along with the Markov Model topology. We note that all other parameters can be directly estimated from the above parameters which need to be updated in-crementally. This database is used by the test procedure to estimate other parameters which are used for test stream classification. The overall architecture for test stream cl assi-fication is illustrated in Figure 2. It is clear that the train ing procedure dynamically updates the intermediate statistic s which is used by the test data stream. In the event that we are dealing with databases instead of data streams, then the statistics at the end of the training procedure are used for test example classification. In the next section, we will discuss how the training parameters are used for the pur-pose of classifying a test example. We also note that while the training procedure needs to sequentially update a cen-tralized parameter database, this is not the case for a test example for which the processing can be easily parallelized or distributed as long as the parameter database is availabl e.
While the testing phase is an inherently more difficult pro-cess from a computational standpoint, the advantage is that each test example can be classified separately. Therefore, the process scales up well with increasing computational speeds, and can be parallelized if desired. The method re-quires little disk I/O since the parameter set can be stored in memory during the classification process and updated only periodically. The procedure discussed in this section requires dynamic programming and is quite efficient in prac-tice because of the nature of the underlying model.
The test instances are strings in which the positions of the begin and end markers have not been specified. We have al-ready seen that there is a single non-zero probability path through the markov model, once these positions have been specified. Therefore, the classification phase of the proble m can be posed as follows: For a given test instance, where should the begin and end markers be inserted so that the probability of the unique pat h through the markov model is maximized.
 We note that a similar problem arises when trying to iden-tify a class of strings using the markov model technique [16] . However, in that case since the problem is only to identify the class of the string rather than its location and extent, the technique required is somewhat simpler. In that case, multiple paths exists through the model, and the identity of the class label is decided by the maximum probability path. Here, the problem posed is somewhat different: For an incompletely specified string, where do we insert the marker labels so as to maximize the probability of the corre-sponding path through the markov model. of transitions out of state i is equal to P N j =1 A ij . This is also equal to the number of symbols encountered at that state, and is equal to P l k =1 b i (  X  k ). This means that P ( i ) = M ( i ). Therefore, for a given state i , the values of  X  ( i ) and  X  ( i ) are the same.
 While the problem of finding the maximum probability path through the markov model can be solved using the Viterbi algorithm [16], this version of the problem is subtly differ-ent, and requires a new solution. It turns out that it is possible to use a dynamic programming algorithm which can optimize these placements. Furthermore, this dyanmic programming algorithm only needs to use the set of parame-ters estimated by the training phase of the algorithm. Since these parameters are estimated dynamically using a one-pass algorithm, the testing process can be applied at any point in the stream computation, and not just at the end of the entire phase. Therefore, the procedure is naturally amenable to an on-demand classification process [2] for the string data stream.

Let us denote the test instance by  X  1 . . .  X  T . We note that this test instance corresponds to a path through the markov model. This path will contain states which generate the actual symbols  X  1 . . .  X  T as well as the markers beg str and end str . Thus, the entire path of states through the markov model will have length greater than T . Let us denote the states which generate the symbols  X  1 . . .  X  T by q i 1 . . . q addition, we assume that the first node on the path is the START state which corresponds to q i 0 and the last node q
T +1 is the END state. The sequence of states q i 1 . . . q may not necessarily be contiguously visited in the markov model in order to generate the string  X  1 . . .  X  T . This is be-cause the markers beg str and end str are also generated by the transitions from the begin struct or end struct states. Thus, if a marker state is present between two such states q k and q i k +1 , then an edge is not present in the markov model between the two states. This is a particular charac-teristic of the topology of the markov models discussed in this paper. Therefore, the following is always true of marko v models discussed in this paper: For any pair of non-marker states q i and q j , if a marker state q b exists such that ( q i , q b ) and ( q b , q j ) are non-zero transition probability edges, then the following properti es are true: (1) No other state q  X  exists such that the edges ( q as well as ( q  X  , q j ) exist. (2) An edge does not exist between q and q j .

In order to facilitate further discussion, we will define a pseudo-transition probability between two such non-adjac ent states. Therefore, for two such states q i and q j , we define the pseudo-transition probability R ( i, j ) as the transition probability of the unique path from q i to q j .
 R ( i, j ) = a ij if an edge exists between q i and q j Thus, R ( i, j ) can be defined in a consistent way because of the afore-mentioned properties of the models in this paper. Let us consider the optimal probability path in the markov model from q i 0 to q i T +1 containing T marker states which generates the string segment G by successive transitions. Let us denote the probability of such a string generation by P ( i 0 , i T +1 , T + 1 , G ). Let us denote the last symbol of the string G by g l . Let us also denote the string G with its last symbol removed by G  X  . Then, if q i 0 , q i 1 , q i 2 , . . . q optimal path, we have:
We note that since  X  0 = 1, it can be ignored from the equa-tion. Furthermore, i 0 and i T +1 are fixed to be the START and STOP states. In order to reduce rounding errors, we define the logarithmic probability  X  ( i 0 , i T +1 , T + 1 , G ) = log ( P ( i 0 , i T +1 , T + 1 , G )). We obtain the following expres-sion for the value of  X  ( i 0 , i T +1 , T + 1 , G ):
P ( i 0 , i T +1 , T + 1 , G ) = log( R ( i 0 , i 1 )) + log( b +log( R ( i 1 , i 2 )) + log( b i 2 (  X  2 )) + . . . log( R ( i The use of logarithms changes the multiplicative expressio n into an additive one as illustrated above. The optimal se-quence of intermediate states may be obtained by using an iterative dynamic programming approach. First, we will set up an iterative condition to determine i T , so that the cor-responding path probability is maximized. The value of i T which maximizes the probability of the corresponding path is as follows: i = argmax j  X  X  {  X  ( i 0 , j, T, G  X  )+log( b j ( g l ))+log( R ( j, i This condition defines the natural iteration of a dynamic programming solution to the problem. In general, when we have to find out the optimal value of  X  for any pair of nodes p and q for a given path of length n , the following expres-sion for  X  (  X  ,  X  ,  X  ,  X  ) turns out to be helpful for the dynamic programming solution: The corresponding intermediate node j can be computed by using the value of j which maximizes the above equation. The value of  X  (  X  ,  X  ,  X  ,  X  ) for n = 1 is special and needs to be calculated separately as an initialization procedure. Spe cif-ically, we have:  X  ( p, q, 1 , {  X  } ) = R ( p, q )  X  b j (  X  )
We note that this process requires T + 1 iterations, and each iteration requires O ( |S| 2 ) computations. Therefore, the total time required by the process is O ( |S| 2  X  T ). Thus, while the testing phase is more expensive than the training phase, it is to be noted that these requirements are only in terms of the CPU requirements, cand can easily be overcome for strings of modest length. We note that all the computations in the testing phase can be accomplished with the use of in-memory parameters which are estimated during the training phase. Therefore, no disk accesses are required during the test phase, which can be performed using an on-demand approach.
The model discussed in this paper can be easily extended to a number of different problems. For example, nested structures can be modeled with this approach. In this sec-tion, we will provide a brief overview of different kinds of models. First, we will discuss the case of direct nesting of substructures. We would like to create the markov model which is able to model the basic hierarchy in the nesting. More complex nesting patterns can then be replicated us-ing this technique. Let us assume that the structure SE is embedded within the structure SS . In Figure 4, we have illustrated the markov model for the simple nesting of two structures. We note that there are four basic kinds of states in Figure 4 which we describe in order from bottom to top: (1) The lowest set of states corresponds to string positions outside either structure. (2) The set of states which are next to the lowest correspond to the string positions outsid e the outer structure SE , but within the inner structure SS and preceding SE . (3) The next set of states correspond to string positions within both SE and SS . (4) The topmost set of states correspond to string positions outside SE but succeeding SS .

In Figure 4, we have marked each set of corresponding states accordingly. While this is an example of a simple nest -ing, the method can be easily extended to recursive nesting by using similar kinds of building blocks.

Often, there may be multiple substructures which may follow one another in the string. Such substructures are re-ferred to as sequential substructures. In combination with the nesting approach discussed above, a powerful tool can be constructed for different categories of structures. In Fi g-ure 5, we have illustrated an example in which we have two sequential substructures which follow one another. There-fore, the figure shows five distinct sets of states which we have ordered from bottom to top in Figure 5: (1) The low-est sequence of states refers to the positions just before bo th structures. (2) The second sequence refers to the positions inside the first structure. (3) The third sequence refers to t he positions inside the two structures. (4) The fourth sequenc e refers to the positions inside the second structure. (5) The final sequence refers to the positions after both structures .
In many cases, different kinds of substructures may be present in the same string. In such a case, only a minor modification is needed to the results of Figure 3. In this case, for the set of states inside the structure, we may have k different possibilities corresponding to the different clas ses that the structures may be drawn from. For example, in the Figure 6, we have illustrated the case when there are two kinds of classes in the strings. These two classes are la-belled Type 1 and Type 2 respectively. When the substring belongs to the class of type 1, then this corresponds to a path through the markov model using the states between those labelled  X  X egin type 1 X  and  X  X nd type 1 X . In this case, the maximum probability path through the markov model also provides information on the identity of the class to which the corresponding substring belongs.
In order to test the effectiveness of the system, we used a number of synthetic and real data sets. We will show that the generalized classification method is not only effective, but is extremely efficient for very fast data streams.
Two classes of data sets and streams were generated by postprocessing synthetic and real data sources. These clas ses are as follows: (1) Market Basket Data Streams: The market bas-ket data streams were generated in order to simulate the presence of different kinds of transactions within the same sequence. Such a situation may arise in unsegmented se-quences from a variety of domains. These data sets were constructed by modifying a generator discussed in [3]. We will first discuss how to convert a transaction generated in [ 3] into a string format. Each transaction consisted of a string of sorted item ids from 1 to 100. Each of these item ids were randomly mapped onto a symbol from  X  = {  X  1 . . .  X  with l = 100. By replacing the item id with the corre-sponding symbol, a string was obtained using the symbols from  X . In order to generate a string with a marker delim-ited substring portion, two separate strings were generate d using different parameter values of the data generator of [3]. The second string was then embedded within the first string at a random position. The begin and end markers of the structure correspond to the beginning and ending po-sitions of the embedded string. We shall refer to the first string as the base string , and the second string as the embed-ded string . Two separate data streams were generated, each corresponding to the different parameter values of the outer and embedded string. These data streams are referred to as MStream1 and MStream2 respectively. In Table 1, we have illustrated the underlying itemset distributions which we re used to generate the base and embedded strings. The no-tations TxIyDz as indicated in Table 1 are directly adapted from [3]. As evident from the notation, 1 million (1 M) base and embedded strings were generated and were paired off to create 1M structurally embedded strings. Each such set was then divided in a ratio of 9 : 1 as the training and test data stream respectively. For the case of the market basket data streams, the training process was performed in paralle l with the classification of the test data stream in order to test effectiveness. (2) Web Access Data Sets: We wanted to test how well the system worked in finding embedded patterns of a tempo-ral nature. In order to do so, we marked all accesses which occurred within a pre-specified time interval of the data in a stream of accesses. The original traces were obtained from [18]. Two sets of 30 traces (each corresponding to one day of access) were used. From each trace, we sampled 100 user accesses repeatedly in the same temporal sequence as the original data. These were then processed in order to con-vert them into string format. The strings were composed of the symbols drawn from { .com, .edu, .gov } , depending upon which web sites were accessed. By using this procedure, a total of 30,000 strings were generated from each set of 30 Figure 7: Effectiveness of Testing Phase with Data Stream Progression traces. All web page accesses occurring between 9:00 and 9:30 AM were marked in 90% of the strings. Two data sets labelled WD1 and WD2 were generated for the two different sets of traces.
In order to test the effectiveness of the technique, we used the length of the intersection of the marked intervals between the true substructure and the substructure found by the algorithm. Specifically, if I T be the true substruc-ture, and I F be the substructure found, then the accuracy AC ( I T , I F ) is defined by AC ( I T , I F ) = | I T  X  I
One way of testing effectiveness was to use a generaliza-tion of a nearest neighbor classifier, which we will hencefor th denote as NNC. In this technique all the substructures in the training data were stored. For a given test instance, all pos -sible consecutive substructures of lengths within pre-defi ned thresholds were enumerated, and the closest substructure from the training data to any of these test substructures was used in order to define the classification markers. The measurements were computed using the edit distance. While such a system was severely limited in its scope for efficient classification of different kinds of complex structures, we w ill show that it does not perform as well as the markov model for even those cases where it is actually feasible to use it.
We have illustrated the accuracy of the two techniques in Table 2. It is clear that the hidden markov model based classifier significantly outperforms the nearest neighbor c las-sifier in each case. This is because the brute force nearest neighbor classifier was unable to exploit the distributions of the symbols within the substructures. The accuracy of the nearest neighbor classifier was particularly poor in the cas e of the web access data sets. This is because the classifi-cation behavior was decided by deeply embedded temporal sequence fragments in the case of the web access data sets. In such cases, the brute force approach is unable to isolate the relevant subsequences for the purpose of classification . In the case of the market basket data streams, the quality of classification by both methods was less disparate, though th e markov model technique did outperform the nearest neigh-bor classifier by an impressive margin in this case as well. In this case, the brute force wss partially able to isolate th e relevant fragments, but was still not completely successfu l Figure 8: Scalability of Training Phase with Data Stream Progression Figure 9: Scalability of Training Phase with Trans-action Size in the classification process. As a result, the hidden markov model technique continued to be significantly more effective .
We also tested the consistency of the classification process with progression of the data stream. In Figure 7, we have illustrated the classification accuracy with test stream pr o-gression for both the market basket data streams. In each case, it is clear that the accuracy of the classification pro-cess increased initially with data stream progression, but stabilized rapidly. The reason for this initial behavior is that the classification model takes a little while to constru ct a statistically robust model of the underlying data behav-ior. An interesting observation is that the improvement of the model to optimal accuracy requires only a few thousand records. Thus, the initialization time is relatively low. F ur-thermore, the consistency of the accuracy with stream pro-gression shows that it is possible to maintain a stable and highly accurate classification process throughout the clas si-fication process.
We also tested the system for efficiency of the classification process. We have already noted that the process of model construction requires exactly one pass over the database in Figure 10: Scalability of Testing Phase with Data Stream Progression each instance. In addition, it is useful to test the scalabil -ity of the model construction and testing phases with in-creasing string length. Specifically, we tested the followi ng: (1) Scalability of model construction with progress of data stream. (2) Scalability of model construction phase with in -creasing string length. (3) Scalability of the testing phas e with progress of data stream. (4) Scalability of the testing phase with increasing string length. The market basket data streams turned out to be quite useful for measuring the scal-ability of the technique, since it was easy to vary the averag e string length by modifying the parameters of the underlying transaction generation model [3].

In order to test the scalability of the testing phase with increasing string length, we generated two data sets which in which the parameter for transaction length was varied for the base and embedded strings. The base strings were de-noted by Tx.I5.D1M and Ty.I4.D1M respectively according to the notations of [3]. The corresponding embedded strings were half the average lengths of the base strings. Thus, thes e transaction sets were T(x/2).I2.D1M and T(y/2).I2.D1M re-spectively. We refer to the corresponding (families of) gen -erated strings as Mart1 and Mart2 respectively.

In order to test the scalability of the model construction phase with increasing training data size, we computed the rate of processing of the data points with progression of the data stream. The results are illustrated in Figure 8. On the X-axis, we have illustrated the progression of the stream in terms of the number of training data points. On the Y-axis, we have illustrated the processing rate in terms of the num-ber of data points per second. It is clear that the training phase exhibited consistent processing performance throug h-out the progress of the data stream. For static databases, this also means that the algorithm illustrates linear CPU scalabality along with one database scan. The excellent ef-ficiency of the model construction phase can be attributed to the effective design of the markov model topology for pa-rameter estimation. We note that each parameter in the Markov model is estimated using simple and linearly addi-tive operations. This makes the process easy to implement for a data stream. In each case, the algorithm was able to process thousands of data points per second.

Next, we tested the scalability of the training procedure with increasing string length. For this purpose, we utilize d Figure 11: Scalability of Testing Phase with Trans-action Size the two string database families Mart1 and Mart2 discussed earlier. The results for both families are illustrated In Fi gure 9. On the X -axis, we have illustrated the string length (in-cluding both the original string and the embedded string), whereas on the Y -axis, we have illustrated the running time for the entire training database of 900K records. While both datasets provided roughly comparable results (illus-trating that the internal characteristics of the string mat -tered less than the string lengths), the primary observatio n was that the running times scaled linearly with transaction sizes. This is because of the number of states in the result-ing markov model scaled linearly with string length. Thus, the time required for parameter estimation scaled linearly as well. The results on scalability of model construction ar e quite important, since many real domains contain very large databases with long strings. Therefore, these results vali date the applicability of the generalized classification method to such problems.

Next, we tested the effectiveness of the testing procedure with progression of the data stream. The results for the data sets MStream1 and MStream2 are illustrated in Figure 10. On the X-axis, we have illustrated the progress of the test data stream in terms of the number of data points. On the Y -axis, we have illustrated the processing rate in terms of the number of data points per second. We note that the testing procedure slightly was slower than the training procedure, though it continued to process several thousand strings per second. We also tested the efficiency of the test-ing procedure with increasing string length. In Figure 11, we have illustrated the efficiency of the testing procedure with increasing string length. On the X-axis, we have il-lustrated the transactions size, whereas on the Y-axis, we have illustrated the testing time for the entire database of 100K test records. In the results of Figure 11, we found that while the behavior of the algorithm was super-linear, it was considerably better than the worst-case complexity of a dynamic programming algorithm. This was because of the fact that a large number of transition probabilities for a given model topology were implicitly set at zero. The corresponding paths could be ignored in the dynamic pro-gramming computation process. This greatly improved the efficiency of the testing procedure.

In this paper, we proposed a generalized classification model which in being able to identify particular fragments of the strings for the classification process. Our approach can support incremental model update and works well with very fast data streams. Such a technique has applicability i n a number of practical scenarios such as text analysis, DNA mining, and web analysis. Standard classification methods cannot be applied easily to this case because of the addi-tional complexity of finding the embedded substructures. We propose an approach based on hidden markov models for this problem, and make it scalable for very large databases. The results were tested over a number of large data sets and data streams, and were found to be qualitatively effective in addition to being very efficient from a computational point of view. [1] C. C. Aggarwal. On Effective Classification of Strings [2] C. C. Aggarwal, J. Han, J. Wang, P. S. Yu. On [3] R. Agrawal, R. Srikant. Fast Algorithms for Mining [4] G. J. Barton. Protein Multiple Sequence Alignment [5] G. A. Churchill. Stochastic Models for Heterogeneous [6] M. Deshpande, G. Karypis. Evaluation of Techniques [7] P. Domingos, G. Hulten. Mining High-Speed Data [8] D. Gusfield. Algorithms on Strings, Trees and [9] D. Haussler, A. Krogh, I. S. Mian, K. Sjolander. [10] D. Haussler, A. Krogh, I. S. Mian, K. Sjolander. [11] G. Hulten, L. Spencer, P. Domingos. Mining [12] K. Karplus, C. Barrett, R. Hughey. Hidden Markov [13] A. Krogh, M. Brown, T. S. Mian, K. Sjolander, D. [14] V. Borkar, K. Deshmukh, S. Sarawagi. Automatic [15] W. R. Taylor. The Classification of Amino Acid [16] A. J. Viterbi. Error Bounds for Convolutional Codes [17] M. S. Waterman. Sequence Alignments. Mathematical [18] ftp://ircache.nlanr.net/Traces/
