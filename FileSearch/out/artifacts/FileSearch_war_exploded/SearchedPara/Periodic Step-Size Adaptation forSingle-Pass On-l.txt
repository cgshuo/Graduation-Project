 number of model corrections made by the algorithm through a s ingle pass of training examples. optimum in a single pass through the training examples. Howe ver, 2SGD requires computing the to approximate the Hessian have been made. For example, one m ay consider to modify L-BFGS [5] can run much faster compared to state-of-the-art algorithm s. Let w  X   X  d be a d -dimensional weight vector of a model. A machine learning pr oblem can be  X  :  X  d  X   X  d , until w  X  =  X  ( w  X  ) . Assume that the mapping  X  is differentiable. Then we accelerate the convergence of the mapping: where J :=  X   X  ( w  X  ) is the Jacobian of the mapping  X  at w  X  . When mapping  X  is guaranteed to converge. That is, when t  X  X  X  , w ( t )  X  w  X  . we can approximate J with the estimates of its i -th eigenvalue and extrapolate at each dimension i by: linear in terms of the dimension. be minimized, Aitken X  X  acceleration is equivalent to Newto n X  X  method, because (1) becomes usually has  X  B  X  = 1 . We consider a positive vector-valued step-size  X   X  d component-wise (Hadamard) product of two vectors. Again, b y exploiting (4), since is given by Therefore, we can update the step size component-wise by law of large numbers and aggregate consecutive SGD mappings into a new mapping sufficiently small [7].
 for each component i : b mini-batches together only have one.
 we introduce a positive constant &lt; 1 as the upper bound of  X   X  b Its components are given by Then we can update the step size every 2 b iterations based on u by: where v is a discount factor with components defined by The discount factor is derived from (5) and the fact that when u &lt; 1 , 1 numerical stability, with m and n controlling the range. Let be the maximum value and be the minimum value of v we have v that SGD can be guaranteed to converge [7].
 complexity per iteration is O ( d Algorithm 1 The PSA Algorithm 1: Given: , , &lt; 1 and b 3: repeat 6: if ( t + 1) mod 2 b = 0 then  X  Update 7: update  X  b 8: For all i , update u i  X  sgn (  X  b 9: For all i , update v i  X  m + u  X  11: else 13: end if 14: t  X  t + 1 15: until Convergence J = Q X Q  X  1 and u i be column vectors of Q and v T i be row vectors of Q  X  1 . Then we have where Now let where e of
J such that ! ij  X  = 0 . Then Therefore, we can conclude that this eigenvalue.
 Theorem 1 Let is bounded by Proof We can show that because for any 0  X  a Now, since we have Though this analysis suggests that for rapid convergence to  X  , we should assign  X  1 with a why this is the case, consider the decreasing factor v the interval ( , ) . Assume that v when ( , ) = (0 . 9999 , 0 . 99) and update. When b = 10 , PSA will update will decrease for the other two settings are 0 . 95 and 0 . 9512 , respectively, nearly identical. titions and the performance was measured by F-score. Weight for CRF reported here is Number of features provided by CRF++. Target provides the empirical optimal performance achieved because it was due to a bug that included true labels as a featu re 1 .
 5.1 Conditional Random Field CRF++ [11]. For SMD, we used the implementation available in the public domain 2 . Our SGD For PSA, we used = 0 . 9 , ( , ) = (0 . 9999 , 0 . 99) , b = 10 , and (0) ran on an Intel Q6600 Fedora 8 i686 PC with 4G RAM.
 PSA has effectively approximated Hessian in CRF training.
 trick as plain SGD [15]. 5.2 Linear SVM of the challenge, and SvmSgd, from Bottou X  X  SGD web site. The y have been shown to outperform many well-known linear SVM solvers, such as SVM-perf [17] an d Pegasos [15]. Method (pass) time F-score time F-score time F-score time F-score SGD (1) 1.15 92.42 13.04 92.26 12.23 66.37 3.18 34.33 SMD (1) 41.50 91.81 350.00 91.89 522.00 66.53 497.71 69.04 PSA (1) 16.30 93.31 160.00 93.16 206.00 69.41 191.61 80.79 L-BFGS (batch) 221.17 93.91 8694.40 93.78 20130.00 70.30 16 01.50 86.82 intact. The experiment was run on an Open-SUSE Linux machine with Intel Xeon E7320 CPU (2.13GHz) and 64GB RAM. Table 3 shows the results. Again, PSA achieves the best single-pass much less time than the other two solvers. PSA (1) is faster th an SvmSgd (1) for SVM because PSA with the sparsity trick for CRF only but not for SVM and CNN .

Table 3: Test accuracy rates and elapsed CPU time in seconds b y various linear SVM solvers. b = 1250 for FD and 500 for OCR. For FD, the worst accuracy by PSA is 94 . 66% with b between very sensitive to parameter settings. 5.3 Convolutional Neural Network plain SGD and PSA. The initial for SGD was 0.7 and decreased by 3 percent per pass. For PSA, we used = 0 . 9 , ( , ) = (0 . 99999 , 0 . 999) , b = 10 , (0) The experiments were ran on an Intel Q6600 Fedora 8 i686 PC wit h 4G RAM. performance further. We ran SGD 100 passes with randomly sel ected 10K training examples then a reference. TONGA is a 2SGD method based on natural gradient .
 [1] S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W. Schm idt, and Kevin P. Murphy. Accel-[5] Jorge Nocedal and Stephen J. Wright. Numerical Optimization . Springer, 1999. [8] Chun-Nan Hsu, Han-Shen Huang, and Bo-Hou Yang. Global an d componentwise extrapola-[9] Han-Shen Huang, Bo-Hou Yang, Yu-Ming Chang, and Chun-Na n Hsu. Global and componen-[16] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines , 2001. [20] Nicolas LeRoux, Pierre-Antoine Manzagol, and Yoshua B engio. Topmoumoute online natural [21] Chun-Nan Hsu, Yu-Ming Chang, Han-Shen Huang, and Yuh-J ye Lee. Periodic step-size adap-
