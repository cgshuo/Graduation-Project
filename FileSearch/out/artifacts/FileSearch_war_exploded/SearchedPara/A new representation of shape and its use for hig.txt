 REGULAR PAPER N. Mezghani  X  A. Mitiche  X  M. Cheriet Abstract The purpose of this study is to investigate a new representation of shape and its use in handwritten online character recognition by a Kohonen associative memory. This representation is based on the empirical distribution of features such as tangents and tangent differences at regularly spaced points along the character signal. Recognition is carried out by a Kohonen neural network trained using the representation. In addition to the Euclidean distance tradi-tionally used in the Kohonen training algorithm to measure the similarities among feature vectors, we also investigate the Kullback X  X eibler divergence and the Hellinger distance, functions that measure distance between distributions. Fur-thermore, we perform operations (pruning and filtering) on the trained memory to improve its classification potency. We report on extensive experiments using a database of online Arabic characters produced without constraints by a large number of writers. Comparative results show the pertinence of the representation and the superior performance of the scheme.
 Keywords Online recognition  X  Representation  X  Histograms  X  Arabic characters  X  Kohonen memory 1 Introduction Character recognition is a fundamental, challenging prob-lem in pattern recognition and one with numerous useful applications ranging from electronic archival of scanned text to human X  X achine interfaces. It has been the subject of a considerable number of studies the last three decades. More recently, online character recognition has come into focus due to new information and telecommunications-related applications such as handheld computers, digital notebooks, and advanced cellular phones. In such applications, tra-ditional means of human X  X achine communication, such as keyboards and pointing pegs, are rather restrictive and inconvenient to use because the applications instruments are so small as to fit in a palm. The current introduction of large, low-cost, writable screens on laptops (Tablet PCs) and the concomitant applications such as the calculator and the MathJournal of xThink ( www.xThink.com ) will provide an additional impetus to make online character recognition a wieldy means of human X  X omputer interaction.
 ter shape representation and category assignment, the latter has been the focus of most studies. The underlying hypoth-esis is that representation by curvature, tangents, moments of various kind, and others, of long-standing use in pattern classification at large, were good enough, and that classifi-cation was the crucial, complex task to focus on. However, notwithstanding the complexity of the category assignment problem, a good representation is as important as a good classifier.
 recognition methods, and there are several review papers on the subject [5 X 7,16,47]. By and large, most of these methods pertain to offline data from scanned paper documents or other such digital pictures. Significantly fewer methods deal with online data because affordable practical applications of online character recognition are much more recent. How-ever, the basic concepts of pattern representation and classi-fication apply to both input modalities, the major difference being the constraint of real-time response generally im-posed by applications of online character recognition. There have been even fewer studies on online Arabic character recognition [1 X 4,10,14,15,26,27]. However, the rapid in-ternational diffusion of information and telecommunication technologies opens up important applications opportunities of online recognition of handwritten Arabic text.
 plement algorithms of the major pattern classification paradigms such as fuzzy logic, neural networks, and hidden Markov models [8,18 X 22,24,34,43,46]. Prominent among these are neural networks methods because of their short time of development, their classification potency, and real-time response to classification requests. In previous studies, we investigated neural networks for both offline and online character recognition. A multilayer perceptron [ 22 , 40 ]and Kohonen associative memories were trained to recognize printed characters from a large number of fonts [ 41 ]and offline handwritten characters [ 31 ]. In the same vein, we investigated Kohonen memories and combinations of Koho-nen memories as online character classifiers [29 X 31,41,46]. We also experimented extensively with the nearest neighbor classifier for character recognition [ 35 , 42 ]. The nearest neighbor classifier has excellent practical performance, at least as good as that of neural networks and other classifiers [ 35 , 42 ]. Its asymptotic error rate is less than twice the optimal Bayes rate [ 13 ]. However, its asymptotic throughput is zero . Although heuristic pruning algorithms can reduce significantly the size of its reference data set [ 17 , 38 ], the nearest neighbor classifier remains substantially slower than neural networks.
 issue in most online Arabic character recognition studies, as these have concentrated more on classification algo-rithms. Decomposition of character patterns into character-istic strokes has been most often used [ 2 , 28 , 45 ]. The x  X  y coordinate string of the input signal [ 43 ], global shape descriptors such as Fourier coefficients [ 32 , 33 ], and local geometric descriptors such as tangents have all been in use [ 29 , 32 ]. All of these representations of shape describe rea-sonably well Arabic characters and have allowed focusing on the development of classification algorithms. In online recognition in general, there have also been efforts to model pen-tip movements [ 36 , 37 ] to extract features such as curvi-linear and angular velocities.
 networks as pattern classifiers. The purpose of this study is to investigate a new representation of shape and its use in a Kohonen associative memory for superior performance in online handwritten Arabic character recognition.
 bution (histograms) of tangents and tangent differences at regularly spaced points along the character signal. Empirical distributions of characteristic features, which can be seen as marginal distributions of the underlying distribution [ 50 ], provide powerful discriminative statistics. Classification is carried out by a Kohonen neural network trained using the representation. Furthermore, in addition to the Euclidean distance traditionally used in the Kohonen training algorithm to measure feature vector similarities, we also investigate the Kullback X  X eibler divergence and the Hellinger distance, functions that measure the distance between distributions. We describe operations (pruning and filtering) we perform on the trained memory to improve its classification potency. We report on extensive experiments using a large database of online Arabic characters pro-duced without constraints on writers. Comparative results show the pertinence of the representation and the superior performance of the scheme.
 Section 2 describes the representation and Sect. 3 the version of the Kohonen associative memory and its training algorithm we used. We also describe operations we perform on the trained memory, i.e., pruning and filtering. Sections 4 and 5 give a detailed account of the experiments and results. Finally, Sect. 6 contains a conclusion and an outlook on future work. 2 Feature representation Feature representation is a crucial step in pattern recogni-tion. It is important in achieving high recognition rates. The goal of a feature extractor is to characterize an object to be recognized by measurements whose values are very similar for objects in the same category and very different for ob-jects in different categories [ 13 ]. The features must also be invariant to relevant transformations such as translation, ro-tation, and scaling.
 empirical distribution (histograms) of features. The features considered are tangents and tangent differences at regularly spaced points along the character signal (Fig. 1 ). Tangents are first-order statistics because they are calculated at sin-gle points. Difference of tangents are second-order statis-tics because they are calculated at pairs of points. Therefore, these second-order statistics capture information that tan-gents do not. Difference of tangents at neighboring points is (a discrete approximation of) curvature. Tangent differences at neighboring points at a distance of  X  can be regarded as (a discrete approximation of) curvature on the recorded sig-nal subsampled by  X  . Tangent differences are expected to be decorrelated beyond the same distance  X  . Moreover, using histograms of tangents and tangent differences (their empir-ical distributions) preserves more information about shape than does reducing them to a set of characteristics. 2.1 Feature extraction Let ( s ) be the arc length parametric representation of the curve of a character signal, and consider N equidistant points on ( s ) : gent angles defined by:  X  measurements are the tangent angle differences defined by: ={  X  ( X ) , X  = 0 , 1 , 2 ,..., N  X  1 } . The tangent angle ( is invariant under translation and scaling, and the tangent angle difference is invariant under rotation as well. 2.2 Statistics of features Now, we compute statistics for each feature  X  ( X ) , namely the empirical distribution (histograms) of features. In the contin-uous case, the marginal distribution of  X  ( X ) on shape ( s defined as:
H ( X ) (, z ) =  X  z  X   X  ( X ) ( s ) ds  X  = 0 , 1 ,..., N  X  1 feature and  X  is the Dirac delta function with unit mass at zero and  X ( x ) = 0for x = 0. Such statistics are powerful in representing images, as demonstrated by the studies [ 49 , 50 ] on texture and shape synthesis.
 tures. The histograms of  X  ( X ) are expressed as: bins, as illustrated in Fig. 2 . Therefore, for each feature the histogram is an m -dimensional vector: acter shape using the histograms of all the features: honen associative memory and the training algorithm used for classification. We also describe operations we perform on the trained memory to improve its classification potency. 3 The Kohonen memory The Kohonen neural network [ 23 ] (also called Kohonen self-organizing map, or SOM) implements an algorithm of the clustering paradigm [ 25 ], similar to K-means [ 13 ]. It can also be seen as a vector quantizer, mapping data patterns onto a computed set of patterns representative of pattern cat-egories [ 9 ].
 ther a one-dimensional or two-dimensional array, as shown in Fig. 3 . The network is an associative memory that en-codes the input patterns in the form of weight vectors of the same dimension and nature as the input patterns stored at the nodes of the network. A characteristic of the Kohonen asso-ciative memory is its self-organizing ordering : neighboring nodes encode neighboring weight values, creating a  X  X opo-logical order X  among nodes. The encoding is performed in an unsupervised mode: the input patterns do not need to be labeled. However, with labeled data patterns, although the labels are not used in the process of encoding, a pattern cate-gory can be assigned to each node once the encoding (called training in pattern recognition) is complete, providing the network with a classification function.
 shown below. Its output is the set of weight vectors W j = (w weights are initialized at small random values, the process consists of finding the node, j  X  , that contains the weight vec-tor closest to the current input, X , and updating the weight vector at each node j of the memory by an amount that is a function of the grid distance to the node j  X  . Function h which defines the influence of node j  X  on node j during up-date at j , decreases with increasing grid distance between nodes j  X  and j . This depends on parameter  X  , which de-creases with the number of iterations between values  X  i and  X  . scales weight change and varies with the number of it-erations from i to f . Parameters  X  i ,  X  f , i ,and f must be chosen appropriately to obtain algorithm convergence and the network topological ordering.
 the network training algorithm, a version slightly different from the one we use [ 39 ]. In particular, it is shown that for a uniformly distributed scalar input, the algorithm yields uni-formly spaced samples of the distribution. Further results are summarized in the books [ 11 , 12 ], in particular concerning convergence and the equivalence of the updating scheme to a Markov process. The Kohonen memory training algorithm can be summarized as in Table 1 . 3.1 Measures for feature vector similarities In addition to the Euclidean distance traditionally used in the Kohonen training algorithm to measure the similarities of feature vectors, we also investigate the Kullback X  X eibler divergence and the Hellinger distance, functions that mea-sure the distance between distributions:  X  Kullback X  X eibler divergence, also known as cross en-tropy and relative entropy:  X  Hellinger distance: distance, the Kullback X  X eibler divergence, and the Hellinger distance will be given in Sect. 5 . Before reporting on ex-perimental results, we will discuss editing operations on the memory for the purpose of improving recognition. 3.2 Memory pruning In its more general meaning, pruning, also called editing or thinning, is a process by which patterns are removed from a reference set without degrading a classifier accuracy, the purpose being to reduce memory requirements and time of execution. A visual inspection of the Kohonen map after training shows that there are nodes that have never been vis-ited and, therefore, never been assigned a label. These be-come  X  X ead nodes, X  which can degrade classification. In a previous study, we verified that dead nodes cause part of the error rate (about 3%) [ 29 ] and slow the recognition process. Therefore, they are pruned out of the memory, i.e., they are given an identifying label to discard them from further con-sideration. 3.3 Memory filtering As we mentioned, Kohonen maps are sometimes called topologically ordered maps. They map points in a source space to points in a target space such that proximity in the target space is reflected by proximity of memory nodes, i.e., neighboring memory nodes store neighboring values to pre-serve topological ordering. However, we can observe  X  X ut-lier nodes, X  which are nodes that carry isolated labels. As illustrated in Fig. 4 , for instance, isolated labels  X  X  X  appear among labels  X  X  X  and  X  X  X . This is due to the fact that these nodes have been closest to only a few input samples, often a single sample, with large distortion. Outlier nodes cause part of the error rate [ 29 ]. Therefore, the memory can be filtered, using mode filtering, for instance, to remove these outlier nodes. We recall that the node filter assigns to a node the label of the most frequently occurring neighbor labels (we used the 8-neighborhood). Pruning and filtering happen only during training and, therefore, do not affect recognition time. 4 The database 4.1 Data collection Data collection was done using a digital Wacom Graphire tablet, with a resolution accuracy of 23 points/cm. It has a sampling frequency of 100 points/s.
 distinct shapes that vary according to their connection to preceding or following letters (Fig. 5 ). Using a combination of dots and symbols above and below these shapes, the full complement of 28 consonants can be constructed. Therefore, we consider classification of the 18 distinct shapes, rather than of the 28 letters. Moreover, because the letters  X  X a X  and  X  X af X  have the same shape except for their position with respect to the baseline, we will retain only 17 shapes. Dia-critical marks could possibly be added for recognition in a postprocessing step, for instance.
 written by 18 scriptors. This is by far the largest database of online Arabic characters we know of. Such a database en-ables us to draw meaningful conclusions regarding the per-formance of our recognition system.
 a wide variety of sizes and orientations. Also, the database contains both clearly written characters and roughly written ones. Figure 6 shows some samples of the letter  X  X a X  written by several writers. Note that points in this figure correspond to the abscissa and the ordinate of the pen position on the tablet, so that the dimension of their corresponding vectors is different: it depends on writer speed. In the next section we describe processing operations to smooth character signals and normalize them with respect to writer speed. 4.2 Preprocessing (smoothing and resampling) We performed a smoothing and resampling of the online character signal inputs to remove noise and normalize them with respect to writer speed. We use a simple smoothing scheme, which is sufficient for this application and which consists of averaging point coordinates over the point neigh-borhood. We used the following weighted 3-point average: where x ( i ) and y ( i ) correspond to the pen position on the tablet.
 every online handwritten recognition system. Points cap-tured during writing are, generally, equidistant in time but not in space. Hence, the number of captured points varies de-pending on the velocity of writing and will vary from char-acter to character. To normalize the number of points, the a sequence of captured points is replaced with a sequence of a fixed number of equidistant points. Normalization in terms of a fixed number of equidistant points has the desired ef-fect of removing the variability due to varying writer speed. This, therefore, eases the burden of recognition. By the same token, this indicates that the method can be used for offline recognition as well. In our experiments we used 100 equidis-tant points (Fig. 7). 5 Experimental results We implemented all aspects of preprocessing, representa-tion, and recognition described in the previous sections. The diagram of the whole system is shown in Fig. 8 .
 ing set contains 4896 samples and the testing set 2448 sam-ples (which corresponds to 288 samples of each character for training and 144 samples of each character for testing). memory training are  X  i = 1,  X  f = 0 . 02, i = 1, and 5.1 Computation of features Use of all the features  X  ( X ) ,  X   X  X  0 , 1 ,..., N  X  1 } results in a vector of significantly high dimension (e.g., for N = 100 and m = 10, the dimension is 1000). Therefore, we use a subset of these features that we have chosen experimentally as follows: we use memories of 256 nodes, which is a sufficient size for this experiment (for each of the three distance measures, see Sect. 3.1 ). Figure 9 shows the recognition rates for  X   X  X  multiples of 10. The tendency is for the rate, as a function of  X  ,togrowtoamaximum and then decrease. For small distances, differences of tangents will accumulate about the origin and histograms will cause only a weak increase in the discrimination power of the representation. For large enough distances, tangents become largely decorrelated and differences of tangents will lose their discrimination power. Only distances in some interval will be significantly informative about shape. This could explain in part the shape of the recognition curves in Fig. 9 . Following these first experiments we retained the features for  X  = 0 , 10 , 20 , 30 , 40 to compose the vector of representation, which is, therefore, of dimension 50 (10 bins for each histogram).
 5.2 Number of iterations and number of nodes In this second experiment, we optimize the number of itera-tions and the number of nodes. Figure 10 shows the variation of the recognition rate vs. the number of iterations for dif-ferent memory sizes. The tendency is for the recognition rate, as a function of the number of iterations, to grow to a maximum and then slightly decrease. This decrease corre-sponds to overtraining. This is because such long training may make the Kohonen map memorize the training patterns, including all of their peculiarities. Therefore, training must be stopped when the recognition rate is at a maximum. Ta b l e 2 summarizes the retained parameters for each distance measure. (94.48%) and the Kullback X  X eibler divergence (94.56%) are close and better than the recognition rate with the Euclidean distance (94.07%). With the Kullback X  X eibler divergence, significantly fewer training iterations (20 vs. 60 iterations with the Euclidean distance and 80 iterations with the Hellinger distance) and memory nodes (256 vs. 361 nodes with the Euclidean distance and 400 nodes with the Hellinger distance) are required. Therefore, in applications where training delay and memory size are important con-siderations, such as PDAs, the Kullback X  X eibler divergence is to be favored. Note also that the recognition rate obtained with the vector of representation using the histograms jointly is far superior than with a representation using any histogram taken individually (Fig. 9 ).
 with the Hellinger distance. The characters in this map are those of the training sample whose representation is closest to the memory content at the corresponding iteration during training. Points  X . X  correspond to  X  X ead nodes X  as explained in Sect. 3.2 . These nodes are discarded and do not interfere with recognition.
 ries for the retained parameters. This organization depends on the distance used. Table 3 summarizes the number of dead nodes and productive nodes for the memories. With the Kullback X  X eibler divergence, fewer memory nodes are required. 5.3 Comparison to the nearest neighbor classifier Let S be a set of labeled samples. The nearest neighbor rule assigns an observation X to the class whose label is that of the sample in S nearest X .The K -nearest neighbor rule as-signs X to the class whose label is that which occurs most frequently among the K samples in S nearest X . The near-est neighbor classifier ( K = 1) is often used as a bench-mark. It has excellent practical performance [ 42 ], at least as good as that of neural networks [ 35 ]. Its asymptotic error rate is less than twice the optimal Bayes rate. However, its asymptotic throughput is zero. Although heuristic pruning algorithms can reduce the size of a reference data set, the nearest neighbor classifier remains significantly slower than neural networks.
 the three measures of distances: the Euclidean distance, the Kullback X  X eibler divergence, and the Hellinger distance. We ran recognition experiments using the same database as before. The database is divided, as before, into two dis-tinct sets, the set of prototypes containing 4896 samples and the testing set of 2448 samples. The recognition rates are shown in Table 4 . The recognition rates obtained with our system for the Hellinger distance (94.07% vs. 95.09%) and the Kullback X  X eibler divergence (94.48% vs. 95.34%) are only slightly lower.
 nearest neighbor classifier and with the Kohonen memory to compare them. Our algorithm executed on a SUNW Ultra-5 with a 360-MHz CPU. Table 5 shows that the recognition time with a Kohonen map is relatively short compared to the recognition time with the nearest neighbor classifier. This is important for an online recognition system. The training time is long, but this is not a serious disadvantage, in general, because training is done only once. Considering memory re-quirements and speed of execution, these results support the conclusion that the proposed classifier shows promising per-formance.
 6 Conclusion and future work The aim of this paper was to develop a new representation of shape and to use it in handwritten online Arabic charac-ter recognition by an associative memory. We investigated statistics of features based on histograms of tangents and tangent differences at regularly spaced points along the char-acter signal. Recognition was carried out by a pruned and filtered Kohonen associative memory. Experimental results show the high performance of the proposed scheme and the pertinence of the representation. As a comparison, we im-plemented a nearest neighbor classifier, often used as bench-mark. The recognition rates obtained support the conclusion that the proposed classifier shows promising performance. The performance of the proposed system can be further im-proved by: 1. Focused classification : The error profile of the confusion 2. Combination of Kohonen maps : In a previous study [ 29 ], 3. Feature selection : To improve on the discriminative 4. Increasing the database size : The size of the training set ters, then our method could serve as a basis for recogniz-ing words. However, segmentation of words into characters is generally acknowledged as a difficult problem at best. Rather, our method can be extended to words without their prior segmentation into characters. Such an extension would consider a word or pseudoword as an undivided entity and compute on it the representation we described, to be used by a Kohonen memory trained with a large database of words and pseudowords.
 References
