 With the fast growth of the digital documents in internet, text categorization or classification (TC) becomes a key role in organizing and retrieving the massive online documents. Text categorization are applied in many application such as, information retrieval, news and e-mail categorization. During a few decades, there were many researchers attracted in this topic. Many statistical techniques were proposed to solve the TC problems, such as, expectation maximization [13], decision tree and rules [11], k-nearest ne ighbor [14], Bayesian classification and Centroid-Based method [16,13]. Among the statistical techniques, the SVM is one of the most efficient technique in solving classification problem. In SVM, the hyperplane is constructed to seperate two groups of data. Not only the accuracy of classification, the margin between cl asses is maximized also. However, SVM require the user to setup some paramet ers to the systems. Firstly, there are many kind of mapping function can be selected. Then, the value of parameters in mapping function and SVM itself need to set. The setting affects directly to the performance of SVM.

Recently, there are some studies focusi ng on parameter tuning for SVM. As an early work, Chapelle and Vapnik [4] pr esented a well-known method called the gradient descent algorithm for choosing multiple SVM parameters. To achieve invariance against linear transformations due to the problem of scaling and rota-tion in the space of SVM parameters, the covariance matrix adaptation evolution strategy (CMAES) was initiated by Friedrichs and Igel [5, ? ]. Howley and Mad-den proposed the genetic kernel for SVM, and evaluation technique of genetic kernel. Moreover, L1 and L2 SVM was proved to use Radius-Margin bound with BFGS Quasi-Newton method [6,10].

In this paper, the technique for tuning SVM parameter is described. The multiple kernel function is presented b y the tree structure, called Kernel Tree. Kernel Tree is guaranteed to be positive semi-define function. In parameter tun-ing step, the genetic programming and BFGS Quasi-Newton method are used to search the optimal in global and local space respectively. In the rest of this paper, Section 2 describes the concept of SVM, kernel mapping and Mercer X  X  theory. Section 3 presents the genetic pr ogramming and kerne l tree. In section 4, the gradient method for tuning SVM parameter is shown. Section 5 shows the idea when genetic and gradient are combined together. In section 6, experimen-tal results from 20Newsgroup and WebKB are given. Finally, the conclusion is made in section 7. 2.1 Support Vector Machines (SVM) SVM [1,2,3] is a linear binary learning system, unlikely with other linear classi-fier e.g. Linear Discriminant Analysis (LDA), SVM finds an optimal separable linear hyperplane by considering both minimum error and maximum margin y strained quadratic optimization problem: subject to and C is the tradeoff between the empirical error and the complexity of model. 2.2 Kernel Functions, Kernel Properties and Mercer X  X  Theory The linear SVM can be applied to non-linear problems easily, by using mapping technique called Kernel trick . Normally, there are some common kernel functions that frequently used such as, Linear ker nel, Polynamial kernel, RBF kernel and Sigmoid kernel. Based on the Mercer X  X  theory, the following properties [2,7] are valid.

Here, K 1 (  X  )and K 2 (  X  ) are two arbitrary kernel functions. 2. K ( x i ,x j )= K 1 ( x i ,x j ) K 2 ( x i ,x j ) 4. K ( x i ,x j )= x T i Ax j where A is n  X  n positive definite matrix.
With these four properties, a multiple kernel function can be created by ap-is Mercer X  X  kernel by cons idering the first and the second properties. Anyway, the more number of functions we combine the more complicated the combined function is and the more parameters the function has. Genetic programming (GP) is one of the most well-known techniques in the field of genetic and evolutionary computation. The population are generate with some randomly citerias. In each generation, there are only good samples are survived and chosen to generate offsprin g (new generation). Unlike other evolu-tional method, GP algorithm works with tree structure data. In this paper, the tree structure is designed for representing a kernel function. 3.1 A Kernel Tree As mentioned above, basically the kernel function satisfies the properties de-scribed in Mercer X  X  theory. The tree stru cture can be designed to fulfill these kernel properties. Therefore, two kinds of nodes can be used to represent a ker-nel function. 1. Operation Node: A node in a tree that presents an operation with the Mercer  X  X  kernel properties. T wo possible nodes are as follow. -Additional Node: it represents the fol-lowing additional properties K ( x i ,x j )=  X  1 K 1 ( x i ,x j )+  X  2 K 2 ( x i ,x j ) -Power/Multiply Node: it represents the following properties where  X  1 and  X  2 are positive integer. This equation can be derived from second prop-erty. 2. Basis Kernel Node: A node in a tree that are con structed for represent-ing common kernel functions. These nod es represent leaf nodes. Some common functions are linear, polynomial, and RBF kernels. -Linear Kernel Node: K ( x i ,x j )= ( x i  X  x j ) -Polynomial Kernel Node: K ( x i ,x j )=(( x i  X  x j )+  X  ) d -RBF Kernel Node:
An operation node and a basis kernel no de naturally satisf y Mercer X  X  proved nodes are guaranteed to be a Mercer kern el. Figure 1 shows an example of ker- X  sen to be a basis function because the sigmoid function may not be a PSD function for all cases of parameters. 3.2 Genetic Programming Algorithm In an initial setup, GP generates the tree population randomly. For each iter-ation, only good trees are survived and used to generate new offspring (new generation). The steps of GP algorithm are: 1. Initial the first generation. In the first generation, p number of trees are 2. Evaluate of each individual. Each individual population is evaluated the 3. Select the best q and generate offspring. The best q individuals are chosen 4. Repeat Step 2 and 3 until converged. There are two conditions for stop, the 5. Build SVM model using the n best tree. Finally, the best s trees can be The gradient method is one of the most effective tools for finding the local optimal solution. The advantages of the gradient method is the convergent speed, comparing to the evolutional strategy. However, this method can find only the optimal point that is close to the current position.

In applying gradient to SVM feature selection, the empirical risk is estimated from radius-magin bound [6,10], or Leave-One-Out (LOO) bound. The gradient of R 2 w 2 can be calculated by f ollowing equations: The partial differtiation are shown here: where  X  i is the parameter of the kernel function.

In the task of minimize the objective function, there are several gradient descent algorithms such as BFGS Quasi-Newton X  X  method. The nature of genetic approach and gradient approach are not the same. The genetic approach is based on evolutional process that work well on finding the optimal in a search space. But the convergence speed of the approach is slow due to high computational. Normally, the improvement of GP becomes slow down when the iteration increases. On the other hand, the gradient approach finds an optimal solution near the initial within short period. But it cannot guarantee that the solution is global optimal. In combining both advantages of these two approach, the genetic programming is used in the first step to search in wide area. As shown in Figure 2, each tree is a sample when varying parameters in a space. The trees that have the same structure are sampled from the same space. The system selects only the points that locally give a good evaluation. The gradient method is used to find the optimal besides each selected point. 6.1 Datasets The experimental was performed on two benchmark datasets that are 20News-Group and Webkb. 20Newsgroups consists of 20 groups of news from Usenet article,which contains around 1,000 documents per group from overall 19,997 documents. WebKB consists of WebKB1 and WebKB2 that contain 7 and 5 classes respectively. The summarized in formation of these datasets are shown in Table 1 also.

In the experiment, we compares two m ethods. The first method is Gradient descent (BFGS Quisi-Newton) tuned by adjusting the parameter of single RBF kernel. The initial values are log ( C )=1and log (  X  ) = 1. The second method is GP+Gradient method. On the part of GP, the maximum depth is 5. The number of populations for each generation is 30,and selects 10 parents. The GP run 5 iterations to get the best 10 trees. The n, the parameters in each selected tree will be finely tuned with BFGS Quisi-Newton. Moreover, 5-fold cross-validation is applied in all experiments.

From the result, the use of GP + Gr method over Tree kernel gets a better performance for all datasets. The main advantage of the proposed method over the gradient method is tree kernel able to represent more the complex function (multiple kernel) while the gradient method needs user to set the function. Also when many different shape tree are generated, it means that we can search more in parameter space. While simple gradient method cannot search out of the func-tion space. However, tree kernel with GP + Gr method need mo re computational than simple gradient method. So, we need to set the number of GP iteration to limit the computational cost. In the paper, the kernel tree is suggested to solve the problem of SVM parameter tuning. The tree kernel can represent multiple kernel and it still holds properties according to Mercer X  X  theo ry. The GP the optimization method that used to find the solution of the problem. In GP, the cross-validate is the fitness function. To limit the problem of slow convergence, BFGS Quasi-Newton method is used to do a fine tuning for all selected tree.
 This work has been supported by Thailand Research Fund (TRF).

