 We are concerned with the effect of using a surrogate as-sessor to train a passive (i.e., batch) supervised-learning method to rank documents for subsequent review, where the effectiveness of the ranking will be evaluated using a dif-ferent assessor deemed to be authoritative. Previous stud-ies suggest that surrogate assessments may be a reasonable proxy for authoritative assessments for this task. Nonethe-less, concern persists in some application domains X  X uch as electronic discovery X  X hat errors in surrogate training as-sessments will be amplified by the learning method, ma-terially degrading performance. We demonstrate, through a re-analysis of data used in previous studies, that, with passive supervised-learning methods, using surrogate assess-ments for training can substantially impair classifier per-formance, relative to using the same deemed-authoritative assessor for both training and assessment. In particular, us-ing a single surrogate to replace the authoritative assessor for training often yields a ranking that must be traversed much lower to achieve the same level of recall as the rank-ing that would have resulted had the authoritative assessor been used for training. We also show that steps can be taken to mitigate, and sometimes overcome, the impact of surro-gate assessments for training: relevance assessments may be diversified through the use of multiple surrogates; and, a more liberal view of relevance can be adopted by having the surrogate label borderline documents as relevant. By taking these steps, rankings derived from surrogate assessments can match, and sometimes exceed, the performance of the rank-ing that would have been achieved, had the authority been used for training. Finally, we show that our results still hold when the role of surrogate and authority are interchanged, indicating that the results may simply reflect differing con-ceptions of relevance between surrogate and authority, as opposed to the authority having special skill or knowledge lacked by the surrogate.  X 
The views expressed herein are solely those of the author and should not be attributed to her firm or its clients. Categories and Subject Descriptors: H.3.4 Systems and Software Performance evaluation (efficiency and effec-tiveness).
 Keywords: recall; assessor error; evaluation; eDiscovery; electronic discovery; relevance ranking; supervised learning.
In high-recall information retrieval tasks X  X uch as elec-tronic discovery ( X  X Discovery X ) in civil litigation [11], sys-tematic review in evidence-based medicine [24], and prepa-ration of test collections for information retrieval research [14] X  X upervised learning is often used to separate relevant from non-relevant documents [37]. In supervised learning, each of a pre-selected set of documents (the  X  X raining set X ) is labeled as relevant or not by a human assessor, and used to train a machine-learning algorithm, which then classifies or ranks the documents in a corpus (the  X  X valuation set X ) according to their likelihood of relevance. We focus here on a type of supervised learning, which, in eDiscovery is re-ferred to as  X  X imple passive learning X  [12], to distinguish it from active learning, where the training set is selected in-crementally, using feedback from the learning algorithm [38, 11].

To measure the effectiveness of a high-recall retrieval ef-fort, it is necessary to estimate the number r of relevant documents from the evaluation set that are retrieved, as well as the number m that are missed. If r and m were known with certainty, it would be a simple matter to com-pute an effectiveness measure: For example, recall = r r + m However, the very notion of relevance is subjective [31, 32, 33], and necessarily relies on imperfect human judgement to determine m and r , and hence recall.

For many high-recall tasks, the opinion of a single subject-matter expert ( X  X he authority X ) provides the ultimate deter-mination of relevance. In the eDiscovery domain, the au-thority might be a senior lawyer representing the respond-ing party; in the intellectual property domain, the authority might be a patent examiner; in the medical domain, the au-thority might be a senior researcher. In all cases, obtaining authoritative opinions for even a small set of documents may be impossible, or may incur unacceptable costs and delays.
Previous studies have investigated the impact of replacing the opinion of the authority with that of a surrogate asses-sor [41, 46, 28]. The results of those studies suggest that, at least under certain experimental conditions, a surrogate can reasonably replace the authority, thereby increasing the allure of using cheaper, more readily available surrogates as proxies for authorities. On the other hand, some practition-ers claim that the use of authoritative assessors is of criti-cal importance in training the machine-learning algorithms used for eDiscovery tasks. One commentator [19], writing in a trade magazine, expressed the concern that errors in relevance assessments will be amplified by a chosen learning method to an irrecoverable extent. Others, including one court [2], have referred to this training issue more generally as  X  X arbage in, garbage out. X 
After a review of related work in Section 2, and a discus-sion of our general experimental methodology in Section 3, in Section 4, we describe our studies using the TREC-4 test collection and supplementary assessments described by Voorhees [41]. Using the official and supplementary sets of assessments, we trained a support vector machine ( X  X VM X ) on each set and ranked the entire corpus through the use of ten-fold cross validation. From these rankings, we deter-mined the depth in the ranking required (i.e., the number of documents reviewed) to achieve a particular level of re-call, when each of the three assessors was deemed to be the authority, while the others were treated as surrogates.
Extending these experiments, we explored whether the in-clusion of training assessments that reflected greater diver-sity in the interpretation of relevance would improve ranking performance. We implemented this diversification strategy in two different ways. First, we created three new surrogate-assessment sets, each corresponding to a pair of the three original assessors, which we imagined as working together to judge the training set. We generated the merged set for a surrogate pair by randomly dividing the documents 50/50 between the two surrogates, with each determining relevance for their half. An SVM was then trained on each of these merged surrogates, and the results evaluated us-ing cross-validation, treating the third assessor X  X  opinion as authoritative.

As an alternate means of diversification, we took the union of each surrogate pair, such that a document was deemed relevant if either constituent had designated it as relevant. Again, an SVM was trained on each of these union surro-gates and evaluated using cross-validation, treating the third assessor X  X  opinion as authoritative.

In Section 5, we directly explore the impact of a more lib-eral interpretation of relevance on passive supervised learn-ing. The University of Waterloo, in the course of participat-ing in the TREC-6 adhoc task, created an independent set of relevance assessments that included a third relevance cate-gory  X  X  X ffy X  X  X enoting documents which they believed to be of borderline relevance. The availability of three relevance categories (i.e., relevant, non-relevant, and iffy) allowed us to take two views of relevance: a  X  X onservative X  view, which considered only those documents actually labeled as  X  X ele-vant X  to be relevant; and a  X  X iberal X  view, which considered documents labeled as  X  X ffy X  to be relevant, in addition to documents actually labeled as  X  X elevant. X 
We continue in Section 6 with an experiment investigat-ing the applicability of liberal and diverse interpretations of relevance in the legal domain. The TREC 2009 Legal interactive task used initial assessments generated by volun-teer law students and contract attorneys, that were subse-quently adjudicated by senior lawyers ( X  X opic authorities X ). In addition, the University of Waterloo generated sets of assessments using a high-recall retrieval system. An SVM was trained using each of: the assessments generated by Waterloo; the initial TREC assessments; a combination of Waterloo and the initial TREC assessments; and the final as-sessments. All classifiers were evaluated with respect to the final assessments. After this, we conducted a brief follow-up experiment to test the hypothesis that adding judgments from a third assessor for documents not originally included in the training set could reduce recall depth.

In Section 7, we discuss our findings and the limitations of our results, and in Section 8, we offer our conclusions.
Voorhees observed that the retired professional analysts who assessed the TREC-4 adhoc task agreed on relevance, as measured by the Jaccard index, less than 50% of the time [41]. As a consequence, when one assessor X  X  judgements were assumed to be correct and used to evaluate the other X  X , the other X  X  judgements were found to have both recall and precision on the order of 65%, leading Voorhees to opine,  X  X  practical upper bound on retrieval system performance is 65% precision at 65% recall since that is the level at which humans agree with one another. X  While Voorhees X  primary measure, mean average precision ( X  X AP X ), is a reasonable choice for ad hoc retrieval, it provides little insight into the performance of high-recall retrieval due to the focus on early precision present in the measure. We know of no study that has investigated the applicability of Voorhees X  results to a measure suitable for evaluating high-recall retrieval.
Voorhees is not alone in noting that relevance judgements differ for different assessors, and for the same assessor at different times [34, 30, 16, 45, 20, 21], or that differences in assessors, while resulting in different estimates of effective-ness measures, have little impact on determining the relative effectiveness of retrieval methods [3, 26, 7, 9, 40].
Webber and Pickens [46], using the same relevance assess-ments as Voorhees, deemed Voorhees X   X  X rimary X  assessor for each topic to be  X  X uthoritative. X  Webber and Pickens re-ported that, on average, using a non-authoritative assessor for training resulted in a 14% decrease in F 1 and a 24% in-crease in the number of top-ranked documents that must be retrieved to achieve a recall of 75%.

Cheng et al. [8] and Scholtes et al. [35] have similarly observed that non-authoritative training assessments have a significant but moderate negative impact on high-recall ef-fectiveness. In contrast, Pickens [28] has suggested that non-authoritative training assessments may improve high-recall effectiveness when active (instead of passive) supervised-learning methods are used.

The issue of mitigating training and evaluation error is one of general interest in machine learning [47]. None of these studies specifically considered the interaction between assessor judgements for training and evaluation. A differ-ent source of interaction X  X nclusion bias introduced by the pooling method X  X s also a subject of current interest [6].
The TREC routing task [29, 44] bears substantial resem-blance to high-recall retrieval, but differs by focusing on disjoint training and test collections, and evaluation using precision, rather than recall. Voorhees has suggested that examining the effect of differing relevance assessments on routing-like tasks is an important area of study [41]. To the best of our knowledge, this has yet to be investigated.
The effect of label noise (i.e., incorrect relevance assess-ments) is an ongoing topic of investigation within the IR Table 1: Summary statistics of all three corpora. The official NIST assessor was deemed as the gold standard for these statistics. community [25, 13, 36], as well as in other communities [5, 17]. A comprehensive study by Fr  X enay and Verleysen [17] outlined research on various facets of label noise (e.g., sources of label noise, its effects on classification, etc). Brod-ley and Friedl found that removing labels identified as incor-rect, primarily by majority or consensus voting filters, im-proved overall classifier performance [5]. The e-mail spam filtering community has also noticed that label noise can drastically affect spam-filter performance [25, 36, 13].
Our experiments follow the Cranfield paradigm [42], us-ing test collections consisting of documents, topics, and rele-vance assessments from several TREC tracks. Table 1 offers summary statistics, including the number of documents and the average prevalence of topics, for each of the test collec-tions used in this work; further details on each collection are provided in subsequent sections. In this section, we provide an overview of the general experimental methodology used in all of our experiments.

In these experiments, we used assessments generated by several independent assessors to train our classifiers and evaluate their performance. For the experiments described in Sections 4 and 5, we used assessments only for documents that were assessed by all of the assessors for the particular collection; any document for which there was not a complete set of assessments was treated as non-relevant. This choice was made to control for the fact that some assessors ren-dered (many) more assessments than others, and using the extra assessments would confound comparison. In addition, using the additional assessments would change the number of training examples and would result in testing a different hypothesis than the one in which we were interested (i.e., the effect of quantity versus quality of assessments).
In contrast, for the experiments reported in Section 6, we used assessments for all documents in the TREC judging pool, while maintaining the roles (initial assessor, topic au-thority, and independent assessor) established in the original experiment. In cases where there was no independent assess-ment for a document in the pool, we evaluated two different methods: (i) deeming the assessment to be  X  X ot relevant, X  and (ii) deeming the assessment to be the same as the initial assessment.

The TREC judging pools used for training form conve-nience samples of the full collections since they are com-posed of the top-ranked documents from participant submis-sions. To mitigate any effects from training on a narrowly selected set of documents (i.e., those that appeared relevant to some participant system), we augmented each training set with 1,000 randomly selected documents that were treated as non-relevant. This step broadened the representativeness of documents in each training set, to ensure that the re-sultant classifier was not focused exclusively on fine-grained distinctions between relevant and non-relevant documents in the judging pool, to the exclusion of non-relevant documents outside the pool.

To rank the documents, we used SVM light [18], with de-fault parameters. The features supplied were tf-idf term scores for all alphabetic words. Scores were generated after the Porter stemmer and case folding were applied.

Because our training and evaluation sets were not disjoint, we used ten-fold cross validation to approximate the effect of an independent evaluation set. Documents appearing in both sets were evenly distributed among 10 splits, as were documents appearing only in the evaluation set. The docu-ments in each split were scored by a classifier whose training set was the union of the other nine splits, and a ranking was formed by sorting documents in the evaluation set according to score. We first tested our experimental methodology by replicating the Webber and Pickens study [46], successfully reproducing their results.

Our primary evaluation measure was Recall Depth , which is the size of the shortest prefix of the ranking that achieved a particular level of recall, expressed as percentage of the size of the corpus.

Our graphical results show, for each method, the average over all topics of (log transformed) recall depth, as a function of recall. In addition, for direct comparison, we show the average of (log transformed) relative recall depth X  X he ratio of recall depths between pairs of interest. Our tabular re-sults show the same recall depth for 75% recall X  X  previously reported recall target [12]. We computed the significance of the surrogate-trained classifiers relative to the authority-trained classifier, applying a t-test to the log-transformed difference. In our tables,  X  denotes p &lt; 0.05;  X  denotes p &lt; 0.0001.
In this section, we describe our experiments using docu-ments, topics, and relevance assessments from the TREC-4 adhoc task [22]. For this test collection, the official relevance assessments were augmented by two independent sets of rel-evance assessments rendered by different assessors within the course of Voorhees X  experiments [41]. We labeled these assessment sets as J1, J2, and J3. While the assessments in J1 were (a subset of) those used for the official TREC evaluation, our experiments treated J1, J2, and J3 equally, treating each in turn as the  X  X uthority, X  and the others as surrogates. We restricted our experiments to topics where all three assessors found at least eight relevant documents, with the intent of reducing variance created by very low prevalence topics, consistent with previous work [46].
J1, J2, and J3 each reflect a single interpretation of rele-vance. To explore our hypothesis that a more diverse inter-pretation of relevance derived from several assessors would result in better training, we took each pair of surrogates and merged their assessments by randomly splitting the train-ing set in half and assigning each half to one of the surro-gates. The resulting merged-surrogate sets, which we de-note J1 | J2, J1 | J3, and J2 | J3, might then be viewed as the result of the two surrogate assessors working together to assess a single set of documents. Classifiers trained using each of the merged-surrogate sets were evaluated using J3, J2, and J1, respectively, as the authoritative assessor. Each merged-surrogate set contains the same documents as the single-surrogate set, reflecting the same amount of training effort.
To explore our hypothesis that a more liberal interpreta-tion of relevance would result in better training, we evalu-ated training using the union of each pair of surrogates, de-noted J1+J2, J1+J3, and J2+J3, in which a document was considered relevant if either of two surrogates considered it relevant. The classifiers constructed using these union sur-rogates were evaluated using J3, J2, and J1, respectively, as the authoritative assessor. Each of the union-surrogate sets contained the same documents as the single-surrogate and merged-surrogate sets, but reflected twice as much assess-ment effort. We do not believe that such a practice would necessarily be cost prohibitive, given the assumption that surrogate assessments are substantially less expensive than authoritative assessments.
Figure 1 shows that the single-surrogate-trained classifiers are generally inferior to the corresponding authority-trained classifiers, requiring greater recall depth to achieve any par-ticular level of recall. This result is reiterated in the relative recall depth plots in Figure 2, and the 75% recall depth val-ues presented in Table 2. The differences among surrogates are most apparent at high levels of recall; as Figure 2 illus-trates, some individual surrogates are substantially better than others.

Table 2 shows that with J1 and J3 as the authority, the authority-trained classifiers significantly outperform classi-fiers trained by individual surrogates. However, with J2 as the authority, the difference is not significant, particularly with respect to the case in which J1 is used as the surro-gate assessor. While this reduced difference may be due to chance, it may also be an artifact of the assessment pro-cess. J1 corresponds to the official NIST assessments, for which the assessor reviewed the entire TREC-4 pool. This pool was much larger and had a lower prevalence of relevant documents than the pool reviewed by J2. An inverse re-lationship between prevalence and recall [39] might account for J1 X  X  assessments being more liberal than they would have been had J1 assessed only the documents that were assessed by J2.

Figures 1 and 2, as well as Table 2, show that the merged surrogates achieve effectiveness close to the better of the individual surrogates, occasionally exceeding both.
The union surrogates trained substantially and signifi-cantly (p &lt; 0.01) superior classifiers compared to the indi-vidual surrogates, as is evident in Figures 1 and 2, as well as Table 2.
 Table 2: 75% recall depth values for the TREC-4 exper-iments, with 95% confidence intervals. Significance is de-termined by comparing surrogate-trained classifiers to the authority-trained classifier. (  X  denotes p &lt; 0.05;  X  denotes p &lt; 0.0001.)
In this section, we describe our experiments using docu-ments, topics, and relevance assessments from the TREC-6 adhoc task [43], augmented by assessments rendered inde-pendently by the University of Waterloo in the course of their participation in TREC-6, using a process of interactive search and judging [10]. While TREC-6 used binary assess-ments, Waterloo used three categories of relevance: relevant, not relevant, and  X  X ffy. X  This  X  X ffy X  label was used to identify documents for which the Waterloo assessors were unsure of the true relevance (i.e., they were of borderline relevance).
In Voorhees X  study, these  X  X ffy X  assessments were treated as non-relevant. One of our hypotheses was that a more lib-eral interpretation of relevance would result in better classi-fier performance with respect to an independent third party (i.e., in this case, NIST). To this end, we compared the two classifiers trained by treating these  X  X ffy X  documents, alternatively, as non-relevant and as relevant. These differ-ent sets of assessments are labeled WaterlooRel and Water-looRel+Iffy, respectively. In this experiment, WaterlooRel+Iffy represents a  X  X iberal X  assessor, while Wa-terlooRel represents a  X  X onservative X  assessor.

We used the NIST assessments to evaluate classifiers trained using each of: the NIST assessments, the Water-looRel assessments, and the WaterlooRel+Iffy assessments. While our primary interest was in the relative effectiveness of using the liberal versus conservative Waterloo assessments for training, we also reversed the roles of surrogate and au-thority, as for our previous experiment. We did not inves-tigate the use of the conservative Waterloo assessments as the surrogate and the liberal Waterloo assessments as the authority, or vice versa, as these sets of assessments were not independent.
Our hypothesis X  X hat surrogate assessors taking a more liberal view of relevance would produce better classifiers X  is supported by the results presented in Figures 3a and 4a, where training using the liberal assessor is seen to achieve significantly better recall depth than both the conservative assessor and the NIST assessor. Table 3 shows the difference at 75% recall depth, with 95% confidence intervals. Across all recall levels, the liberally-trained classifier generally per-forms as well as, or better than, the authority, while the conservatively-trained classifier performs significantly worse.
Table 3, as well as panels (b) and (c) of Figures 3 and 4, show that, consistent with our previous results, classifiers trained using the NIST assessments fall short when evalu-ated using either the liberal or conservative Waterloo assess-ments as the authority. It is no surprise that the shortfall is greater with respect to the liberal assessments. Table 3: 75% recall depth values for the TREC-6 experi-ments for Waterloo and NIST-trained classifiers, evaluated using NIST assessments, with 95% confidence intervals. Sig-nificance is shown relative to the NIST-trained classifier. (  X  denotes p &lt; 0.05;  X  denotes p &lt; 0.0001.)
Figure 2: Relative recall depth plots for the TREC-4 experiments, using (a) J1, (b) J2, and (c) J3, as the authority.
Figure 4: Relative recall depth plots for TREC-6 exper-iments, using classifiers trained by each surrogate, and evaluated by each authority.
T opic I nitial W aterloo W aterloo w/ Initial F inal Table 4: 75% recall depth values for the TREC 2009 Legal experiments, using classifiers trained by Waterloo and initial assessments, and evaluated using final assessments. Table 5: Recall and Precision of initial assessments in the TREC 2009 Legal Track judging pool versus the full corpus.
The TREC 2009 Legal interactive task [23], simulated a high-recall eDiscovery task. For each topic in this task, the judging pool was a stratified sample of the document collec-tion. An initial assessment of the judging pool was rendered using volunteer law students or contract attorneys. The initial assessments were provided to participating teams, who were invited to appeal assessments with which they disagreed. The appealed assessments were adjudicated by a TREC-designated Topic Authority , a senior lawyer who rendered the final, authoritative relevance assessments that were used to evaluate submissions. During the course of their participation in TREC 2009, the University of Water-loo developed an independent set of assessments using their own interactive, high-recall retrieval system for four of the task topics (Topics 201, 202, 203, and 207) [15].

Because the TREC judging pool included a large random sample of the document population, only a relatively small fraction (17.7%) of the documents in the pool were included in the Waterloo assessments; the rest were excluded by Wa-terloo X  X  search method, as unlikely to be relevant. We inves-tigated two approaches to determine the relevance of these documents for training purposes. The  X  X aterloo X  surrogate assessments deemed the excluded documents to be  X  X ot rel-evant X  for the purpose of training, and used the final TREC assessments as the sole authority for evaluation. The  X  X a-terloo w/Initial X  surrogate assessments used the initial NIST assessment for each excluded document. Thus, the  X  X ater-loo X  assessments were fully independent of the initial assess-ments, whereas the  X  X aterloo w/Initial X  assessments, while not fully independent, might better model the situation in which the excluded documents had been manually assessed. We evaluated the effect of training using the two sets of Waterloo surrogate assessments, as well as the initial and final assessments, using the final assessments as authorita-tive.
Figure 5 shows relative recall depth plots, with respect to the final assessments, for each of the four topics. Ta-ble 4 shows 75% recall depth values for the same four top-ics. Overall, the surrogate-trained results appear inferior for Relative Depth Figure 6: Per-topic 75% relative recall depth plots for the retrospective TREC 2009 Legal experiment, using classi-fiers trained on initial assessments, progressively augmented by Waterloo assessments, and evaluated using final assess-ments. high recall, but substantially so only when using the initial assessments as surrogate, and only for Topics 202 and 207. We note that Topics 202 and 207 have much higher preva-lence than Topics 201 and 203, and due to the stratified sampling used to select the judging pool, the initial assess-ments achieved much higher precision and recall within the pool than in the collection at large, as illustrated in Table 5.

Figure 5 and Table 4 further indicate that the combination of assessments generally yields results as good as, and often superior to, the better of the individual surrogates.
Retrospectively, we conducted one final supplemental ex-periment in an effort to shed some light on the applicability of our results to a more interactive, high-recall retrieval ef-fort. Our supplemental experiment tracked improvement in relative recall depth as the initial assessments were supple-mented incrementally with batches of 500 Waterloo assess-ments.

Figure 6 shows 75% relative recall depth, as a function of the number of Waterloo assessments. For Topics 201, 203, and 207, we see a dramatic gain from supplementing the assessments with a small fraction of the Waterloo assess-ments. For Topic 202, we see little improvement over the near-perfect initial assessments. The result suggests that having an independent assessor judge a fairly small fraction of the documents can result in a dramatic improvement in effectiveness, but further study is needed.
Our results show that it matters who assesses relevance; in particular, it matters whether the assessors whose judge-ments are used to train the system are the same as those whose judgements are used to evaluate the result. A statistic like  X 75% recall X  conveys little meaning without considering,  X  X ccording to whom? X 
In one of the first cases where a court ruled in favor of the responding party X  X  use of machine learning for eDiscovery, over the requesting party X  X  objection, the responding party X  X  brief [1] asserts: Relative Depth Relative Depth Relative Depth Relative Depth evaluated by the final assessments.
 The 59.3% recall average was derived from Grossman and Cormack X  X  analysis of the TREC 2009 Legal Track results [20], calculating the precision and recall of the initial as-sessments, evaluated with respect to the final (i.e., indepen-dent) authoritative assessments. The acceptance criterion of 75% recall, however, was established using the responding party X  X  own reviewers, who were, we presume, also involved in training the system. Our results suggest that, had recall been evaluated using an independent assessor, the calculated recall value might have been considerably lower.

The designation of a particular assessor to be  X  X uthori-tative X  is, in many ways, an artificial construct designed to sidestep well-known uncertainties in the definition of rele-vance, and hence recall (see Webber et al. [45]). While some assessors may be more knowledgeable or skillfull than oth-ers, it is well known that even expert assessors will disagree on a substantial number of assessments [3]. The IR litera-ture suggests than an exhaustive assessment effort by one such expert would be unlikely to achieve more than 65% re-call and 65% precision in the eyes of another, equally skilled and knowledgeable expert [41]. The surrogates used for our TREC-4 and TREC-6 experiments achieved comparable re-call and precision levels: When assessed by J1, J2 achieved recall of 52.9% and precision of 80.8%, and J3 achieved re-call of 63.1% and precision of 78.1%; when assessed by NIST, the conservative Waterloo surrogate achieved recall of 62.8% and precision of 65.2%, while the liberal Waterloo surrogate achieved recall of 86.6% and precision of 50.0%.

Our results do not support the proposition that the use of machine learning  X  X mplifies X  inconsistencies between the surrogate and authority. The classifier trained using a surro-gate X  X  assessments achieves higher recall X  X t a recall depth corresponding to a fraction of the corpus X  X han the surro-gate would achieve by assessing the entire corpus. For ex-ample, Figure 1 shows that a classifier trained using J2 X  X  as-sessments achieves 55% recall at a recall depth correspond-ing to 0.1% of the corpus, while a classifier trained using J2 X  X  assessments achieves 75% recall at a recall depth corre-sponding to 0.542% of the corpus.

Notwithstanding the discussion above, it is a worthwhile objective to try to maximize recall with respect to an au-thoritative assessor, either because that assessor has been stipulated to be the purveyor of true relevance, or because that assessor acts as a proxy for an as-yet-unavailable au-thority, such as a judge or regulator. Presumably, if a sys-tem achieves high recall at low recall depth with respect to one reasonable independent assessor, it is likely to achieve similar results with respect to another.

The results from our three experiments indicate that using a surrogate assessor instead of the authoritative assessor for training can dramatically increase recall depth. While the effect is not universally large, the instances in which it is, cannot be attributed to chance (p &lt; 0.05, corrected for multi-ple hypothesis testing). On the other hand, there are several instances where surrogate training appears to be as good as, or better than, authoritative training. The same general ef-fect is observed, regardless of which assessor is deemed to be authoritative.

Our TREC-4 experiments suggest that randomly inter-mingling the assessments of two surrogates achieves recall depth similar to that of the better surrogate, while using the union of the two surrogates X  assessments improves on both, approaching the effectiveness of using the authority X  X  assessments.

Our TREC-6 experiments show that, when the surrogate assessor makes the deliberate choice to label marginally rele-vant documents as relevant, recall depth is substantially and significantly reduced, relative to the case in which such docu-ments are labeled as non-relevant. Furthermore, training us-ing this liberal assessment strategy yields a materially lower 75% recall depth than authoritative training. While caution must be exercised in extrapolating this result to other as-sessors X  efforts, it strongly suggests that a surrogate can, at least in this instance, train a classifier as effectively as the authority, even when the authority X  X  assessments are deemed to be the gold standard.

Our TREC 2009 Legal experiments show the same pat-tern as the others: For two topics, training using the initial assessor yields substantially inferior results to training us-ing the authority; for the other two topics, the difference was small and not significant. This apparent dissonance might be explained by the fact that, for the latter two topics, the recall and precision of the surrogate X  X ith respect to the judging pool X  X ere exceptionally high. For the former two topics, they were substantially inferior.
Our experiments study only the case of simple passive learning, where a fixed training set is to used to train a learning method to rank the entire corpus, and the top-ranked documents are reviewed until high recall is achieved. Although this practice appears to be widely employed in eDiscovery today, the state of the art is perhaps better rep-resented by interactive, active-learning approaches [12, 27]. Accordingly, our results are applicable only to the former method; their utility in guiding individual stages of an in-teractive or active approach has not been established.
Our  X  X onvenience sample X  of available assessments and collections may not be representative of a typical application of passive supervised machine learning. In our experiments, the judging pool was far from a random sample of the col-lection, as evidenced in Table 5. Nor was it the result of uncertainty sampling as commonly used in active learning. The judging pools for the TREC-4 and TREC-6 experiments might be construed to be representative of relevance feed-back, because the documents were those ranked highly by TREC submissions. The judging pool for the TREC 2009 Legal experiments might be construed to represent query-by-committee, as it was constructed using strata to illumi-nate disagreements among the TREC submissions.

While our findings indicate quite strongly that some com-binations of surrogates and authorities fare poorly, while others fare very well, more research is needed X  X ith a larger population of assessors X  X o gain a thorough understanding of the causal factors. That said, our results clearly suggest that, when other factors are held constant, increasing the diversity or liberality of training assessments increases the quality of ranking, relative to that produced by a single sur-rogate.
Situations where assessments from a single authoritative assessor are unavailable, expensive, or limited, may occasion the use of a surrogate assessor to train a passive supervised-learning method to rank documents. In many situations, the resulting ranking is significantly and substantially inferior to that which would have occurred, had the authoritative assessor been used for training. Our experiments indicate that this effect can be mitigated, and sometimes overcome, by merging the assessments of multiple surrogates, or by instructing the surrogate to use a more liberal interpretation of relevance.

We question whether it is possible to sweep away un-certainties in relevance determination simply by arbitrarily deeming relevance to be the judgment of a single author-itative assessor. It is well known that informed, qualified assessors disagree, and even the same assessor will disagree with him or herself, at different times and in different cir-cumstances. We wonder whether it is useful to expend heroic efforts to anticipate the judgments of one particular asses-sor, and posit, instead, that it might be better to target a hypothetical  X  X easonable authority, X  selected from a pool of equally competent choices. In any event, it is important when evaluating the recall of a retrieval effort, to ask,  X  X c-cording to whom? X  75% recall measured through indepen-dent assessment is a formidable achievement, but the same 75% recall measured through self-assessment is unremark-able. [1] Memorandum in Support of Motion for Protective [2] Da Silva Moore v. Publicis Groupe , 287 F.R.D. 182 [3] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. [4] T. Barnett, S. Godjevac, J.-M. Renders, C. Privault, [5] C. E. Brodley and M. A. Friedl. Identifying mislabeled [6] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. [7] R. Burgin. Variations in relevance judgments and the [8] J. Cheng, A. Jones, C. Privault, and J.-M. Renders. [9] C. W. Cleverdon. The Effect of Variations in [10] G. V. Cormack, C. L. A. Clarke, C. R. Palmer, and [11] G. V. Cormack and M. R. Grossman. The [12] G. V. Cormack and M. R. Grossman. Evaluation of [13] G. V. Cormack and A. Kolcz. Spam filter evaluation [14] G. V. Cormack and T. R. Lynam. Spam corpus [15] G. V. Cormack and M. Mojdeh. Machine learning for [16] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. [17] B. Fr  X enay and M. Verleysen. Classification in the [18] T. Joachims. Making large-scale SVM learning [19] D. Gonsowski. A look into the e-discovery crystal ball. [20] M. R. Grossman and G. V. Cormack.
 [21] M. R. Grossman and G. V. Cormack. Inconsistent [22] D. Harman. Overview of the Fourth Text REtrieval [23] B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard. [24] J. P. Higgins, S. Green, eds. Cochrane Handbook for [25] A. Kolcz and G. V. Cormack. Genre-based [26] M. E. Lesk and G. Salton. Relevance assessments and [27] C. Li, Y. Wang, P. Resnick, and Q. Mei. ReQ-ReC: [28] J. Pickens. In TAR, wrong decisions can lead to the [29] S. Robertson and I. Soboroff. The TREC 2002 [30] H. L. Roitblat, A. Kershaw, and P. Oot. Document [31] T. Saracevic. Relevance: A review of the literature and [32] T. Saracevic. Relevance: A review of the literature [33] T. Saracevic. Relevance: A review of the literature and [34] L. Schamber. Relevance and information behavior. [35] J. C. Scholtes, T. van Cann, and M. Mack. The [36] D. Sculley and G. V. Cormack. Filtering email spam [37] F. Sebastiani. Machine learning in automated text [38] B. Settles. Active learning literature survey . TR 1648, [39] M. D. Smucker and C. P. Jethani. Human performance [40] A. Trotman and D. Jenkinson. IR evaluation using [41] E. M. Voorhees. Variations in relevance judgments and [42] E. M. Voorhees. The philosophy of information [43] E. M. Voorhees and D. Harman. Overview of the [44] E. M. Voorhees and D. K. Harman, eds. TREC: [45] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. [46] W. Webber and J. Pickens. Assessor disagreement and [47] X. Zhu and X. Wu. Class noise vs. attribute noise: A
