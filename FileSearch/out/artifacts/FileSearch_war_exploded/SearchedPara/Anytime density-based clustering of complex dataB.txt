 Son T. Mai  X  Xiao He  X  Jing Feng  X  Claudia Plant  X  Christian B X hm Abstract Many clustering algorithms suffer from scalability problems on massive datasets and do not support any user interaction during runtime. To tackle these problems, anytime clustering algorithms are proposed. They produce a fast approximate result which is con-tinuously refined during the further run. Also, they can be stopped or suspended anytime to provide an intermediate answer. In this paper, we propose a novel anytime clustering algo-rithm modeled on the density-based clustering paradigm. Our algorithm called A-DBSCAN is applicable to many complex data such as trajectory and medical data. The general idea of tion to produce multiple approximate results of the true density-based clusters. A-DBSCAN operates in multiple levels w.r.t. the LBs and is mainly based on two algorithmic schemes: (1) an efficient distance upgrade scheme which restricts distance calculations to core objects at each level of the LBs and (2) a local reclustering scheme which restricts update operations to the relevant objects only. To further improve the performance, we propose a significant exten-sion version of A-DBSCAN called A-DBSCAN-XS which is built upon the anytime scheme of A-DBSCAN and the  X  -range query scheme of a data structure called extended Xseedlist. A-DBSCAN-XS requires less distance calculations at each level than A-DBSCAN and thus is more efficient. Extensive experiments demonstrate that A-DBSCAN and A-DBSCAN-XS acquire very good clustering results at very early stages of execution and thus save a large amount of computational time. Even if they run to the end, A-DBSCAN and A-DBSCAN-XS are still orders of magnitude faster than the original algorithm DBSCAN and its variants. We also introduce a novel application for our algorithms for the segmentation of the white matter fiber tracts in human brain which is an important tool for studying the brain structure and various diseases such as Alzheimer.
 Keywords Anytime clustering  X  Density-based clustering  X  Lower bounding distance  X  Fiber segmentation  X  Fiber clustering  X  Diffusion tensor imaging 1 Introduction Clustering is the task of assigning unlabeled objects into groups called clusters such that the similarity of objects within a group is maximized and the similarity of objects between includingdatamining,machinelearning,patternrecognition,imageanalysis,andinformation most of them work in a batch scheme. They only produce a single result, and there is no interaction with end users during their executions.

For large databases, the idea of exploring the results during execution time has been proved to be a very useful approach [ 1  X  3 ]. The algorithms quickly produce an approximate result which is continuously improved over time and allow user interaction during their runtime. Users can terminate the algorithms anytime whenever they satisfied with existing results to
Among various kinds of clustering algorithms such as partitioning methods and hierar-chical methods, density-based clustering algorithms have attracted a lot of attention in the data mining community due to many advantages compared with the others [ 10 , 11 ]. They can are robust to outliers. Besides many others, the density-based notion underlying the algorithm DBSCAN [ 11 ] is one of the most successful approaches to clustering with applications in many fields such as neuroscience and meteorology. Many clustering algorithms are success-fully proposed based on this notion, e.g., [ 10  X  12 ]. However, all of them only work in the batch scheme. 1.1 Contributions Our contributions are summarized as follows: 1. In our paper, we propose for the first time a novel anytime clustering algorithm based 2. In order to further improve the efficiency, we propose a significant extension version 3. We theoretically prove that the final clustering results of our anytime algorithms 4. Extensive experiments on real datasets such as time series and trajectories demonstrate 5. We also introduce a novel application for our algorithms for the segmentation of the The rest of this paper is organized as follows. In Sect. 2 , we illustrate some background. Section 3 describes our anytime clustering algorithm A-DBSCAN. In Sect. 4 , we propose an extension of A-DBSCAN called A-DBSCAN-XS. The distance measure and lower bound-ing functions are described in Sect. 5 . Section 6 reports experimental results. Section 7 is dedicated to related work and discussion. Section 8 presents the fiber segmentation as a novel application for our algorithms. Section 9 concludes with a summary and future research. 2 Background 2.1 Anytime clustering Anytime clustering produces a fast approximate result which is then refined during the further run. Users can examine the intermediate clustering results while the algorithm is continuing to produce the finer results at the next levels. According to [ 2 , 3 ], an anytime clustering algorithm should satisfy some important properties such as: 1. It should produce good results that are close to the result of the batch algorithm at some 2. The final result should be similar to or better than the batch algorithm. 3. The total cumulative runtime of the algorithm should be only slightly larger than the 2.2 The algorithm DBSCAN In density-based clustering, clusters are considered as areas of high object density separated by areas of low object density in the data space. The key idea of density-based clustering has to exceed a predefined threshold.
Given a set of objects S , a distance function d : S  X  S  X  R and two parameters  X  R + and  X   X  N + .
 Definition 1 ( -neighborhood )The -neighborhood of p  X  S , denoted as N ( p ) ,isdefined by N ( p ) ={ q  X  S | d ( p , q )  X  } .

Each object in S is classified as either core object, border object or noise object depending on its neighborhood.
 Definition 2 ( Core object property ) An object p  X  S is a: 1. Core object, denoted as core ( p ) ,iff | N ( p ) | X   X  . 3. Noise object, denoted as noise ( p ) , iff it is not a core object or a border object. Definition 3 ( Directly density-reachable ) An object q  X  S is directly density-reachable from object p  X  S , denoted as p q ,iff | N ( p ) | X   X  and q  X  N ( p ) .
 Definition 4 ( Density-connected ) Two objects p and q  X  S are density-connected, denoted and p x 1  X  X  X  x m q .

A cluster is defined as a maximal set of density-connected objects and is composed of core objects and border objects. A border object could belong to several clusters depending on the order of objects.
 Definition 5 ( Cluster ) A subset C  X  S is called a cluster iff the two following conditions hold: 1. Maximality:  X  p  X  C ,  X  q  X  S \ C : X  p q 2. Connectivity:  X  p , q  X  C : p q
Figure 1 demonstrates some notions of DBSCAN. DBSCAN uses a data structure called the seed list Sl which contains a set of seed objects for cluster expansion. To construct a cluster, DBSCAN continuously extracts objects from Sl and performs the -range query to find neighbor objects and inserts them into Sl . The process is repeated until Sl is empty. Interested readers please refer to [ 11 ] for more details. 2.3 The lower bounding distance Given a set of objects S and a distance function d : S  X  S  X  R between pairs of objects, a lower bounding (LB) distance of d is a distance function d lb : S  X  S  X  R which  X  p , q  X  S : d queryprocessing[ 13  X  15 ].ThereexistintheliteraturemanydifferentkindsofLBdistancesfor many different kinds of distance measures such as Euclidean distance (ED), dynamic time warping (DTW), and longest common subsequence (LCS) [ 16 ]. LB distances are usually much faster than the original ones. The quality of LB distance is usually described by the Despite of their potential, LB distances are however not paid enough attention in the field of clustering with very limited works such as [ 17 , 18 ]. 3 Anytime density-based clustering Given a set of objects S , a distance function d : S  X  S  X  R and two parameters  X  R + and  X   X  N + . Assuming that there exists a sequence D of n distance functions d i : S  X  S  X  R so bound of d than d i ).
 Based on the sequence of LBs D , our anytime density-based clustering algorithm, denoted is performed by using the function d i as the distance measure. 3.1 The naive approach A naive algorithm would calculate the new distances between all objects and would perform the calculation in L i + 1 is thus a reasonable approach to speed up the algorithm. 3.2 Our approach A-DBSCAN maintains a neighborhood graph G = ( S , E ) which connects each object p  X  S with objects inside its -neighborhood. In the beginning, the graph G is fully connected. At the used distance function.

At level L i + 1 ,the -neighborhood of object p is defined as a set of objects q so that q is adjacent to p in the graph G at L i ( G i )and d i + 1 ( p , q )  X  .
 Definition 6 ( -neighborhood w.r.t.G i + 1 )The -neighborhood of p at L i + 1 , denoted as N + 1 ( p ) ,isdefinedby N i + 1 ( p ) ={ q  X  S | ( p , q )  X  E i  X  d
Following Definition 6 , the graph G i + 1 is created by removing every edge ( p , q )  X  E i part of the distances among objects must be updated at each level instead of all distances. However, while the neighborhoods of objects in the naive algorithm at L i + 1 depend only on the distance function d i + 1 , the notion of the neighborhood of A-DBSCAN considers not neighborhood graph G . As we shall see, this scheme is the heart of A-DBSCAN that allows it to be used with arbitrary sequences of LB distance functions, thus enhancing the applicability of our algorithm. 3.3 How the clusters change A-DBSCAN works by exploiting the way the clusters change at each level. Assuming that we are currently at level L i + 1 .
 Lemma 1 For every object p  X  S, the neighborhood of p at L i + 1 is a subset of the neigh-borhood of p at L i .
 Proof Straightforward from Definition 6 .

According to Lemma 1 , the neighborhood of each object p decreases at each level. Thus, from L i to L i + 1 , the core property of each object changes as the following: Lemma 2 From level L i to L i + 1 . 1. If p is a core object in L i , then it is a core, a border, or a noise object in L i + 1 . 2. If p is a border object in L i , then it is a border or a noise object in L i + 1 . 3. If p is a noise object in L i , then it remains a noise object in L i + 1 . Proof According to Lemma 1 and Definition 2 , a core object p will become a border or a neighborhood size never increases. But it will become a noise object if it does not have any core object inside its neighborhood. Since the neighborhood of a noise object p does not contain any core objects, it will remain a noise object in L i + 1 .

In DBSCAN, each cluster contains two kinds of objects: core and border objects. And the a core cluster as a maximal set of density-connected core objects or a set of core objects inside a cluster in other words.
 C  X   X  :  X  is a cluster.
 Lemma 3 For all objects p , q  X  S, if p and q are not density-connected at L i (denoted as  X  p i q), then they are not density-connected at L i + 1 .
 Proof According to Lemmas 1 and 2 ,if p i + 1 q  X  p i q . Assuming that p i + 1 q ,there Definition 4 . Thus, p x 1  X  X  X  x m q at L i . Therefore, we have p i q .
 C v at L i where C u  X  C v .
 and Lemma 3 ,wehave  X  p q at L i  X  X  p q at L i + 1 . Thus, p and q belong to different clusters by Definition 5 .
Figure 2 illustrates the changes of clustering results and core properties of objects from the division of its core cluster. The border objects of a cluster may be possessed by other cluster does not split. Such property of A-DBSCAN reminds us the monotonicity property of the subspace clustering algorithm SUBCLU [ 19 ]. Under the Euclidean distance (ED), the distances in subspace projections of the data could be considered as a sequence of increasing DBSCAN. By exploiting a graph structure, A-DBSCAN acquires the monotonicity property even for arbitrary sequences of LBs and is not restricted to ED like SUBCLU. Therefore, the applicability of the algorithm is increased. We also note that SUBCLU is not an anytime algorithm.

To conclude, the changes of clustering results of A-DBSCAN are monotonic w.r.t. the changes of graph G at each level. 3.4 Anytime DBSCAN algorithm The cluster notion of our anytime algorithm is generally built upon the paradigm of DBSCAN. from a core object q  X  C and q is the closest core object of p among all the core objects inside the -neighborhood of p .
 Definition 8 ( The border of a core cluster )Aset B of border objects is called the border of acorecluster C iff  X  p  X  B : (1)  X  q  X  C : q p (2)  X  X  q  X  S \ C :| N ( q ) | X   X   X  X  X  r  X  C : d ( p , q )&lt; d ( p , r ) .
 C .

In A-DBSCAN, a border object is assigned to its nearest core object instead of being randomly assigned as in DBSCAN. This brings up some benefits: (1) the compactness of clusters and thus the clustering quality are enhanced; (2) the cluster result does not depend on the the order of the input data as in DBSCAN. In our experiments in Sect. 6 ,thisnew notion helps to slightly improve the clustering results measured by NMI scores [ 20 ] on 13/32 datasets acquired from the UCR archives 1 while having the same results on the others [ 21 ].
Since A-DBSCAN operates in multiple levels, we have to efficiently solve two problems at each level: 1. How to upgrade the graph G ? 2. How to perform the clustering?
According to Definition 6 , we have to update the whole graph G from L i to L i + 1 .It is more efficient than calculating all the distances among objects in the naive approach. However, Lemmas 1  X  4 suggest a more efficient way as follows. In graph G ,thereexistfive kinds of edges: core X  X ore, core X  X order, border X  X order, border X  X oise, and noise X  X oise (the core X  X oise edges do not exist according to Definition 2 ). Since the edges between border and noise objects do not involve in clustering process, they can be safely ignored to save computation cost. Therefore, we just need to update the parts that involve the core objects: the core X  X ore and core X  X order edges. This update scheme significantly reduces the cost of words, we only consider the subgraph with core X  X ore and core X  X order edges. Note that the graph G will no longer reflect the neighborhoods of all objects exactly. However, it will not affect the correctness of our algorithm as shown below.

For clustering algorithm, some clusters may be split, but there is no merging of clusters from L i to L i + 1 according to Lemma 4 .
 Corollary 1 From level L i to L i + 1 ,anA-clusterC k may be split if: 1.  X  u ,v  X  C k : ( u ,v)  X  E i \ E i + 1  X  core ( u )  X  core (v) . 2.  X  u  X  C k : core ( u ) at L i  X  X  core ( u ) at L i + 1 .

Corollary 1 is directly inferred from Definitions 4 and 9 . The deletion of an edge between two core objects or degradation of a core object in an A-cluster would break the density of the whole dataset as in naive approach, thus saving significant amount of time (roughly O cluster labels of their nearest core objects following Definition 9 .Incaseweusetheoriginal cluster notion of DBSCAN, only some border objects need to be reassigned, which is faster but may be less effective in terms of clustering quality.

Figure 3 shows the pseudo-codes for the algorithm A-DBSCAN. For every level, first the graph G is updated. After that, we check all clusters to see whether there are splitting scheme of DBSCAN again. All the border objects will then be added to clusters following their nearest core objects. 3.5 Correctness of algorithm We show that the final clustering result of A-DBSCAN is identical to that of DBSCAN except for those objects which can be border objects of two or more different clusters.
Definition 6 guarantees that the final neighborhoods of objects of A-DBSCAN are similar to those of DBSCAN. The graph update scheme ignores only the edges between noise and border objects which do not play any role to determine core properties of objects and density connectedness of clusters. Therefore, the core properties of objects and the core clusters of A-DBSCAN at the last level are identical to those of DBSCAN. Since the border objects are between A-DBSCAN and DBSCAN since the border objects of DBSCAN are assigned based on the order of objects. If we use original notion of cluster of DBSCAN then the final results of A-DBSCAN and DBSCAN are totally identical. 3.6 A-DBSCAN and the naive algorithm Atamiddlelevel L i , the neighborhoods of objects of A-DBSCAN are not identical to those of the naive algorithm due to the new neighborhood notion in Definition 6 . Therefore, the clustering result of A-DBSCAN at L i is actually an approximation of the clustering result under the distance function d i . They are identical if and only if D contains a sequence of LBs. 3.7 Complexity analysis In theory, the time complexity of an anytime algorithm is usually higher than the batch X  X  one since it has to run the clustering procedure many times. In our setting, the complexity of since A-DBSCAN has efficient graph update and reclustering scheme as described above, A-DBSCAN is much faster than DBSCAN as we shall see in Sect. 6 . 4 Speeding up anytime density-based clustering 4.1 The Xseedlist There exist several techniques to speed up DBSCAN in the literature by accelerating the -range query process [ 11 , 17 ]. The algorithm B-DBSCAN [ 17 ], in contrast, relies on the  X  -range query to determine the core property of objects rather than the -range query. Thus, it does not require calculating all the exact distances between p and its neighbors N ( p ) like others. Therefore, the efficiency of the algorithm is much improved. B-DBSCAN uses a lower bounding function of the true distance measure and a data structure called the Xseedlist to perform the clustering.

Figure 4 shows the data structure Xseedlist. It consists of an ordered object list OL . Each (1 all of its neighbors under the LB distance, sorts them, and then updates the distance between p and its neighbors with the true distances until the core property of p is determined. The others will be updated only when they are necessary to determine the density connectivity of objects during the cluster expansion. Due to space limitation, interested readers please refer to [ 17 ] for more details.

Compared with the original Seedlist [ 11 ], Xseedlist can help to reduce the total number drawbacks: (1) It requires the full LB distances between all objects to be calculated; (2) It needs to sort the object list and predecessor list many times during the clustering process. Thus, the use of Xseedlist pays off only when the runtime difference between the LB and true distance is significant; (3) It is originally designed to work with a single LB distance. Though it can be slightly extended as shown in Sect. 6 to work with multiple LBs, all the LB distances still have to be calculated. 4.2 The Proposed algorithm A-DBSCAN-XS In this part, we present our anytime clustering algorithm called A-DBSCAN-XS which is a significant extension of A-DBSCAN described above [ 21 ]. The general idea is to integrate the scheme of the  X  -range query of the Xseedlist into the reclustering process of A-DBSCAN to update the cluster structure from L i to L i + 1 . By this way, we can reduce the number of calculated distances at each level and thus significantly enhance the efficiency of the algorithm. A naive approach could use the Xseedlist with the distance function d i  X  1 as However, it is impossible since the LB function d i  X  1 is generally not a true lower bound of d the last distance function d n (or d ).

To the rest of this section, we present how the Xseedlist can be extended and inte-grated with the anytime clustering scheme of A-DBSCAN to save the distance calculations for all d i . 4.2.1 The extended graph G Since our algorithm tries to reduce the number of calculated distances at each level, the neighborhood graph G should be only partly updated from L i to L i + 1 . Therefore, for each and A-DBSCAN is that all the edges related to the core objects must be updated at each level in A-DBSCAN while only a part of these edges is updated in A-DBSCAN-XS at each level. The extended neighborhood graph G plays a key role to integrate the Xseedlist into the anytime clustering scheme of A-DBSCAN. 4.2.2 The extended Xseedlist The original Xseedlist [ 17 ] must be modified to work with our anytime clustering scheme. In our extended Xseedlist, the PreFlag and PreDist of PL ( o i ) will contain the value of le function  X (( p , q ), ( r , s )) between them as follows: le reduced compared with the original Xseedlist as demonstrated in Fig. 9 . 4.2.3 The algorithm A-DBSCAN-XS We now can extend the clustering process of A-DBSCAN to work with the scheme of our extended Xseedlist. We called our algorithm A-DBSCAN with Xseedlist (A-DBSCAN-XS). Figure 5 shows the pseudo-code of our algorithm. Unlike A-DBSCAN, A-DBSCAN-XS does not separate the update process into two distinguish parts: distance update and cluster update. They are performed simultaneously during the clustering process instead. A-DBSCAN-XS starts with the extended neighborhood graph G and the cluster structure provided by performing DBSCAN with the first LB function d 1 as the main distance measure. At each level L i , A-DBSCAN-XS performs the reclustering on the set of core objects of each cluster following the monotonicity property described in Sect. 3 . Note that A-DBSCAN only reclusters the clusters which may be split. Therefore, in terms of reclustering time, A-DBSCAN is more efficient than A-DBSCAN-XS. However, A-DBSCAN-XS has much better distance pruning power; thus, it is much more efficient than A-DBSCAN as we shall see in Sect. 6 . After all the clusters are reclustered, A-DBSCAN-XS reassigns each border object to its nearest core object as in A-DBSCAN.
 A-DBSCAN-XS relies on two main subroutines, namely Update-Edge and Update-Cluster. To update an edge ( p , q ) to the current level L i , we check all the functions d j changes summarized as follows: (1) There is no range query with LB function. At level L i , | le we can save many distance calculations compared with the naive use of Xseedlist; (2) We used the extended Xseedlist described above instead of the original Xseedlist; (3) The pro-cedure Update-Edge is called to update the distance of each edge to the current function d at L i instead of the true distance function call; (4) Some other minor changes are also implemented to fit the extended Xseedlist with the anytime clustering scheme. 4.2.4 Correctness of the algorithm We prove that the result of A-DBSCAN-XS is identical to that of A-DBSCAN at every level.
First, due to the partly update scheme of graph G , for every object p at L i , the neigh-borhood | N ( p ) | acquired with A-DBSCAN-XS will be a superset of | N ( p ) | acquired with A-DBSCAN. Since the Core-Check function examines all objects inside | N ( p ) | until it with both algorithms. Second, the Update-Cluster function guarantees that an object o will A-DBSCAN-XS and A-DBSCAN are identical at every level. 4.2.5 Complexity analysis Since Xseedlist uses a sorting procedure to perform, the time complexity of A-DBSCAN-XS the distance functions d i are expensive, the sorting cost will become negligible compared with the reduction of the distance calculations. In this case, A-DBSCAN-XS enjoys dramatic performance acceleration due to its efficient distance pruning scheme. 5 Similarity measure and lower bounding Since A-DBSCAN is a general framework, it can be used with any kind of distance functions and their LBs. Given a distance function d , providing a set of lower bounding function is an essential problem in our approach. In order for A-DBSCAN to work properly, these d ( d i ( p , q )  X  d i + 1 ( p , q ) in general). 5.1 Euclidean distance Recent researches have introduced many distance measures for complex data such as Euclid-ean distance (ED), longest common subsequence (LCS), and dynamic time warping (DTW) [ 16 , 22 ]. Among them, ED is the most widely used one due to its simplicity and ubiquitous-ness.

Given two objects A ={ a 1 ,..., a n } and B ={ b 1 ,..., b n } X  S ,wehave: d ( A , B ) =
Though our algorithm can be used with all mentioned distance functions above, we simply choose ED as a representative to demonstrate our algorithm. 5.2 The Haar wavelet transform Lower bounding functions for the ED are well studied in the literature. There exist many proposed techniques such as piecewise aggregate approximation (PAA) [ 9 ], discrete Fourier be directly applied to A-DBSCAN. In this work, we simply choose DWT as a representative to build a sequence of LBs.

The Haar transform [ 8 , 13 ] can be seen as a series of averaging and differencing opera-tions between two adjacent values of a discrete time function f ( x ) at a given resolution to form a smoothed, lower-dimensional representation of signal. The wavelet decomposition is the combination of the coefficients at all resolutions: The first coefficient is the over-all average of f ( x ) , while the other coefficients store the amount of information lost at each resolution. Due to space limitation, interested readers please refer to [ 13 ] for more details.
 averages of (9 5) and (3 7), respectively. The coefficients at resolution 2 are half of the differences of (9 5) and (3 7). The average and coefficient at level 1 are obtained by the at all resolutions.
 Lemma 5 The Haar transform preserves the Euclidean distance [ 13 ].
 Proof See Lemmas 2 and 3 in [ 13 ].

To build a sequence of LBs, we first transform all objects using DWT. Then, at each level L i ,weonlyusethefirst k i coefficients of each object to calculate the dis-tance function d i with k i  X  k i + 1 . According to Lemma 5 , the lower bounding condi-tion 2 holds. Since we only need a few coefficients to have a good approximation of the original distance function, the runtime of the distance functions d i is significantly faster than the original ED function d , which satisfies the runtime condition 1. Due to linear complexity of DWT, the time needed to transform the whole dataset S with For a large time series dataset with 9,236 objects with the length of each object n = 8,192, it costs only 2.1s to transform the whole dataset and an hour for clustering with DBSCAN. 6 Experiments All experiments are conducted on a Workstation with 3.0Ghz CPU, 8GB RAM under Win-dows Server 2008 using Java. 6.1 Evaluation methodology 6.1.1 Datasets We evaluate the performance of our algorithms on real datasets acquired from different sources, including:  X  The ten real datasets from the UCR archives 2 which contain small to large time series data  X  The two datasets Character Trajectory (CT) and Australian Sign Language (ASL) from  X  The COIL20 dataset acquired from the Columbia Object Image Library. 4 This dataset
We note that the UCR and UCI datasets are re-interpolated to the length of 2 log ( n ) + 3 to use with DWT. However, it does not affect the evaluation of our algorithm since all the comparable algorithms described below produce almost identical results with A-DBSCAN and A-DBSCAN-XS (please refer to Sect. 3 and 4 for more details). Also, in our experiments on 32 UCR datasets, the change of the clustering qualities between the original and the re-interpolated datasets are negligible. 6.1.2 Algorithms We compare our algorithms A-DBSCAN and A-DBSCAN-XS with 3 different variants of DBSCAN proposed in the literature [ 11 , 17 ] including:  X  The original DBSCAN algorithm proposed by Ester et al. [ 11 ].  X  DBSCAN with multi-levels filter-and-refinement range query (M-DBSCAN) which uses  X  DBSCAN with Xseedlist (B-DBSCAN) proposed by Brecheisen et al. [ 17 ]. Since the
The comparison between the naive anytime algorithms, A-DBSCAN and A-DBSCAN-XS is omitted for clarity since the naive algorithms clearly perform much worse than DBSCAN, M-DBSCAN and B-DBSCAN. 6.1.3 Comparison criteria To compare the results of different clustering algorithms with the class labels provided for our experimental datasets, we use the DOM [ 23 ], Normalized Mutual Information (NMI) and Adjusted Mutual Information (AMI) [ 20 ]. However, we only show the NMI for clarity because the results of DOM and AMI are the same as NMI. The result of NMI is in [0,1], with 0 means that the clustering result is independent of the ground truth and 1 means that the clustering result is the same as the ground truth.
 For the runtime comparison, we report the cumulative runtime at each level for A-DBSCAN and A-DBSCAN-XS and the final runtime for B-DBSCAN, M-DBSCAN and DBSCAN. All the reported results are averaged over 20 runs. It is important to note that, A-DBSCAN and A-DBSCAN-XS do not necessary to be faster than B-DBSCAN, M-DBSCAN and DBSCAN in general since they have to perform the clustering at each level. However, they should quickly produce good clustering results at early levels [ 2 ]. 6.1.4 Parameter settings For A-DBSCAN, A-DBSCAN-XS, B-DBSCAN and M-DBS-CAN, we use a sequence of d at L i uses first D i % Wavelet coefficients to calculate lower bounding distance (100% means original ED distance). For the two parameters  X  and , we first run DBSCAN with 2  X   X   X  30 and 100 equally distributed values of from min to max distance between objects to find the optimal parameters for DBSCAN for every dataset. Then all other algorithms are evaluated with these parameters. Therefore, we will have fair comparisons among all these algorithms and exclude any possible comparison bias. The selection of parameters will be further studied in Sect. 6.3 . 6.2 Performance evaluation 6.2.1 Performance on real datasets ure, the upper part shows the NMI of the anytime algorithms A-DBSCAN and A-DBSCAN-XS at each level represented as curves and the NMI of the batch algorithms DBSCAN, M-DBSCAN and B-DBSCAN represented as horizontal lines (because these batch algorithms produce only 1 result). The parameters  X  and of DBSCAN are shown beside the name of each dataset, respectively. The lower part of each figure shows the cumulative runtimes of A-DBSCAN and A-DBSCAN-XS at each level represented as curves and the runtimes of B-DBSCAN and M-DBSCAN represented as horizontal lines. The runtimes of DBSCAN are shown beside the name of each dataset.
 Taking the dataset MALLAT (  X  = 7 , = 12 . 8) as an example, the NMI score of DBSCAN (and also M-DBSCAN, B-DBSCAN) is 0.824. The runtimes are 15.7, 23.7, and 164.4s for B-DBSCAN, M-DBSCAN, and DBSCAN, respectively. At the first level L 1 , A-DBSCAN and A-DBSCAN-XS require 9.7 and 8.7s, respectively, to complete, which are about 2, 3, and 17 times faster than B-DBSCAN, M-DBSCAN, and DBSCAN, respec-tively, with a good clustering score of 0.747. The clustering scores of A-DBSCAN and A-DBSCAN-XS come to 0.826 at level 2 X 3 and 0.824 at level 4 X 10. When they come to the end, A-DBSCAN and A-DBSCAN-XS require only 21.3 and 10.3s, respectively and are about 8 and 16 times faster than DBSCAN.
 For most datasets, the clustering scores become very close to the clustering scores of DBSCAN (more than 80% of NMI score of DBSCAN) from level 2. This means that A-DBSCAN and A-DBSCAN-XS acquire very good and stable clustering results at very early stages. For the dataset ECGFiveDays, SonyAIBORobotSurface and Two_Patterns, users can terminate the algorithm at level 3 or later to have satisfactory clustering results. For other datasets, the termination can be even earlier to acquire the speed up of 5 X 10 times compared with M-DBSCAN and B-DBSCAN and more than 10 times compared with DBSCAN. It is a remarkable advantage, especially when we are using the cheap ED as the distance measure. The difference would be much larger when we use expensive distance functions such as DTW or LCS [ 16 ]. We demonstrate this fact in Sect. 8 by using DTW as the true distance measure.
For the dataset Cin_ECG_torso (  X  = 22 , = 63 . 9), the NMI scores at the early levels L 1 to L 3 (0.743, 0.739, and 0.739, respectively) are even slightly better than the NMI scores of DBSCAN (0.738). For the dataset ECGFiveDays, the NMI score at level 3 is 0.724 which is slightly better than the NMI scores of DBSCAN (0.702). The similar result is observed with the dataset Two_Patterns from levels 5 X 9. These facts are interesting since we can have better clustering results at some middle levels which could not be acquired with the true distance function. We will examine this problem more properly in Sect. 6.3 .

For the datasets Mallat and DiatomSizeReduction, the final clustering result is slightly different with those of DBSCAN. It is due to the fact that we assign the border objects to their closest core objects (see Definition 9 in Sect. 3 ). In our experiment, this scheme helps to slightly improve the final clustering scores of A-DBSCAN and A-DBSCAN-XS on 13/32 real datasets from the UCR archives. The final clustering scores of A-DBSCAN and A-DBSCAN-XS are slightly worst than DBSCAN on only 2/32 real datasets [ 21 ]. We note that if we use the original cluster definition of DBSCAN, then the results of DBSCAN, A-DBSCAN, and A-DBSCAN-XS are identical.

For all datasets, the final cumulative runtimes of A-DBSCAN are slightly better than or identical to those of M-DBSCAN. The final cumulative runtimes of A-DBSCAN-XS are much better than those of B-DBSCAN on most datasets, except on the 4 datasets SonyAIBORobotSurface, Plane, COIL20, and Wafer. Besides, A-DBSCAN-XS is faster than A-DBSCAN on 7/13 datasets and is roughly identical to A-DBSCAN on 3/11 datasets. There are only three exception cases on the dataset COIL20, SonyAIBORobotSurface, and Wafer where A-DBSCAN-XS is slower than A-DBSCAN. The fact that A-DBSCAN is faster than M-DBSCAN and A-DBSCAN-XS is faster than B-DBSCAN is very interesting since A-DBSCAN and A-DBSCAN-XS have to perform the clustering process at each level. The main reason is due to the monotonicity property of A-DBSCAN and A-DBSCAN-XS. We will examine this problem below. 6.2.2 Why are the anytime algorithms faster than the batch algorithms? Figure 8 shows the percentages of the total distance function calls for every LB function d on three real datasets CinC_ECG_torso, SonyAIBORobotSurface, and COIL20. We note that the percentages of the distance function d 1 are always 100% since we have to perform the full clustering at level 1.

Due to the use of the monotonicity property to restrict the distance update, A-DBSCAN requires less distance calculations at each level than M-DBSCAN. By incorporating the monotonicity property and the extended Xseedlist, A-DBSCAN-XS clearly is the best among A-DBSCAN and A-DBSCAN-XS must incur a remarkable cost for cluster update, though the monotonicity property helps to significantly reduce this cost following the local update scheme. Compared with A-DBSCAN, the cluster update cost of A-DBSCAN-XS is much larger due to the operation cost of the Xseedlist. A-DBSCAN-XS also has to update every cluster while A-DBSCAN only updates a cluster if it may split. Obviously, if the reduced cost of the distance calculations is larger than the cluster update cost, A-DBSCAN and A-DBSCAN-XS will be faster than their related algorithms M-DBSCAN and B-DBSCAN, respectively, and vice versa. In the cases of the dataset COIL20 and Wafer in Figs. 6 and 7 , the cluster update cost of A-DBSCAN-XS overwhelms the reduced cost for the distance calculations. Thus, A-DBSCAN-XS performs worst.

The more expensive the used distance functions the better the performance of A-DBSCAN and especially A-DBSCAN-XS compared with M-DBSCAN and B-DBSCAN due to the Sect. 8 , we demonstrate this property by using DTW [ 16 ], which is much expensive than ED, as the distance function. The runtime difference among A-DBSCAN, A-DBSCAN-XS, and others is much larger than with the use of ED in this section. 6.2.3 Other experiments The sorting scheme of the extended Xseedlist plays an important role to reduce the total numbers of call to distance functions. Figure 9 shows the total numbers of call to distance functions of our algorithm A-DBSCAN-XS using the new sorting scheme (proposed in Sect. 4 ) and the original sorting scheme for the Xseedlist [ 17 ] on three real datasets. As we can see, the new sorting scheme helps to significantly reduce the numbers of distance function calls, especially for the dataset ECGFiveDays. 6.2.4 Summary A-DBSCAN and A-DBSCAN-XS acquire close clustering scores with DBSCAN at early stages of their execution. Thus, our algorithms accelerate the runtime up to orders of magni-tude compared with B-DBSCAN, M-DBSCAN, and DBSCAN. Though they both use LBs, A-DBSCANandA-DBSCAN-XSarefasterthanM-DBSCANandB-DBSCANbecausethey can exploit the monotonicity property to reduce redundant distance upgrades and recluster-ing cost as described in Sects. 3 and 4 . Due to the high cost of the clustering update in contrast to the distance calculation reduction power of A-DBSCAN-XS, it works best when the used distance functions are expensive. In case the distance functions are less expensive, A-DBSCAN should be chosen. 6.3 Parameter analysis 6.3.1 The parameters  X  and Figure 10 shows the relationships between the two parameters  X  (with = 4), (with  X  = 5) and the performance of A-DBSCAN and A-DBSCAN-XS for the COIL20 dataset. The runtime of A-DBSCAN and A-DBSCAN-XS increases with since the increasing of runtime of A-DBSCAN-XS is more sensitive to the choice of than that of A-DBSCAN due to the expensive cost of the data structure extended Xseedlist. The runtime of A-DBSCAN and A-DBSCAN-XS slightly decreases with  X  since the reduction of the number of core objects helps to reduce the total number of distance updates at each level.

The clustering quality is strongly affected by the choices of .For = 3 . 5, A-DBSCAN and A-DBSCAN-XS quickly reach good NMI scores at level 2. The clustering quality slightly increases at each level until level 6 and then decreases. For = 5 . 0, the clustering quality of A-DBSCAN and A-DBSCAN-XS increases and reaches the maximum value at the last level. This phenomenon can be explained via the relationship between the lower bounding distances and the parameter . Assuming that  X  is the optimal parameter for (  X  = 4 . 75 for the dataset COIL20), if &gt;  X  , then the clustering qualities at all levels of A-DBSCAN and A-DBSCAN-XS will be smaller than the optimal one since the distance functions d i lower bound the true distance d .However,if &lt;  X  , then the distance functions d i at some middle levels may approximate the optimal cluster structure due to the lower bounding property. Thus, the clustering scores at some middle levels of A-DBSCAN and A-DBSCAN-XS are better than that of the final level. It is easy to see that A-DBSCAN and A-DBSCAN-XS work better when &lt;  X  since they acquire the best result earlier at some middle levels and have smaller runtime as discussed above.
 In contrast, the choices of the parameter  X  seem less affect the clustering quality of A-DBSCAN and A-DBSCAN-XS. All the NMI score curves are generally the same with different values of  X  . Therefore, A-DBSCAN and A-DBSCAN-XS work better when  X  is big since they have smaller runtime as discussed above.

For real datasets, the relationship between the true distance and LB distance is somewhat Therefore, by using LBs, A-DBSCAN may reach results which are hard to acquire with DBSCAN at some middle levels since the LB distances may reflect the relationship of all the objects better than the true distance. For example, the best found score for COIL20 is 0.908 for A-DBSCAN and A-DBSCAN-XS (  X  = 3, = 4) at level 7, while the best found score for DBSCAN is only 0.857 (  X  = 5, = 4 . 75).
In this paper, we do not focus on the problem of parameter finding for DBSCAN. The optimal choices for  X  and could be selected by some existing heuristics proposed in the literature such as the k-dist graph [ 11 ] and entropy [ 24 ].

To summarize, A-DBSCAN and A-DBSCAN-XS seem more robust to the choices of parameters than DBSCAN due to its anytime scheme as discussed above. Even the parameters are not optimally chosen, A-DBSCAN and A-DBSCAN-XS may still produce good results at some of their middle levels. 6.3.2 The lower bounding functions Choosing a sequence of LBs is an important aspect of A-DBSCAN and A-DBSCAN-XS. to closely approximate the true ED distance. For example, for dataset ECGFiveDays, the use of the first 5% coefficients at level 1 is too small to have a good approximation of the ED many for dataset Symbols. The lower bounding distance is close to the ED distance. Thus, the clustering qualities are the same at all levels.
 Figure 11 shows the relationship between the performance of A-DBSCAN, A-DBSCAN-XS, and the tightness of LBs for the dataset Symbol (  X  = 30 , = 16 . 1). A-DBSCAN and A-DBSCAN-XS are run with three different sequences of LBs D 1 to D 3 (10 levels with different numbers of coefficients). As we see, the higher the numbers of the used coefficients are.

The relationship between the runtime of A-DBSCAN and A-DBSCAN-XS and the num-bers of the used coefficients at each level is somewhat theoretically complicated. Usually, smaller number of coefficients at each level means that the calculation time required for each LB function d i is smaller. However, the pruning power of LB functions is decreased. Thus, of the distance updating and the time for reclustering at each level. The trade-off between these two problems decides the performance of the algorithm. As we see from Fig. 11 as an example, the runtime of A-DBSCAN is increased w.r.t. the numbers of used coefficients while the runtime of A-DBSCAN-XS is decreased. The tightness of LB function could be a solution for this problem. By observing the tightness of LB function w.r.t. the number of the graph size at the next level, but the larger the distance calculation cost. Therefore, by randomly drawing a subset of data and calculating the averaged tightness of LBs w.r.t. the numbers of the used coefficients as demonstrated in Fig. 11 , users can choose the number of levels and number of used coefficients for each level based on their purposes.
For better understanding the effect of LBs, Fig. 12 illustrates the performances of A-DBSCAN and A-DBSCAN-XS on the two datasets CT and ASL with D = { 0 . ing scores of our anytime algorithms increase at each level instead of remaining the same distance calculation at each level, the final cumulative runtimes of A-DBSCAN-XS decrease from7.1s(Fig. 7 ) to 1.5s for the dataset CT and from 1.5s (Fig. 7 ) to 0.2s for the dataset ASL. The similar results are also obtained with A-DBSCAN. However, one side effect of reducing the number of used coefficients at each level is that A-DBSCAN-XS is slower than A-DBSCAN at some middle levels due to the high operation cost of the extended Xseedlist. 6.4 Comparison with other algorithms In this section, we compare the performance of our algorithms with some other clustering ity of the eigenvector decomposition problem which plays a central role in many clustering algorithms such as spectral clustering [ 25 , 26 ] or weighted graph cuts [ 27 ].
The density-weighted Nystr X m method (DWN) [ 25 ]firstuses k -Means to group n objects kernel submatrix to approximate the eigenvectors of the full kernel matrix. By performing the eigenvalue decomposition on a much smaller number of objects, the runtime of algo-rithm is speeded up significantly. However, DWN is limited under Euclidean distance. In comparison with our algorithms, DWN is an approximation algorithm and not an anytime algorithm. Moreover, it is not designed to deal with expensive distance measures. Thus, in case the distance function d is expensive, DWN may not perform well. Figure 13 shows the performance of the density-weighted Nystr X m method (DWN) [ 25 ] on the dataset Symbols with different number of clusters. While A-DBSCAN and A-DBSCAN-XS need 3.13 and 1.38s, respectively, to finish with NMI score 0.839, DWN requires around 60s for the same result.

Graclus [ 27 ] follows a different approach to avoid the eigenvector decomposition of var-ious weighted graph clustering algorithms by using kernel k -Means in a multilevel graph graphs. Graclus, however, requires an input graph to process. Building this input graph requires the full distance matrix, which is very time consuming when the distance function is expensive. Table 2 shows the comparison between A-DBSCAN, A-DBSCAN-XS, and Graclus [ 27 ] on five real datasets. Obviously, our algorithms outperform Graclus in terms of runtime. The reason is that Graclus has to spend most of its time to build the input graphs. We note that, in this experiments, we simply construct the fully connected graphs as inputs for Graclus. 7 Related work and discussion 7.1 Anytime clustering algorithms result of the batch algorithm at the end. An anytime algorithm should satisfy some important
Zhu et al. [ 2 ] proposed an approximation technique for dynamic time warping which allows it to be used with anytime clustering algorithms. Kranen et al. [ 29 ] proposed an anytime clustering algorithm for streaming data.
 Lin et al. [ 8 , 9 ] exploited the multi-resolution property of DWT and PAA for casting k-Means into an anytime algorithm called I-kMeans. I-kMeans works by using the final cluster is limited only for spherical shape clusters while A-DBSCAN is able to detect clusters with arbitrary shapes and robust to outliers. The lower bounding property of DWT and PAA is also not exploited to construct clusters as in A-DBSCAN and A-DBSCAN-XS. 7.2 Density-based clustering In density-based clustering, clusters are considered as high-density areas, separated by low-density areas. Among various kinds of density-based clustering algorithms, DBSCAN is one of the most successful algorithms with many extensions and applications in many fields such as medicine. [ 10 , 12 , 17  X  19 , 28 , 30  X  34 ].

Ester et al. [ 10 ] proposed an incremental version of DBSCAN in a data warehousing tering only in the neighborhood of this object, their algorithm called I-DBSCAN significantly speeds up DBSCAN even for large numbers of updates in a data warehouse environment. I-DBSCAN also exploits the nature of DBSCAN to do the clustering like our algorithm. However, I-DBSCAN is an incremental clustering algorithm, not an anytime algorithm. The changes of clusters in I-DBSCAN are caused by inserted or deleted objects, while the changes in A-DBSCAN and A-DBSCAN-XS are directed by the changes of the used distance func-tions.

The density-based subspace clustering algorithm SUBCLU [ 19 ] is based on the monotonicity property of DBSCAN w.r.t. distances in subspace projections of the data. The changes of clusters in A-DBSCAN are also monotonic. However, the monotonicity of A-DBSCAN and A-DBSCAN-XS is caused by the reduction of a special neighborhood graph related to a sequence of LBs, which is more general than SUBCLU. Thus, A-DBSCAN can be used with many different kinds of distance measures such as DTW and LCS [ 16 ] and arbi-trary sequences of LBs while SUBCLU is limited to ED. Moreover, SUBCLU is designed for the discovery of subspace clustering of vector data and is also not an anytime algorithm.
In [ 18 ], a client-server parallel version of DBSCAN was proposed. The monotonicity property of DBSCAN [ 11 ]andOPTICs[ 12 ] is used to split objects to different clients and to merge the results returned from clients at the server. This monotonicity property is also similar to SUBCLU and is a special case of the monotonicity property used with A-DBSCAN.
LBs could be used in density-based clustering to accelerate the range query process, thus DBSCAN [ 11 ] to speed up these algorithms based on a data structure called the Xseedlist to reduce the number of true distance calculations. One major drawback of this technique (B-DBSCAN) is that Xseedlist requires a sorting procedure which leads to higher time complexity than the normal Seedlist of DBSCAN. Thus, it may not be suitable for cheap distance functions and very high number of objects. B-DBSCAN produces only one result and thus is not an anytime algorithm. In our algorithm, we use many LBs to produce multiple approximate clustering results during the runtime. A-DBSCAN and A-DBSCAN-XS rely on the monotonicity property and the restricted neighborhood graph to perform the clustering, which is fundamentally different from [ 17 ].

The idea of A-DBSCAN-XS is built upon the ideas of anytime clustering of A-DBSCAN and the ideas of the  X  -range query from B-DBSCAN [ 17 ]. The main advantage of A-DBSCAN-XS over B-DBSCAN and A-DBSCAN is the way the Xseedlist is modified and combined with the neighborhood graph G and the monotonicity property of the cluster inal Xseedlist of B-DBSCAN is only designed to work with a single LB function, though we can easily modify it to work with multiple LB functions in a naive way as proposed in Sect. 6 . Besides the integration of the Xseedlist, the main difference between A-DBSCAN and A-DBSCAN-XS is that A-DBSCAN contains two separated phases: the distance update and the reclustering phase at each level, while A-DBSCAN-XS combines the distance update and reclustering into a single phase at each level. This combination allows A-DBSCAN-XS substantially reducing the total number of distance calculation at each level. However, it increases the cost for updating cluster compared with A-DBSCAN since A-DBSCAN-XS is unable to detect which clusters will be split to recluster like A-DBSCAN. The high oper-ation cost of Xseedlist of A-DBSCAN-XS is another difference with A-DBSCAN. Thus, A-DBSCAN-XS is more suitable to be used with very expensive distance function than A-DBSCAN. 7.3 Distance expensiveness Like many other kinds of clustering algorithms such as spectral clustering, density-based clustering algorithms require all the distance among objects to perform. In many real-life applications, acquiring these distances can be extremely time consuming, e.g., it takes 127 days for calculating the DTW distances among 9,236 star light curves as pointed out in [ 2 ]. However, many state-of-art clustering algorithms are not designed to cope with this distance expensiveness problem.

In contrast to these algorithms, our algorithms A-DBSCAN and A-DBSCAN-XS are designed to overcome this problem. By using faster LB distances d lb to calculate intermediate results, the runtimes of our algorithms A-DBSCAN and A-DBSCAN-XS at middle levels are obviously much faster than using the true distance function d . LB distances also play an and-refinement scheme [ 15 , 17 ]. Moreover, the monotonicity property of our algorithms helps time at each level. As a result, our algorithms only require a small number of expensive distance calculations instead of all the distances among objects. Consequently, the runtimes of our algorithms are reduced significantly. 7.4 Anytime DBSCAN In general, A-DBSCAN and A-DBSCAN-XS are unique in the ways that they: (1) exploit multi-levels lower bounding functions to produce multiple approximate results of the final clustering result; (2) maintain the graph structure to acquire the monotonicity property even for arbitrary sequences of LBs. By this way, A-DBSCAN and A-DBSCAN-XS can be used with any kind of distance measures and arbitrary sequences of lower bounding functions. Thus, they would have great applicability in reality.

Due to their update scheme, A-DBSCAN and A-DBSCAN-XS work very well on noisy datasets which contain large amount of noise and border objects. Also, A-DBSCAN and A-DBSCAN-XS are extremely useful when using with very expensive distance functions such as DTW or LCS [ 16 ]. Moreover, A-DBSCAN and A-DBSCAN-XS can easily be parallelized. 8 Anytime fiber segmentation In this section, we present a novel application for our algorithm for the segmentation of the white matter fiber tracts in human brain. 8.1 Motivation Understanding anatomical connectivity of human brain is one of the major challenges in neuroscience. However, how to study the brain structures in a noninvasive way is a critical issue. In the past decades, the emergence of the diffusion tensor imaging (DTI) technology tensor fields. Such kind of techniques is called fiber tractography [ 36 ]. Fiber tractography has been widely used in visualization and brain connectivity analysis and is an important tool to study the brain structure and various diseases such as Alzheimer.

Exploring and analyzing fiber tracts are non-trivial problems due to the complexity of the white matter structure and huge amounts of fiber tracts (usually from 10 3 to 10 6 fibers) group similar fibers into anatomical bundles. They help to reduce the complexity of data, improve the visualization, and allow robust quantification and comparison between subjects to find out abnormalities or unusual features in the brains.

Although they are useful, most fiber clustering algorithms still suffer from several prob-lems. First, they have very high running times which are caused by the high time complexity of the fiber similarity measures and clustering algorithms (usually quadratic time complexity). For example, the density-based fiber clustering algorithm [ 44 ] with the well-known mean of 10,000 fibers of average length 75 points per fiber on 2Ghz Workstation with 4GB RAM. Since fiber datasets are usually large, this limitation is truly undesirable. Second, interac-tive exploration of DTI fibers has been proved to be a useful approach for brain connectivity work in a batch scheme. They do not allow interaction with experts during their runtime. Third, providing multiple results for experts to examine would be more useful due to complex result.

Since experts usually expect fast response times and interaction with program during the clustering process of the white matter fiber tracts, the anytime clustering scheme would be very useful. The algorithm quickly produces an approximate result which is continuously refined during the further runs. During its runtime, experts can examine the results, suspend, resume, or stop the algorithm anytime whenever they satisfy with the existing results to save computation cost, or continue the algorithm to look for better results. On the other aspect, according to their opinions from many results produced by the algorithm. To the best of our knowledge, this anytime scheme has never been applied for fiber clustering before. 8.2 Fiber similarity measure Recent researches have introduced many similarity models for fibers such as Hausdoff dis-tance [ 40 ], mean of closest point distance [ 40 ], dynamic time warping, and longest common subsequence [ 31 ]. Although these similarity models are efficient and widely used, they have anytime clustering model becomes a useful approach. In this work, we adopt dynamic time warping (DTW) as the main similarity model for fibers due to its simplicity and ubiquitous-ness in many fields [ 14 ]. However, we note that our algorithm can be used with all mentioned techniques above.

Given two fibers A = ( a 1 ,..., a N ) and B = ( b 1 ,..., b M ) which contain N and M points in 3D. Let A i be the first i points of A . We need to find a warping path W = (w 1 ,...,w K ) ( min ( M , N )  X  K  X  M + N  X  1) which contains pairs of indices of A and B ( w k = ( i , j ) ) as f ( a i , b j ) = 3 d = 1 | a id  X  b jd | .

DTW can be calculated by using dynamic programming approach [ 14 ] to fill the cost We have DTW ( A , B ) = D ( N , M ) .

To reduce the effect of different lengths of fibers, we define the similarity of two fibers A and B as follows:
Note that this formula is slightly different with [ 31 ] which uses the length of warping path between 2 fibers as the denominator. 8.2.1 Lower bounding fiber similarity In order to constructa sequence of lower bounding functions, there existmany lower bounding techniques for DTW which can be employed such as LB_Keogh [ 14 ], LB_Zhu [ 47 ], LB_Yi which divides 1D time series into smaller segments and constructs the lower bounding dis-tance for DTW based on the upper and lower values of these segments. By decreasing the length of each segment to 1, the lower bounding distance increases toward the true DTW dis-tance. This property is well suited with our philosophy. Therefore, we extend the LB_Sakurai to use with 3D fiber trajectories.
 Assuming that A and B are divided into n and m non-overlapping segments at positions P = ( p 1 ,..., p n = n ) and Q = ( q 1 ,..., q m = m ) , respectively. For each segment i ,we distance of A and B is calculated by DTW of segmented representations of A (denoted as A )and B (denoted as B ), respectively. Figure 14 shows an example of LB_Sakurai in 1D. Since the size of the cost matrix of A and B is much smaller than the cost matrix of A and B , the running time of LB_Sakurai is much faster than the original DTW. ( a ) (b)
To calculate LB_Sakurai, we define the distance between two segments p and q with length l p and l q as follows: Figure 15 illustrates the distance between two segments p and q in 1D (a) and in 2D (b). Lemma 6 Given two points p i in p and q j in q, we have: Proof The expression is true at each dimension (see Fig. 15 a for an example). So is the sum.
 Lemma 7 Given two fibers A and B and their segmented representation A and B, respec-tively. We have DT W ( A , B )  X  DTW ( A , B ) .
 Proof Following Lemma 6 , the proof of Lemma 7 is similar to the lower bounding proof in [ 15 ].
 FollowingLemma 7 ,theLB_Sakuraiof Sim ( A , B ) isdefinedas DTW ( A , B )/( N + M  X  1 ) . In this work, we divide each fiber into equal segments of length l (except the last segment). fibers are n ), which is l 2 times faster than original DTW. To construct a sequence of lower tightness of lower bounding [ 14 ] which is measured by the ratio between DTW ( A , B ) and the higher the tightness of LBs. Thus, LB_Sakurai satisfies the two conditions described in Sect. 5 . 8.3 Empirical evaluation 8.3.1 Datasets We evaluate the performance of our algorithm on 6 real labeled datasets DS1 X  X S6 which are randomly extracted from Pittsburgh Brain Competition (PBC) dataset ( http://pbc.lrdc.pitt. edu/?q=home ). These datasets contain 500 to 1,500 fibers belonging to 5 to 8 famous fiber bundles namely Arcuate, Cingulum, Fornix, Inferior Occipitofrontal Fasciculus, Superior Longitudinal Fasciculus, Forceps Major, and Corticospinal. For each dataset, 5 fibers from other bundles are also added as noise. 8.3.2 Parameter setting A-DBSCAN and A-DBSCAN-XS always run on 8 different levels with the length of segment l Note that experts can construct different sequences of lower bounding functions based on their need: Large l means worse results and shorter running times and vice versa. We fix the parameter  X  = 5 as suggested by various researchers [ 11 ]. 8.3.3 Performance comparison Figure 17 shows the comparison of A-DBSCAN-XS, A-DBSCAN, M-DBSCAN, B-DBSCAN, and DBSCAN on 6 real datasets DS1 ( = 0 . 07), DS2 ( = 0 . 1), DS3 ( = 0 . 08), DS4 ( = 0 . 12), DS5 ( = 0 . 08), and DS6 ( = 0 . 1) (the runtime of DBSCAN is written beside the name of each dataset). The NMI scores of the anytime algorithm (A-DBSCAN and A-DBSCAN-XS) come closer to NMI scores of the batch algo-rithm (DBSCAN, M-DBSCAN and B-DBSCAN) at each level. On all datasets, the NMI scores of A-DBSCAN-XS become very close to the results of DBSCAN after level 3 or 4. Therefore, experts can stop the algorithm as soon as it reaches level 3 or 4 to save com-putation time. For DS1, A-DBSCAN-XS acquires the NMI score of 0.842 at level 3 which is a good score compared with the final score 0.988. The cumulative running time of A-DBSCAN-XS at level 3 is only 2.62 secs which is 112 times faster than DBSCAN (293.6 secs), 18 times faster than M-DBSCAN (47.9 secs), and 5 times faster than B-DBSCAN (11.4 secs). When it comes to the end, A-DBSCAN-XS requires only 5.3s which is 55 times faster than DBSCAN, 10 times faster than M-DBSCAN, 2 times faster than B-DBSCAN, and 8 times faster than A-DBSCAN. For the dataset DS4, DS5, and DS6, the cumulative runtime of A-DBSCAN-XS at some mid-levels is slightly slower than A-DBSCAN since the overhead of the Xseedlist is much higher than its benefits. For DS1, DS2, DS3, and DS6, the best results are found at some mid-levels instead of the last one. It is interesting since we can have results which might never be acquired with the batch clustering algo-rithm.

Figure 18 a shows the numbers of calls to different LBs for all techniques on dataset DS6 as an example. The result clearly proves the performance acceleration of A-DBSCAN-XS compared with the others. Figure 18 b shows the numbers of calls to different LBs between using the extended Xseedlist with the sorting function  X  and the original one. The extended Xseedlist helps to reduce the numbers of calls to LBs significantly.

To summarize, good results are found at very early levels by A-DBSCAN-XS. Thus, it helps to speed up A-DBSCAN-XS by orders of magnitudes compared with other techniques. Since DTW is much expensive than ED, the performance acceleration of A-DBSCAN and A-DBSCAN-XS on DTW is much higher than on ED as described in Sect. 6 .Evenwhenit runs to the end, A-DBSCAN-XS is about 50 times faster than DBSCAN, 2 times faster than B-DBSCAN, and 8 times faster than A-DBSCAN. 8.3.4 More experiments Figure 19 shows the clustering results of A-DBSCAN-XS at each level for DS5 which contains 5 bundles. A-DBSCAN-XS detects only 1 bundle at level 1, 2 bundles at level 2, and 3 bundles at level 3. From level 4 to 7, it discovers 4 bundles with only some minor changes. The result at level 8 is closest to the ground truth.

One of the advantages of our algorithm is that experts can be able to choose whatever they think reasonable from the bunch of results produced by our algorithm. For example, since many experts consider the 2 bundles Arcuate and Superior as similar (the yellow and red at level 8. This is extremely useful in neuroscience since the goodness of the classification of fibers sometimes depends on different opinions of experts. Experts can stop the algorithm at level 4, thus acquiring up to 70 times speed up compared with DBSCAN. 8.3.5 Corpus Callosum Figure 20 shows the clustering results of A-DBSCAN-XS for the Corpus Callosum (  X  = 5, = 0 . 03), the biggest and the most important bundle which connects the left and right cerebral hemispheres in human brain. Although we do not have the ground truth to compare, the results are well confirmed by our experts. Moreover, the results show the Corpus Callosum with different resolution from the coarser to finer one. 8.3.6 Scalability Figure 21 (left) shows the scalability of different algorithms for very large fiber datasets which contain 8,000 X 30,000 fibers randomly extracted from a PBC full brain dataset with D ={ 8 , 6 , 4 , 2 , 1 } ,  X  = 5and = 0 . 05 on a workstation with 3.4Ghz CPU and 32 GB Ram. A-DBSCAN and A-DBSCAN-XS acquire better performance than M-DBSCAN and B-DBSCAN, respectively. Among all the algorithms, A-DBSCAN-XS is the best one. For the large dataset with 20,000 fibers, it requires only 1,284s to finish, while DBSCAN requires 21,292.4s (around 6h) which is 17 times slower than A-DBSCAN-XS. The bigger the dataset, the more different the cumulative runtime among different algorithms. For the datasetwith30,000fibers,A-DBSCANandA-DBSCAN-XSrequire1and0.8h,respectively, while DBSCAN was stopped after more than 12h running. Figure 21 (right) shows the cumu-lative runtime of A-DBSCAN and A-DBSCAN-XS w.r.t. different datasets (with 18,000, 20,000, 25,000, and 30,000 fibers, respectively). The cumulative runtime of A-DBSCAN-XS increases roughly linear at each level. In contrast to A-DBSCAN-XS, the cumulative run-time of A-DBSCAN shows exponential trends for large datasets. This illustrates the pruning power of A-DBSCAN-XS in comparison with A-DBSCAN. 9 Conclusion and future remark We propose two anytime density-based clustering algorithms called A-DBSCAN and A-DBSCAN-XS which are applicable for many complex data such as time series and trajec-tory. Our algorithms work by exploiting a sequence of LBs to produce multiple approximate results of the true density-based clusters. To enhance the performance, we propose an effi-cient distance update scheme which partially updates the distances among objects and a local reclustering scheme to save computational time at each level. Some changes in the notions of DBSCAN are made to improve the clustering results. An efficient heuristic for parame-ter setting is also proposed. Experiments on real datasets have shown that A-DBSCAN and A-DBSCAN-XS produce very good clustering results at very early stages of execution, thus saving a large amount of computational time. Even if they run to the end, A-DBSCAN and A-DBSCAN-XS are still much faster than DBSCAN and its variants, despite the fact that they have to produce clustering results at every level. We also introduce a novel application for our algorithm for the segmentation of the white matter structure in human brain. References
