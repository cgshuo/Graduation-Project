 Community detection is an important task in network anal-ysis. A community (also referred to as a cluster) is a set of cohesive vertices that have more connections inside the set than outside. In many social and information networks, these communities naturally overlap. For instance, in a so-cial network, each vertex in a graph corresponds to an in-dividual who usually participates in multiple communities. One of the most successful techniques for finding overlapping communities is based on local optimization and expansion of a community metric around a seed set of vertices. In this paper, we propose an efficient overlapping community detection algorithm using a seed set expansion approach. In particular, we develop new seeding strategies for a per-sonalized PageRank scheme that optimizes the conductance community score. The key idea of our algorithm is to find good seeds, and then expand these seed sets using the per-sonalized PageRank clustering procedure. Experimental re-sults show that this seed set expansion approach outper-forms other state-of-the-art overlapping community detec-tion methods. We also show that our new seeding strategies are better than previous strategies, and are thus effective in finding good overlapping clusters in a graph.
 I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms Algorithms, Experimentation Clustering, Community Detection, Overlapping Clusters, Seeds, Seed Set Expansion
In many social and information networks, nodes partic-ipate in multiple communities. For instance, in a social network, a node X  X  communities correspond to its social cir-cles [18]. We study the problem of overlapping community detection to find these groups. More specifically, we investi-gate how to select the seed sets in a method for overlapping community detection that grows communities around seeds. These local expansion methods are among the most suc-cessful strategies for overlapping community detection [26]. However, principled methods to choose the seeds are few and far between. When they exist, they are usually compu-tationally expensive, for instance, using maximal cliques as seeds [23]. Empirically successful strategies include exhaus-tively exploring all individual seeds and greedy methods that randomly pick a vertex, grow a cluster, and continue with any unassigned vertex. The goal of traditional, exhaustive clustering is to determine a cluster for each and every data point or node. In the community detection problem, instead, we wish to relax this goal and allow incomplete coverage of the network. Put another way, the data may not support assigning a node to a community and we want to respect that feature in our output.

The seeding strategies we consider are based on the same distance kernel that underlies the equivalence between ker-nel k -means and spectral clustering [9]. Using this distance function, we can efficiently locate a good seed within an ex-isting set of vertices of the graph. In particular, a strategy we propose involves computing many clusters using a multi-level weighted kernel k -means algorithm on the graph (the Graclus algorithm) [9]. We use the corresponding distance function to compute the  X  X entroid vertex X  of each cluster and then use the neighborhood set of that centroid vertex as the seed region for community detection. This strategy is inspired by recent work on finding the best communities in a network using a carefully selected set of vertex neighbor-hoods as seeds [11]. These seeds were centers within their respective vertex neighborhoods. In this paper, we take this idea further and consider using the centers of larger regions.
The algorithm we use to grow a seed is based on person-alized PageRank random walks, which we explain further in Section 4.3. The full algorithm to compute overlapping clusters from the seeds is discussed in Section 4. The algo-rithm begins by filtering out regions of the graph that will not participate in an overlapping clustering. We run the seed finding algorithm and the seed expansion method on the filtered graph. We then post-process this output to as-sign communities for the vertices removed by filtering. We show that a simple propagation of our communities to the removed regions does not increase the underlying objective function. The main contributions of this paper are: Our method scales to problems with over 45 million edges, whereas other state of the art methods for overlapping clus-tering were unable to complete on these large datasets.
We summarize a few closely related methods from a re-cent survey [26]. The method we employ is called local optimization and expansion. Starting from a seed, such a method greedily expands a community around that seed un-til it reaches a local optima of the community detection objective. In our case, we use a personalized PageRank based cut finder for the local expansion method (see Sec-tion 4.3). Other overlapping community detection methods include line graph partitioning, clique percolation, eigenvec-tor methods, egonet analysis, and low-rank models. Given a graph G = ( V , E ), the line graph of L ( G )  X  also known as the dual graph  X  has a vertex for each edge in G and an edge whenever two edges (in G ) share a vertex. For instance, the line graph of a star is a clique. A partition-ing of the line graph induces an overlapping clustering in the original graph [3]. Clique percolation methods look for overlap between fixed size cliques in the graph [20]. Clique based techniques often fail to scale to large networks. Eigen-vector methods generalize spectral methods and use a soft clustering scheme applied to eigenvectors of the normalized Laplacian or modularity matrix in order to estimate commu-nities [28]. Egonet analysis methods use the theory of struc-tural holes [7], and compute and combine many communities through manipulating egonets [22, 8]. We compare against the Demon method [8] that uses this strategy. Finally, we note that other low-rank methods such as non-negative ma-trix factorizations [15, 27] identify overlapping communities as well. We compare against the Bigclam method [27] that uses this approach.
Determining how to seed a local expansion method is, ar-guably, the critical problem within these methods. Strate-gies to do so include using maximal cliques [26], prior in-formation [10], or locally minimal neighborhoods [11]. The latter method was shown to identify the vast majority of good conductance sets in a graph; however, there was no provision made for total coverage of all vertices.
The foundation of our new seeding strategy is a distance kernel between vertices. Our choice of kernel underlies the relationship between kernel k -means and spectral cluster-ing [9]. Many other graph kernels involve the exponential or inverse of a matrix [14]. Any such kernel with an efficient means of computing distances would suffice.
In this section, we discuss the overlapping community de-tection problem, and review some traditional metrics for graph clustering.
Given a graph G = ( V , E ) with vertex set V and edge set E , the graph clustering problem is to partition the graph into k disjoint clusters C 1 ,  X  X  X  , C k such that V = C 1  X  X  X  X  X  X  graph clustering traditionally finds exhaustive and disjoint clusters, the overlapping community detection problem is to find overlapping clusters that are not necessarily exhaustive. Formally, we seek k overlapping clusters such that C 1  X  X  X  X  X  C k  X  V . Throughout the paper, the terms set , cluster , and community are used interchangeably.
We can represent a graph with n vertices as an n  X  n adjacency matrix A such that A ij = e ij where e ij is the weight of an edge between vertices i and j , or A ij = 0 if there is no edge. We assume that all the graphs are undirected graphs, i.e., A is symmetric. Let us define links ( C p , C be the sum of edge weights between vertex sets C p and C q
Now, we review some popular measures for gauging the quality of clusters: cut, conductance, and normalized cut.
Cut. The cut of cluster C i is defined as the sum of edge weights between C i and its complement, V\C i . That is,
Conductance. The conductance of a cluster is defined to be the cut divided by the least number of edges incident on either set C i or V\C i : By definition, cond ( C i ) = cond ( V\C i ). The conductance of a cluster is the probability of leaving that cluster by a one-hop walk starting from the smaller set between C i and V\C i .
Normalized Cut. The normalized cut of a cluster is defined by the cut with volume normalization as follows: Notice that ncut ( C i ) is always lesser than or equal to cond ( C
It has been shown that a weighted graph clustering objec-tive is equivalent to a weighted kernel k -means objective [9]. For example, for the exhaustive graph clustering problem, the normalized cut objective of a graph G is This objective can be shown to be equivalent to a weighted kernel k -means objective by defining a weight for each data point (vertex) to be the degree of the vertex, and the ker-nel matrix to be K =  X D  X  1 + D  X  1 AD  X  1 , where D is the diagonal matrix of degrees (i.e., D ii = P n j =1 A ij ), and  X  is a scalar typically chosen to make K positive-definite. Then, we can quantify the distance between a vertex v  X  C i and cluster C i , denoted dist ( v, C i ), as follows: where deg ( v ) = links ( v, V ), and deg ( C i ) = links ( C
We use eight different real-world networks from [1], [19], [24]. The networks are presented in Table 1. All the net-works are connected, undirected graphs.

Collaboration networks. In a collaboration network, vertices indicate authors, and edges indicate co-authorship. If authors u and v wrote a paper together, there exists an edge between them. So, if a paper is written by k authors, this is represented by a k -clique in the network. HepPh, AstroPh, and CondMat networks are constructed based on the papers submitted to High Energy Physics (Phenomenol-ogy) category, Astrophysics category, and Condensed Mat-ter Physics category under the arXiv e-print service, re-spectively. The DBLP network is constructed based on the DBLP computer science bibliography website.

Social networks. In a social network, vertices represent individuals and edges represent social interactions between them. Flickr is an online photo sharing application, Myspace is a social entertainment networking service, and LiveJour-nal is a blogging application where users can publish their own journals. Users can make a friendship relationship with each other in each of these websites.

Product network. In the Amazon product network, ver-tices represent products and edges represent co-purchasing information. If products u and v are frequently co-purchased, there exists an undirected edge between them.
 Graph No. of vertices No. of edges Collaboration networks HepPh 11,204 117,619 AstroPh 17,903 196,972 CondMat 21,363 91,286 DBLP 317,080 1,049,866 Social networks Flickr 1,994,422 21,445,057 Myspace 2,086,141 45,459,079 LiveJournal 1,757,326 42,183,338 Product network
Amazon 334,863 925,872
Now, we explain the overall algorithm which consists of four phases: filtering, seeding, seed set expansion, and prop-agation. In the filtering phase, we remove regions of the graph that are trivially separable from the rest of the graph, so will not participate in overlapping clustering. In the seed-ing phase, we find seeds in the filtered graph, and in seed set expansion phase, we expand the seed sets using a per-sonalized PageRank clustering scheme. Finally, in the prop-agation phase, we further expand the communities to the regions that were removed in the filtering phase.
The goal of the filtering phase is to identify regions of the graph where an algorithmic solution is required to identify the overlapping clusters. To explain our filtering step, re-call that almost all graph partitioning methods begin by as-signing each connected component to a separate partition. Any other choice of partitioning for disconnected compo-nents is entirely arbitrary. The Metis procedure [12], for instance, may combine two disconnected components into a single partition in order to satisfy a balance constraint on the partitioning. For the problem of overlapping clustering, an analogous concept can be derived from biconnected com-ponents. Formally, a biconnected component is defined as follows:
Definition 1. Given a graph G = ( V , E ) , a biconnected component is a maximal induced subgraph G 0 = ( V 0 , E 0 remains connected after removing any vertex and its adja-cent edges in G 0 .

Let us define the size of a biconnected component to be the number of edges in G 0 . Now, consider all the biconnected components of size one. Notice that there should be no over-lapping partitions that use these edges because they bridge disjoint communities. Consequently, our filtering procedure is to find the largest connected component of the graph af-ter we remove all single-edge biconnected components. We call this the  X  X iconnected core X  of the graph even though it may not be biconnected. Let E S denote all the single-edge biconnected components. Then, the biconnected core graph is defined as follows:
Definition 2. The biconnected core G C = ( V C , E C ) is the maximum size connected subgraph of G 00 = ( V , E \E S
Notice that the biconnected core is not the 2-core of the original graph. Subgraphs connected to the biconnected core are called whiskers by Leskovec et al. [16]. Formally, whiskers are defined as follows:
Definition 3. A whisker W = ( V W , E W ) is a maximal subgraph of G that can be detached from the biconnected core by removing a bridge, where a bridge is defined as follows:
Definition 4. A bridge is a biconnected component of size one which is directly connected to the biconnected core. Let E B be all the bridges in a graph. Notice that E B  X  X  On the region which is not included in the biconnected core graph G C , we define the detached graph G D as follows:
Definition 5. G D = ( V D , E D ) is a subgraph of G which is induced by V \V C .

Finally, given the original graph G = ( V , E ), V and E can be decomposed as follows:
Proposition 1. Given a graph G = ( V , E ) , V = V C  X  X  D and E = E C  X  X  D  X  X  B .

Proof. This follows from the definitions of the bicon-nected core, bridges, and the detached graph. HepPh 9,945 (88.8%) 116,099 (98.7%) 1,123 21 (0.0019%) AstroPh 16,829 (94.0%) 195,835 (99.4%) 957 23 (0.0013%) CondMat 19,378 (90.7%) 89,128 (97.6%) 1,669 12 (0.00056%) DBLP 264,341 (83.4%) 991,125 (94.4%) 43,093 32 (0.00010%) Flickr 954,672 (47.9%) 20,390,649 (95.1%) 864,628 107 (0.000054%) Myspace 1,724,184 (82.7%) 45,096,696 (99.2%) 332,596 32 (0.000015%) LiveJournal 1,650,851 (93.9%) 42,071,541 (99.7%) 101,038 105 (0.000060%) Amazon 291,449 (87.0%) 862,836 (93.2%) 25,835 250 (0.00075%) Figure 1: Biconnected core, whiskers, and bridges  X  grey region indicates the biconnected core where vertices are densely connected to each other, and blue components indicate whiskers. Red edges in-dicate bridges which connect the biconnected core and each of the whiskers.

Figure 1 illustrates the biconnected core, whiskers, and bridges. The output of our filtering phase is the biconnected core graph where whiskers are filtered out. The filtering phase removes regions of the graph that are clearly parti-tionable from the remainder. More importantly, there is no overlap between any of the whiskers. This indicates that there is no need to apply overlapping community detection algorithm on the detached regions.

Table 2 shows the sizes of the biconnected core and the connectivity of the detached graph in our real-world net-works. Details of these networks are presented in Table 1. We compute the size of the biconnected core in terms of the number of vertices and edges. The number reported in the parenthesis shows how many vertices or edges are included in the biconnected core, i.e., the percentages of |V and |E C | / |E| , respectively. We also compute the number of connected components in the detached graph, and the size of the largest connected component (LCC in Table 2) in terms of the number of vertices. The number reported in the parenthesis indicates the relative size of the largest connected component compared to the number of vertices in the original graph.

We can see that the biconnected core contains the sub-stantial portion of the edges. In terms of the vertices, the biconnected core contains around 80 or 90 percentage of the vertices for all datasets except Flickr. In Flickr, the bi-connected core only contains around 50 percentage of the vertices while it contains 95 percentage of edges. This indi-cates that the biconnected core is dense while the detached graph is quite sparse. Recall that the biconnected core is one connected component. On the other hand, in the de-tached graph, there are many connected components, which implies that the vertices in the detached graph are likely to be disconnected with each other. Notice that each connected component in the detached graph corresponds to a whisker. So, the largest connected component can be interpreted as the largest whisker. Based on the statistics of the detached graph, we can see that whiskers tend to be separable from each other, and there are no significant size whiskers. Also, the size gap between the biconnected core and the largest whisker is significant.
Once we get the biconnected core graph, we find seeds in this filtered graph. The goal of an effective seeding strategy is to identify a diversity of vertices that lie within a cluster of good conductance. This identification should not be too expensive. In this situation, the Andersen-Chung-Lang the-orem about the personalized PageRank community finder [4] suggests it is a good method to grow the seeds (Section 4.3).
Graclus centers. One way to achieve these goals is to first apply a high quality and efficient graph partitioning scheme in order to compute a collection of sets with fairly small conductance. For each set (cluster), we find the most central vertex according to the kernel that corresponds to the normalized cut measure. The idea here is roughly that we want something that is close to the partitioning  X  which ought to be good  X  but that uses overlap to produce bet-ter boundaries between the partitions. See Algorithm 1 for the full procedure. In practice, we perform top-down hier-archical clustering using Graclus [9] to get a large number of clusters. Then, we take the center of each cluster as a seed  X  the center of a cluster is defined to be the vertex that is closest to the cluster centroid; see step 7 in Algorithm 1. If there are several vertices whose distances are tied for the center of a cluster, we include all of them.

Spread Hubs. From another viewpoint, the goal is to select a set of well-distributed seeds in the graph, such that they will have high coverage after we expand the sets. We greedily choose an independent set of k points in the graph by looking at vertices in order of decreasing degree. For this heuristic, we draw inspiration from the distance func-tion (4), which shows that the distance between a vertex and a cluster is inversely proportional to degree. Thus, high degree vertices are expected to have small distances to many other vertices. This also explains why we call the method spread hubs . It also follows from the results in Gleich and Se-shadhri [11], which state that there should be good clusters Algorithm 1 Seeding by Graclus Centers Input: graph G , the number of seeds k .
 Output: the seed set S . 1: Compute exhaustive and non-overlapping clusters C 2: Initialize S =  X  . 3: for each cluster C i do 4: for each vertex v  X  X  i do 5: Compute dist ( v, C i ) using (4). 6: end for 7: S = { argmin 8: end for around high degree vertices in power-law graphs with high clustering coefficients. We use an independent set in order to avoid picking seeds nearby each other. Our full procedure is shown in Algorithm 2. As the algorithm proceeds explor-ing hubs in the network, if there are several vertices whose degrees are the same, we take an independent set of those that are unmarked. This step may result in more than k seeds, however, the final number of returned seeds does not exceed the input k too much because there usually aren X  X  too many high degree vertices.
 Algorithm 2 Seeding by Spread Hubs Input: graph G = ( V , E ), the number of seeds k . Output: the seed set S . 1: Initialize S =  X  . 2: All vertices in V are unmarked. 3: while |S| &lt; k do 4: Let T be the set of unmarked vertices with max de-5: for each t  X  X  do 6: if t is unmarked then 7: S = { t } X  X  . 8: Mark t and its neighbors. 9: end if 10: end for 11: end while
Local Optimal Egonets. This strategy was presented in [11]. Let ego ( s ) denote the egonet of vertex s which is defined to be the union of s and its neighbors. [11] takes an egonet whose conductance is smaller than the conductance of any of its neighbors X  egonets, that is, they select a seed s such that for all v adjacent to s .

Random Seeds. Given the number of seeds k , we ran-domly take k seeds in the graph. Andersen and Lang gave some theoretical justification for why this method should be competitive [5].
Once we have a set of seed vertices, we wish to expand the clusters around those seeds. An effective technique for this task is a personalized PageRank vector, also known as a random-walk with restart (RWR) [21]. A personalized PageRank vector is the stationary distribution of a random walk that, with probability  X  follows a step of a random Algorithm 3 Find a low conductance set near a seed Input: graph G , seed set S , PageRank link-following prob-Output: low conductance set C . 1: Initialize x v ,r v = 0 for v  X  X  , set r v = 1 / |S| for all v  X  X  2: while Any r v &gt; deg ( v )  X  , set v to this vertex do 3: Update x v = x v + (1  X   X  ) r v . 4: For each ( v,u )  X  E , 5: Update r v =  X r v / 2 6: end while 7: Sort vertices by decreasing x v / deg ( v ) 8: For each prefix set of vertices in the sorted list, compute walk and with probability (1  X   X  ) jumps back to a seed node. If there are multiple seed nodes, then the choice is usually uniformly random. Thus, nodes close by the seed are more likely to be visited. Recently, such techniques were shown to produce communities that best match communities found in real-world networks [2]. In fact, personalized PageRank vectors have surprising relationships to graph cuts and clus-tering methods [4]. Andersen et al. show that a particular algorithm to compute a personalized PageRank vector, fol-lowed by a sweep over all cuts induced by the vector, will identify a set of good conductance within the graph. They proved this via a  X  X ocalized Cheeger inequality X  that states, informally, that the set identified via this procedure has a conductance that isn X  X  too far away from the best conduc-tance of any set containing that vertex. More recently, Ma-honey et al. [17] show that personalized PageRank is, effec-tively, a seed-biased eigenvector of the Laplacian. They also show a limit to relate the personalized PageRank vectors to the Fiedler vector of a graph.

We briefly summarize the procedure in Algorithm 3. Please see Andersen et al. [4] for a full description of the algorithm. This procedure is closely related to a coordinate descent optimization procedure [6] on the PageRank linear system. Although it may not be apparent from the procedure, this algorithm is remarkably efficient when combined with ap-propriate data structures. The algorithm keeps two vectors of values for each vertex, x and r . In a large graph, most of these values will remain zero on the vertices and hence, these need not be stored. Our implementation uses a hash table for the vectors x and r . Consequently, the sorting step is only over a small fraction of the total vertices. Typically, we find this method takes only a few milliseconds, even for a large graph.

In the personalized PageRank clustering scheme, there are two parameters:  X  and  X  . We follow standard prac-tice for PageRank clustering on an undirected graph and set  X  = 0 . 99 [16]. This value yields results that are similar to those without damping, yet have bounded computational time. The parameter  X  is an accuracy parameter. As  X   X  0, the final vector solution x tends to the exact solution of the PageRank linear system. When used for clustering, how-ever, this parameter controls the effective size of the final cluster. If  X  is large (about 10  X  2 ), then the output vector is inaccurate, incredibly sparse, and the resulting cluster is small. If  X  is small, say 10  X  8 , then the PageRank vector is accurate, nearly dense, and the resulting cluster may be large. We thus run the PageRank clustering scheme about 13 times, with a range of accuracy parameters that are em-pirically designed to produce clusters with between 1 and 50,000 times the number of edges in the initial seed set. The final community we select is the one with the best conduc-tance score from these 13 possibilities. The seed set for each run is the vertex neighborhood of a seed node. As reported in [11], we found that this yielded better performance and larger clusters.
Once we get the personalized PageRank communities on the biconnected core graph, we further expand each of the communities to the regions that we detached in the filtering phase. Our assignment procedure is straightforward: for each detached whisker connected via a bridge, we add that piece to all of the clusters that utilize the other vertex in the bridge. This procedure is described in Algorithm 4. In this way, each community C i is expanded.

We now show that this method only improves the final clustering result in terms of the normalized cut metric. To do this, we need to fix some notation. Let E B i of bridges which are attached to C i , and W C i be a set of whiskers which are attached to the bridges, i.e., W ( V w j = ( V j , E j )  X  W C i ; V W i = [ Finally, let C 0 i denote the expanded C i , where |C Equality holds in this expression when there is no bridge attached to C i . When we expand C i using Algorithm 4, C is equal to {C i S V W i } . The following results show that we only decrease the size of the (normalized) cut by adding the whiskers.
 Theorem 1. If a community C i is expanded to C 0 i using Algorithm 4, cut ( C 0 i ) = cut ( C i )  X  links ( V W i , C
Proof. Recall that cut ( C i ) is defined as follows: Let us first consider links ( C 0 i , V ) as follows: Notice that links ( V W i , V ) = links ( V W i , V W i )+ links ( V Thus, links ( C 0 i , V ) can be expressed as follows: Now, let us compute cut ( C 0 i ) as follows: cut ( C 0 i ) = links ( C 0 i , V )  X  links ( C 0 i , C 0 Notice that links ( C 0 i , C 0 i ) = links ( V W i , V W links ( V W i , C i ).
 Finally, cut ( C 0 i ) can be expressed as follows: Theorem 2. If a community C i is expanded to C 0 i using Algorithm 4, ncut ( C 0 i )  X  ncut ( C i ) .

Proof. Recall that On the other hand, by Theorem 1, we can represent ncut ( C as follows: Therefore, ncut ( C 0 i )  X  ncut ( C i ). The equality holds when there is no bridge attached to C i , i.e., E B i =  X  . Algorithm 4 Propagation Module Input: graph G = ( V , E ), biconnected core G C = ( V C , E Output: communities of G . 1: for each C i  X  X  do 2: Detect bridges E B i attached to C i . 3: for each b j  X  X  B i do 4: Detect the whisker w j = ( V j , E j ) which is attached 5: C i = C i  X  X  j . 6: end for 7: end for
We summarize the time complexity of our overall algo-rithm in Table 3. The filtering phase requires computing bi-connected components in a graph, which takes O ( |V| + |E| ) time. The complexity of  X  X raclus centers X  seeding strategy is determined by the complexity of hierarchical clustering using Graclus. Recall that  X  X pread hubs X  seeding strategy requires nodes to be sorted according to their degrees. Thus, the complexity of this strategy is bounded by the sorting operation. Let deg max denote the maximum degree in G C . The  X  X ocal optimal egonet X  seeding requires computing tri-angles, which takes O ( |E C | deg max ). Expanding each seed requires solving multiple personalized PageRank clustering problems. The complexity of this operation is complicated to state compactly [4], but it scales with the output size of the final cluster, links ( C i , V C ). Finally, our simple propa-gation procedure scans the regions that were not included in the biconnected core and attaches them to a cluster.
We compare our seed set expansion algorithm with two other state-of-the-art overlapping community detection meth-ods: Demon [8] and Bigclam [27]. For Demon, and Bigclam, we used the software which is provided by the authors of [8] and [27], respectively. All the experiments are performed on a computer with a Xeon X5440 2.83GHz CPU and 32GB memory. In Demon, we set = 0 . 3. In Bigclam, we use default parameter settings the software provides. Our seed set expansion algorithm is written in a mixture of C++ and MATLAB. Bigclam supports multi-threaded execution.

In the following experiments: the labels  X  X emon X  and  X  X ig-clam X  refer to the output from these methods. We refer to our method by the origin of the seeding strategy discussed in Section 4.2. Both  X  X raclus centers X  and  X  X pread hubs X  are the new methods we propose in this manuscript. The  X  X gonet X  method uses the seeding strategy from Gleich and Seshadhri [11], and  X  X andom X  refers to random seeds.
In the first experiment we conduct, we report on the out-put of each of the six methods on the eight networks which are presented in Table 1. Table 4 shows the returned num-ber of clusters and the graph coverage of each algorithm. The  X  X emon X  implementation fails on Flickr, Myspace and LiveJournal on a computer with 32GB memory. (In fact, we tried this algorithm on another machine with 256GB mem-ory and it also failed.) Using 20 threads,  X  X igclam X  does not finish on the Myspace network after running for one week.
The graph coverage indicates how many vertices are as-signed to clusters (i.e., the number of assigned vertices di-vided by the total number of vertices in a graph). Note that we can control the number of seeds k in  X  X raclus centers X ,  X  X pread hubs X , and  X  X andom X  seeds. We also can specify the number of clusters k in  X  X igclam X . We set k (in our meth-ods and  X  X igclam X ) as 100 for HepPh, 200 for AstroPh and CondMat, 15,000 for Flickr, Myspace, and LiveJournal, and 25,000 for DBLP and Amazon. Since we remove duplicate clusters after the PageRank expansion, the returned num-ber of clusters can be smaller than k . Also, since we choose all the tied seeds in  X  X raclus centers X  and  X  X pread hubs X , the returned number of clusters of these algorithms can be slightly larger than k . Recall that we use a top-down hier-archical clustering scheme in the  X  X raclus centers X  strategy. So, in this case, the returned number of clusters before filter-ing the duplicate clusters is slightly greater than or equal to 2 d log k e . On the other hand, the number of seeds in  X  X gonets X  is determined within the seeding algorithm. Demon also de-termines the number of clusters based on datasets. We can see that  X  X raclus centers X  and  X  X pread hubs X  methods cover larger portions of the graph than other methods across all the datasets.
We evaluate the quality of overlapping clustering in terms of the maximum conductance of any cluster. A high qual-ity algorithm should return a set of clusters that covers a large portion of the graph with small maximum conduc-tance. This objective function has been studied theoreti-cally in the context of overlapping clustering [13] and there exists an approximation algorithm, although it is expensive computationally.

Figure 2 shows the quality-vs-coverage for the six algo-rithms we study on the six networks where we do not have ground truth community information. For each method, we first sort the clusters according to conductance measure in ascending order, and then greedily take clusters until a cer-tain percentage of the graph is covered. The x -axis of each plot is the graph coverage, and the y -axis is the maximum conductance value among the clusters we take. We can in-terpret this plot as follows: we need to use the cluster whose conductance score is y to cover x percentage of the graph. Note that lower conductance indicates better quality of clus-ters. That is, the lower curve indicates better clusters. As can be seen in the plots, our algorithm with  X  X raclus centers X  seeding strategy outperforms the other methods. Also the simple  X  X pread hubs X  outperforms  X  X gonet X , random seeds, Demon, and Bigclam. The original motivation for our stud-ies in this paper was that egonet seeding did not produce high coverage. We have ground truth communities for the DBLP and Amazon networks, thus, for these networks, we compare against these communities instead of using the conductance measure. In DBLP, each publication venue (i.e., journal or conference) corresponds to an individual ground truth com-munity. In the Amazon network, each ground truth com-munity is defined to be a product category that Amazon provides. Given a set of algorithmic communities C and the ground truth communities S , we compute F 1 measure and F 2 measure to evaluate the relevance between the algo-rithmic communities and the ground truth communities. In general, F  X  measure is defined as follows: where  X  is a non-negative real value, and the precison and recall of S i  X  S are defined as follows: where C j  X  C , and F  X  ( S i ) = F  X  ( S i ,C j  X  ) where j F ( S i ,C j ). Then, the average F  X  measure is defined to be
Given an algorithmic community, precision indicates how many vertices are actually in the same ground truth commu-nity. Given a ground truth community, recall indicates how many vertices are predicted to be in the same community in a retrieved community. By definition, the precision and the recall are evenly weighted in F 1 measure. On the other hand, the F 2 measure puts more emphasis on recall than precision. The authors in [27] who provided the datasets argue that it is important to quantify the recall since the ground truth communities in these datasets are partially annotated, i.e., some vertices are not annotated to be a part of the ground truth community even though they actually belong to that community. This indicates that it would be reasonable to weight recall higher than precision, which is done by the F measure.

In Figure 3, we report the average F 1 and F 2 measures on DBLP and Amazon networks. On the DBLP network,  X  X pread hubs X  is the best, and  X  X raclus centers X  is the second best in terms of F 1 measure. With respect to F 2 measure,  X  X raclus centers X  is the best and  X  X pread hubs X  is the second best. On Amazon network,  X  X pread hubs X  is the best in terms of both F 1 and F 2 measures. We also notice that all seed set expansion algorithms outperform Demon and Bigclam  X  even when we use  X  X andom X  seeding strategy. We discuss this observation more comprehensively in our discussion section. Graph random egonet graclus ctr. spread hubs demon bigclam
HepPh coverage (%) 97.1 72.1 100 100 88.8 62.1
AstroPh coverage (%) 97.6 71.1 100 100 94.2 62.3
CondMat coverage (%) 92.4 99.5 100 100 91.2 79.5
DBLP coverage (%) 99.9 86.3 100 100 84.9 94.6
Amazon coverage (%) 99.9 100 100 100 79.2 99.2
Flickr coverage (%) 76.0 54.0 100 93.6 -52.1
LiveJournal coverage (%) 88.9 66.7 99.8 99.8 -43.9
Myspace coverage (%) 91.4 69.1 100 99.9 --indicates better communities.
 Figure 4: Runtime on Amazon and DBLP  X  The seed set expansion algorithm is faster than Demon and Bigclam.
Finally, we compare the algorithms by runtime. Figure 4 and Table 5 show the runtime of each algorithm. We run the single thread version of Bigclam for HepPh, AstroPh, CondMat, DBLP, and Amazon networks, and use the multi-threaded version with 20 threads for Flickr, Myspace, and LiveJournal networks.

As can be seen in Figure 4, the seed set expansion meth-ods are much faster than Demon and Bigclam on DBLP and Amazon networks. On small networks (HepPh, AstroPh, CondMat), our algorithm with  X  X pread hubs X  is faster than Demon and Bigclam. On large networks (Flickr, LiveJour-nal, Myspace), our seed set expansion methods are much faster than Bigclam even though we compare a single-threaded implementation of our method with 20 threads for Bigclam.
We now discuss the results from our experimental investi-gations. First, we note that our seed set expansion method was the only method that worked on all of the problems. Also, our method is faster than both Bigclam and Demon. Our seed set expansion algorithm is also easy to parallelize because each seed can be expanded independently. This property indicates that the runtime of the seed set expan-sion method could be further reduced in a multi-threaded version. Also, we can use any other high quality partition-ing scheme instead of Graclus including those with parallel and distributed implementations [25]. Perhaps surprisingly, the major difference in cost between using Graclus centers for the seeds and the other seed choices does not result from the expense of running Graclus. Rather, it arises because the personalized PageRank expansion technique takes longer for the seeds chosen by Graclus and spread hubs. When the PageRank expansion method has a larger input set, it tends to take longer, and the input sets we provide for the spread hubs and Graclus seeding strategies are the neighborhood sets of high degree vertices.

Another finding that emerges from our results is that us-ing random seeds outperforms both Bigclam and Demon. We believe there are two reasons for this finding. First, ran-dom seeds are likely to be in some set of reasonable conduc-tance as also discussed by Andersen and Lang [5]. Second, and importantly, a recent study by Abrahao [2] showed that personalized PageRank clusters are topologically similar to real-world clusters [2]. Any method that uses this technique will find clusters that look real.

Finally, we wish to address the relationship between our results and some prior observations on overlapping commu-nities. The authors of Bigclam found that the dense regions of a graph reflect areas of overlap between overlapping com-munities. By using a conductance measure, we ought to find only these dense regions  X  however, our method pro-duces much larger communities that cover the entire graph. The reason for this difference is that we use the entire ver-tex neighborhood as the restart for the personalized PageR-ank expansion routine. We avoid seeding exclusively inside a dense region by using an entire vertex neighborhood as a seed, which grows the set beyond the dense region. Thus, the communities we find likely capture a combination of commu-nities given by the egonet of the original seed node. To ex-pand on this point, in experiments we omit due to space, we found that seeding solely on the node itself  X  rather than us-Graph random egonet spread hubs graclus ctr. bigclam demon HepPh 21s 25s 31s 4m 12m 46s AstroPh 10s 17s 41s 3m 30s 49m 1m CondMat 7s 35s 52s 1m 35s 8m 1m 14s Flickr 30m 40m 1h 40m 6h 49m 59h 35m -LiveJournal 34m 56m 2h 18m 4h 12m 50h -
Myspace 27m 2h 29m 7h 9m 17h 15m &gt; 7 days -ing the vertex neighborhood  X  resulted in significantly worse performance in terms of conductance-vs-coverage.

Overall, our seed set expansion strategies significantly out-performed both Demon and Bigclam, two state of the art overlapping community methods in runtime, conductance-vs-coverage, and ground-truth accuracy.
 This research was supported by NSF grants CCF-1117055 and CCF-0916309 to ID, and by NSF CAREER award CCF-1149756 to DG. [1] Stanford Network Analysis Project. [2] B. Abrahao, S. Soundarajan, J. Hopcroft, and [3] Y.-Y. Ahn, J. P. Bagrow, and S. Lehmann. Link [4] R. Andersen, F. Chung, and K. Lang. Local graph [5] R. Andersen and K. J. Lang. Communities from seed [6] F. Bonchi, P. Esfandiar, D. F. Gleich, C. Greif, and [7] R. Burt. Structural Holes: The Social Structure of [8] M. Coscia, G. Rossetti, F. Giannotti, and [9] I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph [10] U. Gargi, W. Lu, V. Mirrokni, and S. Yoon.
 [11] D. F. Gleich and C. Seshadhri. Vertex neighborhoods, [12] G. Karypis and V. Kumar. Multilevel k-way [13] R. Khandekar, G. Kortsarz, and V. Mirrokni.
 [14] R. I. Kondor and J. D. Lafferty. Diffusion kernels on [15] D. Lai, X. Wu, H. Lu, and C. Nardini. Learning [16] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. [17] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi. A [18] N. Mishra, R. Schreiber, I. Stanton, and R. E. Tarjan. [19] A. Mislove, H. S. Koppula, K. P. Gummadi, [20] G. Palla, I. Der  X enyi, I. Farkas, and T. Vicsek. [21] J.-Y. Pan, H.-J. Yang, C. Faloutsos, and P. Duygulu. [22] B. S. Rees and K. B. Gallagher. Overlapping [23] H. Shen, X. Cheng, K. Cai, and M.-B. Hu. Detect [24] H. H. Song, B. Savas, T. W. Cho, V. Dave, I. Dhillon, [25] J. J. Whang, X. Sui, and I. S. Dhillon. Scalable and [26] J. Xie, S. Kelley, and B. K. Szymanski. Overlapping [27] J. Yang and J. Leskovec. Overlapping community [28] S. Zhang, R.-S. Wang, and X.-S. Zhang. Identification
