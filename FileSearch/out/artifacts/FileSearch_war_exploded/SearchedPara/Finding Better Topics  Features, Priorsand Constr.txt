 Latent Dirichlet allocation (LDA) [1] is a widely-used probabilistic topic model-ing paradigm, which has found many important applications in natural language processing and computer vision areas. LDA represents documents as mixtures over latent topics, where each topic is a distribution over a fixed vocabulary. Using approximate inference techniques like variational Bayes (VB) [1], Gibbs sampling (GS) [2] or belief propagation (BP) [3], LDA automatically learns the topic-word and document-topic distributions from a large collection of docu-ments. In practice, LDA users usually encounter two problems. First, the com-mon and stop words tend to occupy all topics. For example, if we use LDA to extract topics from a machine learning corpus like NIPS, we find that the common words  X  X earning X  and  X  X odel X  dominate (having very high likelihood) almost all topic-word distributions. This phenomenon makes the interpretability of topics undesirable [4]. Second, there is relatively little guidance on how to im-prove the lower-dimensional topic features for a better retrieval, clustering and classification performance. Therefore, we explore LDA from three perspectives: continuous features, asymmetric Dirichlet priors and sparseness constraints to find better topics.
 LDA has long been used for discrete fea tures such as word tokens and counts. Continuous features or term weighting schemes have been rarely discussed such as term frequency-inverse document frequency (TF-IDF) [5] and LTC [6]. One major concern is that LDA cannot generate continuous observations in its prob-abilistic modeling process. So, in practice users have to manually remove stop words having little contribution to the meaning of the text [7]. But, removing common words requires contex tual knowledge of the entire corpus, which is often a big challenge to users without prior kn owledge. Recently, continuous features for LDA have gained intensive research interests. A simple term-frequency fea-ture scheme [8] has been used for tagged document within the framework of LDA. Point-wise mutual information (PMI) features [9] have been incorporated into the GS inference algorithm referre dtoaspmiGS.ThePMIfeaturegives common and stop words some lower weig hts. Then, pmiGS infers topic-word distributions from weighted word counts. The results show that the PMI fea-ture not only lowers the likelihood of common and stop words in the topic-word distribution, but also gains a no-trivial improvement in cross-language retrieval tasks. This line of research inspires us to consider continuous features for LDA to improve the topic interpretability.

Most LDA algorithms [2, 3, 7] consider fixed symmetric Dirichlet priors over document-topic and topic-word distributions for simplicity. Although it is possi-ble to automatically learn Dirichlet hyperparameters from training data accord-ing to the maxumum-likelihood criterion [10], the extensive empirical studies [11] confirm that the inferred symmetric priors do not significantly improve the topic modeling performance than the fixed ones. However, asymmetric Dirichlet priors over document-topic and symmetric Dirichlet priors over topic-word distribu-tions have substantial advantages on removing the common words and choosing the number of topics [12]. The asymmetric prior over document-topic distri-bution can guide common or stop words to be grouped into a few topics with higher likelihoods because these words oft en occupy the larger proportion of each document. So, asymmetric priors are also effective in finding better topics.
If we can control the sparseness of document-topic and topic-word distribu-tions, we can possibly control the quality and interpretability of lower-dimensional topic features. Sparse topic coding (STC) [13] can directly control the sparsity of the inferred representations by relaxing the normalization constraint, which can be integrated with any convex loss function. STC identifies sparse topic meanings of words and improves time efficiency and c lassification accuracy. Also, sparse cod-ing can be directly combined with LDA X  X  e xtensions [14] for computer vision ap-plications. In sparse coding, each document or word only has a few salient topical meanings or senses. Sparse distributions carry salient information for a better in-terpretability, so that the low-dimensional sparse topic features may be more dis-tinguishable. Therefore, we will consider adding sparse constrains [15] on LDA X  X  document-topic and topic-word distributions.

Although continuous features, asymmetric priors and sparseness constraints for LDA have been studied either by GS [2] or by VB [1] inference algorithms, we re-examine these three perspectives with in the novel BP inference framework [3], which is very competitive in both speed an d accuracy. As a result, we incoporate continuous features, asymmetric Dirichlet priors and sparseness constraints into BP algorithms to find better topics than traditional GS and VB algorithms. Besides, most of previous studies focus only on one of three aspects, and lack a comprehensive comparison in terms of g eneralization performance, document clustering/classification and topic interpretability. Here, we compare these three aspects on different data sets, and pro vide evidence on which one can produce high-quality topics. We begin by reviewing batch BP algorithms for learning collapsed LDA [3,16,17]. The probabilistic topic modeling task can be interpreted as a labeling problem, in which the objective is to assign a set of thematic topic labels, z W  X  D = { z k w,d } , to explain the observed elements in document-word matrix, x W  X  D = { x w,d } . The notations 1  X  w  X  W and 1  X  d  X  D are the word index in vocabulary and the document index in corpus. The notation 1  X  k  X  K is the topic index. The nonzero element x w,d = 0 denotes the number of word counts at the index { w,d } .Foreachwordtoken x w,d,i = { 0 , 1 } , 1  X  i  X  x w,d , there is a topic label z The collapsed LDA [18] has joint probability p ( x , z |  X v k , X u w ), where the Dirichlet hyperparameters {  X v k , X u w } , k v k =1 , w u w =1 , X , X &gt; 0. In prac-tice, we may use the fixed symmetric hyperparameters { v k =1 /K, u w =1 /W } and the concentrat ion parameters {  X ,  X  } are provided by users for simplicity [2]. To maximize the joint probability in terms of z , the BP algorithm [3] computes which can be normalized by local computation, i.e., K k =1  X  w,d ( k )=1.The approximate message update equation is where the sufficient statistics for LDA model are where  X  w and  X  d denote all word indices except w and all document indices except d . Obviously, the message update equation (1) depends on all other neigh-boring messages  X   X  ( w,d ) excluding the current message  X  w,d . Two multinomial parameters, the document-topic distribution  X  and the topic-word distribution  X  , can be calculated from sufficient statistics  X   X  d ( k )and  X   X  w ( k ) by normaliza-tion. Message passing process will iterate Eqs. (1), (2) and (3) until all messages converge to a local stationary point [3].

As mentioned in Section 1, LDA users often use the document-topic distribu-tion in (2) as the lower-dimensional features for document retrieval, clustering and classification. The word-topic distribution in (3) is used to find the hot words in each topic. Usually, users will inspect the hot words with higher likelihood in each topic to understand the topic X  X  semantic meaning. Observing (2) and (3), we find that these two distributions are determined by three factors: 1. The features or observations: the word counts x w,d . 2. The Dirichlet priors or hyperparameters: the base vectors { v k ,u w } and the 3. The message: the K -tuple vector  X  w,d ( k ) for the topic likelihood at index In this paper, we will regulate these three factors to find better topics including document-topic (2) and topic-word distributions (3). The major reason that the common and stop words occupy almost all topics is that LDA uses word counts as features. The bigger the word counts, the higher the influence to the topic distributions. In Eqs. (2) and (3), the normalized mes-sage  X  w,d ( k ) is multiplied by the nonzero word count x w,d .Thus, x w,d can be regarded as the weight of  X  w,d ( k ) in estimating document-topic and topic-word distributions. In this way, the topics may be dominated by those high-frequent common and stop words. We see that the bigger word count x w,d corresponds to the greater influence of the estimated distributions in (2) and (3). This phe-nomenon motivates us to use the continuous features such as TF-IDF or LTC to lower the weights of common and stop words during message passing.
As far as Dirichlet priors are concerned, if we use the symmetric priors { v k = 1 /K, u w =1 /W } , the common and stop words have equal likelihoods to be assigned to all topics in Eq. (1). However, if we use the asymmetric priors, words will have higher likelihood to be assigned to the topic with higher priors. In this way, most common and stop words may be assigned to a few topic groups with higher priors [12]. This phenomenon motivates us to incorporate the asymmetric Dirichlet prior learning into the message passing process (1), (2) and (3). The message  X  w,d ( k ) represents the topic likelihood for each word token x w,d,i . If the message is not sparse, the word token may have multiple topic meanings leading to unclear explanations. So, we encourage passing those sparse messages by adding a weight proportional to the sparseness of the message. This weighted message passing strategy can strengthen the sparseness of document-topic and topic-word distributions in (2) and (3). According to [13] and [14], the sparseness will make the lower-dimensional topic features more distinguishable for clustering or classification purposes. This motivates us to add sparseness constraints on messages during their passing process.

Fig. 1(A) shows the continuous features, asymmetric Dirichlet priors and sparseness constraints denoted by red colors in the generative graphical represen-tation of LDA. The asymmetric Dirichlet priors are divided into the connection parameters {  X ,  X  } and the base measure vectors { v , u } ,and m w,d is the sparse-ness constraints for the message  X  w,d ( k )  X  z k w,d .Notethatif x w,d = i x w,d,i becomes continuous observations like TF-IDF, the generative model in Fig. 1(A) cannot generate such observa tions. However, the factor graph representation of the collapsed LDA [3] shows that it is possible to describe the continuous fea-tures using the undirected factor graph, which does not need to encode the generative relations between variables. In this way, we may think that the factor graph is a close approximation to LDA [3]. Fig. 1(B) shows the factor graph representation and the message passing process based on continuous features, asymmetric Dirichlet priors and sparseness constraints. We see that the mes-sage  X  w,d ( k )  X  z k w,d can be inferred by its neighboring messages including {  X  d and  X  w , respectively. We group the variables ( x w,d , z k w,d , m w,d ) together be-cause they work together to influence the neighboring messages according to (1). From the message passing over factor graphs, we can derive the similar message update equation to (1) that considers continuous features, asymmetric priors and sparseness constraints within the unified BP framework. 3.1 Continuous Features In linguistics, the high frequent stop words like  X  X he, and, of X  which occur in most of the documents do not contribute to the topic formation. To avoid stop words dominating every topic, we have to remove stop words before running LDA according to a corpus-specific stop word list. However, even if the stop words have been removed, there still are many common words such as  X  X odel, learning, data X  in the machine learning corpus. In such cases, we may use the continuous features such as TF-IDF [5] and LTC [6] that give the lower weights to the  X  X ommon word X  messages in (1). Let x w,d / w x w,d be the frequency of word w in document d ,and d x w,d be the total number of times that the word w occurs in all documents. We get the continuous TF-IDF feature as and the LTC feature as The difference between (5) and (4) is that (5) uses the logarithm of word fre-quency and is normalized by the geometric mean of the numerator. This nor-malization makes LTC features more distinguishable than TF-IDF features.
We simply replace the discrete word count feature x w,d by the continuous features x tfidf w,d and x ltc w,d in Eqs. (2), (3) and (1). Without loss of generality, we focus on LTC features for topic modeling. We refer to the message passing algorithms for LTC feature as ltcBP. Obviously in (2) and (3), the higher TF-IDF and LTC values will have the bigger influence to the topic formation. Generally, the stop and common words have lower TF-IDF and LTC weights, so that they will be automatically removed from hot word list in each topic during the message passing process. 3.2 Asymmetric Priors There are several approaches to learn Dirichlet priors from training data. Here, we choose to place Gamma priors on the hyperparameters  X   X  G [ C,S ], where C and S are shape and scale parameters of Gamma distribution. Generally, these parameters are fixed by users during learning Dirichlet priors. We adopt the improved method of Minka X  X  fixed point iteration [10,12]. However, this method is based on discrete counts on topic labels rather than messages in BP (1). To solve this problem, we sample the topic label z k w,d,i for each word token x w,d,i from the conditional probability  X  w,d ( k ). From the sampled [ z k w,d,i = 1], we get two topic count matrices Based on these two count matrices, we can directly use the Minka X  X  fixed point iteration where where b 1=max d  X  d ( k ) ,b 2=max d len ( d ), and len ( d ) is the total number of observations in document d ,and n and f arepositiveintegers.Thevalue  X v k acts as an initial set for the topic k in all documents. I n,k is the number of documents in which the topic k has been seen exactly n times. I n is the number of documents that contain a total of n observations. I n (  X  )= K k =1 I n,k is the total number of documents whose topics (1 ,...,K ) has been seen exactly n times. For the symmetric Dirichlet priors, the base measure is fixed as v k =1 /K and the concentration parameter  X  is updated as where b 3= max d,k  X  d ( k ). It is the same way to learn asymmetric or symmetric  X u w according to the count matrix  X  w ( k ).

Symmetric and asymmetric Dirichlet priors over {  X , X  } play different roles in topic modeling. Similar to [12], we implement an asymmetric prior over  X  and a symmetric prior over  X  , which is referred to as the asBP algorithm. In practice, this implementation performs the best than other combinations of priors [12]. Fig. 2 summaries the asBP algorithm for learning LDA, where T is the total number of learning iterations. The asymmetric prior  X v k can be learned by Eqs. (9), (10), (8). At the first t  X  100 iterations, asBP is the same with the batch BP which updates and normalizes all messages for all topics. For t&gt; 100, we learn the asymmetric prior  X v k and the symmetric prior  X u w every 20 iterations. 3.3 Sparseness Constraints In addition to the continuous features and asymmetric Dirichlet priors, sparse-ness constraints over messages also has an effect on the topic interpretability. In this paper, we adopt a sparseness measure based on the L 1 norm and the L 2 norm [15], where K is the number of topics and the dimensionality of  X  w,d ( k ). The quantity m w,d is the sparseness of  X  w,d . Usually, the messages of stop and common words have relatively lower sparseness beca use they often occupy many topics for a lower interpretability. For example, when the number of topics is 10 in CORA data set, the meaningful words such as  X  X einforcement X ,  X  X ayesian X  have rela-tively higher sparseness values 0 . 9999 and 0 . 9615 than 0 . 8663 and 0 . 8417 of the common words such as  X  X earning X  and  X  X odel X . Our intuition is that we need to encourage passing those messages with higher sparseness values, so we use the sparseness value (12) as the weight of message during message update (1). More specifically, we simply use the weighted sum m w,d x w,d  X  w,d ( k )inEqs.(2),(3) and (1). Such a weighted message passing strategy will encourage sparse mes-sages with higher weights in topic formation. We refer this message passing algorithm as conBP. If all sparseness constraints m w,d = 1, conBP will become the standard BP algorithm for learning LDA [3]. In this section, we evaluate the effectiv eness of the proposed ltcBP, asBP, and conBP algorithms on six publicly available data sets. Table 1 summarizes the statistics of six data sets, where D is the total number of documents, N d the average document length, N the total number of tokens, W the vocabulary size, and  X  X top X  indicates whether there are stop words. All algorithms are evaluated by five performance metrics. Lower perplex ity [3,11] indicates better generaliza-tion performance. The lower -dimensional document-topic distributions can be fed into standard SVM classifiers for document classification. The higher classifi-cation accuracy implies the more distinguishable ability of the lower-dimensional topic features. We can also use the document-topic distribution as the soft doc-ument clustering results. Normalized mu tual information (NMI) [19] evaluates the performance of clustering by compar ing predicted clusters with true class labels of a corpus. When displaying topics to users, each topic is generally rep-resented as a list of the most probable words (for example, top ten hot words in each topic). Topic  X  X oherence X  [20] eval uates the topic quality. Point-wise mu-tual information (PMI) [21] is very sim ilar to coherence. The higher coherence and PMI values correspond to the better topic interpretability.
 For a fair comparison, we implement all algorithms using the MATLAB C/C++ MEX platform publicly available at [22] and run experiments on the Sun fire X4270 M2 server. The initial hyperparameters is set as  X  =50 /K,  X  =0 . 01, where K is the number of topics. We use the same T = 1000 training itera-tions for all algorithms. We compare our algorithms with the four benchmark topic modeling algorithms such as BP [3], asGS [12], pmiGS [9] and STC [13]. Since STC outputs the word-topic distribution containing negative values, we only compare our algorithms with STC in terms of document clustering and classification tasks.

Fig. 3 shows the top ten words of four topics when K = 50. The meaningful key words of each topic are highlighted with the red color, and the stop and common words are highlighted with blue an d black colors, respectively. We use the subjective  X  X ord intrusion X  [4] to evaluate the topic interpretability, i.e., the number of conflict stop and common words in each topic. It is easy to see that ltcBP performs the best to remove al most all stop and common words in each topic, which demonstrates the effectiven ess of the continuous LTC features in topic modeling. Note that asBP can also remove the most stop words by cluster-ing them such as  X  X he of a and is in i for we X  in a separate topic on both NIPS (STOP) and 20NEWS (STOP). This result shows that the asymmetric prior has an effect on allocating the most frequen t stop words to a specific topic with a higher prior value v k . But asBP still has difficulty in handling some common words like  X  X earning X  and  X  X odel X . Note that asGS can also cluster stop words in one topic, but some topics contain more common words than those of asBP. BP performs the worst since its extracted topics are influenced by those high-frequent stop and common words. Although pmiGS uses the continuous PMI feature in topic modeling, it performs significantly worse than ltcBP because it cannot remove most stop and common words in each topic. The underlying reason is that LTC features are more effective in lowering the weights of stop and common words in topic modeling. We see that using sparseness constraints cannot effectively remove s top and common words from each topic. The conBP is only slightly better than BP, but significantly worse than both asBP and ltcBP. So, to find more interpretable topic-word distributions, the continuous features and asymmetric priors provide the best performance.
Fig. 4 shows the training perplexity as a function of the number of topics on CORA, WEK, NIPS and 20NEWS for K = { 50 , 75 , 100 } .NotethatltcBP, pmiGS and STC do not describe how to generate word tokens, so that they can-not be measured by the perplexity metr ic. Except on NIPS, asGS yields a lower perplexity value than BP. We see that conBP has almost the same perplexity of BP, which implies that sparseness constraints do not improve the likelihood of word generation. On all data sets, we see that the training perplexity of asBP is the lowest, showing the highest topic modeling accuracy. The result shows that learning asymmetric Dirichlet prior of  X v k and the symmetric prior  X u w can improve the topic modeling accuracy. The training perplexity has a smaller difference on the NIPS data set. One possible reason is that each document in NIPS contains more word tokens, so that the prior has a smaller impact on the message update (1). To summarize, learning an asymmetric Dirichlet prior over the document-topic distributions and an symmetric Dirichlet prior over the topic-word distributions still has substantial advantages on improving the document-topi and topic-word distributions to generate word tokens.

Fig. 5 shows the document classification accuracy as a function of the number of topics on CORA, WEK, NIPS and 20NEWS for K = { 50 , 75 , 100 } .Inour experiments, we randomly divide each data set into half as training and test sets. Then, we use the standard linear SVM classi fier to classify the lower-dimensional document-topic features produced by the topic modeling algorithms. As far as STC is concerned, it can directly output the class predictions. Also, we can use STC to generate lower-dimensional topic features and use SVM to do the classification.

We see that BP and asBP performs comparably, and outperform other meth-ods. Their classification performance is relatively stable as the number of topics changes. Although ltcBP can effectiv ely remove stop and common words, it does not perform the best in document classification. On possible reason is that the distributions of stop and common words also provide useful information for classification. Surprisingly, STC cannot predict the class label very well when compared with other methods. But STC works well on the lower-dimensional topic features. As we see, conBP works slightly better than BP on classification when K = 100, which implies that sparseness constraints do not provide useful information in this task. Overall, asBP performs the best in document clas-sification. For example, asBP outperforms BP and asGS by around 0 . 6% and 3 . 7% on CORA for K = 50, and by around 4 . 0% and 3 . 9% on 20NEWS data set for K = 100 in terms of classification accuracy. This result shows that the asymmetric priors play an important role in regulating document-topic features for classification. When the dimensionality of latent space is small, learning an asymmetric Dirichlet prior over the document-topic distributions and symmetric Dirichlet prior over the topic-word distributions is worse than heuristically set symmetric Dirichlet priors on NIPS. One reason is that the Dirichlet prior have more effects on shorter documents than longer documents.
Fig. 6 shows the document clustering results measured by NMI. This result confirms that STC and pmiGS often predict the wrong clusters of documents on all data sets. All BP-based algorithms perform equally well but conBP performs slightly better when K = 100. It is interesting to see that the performance of document clustering is not consistent with that of document classification in Fig. 5. One possible reason is the unknown number of clusters in the clustering task.

Fig. 7 shows the coherence on all data sets when K = 100. Because STC has no topic-word distributions, it cannot be measured by the coherence metric. The plot produces a separate box for K = 100 coherence values of each algorithm. On each box, the central mark is the median, the edges of the box are the 25th and 75th percentiles, the whiskers ext end to the most extreme data points not considered outliers, and outliers are plotted individually by the black dot sign. We see that asBP and conBP have higher coherence median values with smaller variances. BP also yields a stable coherence value. However, ltcBP and pmiGS have lower coherence values. The major r eason is that they remove most common words, which contribute much to the coherence metric.

Fig. 8 shows the PMI values of all algorithms when K = 100. Because STC has no topic-word distributions, it cannot be measured by the PMI metric. The plot produces a separate box for K = 100 PMI values of each algorithm. On each box, the central mark is the median, the edges of the box are the 25th and 75th percentiles, the whiskers extend to the most extreme data points not considered outliers, and outliers are plotted individually by the black dot sign. We see that most results are consistent with those of Fig. 7. For example, asBP, conBP and BP have relatively smaller variances and median values, while ltcBP and pmiGS have relatively bigger variances and median values. Both Fig. 7 and 8 confirm that asBP provide more coherent and related word groups. Note that asBP clusters stop and common words in a separate topic, which enhances coherence and PMI when compared with ltcBP.

Table 2 summarizes the overall performance of all algorithms on four data sets when K = 100. We mark the best performance by the bold face. We see that asBP wins 8 / 20 columns and all variants of BP win around 18 / 20 columns. This result confirms that BP and its variants find better document-topic and topic-word distributions. As far as perplexity is concerned, asBP is always the best method, which means that it is very likely to recover the observed words from the document-topic and topic-word distributions. We see that ltcBP and asBP learns better document-topic distributions for soft document clustering with relatively higher NMI values. Moreover, both ltcBP and asBP can ef-fectively remove stop and common wor ds as shown in Fig. 3. Although STC uses sparse coding for document classification, it performs relatively worse than conBP partly because conBP incorporates the sparseness constraints naturally. Note that conBP often prov ides a stable clustering and classification perfor-mances though it is not the best. On CORA and 20NEWS, conBP outperforms BP with a large margin, which reflects that sparseness constraints can improve clustering and classification perform ance. When compared with pmiGS, ltcBP wins all columns, confirming the effectiveness of LTC features for topic modeling as well as BP framework for learning LDA. Form Table 2, we suggest continuous features and asymmetric priors for topic modeling because sparseness constraints do not provide significant improvement. The underlying reason is that the esti-mated document-topic and topic-word distributions are already very sparse so that any sparseness constraints can give only marginal improvement. In this paper, we extensively explore three factors to find better topics: contin-uous features, asymmetric priors, and sparseness constraints within the unified BP framework. We develop several novel BP-based algorithms to study the three perspectives. Through extensive experim ents, we advocate asymmetric priors for topic modeling because they can enhance the overall performance in terms of several metrics. Also, the continuous features can improve the interpretability of topic-word distributions by effectiv ely remove almost all stop and common words. Finally, we find that sparseness constraints do not improve the topic mod-eling performance very much, partly beca use the sparse nature of document-topic and topic-word distributions of LDA.
 Acknowledgements. This work is supported by NSFC (Grant No. 61003154, 61373092, 61033013, 61272449 and 61202029), Natural Science Foundation of the Jiangsu Higher Education Institutions of China (Grant No. 12KJA520004), Innovative Research Team in Soochow University (Grant No. SDT2012B02), and Guangdong Province Key Laboratory Project(Grant No. SZU-GDPHPCL-2012-09).

