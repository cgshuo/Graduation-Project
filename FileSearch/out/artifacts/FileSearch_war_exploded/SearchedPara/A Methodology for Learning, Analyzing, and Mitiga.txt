 The seminal 2003 paper by Cosley, Lab, Albert, Konstan, and Reidl, demonstrated the susceptibility of recommender systems to rating biases. To facilitate browsing and selec-tion, almost all recommender systems display average rat-ings before accepting ratings from users which has been shown to bias ratings. This effect is called Social Influence Bias (SIB); the tendency to conform to the perceived  X  X orm X  in a community. We propose a methodology to 1) learn, 2) analyze, and 3) mitigate the effect of SIB in recommender systems. In the Learning phase, we build a baseline dataset by allowing users to rate twice: before and after seeing the average rating. In the Analysis phase, we apply a new non-parametric significance test based on the Wilcoxon statistic to test whether the data is consistent with SIB. If significant, we propose a Mitigation phase using polynomial regression and the Bayesian Information Criterion (BIC) to predict unbiased ratings. We evaluate our approach on a dataset of 9390 ratings from the California Report Card (CRC), a rating-based system designed to encourage political engage-ment. We found statistically significant evidence of SIB. Mitigating models were able to predict changed ratings with a normalized RMSE of 12.8% and reduce bias by 76.3%. The CRC, our data, and experimental code are available at: http://californiareportcard.org/data/
In the 1950 X  X , Solomon Asch performed a well-known se-ries of experiments [4,5,9] where subjects were asked to choose which of a set of lines matched the length of a ref-erence line. When working individually, 99% of the answers were correct. But when answering in the presence of a group of confederates who agreed on incorrect answers, 25% of participants conformed to the incorrect consensus. These results have been widely repeated to confirm what is now known as social influence bias : the tendency for participants to conform with the perceived community  X  X orm X  [15,27,36].
Susceptibility to influence has been studied in the con-text of recommender systems [12], and, in particular, Cosley et al. explored different rating scenarios and how system-Figure 1: Typical displays of aggregate prior rating values (the mean or median) in Amazon, Netflix, and the California Report Card that has the poten-tial to bias users. generated rating predictions may influence participant rat-ings. They found that in a variety of scenarios including pre-senting manipulated predictions, presenting predictions on already rated items, and changing the rating scale had sta-tistically significant influence on participants ratings. The key conclusion of Cosley et al. is that rating and recom-mender systems are easily biased and they argue that these biases can mask a user X  X  true perception about a rated item.
In almost all recommender systems, participants see the community  X  X orm X  in the form of aggregate statistics (the average or median rating values) before entering a rating of their own; potentially introducing social influence bias into the rating data. This interface paradigm is, of course, rea-sonable to facilitate browsing and selection in a large lists of items. For example, online retailers such as Amazon display the average rating value for products and Netflix displays the average rating value of movies (Figure 1). Display of average ratings values can also be used as an incentive [23] to reveal information about peers after a participant enters his or her own grade. Display of statistics also increases the perceived transparency of open democracy platforms that encourage political engagement [1,30,31]. Social influence bias can yield ratings that are closer to the average, less diverse, and less representative of participants X  true evalua-tions for items, which can in turn affect similarity measures between items and users and reduce the effectiveness of the recommendation system.

In this paper, we propose a methodology to learn, an-alyze, and mitigate the effects of social influence bias in recommender systems. As a case study, we evaluate our methodology on a new recommender system, the California Report Card (CRC). In the CRC, participants assign rat-ings (letter grades A+ to F, a 13 point scale) to the State of California on six political issues. Then, the CRC uses the ratings to place participants in a open-ended political dis-cussion with an initial set of comments from those who rated the state most similarly. Conformity ratings of the state can degrade the performance of the  X  X ecommendation X , a set of comments from like-minded participants.

The CRC has novel interface that allows us to learn the effects of Social Influence Bias. The CRC interface reveals median grade values to participants after they enter their own rating and then allows participants to revise their rat-ing. The key insight is that the combination of initial and revised ratings pairs allows us to determine if the social in-fluence bias is statistically significant, and if so, can be used to build an inference model that can predict the biasing ten-dency; thus mitigate the bias in a dataset of already biased ratings.
 Our methodology has three main components: Learn To initialize with baseline data, an initial  X  X earning X  phase asks an initial set of participants to rate a set of items twice: before seeing the median rating, and again after the median is revealed. This collects triplets of ratings for each participant (initial rating, median rating, and final rating). Analyze Given these triplets, we propose a new nonpara-metric significance test based on the Wilcoxon statistic to determine whether ratings that were changed are signifi-cantly closer to the median, i.e. the degree of social influence bias for each item.
 Mitigate Using the Bayesian Information Criterion (BIC), we learn a polynomial function of optimal degree that esti-mates the initial rating from the final rating and the median. This can be used in a post-learning phase (when medians are always visible), or on historical ratings, to estimate what a participant X  X  rating would be without social influence bias.
A key priority is a nonparametric approach to modeling social influence bias. Many earlier studies of social influence bias have focused on binary ratings (eg. up or down) [28,37]. However, recommender systems often have multi-valued rat-ing scales (eg. 5 stars). Discrete multi-valued rating scales often exhibit multimodality and are not the optimal settings for parametric significance tests (eg. t-test and  X  2 test). In fact, it is known that Wilcoxon Rank statistical significance tests have far higher statistical power in these settings [24], and are further robust to outliers and long tails. We use these results and properties to derive a new significance test for Social Influence Bias.

Not only is our testing framework nonparametric, but we also show that we can relax assumptions about the struc-ture of the social influence bias (eg. linear, conforming vs. contrarian). We use the Bayesian Information Criterion to jointly optimize over the model parameters and the complex-ity hyperparameter in polynomial regression. The result is a predictive model of social influence bias without having to make a strong assumption about the distribution of the data.

Results to date from the CRC suggest that given the op-portunity, many participants will revise their grades/ratings: 862 out of 9390 ratings were changed after participants saw the median value. We found statistically significant effects of social influence bias, with ratings on average 19.3% closer to the median value than ratings that were not changed. We also conducted an independent reference survey using SurveyMonkey to ask a random sample of 611 participants from the company X  X  paid pool of California participants to grade the same set of issues without displaying the median values. This data did not exhibit the same clustering around the median as the CRC, which comparably had ratings that were statistically significantly closer to the median (12.0%), suggesting that social influence bias is an important factor.
In their seminal 2003 work, Cosley, Lam, Albert, Kon-stan, and Reidl [12] studied the broad problem of biases in rating systems and tested the following relevant hypotheses: can manipulated  X  X redicted X  ratings influence a participant to change their rating, how consistent participants when re-rating an item, and how does rating scale (eg. stars, binary, unary) affect the average rating. The seminal result from Cosley et al. is that all of these hypotheses yielded signif-icant influencing tendencies. In this paper, we formulate a predictive model for a specific type of bias, social influence bias, which is learned and isolated through the unique inter-face of the CRC. We also apply a nonparametric significance testing methodology.

The Asch model for conformity is the theoretical basis for what is sometimes called social herding , the tendency to conform [6,33], and this is a well-known choice model in economics [11,16,22]. Such models have also been stud-ied in psychology and behavioral economics as  X  X ersuasion bias X  [14,15,20,21]. In 2011, Lorenz et al. described how these biases can undermine the effectiveness of crowd intel-ligence in estimation tasks [25]. They argue that movement towards the group consensus causes a diminished diversity of opinion potentially leading to inefficiencies and inaccurate collective estimates. Danescu-Niculescu-Mizil et al. analyze helpfulness ratings on Amazon product reviews [13]. They found that the helpfulness ratings did not just depend on the content of the review but also its aggregate score and its relationship to other scores. In order to better distin-guish social influence from other biases, Muchnik et. al de-signed a randomized experiment in which comments in an online forum were randomly up-treated or down-treated [28]. They concluded a statistically significant bias where a posi-tive treatment increased the likelihood of positive ratings by 32%. In both Danescu-Niculescu-Mizil et al. and Muchnik et al., they looked at the problem of social influence bias in an a priori setting, where users see the aggregate statistic be-fore giving their rating. Our work tests for a particular form of social influence where users are given the opportunity to change their opinions following the feedback.

Another line of relevant recommender systems research is the study of the consistency of repeat ratings [2,3]. It is an open problem, how to incorporate models of noisy rat-ings into our framework, however, as our non-parametric significance test is rank-based it statistically robust to small amounts of random noise. There has also been work on explaining recommendations [7,35], and one way to evalu-ate these explanation systems is to give users the option to change their ratings and evaluate how much (or how little) the explanation changes the users rating.

Zhu et al. conducted an experiment in which users eval-uate an image on a subjective question with binary scale (eg.  X  X s this image cute? X ), which was followed (either im-mediately or later) by a presentation of the crowd consensus opinion [37]. Users were given an opportunity to change their response, and they concluded that there was a sig-nificant tendency to change submissions. The tendency to change was the strongest when users were asked to make their second decision much later and not immediately after the first. Along these lines, Sipos et al. argue that context along with an aggregate rating plays a large role in the users X  ratings. That is, users may attempt to  X  X orrect X  the average, by voting in a more polarizing manner (more positively or negatively) [34]. We extend this prior work to measure and predict these changes when the input is more complex than a binary scale, and propose a non-parametric methodology that can be, in principle, extended to a variety of different input mechanisms. Our model can also account for a chang-ing aggregate statistic such as a median rating changing as more data is collected.
In this section, we describe the learning phase of our tech-nique where we collect the triplets (initial rating, final rat-ing, and observed median) for building our model. We will explain in detail the system design of the California Report Card, how we record changed ratings, and define the nota-tion that we will use in the following sections.
The California Report Card (CRC) 1 is a prototype cross-platform web/mobile application designed to allow partici-pants to advise California state leaders on timely policy is-sues. The CRC extends our earlier work with Opinion Space and Eigentaste [8,17 X 19,29]. In the CRC, participants assign letter grades (A+ to F) to the state of California on the following six issues: (1) Implementation of the Affordable Care Act ( X  X bamacare X ), (2) Quality of K-12 public educa-tion, (3) Affordability of state colleges and universities, (4) Access to state services for undocumented immigrants, (5) Laws and regulations regarding recreational marijuana, and (6) Marriage rights for same-sex partners. Grades (Ratings) are assigned on a thirteen point scale (A+,A,A-,...,D-,F). These issues are posed in a fixed order each with the same input scale. Participants submit ratings using a click-and-drag slider interface as illustrated in Figure 2. On mobile devices, participants touch and drag to indicate the desired rating.
 Figure 2: After entering their rating, the median rating over all participants is revealed. Participants have the option to change their rating after seeing the median.

Upon release of the slider, the CRC reveals the median for that issue over all prior participants. Even after the median is revealed the slider is still active and participants can change their ratings. However, it is important to note that participants were not explicitly told that they could change their rating. Another important observation is that participants who accessed the application at different times may have seen different medians as they were calculated based on the data up to that point. We recorded the initial rating, the median that the participant observed, and any
This study was approved by our Human Subjects commit-tee as per IRB Protocol 2014-01-5918. subsequent changes along with timestamps for each of the events. Rating all of the six issues was not mandatory and participant had the option to skip any of the issues. To analyze this data, we mapped these 13 grade values linearly onto a scale from 0 to 1, with 1 being an A+ and 0 being an F.
Let P denote the set of all participants. For each partic-ipant p j  X  P , we associate a 3-tuple of ratings ( g i [ j ], m [ j ], g [ j ]) which represent the initial rating, median observed by the participant, and the final rating. For each issue, we divided the participants into three subsets of P : ones who did not change their ratings P n , ones who changed P c , and ones who skipped the question P s . Our primary objective is to test the distributional properties of rating tuples from participants in P n compared to those in P c .

To ensure that all participants in the set P c had an op-portunity to see the median and then react, we filtered this group using the timestamps. The median appears in the interface with an animation whose completion time varied between devices, so we set a grace period of 3 seconds before we categorized the participant into set P c .

For consistency, we use the same notation to describe par-ticipants in the reference survey. We denote the set of ref-erence survey participants as set R , and each participant is the reference survey does not reveal the median g i [ j ] = g and m [ j ] is the hypothetical median of the prior participants (which is not shown).
In the analysis phase, we determine whether social in-fluence bias is statistically significant by analyzing spread of ratings around the median for the participants that changed their ratings. There are three principle challenges in test-ing this hypothesis. The first challenge is that parametric significance tests comparing two sample means such as the two sample t-test and z-test are known to perform poorly for multimodal and discrete distributions. Another significance test that is commonly applied to compare spreads of distri-butions is the F-test, which is also known to perform poorly for many non-normal distributions [26]. Furthermore, this test is usually used to test the spread of data around the mean, which only in very special conditions, such as normal distributions, aligns with the median which is the parameter of interest in the CRC. The discreteness of our data leads to multi-modal distributions which are not optimal for these testing methods.

The second challenge is that there is a natural tendency for ratings to concentrate around the median even with-out a bias. Consider the following participant behavioral model. Suppose that participants are not accustomed to a slider-based input. We can model the first rating that the participant leaves as uniformly randomly anywhere on the slider. As the participant begins to understand how to use the slider, their use becomes more accurate, ultimately set-tling on a rating from our observed distribution of final rat-ings. This model, the first rating is uniformly random and the second rating is a sample from the observed distribution, would result in a strong regression towards the median; even if there is no causal link with seeing the median.

Finally, the median m i changes as ratings arrive and thus can be different for each participant. The median rating is calculated over all prior participants and thus is depen-dent on when the participant submitted their first rating. In practice, the median will eventually converge for a large number of participants, but it would be incorrect to measure concentration around a final median.

To address these three challenges, we propose a nonpara-metric model based on the Wilcoxon statistic to test the hypothesis that the group of participants that changed their ratings are more tightly centered around the median value that those participants observed. Our tests compare abso-lute deviations around the median for P n , P c , and R ; which, as a relative comparison, controls for the natural tendency for ratings group around the median. Furthermore, it is more robust to the effects of alternate models such as the one described in our second challenge in comparison to a direct test of correlation (see Section 6.2.1).
Recall that P n is the set of participants that did not change their ratings and P c be the set of participants that changed their ratings. We define a set X c ,X n of absolute deviations from the observed median of the final rating for each group: For the purposes of hypothesis testing, we ignore the sign of the deviation. However, in Section 5, where we build a predictive model for the changes, we include the sign.
Now, for the set X c , we calculate the Wilcoxon rank-sum statistic. We assign a rank to each of the absolute deviations in the union set X = X c  X  X n (ie. the largest change has rank 1 and the smallest has rank | X c  X  X n | . For X c the ranks of the deviations within its set:
The Null Hypothesis is that absolute deviations in X are the same size as X n . Under this null hypothesis median ( X n ) = median ( X c ), the ranks will be evenly dis-tributed between each group. Therefore, the null expected value and variance of W is: For the significance level  X  , we can test the probability that our calculated W c comes from the null distribution. In other words, the test calculates the probability that a random subset of users (ignoring the categorization P n and P c ) can have the observed difference in rank-sum values. A signif-icant result means that for the participants that changed their ratings the changed changes are more tightly centered around the median they observed. For many distributions, the Wilcoxon statistic is more robust as it uses ranks rather than the actual values, making it more resilient to outliers. Even in the case where the data is normally distributed, the optimal condition for the t-test, the relative efficiency of the Wilcoxon rank-sum statistic compared to the typically used t-statistic is 3  X  = 95 . 4%. We trade off a small amount of efficiency in the normally distributed case, for increased efficiency and robustness in many non-normal distributions (eg. exponential 3  X  more efficient). Recommender system data is almost always collected from discrete inputs which are usually not normally distributed.

The same analysis can be used to test X c against the absolute deviations in the reference survey X r or for initial vs. final ratings in the change group X 0
In addition to testing social influence bias, we can also estimate by how much the absolute deviations differ. The Wilcoxon statistic can be inverted to estimate a most likely shift parameter  X , that is a shift  X  in the distribution of ab-solute deviations X c that maximally aligns them with X n In other words, X c +  X  is most supported by the null hy-pothesis (no social influence bias), or the distance from this hypothesis. An intuitive interpretation of  X  is that it mea-sures how much our deviations have to be increased so that the no social influence bias hypothesis is the most likely con-clusion. Since X c is a set of absolute deviations,  X  tells us how much more concentrated X c is than X n around the ob-served medians. This parameter is relevant to the design of recommendation algorithms use similarity (eg. clustering or nearest neighbors), as it characterizes how much more on average are participants closer to the median.

We refer to [24] on the derivation of  X  and its confidence interval:
In our learning phase, we collect rating triplets ( g i [ j ], m [ j ], g [ j ]), and in our analysis phase, we determine whether the triplets exhibit statistically significant social influence bias. In the mitigation phase, we propose two models: a correc-tion model (infers the initial rating given a final rating and the median), and a prediction model (predicts final ratings given an initial rating and the median). Once trained, the correction model can be applied to correct final grades col-lected without the triplet (either historical or post-learning). The prediction model can be used to analyze properties of the social influence bias eg. are ratings above the median affected the same way as ratings below the median.

Previous work, suggests that social influence is not a ho-mogeneous bias, namely, positive influences are different from negative influences. In Muchnik et al. [28], they found that when they positively treated posts with higher up-vote counts it lead to a significant increase in the likelihood of additional up votes (32% more likely). On the other hand, they argue negative treatments inspired correction behav-ior; where some participants wanted to correct what they felt was an incorrect score. They found that this also in-creased the likelihood of up-voting (88% more likely); as op-posed to the conforming response which would be increased down-votes.

These results suggest that the effects of viewing median ratings can be non-linear and are very context/question de-pendent. Similar to the previous section where we applied non-parametric tests that did not make a strong assump-tion about the distribution of the data, we propose a infor-mation theoretic polynomial function search that does not make strong assumptions about the nature of the relation-ship.
Recall that g f [ j ] is the final rating for participant j, and m [ j ]  X  g i [ j ] is the difference between the median and the initial rating. We want to find a polynomial function f such that: Let f  X  X  k be a polynomial of degree k . The square loss of f , is the error in predicting m [ j ]  X  g i [ j ] from f ( g For a given k , the best-fit polynomial minimizes this square-loss: For a given k , this problem can be solved with least squares. To search over the space of polynomial models, we apply a well-studied technique called the Bayesian Information Cri-terion (BIC) [10,32]. This technique converts the optimiza-tion problem into a penalized problem that jointly optimizes over the  X  X omplexity parameter X  k . This penalty can be interpreted as bias towards lower degree models, in other words, an Occam X  X  Razor prior belief. Cross-validation is an alternate method to empirically determine optimal model, and in practice, they give very similar results. BIC, however, is derived through maximum likelihood estimate and is not an empirical estimate so the learned model has a notion of optimality conditioned on the BIC prior belief.

Thus, we reformulate the optimization problem in the fol-lowing way to incorporate the BIC penalty: The resulting optimal polynomial will tell how to correct a final rating to infer the initial one. Let q: the predicted initial grade, and this value can be the input to our recommendation algorithm.
There are two ways in which we can apply the correction model to existing recommender systems data. First, we can train our correction on all triplets, including ones that did not change, to get a correction that we can then apply to all ratings in the post-learning phase. The second way is to estimate the probability that a rating is changed, and if that probability is above a threshold  X  (eg. 50%) we can apply the correction. With the second way, the correction model is only trained on those triplets where the initial rating is different from the final one. To estimate this probability, we can apply a logistic regression model to predict whether or not a rating has been changed from all other ratings. Let c ( i,j ) be 1 if participant j changed his or her rating for issue i and 0 if not. Our feature vector is the vector of all final ratings for that participant v [ j ] f = [ g 1 f [ j ] ,...,g we can apply this logistic regression model to estimate the probability that c ( i,j ) = 1, using the logistic function: We include results from both approaches in our experiments.
For the prediction model, we make the dependent variable m [ j ]  X  g i [ j ] and the independent variable g f [ j ]  X  g apply the polynomial regression with the BIC optimization as before, and find an optimal function f such that f is a function of the difference between the initial rating and the median, that predicts the change in rating. This model allows us to reason about the nature of the social influence x &gt; 0, we know that ratings above the median lead to a larger rating change. Additionally, f 0 ( x ) tells us how the change varies as the observed difference with the median increases.
The data for our case study was collected from the Cali-fornia Report Card between January 18th to April 20th. We also conducted an independent reference survey using Sur-veyMonkey X  X  paid random panel system between March 8th and March 14th. As mentioned, ratings of six political issues were collected on a 13-point letter grade scale (A+,A,...,F) and for analysis we mapped these ratings linearly onto a scale from 0 to 1, with an F as 0 and A+ as 1. Participants also had the option to  X  X kip X  issues (not assign a grade). There were 1575 participants from the CRC and 611 partic-ipants from SurveyMonkey. Rating activity is summarized below.

For any given political issue, between 10% and 20% of those who assigned ratings registered a rating change. In all, 556 out of the 1575 CRC participants changed their rating at least once (Figure 3). We also found that the aggregate results of the reference survey matched the CRC nearly per-fectly. On only two of the question (K12 and Immigration), we found a observed differences which were both less than a letter grade (+ or -).
 Figure 3: Among CRC participants, 65% changed none of their ratings, 22.0% changed one rating, 8.6% changed two, and 6.5% changed three or more.
 The lower figure omits those who didn X  X  change and indicates that majority of rating changes were to-wards the median.
In Section 4, we argued that using correlation as a test statistic can lead to erroneous conclusions of social influence bias, and proposed testing the absolute deviations around the median. We ran an experiment to illustrate the prob-lems of using correlation instead of absolute deviation. In this experiment, we iterated through the initial ratings each of participants in the change group P c . For each rating, we randomly sampled a final rating from group P n , the ones that did not change. In this model, since we sample final ratings from the no change group, we know that the social influence bias hypothesis is not true, since in distribution those who changed their ratings and those who didn X  X  are exactly the same. However, when we calculate the Pearson correlation coefficient between g f [ j ]  X  m [ j ] (the set of actual differences, not absolute deviations, between the final grade and the median) and g i [ j ]  X  m [ j ] (the set of actual differ-ences, not absolute deviations, between the initial grade and the median), we find statistically significant correlations.
There is a natural tendency for ratings to group around the median and the correlation coefficient does not account for this. However, if we measure the absolute deviation, we will find there is no statistically significant difference be-tween the absolute deviations since they are the same in distribution.
Using the non-parametric test proposed in Section 4, we tested the hypothesis of whether rating changes led to signif-icantly more concentration around the median. In our first experiment (Figure 4), we tested the absolute deviations of the CRC participants. We compared the group of partici-pants that did not change their ratings to the group that changed their ratings. We found that while there were no statistically significant differences between the initial ratings of the two groups, the final ratings of the group that changed were statistically significantly more concentrated than both their own initial ratings and the ratings of the no change group. On average, the ratings were 19 . 3% closer to the me-dian in the change group. The results of the hypothesis test for the set of participants who changed their ratings P c those who did not P n are (we denote initial grades from P as i and final as f ): Figure 4: For those participants that changed their ratings, final ratings were significantly more concen-trated around the median than their initial ratings. These results are consistent with social influence bias. When participants change their ratings, they are more likely to concentrate around the median. It is however an encour-aging and positive result that the two groups of participants P n and P c are very similar in terms of initial ratings, and the data suggests that a participant X  X  susceptibility to social influence is not correlated with initial ratings.
In our second experiment (Figure 5), we apply the same testing procedure to compare the ratings from the CRC to to those in the reference survey. We compare absolute devi-ations of the group of participants who changed their ratings in the CRC against participants from the reference survey. The final ratings were 12.0% closer to the median in the CRC change group than in the reference survey. We also found that there was no statistically significant difference between the reference survey and initial ratings. The results of the hypothesis test for the set of participants who changed their ratings P c and the reference group R are (we denote initial grades from P c as i and final as f ): Figure 5: We found that final ratings were signifi-cantly more concentrated in the CRC compared to ratings in the reference survey, however the refer-ence and the initial ratings did not differ signifi-cantly.

The results of our two experiments are consistent with social influence bias. We not only found that participants X  changed ratings were statistically significantly more likely to concentrate around the median, they were also more likely in comparison to the reference survey.
In Section 5, we discussed how we could use logistic re-gression to estimate the probability that a rating has been changed. We applied logistic regression, as described in that section, and inferred which ratings were changed. In Figure 7, as is typically used to evaluate binary classifiers, we show the ROC plot of the logistic regression predictor. The pre-diction results were quite accurate with average AUC score objective function showing how we picked an optimal degree of polynomial. Figure 7: The true positive rate (correct classifica-tions) as a function of the false positive rate. Sub-stantially better than random (dashed line) with an average AUC score of 0.8670. over all issues of 0.8670. At the .50 probability threshold (classified as changed if the estimated probability is greater than 0.5), we achieved an average precision of 84.7% and a recall of 70.0%.
In the first experiment, we train the polynomial/BIC cor-rection model proposed in Section 5, and evaluated it in terms of RMSE (Figure 8). We look at model only for those changed their grades, and measure how accurate is the model in predicting the grade changes. We held out a random 20% of rating triplets and calculated the inference error in the correction model. We found that on average over all issues the RMSE was 0 . 1286 which corresponds to a little bit more than a + or -grade.

In the second experiment, we simulated a true post-learning setting. We used the logistic regression model to predict the probability that the participant changed their grade. Then, if this probability was above a threshold, we used 70% which was determined empirically, we then ap-ply the polynomial correction model to infer the unbiased grade. Since the majority of participants did not change their grades, it would not be correct to simply measure RMSE error which would average predictions for those who changed and did not change. Thus, we invert the signifi-cance test proposed in Section 4, to calculate a parameter  X  which measures the distance from the null hypothesis. That is, how much would we have to shift the distribution of absolute deviations so the null hypothesis (of no social influence bias) is the most likely hypothesis. In Figure 8, we show a before and after for apply the correction model. We found that there was on average a 76.3% reduction in  X  for the entire pipeline predicting a change and then correcting for it.
 F igure 8: We found that we could predict changes in all of the issues with less than 2/3 of a letter grade RMSE error. In the lower figure, we applied this model to correct for the social influence bias and found that, on average, we could reduce the effects by 76.3%
We applied the prediction from Section 5 and the results are shown in Figure 6. Our model search and optimization through the BIC discovered that for four out of the six issues, K12, College, Immigration, and Marijuana, the model was linear. This suggests homogeneity in positive and negative social influence effects for these issues. What this implies is that on average participants who rated above the median and below the median moved towards the median with the same magnitude. However, for Obamacare and Marriage Rights, we found that the relationship was quadratic. In-terestingly enough, over the domain of changes, the learned quadratic function had steeper slope for ratings above the median. In other words, participants who initially rated the state higher than the median had a more significant ten-dency to change downwards, in comparison to the upward tendency of those who rated less than the median.
These results suggest that social influence bias can be sig-nificant in recommender systems and that this bias can be substantially reduced with machine learning. To apply this methodology to other recommender systems, a key question for future work is how is how to extend the approach to other recommender systems. We see an opportunity for this methodology in systems that combine their browsing and rating interfaces. For example, after selection, ie. users pur-chase a product, click on a movie, etc. the rating can be hidden. Once the user is ready to rate the item, which can be significant time after selection, we can reveal the average rating again after they have assigned a rating of their own. Then, we can apply our methodology to learn, analyze, and mitigate bias in the recommender systems.

An open question is how to extend this work to large item inventories and how much training data is required in such cases. One idea is to cluster/classify items into a small number of representative categories and train a model for each category. We believe that selecting an optimal set of items for training in this context may be posed as a submodular maximization problem. We are looking at applying this methodology to recommender systems in other domains (eg. movies) with alternative regression methods, such as Gaussian Process Regression and LOESS. We are also interested in performing more user studies where a false median is presented (as in the Asch experiments) and exploring methods to optimally classify participants as conformers and non-conformists. We would also like to study and quantify the role of social influence on textual data.

