 ORIGINAL PAPER Guillaume Lazzara  X  Thierry G X raud Abstract This work focuses on the most commonly used binarization method: Sauvola X  X . It performs relatively well on classical documents, however, three main defects remain: the window parameter of Sauvola X  X  formula does not fit automat-ically to the contents, it is not robust to low contrasts, and it is not invariant with respect to contrast inversion. Thus, on doc-uments such as magazines, the contents may not be retrieved correctly, which is crucial for indexing purpose. In this paper, we describe how to implement an efficient multiscale imple-mentation of Sauvola X  X  algorithm in order to guarantee good binarization for both small and large objects inside a sin-gle document without adjusting manually the window size to the contents. We also describe how to implement it in an efficient way, step by step. This algorithm remains notably fast compared to the original one. For fixed parameters, text recognition rates and binarization quality are equal or better than other methods on text with low and medium x-height and are significantly improved on text with large x-height. Pixel-based accuracy and OCR evaluations are performed on more than 120 documents. Compared to awarded methods in the latest binarization contests, Sauvola X  X  formula does not give the best results on historical documents. On the other hand, on clean magazines, it outperforms those methods. This implementation improves the robustness of Sauvola X  X  algo-rithm by making the results almost insensible to the window size whatever the object sizes. Its properties make it usable in full document analysis toolchains.
 Keywords Binarization  X  Multiscale  X  Document image analysis  X  Algorithm 1 Introduction 1.1 Overview Over the last decades, the need for document image analysis has increased significantly. One critical step of the analy-sis is to identify and retrieve foreground and background objects correctly. One way to do it is to produce a binary image; however, it is not easy to find the best thresholds because of change of illumination or noise presumed issues. As exposed in Sezgin and Sankur X  X  survey [ 1 ], many attempts have been made to find an efficient and relevant binarization method.

Some methods perform globally. Otsu X  X  algorithm [ 2 ]is known as one of the best in that category. It aims at finding an optimal threshold for the whole document by maximizing the separation between two pre-assumed classes. Despite fast computing times, it is not well adapted to uneven illumination and to the presence of random noise.

Other methods perform locally, trying to find the differ-ent satisfying thresholds for specific regions or around every pixels. A well-performing local thresholding method was proposed by Niblack [ 3 ]. The idea is to compute an equa-tion usually based on the mean and the standard deviation of a small neighborhood around each pixel. It works fine on clean documents but can give deceiving results in relatively degraded documents. Then, Sauvola and Pietikainen [ 4 ] proposed an improvement of Niblack X  X  method to improve binarization robustness on noisy documents or when show-through artifacts are present. Currently, this is one of the best binarization methods for classical documents according to several surveys [ 1 , 5 ]. While Niblack X  X  and Sauvola X  X  methods rely on the local variance, other local methods use different local features like the contrast in Bernsen X  X  method [ 6 ]. Some methods also try to mix global and local approaches, like Gabarra X  X  [ 7 ], so that object edges are detected and the image is split into regions thanks to a quadtree. Depending on whether the region con-tains an edge or not, a different threshold formula is used.

Local methods are more robust than global methods but often introduce parameters. Usually, methods are parameter sensitive, and the most difficult part is to find the best val-ues for the set of documents to be processed. Algorithms for automatic estimation of free parameter values have been proposed by Badekas and Papamarkos [ 5 ] and Rangoni et al. [ 8 ]. Unfortunately, even if these values fit many kinds of documents, they may not be generic enough, and some adaptations may be needed with respect to the type of docu-ments to process. That is the main reason why many attempts have been made to improve the best methods by automati-cally adjusting the parameters to the global [ 9 , 10 ] or local contents [ 11 , 12 ]. This also includes some works on getting multiscale versions of common algorithms like Otsu X  X  or Sauvola X  X  [ 10 ]. Eventually, improvements can be effectively observed in specific cases.

Over the last four years, more and more local methods try to rely not only on the pixel values in the threshold deci-sion but also on higher-level information. Lu et al. [ 13 ] model the document background via an iterative polyno-mial smoothing and then choose local thresholds based on detected text stroke edges. Lelore and Bouchara [ 14 , 15 ]use coarse thresholding to partition pixels into three groups: ink, background, and unknown. Some models describe the ink and background clusters and guide decisions on the unknown pixels. Because they rely on the document contents, those methods are usually considered as parameters free. Further-more, the recent contests have proven their efficiency on historical documents: Lu X  X  method won dibco 2009 [ 16 ] and an improved version tied as a winner of hdibco 2010 [ 17 ], whereas Lelore X  X  method won dibco 2011 [ 18 ]. More recently, the winner of hdibco 2012, Howe, proposes a method [ 19 ] which optimizes a global energy function based on the Laplacian image. It uses both a Laplacian operator to assess the local likelihood of foreground and background labels and Canny edge detection to identify likely disconti-nuities. Finally, a graph cut implementation finds the min-imum energy solution of a function combining these con-cepts. Parameters of the method are also adjusted dynam-ically w.r.t the contents using a stability criterion on the final result.

Because Sauvola X  X  binarization is widely used in practice and gives good results on magazines, this paper focuses on that particular method. 1.2 Sauvola X  X  algorithm and issues Sauvola X  X  method [ 4 ] takes a grayscale image as input. Since most of document images are color images, converting color to grayscale images is required [ 10 ]. For this purpose, we choose to use the classical luminance formula, based on the eye perception: Luma = 0 . 299  X  R + 0 . 587  X  G + 0 . 114  X  B .

From the grayscale image, Sauvola proposed to compute a threshold at each pixel using: T = m  X  1 + k  X  s
This formula relies on the assumption that text pixels have values close to black (respectively, background pixels have values close to white). In Eq. 1 , k is a user-defined parame-ter, m and s are, respectively, the mean and the local standard deviation computed in a window of size w centered on the current pixel, and R is the dynamic range of standard devia-tion ( R = 128 with 8-bit gray level images). The size of the window used to compute m and s remains user defined in the original paper.

Combined with optimizations like integral images [ 20 ], one of the main advantages of Sauvola X  X  method is its com-putational efficiency. It can run in &lt; 60 ms on A4 300-dpi documents with a modern computer. Another advantage is that it performs relatively well on noisy and blurred docu-ments [ 1 ].

Due to the binarization formula, the user must provide two parameters (w, k ) . Some techniques have been proposed to estimate them. Badekas and Papamarkos [ 5 ] state that w = 14 and k = 0 . 34 are the best compromise for show-through removal and object retrieval quality in classical documents. Rangoni et al. [ 8 ] based the parameter research on optical character recognition (OCR) result quality and found w = 60 and k = 0 . 4. Sezgin and Sankur [ 1 ] and Sauvola and Pietikainen [ 4 ]used w = 15 and k = 0 . 5. Adjusting those free parameters usually requires an apriori knowledge on the set of documents to get the best results. Therefore, there is no consensus in the research community regarding those parameter values.

Sauvola X  X  method suffers from different limitations among the following ones. 1.3 Missing low-contrast objects Low-contrasted objects may be considered as textured back-ground or show-through artifacts due to the threshold formula (Eq. 1 ) and may be removed or partially retrieved. Figure 1 illustrates this issue. The region of interest considered shows the values taken into account in a window of size w = 51 centered at the central point depicted in green: contrasts are very low. In that case, the corresponding histogram illustrates how sensitive Sauvola X  X  method is to k . Object pixels cannot be correctly retrieved if k is greater than 0.034. A low value of this parameter can help retrieving low-contrasted objects but since it is set for the whole document, it also alters other parts of the result: correctly contrasted objects are thicker in that case, possibly causing unintended connections between components. This is due to the fact that background noise and artifacts are usually poorly contrasted and are retrieved as objects. 1.4 Keeping textured text as is Textures are really sensitive to window size. Figure 2 a and d show binarization results of textured and non-textured text with the same font size. Even though the textured text is bold, inner parts of the characters are missing after binarization (see Fig. 2 b). In Fig. 2 e, the text is still well preserved and suitable for OCR . In Fig. 2 c, using a larger window may improve the binarization results on textured text. However, this solution cannot be applied if it is mixed with plain text since, as shown in Fig. 2 f, the retrieved text would be bolded. 1.5 Handling badly various object sizes In case of both small and large objects in a same document, Sauvola X  X  method will not be able to retrieve all objects cor-rectly. In most cases, one may want to retrieve text in doc-uments, so a small window may be used. Small text should be retrieved perfectly, however, larger text may not. Figure 2 h illustrates what happens when the selected window is too small compared to the objects of the document. We expect the algorithm to retrieve plain objects but in case of a too small window, statistics inside the objects may behave like in back-ground: pixels values are locally identical. Since Sauvola X  X  formula relies on the fact there is a minimum of contrast in the window to set a pixel as foreground, it is unable to make a proper choice. 1.6 Spatial object interference This issue mainly appears with image captions such as in Fig. 3 a. Too large windows may include data from objects of different nature. In Fig. 3 a, data from the image located above the caption are taken into account, leading to irrelevant statistics and invalid binarization. This is prob-ably one of the reasons why Sauvola and Pietikainen [ 4 ] choose to first identify text and non-text regions before binarization.
 Several attempts have been made in order to improve Sauvola X  X  binarization results and to prevent these defects. Wolf and Jolion [ 21 ] try to handle low-contrast and textured text defects. It consists in normalizing the contrast and the mean gray level of the image in order to maximize local con-trast. Text is slightly bold though. Bukhari et al. [ 12 ]tryto improve results by adjusting the parameter k depending on whether a pixel is part of a foreground or background object. They claim that Sauvola X  X  method is very sensible to k and can perform better if it is tuned, which is something we have also noticed. Farrahi Moghaddam and Cheriet [ 10 ]triedto improve the results in case of intensity and interfering degra-dation by implementing a multiscale version of Sauvola X  X  algorithm. First, the average stroke width and line height are evaluated. Then, in each step, the scale is reduced by a fac-tor of 2 and the parameters are adjusted: k is set from 0 to 0 . 01. The idea is to make the results from the lower scale grow while retrieving only text pixels at each step. Yet, this method only works well on uniform text size.

Kim [ 22 ] describes in details issues caused by too small or too large windows. He actually describes some of the limitations cited above and proposes an hybrid solution that takes advantage of two window sizes: a small one in order to get local fine details and a larger one to get the global trend. First, the input image is binarized with a moderate-size win-dow. Then, text lines are located and features are computed from the text: average character thickness and text height. For each text line, two windows are deduced from those features, and two thresholds T large and T small are computed thanks to Sauvola X  X  formula. Finally, the binarization of each text line is performed using: T ( x , y ) =  X  T large ( x , y ) + ( 1  X   X ) T small ( x , y ).
According to the author, this method gives better results than Sauvola X  X  binarization. However, it introduces a new parameter  X  which tends to make the fine tuning of the method more difficult, even if the authors claim that the method is not very sensitive to it. Moreover, the critical part of the algorithm remains the text line detection which assumes that the first binarization has retrieved all the text parts and that text components are correctly grouped. In the case of magazines with different kinds of non-text, we have observed that some text components can be grouped with non-text components which may lead to incorrect features and binarization.

In the remainder of this paper, we present an algorithm to overcome one of the four limitations of Sauvola X  X  bina-rization mentioned previously, e.g., handling various object sizes on a single run of the algorithm, without any prior knowledge on the location of text lines. It is actually penal-izing while processing magazines or commercials where text uses different font sizes: titles, subtitles, subscripts, etc. We also focus on the implementation and computa-tional speed which is also a critical aspect from our point of view.

In Sect. 2 , we first expose the general principle of the pro-posed multiscale algorithm. In Sect. 3 , we describe imple-mentation details and explain how to implement our method efficiently. In Section 4 , we present some results and compare them to other methods. We conclude on the achievements of this work and discuss future work in Sect. 7 . 2 Multiscale binarization Large text in documents like magazines is of prime impor-tance for indexing purpose since it usually contains the main topics of documents. Among the four presented defects, handling object of different sizes is thus a priority. The problem with binarizing objects of different sizes is caused by using a single window size, thus not well-suited to all objects. A local window is needed in order to fit appropri-ately the local document contents. Since we want to pre-serve performance, we need to avoid costly pre-processing algorithms which would require additional passes on data. Therefore, we want to include the window selection inside the binarization process, which is possible with a multiscale approach.

Multiscale strategies are common in image processing literature and are sometimes used in binarization. Farrahi Moghaddam and Cheriet [ 10 ] start by processing the full size image with very strict parameters. After each itera-tion, the input image is subsampled and the parameters are relaxed, so that more pixels are considered as fore-ground. Only those connecting to previously identified com-ponents are kept increasing the area of the components found in the previous iterations. Here, both data and parameters vary. Tabbone and Wendling propose a method [ 9 ] where the image size does not change. A parameter varies in a range of values and the best parameter value is selected by evaluating the result stability at each step. In Gabarra and Tabbone X  X  method [ 7 ], edges are detected then a quad-tree decomposition of the input image is computed. On each area, a local threshold is computed and applied to all the pixels of that area. It is multiscale since parts of an image can be processed at different levels in its quad-tree representation.

In our approach, we choose to run the same process at different scales using the same parameters; the input data is just subsampled. Eventually, the final result is deduced as a merge of the results obtained at different scales. In our approach, we make the assumption that objects are more likely to be well retrieved at one of the scales. The method is described in details in the following subsections. 2.1 Notation We will use the following notation:  X  uppercase letters refer to image names: S, I, ...  X  lowercase letters refer to scalar values: s, n, ...  X  subscript values refer to a scale number: I s is an image I  X  I s ( p ) corresponds to the value of the pixel p of the image 2.2 General description The main goal of the proposed method is to find the best scale for which the algorithm is able to decide correctly whether a pixel belongs to the foreground or to the background. As described in Sect. 2.4 , a relationship exists between the scale where an object is retrieved and the window size which should be used for capturing this object correctly. Our algo-rithm is composed of four main steps described below and illustrated in Fig. 4 : 2.3 Step 1: subsampling First, the input image, once turned into grayscale, I 1 is sub-sampled at three different scales thus producing three other images: I 2 , I 3 , and I 4 . The choice of the number of scales, here 4, is related to the fact that we work mainly on A4 docu-ments between 300 and 600 dpi. Thus, the maximum size of an object is constrained and, because of the object area range accepted (see Sect. 2.4 ), four scales are sufficient to retrieve correctly objects. However, on larger documents and/or with higher resolutions, it might be useful to have a few more scales.

The reduction factor between scales s and s + 1 is mostly set to 2. This value has been chosen because higher val-ues may lead to a loss of precision in the final results. This side effect mainly appears for high scales, where images contain less and less information. Using a reduction fac-tor of 3 for the first subsampling is usually fine, if the image has a minimum resolution of 300 dpi. This reduc-tion factor value may also be useful in the case of large documents in order to improve overall performance. At the end of this step, we have four grayscale images: I 1 , I 2 and I 4 . 2.4 Step 2: object selection at each scale Each input image I s is processed separately thanks to the same processing chain as depicted in Fig. 4 . The goal is to select objects within a specific size (area) range. dow of size w . As shown in Fig. 2 h, the size of the window influences the size and the shape of the retrieved objects. Here, the image I s is a subsampled version of the input and so are the objects. Therefore, working at a scale s with a win-dow of size w is equivalent to work at scale 1 with a window of size w 1 , s regarding the reduction factor q : w
When the scale increases, objects size decreases in the subsampled image, and objects are more likely to fit the win-dow to avoid the defects shown in Fig. 2 g.

AsshowninFig. 4 , the binarization at scale s produces two images: T s , a threshold image storing the point-wise thresh-olds used during the binarization; and B s , the resulting binary image at scale s . T s will be used later, during the final step. The binarization of I s includes connected components of var-ious sizes. Some of them need to be removed because they are too small or too large for giving good results with the current window size. We consider a minimum and a maximum size for acceptable objects. We chose the area, i.e., the number of pixels of a connected component, as size criterion. The differ-ent ranges of area are defined according to the current scale s , the reduction factor q , and the constant window size  X  at scale 1:  X  at scale s :  X  at the last scale s max :
Those area ranges correspond to commonly used body text, small titles, and large titles in 300 dpi magazines doc-uments. In order to disambiguate objects which are at the limit between two scales, object area ranges overlap when considered at scale 1.

A connected component labeling is performed on B s , and a selection of components having their area in the expected range is performed. The result is stored in the binary image S . At the end of this step, eight images are kept for the next steps: T 1 , T 2 , T 3 , and T 4 store the thresholds; S and S 4 store the selection of objects for their corresponding scale. 2.5 Step 3: result merging The main goal of this step (Fig. 4 ) is to prepare the final binarization by mapping each pixel from the input image to a threshold previously computed during Step 2.

Once an object is stored in S s , it means that it has been retrieved at scale s . One wants to merge this piece of infor-mation into a single scalar image E 1 . It consists in mark-ing in E 1 each object in S s using its corresponding scale s (see Fig. 5 ). Since S 1 , S 2 , S 3 , and S 4 are at different scale, objects extracted from S s images must be rescaled before being marked in E 1 .

Sometimes objects are retrieved several times at different scales: during subsampling, components may be connected and may have formed objects large enough to enter several area ranges. Overlapping area criteria may also be responsi-ble for such an issue sometimes. In that case, objects are con-sidered to have been retrieved at the highest scale. This way, we guarantee that the window, even if it is not the best one, will be large enough to avoid degraded objects as depicted in Fig. 2 h.

Once E 1 is computed, every pixel of binarized objects is mapped to a scale. Yet, non-object pixels do not belong to a scale at this step. Most of them are usually background pix-els but others can be pixels around objects, ignored because of the loss of precision due to subsampling. For that rea-son, they must be associated with a scale too in order to be processed like other pixels afterward. Omitting scale map-ping for that pixels and considering them as background information directly would lead to sharp object edges and artifacts.

An influence zone algorithm [ 23 , 24 ] is applied to E 1 propagate scale information and guaranty smooth results. It actually consists in a discrete Vorono X  tesselation where the seeds are the different connected components. The result is stored in E 1 _ IZ , and values of E 1 _ IZ are restricted to scale numbers. Here, the possible values in that image are 1, 2, 3, and 4. E 1 _ IZ maps pixels to scales; yet, to effectively binarize the input image, T 1 _ MS is needed to map scales data to effective thresholds for each pixel. From E _ IZ and the T 1 , T 2 , T 3 , T 4 images, produced during Step 2, T 1 _ MS is deduced: for each pixel p at scale 1, we know its scale s = E 1 _ IZ ( p ) , and we deduce the cor-responding point p at scale s , so that T s ( p ) is stored as T _ MS ( p ) . This image computation is illustrated in Fig. 6 . At the end of this step, T 1 _ MS provides a single threshold for each pixel.

Reusing the thresholds in T s is equivalent to computing the thresholds in I 1 with the window corresponding to scale s . That way, the window is defined pixel-wise which contrasts with the global approach of the original method. 2.6 Step 4: final binarization A point-wise binarization is performed with I 1 and T 1_ MS to get the final result stored in B 1 . 3 Optimization This algorithm is designed to be part of a whole docu-ment processing chain. Performance is thus a requirement. The multiscale approach implies extra computation: in addi-tion to a classical implementation of Sauvola X  X  algorithm, we introduce three new steps. Thanks to the multiscale approach, most of the computation is performed on subsam-pled (smaller) images which limits the impact of the addi-tional steps. Whatever the size of an image, iterations over all its pixels are time consuming, so the main goal of the following optimization is to reduce the number of iterations performed on images. Working at scale 1 is also expensive because of its full resolution. Therefore, step 2 is restricted to scale s &gt; = 2, and the original input image is only used to ini-tialize multiscale inputs and to perform the final binarization. 3.1 Step 1: setup of input data In order to prepare multiscale computations, successive antialiased subsamples are computed. Image at scale s is computed thanks to the image at scale s  X  1 by comput-ing for each pixel at scale s the average value of its neigh-boring pixels at scale s  X  1. Computing this way reduces the number of operations from 3  X  height 1  X  w idth 1 height 1  X  w idth 1  X  1 + 1 w idth 1 are, respectively, the height and the width of images at scale 1.

Subsampling is performed using integer ratios. Images not having dimensions divisible by these ratios are handled by adding special border data: data located on the inner border of the image are duplicated in an added outer image border. The size of this new border is adjusted to make the dimension of the so extended image a multiple of the subsampling ratio. Having images with an exact proportional size is required to find corresponding pixels between images at different scales.
An integral image is also computed in a single pass. The use of integral images in binarization algorithms was first introduced by Shafait et al. [ 20 ] allowing local thresholding methods, such as Sauvola X  X  method, to run in time close to global methods. The idea is to compute an image in which the intensity at a pixel position is equal to the sum of the intensities of all the pixels above and to the left of that position in the original image. Thanks to such an image, as shown in Fig. 8 , computing the mean m ( x , y ) boils down to: m ( x , y ) =
Our algorithm uses integral images to compute both the local means and variances which respectively need local sums and squared sums. For performance reasons, these sta-tistics are stored in a single image as a pair: single data block and directional reading enable data locality. A single inte-gral image is computed from I 1 , so that statistics are exact. It is stored in an image at scale 2 because it is needed only for scales s &gt; = 2 (as explained further) and its reduces the memory footprint. Since there exists a relationship regarding the pixel positions between images at different scales, it is possible to read directly in that image from any scale with the guarantee of computing exact statistics in constant time.
The integral image and the subsampled images are actu-ally computed from the same data and with the same process: iteration over the whole image and computation of sums of values. In our implementation, as shown in Fig. 7 , the first subsampled image, I 2 , and the integral image are computed at the same time. It saves one more iteration on the input image and many floating point operations. 3.2 Step 2: object selection at each scale Through this step, each subsampled image is binarized, objects are selected and finally marked in the scale image E . In the general approach, each subsampled image needs to be binarized and labeled before selecting objects thanks to an area criterion. These three algorithms follow a common pattern:
Thanks to this pattern, these steps can be combined into a single algorithm, described in Algorithm 1 , decreasing the number of iterations on the whole image from five down to two.

In Algorithm 1 , seven images are used: I s , the subsam-pled input image, T s , the image storing thresholds used for binarization, Parent s , storing the parent relationship for the Union-Find algorithm, Card s , storing the components X  area, Algorithm 1 Step 2 algorithm -Union-Find based algorithm combining Sauvola X  X  binarization and object selection. B , the filtered and binarized image, E 2 , the image at scale 2 where retrieved objects are marked with their corresponding scale, and Int 2 , the integral image at scale 2. At any time, for a specific pixel in I s , the corresponding data must be readable in all these images at different scales. This algorithm is com-posed of two passes. In the first pass, the input image at scale s is binarized and the component area is computed. In the second pass, components are filtered with the area criterion and marked in the scale image E 2 .
In the first pass, for each pixel, statistics are computed from the integral image Int 2 (line 14). Note that in com-pute_stats(), the window size effectively used to read Int actually of size w 2 , s (see Sect. 2.4 ). Then, Sauvola thresh-old is computed (line 15), and the pixel value is thresholded (line 16). If the thresholded value returns True, either a new component is created or the pixel is attached to an existing component in its neighborhood. In those both cases, compo-nent area is updated (lines 19 and 24).

In the second pass, all pixels belonging to a component with an area within the valid range are marked in E 2 (line 32). Note that those pixels are marked in E 2 with the current scale as value using mark_pixel_in_scale_image() but only if they have not been marked before (lines 34 and 39). Processing these selected pixels is straightforward and does not require any labeling.

At the end of this step, four images are available for the next steps: E 2 and T 2 , T 3 , and T 4 . Note that since this step is performed on scales s &gt; = 2, the scale image is only known at scale 2 (not at full scale), and there are only three T old images instead of 4. Therefore, avoiding computation at scale 1 reduces memory usage and saves some execution time. 3.3 Step 3: result merging Since E 2 is built iteratively during step 2, no merging is needed anymore here. Only the influence zone is performed on E 2 to produce E 2 _ IZ . 3.4 Step 4: final binarization During this step, the aim is to compute a binarization of the input image I 1 . Images T 2 , T 3 , T 4 , E 2 _ IZ , I 1 put image are browsed simultaneously. The process remains identical to the one depicted in Fig. 6 except that the threshold image T 1 _ MS is never created: the binarization is performed directly once the threshold is found.

To prevent many memory accesses because of the numer-ous images to read, we rely on the scale relationship and iterate over the pixels of all the images simultaneously. We rely on the Morton order [ 26 ] to iterate over pixels in square subparts of images. In Fig. 9 , reading a square of four pixels in the left image is equivalent to read a single pixel in the two other images. A new value is read in subsampled images only if all the pixels corresponding to the current one have been processed. Such an iteration in this final binarization reduces the total number of memory accesses in the images from 6  X  height 1  X  w idth 1 to less than 3  X  height 1  X  w where height 1 and w idth 1 are respectively the height and the width of images at scale 1.
 4 Experimental results 4.1 Analysis of the method During the development of this method, we have used the document dataset of the page segmentation competition from icdar 2009 [ 27 ]. It is composed of 63 A4 full page docu-ments at 300dpi. It contains a mix of magazines, scientific publications, and technical articles. We run our multiscale algorithm with several window values on that dataset and found that, w = 51 gives good results in all cases. This is the reason why we use that value in the following tests and evaluation.

Figure 10 a is a good candidate to illustrate the drawbacks of the original strategy of Sauvola X  X  algorithm. The orig-inal size is 1 , 500  X  1 , 500 pixels, and different sizes of objects are represented: the largest is more than 370 pix-els and the smallest around 10 pixels tall. Object thickness varies from 40 to 1 pixels. Running Sauvola X  X  algorithm leads to almost empty incomplete binarized objects (Fig. 10 b). Figure 10 c shows that the multiscale implementation takes object sizes into account. Here, objects are colored accord-ing to the scale they have been retrieved from: green for scale 2, blue for scale 3, and red for scale 4. It clearly shows the dispatch of the different object sizes into the different scales. As a consequence Large objects are clearly well retrieved (Fig. 10 d).

Figure 11 also shows some examples of the binariza-tion results performed with this method on real documents. One can also notice the limits of using the object area as criterion: in Fig. 11 a thick line separators are retrieved at scale 3 but they should be retrieved at scale 2 for best results.
 Table 1 shows computation times results obtained on an Intel Xeon W3520@2,67Ghz with 6 GB of RAM and pro-grams compiled with GCC 4.4 with -O3 optimization flag. While the classical Sauvola X  X  algorithm runs in 0.05s on A4 document images scanned at 300dpi, the multiscale imple-mentation runs in 0.15 s. Table 2 illustrates in details this difference on a larger image. As expected, multiscale fea-tures imply a cost on computation time: it is about 2.45 times slower than the classical implementation on large images mainly due to the multiscale processing. The computation is constant with respect to the input image size. 4.2 Adjustment of parameter k In the original binarization method proposed by Sauvola, the parameter k is set globally for the whole document. Adjusting this parameter can lead to better results on documents with low contrast or thin characters. In the multiscale approach, it is possible to set a different k value for each scale, e.g., for ranges of object sizes. We have compared such a variant with the classical (monoscale) approach where k is set to 0 . 34 globally. We will notate k s the value of parameter k at scale s . According to our experiment using k 2 = k 3, only small and medium objects are retrieved. They can be thin or not contrasted enough, so setting a low value of k 2 and k 3 allows us to be less strict in Sauvola X  X  formula, i.e., retrieving pixels with lower contrasts. At scale 4, large objects are retrieved and they are large enough not to need some additional precision. This new version, with one value of k per scale, namely Sauvola MS kx , is evaluated in the next section, along with other methods. 5 Evaluation All the material used in this section (datasets, ground truths, implementations, and benchmark tools) are freely available online from the resources page related to this article. 1
The required quality of the binarization highly depends on use cases. We have chosen to evaluate two aspects which are important in a whole document analysis toolchain: pixel-based accuracy and a posteriori OCR -based results.
The evaluation is performed with eight binarization algo-rithms. We propose two implementations of Sauvola Multi-scale: one with a fixed k value, Sauvola MS k , and another one where k is adjusted according to the scale, Sauvola MS kx We compare those two implementations to the classical (monoscale) algorithm from Sauvola and height other state-of-the-art methods: Wolf X  X  [ 21 ], Otsu X  X  [ 2 ], Niblack X  X  [ 3 ], Kim X  X  [ 22 ], tmms [ 28 ], Sauvola MsGb [ 10 ], Su 2001 [ 29 ], and Lelore 2011 [ 14 ]. tmms [ 28 ] is a morphological algorithm based on the mor-phological toggle mapping operator [ 30 ]. The morphological erosion and dilation are computed. Then, if the pixel value is closer to the erosion value than to the dilation value, it is marked as background, otherwise it is marked as fore-ground. If this method was initially dedicated to natural images, hdibco 2009 challenge [ 16 ] shows that this algo-rithm gives also good results on document images. It was ranked 2 nd out of 43.

Multiscale grid-based Sauvola (Sauvola MsGb), intro-duced by Farrahi Moghaddam and Cheriet [ 10 ], is a mul-tiscale adaptive binarization method based on Sauvola for-mula and a grid-based approach. This method was initially dedicated to binarize degraded historical documents while preserving weak connections and stroke. It was ranked 9th out of 15 at hdibco 2010 and 14th out of 24 at hdibco 2012.
Su 2011 [ 29 ] relies on image contrast defined by the local image maximum and minimum. Text is then segmented using local thresholds that are estimated from the detected high contrast pixels within a local neighborhood window. This method was initially dedicated to processing historical doc-uments. It was ranked 2nd out of 18 at dibco 2011.

Lelore and Bouchara X  X  method [ 14 , 15 ] is based on the one which won dibco 2011 but without upscaling the input images. In a first step, the method achieved a rough localiza-tion of the text using an edge-detection algorithm based on a modified version of the well-known Canny method. From the previous result, pixels in the immediate vicinity of edges are labeled as text or background thanks to a clustering algo-rithm while the remaining pixels are temporarily labeled as unknown. Finally, a post-processing step assigns a class to these  X  X nknown X  pixels. For all methods, we have chosen the best parameter values. The parameter k has been set according to the recommended values in the literature. Concerning the window size w ,we have run the algorithms on the document dataset of the page segmentation competition ( pscomp ) from icdar 2009 [ 27 ], and we have tuned the value to obtain results with good visual quality. tmms parameters have been set by its author based on the results on the pscomp dataset. Su 2011 is self-parameterized and so is Sauvola MsGb since we relied on its smart mode . All parameter values are summarized in Table 3 . For each method, the meaning of parameters is detailed in their respec-tive reference article. Note that for tmms , as the author did for dibco challenge, an hysteresis is used to set up the cmin parameter; that is, the reason why there are two values for it. It is important to note that parameters are fixed for the whole evaluation in order to highlight the robustness of the methods.

For most methods, we used their freely accessible imple-mentations. We implemented Kim X  X  technique. tmms , Sau-vola MsGb, and Su 2011 were provided as binaries by their respective authors. Binarization results for Lelore X  X  method have been directly computed by its author.

The evaluation is performed on two document types: his-torical documents and magazines. 5.1 Datasets
As shown in Table 4 , the most common reference datasets for document image analysis provide pieces of documents without the whole context data. A majority of them, dibco and hdibco  X  X  documents, are dedicated to historical docu-ments, which include also handwritten documents. It implies specific cares for reconstructing or preserving thin characters, and for separating background and foreground data which may require special processing before and/or after binariza-tion to obtain best results. Prima Layout Analysis Dataset (LAD) is composed of more than 300 full page documents from magazines and articles with a good quality but it only provides the layout ground truth.

Our method remains a raw binarization method, with-out any pre-or post-processing, and is designed to work best on magazines with less severe degradations than in dibco datasets.

For the next evaluation, we use dibco 2009 and 2011, and hdibco 2010 and 2012 datasets for historical documents, and our own datasets, lrde Document Binarization Dataset ( dbd ) for magazines. lrde dbd is composed of two subsets. Both are based on an original vector-based version of a French magazine. From this magazine, we have selected 125 A4 300-dpi pages with different text sizes.

One subset of images has been directly extracted from the digital magazine and rasterized. Those images are clean with no illumination nor noise issues. For every pages, we have removed the pictures in order to make the groundtruthing and the evaluation easier. Our selection includes pages with background colored boxes and low contrasts. This dataset is used both for the pixel-based accuracy and for the OCR -based evaluation. We will refer to it as the clean documents dataset.

The other subset is based on the same documents that have been first printed as two-sided pages then scanned at 300-dpi resolution. A rigid transform has been applied to each doc-ument, so that text lines match the ones of the correspond-ing clean documents . Therefore, this process has introduced some noise, show-through, and illumination variations. This subset is used for OCR -based evaluation only. We will refer to it as the scanned documents dataset.

For each page, text lines have been grouped into three categories w.r.t. their font size. Lines with a x-height less than 30 pixels are categorized as Small (S) and correspond to core paragraph text; lines with a x-height between 30 and 55 pixels are considered as Medium (M) and correspond to subtitles or small titles; and for higher x-height, lines are considered as Large (L) , e.g., titles. A set of lines composed of 123 large lines, 320 medium lines, and 9,551 small lines is available for both clean documents and scanned documents . For the OCR-based evaluation, we are thus able to measure the binarization quality for each text category independently.
The ground truth images have been obtained using a semi-automatic process. To that aim, we rely on a binarization using a global threshold. Sometimes, due to contrast or color issues, some objects were not correctly binarized or were missing in the output; therefore, we made some adjustments in the input image to preserve every objects.

In order to produce the OCR ground truth, we used Tesser-act 3.02 [ 34 ]onthe clean documents dataset. Errors and missed text were fixed, and text was grouped by lines to pro-duce a plain text OCR output reference.

Datasets, associated ground truths, implementations, and the whole set of tools used for this evaluation are freely avail-able online from the resources page related to this article. 5.2 Evaluation with historical documents We have tested our method on dibco / hdibco datasets from 2009 to 2012 [ 17 , 18 , 32 ] with the parameters given in Table 3 . The results are detailed in Table 5 . On historical doc-uments, the multiscale versions of Sauvola are roughly com-parable to the original Sauvola X  X  method. That was expected since the historical documents of the contest databases do not contain  X  X ultiscale text X ; text is only either of small or of medium size. Note that the original Sauvola method (or any of its variations) is a monolithic general-purpose bina-rization algorithm. It cannot compete with some elaborate binarization chains, including pre-and post-processings, and a fortiori dedicated for historical documents.

All output images are available on the Web page related to this article for extra analysis. 5.3 Evaluation with magazines 5.3.1 Pixel-based accuracy evaluation The clean documents dataset is used for pixel-based accuracy evaluation since the ground truth binarized documents are perfectly known.

Evaluation measure. According to common evaluation protocols [ 18 ], we used the F-measure (FM) in order to com-pare our method with other approaches: FM = where Recall = TP TP + FN and Precision = TP TP + FP , with TP , FP , and FN , respectively, standing for true-positive (total number of well-classified foreground pixels), false-positive (total number of misclassified foreground pixels in binarization results compared to ground truth), and false-negative (total number of misclassified background pixels). In tables, the F-measure is expressed in percentage. Results and Analysis. Table 6 gives the evaluation results.
A selection of three regions of document images is depicted in Figs. 12 , 13 and 14 to compare the results of the different methods. We can see that our approach increases the result quality of the classical binarization method pro-posed by Sauvola by five percentage points. This difference is mainly due to a now-adapted window size that can ade-quately process large objects, as we can see on Figs. 12 and 14 . Thanks to the multiscale approach, locally low-contrasted objects may be retrieved because they are considered on a larger area. This is the case in Fig. 13 where there is a large and low-contrasted object.
 Compared to Sauvola X  X  approach, Sauvola MS k and Sauvola MS kx are able to retrieve the right part of the object. Niblack X  X  method is able to find it but the too small win-dow prevents it from retrieving it completely. Wolf X  X  method performs relatively well but some objects are missing in the output. Otsu X  X  method performs better than any Sauvola X  X  approach. This is understandable because it is known to give good results on clean documents, which is the case here, and can retrieve large objects correctly. Its corresponding score results mainly from missing objects because of low contrasts (see Fig. 13 g). Niblack performs well in the text but does not handle color text boxes edges correctly. Transitions between color boxes and the background lead to some artifacts. Same issues arise with textured background. Kim encounters some trouble with text in colored text box which leads to large arti-facts, the box being considered as object instead of as back-ground. Sauvola MsGb performs as well as Sauvola and sur-prisingly has some difficulties to extract large text like titles and drop capitals. This is also the case for Su X  X  method: large text and large reverse video areas are missing or degraded (Figs. 12 l and 13 l). In addition, small text edges are not as smooth as they should be. Lelore X  X  method gives good over-all results, although it has some difficulties some times on large titles and small text. Its performances are really close to ours.

In Table 6 , the time needed to compute the final bina-rization results on 125 documents confirms the multiscale overheads as compared to the classical Sauvola X  X  implemen-tation. It shows also the large range of computation times, which is a crucial information to take into account while choosing an algorithm.

This evaluation also shows that adjusting the parameter k w.r.t. the scale, in our approach, may improve the qual-ity of the results. Sauvola MS kx gets a three points higher F-measure than Sauvola MS k . Those results highlight that, despite the most recent binarization methods perform very well on historical documents, they may not be able to prop-erly binarize simple and clean magazine document images. 5.3.2 OCR -based evaluation
Evaluation method. Once a document has been bina-rized, and character recognition is performed, line by line, by Tesseract (the options: -l fra -psm 7 specify the recognized language, French, and that the given image con-tains only one text line). The OCR has been run on the two datasets: clean and scanned documents , and the error is com-puted thanks to the Levenshtein distance.

Results and analysis. Table 7 shows the recognition rate for seven binarization algorithms w.r.t. text line quality and x-height.

Our Sauvola MS kx and Sauvola MS k implementations almost always outperformed the original Sauvola X  X  algorithm and some state-of-the-art algorithms. On clean documents, results are very close thanks to clean backgrounds, which dra-matically reduce the risk of erroneous retrievals. The small differences that are encountered here are due to the contrast between the text and the background colors: text is usually well retrieved but edges are not always as clean as those obtained with a multiscale Sauvola X  X  implementation (see Fig. 16 ). This fact is even more true for scanned documents where illumination variations and show-through artifacts are introduced. Sauvola MS k and Sauvola MS kx perform better than the classical Sauvola X  X  algorithm on large text, e.g., text with a high x-height. It is globally more robust, and the results are more stable on a wider set of objects. Moreover, they do not need fine parameter adjustment to deliver acceptable results with any object sizes.

Regarding tmms method, the results are globally equiva-lent to Sauvolas algorithm for clean documents. On scanned documents, the text is correctly binarized but cartouches and colored text boxes are considered as foreground (see Fig. 15 ). They surround text components, preventing the OCR from recognizing the text correctly. Since this is a common sit-uation in this dataset, the OCR error is thus extremely high compared to the other methods. To be usable in our context, this method would require post-processing. Su X  X  method does not scale to large objects thus giving really bad results for that category. Results quality for small and medium text are below average due to many broken and missing characters. This method seems to be really specific to the kind of noise it was designed for. Surprisingly, Otsu X  X  method performs relatively well on both clean and scanned documents despite a non-uniform illumination. Lelore X  X  method performs very well on clean documents, but, on scanned documents. Small characters are usually broken and large ones have holes.
Except for Otsu X  X  and Lelore X  X  methods, the main draw-back of state-of-the-art methods is their difficulty to correctly binarize large objects. 6 Reproducible research We advocate reproducible research [ 35 ], which means that research product shall also include all the materials needed to reproduce research results. To that aim, we provide the com-munity with many resources related to this present article, available from http://publis.lrde.epita.fr/201302-IJDAR .
First a demo is online, so that the user can upload an image and get the binarization result of our method; there-fore, third-party people can quickly reproduce our results or run some extra experiments on their own data. Second, we also provide an implementation of our method. It is part of the scribo module [ 36 ] of the Olena project [ 37 ], along with most of the algorithms discussed in this paper. The Olena project is an open source platform dedicated to image processing, freely available from our Web site. 3 It contains a general-purpose and generic C++ image processing library, described in [ 38 ]. Last the magazine document image data-base (including ground truths and the results of 10 binariza-tion methods, see Table 4 ) that we have set up is now usable by the community.

As a matter of comparison, among the 36 methods that entered the dibco 2011 and hdibco 2012 competitions, an implementation is available for only two of them and 31 of them cannot be reproduced due to more or less partial descrip-tions or missing parameter settings. 7 Conclusion and future work In this paper, we propose an approach that significantly improves the results of Sauvola X  X  binarization on documents with objects of various sizes like in magazines. Sauvola X  X  binarization is made almost insensitive to the window para-meter thanks to this implementation.
 Its accuracy is tested on 125,300-dpi A4 documents. Where on small and medium text sizes, this implementa-tion gets better or similar results than the classical imple-mentation, it dramatically improves the results for large text in magazines. This property is very important for document analysis because text using large font sizes usually corre-spond to titles and may be particularly relevant for indexing purpose. Furthermore, pixel-based accuracy and character recognition rates are also improved by our proposal, that is crucial for a whole document analysis, from the layout to the contents. Sauvola X  X  formula is probably not the best one to use for historical documents but at least our evaluation showed that it still competes with the latest awarded meth-ods regarding magazines and classical documents. We also proposed a fast implementation of our method, limiting the impact of the additional steps t o a 3 times slower method instead of a 7-times slowdown in a naive version.

The proposed implementation is part of the scribo [ 36 ] module from the Olena platform [ 38 ], an open-source plat-form for image processing written in C++, freely available on our Web site. The scribo module also contains the imple-mentation of some algorithms presented in this paper. An online demo of our method is available 4 where documents can be uploaded for testing purpose.

An issue remains though and may be considered for further investigations. The area criterion used to select at which scale an object should be retrieved is probably not precise enough to make a distinction between large thin objects and large thick objects.
 References
