 Current interests in skyline computation arise due to their relation to preference queries. Since it is guaraneed that a skyline point will not lose out in all dimensions when com-pared to any other point in the data set, this means that for each skyline point, there exists a set of weight assignments to the dimensions such that the point will become the top user preference.

We believe that the usefulness of skyline points is not lim-ited to such application and can be extended to data analysis and knowledge discovery as well. However, since the skyline of high dimensional datasets (which are common in data analysis applications) can contain too many points, various means must be developed to filter off the less interesting skyline points in high dimensions. In this paper, we will propose algorithms to find a set of interesting skyline points called strong skyline points . Extensive experiments show that our proposal is both effective and efficient. H.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications X  Data Mining Algorithms Skyline, High Dimensional Space
Given a n-dimensional space which has an order (or par-tial order) associated with each dimension, a point x is said to dominate a point y if x is ranked higher than y in all di-mensions. A skyline point from a n-dimensional dataset D refers to a point x which is not dominated by any other point from D . We call the set of such points in D , the skyline of D , whose calculation has been studied extensively [4, 6]. Current interests in skyline points arise due to their relation to preference queries [3, 5, 7, 8, 9, 10]. Since it is guaran-teed that a skyline point will not lose out in all dimensions when compared to any other point in D , this means that for each skyline point, there exists a set of weight assignments to the dimensions such that the point will become the top user preference [3].

While this analysis can be conducted on low dimensional datasets, doing this for high dimensional datasets (which are common in data analysis applications) remains a challenge. This is because the skyline of high dimensional datasets can contain too many points since it becomes increasingly diffi-cult for one point to dominate another as the dimensionality of the dataset increases.

To overcome this problem, we introduce some new con-cepts in this paper. Given a space S , a subspace is said to be a  X  -subspace if its skyline contains less than  X  points. The union of the skyline points in all  X  -subspace of S are called strong skyline points . Due to such constraint on the skyline point, the number of strong skyline points is much less than the original skyline points.

Based on the property of  X  -subspace which is very simi-lar to Apriori property of frequent itemset [2], we propose two subspace searching algorithms with breadth first strat-egy and depth first strategy respectively, to find those  X  -subspaces. We also improve the existing BNL algorithm [3] to efficiently determine whether a given subspace is  X  -subspace by exploiting the properties of strong skyline points.
The threshold  X  is the most important parameter in our methods. Setting it to too low a value will remove too many skyline points while setting it to too high will have the opposing effect. In this paper, we provide a guidance for setting  X  by extending the analysis from [4] to correlated subspaces and provide an upper bound on setting  X  .
It is not difficult to show that the subspaces of a  X  -subspace must also be  X  -subspaces, which is very similar to the Apri-ori property of frequent itemsets in association rule mining. This gives us the intuition to use the searching strategy of frequent itemset mining in strong skyline problem.
Breadth first search (BFS) [2] and depth first search (DFS) [1] are two basic subspace searching algorithms in frequent itemset mining. The former method finds all the subspaces with less dimensions before moving onto subspaces with more dimensions, while the latter method extends the sub-spaces according to the subspace lexicographical order.
To use BFS in our problem, we split all the subspaces into levels. Level i (1  X  i  X  n ), L i , consists of all the  X  -subspaces with i dimensions. The search process is conducted from low level to high level, since the necessary condition of a  X  -subspace is that all of its subspaces are  X  -subspaces. We can find the candidate subspaces on L k by joining every pair of subspaces on L k  X  1 if the union of their dimensions forms a new subspace on level k .

At the beginning, every dimension of S is checked and those dimensions with more than  X  points ordered on the top of the dimension are pruned. Then, to extend from level k to level k + 1, candidate subspaces are generated by joining pairs of subspaces in level k if two subspaces share the first k  X  1 dimensions, which is called Prefix Property .
To use DFS in our problem, we only need to find all the max  X  -subspaces, which is not subspace of any other  X  -subspace, instead of searching skylines in all  X  -subspaces. On the other hand, if a subspace  X  is the subset of a max  X  -subspace  X , we can assert  X  must be a  X  -subspace with-out necessity to find the exact skyline in  X . Intuitively, a depth first search strategy may save the time by reducing the number of skyline searches without missing any  X  -subspace.
Before the start of the depth first search algorithm, we first construct an empty stack St for  X  -subspaces and an empty list Lt of max  X  -subspaces. Then, we find all the one dimension  X  -subspaces like what is done in the BFS algorithm. All the 1-dimension  X  -subspaces are pushed into the stack St in the reverse order of the dimensions. Every round, we pop out one subspace  X  from St and try to extend to new subspaces by adding new dimension at the tail of the dimension list. If the dimensions of the candidate subspace are covered by some max subspace in Lt , it is directly pushed into the stack with the skylines in  X . If no max subspace can cover the candidate, the skyline search algorithm is invoked to test whether it is a  X  -subspace. If no extension from a subspace is successful and no max subspace can cover  X ,  X  is a max  X  -subspace and it is inserted into Lt .
To further speed up the algorithms, we also propose an im-proved BNL algorithm by exploiting the properties of strong skyline points. When we try to merge two  X  -subspaces  X  and  X  with prefix property, since all skyline points in  X  and  X  must be in the skyline of  X , we can first calculate the union of the skyline sets of  X  and  X , if the size of the union exceeds the threshold, we can discard  X  immediately. The complex-ity of this process is O (  X  ), which we can usually omit since  X  is generally much smaller than the data set size. We can also abandon the calculation of the skyline in a subspace immediately if we have already found  X  + 1 skyline points.
We assume that SP ( D, S ) is the number of skyline points in data set D in space S , where D is a data set with N points.

Theorem 1: If a new independent dimension d is added into a correlated subspace  X , Pr( | SP ( D,  X   X  X  d } ) |  X   X  )  X  ln N/ (ln N  X   X  ) 2
From Theorem 1, we can see that the fewer skyline points are in subspace  X   X  X  d } , the more likely d is independent of the correlated subspace  X .

Theorem 1 also tells us that with specified confidence re-quirement, a proper threshold  X  can be calculated. This turns out to be a useful tool to determine the parameter  X  according to the requirement in practice.
We use the NBA all-time players statistics data as our real data for test. The NBA all-time players statistics is a 17 dimension dataset with 16644 records. It contains all players X  season statistics since NBA X  X  first season in 1946. There are only 13 strong skyline points if  X  = 2, all of which are legend superstars in NBA X  X  history. This shows that our definitions do find some interesting points in the data set.
When the threshold  X  is 5, we find that Michael Jor-dan is a strong skyline point in the subspace with positive  X  X oints X , negative  X  X efensive Rebounds X  and positive  X  X o-tal Rebounds X  attributes. This result shows that Jordan belongs to the group of very few players who can get a lot of points and total rebounds but has very few defensive re-bounds, which implies that Jordan concentrated on attack-ing more than defending in the games.
 In the synthetic data set tests, BFS algorithm outperforms DFS algorithm in almost all the cases. Although DFS in-vokes much less times of the skyline searches on  X  -subspaces, it invokes the skyline searches on those false extension sub-spaces much more frequently. This phenomenon is due to the fact that DFS often underestimates the number of strong skyline points in most of the  X  -subspaces when they are cov-ered by a max  X  -subspace, which reduces the pruning ability of the union of skylines in subspaces.
In this paper, we introduce the strong skyline point to find those special points of a large data set in high dimensional spaces. Strong skyline points are skyline points in subspaces which have very few skyline points. Compared with conven-tional skyline definition, the new concept removes most of the less meaningful points. To efficiently find those strong skyline points and their corresponding correlated subspaces, we present bottom-up subspace search algorithm and im-prove the efficiency of skyline point calculation for a certain subspace. A good bound on the threshold for the subspaces in probability is also derived to help setting the parameter on the theoretical basis. We show that strong skyline point can not only find special points in a large data set but also facilitate some new data analysis methods and knowledge discovery. The experiments indicates that our current algo-rithms can work well on different types of data sets.
