 REGULAR PAPER Zhen He  X  X. Sean Wang  X  Byung Suk Lee  X  Alan C. H. Ling Abstract Recently, periodic pattern mining from time series data has been stud-periodic (PP) correlation in this paper, has not been investigated. An example of PP correlation is that power consumption is high either on Monday or Tues-day but not on both days. In general, a PP correlation is a set of offsets within a particular period such that the data at these offsets are correlated with a cer-tain user-desired strength. In the above example, the period is a week (7 days), and each day of the week is an offset of the period. PP correlations can provide insightful knowledge about the time series and can be used for predicting future values. This paper introduces an algorithm to mine time series for PP correlations based on the principal component analysis (PCA) method. Specifically, given a period, the algorithm maps the time series data to data points in a multidimen-sional space, where the dimensions correspond to the offsets within the period. A PP correlation is then equivalent to correlation of data when projected to a sub-set of the dimensions. The algorithm discovers, with one sequential scan of data, all those PP correlations (called minimum PP correlations) that are not unions of some other PP correlations. Experiments using both real and synthetic data sets show that the PCA-based algorithm is highly efficient and effective in finding the minimum PP correlations.
 Keywords Time series mining  X  Periodic patterns  X  Principal component analysis  X  Correlations 1 Introduction Finding periodicity in time series data is both a challenging and an important problem in many real world applications. Examples of time series that contain periodic patterns are numerous, including the time series of stock price, power consumption, sales data, meteorological data (e.g., temperature, humidity), etc. Many techniques have been developed for searching for periodic patterns in large time series data sets [1, 2, 8, 12, 13, 16, 18, 21 X 24].
 but not all, points in time within a certain period. This type of patterns appears frequently in real world applications. For example, within a period of 1 week Wednesday and low every Friday but not show any regularity on the other days. The usefulness of this type of patterns has been explained in the literature [ 1, 8, 12 , 13].
 ing type we call partial periodic (PP) correlations . Given a period (e.g., week) spanning a fixed number of offsets (e.g., 7 days), a PP correlation is a subset of these offsets such that the correlation of data at these offsets exceeds a user-given strength threshold. This type of pattern is what we study in this paper. practice. The first two examples assume the period of 1 week and the offset of one day.
 Example 1 (Stock market analysis) Suppose a PP correlation has been found in the ticker price time series of a particular stock and it shows that the price is either high on Monday and low on Friday or the converse is true. Then, a stock market analyst may investigate the underlying cause of the PP correlation and find out that a major share holder regularly sells a large quantity of shares on Friday (thus causing the price to fall) and buys them all back on Monday (causing the price to rise) or does the reverse. The analyst can then take advantage of the information to his or her own profit.
 Example 2 (Business logistics) Suppose a PP correlation has been found in the time series of the daily number of customers served at a certain restaurant and it shows that the sum of the numbers on Friday, Saturday and Sunday is roughly a constant. This information may be useful for the restaurant manager to budget the number of waiters accordingly for Saturday and Sunday after knowing the number of customers on Friday. After knowing the numbers on Friday and Saturday, it can be used for budgeting Sunday.
 collected only during weekdays.
 Example 3 (Intelligent web caching) Consider a time series of the hourly number of visits to a particular web page by the client (web browser) of the user named John. Suppose a PP correlation has been found in the time series and it shows that the number of visits is high during either 10 a.m. X 11 a.m. or 5 p.m. X 6 p.m., but not both, on every weekday. This information may be used by the web client to prefetch the page just before 5 p.m. if the number of visits has been low during 10 a.m. X 11 a.m.
 will be shown in Sect. 5.2.3 ). However, no techniques capable of finding such patterns have appeared in the literature. This paper is the first attempt to introduce such a technique.
 data points in a multidimensional space, where the dimensionality is the period size, and each dimension corresponds to one offset within the period. 1 A PP cor-relation is then defined as a subset of the dimensions such that the data at these offsets are correlated with a certain user-desired strength. For each PP correlation found, we can use a linear constraint to express the condition satisfied (within an error threshold) by the data appearing at the corresponding offsets within the period. Below we show two simple but useful cases.
 Example 4 We illustrate PP correlations by using two cases that involve two off-sets within a period: (1) PP negative correlation and (2) PP positive correlation. Consider stock price time series data mapped to data points in a seven-dimensional space d 1  X  d 2  X  X  X  X  X  d 7 ,where d 1 , d 2 ,..., d 7 are mapped from Monday, Tues-day, ..., S unday, respectively. An example of PP negative correlation is the one in Example 1, where either the stock price is high on Monday and low on Friday or the converse is true, within a period of 7 days. The linear constraint for the PP correlation can then be expressed as a 1 d 1 + b 1 d 5 + c 1 =0,where a 1 and b 1 are positive numbers. Note that d 1 corresponds to Monday and d 5 to Friday. Figure 1 illustrates this case using data points in ten periods. Consider only the data points mapped to the two-dimensional space d 1  X  d 5 (bottom of Fig. 1a) out of the time series given in the top of Fig. 1a. As shown in Fig. 1b, a PP negative correlation is found in the seemingly random time series. An example of PP positive corre-lation is that the stock price is either high on both Tuesday and Thursday or low on both days. The linear constraint for this PP correlation can be expressed as a d 2  X  b 2 d 4 + c 2 =0,where a 2 and b 2 are positive numbers. Figure 2 illustrates this case using the same time series.
 in the worst case is exponential to the size of the period. In most cases, the user would not want to find all possible PP correlations. We believe that only a small fraction of the existing PP correlations are of interest to the user. This belief is based on two observations. First, PP correlations with a smaller number of corre-lated dimensions are more useful than those with a larger number. For instance, a PP correlation between the two dimensions d 4 and d 5 is more useful than one among the five dimensions d 1 , d 2 , d 3 , d 4 ,and d 5 . Indeed, in order to predict the value for d 5 , the user needs to know only d 4 when using the former correlation but needs to know all of d 1 , d 2 , d 3 ,and d 4 when using the latter.
 sions may not be so informative X  X ence not so useful X  X s those with a smaller number especially if the larger one can be inferred from the smaller ones. For instance, assume we have the PP correlation between d 1 and d 2 (with the linear possible that the inferred PP correlation does not satisfy the user-desired strength even if the given two do.
 correlations in this paper. A PP correlation is said to be minimum if it is not the union of two or more other PP correlations. For example, a PP correlation with the dimensions { d 1 , d 2 , d 3 } is minimum if we do not have PP correlations among the contrary, the PP correlation is not minimum if we have PP correlations in the subsets { d 1 , d 2 } and { d 2 , d 3 } tions for a given period size, and second, doing it in only one pass of a (large) time series. We address the first issue by exploiting a property of the principal compo-nent analysis (PCA) . More specifically, PCA can be used to find a vector (called minimum variance PCA vector ) in a multidimensional space such that when the multidimensional points (the data) are projected onto the vector, the variance of the projections is minimized among all possible vectors in the space. Using this property, we can find the correlation with the minimum variance , in other words the correlation with the maximum strength 2 ; If the variance of this correlation is below the variance threshold (hence, above the correlation strength threshold), then a PP correlation is found with all the dimensions on which we have applied the PCA. The linear constraint for the found PP correlation is a hyperplane per-pendicular to the minimum variance PCA vector.
 scanning a large time series multiple times. A naive application of PCA to mine PP correlations would require re-scanning the data for each subset of dimensions considered to apply the PCA. We address this issue by optimizing the PCA compu-tation so that it takes only one pass through the time series to build a small amount of statistics (specifically, the covariance matrix) and use the statistics repeatedly to find all the minimum PP correlations without re-scanning the time series. results show that our one-pass approach leads to reduction of several orders of magnitude in the time to find all PP correlations, where the order increases with the increase of such parameters as the period size, data set size, and the maximum size of the PP correlations considered. We have also found that mining for minimum PP correlations produces significantly fewer PP correlations than mining for all PP correlations.
 of PP correlations in time series. Second, we develop a PCA-based approach to finding all minimum PP correlations in one-pass scan of the time series. Third, we demonstrate the efficiency and efficacy of the PCA-based approach through experiments with both real and synthetic data sets.
 discuss related work in Sect. 2, give an overview of the PCA and introduce key terms and concepts of PP correlations in Sect. 3, present our PCA-based approach to finding PP correlations in Sect. 4, and present the experiments in Sect. 5. Then, we present a simple extension of our PCA-based approach to consider multiple periods in Sect. 6, and conclude the paper with Sect. 7. 2 Related work Two areas of related work are periodic pattern mining [ 1, 2, 8, 12, 13, 16, 18, 21, 22 , 24] and forecasting [3, 4, 6, 9 X 11, 19]in time series.
 area of mining periodic patterns in time series [1, 2, 8, 12, 13, 16, 18, 21 X 24]. Han et al. [ 12, 13] first introduced the notion of partial periodic patterns and presented two algorithms (in [ 12]and[ 13], respectively) for mining this type of patterns. Yang et al. [ 24] proposed mining asynchronous periodic patterns, which are pat-terns that may be present only within a subsequence and whose occurrences may be shifted due to  X  X isturbance X . A number of papers [ 2, 8, 16, 18] center on dis-covering potential periods of time series data with high computational efficiency. Aref et al. [ 1] propose an incremental algorithm for mining partial periodic pat-terns. Yang et al. [ 22] mine  X  X urprising periodic patterns X  by using the concept of information gain to measure the overall degree of surprise within a data sequence. Wang et al. [ 21] propose an approach for mining patterns that are hierarchical in nature, where a higher level pattern consists of repeating lower level patterns. Jiong Yang et al. [ 23] propose efficient algorithms for mining high level patterns which would not be properly recognized by any existing method. sume discrete symbols in time series, not continuous numerical values as assumed in our work. Since symbols are not values that can be compared numerically, the methods of the existing works cannot find PP correlations as can be done with the technique of this paper.
 exponential smoothing [ 4], Holt X  X  linear trends model [ 11], Holt X  X inters seasonal model [ 6, 19], parametric regression [ 9, 10], Box X  X enkins [ 3], and linear mod-els [ 3]. We focus our discussion on linear models here since they also use lin-ear equations for characterizing the time series. There are many different linear models that are used for forecasting time series. They include autoregression [ 3], moving average [ 3], autoregression and moving average (ARMA) [ 3], integrated ARMA [ 3], and fractional integrated ARMA [ 3]. The basic ideas are illustrated by the ARMA(M,N) model, where the equation for value at time t , x t ,isgivenby where M is the order of the autoregressive process, N is the order of the moving model, and e t , e t  X  1 ,..., e t  X  N are the observable random shocks. In the ARMA and subsequent models, the linear models are designed solely for predicting the value at x t based on a prior window of observations. In contrast, our work mines potentially useful knowledge from the time series by finding correlations that in-volve partial data (i.e., subsets of the offsets within a period) from the time series. The linear modeling techniques attempt to fit a single model (e.g., Eq. ( 1)) for forecasting all future values, whereas we mine a set of PP correlations that can be used for forecasting some future values at predictable points in time. 3 Preliminaries In this section, we first give an overview of principal component analysis (PCA) techniques and then introduce important terms and concepts pertinent to PP cor-relations. 3.1 An overview of PCA  X  X rincipal component analysis has been called one of the most valuable results from linear algebra X  [ 20]. It is a simple non-parametric technique for extracting relevant information from a set of apparently random data. It is widely used for dimensionality reduction [ 5, 7, 15] and for finding correlations among attributes of data [ 17]. However, to our knowledge, PCA has never been used to look for PP correlations in time series data.
 nal k -dimensional eigenvectors v 1 , v 2 ,... v i ,..., v k (which we call PCA vectors ) such that  X  Each PCA vector is a unit vector, i.e.,  X   X  The variance along v formation from the coordinates of the data points to new coordinates such that the covariance matrix of the new coordinates is diagonalized.
 where V is a matrix whose rows are the PCA vectors: and X is a matrix whose columns are vectors of the coordinates of data points in the original k -dimensional space: where X ij ( i = 1 , 2 ,..., k ; j = 1 , 2 ,..., m )isthe i th dimensional coordinate of the j th data point,  X  X i is the average i th dimensional coordinate (in order to give data points zero mean), and Y is a matrix whose columns are vectors of the new coordinates of the data points in the transformed k -dimensional space: where Y ij ( i = 1 , 2 ,..., k ; j = 1 , 2 ,..., m ) is the new i th dimensional coordi-nate of the j th data point.
 Then, the covariance matrix of Y , COV ( Y ) , is computed as the eigenvectors of COV ( X ) ; the variance along each vector corresponds to its eigenvalue. These eigenvectors are the PCA vectors.
 minimum variance property.
 Property 1 (Minimum variance) The last PCA vector v k of the ranked set of PCA vectors V  X  v 1 , v 2 ,..., v k has the minimum variance along its direction among all the vectors in a k -dimensional space.
 Proof The proof of thisproperty is based on the fact that the orthonormal transfor-mation coming from the eigenvectors minimizes the trace of the matrix COV ( Y ) . For details, we refer the reader to Property A2 on page 11 of Jolliffe [ 17]. Example 5 Figure 3 shows an example of the PCA vectors v 1 (first) and v 2 (sec-ond) found for the data points in a two-dimensional space d 1  X  d 2 . Note that the data points have the higher variance along v 1 (i.e., 1 , 1 ) and the lower variance along v 2 (i.e.,  X  1 , 1 ). Hence, v 2 comes after v 1 in the PCA result. 3.2 Terms and concepts of PP correlations A time series S is a finite sequence of numerical values, a 1 , a 2 ,..., a n , denoted as a time series. Then, S can be divided into disjoint subsequences of equal length p . That is, S  X  S 0 , S 1 ,..., S i ,..., S n / p  X  1 where, for each i = 0 , 1 ,..., n / p  X  within the period p . We can map each S i to a data point in a p -dimensional space, where each dimension corresponds to one of the p offsets. In other words, each of the p dimension variables represents each of the p offsets. We use a set of dimension variables (or a dimensional set ), denoted as D p  X { d 1 , d 2 ,..., d p } ,to represent a p -dimensional space.
 Example 6 Figure 4 illustrates a time series ( S ) of length 9 divided into 3 sub-S 2 is mapped to a point in a three-dimensional space with the dimensional set D Definition 1 (PP correlation) Given a period p and a variance threshold V rmth ,a set D k  X { d 1 , d 2 ,..., d k } of offsets (or dimensions) within the period p ( k  X  p ) is a PP correlation if there exists a linear constraint of the form where  X  0 , X  1 ,..., X  k are constants satisfying the following three conditions. First,  X  computed as where m is the number of data points mapped from the time series of length n within each period p i.e., m = n / p ), X ij ( i = 1 , 2 ,..., k , j = 1 , 2 ,..., m ) is the i th dimensional coordinate value of the j th data point,  X  X i is the average value of the i th dimensional coordinate, and v ar v is the variance along the vector v  X   X   X  tions from being PP correlations. Geometrically, v ar v is a measure of how closely Eq. ( 9) fits the data points. Note that, when using Eq. ( 9) (for predicting future values), we do not need to subtract the mean (  X  X i ) from the i th coordinate value of a data point for each dimension variable d i ( i = 1 , 2 ,..., k ). It is compensated by subtracting the term  X  0 .
 Example 7 Figure 5 illustrates a PP correlation with the dimensional set D 2  X  { d The three points at the coordinates ( d 1 , d 2 )  X  (1,9), (2,7), and (4,1) are mapped from the first two offsets ( j = 1 , 2) of the subsequences S 1 , S 2 ,and S 3 (of pe-riod 3), respectively. The linear constraint for the PP correlation is expressed as  X  lar to the line given by the linear constraint, and v ar v corresponds to the variance of the three points projected to v .
 Definition 2 (Minimum PP correlation) A PP correlation is called minimum if it is not the union of two or more PP correlations. 4 Mining PP correlations using PCA The problem addressed in this paper can be stated as follows: given a period p and a variance threshold V th , find the set of all minimum PP correlations whose variances are smaller than V th in all dimensional sets of size up to p (or some lower value for computational efficiency). In this section, we provide and analyze an algorithm that addresses this problem. 4.1 Algorithm The algorithm find minimum, shown in Fig. 6,mines for the minimum PP corre-lations for a given period p . As mentioned in the introduction, we speed up the mining process by mining all the PP correlations in one pass. This is done by pre-computing the covariance matrix (see Eq. ( 6)) (and a  X  X oordinate mean vector X  to be described below) in a single pass and, then, extracting parts of it to find PP correlations without revisiting the time series data. In the remainder of this sub-section, we first describe the algorithm X  X n overview and the details X  X nd explain the correctness of the algorithm. 4.1.1 Overview Once the pre-computations are done, the algorithm first finds all minimum PP correlations of size one. Then, it looks for minimum PP correlations with an in-crementally larger dimensional set. For each dimensional set thus considered, the algorithm uses an appropriate part (to be detailed below) of the pre-computed covariance matrix and find the vector along which the data have the lowest vari-ance. This is done using the PCA technique explained in Sect. 3.1 . The key idea is to exploit the minimum variance property (Property 1) to find the PCA vector of minimum variance, v min . If the minimum variance is below the threshold V th , then we have found a PP correlation, with its linear constraint being the hyper-plane that uses v min as its normal vector. The output of the algorithm is a set of linear constraints of the PP correlations. As mentioned in the introduction, such linear constraints are useful in many applications (see Examples 1, 2,and 3). 4.1.2 Details The algorithm needs the time series data ( S ) to be mined, a known period ( p )and also two user-provided parameters: the variance threshold ( V th ) and the maximum PP correlation size ( Max dsize ) considered by the algorithm. It is straightforward to the user to determine an appropriate value of Max dsize , but it may not be so for V th . In this case, the user may run a small-scale test of the algorithm with the maximum possible value of V th and rank the PP correlations found from the one with the smallest variance first. This will give the user a clue to an appropriate value.
 a covariance matrix ( A )anda coordinate mean vector ( D )ofthe p -dimensional data points mapped from S . A is computed as in Eq. ( 6) ( with k equal to p ); this results in a p  X  p matrix: where A ij is the covariance between the i th and the j th dimensions. D is computed mean of the i th coordinate values of data points, in the p -dimensional space. These two data are used to find, for each dimensional set searched, the equation of the hyperplane with minimum variance.
 and D , the algorithm searches through all the minimum correlated dimensional size one and increases the size by one at each iteration (Line 3). This bottom-up strategy allows for pruning the search space by not searching PP correlations that are not minimum, that is, equal to the union of some smaller PP correlations (Line 4). At each iteration, the algorithm finds the minimum PP correlation in covariance matrix A a smaller covariance matrix ( C DSet ) for the dimensional set C DSet is given as where each element A d Eq. ( 13 ).
 computing the eigenvectors of C DSet , as mentioned in Sect. 3.1 (Line 6). Then, it takes the last PCA vector v k , and checks if the variance along its direction is below the threshold V th to determine whether a PP correlation exists in DSet (Line 7). As mentioned in Sect. 3.1 , the variance along v k is its eigenvalue. This is the same variance as v ar v defined in Eq. ( 12 ).
 of the hyperplane, E DSet , representing the PP correlation. The equation involves DSet , i.e., { d 1 , d 2 ,..., d k } , and the minimum variance PCA vector v k (denoted as  X  1 , X  2 ,..., X  k ) as its normal vector, and can be written as follows: where  X  i is the i th component of v k and  X  X i is the i th coordinate mean in the k -dimensional space d 1  X  d 2  X  ...  X  d k . Note that this equation is of the same form as Eq. ( 9). The first term is the same as Eq. ( 11 ). 4.1.3 Correctness The following proposition states that the algorithm find minimum is correct. Proposition 1 Given a dimensional set of size M ax si ze, the algorithm find minimum finds all possible minimum PP correlations in the dimensional set. tion is said to exist in a dimensional set if the variance along the last PCA vector ( v k )issmallerthan V th . Since this is the condition checked by the algorithm to find a PP correlation (in Line 7), the algorithm does find one if it exists in a given dimensional set. Moreover, the algorithm searches every minimum correlated di-mensional set of size one to Max size because of its bottom-up search strategy which starts with the dimensional sets of the smallest size (see Line 3). Hence, the algorithm finds all the minimum PP correlations.
 Example 8 Figures 7 and 8 show examples of how PCA can be used to find PP correlations. Suppose the given period is four and V th = 1 . 0. Figure 7ashowsthe time series S in three periods and the three four-dimensional data points (point 1, point 2, point 3) mapped from S ;Fig. 7b shows the pre-computed X , A ,and D . Figure 8a and b shows the intermediate covariance matrices and PP correlation equations computed when finding PP correlations in the dimensional sets { d 1 , d 2 } and { d 3 } , respectively. Specifically, Fig. 8a shows the minimum variance PCA vector v 2 and the extracted covariance matrix C DSet used to find the PP correlation 4.2 Analysis 4.2.1 Number of passes over time series As mentioned already, the algorithm find minimum needs to scan the time series data only once to find all the minimum PP correlations. This is due to a combina-tion of the following two properties. First, the algorithm computes D and A once and reuses it to find PP correlations in all candidate dimensional sets without re-visiting the data. Second, the computation of D and A is done incrementally as more data points are added. The first property has been explained in the algorithm description above. The reason for the second property is as follows. It is trivial to see that D can be computed incrementally. For A ,usingEq.( 6), each of its elements A ij is computed as The first summation multiplies two dimensions always at the same r th data point and, therefore, the data point can be discarded once used to update the summa-tion incrementally. The second summation can be computed incrementally since the average of the data values in the j th dimension (  X  X j ) can be computed incre-computed incrementally. Following this argument, the third summation can also be computed incrementally. Finally, the fourth summation can be computed incre-mentally since it equals m multiplied by the two averages  X  X i and  X  X j (which can both be computed incrementally). 4.2.2 Memory required The amount of memory required to find the PP correlations is the sum of the amounts of memory needed to store the A matrix (whose size is proportional to p 2 ), the coordinate mean vector D (whose size is proportional to p )andthe PP correlation set (resulting from the algorithm find minimum). Thus, the amount of memory required( MR ) can be given as where CM denotes the amount of memory for storing the PP correlation set. Note that MR is shown as a function of p to reflect the p 2 term (for storing A )andthe 2 p term (one p for storing D and another p for storing the sum of each dimension needed during the computation of A ). 4.2.3 Run time complexity The time complexities of pre-computing a covariance matrix A and a coordinate mean vector D (Line 1) are as follows. Computing the covariance matrix A in-volves the computation shown in Eq. ( 6); its complexity is O ( mp 2 ) where p is the period and m is the number of data points. Computing D involves only averaging the values of coordinates in each dimension; therefore, its complexity is O ( pm ) . Thus, the total pre-computation complexity is O ( mp 2 ) .
 min ( p , Max dsize ) since we may need to consider all subsets of up to size p or Max dsize , whichever is smaller. Note this complexity is independent of m (the number of data points). The average time complexity in practice, however, is not as bad, as we will show in our experimental results. 5 Evaluations We have evaluated the efficiency and efficacy of our PCA-based technique through three experiments. The first experiment regards the effects of the following param-eters on the execution time: the number of data points ( m ) (i.e., the number of peri-ods considered), the number of dimensions (i.e., the period p ), and the maximum size of a dimensional set ( Max dsize ). Naturally, we use synthetic time series for this experiment. The second experiment regards comparing the number of PP correlations found between mining minimum PP correlations and mining all PP correlations. We use both synthetic and real time series for this experiment. The third experiment regards the actual minimum PP correlations found in real time series. In this section, we first describe the experimental setup and then present the experimental results. 5.1 Experimental setup Variants of the PCA-based technique: To demonstrate different aspects of our technique, we have generated three variants of the algorithm find minimum .The first one is find minimum multipass , which is the same as find minimum (see Fig. 6) with the exception that, in Line 5 of the algorithm, the one-pass covari-ance computation is disabled so that each covariance matrix C DSet is computed directly from the time series data instead of being extracted from A . The second one is find all , which finds all PP correlations (i.e., including non-minimum PP correlations) using the same algorithm as find minimum except that, in Line 4 of the algorithm, all the dimensional sets of size dsize are searched. The third one is find all multipass , which is a combination of the first two variants.
 works as follows. It needs two input parameters: the period ( p ) and the number of data points ( m ). It first generates 2 p  X  2 potentially overlapping dimensional sets, consisting of two-dimensional sets of size one, two-dimensional sets of size two, and so on, up to two-dimensional sets of size p  X  1. Then, it generates a time series running for m periods while introducing one PP correlation using each of the 2 p  X  2 dimensional sets generated. The domain of the value of a time se-ries element is a real number in the range of 0 to 100. To add some variance to the data, we add to each element value a random number with the uniform distribution between 0 and 1.0% of the value.
 time series of hourly residential power consumption over a period of 1 year. It is the same time series as the one used in [ 8]. The second one is a time series of daily (weekdays only) closing price of IBM stock from January 1, 1980 to October 8, 1992 [ 14].
 puter with a single 1.8 GHz Intel Centrino CPU, 1 GB RAM, and 60 GB hard disk. 5.2 Experimental results 5.2.1 Experiment 1: execution time for varying parameter values Figure 9 shows the execution time of find minimum , fund minimum multipass , find all ,and find all multipass measured while varying the value of each of the three parameters ( m , p , Max dsize ) with the other two set to the medians of their respective ranges.
 ber of data points. (The curve for find all multipass does not show in the figure because the execution time exceeds the maximum value that can be shown in the figure (2000 s) for even the smallest number of data points used in the figure (1000 points).) The figure shows that one-pass algorithms (i.e., find minimum, find all) scale much better with the number of data points than the multipass counterparts (i.e., find minimum multipass, find all multipass). This is as expected, because repeated passes through the time series data become increasingly more costly due to an increase in the PCA computation cost per scan as the number of data points (i.e., time series periods scanned) increases. Since find minimum and find all take only one pass through the time series data, their relative performances are largely unaffected by the varying number of data points. riod. (The curves for find all multipass and find minimium multipass do not show the points at the periods 35 and 45 respectively, or larger because the execution times at those periods exceed the maximum value that can be shown in the fig-ure (2000 s).) The figure shows that one-pass algorithms are increasingly more efficient than multipass algorithms as the period increases. The reason is that the number of dimensional sets searched increases with the increasing period and, therefore, so does the overhead of scanning the time series for all the dimensional sets.
 ing maximum dimension-set size ( Max dsize ). The curves show the same find minimum when Max dsize is large. This happens when find minimum X  X  overhead of checking if a dimensional set equals the union of any PP correlations already found is higher than find all X  X  overhead of looking for all PP correlations (in one scan). 5.2.2 Experiment 2: the number of PP correlations found In this experiment, we use only the one-pass algorithms since the number of PP correlations found is independent of the number of passes. We use both synthetic and real data sets for this experiment. Figure 10 shows the results for varying the variance threshold V th while setting Max dsize to 5 for all the data sets and setting the period to 24 for the synthetic or power consumption data set and to 5 for the IBM stock price data set. The figure shows that the number of PP correlations found using find minimum are orders of magnitude smaller than that found using find all. This demonstrates how effective the pruning of find minimum is. a different trend from those for the synthetic (Fig. 10 a) and IBM stock price (Fig. 10 c) data sets. The difference is that the number of PP correlations found de-creases dramatically as V th decreases for the power consumption data set whereas the power consumption data have higher variance than data in the other data sets and, therefore, far fewer PP correlations can be found at a lower variance threshold( V th ).
 decreases with an increasing threshold value. This happens if, with a higher thresh-old value, more PP correlations are found in small dimensional sets but they are used to prune out even more PP correlations from larger dimensional sets. This phenomenon is more likely to happen with the IBM stock price data set because the number of possible PP correlations is far smaller (i.e., 26 (= 5 i = 2 5 C i ) instead of 16,777,191 (= 24 i = 2 24 C i )) given the shorter period (i.e., 5 instead of 24). 5.2.3 Experiment 3: PP correlations found in real data sets number of PP correlations found in real data sets and the example minimum PP correlations. used is 24 (hours), and the dimensions d 1 , d 2 ,..., d 24 correspond to the 24 h of the day, i.e., 12 a.m., 1 a.m., ..., 11 p.m. Interestingly, we have observed that PP correlations with low variances (with variances below 6) are found among the dimensions d 1 through d 8 . Since these dimensions correspond to early morning hours (12 a.m. X 7 a.m.), we infer that the correlation exists because there is not much human activity during these early, sleeping hours and, consequently, the power consumption is stable, limited to refrigerators, hot water furnaces, etc. that consume power with little fluctuation.
 is 5 (days), and the dimensions d 1 , d 2 ,..., d 5 correspond to the weekdays, i.e., Monday, Tuesday, ..., Friday. Alt hough we know that the data set contains cyclic correlations conforming to the definition of PP correlations, it is hard for us to identify the cause of the PP correlations found, due to our insufficient domain knowledge in IBM stock price data. Such a cause analysis is beyond the scope of this paper anyway. 6 Extension to multiple periods In this section, we discuss a simple extension of our technique, by which multiple periods in a given range are considered in one pass. Consider a range of periods p , p tor computed for a given p k ( l  X  k  X  h )as A ( p k ) and D ( p k ) , respectively. Then, since each A ( p k ) and D ( p k ) can be computed incrementally, we can compute all the pair of A ( p k ) and D ( p k ) can be passed to the algorithm find minimum to find the minimum PP correlations.
 passes required is still only one. Given the range of periods p l  X  p h , the amount of memory required, MRR ( p l , p h ) , is given by the following equation. where CMR denotes the amount of memory for storing the PP correlations found for all periods in the given range. Similarly to Eq. ( 18 ), the p 2 k term is for storing the pre-computed A ( p k ) and the 2 p k term is for storing the pre-computed D ( p k ) and the sum of each dimension needed during the computation of A ( p k ) . 7 Conclusions time series, which are shown in our experiments to appear frequently in real data. For mining the PP correlations, we developed a PCA-based technique. In order to avoid returning too many PP correlations, we focused on finding the minimum needs to have one pass through the time series data. Finally, we used experiments involving both real and synthetic data sets to demonstrate that our approach is computationally efficient and do find PP correlations existing in real time series. is to use our PCA-based technique to mine PP correlations in data streams .Thisis possible since the covariance matrix can be computed incrementally (as explained in Sect. 4.2 ). Our technique can then use the matrix to mine PP correlations on demand without revisiting the data. The second area is to further develop the ap-proach described in Sect. 6 to create a more efficient way of mining PP corre-lations for a range of periods. One approach may be to reuse some information (e.g., co-variance matrix) of one period to avoid or accelerate the computation of PP correlations for a different period.
 References Author Biographies
