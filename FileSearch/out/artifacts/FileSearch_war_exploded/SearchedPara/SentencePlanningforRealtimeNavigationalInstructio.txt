 Dialog agents have been developed for a variety of navigation domains such as in-car dri ving directions (Dale et al., 2003), tourist information portals (John-ston et al., 2002) and pedestrian navigation (Muller , 2002). In all these applications, the human partner recei ves navigation instructions from a system. For these domains, conte xtual features of the physical setting must be tak en into account for the agent to communicate successfully .

In dialog systems, one misunderstanding can of-ten lead to additional errors (Moratz and Tenbrink, 2003), so the system must strate gically choose in-structions and referring expressions that can be clearly understood by the user . Human cognition studies have found that the in front of/behind axis is easier to percei ve than other relations (Bryant et al., 1992). In navigation tasks, this suggests that de-scribing an object when it is in front of the follo wer is preferable to using other spatial relations. Studies on direction-gi ving language have found that speak-ers interlea ve repositioning commands (e.g.  X T urn right 90 degrees X ) designating objects of interest (e.g.  X See that chair? X ) and action commands (e.g.  X K eep going X )(Tv ersk y and Lee, 1999). The con-tent planner of a spok en dialog system must decide which of these dialog mo ves to produce at each turn.
A route plan is a link ed list of arcs between nodes representing locations and decision-points in the world. A direction-gi ving agent must perform sev-eral content-planning and surf ace realization steps, one of which is to decide how much of the route to describe to the user at once (Dale et al., 2003). Thus, the system selects the next tar get destination and must describe it to the user . In an interacti ve system, the generation agent must not only decide what to say to the user but also when to say it. Our task setup emplo ys a virtual-reality (VR) world in which one partner , the direction-follo wer (DF), mo ves about in the world to perform a series of tasks, such as pushing buttons to re-arrange ob-jects in the room, picking up items, etc. The part-ners communicated through headset microphones. The simulated world was presented from rst-person perspecti ve on a desk-top computer monitor . The DF has no kno wledge of the world map or tasks.
His partner , the direction-gi ver (DG), has a paper 2D map of the world and a list of tasks to complete. During the task, the DG has instant feedback about the DF' s location in the VR world, via mirroring of his partner' s screen on his own computer monitor . The DF can change his position or orientation within the virtual world independently of the DG' s direc-tions, but since the DG kno ws the task, their collab-oration is necessary . In this study , we are most inter -ested in the beha vior of the DG, since the algorithm we develop emulates this role. Our paid participants were recruited in pairs, and were self-identied na-tive speak ers of North American English.

The video output of DF' s computer was captured to a camera, along with the audio stream from both microphones. A logle created by the VR engine recorded the DF' s coordinates, gaze angle, and the position of objects in the world. All 3 data sources were synchronized using calibration mark ers. A technical report is available (Byron, 2005) that de-scribes the recording equipment and softw are used.
Figure 2 is a dialog fragment in which the DG steers his partner to a cabinet, using both a sequence of tar get objects and three additional repositioning commands (in bold) to adjust his partner' s spatial relationship with the tar get. 2.1 De veloping the Training Cor pus We recorded fteen dialogs containing a total of 221 minutes of speech. The corpus was transcribed and word-aligned. The dialogs were further anno-tated using the An vil tool (Kipp, 2004) to create a set of tar get referring expressions. Because we are interested in the spatial properties of the referents of these tar get referring expressions, the items in-cluded in this experiment were restricted to objects with a dened spatial position (buttons, doors and cabinets). We excluded plural referring expressions, since their spatial properties are more comple x, and also expressions annotated as vague or abandoned . Ov erall, the corpus contains 1736 markable items, of which 87 were annotated as vague, 84 abandoned and 228 sets.

We annotated each referring expression with a boolean feature called Locate that indicates whether the expression is the rst one that allo wed the fol-lower to identify the object in the world, in other words, the point at which joint spatial reference was achie ved. The kappa (Carletta, 1996) obtained on this feature was 0.93. There were 466 referring ex-pressions in the 15-dialog corpus that were anno-tated TR UE for this feature.

The dataset used in the experiments is a consensus version on which both annotators agreed on the set of markables. Due to the constraints introduced by the task, referent annotation achie ved almost perfect agreement. Annotators were allo wed to look ahead in the dialog to assign the referent. The data used in the current study is only the DG' s language. The generation module recei ves as input a route plan produced by a planning module, composed of a list of graph nodes that represent the route. As each sub-sequent tar get on the list is selected, content plan-ning considers the tuple of variables ID, LOC where ID is an identier for the tar get and LOC is the DF' s location (his Cartesian coordinates and ori-entation angle). Target ID' s are always object id' s to be visited in performing the task, such as a door that the DF must pass through. The VR world up-dates the value of LOC at a rate of 10 frames/sec. Using these variables, the content planner must de-cide whether the DF' s current location is appropriate for producing a referring expression to describe the object.

The follo wing features are calculated from this in-formation: absolute Angle between tar get and fol-lower' s vie w direction, which implicitly gives the in front relation, Distance from tar get, visible distrac-tors ( VisDistracts ), visible distractors of the same semantic cate gory ( VisSemDistracts ), whether the tar get is visible (boolean Visible ), and the tar get' s semantic cate gory ( Cat : button/door/cabin et) . Fig-ure 3 is an example spatial conguration with these features identied. 3.1 Decision Tree Training Training examples from the annotation data are tu-ples containing the ID of the annotated description, the LOC of the DF at that moment (from the VR en-gine log), and a class label: either Positi ve or Ne ga-tive. Because we expect some latenc y between when the DG judges that a felicity condition is met and when he begins to speak, rather than using spatial conte xt features that co-occur with the onset of each description, we averaged the values over a 0.3 sec-ond windo w centered at the onset of the expression.
Ne gati ve conte xts are dif cult to identify since the y often do not manifest linguistically: the DG may say nothing and allo w the user to continue mo v-ing along his current vector , or he may issue a mo ve-ment command. A minimal criterion for producing an expression that can achie ve joint spatial reference is that the addressee must have perceptual accessi-bility to the item. Therefore, negati ve training exam-ples for this experiment were selected from the time-periods that elapsed between the follo wer achie v-ing perceptual access to the object (coming into the same room with it but not necessarily looking at it), but before the Locating description was spok en. In these negati ve examples, we consider the basic felic-ity conditions for producing a descripti ve reference to the object to be met, yet the DG did not produce a description. The dataset of 932 training examples was balanced to contain 50% positi ve and 50% neg-ative examples. 3.2 Decision Tree Perf ormance This evaluation is based on our algorithm' s ability to reproduce the linguistic beha vior of our human subjects, which may not be ideal beha vior .
The Weka 1 toolkit was used to build a decision tree classier (W itten and Frank, 2005). Figure 4 sho ws the resulting tree. 20% of the examples were held out as test items, and 80% were used for train-ing with 10 fold cross validation. Based on training results, the tree was pruned to a minimum of 30 in-stances per leaf. The nal tree correctly classied 
The number of positi ve and negati ve examples was balanced, so the rst baseline is 50%. To incor -porate a more elaborate baseline, we consider that a description will be made only if the referent is visi-ble to the DF . Marking all cases where the referent was visible as describe-id and all the other examples as delay gives a higher baseline of 70%, still 16% lower than the result of our tree. 2
Pre vious ndings in spatial cognition consider an-gle, distance and shape as the key factors establish-ing spatial relationships (Gapp, 1995), the angle de-viation being the most important feature for projec-tive spatial relationship. Our algorithm also selects Angle and Distance as informati ve features. Vis-Distracts is selected as the most important feature by the tree, suggesting that having a lar ge number of objects to contrast mak es the description harder , which is in sync with human intuition. We note that Visible is not selected, but that might be due to the fact that it reduces to Angle . In terms of the referring expression generation algorithm described by (Reiter and Dale, 1992), in which the description which eliminates the most distractors is selected, our results suggest that the human subjects chose to re-duce the size of the distractor set before producing a description, presumably in order to reduce the com-putational load required to calculate the optimal de-scription.

The exact values of features sho wn in our deci-sion tree are specic to our environment. Ho we ver, the features themselv es are domain-independent and are rele vant for any spatial direction-gi ving task, and their relati ve inuence over the nal decision may transfer to a new domain. To incorporate our nd-ings in a system, we will monitor the user' s conte xt and plan a description only when our tree predicts it. We describe an experiment in content planning for spok en dialog agents that pro vide navigation in-structions. Na vigation requires the system and the user to achie ve joint reference to objects in the envi-ronment. To accomplish this goal human direction-givers judge whether their partner is in an appropri-ate spatial conguration to comprehend a reference spok en to an object in the scene. If not, one strate gy for accomplishing the communicati ve goal is to steer their partner into a position from which the object is easier to describe.

The algorithm we developed in this study , which tak es into account spatial conte xt features replicates our human subject' s decision to produce a descrip-tion with 86%, compared to a 70% baseline based on the visibility of the object. Although the spatial details will vary for other spok en dialog domains, the process developed in this study for producing de-scription dialog mo ves only at the appropriate times should be rele vant for spok en dialog agents operat-ing in other navigation domains.

Building dialog agents for situated tasks pro vides a wealth of opportunity to study the interaction be-tween conte xt and linguistic beha vior . In the future, the generation procedure for our interacti ve agent will be further developed in areas such as spatial de-scriptions and surf ace realization. We also plan to investigate whether dif ferent object types in the do-main require dif ferential processing, as prior work on spatial semantics would suggest.
