 1. Introduction available [18] .
 all frequent sequences of itemsets in a dataset.

The problem of sequence classification has been addressed in the literature in many ways; the earliest approaches em-input in a standard classification algorithm. The FeatureMine algorithm uses these features with the na X ve Bayes and performs CBS_ALL [38] .
 problems.
 ported CBS_CLASS algorithm. The methodology can be considered as a compound data mining method that uses sequential matically. The methodology employs a sequential pattern mining algorithm, a scoring function that uses the sequential correspond to two biological problems of high importance: protein fold recognition and class prediction. eric and can incorporate different algorithms/approaches in any of its stages. 2. Methods
The list of symbols employed in this work and their explanation are summarized in Table 1 . The proposed methodology the realization of stage 1, a dataset D ={ S i , c i }, i =1, ... , l ( c = {1, ... , l c }) and l S is the number of sequences in the dataset ( j D j l 2.1. Stage 1: Sequence classification
Step 1: Sequential pattern mining. The training sequences are divided into l
Appendix A ) is applied to each subset, generating l c sets of sequential patterns ( P
The above procedure is also followed by the CBS_CLASS [38] algorithm, and reported to outperform both CBS_ALL, who patterns P j , j =1, ... , l c , characterizing the l c classes.
 pattern score matrix PSM j , j =1, ... , l c . Each PSM j class j with all S i sequences; thus, its size is l j l S
Step 3: Update of the pattern score matrices. Each row of every PSM the weight of a specific pattern; thus each matrix PSM j is updated as follows: row m row of the j th PSM matrix and wp j ( m ) is the weight of the m th sequential pattern of the j th pattern set.
Step 4: Calculation of the class score matrix. From the updated PSM is l
Step 5: Update of the class score matrix. Each row of the matrix CSM is multiplied by a parameter, which denotes the weight of a specific class; thus the matrix CSM is updated as follows: row CSM matrix and wc ( j ) is the j th element of the class weight vector.

Step 6: Calculation of the confusion matrix. For each sequence S pc i  X  argmax j  X  1 ; ... ; l c  X  CSM  X  j ; i  X  X  . Based on the real class c sequences is calculated. The pseudocode for the above six steps of stage 1 is given in Fig. 3 . 2.2. Stage 2: Optimization sification accuracy of the sequences in the dataset. Initially, a cost (or objective) function is defined: procedure are the optimal weights wp , wc . 3. Implementation optimization procedure, the optimization algorithm and the optimization approaches for the calculation of the optimal weights.
 3.1. Sequential pattern mining algorithm 1-sequences, another for frequent 2-sequences, and one more for generating all frequent k -sequences. The performance of the cSPADE algorithm has been proven superior, compared to other constrained SPM approaches [17,23,37] . 3.2. Scoring function
The scoring function assigns scores to the sequential patterns in the following way: if the P the j th class is contained in the S i sequence then the ( m , i ) element of the PSM minus 1, divided by the number of patterns which describe the j th class. If the P
Wesubtract1fromthelengthofthepattern,inordertoassigntheminimumscore,whichis1,totheminimalpattern,whose Also,thescore ofa sequencewithrespectto aclassis dividedbythenumberofsequential patternsextractedfromthis classset. Thus, the smaller the number of patterns that describe a class, the more significant is each one of these patterns. class are contained in a sequence, the higher the score of the sequence for this class is. 3.3. Optimization elements the output is a function of the confusion matrix g (CM). The following objective function g (CM) is selected: sification model.

The local optimization strategy is chosen, since there is an extremely large number of parameters which must be The result of Roll optimization method is a local optimum of the objective function, for wp and wc . The following optimization approaches were employed: application of stage 1 only of the methodology): class weights wc * : weights wp * : to identify the optimal class weights wc * : identify the optimal pattern weights wp * : 4. Application to artificial data 4.1. Dataset ( l = 3), thus D ={ S i , c i } with S i being the sequence and c remaining are used for testing. Thus, the training dataset D
D test consists of 6 sequences ( j D test j =6)( Table 2 ). 4.2. Generation of the sequence classification model 4.2.1. Stage 1: Sequence classification
Step 1: Sequential pattern mining. D train is divided into three subsets, S class only, thus j S 1 j =6, j S 2 j =6, j S 3 j = 6. Then, SPM is applied to each set S present in at least half of the sequences of the respective S as sequential patterns). Based on these, the following sequential patterns are extracted: from the S
S 2 ab , ba , cb and aba , from the S 3 ac and ca . All patterns have 50% support, except ca that presents 66.7%. Thus,
P ={ bb , bc , bbc }, P 2 ={ ab , ba , cb , aba } and P 3 ={ ac , ca }.

Step 2: Calculation of the pattern score matrices . In order to create the PSM tained in each sequence (sequential pattern matching). Here, since the sequential patterns have been extracted using uous subsequence. Then, for each set P j and based on the existence or not of a pattern in a sequence, a PSM is PSM 1 (1,1) = ( length ( bb ) 1)/ j P 1 j , and length ( bb )=2, j P
Step 3: Update of the pattern score matrices . Each row of the three PSM matrices is multiplied by a weight wp denotes the weight of the i th pattern of the j th set of pattern (PSM PSM j matrices remain the same (the weights are tuned in stage 2 of the methodology). summing the scores of the patterns of the same class ( Fig. 3 ).
 the first cell, 1.33 is the sum of scores of all patterns of class 1 ( P
P set to 1 so the updated CSM matrix remains the same (the weights are tuned in stage 2 of the methodology). quences of the training set D train . Using pc i = argmax methodology for each sequence, as it is depicted in Table 5 .

From the above, we derive the confusion matrix (CM) for the sequences of D
The accuracy is given by the trace of the CM, which in our case equals to 10, thus the accuracy is 55.6%. 4.2.2. Stage 2: Optimization f ( D f ( D tions of the weights wp and wc . The initial value of the objective function is f ( D mize f ( D train , wp , 1 )= j D train j trace(CM) with respect to wp ,so wp * is calculated, and then optimize f ( D j D train j trace(CM) with respect to wc , thus resulting to wc are shown in Table 7 .
 The updated PSM 1 matrix is shown in Table 8 , while the new CSM is presented in Table 9 . Based on the updated CSM matrix, we derive the new predictions for each sequence, as it is shown in Table 10 . classified into class 2, with the multiplication of the optimal pattern weights. The new confusion matrix is shown in Table 11 .
 the method is increased by 5.5% (from 55.6% to 61.1%).

Having found the optimal pattern weights, we can calculate the optimal class weights. Thus, optimizing the objective function, f ( D train , wp * , wc )= j D train j trace(CM), the optimal class weights are identified: wc [0.9,1.2,0.8].
 With the above class weights, the CSM is updated, as it is shown in Table 12 . c is higher that the score of c 3 ; however, the scores have been rounded for simplicity), as shown in Table 13 . class 2, with the multiplication of the optimal class weights. The new confusion matrix is shown in Table 14 . is 11.1%. 4.3. Evaluation of the generated sequence classification model
The generated sequence classification model was evaluated with six sequences that comprise D We test the method using three different approaches: (i) Set both pattern and class weights to 1 : We create the PSM match the patterns as contiguous subsequences since they have been extracted using max _ gap = 1). Then, based on the scoring function, the PSM j matrices for the test sequences are created (e.g. PSM PSM matrices, we derive the CSM matrix, shown in Table 17 .

Subsequently the predictions are shown in Table 18 . From Table 18 , the confusion matrix for the test sequences is derived, shown in Table 19 . Thus, trace(CM) = 3 and accuracy = 50%. obtained from model generation, the updated PSM j matrices are obtained (e.g. the updated PSM (from 50% to 66.7%).
 racy using both sets of weights is increased up to 83.3%.
 5. Application to real data
The proposed methodology is evaluated using a biological sequence dataset. Results of the proposed methodology are presented without the use of stage 2 [14,15] and with the use of stage 2, by applying all five different optimization approaches (App. 2 X  X pp. 6). 5.1. Dataset mary structures and each one of them has a sequence id . The set of items I is the 20 amino acids that compose the protein primary structures plus one for the unknown amino acid. An itemset in a transaction consists of a single item time.

A group of primary protein sequences is taken from the Protein Data Bank (PDB) [7] . All members of this group corre-two thirds from each category are randomly selected and used for training, while the rest for testing. classification experiments are performed. Each dataset is divided into training and test sets: consist of 666 and 334 proteins, respectively.
 proteins and the test set of 203 proteins.
 proteins and the test set of 131 proteins.
 constitute the training set and 334 proteins the test set. 5.2. Sequence classification model generation sequential patterns.) 5.3. Evaluation of the generated sequence classification model  X  comparative study sequential pattern is calculated for each sequence, as follows: if the P element of the PSM j matrix is equal to length  X  P j m  X  = predicted class ( pc ) with the proposed methodology, but without the use of the optimization stage. responding sets should not contain any redundant features.
 different approaches of the proposed methodology. 6. Discussion calculated using an optimization technique. The obtained optimal pattern and class weights along with the extracted sequential patterns compose the sequence classification model, which is used to classify the test sequences.
The proposed methodology introduces several innovative features. To our knowledge, the automatic assignment of patterns.
 database.

The proposed methodology has been evaluated systematically, using 12 different evaluation experiments (four datasets uation experiments, resulting from the wide range of parameters, ensures the reliable evaluation of the proposed methodology.

FeatureMine presents the lowest computational complexity compared to all other methodologies, due to the employment number of sequential patterns and subsequently the computation cost for all methodologies increases. sequence classification model.

A comparison between the five different approaches of the proposed methodology that employ the weight introduction ever, App. 5 is slightly better overall, compared to the other approaches, but not in every different experiment. the class weights), and the respective results follow also the same increasing trend. difference in the classification accuracy. The same applies to App. 3 and its extension, App. 4. A comparative study has been conducted between the different approaches of the proposed methodology and the CBS and tureMine present similar average accuracy, and in Exp. 4, the proposed methodology outperforms FeatureMine. of 12 experiments).

Comparing directly the proposed methodology with the FeatureMine algorithm, the proposed methodology presents bet-cation experiments in the test set (plus one where both the proposed methodology and FeatureMine have the same accuracy). The advantage of FeatureMine compared to the proposed methodology is the lower computational complexity as classification accuracy.
 case where the number of patterns is reduced). 7. Conclusions
A two-stage methodology for sequence classification has been presented along with an extensive evaluation. The meth-in order to handle time series, through the use of discretization techniques will be examined. Appendix A. Sequential pattern mining  X  definitions and terminology
SPM is defined as follows [2] : Let I ={ i 1 , i 2 , ... , i sequence s =( s 1 , s 2 , ... , s p ) is an ordered list of itemsets, where s of itemsets in the sequence, i.e. j s j . The length l of a sequence s =( s l is an l -sequence. A sequence s a =( a 1 , a 2 , ... , a 1 6 i 1 &lt; i 2 &lt; &lt; i q 6 r such that a 1 b i 1 ; a 2 dual representation of D as its sequence representation.
 s 2 D which contain s a and is denoted by supD ( s a ). Given a support threshold minSup , a sequence s pattern on D (or frequent l -sequence) with l being the length of the sequence, if supD ( s sequential patterns is to find all frequent sequential patterns for a database D , given a support threshold minSup . A.1. Sequential pattern mining constraints sequence s a =( a 1 , a 2 , ... , a q )isa d -distance subsequence of s sequence of s b , i.e. the items of s a can be mapped to a contiguous segment of s of having gaps between consecutive items is eliminated. Similar to the maximum gap constraint is the minimum gap con-straint, which states that the distance between two consecutive items must be larger than a specified value ( i Appendix B. Roll optimization method
The Roll optimization method [13,32] works as follows: for each variable X
Let X c  X  X  X c 1 ; X c 2 ; ... ; X c n  X  be the current point and let f (1) Pick a trial point: X t j  X  X c j for all j 6  X  i and X (2) Calculate f + = f ( X t ). (3) If f + &lt; f c set X c = X t , f c = f + and s i = as (4) If f + P f c pick another trial point as: X t j  X  X c (5) Calculate f = f ( X t ). (6) If f &lt; f c set X c = X t , f c = f and s i = as i proceed from step 1 for the next value of i . (7) If f P f c calculate an appropriate step by: s i  X  1 2 (8) Proceed from step 1 for the next value of i . performed the line search, the relative progress made (i.e. [ f
References
