 Recently, trie-based data structures [1  X 3] have attracted much attention from the academia as a competitive alternative for hash table [4 X 7] and B-tree [8, 9]. A trie is a multi-way tree. Unlike B-tree, a trie does not store any string explicitly in its nodes, but represents a string by the path from the root to a node associated with the string. To search a string, we start at the root, examine each character of the string, and follow the corresponding edges. When the string is completely consumed, we reach the node that represents the string and get the information associated with it. If no matching path is found during the retrieval, we are sure that the string does not exist.

In this paper, we propose a novel approach for bulk loading trie-based data structures especially on external storage such as magnetic disks and flash mem-ories. In particular, we implement a B-trie [3] and apply the proposed bulk loading algorithm. Like most B-tree bulk loading algorithms, our algorithm con-sists of two steps. First, the data are sorted. Then a B-trie is built directly from the sorted data. Data in the bulk loaded B-trie are compacted and physically sorted, so that range queries (e.g., prefix matching) can be performed very effi-ciently. To help understand the performance of the proposed algorithm, we test our implementation with six real-world datasets and two synthetic datasets. Ex-perimental results show that our algorithm outperforms the baseline insertion method dramatically if the dataset is large enough and is almost always superior to the basic sort-and-insert algorithm.
 The trie access method is first proposed i n [10, 11]. In standard trie structure, a node maintains a pointer for each chara cter in the alphabet and is therefore memory-hungry. One possible optimization is to compact the nodes by removing null pointers. Another is to reduce the total number of trie nodes, such as compressing chain paths [12, 13], adjacent levels [14], or both [15]. [16] proposes compressed trie, or C-trie, which is only feasible to store static data. And [17] studies the complexity of trie construction.

Burst trie [2] employs a different method to reduce the space requirement of a trie. Taking advantage of the fact that nodes close to the root tend to be dense and vice versa, burst trie stores the upper -level nodes as a conventional trie called access trie , and compacts others in small data structures, called containers .To search a string, we first follow the path corresponding to a prefix of the string in the access trie, reach a container, and t hen search in the container using the remaining suffix of the string. When a container contains too many strings to search, it is burst -ed according to the first characters of the stored suffixes. B-trie [3] is a variant of burst trie which is designed for external storage. In order to reduce the overhead of split operations, [3] suggests that only one bucket 1 should be created if a bucket overflows. Obviously, this goal requires that a bucket should be able to accommodate suffixes with different leading characters, and therefore the concepts of pure and hybrid buckets are introduced.
One potential drawback of B-trie and other trie structures is that the position of a string is uniquely determined by its prefix. (This is also the reason why trie can be more efficient than binary search tree and B-tree.) To build a B-tree, we can split the sorted data into leaves at arbitrary position as long as the leaves do not exceed the size limit. But in a trie, we always need to search from the root to find the right place of a string, and therefore efficient bulk loading is not applicable for trie structures, as asserted in [3]. We are going to show, in this paper, that this assertion is not accurate. By carefully choosing potential bucket boundaries, we can build a B-trie directly from the sorted data. Like other bulk loading algorithms, the proposed algorithm first sorts the data in desired order and then builds the B-trie directly from the sorted data. The fundamental question during this process is how to split the data into buckets.
Before going into details, we first introduce the basic idea of the proposed bulk loading algorithm by a top-down approach. In Fig. 1, we build a B-trie step by step. Initially, all strings are put in a single bucket (Fig. 1(a)). It is obvious that the total size exceeds the space limit of a bucket (10 bytes in our example). Therefore, the bucket is split according to the first characters (Fig. 1(b)). This procedure continues until all buckets meet the size limitation.
It should be noted that in Fig. 1(c), the second characters of case and char are not consumed by the path after th e split, because they are in a hybrid bucket, while const and continue have the same third character and the bucket in Fig. 1(d) is pure, and therefore the character is discarded in the bucket.
From this example, we can see that strings in a bucket share the same prefix (not necessarily the longest one) as indicated by the path 2 . And for a group of strings to be packed in the same bucket, the path that leads to the bucket should be able to uniquely distinguish those strings, or in other words, no string in other buckets may share the same prefix 3 . Accordingly, the proposed algorithm works by examining the length of the common prefix between adjacent strings and choosing potential bucket boundaries.

First of all, we need to know how to calculate the common prefix length between any arbitrary pair of strings in a sorted list. We start with two theorems. Theorem 1. Suppose s 1 ,s 2 and s 3 are three strings, and we have prefix( s 1 ,s 2 ) =  X  and prefix( s 2 ,s 3 ) =  X  ,where prefix() returns the common prefix length of the two input strings, then prefix( s 1 ,s 3 )  X  min(  X  ,  X  ) .
 of s 2 must be equivalent, which means that a i = b i for all i  X  [1, min(  X  ,  X  )]. Therefore the first min(  X  ,  X  ) characters of s 1 and s 3 are identical, or in other words, prefix( s 1 , s 3 )  X  min(  X  ,  X  ).
 Theorem 2. InTheorem1,ifs 1 &lt; s 2 &lt; s 3 ,then prefix( s 1 ,s 3 ) = min(  X  ,  X  ) . Proof. According to Theorem 1, we know that prefix( s 1 , s 3 )  X  min(  X  ,  X  ). We will prove that, in three different conditions, the { min(  X  ,  X  )+1 } -th characters of s 1 and s 3 cannot be equivalent. (1) If  X  =  X  ,then x 1 &lt;y 1 = p 1 &lt;q 1 .(2)If  X &lt; X  ,then x 1 &lt;y 1 = b  X  +1 .(3)If  X &gt; X  ,then a  X  +1 = p 1 &lt;q 1 . Corollary 1. Suppose a sorted list of strings, s 1 ,s 2 , ... ,s n ( n  X  3 and s 1 &lt; s p , ... ,p n  X  1 ) .
 Proof. When n =3,prefix( s 1 , s 3 )=min( p 1 , p 2 ) holds (Theorem 2). Assume for thereby showing that the above equation holds for k +1. It has now been proved by mathematical induction that prefix( s 1 , s n )=min( p 1 , p 2 , ... , p n  X  1 ).
Since all data are sorted in the first step, we could calculate the common prefix length of any two strings in the dataset according to Corollary 1.
In the rest of this section, we assume a sorted list of strings, s 1 , s 2 , ... , s n ( s 1 &lt; s 2 &lt; ... &lt; s n ) and prefix( s i , s i +1 )= p i (1
To determine the maximal length of the common prefix shared by a continuous segment of the string list, we define bucket depth as follows.
 Definition 1. The depth of bucket candidate B ij (abbreviated as D ij ) is defined as the length of the longest common prefix shared by all strings in B ij . D ij is the maximal possible depth in the B-trie if B ij is a pure bucket. Therefore let D ij be the actual depth of B ij in the B-trie, and we have
A bucket can be hybrid only when it contains at least two strings. It should also be noted that in order to discuss feasibility and correctness, we use the maximal value of D ij mostofthetimeinthispaper.
 Theorem 3. Let D ij be the depth of bucket candidate B ij ,then Proof. If i = j , the only string, s i , is of course the longest common prefix. If and s k are identical according to Corollary 1. Since min( p i , p i +1 ,..., p k  X  1 )  X  s cannot match.

In order to make sure that two buckets do not conflict with each other, we define the repellency of two buckets.
 Definition 2. The repellency of two buckets, B ij and B kl , is defined as We further define the repellency between bucket B ij and its left (right) neighbor as B ij  X  X  left (right) repellency and abbreviate it as (  X  X  X 
R
Repellency measures the minimal lengt h (exclusive) of path that is needed to separate two buckets. Given two strings, s 1 and s 2 , they cannot be distinguished by the first prefix( s 1 , s 2 ) characters. Theref ore, if two strings, s 1 and s 2 ,come from two different buckets, the paths t hat lead to the two buckets in the access trie should be longer than prefix( s 1 , s 2 ).
 Theorem 4. Assume a bucket generation plan splits the sorted string list, s 1 , s , ... ,s n , into k buckets, B 1 , B 2 , ... , B k . The plan is legal iff Proof. Necessity: A split plan is legal only when the path to any bucket can distinguish the strings in that bucket with those in others. Suppose the paths be ranges). There must be some differ ence between the two paths in the first min( m, n ) characters. Therefore, repel( B i , B j ) &lt; min( m, n )  X  m = D i holds. Sufficiency: If (4) holds, the path to a bucket will never be a prefix of another path. As a result, inserting a bucket in the B-trie will always create a new path in the access trie and no conflict will occur. So the plan is legal.

Theorem 4 provides a method to check the validity of a bucket generation plan. However, it is still impossible to design a practical algorithm accordingly, since Theorem 4 requires the existence of all buckets. To solve this problem, we introduce another theorem.
 Theorem 5. Assume B i , B j and B k are three different buckets from a sorted bucket list and i&lt;j&lt;k . Then we have Proof. For any s x  X  X  i , s y  X  X  j ,and s z  X  X  k ,wehave  X  prefix( s x , s z ). Based on Definition 2, it can be easily seen that (5) holds.
Therefore it is unnecessary to check the repellency with all other buckets when building a bucket. Instead, we only have to pay attention to its adjacent neighbors.
 Corollary 2. Bucket B ij is legal when Proof. According to Theorem 5, B i  X  1 has the largest repellency with B i among all buckets on B i  X  X  left, while B i +1 has the largest repellency with B i among all buckets on B i  X  X  right. Therefore, bucket B i is legal when D ij &gt; according to Theorem 4. The maximal possible value of the actual depth can be easily figured out by the depth of a bucket (Definition 1). Therefore, we have proved (6). Note that we do not distinguish the left part according to whether i&lt;j , because duplication is not allowed in a B-trie and for a single string as a bucket, it is impossible for its left neighbor string to share a prefix as long as its length. (See Theorem 6 for more information.)
Now, the last question lies in how to calculate the repellency of two adjacent buckets, especially when the right one has not been generated. We introduce a much easier method in Theorem 6, leveragi ng the fact that all strings are sorted. Theorem 6. Repellency of two adjacent buckets equals to the common prefix length of the two nearest strings from the two buckets, or in other words, Proof. First, we choose two strings, s j  X  X  ij and s j +1  X  X  j +1 ,k . According to according to Corollary 1, we get repel( B ij , B j +1 ,k )  X  p j . Therefore, (7) holds.
Finally, we provide a general description of the proposed algorithm and leave the details to the next section. First, all strings are sorted in the desired order and the common prefix length between adja cent strings are calculated. Then, we split the sorted data into buckets and build the access trie directly. The buckets are generated by scanning the sorted data and looking for the largest continuous segment of the string list that fits for a bucket and meets Corollary 2. After deciding a bucket, we obtain the prefix shared by its content according to the bucket depth (Definition 1) and then build the path in the access trie. Algorithm 1. bucket size() -Calculate the candidate bucket size In this section, we explain the details of the proposed algorithm. For the sake of simplicity, we maintain a few global variables that indicate the current state of the algorithm. They are summarized in Table 1.

Algorithm 1 calculates the amount of bytes that the current strings will take if they are going to be packed together. First, we calculate the space that the suffixes will take (Line 3-15). This can be done by removing the shared prefix from the total length. In case the first string is of the same length as the common prefix, we may either return 0 directly 4 if there is only one string in the current set or treat it as an internal string otherwise (Line 4-8). It should be noted that if all the suffixes have the same leading character, which indicates a pure bucket, we may remove it from the suffixes (Line 13). Otherwise, the characters have to be stored in the bucket (Line 15). Finally, we obtain the total amount of bytes by the expression in Line 16. (Layout of a bucket will be explained in Section 5.1.) Algorithm 2. build bucket() -Build the current bucket
If the current group of strings can fit in a bucket, we may build the bucket and insert it in the B-trie (Algorithm 2). First, we create a bucket with proper pointer range (Line 5-7), and then insert all the suffixes in the bucket (Line 8-13). Line 14 builds the path that leads to the bucket. It should be noted that all pointers in the range should point to the bucket (Line 15-16), including those that do not appear in the bucket. Finally, we insert the internal strings in the access trie (Line 17-19).

With Algorithm 1 and 2 as building blocks, we can now introduce the pro-posed bulk loading algorithm. The pseudo code is provided in Algorithm 3. As described in the previous section, the proposed algorithm works on sorted data (Line 1). It scans the sorted data and builds the buckets one by one. Therefore, the body of Algorithm 3 is mainly a single while -loop (Line 3-29). In each loop, we build a single bucket. The algorithm runs until all strings are consumed. To build a bucket, we scan the remaining strings, find the largest set of strings that fit in a bucket (Line 6-26), build the bucket (Line 27), and remove them from the dateset (Line 28). Line 2, 4-5, 7-13, and 29 are used to maintain the global variables, and we will focus on the other three if -conditions in the for -loop.
Since we are trying to find the largest set that can fit in a bucket, we will continue scanning until the size limit ation is exceeded. And then we will use the largest possible group that is ever found (Line 21-22). Otherwise, the first remaining string should be put in the internal set (Line 16-19). The reason lies in that the first string should always be able to make a potential bucket, unless it is a prefix of the second one.
 Algorithm 3. bulk load() -Bulk load a B-trie
During the search procedure, if we should ever find a position where the right repellency is smaller than the left one, we can stop here and build the bucket (Line 23-24), because beyond this position, the bucket depth will always be under the left repellency and there can be no valid bucket candidate any more according to Corollary 2.

The last issue that is worth mentioning is that we should remember the last valid bucket candidate during the search process (Line 25-26). Acute readers may have noticed that we only check right part of the condition (Corollary 2). Actually, the left part ( prefix  X  left ) always holds during the search process. Initially, prefix is set to the length of the first string (Line 9), which is, of course, at least as long as the common prefix with another string. After that, prefix can only be lowered by the right repellency right (Line 11), and when right is getting too low, we build the bucket immediately (Line 23-24). 5.1 Implementation Details We implement a B-trie as well as the proposed algorithm. The standard ASCII code is employed as the alphabet. We assume that the access trie is small enough to reside in main memory entirely.

The buckets are limited to 4 KB. Figure 2 shows the layout of our bucket implementation. The first two bytes contain the character (pointer) range of the bucket. The next four bytes contain the number of strings stored in the bucket. Then follows an array of exactly the same number of pointers, each of which points to a suffix. Buckets are stored in external files.
 The experiments are done on a Lenovo T430s laptop running Ubuntu 12.04.2 LTS. We allocate 64 MB as the memory pool to generate initial runs or as the bucket cache. If the data are sorted before loading, we will flush the cache each time it is filled up.
 Six publicly available real-world datasets and two synthetic datasets are used. We believe that these datasets are representative and can cover a wide range of applications. Table 2 summarizes the major characteristics of the datasets. Be-sides, duplicate strings and non-ASCII characters are removed from the datasets. Before execution, we always drop the sy stem cache to minimize its effects. All results reported in this paper are the average of at least five independent runs. 5.2 Results and Analysis We implement four different algorithms, the na  X   X ve insert method, two sort-and-insert methods, and the proposed bulk loading algorithm. The first sort-and-insert method always tries to split full buckets evenly, and therefore may result in many half-filled buckets, while the second always chooses the last possible split point, which is a common practice for bulk loading tree-structured indexes.
Results of the experiments are shown in Fig. 3. As the figures indicate, per-formance of the na  X   X ve insert method 11 is very well when the dataset can be kept in memory ( aol and trec ). Sometimes, it even outperforms the proposed bulk loading algorithm, thanks to its linear complexity. All the other three methods have to sort the data first and may not be very competitive. However, when the dataset exceeds the memory capa city, performance of the na  X   X ve insert method drops rapidly due to its random access pa ttern. On the contrary, the other three sort-based algorithms can always respond in reasonable time, and for all datasets except uniref , the proposed bulk loading algorithm outperforms the other sort-based algorithms, which insert the sorted data in the normal way.

The synthetic datasets ( random and zipf ) aim to simulate large-scale applica-tions. As can be seen from Fig. 3(g)(h), the external sort procedure causes long response time. Fortunately, this step can be easily parallelized and there exist many implementations and tools (e.g., Hadoop). Besides, if the dataset is the output of another application, it might be sorted already. For these datasets, the proposed method is always superior to the others. This paper introduces a novel approach for bulk loading a trie-based index struc-ture. We also provide detailed proof on the correctness of the algorithm. Ex-perimental results show that our algorithm outperforms the baseline insertion method dramatically when the dataset is large enough and is almost always superior to the basic sort-and-insert algorithm.
 Acknowledgements. This work was partly supported by NSF of China (61272090), Tsinghua-Samsung Joint Laboratory,  X  X ExT Research Center X  funded by MDA, Singapore (WBS: R-252-300-001-490),and FDCT/106/2012/A3.
