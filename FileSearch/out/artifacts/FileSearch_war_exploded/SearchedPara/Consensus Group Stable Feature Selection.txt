 Stability is an important yet under-addressed issue in fea-ture selection from high-dimensional and small sample data. In this paper, we show that stability of feature selection has a strong dependency on sample size. We propose a novel framework for stable feature selection which first identifies consensus feature groups from subsampling of training sam-ples, and then performs feature selection by treating each consensus feature group as a single entity. Experiments on both synthetic and real-world data sets show that an algo-rithm developed under this framework is effective at alleviat-ing the problem of small sample size and leads to more stable feature selection results and comparable or better generaliza-tion performance than state-of-the-art feature selection al-gorithms. Synthetic data sets and algorithm source code are available at http://www.cs.binghamton.edu/  X  lyu/KDD09/. H.2.8 [ Database Management ]: Database Applications-data mining; I.2.6 [ Artificial Intelligence ]: Learning Algorithms Feature selection, stability, ensemble, high-dimensional data, small sample
High-dimensional small-sample data is common in biologi-cal applications like gene expression microarrays [8] and pro-teomics mass spectrometry [20]. Classification on such data is challenging due to the two distinct data characteristics: high dimensionality and small sample size. Many feature selection algorithms have been developed with a focus on improving classification accuracy while reducing dimension-ality for such data [3, 9, 10, 14, 15, 22]. The issues of feature relevance and redundancy have also been well studied [1, 5, 27]. A relatively neglected issue is the stability of feature se-lection -the insensitivity of the result of a feature selection algorithm to variations in the training set. This issue is im-portant in many applications where feature selection is used as a knowledge discovery tool for identifying characteristic markers for the observed phenomena [19]. For example, in microarray data analysis, a feature selection algorithm may select largely different subsets of features (genes) under vari-ations to the training data, although most of these subsets are as good as each other in terms of classification perfor-mance [11, 26]. Such instability dampens the confidence of domain experts in investigating any of the various subsets of selected features for biomarker identification.
In this paper, we demonstrate that stability of feature se-lection has a strong dependency on sample size. Moreover, we show that exploiting intrinsic feature groups in the un-derlying data distribution is effective at alleviating the effect of small sample size for high-dimensional data. Therefore, we propose a novel feature selection framework (as shown in Figure 1) which approximates intrinsic feature groups by a set of consensus feature groups and performs feature se-lection and classification in the transformed feature space described by consensus feature groups.

Our framework is motivated by a key observation that intrinsic feature groups (or groups of correlated features) commonly exist in high-dimensional data, and such groups are resistant to the variations of training samples. For ex-ample, genes normally function in co-regulated groups, and such intrinsic groups are independent to the set of observed microarray samples. Moreover, the set of intrinsic feature groups can be approximated by a set of consensus feature groups obtained from subsampling of the training samples. Another observation is that treating each feature group as a single entity and performing learning at the group level allows the ensemble effect of each feature group to offset the random relevance variation of its group members. In-tuitively, it is less likely for a group of irrelevant features to exhibit the same trend of correlation to the class (hence, showing artificial relevance) than for each group member to gain some correlation to the class under random subsam-pling, unless all features in the group are perfectly corre-lated. Therefore, discriminating relevant groups from irrel-evant ones based on group relevance is less prone to overfit-ting than detection of relevant features on small samples.
As shown in Figure 1, there are two new issues in consen-sus group based feature selection: (1) identifying consensus feature groups from the given training data, and (2) repre-senting each feature group by a single entity so that feature selection and classification can be performed on the trans-formed feature space. In our previous work [26], we devel-oped an algorithm, DRAGS, which identifies dense feature groups in the sample space and uses a representative feature from each group in the subsequent feature selection and clas-sification steps. The algorithm has shown some promising results w.r.t. the stability of the dense groups and the gen-eralization ability of the selected features. However, there are two major limitations about DRAGS. First, DRAGS tries to identify dense feature groups in the sample space with dimensionality as high as dozens or a few hundreds (of samples) which makes density estimation difficult and unre-liable. As a result, the feature groups found are not always stable under training data variations. Second, DRAGS faces the density vs. relevance dilemma -it limits the selection of relevant groups from dense groups for better stability of the selection results, however, it will miss some relevant features if those features are located in the relatively sparse regions. The new framework of consensus group based feature selec-tion addresses these two issues.

The main contributions of this paper are: (i) conducting an in-depth study on the sample size dependency for the stability of feature selection; (ii) proposing a novel frame-work of consensus group based feature selection which alle-viates the problem of small sample size; and (iii) developing a novel algorithm under this framework which overcomes the limitations of DRAGS. Experiments on both synthetic and real-world data sets show that the new algorithm leads to more stable feature selection results and comparable or bet-ter generalization performance than state-of-the-art feature selection algorithms DRAGS and SVM-RFE.
High-dimensional data with small samples permits too large a hypothesis space yet too few constraints (samples), which makes learning on such data very difficult and prone to model overfitting. In order to find a probably approxi-mately correct (PAC) hypothesis, PAC learning theory [12] gives a theoretic relationship between the number of sam-ples needed in terms of the size of hypothesis space and the number of dimensions. For example, a binary data set with binary classes has a hypothesis space of size 2 2 n where n is the dimensionality. It would require O (2 n ) samples to learn a PAC hypothesis without any inductive bias [17].
Feature selection [7, 13] is one effective approach to re-ducing dimensionality -finding a subset of features from the original features. The reduction of dimensionality results in an exponential shrinkage of the hypothesis space, and hence reduces the chance of model overfitting and improves the generalization of classification algorithms [18]. However, feature selection itself is a challenging problem and receives increasing and intensified attention [16]. The shortage of samples in high-dimensional data increases the difficulty in finding relevant features, and reduces the stability of feature selection results under variations of training samples.
We next illustrate based on synthetic data that success-ful detection of relevant features and the stability of feature selection results can have a strong dependency on the sam-ple size. The merit of using synthetic data for illustration is two-fold. First, it allows us to examine the sample size dependency using training sets with a wide range of sample sizes and other properties being equal; second, it provides us prior knowledge about truly relevant features.
The data set used here consists of 1000 training samples randomly drawn from the same distribution P ( X, Y ). The feature set X consists of 1000 features, including 100 mutu-ally independent features, X 1 , X 2 , ..., X 100 , and a number of (10  X  5) highly correlated features to each of these 100 fea-tures. Within each correlated group, the Pearson correlation of each feature pair is within (0.5,1), and the average pair-wise correlation is below 0.75. The balanced binary class label Y is decided based on X 1 , X 2 , ..., X 10 using a linear function of equal weight to these 10 truly relevant features.
We study SVM-RFE [9], an algorithm well known for its excellent generalization performance on high-dimensional small-sample data. The main process of SVM-RFE is to recursively eliminate features based on SVM, using the co-efficients of the optimal decision boundary to measure the relevance of each feature. At each iteration, it trains a lin-ear SVM classifier, and eliminates one or more features with the lowest weights. We apply SVM-RFE on the above train-ing set with sample size 1000, and its three randomly drawn subsets of training samples with decreasing sample sizes 500, 200, and 100, in order to observe the sample size dependency of SVM-RFE w.r.t. successful detection of relevant features and the stability of the selected feature subsets.
To evaluate the stability of SVM-RFE for a given training set, we can simulate training data variation by a resampling procedure like bootstrapping or N-fold cross-validation. We choose 10-fold cross-validation. For each training set, SVM-RFE is repeatedly applied to 9 out of the 10 folds, while a dif-ferent fold is hold out each time. The stability of SVM-RFE is calculated based on the average pair-wise subset similar-ity of the top 10 features (the optimal number of features) selected over the 10 folds. To evaluate the effectiveness of SVM-RFE in detecting relevant features, the average preci-sion of the top 10 features w.r.t the 10 truly relevant features over the 10 folds is also calculated.

To illustrate the effectiveness of feature selection based on intrinsic feature groups, we exploit the prior knowledge Figure 2: Precision (a) and stability (b) of the selected features by SVM-RFE and group-based SVM-RFE on various training sample sizes. of existing feature groups in the synthetic data sets and re-place each known feature group by its representative feature (the one closest to the group center). We then apply the SVM-RFE algorithm and the simple F -Statistic ranking to each transformed data set to selects the top 10 representa-tive features, respectively. The group-based algorithms are evaluated in the same experimental setting as SVM-RFE.
Figure 2 shows the precision and stability of the selected top 10 features by SVM-RFE and the group-based SVM-RFE across different training sample sizes. The results of group-based F -Statistic ranking are almost the same as group-based SVM-RFE, and hence are not shown in the figures. Clearly, SVM-RFE shows a strong dependency on the train-ing sample size w.r.t. successful detection of the truly rele-vant features as well as the stability of the selected features in this example. When the sample size reduces from 1000 to 100, both precision and stability curves drop sharply. In contrast, the group-based algorithm shows much less depen-dency on the sample size, especially when sample size is over 200. Moreover, the group-based algorithm consistently out-performs SVM-RFE. Such observations suggest that intrin-sic feature groups are stable under training sample variations even at small sample size, and discriminating relevant fea-tures from irrelevant ones at the group level is more effective than at the feature level on small samples.
The study in the previous section illustrates the effective-ness of feature selection based on intrinsic feature groups in the underlying data distribution in an ideal situation. In practice, it is a challenging problem to identify intrinsic fea-ture groups from a small training set due to the shortage of samples to observe feature correlation. In this section, we first review a previously proposed framework of dense group based feature selection and the DGF and DRAGS algorithms. We then describe the details of the new frame-work of consensus group based feature selection (as outlined in Figure 1) and a new algorithm under this framework.
The dense group based framework is motivated by a key observation that in the sample space, the dense core regions (peak regions), measured by probabilistic density estima-tion, are stable with respect to sampling of the dimensions (samples). For example, a spherical Gaussian distribution in the 100-dimensional space will likely be a stable spherical Gaussian in any of the subspaces. The features near the core Algorithm 1 DGF (D ense G roup F inder) Input: data D = { x i } n i =1 , kernel bandwidth h
Output: dense feature groups G 1 , G 2 , . . . , G L for i = 1 to n do end for
For every unique peak p r , add x i to G r if || p r  X  x i of the spherical Gaussian, viewed as a core group are likely to be stable under sampling, although exactly which feature is closest to the peak could vary [26].

Given a training set D composed of n features and m samples, the data matrix is transposed such that original feature vectors become data points in the new feature space defined by the original samples. Algorithm 1, DGF (Dense Group Finder) was introduced in [26] as a means to locate dense feature groups from data. DGF works by employing kernel density estimation [24] and the iterative mean shift procedure [4] on each of the features in the sample space. When the mean shift process converges, nearby features are gathered into feature groups and returned by the algorithm.
The main part of DGF is the iterative mean shift proce-dure for all features, which locates a density peak by starting the mean at a given feature x i and using other features in the local neighborhood (determined by a kernel bandwidth h ) to shift the mean to a denser location. Specifically is used to determine the sequence of successive locations of the kernel K . The algorithm has a time complexity of O (  X n 2 m ), where  X  is the number of iterations for each mean shift procedure to converge. Details of the algorithm and the choice of kernel function K and bandwidth are given in [26].
The groups found by DGF may or may not be relevant, and so these groups must be processed by a second algorithm DRAGS (Dense Relevant Attribute Group Selector) which relies on DGF to find dense feature groups and then eval-uates and ranks the relevance of each feature group based on the average relevance ( F -Statistic score) of features in each group. A representative feature (the one closest to the group center) from each selected top relevant group will be used for classification.

While the DGF and DRAGS algorithms have shown some promise w.r.t. the stability of the dense groups and the gen-eralization performance of the selected features, the dense group based framework has two major limitations. In the first step, as mentioned in the Introduction, density esti-mation can be unreliable in high-dimensional spaces. Since the dimensionality of the feature space for estimating den-sity peaks is as high as dozens or a few hundreds (i.e., the training sample size), the identified peaks are susceptible to variations of the dimensions (samples), and the stability of the identified dense groups suffer accordingly. Moreover, the overall sample size is still much smaller than the sample distribution, which can add to the instability of the groups Algorithm 2 CGS (C onsensus G roup S table Feature Selec-tion Input: data D , # of subsampling t , relevance measure  X  Output: selected relevant consensus feature groups
CG 1 , CG 2 , . . . , CG k // Identifying consensus groups for i = 1 to t do end for for every pair of features X i and X j  X  D end for
Create consensus groups CG 1 , CG 2 , . . . , CG L by perform-ing hierarchical clustering of all features based on W i,j // Feature selection based on consensus groups for i = 1 to l do end for Rank CG 1 , CG 2 , . . . , CG L according to  X ( X i )
Select top k most relevant consensus groups found under training sample variations. In the second step, the framework limits the selection of relevant groups from dense groups, and will miss some relevant features if those features are located in the relatively sparse regions. The new framework proposed next addresses these two issues.
The consensus group based framework first approximates intrinsic feature groups by a set of consensus feature groups, and then performs feature selection in the transformed fea-ture space described by consensus feature groups. We next describe each component in detail, and present a new al-gorithm CGS (Consensus Group Stable feature selection) which instantiates the proposed framework.
In practice, it is a challenging problem to identify intrinsic feature groups from a small training set due to the shortage of samples to observe feature correlation. Feature groups found on small samples can be suboptimal and instable un-der training sample variations. Our idea of approximating intrinsic feature groups by a set of consensus feature groups aggregated from multiple sets of feature groups originates from ensemble learning. It is well known that ensemble methods [2] for classification which aggregate the predic-tions of multiple classifiers can achieve better generaliza-tion than a single classifier, if the ensemble of classifiers are correct and diverse. Similar to the idea of ensemble clas-sification, ensemble clustering methods [6, 23] have also be extensively studied, which aggregate clustering results from multiple clustering algorithms or from the same clustering algorithm under data manipulation. Although finding con-sensus feature groups can be considered as ensemble feature clustering, to the best of our knowledge, this is the first time that ensemble learning is applied to identify consensus feature groups for stable feature selection.

Similar to ensemble construction in classification and clus-tering, there are two essential steps in identifying consensus feature groups: Step (1), to create an ensemble of feature grouping results, and Step (2), to aggregate the ensemble into a single set of consensus feature groups. Algorithm 2, CGS , describes the key procedure for each of the two steps. In Step (1) CGS adopts the DGF algorithm introduced be-fore as the base algorithm, and applies DGF on a number of (user-defined parameter t ) bootstrapped training sets from a given training set D . The result of this step is an ensemble of feature groupings, { G 1 1 , . . . , G 1 L G j represents the j -th feature group formed in the i th DGF run. This straightforward step has time complexity O ( t X n as the base DGF algorithm has time complexity O (  X n 2 m ).
In Step (2), it is a non-trivial issue to aggregate a given en-semble of feature groupings { G 1 1 , . . . , G 1 L into a final set of consensus groups { CG 1 , . . . , CG L CG i is a consensus group. This issue resembles the well-studied cluster ensemble problem -combining a given en-semble of clustering solutions into a final solution [6, 23]. Previously, Strehl and Ghosh [23] proposed two approaches, instance-based or cluster-based, to formulate the cluster en-semble problem. The instance-based approach models each instance as an entity and decides the similarity between each pair of instances based on how frequently they are clustered together among all clustering solutions. The cluster-based approach models each cluster in the ensemble as an entity, and decides the similarity between each pair of clusters based on the percentage of instances they share. Given the sim-ilarity matrix for all pairs of entities in either approach, a final clustering can be produced based on any hierarchical or graph clustering algorithm.

In this work, we chose the instance-based approach for the proposed CGS algorithm for two reasons. First, this approach is more efficient than the cluster-based approach w.r.t. both computation and space as the number of en-tities in the instance-based approach (i.e., the number of features) is often much smaller than the number of enti-ties in the cluster-based approach (i.e., the total number of feature groups perimental settings described in the following section. Sec-ond, our preliminary evaluation of both approaches shows that the consensus groups formed by the instance-based ap-proach are consistently more stable than the cluster-based approach. Once W i,j for every feature pairs is computed, the CGS algorithm applies agglomerative hierarchical clustering to group features into a final set of consensus feature groups. To reduce the effect of outliers, we use average linkage in deciding the similarity between two groups to be merged. The merging process continues as long as the two feature groups to be merged has a similarity value &gt; 0.5, indicat-ing, on average, the feature pairs in the resulting group are also grouped together by a majority of the DGF runs. The use of majority voting provides a natural stopping criterion for deciding the final number of feature groups. The time complexity for Step (2) is O ( n 2 t + n 2 logn ).
Consensus groups found by CGS can still be comprised of irrelevant features, so, CGS continues to identify rele-vant groups from the consensus feature groups. CGS works by first forming a representative feature for each consen-sus feature group, and then evaluates the relevance of each feature group based on its representative feature. In our implementation, we use the feature closest to the group cen-ter to represent the group. Different feature selection algo-rithms and relevance measures can be adopted in the same framework to select relevant feature groups since each group has been represented by a single entity. In this work, since our investigative emphasis is on the effectiveness of consen-sus feature groups for stable feature selection, we use the simple method of individual feature evaluation based on F -Statistic to determine the group relevance in CGS, as we did for DRAGS in [26]. However, there are two key differences between CGS and DRAGS. First, CGS relies on consensus feature groups, while DRAGS relies on dense feature groups. Second, CGS considers all consensus groups during the rel-evance selection step, while DRAGS limits the selection of relevant groups to the top dense groups. Therefore, CGS ad-dresses two key limitations of DRAGS discussed previously.
In this section, we empirically study the framework of stable feature selection based on consensus feature groups. Section 4.1 introduces stability measures, Section 4.2 de-scribes the data sets used and experimental procedures, and Section 4.3 presents results and discussion.
Evaluating the stability of feature selection algorithms re-quires some similarity measures for two sets of feature se-lection results. In our previous work [26], we introduced a general similarity measure for two sets of feature selection R 2 can be either two sets of features or two sets of feature groups. R 1 and R 2 together are modeled as a weighted bi-partite graph G = ( V, E ), with vertex partition V = R 1 and edge set E = { ( G i , G j ) | G i  X  R 1 , G j  X  R 2 } , and weight w ( G i ,G j ) associated with each pair ( G i , G j ). The overall similarity between R 1 and R 2 is defined as: where M is a maximum matching in G (i.e., a subset of non-adjacent edges in E with largest sum of weights).
Depending on how to decide w ( G two forms: Sim ID and Sim V , where the subscripts ID and respectively indicate that each weight is decided based on feature indices or feature values. When G i and G j represent feature groups, for Sim ID , each weight w ( G by the overlap between the two feature groups, For Sim V , each weight is decided by the Pearson correlation coefficient between the centers of the two feature groups. In the special case when G i and G j only contain one feature, wise; for Sim V , each weight can be simply decided by the correlation coefficient between the two individual features.
Given the similarity measure, the stability of a feature se-lection algorithm is then measured as the average similarity of various feature selection results produced by the same al-gorithm under training data variations. In [26], we used the feature selection result from a full training set as a reference to compare various results under subsampling of the full training set. Although the procedure is more efficient than pair-wise comparison among various results, the evaluation result can be biased since the individual difference between two sets of results can be greater than their difference to a reference set. We use pair-wise similarity comparison in stability calculation in this paper.
We perform our study on both synthetic data sets and real-world data sets. Besides the synthetic data set used in Section 2, we also create another data set with higher di-mensionality and larger feature groups. A summary of these data sets is provided in Table 1. To assure comparable re-sults, we follow the same procedure in generating both data sets as described in Section 2. For each data set, four differ-ent training sample sizes (100, 200, 500, and 1000) will be used to study the sample size dependency of the group-based algorithms as in Section 2. For real-world data, we use six frequently studied public microarray data sets characterized in Table 2.
 To empirically evaluate the stability and accuracy of SVM-RFE, DRAGS, and CGS on a given data set, we apply the 10 fold cross-validation procedure. For each microrray data set described above, each feature selection algorithm is re-peatedly applied to 9 out of the 10 folds, while a different fold is hold out each time. Different stability measures are calculated. In addition, a classifier is trained based on the selected features from the same training set and tested on the corresponding hold-out fold. The CV accuracies of lin-ear SVM and KNN classification algorithms are calculated. The above process is repeated 10 times for different ran-dom partitions of the data set, and the results are averaged. For each of the two synthetic data sets, we follow the same 10  X  10-fold CV procedure above with two changes. First, an independent test set of 500 samples randomly generated from the same distribution as the training set is used in replacement of the hold-out fold. Second, in addition to sta-bility and accuracy measures, we also measure the precision w.r.t. truly relevant features during each iteration of the 10  X  10 CV and obtained the average values. For each per-formance measure, two-sample paired t-tests between the best performing algorithm and the other two algorithms is used to decide the statistical significance of the difference between the two average values over the 10 random trials.
As to algorithm settings, for SVM-RFE, we eliminate 10 percent of the remaining features at each iteration. We use Weka X  X  implementation [25] of SVM (linear kernel, default C parameter) and KNN (K=1). For DRAGS, the selection of relevant groups is limited to the top 50 dense feature groups. The kernel bandwidth parameter h for the base DGF algo-rithm is set to be the average of the average distance to its K-nearest neighbors for all features. As discussed in [26], a reasonable K value should be sufficiently small in order to capture the heterogeneity of the data. In our experiments, for each synthetic data, the K value is set to be the average group size. For each microarray data set, the K value is chosen from 3 to 5 based on the stability of DGF through cross-validation. For CGS , the number of subsampling t is set to be 10.
Figure 3 compares SVM-RFE, DRAGS, and CGS algo-rithms by various performance measures on the two syn-thetic data sets D 1 k and D 5 k under increasing training sam-ple size. For SVM-RFE, the same trends as seen in Figure 2, Section 2 can be observed here on both data sets. SVM-RFE shows a strong dependency on the sample size w.r.t. suc-cessful detection of the truly relevant features (as shown in the precision figures in the left column) as well as the stabil-ity of the selected features (the Sim ID figures in the middle column). The SVM accuracies based on the selected features (right column) are consistent with the precision values.
CGS also shows the same trends as the group-based al-gorithms which perform feature selection based on repre-sentative features of the intrinsic groups in Section 2. Our initial experimental results (not included due to space limit) showed that the consensus groups identified by the ensem-ble version of the DGF algorithm approximate the intrinsic groups very well on these synthetic data sets even when the sample size is small. As a consequence, CGS which performs feature selection based on representative features of the con-sensus groups shows a much less dependency on the sample size than SVM-RFE. A close look at Figure 3 shows that the performance of CGS at sample size 200 is usually as good as SVM-RFE at over twice the sample size. Such observa-tion indicates that consensus group based feature selection is an alternative way of improving the stability of feature selection instead of increasing the sample size. It is worthy to note that in many applications, increasing the number of training samples could be impractical or very costly. For example, in gene expression microarray data, each sample is from the tissue of a cancer patient, which is usually hard to obtain and costly to perform experiments on.

The inferior performance of DRAGS may appear surpris-ing at the first look. Such performance is due to the the fact that for each data set, different feature groups have similar density, and the ratio of relevant to irrelevant groups is low (1/9 for D 1 k and 1/24 for D 5 k ). Therefore, the probabil-ity that a relevant group happens to be among the top 50 dense groups and considered by DRAGS is low. If DRAGS allowed all groups found by DGF to be considered for rel-evant group selection, its performance would be better on these data sets since the groups found by DGF are reason-ably close to the true feature groups. However, for real-world data which could have heterogenous density among various groups, the dense group based framework faces the dilemma of the tradeoff between density v.s. relevance. Allowing DRAGS to select features from low density groups may in-crease the selection accuracy but the low density groups are sensitive to training data variations. The consensus group based framework proposed in this work avoids the above dilemma; it does not limit the selection from dense groups, and it improves the stability of the resulting feature groups based on the ensemble mechanism.
Figures 4 and 5 compare SVM-RFE, DRAGS, and CGS by various performance measures on the six microarray data sets used in this study. Figures in the left column compare the stability of CGS and DRAGS w.r.t. the similarity of the selected features groups. CGS shows significantly better stability than DRAGS for all six data sets except Leukemia and Lung. This verifies the effectiveness of the ensemble mechanism of CGS at stabilizing the feature groups pro-duced by the DGF algorithm. Figures in the middle column compare the stability of CGS , DRAGS, and SVM-RFE, w.r.t. the similarity of the selected features. CGS is sig-nificantly better than DRAGS for all six data sets except Leukemia. Overall, the stability of CGS is the best among all three algorithm in comparison. Figures in the right col-umn compare the SVM accuracy of the three algorithms. CGS in general results in significantly higher accuracy than DRAGS and SVM-RFE on two data sets, Colon and SR-BCT, and comparable results in the other data sets. For all data sets, the stability trends w.r.t. Sim V measure (in Section 4.1) are consistent with those w.r.t Sim ID , and the accuracy trends from KNN are in general similar to SVM. Due to space limit, curves for Sim V and KNN accuracy are not reported.

Although CGS is computationally more costly than DRAGS and SVM-RFE, the payoff of significantly improved stabil-ity makes CGS a valuable tool for biologists who seek to identify not only highly predictive features but also stable feature groups. Such feature groups provide valuable in-sights about how relevant features are correlated, and may suggest high-potential candidates for biomarker detection.
There exist very limited studies on the stability of feature selection algorithms. An early work in this direction was done by Kalousis et al. (2007) . Their work raised the issue of feature selection stability and compared the stability of a number of conventional feature selection algorithms under training data variation based on three stability measures on high-dimensional data. More recently, two approaches were proposed to explicitly achieve stable feature selection with-out sacrificing classification accuracy: the dense group based feature selection in our previous work [26], and ensemble fea-ture selection [21]. In the later, Saeys, et al. studied ensem-ble feature selection which aggregates the feature selection results from a conventional feature selection algorithm such as SVM-RFE repeatedly applied on different bootstrapped samples of the same training set. Their results show that the stability of ensemble SVM-RFE does not improve signifi-cantly from single run of SVM-RFE. Our ensemble approach in the proposed feature selection framework is different as it applies the idea of ensemble learning to identify consen-sus feature groups instead of consensus feature rankings or feature subsets. We also evaluated ensemble SVM-RFE and observed a similar trend as in [21] in our initial study.
In this paper, we have studied the sample size dependency for stability of feature selection. We have proposed a novel consensus group based framework for stable feature selec-tion. Experiments on both synthetic and real-world data sets show that the CGS algorithm is effective at alleviat-ing the problem of small sample size, and the algorithm in general leads to more stable feature selection results and comparable or better generalization performance than two state-of-the-art feature selection algorithms, DRAGS and SVM-RFE. Future work is planned to investigate different ensemble methods for identifying consensus feature groups, for example, different ways of generating and aggregating ensemble based on DGF algorithm or other robust cluster-ing algorithms.
The authors would like to thank anonymous reviewers for their helpful comments. C. Ding is partially supported by NSF-CCF-0830780 and NSF-DMS-0844497. [1] A. Appice, M. Ceci, S. Rawles, and P. Flach.
 [2] E. Bauer and R. Kohavi. An empirical comparison of [3] X. Chen and J. C. Joeng. Minimum reference set [4] Y. Cheng. Mean shift, mode seeking, and clustering. [5] C. Ding and H. Peng. Minimum redundancy feature [6] X. Z. Fern and C. Brodley. Random projection for [7] G. Forman. An extensive empirical study of feature [8] T. R. Golub, D. K. Slonim, P. Tamayo, et al.
 [9] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [10] K. Jong, J. Mary, A. Cornuejols, E. Marchiori, and [11] A. Kalousis, J. Prados, and M. Hilario. Stability of [12] M. J. Kearns and U. V. Vazirani. An Introduction to [13] R. Kohavi and G. H. John. Wrappers for feature [14] T. Li, C. Zhang, and M. Ogihara. A comparative [15] H. Liu, J. Li, and L. Wong. A comparative study on [16] H. Liu and L. Yu. Toward integrating feature selection [17] T. M. Mitchell. Machine Learning . McGraw-Hill, 1997. [18] A. Y. Ng. On feature selection: learning with [19] M. S. Pepe, R. Etzioni, Z. Feng, et al. Phases of [20] E. F. Petricoin, A. M. Ardekani, B. A. Hitt, et al. Use [21] Y. Saeys, T. Abeel, and Y. V. Peer. Robust feature [22] L. Song, A. Smola, A. Gretton, K. M. Borgwardt, and [23] A. Strehl and J. Ghosh. Cluster ensembles -a [24] M. P. Wand and M. C. Jones. Kernel Smoothing . [25] I. H. Witten and E. Frank. Data Mining -Pracitcal [26] L. Yu, C. Ding, and S. Loscalzo. Stable feature [27] L. Yu and H. Liu. Efficient feature selection via 1 k 5 k representative features under increasing sample size. for various numbers of selected features (or groups). the selected feature groups, the stability w.r.t. Sim ID of the selected representative features, and the SVM
