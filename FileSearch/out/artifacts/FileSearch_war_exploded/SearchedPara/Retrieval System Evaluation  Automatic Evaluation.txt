 In information retrieval (IR), research aiming to reduce th e cost of retrieval system evaluations has been conducted alo ng two lines: (i) the evaluation of IR systems with reduced (i.e. incomplete) amounts of manual relevance assessments , and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The proposed methods in both areas are commonly evaluated by comparing their performance estimates for a set of sys-tems to a ground truth (provided for instance by evaluating the set of systems according to mean average precision). In contrast, in this poster we compare an automatic system evaluation approach directly to two evaluations based on incomplete manual relevance assessments. For the particu-lar case of TREC X  X  Million Query track, we show that the automatic evaluation leads to results which are highly corr e-lated to those achieved by approaches relying on incomplete manual judgments.
 Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms : Experimentation, Performance Keywords: Automatic System Evaluation
In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with re-duced amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoin g the need for manual assessments altogether. The two most important approaches in the first category are the determi-nation of good documents to assess (the MTC approach) [6] and the proposal of alternative pooling methods (the statAP approach) [4]. Both, MTC and statAP , are now accepted to the depth pooling methodology which has until recently been employed at TREC; due to the ever increasing size of test collections and query sets though, pooling the top 100 documents of each retrieval run participating in a bench-mark and assessing those documents manually for their rel-evance, has become infeasible. The earliest method for a fully automatic evaluation was proposed by Soboroff et al. http://trec.nist.gov/ (the RS approach) [7]. It relies on drawing random samples from the pool of top retrieved documents.

The quality of statAP , MTC and RS is usually evaluated by comparing the performances of a set of retrieval runs for which sufficient relevance judgments are available accordin g to a standard effectiveness metric (mean average precision) with the estimated system performances. Generally missing though is a direct comparison between statAP / MTC and an automatic method such as RS .

In recent work [3], we found the commonly reported prob-lem of automatic evaluation approaches (the severe mis-ranking of the very best retrieval runs [5]) not to be inheren t to automatic system evaluation methods. The extent of this problem is strongly related to the degree of human inter-vention in the best retrieval runs: the larger the amount of human intervention, the less able automatic approaches are to identify the best runs correctly.

In this poster, we turn to investigating how closely the au-tomatic evaluation of retrieval runs approximates the eval -uation with incomplete manual relevance assessments. We perform this analysis in a setting which favors automatic evaluation: TREC X  X  Million Query tracks of 2007 (MQ-2007) [2] and 2008 (MQ-2008) [1]. Due to the size of the query sets, creating retrieval runs with a great amount of human intervention is virtually impossible. We thus expect the RS approach to lead to similar estimates of system per-formances as statAP and MTC respectively. If this would indeed be the case, it would bring into question the need for manual assessments in this type of setting.
For our experiments, we relied on the twenty-nine retrieval mitted to MQ-2008. Both sets of retrieval runs as well as their retrieval effectiveness scores according to statAP and MTC are available from the TREC website. Specifically, for MQ-2007, TREC provides the statAP measures 3 , while for MQ-2008 both, MTC and statAP , are provided. Of the 10000 queries that were released for each year, 1153 (MQ-2007) and 564 (MQ-2008) queries respectively have valid statAP measurements; 784 (MQ-2008) queries have valid MTC measurements. These are the queries we also rely on in the RS approach. from the TREC website and thus had to be ignored. site. Table 1: Kendall X  X  Tau rank correlation coeffi-cient between the automatic RS approach and statAP / MTC respectively. All correlations are sig-nificant ( p &lt; 0 . 01 ). Column 2 contains the average number of sampled documents from the pool.

For the automatic evaluation, we implemented the ran-dom sampling approach [7]: first, the top p retrieved docu-ments of all retrieval runs for a particular query are pooled together such that a document that is retrieved by x runs, appears x times in the pool. Then, a number m of docu-ments are drawn at random from the pool; those are now considered to be the pseudo relevant documents. This pro-cess is performed for each query and the subsequent evalua-tion of each system is performed with pseudo relevance judg-ments instead of relevance judgments. Due to the random-ness of the sampling, we performed 20 trials per query and averaged the pseudo relevance based system performance. We fixed the number m of documents to sample 5% of the number of unique documents in the pool and evaluated pool depths of p = { 10 , 50 , 100 , 250 } .

In Table 1 (column 3) we report the rank correlation coef-ficient Kendall X  X  Tau (  X  ) between the performance scores es-timated by the automatic RS approach and the performance scores estimated by statAP / MTC which exploit manual rel-evance assessments. In the ideal case,  X  = 1 . 0, that is, RS leads to the same rank estimate of system performances as statAP / MTC . It is apparent, that although the correlations are not perfect, the correlation coefficients are consistent ly high; in the worst instance the correlation reaches  X  = 0 for MQ-2007 statAP and a pool depth of p = 250; at best the correlation reaches  X  = 0 . 84 for MQ-2008 statAP and p = 250.

Figures 1 and 2 show scatter plots of MQ-2007 statAP scores versus RS scores and of MQ-2008 MTC scores versus RS scores respectively. It is evident that the best retrieval runs as identified by statAP / MTC are also identified cor-rectly by the automatic RS approach.
In this poster, we investigated the ability of an automatic system evaluation approach ( RS [7]) to approximate the system performance estimates as derived by two evaluation methods that rely on manually derived incomplete relevance judgments: statAP and MTC . Experiments on TREC X  X  Mil-Figure 1: MQ-2007 statAP scores (x-axis) versus RS scores (y-axis) for a pool depth of p = 10 . Figure 2: MQ-2008 MTC scores (x-axis) versus RS scores (y-axis) for a pool depth of p = 250 . lion Query tracks showed that RS is highly correlated to statAP and MTC , an outcome which implies that retrieval runs, which are automatic in nature, can be evaluated by an automatic approach such as RS which requires no manual assessments at all.

One direction of future work will be the adaptation of RS to further improve the method X  X  correlation with statAP and MTC by for instance taking advantage of the relationship between queries of a query set (as is possible for larger sets of queries) in contrast to the current approach where each query is viewed in isolation.
