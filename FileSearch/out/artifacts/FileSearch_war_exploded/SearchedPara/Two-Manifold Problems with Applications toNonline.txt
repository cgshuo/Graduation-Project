 Byron Boots beb@cs.cmu.edu Geoffrey J. Gordon ggordon@cs.cmu.edu Manifold learning algorithms are nonlinear meth-ods for embedding a set of data points into a low-dimensional space while preserving local geom-etry. Recently, there has been a great deal of interest in spectral approaches to learning man-ifolds. These kernel eigenmap methods include Isomap (Tenenbaum et al., 2000), Locally Linear Em-bedding (LLE) (Roweis &amp; Saul, 2000), Laplacian Eigenmaps (LE) (Belkin &amp; Niyogi, 2002), Maximum Variance Unfolding (MVU) (Weinberger et al., 2004), and Maximum Entropy Unfolding (MEU) (Lawrence, 2011). Despite the popularity of kernel eigenmap methods, they are limited in one important respect: they generally only perform well when there is little or no noise . Several authors have attacked the prob-lem of learning manifolds in the presence of noise us-ing methods like neighborhood smoothing (Chen et al., 2008) and robust principal components analysis (Zhan &amp; Yin, 2009; 2011), with some success when noise is limited. Unfortunately, the problem is fundamentally ill posed without some sort of side information about the true underlying signal: by design, manifold meth-ods will recover extra latent dimensions which  X  X x-plain X  the noise.
 We take a different approach to the problem of learning manifolds from noisy observations. We assume access to instrumental variables , which are correlated with the true latent variables, but uncorrelated with the noise in observations. Such instrumental variables can be used to separate signal from noise, as described in Section 3. Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear re-gression (Pearl, 2000), principal component analysis (PCA) (Jolliffe, 2002), and temporal difference learn-ing (Bradtke &amp; Barto, 1996). Here we extend the scope of this technique to manifold learning. We will pay particular attention to the two-manifold problem , in which two sets of observations each serve as instru-ments for the other. We propose algorithms for two-manifold problems based on spectral decompositions related to cross-covariance operators ; and, we show that the instrumental variable idea suppresses noise in practice.
 Finally we look at a detailed example of how two-manifold methods can help solve difficult machine learning problems. Subspace identification approaches to learning nonlinear dynamical systems depend crit-ically on instrumental variables and the spectral de-composition of (potentially infinite-dimensional) co-variance operators (Hsu et al., 2009; Boots et al., 2010; Song et al., 2010). Two-manifold problems are a natural fit: by relating the spectral decomposition to our two-manifold method, subspace identification techniques can be forced to identify a manifold state space, and consequently, to learn a dynamical system that is both accurate and interpretable, outperforming the current state of the art. 2.1. Kernel PCA Kernel PCA (Sch  X olkopf et al., 1998) generalizes PCA to high-or infinite-dimensional input data, represented implicitly using a reproducing-kernel Hilbert space (RKHS). If the kernel K ( x , x 0 ) is sufficiently expres-sive, kernel PCA can find structure that regular PCA misses. Conceptually, if we write the  X  X eature func-tion X   X  ( x ) = K ( x ,  X  ), and define an infinitely-tall  X  X a-trix X   X  with columns  X  ( x i ), our goal is to recover the eigenvalues and eigenvectors of the centered covariance operator  X   X  XX = 1 n  X H X  T . Here H is the centering matrix H = I n  X  1 n 11 T . For efficient computation, we work with the Gram matrix G = 1 n  X  T  X  instead of the large or infinite  X   X  XX . The centered Gram ma-trix C = HGH has the same nonzero eigenvalues as  X   X 
XX ; the eigenvectors of (  X  i , v i ) are the eigenpairs of C (Sch  X olkopf et al., 1998). 2.2. Manifold Learning Kernel eigenmap methods seek a nonlinear function that maps a high-dimensional set of data points to a lower-dimensional space while preserving the mani-fold on which the data lies. The main insight behind these methods is that large distances in input space are often meaningless due to the large-scale curvature of the manifold; so, ignoring these distances can lead to a significant improvement in dimensionality reduction by  X  X nfolding X  the manifold.
 Interestingly, these algorithms can be viewed as spe-cial cases of kernel PCA where the Gram matrix G is constructed over the finite domain of the training data in a particular way (Ham et al., 2003). For example, in Laplacian Eigenmaps (LE), we first compute an adja-cency matrix W by nearest neighbors: w i,j is nonzero if point i is one of the nearest neighbors of point j , or vice versa. We can either set non-zero weights to 1, or compute them with a kernel such as a Gaussian RBF. Next we let S i,i = P j w i,j , and set L = ( S  X  W ). Fi-nally, we eigendecompose L = V X V T and set the LE embedding to be E = V 2: k +1 , the k smallest eigen-vectors of L excluding the vector corresponding to the 0 eigenvalue. (Optionally we can scale according to nel PCA, Ham et al. (2003) showed that one can build a Gram matrix from L , G = L  X  ; the LE embedding is given by the top k eigenvectors (and optionally eigen-values) of G .
 In general, many manifold learning methods (including all the kernel eigenmap methods mentioned above) can be viewed as constructing a matrix E of embedding coordinates. From any such method, we can extract an equivalent Gram matrix G = EE T . So, for the rest of the paper, we view a manifold learner simply as a black box which accepts data and produces a Gram matrix that encodes the learned manifold structure. This view greatly simplifies the description of our two-manifold algorithms below. Kernel eigenmap methods are very good at dimension-ality reduction when the original data points sample a high-dimensional manifold relatively densely, and when the noise in each sample is small compared to the local curvature of the manifold. In practice, how-ever, observations are frequently noisy, and manifold-learning algorithms applied to these datasets usually produce biased embeddings. See Figures 1 X 2, the  X  X oisy swiss rolls, X  for an example.
 Our goal is therefore to design a more noise-resistant algorithm for the two-manifold problem. We begin by examining PCA, a linear special case of manifold learning, and studying why it produces biased embed-dings in the presence of noise. We next show how to overcome this problem in the linear case, and then use these same ideas to generalize kernel PCA, a nonlinear algorithm. Finally, in Sec. 4, we extend these ideas to fully general kernel eigenmap methods. 3.1. Bias in Finite-Dimensional Linear Models Suppose that x i is a noisy view of some underlying low-dimensional latent variable z i : x i = Mz i + i for a linear transformation M and i.i.d. zero-mean noise term i . 1 Without loss of generality, we assume that x i and z i are centered, and that Cov[ z i ] and M both have full column rank. In this case, PCA on X will generally fail to recover Z : the expectation of  X   X  XX = 1 n XX T M Cov[ z i ] M T + Cov[ i ], while we need M Cov[ z i ] M to be able to recover a transformation of M or Z . The unwanted term Cov[ i ] will, in general, affect all eigen-values and eigenvectors of  X   X  XX , causing us to recover a biased answer even in the limit of infinite data. 3.1.1. Instrumental Variables We can fix this problem for linear embeddings: in-stead of plain PCA, we can use what might be called two-subspace PCA. This method finds a statistically consistent solution through the use of an instrumen-tal variable (Pearl, 2000; Jolliffe, 2002), an observation y i that is correlated with the true latent variables, but uncorrelated with the noise in x i . Importantly, picking an instrumental variable is not merely a statistical aid, but rather a value judgement about the nature of the latent variable and the noise in the observations. We are defining the noise to be that part of the variability which is uncorrelated with the instrumental variable, and the signal to be that part which is correlated. In our example above, a good instrumental variable y i is a different (noisy) view of the same underlying low-dimensional latent variable: y i = Nz i +  X  i for some full-column-rank linear transformation N and i.i.d. zero-mean noise term  X  i . The expectation of the empirical cross covariance  X   X  XY = 1 n XY T is then M Cov( z i ) N T : the noise terms, being independent and zero-mean, cancel out. (And the variance of each element of  X   X  XY goes to 0 as n  X  X  X  .) Now, we can identify the embedding by computing the truncated singular value decomposition (SVD) of the covariance:  X  U , D , V  X  = SVD(  X   X  XY ,k ). If we set k to be the true dimension of z , then as n  X   X  , U will converge to an orthonormal basis for the range of M , and V will converge to an orthonormal basis for the range of N . The corresponding embeddings are then given by U T X and V T Y . 2 Interestingly, we can equally well view x i as an instrumental variable for y i we simultaneously find consistent embeddings of both x i and y i , using each to unbias the other. 3.2. Bias in Nonlinear Models We now extend the analysis of Section 3.1 to nonlinear models. We assume noisy observations x i = f ( z i ) + i where z i is the desired low-dimensional latent variable, i is an i.i.d. noise term, and f is a smooth function with smooth inverse (so that f ( z i ) lies on a manifold). Our goal is to recover f and z i up to identifiability. Kernel PCA (Sec. 2.1) is a common approach to this problem. In the (restrictive) realizable case, kernel PCA gets the right answer: that is, suppose that z has dimension k , that i has zero variance, and that we have at least k independent samples. And, suppose that  X  ( f ( z )) is a linear function of z . Then, the Gram matrix or the covariance  X  X atrix X  will have rank k , and we can reconstruct a basis for the range of  X   X  f from the top k eigenvectors of the Gram matrix. (Similarly, if  X   X  f is near linear and the variance of is small, we can expect kernel PCA to work well, if not perfectly.) However, just as PCA recovers a biased answer when the variance of i is nonzero, kernel PCA will also re-cover a biased answer under noise, even in the limit of infinite data. The bias of kernel PCA follows immedi-ately from the example at the beginning of Section 3: if we use a linear kernel, kernel PCA will simply re-produce the bias of ordinary PCA. 3.2.1.Instrumental Variables in Hilbert Space By analogy to two-subspace PCA, a natural gener-alization of kernel PCA is two-subspace kernel PCA , which we can accomplish via a kernelized SVD of a cross-covariance operator in Hilbert space. Given a joint distribution P [ X,Y ] over two variables X on X and Y on Y , with feature maps  X  and  X  (corresponding to kernels K x and K y ), the cross-covariance operator  X 
XY is E [  X  ( x )  X   X  ( y )]. The cross-covariance opera-tor reduces to an ordinary cross-covariance matrix in the finite-dimensional case; in the infinite-dimensional case, it can be viewed as a kernel mean map de-scriptor (Smola et al., 2007) for the joint distribution P [ X,Y ]. The concept of a cross-covariance operator allows us to extend the methods of instrumental vari-ables to infinite dimensional RKHSs. In our example above, a good instrumental variable y i is a different (noisy) view of the same underlying latent variable: y i = g ( z i ) +  X  i for some smoothly invertible function g and i.i.d. zero-mean noise term  X  i . 3.2.2. Two-subspace PCA in RKHSs We proceed now to derive the kernel SVD for a cross-covariance operator. 3 Conceptually, our inputs are  X  X atrices X   X  and  X  whose columns are respectively  X  ( x i ) and  X  ( y i ). The centered empirical covariance operator is then  X   X  XY = 1 n (  X H )(  X H ) T . The goal of the kernel SVD is then to factor  X   X  XY so that we can recover the desired bases for  X  ( x i ) and  X  ( y i ). Un-fortunately, this conceptual algorithm is impractical, since  X   X  XY can be high-or infinite-dimensional. So, instead, we perform an SVD on the covariance oper-ator in Hilbert space via a trick analogous to kernel PCA.
 To understand SVD in general Hilbert spaces, we start by looking at a Gram matrix formulation of finite di-mensional SVD. Recall that the singular values of  X   X 
XY = 1 n ( XH )( YH ) T are the square roots of the eigenvalues of  X   X  XY  X   X  Y X (where  X   X  Y X =  X   X  T XY ), and the left singular vectors are defined to be the corre-sponding eigenvectors. We can find identical eigenvec-tors and eigenvalues using only centered Gram matri-ces C X = 1 n ( XH ) T ( XH ) and C Y = 1 n ( YH ) T ( YH ). Let v i be a right eigenvector of C Y C X , so that C
Y C X v i =  X  i v i . Premultiplying by ( XH ) yields and regrouping terms gives us  X   X  XY  X   X  Y X w i =  X  i where w i = ( XH ) v i . So,  X  i is an eigenvalue of  X   X  ( XH ) v i  X   X  1 / 2 i is the corresponding unit length left sin-gular vector. An analogous argument shows that, if w i is a unit-length right singular vector of w i = ( YH ) v eigenvector of C Y C X .
 This machinery allows us to solve the two-subspace kernel PCA problem by computing the singular val-ues of the empirical covariance operator  X   X  XY . We define G X and G Y to be the Gram matrices whose and then compute the eigendecomposition of C Y C X = ( HG Y H )( HG X H ). This method avoids any compu-tations in infinite-dimensional spaces; and, it gives us compact representations of the left and right singular vectors.
 Under appropriate assumptions, we can show that the SVD of the empirical cross-covariance operator  X   X 
XY = 1 n  X H X  T converges to the desired value. Sup-pose that E [  X  ( x i ) | z i ] is a linear function of z of z i . 4 The noise terms  X  ( x i )  X  E [  X  ( x i ) | z  X  ( y i )  X  E [  X  ( y i ) | z i ] are by definition zero-mean; and they are independent of each other, since the first de-pends only on i and the second only on  X  i . So, the noise terms cancel out, and the expectation of  X   X  is the true covariance  X  XY . If we additionally as-sume that the noise terms have finite variance, the product-RKHS norm of the error  X   X  XY  X   X  XY van-ishes as n  X  X  X  .
 The remainder of the proof follows from the proof of Theorem 1 in (Song et al., 2010) (the convergence of the empirical estimator of the kernel covariance oper-ator). In particular, the top k left singular vectors of  X   X 
XY converge to a basis for the range of E [  X  ( x i ) | z (considered as a function of z i ); similarly, the top right singular vectors of  X   X  XY converge to a basis for the range of E [  X  ( y i ) | z i ]. Now that we have extended the instrumental variable idea to RKHSs, we can also expand the scope of mani-fold learning to two-manifold problems , where we want to simultaneously learn two manifolds for two covary-ing lists of observations, each corrupted by uncorre-lated noise. 5 The idea is simple: we view manifold learners as constructing Gram matrices as in Sec. 2.2, then apply the RKHS instrumental variable idea of Sec. 3. As we will see, this procedure allows us to regain good performance when observations are noisy. Suppose we are given two set of observations residing on (or near) two different manifolds: x 1 ,..., x n  X  R d on M X and y 1 ,..., y n  X  R d 2 on M Y . Further sup-pose that both x i and y i are noisy functions of a latent Algorithm 1 Instrumental Eigenmaps In : n i.i.d. pairs of observations { x i , y i } n i =1 Out : embeddings E X and E Y 1: Compute Gram matrices: G X and G Y from x 1: n 2: Compute centered Gram matrices: 3: Perform a singular value decomposition and trun-4: Find the embeddings from the singular values: variable z i , itself residing on a latent k -dimensional manifold M Z : x i = f ( z i ) + i and y i = g ( z i ) +  X  assume that the functions f and g are smooth, so that f ( z i ) and g ( z i ) trace out submanifolds f ( M Z )  X  X  and g ( M Z )  X  X  Y . We further assume that the noise terms i and  X  i move x i and y i within their respective manifolds M X and M Y : this assumption is without loss of generality, since we can can always increase the dimension of the manifolds M X and M Y to allow an arbitrary noise term. See Figure 1 for an example. If the variance of the noise terms i and  X  i is too high, or if M X and M Y are higher-dimensional than the latent M Z manifold (i.e., if the noise terms move x i and y i away from f ( M Z ) and g ( M Z )), then it may be difficult to reconstruct f ( M Z ) or g ( M Z ) separately from x i or y i . Our goal, therefore, is to use x y i together to reconstruct both manifolds simultane-ously: the extra information from the correspondence between x i and y i will make up for noise, allowing suc-cess in the two-manifold problem where the individual one-manifold problems are intractable.
 Given samples of n i.i.d. pairs { x i , y i } n i =1 from two manifolds, we propose a two-step spectral learning al-gorithm for two-manifold problems: first, use either a given kernel or an ordinary one-manifold algorithm such as LE or LLE to compute centered Gram matri-ces C X and C Y from x i and y i separately. Second, use kernel SVD to recover the embedding of points in M
Z . The procedure, called instrumental eigenmaps, is summarized in Algorithm 1.
 As shown in Figure 2, computing eigenvalues of C X C Y instead of C X or C Y alone alters the eigensystem: it promotes directions within each individual learned manifold that are useful for predicting coordinates on the other learned manifold, and demotes directions that are not useful. This effect strengthens our ability to recover relevant dimensions in the face of noise. A fundamental problem in machine learning and robotics is dynamical system identification. This task requires two related subtasks: 1) learning a low di-mensional state space, which is often known to lie on a manifold ; and 2) learning the system dynamics. We propose tackling this problem by combining two-manifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots &amp; Gordon, 2010; Hsu et al., 2009; Boots et al., 2010). Here, we focus on a specific example: we show how to combine HSE-HMMs (Song et al., 2010), a powerful nonparametric approach to system identification, with manifold learning. We demon-strate that the resulting manifold HSE-HMM can out-perform standard HSE-HMMs (and many other well-known methods for learning dynamical systems): the manifold HSE-HMM accurately discovers a curved low-dimensional manifold which contains the state space, while other methods discover only a (poten-tially much higher-dimensional) subspace which con-tains this manifold. 5.1. Hilbert Space Embeddings of HMMs The key idea behind spectral learning of dynamical systems is that a good latent state is one that lets us predict the future. HSE-HMMs implement this idea by finding a low-dimensional embedding of the condi-tional probability distribution of sequences of future observations, and using the embedding coordinates as state. Song et al. (2010) suggest finding this low-dimensional state space as a subspace of an infinite dimensional RKHS.
 Intuitively, we might think that we could find the best state space by performing PCA or kernel PCA of se-quences of future observations. That is, we would sam-ple n sequences of future observations x 1 ,..., x n  X  R d from a dynamical system. We would then construct a Gram matrix G X , whose ( i,j ) element is K x ( x i , x j Finally, we would find the eigendecomposition of the centered Gram matrix C X = HG X H as in Section 2.1. The resulting embedding coordinates would be tuned to predict future observations well, and so could be viewed as a good state space. However, the state space found by kernel PCA is biased : it typically includes noise , information that cannot be predicted from past observations. We would like instead to find a low di-mensional state space that is uncorrelated with the noise in the future observations.
 So, in addition to sampling sequences of future ob-servations, we sample corresponding sequences of past observations y 1 ,..., y n  X  R d 2 : sequence y i ends at time t i  X  1. We view features of the past as instru-mental variables to unbias the future. We therefore construct a Gram matrix G Y , whose ( i,j ) element is K y ( y i , y j ). From G Y we construct the centered Gram matrix C Y = HG Y H . Finally, we identify the state space using a kernel SVD as in Section 3.2.2:  X  U ,  X  , V T  X  = SVD( C X C Y ,k ). The left singular  X  X ec-tors X  (reconstructed from U as in Section 3.2.2) now identify a subspace in which the system evolves. From this subspace, we can proceed to identify the parame-ters of the system as in Song et al. (2010). 5.2. Manifold HSE-HMMs In contrast with ordinary HSE-HMMs, we are inter-ested in modeling a dynamical system whose state space lies on a low-dimensional manifold, even if this manifold is curved to occupy a higher-dimensional sub-space (an example is given in Section 5.3, below). We want to use this additional knowledge to constrain the learning algorithm and produce a more accurate model for a given amount of training data. To do so, we replace the kernel SVD by a two-manifold method. That is, we learn centered Gram matrices C X and C Y for the future and past observations, using a manifold method like LE or LLE (see Section 2.2). Then we apply a SVD to C X C Y in order to recover the latent state space. 5.3. Slotcar: A Real-World Dynamical System To evaluate two-manifold HSE-HMMs we look at the problem of tracking and predicting the position of a slotcar with attached inertial measurement unit (IMU) racing around a track. Figure 3(A) shows setup. We collected 3000 successive observations of 3D accel-eration and angular velocity at 10 Hz while the slot car circled the track controlled by a constant policy (with varying speeds). The goal was to learn a dynam-ical model of the noisy IMU data, and, after filtering, to predict current and future 2-dimensional locations. We used the first 2000 data points as training data, and held out the last 500 data points for testing the learned models. We trained four models, and evalu-ated these models based on prediction accuracy, and, where appropriate, the learned latent state.
 First, we trained a 20-dimensional embedded HMM with the spectral algorithm of Song et al. (2010), using sequences of 150 consecutive observations and Gaus-sian RBF kernels. Second, we trained a similar 20-dimensional embedded HMM with normalized LE ker-nels. (Normalized LE differs from LE by utilizing the normalized graph Laplacian instead of the standard graph Laplacian.) The number of nearest neighbors was selected to be 50, and the other parameters were set to be identical to the first model. (So, the only dif-ference is that the first model performs a kernel SVD, while the second model solves a two-manifold prob-lem.) Third, we trained a 20-dimensional Kalman fil-ter using the N4SID algorithm (Van Overschee &amp; De Moor, 1996) with Hankel matrices of 150 time steps; and finally, we learned a 20-state HMM (with 400 lev-els of discretization for observations) via the EM algo-rithm.
 We compared the learned state spaces of the first three models. These models differ mainly in their kernel: Gaussian RBF, learned manifold from normalized LE, or linear. As a test, we tried to reconstruct the 2-dimensional locations of the car (collected from an overhead camera, and not used in learning the dynam-ical systems) from each of the three latent state spaces: the more accurate the learned state space, the better we expect to be able to reconstruct the locations. Re-sults are shown in Figure 3(B).
 Finally we examined the prediction accuracy of each model. We performed filtering for different extents t = 100 ,..., 350, then predicted the car location for a further t 2 steps in the future, for t 2 = 1 ,..., 100. The root-mean-squared error of this prediction in the 2-dimensional location space is plotted in Figure 3(C). The Manifold HMM learned by the method detailed in Section 5.2 consistently yields lower prediction error for the duration of the prediction horizon. While preparing this manuscript, we learned of the simultaneous and independent work of Mahadevan et al. (2011). That paper defines one particular two-manifold algorithm, maximum covariance unfolding (MCU). We believe the current paper will help to elu-cidate why two-manifold methods like MCU work well. A similar problem to the two-manifold problem is manifold alignment (Ham et al., 2005; Wang &amp; Ma-hadevan, 2009), which builds connections between two or more data sets by aligning their underlying mani-folds. Our aim is different: we assume paired data, where manifold alignments do not; and, we focus on learning algorithms that simultaneously discover man-ifold structure and connections between manifolds (as provided by, e.g., a top-level learning problem defined between two manifolds).
 Interconnected dimensionality reduction has been explored before in sufficient dimension reduction (SDR) (Li, 1991; Cook &amp; Yin, 2001; Fukumizu et al., 2004). In SDR, the goal is to find a linear subspace of covariates x i that makes response vectors y i condi-tionally independent of the x i s. The formulation is in terms of conditional independence. Unfortunately, the solution to SDR problems usually requires a difficult nonlinear non-convex optimization. A related method is manifold kernel dimension reduction (Nilsson et al., 2007), which finds an embedding of covariates x i using a kernel eigenmap method, and then attempts to find a linear transformation of some of the dimensions of the embedded points to predict response variables y i . The response variables are constrained to be linear in the manifold, so the problem is quite different from a two-manifold problem. In this paper we study two-manifold problems , where two sets of corresponding data points, generated from a single latent manifold and corrupted by noise, lie on or near two different higher dimensional manifolds. We design algorithms by relating two-manifold problems to cross-covariance operators in RKHSs, and show that these algorithms result in a significant improve-ment over standard manifold learning approaches in the presence of noise. This is an appealing result: manifold learning algorithms typically assume that ob-servations are (close to) noiseless, an assumption that is rarely satisfied in practice.
 Furthermore, we demonstrate the utility of two-manifold problems by extending a recent dynamical system identification algorithm to learn a system with a state space that lies on a manifold. The resulting al-gorithm learns a model that outperforms the current state of the art in predictive accuracy. To our knowl-edge this is the first combination of system identifica-tion and manifold learning that accurately identifies a latent time series manifold and is competitive with the best system identification algorithms at learning accurate predictive models.
 Acknowledgements Byron Boots and Geoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052. By-ron Boots was supported by the NSF under grant num-ber EEEC-0540865.

