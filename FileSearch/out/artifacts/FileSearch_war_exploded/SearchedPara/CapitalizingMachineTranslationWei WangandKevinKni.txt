 Capitalization is the process of reco vering case in-formation for texts in lowercase. It is also called truecasing (Lita et al., 2003). Usually , capitalization itself tries to impro ve the legibility of texts. It, how-ever, can affect the word choice or order when inter -acting with other models. In natural language pro-cessing, a good capitalization model has been sho wn useful for tasks lik e name entity recognition, auto-matic content extraction, speech recognition, mod-ern word processors, and machine translation (MT).
Capitalization can be vie wed as a sequence la-beling process. The input to this process is a sen-tence in lowercase. For each lowercased word in the input sentence, we have several available cap-italization tags: initial capital (IU), all uppercase (AU), all lowercase (AL), mix ed case (MX), and all having no case (AN). The output of capital-ization is a capitalization tag sequence. Associ-ating a tag in the output with the corresponding lowercased word in the input results in a surf ace form of the word. For example, we can tag the input sentence  X  X lick ok to save your changes to /home/doc.  X  into  X  X lick IU ok AU to AL save AL your AL changes AL to AL /home/doc MX . AN X , getting the surf ace form  X  X lick OK to save your changes to /home/DOC . X .

A capitalizer is a tagger that reco vers the capi-talization tag for each input lowercased word, out-putting a well-capitalized sentence. Since each low-ercased word can have more than one tag, and as-sociating a tag with a lowercased word can result in more than one surf ace form (e.g., /home/doc MX can be either /home/DOC or /home/Doc), we need a capitalization model to solv e the capitalization am-biguities. For example, Lita et al. (2003) use a tri-gram language model estimated from a corpus with case information; Chelba and Acero (2004) use a maximum entrop y Mark ov model (MEMM) com-bining features involving words and their cases.
Capitalization models presented in most pre vi-ous approaches are monolingual because the models are estimated only from monolingual texts. Ho w-ever, for capitalizing machine translation outputs, using only monolingual capitalization models is not enough. For example, if the sentence  X  X lick ok to save your changes to /home/doc . X  in the abo ve example is the translation of the French sentence  X  CA TIONS DANS /HOME/DOC . X , the correct capitaliza-tion result should probably be  X  CLICK OK TO SA VE are in all upper -case. Without looking into the case of the MT input, we can hardly get the correct capi-talization result.

Although monolingual capitalization models in pre vious work can apply to MT output, a bilingual model is more desirable. This is because MT out-puts usually strongly preserv e case from the input, and because monolingual capitalization models do not always perform as well on badly translated text as on well-formed syntactic texts.

In this paper , we present a bilingual capitalization model for capitalizing machine translation outputs using conditional random fields (CRFs) (Laf ferty et al., 2001). This model exploits case information from both the input sentence (source) and the out-put sentence (tar get) of the MT system. We define a series of feature functions to incorporate capitaliza-tion kno wledge into the model.

Experimental results are sho wn in terms of BLEU scores of a phrase-based SMT system with the cap-italization model incorporated, and in terms of cap-italization precision. Experiments are performed on both French and English tar geted MT systems with lar ge-scale training data. Our experimental re-sults sho w that the CRF-based bilingual capitaliza-tion model performs better than a strong baseline capitalizer that uses a trigram language model. A simple capitalizer is the 1 -gram tagger: the case of a word is always the most frequent one observ ed in training data, with the exception that the sentence-initial word is always capitalized. A 1 -gram capital-izer is usually used as a baseline for capitalization experiments (Lita et al., 2003; Kim and Woodland, 2004; Chelba and Acero, 2004).

Lita et al. (2003) vie w capitalization as a lexi-cal ambiguity resolution problem, where the lexi-cal choices for each lowercased word happen to be its dif ferent surf ace forms. For a lowercased sen-tence e , a trigram language model is used to find the best capitalization tag sequence T that maximizes p ( T, e ) = p ( E ) , resulting in a case-sensiti ve sen-tence E . Besides local trigrams, sentence-le vel conte xts lik e sentence-initial position are emplo yed as well.

Chelba and Acero (2004) frame capitalization as a sequence labeling problem, where, for each low-ercased sentence e , the y find the label sequence T that maximizes p ( T j e ) . The y use a maximum en-trop y Mark ov model (MEMM) to combine features of words, cases and conte xt (i.e., tag transitions).
Gale et al. (1994) report good results on capital-izing 100 words. Mikhee v (1999) performs capital-ization using simple positional heuristics. Translation and capitalization are usually performed in two successi ve steps because remo ving case infor -mation from the training of translation models sub-stantially reduces both the source and tar get vocab u-lary sizes. Smaller vocab ularies lead to a smaller translation model with fewer parameters to learn. For example, if we do not remo ve the case informa-tion, we will have to deal with at least nine prob-abilities for the English-French word pair (click, cliquez). This is because either  X  X lick X  or  X  X liquez X  can have at least three tags (IU, AL, AU), and thus three surf ace forms. A smaller translation model re-quires less training data, and can be estimated more accurately than otherwise from the same amount of training data. A smaller translation model also means less memory usage.

Most statistical MT systems emplo y the monolin-gual capitalization scheme as sho wn in Figure 1. In this scheme, the translation model and the tar get lan-guage model are trained from the lowercased cor -pora. The capitalization model is trained from the case-sensiti ve tar get corpus. In decoding, we first turn input into lowercase, then use the decoder to generate the lowercased translation, and finally ap-ply the capitalization model to reco ver the case of the decoding output.

The monolingual capitalization scheme mak es man y errors as sho wn in Table 1. Each cell in the table contains the MT -input and the MT -output. These errors are due to the capitalizer does not have access to the source sentence.

Re gardless, estimating mix ed-cased translation models, howe ver, is a very interesting topic and worth future study . 4.1 The Model Our probabilistic bilingual capitalization model ex-ploits case information from both the input sentence to the MT system and the output sentence from the system (see Figure 2). An MT system translates a capitalized sentence F into a lowercased sentence e . A statistical MT system can also pro vide the align-ment A between the input F and the output e ; for example, a statistical phrase-based MT system could pro vide the phrase boundaries in F and e , and also the alignment between the phrases. 1
The bilingual capitalization algorithm reco vers the capitalized sentence E from e , according to the input sentence F , and the alignment A . Formally , we look for the best capitalized sentence E  X  such that where GEN ( e ) is a function returning the set of possible capitalized sentences consistent with e . No-tice that e does not appear in p ( E j F, A ) because we can uniquely obtain e from E . p ( E j F, A ) is the cap-italization model of concern in this paper . 2
To further decompose the capitalization model p ( E j F, A ) , we mak e some assumptions. As sho wn in Figure 3, input sentence F , capitalized output E , and their alignment can be vie wed as a graph. Ver-tices of the graph correspond to words in F and E . An edge connecting a word in F and a word in E corresponds to a word alignment. An edge between two words in E represents the dependenc y between them captured by monolingual n -gram lan-guage models. We also assume that both E and F have phrase boundaries available (denoted by the square brack ets), and that A is the phrase alignment. In Figure 3,  X  F phrase of E , and the y align to each other . We do not require a word alignment; instead we find it reason-able to think that a word in  X  E word in  X  F graph is a Conditional Random Field. Therefore, it is natural to formulate the bilingual capitalization model using CRFs: 3 where f i ( E, F, A ) , i = 1 ...I are the I features, and  X  = (  X  1 , ...,  X  I ) is the feature weight vector . Based on this capitalization model, the decoder in the cap-italizer looks for the best E  X  such that 4.2 Parameter Estimation Follo wing Roark et al. (2004), Laf ferty et al. (2001) and Chen and Rosenfeld (1999), we are looking for the set of feature weights  X  maximizing the regu-larized log-lik elihood LL
The second term at the right-hand side of For-mula 5 is a zero-mean Gaussian prior on the pa-rameters.  X  is the variance of the Gaussian prior dictating the cost of feature weights mo ving away from the mean  X  a smaller value of  X  keeps feature weights closer to the mean.  X  can be determined by linear search on development data. 4 The use of the Gaussian prior term in the objecti ve function has been found effecti ve in avoiding overfitting, leading to consistently better results. The choice of LL an objecti ve function can be justified as maximum a-posteriori (MAP) training within a Bayesian ap-proach (Roark et al., 2004). 4.3 Featur e Functions We define features based on the alignment graph in Figure 3. Each feature function is defined on a word.
 Monolingual language model featur e. The monolingual LM feature of word E rithm of the probability of the n -gram ending at E : f p should be appropriately smoothed such that it never returns zero.
 Capitalized translation model featur e. Sup-pose E phrase  X  X lick OK X  is aligned to F phrase  X  X liquez OK X . The capitalized transla-tion model feature of  X  X lick X  is computed as log p (Clic k j Cliquez) +log p (Clic k j OK) .  X  X lick X  is assumed to be aligned to any word in the F phrase. The lar ger the probability that  X  X lick X  is translated from an F word, i.e.,  X  X liquez X , the more chances that  X  X lick X  preserv es the case of  X  X liquez X . For-mally , for word E and  X  F model feature of E p ( E i j  X  F m,k ) is the capitalized translation table. It needs smoothing to avoid returning zero, and is esti-mated from a word-aligned bilingual corpus.
 Capitalization tag translation featur e. The fea-ture value of E word  X  X lick X  aligning to F phrase  X  X liquez OK X  is log p (IU j IU) p (click j cliquez ) + log p (IU j AU) p (click j ok) . We see that this feature is less specific than the capitalized translation model feature. It is computed in terms of the tag transla-tion probability and the lowercased word translation probability . The lowercased word translation proba-bility , i.e., p (click j ok) , is used to decide how much of the tag translation probability , i.e., p (IU j AU) , will contrib ute to the final decision. The smaller the word translation probability , i.e., p (click j ok) , is, the smaller the chance that the surf ace form of  X  X lick X  preserv es case from that of  X  X k X . Formally , this fea-ture is defined as p ( e i j  X  f m,k ) is the t-table over lowercased word pairs, which is the usual  X  X -table X  in a SMT system. italization tag given a source capitalization tag and can be easily estimated from a word-aligned bilin-gual corpus. This feature attempts to help when f seen). Smoothing is also applied to both p ( e and p (  X  ( E word pairs).
 Upper -case translation featur e. Word E all upper case if all words in the corresponding F phrase  X  F ture can also be captured by the capitalization tag translation feature in the case where an AU tag in the input sentence is most probably preserv ed in the output sentence, we still define it to emphasize its effect. This feature aims, for example, to translate  X  X BC XYZ X  into  X  X UU VVV X  even if all words are unseen.
 Initial capitalization featur e. An E word is ini-tially capitalized if it is the first word that contains letters in the E sentence. For example, for sentence  X  Please click the button X  that starts with a bul-let, the initial capitalization feature value of word  X  X lease X  is 1 because  X   X  does not contain a letter . Punctuation featur e template. An E word is ini-tially capitalized if it follo ws a punctuation mark. Non-sentence-ending punctuation marks lik e com-mas will usually get negati ve weights.

As one can see, our features are  X  X oarse-grained X  (e.g., the language model feature). In contrast, Kim and Woodland (2004) and Roark et al. (2004) use  X  X ine-grained X  features. The y treat each n -gram as a feature for , respecti vely , monolingual capitaliza-tion and language modeling. Feature weights tuned at a fine granularity may lead to better accurac y, but the y require much more training data, and re-sult in much slo wer training speed, especially for lar ge-scale learning problems. Coarse-grained fea-tures enable us to efficiently get the feature values from a very lar ge training corpus, and quickly tune the weights on small development sets. For exam-ple, we can train a bilingual capitalization model on a 70 million-w ord corpus in several hours with the coarse-grained features presented abo ve, but in sev-eral days with fine-grained n -gram count features. 4.4 The GEN Function Function GEN generates the set of case-sensiti ve candidates from a lowercased tok en. For exam-ple GEN ( mt ) = f mt , mT , Mt , MT g . The follo w-ing heuristics can be used to reduce the range of GEN . The returned set of GEN on a lower -cased to-ken w is the union of: (i) f w, AU ( w ) , IU ( w ) g , (ii) f v j v is seen in training data and AL ( v ) = w g , and (iii) f  X  F tic (iii) is designed to pro vide more candidates for w when it is translated from a very strange input word  X  F phrase that w is in. This heuristic creates good capi-talization candidates for the translation of URLs, file names, and file paths. Training the bilingual capitalization model requires a bilingual corpus with phrase alignments, which are usually produced from a phrase aligner . In practice, the task of phrase alignment can be quite computa-tionally expensi ve as it requires to translate the en-tire training corpus; also a phrase aligner is not al-ways available. We therefore generate the training data using a na  X  X ve phrase aligner (NP A) instead of resorting to a real one.

The input to the NP A is a word-aligned bilingual corpus. The NP A stochastically chooses for each sentence pair one segmentation and phrase align-ment that is consistent with the word alignment. An aligned phrase pair is consistent with the word align-ment if neither phrase contains any word aligning to a word outside the other phrase (Och and Ne y, 2004). The NP A chunks the source sentence into phrases according to a probabilistic distrib ution over source phrase lengths. This distrib ution can be ob-tained from the trace output of a phrase-based MT decoder on a small development set. The NP A has to retry if the current source phrase cannot find any consistent tar get phrase. Unaligned tar get words are attached to the left phrase. Heuristics are emplo yed to pre vent the NP A from not coming to a solution. Ob viously , the NP A is a special case of the phrase extractor in (Och and Ne y, 2004) in that it considers only one phrase alignment rather than all possible ones.

Unlik e a real phrase aligner , the NP A need not wait for the training of the translation model to fin-ish, making it possible for parallelization of transla-tion model training and capitalization model train-ing. Ho we ver, we belie ve that a real phrase aligner may mak e phrase alignment quality higher . 6.1 Settings We conducted capitalization experiments on three language pairs: English-to-French (E ! F) with a bilingual corpus from the Information Technology (IT) domain; French-to-English (F ! E) with a bilin-gual corpus from the general news domain; and Chinese-to-English (C ! E) with a bilingual corpus from the general news domain as well. Each lan-guage pair comes with a training corpus, a develop-ment corpus and two test sets (see Table 2). Test-Precision is used to test the capitalization precision of the capitalizer on well-formed sentences dra wn from genres similar to those used for training. Test-BLEU is used to assess the impact of our capitalizer on end-to-end translation performance; in this case, the capitalizer may operate on ungrammatical sen-tences. We chose to work with these three language pairs because we wanted to test our capitalization model on both English and French tar get MT sys-tems and in cases where the source language has no case information (such as in Chinese).

We estimated the feature functions, such as the log probabilities in the language model, from the training set. Kneser -Ne y smoothing (Kneser and Ne y, 1995) was applied to features f and f the CRF-based bilingual capitalization model using the development set. Since estimation of the feature weights requires the phrase alignment information, we efficiently applied the NP A on the development set.

We emplo yed two LM-based capitalizers as base-lines for performance comparison: a unigram-based capitalizer and a strong trigram-based one. The unigram-based capitalizer is the usual baseline for capitalization experiments in pre vious work. The trigram-based baseline is similar to the one in (Lita et al., 2003) except that we used Kneser -Ne y smoothing instead of a mixture.

A phrase-based SMT system (Marcu and Wong, 2002) was trained on the bite xt. The capitalizer was incorporated into the MT system as a post-processing module  X  it capitalizes the lowercased MT output. The phrase boundaries and alignments needed by the capitalizer were automatically in-ferred as part of the decoding process. 6.2 BLEU and Pr ecision We measured the impact of our capitalization model in the conte xt of an end-to-end MT system using BLEU (Papineni et al., 2001). In this conte xt, the capitalizer operates on potentially ill-formed, MT -produced outputs.

To this end, we first inte grated our bilingual capi-talizer into the phrase-based SMT system as a post-processing module. The decoder of the MT sys-tem was modified to pro vide the capitalizer with the case-preserv ed source sentence, the lowercased translation, and the phrase boundaries and their alignments. Based on this information, our bilin-gual capitalizer reco vers the case information of the lowercased translation, outputting a capitalized tar -get sentence. The case-restored machine transla-tions were evaluated against the tar get test-BLEU set. For comparison, BLEU scores were also com-puted for an MT system that used the two LM-based baselines.

We also assessed the performance of our capital-izer on the task of reco vering case information for well-formed grammatical texts. To this end, we used the precision metric that counted the number of cor -rectly capitalized words produced by our capitalizer on well-formed, lowercased input
To obtain the capitalization precision, we im-plemented the capitalizer as a standalone program. The inputs to the capitalizer were triples of a case-preserv ed source sentence, a lowercased tar get sen-tence, and phrase alignments between them. The output was the case-restored version of the tar get sentence. In this evaluation scenario, the capitalizer output and the reference dif fer only in case infor -mation  X  word choices and word orders between them are the same. Testing was conducted on Test-Precision. We applied the NP A to the Test-Precision set to obtain the phrases and their alignments be-cause the y were needed to trigger the features in testing. We used a Test-Precision set that was dif-ferent from the Test-BLEU set because word align-ments were by-products only of training of transla-tion models on the MT training data and we could not put the Test-BLEU set into the MT training data. Rather than implementing a standalone word aligner , we randomly divided the MT training data into three non-o verlapping sets: Test-Precision set, CRF capitalizer training set and dev set. 6.3 Results The performance comparisons between our CRF-based capitalizer and the two LM-based baselines are sho wn in Table 3 and Table 4. Table 3 sho ws the BLEU scores, and Table 4 sho ws the precision. The BLEU upper bounds indicate the ceilings that a perfect capitalizer can reach, and are computed by ignoring the case information in both the capitalizer outputs and the reference. Ob viously , the precision upper bounds for all language pairs are 100% .
The precision and end-to-end BLEU based com-parisons sho w that, for European language pairs, the CRF-based bilingual capitalization model outper -forms significantly the strong LM-based baseline. We got more than one BLEU point impro vement on the MT translation between English and French, a 34% relati ve reduction in capitalization error rate for the French-to-English language pair , and a 42% rel-ative error rate reduction for the English-to-French language pair . These results sho w that source lan-guage information pro vides significant help for cap-italizing machine translation outputs. The results also sho w that when the source language does not have case, as in Chinese, the bilingual model equals a monolingual one.

The BLEU dif ference between the CRF-based capitalizer and the trigram one were lar ger than the precision dif ference. This indicates that the CRF-based capitalizer performs much better on non-grammatical texts that are generated from an MT system due to the bilingual feature of the CRF capi-talizer . 6.4 Effect of Training Cor pus Size The experiments abo ve were carried out on lar ge data sets. We also conducted experiments to exam-ine the effect of the training corpus size on capital-ization precision. Figure 4 sho ws the effects. The experiment was performed on the E ! F corpus. The bilingual capitalizer performed significantly better when the training corpus size was small (e.g., un-der 8 million words). This is common in man y do-mains: when the training corpus size increases, the dif ference between the two capitalizers decreases. In this paper , we have studied how to exploit bilin-gual information to impro ve capitalization perfor -mance on machine translation output, and evaluated the impro vement over traditional methods that use only monolingual language models.

We first presented a probabilistic bilingual cap-italization model for capitalizing machine transla-tion outputs using conditional random fields. This model exploits bilingual capitalization kno wledge as well as monolingual information. We defined a se-ries of feature functions to incorporate capitalization kno wledge into the model.

We then evaluated our CRF-based bilingual capi-talization model both on well-formed texts in terms of capitalization precision, and on possibly ungram-matical end-to-end machine translation outputs in terms of BLEU scores. Experiments were per -formed on both French and English tar get MT sys-tems with lar ge-scale training data. Our experimen-tal results sho wed that the CRF-based bilingual cap-italization model performs significantly better than a strong baseline, monolingual capitalizer that uses a trigram language model.

In all experiments carried out at Language Weaver with customer (or domain specific) data, MT sys-tems trained on lowercased data coupled with the CRF bilingual capitalizer described in this paper consistently outperformed both MT systems trained on lowercased data coupled with a strong monolin-gual capitalizer and MT systems trained on mix ed-cased data.

