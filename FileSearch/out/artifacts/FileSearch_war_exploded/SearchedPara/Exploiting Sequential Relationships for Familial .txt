 The pervasive nature of the internet has caused a significant transformation in the field of genealogical research. This has impacted not only how research is conducted, but has also dramatically increased the numbe r of people discovering their family history. Recent market research (Maritz Marketing 2000, Harris Interactive 2009) indicates that general interest in the United States has increased from 45% in 1996, to 60% in 2000, and 87% in 2009. Increased popularity has caused a dramatic need for improvements in algorithms related to extracting, accessing, and processing genealogica l data for use in bui lding family trees. This paper presents one approach to algorithmic improvement in the family history domain, where we infer the familial relationships of households fou nd in human transcribed United States census data. By applyi ng advances made in natural language processing, exploiting th e sequential nature of the census, and using state of the art machine learning algorithms, we were able to decrease the error by 35% over a hand coded baseline system. The resulting system is immediately applicable to hundreds of millions of other genealogical records where families are represented, but the familial relationships are missing. I.2.6 [ Learning ]: Knowledge acquisition Performance, Design Keywords Classification, CRF, ge nealogy, family history The advent of the inexpensive digital storage, effective large scale information retrieval systems, an d ubiquitous internet access has caused a recent explosion in family history research. According to a 2010 comSore survey, 12.7 million people researched their family history online during the month of March. Ancestry.com, an internet family history pr ovider, ended 2009 with a paid subscriber base of over one million customers. The number of digitized records held at Ancestry.com currently exceeds five billion. These collections represent historical documents in multiple languages, spanning multiple centuries, from sources such as census, newspapers, vital records, and family histories. While curren t algorithms have made these collections accessible to the dedicated researcher, new advances are required in order to make fami ly history research truly usable. This includes advances in fields such as information retrieval, information extraction, image pro cessing, distributed systems, and record linkage. This paper focuses on the task of labeling familial relationships in United States federal census data. The census data is a sequential listing, by household, of the population of the United States. In this task the classification of on e person in the census is strongly correlated to the correct classifica tion of the preceding people. In fact, because the familial relationships are relative to the head of previous instance. Additionally, th e discriminating features of the task are primarily derived by computing the difference, or similarity, of these two related instances. In many ways, labeling familial re lationships in se quential data is similar to many tasks found in processing sequences of natural language text. Both tasks are benefited by a segmentation of the data and a representation of the conditional dependency between sequential instances. Accurately representing the underlying relationships found in sequenti al data requires additional computational and representational challenges that are not always present in non-sequential data tasks. Many algorithmic simplifications made for representational complexity or availability reason s, have a greate r impact when applied to sequential data. Fo r example, the independence assumption made with the naive Bayes algorithm trades off representational power for a reduction in computational complexity at the cost of properly representing the conditional probability of the classification sequence. Likewise, the choice to only generate features that are dependent upon the current instance X  X  features reduces the ab ility of the model to accurately represent the underlying distribution. Taking our cue from advances in part of speech tagging we dependencies, and features that explicitly strengthe n the signal of those dependencies. This approach is experimental ly contrasted with models and features that, at best, implicitly capture the sequential nature of the data. The United States conducts a decennial population census. The census is recorded as a sequential list of the populous ordered by household and geographical location. The fields enumerated vary by census. Between 1850 and 1870 all household members were enumerated, but no familial relationship to the head of household was represented. Starting in 18 80 the familial relationship was added. Having the familial relationship of household members enables an exponential increase in the represen tational power of the data in applications such as information retrieval and record linkage, because you can derive relational features (e.g. Find William with a wife named Sarah). It is therefor e desirable to be able to infer the relationships for the 93 million people represented by the 1850-1870 census years, and thousands of similar collections such as immigration rolls or passenger lists. Additionally, because the later census years (1880-1930) contain the labeling for the familial relationship classification, we have a large set of labeled training data that we can immedi ately exploit. For this work we exclusively used the 1880 census. An 1880 census record provides the following standard fields: household number, given name, surn ame, race, gende r, age, and relationship to head of househol d. People are enumerated by a contiguous sequence of househol d members. Relationships are defined by first identifying the head of household (or simply head ). The head is usually the first person in the household sequence, and represented as a bl ank field on the census form. A household sequence is usually de termined by the household ID. The other household member relati onships are then labeled based upon their relationship to the he ad. (See figures 1 and 2.) If you take a single census record for a person in exclusion, and hide its relationship field, there is little evidence of its relationship to the household containing it. However, when viewed as a sequence of data, general infe rences can be made with a significant level of accuracy. The difficulty of the task is increased by two characteristics of the data. First, the data contains not only immediate familial relationships (i.e. husband, wife , son, and daughter), but also extended familial relationships (e.g. great granddaughter, son in law, cousin, and stepsister). Extended relationships represent approximately 7.5% of the instances and an additional 50 classifications. This creates a si gnificant sparse labeling problem. Secondly, households contai n a significant number of non-familial members (e.g. boarders and servants). All non-familial relationships are represented by an Other label. The non-familial members represent approximately 20% of the instances. Due to the close similarity of these classes X  features, the task of distinguishing between extended family members and non-family members is a non-trivial exercise, even for humans. In order to determine the impact of sequence oriented features on the accuracy of the task we divi ded our feature sets into three distinct categories. This in cludes non-sequential features, sequential features, and relative features. Three features are derived only from the instance itself. This includes race, gender, and discretized age (by decade). Features such as surname, given name, and household ID were not used because they are assumed to carry little or no signal when considered in isolation. This assumption was verified through experimentation. Sequential features represent differences between an instance and the surrounding instances at a given sequential position . A window of the preceding seven instances and the following two instances were used for calculating the relative features. The window size was chosen so that children would include their parent instances and parents would include their following child instances. Five seque ntial features were generated for each instance pair in the window. Th is included matching surname, matching household ID (HID), ma tching gender, matching race, 
Figure 2: Transcription of fi gure 1. Household 132 is a only 
The surname matches for positi on -2 (Lodge/Lodge), and Relative features are generated by using a rule based selection of a previous instance in order to define features that are directly correlated to the target classification. Specifically, we defined features that are relative to the assumed head for the household. The intuition is that the target classification is based upon a relationship between the instance representing the current person and a previous instance in the sequence that represents the person that is assumed to be the head. By directly calculating these features we attempt to alleviate the model from having to disambiguate the features repres enting the head relationship from the noise introduced by features that are based purely on sequence position. Five features were derived fro m relative features based upon matching surname, matching hous ehold ID, matching gender, matching race, and a discretized age difference relative to the head. In addition to using features that take advantage of the sequential nature of the data we wanted to use a model that would be able to take advantage of the sequential nature of the instance labels. We assumed that inferring the label of other instances in the sequence would be a significant indicator of the current instances label. For example, a wife instance will often follow a head instance, or conversely, a wife instance seldom follows a son instance. To this end we chose conditional random fi elds (CRF) [1][2] due to its success in dealing with sequence oriented data [3]. CRFs are a probabilistic framework for labeling and segmenting structured data, such as sequences, trees, and lattices. The underlying idea is that of defining a conditional probability distribution over label sequence s given a particular observation sequence, rather than a joint distribution over both label and observation sequences. CRFs, a type of discriminative probabilistic model, have led to state of the art performances in various application domains su ch as text processing [4], bioinformatics [5], and computer vision [6]. In order to validate the contri bution of the CRF model to the overall success of the task, we also executed all of the experiments using naive Baye s [7] and J48 [8] (a C4.5 implementation). Naive Bayes is based upon the foundation of Bayes theorem. This generative model computes the joint probability of an input variable and the target class. J48 was created as part of the WEKA [9] toolkit as an implementation of the C4.5 decision tree algorithm [10]. A decision tree uses a recursive partitioning algorithm based upon the amount of entropy contained in the instance set under consideration. By comparing the results of these three contrasting models, we hoped to determine the impact of sequentially oriented algorithms. In order to determine the impact of sequentially oriented features and models we created an experiment for each of the feature sets defined in section 3. The features for each experiment were cumulative, meaning that the features from the previous experiment were included in the current experiment. Each of the three models were appl ied to a training set of 90,000 instances and evaluated with 10,000 instances. 
Table 1: Accuracy for each of the three feature sets. Feature sets are cumulative (i.e. they include the features from the Because of the sparse nature of the extended family classifications and instances, we found that a simple accuracy measure was generally indicative of the performance of the system. Additionally, we tracked the macro averaged F1-measure of the main five classifications (self, wife, daughter, son, and other), and a separate F1-measure macro average for the remaining extended family classifications. We determined the relative difficulty of the problem by creating a simple expert system based upon ru les for classifying the classes that represented immediate fa milies. The immediate family classes represent 93.39% of the data. The baseline classifier determined the head by detectin g a change in the household ID. The other classes were then dete rmined by comparing a person X  X  gender and age against the head. If the surname did not match the head then they were considered to be of the other classification. If the surname did match and the age difference was over a given boundary then they were assumed to be a son or daughter. If the age difference was under a given boundary, and the gender was different, then they were classified as either a husband or a wife. Figure 4: Example of relative features for Charles Lodge. 
The head is William because the house ID changes from 5 The baseline expert system was able to obtain an accuracy of 85.85% and an F1-measure of 88.45% on the top five classes. This demonstrated that the majority of the data can be labeled successfully with relative ease. Using the non-sequential features described in section 3.1, we trained and tested the three classification models. This experiment demonstrated the impact of explicitly ignoring the sequential features of the data. As anticipated, the sequence oriented CRF was able to do significantly bett er (15.5% accuracy improvement) than the non-sequence oriented algorithms, naive Bayes and J48. From this we learn that CRFs are able to provide a significant advantage in situations where the sequential features of the data cannot be explicitly stated, but where a direct sequential relationship of the classifications exists. The fact that all the models with this feature set are inferior to the baseline system is only indicative of their comparatively restricted feature set. The next experiment sought to explicitly provide sequence relative features as defined in s ection 3.2. These features do not define a specific relationship between any two instances, but instead define the same features for each instance within the sequence window. By adding thes e features we saw a significant improvement in all of the models. Of particular interest was the success of J48 in comparison with CRF. In this case their accuracy rivals each other, and the F1-measure for J48 exceeds CRF by 1.9%. This suggests th at by providing sequentially oriented features you can replicate, or augment, the benefit provided by a sequence oriented algorithm. The previous experiment showed success at providing instance relative features without explic itly differentiating any of the surrounding instances in the comparison window by anything other than position. Intuitively, some relative instances are going to have more significant value th an other instances regardless of their position. For example, explicitly knowing the features generated when comparing an instance to a head should represent more information than ones created by comparing an instance representing a servant or son-in-l aw. Likewise, features generated from comparing to a head should provide more information than features generated from comparing to a person at an arbitrary sequence position. By taking the f eatures represented in section 3.3 we show the impact of this approach. Relative features significantly impacted the performance of the naive Bayes model, providing a 46.2% reduction in error, and an F1-measure that exceeds the CRF result by 1.2%. The error for CRF was reduced by 7.64% and by 11.58% for J48. This puts both algorithms within the margin of error of each other and further reinforces the implication that sequential data can be represented by a simpler model if sequential features are used. The family history domain is rich with opportunities for applying advanced and novel machine lear ning algorithms and approaches. In this work we addressed th e problem of labeling familial relationships in sequences of United States census data. Sequence oriented data provide s unique characteristics and challenges. These challenges incl ude opportunities to exploit both the interplay of features as well as the classification dependency of sequences of labels. We s howed the significant impact of features that take advantage of the sequential nature of the data. More concretely, we showed a 35% reduction in error from a hand coded baseline system by introducing sequence oriented algorithms and features. Our experiments with non-sequential features demonstrated that CRFs are able to achieve a significant advantage over models where no label sequence was inferre d. This insight suggests that models that are able to infer a label sequence can at least partially compensate for situations where sequence oriented features are unavailable, or expensive to calculate. We showed that when we used features that represent the sequential nature of the data, it appeared unnecessary to use a model that exploits the sequential nature of the classifications. This would be a significant obser vation if it were reproduced with additional data and algorithms. We believe that future work could benefit from treating the classification of the head as a separate task and then using that classification to derive the relative features. This would elevate some of the errors that occurred due to the ambiguity of extended and boarder families as well as make this work more applicable to datasets where no household IDs are available. [1] J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional [2] N. Okazaki. http://www.chokka n.org/software/crfsuite/ [3] C. Sutton, A. McCallum. 2007. An Introduction to [4] F. Sha, F. Pereira. 2003. Shallow parsing with conditional [5] B. Settles. 2005. Abner: an open source tool for [6] X. He, R. Zemel, and M. Carreira-Perpi X i X n. 2004. [7] T. Mitchell. 1997. Machine Learning . McGraw-Hill [8] R. Quinlan. 1993. C4.5: Programs for Machine Learning . [9] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, [10] L. Breiman, J. Friedman , C. J. Stone , and R.A. Olshen. 
