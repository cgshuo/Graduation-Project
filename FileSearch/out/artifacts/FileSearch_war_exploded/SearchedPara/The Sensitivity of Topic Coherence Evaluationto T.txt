 Latent Dirichlet Allocation ( X  X DA X : Blei et al. (2003)) is an approach to document clustering, in which  X  X opics X  (multinomial distributions over terms) and topic allocations (multinomial distribu-tions over topics per document) are jointly learned. When the topic model output is to be presented to humans, optimisation of the number of topics is a non-trivial problem. In the seminal paper of Chang et al. (2009), e.g., the authors showed that  X  contrary to expectations  X  extrinsically measured topic coherence correlates negatively with model perplexity. They introduced the word intrusion task, whereby a randomly selected  X  X ntruder X  word is in-jected into the top-N words of a given topic and users are asked to identify the intruder word. Low reliability in identifying the intruder word indicates low coherence (and vice versa), based on the in-tuition that the more coherent the topic, the more clearly the intruder word should be an outlier.
Since then, several methodologies have been in-troduced to automate the evaluation of topic coher-ence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Ale-tras and Stevenson (2013) showed that coherence can be measured by a classical distributional similar-ity approach. More recently, Lau et al. (2014) pro-posed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence.

A hyper-parameter in all these methodologies is the number of topic words, or its cardinality . These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009), N = 5 , whereas for Newman et al. (2010), Aletras and Stevenson (2013) and Lau et al. (2014), N = 10 .
The germ of this paper came when using the automatic word intrusion methodology (Lau et al., 2014), and noticing that introducing one extra word to a given topic can dramatically change the accu-racy of intruder word prediction. This forms the ker-nel of this paper: to better understand the impact of the topic cardinality hyper-parameter on the evalua-tion of topic coherence.

To investigate this, we develop a new dataset with human-annotated coherence judgements for a range of cardinality settings ( N = { 5 , 10 , 15 , 20 } ). We experiment with the automatic word intrusion (Lau et al., 2014) and discover that correlation with hu-man ratings decreases systematically as cardinality increases. We also test the PMI methodology (New-man et al., 2010) and make the same observation. To remedy this, we show that performance can be substantially improved if system scores and human ratings are aggregated over different cardinality set-tings before computing the correlation. This has broad implications for topic model evaluation. To examine the relationship between topic cardinal-ity and topic coherence, we require a dataset that has topics for a range of cardinality settings. Although there are existing datasets with human-annotated co-herence scores (Newman et al., 2010; Aletras and Stevenson, 2013; Lau et al., 2014; Chang et al., 2009), these topics were annotated using a fixed car-dinality setting (e.g. 5 or 10). We thus develop a new dataset for this experiment.

Following Lau et al. (2014), we use two do-mains: (1) W IKI , a collection of 3.3 million English Wikipedia articles (retrieved November 28th 2009); and (2) N EWS , a collection of 1.2 million New York Times articles from 1994 to 2004 (English Giga-word). We sub-sample approximately 50M tokens (100K and 50K articles for W IKI and N EWS respec-tively) from both domains to create two smaller doc-ument collections. We then generate 300 LDA top-
There are two primary approaches to assessing topic coherence: (1) via word intrusion (Chang et Table 1: Mean rating across different N (numbers in parentheses denote standard deviations) Table 2: Correlation between different pairwise car-dinality settings. al., 2009); and (2) by directly measuring observed coherence (Newman et al., 2010; Lau et al., 2014). With the first method, Chang et al. (2009) injects an intruder word into the top-5 topic words, shuffles the topic words, and sets the task of selecting the single intruder word out of the 6 words. In prelimi-nary experiments, we found that the word intrusion task becomes unreasonably difficult for human an-notators when the topic cardinality is high, e.g. when N = 20 . As such, we use the second approach as the means for generating our gold standard, asking users to judge topic coherence directly over different To collect the coherence judgements, we used Amazon Mechanical Turk and asked Turkers to rate topics in terms of coherence using a 3-point ordi-nal scale, where 1 indicates incoherent and 3 very coherent (Newman et al., 2010). For each topic (600 topics in total) we experiment with 4 cardinal-ity settings: N = { 5 , 10 , 15 , 20 } . For example, for N = 5 , we display the top-5 topic words for coher-ence judgement.

For annotation quality control, we embed a bad topic generated using random words into each HIT. Workers who fail to consistently rate these bad top-approximately 9 ratings per topic in each cardinality setting (post-filtered), from which we generate the gold standard via the arithmetic mean.

To understand the impact of cardinality ( N ) on topic coherence, we analyse: (a) the mean topic rat-ing for each N (Table 1), and (b) the pairwise Pear-son correlation coefficient between the same topics for different values of N (Table 2).

Coherence decreases slightly but systematically as N increases, suggesting that users find topics less coherent (but marginally more consistently in-terpretable, as indicated by the slight drop in stan-dard deviation) when more words are presented in a topic. The strong pairwise correlations, however, indicate that the ratings are relatively stable across different cardinality settings.

To better understand the data, in Figure 1 we present scatter plots of the ratings for all pair-wise cardinality settings (where a point represents a topic). Note the vertical lines for x = 3 . 0 (cf. the weaker effect of horizontal lines for y = 3 . 0 ), in par-ticular for the top 3 plots where we are comparing N = 5 against higher cardinality settings. This im-plies that topics that are rated as perfectly coherent (3.0) for N = 5 exhibit some variance in coherence ratings when N increases. Intuitively, it means that a number of perfectly coherent 5-word topics become less coherent as more words are presented. Lau et al. (2014) proposed an automated approach to the word intrusion task. The methodology computes pairwise word association features for the top-N words, and trains a support vector regression model to rank the words. The top-ranked word is then se-lected as the predicted intruder word. Note that even though it is supervised, no manual annotation is re-quired as the identity of the true intruder word is known. Following the original paper, we use as fea-tures normalised PMI (NPMI) and two conditional probabilities (CP1 and CP2), computed over the full collection of W IKI (3.3 million articles) and N EWS (1.2 million articles), respectively. We use 10-fold cross validation to predict the intruder words for all topics.

To generate an intruder for a topic, we select a random word that has a low probability ( P &lt; 0 . 0005 ) in the topic but high probability ( P &gt; 0 . 01 ) in another topic. We repeat this ten times to gen-erate 10 different intruder words for a topic. The 4 cardinalities of a given topic share the same set of intruder words.

To measure the coherence of a topic, we compute model precision , or the accuracy of intruder word prediction. For evaluation we compute the Pearson correlation coefficient r of model precisions and hu-man ratings for each cardinality setting. Results are summarised in Table 3. Table 3: Pearson correlation between system model precision and human ratings across different values of N for word intrusion.  X   X   X  denotes statistical sig-nificance compared to aggregate correlation.
Each domain has 2 sets of correlation figures, based on in-domain and out-of-domain features. In-domain (out-of-domain) features are word associ-ation features computed using the same (different) domain as the topics, e.g. when we compute coher-ence of W IKI topics using word association features
The correlations using in-domain features are in general lower than for out-of-domain features. This is due to idiosyncratic words that are closely related in the collection, e.g. remnant Wikipedia markup tags. The topic model discovers them as topics and the word statistics derived from the same collection supports the association, but these topics are gen-erally not coherent, as revealed by out-of-domain statistics. This result is consistent with previous studies (Lau et al., 2014).

We see that correlation decreases systematically as N increases, implying that N has high impact on topic coherence evaluation and that if a single value of N is to be used, a lower value is preferable.
To test whether we can leverage the additional in-formation from the different values of N , we aggre-gate the model precision values and human ratings per-topic before computing the correlation (Table 3: Cardinality =  X  X vg X ). We also test the significance of difference for each N with the aggregate correla-tion using the Steiger Test (Steiger, 1980); they are Table 4: Pearson correlation between system topic coherence and human ratings across different values of N for NPMI.  X   X   X  denotes statistical significance compared to aggregate correlation.

The correlation improves substantially. In fact, for N EWS using in-domain features, the correlation is higher than that of any individual cardinality set-ting. This observation suggests that a better ap-proach to automatically computing topic coherence is to aggregate coherence scores over different cardi-nality settings, and that it is sub-optimal to evaluate a topic by only assessing a single setting of N . In-stead, we should repeat it several times, varying N . The other mainstream approach to evaluating topic coherence is to directly measure the average pair-wise association between the top-N words. New-man et al. (2010) found PMI to be the best associa-tion measure, and later studies (Aletras and Steven-son, 2013; Lau et al., 2014) found that normalised PMI (NPMI: Bouma (2009)) improves PMI further.
To see if the benefit of aggregating coherence measures over several cardinalities transfers across to other methodologies, we test the NPMI method-ology. We compute the topic coherence using the full collection of W IKI and N EWS , respectively, for varying N . Results are presented in Table 4.
The in-domain features perform much worse, es-pecially for the W IKI topics. NPMI assigns very high scores to several incoherent topics, thereby re-ducing the correlation to almost zero. These top-ics consist predominantly of Wikipedia markup tags, and the high association is due to word statistics id-iosyncratic to the collection.

Once again, aggregating the topic coherence over multiple N values boosts results further. The cor-relations using aggregation and out-of-domain fea-tures again produce the best results for both W IKI
It is important to note that, while these find-ings were established based on manual annotation of topic coherence, for practical applications, topic co-herence would be calculated in a fully-unsupervised manner (averaged over different topic cardinalities), without the use of manual annotations. We investigate the impact of the cardinality of topic words on topic coherence evaluation. We found that human ratings decrease systematically when cardi-nality increases, although pairwise correlations are relatively high. We discovered that the performance of two automated methods  X  word intrusion and pairwise NPMI  X  can be substantially improved if the system scores and human ratings are aggre-gated over several cardinality settings before com-puting the correlation. Contrary to the standard prac-tice of using a fixed cardinality setting, our findings suggest that we should assess topic coherence using several cardinality settings and then aggregate over them. The human-judged coherence ratings, along with code to compute topic coherence, are available online. This research was supported in part by funding from the Australian Research Council.

