 Image classification is a highly useful yet still challenging task in the computer vision community due to the large intra-class variances and ambiguities of images. In most studies, the first step of image classification is to extract salient features from raw images for representation. Usually this is achieved by densely sampling keypoints in each image and describing the patches around each keypoint by some local descriptors like SIFT [Lowe 2004] and SURF [Bay et al. 2008]. In such a setting each image is rep-resented by a set of local features, where the number of features in different images are not necessarily equal. This is different from traditional classification problems where each image is represented by just one vector. Classifying images using traditional clas-sifiers in machine learning research like SVM, kNN, or AdaBoost usually requires feature quantization techniques that convert a set of local features to one fixed-length vector for each image. Such conversion may lose the discriminative information from the original representation and decrease the classification performance, therefore re-cent research on image classification tries to skip such conversion and directly classify images via the original local features.
 Image-To-Class (I2C) distance is a novel distance recently proposed in the work of Naive-Bayes-based Nearest Neighbor (NBNN) [Boiman et al. 2008], which can directly classify images via the local features without feature quantization. Specifically, given a set of local features in a test image, its I2C distance to each class is constructed by summing the Euclidean distances between every local feature and its Nearest Neighbor (NN) in the candidate class (1-NN rather than K-NN). Then the prediction for the test image can be simply achieved by choosing the class with the shortest I2C distance. Compared to traditional Image-To-Image (I2I) distance, this new distance not only avoids feature quantization, but also has better generalization capability. In addition, the I2C distances from a test image to all the candidate classes directly reflect the probability of classifying this test image to each class, so the prediction using I2C distance is straightforward.

However, Euclidean I2C distance may not be the optimal distance metric for mea-suring the distance between local features in real image classification problems, since it ignores the sematic gap between local features and class labels. Therefore, choos-ing a good distance metric is a critical problem for improving the performance of I2C distance. In this article, we propose a distance metric learning method to learn a Ma-halanobis I2C distance. For each class, we learn the class-specific Mahalanobis metric to combine with the corresponding I2C distance. Compared to learning only a global metric, the class-specific metric can better preserve the discriminative information for each class during the learning procedure and make the Mahalanobis I2C distance more discriminative, especially when there are a large number of classes, since learning a global Mahalanobis distance metric may not be sufficient to improve the I2C distance. See Figure 1 as example. We learn a metric M c for each class c , so the I2C distance Dist ( X i , c ) from image X i to each class c is combined with the learned metric M prediction, the class with the shortest I2C distance for this image is selected as the predicted class.

To learn these per-class metrics, we adopt the idea of large margin from SVM in our optimization problem. For each training image, we separate the I2C distance to its belonging class from the distance to any other class by a large margin, and form a large margin convex optimization problem as an instance of SemiDefinite Program-ming (SDP). Then we apply an efficient subgradient descent method to solve this optimization problem. Since all these per-class metrics are learned simultaneously in the optimization problem, we ensure the learned Mahalanobis I2C distances to dif-ferent classes are comparable during the prediction. We also try to further improve both the efficiency and effectiveness of our learned I2C distance. For improving the efficiency, we simplify the method to learn a diagonal matrix for each class, so that the optimization problem can quickly converge with a large number of classes. This makes our method scalable to large-scale problems. For improving the classification performance, we adopt two strategies of spatial pyramid match [Lazebnik et al. 2006] and learning weighted I2C distance function [Wang et al. 2009] to combine with our Mahalanobis I2C distance. With these two strategies, our learned I2C distance is able to achieve state-of-the-art performances.

Another contribution of this distance metric learning method is that it can correct the class imbalance problem, in which some classes contain much more training images than other classes. In such situations the original I2C distance has a bias towards these big classes like many other NN-based methods. However, the proposed method is able to correct this bias by learning the per-class metrics since the discriminative information in each class is preserved by its associated metric.

This article is an extension work of our published conference paper in Wang et al. [2010b]. The main extension work includes: (1) We provide a more complete descrip-tion on the background and technique detail. (2) We simplify the method to learn only a diagonal matrix for each class instead of a full matrix, which can significantly re-duce the computational time and memory usage while preserving the classification performance. (3) We add an additional Caltech 256 dataset in the experiment section and provide more comparison to show the effectiveness and efficiency of our proposed method. (4) We add another experiment to show that the proposed method is able to correct the class imbalance problem.

The rest of this article is organized as follows. Section 2 reviews previous research for I2C distance and distance metric learning that is related to our work. We describe our large margin optimization problem for learning the per-class metrics as well as an efficient solver in Section 3, where we also show how to further improve both the efficiency and classification performance of our learned I2C distance. We evaluate our method and compare it with other methods in Section 4. Finally, we conclude this article and discuss future work in Section 5. The idea of Image-To-Class (I2C) distance is firstly introduced in the work of NBNN [Boiman et al. 2008], which calculates the distance from a test image to a candidate class directly based on the original local features. For each class c , a feature set is constructed by gathering all the local features from training images belonging to this class. A test image X i is represented by the local feature set F where m i represents the number of features in X i and each feature is denoted as f need to find the NN of each local feature f ij in class c , which is denoted as f shows such NN search between local features. The sum of Euclidean distances between each feature in image X i and its NN in class c is denoted as the I2C distance from image X i to class c and can be formulated as
This I2C distance is able to handle the dataset with large intra-class variance and gets better generalization capability than traditional I2I distance, especially when the training set is limited. This is because the collection of local features in one class is abundant compared to those available in a single image. The effectiveness of I2C distance makes it useful in many computer vision applications. For example, Huang et al. [2010] applied it in face and human gait recognition and Wang et al. [2009] used it for human action recognition. Since the original I2C distance in NBNN treats discriminative and nondiscriminative local features equally, which limits its classification performance, later studies also focus on learning a parametric I2C distance. For example, Wang et al. [2009] learn a weighted I2C distance by associating each local feature with a weight and Behmo et al. [2010] learn an optimal NBNN by hinge-loss minimization to further enhance its generalization ability. However, all the previous works on I2C distance use Euclidean distance as the distance metric for measuring the distance between local features, which may not be the optimal metric in real image classification problems. Therefore, in this article, we propose a distance metric learning approach to learn the Mahalanobis distance metric for this I2C distance. We are aiming to learn a better distance metric than Euclidean distance and improve the classification performance of I2C distance. To the best of our knowledge, this is the first study of distance metric learning for the I2C distance. A good distance metric plays an important role in many real-world problems. In recent years, a number of methods have been proposed for distance metric learning. We start reviewing these methods by first giving the formulation of the distance measure. Given two data points X i and X j , the square of their Mahalanobis distance is expressed as where the symmetric matrix M is the Mahalanobis metric, which is valid only when it is positive semidefinite. When M is equal to the identity matrix I, this distance is reduced to Euclidean distance.
 The goal of distance metric learning is focused on learning this Mahalanobis metric M . Previous studies on distance metric learning can be roughly divided into three cate-gories according to the learning paradigm. The first category is unsupervised learning, which learns the distance metric without label information and is used for unsuper-vised learning tasks like clustering or dimension reduction. Representative methods include Principle Component Analysis (PCA), Multiple Dimension Scaling (MDS) [Cox and Cox 1994], ISOMAP [Tenenbaum et al. 2000], and Locally Linear Embedding (LLE) [Roweis and Saul 2000]. The second category is supervised learning, which learns the distance metric from training data with class labels. By using these labels, most su-pervised distance metric learning methods try to keep data points in the same class similar while separating them far away if coming from different classes. Recent rep-resentative methods include Relevance Component Analysis (RCA) [Bar-Hillel et al. 2005], Neighborhood Components Analysis (NCA) [Goldberger et al. 2005], Discrimi-native Component Analysis (DCA) [Hoi et al. 2006], Large Margin Nearest Neighbor (LMNN) [Weinberger and Saul 2009], Local Distance Metric Learning (LDM) [Yang et al. 2006], Information-Theoretic Metric Learning (ITML) [Davis et al. 2007], Proba-bilistic Relevance Component Analysis (pRCA) [Wu et al. 2009], Semantics-Preserving Bag-of-Words (SPBoW) [Wu et al. 2010], etc. The third category is semisupervised distance metric learning, which makes use of both labeled and unlabeled data for learning the distance metric. For the problem with only limited labeled data but plenty of unlabeled data, semisupervised learning methods would be more useful. A recent example of semisupervised distance metric learning is Hoi et al. [2008] for the task of collaborative image retrieval.

Besides these three categories of distance metric learning, recently there is also another new category named transfer distance metric learning methods [Zha et al. 2009; Zhang and Yeung 2010], which not only utilize labeled data from its own domain, but also use the labeled data from other domains for learning the distance metric. Transfer distance metric learning would be suitable for the problem where different domains are relevant and the target domain contains less labeled data, but the source domains have plenty of labeled data to help learning the distance metric for the target domain.

When applying these distance metric learning methods for image classification, fea-ture quantization is required to convert the local features into one vector for each image since these methods are only dealing with I2I distance. Such quantization may lose the discriminative information before the learning procedure and decrease the final classification performance. In this article, we propose to learn the distance metric for I2C distance and avoid such quantization, which is expected to provide a more effective classifier for image classification. We describe our distance metric learning method for I2C distance in this section. We first formulate a convex optimization problem in a large margin framework to learn the per-class Mahalanobis metrics, which are then solved by an efficient subgradient de-scent solver. We also discuss a simplified diagonal version of our distance metric learn-ing method for improving the learning efficiency and making it scalable for large-scale problems. Finally, two strategies to further enhance the performance of our learned I2C distance are discussed.
 The goal of learning the I2C distance metric is to learn a Mahalanobis distance between a set of local features in an image and their NN features in a candidate class. The dimension of the learned matrix depends on the dimension of the local features. This is different from traditional distance metric learning methods that learn the distance between different images, where the dimension of the learned matrix depends on the dimension of the only one vector that represents an image. Given an image X learned Mahalanobis I2C distance to a candidate class c is given as where notations used here are consistent with Section 2.1. Compared to Eq. (1), the only difference is that we change the Euclidean distance between local feature f its NN feature f c ij to a Mahalanobis distance with the matrix M
As there may be tens or hundreds of classes in many image classification problems, learning a global metric may not be sufficient to improve the I2C distance. Therefore, our method learns a different Mahalanobis matrix for every class. Such a class-specific metric can better preserve the discriminative information for different classes during the learning procedure, which is able to provide better performance than learning only a global metric as shown in Weinberger and Saul [2009]. Another contribution of the per-class metric learning is its ability to correct class imbalance problems, where some classes contain more training images than other classes. Like many NN-based methods, the original NBNN classifier has a bias towards these big classes. After learning a distance metric for each class, the discriminative information in each class is preserved by its associated metric, which is able to correct such bias. However, since the prediction for a test image requires comparing the learned I2C distances to different classes, all the metrics must be learned simultaneously to ensure the global consistency for prediction.

We adopt the idea of large margin to learn these metrics. This idea is popular due to its success in the SVM classifier, which simultaneously minimizes the empirical classification error and maximizes the geometric margin. For the binary classification problem, a hyperplane is optimized to create the largest separation between two classes. LMNN [Weinberger and Saul 2009] also adopts this idea to keep the distance between differently labeled images far away from the distance between similar images with a large margin. In the case of I2C distance, we try to keep the distance from the image to its belonging class less than the distance to any other class with a large margin, so that the distance to the belonging class can be the shortest among all classes. Specifically, for each training image X i , a triplet constraint is formulated by selecting its belonging class p (named as positive class) and any other class n (named as negative class). We constrain in each triplet that the I2C distance to positive class p should be less than that to negative class n with a large margin.
 As it is less likely to make all triplets satisfied during the training phase, we add a nonnegative slack variable  X  for each triplet constraint, which is known as the soft margin. So the preceding formula becomes
The objective function in our optimization problem is composed of two terms: the regularization term and error term. This is analogous to the optimization problem of SVM. We minimize all the slack variables in the error term. For the regularization term, we use the Frobenius norm of each metric to penalize its complexity. This regularizer is able to prevent any elements within each matrix from being too large as shown in Zhang and Yeung [2010] and Si et al. [2006]. The whole optimization problem is therefore formed as Here the variable  X  controls the trade-off between the regularizations term and error term and is usually empirically determined by cross-validation. The last constraint M c 0 is to make sure that every learned matrix is positive semidefinite. This op-timization problem is an instance of SDP, which can be solved using standard SDP solvers. However, the standard SDP solvers are usually computation expensive and less efficient. Since our optimization problem is convex, we use an efficient subgradient-descent-based method derived from Weinberger and Saul [2009] to solve this problem.
It should be noted that though our I2C distance metric learning method adopts the idea of large margin similar to that in LMNN, there are three major differences between these two works: (1) LMNN uses positive and negative instances from the k nearest neighbors of each training image to form the triplet, while in our method each triplet tries to keep the I2C distance to the belonging class less than to other classes by a large margin. So our triplet is generated at class level instead of image level for each training image, which reduces the number of constraints significantly. (2) LMNN uses image-to-image distance while we use image-to-class distance. Besides the advantages of I2C distance as described in Boiman et al. [2008], this distance with the per-class metrics can make the Mahalanobis I2C distance from the test image to its related class more discriminative, since the metric of the related class is able to learn the discriminative information for this class during the training phase. (3) The representation for each image is different. In LMNN each image must be represented by one feature vector, so local features must be quantized before using LMNN. However, our method directly applies to the origin representation and avoids feature quantization. To solve the optimization problem in Eq. (6), we first rewrite the Mahalanobis I2C distance in Eq. (3) by introducing a new term X ic ,whichisa m the difference between all features in the image X i and their nearest neighbors in the class c formed as The learned I2C distance from image X i to class c can then be reformulated as and the constraint in the optimization problem of Eq. (6) is also reformulated as The advantage of using this distance form for the Mahalanobis I2C distance is that the term X T ic X ic is a d  X  d matrix, which does not depend on the number of local features m i in the image and can be precalculated before the optimization problem.

We use a subgradient-descent-based solver to update each matrix since our optimiza-tion problem is convex with respect to all the matrices. At each step, every matrix is updated by taking a small step along the negative subgradient direction to reduce the objective function and then projected onto a feasible set to ensure the positive semidef-initeness. To calculate the subgradient of the objective function over each matrix, we need to incorporate each triplet constraint into the objective function. As the slack variable  X  is nonnegative, its value should be 0 if the corresponding triplet constraint is satisfied, and a positive value if the triplet constraint is unsatisfied. This can be expressed as
Therefore, all the zero-value slack variables can be removed in the optimization problem and these positive values can be formulated as:
At each iteration t, we can first scan over all triplets to find a set of unsatisfied triplets. We denote N t as the set of triplet indices such that ( i Then the objective function at t th iteration can be reformulated by replacing those positive slack variables according to Eq. (11).

For each matrix M t c at t th iteration, its subgradient G ( M the derivative of the preceding objective function with respect to M
Directly calculating the subgradient in each iteration using this formula would be computationally expensive. As the changes in the subgradient from one iteration to the next are only determined by the differences between the sets N use G ( M t c ) to calculate the subgradient G ( M t + 1 c efficient.
 Here ( N t + 1  X  N t ) denotes the set of triplets not existing in the t iteration but removed in the ( t + 1) th iteration.

Since the term ( X T ic X ic ) is precalculated before the first iteration, it is treated as a constant during the iterations. The matrix is updated by taking a small step along the negative subgradient direction for each iteration. To enforce the positive semidefinite-ness, the updated matrix needs to be projected onto a feasible set. This projection is done by eigen-decomposition of the matrix and truncating all the negative eigenvalues to zeros. As the optimization problem is convex, this solver is guaranteed to converge to the global optimum. During the classification process, the learned Mahalanobis I2C distances to all classes can be directly compared for prediction since all the metrics are updated simultaneously in the learning procedure, therefore guarantee the consistency for prediction. We summarize the whole work flow in Figure 3.

Next we analyze the efficiency of this solver and its scalability. During the metric up-dating iterations, the main memory cost is to load the precalculated value of X for each training image X i to each class c , which is a fixed dimension of d regardless of the number of local features in the image. So the total memory cost is O( CNd 2 ) for a dataset with C classes and N training images per class. Since there are 1 positive class and ( C  X  1) negative classes in each training image, the total number of to image-to-image distance whose complexity would be O( C updating iterations, we found only a small portion of them are nonzero, which are put into the error index set which maintains an active set of triplets for calculating N rather than scanning all the triplets at every iteration. So this solver runs quickly for updating even hundreds of metrics simultaneously. It should be noted that our method also supports incremental learning. When new training images of an existing class or new classes are added, per-class metrics do not need to be retrained from the beginning. The current learned metric can be used as initial estimates by changing the identity matrix I to current matrix in Figure 3, and new triplets are added to update all metrics. The updating procedure will converge quickly since prelearned metrics are relatively close to the optimal. This incremental learning ability supports our method for being scaled-up to handle a large number of classes and support for online learning. To further improve the efficiency during the training phase, we can learn a diagonal matrix for each class instead of a full matrix. This simplification can substantially improve the training efficiency, especially when dealing with a large number of classes and enhance the scalability of our method. The method for learning the diagonal ma-trix is simply derived from our method for learning the full matrix in the previous subsection. Recall that the Mahalanobis I2C distance from image X class c is Since M c is a diagonal matrix here, only diagonal elements of X this I2C distance can be simplified as Here Diag ( X T ic X ic ) indicates the diagonal matrix which only preserves the diagonal elements of X T ic X ic and is denoted as D ( X ic ) for simplicity. We use this new notation of I2C distance in the optimization problem and rewrite Eq. (6) as These diagonal matrices can be solved by the subgradient descent solver introduced previously. The new equation of subgradient G ( M t c ) is formulated simply by replacing X T ic X ic in Eq. (13) to its diagonal matrix D ( X ic ).
 Eq. (14) is reformulated in the same way to calculate G ( M iteration.

The work flow of learning diagonal matrices is similar to Figure 3. Note that eigen-decomposition is not required here after updating a new M matrix. The positive semidefiniteness of every updated matrix can be preserved by simply truncating all the negative elements to zeros. This simplification also improves the efficiency of our method. Moreover, learning diagonal matrices also reduces the memory cost from O( CNd 2 )toO( CNd ) for a dataset with C classes and N training images per class. Such reduction makes it possible to load all triplets into the memory even when the training set is very large. To further improve the final recognition performance, we adopt the idea of spatial pyramid match [Lazebnik et al. 2006] and learning I2C distance function [Wang et al. 2009] in our class-specific Mahanobis I2C distance.

Spatial Pyramid Match (SPM) is proposed by Lazebnik et al. [2006] which makes use of spatial correspondence, and the idea of pyramid match is adapted from Grauman and Darrell [2005]. This method recursively divides the image into subregions at in-creasingly fine resolutions and uses a pyramid matching kernel to match corresponding subregions in different levels. We adopt this idea in our NN search by limiting each local feature in the image to find its NN only in the same subregion from a candidate class at each level. So the feature searching set in the candidate class is reduced from the whole image in the top level to only the corresponding subregion in a finer level. This spatial restriction enhances the robustness of NN search by reducing the effect of noise due to wrong matches from other subregions. The learned distances from all lev-els are then merged together as pyramid combination, which makes the I2C distance more discriminative than either single level.

We also use the method of learning I2C distance function proposed in Wang et al. [2009] to combine with the learned Mahalanobis I2C distance. The idea of learning a local distance function is originally proposed for image classification and retrieval in Frome et al. Fromeet al. [2006, 2007]. Their method learns a weighted distance function for measuring the I2I distance, which is achieved by also using a large margin framework to learn the weight associated with each local feature. Wang et al. [2009] have used this idea to learn a weighted I2C distance function from each image to a candidate class. The learned weights of local features can be combined with our class-specific Mahalanobis metrics. For each class, its weighted I2C distance is multiplied with our learned per-class matrix to generate a more discriminative weighted Mahalanobis I2C distance. Details of the local distance function for learning weight can be found in Frome et al. [2007] and Wang et al. [2009]. We evaluate our proposed method on five datasets: Scene-15, Sports, Corel, Caltech 101, and 256 datasets. We describe them briefly as follows.  X  Scene-15 . This dataset consists of 15 scene categories, among which 8 were orig-inally collected by Oliva and Torralba [2001], 5 were added by Feifei and Perona [2005], and a other 2 were from Lazebnik et al. [2006]. Each class has about 200 to 400 images, and the average image size is around 300  X  250 pixels. Following Lazeb-nik et al. [2006], we randomly select 100 images per class for training and test on the rest. The mean per-class recognition rate is reported as classification accuracy.  X  Sports . This event dataset was firstly introduced in Li and Fei-Fei [2007], consisting of 8 sports event categories. The number of images in each class ranges from 137 to 250. We follow Li and Fei-Fei [2007] to select 70 and 60 images per class for training and test, respectively. Since images in this dataset are usually very large, they are first resized such that the largest x/y dimension is 500.
  X  Corel . This dataset contains 10 scene categories published by Corel Corporation.
Each class contains 100 images, which is randomly separated into two subsets of equal size for training and testing as Lu and Ip [2009b]. All the images are of the size 384  X  256 or 256  X  384.  X  Caltech 101 . This dataset is a large-scale dataset containing 101 categories [Fei-Fei et al. 2004]. The number of images in each class varies from about 30 to 800. This dataset is more challenging due to the large number of classes and intra-class variances. Following the common configuration in the community, we randomly select 15 images per class for training. For test set, we also select 15 images from each class and report the mean accuracy.  X  Caltech 256 . This dataset [Griffin et al. 2007] is even larger than Caltech 101. Each class contains at least 80 images. The total of 256 categories make this dataset much more challenging than the other four datasets. We use this dataset to validate the scalability of our methods. In the experiment we randomly select 15 images per class for training and 25 different images per class for test.
 Since the training and test set are selected randomly, we repeat the experiment for 5 times in each dataset and report the average results. For feature extraction, we densely sample patches and extract SIFT features [Lowe 2004] as the descriptors, which are computed on a 16  X  16 patches over a grid with spacing of 8 pixels for all datasets. Each image is then represented by about 1000 feature vectors of 128 dimension. We denote our method as I2CDML (Image-To-Class Distance Metric Learning), where the value of  X  is determined via a separate validation set in each dataset and ranging from 0 . 85 to 0 . 95, and the learning rate  X  is set as 0 . 005. The updating procedure stops when the objective function converges within 0 . 05 between two consecutive iterations or meets the maximum number of iterations, which is set as 100 in the experiment. We first show the experiment results on three small datasets: Scene-15, Sports, and Corel. We compare our learned Mahalanobis I2C distance to the original Euclidean I2C distance (NBNN [Boiman et al. 2008]), Mahalanobis I2I distance and Euclidean I2I distance to evaluate the effectiveness of our method. For the I2I distance, we need to first convert the local features into a fixed-length vector for representing each image. In this experiment, we use the BoW model for such feature quantization due to its effectiveness in recent research. The codebook size is set as 400 in every dataset generated by K-means clustering, so each image is represented as a 400-dimension vector and each component in the vector represents the frequency of the corresponding visual word. For the prediction of I2I distance, 3-NN strategy is used as in Weinberger and Saul [2009]. We also tried other values for the number of NN in prediction but did not find much difference in the classification result. For the Mahalanobis I2I distance we implemented several distance metric learning methods including: LMNN [Weinberger and Saul 2009], ITML [Davis et al. 2007], DCA [Hoi et al. 2006], RCA [Bar-Hillel et al. 2005], NCA [Goldberger et al. 2005], and LDA. Table I summaries the results on these three datasets. We can see that our Mahalanobis I2C distance significantly outperforms the all other distances in every dataset, which illustrates the effectiveness of our learning method. We also find that even using Euclidean distance, NBNN can still beats most Mahalanobis I2I distances, while the Euclidean I2I distance performs the worst in all the three datasets. This comparison shows that I2C distance is more discriminative than the traditional I2I distance due to the avoidance of feature quantization, which may lose the discriminative information that would not be compensated even after learning the distance metric.

We also compare the computation cost of our learning procedure to LMNN since both methods are learned in a large margin optimization framework. Table II lists the running time of the learning procedure required for each dataset and the number of triplets used in the two methods. As our method formulates the triplet constraints by selecting one positive class and one negative class at each time, and each training image has one positive class and ( C  X  1) negative classes, there are totally ( C for a dataset with C classes and N training images per class. However, in LMNN, the triplet for each training image is constructed by selecting a positive image in its kNN and a negative image. Since the number of images is usually much more than the number of classes, the triplets required for LMNN are much more than those for our method as shown in Table II. Therefore, our method needs much less computation time during the learning phase, while the classification performance of our method is significantly better than LMNN due to the effectiveness of I2C distance.

To further investigate the learned metric in each class, we also compare the per-class accuracy between I2CML and NBNN in Figure 4. It is shown that for those easily classified categories, our method is comparable to NBNN. However, for those challeng-ing categories where NBNN performs poorly (for example, the last three categories in Scene-15, the last four in Sports, and the last two in Corel, as indicated in Figure 4), our method can improve the accuracy substantially. Therefore our method improves the average accuracy by enhancing the classification on the challenging categories and still maintains the performance for the easily classified categories. We show some ex-amples that are wrongly classified by NBNN but correctly classified by our method in the left four columns of Figure 5. We note that the predicted class labels of these examples by NBNN are not related to the image contents at all, due to the use of Euclidean distance. Our method is able to correct these wrong predictions by learning the distance metric for each class, which again shows the importance of using a good distance metric for classification. However, due to the large intra-class variances and ambiguities, some challenging images cannot be correctly classified by our method as well, which are shown in the right two columns of Figure 5.

We also show the improved I2C distance through spatial pyramid restriction from the idea of Spatial Pyramid Match (SPM) in Lazebnik et al. [2006] and learning weight (Weight) associated with each local feature in Wang et al. [2009]. For SPM we use 3 spatial levels: 1  X  1, 2  X  2, and 4  X  4 subregions. From Table III, we can see that both strategies improve the classification accuracy for every dataset. Specifically, SPM achieves more improvement than learning weight in the Scene-15 dataset but less improvement in the other two datasets. This is likely due to the fact that the geometric structures of scene images in the Scene-15 dataset fit in the spatial pyramid very well, while in the other two datasets discriminative local features associated with the large weights learned from Wang et al. [2009] play a more important role for classification. By using both strategies, we achieve the best results in all the three datasets.
To alleviate the computation cost, we simplify our method to learn only a diagonal matrix for each class and compare it to the original I2CML which learns full matrices as shown in Table IV. As mentioned in Section 3.3, the space complexity is reduced from O( CNd 2 )toO( CNd ) for a dataset with C classes and N training images. Therefore, the memory space and computation time are significantly reduced for learning diagonal matrices, while it achieves a comparable classification performance.

We compare our best results with recently published result on every dataset. All the results are listed in Table V. In every dataset, our result is able to outperform or at least compare to recent reported results, which shows that our method is able to achieve state-of-the-art performance on these three datasets. In particular, we find our result is close to Wu and Rehg [2009] in both Scene-15 and Sports datasets. However, we notice they have used a multiscale and denser grid to extract features as well as combining an additional Sobel gradient, while our feature extraction is very simple but still comparable to theirs. When using the same configuration, their approach is worse than ours, as either indicated in Wu and Rehg [2009] as well as implemented by us using their published LibHIK 1 code. Caltech 101 and 256 datasets contain more classes than the previous three datasets. We evaluate our method on these two datasets to demonstrate its scalability on large-scale datasets. Again we compare our method with Euclidean I2C distance (NBNN [Boiman et al. 2008]), Mahalanobis I2I distance (LMNN [Weinberger and Saul 2009], ITML [Davis et al. 2007], DCA [Hoi et al. 2006], RCA [Bar-Hillel et al. 2005], NCA [Goldberger et al. 2005], and LDA) and Euclidean I2I distance. For the I2I distance we use the same implementation in the previous three datasets. For I2C distance, we accelerate the NN search by using KD-tree implemented by Vedaldi and Fulkerson [2008] to find the approximate NN for each local feature, with a maximum of 100 comparisons for each NN search. We also use a single spatial level of 4 to further reduce the size of the feature set.

Table VI summaries the results on Caltech 101. Three conclusions can be drawn from this table: First, due to the increased number of classes, both I2I and I2C Euclidean distances do not perform well in this dataset. Second, after learning the per-class metric, our method improves the performance of I2C distance for 10.3%, which is better than LMNN and ITML over the original Euclidean I2I distance, while other distance metric learning algorithms improve little over the I2I Euclidean distance. This may because our method learns the distance metric for every class rather than learning one global distance metric. Third, this table again shows I2C distance is much better than I2I distance, since even the performance of Euclidean I2C distance is better than Mahalanobis I2I distance.

It should be noted that our implementation of NBNN does not meet the reported re-sults in Boiman et al. [2008]. This may be due to the different feature extraction meth-ods. In Boiman et al. [2008], they extracted SIFT features using multiscale patches densely sampled from each image, which results in many redundant features on the training set (about 15000 to 20000 features per image). So the NN search for I2C distance calculation takes expensive computation cost. Even using KD-tree for accel-eration, it takes about 1.6 seconds per class for the NN search of each test image as reported in Boiman et al. [2008] and thus around 160 seconds for 101 classes to clas-sify only one test image. This is unacceptable during the testing phase and makes it difficult for real-world application. In our experiment, we only generate less than 1000 features per image on average using our feature extraction strategy, which is about 1/20 compared to the size of feature set in Boiman et al. [2008]. So our implementation needs much less computation cost for the NN search during the online testing phase with the additional offline training phase. Since in our experiment both methods use the same feature set as input, the comparison should be fair.

We also use spatial pyramid match and learning weighted I2C distance to further improve our Mahalanobis I2C distance, which require little overhead during the online testing phase. From Table VII, we can see that both methods are able to improve the performance and combining both we achieve even better results than the reported result of NBNN in Boiman et al. [2008], which has no training phase but requires heavy computation cost in the testing phase.

Due to the large number of classes in Caltech 101, efficiency is also an important factor for distance learning methods. Our method requires learning 101 metrics in this dataset, which makes it more time consuming compared to the previous three datasets. To alleviate the computation cost, we simplify our method to learn only a diagonal ma-trix for each class and compare it to learning full matrices as shown in Table VIII. Only 156MB space is required to be loaded into the memory for learning diagonal matri-ces, while learning full matrices requires about 20GB space, which makes it difficult to load all data into the memory at the same time. The computation cost of learning full matrices is also much more than that in learning the diagonal matrices. Mean-while, learning diagonal matrices achieves a comparable classification performance to learning full matrices. Therefore, learning a diagonal matrix instead of full matrices is especially useful for a large-scale image classification problem where efficiency is more important.

Caltech 256 contains more classes than Caltech 101 dataset. Even using 15 images per class for training, the memory cost of learning full matrices is nearly 128GB, which is too large to be loaded into the memory. Therefore we only learn diagonal matrices in this dataset, which needs only about 1GB memory space and can be loaded into the memory during the learning procedure. The computation time of the learning procedure is less than 1 hour, which is nearly negligible compared to the computation cost of the feature extraction procedure and NN search. Such small overload is able to improve the classification compared to the original Euclidean I2C distance. We show the results of our method and other methods as compared in the previous four datasets in Table IX. Similar conclusions from the results on Caltech 101 can be drawn again in this dataset. Due to the large number of classes, I2I distance metric learning methods that learn only one global metric gain little improvement over the Euclidean I2I distance, while our method can still improve I2C distance because of its per-class metric learning. Combining spatial pyramid match and learning weighted I2C distance, our I2C distance can be further improved to a recognition accuracy of 30 performance gap between I2C distance and the traditional I2I distance becomes more prominent in this dataset as it contains larger number of classes than the previous four datasets.

In all the previous experiments we use recognition accuracy to evaluate the perfor-mance of each method. However, for the Caltech 256 dataset with such a large number of classes, this evaluation method cannot fully reflect the improvement of our method compared to NBNN. The I2C distance to all classes can produce a label rank, while only the top class is selected for prediction and evaluated by the recognition accuracy. It may be possible that the position of the ground-truth class is improved in the label rank but still does not reach the top position, which cannot be reflected using the previous evaluation method. Taking the test image in Figure 6 as example. Both our method and NBNN can produce the label ranks by sorting the I2C distances to all classes. The ground-truth class for this test image is ranked at the 11th position by NBNN, while our method improves it to the 2nd position in the rank. However, such improve-ment is not reflected by the previous evaluation method since both algorithms predict incorrectly. Therefore in order to provide more detail improvement, we also compare the position of the ground-truth class in the label rank generated by our method to NBNN. In average, the label rank position of the ground-truth class is improved by 8 . 77 compared to NBNN. There are 51 . 3% test images from the whole test set where our method improves their positions of the ground-truth class compared to NBNN, while another 21 . 7% are the same as that in NBNN. This is a very large improvement since it covers more than half of the test set, while using the previous evaluation method we only find small improvement of the recognition accuracy since only 5 images are improved to the top position in the label rank by our method. Therefore this new evaluation method is able to provide more detail improvement and validate the effectiveness of our distance learning method.

Finally we compare our best result to recent published results in Table X. Since we only use a single SIFT feature in our experiment, we compare the results of other works that are also achieved by a single feature. This table shows that our result outperforms or is comparable to most state-of-the-art in these two large-scale datasets, which again validates the effectiveness of our distance metric learning method. In this experiment we show that the proposed method can solve the class imbal-ance problem. We use the Caltech 101 dataset for this experiment since the num-ber of training images per class varies largely in this dataset, ranging from 31 to 800. Eight groups with different training sizes are formed for evaluating the performance of our method compared to NBNN. The test set for all groups are kept the same by select-ing 15 images per class. For the training set, we first select 15 different images per class as a balanced group in Group 1. Then we increase the maximum number of training images per class by 15 for each group. Since some classes have limited images in this dataset and their training sizes cannot be increased, while other classes have plenty of images in the dataset and their training sizes can be increased, the size of training set in each class will be different and result in an imbalanced group. With the increased maximum number of training images per class, the variance of the training set per class is also increased and leads the group to be more imbalanced, as shown in Table XI.
We compare our distance metric learning method with NBNN in each group. We use the diagonal metric learning in our method while other settings are kept the same as the previous experiment. The recognition accuracies of our method and NBNN are shown in Figure 7. We can see that the performance of NBNN is firstly increased in Group 2 but then decreased in other groups. The reason for the improvement from Group 1 to Group 2 is due to the largely increased training set with less variance. However, though the training set is continually increased, the increase of variance and imbalance is more prominent from Group 3 to Group 8 and causes the performance of NBNN to decrease. Therefore we can see that NBNN cannot work well for imbalanced classes. However, with the increased imbalance our method can still keep the same recognition accuracy, which validates that the learned per-class metrics are able to correct the class imbalance problem. In this article, we proposed a distance metric learning method to learn a Mahalanobis I2C distance. Instead of learning one global distance metric, we learned the class-specific Mahalanobis metric for each class, which makes the learned Mahalanobis I2C distance more discriminative, especially when there are a large number of classes. A large margin framework has been formulated to simultaneously learn the per-class Mahalanobis distance metrics, with the constraint that the I2C distance for each training image to the belonging class should be less than the distance to any other class with a large margin. We proposed a subgradient descent method to efficiently solve this optimization problem. For large-scale classification problems, a simplified diagonal version of Mahalanobis I2C distance is also proposed. By only learning a class-specific diagonal matrix for every class, the computational time and memory usage can be significantly reduced while the classification performance is preserved. The experiment results on five image datasets of Scene-15, Sports, Corel, Caltech 101, and 256 verified that our I2CDML method can significantly outperform the original Euclidean I2C distance as well as other distance metric learning methods that learn a Mahalanobis I2I distance. The proposed method can be further integrated with spatial pyramid match and learning weights associated with local features to further improve the performance. With these two strategies, our learned I2C distance is able to achieve state-of-the-art performances. We also show that the learned per-class metrics are able to correct the class imbalance problem.
 For the future work, we will consider learning a hierarchial class structure for the I2C distance. In this article, we learn the distance metric for each class, so the com-putation time and memory cost are linear to the number of classes. This will limit its scalability to extremely large-scale datasets that have thousands or tens of thousands of classes. At that scale, learning a hierarchial structure will be more practical, since not every class is required to learn the distance metric in this hierarchial structure. There-fore, the complexity is reduced to sublinear in the number of classes. We will also con-sider transfer learning for the I2C distance metric. In some problems the target domain contains less labeled data, but the source domains have plenty of labeled data and they are relevant to the target domain. We will study how to explore the label information in the source domain to further help learning the distance metric for the target domain.
