 1. Introduction
Users of information systems often have very specific questions, possibly complex, for which the system has to return precise answers. Since decades various technologies have been developed to resolve this problem, which are generally re-ferred to as question answering technologies. Question answering is a sophisticated form of information retrieval character-ized by information needs that are at least partially expressed as natural language statements or questions. Natural language is one of the most natural forms of interaction by a human with a machine. In contrast with classical information retrieval, where complete documents are considered relevant to the information request, in question answering specific pieces of information are returned as an answer. The user of a question answering system is interested in a concise, comprehensible and correct answer, which may refer to a word, a sentence, a paragraph, an image, an audio fragment, or, but more rarely, to an entire document.

Besides textual databases, repositories of multimedia information are becoming increasingly prominent. Content recog-nition in media such as images, video and audio is a major research domain, and recently there has been an interest in using text based queries over multimedia resources. Although querying multimedia databases has traditionally been implemented with a structured query language ( Elmasri &amp; Navathe, 2006 ), a growing interest in using natural language statements or questions is noticed ( Kucuktunc, Gudukbay, &amp; Ulusoy, 2007 ). Another factor that will probably boost the demand for ques-tion answering technology is the increasing use of mobile devices, such as smartphones, to access information ( Liu &amp; Doer-mann, 2008 ). In such a setting, traditional queries consisting of typed keywords are not very user-friendly. Moreover, speech interfaces, also called voice-enabled natural language interfaces, will very soon allow asking real questions in spoken natural language.

A number of various question answering technologies have been investigated and developed to facilitate the interroga-tion of documents. The annual Text Retrieval Conference (TREC) its competition tracks. The challenge is to provide a concise answer to a natural language question, given a large collection of textual documents. In open domain question answering, the range of question topics is unrestricted. The Cross-Language
Evaluation Forum or CLEF 2 , established in 2000, promotes multilingual question answering, where the question is asked in a different language than the language of the documents in the repository. This requires the translation of a question or of potential answer sentences in order to match the question with information in the documents when finding the an-swer. Cross-language question answering may inspire cross-media question answering, where a question or statement in natural language interrogates image, video or audio archives. With the increasing power of analysis tools for processing questions or any kind of natural language statements, we expect this interest to persist. The NTCIR Workshop, launched in 1998, is a series of evaluation workshops designed to enhance research in information access technologies including dis-ciplines such as information retrieval, question answering, text summarization, and information extraction. With an empha-sis on large-scale test collections reusable for experiments and a common evaluation infrastructure, this venue accomodates research groups working with Japanese and other Asian languages. answering more recently NTCIR launched a geo-temporal QA track addressing geographic and temporal aspects in informa-tion retrieval (see below). Most of existing technologies focus on answering simple questions that require factual answers.
The successful results lead to a growing interest in commercial use of this work. However, evaluation campaigns have grad-ually increased the complexity of the natural language questions.

The following milestones were realized in question answering research: The incorporation of expected answer type and classification of the question according to the answer type as in Li and Roth (2002), Moldovan, Pas  X  ca, Harabagiu, and Surdeanu (2003) ; logic-based representations of question and candidate answer sentences ( Moldovan, Clark, Harabagiu, &amp; Maiorano, 2003 ); the integration of syntactic and semantic analyses of questions and candidate answers ( Hovy et al., 2001;
Alfonseca, De Boni, Jara-Valencia, &amp; Manandhar, 2001; Cui, Sun, Li, Kan, &amp; Chua, 2005 ) among which is semantic role labeling ( Narayanan &amp; Harabagiu, 2004; Shen &amp; Lapata, 2007 ); temporal question answering, where answers are filtered by the tem-poral constraints posed in the question ( Moldovan, Clark, &amp; Harabagiu, 2005; Ahn, Schockaert, De Cock, &amp; Kerre, 2006; Pust-ejovsky et al., 2004 ), geographical question answering where geographic constraints limit the answer ( Mandl et al., 2008 ); and the analysis of discourse relationships and resolution of textual entailments ( Braz, Girju, Punyakanok, Roth, &amp; Sammons, 2005; Harabagiu &amp; Hickl, 2006 ). The more complex questions require an advanced analysis and decomposition of the ques-tion and a sophisticated processing of the information sources. The answer finding methods need to integrate fusion and reasoning strategies. Such question answering systems have the potential to grow into real decision support systems and could find their way in enterprises and governmental organizations, apart from helping citizens finding efficiently and effec-tively correct information via Web interfaces. These goals are not yet realized mainly because our understanding of natural language is not yet fully developed and lacks technologies for the recognition of content beyond the analysis of individual sentences. We still lack methods for dealing with the large variety of patterns in text or other media that often express the same or similar information, and we miss a suitable framework to efficiently retrieve and reason with information.
Question answering is multidisciplinary. It involves information technology, artificial intelligence, natural language pro-cessing, knowledge and database management and cognitive science. From the technological perspective, question answer-ing uses natural or statistical language processing, information retrieval, and knowledge representation and reasoning as potential building blocks. It involves text classification, information extraction and summarization technologies. In this spe-cial issue on question answering we have selected five papers, which are summarized as follows.
 Hyo-Jung Oh, Ki-Youn Sung, Myung-Gil Yang and Sung Hyon Myaeng present an interesting paper entitled Compositional
Question Answering: A Divide and Conquer Approach . The authors propose a model for breaking up complex questions that cannot be answered directly but can be divided into simpler ones, which can be answered by means of current question answering capabilities. The individual answers are then fused to generate a final answer. Extensive evaluations and error analyses show the importance of a correct analysis of complex questions and the advantage of breaking a complex question into simpler questions for which the answer is easier to find. Some interesting answer fusion techniques are discussed.
The paper Linguistic Kernels for Answer Re-ranking in Question Answering Systems by Alessandro Moschitti and Silvia Quar-teroni focuses on answer selection, which is the most complex phase of any question answering system. To solve this task, the authors study supervised discriminative models that learn to select or rank answers using examples of question and an-swer pairs. To reduce the burden of large amounts of manual annotation, they represent question and answer pairs by means of powerful generalization methods. The authors propose a set of valid structural kernels, including string kernels, syntactic and shallow semantic tree kernels, applied to part-of-speech tag sequences, syntactic parse trees and predicate argument structures. The results of answer selection show that the best model improves the bag-of-words model by 63% on a TREC dataset. The authors compare an answer reranking technique based on bag-of-words features with a reranker based on structural features. They conclude that answer reranking is beneficial in terms of answer accuracy measured by the mean reciprocal rank, but also that tree kernels applied on structural features are most effective in identifying correct answers to complex questions.
 Improving Graph-based Random Walks for Complex Question Answering Using Syntactic, Shallow Semantic and Extended
String Subsequence Kernels by Yllias Chali, Sadid A. Hasan and Shafiq R. Joty extends the use of linguistic kernels in question answering. The task of answering complex questions requires inferencing and synthesizing information from multiple doc-uments. It can be seen as a kind of topic oriented, informative multi-document summarization. The paper presents the im-pact of syntactic and semantic information in a graph based random walk method for answering complex questions.
Alberto T X llez-Valero, Manuel Montes-y-G X mez, Luis Villase X or-Pineda and Anselmo Pe X as Padilla focus on the problem that different question answering methodologies each can give additional evidence about the correctness of a candidate an-swer. Their paper entitled Learning to Select the Correct Answer in Multi-Stream Question Answering focuses on the selection of the correct answer from a given set of responses provided by different systems. Multi-stream systems attempt to improve the individual results by taking advantage of the complementarity from existing question answering systems. Therefore, the major challenge is to select the correct answer for a given question by combining the evidence from different input systems (or streams). In particular, this paper proposes a supervised multi-stream approach that decides about the correctness of answers based on a set of features. These features include the compatibility between question and answer types, the redun-dancy of answers across streams, and the degree of overlapping information between the question X  X nswer pair and the sup-port text. Experimental results evaluated over a set of 190 questions in Spanish and using answers from 17 different question answering systems show that this multi-stream approach significantly improves the performance of the individual systems and a state-of-the-art multi-stream approach.

P. Moreda, H. Llorens, E. Saquete and M. Palomar present Combining Semantic Information in Question Answering Systems , i.e., an approach to integrate semantic information into open domain answer finding. The authors propose to use semantic information, where a list of semantic classes of an answer is extracted from the hypernym hierarchy of WordNet, and show that the integration of semantic information improves the performance especially in terms of the precision of the answers. In the paper A Structural Support Vector Method for Extracting Contexts and Answers of Questions from Online Forums , Yunbo
Cao, Wen-Yun Yang, Chin-Yew Lin, and Yong Yu address the issue of extracting question contexts and answers on posts from online discussion forums. Although somewhat marginal to the scope of typical question answering, this paper moves beyond sentence processing and studies discourse understanding by means of promising machine learning techniques for structured output. The proposed graphical representations allow exploring various relations among sentences, and more specifically the detection of complex structures of threads. In addition, a new inference algorithm is designed to find or approximate the constraint that is the most violated. The authors also optimize practical performance measures by varying loss functions adapted to specific application requirements. Experimental results show that the methods are both promising and flexible.
Finally, Patrick Saint-Dizier and Marie-Francine Moens design some elements of a roadmap for advanced question answering research in their paper entitled Knowledge and Reasoning for Question Answering: Research Perspectives . Advanced question answering refers to a situation where a deep understanding of the meaning of the question and the information source, together with techniques for answer fusion and generation are needed. This is the case e.g. for How-to or Why ques-tions. Moreover, research with regard to the analysis of and answer finding for evaluative or comparative questions is very crucial demanding for contextualized deep semantics of question and candidate answers. This paper introduces new consid-erations as well as it consolidates findings from previous raodmaps.
 References
