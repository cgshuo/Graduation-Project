 RWTH Aachen University RWTH Aachen University
Evaluation and error analysis of machine translation output are important but difficult tasks. In this article, we propose a framework for automatic error analysis and classification based on the identification of actual erroneous words using the algorithms for computation of Word Error Rate (WER) and Position-independent word Error Rate (PER), which is just a very first step towards development of automatic evaluation measures that provide more specific information of certain translation problems. The proposed approach enables the use of ways. This work focuses on one possible set-up, namely, on five error categories: inflectional errors, errors due to wrong word order, missing words, extra words, and incorrect lexical choices. For each of the categories, we analyze the contribution of various POS classes. We compared the results of automatic error analysis with the results of human error analysis in order to investigate two possible applications: estimating the contribution of each error type in a given translation output in order to identify the main sources of errors for a given translation system, and comparing different translation outputs using the introduced error categories in order to obtain more information about advantages and disadvantages of different systems and possibilites for improvements, as well as about advantages and disadvantages of applied methods for improvements. We used Arabic X  X nglish Newswire and Broadcast News and Chinese X  X nglish Newswire outputs created in the framework of the GALE project, several
Spanish and English European Parliament outputs generated during the TC-Star project, and three German X  X nglish outputs generated in the framework of the fourth Machine Translation systems. 1. Introduction
The evaluation of machine translation output is an important and at the same time difficult task for the progress of the field. Because there is no unique reference trans-lation for a text (as for example in speech recognition), automatic measures are hard to define. Human evaluation, although of course providing (at least in principle) the most reliable judgments, is costly and time consuming. A great deal of effort has been spent on finding measures that correlate well with human judgments when determining which one of a set of translation systems is the best (be it different versions of the same system in the development phase or a set of  X  X ompeting X  systems, as for example in a machine translation evaluation).
 finding a ranking between different machine translation systems. Although this is use-ful information and helps in the continuous improvement of machine translation (MT) systems, MT researches often would find it helpful to have additional information about their systems. What are the strengths of their systems? Where do they make errors? Does a particular modification improve some aspect of the system, although perhaps it does not improve the overall score in terms of one of the standard measures? Does a worse-ranked system outperform a best-ranked one in any aspect? Hardly any systematic translation outputs in order to obtain an insight of the actual problems of their systems. A framework for human error analysis and error classification has been proposed by consuming task.
 calculation of Word Error Rate ( WER ) and Position-independent word Error Rate (
The extracted erroneous words can then be used in combination with different types of linguistic knowledge, such as base forms, Part-of-Speech ( tags, compound words, suffixes, prefixes, and so on, in order to obtain various details errors, reordering errors, missing words), contribution of different word classes (e.g., POS , NE ),andsoforth.
 and the comparison of the results of automatic error analysis with those obtained by translation outputs used for the comparison of human and automatic error analysis 658 were produced in the frameworks of the G ALE 1 project, the TC-fourth Workshop on Statistical Machine Translation 3 ( human error analysis is done considering two possible applications: estimating the contribution of each error category in a particular translation output, and comparing different translation outputs using these categories. In addition, we show how the new error measures can be used to get more information about the differences between translation systems trained on different source and target languages, between different training set-ups for a same phrase-based translation system, as well as between different translation systems. 1.1 Related Work
A number of automatic evaluation measures for machine translation output have been related NIST metric (Doddington 2002), along with WER used by many machine translation researchers. The Translation Edit Rate ( et al. 2006) and the CD ER measure (Leusch, Ueffing, and Ney 2006) are based on the edit distance ( WER ) but allow reordering of blocks. TER uses an edit distance with additional costs for shifts of word sequences. The CD ER measure drops certain constraints for the hypothesis: Only the words in the reference have to be covered exactly once, whereas those in the hypothesis can be covered zero, one, or multiple times. Preprocessing and normalization methods for improving the evaluation using the standard measures
PER , BLEU ,and NIST are investigated by Leusch et al. (2005). The same set of measures is examined by Matusov et al. (2005) in combination with automatic sentence segmen-tation in order to enable evaluation of translation output without sentence boundaries (e.g., translation of speech recognition output). The METEOR 2005) first counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. A method that uses the concept of maximum matching string (MMS) is presented by Turian, Shen, and Melamed (2003). IQ (Gim  X  enez and Amig  X  o 2006) is a framework for automatic evaluation in which evaluation metrics can be combined.
Nevertheless, none of these measures or extensions takes into account any details about actual translation errors, for example, what the contribution of verbs is in the overall error rate, how many full forms are wrong although their base forms are correct, or how many words are missing. A framework for human error analysis and error classification and Lavie 2005) is presented together with a detailed analysis of the obtained results. identification of patterns in translation output using
Lopez and Resnik (2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between characteristics of source documents such as genre, domain, language, and so on. Zhou et al. (2008) propose a diagnostic evaluation of linguistic check-points obtained auto-matically by aligning parsed source and target sentences. For each check-point, the number of matched n -grams of the references is then calculated. Linguistically based reordering along with the syntax-based evaluation of reordering patterns is described in Xiong et al. (2010).
 chine translation output based on WER and PER , and systematically investigate a set of possible methods to carry out an error analysis at the word level. 2. A Framework for Automatic Error Analysis
The basic idea for automatic error analysis described in this work is to take into ac-count details from the WER (edit distance) and PER algorithms, namely, to identify all erroneous words which are actually contributing to the error rate, and then to combine these words with different types of linguistic knowledge. The general procedure for standard error rates WER and PER is given in Section 2.1, and methods for extracting actual errors are described in the following sections.
 base forms of the words and POS tags as linguistic knowledge. However, the analy-presents only one of many possibilities X  X his framework enables the integration of various knowledge sources such as deeper lingustic knowledge, the introduction of source words (possibly with additional linguistic information) if appropriate alignment instead of only at the word level is possible as well. The error analysis presented in this 660 work is language-independent X  X evertheless, availability of base forms and for the particular target language is a requisite. 2.1 Standard Word Error Rates (Overview)
The standard procedure for evaluating machine translation output is done by com-paring the hypothesis document hyp with the given reference document ref , each one consisting of K sentences (or segments). The reference document ref consists of N reference translations of the source text. N R = 1 stands for the case when only a single reference translation is available, and N R &gt; 1 denotes the case of multiple references.
Let the length of the hypothesis sentence hyp k be denoted as N of each reference sentence N ref
N hyp = k N hyp k and the total reference length is N ref = shown to be optimal with respect to the correlation with the human evaluation score X  X  adequacy and fluency (Leusch et al. 2005). The overall error rate is then obtained by normalizing the total number of errors over the total reference length.
 1966) X  X he minimum number of substitutions, deletions, and insertions that have to be performed to convert the generated text hyp into the reference text ref . A shortcoming of the WER is the fact that it does not allow reorderings of words, although the word order of the hypothesis can be different from the word order of the reference even though it is a correct translation. The position-independent word error rate ( substitutions, deletions, and insertions but without taking the word order into account.
The PER is always lower than or equal to the WER . On the other hand, a shortcoming of the PER is the fact that it does not penalize a wrong word order.
 ref is calculated as where d L ( ref k , r , hyp k ) is the Levenshtein distance between the reference sentence ref and the hypothesis sentence hyp k . The calculation is performed using a dynamic pro-gramming algorithm.
 in a multi-set of words setw .The PER can be calculated using the counts n ( e , hyp n ( e , ref k , r )ofaword e in the hypothesis sentence hyp respectively: where 2.2 Identification of WER Errors
The dynamic programming algorithm for WER enables a simple and straightforward identification of each erroneous word which actually contributes to a reference sentence and hypothesis sentence along with the corresponding Levenshtein alignment and the actual words participating in WER is shown in Table 1. The reference words involved in WER are denoted as reference errors, and hypothesis errors refer to the hypothesis words participating in WER .
 base forms and POS tags. This allows us to compute the contribution of each sentence k according to alignment links to the best reference and p is a N (
WER (p)) = e  X  p n ( e , werr k ) is the number of WER errors in werr belonging to the POS class p. For the substitution and the deletion errors, 662 the reference words are used, and for the insertion errors, words are used. The WER for the word class p can be calculated as the standard normalizing the number of errors over the total reference length:
Standard WER of the whole sentence is equal to 5 / 12 = 41 . 7%. The contribution of nouns is WER (N) = 1 / 12 = 8 . 3%, of verbs is WER (V) = 2 / 12 = 16 . 7%, and of adverbs is
WER (A DV ) = 2 / 12 = 16 . 7%. 2.3 Identification of PER Errors In contrast to WER , the standard efficient algorithms for the calculation of give precise information about contributing words. However, it is possible to identify all words in the hypothesis which do not have a counterpart in the reference, and vice versa. These words will be referred to as PER errors.
 to the standard PER according to Equation (3) is 3 X  X here are two substitutions and one deletion. The problem with standard PER is that it is not possible to detect which words are deletion errors, which are insertion errors, and which words are substitution errors. Therefore we introduce alternative PER -based measures which correspond to hypothesis sentence k which do not appear in the reference sentence k (referred to as hypothesis errors ). Analogously, let rerr k denote the multi-set of words in the reference sentence k which do not appear in the hypothesis sentence k (referred to as reference errors ). Then the following measures can be calculated:
For the example sentence presented in Table 1, the number of hypothesis errors n ( e , herr k ) is 2 and the number of reference errors e rors contributing to the standard PER is 3 according to Equation (3), since 1and e | n ( e , ref k )  X  n ( e , hyp k ) | = 5. The standard length N ref = 12, thus being equal to 25%. The R PER considers only the reference errors, R PER = 3 / 12 = 25%, and H PER only the hypothesis errors, H F
PER is the sum of hypothesis and reference errors divided by the sum of hypothesis and reference length: F PER = (2 + 3) / (11 + 12) = 5 / 23 = 21 . 7%.
 the hypothesis is H PER (N) = 1 / 11 = 9 . 1%, and together F contribution of verbs in the reference is R PER (V) = 2 / 12 = 16 . 7%, in the hypothesis is H PER (V) = 1 / 11 = 9 . 1%, and together F PER (V) = 3 / 23 = 13%.

WER as well as for R PER and H PER calculation; POS tags are added afterwards as an additional knowledge. Exact distribution of errors over POS implementation of the WER and the R PER and H PER algorithms. For example, if light#A and light#N occur in the reference and light#V occurs in the hypothesis, there are two possibilities: either light#A is linked to light#V and light#N is a missing word, or light#N is linked to light#V so that light#A is a missing word. 3. Methods for Automatic Error Analysis and Classification
The error details described in Section 2.2 and Section 2.3 can be combined with different types of linguistic knowledge in different ways. Examples with the base forms and tags as linguistic knowledge are presented in Tables 2 and 4. The described error rates 664 of particular POS classes give more details than the overall standard error rates and can be used for error analysis to some extent. However, for more precise information about certain phenomena some kind of further analysis is required. In this work, we examine the following error categories:
Furthermore, the contribution of various POS classes for the described error categories is estimated.
 reference(s) and for the hypothesis. The performance of morpho-syntactic analysis is slightly lower on the hypothesis, but this does not seem to influence the performance of the error analysis tools. We choose to use reference words for all cases where it can be chosen between the reference and the hypothesis, however. Nevertheless, it would compare the results. 3.1 Inflectional Errors
An inflectional error occurs if the base form of the generated word is correct but the full form is wrong. Inflectional errors can be estimated using R in the following way: From each reference X  X ypothesis sentence pair, only erroneous words which have common base forms are taken into account: where eb denotes the base form of the word e and rberr base form errors in the reference. The number of words with erroneous base forms (representing a multi-set of non X  X nflectional errors) is subtracted from the number of total errors. For example, from the PER errors presented in Table 3, the word is will be detected as an inflectional error because it shares the same base form with the reference error be .
 at the beginning of this section, however, we choose to use the reference words because the results of the morpho-syntactic analysis are slightly more reliable for the references than for the hypotheses. 3.2 Reordering Errors into account only by WER and not by PER . Therefore, a word which occurs both in the reference and in the hypothesis but is marked as a reordering error. The contribution of reference reordering errors can be estimated in the following way: where suberr k represents the multi-set of WER substitution errors, delerr of WER deletion errors, and rerr k the multi-set of R PER errors. A definition using H errors with substitutions and insertions is also possible; this work, however, is focused on the reference errors. For the example in Table 1, the word sometimes is identified as a reordering error. 3.3 Missing Words Missing words can be identified using the WER and PER errors in the following way: The words considered as missing are those which occur as deletions in at the same time occur only as reference PER errors without sharing the base form with any hypothesis error, that is, as a non X  X nflectional R PER
The multi-set of deletion WER errors is defined as delerr multi-set of base form R PER errors. The use of both more reliable than using only the WER deletion errors because not all deletion er-rors are produced by missing words X  X  number of WER missing. 3.4 Extra Words Analogously to missing words, extra words are also detected from the errors: The words considered as extra are those that occur as insertions in 666 and at the same time occur only as hypothesis PER errors without sharing the base form with any reference error: where inserr k is the multi-set of insertion WER errors and hberr form H PER errors. In the example in Table 1 none of the words will be classified as an extra word. 3.5 Incorrect Lexical Choice
The erroneous words in the reference translation that are classified neither as inflectional errors nor as missing words are considered as incorrect lexical choice:
As in the case of the inflectional and reordering errors, a definition using hypothesis errors. In the example in Table 1, the word Mister in the reference (or the word Mrs in the hypothesis) is considered as an incorrect lexical choice. 4. Comparison with Human Error Analysis
In order to compare the results of the proposed automatic error analysis with human error analysis, the methods described in the previous sections are applied on several translation outputs with the available results of human error analysis. These translation outputs were produced in the framework of the G ALE project, the TC-the shared task of the fourth Statistical Machine Translation Workshop ( 4.1 Human Error Analysis
Human error analysis and classification is a time-consuming and difficult task, and it can be done in various ways. For example, in order to find errors in a translation output correct translations of a given source sentence, however, and some of them might not error analysis. The errors can be counted by doing a direct strict comparison between expressions by synonyms, syntactically correct different word order, and so on, which aspect, namely, to look only whether the main meaning is preserved. It is even possible not to use a reference translation at all, but compare the translation output with the source text. There are also other aspects that may differ between human evaluations, for example, counting each problematic word as an error or counting groups of words as one error, and so forth. Furthermore, the human error classification is definitely not unambigous X  X ften it is not easy to determine in which particular error category some error exactly belongs, sometimes one word can be assigned to more than one category, and variations between different human evaluators are possible. For error categories described in previous sections, especially difficult is disambiguating between incorrect lexical choice and missing words or extra words. For example, if the translation output is the day before yesterday and translation reference is yesterday , it could be considered as a group of incorrectly translated words, but also as a group of extra words. Similarly, there are several possible interpretations of errors if the one who will come is translated as which comes .
 rank (  X  ) correlation coefficients between human and automatic results are calculated.
Both coefficients assess how well a monotonic function describes the relationship be-tween two variables: The Pearson correlation assumes a linear relationship between the variables, and the Spearman correlation takes only rank into account. Thus Spearman X  X  668 rank correlation coefficient is equivalent to a Pearson correlation on ranks. A Pearson correlation of +1 means that there is a perfect positive linear relationship between the exactly the same. A Pearson correlation of  X  1 means that there is a perfect negative linear relationship between variables, and a Spearman correlation of exactly inverse ranking. A correlation of 0 means there is no linear relationship between the two variables. Thus, the higher value of r and  X  , the more similar the metrics are. 4.2 Distribution of Errors Over Categories of errors over the categories, that is, how well the automatic methods are capable of capturing differences between error categories and determining which of those are particularly problematic for a given translation system. For each of the error categories, the distribution of errors over the basic POS classes X  X ouns ( ( ), adverbs ( ADV ), pronouns ( PRON ), determiners ( DET ), prepositions ( tions ( CON ), numerals ( NUM ), and punctuation marks ( PUN obtaining more details about errors, namely, which POS of errors for a particular error category. For the G ALE corpora, the strict human error analysis is carried out, and for the TC-STAR corpora, the free one. 4.2.1 Results on G ALE Corpora. The raw error counts for each category obtained on the G
ALE corpora both by human and automatic error classification are shown in Table 6. It can be seen that both the results of the human analysis as well as the automatic analysis show the same tendencies: For the Arabic-to-English Broadcast News translation, the main sources of errors are extra words and incorrect lexical choice, for the Newswire corpus the predominant problem is incorrect lexical choice, and for the Chinese-to-
English the majority of errors are caused by missing words, followed by incorrect lexical choices and wrong word order.
 egory both by human and by automatic analysis, namely, as a reordering error and a missing word, respectively. The words feeling for represent an example where the human analysis assigns the error to the category of missing words, but the automatic words by humans, but as lexical errors by automatic tools. These examples illustrate the previous statements about difficulties in disambiguation between missing words and extra words vs. lexical errors. In the second sentence, the inflectional error based/base is detected both by humans and by automatic tools. However, contribution is classified as a lexical error by humans and as a missing word by automatic tools. The human is present only as an WER error and neither as an H third sentence illustrates a total agreement between the human and automatic error classification: Both words are assigned to the same category. Results for the ten basic classes are shown in Table 8, and again from both human and automatic error analysis the same conclusions can be drawn.
 analysis. The correlation function presented in Table 9(a) is measured between the error counts in each category, and Table 9(b) presents the correlation between the error counts for each POS class within a particular error category. It can be seen that the automatic measures have very high correlation coefficients with respect to the results of human evaluation. The correlations for the inflectional error category are higher than for the other categories, which can be explained by the fact mentioned in previous sections that the disambiguation between missing words, extra words, and incorrect lexical choice is often difficult, both for humans and for machines. 4.2.2 Results on TC-STAR Corpora. The experiments on the TC-those on the G ALE corpora. There are some differences, however, because human error classification is carried out in a somewhat different way and completely independently.
The error categories considered by the human error analysis were inflectional errors, missing words, reordering errors, and incorrect lexical choice X  X hat is, the same as in the G ALE experiments except extra words. The distribution of errors over investigated: verb tense errors, verb person errors, adjective gender errors, and adjective 670 number errors. The category of inflectional errors is also different: It is obtained as a sum of these particular inflectional categories. Correlation coefficients are calculated both for general error categories and for inflectional details.
 well that the numbers of errors obtained by automatic methods is much higher than the numbers obtained by the free human evaluation.
 (i.e., tense, person, gender, and number). Both human and automatic error analysis indicate that the most problematic inflectional category is the tense of verbs, especially for the translation into Spanish.
 the correlations for the error categories, although all rather high (above 0.5), are lower than for the G ALE corpus. This is due to the free human evaluation which is carried out on this corpora, that is, without taking the reference translation strictly into account.
However, for the inflectional error analysis the correlations are very high, above 0.9. 4.3 Differences Between Translation Systems and Methods for Improvements
The focus of this set of experiments is to examine how well the automatic methods are capable of capturing differences between systems and methods for improvements in order to 672
The experiments should also show how reliable each error category is for the compar-ison of translation outputs. For each of the error categories, the basic analyzed as well in order to estimate which POS classes of each category are reliable for the comparison of translation outputs.
 erated by phrase-based systems (Vilar et al. 2005) and three German-to-English outputs produced in the framework of the fourth shared translation task (Callison-
Burch et al. 2009). Two of the WMT 09 outputs are generated by standard phrase-based systems (Zens, Och, and Ney 2002) and one by a hierarchical phrase-based system (Chiang 2007). For the TC-STAR outputs two reference translations are available for the automatic error analysis, and for the WMT 09 outputs only a single reference is available. summarize all the results along with the Spearman and Pearson correlation coefficients calculated across the different translation outputs. 4.3.1 Results on TC-STAR Corpora. The error analyses were carried out on six Spanish-to-English outputs generated by phrase-based translation systems built on different sizes of training corpora in order to examine the effects of data sparseness (Popovi  X  cand
Ney 2006b). In addition, the effects of local POS -based word reorderings of nouns and adjectives (Popovi  X  c and Ney 2006a) were analyzed in order to examine improvements of the baseline system. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way around. Therefore, local reorderings of nouns and adjective groups in the source language were applied. If the source language is Spanish, each noun is moved behind the corresponding adjective corresponding noun. An adverb followed by an adjective (e.g., more important )ortwo adjectives with a coordinate conjunction in between (e.g., economic and political )are treated as an adjective group. Reorderings were applied in the source language, then training and search were performed using the transformed source language data. Mod-ifications of the training and search procedure were not necessary. In this work, only
Spanish-to-English translation is analyzed. The English outputs of following training set-ups for the same phrase-based translation system are examined: The effects of local reorderings are investigated for each size of the training corpus. outputs in the form N hum / N aut . In the last row of the table, the Spearman and Pearson correlation coefficients  X  and r for each category across different translation outputs are shown. In addition, the correlations for error distributions within a translation output (as in Section 4.2) are presented as well in the rightmost column, and it can be seen that, as in the previous experiments, they are high for each of the translation outputs. As for the correlations of error categories across translation outputs, the class of inflectional er-rors and of incorrect lexical choice have very high correlations, which suggests that these two categories reflect well the differences between translation outputs. The reordering errors and missing words are also suitable for comparison, whereas the category of extra words has even a negative correlation X  X t is not possible to draw conclusions about differences between translation outputs looking into this category.
 number of inflectional errors is low, and becomes slightly higher for the system trained on a dictionary, although there is a number of reordering errors, missing words, and extra words. The most problematic category is the incorrect lexical choice, especially for the small training corpora. The number of reordering errors and missing words is also increasing when the corpus size is decreasing, although to a lesser extent. The based local reordering technique reduces the number of reordering errors, especially for the small training corpora, and does not harm the other error categories. 4.3.2 Results on WMT Corpora. Three outputs generated by three German-to-English statistical translation systems are analyzed in order to examine the differences between a standard phrase-based translation model and a hierarchical translation model. In the standard phrase-based model, phrases are defined as non-empty contiguous sequences of words. The hierarchical phrase-based model is an extension of this model where the phrases are allowed to have  X  X aps X  (i.e., non-contiguous parts of the source sentence are allowed to be translated into possibly non-contiguous parts of the target sentence). 674
In this way, long-distance dependencies and reorderings can be modelled better. In addition, we investigate the effects of long range verbs (Popovi  X  c and Ney 2006a) used to improve the phrase-based system. Verbs in the
German language can often be placed at the end of a clause. This is mostly the case with infinitives and past participles, but there are many cases when other verb forms also occur at the clause end. For the translation from German into English, the following verb types were moved towards the beginning of a clause: infinitives, infinitives with search are performed using the transformed source language data.
 translation outputs. Spearman and Pearson correlation coefficients are shown as well for each translation output across error categories (rightmost column), and for each error category across different translation outputs (last row). Again, the correlations across the error categories are very high for all translation outputs, although slightly smaller than for the TC-STAR data. It can be seen that the extra words also have the weakest correlation among error categories, although the coefficients are not negative as in the case of the TC-STAR data. This confirms the previous hypothesis that this error category is not suitable for looking into details about differences between translation outputs. errors. The main advantage of the hierarchical system compared to the standard phrase-based system is the smaller number of missing words, and the main disadvantage the hierarchical system should actually better deal with various reorderings, both local and long-range. Nevertheless, the produced reorderings are rather unconstrained so that a number of them are beneficial, but there are also a number of reorderings that make the translation quality worse. One possibility for improving the hierarchical system is to introduce certain phrase/gap constraints using POS tags or deeper syntax knowledge sources. The long-range POS -based reorderings of verbs used to improve the standard phrase-based system reduced the number of reordering errors, and also the number of lexical errors. A discrepancy considering the missing words can be observed for this method: The human error analysis reports reduction of missing words whereas this is not captured by automatic tools. Deeper analysis regarding different could possibly reveal more details about this. It also can be seen that the hierarchical system produces fewer lexical errors than the baseline phrase-based system, but more lexical errors than the phrase-based system with reorderings. Syntactic constraints for the hierarchical model might improve this error category as well.
 of improvements along with some differences between human and automatic error analysis. 4 The first sentence represents an example for the missing words error problem X  X ne can see that there are a number of missing words in the output of the phrase-based system. All of these missing words are detected both by human and 676 by automatic error analysis, except for the word amount , which is considered by the automatic tools as a reordering error (for the same reason as the example in Table 7).
When the reordering technique is applied, the number of errors is reduced and the translation becomes understandable although still not grammatically correct: The sequence total amount becomes a reordering error both for humans and for automatic tools. For the words designed and is , the confusion between missing words (human) and lexical errors (automatic) is present. The hierarchical system generates even fewer missing words, although neither this system nor the phrase-based system with reorderings is able to overcome the reordering problem. It should also be noted that for all three systems, the word assistance is detected as a word translated by incorrect lexical choice aid/to help , whereas by the humans it is not considered as an error. yielded by POS -based reorderings, as well as advantages and disadvantages of the hierarchical system. The baseline phrase-based system produces several missing words reduced, although there are still some words considered as errors only by automatic hierarchical system also does not contain missing words, and the problem of the verb reordering is solved as well ( pass on ), but some other reordering errors are introduced: the price increase . It should be noted that for this sentence automatic tools detect that want is a lexical error because it is translated as wish , whereas the humans of course do not consider this an error.
 phrase-based system has several missing words ( made , that will ) which are not present when the reorderings are applied, and are also not present in the output of the hierar-chical system. The same happens with the verb reordering error be . However, the noun reordering error Bush is not resolved by the POS -based reorderings of verbs, whereas it is at the correct position in the hierarchical system output. Apart from this, all outputs caused by an out-of-vocabulary word effective on market/marktwirksam . For the phrase-based output, this error is classified as a lexical error both by humans and by automatic tools, but for the other two outputs, the automatic tools classified the words effective on as missing words.
 differences between translation outputs. The baseline phrase-based output contains verb reordering errors has been abandoned . The same error is present in the output of the hierarchical system, whereas the sentence obtained by the phrase-based system with reorderings is completely correct. Nevertheless, these reordering errors are not at all same words in the reference and in the hypothesis are present. Therefore there are more discrepancies between the human and automatic error analysis for all three outputs: system output have been are considered as extra words. This example, together with the fact that in general the automatic error analysis detects much higher numbers of lexical errors than the human evaluation, indicates that the automatic error analysis could be improved by using a list of synonyms. 4.3.3 Correlations of the POS Classes Across the Translation Outputs. Correlation coefficients of the POS classes across different TC-STAR and WMT 09 translation outputs for each error category are presented in Table 16. For the cases when both error analyses detected zero errors, the correlation coefficients are omitted. It can be seen that the verbs, nouns, and adverbs have high correlations for each of the error categories (except extra words), as well as that the inflectional and reordering errors have high correlations for almost all
POS classes. These error categories and POS classes are used in the further experiments described in the following section.
 lexical errors, the correlations for some of the POS discrepancies between human and automatic error analysis in general is the difficulty of disambiguation between missing words, lexical errors, and extra words. Nevertheless, a deeper analysis (an  X  X rror analysis of error analysis X ) should be carried out in order 678 to better understand all the details. Such an analysis is an interesting and important direction for future work. 5. Comparison of Translation Systems
The experiments in the previous sections showed that the proposed error categories (with the exception of extra words) can be useful for obtaining more information about differences between translation outputs. In this section, we will show some applications.
In order to be able to compare translation outputs generated under different conditions (different target languages, different test set sizes, etc.), we introduce word error rates for each error category, that is, we normalize the number of errors over the total refer-ence length. We do not omit the extra words because this category is informative for the distribution of errors in a particular translation output. Thus the following novel metrics are defined: INF ER ( inf lectional e rror r ate): R ER ( r eordering e rror r ate): MIS ER ( mis sing word e rror r ate): EXT ER ( ext ra word e rror r ate): LEX ER ( lex ical e rror r ate):  X  ER ( sum of e rror r ates):
An overview about how these metrics behave in comparison with the standard word error rates WER , PER ,and TER along with Spearman and Pearson correlation coefficients is presented in Table 17. The BLEU score is also shown as illustration. All error rates are calculated on the translation outputs analyzed in Section 4. We can see that the sum of all error categories  X  ER is always greater than PER , lower than in a majority of cases lower, than TER .
 differences between 680 5.1 Different Language Pairs
Table 18 presents the error rates and the BLEU scores for six translation outputs from the News domain generated by phrase-based systems in the framework of the only a baseline phrase-based system is used in order to focus only on the language-dependent differences. It can be seen that for all English outputs, independently of the source language, the inflectional error rate (INF ER ) of about 2% is the smallest error rate. For the other target languages this is not the case: The French and Spanish outputs have an INF ER between 6% and 7%, and the German output more than 8%. These results could be expected knowing that the English language is not morphologically rich, whereas Spanish, French, and especially German are.
 German, and the highest missing word error rate (MIS
English output generated from the German source text. The category of extra words has been shown not to be reliable for comparison of translation outputs, although it can be seen that all EXT ER scores are rather low in comparison to the other error categories.
The highest lexical error rate (LEX ER ) of 35% can be observed for the German output, followed by the English output generated from the German source text (33%). The lexical error rates for the translation from and into Spanish and French are lower and similar, between 29% and 30%.
 tion from and particularly into German is the hardest. It can also be observed that trans-lation into French and Spanish is more difficult than translation from these languages into English. The results of the error analysis give more details, for example, such that for translation from and into German language the number of reordering errors is higher than for the other language pairs. Furthermore, when translating from German into English, a high number of missing words should be expected. For translation into
German, morphology is an important issue. A high number of inflectional errors is also present for the French and Spanish outputs X  X igher INF translation into French and Spanish is more difficult than the other translation direction. 5.2 Methods for Improvement X  X ore Details About POS -Based Reorderings
In order to better understand the reordering problems and improvements obtained by POS -based reordering techniques, we investigate some more details. For the full
TC-STAR training corpus, we separate the test corpus into two sets X  X ne containing sen-tences whose source sentences have been actually reordered and the other containing the rest of the sentences. Then we calculate the overall R of noun X  X djective groups and the R ER of verbs for each of the sets translated by both systems (without and with reorderings). The results in Table 19 show that the overall R
ER of the reordered set is decreased by the local reorderings whereas for the rest of the sentences a small increase can be observed. Furthermore, it can be noted that for the reordered set the R ER of verbs is significantly smaller than the R adjectives which has been improved by local reorderings. For the rest of the sentences there are no significant differences either between R between the system with reorderings and the baseline system. The same tendencies occur for the other translation direction.
 out a similar experiment on the WMT 09 German-to-English translation output: We separate the test corpus into two sets, and calculate the R calculate the MIS ER s, because the category of missing words has also been shown to be rather problematic for the translation from German into English, and to be improved by long-range reorderings. The results are presented in Table 20. It can be observed that the reordered part of the test corpus has a higher R ER , which is improved by long-range reorderings. However, the R ER of the other part is also indirectly improved. Looking into the specific POS classes, it can be seen that the reordering error rate of nouns and adjectives R ER (N,A) is only slightly higher for the reordered sentences than for the rest, whereas the R ER (V) is significantly higher for the reordered sentences. When the long-range reorderings are applied, the R ER (V) is reduced, but indirectly also the R
As for missing words, the overall MIS ER is similar for both test sets. For the reordered set it is reduced by applying long-range reorderings, and for the rest of the sentences it is slightly increased. The number of missing verbs is much higher for the reordered 682 set than for the rest, whereas the number of missing nouns and adjectives is similar for both sets. The MIS ER (V) of the reordered set is significantly reduced by long-range reorderings, whereas the MIS ER (N,A) remains the same. For the rest of the sentences, there are basically no differences between different POS classes and systems with and without reorderings.
 the specific word classes (i.e., nouns and adjectives), improving mostly these words, whereas long-range reorderings do not affect only the words which are actually reordered (in this case: verbs) but they also introduce some indirect improvements, such as reducing errors of other word classes and reducing the number of missing words. This happens due to better alignment learning and better phrase extraction enabled by applying long-range reorderings. 5.3 Different Translation Systems
For the translation outputs analyzed in the previous section, the same phrase-based translation system is used for all experiments. In order to examine how the new error rates reflect the differences between distinct translation systems, we carried out an error analysis of different translation outputs generated by five distinct translation systems in the second TC-STAR evaluation. A total of nine different systems participated in the evaluation, and we selected five representative systems for our experiments which will of four systems A, B, C, and D, and for Spanish additionally the output of a system
E. The systems A, B, and C are statistical phrase-based and the systems D and E are rule-based.
 the BLEU score as the official metric of the evaluation. For translation into English, the systems A, B, and C have very similar BLEU scores as well as all error categories. The worst ranked system according to the BLEU is system D, and from the error rates it can be seen that the main problem for this system is the incorrect lexical choice. The number of reordering errors is also larger for this system than for the others. and C with the two systems D and E having lower scores. The error rates show that the main differences between systems A, B, and C on the one side and systems D and E on the other are incorrect lexical choices. The number of reordering errors is also higher for systems D and E, and system D in addition has a higher INF WMT 09 systems. Another interesting example can be found for the
English translation task of News data: Among twenty participants, the output gener-ated by the Google system 6 has the best BLEU score as well as the best human sentence ranking score (Callison-Burch et al. 2007), whereas the scores for the translation pro-duced by University of Geneva (Wehrli, Nerima, and Scherrer 2009) are the lowest. The sentence rank of a translation system is defined as the percentage of sentences for which this system is judged by human evaluators to be better or equal than any other system.
The results for these two systems along with two additional medium ranked sys-tems, the statistical Limsi system and a rule based rbmt3 system, can be seen in Table 22.
The BLEU score and the official human rank score are shown along with the five error categories.
 is the lexical error rate LEX ER . The reordering error rate R as the official overall scores. Nevertheless, the Geneva translation output significantly outperforms the other systems in terms of the inflectional error rate INF 684 into the details about POS classes in Table 23, it can be seen that the INF themainreasonforthelowINF ER of the Geneva translation. 6. Discussion
This work describes a framework for automatic error analysis of translation output that presents just a first step towards the development of automatic evaluation measures which provide partial and more specific information of certain translation problems.
The basic idea is to use the actual erroneous words extracted from the standard word error rates WER and PER in combination with linguistic knowledge in order to obtain more information about the translation errors and to perform further analysis of par-ticular phenomena. The overall goal is to get a better overview of the nature of actual translation errors X  X o identify strong and weak points of a translation system, to get ideas about possible improvements of the system, to analyze improvements achieved by particular methods, and to better understand the differences between different trans-lation systems.
 proposed framework. The focus of this work is classifying errors into the following five categories: morphological (inflectional) errors, reordering errors, missing words, extra over POS classes is investigated. This method can be applied to any language pair; the prerequisite is the availability of a morpho-syntactic analyzer for the target language. human error analysis. Detailed experiments on different types of corpora and various language pairs are carried out in order to investigate two applications of error analysis: estimating the contribution of each error category within one translation output, and comparing different translation outputs using the introduced error categories. For the distribution of error categories within a translation output, we show that the results of automatic error analysis correlate very well with the results of human error analysis for all translation outputs. In addition, we show that the differences between results of human and automatic error analysis occur mainly due to the difficulty of disambigua-tion between missing words, lexical errors, and extra words. As for the comparison of different translation outputs, we show that all error categories except extra words correlate well with the human analysis. We also show that verbs, nouns, and adverbs and reordering errors correlate well for almost all POS classes. Nevertheless, for missing words, lexical errors, and extra words, some of the POS classes have low correlations.
The main reason for these discrepancies is again the problematic disambiguation be-tween the three error categories. A deeper analysis should be carried out in order to understand all details, such as to examine which words/ POS particular category by humans but in another category automatically. Such an  X  X rror analysis of error analysis X  is an interesting and important direction for future work.
The proposed framework can be extended in various ways such as going beyond the word level, introducing deeper linguistic categories, using other alignments apart from
WER and R PER /H PER (such as TER , GIZA , etc.), investigating the contribution of source words if source X  X arget alignment information is available, measuring correlations with the human error analysis carried out without reference translations, measuring inter-and intra-annotator agreement for human error analysis and its effects, and so forth. 7. Conclusions tion output. We show that the results obtained by the proposed framework correlate of discrepancies is disambiguation between missing words, lexical errors, and extra words. The new error rates are then calculated for various translation outputs in order to compare them. Different source and target languages are compared to see particular problems for each language pair and translation direction. An analysis of improvements yielded by POS -based reorderings is carried out as well. Finally, we show how the new measures can show differences between distinct translation systems, namely, what are the weak/strong points of particular systems. The presented framework offers a number investigate other set-ups.
 Acknowledgments 686
