 Ta o L i Abstract Clustering is the problem of identifying the distribution of patterns and intrinsic a number of methods have been proposed and demonstrated good performance based on matrix approximation. Despite significant research on these methods, few attempts have been made to establish the connections between them while highlighting their differences. In this paper, we present a unified view of these methods within a general clustering framework where the problem of clustering is formulated as matrix approximations and the clustering objective is minimizing the approximation error between the original data matrix and the reconstructed matrix based on the cluster structures. The general framework provides an elegant base to compare and understand various clustering methods. We provide character-izations of different clustering methods within the general framework including traditional one-side clustering, subspace clustering and two-side clustering. We also establish the con-nections between our general clustering framework with existing frameworks.
 Keywords Clustering  X  Matrix approximation  X  Alternating optimization  X  Subspace 1 Introduction mined by four basic components: (a) the (physical) representation of the given data set; (b) the distance/dissimilarity measures between data points; (c) The criterion/objective function which the clustering solutions should aim to optimize; (d) The optimization procedure. For a given data clustering problem, the four components are tightly coupled. Clustering has been extensively studied in machine learning, databases, and statistics from various perspectives. Many applications of clustering have been discussed and many clustering techniques have been developed.
 ods based on matrix computations and have demonstrated good performance on various datasets. These methods are attractive as they utilize many existing numerical algorithms in matrix computations. Nevertheless, the use of matrix computations in the context of cluster-ing needs more studies. In this paper, we present a generalized clustering framework 1 where the problem of clustering is formulated as matrix approximations. The goal of clustering is then transformed to minimizing the approximation error between the original data matrix and the reconstructed matrix based on the cluster structures. In the framework, the data are usually represented as matrices and the distance measures between data points are Euclidean distances. Hence our discussion in this paper focuses on the criterion/objective function and the optimization procedure. This framework encompasses many previously known clustering algorithms including traditional one-side clustering, co-clustering, and subspace clustering and provides an elegant base to compare and understand various clustering methods. While variations derived from the general framework with different constraints and relaxations. In other words, the general framework provides a basis to establish the connections between various methods while highlighting their differences.

We address the following two questions in the paper: ( a ) What are the different possible model? ( b ) What are the relations between the general model with other existing models? To address the first question, we provide characterizations of different clustering methods within the general framework. We show the close connections between various clustering methods and also explain their distinguishing features. To address the second question, we explore the relationships between our general framework with other existing models. In par-ticular, we show the connections between our general model with the information-theoretic co-clustering framework.

The rest of the paper is organized as follows: Sect. 2 introduces the notations and describes ods within the general framework; Sect. 4 explores the connections between our general model with other models, and finally, our conclusions are presented in Sect. 5 . 2 Clustering model The notations used in the paper are introduced in Table 1 .Wefirstpresentageneralmodel for clustering problem. The model is formally specified as follows: where matrix E denotes the error component. The first term AX B T characterizes the informa-tion of W that can be described by the cluster structures. A and B designate the cluster mem-denote the approximation AX B T and the goal of clustering is to minimize the approximation error (or sum-of-squared-error ) Note that the Frobenius norm, M F , of a matrix M = ( M ij ) is given by M F =
The general model provides a good basis for characterizing various matrix-based cluster-ing approaches and it encompasses many previously known clustering algorithms including traditional one-side clustering, co-clustering, and subspace clustering. 3 Different clustering algorithms Based on different constraints on the matrices A , B and X , this general model encompasses different clustering algorithms. In this section, we provide characterizations of different clustering methods within the general framework. A summary of the derivations is listed in Table 2 . 3.1 One-side clustering Consider the case when C = m , then each feature is a cluster by itself and B = I m  X  m . The model thus reduces to popular one-side clustering, i.e., grouping the data points into clusters. 2 3.1.1 One-side K-means clustering the model reduces to Given A , the objective criterion O is minimized by setting x kj = y kj = 1 p next, etc. 3 Then A can be represented as Note that is a diagonal matrix with the cluster size on the diagonal. The inverse of A T A serves as a weight matrix to compute the centroids. Thus we have the following equation for representing centroids:
On the other hand, given X , O ( A , X ) is minimized by The alternative minimization leads to traditional the K-means clustering procedure [ 14 ]. 3.1.2 One-side low dimensional clustering When data are column-centered, the K cluster centroids always define a ( K  X  1 ) dimensional subspace [ 24 ]. Sometimes, a low-dimensional representation of the cluster structure is very useful and each cluster is represented by a centroid in a low dimensional space. To achieve dimensional reduction, we can restrict that the K centroids lie in an t -dimensional subspace by restricting Rank ( X ) = t , t &lt; = min ( K  X  1 , m ) .

BasedonEq.( 4 ), if we treat A as a constant, then minimizing O ( A , X ) reduces to mini-mizing the following optimization criterion: with the rank constraint on X . This can be solved using low-rank approximation. Mathemat-ically, the optimal rank r approximation of a matrix W , under the Frobenius norm can be
X  X   X  X 2 F . The matrix  X  X can be readily obtained by computing the Singular Value Decom-position (SVD) of X , as stated in the following theorem [ 11 ].
 Theorem 1 Let the Singular Value Decomposition of X  X  R n  X  m be X = USV T ,whereU by the first r columns of U and V , respectively.
 3.1.3 Spectral relaxation BasedonEq.( 5 ), we have If we denote then RR T = A ( A T A )  X  1 A T . Hence Since I  X  RR T is a projection matrix, so ( I  X  RR T )( I  X  RR T ) T = I  X  RR T .
Here minimizing O ( A , X ) is reduced to maximizing Trace ( RW W T R T ) . If we ignore lem then reduced to the trace maximization problem which can be solved by eigenvalue decomposition of the symmetric matrix WW T [ 27 ]. 3.1.4 Concept factorization In K-means clustering described in Sect. 3.1.1 , X represents the centroid (i.e., the average mean) of the data points in the cluster. In general, the cluster centroid can be thought as a linear combination of the data points in the cluster [ 7 ]. In other words, X = SW where S is a K  X  n coefficient matrix. Then If we also treat A as a non-negative coefficient matrix, which denotes the associated degrees of each data point to the clusters, we can use the multiplicative update algorithm described in [ 22 , 25 ] to perform the optimization. If we require the entries in both A and X to be non-negative, the one-side clustering problem is then related to non-negative matrix factor-ization [ 16 ]. The minimization problem is then a constrained optimization problem which can be solved use the Lagrange multiplier methods [ 26 ]. 3.2 Subspace clustering The general model can also be reduced to subspace clustering. Many of the existing clustering algorithms do not work efficiently in high dimensional spaces ( curse of dimensionality ). As in the sense that some data points are correlated with a given set of features and others are cluster usually has its own subspace structure.

To explicitly model the subspace structure for each cluster, let B be a m  X  K matrix, whose entries denote the coefficients of each feature associated with each cluster. Note that WB is the projection of W into the subspace defined by B .Since AX is an approximation of W , hence AX B gives the approximation of WB . To perform the subspace clustering, we want the approximation loss in the projected space to be minimized. This can be thought as a space is minimized.
 The approximation error in the projected space is The columns of B are the coefficients of the features associated with different clusters. They are usually orthogonal. So, the objective criterion is minimized by taking the small-minimization [ 19 ]. 3.3 Two-side clustering Now suppose B is not an identity matrix, then the model leads to many formulations of two-side clustering, i.e., the problem of simultaneously clustering both data points (rows) and features (columns) of a data matrix [ 5 , 13 ]. 3.3.1 Double K-means approach we obtain
For fixed P k and Q c , it is easy to check that the optimum X is obtained by In other words, X can be thought as the matrix of centroids for the two-side clustering prob-O ( A , X , B ) can then be minimized via an iterative procedure of the following steps: 1. Given X and B , then the feature partition Q is fixed, O ( A , X , B ) is minimized by 3. Given A and B , X can be computed using Eq. ( 9 ).
 In general, if we do not require a ik  X  X  0 , 1 } and b jc  X  X  0 , 1 } ,then
For fixed A and B , the optimum X is obtained by
The optimization of A and B can be performed via a penalty clustering which considers both the objective function X  X  partial derivatives and the constraints [ 10 ]. 3.3.2 Iterative feature and data clustering Consider the case when X is a diagonal matrix. Then in the general model, we have C = K , i.e., both data points and features have the same number of clusters. The assumption also implies that, after appropriate permutation of the rows and columns, the approximation data take the form of a block diagonal matrix [ 12 ].
When W is binary data matrix and X is identity matrix, this leads to the cluster model described in [ 18 ]. The objective function can be rewritten as Note that if we relax A and B and let them be arbitrary matrices, then based on ing orthogonal requirements, we could obtain two simplified updating rules which has a natural interpretation analogous to the HITS ranking algorithm [ 15 ].
 In fact, Eqs. ( 15 )and( 16 ) can be thought of as the use of power iteration method for com-puting the singular vectors of WW T [ 11 ]. Basically, the optimizing rules show a mutually reinforcing relationship between the data and the features for binary dataset which can be has a high weight associated with c . 3.3.3 Two-side spectral relaxation In general, if A and B denote the cluster membership, then A T A = diag ( p 1 ,..., p K ) and B algorithm. Note that The minimum of Eq. ( 17 ) is achieved where X = A T WB as  X  O  X  X = X  X  A T WB . Plugging X = A T WB into Eq. ( 17 ), we have maximizing Trace ( A T WBB T W T A ) .

To maximize Trace ( A T WBB T W T A ) , we perform the following alternating optimiza-tion procedure. Let G = WB .Given B , A should maximize Trace ( A T GG T A ) . This can be easily obtained by constructing A with the eigenvectors of GG T corresponding to the Denote H = W T A . So, given A , B should maximize Trace ( B T HH T B ) . This can be easily obtained by constructing B with the eigenvectors of HH T corresponding to the C largest eigenvalues [ 11 ]. The above alternative optimization procedure can be thought as a two-side assignments of the data points and features are obtained by applying ordinary K-means clus-tering in the reduced spaces. A short description of the clustering procedure is presented as Algorithm 1.
 Algorithm 1 Two-side spectral relaxation
It should be noted that there are some connections between the cluster solutions to itera-tive feature and data clustering and the two-side spectral relaxation. If we compute the QR ation, if we compute Singular Value Decomposition(SVD) of X = USV and set A = AU S and B = VB , we could derive the cluster solutions to iterative feature and data clustering. 4 Relations with other models In this section, we show the relations between our general models with the information-theoretic clustering framework and the error-variance approach. 4.1 Information-theoretic clustering Recently, an information-theoretic clustering framework applicable to empirical joint prob-ability distributions was developed for two-dimensional contingency table or co-occurrence matrix [ 6 ]. In this framework, the (scaled) data matrix W is viewed as a joint probability distribution between row and column random variables taking values over the rows and col-umns. The clustering objective is to seek a hard-clustering of both dimensions such that minimized [ 23 ].

In this section, we explore the relations between our general framework and the tributionbetweenrowandcolumnrandomvariables,then I ( W ) = n i = 1 m j = 1 w ij log w ij w
Once we have a simplified K  X  C matrix  X  W , we can construct an n  X  m matrix  X  W as the approximation of original matrix W by preserves marginal probability [ 6 ], it can easily check that Hencewehave So The last step from the above derivation is based on power series approximation of logarithm. of mutual information , i.e., I ( W )  X  I (  X  W ) . 4.2 Error-variance approach in [ 9 ] the variance V defined above. 5Conclusion In this paper, we present a generalized clustering framework by formulating the problem as matrix approximations. The clustering procedure then aims at minimizing the approximation error between the original data matrix and the reconstructed matrix induced by the clus-ter structures. We also provide characterizations of different clustering methods within the general framework including traditional one-side clustering, subspace clustering and two-side clustering and establish the connections between our general clustering framework with existing frameworks.
 References Author Biography
