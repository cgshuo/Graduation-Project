 randomly.
 then binary search only needs O (ln 1 ing requires O ( 1 exponential savings in the label complexity [Das05].
 Any active learning algorithms needs O ( 1 the label complexity [Kaa06].
 learning guaranteed to require fewer labels than passive learning. and the distribution are infinitely smooth, A 2 gives exponential savings. 1.1 Related Works on the realizable case and does not apply to the agnostic setting studied here. our results apply to this algorithm by refined analysis of the normalized bounds. x on a finite sample S is er S ( h ) = 1 disagreement.
 disagree on x . Formally, for any V  X  X  , The volume of DIS ( V ) is denoted by  X ( V ) = Pr X  X  X  characterized by the disagreement coefficient  X  introduced in [Han07]. disagreement coefficient  X  (  X  ) is Note that  X  depends on H and D , and 1  X   X  (  X  )  X  1 for finite smoothness, and exponential for infinite smoothness. terms of the disagreement coefficient. These lead to our main results. 3.1 Smoothness let | k | = P d where is the differential operator.
 to a constant C , if k f k K  X  C . The set of K th order smooth functions is defined as the K  X  1 th order partial derivatives are Lipschitz.
 is denoted by F  X  Definition 4 ( Hypotheses with Smooth Boundaries ) A set of hypotheses H K is said to have K th order smooth boundaries, if for every h  X  H K a a hypothesis space H  X  classification boundary is the graph an infinitely smooth function on [0 , 1] d . Compared with the VC class, H K bers [vdVW96]. But uniform convergence bound still holds for H K probability at least 1  X   X  over the draw of the training set S of n examples, holds simultaneously for all h  X  X  K only on d , K , C and M .
 satisfies log N [ ] (  X , H K that  X  = c 1 exp er
S ( h ) +  X  3.2 Disagreement Coefficient results of large  X  . For instance the interval learning problem,  X  (  X  ) = 1 coefficient  X  (  X  ) for smooth problems is small.
 Theorem 6 Let the hypothesis space be H K  X  [0 , 1] d +1 , then  X  (  X  ) = O Theorem 7 Let the hypothesis space be H  X   X  (  X  ) = O (log d ( 1  X  )) .
 require the density does not change too rapidly.
 boundaries of h  X  and h , and suppose  X  ( h, h  X  ) is small, where  X  ( h, h  X  ) = Pr x  X  X  which leads to a small disagreement coefficient.
 The proofs of Theorem 6 and Theorem 7 rely on the following two lemmas. Lemma 8 Let  X  be a function defined on [0 , 1] d and Lemma 9 Let  X  be a function defined on [0 , 1] d and k  X  k  X  = O ( r  X  log d ( 1 r )) proofs are given in the supplementary file.
 written equivalently as Consider any h  X  B ( h  X  , r ) . Let f h , f h  X   X  F K and h  X  respectively. If r is sufficiently small, we must have  X  ( h, h  X  ) = Pr Denote function g ( x 1 , . . . , x d +1 ) ; and note that  X   X  h ( x 1 , . . . , x d ) = a we have sup h ( x ) 6 = h  X  ( x ) } . Hence The theorem follows by the definition of  X  (  X  ) .
 Theorem 7 can be proved similarly by using Lemma 9. 3.3 Label Complexity It was shown in [Han07] that the label complexity of A 2 is complexity becomes O ( 1 sumption on the noise, the O ( 1 active learning [Kaa06].
  X  ( h, h  X  ) and the excess risk er D ( h )  X  er D ( h  X  ) : where h  X  is the Bayes classifier, c 0 is some finite constant. Here  X  = 1+  X  unbounded noise.
 fact is that  X  O (( 1 learning has very limited improvement over passive learning whatever other factors are. labels requested by A 2 is O (  X  2 ln 1 assume there exist c 1 , c 2 &gt; 0 and T 0 &gt; 0 such that for all T  X  T 0 . It is not difficult to show that (8) implies plexity.
  X  O (  X  2 (  X  )  X  ln 1  X   X  polylog ( 1  X  )) .
 V er i ( h ) is the error rate of h conditioned on DIS ( V i ) . These guarantees er D ( If  X ( V j occurs. By the definition of  X  (  X  ) , if  X  ( h, h  X  )  X   X ( V j k )  X  ( h ) = er D ( h )  X  er D ( h  X  ) . By the noise assumption (9) we have that if then  X ( V i ) &lt; 1 line to line. Note that (10) holds if  X  ( h )  X  c  X ( V j k )  X ( V j Now we give our main label complexity bounds for agnostic active learning. Theorem 11 Let the instance space be [0 , 1] d +1 . Let the Hypothesis space be H K Assume that the Bayes classifier h  X  of the learning problem is in H K most  X  O bounded by a constant M . Combining Theorem 5, 6 and 10 the theorem follows. Combining Theorem 5, 7 and 10 we can show the following theorem. Theorem 12 Let the instance space be [0 , 1] d +1 . Let the Hypothesis space be H  X  the Bayes classifier h  X  of the learning problem is in H  X  O  X  smoothness is of finite order or infinite.
 N proofs can be found in the supplementary file.
 Ideas to Prove Lemma 8 First consider the d = 1 case. Note that if f  X  F K any f such that | f ( K  X  1) ( x )  X  f ( K  X  1) ( x 0 ) | X  C | x  X  x 0 | , if To show this, note that in order that k f k  X  achieves the maximum while of bound of the Lipschitz constant.) for all x, x 0  X  [0 ,  X  ] , where  X  is determined by is then easy to check that k f k  X  = O ( r K K +1 ) .
 constraint, (one of) the optimal f has the form where  X  is determined by f  X  F  X  C , if will be out of [0 , 1] .) Since that ( 1 For the d &gt; 1 case, let K + d = log 1 r This work was supported by NSFC(60775005).
