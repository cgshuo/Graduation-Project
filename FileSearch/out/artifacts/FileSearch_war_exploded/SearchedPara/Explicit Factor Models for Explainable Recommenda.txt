 Collaborative Filtering(CF)-based recommendation algorithms, such as Latent Factor Models (LFM), work well in terms of prediction accuracy. However, the latent features make it difficulty to explain the recommendation results to the users.
Fortunately, with the continuous growth of online user re-views, the information available for training a recommender system is no longer limited to just numerical star ratings or user/item features. By extracting explicit user opinions about various aspects of a product from the reviews, it is possible to learn more details about what aspects a user cares, which further sheds light on the possibility to make explainable recommendations.

In this work, we propose the Explicit Factor Model (EFM) to generate explainable recommendations, meanwhile keep a high prediction accuracy. We first extract explicit product features (i.e. aspects) and user opinions by phrase-level sen-timent analysis on user reviews, then generate both recom-mendations and disrecommendations according to the spe-cific product features to the user X  X  interests and the hid-den features learned. Besides, intuitional feature-level ex-planations about why an item is or is not recommended are generated from the model. Offline experimental results on several real-world datasets demonstrate the advantages of our framework over competitive baseline algorithms on both rating prediction and top-K recommendation tasks. Online experiments show that the detailed explanations make the recommendations and disrecommendations more influential on user X  X  purchasing behavior.
 H.3.3 [ Information Storage and Retrieval ]: Information Filtering; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Recommender Systems; Sentiment Analysis; Collaborative Filtering; Recommendation Explanation
In the last few years, researchers have found or argued that explanations in recommendation systems could be very beneficial. By explaining how the system works and/or why a product is recommended, the system becomes more trans-parent and has the potential to allow users to tell when the system is wrong (scrutability), increase users X  confidence or trust in the system, help users make better (effectiveness) and faster (efficiency) decisions, convince users to try or buy (persuasiveness), or increase the ease of the user enjoyment (satisfaction). A variety of techniques have been proposed to generate explanations, mainly for content based recom-mendation algorithms, neighbor based algorithms, or simple statistics analysis based algorithms.
 Meanwhile, Latent Factor Models (LFM) such as Matrix Factorization (MF) [14] techniques have gained much at-tention from the research community and industry due to their good prediction accuracy on some benchmark datasets. However, recommender systems based on these algorithms encounter some important problems in practical applica-tions. First, it is difficult to know how users compose their judgement of the various attributes of an item into a single rating, which makes it difficult to make recommendations according to the specific needs of the users. Second, it is usually difficult to give intuitional explanations of why an item is recommended, and even more difficult to explain why an item is not recommended given other alternatives. Lack of explainability weakens the ability to persuade users and help users make better decisions in practical systems [39].
A dilemma practitioners often face is whether to choose an understandable/explainable simple algorithm while sacrific-ing prediction accuracy, or choose an accurate latent factor-ization modeling approach while sacrificing explainability. A major research question is: can we have a solution that is both highly accurate and easily explainable?
Fortunately, the advance detailed sentiment analysis and the ever increasing popularity of online user textual reviews shed some light on this question. Most e-commerce and re-view service websites like Amazon and Yelp allow users to write free-text reviews along with a numerical star rating. The text reviews contain rich information about user sen-timents, attitudes and preferences towards product features [19, 8, 9], which sheds light on new approaches for explain-able recommendation. For example, from the review text  X  X he service rendered from the seller is excellent , but the battery life is short  X , the entries ( service, excellent , +1) and ( battery life, short ,  X  1) of the form ( F,O,S ) could be ex-tracted by phrase-level sentiment analysis [19], where F is for Feature word or phrase that reveals some product as-pect, O is for Opinion word or phrase that the user chose to express the attitude towards the feature, and S is the Senti-ment of the opinion word when commenting on the feature word, which could be positive or negative.
 Different users might care about different product aspects. We found that users tend to comment on different features in textual reviews, e.g. , one would mostly care about the screen size of a mobile phone, while another might focus on its battery life, although they might even make the same star rating on the product. Extracting the explicit product fea-tures and the corresponding user opinions from reviews not only helps to understand the different preferences of users and make better recommendations, but also helps to know why and how a particular item is or is not recommended, thus to present intuitional explanations. In this way, we could not only recommend to users about which to buy, but also presenting disrecommendations by telling the users why they would better not buy.

Based on our preliminary analysis, we propose a new Ex-plicit Factor Model (EFM) to achieve both high accuracy and explainability. Figure 1 illustrates the overview of the proposed solution with an example. First, phrase-level sen-timent analysis over textual review corpus generates a sen-timent lexicon, where each entry is an ( F,O,S ) triplet, and the feature words together serve as the explicit feature space. Then, user attentions and item qualities on these features are integrated into a unified factorization model (i.e. EFM), which are later used to generate personalized recommenda-tions and explanations. In this example, the system iden-tified that a user might care about memory , earphone and service , thus a product that performs especially well on these features would be a promising recommendation for him.
In the rest of this paper, we first review some related work (Section 2) and provide detailed expositions of our approach, including the algorithm for model learning (Section 3). Then we describe the offline experimental settings and results for verifying the performance of the proposed approach in terms of rating prediction and top-K recommendation (Section 4), as well as online experiments for testing the effect of in-tuitional explanations (Section 5). Finally, we conclude the work, discuss the limitations of the work and point out some of the future research directions in Section 6. With the ability to take advantage of the wisdom of crowds, Collaborative Filtering (CF) [33] techniques have achieved great success in personalized recommender systems, espe-cially in rating prediction tasks. Recently, Latent Factor Models (LFM) based on Matrix Factorization (MF) [14] techniques have gained great popularity as they usually out-perform traditional methods and have achieved state-of-the-art performance in some benchmark datasets [33]. Various MF algorithms have been proposed in different problem set-tings, such as Singular Value Decomposition (SVD) [14, 32], Non-negative Matrix Factorization (NMF) [15], Max-Margin Matrix Factorization (MMMF) [29], Probabilistic Matrix Factorization (PMF) [30], and Localized Matrix Factoriza-tion (LMF) [45, 44]. They aim at learning latent factors from user-item rating matrices to make rating predictions, based on which to generate personalized recommendations. How-ever, their latent characteristic makes it difficult to make recommendations in situations where we know a user cares Figure 1: The product feature word and user opin-ion word pairs are extracted from user review corpus to construct the sentiment lexicon, and the feature word set further serves as the explicit feature space. An item would be recommended if it performs well on the features that a user cares. about certain particular product features. Further more, it is also difficult to generate intuitional explanations for the recommendation results. Besides, the frequently used met-rics such as RMSE and MAE do not necessarily have direct relationship with the performance in practical top-K recom-mendation scenarios [4].

It is important to notice that the increasingly growing amount of user generated textual reviews contain rich infor-mation about product features and user preferences, which can be extracted, summarized and structured by Sentiment Analysis (SA) [18, 24]. Sentiment analysis can be conducted on three different levels: review/document-level, sentence-level and phrase-level. Review-[25, 42] and sentence-level [40, 22] analysis attempt to classify the sentiment of a whole review or sentence to one of the predefined sentiment po-larities, including positive , negative and sometimes neutral , while phrase-level analysis [41, 19, 6] attempts to extract ex-plicit features of the products, and further analyze the senti-ment orientations that users express on these features based on opinion words that the users use to express the attitude towards the features [11]. The core task in phrase-level sen-timent analysis is the construction of Sentiment Lexicon [34, 17, 6, 19], where each entry is a (Feature,Opinion,Sentiment) triplet. This is generated by extracting feature-opinion word pairs and determining their sentiments from text reviews. The lexicon could be used in various sentiment related tasks. It is necessary to note that the sentiment lexicon could be contextual [41]. For example, the opinion word high has a positive sentiment when modifying the feature word quality , yet has a negative sentiment when accompanied by noise .
With the continuous growth of such information-rich sources, how to use textual reviews in recommender systems has re-ceived increasing attention over the last few years [1, 20, 21, 26, 36, 12, 13, 7, 27]. The most early approach con-structs manually defined ontologies to structure the free-text reviews [1]. However, constructing ontologies is domain-dependent and time consuming, and it is also not nicely integrated with the wisdom of crowds as in CF. More recent work leverages review-or sentence-level sentiment analysis to boost the performance of rating prediction [12, 13, 7, 27, 26]. However, rating prediction accuracy is not necessar-ily related to the performance in practical applications [4]. Besides, the sentence-or review-level analysis does not of-fer rich finer-grained sentiment information, which can be better utilized in recommendation and explanation.
In an attempt to address the problems, some recent work conducted topic discovery from the reviews to estimate user preferences for personalized recommendations [20, 21, 36]. However, as a user could in fact be criticizing rather than appreciating the product on a mentioned topic, simple topic level analysis without more detailed natural language pro-cessing makes such approaches biased and limited. In con-trast, we leverage phrase-level sentiment analysis to model user preferences and item performances on an explicit fea-ture/aspect for more detailed user preference modeling, more accurate predicting, and more intuitional explanations.
Researchers have shown that providing appropriate expla-nations could improve user acceptance of the recommended items [10, 37], as well as benefit user experience in various other aspects, including system transparency, user trust, ef-fectiveness, efficiency, satisfaction and scrutability [38, 3, 2]. However, the underlying recommendation algorithm may in-fluence the types of explanations that can be constructed. In general, the computationally complex algorithms within various latent factor models make the explanations difficult to be generated automatically [38].

Many meticulously designed strategies have been investi-gated to tackle the problem, ranging from the simple  X  X eo-ple also viewed X  explanations in e-commerce websites [38] to the more recent social friends or social tags based explana-tions [31, 39]. However, such explanations are either over simplification of the true reasons, or difficult to generate in non-social scenarios. Nevertheless, an important advantage of utilizing explicit features for recommendation is its ability to provide intuitional and reasonable explanations for both recommended and disrecommended items. Besides, the pro-posed model can also integrate social or statistical features, and thus generalizes the existing explanation solutions.
This section describes the major components of the pro-posed framework (Figure 1). The major notations used in the rest of the paper are summarized in Table 1. L is gener-ated by phrase-level sentiment analysis on user reviews. A is the numerical user-item rating matrix. The matrices X and Y are constructed by mapping the reviews of each user and item onto the lexicon L . The following sub-sections describe more details about the framework.
In the first stage, the contextual sentiment lexicon L is constructed from textual user reviews based on the state-of-the-art optimization approach described in [19, 43]. This process mainly consists of three steps. First, the feature word set F are extracted from the text review corpus using grammatical and morphological analysis tools. Then, the opinion word set O is extracted and further paired up with the feature words where possible. This leads to the feature-opinion pairs ( F,O ). Finally, sentiment polarity labelling of these pairs are conducted based on an optimization frame-Figure 2: An example of user-item review matrix, where each shaded block is a review made by a user towards an item; the entries included in the review are extracted, and further transformed to feature scores while considering the negation words. work, and each pair is assigned a sentiment value S , and this leads to the final entries ( F,O,S ) in L .

Since the lexicon construction procedure is not the focus of this paper, we refer the readers to the related literature such as [19, 43] for more details. The quality of the constructed lexicon will be reported later in the experimental section.
Given the sentiment lexicon L and a piece of text review, we generate a set of feature-sentiment ( F,S 0 ) pairs to repre-sent the review, where S 0 is the reviewer X  X  sentiment on the particular product feature as expressed in the review text. To illustrate the idea, we consider a toy example shown in Figure 2. Each shaded block is a user X  X  review about an item, which includes a numerical rating and a piece of review text. Based on that, we need to identify which lexicon entries are mentioned by the user in the review text, and whether the sentiment polarity of the entry is reversed by negation words like  X  X ot X  or  X  X ardly X . Then we can generate a set of feature-sentiment pairs to represent a piece of review text. In this example, the set includes ( screen , +1) and ( earphone , -1).
Without lose of generality, we adopt the approaches in [19] and [35]. The algorithm analyzes the review text on clause level by parsing sentences into syntactic tree structures, with which a rule-based finite state matching machine approach is conducted for feature-opinion pair mapping, and the en-tries such as ( screen, perfect , +1) and ( earphone, good , +1) in L are matched in the review. Then, negation words de-tection is conducted to check whether the sentiment of each matched entry is reversed. For example, the sentiment of en-try ( earphone, good , +1) is reversed because of the presence of the negation word  X  X ot X . Then we have:
We assume that different users might care about differ-ent features, and they tend to comment more frequently on those features that he or she particularly cares. Thus we construct a user-feature attention matrix X , where each el-ement measures to what an extent a user cares about the corresponding product feature/aspect.

Let F = { F 1 ,F 2 ,  X  X  X  ,F p } be the set of explicit product features/aspects, and let U = { u 1 ,u 2 ,  X  X  X  ,u m } denote the m users. To generate the matrix, we consider all the text reviews written by a user u i , then extract all ( F,S 0 ) entries in the collection. Suppose feature F j is mentioned by user u for t ij times, we define each element in the user-feature attention matrix X as follows:
The major goal of (2) is to rescale t ij into the same range [1 ,N ] as the rating matrix A by reformulating the sigmoid function. The choice of N is 5 in many real-world five stars based reviewing systems, such as Amazon and Yelp.
We also construct item-feature quality matrix Y , where each element measures the quality of an item for the corre-sponding product feature/aspect.
 Let P = { p 1 ,p 2 ,  X  X  X  ,p n } denote the n items/products. For each of the items p i , we use all of its corresponding re-views, and extract the corresponding ( F,S 0 ) pairs. Suppose feature F j is mentioned for k times on item p i , and the av-erage of sentiment of feature F j in those k mentions are s We define the item-feature measure Y ij as:
This measure captures both the sentiment orientation (through s ) and the popularity (through k ) of feature F j for product p . It is also rescaled into the range of [1 ,N ].
The non-zeros in matrices X and Y indicate the observed relations between users, items and explicit features. Now we exposit how to integrate these into a factorization model for both accurate prediction and explainable recommendations.
Similar to factorization models over user-item rating ma-trix A , we can build a factorization model over user-feature attention matrix X and item-feature quality matrix Y , which means estimating hidden representations of users, features, and items based on the observed user-feature and item-feature relations. This can be done as follows: where  X  x and  X  y are regularization coefficients, and the number of explicit factors r represents the number of factors.
We assume a user X  X  overall star ratings about an item (i.e. an element in matrix A ) is based on her underlying opinions over various product aspects. Based on this assumption, we estimate the rating matrix using the latent representations U 1 and U 2 , which captures the user attentions and item qualities on the explicit product features. However, we also acknowledge that the explicit features might not be able to fully explain a rating and a user might consider some other hidden factors when making a decision (i.e. rating) about a product. As a result, we also introduce r 0 latent factors H Q = [ U 2 H 2 ] to model the overall rating matrix A as follows:
To put Eq.(4) and Eq.(5) together, we have a factorization model that integrates explicit and implicit features. Figure 3: The relationships of product features, par-tially observed matrices, and hidden factors in the EFM framework.

The underlying relationships of product features, partially observed matrices, and hidden factors are illustrated in Fig-ure 3. The hidden factors can be estimated through the following optimization task:
When r = 0, this model reduces to a traditional latent fac-torization model on user-item rating matrix A, which means that the explicit features are not used for recommendations. The optimal solution of Eq.(6) can be further used for mak-ing recommendations and explanations.
There is no closed-form solution for Eq.(6). Motivated by [5], we introduce an alternative minimization algorithm (Algorithm 1) to find the optimal solutions for the five pa-rameters U 1 ,U 2 ,V,H 1 ,H 2 . The key idea is to optimize the Algorithm 1: Explicit Factor Model Input : A,X,Y,m,n,p,r,r 0 , X  x , X  y , X  u , X  h , X  v ,T Output : U 1 ,U 2 ,V,H 1 ,H 2 t  X  0; repeat until Convergence or t &gt; T ; return U 1 ,U 2 ,V,H 1 ,H 2 ; objective function with respect to one parameter, while fix-ing the other four. The algorithm keeps updating the param-eters repeatedly until convergence or reaching the maximum number of iterations. Due to the limited space, we sketch the derivations of the updating rules in the Appendix. Similar to [15], the correctness and convergence of Algorithm 1 can be proved with the standard auxiliary function approach.
Given the optimal solution of the factorization model, we can estimate any missing element in the user-feature at-tention matrix X , item-feature quality matrix Y and user-item rating matrix A , as  X  X = U 1 V T ,  X  Y = U 2 V  X  A = U 1 U T 2 + H 1 H T 2 . Based on that, we can generate per-sonalized top-K recommendations and provide feature-level explanations, as described in the following subsections.
We assume that a user X  X  decision about whether to make a purchase is based on several important product features to him or her, rather than considering all hundreds of possible features. Thus we use the most cared k features of a user when generating the recommendation list for him/her. For user u i (1  X  i  X  m ), let the column indices of the k largest values in row vector  X  X i  X  be C i = { c i 1 ,c i 2 ,  X  X  X  ,c the ranking score of item p j (1  X  j  X  n ) for user u i as follows: where N = max( A ij ) is used to rescale the first part and N = 5 in most rating systems. The first part is a user-item similarity score based on the k most important product features that user u i cares. 0  X   X   X  1 is a scale that controls the trade off between feature based score and direct user-item ratings. The top-K recommendation list for user u can be constructed by ranking the items based on R ij .
Traditional factorization models are usually difficulty to understand and it is hard to automatically generate explana-tions about why or why not an item is recommended through the hidden factors. An important advantage of our EFM framework is its ability to analyze which of the various fea-tures play a key role in pushing an item to the top-K. Besides recommending products, we also study an uncommon usage scenario of our framework: disrecommending an item that the system considers  X  X oes not worth buying X  when a user is viewing it. We believe that this is an important usage of a filtering/recommendation system, and that explaining why the system  X  X hinks X  the user should not buy a product might gain user trust and help user make a more informed purchasing decision.

There could be many different ways to construct effective explanations with the explicit features. However, due to the limited experimental resources, we designed a straightfor-ward template based explanation and an intuitional word cloud based explanation for the recommended and disrec-ommended items. The templates are shown as follows:
For each user u i and a recommended item p j , the feature used for explanation construction is F c , where:
While for each disrecommended item p j , the feature F c is:
We focus on the persuasiveness of the explanation. Among the user X  X  most cared features, we select the one that has the best or worst performance on the corresponding product.
The word cloud based explanation further attempts to val-idate the textual explanations by listing the feature-opinion pairs that are matched on the reviews of a (dis)recommended item. The recommendation interface, word cloud as well as the practical effects regarding user acceptance of the recom-mendations will be discussed in details later.
In this section, we conduct extensive experiments to evalu-ate the EFM framework in offline settings. We mainly focus on the following research questions: 1) How do users care about the various explicit product features extracted from reviews, and 2) What is the performance of the EFM frame-work in both the task of rating prediction and the more practical task of top-K recommendation.

We begin by introducing the experimental setup, and re-port the quality evaluation of the constructed sentiment lex-icon, then we report and analyze the experimental results to attempt to answer the research questions.
We choose the Yelp 1 and Dianping 2 datasets (Table 2) for experimentations. The Yelp dataset consists of user re-views on the businesses located in the Phoenix City of the US, and the Dianping dataset consists of the reviews on the restaurants located in several main cities of China, where each user made 20 or more reviews.
 Table 2: Statistics of Yelp and Dianping datasets. These datasets are of two different languages, which are English on Yelp, and Chinese on Dianping. As a kind of nat-ural language processing techniques, phrase-level sentiment analysis could be language dependent. By these completely different language settings we would like to verify whether our framework works in different language environments.
It is necessary to note that the Yelp dataset includes those users that made very few reviews. In fact, 49% of the users made only one review, which makes it difficult to evaluate the performance of top-K recommendation. As a result, we selected the users with 10 or more reviews for the experi-mentation of top-K recommendation, which constitutes the dataset  X  X elp10 X , as shown at the bottom line of Table 2.
The maximum number of iterations T in Algorithm 1 is set to 100 to ensure convergence. We conducted grid search for the hyper-parameters one-by-one in the range of (0 , 1] with a step size of 0 . 05, and 5-fold cross-validation was conducted in performance evaluation for all methods. http://www.yelp.com/dataset_challenge http://www.dianping.com
We shall note that, the task of phrase-level sentiment lex-icon construction is inherently difficult. One always need to trade off between precision and recall. As a primary step to-wards using sentiment lexicon for EFM, we focus on the pre-cision as we will only use the top features in our framework, primarily to avoid the negative effects of wrong features as much as possible. We expect as the research in sentiment analysis advances, the performance of our framework will further improve as well.

Similar to [19], manual labellings from three human an-notators are used to evaluate the lexicon quality. A feature word or opinion word is considered to be proper if it is ap-proved by at least two annotators. We found that the aver-age agreement among annotators is 85 . 63%. To evaluate the sentiments, we transform each polarity value S  X  [  X  1 , 1] into a binary value ( positive or negative ). There is no neutral as S = 0 does not exist. The annotators were asked to label the sentiment correctness for each entry, and the average agreement is 82 . 26%. The statistics and evaluation results of the lexicons are shown in Table 3.
 Table 3: Some statistics and evaluation results of the sentiment lexicons, where  X  X ,O,S X  are for feature word, opinion word and sentiment respectively, and  X  X rec X  stands for  X  X recision X .

The lexicon construction process gives us around 100 high quality product features in both datasets. Some sampled entries on Yelp are presented in Table 4, showing how the sentiment lexicon looks in different contextual text reviews. Table 4: Sampled entries from the Yelp dataset.

To verify the assumption that users might care about dif-ferent features, we construct the user-feature attention ma-trix X on  X  X elp X  dataset, and conduct a simple fuzzy k -means clustering to map the users into 5 fuzzy clusters within the Euclidean distance space. The reason to choose fuzzy clus-tering is due to the fact that a users might be interested in two or more product aspects. For the k -th ( k = 1  X  X  X  5) cluster, we rank the features according to the weighted sum frequency Freq( F j ) = P m i =1 w ik t ij , where w ik is the degree that user u i belongs to the k -th cluster, and t ij is the fre-quency that user u i mentioned feature F j . We then select the top-5 frequent features from each cluster and rank them in descending order of term frequency, as listed in Table 5.
Table 5: Top-5 frequent features of the 5 clusters.
We see that the users in each cluster care about a different subset of features, and each subset mainly reveals a different product aspect. The modularity (measures the quality of a division of a network into communities, which is commonly used in community detection [23]) among the clusters is Q = 0 . 605, exhibiting a high inner-cluster relationship against inter-cluster relationship of the users. The results indicate that users do comment on different features, which matches our assumption that users care about different aspects. In the following experiments, we will continue to investigate the effect of explicit features in various conditions.
We use the original sentiment lexicon and do not use any human annotation information in the following experiments, so as to avoid any manual effort in our framework.
We investigate the performance in approximating the rat-ing matrix A . Three popular and state-of-the-art latent fac-tor models are chosen for comparison, which are Nonnega-tive Matrix Factorization (NMF) [15], Probabilistic Matrix Factorization (PMF) [30] and Max-Margin Matrix Factor-ization (MMMF) [29]. The hyper-parameters are selected by grid search in 5-fold cross-validation. We also compared with the Hidden Factors as Topics (HFT) model in [20], which achieved the state-of-the-art performance in terms of leveraging text reviews by extracting hidden topics. We set the number of topics as 5 in HFT as reported in [20], and assigning more topics does not further improve the perfor-mance. We evaluate by Root Mean Square Error (RMSE).
The number of explicit factor r and latent factors r 0 are important because they capture two different types of fea-tures in the factorization process. In this subsection, we investigate the effect of explicit and latent factors by fixing their total number r + r 0 = 100, and tuning their ratio. We also use 100 latent factors for NMF, PMF and MMMF to ensure equal model complexity 3 . The experimental results of RMSE vs the number of explicit factors r are shown in Figure 4. The standard deviations in 5-fold cross-validation of each baseline algorithm and on each experimental point of our EFM framework are  X  0 . 002. Figure 4: RMSE vs Number of Explicit Factors r .

We see that when using a small number of explicit fac-tors and keeping the majority to be latent, the performance of EFM is comparable to that of HFT or NMF. This is as expected, because the EFM algorithm simplifies into a kind of nonnegative matrix factorization when r = 0. However, when an appropriate percentage of explicit factors (around 30%  X  80%) are used, EFM is statistically significantly better than the baselines. The best prediction accuracy RMSE=1.212 is achieved on Yelp when r = 35, and the best RMSE=0.922 when r = 50 on Dianping. We also found that too many explicit factors hurts the performance. This sug-gests that although incorporating explicit factors improves
Besides, increasing the number of factors beyond 100 would not affect the performance much. the prediction accuracy, keeping a moderate amount of la-tent factors is necessary to ensure model flexibility, as the ex-plicit factors might not capture all user criteria completely.
We further investigate the total number of factors r + r 0 verify the performance under different model complexities. We fix the percentage of explicit factors to 40%, which gives near optimal performances on both datasets in the previ-ous experiment. For comparison, we experiment with HFT, which achieved the best performance among baseline algo-rithms, and NMF, which is the best among three of the LFM models. The results are shown in Figure 5.
 Figure 5: RMSE vs total number of factors r + r 0 .
For both the NMF algorithm and EFM framework, RMSE tends to drop as r + r 0 increases. However, the performance of EFM would not surpass NMF or HFT until r + r 0 is big enough. This suggests that we need enough latent factors in order to make EFM framework work effectively. Neverthe-less, as long as the number of latent factors are big enough to capture the underlying user rating patterns, the existence of explicit factors further improves rating prediction.
In this set of experiment, we study the EFM framework in the top-K recommendation task. We did further analysis of the framework to find whether and how the performance is affected by some specific features, and which features are of key importance to the users in the task.
 We still set the percentage of explicit factors to be 40%. We set the total number of factors r + r 0 as 65, a number that is big enough on both datasets, because assigning more fac-tors would not affect the performance much. We compared EFM with the following baseline algorithms:
MostPopular : Non-personalized recommendation where the items are ranked by the number of ratings given by users.
SlopeOne : A neighborhood-based CF algorithm with easy implementation and comparable or better performance than user-or item-based approaches [16].

NMF : Nonnegative Matrix Factorization algorithm that is among the best latent factor models in the previous ex-periment.

BPRMF : Bayesian Personalized Ranking (BPR) opti-mization for MF [28], which is the state-of-the-art algorithm for top-K recommendation with numerical ratings.

HFT : The state-of-the-art algorithm in terms of making rating prediction with textual reviews [20].

We conduct top-5 (K=5) and top-10 (K=10) recommen-dation on Yelp10 and Dianping correspondingly, as the min-imum number of reviews per user is 10 on Yelp10 and 20 on Dianping. We holdout the latest K reviewed items from each user for testing, assuming they are the only relevant items. The other reviews are used for training. Five-fold cross-Table 6: Top-K recommendation results on Dian-ping dataset, where the result listed for EFM is the best performance with the corresponding k .
 validation is used for parameter tuning and performance evaluation. Following [28], we used Normalized Discounted Cumulative Gain (NDCG) and Area Under the ROC Curve (AUC) to help evaluate the performance of different models.
We first fix the number of most cared features k = 10 and found that the optimal value for the weighing scalar  X  is 0 . 85 in terms of NDCG. Then we fix  X  = 0 . 85 throughout the following experiments to focus on the key parameter k .
We study how the performance (NDCG and AUC) changes as k increases from 5 to the maximum value possible (96 for Yelp10 and 113 for Dianping), and the results on Yelp10 are shown in Figure 6. It shows that the performance of EFM continues to rise with the increase of k until around 15, and tends to be stable before it begins to drop when k = 45. It outperforms all other algorithms when k  X  70, and in the best case where k = 45, EFM is 12 . 3% better than the best baseline algorithm BPRMF. However, the NDCG tends to drop when k is too high. The results on Dianping are similar, as shown in Table 6. EFM is statistically significantly bet-ter than other algorithms on Yelp10 and Dianping datasets. The standard deviations of both NDCG and AUC in five-fold cross-validation for each baseline algorithm and on each experimental point of EFM are  X  0 . 006. Figure 6: NDCG and AUC vs the number of most cared features k on the Yelp10 dataset.

This observation confirms the hypothesis that incorporat-ing too many features would introduce noise into the rec-ommendation procedure, which is consistent with the ob-servations in a recent work HFT [20]. However, the results on AUC show that the EFM framework is better than the comparative algorithms consistently. As AUC evaluates only the pairwise rankings rather than the positions, this obser-vation suggests that incorporating irrelevant features affects the position rankings of relevant items more than their pair-wise rankings.
Although the results show that the performance on NDCG begins to keep stable when k  X  15, it is still surprising for us, as we do not expect that users would consider tens of features when making decisions. To further analyze the user-feature relationships and learn the impact of explicit features, we calculate the averaged coverage of the k most cared features in term frequency, against all the features in the reviews of a user, as shown in Eq.(10), where t ij is the term frequency of feature F j in the reviews of user u i , and the relationship of Coverage versus the number of most cared features k is shown in Figure 7 below. Figure 7: Coverage of term frequency vs the number of most cared features k on both of the datasets.

We notice that a small number of the most cared features dominate the coverage of term frequency in user reviews. For example, about 24 out of 96 features cover up to 80% of the feature occurrences on Yelp. This implies that users usually put most of the attentions on several most cared features, which verifies our motivation to adopt the top k features for recommendation, because incorporating more features could bring about negative effects as they might be irrelevant to users X  preferences.

However, it needs 46 features to achieve the same 80% cov-erage on Dianping dataset, which is nearly twice the number of Yelp10. To understand the reason, we group the feature words into synonym clusters using WordNet 4 on Yelp10, and HowNet 5 on Dianping. We find that different feature words are used to describe the same aspect of the items. For ex-ample, the features price and cost are grouped into a single cluster. Some key statistics are shown in Table 7.
The results show that on average there are more synonyms in the Chinese language, and the existence of synonyms di-lutes the explicit feature space. For example, while there are two features price and cost grouped into one cluster on Yelp10, the semantically corresponding cluster on Dianping contains four feature words. This suggests that the optimal number of features might be different for different languages. The top 15 features on Yelp10 are grouped into 7 clusters, as shown in Table 8, where the top 15 features are bolded and the clusters are ranked by total term frequency. http://wordnet.princeton.edu http://www.keenage.com (a) In browser recommendation (b) Word cloud Figure 8: Top-4 recommended items are presented by the browser at right hand side when the user is browsing an online product, and the feature-opinion word pair cloud is displayed to assist explanations when the user hovers on a recommended item.

The observations tell us that while the NDCG continues to rise when k  X  15, the underlying features are still limited to about 7 key aspects. Besides, we also find that incorporat-ing the top 15  X  55 features are in fact appending features into the previously existed clusters. This might explain why NDCG keeps stable in that range before getting to drop.
The experimental results confirmed our intuition of focus-ing on the top product features in recommender systems. More importantly, incorporating explicit features makes it possible to leverage the many promising natural language processing techniques in recommender systems, which can be used to analyze the specific interests of different users, and thus to develop effective marketing strategies in e-commerce.
In this section, we conduct online experiments with real-world e-commerce users to investigate the effect of automat-ically generated intuitional feature-level explanations, focus-ing on how the explains affect users X  acceptance of the rec-ommendations (i.e. persuasiveness).
We conduct A/B-tests based on a popular commercial web browser which has more than 100 million users, with 26% monthly active users. Our experiments attempt to recom-mend relevant phones when a user is browsing mobile phones in a very popular online shopping website JingDong 6 .
Figure 8(a) shows our recommendation interface, where the recommendations for the current browsing item are dis-played in the right side popup panel. At the top of the panel is an  X  X ndicator X , which shows whether the current brows-ing item is recommended or disrecommended, followed by a  X  X ist X  of top-4 recommendations for the current user. Figure 8(b) shows the word cloud where the positive feature-opinion pairs are green and negative pairs are blue, and the size is proportional to the term frequency in the reviews of the rec-ommended item. For example, the largest pair in Figure 8(b) means  X  X ictureClarity-High X .

We select those users who made ten or more reviews as tar-get users according to the browser log, and collect their re-views to employ the EFM framework to generate (dis)recom-mendations and explanations for these selected users. The textual explanation and word cloud for the  X  X ndicator X  or a recommended item in the  X  X ist X  will be displayed in a popup panel when the mouse is hovered on an item, so that we can detect whether a user has examined a recommendation and its explanation. http://www.jd.com
To study the explanations in the  X  X ist X , we randomly as-signed each subject to one of the following three groups. The A (experimental group) users receive our feature-level expla-nations, the B (comparison group) users receive the famous  X  X eople also viewed X  explanations, and the C (control group) users receive no explanation. In our result analysis, we only consider those users who hovered the mouse on the List, so we know that they have examined the recommendation explanations. Besides, we only consider the browsing logs corresponding to those items common to A, B and C, re-sulting in 44,681 records related to 944 common items. The results of Click Through Rate (CTR) are shown in Table 9. Table 9: The number of browsing records, clicks and click through rate for the three user types.

Based on ten-fold t-test, we found that the CTR of the experimental group A is significantly higher than that of the comparison group B and control group C, at p = 0 . 033 and 0 . 041, respectively, demonstrating that our feature-level explanations are more effective in persuading users to exam-ine the recommended items in terms of click through rate in e-commerce recommendation scenarios.
To study how (dis)recommendation explanations affect a user, we present the Indicator for both user group A (exper-imental group) and B (control group). The only difference is that the A users could see the popup explanations when hovering the mouse over the Indicator, while nothing shows for the B users. We did not assign a comparison group with other explanations, because to the best of our knowledge, there is no previous work on presenting disrecommnedation explanations. Due to some security reasons, we were unable to track whether a user paid for an item, so we recorded whether a user added the current browsing item into his/her cart as an approximation of user purchasing behavior.
This online experiment resulted in 53,372 browsing records of 1,328 common items, including 20,735 records from 582 A users and 32,637 records from 733 B users. The over-all confusion matrix is shown in Table 10, where  X  X ecom-mend X  means that the current item is recommended, and  X  X dd X  means that the item is added to the cart. Besides, adding to cart; Agreement%= x 11 + x 22 x percentage of user agreement with the (dis)recommendation; percentages of user agreement when an item is recommended or disrecommended, respectively.

Both AddToCart% and Agreement% of the experimental group A are significantly higher than the control group B, with the two-tailed p -values being 0.0374 and 0.0068, respec-tively. This demonstrates that presenting the explanations improves recommendation persuasiveness, and at the same time improves the conversion (i.e. adding-to-cart) rate in real-world online shopping scenarios.

More interestingly, we find that presenting disrecommne-dation explanations helps to prevent users from adding an Table 10: The overall confusion matrix correspond-ing to the 1328 common items of A (with explana-tions) and B (without explanations) users.
 item to cart when the system  X  X hinks X  that the item is not good enough for him/her. This observation reveals a strong persuasiveness and flexibility of the feature-level explana-tions, and it might bring new and promising marketing and advertising strategies for e-commerce users.
In this paper, we propose to leverage phrase-level senti-ment analysis of user reviews for personalized recommenda-tion. We extract explicit product features and user opin-ions from reviews, then incorporate both user-feature and item-feature relations as well as user-item ratings into a new unified hybrid matrix factorization framework. Besides gen-erating personalized recommendations, we designed explicit feature-level explanations for both recommended and disrec-ommended items. Our analysis shows that different users could focus on different product aspects, and our experi-ments suggest that the size of the underlying feature space that users care about varies for different users, domains and countries. Both online and offline experiments show that our framework compares favourably with baseline methods in three tasks: rating prediction, top-K recommendation, and explanation based user persuasion.

This is a first step towards integrating detailed sentiment analysis for aspect/feature based explainable hybrid factor-ization models for recommendations, and there are much room for improvements. Instead of one measure per as-pect, we can introduce more measures (sentiment, popu-larity, etc.) or capture more complex interaction between features in the future. Except for EFM, we can adapt or de-velop other hybrid factorization models such as tensor fac-torization or deep learning to integrate phrase-level senti-ment analysis. Besides, we focused on the persuasiveness in explanation generation and experimental design, while it is worth studying other utilities (transparency, user trust, ef-fectiveness, efficiency, satisfaction, and scrutability, etc.) of explanations and generating explanations automatically to optimize one or a combination of the utilities in the future.
Let  X  be the Lagrange multiplier for the constraint U 1  X  0, then the Lagrange function is: and the corresponding gradient is: By setting  X  U 1 = 0 we get: With the KKT complementary condition for the constraint U 1  X  0, we have  X  ij  X  U 1 ij = 0, giving us the following: which further leads to the updating rule of U 1 :
The updating rules for U 2 ,V,H 1 ,H 2 can be derived in a similar way.
 The authors thank the reviewers for the constructive sugges-tions. Part of this work was supported by Chinese Natural Science Foundation (60903107, 61073071) and National High Technology Research and Development Program (2011AA01 A205), and the fourth author is sponsored by the National Science Foundation (IIS-0713111). The opinions, findings or conclusions expressed in this paper are the authors X , and do not necessarily reflect those of the sponsors.
