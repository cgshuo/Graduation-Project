 Search engine has played a very important role in the growth of the Web. Web crawler forms an integral part of any search engine. The basic task of a crawler is to fetch pages, parse them to get more URLs, and then fetch these URLs to get even more URLs. In this process crawler can also log these pages or perform several other operations on pages fetched according to the requirements of the search engine. Most of these auxiliary tasks are orthogonal to the design of the crawler itself. The explosive growth of the web has rendered the simple task of crawling the web non-trivial. The architecture of the current crawler [1] [2] is based on a single architecture design. Centralized solutions are known to have problems like link congestion, being single point of failure, and expensive administration. 
To address the shortcomings of centralized search engines, there have been several proposals [3, 4] to build decentralized search engines over peer-to-peer networks. Peer to Peer system are massively distributed computing systems with each node communicating directly with one another to distribute tasks or exchange information or accomplish task. The challenge, while using a distributed model such as one described above, is to efficiently distribute the computation tasks avoiding overheads for synchronization and maintenance of consistency. Scalability is also an important issue for such a model to be usable. To improve the quality of service, we adopt the grid service as the distributed environment. Several crawlers can run in one node and the number of the crawler is impacted by the bandwidth and computing ability of the node. The information services are organized with P2P network----CAN [5]. URLs are collected by information service with the semantic vectors of URLs and the ID of the information service. The semantic vect ors of URLs are computed with Latent Semantic Indexing (LSI). In this way IglooG can scale up to the entire web and has been used to fetch tens of millions of web documents. 
The rest of the paper is organized as follows. Section 2 introduces the related work about crawler. Section 3 introduces the Latent Semantic Indexing. Section 4 proposes the architecture of IglooG. Section 5 desc ribes the experiment and results. We conclude in Section 6 with lessons learned and future work. The first crawler, Matthew Gray X  X  Wanderer, was written in the spring of 1993, roughly coinciding with the first release of NCSA Mosaic [6]. Several papers about web crawling were presented at the first two World Wide Web conferences [7, 8, 9]. However, at the time, the web was two to three orders of magnitude smaller than it is today, so those systems did not address the scaling problems inherent in a crawl of today X  X  web. All of the popular search engines use crawlers that must scale up to substantial portions of the web. However, due to the competitive nature of the search engine business, the designs of these crawlers have not been publicly described. There are two notable exceptions: the Google crawler and the Internet Archive crawler. Unfortunately, the descriptions of these crawlers in the literature are too terse to enable reproducibility. 
The google search engine is a distributed system that uses multiple machines for crawling [10, 11]. The crawler consists of five functional components running in different processes. A URL server process reads URLs out of a file and forwards them to multiple crawler processes. Each crawler process runs on a different machine, is single-threaded, and uses asynchronous I/O to fetch data from up to 300 web servers in parallel. The crawlers transmit downloaded pages to a single store server process, which compresses the pages and stores them to disk. The pages are then read back from disk by an indexer process, which extracts links from HTML pages and saves them to a different disk file. A URL resolver process reads the link file, derelativizes the URLs contained therein, and saves the abso lute URLs to the disk file that is read by the URL server. Typically, three to four crawler machines are used, so the entire system requires between four and eight machines. The internet archive also uses multiple machines to crawl the web [12, 13]. Each crawler process is assigned up to 64 sites to crawl, and no site is assigned to more than one crawler. Each single-threaded crawler process reads a list of seed URLs for its assigned sites from disk into per-site queues, and then uses asynchronous I/O to fetch pages from these queues in parallel. Once a page is downloaded, the crawler extracts the links contained in it. If a link refers to the site of the page it was contained in, it is added to the appropriate site queue; otherwise it is logged to disk. Periodically, a batch process merges these logged  X  X ross-site X  URLs into the site-specific seed sets, filtering out duplicates in the process. In the area of extensible web crawlers, Miller and Bharat X  X  SPHINX system [14] provides some of the same customizability features as Mercator. In particular, it provides a mechanism for limiting which pages are crawled, and it allows customized document processing code to be written. However, SPHINX is targeted towards site-specific crawling, and therefore is not designed to be scalable. Literal matching schemes such as Vector Space Model (VSM) suffer from synonyms and noise in description. LSI overcomes these problems by using statistically derived conceptual indices instead of terms for retrieval. It uses singular value decomposition (SVD) [15] to transform a high-dimensional term vector into a lower-dimensional semantic vector. Each element of a semantic vector corresponds to the importance of an abstract concept in th e description or query. 
Let N be the number of description in the collection and d be the number of description containing the given word. The inverse description frequency ( IDF ) is defined as 
The vector for description Do is constructed as below description Do . The vectors computed for description are used to form a description matrix S . Suppose the number of returned description is m, the description matrix S is constructed as 12 [ , ,..., ] m SSS S = . Based on this description matrix S , singular value decomposition (SVD) of matrix is used to extract relationship pattern between description and define thresholds to find matched services. The algorithm is described as follow. Since S is a real matrix, there exists SVD of : T mm nn U and V are orthogonal matrices. Matrices U and V can be denoted respectively (1) o  X  &lt; X  , we choose a parameter k such that information among the description. In this algorithm, the descriptions matching queries are measured by the similarity between them. For measuring the descriptions similarity based on k S , we choose the ith row i R of the matrix k vector of description i in a k-dimensional subspace: 4.1 The Web Crawler Service We wrap each web crawler as a grid service and deploy it in grid platform. This paper uses crawler of Igloo as single crawler to construct IglooG. First we introduce the architecture of single crawler (Fig. 1). 
Each crawler can get IP of host with URL by DNS. Then it downloads the web page through HTTP module if Robot allows access to the URL. URL extractor extracts the URL from the downloaded web page and URL filter check whether the URL accord with the restrictions. Then the cr awler uses hash function to compute the hash ID of URL. The crawler inserts the URL into its URL database. Policy of crawling is used to sort the rank of pages to make higher important resource to be crawled more prior. We adopt the PageRank [16] method to evaluate the importance of web page. HTTP module consists of several download threads and each thread has a queue of URL. 4.2 The Information Service IglooG is designed to use in Grid environment. Information service is in charge of collecting the information about resource and the distribution of URL. Also it adjusts the distribution of URL to make crawlers have good load balance. The system we design is used to deal with large-scale web page download so the number of information service is much. How to organize these information services is challengeable. These information services are regarded as index service in GT3 and is defined as a service that speaks two basic protocols. GRIP [17] is used to access information about resource providers, while the GRRP [17] is used to notify register nodes services of the availability of this information. Each resource has two attributes. One is resource type and the other is the value of the resource. Crawler being a special resource is recorded in information service. The number of URLs in crawling queue example of GRIP data model: 
We organize these information services in CAN. Our design centers around a virtual 4-dimensional Cartesian coordinate space. At any point in time, the entire coordinate space is dynamically partitioned among all the information services in the system such that every service owns it individual, distinct zone within the overall space. In our system one node can start at most one information service. We assume point in a virtual 4-dimensional Cartesian coordinate space which is defined as S ={(0,0,0,0),(255,255,255,255)} . We assume the 4 axes of S a are x, y, z, w . The first information service R1 holds space S a . When the second information service R2 joins, S other space. R1 records the IP of R2 and the space controlled by R2 and R2 records IP of R1 and the space controlled by R2 . In this way the neighbor relationship between R1 and R2 sets up. After the information service overlay network contains m service central point is closer to Rn belongs to Rn and the other one is held by Rm+1 . When service Ry leaves information service overlay network it notifies its neighbor Rt does not affect the function of our system. Each information service sends message periodically to detect its neighbors exists. If Rt fails, its neighbor knows its lost after by Rt  X  X . In this way we construct the information service overlay network. 4.3 The Architecture of IglooG node crawler being in and number is a random number that is not used by active crawlers. This pair is added to GRIP data when crawler is registering. Each crawler that joins IglooG must know at least one information service. Then the joining crawler P1 sends packet containing its GRIP data to the information service R1 to ask for joining. R1 checks the IP of P1 . If the IP of P1 is in the space controlled by R1 , R1 records GRIP data of P1. Otherwise R1 transfer GRIP data to its neighbor which coordinate is closest to the IP of P1. Then the neighbor does the same work as R1 until find an information service which controls the space containing the IP of P1 . Fig. 3 shows a sample of crawler discovering information service. In Fig. 3 P1 is the crawler which IP is (162.146.201.148) and it knows the information service R1 (28.18.36.112). Then P1 sends its GRIP data to R1. R1 checks IP of P1 and its space then transfer the GRIP data of P1 to its neighbor . The neighbor of R1 does the same as R1 and Finally the GRIP data is received b y R2 . Then R2 sends information about information. In this way P1 registers successfully. 
In this way crawlers are organized by information services. The information services are used to collect URLs and redistri bute them to crawlers to balance the load of crawler. The semantic vectors of URLs are generated with LSI. We can set the dimension of semantic vector of URLs to be 4 by adjusting  X  in order to resolve match between CAN and semantic vector, where the number of dimension of CAN is 4. After the crawler c1 download web page and extract URL u1 , it checks whether the the web page later. Otherwise it transmits the URL and semantic vector to information service R1 . We define the semantic vector of u1 as v(u1). After R1 receives u1 and v(u1) , it transfers them to its neighbor which coordinate is closest to the u1. Then the neighbor does the same work as R1 until find an information service R2 that controls the space containing the u1 . Then R2 selects a crawler which load is lightest to download u1 . Fig. 4 shows the architecture of IglooG. We generate 960 computing nodes by GT-ITM models. Then we divide these nodes into seven groups. We implements simulation of CAN. Nodes of one group containing 60 nodes are used as information services and organized by CAN. Each of the other 6 groups of nodes contains 150 nodes and every node runs a crawler. We use seven 1.7GHz Pentium IV machine with 1GB of memory as our experiment hardware. One of these computers is used to run CAN that organizes information services. Each of the other 6 machines run 150 nodes to run crawlers. Seven machines are connected to a 100Mb/s Ethernet LAN that connects to CERNET by a gateway. Igloo is implemented with JAVA that makes it can run in Windows and Linux etc. We choose 20 homepages of Chinese colleges as seed URLs. Owning to the limitation of disk, the web pages downloaded are less than 500 KB and the total crawling of single crawler is less than 1 GB. 
We have experimented 15 times and for each separate experiment we varied the number of participating crawlers from 60 through 900. We run crawlers 24 hours in each experiment. From Fig. 5 we find that the download speed of simulator is at about 66 URL/s and 660KB/s. So the network and web sites we use to crawl are stable. Fig. 6 shows the average download speed of IglooG scales linearly as the number of participating crawlers increase. Web crawler is program used to download documents from the web site. This paper presents the design of a distributed web crawler on grid platform. The challenge, while using a distributed model such as one described above, is to efficiently distribute the computation tasks avoi ding overheads for synchronization and maintenance of consistency. Scalability is also an important issue for such a model to be usable. To improve the quality of service, we adopt the grid service as the distributed environment. Several crawlers can run in one node and the number of the crawler is impacted by the bandwidth and computing ability of the node. The information services are organized with P2P network URLs are collected by information service with the semantic vectors of URLs and the ID of the information service. The semantic vectors of URLs are computed with Latent Semantic Indexing (LSI). In this way IglooG are load-balanced and can scale up to the entire web and has been used to fetch tens of millions of web documents. 
However, there are still some problems to be explored. How to effectively store the web page and construct indices in the distributed environment? We will explore these issues in the future. Acknowledgements. This paper is supported by 973 project (No.2002CB312002) of China, ChinaGrid Program of MOE of China, and grand project of the Science and Technology Commission of Shanghai Municipality (No. 03dz15026, No. 03dz15027 and No. 03dz15028). 
