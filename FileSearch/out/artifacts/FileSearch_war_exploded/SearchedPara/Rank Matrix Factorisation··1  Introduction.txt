 In this paper, we study a specific type of matrix called rank matrices ,inwhich each row is a (partial or complete) ranking of all columns. This type of data naturally occurs in many situations of interest. Consider, for instance, sailing competitions where the columns could be sailors and each row would correspond to a race, or consider a business context, where the columns could be companies and the rows specify the rank of their quotation for a particular service. Rankings are also a natural abstraction of numeric data, which often arises in practice and may be noisy or imprecise. Especially when the rows are incomparable, e.g., when they contain measurements on different scales, transforming the data to rankings may result in a more informative representation.
 Given a rank matrix, we are interested in discovering a set of rankings that repeatedly occur in the data. Such sets of rankings can be used to succinctly summarise the given rank matrix. With this aim, we introduce the problem of rank matrix factorisation (RMF). That is, we consider the decomposition of a rank matrix into two smaller matrices. toy example in Figure 1 . It depicts a rank matrix that is approximated by the product of two smaller matrices. Rank matrix M consists of five rows and six columns. Assuming no ties and complete rankings, each row contains each of the numbers one to six exactly once. Now, the task is to decompose a n matrix M into a n  X  k matrix C and a k  X  m matrix F , where C is a binary indicator matrix, F consists of rank profiles ,and k is a user-specified parameter. Intuitively the rank profiles in F are (partial) rankings and can be interpreted as local patterns . For example, together C and F show that the first two columns are ranked first and second in the first row.
 problem of finding sparse rank profiles where rows of F contain zeroes. This allows us to discover recurrent structure that occurs in the rankings of M ,and not to focus on any noise that may be present. Within this setting we do not necessarily aim at finding a factorisation that approximates the original matrix as closely as possible; the reconstructed rank matrix C  X  as long as its overall structure is captured. Hence, here we focus on one specific of choices within the RMF framework; we would like to stress that within the generic framework many other choices are possible. The same can be said with regard to the choices made for, e.g., rank profile aggregation and quantification of the reconstruction error. RMF is a general framework with numerous possi-bilities, and we propose and solve a first instance to demonstrate its potential. rank matrix factorisation (RMF), 2) the introduction of a scoring function and an algorithm, based on integer linear programming, for Sparse RMF, an instance of rank matrix factorisation, and 3) an empirical evaluation on synthetic and real-life datasets that demonstrates the potential of RMF. It is shown that rank matrix factorisations can provide useful insights by revealing the rankings that underlie the data. To the best of our knowledge, we are the first to investigate the problem of rank matrix factorisation. Mining rank data, although a very new topic, has attracted some attention by the community lately. In our earlier work [ 1 ]weproposed to mine ranked tiles , e.g., rectangles with high ranks, and we will empirically compare to them in the experiments. Furthermore, Henzgen and H  X  ullermeier [ 2 ] proposed to mine frequent subrankings. The latter approach aims to mine individual patterns, whereas we aim to find a set of patterns that together covers most of the data.
 RMF is clearly related to matrix factorisation approaches such as NMF [ 3 , 4 ], BMF [ 5 , 6 ], and positive integer matrix factorisation (PIMF) [ 7 ]. NMF, PIMF, and RMF have in common that the values in the factorisation are constrained to be positive, but are quite different otherwise. RMF specifically targets rank data, which requires integer values, making the results easier to interpret, a different scoring function, and a different algebra. RMF considers rank matrices instead of Boolean matrices and is therefore clearly different from BMF. In this section we formally define rank matrices and introduce the rank matrix factorisation problem that we consider.
 Definition 1 (Rank matrix). Let M be a matrix consisting of m rows and n columns. Let R = { 1 , ..., m } , C = { 1 , ..., n } be index sets for rows and for columns respectively. The matrix M is a rank matrix iff: where  X  = { 1 , 2 , ..., n } X  X  0 } .
 In our setting, columns are items or products that need to be ranked; rows are rankings of items. Here, the rank value 0 has a special meaning. It denotes unknown rankings. For example, in rating datasets, it might happen that there are items that are not rated. Such items will have rank value 0.
 Given a rank matrix, we would like to find a short description of the rank matrix in terms of a fixed number of rank profiles, or patterns, consisting of partial rankings. We formalise this problem as a matrix factorisation problem. Problem 1 (Rank matrix factorisation) . Given a rank matrix M and an integer k , find a matrix C  X   X  X  0 , 1 } m  X  k and a matrix F that: where d ( , ) is a scoring function that measures how similar the rankings in the two matrices are, and  X  is an operator that creates a data matrix based on two factor matrices. Rows F i, : of matrix F indicate partial rankings, columns C matrix C indicate in which rows a partial ranking appears.
  X  . If multiple patterns are present in one row, this operator essentially needs to combine the different partial rankings into a single ranking. This problem is well-known in the literature as the problem of rank aggregation . In this first study, we use a very simple aggregation operator, namely, we use normal matrix multiplication to combine the matrices. More complex types of aggregation are left for future work.
 CF is not necessarily a rank matrix even if C is binary and F contains partial rankings. We address this here by restricting the set of acceptable matrices to those for which ( CF ) ij  X  n for all i  X  X  and j  X  X  .
 tion we first need the concept of a cover for a rank matrix factorisation. The cover of a factorisation is the set of cells in the reconstructed matrix where at least one pattern occurs, i.e., where the reconstructed matrix is non-zero. Definition 2 (Ranked factorisation cover) Coverage is the size of the cover, i.e., coverage ( C , F )= d ( , ) in Equation 2 needs to be designed in such a way that it: 1) rewards patterns that have a high coverage, 2) penalises patterns that make a large error within the cover of the factorisation.
 quantifies the disagreements between the reconstructed and the original rank matrix. We first define notation for the data matrix identified by the cover of a factorization.
 Definition 3 (Ranked data cover). The ranked data cover matrix U ( M , C , F ) is a matrix with cells u ij ,where: Now the ranked factorisation error is defined as follows.
 Definition 4 (Ranked factorisation error) Here, d (  X  ,  X  ) is a function that measures the disagreement between two rankings over the same items. Hence, the ranked factorisation error is the total of rank disagreements between the reconstructed rank matrix and the true ranks in the original rank matrix. The score is calculated row by row.
 Many scoring functions can be used to measure the disagreement between rows, for instance, Kendall X  X  tau or Spearman X  X  Footrule (see [ 8 ]forasurvey). For an efficient computation, we choose the Footrule scoring function. Definition 5 (Footrule scoring function). Given two rank vectors, u = ( u ,...,u n ) and v =( v 1 ,...,v n ) , the Footrule scoring function is defined as d ( u , v )= n i =1 | u i  X  v i | .
 Having defined the ranked factorisation coverage and ranked factorisation error, we now can completely define the Sparse Rank Matrix Factorisation (Sparse RMF) problem as solving the following maximisation problem: where  X  is a threshold and [.] are the Iverson brackets.
 Note that in this scoring function, for each cell we have a positive term if the error is smaller than  X  ; we have a negative term if the error is larger than  X  . In practice, we often use a relative instead of an absolute threshold. We denote such a threshold as a percentage, i.e.,  X  = a % implies  X  = a % We propose a greedy algorithm that uses integer linear programming (ILP). First, we present two theorems that can be used to calculate the ranked factori-sation coverage and ranked factorisation error. Then, we present the algorithm. Theorem 1. Let CF be a decomposition of a rank matrix M .Let A satisfy the following two properties: then Theorem 2. Let A be a binary matrix that satisfies Theorem 1, then Equation 8 can be formulated as: subject to where Y i,j is the upper bound of | M i,j  X  k t =1 C i,t F 1 ,...,n .
 Inequality ( 18 ) ensures that the reconstructed matrix is a rank matrix. problem if either C or F is known. This makes it possible to apply an EM-style algorithm as shown in Algorithm 1 , in which the matrix F is optimised given matrix C , and matrix C is optimised given matrix F , and we repeat the iterative optimisation till the optimal score cannot be improved any more.
 able way, i.e., smarter than random. The solution we choose is to initialise the matrix C using the well-known K-means algorithm. To compute the similarities of rank vectors in K-means, we use the Footrule scoring function. The K-means algorithm clusters the rows in k groups, which can be used to initialise the k columns of C . Note that this results in initially disjoint patterns, in terms of their covers, but the iterative optimisation approach may introduce overlap. We implemented the algorithm in OscaR 1 , which is an open source Scala toolkit for solving Operations Research problems. OscaR supports a modelling language for ILP. We configured OscaR to use Gurobi 2 as the back-end solver. Source code can be downloaded from our website, http://dtai.cs.kuleuven.be/ CP4IM/RMF .
 Algorithm 1. Sparse RMF algorithm The goal of Sparse RMF is to find a set of rank profiles (local patterns), which can be used to summarise a given rank matrix. Alternative methods for summaris-ing matrices are bi-clustering [ 9 ] and ranked tiling [ 1 ]. While ranked tiling and Sparse RMF work on ranked data, bi-clustering algorithms are mostly applied to numeric data. Hence, to compare to all of these algorithms, we first gener-ate continuous data and then convert them to ranked data as in [ 1 ]. The main idea is to benchmark the performance of the considered algorithms in terms of recovering implanted tiles in the synthetic data. Different from ranked tiling [ 1 ], where implanted tiles only have high average values, we now implant tiles that have both low and high average values.
 Data Generation. We use the generative model that we introduced in [ 1 ]to generate continuous data. First, we generate background data whose values are sampled from normal distributions having mid-ranged mean values. Second, we implant a number of constant-row tiles whose values are sampled from normal distributions having low/high mean values. Finally, we perform a complete rank-ing of columns in every row to obtain a rank matrix.
 Formally, background data is generated by this generative model: where  X  1 r  X  U (3 , 5) , X  2 r  X  U (  X  5 ,  X  3) , X  3 r  X  U ( { 0 , 1 } , i x i =1 ,x has mass probability function  X  =( p, p, 1 mean values are sampled from a different uniform distribution: U ( Setup. We generate four 500 rows  X  100 columns datasets for different p , i.e., p Three tiles have low average values, the other four have high average values. tiles. We do this by measuring recall and precision , using the implanted tiles as ground truth. Overall performance is quantified by the F1 measure, which is the average of the two scores.
 Varying the Parameters. We var-ied the parameters k and  X  in Equa-tion 8 and then applied the Sparse RMF algorithm on the four synthetic datasets. For each parameter combi-nation, the algorithm was executed ten times and the result maximising the score was used. This is to get rid of effects that are due to differences in the initialization based on K-means.
 based on the union of the cover-age area, which has non-zero val-ues, by the k components C (: ,i ) F ( i, :) ,i =1 ...k . The average perfor-mance of the algorithm on the four datasets is summarised in Figure 2 .
 The figure shows that the Sparse RMF can recover implanted tiles and remove noise outside when k =5and  X   X  15%. When  X  is too small, i.e., 5%, the algorithm cannot recover the tiles. In general, the algorithm has high per-formances when k is large. This matches our expectation though, since a higher  X  results in higher tolerance to noise and a larger k results in more patterns and hence a more detailed description of the data.
 Comparison to Other Algorithms. In this experiment, we compare our app-roach to ranked tiling [ 1 ] and several bi-clustering algorithms. These include CC [ 10 ], Spectral [ 11 ], Plaid [ 12 ], FABIA 3 [ 13 ], SAMBA Spectral and Plaid are part of the R biclust 6 package.
 Since large noise levels may conversely affect the performance of the algo-rithms, we use a dataset also used for the previous experiments, with p =0 . 05 (low noise level). We ran all algorithms on this dataset and took the first seven tiles/bi-clusters they produced, which have the highest scores (SAMBA) or largest sizes (all other). For most of the benchmarked algorithms, we used their default values. For CoreNode, we use msr =1 . 0and overlap =0 . 5. For ISA, we applied its built-in normalised method before running the algorithm itself. The results in Table 1 show that our algorithm achieves much higher precision and recall on this task than any of the ranked tiling and bi-clustering methods. Note that Spectral could not find any patterns. Ranked tiling only finds highly ranked tiles, whereas our rank matrix factorisation is more general and allows to capture any recurrent partial rankings in the rank matrix. Some of the bi-clustering methods attain quite high precision, e.g., FABIA and Plaid, but their recall is much lower than for Sparse RMF. The reason is that the synthetic data contains incomparable rows, with values on different scales. These results confirm that converting such data to rank matrices is likely to lead to better results. This section presents results on three real world datasets: 1) Eurovision Song Contest voting data, 2) Sushi preferences, and 3) NBA basketball team rankings. We previously collected the European Song Contest (ESC) dataset [ 1 ]. This dataset contains aggregated voting scores that participating countries gave to competing countries during the period from 2010 to 2013. We aggregated the data by calculating average scores that voting countries award to competing countries and transformed it to ranked data. The NBA basketball team ranking dataset was collected by the authors of [ 17 ]. It consists of rankings of 30 NBA basketball teams by a group of professional agencies and groups of students. The Sushi dataset was collected by the authors of [ 18 ]. It contains preferences of five thousand people over ten different sushi types.
 varying  X  and k . Based on these preliminary experiments, we used the following heuristics to choose reasonable parameter values to report on. We choose  X  such that it results in high coverage and low error. Given the chosen  X  ,for k we choose the largest value such that each resulting pattern is used in at least two rows. When k is further increased, patterns are introduced that are used in only one row of the rank matrix, or even in none. This would clearly result in redundancy, which we would like to avoid.
 algorithm on all datasets. The upper five rows describe dataset properties and the used parameter values. For each dataset, the algorithm is executed a number of times (#runs) and the highest-scoring result is used for the remaining statistics (except for convergence and time/run, for which all runs are used). can achieve high coverage with low error. With the Sushi dataset, for example, 78% of the matrix can be covered by just 8 rank profiles, and on average the ranks in the reconstructed rank matrix differs just 1.3 from those in the covered part of the matrix. The numbers of zeroes per pattern demonstrate that the algorithm successfully finds local patterns in the three studied datasets: partial rankings are used to cover the matrix. The overlapping statistic indicates that only the ESC dataset needs multiple patterns per row to cover a large part of the matrix: 2% of the rows are covered by more than one pattern. Convergence indicates the average number of iterations in which the run converges, with standard deviation. This shows that the algorithm needs only few iterations to converge, typically 3 to 6. Finally, the average time per run shows that our algorithm runs very efficiently on modestly sized rank matrices, but making it more efficient for larger datasets is left for future work. To show how rank patterns can provide insight into the data, we visualise two typical rank profiles obtained on the European Song Contest data in Figure 3 . Both depict a set of voting countries (in green) and their typical voting behaviour (in red). For example, countries in Eastern Europe tend to give higher scores to Russia and nordic countries than to other countries. We introduced the novel problem of rank matrix factorisation (RMF), which concerns the decomposition of rank data. RMF is a generic problem and we therefore introduced Sparse RMF, a concrete instance with the goal to discover a set of local patterns that capture the recurrent rankings in the data. We formalised Sparse RMF as an optimisation problem and proposed a greedy, alternate optimisation algorithm to solve it using integer linear pro-gramming. Experiments on both synthetic and real datasets demonstrate that our proposed approach can successfully summarise rank matrices by a small number of rank profiles with high coverage and low error.

