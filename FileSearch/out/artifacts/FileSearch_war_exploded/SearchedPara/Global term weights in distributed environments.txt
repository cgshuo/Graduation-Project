 1. Introduction
In information retrieval applications, there are certain scenarios where a global view on the document col-long-term query is matched against a stream of incoming documents that are initially unknown.
Another important scenario lacking a global view is that of distributed IR where documents are stored and databases need to be merged by a central broker, which may not have a global view on the collection.
A related, more strictly distributed scenario is that of peer-to-peer information retrieval (P2PIR) where each peer is connected to a number of other peers and routes incoming queries to one or more of its neigh-bours. Results are merged at the peer where the query was issued; this peer almost never has a global view work.
What challenges does the lack of global information introduce for information retrieval systems? In most of w.r.t. a given query or document. Term weighting often consists of a local and a global component. The local component measures the extent to which a term is relevant w.r.t. a query or document and is often related to its frequency within that query or document. The global component measures the amount of information that document frequency.

The lack of global information about a document collection is hence a problem for the global component of weighting schemes: in order to compute IDF, one needs to know all the documents in the collection.
In this work, we examine the two major solutions to this problem: either compute global term weights from means exactly here, but the impact of the choice of a reference corpus is examined later. Term weights are lections (henceforth called  X  X  X etrieval collections X  X ) as if they were valid globally.

One advantage of reference corpora is that estimates derived from them never need to be updated, even tributed setting: since global term weights never change, scores for documents remain comparable throughout the whole distributed system.
 message overhead when distributing new weight estimates to all peers. With  X  X  X istributed sampling X  X  on the other hand  X  e.g. each peer samples documents from its own neighbourhood  X  this problem does not arise, but results merging is much more complicated, because global term weights depend on the location where they are computed.

The rest of this paper is organised as follows: Section 2 reviews some related work on estimation of global results of experiments in ad hoc retrieval are presented and discussed before Section 5 concludes. 2. Related work
Depending on the application, various ways of estimating global term weights have been developed. Esti-mating global term weights from just a sample of the collection was initially used with dynamic collections, where the collection changed so quickly that recomputing global term weights seemed too costly. In Viles and French (1995b) , the effect of not updating IDF when adding new documents to a collection was studied and it was found that doing so did not degrade effectiveness seriously at least as long as new documents did not contain too many new terms. In Chowdhury (2001) , similar experiments with larger collections on delayed IDF updates revealed that a sample of 30 X 40% of the whole collection is sufficient for robust IDF estimation. seems to be an important variable, but no systematic evaluation was performed on this parameter.
Updating IDF values from incoming documents  X  starting with no information at all  X  was examined in of updating estimates from a reference corpus with ones sampled from the incoming documents ( X  X  X ncremental thoroughly evaluated.
 In the field of distributed and P2P information retrieval, various options were explored:
Coarser information : instead of computing global weights like IDF on the document level, distributed sys-tems sometimes do so on the resource or peer level. In French et al. (1999), Callan (2000) , for example, document frequency is replaced with the number of databases a term occurs in and results are merged using a combination of the local document score (i.e. using local IDF) and the score of the database. In Cuenca-
Acuna, Peery, Martin, and Nguyen (2003) , inverse peer frequency (IPF) is used both to rank peers and doc-uments, i.e. the number of peers a term appears in is used as a full replacement of document frequency. Although it is easier to obtain, these approaches still use global information.

Sampling has been used both in distributed and peer-to-peer IR. In Viles and French (1995a) , databases reveal information derived from part of their collection to other databases so that each database knows its own collection and part of the others (resulting in what we defined to be distributed sampling). The authors found that full dissemination was not necessary and that the level of dissemination needed depends on the degree of randomness applied when allocating documents to databases. In P2PIR, sampling has been explored in Klemm and Aberer (2005) and Tang, Xu, and Mahalingam (2003) , where Klemm and
Aberer (2005) use complete information from all peers and Tang et al. (2003) mix a reference corpus with a growing set of documents sampled from the peers using centralised sampling.

Reference corpora : These have only rarely been used in distributed IR. As mentioned above, Tang et al. (2003) start out with estimates derived from a reference corpus, but then mix these with samples (unfortu-nately, there is no discussion of how exactly this is done). The FASD system ( Kronfol, 2002 ) uses estimates derived from a reference corpus only, but does not evaluate its impact. 2.1. Contribution of this work of term weight estimates from a reference corpus. The effect of sampling has been (partly) evaluated in some using a reference corpus.
 et al., 2001 ), but not to term lists. 3. Estimation 3.1. Weighting schemes
Generally, in term weighting, rare terms are treated as informative. Hence, all variants of global term weights make use of term frequency information. There are two sorts of frequency commonly used: doc-ument frequency (DF), which denotes the number of documents within a collection that are indexed by the term, and collection frequency (CF) which refers to the overall number of a term X  X  occurrences within the collection.

In this paper, we consider one representative example of each of those two possibilities, namely BM25 and results are not artifacts of a particular weighting scheme.

In BM25, the global component is a special form of inverse document frequency (IDF), namely the Rob-ertson X  X parck X  X ones weight (RSJ) which was introduced in Robertson and Jones (1976) . In the experimental section below, however, this weight was replaced with the classical formulation of IDF (see Robertson, 2004 for a discussion of the relation between the two): where N is the number of documents in the collection and DF been assigned as an index term. The rest of the BM25 formula was left unchanged and used with the usual setting of free parameters, (see Robertson, Walker, Hancock-Beaulieu, Gull, &amp; Lau, 1994 ). In formula (1) , both N and DF t require knowledge of the whole collection. Alternatively, one could say that is the inverse of what can be interpreted as an estimate of a term X  X  probability of being contained in an arbitrary document (cf. Robertson, 2004 ), so that we only need to estimate this probability, which will be called p doc ( t ) in the remainder of this work.

As mentioned above, the collection frequency of terms is used in language modeling. Here, we estimate the probability that a query q is generated from a document d  X  X  unigram language model: where p ( t j d ) is the probability that term t is generated from d  X  X  language model. Now, we need to apply would make the whole product 0). A commonly used smoothing technique in that context is Dirichlet smoothing: quires global knowledge. Alternatively, it can be estimated from a reference corpus.

In the following, we estimate p doc ( t )and p coll ( t ) of terms from a given sample of language. 3.2. Estimating and smoothing
Estimating the probabilities introduced above can be treated as a problem of inferring unigram language models , i.e. probability distributions over terms.

The main problem that has to be solved in this task is that of smoothing , i.e. of assigning a probability to terms that have not occurred in the sample. Since samples never cover the whole vocabulary of a language, we ing, we reserve the total amount P 0  X  E  X  n 1  X  times in the sample with
Here, N is the sample size and n r is the number of terms seen exactly r times. In the simple Good-Turing ap-proach of Gale and Sampson (1995) , E ( n r ) is a function of n through the r  X  n r -curve in the log domain. 1
For estimating p doc , we take n r to be the number of terms with DF equal to r . For p CF.

We still need to decide how to distribute P 0 among the terms that we have not observed in the sample. Since we have no good reason to do otherwise, the obvious solution is to divide P order to do so, we need to know their number. Up to date, there seems to be no reliable way of estimating the number of terms missing from a sample; estimates in the literature range from rather small numbers ( Boneh, Boneh, &amp; Caron, 1998; Efron &amp; Thisted, 1976 ) to an infinite number of terms ( Kornai, 2002 ).
In the experiments, I therefore used an estimate without theoretical grounding, but yielding probabilities that did not widely differ from those computed for terms seen once in the sample. The estimation was done by assuming the number of unseen terms to be equal to the number of distinct terms in the sample. This was found to work better than other estimates based e.g. on Zipf X  X  law, which  X  depending on the way a line is fitted through the frequency points in the log domain  X  often yields unstable results.
Note that smoothing needs to be done regardless of whether the sample is a subset of the retrieval collection with. It may well be that Viles and French (1995b) and Chowdhury (2001) reported problems with new terms because of inferior smoothing. 4. Experiments 4.1. Setup
The evaluation was done using IR test collections of various sizes and topical homogeneity. The basic approach consists in first estimating global term weights either from a reference corpus or from a sample the full test collection.

Retrieval runs are evaluated using the relevance judgments provided with each collection and mean average precision (MAP) as a performance measure. The Wilcoxon test with a 95% confidence level is used to test for between runs are mentioned in the remainder of this work, this refers to the Wilcoxon test with 95% confidence.

The collections represent two scenarios in P2PIR: a number of small, topically homogeneous and rather specialised collections (Medline, CACM, Cranfield, CISI) represent a scenario where a group of specialists exchange documents of their common interest. On the other hand, some larger and thematically heteroge-The Ohsumed collection is somewhere in between these two extremes, being large, but highly specialised. Table 1 summarises the characteristics of the collections.
 general English usage. The idea is that terms frequent in general English usage will also be frequent in many hence informative.

As a simple working hypothesis, I assumed the British National Corpus (BNC) to be such a sample since it was created explicitly with the above considerations in mind. The BNC is used as a reference corpus in all experiments described below. It is not altogether unproblematic because it is British English  X  and most col-tions so that document frequency estimates are based on rather few evidence. 4.2. Reference corpora compared to weights derived from the collections themselves and to uniform global weights  X  i.e. substituting IDF = 1.0 and CF = 1.0 in the respective retrieval formulae.

It should be mentioned here that BM25 requires the average length of documents ( avdl ) to be known  X  another piece of global knowledge which is not generally available. In the experiments, I used avdl as estimating avdl from other sources. For the l parameter in Dirichlet smoothing, I used two settings: l = avdl (computed from the whole collection) and l = 2000, a value which was found to be generally well-performing in Zhai and Lafferty (2004) .
 Table 2 shows mean average precision for the TREC collections and Table 3 for the other collections. For TREC, I used various query lengths  X  T = title only, TD = title and description and TDN = all fields.
BNC estimates generally lead to a small, but significant degradation in MAP when compared to the whole be observed, e.g. for Medline. However, the large picture suggests that the BNC is generally inferior to the than using uniform weights.

Furthermore, language models with l = 2000 almost always yield better results than using l = avdl . Sub-sequently, only l = 2000 was used in language models. 4.2.1. Domain-adapted reference corpus In order to test the effect of a domain-adapted reference corpus, a small experiment was conducted with the
Ohsumed collection. I used 500,000 abstracts from the PubMed database as a new reference corpus, a set of documents which is disjoint from Ohsumed, but covers similar topics.

The mean average precision thus achieved was 0.311 for BM25 and 0.300 for language models ( l = 2000) as tistically significant.

This demonstrates that a well-chosen reference corpus of suitable size can outperform global weights as computed from the retrieval collection itself. 4.2.2. Qualitative analysis
Next, a qualitative analysis of results was performed to find out how BNC estimates could be improved. In most from BNC estimates were at the top of this list and will be called problematic queries from now on.
A close analysis of the differences showed that the BNC estimates were detrimental for only little more than 50% of the queries. However, the (negative) difference in MAP scores was quite large in some cases at the top of the list.

From a manual analysis of these problematic queries, it was rather striking that all contained certain but few terms that caused the dramatic MAP degradation. These terms have BNC estimates that differ widely 4 shows one such term for some of the collections.

For TREC, the discrepancy is due to the BNC being British (where the U.S. are rarely mentioned). In the be called  X  X  X omain-specific stop words X  X  from now on. Although there are not many of them, they have a large effect on MAP since they occur in many queries. 4.3. Sampling and mixing 4.3.1. Pure sampling
In this section, we examine the retrieval performance that can be reached when estimating global term weights from a sample of the retrieval collection.

In the experiments below, we do not make any strong assumptions on the sampling process, i.e. no docu-uments are numbered consecutively and samples of size s n collection in the following way: we calculate m  X b N ( m + j ) = 0. The parameter j is used to produce various disjoint samples of size s are averaged.

Fig. 1 shows MAP as a function of sample size for CACM and TREC-7. The leftmost data point corresponds to using uniform global weights whereas the rightmost point corresponds to using the full collection.

It can be seen that small sample sizes already yield good performance. The curves are much smoother and sizes (see Fig. 1 b).

Table 5 gives information for all collections. What is listed is the median minimum number of documents in a sample that is needed in order to perform not significantly worse than when using the whole collection. the results given in Viles and French (1995b) and Chowdhury (2001) . However, as mentioned above, the likely reason for the discrepancy is the lack of smoothing in Viles and French (1995b), Chowdhury (2001) .
Although the percentage of the whole collection is very small, this method still requires to sample a few hundred or thousand documents for large collections. Depending on how sampling is done, this might be rather costly. In the next section, we examine whether sample sizes can be reduced further. 4.3.2. Mixing with reference corpora
We have observed that the main deficiency of reference corpora is that they do not contain the domain-spe-cific stop words. Since these words can probably be sampled from very small sets of documents, we shall mix sample estimates with BNC estimates. One obvious solution is to: (1) Sample documents from the retrieval collection as in the previous section. (2) For each sample S of size s , remove all terms that occurred only once. (3) For the remaining terms compute a new pseudo CF (4) a can be any number between 0 and 1. In the experiments, I used a  X  1 1 (5) For DF, s is now the number of documents instead of the number of tokens in the corpus and CF is
Fig. 2 shows that mixing really helps to improve both the baseline BNC estimates and the pure sampling strategy.

Table 6 shows the (median) smallest sample size for which the new strategy does not perform significantly ber of documents needed has decreased, in most cases rather drastically. In fact, an absolute number of 32 documents was sufficient for BM25 in all cases except TREC-3. 4.4. Pruning term lists Computing global term weights results in a list of terms that occurred in the respective sample  X  be it the BNC or (part of) the retrieval collection  X  together with their global weights.

In a P2PIR setting, this list is made available to each peer in the system, sometimes it also needs to be it worthwhile to investigate pruning of the list to minimise storage cost and message size.
The pruning approach taken here is very simple: terms with low frequency in the sample are pruned from unseen terms.

Fig. 3 shows MAP as a function of the frequency threshold used for pruning a BNC term list. The two curves exemplify the two types of behaviour that can be observed in all collections. The behaviour found with
CACM seems inconclusive, but can be explained as follows: some domain-specific stop words (e.g.  X  X  X lgo-rithm X  X  in CACM) have a medium frequency in the BNC. When the corresponding threshold is reached, they are pruned from the list and hence treated as very rare terms, which causes a steep drop in MAP. Later, when nearly all terms are treated equally (as being very rare), performance rises again.

Table 7 shows the maximum frequency threshold for which the effectiveness reached with the pruned list is not significantly worse than when using the whole list, both for using the BNC and the retrieval collections themselves for deriving the original list.

These figures show that in most cases we can safely prune terms up to a frequency of 100 from the lists. This old is small (e.g. Ohsumed and BM25), the MAP difference when using t = 100 is significant but below 1% so that the effectiveness reached with the pruned list may still be acceptable.

This tells us that what is really needed for good retrieval performance is just a small list of very frequent list comprehends around 23,000 terms and occupies around 300 kB of disk space in uncompressed form. 5. Conclusions
In this paper, the estimation of global term weights from a reference corpus in a distributed IR setting was compared to using a sample of the retrieval collection or the whole collection itself.

The results showed that a general-language reference corpus (such as the BNC) may fail to identify words that appear very frequently in the domain of the retrieval collection, but not in general-language. A domain-adapted reference corpus  X  if available  X  can help to avoid this problem.
 lection was sufficient for close to optimal performance. However, the necessary sample size can be further reduced if sample estimates are mixed with those derived from a general reference corpus. This combination works well because domain-specific stop words can be found even in very small samples of the collection  X  an absolute number of 30 documents was found to be safe in almost all cases; mixing these into e.g. BNC esti-mates is sufficient in order to compensate for the BNC X  X  deficiencies.

As an experiment with frequency-based pruning of term lists showed, it is generally sufficient for good retrieval performance to know the most frequent words. All the others can be treated equally, namely as very space is scarce.

For a P2PIR scenario, we can conclude that linearly mixing term frequency estimates derived from a ref-erence corpus with a small sample of documents derived from a peer X  X  direct neighbourhood probably yields good retrieval effectiveness.

An alternative would be to use a centralised approach to collect the sample, which guarantees comparable document scores. Although this partly violates the peer-to-peer paradigm, it can be accommodated easily since many P2P systems use a centralised neighbour bootstrapping algorithm.

In Gnutella, for instance, a peer that joins a network can ask a so-called GWebCache for addresses of other to a GWebCache. This mechanism can be extended for a centralised sampling approach in the following way: peers that report their IP to a GWebCache attach a randomly sampled document vector from their collection with certain probability. The cache uses this to compile a sample of the distributed collection and provides peers joining the network with (short) lists of the most frequent terms from the sample. These can then be merged with frequency estimates from a reference corpus.

Since we have not examined the influence of semantically biased samples  X  which are likely to occur if peers results of this work.
 References
