 if they focus on the same topic or two images are similar if the y contain the same objects. The hashing approach motivated by the idea that a good encoding scheme sh ould minimize the sum of Hamming distances between pairs of code strings weighted by the value of a Gaussian kernel be-the number of bits in the code increases.
 the random Fourier features of Rahimi and Recht [8]), and convert this mapping to a binary one with similar guarantees. In particular, we show that the normalized Hamming distance (i.e., Ham-ming distance divided by the number of bits in the code) betwe en any two embedded points sharply be embedded in a binary cube of dimension O (log N ) in a similarity-preserving way: with high hashing (LSH) [1], which is a family of methods for deriving low-dime nsional discrete represen-tations of the data for sublinear near-neighbor search. How ever, our scheme differs from LSH in that we obtain both upper and lower bounds on the normalized H amming distance between any two database [10] represented by high-dimensional GIST descri ptors [7]) and compared its performance have been able to obtain very encouraging experimental resu lts. Consider a Mercer kernel K ( , ) on R D that satisfies the following for all points x , y  X  R D : R
D into the binary cube { 0 , 1 } n such that for any pair x , y the normalized Hamming distance between F n ( x ) = ( F where h h (0) = h 2 (0) = c &gt; 0 . In other words, we would like to map D -dimensional real vectors into n kernel K . We will achieve this goal by drawing F n appropriately at random.
 Random Fourier features. Recently, Rahimi and Recht [8] gave a scheme that takes a Merc er kernel satisfying (K1) and (K2) and produces a random mapping  X  n : R D  X  R n , such that,  X  probability measure P where  X   X  P take  X   X  Normal (0 ,  X I The scheme of [8] is as follows: draw an i.i.d. sample ((  X   X  From random Fourier features to random binary codes. We will compose the RFFs with Q t : [  X  1 , 1]  X  { X  1 , +1 } sgn( u ) = +1 if u  X  0 . We note the following basic fact (we omit the easy proof): Lemma 2.1 For any u, v  X  [  X  1 , 1] , P Now, given a kernel K , we define a random map F where t  X  Unif [  X  1 , 1] ,  X   X  P on, we will often omit the subscripts t,  X  , b and just write F for the sake of brevity. We have: Lemma 2.2 Proof: Using Lemma 2.1, we can show E 1 Using this together with the fact that E Lemma 2.3 Define the functions where u  X  [0 , 1] . Note that h h ( K ( x  X  y ))  X  h K ( x  X  y )  X  h 2 ( K ( x  X  y )) for all x , y .
 Proof: Let  X   X  = cos(  X  x + b )  X  cos(  X  y + b ) . Then E |  X  | = E uses concavity of the square root). Using the properties of t he RFF, E  X  2 = (1 / 2) E [( X   X   X  ,b ( y )) 2 ] = 1  X  K ( x  X  y ) We also have This proves the upper bound in the lemma. On the other hand, si nce K satisfies (K3), because the m th term of the series in (3) is not smaller than 1  X  K ( x  X  y ) / (4 m 2  X  1) . Fig. 1 shows a comparison of the kernel approximation proper ties of the RFFs [8] with our scheme for the Gaussian kernel. Now we concatenate several mappings of the form F binary cube { 0 , 1 } n . Specifically, we draw n i.i.d. triples ( t bits where the binary strings F n ( x ) and F n ( y ) disagree sharply concentrates around h provided n is large enough. Using the results proved above, we conclude that, for any two points x will disagree in about 40% or more of their bits.
 rescaling by constants.
 Theorem 2.4 Fix  X ,  X   X  (0 , 1) . For any finite data set D = { x well as the bounds of Lemma 2.3. We also prove a much stronger r esult: any compact subset X  X  R and the diameter of X and on the second moment of P the following [5]: Definition 2.5 The Assouad dimension of X  X  R D , denoted by d that, for any ball B  X  R D , the set B  X X can be covered by 2 k balls of half the radius of B . if d
X = O ( d ) bounded curvature, then d Theorem 2.6 Suppose that the kernel K is such that L then, with probability at least 1  X   X  , the mapping F n is such that, for every pair x , y  X  X  , Proof: For every pair x , y  X  X , let A F For any sequence  X  n = (  X  For every 1  X  i  X  n and an arbitrary  X   X  Then |  X (  X  n )  X   X (  X  n Now we need to bound E where  X  n = (  X  The quantity R ( A ) can be bounded by the Dudley entropy integral [14] where C function class {  X  7 X  1 of to the Euclidean norm on R D . It can be shown that, for any four points x , x  X  , y , y  X   X  X  , where  X  denotes symmetric difference of sets, and B (details omitted for lack of space). Now, 2 ( B x  X  B x  X  ) = 2 E  X  ,b h P t Q t (cos(  X  x + b )) 6 = Q t (cos(  X  y + b )) i Then ( B so for some constant C C 2 = 2 C 1 For example, with the Gaussian kernel K ( s ) = e  X   X  k s k 2 / 2 on R D , we have L the number of bits needed to guarantee the bound (6) is n =  X (( d in principle, to construct a dimension-reducing embedding of X into a binary cube, provided the number of bits in the embedding is larger than the intrinsic d imension of X . (a) (b) (c) (d) (e) (f) that has been reported to obtain better results than several other well-known methods, including LSH [1] and restricted Boltzmann machines [11]. Unlike our m ethod, spectral hashing chooses code parameters in a deterministic, data-dependent way, mo tivated by results on convergence of eigenvectors of graph Laplacians to Laplacian eigenfuncti ons on manifolds. Though spectral hash-ing is derived from completely different considerations th an our method, its encoding scheme is hashing and our method makes comparison between them approp riate.
 thetic data using a protocol similar to [15] (we have also con ducted tests on higher-dimensional curves for both methods using a range of code sizes. Since the normalized Hamming distance for our method converges to a monotonic function of the Euclidea n distance, its performance keeps Next, we present retrieval results for 14,871 images taken f rom the LabelMe database [10]. The neighborhood size, or setting a target Hamming distance for neighbors at a given Euclidean dis-(although, as demonstrated in [11, 13], even brute-force se arch with binary codes can already be our scheme for large-scale indexing and search application s.
 Acknowledgments
