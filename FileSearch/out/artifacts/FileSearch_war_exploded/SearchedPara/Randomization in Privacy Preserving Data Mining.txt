 Suppose there are many clients, each having some personal information, and one server, which is interested only in ag-gregate, statistically significant, properties of this informa-tion. The clients can protect privacy of their data by per-turbing it with a randomization algorithm and then submit-ting the randomized version. The randomization algorithm is chosen so that aggregate properties of the data can be recovered with sufficient precision, while individual entries are significantly distorted. How much distortion is needed to protect privacy can be determined using a privacy mea-sure. Several possible privacy measures are known; finding the best measure is an open question. This paper presents some methods and results in randomization for numerical and categorical data, and discusses the issue of measuring privacy. Suppose that some company needs to construct an aggre-gate model of its customers X  personal data. For example, a retail store wants to know the age and income of its cus-tomers who are more likely to buy DVD players or mountain ski equipment; a movie recommendation system would like to learn users X  movie preferences in order to make adver-tisements more targeted; or an on-line business arranges its webpages according to an aggregate model of its website visitors. In all these cases, there is one central server (the company), and many clients (the customers), each having a piece of information. The server collects this information and builds its aggregate model using, for example, a clas-sification algorithm or an algorithm for mining association rules. Often the resulting model no longer contains person-ally identifiable information, but contains only averages over large groups of clients.
 The usual solution to the above problem consists in hav-ing all clients send their personal information to the server. However, many people are becoming increasingly concerned about the privacy of their personal data. They would like to avoid giving out much more about themselves than is re-quired to run their business with the company. If all the company needs is the aggregate model, a solution is pre-ferred that reduces the disclosure of private data while still allowing the server to build the model. One possibility is as follows: before sending its piece of data, each client perturbs it so that some true information is taken away and some false information is introduced. This approach is called random-ization . Another possibility is to decrease precision of the transmitted data by rounding, suppressing certain values, replacing values with intervals, or replacing categorical val-ues by more general categories up the taxonomical hierarchy, see [8; 14; 23; 24].
 The usage of randomization for preserving privacy has been studied extensively in the framework of statistical databases [9; 10; 12; 13; 20]. In that case, the server has a complete and precise database with the information from its clients, and it has to make a version of this database public, for others to work with. One important example is census data: the government of a country collects private information about its inhabitants, and then has to turn this data into a tool for research and economic planning. How-ever, it is assumed that private records of any given per-son should not be released nor be recoverable from what is released. In particular, a company should not be able to match up records in the publicly released database with the corresponding records in the company X  X  own database of its customers.
 In the case of statistical databases, however, the database is randomized when it is already fully known. This is differ-ent from our problem, where the randomization procedure is run on the client X  X  side, and must be decided upon be-fore the data is collected. A randomization for a statisti-cal database is usually chosen so that it preserves certain aggregate characteristics (averages and covariance matrices for numerical data, or marginal totals in contingency tables for categorical data), or changes them in a predetermined way [12; 15]. Besides randomization, other privacy preserv-ing transformations are used such as sampling and swapping values among records [15; 27]. Our choice is more limited due to the nature of our problem. Let each client C i , i = 1 , 2 , . . . , N , have a numerical at-tribute x i . Assume that each x i is an instance of random variable X i , where all X i are independent and identically distributed. The cumulative distribution function (the same for every X i ) is denoted by F X . The server wants to learn the function F X , or its close approximation; this is the ag-gregate model which the server is allowed to know. The server can know anything about the clients that is derivable from the model, but we would like to limit what the server knows about the actual instances x i .
 The paper [4] proposes the following solution. Each client randomizes its x i by adding to it a random shift y i . The shift SIGKDD Explorations. Volume 4, Issue 2 -page 43 values y i are independent identically distributed random variables with cumulative distribution function F Y ; their distribution is chosen in advance and is known to the server. Thus, client C i sends randomized value z i = x i + y i to the server, and the server X  X  task is to approximate function F given F Y and values z 1 , z 2 , . . . , z N . Also, it is necessary to understand how to choose F Y so that The amount of disclosure is measured in [4] in terms of con-fidence intervals. Given confidence c %, for each randomized value z we can define an interval [ z  X  w 1 , z + w 2 ] such that for all nonrandomized values x we have P [ Z  X  w 1 6 x 6 Z + w 2 | Z = x + Y, Y  X  F Y ] &gt; c % . In other words, here we consider an  X  X ttack X  where the server computes a c %-likely interval for the private value x given the randomized value z that it sees. The shortest width w = w 1 + w 2 for a confidence interval is used as the amount of privacy at c % confidence level.
 Once the distribution function F Y is determined and the data is randomized, the server faces the reconstruction prob-lem : Given F Y and the realizations of N i.i.d. random sam-ples Z 1 , Z 2 , . . . , Z N , where Z i = X i + Y i , estimate F In [4] this problem is solved by an iterative algorithm based on Bayes X  rule. Denote the density of X i (the derivative of F X ) by f X , and the density of Y i (the derivative of F by f Y ; then the reconstruction algorithm is as follows: 1. f 0 X := Uniform distribution; 2. j := 0 // Iteration number; 3. repeat For efficiency, the density functions f j X are approximated by piecewise constant functions over a partition of the attribute domain into k intervals I 1 , I 2 , . . . , I k . The formula in the algorithm above is approximated by ( m ( I t ) is the midpoint of I t ): f
X ( I p ) := It can also be written in terms of cumulative distribution functions, where  X  F X (( a, b ]) = F X ( b )  X  F X ( a ) = P [ a &lt; X 6 b ] and N ( I s ) is the number of randomized values z inside interval I s : In paper [1] it has been shown that if the formula (1) is replaced with formula where z  X  ( a, b ] = [ z  X  b, z  X  a ), then the quality of ap-proximation is somewhat better. This formula is derived using the framework of Expectation-Maximization (EM) al-gorithms [7; 19], which allows to consider a more general randomization setting. Suppose the server wants to approx-imate density f X of nonrandomized (original) attribute dis-tribution by some density from a parametric family { f X ; X  If the server knew x 1 , x 2 , . . . , x N , it could find a maximum likelihood parameter value  X   X  by computing However, the server knows only z 1 , z 2 , . . . , z N ; so this for-mula is replaced by an iterative EM procedure: A similar approach has been described earlier for statisti-cal databases [18]. Both papers [4; 1] consider two specific examples of additive randomization: with a uniform (on a segment) and a Gaussian density f Y for the shift distribu-tion.
 In [4] it is shown how to use randomized numerical data in classification, namely in building a decision tree [5]. The main problem in decision tree construction is finding the right split point at each node. The quality of a split point depends on the frequency of records from each class in the subsets to the left and to the right of the split point. So, the tree building algorithm has to estimate the frequency distri-butions for points of each class, split by different split points for all numerical attributes. Several estimating strategies are considered, which differ in how many times the iterative reconstruction algorithm is applied: Since the reconstruction algorithm requires partitioning the attribute domains into intervals, the only split points con-sidered are the interval boundaries. To estimate the class frequencies for a split at a non-root node, the records are associated with attribute intervals as follows. The records are sorted by each (randomized) attribute, and then, given the reconstruction of distribution F X , they are associated with an interval according to their order and so as to ob-serve the distribution. Experimental results show that the class prediction accuracy for decision trees constructed over randomized data (using ByClass or Local ) is reasonably close (within 5% X 15%) to the trees constructed over orig-inal data, even with heavy enough randomization to have 95%-confidence intervals as wide as the whole range of an attribute. The training set had 100,000 records. Papers [22; 11] consider randomization of categorical data, in the context of association rules. Suppose that each client C i has a transaction t i , which is a subset of a given finite set of items I , |I| = n . For any subset A  X  I , its support SIGKDD Explorations. Volume 4, Issue 2 -page 44 in the dataset of transactions T = { t i } N i =1 is defined as the fraction of transactions containing A as their subset: an itemset A is frequent if its support is at least a certain threshold s min . An association rule A  X  B is a pair of disjoint itemsets A and B ; its support is the support of A  X  B , and its confidence is the fraction of transactions containing A that also contain B : An association rule holds for T if its support is at least s and its confidence is at least c min , which is another threshold. Association rules were introduced in [2], and [3] presents efficient algorithm Apriori for mining association rules that hold for a given dataset. The idea of Apriori is to make use of antimonotonicity property : Conceptually, it first finds frequent 1-item sets, then checks the support of all 2-item sets whose 1-subsets are frequent, then checks all 3-item sets whose 2-subsets are frequent, etc. It stops when no candidate itemsets (with frequent subsets) can be formed. It is easy to see that the problem of finding association rules can be reduced to finding frequent itemsets. A natural way to randomize a set of items is by deleting some items and inserting some new items. Paper [11] con-siders a family of randomization operators called select-a-size . A select-a-size randomization operator is defined for a fixed transaction size | t | = m and has two parameters: a randomization level 0 6  X  6 1 and a probability distri-transaction t of size m , the operator generates a randomized transaction t 0 as follows: 1. The operator selects an integer j at random from the 2. It selects j items from t , uniformly at random (without 3. It considers each item a 6 X  t in turn and tosses a coin If different clients have transactions of different sizes, then select-a-size parameters have to be chosen for each trans-action size. So, this (nonrandomized) size has to be trans-mitted to the server with the randomized transaction. The randomization operator used in [22] does not have this draw-back; it has only one parameter 0 6 p 6 1 which determines, for each item independently, the probability of the item not being  X  X lipped X  (discarded if present, or inserted if absent) in the transaction. For any fixed transaction size m , this op-erator becomes a special case of select-a-size, with  X  = 1  X  p and p [ j ] = m j p j (1  X  p ) m  X  j .
 In the set T 0 of randomized transactions available to the server, itemsets have supports very different from their sup-ports in the nonrandomized dataset T . Therefore, tech-niques were developed that allow to estimate original sup-ports given randomized supports. It is important to note that randomized support of an itemset A is a random vari-able that depends on the original supports of all subsets of this itemset. Indeed, a transaction that contains all but one item of A has a very different probability to contain A after randomization than a transaction that contains no items of A . So, in [11] the behavior of itemset A , | A | = k , w.r.t. randomization is characterized by the ( k + 1)-vector of its partial supports ~s = ( s 0 , s 1 , . . . , s k ) T It is shown that the vector ~s 0 of randomized partial supports is distributed as 1 /N times a sum of multinomial distribu-tions, with its expectation and covariance matrix being depend on the parameters of the randomization operator. Matrix P is defined as where R is the randomization operator. Computing the in-verse matrix Q = P  X  1 gives an unbiased estimator ~s est as well as the estimator X  X  covariance matrix and its unbiased estimator: ~s In particular, it lets us estimate the nonrandomized sup-port s of A and its variance: s The support estimator formula can be used inside Apriori al-gorithm for mining frequent itemsets, so that the algorithm works over randomized dataset. However, since the estima-tor is a random variable, it may violate the antimonotonicity property. This may cause an itemset to be discarded even though its estimated support, as well as its true support, is above threshold. This effect can be reduced by lowering the threshold by an amount proportional to the standard deviation of the estimator. Consider the following simple randomization R : given a transaction t , we consider each item in turn, and with prob-ability 80% replace it with a new random item; with proba-bility 20% we leave the item unchanged. Since most of the items get replaced, we may suppose that this randomization preserves privacy well. However, it is not so, at least not all the time. Indeed, let A = { x, y, z } be a 3-item set with partial supports Assume that overall there are 10,000 items and 10 million transactions, all of size 10. Then 100,000 transactions con-tain A , and 500,000 more transactions contain all but one items of A . How many of these transactions contain A af-ter they are randomized? The following is a rough average SIGKDD Explorations. Volume 4, Issue 2 -page 45 estimate: | A  X  t | = 2 and A  X  R ( t ) : 500 , 000  X  0 . 2 2  X  8  X  0 . 8 | A  X  t | 6 1 and A  X  R ( t ) : &lt; 10 7  X  0 . 2  X  9  X  0 . 8 So, there will be about 814 randomized transactions con-taining A , out of which about 800, or 98%, contained A before randomization as well. Now, suppose that the server receives from client C i a randomized transaction R ( t ) that contains A . The server now knows that the actual, non-randomized transaction t at C i contains A with probability about 98%. On the other hand, the prior probability of A  X  t is just 1%. The disclosure of A  X  R ( t ) has caused a probability jump from 1% to 98%. Paper [11] calls this situation a privacy breach .
 Intuitively, a privacy breach with respect to some property P ( t ) occurs when, for some possible outcome of random-ization (= some possible view of the server), the posterior probability of P ( t ) is higher than a given threshold called the privacy breach level . Of course, there are always some properties that are likely; so, we have to only look at  X  X n-teresting X  properties, such as the presence of a given item in t . Statistical database literature considers a similar notion ( pessimistic risk ) for record identification [17]. In [11], the following special case of breaches is considered: Itemset A causes a privacy breach of level p b if for some item a  X  A and some transaction t we have P [ a  X  t | A  X  R ( t )] &gt; p It is assumed here that the transaction at each client is an independent instance of a distribution over transactions. In order to prevent privacy breaches from happening, pa-per [11] suggests to randomize transactions by inserting many  X  X alse X  items, as well as deleting some  X  X rue X  items. So many  X  X alse X  items should be inserted into a transaction that one is as likely to see a  X  X alse X  itemset as a  X  X rue X  one. In select-a-size randomization operator, it is the randomiza-tion level  X  that determines the probability of a  X  X alse X  item to be inserted. The other parameters, namely the distribu-tain  X  X utoff X  integer K , any number of items from 0 to K is retained from the original transaction with probability 1 / ( K + 1), while the rest of the items are inserted indepen-dently with probability  X  . The question of optimizing all select-a-size parameters to achieve maximum recoverability for a given breach level is left open.
 The parameters of randomization are checked for privacy as follows. It is assumed that the server knows the maxi-mum possible support of an itemset for each itemset size, among transactions of each transaction size, or their upper bounds. Based on this knowledge, the server computes par-tial supports for (imaginary) privacy-challenging itemsets , and tests randomization parameters by computing posterior probabilities P [ a  X  t | A  X  R ( t )] from the definition of pri-vacy breaches. The randomization parameters are selected to keep variance low while preventing privacy breaches for privacy-challenging itemsets.
 Graphs and experiments with real-life datasets show that, given several million transactions, it is possible to find ran-domization parameters so that the majority of 1-item, 2-item, and 3-item sets with support at least 0.2% can be recovered from randomized data, for privacy breach level of 50%. However, long transactions (longer than about 10 items) have to be discarded, because the privacy-preserving randomization parameters for them must be  X  X oo random-izing, X  saving too little for support recovery; in both real-life datasets used in [11] most transactions had 5 items or less. Those itemsets that were recovered incorrectly ( X  X alse drops X  and  X  X alse positives X ) were usually close to the sup-port threshold, i.e. there were few outliers. The standard deviation for 3-itemset support estimator was at most 0.07% for one dataset and less than 0.05% for the other; for 1-item and 2-item sets it is smaller still. Each of the papers [4; 1; 22; 11] suggests its own way of measuring privacy, and there are other suggestions in the lit-erature. In [4] (see Section 2) privacy is measured in terms of confidence intervals. The nonrandomized numerical at-tribute x i is treated as an unknown parameter of the dis-tribution of the randomized value Z i = x i + Y i . Given an instance z i of the randomized value Z i , the server can com-pute an interval I ( z i ) = [ x  X  ( z i ) , x + ( z i )] such that x with at least certain probability c %; this should be true for as a privacy measure of the randomization.
 Unfortunately, as pointed out in [1], this measure can be misleading. One problem is that the domain of the nonran-domized value and its distribution are not specified. Con-sider an attribute X with the following density function: Assume that the perturbing additive Y is distributed uni-formly in [  X  1 , 1]; then, according to the confidence interval measure, the amount of privacy is 2 at confidence level 100%. However, if we take into account the fact that X must be within [0 , 1]  X  [4 , 5], we can always compute a confidence in-terval of size 1 (not 2). The interval is computed as follows: Moreover, in many cases the confidence interval can be even shorter: for example, for z =  X  0 . 5 we can give interval [0 , 0 . 5] of size 0.5.
 Paper [1] suggests to measure privacy using Shannon X  X  infor-mation theory [26; 25]. The average amount of information in the nonrandomized attribute X depends on its distribu-tion and is measured by its differential entropy h ( X ) = E The average amount of information that remains in X after the randomized attribute Z is disclosed can be measured by the conditional differential entropy The average information loss for X that occurs by disclos-ing Z can be measured in terms of the difference between SIGKDD Explorations. Volume 4, Issue 2 -page 46 the two entropies:
I ( X ; Z ) = h ( X )  X  h ( X | Z ) = E This quantity is also known as mutual information between random variables X and Z . It is proposed in [1] to use the following functions to measure amount of privacy ( X ( X )) and amount of privacy loss ( P ( X | Z )): In the example above (see (2)) we have  X ( X ) = 2;  X ( X | Z ) = 2 h ( X | Z )  X  0 . 84; P ( X | Z )  X  0 . 58 . A possible interpretation of these numbers is that, without knowing Z , we can localize X within a set of size 2; when Z is revealed, we can (on average) localize X within a set of size 0.84, which is less than 1.
 However, even this information-theoretic measure of privacy is not without some difficulties. To see why, we have to turn to the notion of privacy breaches from [11]. In the example above, suppose that clients would not like to disclose the property  X  X 6 0 . 01. X  The prior probability of this property is 0.5%; however, if the randomized value Z happens to be in [  X  1 ,  X  0 . 99], the posterior probability P [ X 6 0 . 01 | Z = z ] becomes 100%. Of course, Z  X  [  X  1 ,  X  0 . 99] is unlikely:
P [  X  1 6 Z 6  X  0 . 99] = Therefore, Z  X  [  X  1 ,  X  0 . 99] occurs for about 1 in 100,000 records. But every time it occurs the property  X  X 6 0 . 01 X  is fully disclosed , becomes 100% certain. The mutual infor-mation, being an average measure, does not notice this rare disclosure. Nor does it alert us to the fact that whether X  X  [0 , 1] or X  X  [4 , 5] is fully disclosed for every record; this time it is because the prior probability of each of these properties is high (50%).
 The notion of privacy breaches, on the other hand, cap-tures these disclosures. Indeed, for any privacy breach level  X  &lt; 100% and for some randomization outcome (namely, for Z 6  X  0 . 99) the posterior probability of property  X  X 6 0 . 01 X  is above the breach level. The problem with the defi-nition of privacy breaches from [11] is that we have to spec-ify which properties are privacy-sensitive, whose probabili-ties must be kept below breach level. Specifying too many privacy-sensitive properties may require too destructive a randomization, leading to a very imprecise aggregate model at the server. Thus, the question of the right privacy mea-sure is still open.
 A completely different approach to measuring privacy is sug-gested in [16]. This paper measures private information in terms of its monetary value, as a form of intellectual prop-erty. The cost of each piece of information must be deter-mined in a  X  X air X  way, so as to reflect the contribution of this piece in the overall profit. Two notions of fairness are analysed in [16], both coming from the theory of coalitional games: the core and the Shapley value . Let S be the set of potential participants in a business, and for every subset S  X  S we know the payoff v ( S 0 ) that occurs if only the par-ticipants in S 0 actually cooperate. Then the core consists of all possible ways to divide the total payoff v ( S ) between all participants so that, for all S 0  X  S , the share given to S least v ( S 0 ). In other words, the core contains all  X  X air X  ways of dividing the total payoff, in the sense that no group of participants has an incentive to secede. The Shapley value is a way to divide the payoff so that each participating agent is awarded an amount equal to the average contribution of this agent to the payoff of the group at the time of his or her arrival, where the average is taken over all arrival orders of the agents. In our case, participating clients disclose cer-tain private information to the server and then benefit from the data model built by the server. The paper analyses the core and the Shapley value for several simplified scenarios involving private information. The research in using randomization for preserving privacy has shown promise and has already led to interesting and practically useful results. It gives an impression of being a part of some deeper statistical approach to security and privacy, providing a connection to the groundbreaking work of Claude Shannon on secrecy systems [25], and allows us to look at privacy under a different angle than the conven-tional cryptographic approach [6; 21]. It raises an important question of measuring privacy, which should be addressed in the purely cryptographic setting as well since the disclosure through legitimate query answers must also be measured. Randomization does not rely on intractability hypotheses from algebra or number theory, and does not require costly cryptographic operations or sophisticated protocols. It is possible that future studies will combine statistical approach to privacy with cryptography and secure multiparty compu-tation, to the mutual benefit of all of them. [1] D. Agrawal and C. C. Aggarwal. On the design and [2] R. Agrawal, T. Imielinski, and A. Swami. Mining asso-[3] R. Agrawal and R. Srikant. Fast algorithms for mining [4] R. Agrawal and R. Srikant. Privacy preserving data [5] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. SIGKDD Explorations. Volume 4, Issue 2 -page 47 [6] C. Clifton, M. Kantarcioglu, X. Lin, J. Vaidya, and [7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-[8] G. T. Duncan, R. Krishnan, R. Padman, P. Reuther, [9] G. T. Duncan and S. Mukherjee. Optimal disclosure [10] T. Evans, L. Zayatz, and J. Slanta. Using noise for dis-[11] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. [12] S. E. Fienberg, U. E. Makov, and R. J. Steele. Disclo-[13] J. M. Gouweleeuw, P. Kooiman, L. C. R. J. Willen-[14] V. S. Iyengar. Transforming data to satisfy privacy [15] J. J. Kim and W. E. Winkler. Masking microdata files, [16] J. Kleinberg, C. H. Papadimitriou, and P. Raghavan. [17] D. Lambert. Measures of disclosure risk and harm. [18] R. J. A. Little. Statistical analysis of masked data. [19] G. J. McLachlan and T. Krishnan. The EM Algorithm [20] S. Mukherjee and G. T. Duncan. Disclosure limita-[21] B. Pinkas. Cryptographic techniques for privacy-[22] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy [23] P. Samarati and L. Sweeney. Generalizing data to pro-[24] P. Samarati and L. Sweeney. Protecting privacy when [25] C. E. Shannon. Communication theory of secrecy sys-[26] C. E. Shannon and W. Weaver. The Mathematical [27] L. Zayatz, R. Moore, and B. T. Evans. New directions in
