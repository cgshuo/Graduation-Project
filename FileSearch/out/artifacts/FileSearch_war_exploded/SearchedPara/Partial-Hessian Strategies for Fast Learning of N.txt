 Miguel  X  A. Carreira-Perpi  X  n  X  an MCARREIRA -PERPINAN We consider a well-known formulation of dimensionality reduction: we are given a matrix of N  X  N (dis)similarity values, corresponding to pairs of high-dimensional points y , . . . , y N (objects), which need not be explicitly given, and we want to obtain corresponding low-dimensional points x tances optimally preserve the similarities. Methods of thi s type have been widely used, often for 2D visualization, in all sort of applications (notably, in psychology). They in-clude multidimensional scaling (originating in psychomet -rics and statistics; Borg &amp; Groenen , 2005 ) and its variants such as Sammon X  X  mapping ( Sammon , 1969 ), PCA defined on the Gram matrix, and several methods recently devel-oped in machine learning: spectral methods such as Lapla-cian eigenmaps ( Belkin &amp; Niyogi , 2003 ) or locally linear embedding ( Roweis &amp; Saul , 2000 ), convex formulations such as maximum variance unfolding ( Weinberger &amp; Saul , 2006 ), and nonconvex formulations such as stochastic neighbor embedding (SNE; Hinton &amp; Roweis , 2003 ) and its variations (symmetric SNE, s-SNE: Cook et al. , 2007 ; Venna &amp; Kaski , 2007 ; t -SNE: van der Maaten &amp; Hinton , 2008 ); kernel information embedding ( Memisevic , 2006 ); and the elastic embedding (EE; Carreira-Perpi  X  n  X  an , 2010 ). Spectral methods have become very popular because they have a unique solution that can be efficiently computed by a sparse eigensolver, and yet they are able to unfold nonlinear, convoluted manifolds. That said, their embed-dings are far from perfect, particularly when the data has nonuniform density or multiple manifolds. Better results have been obtained by the nonconvex methods, whose ob-jective functions better characterize the desired embed-dings. Carreira-Perpi  X  n  X  an ( 2010 ) showed that several of these methods (e.g. SNE, EE) add a point-separating term to the Laplacian eigenmaps objective. This causes im-proved embeddings: images of nearby objects are encour-aged to project nearby but, also, images of distant objects are encouraged to project far away.
 However, a fundamental problem with nonconvex meth-ods, echoed in most of the papers mentioned, has been their difficult optimization. First, they can converge to bad local optima. In practice, this can be countered by using a good initialization (e.g. from spectral meth-ods), by simulated annealing (e.g. adding noise to the up-dates; Hinton &amp; Roweis , 2003 ) or by homotopy methods merical optimization has been found to be very slow. Most previous work has used simple algorithms, some adapted from the neural net literature, such as gradient descent wit h momentum and adaptive learning rate, or conjugate gradi-ents. These optimizers are very slow with ill-conditioned problems and have limited the applicability of nonlinear embedding methods to small datasets; hours of training for a few thousand points are typical, which rules out interac-tive visualization and allows only a coarse model selection . Our goal in this paper is to devise training algorithms that are not only significantly faster but also scale up to larger datasets and generalize over a family of embedding algo-rithms (SNE, t -SNE, EE and others). We do this not by simply using an off-the-shelf optimizer, but by understand -ing the common structure of the Hessian in these algo-rithms and their relation with the graph Laplacian of spec-tral methods. Thus, our first task is to provide a general formulation of nonconvex embeddings (section 1 ) and un-derstand their Hessian structure, resulting in several opt i-mization strategies (section 2 ). We then empirically evalu-ate them (section 3 ) and conclude by recommending a strat-egy that is simple, generic, scalable and typically (but not always) fastest X  X y up to two orders of magnitude over ex-isting methods. Throughout we write pd (psd) to mean pos-itive (semi)definite, and likewise nd (nsd). Call X = ( x sional points, and define an objective function: where E + is the attractive term , which is often quadratic psd and minimal with coincident points, and E  X  is the re-pulsive term , which is often nonlinear and minimal when points separate infinitely. Optimal embeddings balance both forces. Both terms depend on X through Euclidean distances between points and thus are shift and rotation in-variant. We obtain several important special cases: Normalized symmetric methods minimize the KL diver-Normalized nonsymmetric methods consider instead Unnormalized models dispense with distributions and Spectral methods such as Laplacian eigenmaps or LLE This formulation suggests previously unexplored algo-rithms, such as using an Epanechnikov kernel, or a t -EE, or using homotopy algorithms for SNE/ t -SNE, where we follow the optimal path X (  X  ) from  X  = 0 (where X = 0 ) to  X  = 1 . It can also be extended to closely related methods for embedding (kernel information embedding; Memisevic , 2006 ) and metric learning (neighborhood com-ponent analysis; Goldberger et al. , 2005 ), among others. We express the gradient and Hessian (written as matrices of d  X  N and N d  X  N d , resp.) in terms of Laplacians, follow-ing Carreira-Perpi  X  n  X  an ( 2010 ), as opposed to the forms used in the SNE papers. This brings out the relation with spectral methods and simplifies the task of finding pd terms. Given an N  X  N symmetric matrix of weights W = ( w define its graph Laplacian matrix as L = D  X  W where D = diag ( P N n =1 w nm ) is the degree matrix. Likewise we get L + from w + negative (since u T Lu = 1 0 ). The Laplacians below always assume summation over points, so that the dimension-dependent N d  X  N d Lapla-cian L xx (from weights w xx cian for each ( i, j ) point dimension. All other Laplacians are dimension-independent, of N  X  N . Using this conven-tion, we have for normalized symmetric models:  X  E = 4 XL (2)  X  2 E = 4 L  X  I d + 8 L xx  X  16  X  vec ( XL q ) vec ( XL q where I K = kernel , K 1 = (log K )  X  = K  X  /K, K 2 = K  X  X  /K
K 21 = (log K )  X  X  = ( KK  X  X   X  ( K  X  ) 2 ) /K 2 = K 2  X  K 2 and weights ( K In particular, for s-SNE the weights are as follows: and for t -SNE they are ( K means 1 / (1 + k x w For the elastic embedding (an unnormalized model): Note the Hessian of the spectral method (i.e., for  X  = 0 , with constant weights w + Our goal is to achieve search directions that are fast to com-pute, scale up to larger N , and lead to global, fast con-vergence. This rules out computing the entire Hessian. Carreira-Perpi  X  n  X  an ( 2010 ) derived pd directions for EE by using splits of the gradient such as  X  E = 4 X ( D + + ( L  X  D + )) = 0 (where D + is the degree matrix of L + = D +  X  W + ), then deriving a fixed-point iterative scheme ( ` a la Jacobi) such as X = X ( D +  X  L )( D + )  X  1 and a search direction X ( D +  X  L )( D + )  X  1  X  X . Here we use a more general approach that illuminates the merits of each method, by directly working with the Hessian  X  2 E . We define directions p where g trix (this ensures a descent direction: p T a line search on the step size  X  conditions to obtain the next iterate x ( Nocedal &amp; Wright , 2006 ). This defines a range of meth-ods from B  X  2 E ( X k ) (Newton X  X  direction, which would require mod-ification to ensure descent but is too expensive anyway). We construct B partial Hessian ). Inspection of the very special structure of  X  2 E in eqs. ( 2 ) and ( 3 ) immediately shows what parts we can use. Our driving principle is to use as much Hessian in-formation as possible that is psd, fast to compute and leads to an efficient solution of the p or constant B solving a Hessian nonsparse linear system is O ( N 3 d 3 ) Search directions For normalized symmetric (and non-symmetric) models ( 2 ), we consider functions K with a nonnegative argument t  X  0 and satisfying K ( t ) &gt; 0 and K ( t ) &lt; 0 , i.e., positive and decreasing. The term on contains a psd part  X  K and EE) and a nsd part  X K guaranteed to contain a psd part for i = j and depend-ing on the signs of K always nsd. These psd parts can be used to construct de-(
K ( t ) = e  X  t , K 1 =  X  1 , K 2 = 1 , K 21 = 0 ) and t -SNE (
K ( t ) = 1 EE (an unnormalized model with K ( t ) = e  X  t ), we follow an analogous but simpler process: the Hessian lacks some of the nsd parts in normalized models, e.g. the vec ( ) term, so it should afford better psd Hessian approximations. The Spectral Direction (SD) We have found that in most cases a particular partial Hessian strikes the best compro-mise between deep descent and efficient computation, and yields what we call the spectral direction (SD). It is con-structed purely from the attractive Hessian  X  2 E + ( X ) = 4 L +  X  I d , which as noted earlier is psd, and consists of identical diagonal blocks of N  X  N . For EE and s-SNE this amounts to taking  X  = 0 and so using the Hessian of the spectral method, thus it would achieve quadratic con-vergence in that case. We find it works surprisingly well for  X  &gt; 0 . Effectively, we  X  X end X  the exact gradient of the nonlinear E using the curvature of the spectral E + . This basic direction is refined as follows. (1) Owing to the shift invariance of E , the resulting linear system is not pd but psd. To prevent numerical problems we add a small to it ( B systems R T is the upper triangular Cholesky factor of B computed in place in O ( 1 routines, and is sparse if B scalability. For Gaussian kernels (SNE, EE) L + is constant and it need only be factorized once in the first iteration. If L + depends on X , as in t -SNE, scalability is achieved by taking it constant (e.g. L + at X = 0 ). (3) We allow the user to sparsify L + through (say) a  X  -nearest-neighbor graph, which is often available as part of the data (the affinities w  X  = N (no sparsity), which yields B k = L + , to  X  = 0 (most sparsity), which yields B diagonal fixed-point method of Carreira-Perpi  X  n  X  an , 2010 ). We explored further variations in the experiments, such as updating the diagonal of R of the full Hessian, with little improvement. Using the technique of Carreira-Perpi  X  n  X  an ( 2010 ) of fixed-point iter-ation from gradient splits, van der Maaten ( 2010 ) derives a nonsparse spectral direction for t -SNE, but he overlooks to introduce spectral information during the optimization , Memisevic &amp; Hinton ( 2005 ) use a search direction where B over the gradient but, as one would expect, experimentally it is not competitive with our spectral direction. From the user point of view this yields a simple recipe that, given the gradient of E , does not need the more complex Hessian of E  X  . The only user parameter is the sparsity level  X  (number of neighbors) to tune the speed of conver-gence; convergence itself is guaranteed for all  X  by th. 2.1 .  X  should be simply tuned to as large as computation will allow, while thresholding otherwise negligible values. Th e cost of computing the direction is O ( N 2 d ) , the same order (less if sparse) than computing the gradient or E in the line search, and we find its overhead negligible in practice. This affords directions that descend far deeper than gradient or diagonal-Hessian at the same cost per iteration.
 In summary, the spectral direction works as follows. Be-fore starting to iterate, compute the attractive Hessian  X  2 E + ( X ) = 4 L +  X  I d , sparsified to  X  nearest neighbors, add the small I to it, and cache its sparse Cholesky factor R . At iteration k , given the gradient g R T ( Rp k ) =  X  g k to obtain the spectral direction p k . Convergence The following theorem guarantees global convergence (to a stationary point from any initial x can be derived from Zoutendijk X  X  condition and exercise 3.5 in Nocedal &amp; Wright ( 2006 , p. 39,63).
 Theorem 2.1. Consider the iteration x where p pd and  X  If
E is bounded below in R Nd and continuously differen-tiable in an open set N containing the level set of x is Lipschitz continuous in N , and the condition number of B In our case, we can ensure the condition number is bounded by simply adding is bounded), which we do in practice anyway since some of our B functions we use. From eq. (10.30) in Nocedal &amp; Wright ( 2006 ) and with bounded condition number, it follows that where x  X  is a minimizer of E , H ( x  X  ) and B ( x  X  ) its Hes-sian and matrix B , and r = B  X  1 ( x  X  ) H ( x  X  )  X  I . Thus the iterations have locally linear convergence with rate r we use unit step sizes (which we see in practice). The better the Hessian approximation B the smaller r and the faster the convergence. This is quantified in the experiments. Other Partial-Hessians B solve a nontrivial linear system B accelerated in several ways: (1) by solving the system in an inexact way using linear CG initialized at the previous iter -ation X  X  solution, exiting the solver after a certain tolera nce  X  &gt; 0 is achieved. (2) By updating B k and its Cholesky factor every T  X  1 iterations. The user has control on the exactness of the solution through  X  or T . The gradient is al-ways updated at each iteration. For the experiments in this paper we will focus on strategies with  X  &gt; 0 and T = 1 We have explored a number of partial Hessians as well as different strategies for efficient linear system solu-tion, in datasets with s-SNE, t -SNE and EE. Here we report a representative subset of results, including what we consider the overall winner (the spectral direction). We compare the following methods: gradient descent (GD), used in SNE ( Hinton &amp; Roweis , 2003 ) and ( van der Maaten &amp; Hinton , 2008 ); fixed-point diagonal it-eration (FP), used in EE ( Carreira-Perpi  X  n  X  an , 2010 ), much faster than GD; the diagonal of the full Hessian (DiagH); nonlinear conjugate gradients (CG) and L-BFGS (typical choices for large problems); spectral direction (SD), pos-sibly sparsified and caching the Cholesky factor before the first iteration; and a partial Hessian 4 L + + 8  X  L xx (which we call SD X ). The latter consists of positive block-diagonal elements of 8  X  L xx corresponding to entries asso-ciated with the same dimension ( i = j in w xx ensures a psd approximation and adds information about the Hessian of the repulsive term E  X  ( X ) . Except for GD, FP and CG, all the other methods have not been applied to SNE-type methods that we know. Several of these methods require the user to set parameter values. For L-BFGS we tried several values for its user parameter m (the number of vector pairs to store in memory) and found m = 100 best. For SD X , we solve the linear system with linear CG, exiting early when the relative tolerance  X  drops below 0 . 1 or we reach 50 linear CG iterations. Generally, these parameters are hard to tune and there is little guidance on which values are the best. This is an important reason why the spectral direction, which requires no parameters to tune and per-forms very well, is our preferred method.
 We also tried other methods that were not generally com-petitive and do not report them, to keep the presentation clearer. For example, adding to the SD Hessian the diago-nal of the full Hessian (which depends on X and so varies over iterations), and solving the linear system by approxi-mately updating the Cholesky factorization or by using CG. Once the direction is obtained for a given method, we use a backtracking line search ( Nocedal &amp; Wright , 2006 ) to find a step size satisfying the first Wolfe condition (sufficient decrease). As initial step size we always try the natural ste p  X  = 1 (recommended for quasi-Newton updates). How-ever, we observed that some methods (in particular SD) tend to settle to accepted step sizes that are somewhat less than 1. For such cases we used an adaptive strategy: the initial backtracking step at iteration k equals the accepted step from the previous iteration, k  X  1 . This is a conser-vative strategy because once the step decreases it cannot increase again, but it compensates in saving line searches with require expensive evaluations of the error E . For nonlinear CG, we use Carl Rasmussen X  X  implementation minimize.m , which uses a line search that is more sophis-ticated than backtracking, and allows steps longer than 1. We evaluated these methods in a small dataset in three con-ditions (converging to the same minimum, converging to different minima, and homotopy training), and in a large dataset. For EE we used  X  = 100 . 3.1. Small dataset: COIL-20 image sequences The COIL-20 dataset contains rotation sequences of ob-jects every 5 degrees, so each data point is a grayscale im-age of 128  X  128 pixels. We selected sequences for ten objects for a total of N = 720 points in D = 16 384 dimen-sions, corresponding to ten loops (1D closed manifolds) in R
D . In all the experiments we used SNE affinities with perplexity k = 20 , resulting in a nonsparse N  X  N matrix W + , and reduced dimension to d = 2 , so visual inspection could be used to assess the quality of the result. For SD we used no sparsification (  X  = N ).
 Convergence to the same minimum from the same ini-tial X We determined embeddings X X that all methods converged to X X tination. This allows us to reduce effects due to different local minima of the error E having possibly different char-acteristics. Fig. 1 shows learning curves for EE and s-SNE as a function of the number of iterations and the runtime. In decreasing runtime, the methods can be roughly ordered as GD  X  (FP,DiagH) &gt; (CG,SD X ) &gt; (L-BFGS,SD), with GD being over an order of magnitude slower than FP, and FP about an order of magnitude slower than SD (note the log X axis). The runtime behavior and the number of iterations required agrees with the intuition that the more Hessian in-formation the better the direction, as long as the iteration s are not too expensive. Note how the method using most Hessian information, SD X , uses the fewest iterations (left panels), but these are also the slowest, which shifts its run -time curves right. For all the other methods, computing the direction costs less than computing the gradient itself. FP is very similar to DiagH. GD was the only method that did not reach the convergence value even after 10 000 iterations (20 minutes runtime).
 L-BFGS is a leading method for large-scale problems. It estimates inverse Hessian information through rank-2 up-dates, which gives better directions than the gradient, and obtains the direction from a series of outer products rather than solving a linear system, which is fast. The main prob-lems of L-BFGS ( Nocedal &amp; Wright , 2006 , p. 180,189) are that it converges slowly on ill-conditioned problems and that, with large N d , it requires an initial period of many iterations before its Hessian approximation is good. While for the small problem of fig. 1 L-BFGS is almost compet-itive with the SD, in the larger problem of fig. 4 it is not: 70 iterations (for EE) give a rank-140 approximation to a 40 k  X  40 k Hessian matrix, which fails to decrease the error. Nonlinear CG is generally inferior to L-BFGS and this is seen in the figure too. (Our results unfairly favor CG be-cause its minimize.m implementation uses a better line search than in our implementation of the other methods.) From the beginning, the SD has an exact part of the Hessian that is pd, and obtains the direction from triangular back-solves (same cost as matrix-vector product, and dominated by the cost of computing the gradient). The only overhead is in the initial Cholesky decomposition, which is small, and progress thereafter is consistently fast.
 Convergence from random initial X to possibly differ-ent minima We generated 50 random points X small values) and ran each method initialized from each X error E and number of iterations for each initialization, for EE and s-SNE. They confirm the previous observations, in particular SD and L-BFGS achieve the lower errors, but SD does so more reliably (less vertical spread). GD (outside the plot) barely moved from the initial X x Homotopy optimization for EE The EE error function E ( X ;  X  ) can be optimized by homotopy, by starting to min-imize over X at  X   X  0 , where E is convex, and following a path of minima to a desired  X  by minimizing over X as  X  increases ( Carreira-Perpi  X  n  X  an , 2010 ). This is slower than directly minimizing at the desired  X  from a random initial X , but usually finds a deeper minimum. We used 50 log-spaced values of  X  from 10  X  4 to 10 2 and minimized E at each  X  value until the relative error decrease was less than 10  X  6 or we reached 10 4 iterations. Tracking the path X (  X  ) so closely, we were able to have all methods converge to es-sentially the same embedding (shown in fig. 3 ), except for GD, whose embedding was still evolving (it would have re-quired many more iterations to converge). Fig. 3 shows the runtime and the number of iterations for each  X  value. The results again demonstrate a drastic improvement of our SD over existing methods (GD and FP from Carreira-Perpi  X  n  X  an , 2010 ), and confirm that more Hessian information results in fewer iterations and function evalu -ations required. Also, we observe that the SD step sizes decrease from 1 for  X  &lt; 0 . 02 to 0 . 1 for the final  X  though we reset to 1 the initial backtracking step every time we increase  X  ). Presumably, as  X  increases, so does the ef-fect of the term E  X  ( X ) , which the SD Hessian ignores. 3.2. Large dataset: MNIST handwritten digit images While more Hessian information enables deeper decreases of the error per iteration, this comes at the price of solving a more complex linear system. To see how the different optimization methods scale up, we tested them on a dataset considerably larger than those in the literature ( N = 6 000 points in van der Maaten &amp; Hinton , 2008 ). We used N = 20 000 MNIST images of handwritten digits (each a 28  X  28 pixel grayscale image, i.e., of dimension D = 784 ). We used SNE affinities with perplexity k = 50 and reduced dimension to d = 2 . All our experiments were run in a 1.87 GHz workstation, without GPUs or parallel processing. We ran several optimization methods (GD, FP, L-BFGS, SD, SD X ) for 1 hour each, for both EE and t -SNE. For the SD we used a sparse L matrix with  X  = 7 .
 As noted in section 2 , for EE and s-SNE the Hessian of E + ( X ) (i.e., the matrix L + ) is constant, so we cache its Cholesky factor before starting to iterate. For t -SNE, this Hessian depends on X , and recalculating it and solving a linear system (even sparse and using linear CG) at each iteration is too costly. Thus, we fix it to the Hessian at the initial X and cache its Cholesky factor just as with EE. This still gives descent directions that work very well. Fig. 4 shows the resulting learning curves for EE and t -SNE as a function of the number of iterations and the runtime. Some methods X  deficiencies that were already detectable in the small-scale experiments become exaggerated in the larger scale. The SD X  direction, while still able to produce good steps, now takes too much time per iteration (even though it is solved inexactly by CG), and is able to com-plete only 37 iterations for EE and 13 for t -SNE within the allotted time (1 hour). Note SD X  does worse than SD in number of iterations even though it uses more Hessian information; this is likely due to the inexact linear system solution. In general, all methods run more iterations for EE than for s-SNE and t -SNE, indicating EE X  X  simpler er-ror function E is easier to minimize. GD is omitted, be-cause it showed no decrease of the objective function. For both EE and t -SNE we never observe any decrease with L-BFGS within 1 hour, although we have tried various val-ues for m ( m = 5 , 50 , 100 ); it does decrease a little after 3 hours. This is due to the long time needed to approximate the enormous Hessian. Nonlinear CG does decrease the objective function for EE, but most of the computational resources are spent on the line search. Thus CG did least number of iterations compared to other methods. Our SD has mostly converged already in 15 minutes. SD has a rea-sonable setup time of 5 min. in both EE and t -SNE to com-pute the Cholesky factorization (this time can be controlle d with the sparsification  X  ), and it is amply compensated for by the speed of the sparse backsolves in computing the di-rection at each iteration (which are essentially for free co m-pared to computing the gradient). SD decreases the objec-tive consistently and efficiently from the first iterations. FP does scale up in terms of cost per iteration, but, as in the small dataset, each step makes considerably less progress than a SD step. In summary, FP, SD X  and L-BFGS are clearly not competitive with SD, which is able to scale its computational requirements and still achieve good steps. Fig. 4 also shows the resulting embeddings for FP from SD at an intermediate stage (after 20 runtime for EE and 1 hour for t -SNE). The difference is qualitatively obvious. The SD embedding already separates well many of the dig-its, in particular zeros, ones, sixes and eights. The FP em-bedding shows no structure whatsoever. Given the exceedingly long runtimes of gradient descent, we suspect some of the embeddings obtained in the lit-erature of SNE using gradient descent could be actually far from a minimum, thus underestimating the power of SNE. The optimization methods we present, in particular the spectral direction, should improve this situation. Experimentally, no single method is always the best. If we weigh efficiency, robustness (to user parameters) and simplicity (of implementation using existing linear alge-bra code and of user parameter setting), we believe that the spectral direction with cached Cholesky factor, possib ly sparsified, is the preferred strategy. It achieves good step s and can be computed in less time than the gradient and ob-jective function E . However, in really large problems even computing E and  X  E may be too time consuming. Note that, for SNE and t -SNE, even if p tractive term, the negative term is still a full N  X  N matrix (though the matrix itself need not be stored for E or  X  E be computed). One solution to this is to use there a sparse embedding may be affected depending on the sparsity level. (Note this would not affect the construction of our spectral direction, since it does not depend on E  X  .) Another way to accelerate the computations of sums of many Gaussians, needed in E and  X  E , is to use fast multipole methods ( Greengard &amp; Strain , 1991 ; Raykar &amp; Duraiswami , 2006 ), which can reduce the time to O ( N ) if the dimension d is low (which should be the case in practice). We have provided a generalized formulation of embed-dings resulting from the competition between attraction and repulsion that includes several important existing al-gorithms and suggests future ones. We have uncovered the relation with spectral methods and the role of graph Laplacians in the gradient and Hessian, and derived sev-eral partial-Hessian optimization strategies. A thorough empirical evaluation shows that among several competitive strategies one emerges as particularly simple, generic and scalable, based on the Cholesky factors of the (sparsified) attractive Laplacian. This adds a negligible overhead to the computation of the gradient and objective function but improves existing algorithms by 1 X 2 orders of magnitude. The quadratic cost of the gradient and objective function re -mains a bottleneck which future work may address. Code implementing the algorithms is available from the authors. Work funded in part by NSF CAREER award IIS X 0754089. Objective function value Number of iterations Fixed-point iteration (
Carreira-Perpi  X  n  X  an , 2010 )
Spectral direction (this paper)
