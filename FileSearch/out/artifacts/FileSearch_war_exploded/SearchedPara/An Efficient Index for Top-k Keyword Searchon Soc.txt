 Social networks have attracted signific ant attention recently due to its fast user growth and high user stickiness. Many users have a search requirement to find new or existing friendships with similar interests in social networks. A well-known computing model is keyword search, whic h provides a user-friendly interface to meet users search demands. For exampl e, Andy a Facebook user wants to invite several people to join a football match with him. He may search  X  X ootball X  and get some football fan candidates. There are tons of football fans, however Andy actually prefer to candidates who has a closer relationship with him. Another example, Yvonne a user of Linkedin is seeking for an IT job, and she wants to get some references through her social connections. She may search  X  X ilicon Valley X  or  X  X oogle X  to find some people who are both highly related to the keywords and more familiar with her. Traditional keyword search techniques only consider the textual proximity and ignore the relationship closeness between users, So our user may not satisfied with the results.

Although techniques of traditional keyword search are well studied [14], it is a big challenge to integrate social relationship and textual proximity to support keyword search on social networks. On social networks, users are connected by friendship or similar relationships. In this paper, relationship closeness of di-rectly connected users can be identified by i nteractive activities such as replies, visits and so on. Relationship closeness o f indirectly connected users are associ-ated with the friendship chains between them. The idea is very intuitive from observation of real life. Two strangers in real life usually know each other by some mutual friends and the closer their friendship with the mutual friends the closer relationship they may have. With out an intergraded framework, we need to carry out keyword search and relationship closeness calculation separately. One possible solution is that we first carry out traditional keyword search to generate some candidates and then calculate relationship closeness for them to generate the final top-k. Or we may follow a breadth-first idea to calculate their textual proximities until top-k vacancies are full filled. Neither of the two solu-tions are efficient because candidates wit h high textual proximity may far away from the query user and candidates close to the query user may not related to the keywords at all.

In this paper, we map social networks to a graph structure and reduce the problem to a top-k keyword search over graph and focus on ranking functions constituted by textual proximity and relationship closeness. We make the fol-lowing major contributions: 1. We propose TDK-Index which integrate keyword index and relationship 2. We develop Two-phase TA algorithm which narrows the threshold obviously 3. Our solutions are flexible for parameter adjustment to adopt different appli-The rest of the paper is organized as follows. We formulate the problem in Section 2 and present TDK-Index and Two-phase algorithm in Section 3 and Section 4. Experiment results are reported in Section 5 and related works are discussed in Section 6. Finally, we make conclusions in Section 7. Given an undirected positive weight keyword embedded graph G ( V,E,W,T ), where vertex v  X  V edge G ( V,E,W,T ), and w ij  X  W present user, friendship and distance between users. Keyword set t i  X  T belongs to user v i . We define the top-k problem as bellow.
 Definition 1 (UCK Query(User Closeness aware top-k Keyword query)). AUCKQuery Q =( v,k,t 1 ,t 2 ,... ) is asked by user v and consti-tuted by keywords { t 1 ,t 2 ,... } to retrieve other users with highest top-k ranking score which is monotonic increasing with the user closeness and keywords score. v is called the owner of Q . User closeness scores are measured by a function monotonic decreasing with the shortest distance between users over the graph. In this paper, we use the following ranking function as an example. User closeness score for u is determined by reciprocal of its shortest distance from query owner v . Two parts of the score are normalized and weighted by parameters. The basic idea of TDK-Index is to integra te keyword and user closeness indexes together and provide a standard sorted and random access interface for TA [5] alike framework. Fig.1 gives a overview of the TDK-Index. The TDK-index is a tree structure and each tree node are constituted by several vertexes of original social graph. Offline calculated distance matrix and other useful information are stored at each node to speed up shortest di stance calculation of vertexes. And we have two type of keyword indexes, a single global index located at the root and multiple local indexes located at some selected nodes. Now, We will discuss why and how to build the index structure in detail.
To solve the top-k query problem in a TA [5] alike framework, we need to pro-vide sorted and random access for keywor ds rank lists and user closeness rank list. For keywords, sorted and random a ccess are available by building keyword inverted lists and forward lists. The challenges are how to provide efficient ran-dom and sorted access to user closeness. One observation of sorted access to a query owner X  X  closeness rank list is that we visit the nearest(the nearest one has the highest user closeness score) unv isited candidate one by one, so we do not need to prepare a completed rank list. Actually we can do it in a lazy manner by maintaining a priority queue. When a sorted access is needed, we pop out the nearest candidate, push its unvisited neighbors and update distance if needed. For the random access, we adopt a similar technique as TEDI [11] to do tree decomposition on social graph and transform shortest distance calculation from graph to tree.
 3.1 Tree Decomposition Based Random Access of Relationship In graph theory, tree decomposition is a methodology of mapping a graph into a tree to speed up problems on the original graph.
 Definition 2 (Tree Decomposition 1 ). Given graph G =( V,E ) ,atreede-composition is a pair ( X,T ) where X = X 1 ,...,X n is a family of subsets of V , and T is a tree whose nodes are subsets X i , satisfying the following properties: 1. i X i = V . 2. For every edge ( u, v )  X  E ,itexistsatleastone X i satisfying u  X  X i and 3. If X i and X j both contain vertex u , then all nodes X k between X i and X j According to the definition, one graph may be mapped into multiple different trees. A trivial mapping is a tree with only one node which contains all the vertexes of original graph. However, we ar e interested in better trees whose nodes contain fewer vertexes. The vertex card inality of a node is called the node size here. We try to generate some trees with the maximal node size smaller. Finding a best tree decomposition with smallest maximal node size is NP-hard problem. Inspired by TEDI [11], we adopt a similar technique to do tree decomposition on our social network graph and transform the graph into a tree like structure. The methodology deletes vertexes with smallest degree one by one until degree of the vertexes left is larger than parameter d or an empty graph remains. Different from TEDI [11], we also stop if there are less or equal than d + 1 vertexes remaining. when a vertex is deleted, the vertex and its neighbors are united to be a node. We push the node into a stack and new edges are added to make its neighbors left to be clique. After the del eting process, tree structure is built by vertexes left and nodes in the stack. We hang every node to a as low as possible place satisfying the parent node should contain all the users in the child node except the deleted user when the child node is generated in previous process. Fig.2 shows the process of our running example with parameter d =2.Atthe beginning, vertex 1 is deleted because its degree is the smallest. We unite vertex 1 and its neighbors vertex 4 and 5 to be a node and push it into the stack. The same operations are carried out when d eleting vertex 2 and 3. Once a vertex is deleted, we need to exam if its neighbors are clique. If not, we need to add necessary edges. In our example, when vertex 3 is deleted, new edge between 5 and 6 is created. When only d + 1 = 3 vertexes are left, we unit vertex 4, 5 and 6 into a node. Next, we will build the tree structure in a reversed order. Last generated node will be insert into the tree first. When node A with vertex 4, 5 and 6 is inserted into an empty tree, it becomes the root. Next one is node B, the lowest possible location is the child of A satisfying that node A contains all vertexes in node B except vertex 3 wh ich is the deleted vertex when node B is generated. After every nodes are i nserted one by one, we construct a tree composition of original graph. Follow the method, every node size is smaller than or equal to d + 1 except the root, and the root size can be controlled by adjust parameter d . The good feature is very helpful to reduce shortest distance calculation complexities.
Every vertex may exist in serval nodes, we call the highest node is the vertex X  X  node or the vertex is the node X  X  owner. In our running example, vertex 3 X  X  node is B. With the structure, shortest distance of two vertexes in original graph can be solved by a bottom up algorithm. The algorithm starts from the two vertexes X  node to their common ancesto r. The connection of two nodes of the tree is constituted by the same vertexes they share and there are at most d shared vertexes. The shortest path is formed by choosing the best shared vertex to step up until the common ancestor and the steps is at most equal to the height of the tree h . So, shortest path calculation complexity is O ( d 2  X  h ). Proof of the algorithm correctness can be found in TEDI [11] which realizes very efficient shortest hop distance calculation on the structure. In our problem, shortest distance is not hop counts but weights sum along the path.

With the tree decomposition based inde x, random access of relationship close-ness measured by shortest distance is well solved. Combined with inverted lists, forward lists and priority queue techniques, We have already make the top-k problem applicable for a TA [5] alike framework. 3.2 Hierarchical Keyword Index When doing TA [5] algorithm over multiple rank lists, candidates with high score at one list may not able to rank into final top-k because they may rank very low at other lists. In our problem, rela tionship closeness score is decreasing with shortest distance. One observation is that many candidates far away from the query owner may get very high textual proximity score, but fail to rank into final top-k according to our ranking function. If we directly use previously discussed techniques to answer the top-k queries, lots of such useless candidates will be visited. Actually, we can reduce the useless visits by filtering candidates far from query owner. Our method is to create a hierarchical keyword index structure instead of a single global index.
 Theorem 1. Based on our tree decomposition method, the social graph has been transformed into a tree structure and every directly connected vertexes of v is guaranteed to be contained by the subtree whose root is v  X  X  node.
 Suppose N v  X  X  owner is v and there is a u which is a neighbor of v , then there must be a node N containing both u and v according to tree decomposition definition property 2. There must be a connected path from N and N v and every node along the path contains v according to property 3. If N does not belong to N v  X  X  subtree, there must be some other node N along the path with a higher position than N v .Thatistosay N v can not be v  X  X  node. The theorem is proved.

Every vertex X  X  neighbors are contain ed by its node X  X  subtree, and if we build some local keyword indexes on some selected nodes, we may do keyword search on some smaller index which locates next to the query owner X  X  node to filter candidates far away. It is not wise to build local index on every node, because the results generated may not full fill top-k vacancies and the large space cost is incurred. Besides a global keyword index, our TDK-Index build an local keyword index every m un-indexed vertexes accumulate dfromleaftoroot.Andevery local index will cover all the keywords of the subtree. To reduce the storage cost, only the offset positions of the global index are stored in local indexes. We will use our running example to show our local index strategies. Given the tree structure of Fig.2(e) and parameter m = 4, we start from leaf node D and 3 un-indexed vertexes here. We move up to node B and accumulated un-indexed number reaches 4 equals to m , so we create a local index at node B containing all the keywords of vertexes 2 , 3 , 5 , 6. We continue the process until the root. The same operations are carried out from the other leaf C and finally we need to build local index on node B and A. Because A is the root already with global index, we do not need to create an local index again. Fig.3 builds global and local keyword indexes for our running example.
To sum up, based on tree decomposition, TDK-Index map original social network graph into tree structure and build both shortest path indexes and keyword indexes on the integrated system. To filter candidates far away from query owner, hierarchical local index structure is adopted. We will present Two-phase TA algorithm to efficiently answer top-k queries based on TDK-Index and discuss parameter optimizations in following section. For keyword search on social networks, many candidates far away from the query owner may get very high textual proximity score, but fail to rank into final top-k according to overall ranking function. Traditional TA [5] algorithm will visit candidates at every rank list from top to down. However, candidates far away from query owner actually should be skipped even if they have very high rank at textual proximity lists. It calls efficient algorithms to reduce the visits to candidates with low probability of top-k. 4.1 One Phase Solutions Before discussion of Two-phase TA algorithm, Let X  X  first take a look at how one phase TA [5] works. The algorithm make sorted access to rank lists from top to down and calculate candidates overall score by necessary random access. It terminates once enough results with score above threshold. We will use an example UCK query Q =(2 , 2 ,t 1 ,t 2 ) to show the process. The query owner user 2 is trying to find top-2 users(user 2 herself is excluded) with highest overall score. For simplicity, ranking function Score ( uc, t 1 ,t 2 )= score uc + score t 1 + score t 2 is used here, where uc is user closeness. As shown in Fig.4, user 3, 1 and 4 are visited in the first round by sorted access to uc, t 1 and t 1 rank lists separately. To calculate overall score for user 3, random accesses to rank lists of t 1 and t 1 are carried out. We calculate overall score for user 1 and 4 similarly. After the first round, the top-2 score are 2.1 and 1.5 and the calculations cost us 3 sorted accesses and 6 random accesses. A threshold is set to estimate the highest possible score of unseen candidates, and the algorithm can terminate once there are 2 candidates X  score larger or equal than the threshold. Sum of currently lowest sorted access score of ev ery rank list is used as the threshold in traditional TA [5]. As the threshold decreases to be 1.7 after the third round, the algorithm terminates with final top-2 user 6 and 3. The whole process costs us 9 sorted accesses and 16 random accesses.

A obvious shortcoming of traditional TA [5] is that it may visit a candidate multiple times by random access and sort ed access. In our example, there are 5 candidates with totally 14 occurrences in rank lists. However TA [5] visits them 25 times. BPA2 algorithm [1] keeps visited positions of every rank lists and guarantees every candidate occurrence o f rank lists is visited only once. And the algorithm use the sum of highest possible unseen score of every rank list as the threshold which is much tighter than TA [5]. We use the same query example to show its calculation process in Fig.5. The same as TA [5], sorted and random access to user 3, 1 and 4 are carried out in first round. After the second round, the top 4 positions of t 1 are all be visited by sorted access or random access, so the highest possible unseen score is equal to 0.3. Similarly, the highest possible unseen score for uc and t 2 are 0.1 and 0.2 respectively. The overall threshold reduces to 0.6 which is much narrower than TA [5] and the algorithm terminates immediately with totally sorted a ccess cost 5 and random access cost 9. 4.2 Two-Phase TA Algorithm It is obviously that user 1 and 4 have very high textual proximity score for t 1 and t 2 . Both TA [5] and BPA2 [1] pay visiting costs for them. Our Two-phase TA algorithm will help to reduce the extra costs. Two-phase TA is constituted by two phase. It generates local top-k candidates(with high probability of final top-k) and tight the threshold in phase 1 and enrich and verify the final result in phase 2. Algorithm.1 shows the pseudo code of Two-phase TA algorithm. First, we need to determine a proper local index to run phase 1. Here we assume local index of the nearest ancestor of the quer y owner is selected and we will discuss local index selection later. In this pap er, Very similar to BPA2 [1], we gener-ate local top-k over the local rank lists. One difference is that we maintain 2 thresholds: one is called local threshold and the other is global threshold. They are calculated by the sum of highest possible unseen score of every local rank list and global rank list respectively. If local top-k with score higher than global threshold, the algorithm terminates. If local top-k with score higher than local threshold, the algorithm enters phase 2. Phase 2 is carried out over global rank lists and the process is similar to BPA2 [1].
 Algorithm 1. Two-phase TA Algorithm
Fig.6 shows how the algorithm works for the same example query Q = (2 , 2 ,t 1 ,t 2 ). Local index B is the nearest index for query owner user 2, so we run phase 1 over the local rank lists of keyword index B. User 3, 6 and 5 are visited by sorted and random access in first round and local threshold is reduces to 1.6 calculated by the sum of highest possible unseen score of local rank lists. However global threshold is 2.5 and early termination is not satisfied. The algo-rithm enters phase 2 running on the global keyword index A. After sorted access to user 1 and 4, the algorithm terminates without further random accesses as the threshold is reduced to be 1.8. Sort ed access and random access costs are 10 in total which is much smaller compared to 14 of BPA2 [1] and 25 of TA [5]. 4.3 Optimization and Parameters Selections Along the path from the query owner X  node to the root, there may be several nodes with local index. In previous section, we assume phase one of the Two-phase TA algorithm is carried out on the nearest ancestor with local index. Actually, we can do some optimizations to realize better performance under different application circumstances. Intuitively, in top-k query, the larger k is, the larger probability for candidate with far distance to be a final result. So, we need to pick up a proper local index with consideration of the result number needed. We use a parameter  X  to help with selection of local index. In algorithm phase one, we prepare local rank list for every keyword with length at least  X   X  k . That is to say we will keep on moving up along the path from the query owner X  X  node until rank list with enough length is reached or root node is reached. And rank lists for different keywords may come from different nodes along the path. Experimental study will show the benefits of our strategies. The graph data is generated from random sample from DBLP dataset with vertexes number range from 5 thousand to 20 thousand and keywords number associated with a vertex follow a normal distribution with mean 100 and stan-dard deviation 100. The keywords number of every test query follow a normal distribution with mean 2 and standard deviation 1. All the experiments are done on a computer with Four-core Intel(R) Xeon(R) CPU E5420 @ 2.50GHz and 16G memory.
 To evaluate our solution, some existi ng solutions are selected as baseline. Without an integrated index structure, we may follow a breadth-first alike man-ner to calculate scores from near to far until enough results are found. In Section 4 we have discussed our Two-phase TA X  X  advantages compared to one phase algo-rithms. One of the most efficient algorithm BPA2 [1] is used as another baseline.
As shown in Fig.7(a), Two-phase TA algorithm is always better than breadth-first algorithm without TDK-Index and One phase BPA2 [1] algorithm with TDK-Index. Fig.7(b) presents the efficient memory costs of our TDK-Index(Global keyword index is not included here, as it is not changed by our solution) and Parameter adjustments under different application circumstance is presented in Fig.7(c). Recent years, search problems in socia l networks have attracted a significant attentions [4,3,7,9,2]. Some researches of top-k query over social networks focus on building ranking score model by shared tags [9]. Some papers design and evaluate new ranking functions to incorporate different properties of social net-works [2,7]. Access control of keyword search over social networks is also attract research studies [4,3].

Although the problem of top-k keyword search with consideration of rela-tionship closeness measured by shortes t distance is not solved well by existing techniques, traditional keyword search and shortest distance calculation are well studied separately. Inverted list [14] is a state-of-the-art technique and there are many studies with a focus on inverted file compression [10,13] and man-agement [8]. For shortest distance probl ems, there is a survey paper [6] cover-ing different algorithms. Instead of online calculation, index based methods are studied [12,11]. Our study in this paper is different and our solution efficiently integrates top-k keyword search and shortest path index problems. In this paper, we have proposed TDK-Index and Two-phase algorithm which solve the big challenge of integrating social relationship and textual proximity for keyword search over social networks. E xperimental studies provide evidence of efficiency of our solutions and the flexibility to adopt different application circumstances.

