 In this paper, we propose Latent Dirichlet Allocation (LDA) [1] based document classification algorithm which does not require any labeled dataset. In our algorithm, we construct a topic model using LDA, assign one topic to one of the class labels, aggregate all the same class label topics into a single topic using the aggregation property of the Dirichlet distri-bution and then automatically assign a class label to each unlabeled document depending on its  X  X loseness X  to one of the aggregated topics.
 We present an extension to our algorithm based on the com-bination of Expectation-Maximization (EM) algorithm and a naive Bayes classifier. We show effectiveness of our algo-rithm on three real world datasets.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis Experimentation, Performance, Theory, Verification Expectation-Maximization, Text classification, Topic Mod-elling
With the advent of cheap and fast storage, there is an ex-plosive growth in the size and number of documents available in electronic format. Document classification is a technique which helps users to make effective use of the knowledge hidden in the documents.
 Traditional supervised document classifiers require a large c  X  2013 Association for Computing Machinery. ACM acknowledges that number of labeled dataset. Many times obtaining such a la-beled dataset is expensive. Nigam et al. [5] proposed semi-supervised approaches for document classification based on labeled and unlabeled datasets. McCallum and Nigam [4] proposed a semi-supervised approach based on labeling of keywords. In keyword based approaches, finding right set of keywords is a challenge.
 In this paper, we propose Latent Dirichlet Allocation (LDA) [1] based document classification algorithm. Our algorithm does not require any labeled dataset. In our algorithm, we construct a topic model using LDA, assign one topic to one ics into a single topic using the aggregation property of the Dirichlet distribution and then automatically assign a class label to each unlabeled document depending on its  X  X lose-ness X  to one of the aggregated topics.
 In our algorithm an expert assigns one topic to one of the class labels, also as LDA topics correlate with human as-signed class labels [6], our algorithm exerts a low cognitive load on the expert.
 Class labels predicted by our algorithm may be approxi-mate or noisy. In order to reduce the influence of such an approximate or noisily labeled documents, we present an extension to our algorithm based on the combination of the Expectation-Maximization (EM) algorithm and a naive Bayes classifier. We show effectiveness of our algorithm on three real world datasets.
 The paper is organized as follows: In section 2 we give brief introduction to LDA and the Dirichlet distribution. Section 3 contains our document classification algorithm. Section 4 demonstrates effectiveness of our algorithm with experi-ments on three real world datasets. We end our paper with conclusions and future prospects of our work in section 5.
LDA is an unsupervised generative probabilistic model for collections of discrete data such as text documents. In LDA, each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to the distribution [3]. Generative process of LDA can be described as follows: 1. for t = 1 ...T 2. for each document d  X  D Where, T is the number of topics,  X  t is the word probabil-ities for topic t ,  X  d is the topic probability distribution, z is topic assignment and w n d is word assignment for nth word position in document d respectively.  X  and  X  are topic and word Dirichlet priors respectively.
 Training an LDA model is estimation of the word-topic dis-tributions and the topic distributions for all documents in the corpus. Direct and exact estimation of these parame-ters is intractable. Collapsed Gibbs sampling is one of the techniques used for the parameter estimation of LDA [3]. Af-ter performing collapsed Gibbs sampling, probability of the word w assigned to the topic t (  X  w,t ) and the probability of the topic t assigned to document the d (  X  t,d ) is estimated as: Where  X  w,t is the count of the word w assigned to the topic t ,  X  t,d is the count of the topic t assigned to words in the document d and W is the vocabulary of the corpus.
 LDA discovers a set of topics present in the documents and gives probabilities of observing each word in each topic. Most prominent words in a topic frequently co-occur with each other in the documents so one can infer context of the words in a topic. Using the word probabilities one can interpret meaning of topics and find major themes in the documents. The topic probabilities of a document provide its explicit representation and these probabilities can be em-bedded in more complex model.
The Dirichlet distribution is defined as: (i.e. 0 &lt;  X  t &lt; 1 and P T t =1  X  t = 1) and  X  = (  X  is a set of parameters with  X  t &gt; 0. So, Aggregation property of the Dirichlet distribution: The Dirichlet distribution has a fractal like aggregation property [2]. It is defined as the aggregation of any subset of Dirichlet distribution variable yields a Dirichlet distribu-tion, with corresponding aggregation of the parameters. If { A 1 ,A 2 ,...,A r } is a partition of { 1 , 2 ,...,T } then,
In this section, we propose our document classification algorithm based on LDA (ClassifyLDA) and an extension of the algorithm based on the combination of EM algorithm and a naive Bayes classifier (ClassifyLDA-EM).
Our algorithm is based on generative property of LDA and the aggregation property of the Dirichlet distribution. Let us assume, we want to classify each document to one of the class labels from C = { 1 , 2 ,...,m } . Using Collapsed Gibbs sampling for LDA, Z = { z 1 ,z 2 ,...,z T } topics are learnt on the document corpus D . Now an expert will assign a class label, i  X  C to each topic z t  X  Z based on its most prominent words. Create Z 0 = S m i =1 Z i , the partition of Z such that Z i = { z t | z t  X  Z and class label of z t is i } . If for a document d in the corpus D ,  X  d = (  X  aggregation property of the Dirichlet distribution define  X  as: Initialize  X  0 t and  X  0 d using following equations: Using Collapsed Gibbs sampling for LDA, update  X  0 t and  X  A class label c  X  C is assigned to document d  X  D such that: c = arg max Algorithm 1 describes our for document classification algo-rithm .

Algorithm 1: ClassifyLDA 1 begin 2 Use LDA to learn Z = { z 1 ,...,z T } topics on D ; 3 Compute  X  t and  X  d using equations 1 ; 4 Expert will assign a class label, i  X  C to each 5 Create a partition Z 0 = S m i =1 Z i such that: 6 Initialize  X  0 t and  X  0 d using equations 6 and 7; 7 Update  X  0 t and  X  0 d using Collapsed Gibbs 8 for d  X  D do 9 Infer  X  0 Z i ,d ;  X  i  X  C using equation 7 ; 10 d c = arg max 11 end 12 end
In ClassifyLDA-EM algorithm, we build a classifier us-ing the combination of EM and a naive Bayes classifier. In this algorithm we use EM iterations along with the relation between word co-occurrence knowledge and class labels to improve the parameters of a naive Bayes classifier. Initially, we label all the unlabeled documents in the cor-pus using ClassifyLDA algorithm described in algorithm 1. Then, we build a naive Bayes classifier using these labeled documents and estimate class probabilities for each docu-ment. Using these estimated class probabilities we reassign a class label to each document and rebuild a new naive Bayes classifier.
 We iterate this process of reassigning class labels to the doc-uments and rebuilding a naive Bayes classifier until it con-verges to a stable classifier. We say a classier is stable when the change in log likelihood of the parameters of the classier is below a threshold. ClassifyLDA-EM can be described as:
We determine the effectiveness of our algorithm in relation to semi-supervised text classification algorithm proposed in [5] (NB-EM). We report the minimum number of labeled documents at which the performance of ClassifyLDA-EM and NB-EM are almost similar. We evaluate the effectiveness of ClassifyLDA and ClassifyLDA-EM on following three real world text classi-fication datasets. 1. 20Newsgroup: This dataset contains messages across twenty newsgroups. In our experiments, we use bydate ver-sion of the 20Newsgroup dataset 1 . This version contains separate train and test datasets of 20 newsgroups which are grouped into 6 major categories. We selected 4 major cat-egories: comp, politics, rec, and religion. Following are the newsgroups in each selected category. 1. comp: comp.graphics, comp.os.ms-windows.misc, 2. politics: talk.politics.misc, talk.politics.guns, 3. rec: rec.autos, rec.motorcycles, rec.sport.baseball, 4. religion: talk.religion.misc, alt.atheism, We experimented with all possible combinations of these ma-jor categories. 2. SRAA: Simulated/Real/Aviation/Auto UseNet data 2 : This dataset contains 73,218 UseNet articles from http://qwone.com/~jason/20Newsgroups/ http://people.cs.umass.edu/~mccallum/data.html four discussion groups, for simulated auto racing (sim auto), simulated aviation (sim aviation), real autos (real auto), real aviation (real aviation). Following are the three clas-sification tasks associated with this dataset. 1. sim auto vs sim aviation vs real auto vs real aviation 2. auto (sim auto + real auto) vs aviation (sim aviation 3. simulated (sim auto + sim aviation) vs real (real auto 3. WebKB 3 : This dataset contains 4199 university web-pages. The task is to classify the webpages as student, course, faculty or project .
 We randomly split SRAA and WebKB datasets such that 80% is used as training data and remaining 20% is used as test data.
We did preprocessing on the dataset by removing headers and stopwords. We evaluated effectiveness of our algorithm by computing the Macro F-measure ( F 1 ).
 For classification tasks of 20Newsgroup related dataset we choose number of topics (T) equal to two times number of classes. For SRAA dataset we learnt 10 topics on the com-classification tasks. For WebKB dataset we learnt 10 topics. The Dirichlet parameter  X  was chosen to be 0.01 and  X  was 50/T. We used Mallet 4 to run LDA on the documents. In NB-EM algorithm, we do 10 trails per number of labeled documents and report average Macro F1. Table 1 shows experimental results. We can observe that, ClassifyLDA-EM algorithm can achieve almost similar per-formance in relation to NB-EM with significant reduction in labeling efforts and for most of the datasets performance of our algorithm is above 0.9. In table 1, we can also observe improvement in the performance of ClassifyLDA-EM over ClassifyLDA which proves that the combination of EM and a naive Bayes classifier reduces the influence of approximate or noisily labeled documents.
 We observed that, performance of NB-EM depends on initial labeled documents.
Table 2 shows topics learnt and classification of the top-ics on  X  X olitics vs rec X  dataset. With the help of most prominent words in a topic an expert can assign a class label to the topic. Due to generative property of LDA, topics labeled with class label  X  X olitics X  will generate pol-itics related documents with high probability. Now we will Z = { z 0 ,z 1 ,z 2 ,z 3 } . Using the aggregation property of the Dirichlet distribution, all the same class label topics are ag-gregated into a single topic. Now, we can use algorithm 1 and the combination of EM algorithm and a naive Bayes classifier to estimate class labels for unseen documents. We also explored how well a topic correlates with the class http://www.cs.cmu.edu/~webkb/ http://mallet.cs.umass.edu/
Data set ClassifyLDA 20Newsgroup comp vs politics 0.960 0.976 4 0.974 20 comp vs rec 0.903 0.949 4 0.947 25 comp vs religion 0.953 0.979 4 0.981 25 politics vs rec 0.957 0.980 4 0.978 70 politics vs religion 0.872 0.929 4 0.927 65 rec vs religion 0.959 0.988 4 0.986 105 comp vs politics vs rec 0.932 0.960 6 0.960 125 comp vs politics vs religion 0.896 0.932 6 0.929 115 comp vs rec vs religion 0.936 0.965 6 0.964 105 politics vs rec vs religion 0.889 0.937 6 0.935 190 comp vs politics vs rec vs religion 0.891 0.936 8 0.934 480
SRAA sim auto vs sim aviation vs real auto vs real aviation auto vs aviation 0.908 0.929 10 0.927 300 simulated vs real 0.917 0.933 10 0.931 5250
WebKB student vs course vs faculty vs project 0.711 0.719 10 0.730 1150
ID Most prominent words in the topic Class 0 gun armenian turkish didn guns killed file 1 israel government president jews american 2 team game play season hockey players win 3 car bike front road buy drive speed engine rec
Table 2: Topic labeling on the politics vs rec dataset assigned to it. We represented each class as probability dis-tribution over words. We computed P ( w | c j ), the probability of the word w belonging to the class c j as the fraction of the number of times word w appears among all the words in documents of class c j . Then we computed Kullback-Leibler (K-L) divergence between each class and a topic. Table 3 shows K-L divergence between each class and a topic for the same dataset. We can observe that the K-L divergence is least for the class assigned to a topic by the expert. Table 3: K-L Divergence between each class and a topic
In this paper, we propose a novel, inexpensive document classification algorithm which requires minimal supervision. Our algorithm is based on the generative property of LDA and the aggregation property of the Dirichlet distribution. We also show effectiveness of our algorithm with the help of experiments. Our approach is specifically suited for domains where establishing a mapping from topics to class labels is easier than acquiring a labeled collection of documents. In future we would like to carry out experiments on datasets like Reuters-21578 and a more detailed investigation on how the topic-class mapping influences the classification effec-tiveness. We will also explore tools that help experts arrive at the most appropriate topic-class mapping. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] B. A. Frigyik, A. Kapila, and M. R. Gupta.
 [3] T. L. Griffiths and M. Steyvers. Finding scientific [4] A. Mccallum and K. Nigam. Text classification by [5] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. [6] A. Chanen and J. Patrick. Measuring Correlation
