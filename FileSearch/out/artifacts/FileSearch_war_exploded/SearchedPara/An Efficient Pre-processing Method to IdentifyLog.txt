 Most prior works on information extraction in digital documents have focused on extracting information from texts. H owever, often, the most important in-formation being reported in a digital document is presented in multiple com-ponents (e.g., tables, equations, figures , algorithms, etc.) a nd special sections (e.g., abstract and conclusion). These components are widely used in web pages, scientific documents, financial reports, etc. Researchers always adopt these com-ponents to introduce a new theory or an important algorithm, to display the latest experimental results, the valuable statistical information, or a summary of their exciting findings in a condensed fashion. Other researchers, for example, who are conducting an empirical study in the same topic, can quickly obtain valuable insights via examining these components.

Although these components are ubiquitous with a history that pre-dates that of sentential texts, they are far from been fully analyzed and utilized, especially the tables and equations. Along with the explosive development of the digital li-brary and internet, these components have became a valuable information source for information seeking and data analysis. Based on the increasing demands to unlock the information inside, more applications appear, e.g., the table search [12] or equation understanding. Considering the life cycles of these components, they are generated and som etimes modified to fit new regulations or to display in new devices. With the content repur posing, each component can be viewed and shared by more people. If the data r eported in these components can be extracted and stored in a database, the data can be queried and integrated with other data using database management systems. In order to prepare the data source for further applications, accurately detecting the boundaries of these com-ponents plays a crucial role for the later information extraction. While there has been some research on component boundary detection, most them only focus on a single component each time. Although approaches on component analysis are diverse, they share two analyzing steps: boundary detection and structure decomposition. For the further content storage and sharing (e.g., the table data extraction or the equation search), locating the component boundary is the first and crucial step.

In this paper, we propose a novel, efficient and universal method to detect the boundaries of all the listed specific components. Comparing with the previous works, our method has the following contributions: 1) Although document component boundary detection is not a new topic, most previous works only focus on an individual component and it is difficult to apply their methods/algorithms to another different component without much modification. Our method can detect the boundary of a batch of important but different components from a document in one simple iteration. In this paper we focus on the table and equation boundary detection. However, other components e.g., figures and algorithms can also be detected simultaneously. 2) In general, the component boundary detection problem can be transformed into the problem of identifying the lines of the target components, which consti-tute the component boundaries. With observation of multiple components with diverse layouts from different documents, we notice an interesting phenomenon according to the nature of document layouts, almost all the lines within the com-ponent boundaries share an important property: majority lines belonging to the specific component components are sparse in terms of the text density. Based on this interesting property, most document logical components can be identified together with different features. Existing filter-out based component discovering methods identify the target lines from th e entire set of document lines according to certain rules, which in turn results in low recall . However, for applications such as table search [12], recall is more important than precision because once the true target lines are removed, it is difficult to retrieve them back. Fortunately, the false positive rate can be easily lowered in later structure decomposition step. In this paper, we propose a novel but effective method to quickly locate the component boundary by taking advantage of the aforementioned property. We also propose an exclusive based method for identifying component lines, which generates high recall and saves substantial effort to analyze the noisy lines. 3)The PDF format is commonly used for the exchange of documents on the Web and there is a growing need to understand and extract or repurpose data held in PDF documents. Many systems for processing PDF files use algorithms designed for scanned documents, which analyze a page based on its bitmap representation. We believe this approach to be inefficient. Not only does the rasterization step cost processing time, but information is also lost and errors can be introduced. Although we can conv ert PDF documents into other media (e.g., html, text), then analyze the new documents using the existing methods, identifying various component logical c omponents as well as the contents [2] is still a challenging problem. The major difficulties come from the following aspects: most PDF documents are untagged and do not have basic high-level document logical structural information, which makes the reuse or modification of the documents difficult. In addition, almost all the PDF text extraction tools have the text sequence errors. If converting PDF documents into other media (e.g., image), new noises can be generated by some necessary tools (e.g., OCR). The rest of the paper is organized as follows. Section 2 reviews several relevant studies in component boundary detection area and the applied machine learning methods in this field. Section 3 introduces the sparse-line property we proposed, describes in detail the sparse line detection and the noise line removing using the conditional random field and support vector machine (SVM) techniques. We also elaborate the label types and the feature sets. Section 4 explains how to locate the component boundary based on the labeled lines as well as other important heuristic rules, e.g., keywords. The deta iled experimental results are displayed in Section 5. We conclude our paper with plans for future work in Section 6. Chao et al. [2] reported their work on extracting the layout and content from PDF documents. Hadjar et al. developed a tool for extracting the structures from PDF documents. They believe that, to discover the logical components of a document, all/most of the page objects listed by PDF document content stream need to be analyzed (e.g., text objects, image objects, path objects). However, the object overlapping problem happens frequently. If all the objects are analyzed, more effort needs to be spe nt to firstly segment them from each other. In addition, even we identified th ese objects, they are still too high level to fulfill many special goals, e.g., detecting the tables, figures, mathematical formulas, footnotes, references, etc. I nstead of converting the PDF documents into other media types (e.g., image or HTML) and then applying the existing techniques, we process PDF documents directly from the text level.

In the past two decades, a good number of researches have been done to discover the document layout by conve rting the PDFs to image files. However, the image analysis step can introduce noise (e.g., some text may not be rec-ognized or some images may not be correctly recognized). In addition,because of the limited information in the bitmap images, most of them only work on some specific document types with minimal ob ject overlap: e.g., business letters, technical journals, and newspapers. Some researchers combine the traditional layout analysis on images with low-lev el content extracted from the PDF file. Even if the version 6 of PDF allows a user to create a file containing structure information, most of them do not contain such information. In this paper, we propose a method that relies solely on the PDF extracted content, not longer requiring the conversion to any other document medium and apply any further processing methods. Although the PDF was designed as a powerful fixed-layout format that allows documents created within any desktop publishing package to be viewed in the original typeset design, regardless of the systems where it is being displayed, there were trends to recover the document structural layouts and to identify logical components from PDFs. To successfully get the data from an important component, detecting the component boundary is a crucial step. Based on the observation, we notice that different lines in a same document page have different widths, text densities, and the sizes of the internal spaces between words. A document page contains at least one column. Many journals/conferences require two (e.g., ACM and IEEE templates) or three even four columns. Some document lines have the same length as the width of the regular document column (e.g., most lines in this paper), some others are much longer (e.g., cross over multiple document columns) or shorter. Based o n the size of the internal spaces within a document line, the majority of docum ent lines contain normal space sizes between two adjacent words while some lines have large spaces.

Sparse line is originally proposed in [14]. Because we extend the table compo-nent to all the document logical components in this paper, we modify the sparse line definition as follow. A document line is a sparse line if either condition is satisfied: 1) The maximum space gap betw een a pair of consecutive words within the line is larger than a threshold sg ; 2) The length of the line is shorter than a threshold ll . Different  X  ll  X  may generate different sparse line labeling results. We define it as the half of the regular document column width.

Figure 1 shows a snapshot of a PDF document page as an example. We highlight the sparse lines in red rectan gles and the lines with specific keywords in blue rectangles. Apparently, the table/ equation/ figure body-content lines are labeled as sparse lines according to the definition. Four sparse lines are not located within the component boundary we pre-defined: one caption lines and three short lines that are the last line in a paragraph. We label them as sparse lines because they satisfy the second condition. Heading/footer lines may also belong to this category. Since such short-length lines also happen in some table rows with only one filled cell, we consider them as sparse lines to avoid missing out the potential table lines. Such noise sparse lines are very few because they usually only exist at the headings or the last line of a paragraph. In addition, the short length restriction also reduce the frequency. We can easily get rid of them based on the coordinate information later. Comparing with [14], which automatically identified the table boundary based on sparse lines analysis, this paper aims to identify multiple document components (tables, equations, figures, etc.) simultaneously. 3.1 Machine Learning Methods In this paper, we apply two machine learning methods  X  Support Vector Ma-chine (SVM) [1] and Conditional random fields (CRF) [11] on the component boundary detection Furthermore, we elaborate the feature selection, analyze the factor effects of different features, an d compare the performance of CRF/SVM approaches with our proposed rule-based method. Different from most CRF ap-plications, the unit of our problem is a document line, instead of a single word. Before classifying the document lines, w e have to construct the lines first. Our bottom-up line construction approach can be found in [14].

Overall, our features can be cla ssified into three categories: the orthographic features ,the lexical features, and the document layout features . Most related works treat the vocabulary as the simplest and most obvious feature set. Such features, named as orthographic features, define how these input data appear based on regular expressions as well as prefixes and suffixes. For the table bound-ary detection, layout features are the most important parts. However, for this paper, we found that the first two types are more crucial because of the nature of document components. The detailed features can be found in [14].
Different from the traditional component boundary detection works, we use an exclusive method to label all the potential target lines. Each document line will be examined and labeled with a predefined category. For the boundary detection of a specific document compon ent, we can easily identify the noisy document zones and remove them as early as possible. After labeling each line using machine learning methods, Figure 2 shows the distribution of the sparse lines in a document page. In order to detect multiple component boundaries in a batch way, we adopt a universal boundary detection method, which contains the follow four steps: 1) Locating one end of the boundary by detecting the pre-defined keywords. Although caption lines are not included in the boundary, the keywords are usually powerful indicators, which facilitates the boundary detection. 2) Deciding the other end of the boundary by judging the caption location. Although most caption lines are not included in the boundary area, the caption location (below/top) is crucial to find the other side of the boundary, especially when multiple components appear in a same page altogether. 3) Analyzing the collected sparse lin es between two ends; We focus on differ-ent aspects for different components. For example, tabular structure is the key feature of a table. Because of the inherited sequence error problem of PDF text extraction tools, the line sequence resorting work usually happens in this step. 4) Removing noisy sparse lines and retrieving the positive lines labeled as non-sparse lines that are filtered out in t he early stage, if needed. Usually the noisy sparse lines are located just beyond the component boundary. Using table boundary as an example, a typical positive line to be retrieved is the long table lines (e.g., long nesting table lines).
 Because of the limited space, we use table and equation boundary detection as two examples. We define the main table content rows as the table boundary, which does not include the table caption and the footnote. In order to facilitate the table starting boundary detection, we define a keyword list, which contains all the possible starting keywords of table captions, such as  X  X able, TABLE, Form, FORM, X  etc. Most tables have one of these keywords in the captions. If more than one tables are displayed together, the keyword is very useful to separate the tables from one another. Once we detect a line (not only the sparse line) starting with such a keyword, we treat it as a table caption candidate. Then we check all the nearby sparse lin es and merge them with the sparse area according to the vertical distances betw een adjacent lines. Tabular structure within the sparse area is the key feature to verify a table boundary.
Comparing with other components, the equation boundary always contains a set of sparse lines which satisfy some particular patterns: short lengths, disor-dered sequences, inconsistent font information, and single characters or words together with mathematic operation symbols. In addition, the most important difference between equation boundary and table boundary is that there is no obvious tabular structure within an equation.

In previous researches with imbalanced data, recall is usually lower than preci-sion for the true class. However, in the document component boundary detection problem, recall is more important than precision at least in the information re-trieval (IR) field, because collecting t he real target lines as more as possible is crucial to the overall boundary detection accuracy. The texts within the detected component boundary will be analyzed carefully in the later structure decompo-sition phase. Once we locate a component boundary, we zoom in the detected boundary and try to retrieve the missing lines (e.g, long table lines) that are labeled as non-sparse lines or to remove the false lines (e.g., surrounding noisy sparse lines) to improve the recall . In this section, we demonstrate the experimental results of evaluating the doc-ument component boundary detection with two machine learning methods Our experiments can be divided into four parts: the performance evaluation of differ-ent methods, different feature settings, different datasets, and different parameter settings. 5.1 Data Set Instead of analyzing document compon ents from a specific domain, we aim to collect components as much different variet ies as possible from digital libraries. We continue our experiments based on the same data sources in [14]: chemical scientific digital libraries (Royal Chemistry Society 1 ), Citeseer 2 , and archeology 3 in chemistry, computer science and archeology fields. The size of each PDF repository we collected exceeds 100 , 000, 10 , 000 and 8 , 000 respectively in terms of scientific papers. We extended the size of our training set to 450 randomly selected pages. Among these pages, we refer 150 pages from the chemistry field as the dataset H , 150 pages from the computer science field as the dataset S ,and 150 come from the archeology field as the dataset A . The total number of the lines in three datasets are 17035, 20468, and 13945 respectively. For every document line, we manually identify it with a defined label. In order to get an accurate and robust evaluation on the component boundary detection performance, we adopt a hold-out method by randomly dividing the dataset into five parts and in each round we train four of the five parts and tested on the remaining one part. The final overall performance comes from the combined five results. In our experiment, we use the java-impleme nted, first-order CRF implementation  X  Mallet  X  to train two versions of the CRF with binary features and the actual values. For SVM, we adopt SVM light [7].

Because this work can be viewed as an extension of our previous work in [14], the results of the line construction can be found there. In this paper, we mainly test the performance of line labeling for each specific component by considering the pre-defined keywords and rules. 5.2 Performance of Sparse Line Detection We perform a five-user study to evaluate the quality of the sparse line detec-tion. Each user checks the detected sp arse lines in 30 randomly selected PDF document pages in each dataset. The evaluation metrics are precision and recall , which are defined in [14]. The results are listed in Table 2.

There are two reasons for the potential errors: 1) some tables have long cells or very small spaces between the adjacen t table columns because of the crowd layout. 2) The mathematical equation boundary may include some surrounding noisy sparse lines. In order to include the correct lines in each component, we regulate thresholds by setting ll with a tolerate value and sp with a smaller value. The trade off is mislabeling some non-sparse lines as sparse lines. However, we have further steps to do the boundary structure analysis, including such non-sparse or noisy lines(low precision ) is not a big problem. Within the datasets H , A and S ,80 . 25% lines are labeled as non-sparse lines and can be easily removed as noise. 5.3 Performance of Noise Line Removal Different from [14], the noise lines here refer all the sparse lines that are not belong to the target component boundary, e.g., the last sparse line in Figure 2. The input of this step is all the detected sparse lines while the output should be the filtered sparse lines that are loc ated in the components. Because of the limited space, we use equation and table as examples.

Table 2 lists the noise removal results, measured in precision and recall . FP refers the beginning dataset  X  all lines in a page. RN S refers the non-sparse lines. RH refers the noisy heading lines. HF refers the noisy header and foot-note lines. CAP is the noisy caption lines, REF is the noisy reference lines, and PP represents the postprocessing step. Along with the noise removing, the size of the sparse line dataset decreases and the precision of the table/equation line labeling increase steadily. Non-sparse line removing and the postprocessing are two crucial steps for the table/equation boundary detection problem. The results on three datasets are consisten t without any remarkable difference. The precision value is improved from 10 . 48% to 61 . 47% after removing all noises. In addition, the further steps are much easier because of the dramatically reduced sparse line set. Although the results are still not satisfying, the remaining false positive table/equation lines scatter in the page and the large distance to the table caption is an important feature to identify the table lines.
 Table 2 also shows the recall curves with the same experimental conditions. The initial recall values are 100% because no line is removed. Along with each step, the recall is decreasi ng because few true table/equation lines are mislabeled and removed. Within three datasets, dat aset S has the worst recall value because most computer science documents do not follow the standard template strictly and some true table lines are mislabeled. 5.4 Table/Equation Boundary Detection Although the remaining sparse line set contains almost all the table lines and the equation lines, the accuracy is still not satisfying. the typical false positive table lines are the lines with short length. Such lines are usually located at the end of paragraphs, the last line of a table caption, or a short table footnote without special beginning symbol etc. Considering the distance features, most of the first type can be filtered out. For those missed true table lines, analyzing the location information of adjacent sparse line sections together with the table caption help us to retrieve them back. With these heuristical rules, the precision values is enhanced to 95 . 79% and the recall values is close to 98 . 61%. After we decide the table boundary, we treat all the remained of the sparse lines as the potential equation boundary. Based on the dataset in section 5.1, the recall of equation boundary detection is 95 . 25 and the precision is 87 . 85. Because we will launch the equation structure decomposition (see details in another paper) based on the detected equation boundary, we welcome high recall and the relatively low precision will be improved easily. In this paper, we propose a preprocessing method to detect the boundaries of a batch of different document components . With an interesting observation of the document lines, we simplify the document boundary detection problem into a line labeling problem by considering the nature of document lines. The whole methodcanbeviewedasaclassificationa nd filtering sequence. As the noisy lines are removed, we can easily detect the tar get lines to constitute the components. Combining different heuristic rules, ou r method is applicable to detect multiple document components.

