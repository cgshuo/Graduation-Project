 Users look for information that can suit their level of exper-tise, but it often takes a mammoth effort to trace such infor-mation. One has to sift through multiple pages to look for one that fits the appropriate technical background. In this paper, a query-independent ranking system is proposed for technical web pages. The pages returned by the system are sorted by their relative technical difficulty in either ascend-ing or descending order specified by the user. The technical difficulty of a document i.e. terms in sequence, is first com-puted by the combination of each individual term X  X  geom-etry in the low-dimensional latent semantic indexing (LSI) space, which can be visualized as a conceptual terrain. Then the pages are ranked based on the expected cost to get over the terrain. Results indicate that our terrain based method outperforms traditional readability measures.
 H.3.3 [ Information Search and Retrieval ]: Information filtering, Retrieval models, Search process, Selection process; H.3.7 [ Digital libraries ] Algorithms, Experimentation  X  The work is supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: CUHK4128/07 and CUHK413510). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies.  X  Au Yeung contributed to this paper while he was at the NTT Communication Science Laboratories, Kyoto, Japan. Conceptual Hop Model, LSI, Technical Expertise
Readers can find it difficult to comprehend technical arti-cles outside their domain of specialization because they are not acquainted with the jargon or technical concepts of that domain. A computer scientist looking for introductory in-formation about  X  X ever X  will probably want to visit a web page meant for general public. A medical research student would probably want to go to websites meant for medical researchers in order to know the current trends in phar-maceutical research. Search engines currently return a mix of introductory and advanced documents [8], which means that a person has to carefully read the web pages one by one in order to stumble upon the one that can fit her back-ground knowledge and technical understanding. We propose a ranking system that can sort web pages based on techni-cal difficulty for any domain. We evaluate the model on the medical domain.

Domain expertise relates to the knowledge of the subject of a particular domain [15]. In our work, technicality of each term is computed based on the observation in the LSI space. Then, we apply this formulation towards web page ranking where the  X  X onceptual Hop Model X  (CHM) follows term or-der in the web page and  X  X ops X  from one term to another in succession which incurs certain  X  X ost X . In the end, an ex-pected cost is computed. If the expected cost is high, the web page is more technical. To make text comprehensibility easy for the learner, the path that the learner has to face during reading process should be as easy as possible [12].
The main contributions of this work are: a new method to find the technicality of every term in the web page in the low dimensional latent semantic space and a new method which computes the  X  X ost X  involved when hopping from one term to another in the terrain.
Traditional readability formulae have been used exten-sively in determining the grade level of articles. Flesch-Kincaid [7], Dale-Chall[1] and Lexile TM [14] etc. are some of the commonly used readability measurement techniques. Wolfe et al. [16] describe the use of latent semantic analy-sis (LSA) in matching readers and texts. Textual coherence Table 1: A TDM built by selecting terms from Pu bMed and Yahoo! Health documents.
 is another related concept that uses statistical or natural l anguage processing techniques to determine the text com-prehensibility [5, 4].

In [8], the authors begin with the definition of the intro-ductory and advanced documents. They have used a su-pervised learning method to build a classifier (FAMCLASS) to predict web pages as introductory or advanced. In [2], the authors build a smoothed unigram model to predict the grade level of the text. The authors have stated that tra-ditional readability measures perform poorly on web pages because they are short, contain lots of noise (like irrelevant strings due to pre-processing), presence of tables, images etc.
Nakatani et al. [13], describe a method for re-ranking search results of a web search engine (Easiest-First Search) in descending order of their comprehensibility using the Japa-nese Wikipedia. The work of Yan et al. [17] has similar objective as in this paper. The authors state that document scope and document cohesion are important parameters in finding simple texts. The authors have used a controlled vocabulary thesaurus termed as Medical Subject Headings (MeSH). However, the applicability of such methods in the other domains will always be a concern as one requires sim-ilar kind of controlled vocabulary. In our model, CHM, dis-tance between consecutive terms is a measure of the  X  X on-ceptual cohesion X  between terms and  X  X ops X  between terms gauge the difficulty in conquering that bigram.
Latent Semantic Indexing [3] makes use of the Term Doc-ument Matrix (TDM) on which a matrix decomposition method  X  X ingular Value Decomposition X  (SVD) is applied. The most noticeable observation is that terms which are unique or central to a document remain close to the doc-ument vectors. Terms which are used frequently, e.g. the general English language words like  X  X esult X  ,  X  X uman X  ,  X  X ody X  etc., are usually far away from the highly technical document vectors. Only few technical terms move far away from their document vectors.

We construct a small document collection by selecting sen-tences from Yahoo! Health (document IDs beginning with alphabet Y) and PubMed (document IDs beginning with alphabet P) for illustrating our idea. These terms have been taken from sentences present in different documents from our real test collection, for example one such sentence Figure 1: A two-dimensional plot showing the po-si tion of terms (circular markers) and documents (plus shaped markers) in the latent semantic space. from PubMed corpus is - X  Mitotic recombination in patients with ichthyosis causes reversion of dominant mutation s in KRT10. X  , where the terms in bold have been considered in the TDM. The choice of terms in the TDM is based on the property of the corpus itself. For instance, PubMed arti-cles are more technical than Yahoo! Health, therefore more technical terms have been chosen from PubMed than Ya-hoo! Health. SVD is then applied on the TDM to get the term and the document co-ordinates in two dimensions. A similar small example can be found in [10]. Figure 1 is a re-construction of the original vector space after reducing the dimensions to two by applying SVD to the TDM in Table 1. From Figure 1 it is evident that PubMed and Yahoo! Health articles form two separate clusters in the latent space. Technical terms like  X  X ollicle X  ,  X  X ostrandial X  etc., are very close to their own document vectors. General terms like  X  X ody X  ,  X  X ound X  ,  X  X ransformed X  ,  X  X uman X  ,  X  X esult X  etc., can be seen far from their document vectors. Y 1 and P 1 de-scribe about the same theme  X  X cne X  , hence are close to each other. But P 1 being more technical, has many technical terms in close proximity. Technical terms are closer to P 1 than Y 1, suggesting P 1 is more technical than Y 1.
Similarity between a document and a term in the latent semantic space is inversely proportional to the distance be-tween them [9]. Based on the concept of similarity,  X  X echni-cality X  is the amount of technical tinge a term gives to the document and is based on the closeness to the document vector in the low dimensional latent semantic space.
Mathematically, the technicality F d t of a term t in docu-ment d can be written as: where, r d t is the semantic distance (Euclidean) between the term and document vectors in the latent semantic space.  X  is a small constant added to avoid the case when r d t = 0. idf t is the Inverse Document Frequency of term t .

Consider the term  X  X ater X  in Figure 1. This is a general term but is located close to the document vector Y 5 in the latent space. In a large document collection, the global im-portance of this term will be less. Hence, idf t ensures that technicality of a general term remains low.  X  X ncogenes X  is Figure 2: Four important scenarios when transiting f rom one term to another in the terrain. shared among three documents. It is a technical term and hence the global importance will be high. It can be clearly seen from the TDM (Table 1) that Y 2 has more general terms than Y 4. Hence, Y 4 is more technical than Y 2 (and also Y 3). Therefore,  X  X ncogenes X  is more central to Y 4. Technical term like  X  X ollicle X  is closer to the document vec-tor and its idf ( follicle ) will be high, hence technicality will be high. General terms like  X  X ound X  ,  X  X uman X  etc. will have low technicality as their idf t will be low and their distance from the document vectors will be large in which they occur.
Hops in CHM occur by taking two consecutive terms at positions i and ( i + 1) i.e. a bi-gram in the document d . In doing so, some cost C d i,i +1 is incurred. Let s d i,i +1  X  X uclidean distance X  between terms in the bi-gram in the latent semantic space. Let n d i,i +1 be the number of  X  X ame bigram X  that have been encountered previously in the web page d until the current position ( i + 1).

The cost C d i,i +1 incurred when hopping from term i to term ( i + 1), which are characterized by technicalities F and F d i +1 respectively in document d , separated by a distance s where sgn ( ... ) is defined as the signum function that extracts the sign of a real number.
Figure 2 shows four important cases in hopping, and will be used as a graphical depiction in order to justify the ex-pressions in Equation 2. The distance s d i,i +1 has been kept constant in all the cases for better understanding. ( F d i + F d i +1 ) measures the combined technicality of the bi-gram. The limitation of this expression is that it fails to dis-tinguish cases in Figure 2( i ) and 2( ii ). Cohesion property in the conceptual terrain is handled by [ s d i,i +1 ] sgn ( s If the terms at positions i and ( i + 1) come from completely different concept spaces, difficulty in understanding them will be high as the reader encounters a  X  X ig conceptual leap X  in order to relate two completely different concepts. There might be cases where the Euclidean distance between the term pairs may be 0 &lt; s d i,i +1 &lt; 1 which will affect the mono-tonicity of the function. Thus the expression sgn ( s d i,i +1 is introduced. A document contains more number of non-domain concepts than domain specific concepts. Conse-quently, the cost values for such documents will be much skewed. In order to make the cost computation more sen-sitive to different technicalities, F d i +1 has been chosen as an exponent to compensate for the skewness. If a reader has already encountered the same bigram before; then the ease with which a reader will cross the same bigram in the next encounter will be relatively more than that in the previous encounters. Division by the number of same bigrams, n d i,i +1 seen so far ensures that there is a constant reduction in cost in every same encounter.
If the terrain is highly rugged or the document is too tech-nical; then the reader will face immense difficulties in com-prehending it. The ranking function considers the expected cost in order to complete the terrain. The expected cost E for document d can be written as: where C d i,i +1 is defined in Equation 2 and W d is the number of terms in document d .
Medical documents contain a high distribution of tech-nical concepts. We crawled web pages from two different sources, Yahoo! Health (2976 documents with 20109 unique terms) and PubMed (3087 documents with 145016 unique terms). Yahoo! Health articles are written for general pub-lic who have little or no understanding of medical concepts. PubMed documents are research papers written by health professionals.
Most of the procedures for experimentation were followed from [2]. Hence, major details about the baselines (the base-lines are Models: M3 and M4 in Table 2) described there are being omitted here. One change that is made relates to the number of words used, where in [2] the authors have used 100 token passages but we use full length passages and then normalize by the total number of words in the docu-ment. CHM X  X  semantic components are  X  X echnicality X  and  X  X onceptual cohesion X  with no syntactic component.
Flesch-Kincaid readability metric with both semantic and syntactic components is considered in our experiments. SM-OG [11] with its semantic and syntactic components is an-other baseline. To compute these measures  X  X tyle X  1 package was used. We add all the technicalities for a web page (com-puted in Section 3 . 1) and take its mean (Model M 7). Hence, term order is lost in this baseline. Model M 6 is another base-line where we remove the technicalities from the CHM and use the tf  X  idf values instead.
Normalized Cumulative Discounted Gain (nDCG) [6] was chosen as the performance measure, and is calculated as: where Z n is the normalization constant such that a perfect list gets a score of 1, r ( i ) denotes the rank label of the i document in the ranked list, n is length of the ranked list.  X  X epth-k X  pooling was adopted where  X  X op-k  X  documents from all models were accumulated and the rest were assumed irrelevant. In the X  X op-k  X  X ool where k = 200, the number of unique documents were 1249. Annotations from three peo-ple were used (mode of the scores was considered) who gave the relevance rating values. Two of them had basic knowl-edge of medical science while one was a medical student.
Relevance rating is in the range of  X 0 and 4 X .  X 0 X  means that the document is completely advanced at that position.  X 1 X  indicates that the document contains rich use of jargon or technical terms without explaining them. Relevance rat-ing of  X 2 X  means that the text requires some background knowledge of the topic because some technical terms are not explained.  X 3 X  means that the author has used technical terms and has defined them. Relevance rating of  X 4 X  means that the document is technically simple without the use of any jargon and the text can be easily comprehended by a beginner looking for technically introductory articles.
Table 2 shows the results when the documents are or-dered from technically introductory to advanced. CHM with stop words (Model M 2) has fared well as compared to the other models as it ranked technically introductory web pages at the top of the results containing no jargon. Model M 1 ranked some of PubMed X  X  case study articles which con-tained jargon. Other models ( M 3, M 4 and M 5) mainly tap readability features but CHM is able to capture technical difficulty i.e. technical concepts and the cohesion between concepts. These models ( M 3, M 4, M 5 and also M 6, M 7) ranked few medical news and quiz web pages at the top of the results, which indeed were not meant for a beginner who wanted to read technically simpler articles.
In this paper, we presented the problem of determining the technical difficulty of a web page in domain specific in-formation retrieval. Ranking documents based on techni-cal expertise can help in learning and also development of reader X  X  expertise [15]. Different readability formulae are not directly applicable even though their syntactic and se-mantic components are used. CHM is able to capture word level technical difficulty, concept level cohesion, and the in-tricacy experienced when hopping from one term to another in the concept terrain. In future, importance of hyperlinks would be studied, which act as an  X  X dded help X  given by the author to the reader. If a technical term has been hyper-linked to another web page, the author expects the reader to go to that web page, get acquainted with the definition of the term and come back to the original web page. The authors would like to thank Wei Gao, Bo Chen, Binyang Li and Haiqin Yang for their comments in the paper.
