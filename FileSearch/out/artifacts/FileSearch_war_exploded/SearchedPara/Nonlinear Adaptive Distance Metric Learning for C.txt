 A good distance metric is crucial for many data mining tasks. To learn a metric in the unsupervised setting, most metric learning algorithms project observed data to a low-dimensional manifold, where geometric relationships such as pairwise distances are preserved. It can be extended to the nonlinear case by applying the kernel trick, which embeds thedataintoafeaturespacebyspecifyingthekernelfunc-tion that computes the dot products between data points in the feature space. In this paper, we propose a novel un-supervised N onlinear A daptive M etric L earning algorithm, called NAML , which performs clustering and distance met-ric learning simultaneously. NAML first maps the data to a high-dimensional space through a kernel function; then ap-plies a linear projection to find a low-dimensional manifold where the separability of the data is maximized; and finally performs clustering in the low-dimensional space. The per-formance of NAML depends on the selection of the kernel function and the projection. We show that the joint ker-nel learning, dimensionality reduction, and clustering can be formulated as a trace maximization problem, which can be solved via an iterative procedure in the EM framework. Experimental results demonst rated the efficacy of the pro-posed algorithm.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms Clustering, distance metric, kernel, convex programming The first two authors contribute equally to the paper.
Good distance metrics are crucial to many areas in data mining, such as clustering, classification, regression, and semi-supervised learning. In distance metric learning, the goal is to achieve better compactness (reduced dimensional-ity) and separability (inter-cluster distance) on the data, in comparison with usual distance metrics, such as Euclidean distance. With a good distance metric, the construction of the learning models becomes easier and the accuracy of the learning models usually improves [34]. Based on the avail-ability of the constraint information (class label), distance metric learning algorithms fall into two categories: super-vised distance metric learning [26, 31, 32, 35] and unsuper-vised distance metric learning [4, 10, 17, 23, 29].
The performance of unsupervised learning algorithms, such as K -means is largely dependent on the pairwise similarity, which is commonly determined via a pre-specified distance metric. However, learning a good distance metric in the un-supervised setting is challenging due to the absence of any prior knowledge on the data. In this paper, we focus on the problem of unsupervised distance metric learning for clus-tering. Without any constraint or class label information, most unsupervised metric learning algorithms apply the pro-jection method such that geometric relationships, such as the pairwise distances are preserved in a low-dimensional manifold. Commonly used projection (dimensionality re-duction) methods include the Principle Component Analysis (PCA) [17], Locally Linear Embedding (LLE) [23], Lapla-cian Eigenmap [4], and ISOMAP [29]. Unsupervised learn-ing algorithms, such as K -means can then be applied in the dimensionality-reduced space, avoiding the curse of dimen-sionality .

In unsupervised learning, the goal is to find a collection of clusters in the data, which achieves the maximum inter-cluster separability. Traditionally, dimensionality reduction and clustering are applied in two separate steps. If distance metric learning (via dimensionality reduction) and cluster-ing can be performed together, the cluster separability in the data can be better maximized in the dimensionality-reduced space. In this paper, we propose a novel algorithm for non-linear adaptive distance metric learning, called NAML for si-multaneous distance metric learning and clustering. NAML first maps the data to a high-dimensional space through a kernel function; next applies a linear projection to find a low-dimensional manifold; and then perform clustering in the low-dimensional space. The performance of NAML de-pends on the selection of the kernel function and the projec-tion. The key idea of NAML is to integrate kernel learning, dimensionality reduction, and clustering in a joint frame-work so that the separability of the data is maximized in the low-dimensional space.

One aspect of NAML shares the same goal of supervised metric learning approaches, which try to adjust the distance among the instances to improve the separability of the data. For example, in [26, 32], the distance metric adjusts the ge-ometry of data, so that the distance between data points from the same class under the metric is small. The metric improves the separability of the data and enhances the per-formance of classifiers, such as K -Nearest-Neighbor (K-NN). In [9, 19, 37], a linear projection is performed to learn the distance metric for clustering, which assumes linear separa-bility of the data as in [26, 32]. However, many real-world applications may involve data with nonlinear and complex patterns. Kernel methods [24, 25] have been commonly used to deal with this problem. They work by embedding the in-put data into a high-dimensional feature space through the so-called kernel function . The key to the success of kernel methods is that the embedding into a feature space can be uniquely determined by specifying the kernel function that computes the dot products between data points in the fea-ture space. One of the central i ssues in kernel methods is the selection (learning) of a good kernel function. The problem of kernel learning has been an active area of recent research [2, 3, 13, 16, 18, 20, 21, 28, 33, 37]. The novel aspect of the proposed approach in comparison with these approaches is that NAML does not use any class label. In [30], generalized maximum margin clustering was proposed for simultaneous kernel learning and clustering, which was formulated as a semidefinite program (SDP). Besides its high computational cost of solving a SDP problem, the proposed formulation in [30] is restricted to the two-cluster problems only.
We show in this paper that the simultaneous kernel learn-ing, dimensionality reduction, and clustering in NAML can be formulated as a trace maximization problem, which can be solved by an iterative algorithm based on the EM frame-work. In particular, we show that both dimensionality re-duction and clustering can be solved by spectral anslysis, while the kernel learning can be formulated as a Quadrat-ically Constrained Quadratic Programming (QCQP) prob-lem, which can be solved more efficiently than SDP. We eval-uate the proposed algorithm using benchmark data sets, and the experimental results show the effectiveness of the pro-posed algorithm.

The remainder of the paper is organized as follows. We introduce the formulation of distance metric learning for the linear case in Section 2. The formulation is then extended to the nonlinear case in Section 3. Experimental results are presented in Section 4. This paper concludes with discussion and future work in Section 5.

Forconvenience,wepresentinTable1theimportantno-tations used in the rest of this paper.
In this section, we present the linear adaptive distance metric learning algorithm from [37], which will then be ex-tended to the nonlinear case in the next section.
Assume we are given a data set of zero mean, which consists of n samples { x i } n i =1 ,where x i  X  IR m .Denote X =[ x 1 ,x 2 ,  X  X  X  ,x n ] as the data matrix. Consider the pro-Table 1: Important notations used in the paper.
 jection of the data via a linear transformation W  X  R m  X  l Thus, each x i in the m -dimensional space is mapped to a vector  X  x i in the l -dimensional space as follows: It has been shown [8, 15] that for most high-dimensional data sets, almost all low dimensional projections are nearly normal. That is, for large m the projected data {  X  x i } n expected to be nearly normal. In this case, a good distance measure is the well-known Mahalanobis distance measure defined as follows: where  X  S is the covariance matrix defined as follows: is the class covariance matrix of the original data in X .For high-dimensional data, the estimation of the covariance ma-trix in Eq. (5) is often not reliable. Thus, the regularization technique [11] is applied to improve the estimation as fol-lows: where I m is the identity matrix of size m and  X &gt; 0isa regularization parameter.

Under this new distance measure, K -means clustering can be applied to assign the data {  X  x i } n i =1 into k disjoint clus-ters, { C 1 ,C 2 ,  X  X  X  ,C k } , which minimize the following Sum of Squared Error (SSE): where the Mahalanobis distance d M (  X  ,  X  )isdefinedasin Eq.(2),and  X  j is the mean of the j -th cluster C j .
As the summation of all pair-wise distances is a constant for a fixed W . The minimization of the SSE is equivalent to the maximization of Sum of Squared Intra-cluster Error (SSIE) defined as follows: where n j is the sample size of the j -th cluster C j ,  X  mean of the j -th cluster C j ,and X   X  is the global mean as defined above. SSIE can be expressed in a compact matrix form as follows. Let F  X  R n  X  k be the cluster indicator matrix defined as follows: The weighted cluster indicator matrix L =[ L 1 ,L 2 ,  X  X  X  is defined as [7, 9]: where the i -th column of L is given by With the weighted cluster indicator matrix L , the Sum of Squared Intra-cluster Erro r (SSIE) can be expressed as: The joint metric learning and clustering problem can be for-mulated as follows [37]: The optimization problem in Eq. (12) maximizes the inter-cluster distance under the Mahalanobis distance measure determined by the transformation W . Thus, it computes the distance metric and performs the clustering simultaneously.
In this section, we first review the basics of kernel meth-ods. We then present the nonlinear formulation of the adap-tive metric learning algorithm from the last section using the kernel trick.

Kernel methods [24, 25] work by mapping the data into a high-dimensional Hilbert space (feature space) F equipped with an inner product through a nonlinear mapping  X  K as: The nonlinear mapping can be implicitly specified by a sym-metric kernel function K , which computes the inner product of the images of each data pair in the feature space, that is where x i ,x j  X  IR m are training data points. A kernel func-tion K satisfies the finitely positive semidefinite property: for any x 1 ,  X  X  X  ,x n  X  IR m , the so-called kernel Gram matrix G , defined as G ij = K ( x i ,x j ), is symmetric and positive semidefinite.

The adaptive metric learnin g problem in Eq. (12) can be extended to the nonlinear case using the kernel trick. De-note  X  K ( X ) as the data matrix in the feature space. For a given kernel function K , the nonlinear adaptive metric learning problem can be formulated as the following trace maximization problem: max where W K is the transformation in the feature space. As-sume the data in the feature space has been centered, i.e., i =1  X  K ( x i ) = 0. Otherwise the kernel centering technique in [24] can be used. Thus the covariance matrix S K can be expressed as It follows from the Representer Theorem [24] that the opti-mal transformation W K is in the span of the images of the data points in the feature space. That is, for some matrix Q  X  IR n  X  l . Thus, the objective function for NAML can be rewritten as max Q,L trace L where G =  X  K ( X ) T  X  K ( X ) is the kernel matrix. Here we assume that the matrix GG +  X G is nonsingular, and we can use pseudo-inverse [14] to deal with the singular case.
In essence, NAML maps the data into a high-dimensional feature space through a nonlinear mapping, where linear projection and clustering are performed to maximize the cluster separability. The representation of the data in the feature space is determined by the nonlinear mapping, which can be implicitly specified by a kernel matrix. The perfor-mance of NAML is dependent on the choice of the kernel matrix. We propose to learn an appropriate kernel matrix for NAML in a joint framework, which leads to the following joint trace optimization problem: max where the kernel matrix G is restricted to be a convex com-bination of a given set of p kernel matrices, defined as
G  X  X  = The formulation in Eq. (16) performs kernel learning, dimen-sionality reduction, and clustering simultaneously. However, the joint optimization problem is highly nonlinear and diffi-cult to solve. One key observation is that if two of the three components L , G ,and Q are fixed, the optimization problem is easy to solve. This enables us to solve the problem in the EM framework, in which we update L , G ,and Q iteratively to find a local solution.
For a given matrix Q and a given kernel matrix G ,com-puting the optimal L in Eq. (16) is equivalent to solving the following trace maximization problem: where  X  G is defined as Recall that the entries of the i -th column of the weighted cluster indicator matrix L are either 0 or 1 / in Eq. (11). It follows that L T L = I k , i.e., the columns of L are orthonormal. We apply the spectral relaxation tech-nique [38] for the computation of the optimal L ,whichis given by the eigenvectors of  X  G as follows:
Theorem 3.1. (Ky Fan) Let  X  G be a symmetric matrix with eigenvalues  X  1  X   X  2  X  X  X  X  X  X   X  n , and the corresponding eigenvectors U =[ u 1 ,  X  X  X  ,u n ] .Then Moreover, the optimal L  X  is given by L  X  =[ u 1 ,  X  X  X  ,u for an arbitrary orthogonal matrix P  X  IR k  X  k . In the implementation, we choose the first k eigenvectors of  X  G corresponding the largest k eigenvalues, where k is the number of clusters. For simplicity, we set P to be the iden-tity matrix. Note that we compute the trace value of the matrix L T  X  GL in each iteration as the measure of conver-gence. When the relative change of the trace value is smaller than a pre-specified threshold , the iterative process stops.
For a given kernel matrix G and a given relaxed weighted cluster indicator matrix L , the trace maximization problem in Eq. (16) is equivalent to the maximization of the following objective function: where the matrices S K 1 and S K 2 are defined as The optimal Q  X  which maximizes F 1 ( G, Q ) in Eq. (20) is given by solving an eigenvalue problem associated with S K and S K 2 , as summarized below:
Theorem 3.2. Let S K and V =[ v 1 ,  X  X  X  ,v q ] be the matrix consisting of the first q eigenvectors of S + K 2 S K 1 corresponding to the largest q eigen-values, where q = rank ( S K 1 ) .Let Q  X   X  argmax Q F 1 ( G, Q ) . Then Q  X  = V .

Proof. Let G = U  X  U T be the Singular Value Decom-position (SVD) [14] of G ,where U  X  IR n  X  n is orthogonal and  X  = diag ( X  t , 0)  X  IR n  X  n is diagonal,  X  t  X  IR t  X  t agonal with positive diagonal entries, and t =rank( G ). Let U 1  X  IR n  X  t consist of the first t columns of U .Then Denote P =( X  2 t +  X   X  t )  X  1 2  X  t U T 1 L and let P = M  X  be the SVD of P ,where M and N are orthogonal and  X  P is diagonal with rank( X  P )=rank( S K 1 )= q .Let Z be a nonsingular matrix defined as where I n  X  t is the identity matrix of size n  X  t . It follows that where  X   X =( X  P ) 2  X  IR t  X  t is diagonal with the diagonal en-tries sorted in non-increasing order. It is clear that the op-timal Q  X  , which maximizes F 1 ( G, Q ), consists of the first q columns of Z . It can be verified that the first q columns of Z gives V =[ v 1 ,  X  X  X  ,v q ] which consists of the first q eigenvec-tors of S + K 2 S K 1 corresponding to the largest q eigenvalues. This completes the proof of the theorem.
 It is worth noting that the above trace maximization prob-lem is similar to the well-known linear discriminant analy-sis (LDA) [12]. However, they are fundamentally different, as S K 1 is different from the so-called between-class scatter matrix in LDA, due to the spectral relaxation in L .
Given Q and L ,theoptimal G can be computed by max-imizing F 1 ( G, Q ) in Eq. (20), where the kernel matrix G is restricted to be a convex combination of a set of pre-specified kernel matrices as in Eq. (17). One key observation for the computation of the optimal G is that the Q matrix in F 1 ( G, Q ) can be replaced by its optimal value Q  X  given in Theorem 3.2. This significantly simplifies the derivation. Denote It follows from Theorem 3.2 that
F  X  1 ( G )=trace(  X   X ) = trace ( S K 2 ) + S K 1 we have
F 1 ( G )=trace L T L  X  trace L T I + Thus, the optimal G  X  , which maximizes F  X  1 ( G ), is given by minimizing the following objective function: The minimization of F 2 ( G ) can be solved by gradient de-scent methods [6]. However, the computation of its gradient is expensive for each iteration. Following the recent work in [36], we can show that this minimization problem can be formulated as a Quadratically Constrained Quadratic Pro-gramming (QCQP) problem as follows:
Theorem 3.3. Given a set of p centered kernel matri-ces G 1 ,  X  X  X  ,G p as defined in Eq. (17), the minimization of F ( G ) defined above can be formulated as a QCQP problem as follows: where L =[ L 1 ,  X  X  X  ,L k ] and r i = trace ( G i ) . The coefficient  X  i for the i -th kernel G i is given by the dual variable corresponding to the i -th constraint in Eq. (29) di-vided by r i . Note that the general-purpose optimization software packages like MOSEK [1] also report the dual vari-ables by solving the dual problem.
Based on the discussion described above, we propose to develop an iterative algorithm, called NAML, for N onlinear A daptive M etric L earning. The pseudo-code of the NAML algorithm is given as below.
 Algorithm : NAML 1. Randomly choose one kernel matrix G from { K i } p i =1 2. Compute the initial cluster indicatior matrix L by 3. While relative change of the trace value  X  do 4. Update Q as in Section 3.2; 5. Update G as in Section 3.3; 6. Update L as in Section 3.1; 7. Compute the trace of L T  X  GL as in Eq. (19); 8. End 9. return G , L and Q ;
The final clustering result is obtained by applying K -means on the relaxed cluster indicator matrix L . The con-vergence of the NAML algorithm is guaranteed, as summa-rized in the following theorem:
Theorem 3.4. Algorithm NAML converges in a finite num-ber of steps.

Proof. The NAML algorithm updates G , Q ,and L it-eratively, by maximizing the same objective function, i.e., trace( L T  X  GL ). As the objective value is non-decreasing and is bounded from above by a finite number, the algorithm converges in a finite number of steps.
 In the implementation, we set =10  X  5 for checking the con-vergence. We observe from our experiments that the NAML algorithm typically converges within 3 to 4 iterations. The time complexity of the NAML algorithm is dominated by the QCQP problem in Theorem 3 .3, whose worst-case com-plexity is O ( pk 3 n 3 ).
In this subsection, we show the close connection between the proposed formulation and regularized spectral clustering [27]. From Eq. (26), the eigenvectors of G corresponding to the zero eigenvalues can be removed without affecting the value of the objective function. In the following discussions, we assume that G is nonsingular. It follows from Eq. (26) that Next, we consider a specific choice of G by setting G = L  X  1 where L is the Laplacian matrix [4] defined as follows. Let W X  R n  X  n be a symmetric similarity matrix, and D X  R n  X  n be a diagonal matrix with D ii = n j =1 W ij .The Laplacian L is defined as [4] The centering of the kernel matrix, which is equivalent to the data centering step in NAML, is not required when G = L  X  1 . It is based on the fact that the inverse of the Laplacian matrix is already centered, as summarized in the following proposition:
Proposition 3.1. Let L be the Laplacian matrix defined above. Then the inverse of Laplacian, denoted as L  X  1 , has zero row and column means. In other words, let e n be the vector of all ones of size n ,then e T n L  X  1 e n =0 .
Proof. Since L is symmetric and positive semidefinite, let L = U n  X  n U T n be SVD of L ,where U n is orthogonal and  X  n has nonnegative diagonal entries. It follows that proof of the proposition.
 We have assumed that L is nonsingular in the above deriva-tion. For singular L , we can use its pseudo-inverse and the result in the proposition above still holds. With this partic-ular choice of G , the objective function in Eq. (30) becomes: which corresponds to clustering with a regularized Laplacian matrix [27].
We now empirically evaluate the performance of the NAML algorithm in comparison with representative algorithms, and conduct a sensitivity study to evaluate its various compo-nents, such as the effect of the regularization parameter  X  , and the input kernels. These studies will help us better understand the proposed algorithm, and delineate new chal-lenges and research issues.
To evaluate the performance of NAML, we use the K -means algorithm as the baseline for comparison. We also compare the proposed algorithm with three representative unsupervised distance metric learning algorithms: Princi-ple Component Analysis (PCA), Local Linear Embedding (LLE), and Laplacian Eigenmap (Leigs). The Matlab im-plementations of these algorithms are obtained from cor-responding authors X  websites respectively. NAML is also implemented in the Matlab environment and we solve the QCQP problem using MOSEK [1]. All experiments were conducted on a PENTIUM IV 2.4G PC with 1.5GB RAM.
We test the distance metric learning algorithms and K -means on eight benchmark data sets. They are six UCI data sets [5]: iris, lymph, promoter, satimage, solar, wine, and two image data sets: AR03P 1 and ORL10P 2 .SinceMOSEK gives memory overflow error when the number of instances is large, for the satimage data set, we randomly sample 80 instances from each class. The information on the eight test data sets is summarized in Table 2.
 Table 2: Summary of the benchmark data sets.
 We compare the performance of the algorithms as follows. For each data set, we first run K -means and record its clus-tering results as a baseline. To make the results of different distance metric learning algorithms comparable, the cluster-ing result of K -means is used to construct C ,thesetof k initial centroids, for later experiments. Here k is the num-ber of clusters of the data. We apply PCA, LLE, and Leigs on each data set to learn distance metrics, which are used by K -means to learn clusters with the initial centroid set Their clustering results are recorded. We also run NAML with C and record its clustering results. This process is re-peated for 20 times with different initial centriods for each data set.
As we have the label information of all eight benchmark data sets, the clustering results are evaluated by comparing the obtained label of each data point with the ground truth. We use two standard measurements: the accuracy (ACC) and the normalized mutual information (MI) measures de-fined as below. Given a data point x i ,let c i and y i be the obtained cluster indicator and the true class label from the data, respectively. The accuracy measure is defined as: where http://rvl1.ecn.purdue.edu/  X  aleix/face DB.html. Data set is subsampled down to the size of 60  X  40 = 2400. http://www.uk.research.att.com/facedatabase.html. Data set is subsampled down to the size of 100  X  100 = 10000 In the equation above, n is the total number of data points and map( c ) is the permutation mapping function that maps each cluster indicator c to its equivalent class label. The mapping is found by using the Kuhn-Munkres algorithm [22].
Let C and Y be the set of cluster indicators and the set of class labels, respectively. The normalized mutual infor-mation is defined as:
MI( Y, C )= where p ( c i )(or p ( y j )) denotes the probability that an in-stance randomly selected from X belongs to cluster c i (or class y j ), p ( y i ,c j ) denotes the joint probability, and H( Y ) (or H( C )) is the entropy of Y (or C ).

Since each algorithm is tested for 20 times on each data set, we obtain 20 performance evaluations from ACC and MI measures, respectively. These performance evaluations are averaged and yield 2 final performance evaluations per algorithm on each data set. In the experiment, the reduced dimensionality ( l ) of PCA is selected to retain at least 95% information of the original data, and the reduced dimension-ality ( l ) of NAML is set to k . For each data set, we construct 10 RBF kernels for NAML.
Table 3 presents the accuracy (ACC) and normalized mu-tual information (MI) results on each data set. The results of NAML using 3 different  X  values (10  X  6 ,10  X  4 ,and10 are shown. NAML with  X  =10  X  2 performs the best (in-cluding the second best without a significant difference with the best) on 6 data sets in terms of accuracy. On the eight data sets, NAML with  X  =10  X  2 performs the best with an average accuracy of 0.747, which is followed by NAML with  X  =10  X  4 with an average accuracy of 0.743. LLE performs the third best and PCA is the fourth. Similar trends can also be observed in the MI results.

Experimental results also show that NAML does improve the performance of K -means on all eight data sets. For ex-ample, on AR03P data, NAML with  X  =10  X  2 improves its accuracy from 0.462 to 0.615, a 15.3% improvement. In our experiment, NAML converges in less than eight iterations and usually converges in 3-4 iterations.
In this subsection, we study the effects of various compo-nents of NAML. More specifically, we study the effect of the input kernels and the regularization parameter  X  .
In Table 4, we compare the performance of NAML with that of K -means using each of ten input kernels, respec-tively. Thus, we can obtain 10 clustering results from K -means with respect to 10 kernels, and further calculate max Ker , min Ker ,and ave Ker corresponding to the best, worst, and average performance. NAML uses the same 10 kernels as its input. We can observe from the table that in most cases, NAML performs much better than min Ker ( K -means using the worst kernel) and is comparable to max Ker ( K -means using the best kernel). This set of results has its ramifications for unsupervised metric learning. When we do not have the prior knowledge about the kernel quality, with the highest one, according to p -val &gt; 0.1.
ACC wine 0.413 0.972 0.972 0.954 0.954 0.961 0.964 NAML provides a way to learn from multiple input kernels and generate a metric, with which an unsupervised learning algorithm, like K -means, is more likely to perform as well as with the best input kernel. Hence, NAML has interest-ing applications in solving real-world clustering problems. For example, in a learning task, the pairwise instance re-lationship is calculated by a RBF kernel function, and the kernel parameter  X  is estimated by several domain experts. According to different understandings of the problem, dif-ferent experts can assign different values for  X  .Inthiscase, NAML X  X  capability of combining different perspectives from multiple experts and learning a good metric can be essential for unsupervised learning of nonlinear patterns. Figure 1 shows two sample cases that NAML converges to a good result, even though the quality of the initial kernel is low.
As discussed in Section 2, a regularization parameter  X  is introduced to improve the reliability of the estimation of the Figure 1: Clustering performance improves during the iterative process of NAML on AR03P data set:  X  =0 . 01 (left plot) and  X  = 100 (right plot). covariance matrix. The performance of NAML is dependent on the value of  X  . In the following experiment, we study the effect of this parameter on the clustering performance of NAML. We can observe from Table 3 that, in general, the regularization helps to imp rove the performance of NAML. achieved by K -mean on ten input kernels.

ACC promoter 0.689 0.698 0.698 0.585 0.734 0.585 0.686 In terms of accuracy, on four of eight data sets, the perfor-mance of NAML with  X  =10  X  2 is significantly better than that with a very small value of regularization (  X  =10  X  6 Similar improvement can be observed in MI results.
To obtain a better understanding of the effect of the reg-ularization parameter, we t ried a series of different  X  values ranging from 10  X  8 to 10 5 . The ACC and MI results using various  X  values are plotted in Figure 2. We can observe from the figure that, in general, NAML is not very sensitive to the value of  X  , except for the case when  X  is very large ( &gt; 10 2 )orverysmall( &lt; 10  X  6 ). The use of a  X  value in the range of [10  X  4 , 10 2 ] is helpful in most cases.
We can observe from Figure 2 that a small  X  value is less effective than a large  X  value. In most cases, a large  X  value does not significantly degrade the performance, which is not the case when  X  value is very small. Further studies show that when  X  is very small, the kernel weights learnt by NAML become close to each other; while when  X  is very large, the kernel weight vector becomes sparse (many of them are zero) and only the best kernels or those close to the best ones have non-zero weights. We show in Table 5 the weight of each input kernel, when using different  X  val-ues on AR03P data. The result suggests that when  X  is set to 0, the weights of all kernels are not zero; while when  X  is large, the weight vector becomes sparse and only a very small number of kernels has non-zero weights.

Recall that the optimal combination of kernels is obtained by maximizing trace L T G ( GG +  X G ) + GL . It is clear that when  X  approaches to 0, G ( GG +  X G ) + G approaches to a matrix, which contains 1 as the only nonzero eigenvalue. In this case, the optimization in NAML becomes degenerate. On the other hand, when  X  becomes large, the  X G term in ( GG +  X G ) + dominates. In this case, the optimization prob-lem is reduced to the maximization of trace L T GL ,which is essentially equivalent to the selection of a single kernel that maximizes trace L T KL . These explain the behavior of NAML for different  X  values in Table 5. However, we can also observe from Figure 2 and Table 5 that NAML per-forms the best when the value of  X  is neither too small nor too large. This is partly due to the complementary informa-tion that exists among the given collection of kernels, which may be exploited by NAML.
In this paper, we propose a nonlinear adaptive distance metric learning algorithm, called NAML. We show that the joint kernel learning, metric learning, and clustering can be formulated as a trace maximization problem, which can be solved iteratively in an EM framework. More specifically, we show that both dimensionality reduction and clustering can be solved by spectral analysis, while the kernel learning can be formulated as a Quadratically Constrained Quadratic Programming problem.

Experimental results on a collection of benchmark data sets demonstrate that NAML is effective in learning a good distance metric and improving the clustering performance. In general, approaches based on learning a convex combina-tion of kernels can be applied for heterogeneous data inte-gration from different data sources. We plan to apply the proposed algorithm for clustering from multiple biological data, e.g. , amino acid sequences, hydropathy profiles, and gene expression data. We reveal the close connection be-tween the proposed algorithm and regularized spectral clus-tering. The selection of a good Laplacian matrix, which is determined by several parameters such as the number of nearest neighbors, is one of the key issues in spectral cluster-ing. Another line of future work is to study how to combine a set of pre-specified Laplacian matrices to achieve better performance in spectral clustering.
 This research is sponsored in part by Arizona State Uni-versity and by the National Science Foundation Grant IIS-0612069.
 corresponds to ACC or MI values. [1] E. D. Andersen and K. D. Andersen. The MOSEK [2] A. Argyriou, R. Hauser, C. Micchelli, and M. Pontil. [3] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. [4] M. Belkin and P. Niyogi. Laplacian eigenmaps for [5] C. Blake and C. Merz. UCI repository of machine [6] S. Boyd and L. Vandenberghe. Convex Optimization . [7] I. S. Dhillon, Y. Guan, and B. Kulis. A unified view of [8] P. Diaconis and D. Freedman. Asymptotics of [9] C. Ding and T. Li. Adaptive dimension reduction [10] C. Domeniconi, D. Papadopoulos, D. Gunopulos, and [11] J. H. Friedman. Regularized discriminant analysis. [12] K. Fukunaga. Introduction to Statistical Pattern [13] G. Fung, M. Dundar, J. Bi, and B. Rao. A fast [14] G. H. Golub and C. F. Van Loan. Matrix [15] P. Hall and K. Li. On almost linearity of low [16] T. Jebara. Multi-task feature and kernel selection for [17] I. Jolliffe. Principal Component Analysis . Springer; [18] S.-J. Kim, A. Magnani, and S. Boyd. Optimal kernel [19] F. D. la Torre Frade and T. Kanade. Discriminative [20] G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, [21] D. Lewis, T. Jebara, and W. S. Noble. Nonstationary are shown in the first row.

AR03P KER 1 KER 2 KER 3 KER 4 KER 5 KER 6 KER 7 KER 8 KER 9 KER 10 10  X  8 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.002 10  X  7 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.002 10  X  6 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.002 10  X  5 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.002 10  X  4 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.002 10  X  3 0000000.0010.0010.0010.014 10  X  2 0000000000.017 10  X  1 0000000000.017 10 0 0000000000.017 10 1 0000000000.017 10 2 0000000000.017 10 3 0000.01300000 0 10 4 0000.01400000 0 10 5 0000.01300000 0 [22] L. Lovasz and M. Plummer. Matching Theory .North [23] L. K. Saul and S. T. Roweis. Think globally, fit [24] S. Sch  X  olkopf and A. Smola. Learning with Kernels: [25] J. Shawe-Taylor and N. Cristianini. Kernel Methods [26] N. Shental, T. Hertz, D. Weinshall, and M. Pavel. [27] A. Smola and I. Kondor. Kernels and regularization [28] S. Sonnenburg, G. R  X  atsch, C. Sch  X  afer, and [29] J. Tenenbaum, V. de Silva, and J. Langford. A global [30] H. Valizadegan and R. Jin. Generalized maximum [31] K. Weinberger, J. Blitzer, and L. Saul. Distance [32] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance [33] B. Yan and C. Domeniconi. An adaptive kernel [34] L. Yang and R. Jin. Distance metric learning: A [35] L. Yang, R. Jin, R. Sukthankar, and Y. Liu. An [36] J. Ye, S. Ji, and J. Chen. Learning the kernel matrix [37] J. Ye, Z. Zhao, and H. Liu. Adaptive distance metric [38] H. Zha, C. Ding, M. Gu, X. He, and H. Simon.
