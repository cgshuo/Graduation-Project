 Calculation of object similarity, for example through a dis-tance function, is a common part of data mining and ma-chine learning algorithms. This calculation is crucial for ef-ficiency since distances are usually evaluated a large number of times, the classical example being query-by-example (find objects that are similar to a given query object). Moreover, the performance of these algorithms depends critically on choosing a good distance function. However, it is often the case that (1) the correct distance is unknown or chosen by hand, and (2) its calculation is computationally expensive ( e.g., such as for large dimensional objects). In this paper, we propose a method for constructing relative-distance pre-serving low-dimensional mappings (sparse mappings). This method allows learning unknown distance functions (or ap-proximating known functions) with the additional property of reducing distance computation time. We present an algo-rithm that given examples of proximity comparisons among triples of objects (object i is more like object j than ob-ject k ), learns a distance function, in as few dimensions as possible, that preserves these distance relationships. The formulation is based on solving a linear programming op-timization problem that finds an optimal mapping for the given dataset and distance relationships. Unlike other popu-lar embedding algorithms, this method can easily generalize to new points, does not have local minima, and explicitly models computational efficiency by finding a mapping that is sparse, i.e., one that depends on a small subset of features or dimensions. Experimental evaluation shows that the pro-posed formulation compares favorably with an state-of-the art method in several publicly available datasets. G.4 [ Mathematical Software ]; H.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval; H.4 [ Information Systems Applications ]: Miscellaneous Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. Algorithms, measurement, performance Metric learning, dimensionalit y reduction, linear program-ming, linear projections, convex optimization, relative dis-tance constraints
The notion of a distance is essential in many machine learning and data mining concepts. From a practical point of view, the choice of a distance has a direct effect on the performance of many algorithms, both in terms of accuracy and of efficiency. From an accuracy perspective, numerous algorithms rely on the user being able to provide a good distance function ( e.g., nearest neighbor, clustering meth-ods, SVM, etc). Here, a good distance function is roughly one that is small for similar objects and large for dissimilar ones. Clearly, since objects can be similar or dissimilar in many respects, similarity is not absolute and depends on the task of interest; thus, different distance functions should in theory be chosen for different tasks. How to best choose an appropriate distance function remains an interesting prob-lem.

From an efficiency perspective, many widely used approaches for data mining, the classical example being query-by-example (find objects that are similar to a given query object), re-quire evaluating a distance function between a large number of points 1 . These calculations often take an important part of the available CPU time and therefore, a relevant prob-lem is how to automatically find accurate approximations to these distance functions but that are efficient to evaluate.
In this paper we present an approach, based on formu-lating and solving a simple linear programming problem, that allows for automatically building distance functions from examples (1) that are tailored to the problem at hand and (2) that have the additional property of being compu-tationally efficient to evaluate. We are interested in solv-ing the following problem. Let us represent the objects of interest 2 by points x k in a D-dimensional space D ,with k = { 1 , 2 , ..., N } . The problem is how to change this repre-sentation to points  X  x k such that distance relationships be-
Other examples include K-means and kernel-based meth-ods in general.
Examples of objects of interest are: database records, user opinions, product characteristics, etc.
 tween points are appropriate for the task of interest accord-ing to side information provided by a user in the form of distance relationships (see Sec. 1.1) and in addition, so that the points lie on a lower dimensional space d .

In order to achieve this, we are interested in finding a transformation A : D  X  d that relates any point in the original space to its low dimensional counterpart. When A is a linear transformation, this can be thought of as learn-ing a Mahalanobis distance ( e.g., see [18, 12, 16]). Sec. 1.2 describes the connection and differences between the formu-lation introduced in this paper, the above, and other related methods.
In this paper, only distance relationships among a number of points are used to capture the structure of the space, no absolute distances are necessary. These relationships may be provided by a user; thus, it would be beneficial to make the information required from the user (1) simple to obtain and (2) easy to provide. We believe that relationships of the form object i is closer to j than to k are both simple to specify and sufficiently informative to capture the prop-erties of the task of interest. In case that an appropriate metric or an algorithm for determining relative similarity were available (but cannot always be used because e.g., it is expensive to evaluate), this information can be obtained more automatically.

In choosing this type of relative relationships, we were inspired by the work in [1]; however, distance relationships of this type were also employed in [15, 12]. Note that we are not interested in preserving absolute distances, which are in general much more difficult to obtain 3 .

Another interesting property of this type of distance re-lationships is that they do not require the concept of class labels ( e.g., [16]) or the specification of examples of similar objects vs. dissimilar ones ( e.g., [18]). The concepts of sim-ilar vs. dissimilar are limited by the fact that a user would need knowledge of at least some (and preferably all) of the rest of the objects in order to determine whether two objects are similar or dissimilar; that is, a reference frame is needed.
The framework presented in this paper is related to dif-ferent sets of approaches. A first set can be represented by unsupervised methods that have approached the prob-lem of finding low dimensional representations of the data. Some of these approaches attempt to capture the variance of the data such as Principal Components Analysis [9], while others build low-dimensional embeddings, that is, by trans-forming a set of data points into a lower dimensional one such that (some or all) distances are preserved. Examples of these approaches include algorithms such as Multidimen-sional Scaling (MDS) [5], Locally Linear Embeddings (LLE) [11], ISOMAP [13], and low-dimensional embeddings via SDP [17]. These methods can implicitly reduce the amount of computation regarding distance calculations; however due to their purely unsupervised nature, they rely on a distance function to be given and cannot build a function such that
Absolute distances imply relative distances, but the con-verse is not true. the accuracy of certain ( e.g., classification) algorithms is im-proved. In other words, they are not designed to capture the concept of an appropriate distance.

Another set of approaches that are related to ours in a different manner are those that attempt to learn an appro-priate distance function from examples. The most closely related approaches are [18, 12, 1]. The first two require some form of supervision and are designed just to learn good distance functions, without explicitly attempting to improve the efficiency of distance calculations. BoostMAP [1] is the most related to our approach in terms of the goals targeted, that is, finding a distance function that is both accurate for the task and efficient to evaluate. Like our method, both [1] and [12] attempt to preserve distance rela-tionships. However, BoostMAP, based on using AdaBoost to combine multiple 1D embeddings to preserve the proxim-ity structure of the data, has the disadvantage that it leads to an iterative, greedy algorithm to optimize the embedding and thus does not have strong optimality guarantees. In contrast, the method in [12] proposes a convex optimiza-tion problem based on SVMs. Its disadvantages include the facts that the problem requires quadratic programming and is only designed to find appropriate weights for the different coordinates of the data; thus a very small subset of lin-ear transformations is explored to obtain a solution. When comparing the properties of the solution space, [18] is the most related approach. This is based on finding a square matrix that defines a Mahalanobis distance. The distance is optimized to respect distance constraints represented by two sets, one of similar and one of dissimilar points. The formulation leads to a convex optimization problem.
As we will show, while the general problem formulated in this paper does not appear to accept efficient algorithms, a proposed comparable problem can be solved by using just linear programming.

Table 1 shows a collection of approaches that share some of the goals or motivations of the approach presented in this paper. It also highlights several important distinguishing attributes. In particular we have considered (1) computa-tional efficiency: whether the method attempts to find low dimensional representations for efficient distance evaluation, (2) generalization to new points: whether the method can easily generalized to new unseen points, (3) distance learn-ing capability: whether it can learn a distance function from user examples, and (4) power of learning algorithm: whether the learning algorithm finds local-free optima (such as it is the case for convex formulations). The method proposed in this paper has a unique set of positive attributes that provide significant advantages over recent approaches.
In the following, vectors will be assumed to be column vectors unless transposed to a row vector by a superscript . The scalar (inner) product of two vectors x and y in the d -dimensional real space d will be denoted by x y .The 2-norm and 1-norm of x will be denoted by x 2 and x 1 respectively. For a matrix A  X  m  X  n ,A i  X  n denotes a row vector formed by the elements of the i -th row of A . Similarly A j  X  m denotes a column vector formed by the elements of the j -th column of A . A column vector of ones of arbitrary dimension will be denoted by e , and one of zeros will be denoted by 0. The identity matrix of arbitrary dimension will be denoted by I .

Let us say we are given a set of points x k  X  D with k = { 1 , ..., N } for which an appropriate distance metric is unknown or expensive to compute. In addition we are given information about a few relative distance comparisons. For-mally, we are given a set T = { ( i, j, k ) | f ( x i , x for some distance function f . As indicated above, f may not be known explicitly, instead a user may only be able to provide these distance relationships sparsely or by exam-ple. We are interested in finding a linear transformation A : D  X  d such that: where  X  x k = A x k . Additionally, since we would like to ac-count for efficiency, we will prefer transformations such that d D .

In summary, we would like to find a new representation of the original space that uses fewer dimensions and where the L 2 norm respects the desired distance relationships.
For ease of notation, we represent the transformation as a square matrix A  X  D  X  D . It is easy to show that if A k = 0 (the k -th column of A is equal to the zero vector), then the k -th original dimension (in D ) can be ignored when calcu-lating the projection and subsequently when evaluating the implied distance function. If z is the number of columns of A that are zero, then d = D  X  z (the dimension of the space induced by A clearly depends on z ). Because of the rela-tionship between this type of matrix sparsity and efficient metric distance evaluation, we have used the term sparse metric . Putting the above ideas together, the projection matrix A can be formally defined as the optimal solution to the following optimization problem: where the set T of triples indicates the distance relationships to be satisfied and 1 ( E ) is the indicator function which re-turn the value 1 if the logical expression E evaluates to true and zero otherwise. The above definition of A is useful at formalizing the desired concept of an optimal projection in the sense presented in this paper. However, as formulated, it is not amenable to practical calculation since it is un-clear whether there exist an efficient algorithm for finding A given the set T . We now concentrate on formulating similar problems that can be more efficiently approached.
Note that for d&lt;D the feasible set for the above problem couldbeempty.Inthatcase,thereisnomatrix A that can solve the problem and A is undefined. Since in practice we may still be interested in finding a good matrix A ,evenif it does not satisfy all of the constraints, in the following we also study variations of the problem to address these cases.
Here we concentrate on convex approximations to the problem in Sec.3.
In order to address the discrete nature of the base for-mulation, we transform the ove rall problem into one with a continuous cost function. Additionally, to address the case where the feasible set is empty, we relax the constraints by introducing slack variables t . The problem can be reformu-lated as follows: where t indexes the set T ,  X   X  is a scalar that controls the trade off between the sparsity of A and compliance with the inequalities generated by the triples in T ,and t  X  repre-sents a slack variable. This new problem is clearly not equiv-alent to the base formulation; however, the 1-norm tends to suppress terms and to produce sparse solutions. This has been empirically validated, in particular in the SVM frame-work e.g., [4, 7]. Hence, the expression formulation (3) will be used to replace the ideal expression  X  P teger Programming (MIP) problem, known to be NP-hard.
Each distance constraint can be written as:  X  ( Ax i ) ( Ax i )+2( Ax i ) ( Ax k )  X  ( Ax k ) ( Ax k )  X  and by defining B = A A  X  D  X  D ,Eq.4canbefurther simplified to: ( x j Bx j )  X  ( x k Bx k )+2[( x i Bx k )  X  ( x i Bx j )]
The main advantage of the new equation is that it pro-duces linear constraints in the new variable B , instead of quadratic constraints in A . However, in order for the equiv-alence to hold, we must have B symmetric and positive semidefinite. Assuming the cost function remains convex in B , this problem is still convex in B . Technically, this formulation is sufficient to approach the problem at hand. However, it becomes a (more expensive to solve) semidefi-nite programming problem (SDP). Next, we will show how the much more efficient linear programming method (LP) can be employed instead to solve different instances of this formulation. We first focus on finding a cost function equivalent to Eq. 3. For this we note that for B = A A : At the distance calculation level, we can easily note that since: we have that:
B k = 0  X  X |  X  x i  X   X  x j || 2 2 does not depend on dimension k as expected.

We can then define a new cost function in terms of B ,and arrive to the following problem: where the set of constraints involving B follow from Eqs. 5 This is also a semi-definite programming (SDP) problem which is convex and can be solved using specialized SDP solvers like [14]. However, we will further develop the for-mulation presented above by restricting our solution space to a subfamily of the PSD matrices: the set of diagonal dominant matrices.
In order to provide a better understanding of the mo-tivation for our next formulation we present the following theorem stated in [8]:
Theorem 4.1. Diagonal Dominance Theorem Sup-pose that M  X  D  X  D is symmetric and that for each i = 1 ,...,n , we have: then M is positive semi-definite (PSD). Furthermore, if the inequalities above are all strict, then M is positive definite.
Based on the diagonal dominance theorem (for matrices with positive diagonal elements) above, we arrive to the fol-lowing formulation, which constrains the feasible set to a subset of that in the formulation (7):  X  ( i, j, k )  X  X  ,  X  2( x where the last constraint is equivalent to diagonal domi-nance which implies positive semidefiniteness according to theorem 4.1. As it was explained before, the sum of 1-norms in the cost function causes preference for sparse solutions. The second term in the cost function combined with the last constraint has a similar effect. Note also that since B mm (as implied by the constraints), the second term is equivalent to Trace( B ).

The projection matrix A can be recovered by Cholesky factorization of the symmetric matrix B [8] or by an eigen-value decomposition. The constraint involving x and B can be rewritten as follows. For any x i , x j  X  D , define  X  X ij =vect( x i x j ), where vect() means (column-wise) align-ment of all of the matrix elements in a column vector. Define b =vect( B ). The constraint can now be written: Finally, using Eqs. 9 and defining auxiliary variables S mn ( m, n  X  X  1 , ..., D } ), formulation (8) can be rewritten as a Linear Program in the following way:
Since the matrix B is symmetric, the number of compo-nents of B to be found can be reduced to D ( D +1) / 2instead of D 2 , furthermore by doing this, the constraints B = B can be discarded. The right hand side of the first set of inequalities has been changed from t to t  X  1inorderto enforce numerical stability and to avoid obtaining the trivial solution B = 0. In order to better understand the motiva-tion for formulation (10) it is important to note that: ( i ) Minimizing ( ii ) Since we are implicitly minimizing ( iii ) Combining ( i )and( ii ) we obtain:
This last formulation works quite effectively in practice as indicated by the numerical examples presented in the next section.
This section presents numerical results obtained by apply-ing the above formulation to the problem of learning metrics.
We tested our approach in a collection of nine publicly available datasets. These datasets are part of the UCI repos-itory 4 . The overall properties of the datasets are shown in Table 2. These datasets are commonly used in machine learning tasks as benchmark for performance evaluation. In particular, a related, competing approach [18] (Xing et al.) was evaluated using these datasets. We have chosen to com-pare our formulation against this method. In addition to being a state-of-the art method, several other reasons mo-tivated this choice: the code has been made public 5 and, like our approach, it attempts to find a linear transforma-tion of the original space and it is supervised (learns from user input). In addition, as shown in [18], this method com-pared well against K-means in the task of findi ng a distance function that produced good clusterings.

The datasets employed in these experiments are generally used for classification since class labels are available for ev-ery point. Our method does not require class labels, but instead only relative distance comparisons among a subset of points (clearly class labels provide more information). We use the available class labels to generate a set of triples with distance comparisons that respect the classes. More explic-itly, given a randomly chosen set of three points (from the training set), if two of these belong to the same class and a third belongs to a different class, then we place this triple in our set T ( i.e., i and j arethepointsinthesameclass, k is the remaining point). In other words, in order to make use of the available class labels, we decided that points in the same class should have smaller pairwise distances between themselves than with points in any of the other classes (after projected by A ). For [18], the supervision is in the form of two sets, one called a similar set and the other a dissimilar set. For this model, we can again use the class labels, now to build a similar set of pairs (likewise for a dissimilar set of pairs). Given this level of supervision, this method at-tempts to find an optimal Mahalanobis distance matrix to have same-class points closer to each other than different-class points (see [18] for details).

For every triple ( i, j, k )  X  X  used in our approach for learning, we use ( i, j )  X  X  and ( i, k )  X  X  for learning in [18]; where S and D are the similar and dissimilar sets required. We believe this provides a fair level of supervision for both http://www.ics.uci.edu/  X  mlearn/MLRepository.html.
Data for all experiments and code for [18] was downloaded from http://www.cs.cmu.edu/  X  epxing/papers/. The class for dataset 1 was obtained by thresholding the median value attribute to 25K. algorithms since roughly the same information is provided. It is possible to obtain a superset of T from S and D ,and by construction S and D can be obtained from T .

In order to evaluate performance, we use a 0 . 85 / 0 . 15 split of the data into training and testing. From the training portion, we generate 1500 triples, as explained above, for actual training. This information is provided, in the appro-priate representation, to both algorithms. For testing, we randomly choose three points, and if their class labels imply that any two points are closer to each other than to a third ( i.e., again if two points have the same class and a third has a different class label), then we check that the correct relationships are satisfied. That is, whether the two points in the same class are closer to each other than any of these points (chosen at random) to the third point. This same measure is used for both algorithms. Thus, we define the percentage correct simply as the proportion of points from the test set (sampled at random) that respect the class-implied distance relationship. Both methods compared are attempting to improve this performance measure given the training data and thus we believe this is a valid measure.
Since our method requires setting the balancing parameter  X  , we chose it using cross validation (on the training set) by letting  X  take values in { 10  X  4 , 10  X  3 , 10  X  2 , 10  X  1 This indirectly influences d , the optimal number of dimen-sions the data should be projected to, since a small  X  fa-vors low-dimensionality ( i.e., a  X  close to zero practically ignores the number of non-zero dimensions and concentrate on just fitting the data). However, note that  X  does not im-ply dimensionality since the dimensionality depends on the randomly built training set itself. This automatic choice of dimensionality is a valuable property of the method pre-sented, and to the best of our knowledge, it is not present in the related methods (with the exception, to some degree, of [1]).

Fig. 1 shows the average optimal number of dimensions found by this process in a 10-fold cross validation exper-iment and the corresponding one-standard-deviation error bars. Note than in some cases the reduction is consider-ably large, this reduction depends on the properties of the dataset. The number of dimensions was identified by looking at the number of rows in A different than 0; no thresholding was necessary.

Fig. 2 shows the percentage correct averaged over 10 ran-dom splits of the data along with one-standard-deviation bars. For each of the 10 splits, 1000 triples from the test set are randomly chosen. When comparing the performance of both methods, we note that, except for dataset 5, our method clearly outperforms the competing approach. In-terestingly, for this dataset, the optimal dimension was de-termined to always (for all randomly chosen data splits) be equal to the original dimensionality (four dimensions).
Since both methods can be seen as trying to learn a Maha-lanobis distance so that distance constraints are satisfied, we believe the main reason for a superior performance is related to the possibility to discover projections into lower dimen-sional spaces. The method presented in this paper always attempt to reduce the dimensionality while the competing method always use all the dimensions. Given the results obtained, this reduction appears to provide an important advantage at the time of generalization. It is generally ac-cepted that a simpler model ( i.e., one with less parameters) is preferable ( e.g., [3]) and it can reduce overfitting.
Regarding computational at training time. As expected, parameter search affects overa ll run-time (linearly). The cost is proportional to the number of  X  values explored. Given  X  , training time is comparable with [18]. In terms of memory requirements, since the solution involves finding a full (symmetric) matrix, the parameter space increases quadratically with D .

From a computational efficiency perspective at test time, being able to represent the original data more succinctly is especially advantageous. In particular, when distances can be calculated directly using a low dimensional repre-sentation, computational time savings can be significant for on-line applications. The projection step in this approach can be precomputed off-line. In retrieval applications ( e.g., query-by-example), the objects can be stored in their low dimensional representation.

From a conceptual point of view, our approach also has the advantage, over other methods, of providing a more effective tool for understanding the data since it can identify whether variables (dimensions) are of high or low relevance for a task of interest.
We have developed a new approach for learning distance functions from a set of relative distance relationships. An important property of this approach is that it targets lower dimensional representations, and the dimensionality is de-termined automatically depending on the characteristics of the dataset in question and a balancing parameter  X  .Akey distinction is that, unlike a large number of dimensional-ity reduction approaches, our approach does not attempt to build an isometry or distance preserving mapping 6 , but to respect the proximity relationships between pairs of points. We believe this allows more freedom for finding lower di-mensional representations.

We considered the general problem and then designed spe-cific formulations, based on minimizing the norm of the rows of a transformation matrix A , that allowed the use of ef-ficient convex optimization algorithms. In particular, we showed how the diagonal dominance constraint on B = A A leads to a general formulation that can be solved very effi-ciently using linear programming methods.

Our approach can also be seen as a form of supervised
For example by minimi zing distortion. Number of dimensions Figure 1: Dimensionality reduction. Total number of di-dimensionality reduction, where the supervision comes in the form of distance rankings.

We note that another way to arrive at the formulation (10) is by noticing that A k = 0  X  B kk , which also justifies the second term of the chosen cost function.

The results from the experimental evaluation show that our method can outperform state-of-the-art approaches in terms of accuracy, and also provides the additional benefit of finding low dimensional representations.

Being able to obtain a distance that depends on a rel-atively small number of features permits to define kernels and/or similarity matrices for classification that may de-pend on an small number of features. For example, instead of using the standard Gaussian kernel (  X  is the Gaussian kernel parameter): that depends on all the features, including irrelevant features % correct distance relationship (  X  100) Figure 2: Performance comparison between competing for classification, we could use a modified Gaussian kernel as follows: that would only depend on relevant features. This simple but powerful change may increase classification performance considerably and it is part of our future work.

Another idea worth exploring is the application of LP-boost algorithms [2] to allow our method the handling of datasets in very high dimensional spaces.

The approach followed in this paper have possible implica-tions in other areas. Our results suggest that the technique applied in this paper to approximate an SDP problem by a linear programming problem (which is much easier to solve) has the potential to be applied to other recently proposed machine learning related problems involving SDP formula-tions. [1] V. Athitsos, J. Alon, S. Sclaroff, and G. Kollios. [2] K. P. Bennett, A. Demiriz, and J. Shawe-Taylor. A [3] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. [4] P. S. Bradley and O. L. Mangasarian. Feature [5] T. Cox and M. Cox. Multidimensional Scaling. [6] C. Faloutsos and K. Lin. Fastmap: A fast algorithm [7] G. Fung, O. L. Mangasarian, and A. Smola. Minimal [8] G. H. Golub and C. F. Van Loan. Matrix [9] I. Jolliffe. Principal Component Analysis .
 [10] K.Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. [11] S. Roweis and L. Saul. Nonlinear dimensionality [12] M. Schultz and T. Joachims. Learning a distance [13] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A [14] K. C. Toh, M. J. Todd, and R. Tutuncu. SDPT3  X  a [15] W.Cohen, R.Schapire, and Y. Singer. Learning to [16] K. Weinberger, J. Blitzer, and L. Saul. Distance [17] K. Q. Weinberger, B. D. Packer, and L. K. Saul. [18] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance
