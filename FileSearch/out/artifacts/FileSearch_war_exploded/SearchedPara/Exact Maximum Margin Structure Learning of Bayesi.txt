 Robert Peharz robert.peharz@tugraz.at Franz Pernkopf pernkopf@tugraz.at Bayesian networks (BNs) are an important type of probabilistic graphical models (Pearl, 1988; Koller &amp; Friedmann, 2009) and specify a probability distribu-tion over a set of random variables (RVs). They make use of a directed acyclic graph (DAG), with nodes cor-responding to the RVs, representing the factorization of the joint distribution. Learning the structure of Bayesian networks from data can be cast as optimiza-tion problem, where the goal is to find a DAG max-imizing some score function. This is a combinatorial problem and known to be NP-hard in general (Chick-ering, 1996). Therefore, most approaches to learn the BN structure are approximative or greedy heuristics. Recently, there has been much interest in exact struc-ture learning, i.e. in finding globally optimal DAGs. Koivisto et al. (2004) use a dynamic programming ap-proach for efficiently summing over all variable orders, leading to exponential (rather than super-exponential) run-time. Further development of this approach can be found in (Silander &amp; Myllym  X aki, 2006; Parviainen &amp; Koivisto, 2009). Due to exponential run-time, these methods are currently restricted to approximately 30-50 variables. Alternatively to dynamic programming, branch-and-bound (B&amp;B) techniques have been ex-ploited for exact structure learning (de Campos et al., 2009; de Campos &amp; Ji, 2011; Jaakkola et al., 2010). In comparison to dynamic programming, these tech-niques offer the advantage of an any-time solution, i.e. as soon as some feasible solution has been found, the algorithm can be interrupted and returns the cur-rently best solution, together with a worst-case sub-optimality bound. However, if the algorithm is kept running, it eventually finds a globally optimal solu-tion.
 These methods have been developed for generative structure learning, i.e. they aim to maximize genera-tive scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper &amp; Herskovits, 1992; Heckerman et al., 1995). These scores are de-composable, i.e. they can be written as a sum over local scores, one for each random variable. On the other hand, when the learned BN shall be used as classifier, we aim to maximize a discriminative score, such as (parameter penalized) conditional likelihood, classification rate, or a recently proposed probabilistic margin formulation (Guo et al., 2005; Pernkopf et al., 2012). Using a discriminative criterion typically leads to classifiers with higher accuracy, especially, when the selected model class does not capture the underlying data distribution. However, since these discriminative BN scores are not decomposable, the discussed meth-ods for exact generative structure learning cannot be directly applied.
 In this paper, we propose an exact method for learn-ing a BN structure maximizing the probabilistic mar-gin. For this purpose, we use concepts developed in (de Campos et al., 2009; Jaakkola et al., 2010; Cussens, 2011), leading to a formulation as mixed integer lin-ear program (MILP). For solving a MILP, the prob-lem is relaxed in a linear program (LP), and a B&amp;B method is used to enforce integrality. Therefore, our work falls within the line of research using B&amp;B for exact structure learning, but maximizing a discrimi-native criterion. Similar as in (Cussens, 2010), we use a set of order constraints to enforce acyclicity in the di-rected graph, rather than the cluster constraints used in (Jaakkola et al., 2010). The advantage of this for-mulation is, that for N RVs, we only require N 2  X  N linear constraints for enforcing acyclicity, rather than super-exponentially many cluster constraints. Conse-quently, we are able to compactly represent our prob-lem and to use powerful general-purpose solvers for structure learning. Although we learn a discrimina-tive BN structure in order to obtain good classifiers, we still use maximum likelihood parameters. Therefore, the resulting BN consistently approximates the true underlying distribution and is suitable for all kinds of inference scenarios.
 Similarly as in (Guo et al., 2005), the margin formu-lation needs one linear constraint per training sam-ple and per competing class. This can render the approach infeasible for problems with many training samples and many class values. Therefore, as a second contribution, we propose a binary margin formulation, which can be interpreted as a local (sample-wise) one-versus-all classification scheme. The problem size us-ing the binary margin does not depend on the num-ber of classes, which is computationally beneficial for problems with many classes. For binary classification problems, the two margin formulations are equivalent. For multi-class problems, we empirically shown that the binary margin classifier competes with the origi-nal max-margin structure. We perform classification experiments on 31 datasets, and compare our algo-rithms with naive Bayes, tree-augmented naive Bayes, generative learned BNs and support vector machines (SVMs). The max-margin structures outperform the other BN classifiers on most datasets, and compete with support vector machines.
 The paper is organized as follows. In section 2, we re-view BNs and introduce our notation, and in section 3, we review related work. We present our method for max-margin structure learning in section 4. The bi-nary margin formulation is introduced in 5. In section 6 we present our experiments and section 7 concludes the paper. Throughout the paper we assume discrete RVs, where plain capital letters denote single RVs and capital boldface letters represent sets of RVs. The set of states which can be assumed by RV X is denoted as val ( X ), and we define sp ( X ) = | val ( X ) | . For simplicity of no-tation, we identify the states of an RV X with natural numbers, i.e. val ( X ) = { 1 ,..., sp ( X ) } . However, we do not assume a particular ordering or interpretation of these states. Furthermore, we use val ( X ) to denote the set of possible joint states of a set of RVs X , and let sp ( X ) = | val ( X ) | . Lower-case plain letters repre-sent values or states of RVs, e.g. x is a value of RV X . Similarly, lower-case boldface letter represent joint states of variable sets, e.g. x is a state of RV set X . When y is a state of Y , and X is a subset of Y , then y ( X ) denotes the corresponding state of X .
 A BN B over a set of N RVs X = { X 1 ,...,X N } is defined as a tuple  X  X  ,  X   X  , where G is a DAG, with nodes corresponding to the RVs in X . The set of parents of X i according to G is denoted as Pa i . The set  X  = {  X  1 ,..., X  N } contains parameter sets  X  i = {  X  i j | h ,  X  j  X  val ( X i ) ,  X  h  X  val ( Pa variable X i , parameterizing a conditional probability distribution: P ( X i = j | Pa i = h ;  X  i ) =  X  i j | h . A BN de-fines a probability distribution over the RVs, according to where  X  i j | h is the indicator function 1 ( x i = j and x ( Pa of the variables in X represents the class variable C , where without loss of generality, we assume that X 1 = C and let Z = { X 2 ,...,X N } . Let D = { x 1 ,... x M } be a collection of M i.i.d. samples drawn from some unknown distribution. It is well known that for a fixed BN structure G , the maximum likelihood (ML) parameters  X   X  are given as  X  these parameters using Laplace-smoothing, by replac-ing n i j | h with n i j | h + 1. The smoothed parameters are also consistent ML estimators, although biased to-wards a uniform distribution.
 We adopt the framework developed in (Jaakkola et al., 2010), where the aim was to maximize a generative score such as MDL/BIC or BDe. For each vari-able X i , we identify a set of possible parent-sets S i = { S i, 1 ,... S i,Q i } , where Q i = |S i | and each S i,j  X  X \ X i , j  X  { 1 ,...,Q i } . A specific net-work structure G is represented by selecting a single parent-set from each S i , i  X  { 1 ,...,N } . The sets S i have to be reasonable large to represent a vari-ety of solutions, while being reasonable small, such that the algorithm remains tractable. In (de Cam-pos et al., 2009), a pruning strategy was presented, for a-priori excluding all parent-sets, which can not occur in an optimal DAG. Since not every combina-tion of parent-sets yields an acyclic graph, additional acyclicity constraints have to be imposed. More for-computed local scores, where  X  i,j is the local score for S i,j . Furthermore, let  X  i = (  X  i, 1 ,..., X  i,Q i parent-set indicator vector, which contains exactly one 1, indicating the selected parent-set in S i , and which is 0 elsewhere. All  X  i are stacked into a single vector  X  = (  X  T 1 ,...,  X  T N ) T , and similarly  X  = (  X  T 1 ,...,  X  Let P be the convex hull of all vectors  X  which rep-resent valid DAGs. Consequently, all vertices of P represent DAGs, and as easily shown, all vectors  X  representing cyclic graphs are not elements of P . Gen-erative structure learning is cast as the LP Since there is always an optimal solution in some ver-tex of P , and since each vertex represents a DAG, we are in principle able to recover an optimal structure by solving (3). Note that P has super-exponentially many facets in the number of variables, which, in agreement with theory (Chickering, 1996), makes the problem hard. Unfortunately, no representation of P via linear inequalities is known. Therefore, in (Jaakkola et al., 2010; Cussens, 2011) the constraint in problem (3) was replaced with  X  i,j  X  0 i  X  X  1 ,...,N } , j  X  X  1 ,...,Q i } (4) X  X  i,j  X  Z i  X  X  1 ,...,N } , j  X  X  1 ,...,Q i } (6)
X The constraints (4), (5) and (7) were used as approx-imation for P . However, since the solution of the LP might be fractional, the integrality constraint (6) is re-quired. The constraints (4)-(6) can be interpreted as the constraint  X   X  represents a directed graph X , since they enforce that exactly one entry in each  X  i is 1, and all others are 0. Constraints (7) enforce acyclicity, since they enforce that for each cluster C  X  X , there is at least one variable X i  X  C whose parent-set is either outside C , or which is empty. Thus, constraints (4)-(7) force  X  to represent a DAG. Note that the problem has super-exponentially many constraints. Jaakkola et al. (2010) solve the relaxed problem in the dual, where each in-active cluster constraint corresponds to a zero dual variable, and Cussens (2011) uses a cut-ting plane approach, iteratively adding violated cluster constraints.
 As already noted, it is the decomposability of gener-ative scores which yields the linear objective  X  T  X  in (3), leading to the LP formulation. Since discrimi-native scores are usually not decomposable, the LP approach cannot be directly applied. However, in the next section we derive an exact MILP formulation for the so-called probabilistic soft margin. The margin  X  m of the m th sample is defined as (Guo et al., 2005): When  X  m &gt; 1, then the m th sample is correctly clas-sified, and when  X  m &lt; 1, it is wrongly classified. Mo-tivated by SVMs, Pernkopf et al. (2012) defined a soft margin (SM) using the hinge loss: The log-margin of each sample contributes linearly to the overall score. To avoid that the score is mainly de-termined by a few samples with overly large margin, the sample margins are limited with the hinge func-tion min(  X  , X  ). The parameter  X  , which is obtained by cross validation, has the interpretation as  X  X esired log-margin X  for each sample. In (Pernkopf et al., 2012) this score was used for parameter learning using a con-jugate gradient method, and in (Pernkopf et al., 2011; Pernkopf &amp; Wohlmayr, 2012) the same score was used for inexact BN structure learning, based on greedy hill-climbing and simulated annealing. We aim to find a BN structure G globally maximiz-ing the SM score in (9). First, we restrict the max-imal number of parents for each variable, to obtain a tractable number of parent-sets. For a variable X i 6 = C , we only need to consider the empty par-ent set, and parent-sets containing C , since all other parent-sets do not influence the margin. 1 We further assume Laplace-smoothed ML parameters while learn-ing the structure. Firstly because simultaneous learn-ing of max-margin structure and parameters would render our approach intractable. Secondly, by using generative parameters, the resulting BN can still be interpreted as generative model, although its structure is determined discriminatively.
 Using the notation introduced in section 3, we can expand the BN distribution (1) according to P B ( x ) = where  X  i,k j | h are the ML parameters when S i,k is the parent-set of variable X i , and  X  represents the same distribution as (1), where the structure G is explicitly encoded with  X  . Inserting (10) in the margin definition (8) and taking the log, gives value in x m is replaced with the value c . The coeffi-cient  X  ( i,k,m,c ) is given as  X  ( i,k,m,c ) = By defining a vector  X  m,c containing the coefficients  X  ( i,k,m,c ) corresponding with the entries of  X  , the log-margin in (11) can be written as a minimum over inner products: Using a standard technique from linear programming, we can express the SM score in (9) as follows. We introduce a variable  X  m for each sample, together with the constraints and maximize P M m =1  X  m . In an optimal LP solu-tion we have  X  m = min(log  X  m , X  ), and P M m =1  X  m precisely the SM score. As in generative structure learning, the DAG constraint could in principle be ad-dressed by constraints (4)-(7). However, in this paper we use a more convenient way to express acyclicity, allowing a compact MILP representation of our prob-lem. Therefore, we replace the cluster constraints (7) with alternative order constraints , enforcing a topolog-ical ordering among the nodes and thus acyclicity of the resulting graph. We introduce a real-valued order variable o i for each variable X i , which is constrained to 0  X  o i  X   X , where  X  is some arbitrary positive number. The order constraints are: ing proposition shows that these constraints enforce acyclicity.
 Proposition 1. A vector  X  represents a DAG if and only if there exist some o i , i  X  { 1 ,...,N } , with 0  X  o i  X   X  , for some arbitrary  X  &gt; 0 , such that con-straints (16) and (4) -(6) are fulfilled.
 Proof. First we show that when the conditions in the proposition hold, then  X  is necessarily a DAG. Con-straints (4)-(6) enforce that  X  represents some directed graph G , since in this case each  X  i contains exactly one 1 and is 0 elsewhere. It follows that also a i,j is either 1 or 0, and equals an entry of the adjacency matrix of G , indicating an edge from X i to X j . We switch cases: When a i,j = 0, i.e. when there is no edge X i  X  X j , then (16) yields and the constraint is fulfilled regardless of o i ,o j . When a i,j = 1, i.e. when there is an edge X i  X  X j , then we have which implies that o j is strictly larger than o i . By sorting all o i , we obtain an ordering among the vari-ables (among several o i with the same value, we pick an arbitrary ordering). Since there can not be an edge X i  X  X j when X i comes after X j , this ordering is a topological ordering, and thus the resulting directed graph is acyclic.
 It remains to show that when  X  represents a DAG, then the conditions in proposition 1 hold. When  X  rep-resents a DAG, then it fulfills constraints (4)-(6), since it is a directed graph. Furthermore, we can obtain some topological ordering from the DAG. Let  X  o i be the index of X i in this ordering. Setting o i = (  X  o i  X  1) fulfills constraints (16) and 0  X  o i  X   X .
 In contrast to the super-exponentially many cluster constraints in (7), we only need N 2  X  N linear order constraints and N additional real-valued order vari-ables to enforce acyclicity. Thus, the resulting prob-lem has a more compact representation and can be solved by general-purpose MILP solvers. Similar con-straints, using the same mechanism as depicted here, have been proposed for maximum-likelihood pedigree learning (Cussens, 2010). To summarize, our MILP formulation for finding a DAG maximizing the SM in (9), is given as: s . t .  X  m  X   X  T m,c  X  ,  X  m,  X  c 6 = c m Note that this formulation can immediately be applied to decomposable (i.e. practically all generative) scores. We simply remove the first two constraints (the margin constraints), and replace the objective with the objec-tive of problem (3). Before we present experimental results in section 6, we introduce an alternative mar-gin formulation, leading to a simpler problem. The main limitation of problem (19) is that we have a constraint  X  m  X   X  T m,c  X  for each sample and each competing class, i.e. in total M ( sp ( C )  X  1) linear con-straints for specifying the margin. 2 One approach would be to reduce the number of samples M , by using a representative sub-set of D . Similar techniques have been proposed for clustering and mixture model train-ing, and we plan to address this point in future work. Here, we propose a formulation which only needs M constraints rather than M ( sp ( C )  X  1), which alleviates especially problems with many class values. The basic idea is to employ a one-versus-all classification scheme. When we desire to train a max-margin BN for classi-fying class c versus all other classes, we replace the parameters used in (10)-(12) with where In words,  X  i,k j | h ( c ) are those ML parameters when we re-interpret class value c as class 1 and all other class values as class 2. We could now train a one-versus-all classifier for each c  X  X  1 ,..., sp ( C ) } and combine their decisions for the multi-class problem. However, it is generally unclear how to correctly combine the deci-sions of one-versus-all classifiers. Furthermore, we now would have to train sp ( C ) different classifiers instead of a single one, although each problem would have only M instead of ( sp ( C )  X  1) M margin constraints. In-stead, we use the following local, i.e. sample-wise, one-versus-all scheme. Let  X ( c ) be the collection of the pa-rameters according to (20), for all c  X  X  1 ,..., sp ( C ) } . Let P B ,  X ( c ) ( X ) be the BN distribution with parame-ters  X ( c ) and P B ,  X  ( X ) the BN distribution with origi-nal parameters  X . We define the binary margin  X   X  m of the m th sample as used exactly the same statistics for class c in the original parameterization  X , and for class 1 in the alternative parameterization  X ( c m ). When sp ( C ) = 2, then  X (1) and  X (2) are sim-ply redundant versions of  X . In this case we  X   X  m =  X  m , i.e. the margin and the binary margin are equivalent for binary classification problems. For P Note that the additional parameters can be stored effi-ciently, since the parameters for class 1 (for each  X ( c )) are already stored in the original parameter set  X . Therefore, we need only twice as much memory for storing  X  and  X ( c ), c  X  X  1 ,..., sp ( C ) } , than for stor-ing  X  alone. Following a similar derivation as for (13), we obtain where  X   X  m contains the coefficients (cf. (12)) Similar as in (9), we define a soft binary margin SBM ( B ) = P M m =1 min(log  X   X  m , X  ) . The MILP for find-ing a DAG maximizing the SBM is defined as in (19), except that the constraints are replaced with The alternative parameters  X ( c ) are only needed to obtain the coefficients  X  a ( i,k,m ). For the final BN clas-sifier, we use the original parameters  X . We performed classification experiments on 31 dataset obtained from the UCI machine learning repository (Frank &amp; Asuncion, 2010). We used the 25 datasets already used in (Friedman et al., 1997), plus six ad-ditional datasets:  X  X balone X ,  X  X dult X ,  X  X ar X ,  X  X ush-room X ,  X  X ursery X , and  X  X pambase X . These datasets have between 4 and 57 input features and contain between 80 and 45222 samples. For a more de-tailed information we refer the reader to (Frank &amp; Asuncion, 2010). To estimate the accuracy of the classifiers, a test set was used for the datasets  X  X hess X ,  X  X etter X ,  X  X ofn-3-7-10 X ,  X  X atimage X ,  X  X eg-ment X ,  X  X huttle-small X ,  X  X aveform-21 X , and the six additional datasets. For the remaining datasets, 5-fold cross-validation was used to estimate the ac-curacy. Samples with missing features were re-moved beforehand, and continuous features were dis-cretized using the method described in (Fayyad &amp; Irani, 1993). We compared our methods with naive Bayes (NB), the tree-augmented naive Bayes (TAN) (Friedman et al., 1997) and with a BN with gener-atively trained structure, using MDL as score func-tion (Suzuki, 1993). Furthermore, we compared with SVMs using a Gaussian kernel, using the LIB-SVM implementation (Chang &amp; Lin, 2011). For the SVM parameters  X  (width of Gaussian kernel) and C (trade-off factor), we validated all combinations of  X   X  { 2 our methods, BN structure learning using the SM and SBM, we validated the parameter  X  (desired log margin), where we used  X  = log p 1  X  p , with tionally, we validated the maximal number of parents, where we used 1 or 2 parents per node. When the training set was sufficiently large ( &gt; 1000 samples) we used 20% of the training samples as validation set. Otherwise, we used 5-fold cross validation. In all cases, we used the same validation set/cross-folds for SVM training and for our algorithms.
 For solving MILPs we used the ILOG CPLEX opti-mizer. 3 For each optimization problem we set a time limit of 2 hours, i.e. if after 2 hours an optimization had not finished, we stopped it and used the best solu-tion found so far. When maximizing the SM, for most datasets an optimal solution was found within these two hours, except for  X  X etter X ,  X  X atimage X ,  X  X egment X ,  X  X oybean-large X ,  X  X ehicle X ,  X  X dult X , and  X  X pambase X . For the datasets  X  X etter X  and  X  X oybean-large X  the re-sulting MILPs were too large to return a reasonable so-lution at all: only the trivial unconnected DAG, found by an internal CPLEX heuristic was returned. How-ever, when maximizing the SBM, a reasonable solution was found in any case. Table 1 shows the worst-case sub-optimality bounds for the  X  X roblematic X  datasets, bound of the margin score, and z is the objective of the best feasible solution. For the generative BNs using the MDL score, we used our formulation (19) without margin constraints and used the linear MDL objec-tive (cf. (3)). We used the same set of parent-sets as for max-margin training, and also cross-validated the number of parents per node (1 or 2 parents). For all datasets an optimal solution was found, where the optimization time was typically under 1 second. The longest optimization time of 716 seconds was needed for  X  X pambase X  (58 variables).
 The classification results for all datasets are shown in table 2, where the estimated accuracy, together with the 95% confidence intervals is shown (Mitchell, 1997). Table 3 summarizes these results, where the plain and boldface numbers are the number of times a classi-fier outperforms an other classifier with a significance level of 68% and 95%, respectively. For the signifi-cance tests, we used a one-sided paired t-test for the datasets with cross-validation, and a one-sided bino-mial test for the other datasets. We see that the max-margin structures SM and SBM perform clearly better than NB, TAN and MDL, since a max-margin struc-ture outperforms the other BNs at least 17 times (11 times significantly), while NB, TAN and MDL out-perform a max-margin structure maximal 7 times (2 times significantly). On the other hand, the SVM out-performs the max-margin structures at least 11 times (6 times significantly), while a max-margin structure outperforms the SVM maximal 9 times (3 times sig-nificantly). Therefore, there is a trend in favor of the SVM, although the results for the max-margin struc-tures are in the same range. As already mentioned, the BNs with max-margin structure still use genera-tive parameters. Therefore, the resulting models still consistently approximate the empirical data distribu-tion and are amenable for other inference tasks than classification. Additionally, we could train the param-eters of the resulting BNs in a discriminative way, to further improve classification results (Guo et al., 2005; Pernkopf et al., 2012). We plan to address this in fu-ture work.
 We proposed an exact method for the combinato-rial problem of finding a BN structure maximiz-ing the probabilistic soft margin, extending previous methods for exact generative BN structure learning. We demonstrated the applicability of our methods on small and medium sized datasets and produced promising results. Having an exact algorithm is valu-able  X  although the problem is NP-hard. Firstly, it is important to address those datasets were the problem turns out to be tractable. Secondly, a key feature of the methods presented in this paper is that they pro-vide any-time solutions, i.e. when the problem turns out to be infeasible, they still return an approximation together with a worst-case estimate of sub-optimality. Therefore, these methods can also provide satisfying results on more difficult problems. Furthermore, exact methods provide interesting theoretical insights into the problem nature, and possibly motivate new heuris-tics and approximations for inexact structure learning. This work was supported by the Austrian Science Fund (project number P22488-N23).
 Buntine, W. Theory refinement on Bayesian networks. In UAI , pp. 52 X 60, 1991.
 Chang, C.-C. and Lin, C.-J. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology , 2:27:1 X  27:27, 2011. URL http://www.csie.ntu.edu.tw/ ~ cjlin/libsvm .
 Chickering, D. M. Learning Bayesian networks is NP-complete. In Learning from Data: Artificial In-telligence and Statistics V , pp. 121 X 130. Springer-Verlag, New York, 1996.
 Cooper, G. F. and Herskovits, E. A Bayesian method for the induction of probabilistic networks from data. Machine Learning , 9:309 X 347, 1992.
 Cussens, J. Maximum likelihood pedigree reconstruc-tion using integer programming. In Proceedings of WCB-10 , 2010.
 Cussens, J. Bayesian network learning with cutting planes. In UAI , pp. 153 X 160, 2011. de Campos, C. P. and Ji, Q. Efficient structure learn-ing of Bayesian networks using constraints. Journal of Machine Learning Research , 12:663 X 689, 2011. de Campos, C. P., Zeng, Z., and Ji, Q. Structure learning of Bayesian networks using constraints. In ICML , pp. 113 X 120, 2009.
 Fayyad, U.M. and Irani, K.B. Multi-interval dis-cretization of continuous-valued attributes for clas-sification learning. In UAI , pp. 1022 X 1027, 1993. Frank, A. and Asuncion, A. UCI machine learning repository. University of California, Irvine, School of Information and Computer Sciences, 2010. URL http://archive.ics.uci.edu/ml .
 Friedman, N., Geiger, D., and Goldszmidt, M.
Bayesian network classifiers. Machine Learning , 29: 131 X 163, 1997.
 Guo, Y., Wilkinson, D., and Schuurmans, D. Maxi-mum margin Bayesian networks. In UAI , pp. 233 X  242, 2005.
 Heckerman, D., Geiger, D., and Chickering, D. M.
Learning Bayesian networks: the combination of knowledge and statistical data. Machine Learning , 20:197 X 243, 1995.
 Jaakkola, T., Sontag, D., Globerson, A., and Meila,
M. Learning Bayesian network structure using LP relaxations. In AI Statistics , pp. 358 X 365, 2010. Koivisto, M. and Sood, K. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research , 5:549 X 573, 2004.
 Koller, D. and Friedmann, N. Probabilistic Graphi-cal Models -Principles and Techniques . MIT Press, 2009.
 Mitchell, T. Machine Learning . McGraw-Hill, Inc., 1997.
 Parviainen, P. and Koivisto, M. Exact structure dis-covery in Bayesian networks with less space. In UAI , pp. 436 X 443, 2009.
 Pearl, J. Probabilistic Reasoning in Intelligent Sys-tems: Networks of Plausible Inference . Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.
 Pernkopf, F. and Wohlmayr, M. Stochastic margin-based structure learning of Bayesian network classi-fiers. Pattern Recognition, revised , 2012.
 Pernkopf, F., Wohlmayr, M., and M  X ucke, M. Maxi-mum margin structure learning of Bayesian network classifiers. In ICASSP , pp. 2076 X 2079, 2011. Pernkopf, F., Wohlmayr, M., and Tschiatschek, S. Maximum margin Bayesian network classifiers. IEEE TPAMI , 34:521 X 532, 2012.
 Rissanen, J. Modeling by shortest data description. Automatica , 14:465 X 471, 1978.
 Silander, T. and Myllym  X aki, P. A simple approach for finding the globally optimal Bayesian network structure. In UAI , pp. 445 X 452, 2006.
 Suzuki, J. A construction of Bayesian networks from databases based on an MDL principle. In UAI , pp.
