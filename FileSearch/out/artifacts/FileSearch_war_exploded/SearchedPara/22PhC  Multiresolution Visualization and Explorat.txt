 LUIGI DI CARO and MARIA LUISA SAPINO , University of Torino Today, text is being produced and consumed in a wide variety of applications, including science, news, e-commerce, blogs, and social networking sites. This flood of data in textual form brings forth a need for effective ways to visualize and analyze large text collections.

The need for informed navigation within large text collections has been highlighted in the literature [Bates 1989], but effective solutions are elusive. Most commonly used text visualization tools, such as term-or tag-clouds (Figure 1) have significant limita-tions. A tag, whether provided by the user or extracted from the textual content itself, provides an easy way to search and index blogs and other online media and documents. For example, most visualizations of tag (or keyword) clouds vary the sizes of the fonts to differentiate most important tags from those that are less important (Figure 1(a)). While this helps users quickly observe the most frequent terms in a text collection, this representation falls short in making the context in which these terms/tags co-occur apparent. While some existing text collection visualization schemes, such as tag-flakes [Di Caro et al. 2008] and ContexTour [Lin et al. 2010], attempt to visu-ally organize the terms extracted from the text collection in a way that highlights co-occurrences of tags in that collection, or re flects the underlying community clusters, even these fail to make it possible for a user to understand how the documents are distributed in the inherently multidimensional space with respect to a set of relevant terms. Complex, multidimensional data do not fit well into 2D screens. Consequently, a key challenge in multidimensional data visualization is to map data on 2D graphi-cal displays in a way that preserves the underlying information and is easy to view and explore. Existing techniques, including Parallel Coordinates (PC) [Inselberg and Dimsdale 1990], radial visualization [Hoffman et al. 1997], circle segments [Ankerst et al. 1996], heat maps [Eisen et al. 1998], and treemaps [Shneiderman 1992], all ad-dress this challenge differently.

The Parallel Coordinates (PC) technique [Inselberg and Dimsdale 1990], for exam-ple, is based on the following data mapping strategy: the dimensions of the data (e.g., the attributes of a data relation) are represented as (often equispaced) vertical parallel lines or parallel coordinates . Each distinct value in the domain of a given data dimen-sion corresponds to a point on the corresponding vertical line. Given a dataset, each multidimensional data element (e.g., a tuple in a relation) is converted into a poly-line or a continuous curve which passes through the corresponding value points on the vertical coordinate lines. By mapping data elements onto the 2D space in such a way that similar elements are represented as similar poly-lines or curves on this space, PC is able to help users discern dominant patterns in the multidimensional data. For instance, a cluster of objects in the multidimensional space will appear as a dense cluster of lines that have similar flows in the 2D space.

In this article, we first note that parallel-coordinates-based visualization is suitable for helping the users observe the distribu tion of the data within a text collection and observe the underlying patterns. Figure 2 presents an example. Here, the user is an-alyzing a collection of NSF award abstracts, within the context of the query keyword  X  X cean X . At the right-hand side of the interface, the underlying tag cloud is presented to the user. In this example, the user has selected three of the terms,  X  X ystem X ,  X  X odel X , and  X  X rocess X  for further analysis and the system mapped the matching award docu-ments onto the parallel coordinates based on corresponding term frequencies 1 .This visualization, for instance, helps the user observe that, within this context, those docu-ments that contain the term  X  X ystem X  frequently contain the term  X  X odel X  very rarely. We, however, also note that, despite its ability to map multidimensional data to 2D visualization space, PC-based visualization also faces some challenges. In particular, the reduction in the dimensionality during the mapping of many-dimensional points to 2D lines often implies visual clutter and this visual clutter is obvious in Figure 2. Visual clutter in PC occurs because data clusters that are separate in the original space may end up overlapping with each other in the 2D space, making them harder to distinguish. These overlaps often appear as crossings of the edges in the visualization or unnecessarily dense regions of the 2D space (Figure 3). When overly dense, such crossings can create significant visual clutter, rendering the patterns in the data hard to discern.

In this article, we focus on the reduction of the visual clutter in PC for obtaining cleaner and easier to interpret visualizations of text collections. Figure 4 shows that one way to achieve this is to cluster the values along the parallel coordinates: this helps reduce the visual clutter in Figure 2 and helps highlight the data patterns. In this visualization, each ellipse corresponds to a cluster of values along a tag-coordinate and the width of the ellipse corresponds to the amount of curves (i.e., documents) that fall into that cluster of values. The curves cross the clusters (not at the centers of the ellipses, but) at the cluster centroids. Note that, in the alternative visualization in Figure 4, the data patterns are visible with ease and this visualization imposes less visual load than the one in Figure 2.

It is, however, important to note that reducing the detail of the data in the individual coordinates may lead to certain degree of information loss. In fact, given the same data while some value clusterings may help improve the visualization, some others may collapse distinct patterns and make them impossible to distinguish. Thus, which values are clustered and how much clustering is applied to each dimension have to be selected carefully. In this article, we present the Parallel hierarchical Coordinates ( PhC ) approach to multiresolution visualization of large text corpora. PhC relies on a parallel coordinates (PC)-based mapping, where user-selected visualization terms are represented as ver-tical parallel coordinates and each document in the dataset is drawn as a continuous curve which passes through the corresponding term frequency values on the vertical coordinate lines. In the resulting visualization, patterns in the document collection appear as dense regions.

While we argue that PC-based visualizations can be effective in text document col-lection visualization and analysis, in this article, we also note that naive PC-based visualizations often suffer significant visual clutter and propose the Parallel hierar-chical Coordinates ( PhC ) approach to alleviate this problem. (1) PhC takes as input a document collection and produces a tag cloud from which the (2) Next, for each term a value clustering hierarchy is created using a hierarchical (3) Given a value clustering hierarchy for each of the term-coordinates and a user-(4) The user can then navigate over the resulting multiresolution parallel coordinate The article is organized as follows: In Section 2, we discuss the related work. In Section 3, we describe the utilization of Parallel hierarchical Coordinates (PhC) for vi-sualizing large text documents. In Section 4, we describe how PhC helps control the information loss during visual clutter reduction. In this article, in addition to propos-ing a novel multiresolution PC technique for visual clutter reduction in visualization of text document collections, we also present novel visual clutter and information loss measures for PC-based visualizations (Section 5). Evaluation results, presented in Sec-tions 6, show that PhC provides significant visual gains (i.e., clutter reduction) with small information loss. In this section, we present an overview of the related work in the areas of document collection visualization as well as discuss prior work on parallel coordinates. A critical aspect of the knowledge discovery process is the presentation layer, that is, information visualization and browsing. Effective and interactive designs can em-power users, on the other hand, an ineffective design may cause the user be lost in a sea of information.

The image hosting Web site Flickr 2 was one of the first systems that used tag clouds for visualizing lists of words, associated to a given media object, where word impor-tance is represented with font sizes. Hassan-Montero and Herrero-Solana [2006a] propose a graphical visualization of tag clouds, where the tags are selected on the ba-sis of their frequency of use. Relationships among tags are defined in terms of their similarity, quantified by means of the Jaccard coefficient. K-means clustering is then applied on the tag similarity matrix, with an a priori chosen number of clusters and fixed number of selected relevant tags. Hassan-Montero and Herrero-Solana [2006b] further the work in Hassan-Montero and Herrero-Solana [2006a] by applying Multi Dimensional Scaling, MDS , (using Pearson X  X  correlation as the tag similarity function) to create a bidimensional space, which is then visualized through a fish-eye system. We note, however, that while preserving distances, MDS does not preserve the energies of the input tags. Similarly, using only two dimensions can be overly lossy. Research on effective use of 2D spaces for multidimensional data visualization focuses on careful selection of the relevant dimensions [Seo and Shneiderman 2004] and organizing data in hierarchical visualization structures, such as TreeMaps, along the relevant dimen-sions, and mapping these two 2D spaces [Chintalapani et al. 2004].

PhaseTwo X  X  goal is to create visually pleasant tag clouds, by presenting tags in the form of seemingly random collections of circles with varying sizes 3 :thesizeofthecircle denotes its frequency. Each tag circle is first placed in the center of the cloud and then fired from the center along a random angle. The tag circle stops when it collides with another circle. This visualization scheme intentionally randomizes the placement of the tags with the hope of projecting a more pleasant (if not highly informative) feeling.
In Fortuna et al. [2005] the authors describe a system to visualize the semantics contained in a textual corpus. They rely on a Latent Semantic Analysis technique to extract the information about the principal dimensions emerging from the text by means of Singular Value Decomposition (SVD) applied on the term-document fre-quency matrix [Eckart and Young 1936]. They automatize the choice of the concept space dimensions, by choosing the minimum k such that k i =1 S i n singular values on the diagonal of the singular value matrix, and  X  is a system pa-rameter threshold, set for example to 0 . 5. After identifying the relevant dimensions, in Fortuna et al. [2005], the authors apply MultiDimensional Scaling ( MDS )[Coxand Cox 2001] to reduce the dimensionality all the way down to two dimensions, to allow the visualization on the 2D plane.

In Don et al. [2007] the authors address the problem of making text mining results more comprehensible to humanities scholars, journalists, intelligence analysts, and other researchers, using the FeatureLens system, that visualizes a text collection at several levels of granularity and enables users to explore interesting text patterns. The implementation has panels in which different document features could be focused and analysed. First of all, the authors focu s on frequent itemsets of n-grams, so they capture the repetition of exact or similar expressions in the collection. Users can find meaningful co-occurrences of text patterns b y visualizing them within and across doc-uments in the collection. The system allows users to identify the temporal evolution of usage such as increasing, decreasing, or sudden appearance of text patterns. In order to provide repeated expressions that are exact repetitions as well as repetitions with slight variations, in Don et al. [2007] the authors propose to use frequent closed item-sets of n-grams, according to a specific definition of pattern: a set of n-grams X is a pattern if there exists no set of n-grams X  X  such that X  X  is a proper superset of X ,and every paragraph containing X also contains X  X . Don et al. [2007] also provide tooltips that show the paragraphs where some select ed pattern occurs, making explicit their context of usage. As we mentioned in the Introduction, PC-based visualization faces visual clutter due to dimensionality reduction. In this section, we provide an overview of the prior attempts to tackle this challenge. 2.2.1. Dimension Ordering. In Peng et al. [2004], Peng et al. introduce a visual clutter measure for PC. They define visual clutter as the ratio of outliers in the visualization and recognize that the number of outliers can be reduced by careful ordering of the dimensions on the parallel coordinates visualization. Yang et al. [2003] also recognize that careful clustering and ordering of the dimensions (based on similarities) can help improve visualization effectiveness. In general, the optimal ordering of the dimensions is an NP-complete problem [Ankerst et al. 1998]. In this article, we assume that the appropriate order of dimensions is selected in advance, either by the user or by a dimension-ordering algorithm. 2.2.2. Data Filtering. An alternative approach for reducing visual clutter is to re-duce the data themselves. Zhou et al. [2009] propose a splatting -based noise removal method which eliminates outlier poly-lines. The algorithm randomly selects a set of poly-lines and augments its and its neighbors intensities. Since outliers are not neigh-bors to many other poly-lines, the intensities of the outliers gradually become rela-tively lower and they disappear from the visualization. Artero et al. [2004] use data frequency for selecting the data to be presented. Instead of applying the reduction on the entire data visualization, Ellis and Dix [2006] propose a sampling lens , a movable region within which the data are presented in a down-sampled or reduced manner. 2.2.3. Data Clustering. Visual clutter can also be reduced by clustering the poly-lines as opposed to filtering some of them out. Siirtola [2000] proposes a poly-line averaging technique where a set of poly-lines, selected by the user, are represented by using the corresponding average poly-line. In the visual abstraction scheme presented in Novotn [2004], clusters of poly-lines are visualized using their bounding polygons.
In Cui et al. [2006], the authors introduced measures of quality for sampling and clustering, in order to reduce the data to be visualized.

Edge bundles [Holten 2006] and Wavelets [Wong and Bergeron 1996] visualize datasets that contain hierarchical information or adjacency relations for reducing vi-sual clutter.

In Fua et al. [1999a, 1999b], all the poly-lines are displayed; however, the elements belonging to different data clusters are differently colored to help the user discern them better. The colors are selected in a way that reflects the similarities and distances between the clusters. Then, the authors select the data clusters using a hierarchical clustering scheme and thus enable a multiresolutional view of the data (i.e., the user can select how much detail has to be preserved in the visualization).

In this article, we also consider clustering of the data. However, instead of using multidimensional clustering techniques that ignore the impact of the data clustering on the projections of the data on the given data dimensions, we primarily focus on controlling the reductions of the resolutions of the individual data dimensions when the data is clustered. 2.2.4. Other Visualization Enhancements. Graham and Kennedy [2003] introduce var-ious refinements to the PC-based visualization. One of these enhancements is to replace the poly-lines with smooth B-spline curves (as we do in this article 4 ). The ad-vantage of using B-spline curves is that poly-lines that would partially overlap in space are differentiated from each other when plotted as smooth curves and this allows dis-cerning individual data from each other even if they have shared values [Bartels et al. 1995]. This reduces ambiguity. As a downside, however, this increases the number of distinct curves on the visualization space, potentially worsening the visual clutter. To reduce clutter, Zhou et al. [2008] control the curvatures in a way that maximizes parallelism of related data curves. Wong and Bergeron [1997] augment the PC with a low-dimensional overview of the data obtained using principal component analysis. While not reducing the visual clutter, this often helps the user locate and track data clusters more effectively. As we mentioned in the Introduction, in this article, we present the Parallel hierar-chical Coordinates (PhC) technique for visually analyzing large document collections. Unlike traditional tag-or term-clouds, PhC aims to help the user observe not only the frequently occurring terms, but the underlying patterns within this term space (Figure 6). In order to map multidimensional document data on 2D graphical displays in a way that preserves the underlying information and is easy to view and explore, PhC relies on a Parallel Coordinates (PC)-based mapping, where user-selected visu-alization terms are represented as vertical parallel coordinates and each document in the dataset is drawn as a continuous curve across these coordinate lines. In order to further help the user, PhC eliminates visual clutter relying on value clustering hier-archies obtained by analyzing the input set of documents. Based on the user input, the system suggests a clutter reduction strategy in a way that maintains as much in-formation as possible; the user can then use OLAP-like navigational operators, such as drill-down and roll-up, to increase or decrease the hierarchical resolution to better observe data patterns.
In Section 4, we will discuss how PhC minimizes information loss during visual clutter reduction. Before that, however, in this section, we present an example user interaction sequence which describes how PhC helps the user visually analyze a docu-ment collection.  X  Step 1: Data selection. In the first step, the user selects a data source and further focuses the analysis by providing a filter condition 5 .

In our example shown in Figure 7(a), the user has selected the  X  X atrina X  dataset and provided  X  X urricane X  as the filter condition; the system then created and presented the user a term-cloud consisting of frequent terms in the documents that match this filter condition.  X  Step 2: Coordinate selection. Given the term-cloud, the user can select any of these terms for analysis or can provide additi onal terms that do not occur in this term-cloud. In Figure 7(b), the user has selected the terms  X  X overnment X  and  X  X ederal X  and the system created a PC visualization with two parallel coordinates.

Here, each document is represented as a curve that crosses each term coordinate at a point corresponding to the term X  X  frequency in the document (in order to help the visualization, by default only the relevant frequency range is visualized, though the user can optionally select any frequency range for visualization).  X  Step 3: Coordinate selection (cont.). In Figure 7(c), the user has selected two more terms for inclusion in the analysis:  X  X ity X  and  X  X tate X . As a result, the PC visualiza-tion now contains four parallel coordinates.  X  Step 4: Coordinate selection (cont.). The selection can be very large, even though the displayed information becomes difficult to manage. In Figure 7(d), the user has selected an additional three terms for a total of seven terms.
At this point, the user can continue the iteration with the system using several available functionalities.  X  Data reduction through  X  X kyline X  selection. At this point, the user may be uncom-fortable with the amount of curves in the visualization and may want to reduce the clutter.

One way to achieve this is to keep only those documents that are not dominated by any others in the dataset. This set is also known as the skyline set [Borzsonyi et al. 2001].  X  No document in the skyline has a higher value than any other one in the skyline  X  no document in the dataset is in the skyline if there is at least one other document
Intuitively, for text exploration this implies that the skyline set contains documents that are the best representatives of different weight combinations. For example, if the document with weights d 1 = &lt; 0 . 8 , 0 . 6 , 0 . 1 &gt; is in the skyline, then the docu-document d 4 = &lt; 0 . 3 , 0 . 3 , 0 . 4 &gt; on the other represents a document which is rel-atively more dominant in terms of the 3rd dimension in contrast to the first text document, d 1 , and therefore can be in the skyline along with the initial document.
Figure 8(a) shows the skyline set of the documents in the dataset selected by the user (see Figure 7(c)).  X  Data reduction through coordinate filtering. Alternatively, the user can reduce the documents in the visualization shown in Figure 7(c) by putting a lower and/or upper bound on the frequency values of the coordinates of the visualization space. In
Figure 8(b), for example, the user instructed the system to eliminate any documents that have a 0 . 0 value along any of the visualization dimensions (i.e., missing any of the four visualization terms).  X  Reduction of the visualization resolution by document clustering (k = 2). Another way to reduce the clutter in the visualization in Figure 7(c) is to cluster similar documents together. In Figure 9(a), the user instructs PhC to reduce the amount of details in Figure 7(c) by ensuring that each curve in the display corresponds to at least two documents (the user achieves this by bringing the sliding bar wid-get named  X  X  X  to 2). The system selects a value clustering strategy that achieves this effect with the minimal amount of information loss (see Section 4 for more de-tails). As can be seen in the size map at the lower right corner of the interface in
Figure 9(a), the system achieves this by creating document clusters of varying sizes, the smallest having 2 and the largest having 26 documents. In the main window, each of these document clusters are represented using a single curve and the shade of the curve is used to denote the size of the corresponding cluster (the darker the curve, the larger the corresponding document cluster).

Note that, in order to achieve the target document clustering, PhC needed to cluster some of the values along the coordinates. The visualization interface shows the user the clustering strategy selected by the system by marking each value cluster using an ellipse spanning the corresponding value range. The document cluster curves cross the value clusters at the cluster centroids and the width of each ellipse rep-resents the number of document clusters passing through the corresponding value cluster.  X  Selective detail visualization. At any point in time, the user can peek into any of the resulting value clusters to see how the individual curves are distributed within the value cluster. Figure 9(b) shows an example where the user selected the term coordinate  X  X overnment X  and instructed the system to display how precisely the document cluster curves are distributed inside the corresponding value cluster.  X  Curve tracing. The user can select and remove any of the uninteresting document clusters from visualization or can focus onto any of the interesting ones. For exam-ple, as shown in Figure 9(c), at any point, the user can click onto any interesting curve and ask the system to separate it from the crowd for better visualization.  X  Document visualization. The user can also instruct the system to provide more details about the selected document cluster; for example, the system can compute and return a new term-cloud for this document cluster or simply open the documents in the cluster for the user X  X  browsing (Figure 9(d)).  X  Curve group tracing. Alternatively, as shown in Figure 9(e), the user can select a set of document cluster curves to be separa ted from the crowd for better comparison.  X  Curve tracing by size map. The user can also highlight document cluster curves by interacting with the document cluster size map at the bottom right corner. In
Figure 9(f), the user clicked on the dots corresponding to clusters with 16 documents and the corresponding curves are highlighted to the user by the system.  X  Roll-up. Figure 10(a) shows the roll-up operation. Here, the user selected the  X  X ity X  coordinate and instructed the system to climb up on the corresponding hierarchy. As a result, the three value clusters that PhC has selected for visualization are further clustered into two value clusters, corresponding to the clusters at a higher level of the value clustering hierarchy.  X  Drill-down. In contrast, in Figures10(b) and (c), the user drills-down on the  X  X ity X  co-ordinate, thus obtaining smaller and smaller value clusters. In fact, in Figure10(c), the user has drilled-down in the hierarchy all the way to the level of individual data values: in the figure, there are no value clusters on the  X  X ity X  coordinate.  X  Elimination of outliers. Note that not all curves on Figu re10(c) are of the same color: while some curves are shades of gray as before, there are also some curves that are reddish in color. These reddish curves are those that violate the user-provided document clustering lower bound constraint  X  k = 2 X , which requires that all curves correspond to at least 2 documents in the document base. When the user drills-down along a coordinate, she is increasing the resolution along that coordinate and this may lead to the dissolution of some of the document clusters originally selected by
PhC. This may consequently result in outlier documents that cannot be clustered with the rest of the documents at the selected resolution.

In Figure10(d), the user instructs the system to eliminate these outliers from the visualization, keeping in the visualization only those curves that correspond to doc-ument clusters with at least two documents in them.  X  Reducing the resolution of the visualization by increasing the lower bound of the document cluster sizes. In Figures 11(a) to (c), the user varies the value of k (i.e., the lower bound of the document cluster sizes) from 2 to 6. As a result of the reduction in the visualization resolution, the numbers of document cluster curves as well as the value clusters along the visualization coordinates drop.  X  Decreasing the upper bound of the document cluster sizes. The user can also place an upper bound on the number of documents in each document cluster created by PhC and displayed as a curve on the screen. Noticing from the  X  X ize map X  corresponding to k = 2 (lower right corner in Figure 11(a)) that there is one curve representing 26 documents, in Figure 11(d) the user instructs the system (using the sliding bar widget marked as  X  X  X ) to find an alternative clustering strategy where there are no document cluster curves representing more than 20 documents.

As a result, the system selects an alternative clustering strategy which repartitions the values along the  X  X overnment X ,  X  X ity X , and  X  X tate X  coordinates into finer value clusters and identifies and plots new document cluster curves. The new  X  X ize map X  on the lower right corner of the interface in Figure 11(d) shows that, now, the maxi-mum document cluster size is 18.

One consequence of placing an upper bound on the sizes of document clusters along with a lower bound is that (as later explained in Section 4) this may result in clus-ters that have less documents than the user-selected lower bound ( k = 2 in this example). In our example, the lower and upper bounds selected for the document cluster sizes by the user necessitate  X  60% of the documents being identified as out-liers that cannot be clustered with the rest. This is communicated to the user with the slider bar at the lowest left corner of the interface.  X  Increasing the maximum suppression rate. Alternatively, instead of providing an upper bound on the document cluster sizes, the user can allow the system to mark up to a specific portion of the documents as outliers. In Figure 11(e), the user allows the system to consider up to 20% of the documents in the dataset as outliers. This results in a clustering strategy which maintains more details in the visualization while eliminating some of the documents from the visualization as outliers (compare the value clusters along the dimensions in Figures 11(a) and (e)).  X  Changing the visualization coordinates. At any point, the user can drop any of the current visualization coordinates and/or add one or more new terms. In
Figure 12(a), we see that the user has dropped the coordinate  X  X overnment X  from the visualization and has added the new coordinate,  X  X resident X . After this change of coordinate, the user can continue exploring the patterns in the document set along this new set of dimensions.
  X   X  X ormalized X  view. In Figure 12(b), the user has decided to investigate the docu-ments in a normalized document vector space, instead of in the original document vector space. In other words, each document vector v in the 4D visualization space is normalized into v = v | v | . Consequently, any two document curves that have simi-lar keyword compositions, but of are different length (such as 0 . 4 , 0 . 2 , 0 . 1 , 0 . 3 and be clustered with each other. Note that this implies that the document clusters ob-tained by varying the resolution of the visualization space (Figure 12(c) and (d)) will likely contain documents that are similar to each other in terms of relative keyword composition (i.e., in terms of cosine similarity ).
 Note that in the preceding example sequenc e, each parallel coordi nate correspond to a single term. In general, however, multiple terms may be grouped by the user into a combined concept and assigned to a visualization coordinate. Alternatively, as mentioned earlier, a latent analysis algorithm, such as the standard LSA Deerwester [1989, 1990] or Latent Dirichlet Allocation [Song et al. 2009], may be used to identify significant term vectors or topics to be visualized as coordinates.
 As we discussed in Section 2.2, there have been various attempts to leverage clus-tering for visual clutter reduction in parallel-coordinates-based visualizations. It is important to note that tackling visual clutter through filtering or clustering may lead to information loss and what is clustered and how much clustering has been used have to be decided carefully. In fact, a common deficiency of the existing approaches, such as Fua et al. [1999a], is that they focus on clustering of the dataset, ignoring the characteristics of the individual data dimensions/coordinates. In this article, we note that starting from the (hierarchical) clusterings of the values along the individual co-ordinates can help better control the amount of loss in resolution along the individual dimensions of the data.

Thus, to tackle visual clutter, PhC relies on hierarchical clusterings of the values along the visualization coordinates (Figure 5). During the user X  X  exploration of the document collection, PhC often decides for each dimension the most appropriate res-olution based on a user-provided target document clustering rate (see Figures 4, 5, and 9(a)). As described in the earlier section, the user can then interact with the sys-tem to selectively roll-up or drill-down along the hierarchies corresponding to different visualization coordinates to explore the patterns in the document set.

In the rest of this section, we describe ho w the Parallel hierarchical Coordinates (PhC) controls the degree of value and document clustering in such a way that the information loss is minimized. Let A be a user-selected term (or a  X  X oncept X  consisting of a set of terms). The cor-responding value clustering hierarchy (identified using a hierarchical clustering algo-rithm) is a tree H A ( V , E ), where  X  X ach v =( no deid : v alue )  X  V is a node in the tree and v.v alue is either the weight of a document in the dataset for A or is the value range using the value clustering algorithm, and  X  e = v i  X  v j  X  E is a directed edge denoting that the value encoded by the node v j can be clustered under the value encoded by the node v i .
 Given a value hierarchy H A ,atreenode v i is a clustering of a tree node v j , denoted by v  X  v
Note that these value-clustering hierarchies are computed for each of the user-selected visualization coordinates (i.e., terms) before the PhC visualization for the given set it created. In the rest of this section, we present the algorithms PhC uses for reducing the visual clutter in PC, given a target document clustering resolution. The idea is to cluster the values in each dimension in a controlled manner using the given value-clustering hierarchies, in such a way that each poly-line or curve in the resulting PC visualization will group at least k individual documents in the docu ment collection. We refer to this as the k -clustering of the documents.

Intuitively, k is the parameter controlling the degree of resolution of the PC visual-ization. Note that a given document set can be k -clustered in many different ways. For example, a simple (but obviously unacceptable) k -clustering strategy would be using a single poly-line to represent the whole document collection. The challenge is thus to identify a value clustering strategy (i.e., a level in the corresponding hierarchy for each term) in a way that achieves the k -clustering goal, yet loses as little information as possible (see Section 5 for more details on information-loss measures).
To obtain good k -clusterings of the data, we build on the privacy-preserving data publication approaches presented in the literature [LeFevre et al. 2005; Li and Li 2007; Machanavajjhala et al. 2007; Samarati and Sweeney 1998]. Given a table to be pub-lished consisting of sensitive attributes and their values, the goal in these approaches is to limit the amount of data leaked by replacing the specific entries in the database table with more general cluster labels. In k -anonymization 6 problems, for example, the acceptable degree of hiding is defined as the generalization in which each row in the published table is indistinguishable from at least k  X  1 other rows [Ciriani et al. 2007] This k -anonymization approach eliminates the possibility of linkage attacks by ensur-ing that, in the disseminated table, each value combination of attributes is matched to k others. To achieve this, these techniques rely on a priori knowledge about ac-ceptable value generalizations. Thus, most of these algorithms assume that there is a taxonomy associated to each sensitive attribute and that this taxonomy can serve as the value-clustering hierarchy for that attribute. Given that there may be many publishable tables, each providing the same level ( k ) of row hiding, most approaches aim to locate a publishable table that also preserves as much information in the orig-inal table as possible to make sure that the published data table will be of use to its recipient after its generalization. In a low-resolution representation of the data, an internal node of the hierarchy can be used to cluster all the values (i.e., leaves). On the other hand, more general cluster labels will also cause a higher degree of information loss; thus, among all possible clusterings that put each tuple with k  X  1 other ones, Samarati and Sweeney [1998] and many others aim to find those that require minimal generalizations.

Cell generalization schemes [Aggarwal et al. 2005] treat each cell in the data table independently. Thus, different cells for the same attribute (even if they have the same values) may be generalized in a different way. This provides significant flexibility in anonymization, while the problem remains extremely hard (NP-hard [Meyerson and Williams 2004]) and only approximation algorithms are applicable under realistic us-age scenarios [Aggarwal et al. 2005]. Attribute generalization schemes [LeFevre et al. 2005; Samarati 2001] treat all values for a given attribute collectively; that is, all val-ues are generalized using the same unique domain generalization strategy. While the problem remains NP-hard [Meyerson and Williams 2004] (in the number of attributes), this approach saves a significant amount of time in processing and may eliminate the need for using approximation solutions, since it does not need to consider the individ-ual values. Most of these schemes, such as Samarati X  X  original algorithm [Samarati 2001], however, rely on the fact that, for a given attribute, applicable generalizations can be put into a total order of information loss; that is, the higher you go in the hi-erarchy, the more you lose information. More specifically, if there is a generalization at depth d that puts all tuples into clusters of size k , then any other generalization at level d  X  d will also group all tuples into clusters of size at least k , but it will have more loss; conversely, if one can establish that there is no generalization at level d that is a k -clustering, then there is no other clustering of level d &gt; d that can cluster all tuples into clusters of size at least k . LeFevre et al. [2005] leverages this to develop an algorithm which achieves attribute-based k -anonymization one attribute at a time, while pruning unproductive attribute generalization strategies. Samarati [2001], on the other hand, leverages this to develop a binary search scheme to efficiently identify the most specific generalization which guarantees clusters of size at least k :Let D be a dataset and GA = A 1 , ..., A m , be the set of generalization attributes. For each at-tribute in A i , the algorithm takes a value-clustering hierarchy (a taxonomy, T i )which describes the generalization/specialization relationship between the possible values in the domain of the attribute A i . Let the height of T i be h i (1) The algorithm first computes the maximum amount of information loss possible: (2) Max = H and Min = 0 (0 corresponds to the original table, where none of the values (3) The algorithm then considers all possible generalizations that involve a total of (4) Step 3 is repeated is repeated until Max = Min and the k -clustering with the small-In this article, we note that a similar strategy can be used for reducing the visual clut-ter in parallel coordinates-based visualization of text documents. In particular, given a document set and a value-clustering hierarchy for each of the terms selected by the user for visualization, we can locate a k -(document)-clustering strategy that requires the least amount of value generalizations 7 . This way, the algorithm would identify the appropriate resolution for each dimension that collectively minimizes the informa-tion loss while ensuring that each document cluster will contain at least k documents. Figure 9(a) shows the application of k -clustering applied to the PC visualization of document sets.

The computational complexity of the PhC visualization is determined by the under-lying k -clustering process. In our implementation, we use the k -clustering strategy presented in Ciriani et al. [2007], which performs binary search on the levels of the input hierarchy. The k -clustering problem and the specific algorithm [Ciriani et al. 2007] we use are known to be exponential in the number of attributes (i.e., the visu-alization dimensions), but only quadratic in the number of data entries. Moreover as k increases, the complexity of the problem te nds to drop as it is easier to find clusters that satisfy the given clustering target. Since the number of visualization attributes are often small and since the quadratic processing of the data entries (to compute a so-called pairwise distance table can be done offline as a preprocessing step), in prac-tice the runtime cost of PhC is not a major obstacle. When the number of visualization attributes is large, the clusterings may need to be precomputed and cached to support interactive exploration. PhC also leverages caching of k -clustering solutions to ensure that once the clustering is computed, it can be reused throughout the user interaction process. One difficulty with k -clustering is that clustering the outlier documents with the rest of the document to obtain the lower bound cluster size, k , may necessitate high degrees of value clusterings, which in turn would cause document clusters much larger than the desired lower bound, k . This explains the large document cluster with 26 documents in Figure 9(a). Given such large value or document clusters, the user can either drill-down along a selected dimension explicitly or ask the system to try to suppress the outliers from the visualization. 4.3.1. [k, w]-Clustering of Documents. With the basic k -clustering scheme, the user spec-ifies the lower bound clustering constraint k , but no constraints are imposed on the maximum level of clustering. This means that in the visualization we can have poly-lines or curves that correspond to k data elements; more importantly, the number of elements represented by different curves in the visualization may differ significantly from each other. While this variation in cluster sizes may be communicated to the user with visual cues, such as line thickness, PhC also allows the user to place an upper bound constraint on the number of elements captured by each curve. We refer to this as the [ k , w ]-clustering of the data, where k represents the lower bound and w is the upper bound of the clustering rate.

One difficulty with placing an upper bound on the cluster sizes is that there may be situations in which there are no generalizations that can satisfy both lower bound and upper bound constraints. This situation occurs especially when the document set has outliers: Let d be an outlier document, with one or more of the dimension weights significantly different than the rest. Clustering the document d with k  X 1othersmay require increasing the ranges of some of the value clusters so much that, inadvertently, many other documents may fall into this range resulting in a document cluster with asize k . Therefore, given k and w , if no appropriate [ k , w ]-clustering is found, then PhC identifies the minimum number of outlier documents whose suppression will ensure [ k , w ]-clustering of the remaining documents. This is achieved by, if needed, varying the degree of suppression using binary search until a [ k , w ]-clustering is found.
To search for suppressions, we build on a variation of the k -anonymization problem where the user is allowed to specify the maximum number ( maxsup ) of suppressions allowed when an appropriate generalization cannot be found. When the maxsup is specified by the user, the step 3 of the algorithm in the previous step is modified in such a way that the system searches not for k -clusterings of the table, but k -clusterings which have at most maxsup suppressions. Samarati [2001], for example, presents a dy-namic programming-based algorithm that can verify if a given generalization strategy provides a k -clustering with at most maxsup submissions or not in quadratic time.
Given a [ k , w ]-clustering target, PhC first locates a k -clustering and checks the size of the maximum cluster size, if the maximum cluster size is  X  w , then this solution is returned. If the maximum cluster size is greater than w , then the algorithm searches for a k -clustering using binary search, where each iteration a different maxsup rate is considered. Starting from N 2 ,where n is the number of documents, the algorithm considers different suppression rates, each time halving the range and decreasing the target suppression rate when a [ k , w ]-clustering is located and increasing the suppres-sion rate when a [ k , w ]-clustering is not found. Note that this scheme differs from naive binary search in that the algorithm does not stop immediately when a [ k , w ]-clustering is identified; instead it tries to see whether there exist [ k , w ]-clusterings with lower suppression rates. Overall, the algorithm continues O ( logN ) iterations, where N is the size of the document set or until the [ k , w ]-clusterings converge.
Figure 11(d) in the previous section presents an example [2 , 20]-clustering, which results in a maximum document cluster size of 18. Note that, in addition to reducing the size of the largest document cluster, this also helps further partition the values along the visualization dimensions, resulting in more detailed visualizations. Note also that to achieve the desired lower and upper bounds, the algorithm has selected a suppression rate of 60% of the documents. Of course, suppression does not mean that these documents are lost, but only that they are visualized differently (as outliers) than the rest: suppressed documents can either be hidden from the visualization or, as shown in Figure10(c) , they can optionally be included in the visualization in red. 4.3.2. Direct Selection of Suppression Rate. Alternatively, PhC can take the acceptable suppression rate, maxsupp , directly as an input from the user. Given maxsup &gt; 0, the algorithm would identify a value-clustering strategy which would require as little clustering as possible by suppressing up to maxsup many documents. As described in this section, PhC allows the u ser to explore the data at different reso-lutions, specified by a clustering lower bound k and an upper bound w (as well as the outlier ratio suppr ). Within these bounds, the number of documents included in differ-ent document clusters may differ from each other. Similarly, the number of general-ization steps required for obtaining document clusters may also vary from cluster to cluster. Thus, PhC also provides visualization mechanisms to help the user isolate in-dividual document clusters represented based on their sizes/generalizations and study the relationships between these two cluster properties.

One of these mechanisms is a tool called document cluster size map which allows the user to explore document clusters based on their sizes. The cluster size map can be seen at the lower right corner of the user interfaces in Figure 13. For example, in Figure 13(a), the sizes of the document clusters range from 2 to 53 documents. A related exploration tool PhC provides is the document cluster generalization (or loss) map , where each document cluster is visualized in terms of the number of value gen-eralization steps required to obtain this cluster (i.e, the amount of loss in precision).
Figure 13(a) presents an example where one cluster has significantly more docu-ments than the others; as mentioned earlier, this may be due to an outlier or may simply be because there are a lot of entries that have similar values. If the cluster is large and the corresponding loss is also high, this may be due to an outlier in the document set which may call for a high degree of clustering. In contrast, a cluster with a lot of documents, but a low degree of loss would indicate a strong pattern in the dataset (as is the case in the example in Figure 13).
 For an unknown text document collection without any a priori statistics, there is no principled way to pick the very initial k / w values. However, the visualization interface provides exploration widgets and so  X  X luster size X  and  X  X luster generalization X  maps to help the user change k and w values in a way that maintains sufficient detail, with the least visual clutter. While different users can use these tools differently and differ-ent applications may put different emphasis on patterns versus outliers, the common approach is to vary k values until sufficient detail versus clutter trade-off is achieved. If during the process, one recognizes (using the  X  X luster size X  map) that some of the clusters remain very large clusters and this makes it hard to investigate the collec-tion, such clusters are further partitioned by putting a tighter limit on the minimum document cluster size w . As we stated in the Introduction, the goal of PhC is to reduce visual clutter, while preventing information loss as much as possible. In this section, we formalize visual clutter and loss and present alternative quantifiable measures for assessing different PC-based visualization strategies. In Yang et al. [2003], Yang et al. define visual clutter in terms of the number of outliers in the visualization. While this measure is attractive when focusing solely on outliers, in this article we focus on two alternative sources of visual clutter; namely, (a) the line density and (b) crossings of poly-lines or curves in the visualization. 5.1.1. Clutter Due to Line Density. In Tufte [2001], Tufte argues that when visualiz-ing information, for each active point in the screen, there is a visual cost associated. Therefore, the number of active points/pixels should be proportional to the amount of information being represented in the visualization. Thus, given two visualizations that communicate the same information, the one with less active points/pixels is preferable. Based on this intuition, the first measure we define for visual clutter is the number of poly-lines or curves in the visualization. That is, among two PC visualizations that are able to communicate the same patterns (e.g. clusters and outliers) to the user, the one which uses the smaller number of lines or curves is more desirable (Figure 14). 5.1.2. Clutter Due to Line Crossings. Large numbers of line crossings can render pat-terns in the data hard to discern. Consider, for example, two clusters in Figure 3 that are separate in the original space; due to dimensionality reduction, the corresponding poly-lines or curves in the PC visualization may share the same visualization space resulting in large numbers of crossings. As a result, especially when the number of distinct clusters is large, these pattern s may become increasin gly harder to discern (Figure 15). Therefore, our second visual cl utter measure focuses on the line crossings (excluding the crossings on the coordinate-lines due to value sharing). In particular, we use two different line-crossings-based measures.  X  Total number of crossings. This simply counts the total number of crossings in the visualization. Since in the curve-based PC, a pair of curves may cross each other multiple times, we consider both total and unique-total measures (the latter includ-ing each crossing curve pair in the total only once).  X  Document cluster confusion. Two document clusters would be easier to discern if their curves did not intersect in the visualization space. Therefore, given a docu-ment set with m a priori known document clusters, C = &lt; c 1 , c 2 , ..., c m &gt; , we define the corresponding cluster confusion degree as the total number of line crossings, where the crossing lines belong to different document clusters.

Note that for a given k -or[ k , w ]-clustering, the line density and line-crossing mea-sures of clutter are compared to the default PC visualization, which corresponds to k =1. In this section, we present measures that quantify the inadvertent visual loss that occurs during the visual clutter reduction process. 5.2.1. Visual Compression. A drop in the resolution of the visualization will affect the visual information communicated to the user. We measure this loss in the visualization in terms of the average compression in the visual distances among the curves.
Let do c 1 and do c 2 be two documents. The curves corresponding to do c 1 and do c 2 (along with the parallel coordinates that are at the end points) will define a closed space in the PC space. Relying on the observation that the amount of visual informa-tion conveyed by a shape on a 2D graph is proportional to the space (area) covered by it [Tufte 2001], we define the visual information conveyed by these two curves (i.e., sponding to do c 1 and do c 2 (Figure 16(a)).

Let D be a set of documents, v isual () denote the function that returns visual dis-tances in the original high-resolution (low clustering) visualization, and v isual () denote the visual distances in the low-resolution (high clustering) visualization. Given these, the amount of visual compression in the visualization is defined as Intuitively, X % visual compression implies that curves are perceived to be X %closer in the average to each other than they really are after the clustering (Figure 16(b)). 5.2.2. Value-Normalized Visual Compression. Note that the preceding visual diversity measure does not consider the actual values that are being visualized to the user. One can argue that the visual diversity is more important when the documents are diverse; that is, if the document set does not contain diverse documents, then the visualization does not need to be diverse either. Let us define the value difference between two documents do c 1 and do c 2 as Note that the term v al , i can be defined in different ways. One possible definition is in terms of the absolute difference between the values of the corresponding terms (i.e., L1-distance). Since it considers the absolute difference between the term values, this definition does not take into account the value distribution. Alternatively, one can consider the struc-ture of the value-clustering hierarchies and define v al , i as where hier , i ( do c 1 , do c 2 ) is the amount of loss in precision (as defined in Section 4.1) needed to cluster the values do c 1 . term i and do c 2 . term i into the range represented by their closest common ancestor in the value clustering hierarchy.

Alternatively, one can define v al ( do c 1 , do c 2 ) directly using a dot-product-based in-terpretation of document similarity.
Given the appropriate definition of v al , we define value-normalized compression in visual diversity as In this section, we study the visual clutter and loss behaviors of the proposed PhC scheme. For this purpose, we use two datasets.  X  Katrina news dataset , which was also used in our past work [Di Caro et al. 2008]: the dataset contains 750 hurricane Katrina-related news articles, published between
August 25, 2005 and February 26, 2008. The reason why we chose this dataset as a case study earlier in the article and for evaluation in this section is that the event has a multitude of, now well-understood, facets, including geographic, humanitar-ian, economic (e.g., employment-and energy-related), and political (local, regional, and federal) aspects. In our experiments we used the filter and term context shown in Figure 7(c) (keyword  X  X urricane X  as filt er, with the coordinates  X  X overnment X ,  X  X ederal X ,  X  X ity X , and  X  X tate X ).  X  NSF abstracts dataset , which contains 1000 abstracts of National Science Founda-tion (NSF) funded research 8 . This dataset contains abstracts that describe a diverse spectrum of scientific research topics. Furthermore, given the interdisciplinary na-ture of most NSF funded research, the dataset also provides opportunities to inves-tigate interrelationships between different research areas. In our experiments we used filter and term context shown in Figure 2 (keyword  X  X cean X  as filter, with the coordinates:  X  X ystem X ,  X  X odel X , and  X  X rocess X ).
 For each user-selected visualization coordinate (i.e., term), we have created the corresponding value-clustering hierarchy by recursively splitting the value range [0 , 1] using EM clustering of the values along that coordinate in the selected dataset [Dempster et al. 1977], until no further splitting is possible or until a predetermined depth is reached 9 . Figure 17 shows the drop in the values of the various visual clutter measures intro-duced in Section 5 (see Table I for the acronyms) as a function of the visualization resolution (i.e., minimum document cluster size k ) selected by the user. As can be seen here, in both datasets, the amount of clutter, especially the number of line cross-ings, can be reduced multiple ord ers by using even relatively low k values, such as k = 2. While the absolute value of the reduction is data dependent, the results are very similar for both datasets and highlight the fact that one can achieve less cluttered visualizations of the data even with low reductions in resolution.
The quick flattening of the curves in Figure 17 indicates that a significant portion of the visual clutter can be eliminated using only small degrees ( k ) of clustering, as noninformative line crossings are quickly removed from the PhC visualization. In order to study the impact of low-resolution visualizations on the visual loss, we consider the loss measures presented in Section 5.2. Figure 18 shows the values of the various loss measures as function of the minimum document cluster size lower bound ( k ) selected by the user. As can be seen here, despite the multiple orders of reductions in visual clutter as k increases (see Figure 17), the amount of compression in visual distances grows much slower. This indicates that the generalization scheme presented in this article is able to remove visual clutter while maintaining the major patterns intact. Especially in terms of pure visual compression and compression normalized based on L1 interpretation of document dist ances, the amount of visual compression is only  X  20; that is, on the average, curves are perceived to be 20% closer to each other than they really are. When hierarchy-or dot-product-based document distances are considered, the amount of value-normalized visual loss is relatively higher, but still on the average, curves are perceived to be only  X  30%  X  40% closer to each other than they really are. Most importantly, all four measures stabilize beyond a small level of k , indicating that the stable patterns with only limited information loss emerge as k increases. Figure 19 shows the impact of providing a w upper bound (i.e., using [ k , w ]-clustering instead of k -clustering; note that in these charts w = 100 corresponds to the default case where no upper bound is provided).

As Figures 19(a) and (b) show, providing looser (i.e, higher) upper bounds on the document cluster sizes tends to reduce the line density as well as the crossings, thus eliminating visual clutter. However, as can be seen in Figures 19(c) and (d), this also corresponds to relatively hi gh visual compression rates.

One way to reduce the loss due to visual compression is to provide tighter document cluster size upper bounds. As Figures 19(c) and (d) show, an upper bound of  X  25% is able to reduce the visual compression rate significantly (in some cases very close to 0 . 0%), without causing significant document suppression ( &lt; 5% in these experi-ments)). A quick look at Figures 16(a) and (b) also confirms that the amount of visual clutter is still very low at 25% upper bound rate.

Further reducing the document cluster size upper bound has three undesirable impacts: First of all, the rate of document suppression jumps to significant rates. Secondly, the resulting increase in the number of document clusters would imply that line density and other visual clutter measures, including the line crossings, would in-crease. Finally, pushing the document cluster size upper bound further down than 25% may in fact cause a rebound in the information loss (visual compression) as, after the resulting suppression of a large portion of outlier documents, the system would try to merge as many of the new clusters of documents as possible to resist the increase in visual clutter. We conducted a series of user studies to understand whether (and why) our proposed system, Parallel hierarchical Coordinates (PhC) is effective in helping users observe and understand text corpora.

The participants that were involved in this study had different technical back-grounds. None of them was an expert in the data visualization domain and they were not part of the team that has developed PhC. The study is divided in four sets of experiments. (1) subjective assessment of the usefulness of PC-based approaches for the exploration (2) task-oriented assessment of the effectiveness of the proposed approach in commu-(3) assessment of the effectiveness of our proposed scheme PhC with respect to the (4) verification of the information loss measures used for the analytical evaluation
Depending on the particular evaluation goal, we have used both subjective and task-oriented evaluation strategies. In subjective studies, users were asked to respond to a series questions about PC, PhC, and other visualization schemes; Table II lists the rubric scale used in the subjective studies.

For these studies, again depending on the particular evaluation goal, we used both real and synthetic data. If specified otherwise, the studies were done on the Katrina news dataset described earlier. Although PC visualization [Inselberg and Dimsdale 1990] is not our contribution, our first goal was to verify whether using a PC-based scheme for visualizing data collec-tions is indeed the right strategy. 7.1.1. Subjective Assessment. Therefore, this subjective user study aimed at assessing the effectiveness of PC-based visualizations for the exploration of collections. For this study, we constructed a synthetic dataset with 4 visualization attributes and 10 data entries. The data entries were created such that they form two different clusters: chosen according to a normal distribution with mean 0 . 5 and variance 0 . 5, while the where v i , j is a normally distributed random variable with mean c i , j and variance 0 . 2.
The participants were presented with thre e scenarios, each scenario were visualized using Radviz [Hoffman et al. 1997], matrix-oriented visualization[Keim 2002], and PC-based schemes (see Figure 20 for samples), and the participants were asked to respond to the following three questions for each case. ( Q1 ) Are you able to observe and discriminate the relationships (e.g., similarity, dif-( Q2 ) Are you able to observe and discriminate the relationships (e.g., similarity, dif-( Q3 ) How intuitive do you think this tool is for exploring the relationships among the
The participants were classified in advance into two groups based on their expertise in databases and data mining. Even though none of the participants was an expert in data visualization, 8 of the participants (which we refer to as experts )hadexperi-ence in data management, while 13 users (which we call nonexperts ) did not have any knowledge in the data management area.

The results, reported in Table III, indica te a major difference between nonexperts in data management and experts. According to t hese results, subjectively, nonexperts preferred Radviz or Matrix visualizations, whereas (again subjectively) data manage-ment experts did not prefer these visualization schemes. This, we believe, was the case because (while a PC-based visualization scheme looks less familiar to users at the first sight) data managements experts are able to perceive patterns in the data using a PC-based approach better than they do with Radviz or Matrix. 7.1.2. Task-Oriented Assessment. We next considered task-oriented verification of the effectiveness of the proposed visualization approach. In particular we focused on tasks involving identification of the numbers of clusters in the data. For this pur-pose, we again considered three multidimensional data visualization techniques: the pixel-oriented matrix visualization [Keim 2002], Radviz [Hoffman et al. 1997], and our proposal.

In order to better observe the advantages and disadvantages of different schemes in helping users identify the data clusters, we focused on challenging situations with similar, difficult to distinguish clusters coexisting in the dataset. In particular, we considered three types of data distribution scenarios.

Single-shift scenarios. This scenario consists of data clusters that differ from each
Equiratio scenarios. In this scenario, the data clusters have similar compositions
Polar equidistance scenarios. This set of scenarios specifically focuses on the Rad-
We constructed two scenarios (with two and three clusters respectively) for each of these types for a total of six datasets. Given a scenario with n cluster centroids and 4 visualization attributes, we generated m = 50 data entries for each centroid such that they form n different clusters around the 4-dimensional cluster centroids c v j is a normally distributed random variable with mean c i , j and variance 0
We finally built three cases of six random visualizations each, and each of the 18 participants was asked to work only on one of them. The participants were presented with the three visualizations of different scenarios (sequentially in random order) and asked to identify the numbers clusters in the data. The random order ensured data participants could not guess the number of clusters for one visualization using the hints from a previous visualization for the same scenario. The participants were also not told which was our contribution.

Table IV shows the results of the study in terms of the percentage of cases where the number of clusters were correctly identified by the users. As these results demonstrate, PC-based visualizations helped the participants achieve 80% of accuracy on average, whereas the other visualizations were not effective on these difficult scenarios. The single-shift scenarios proved more difficult cases for PC-based schemes; but Radviz and Matrix faired worse even in those scenari os. In the polar equidistance scenarios, we were expecting Radviz to be ineffective a nd this was indeed the case. Interestingly, however, the matrix visualization also fared poorly for those scenarios. Furthermore, in equiratio scenarios users of neither the Radviz nor the Matrix visualizations could identify a single case correctly.

Although, in this set of experiments, our focus was to objectively evaluate the users X  preferences on difficult cases, we also asked each participant to provide subjective feedback through an exit questionnaire. The users were asked to give a rating from 1 to 5 (see Table II) to questions about the ease and the effectiveness of the three schemes. The results reported in Table V confirm our previous observations about the user perception and the effectiveness of the three schemes. After confirming the potential of PC for data collection visualization, our next goal was to see whether the proposed PhC approach provides additional benefits. For this purpose, we ran a second set of subjective s tudies where participants were asked to compare standard PC visualization with PhC using the same document collection. For this purpose, we selected three combinati ons of terms which express three different facets of the discussion on events related to hurricane Katrina.

Case 1  X  terms: bush, state, louisiana. Analysis of the news about the relationships be-tween the government and the management of the emergency in the state of Louisiana.
Case 2  X  terms: hurricane, damage, louisiana, mexico. Analysis of the news about the geographic extent of the physical damages of the hurricane.

Case 3  X  terms: hurricane, oil, price, production, gas, energy. Analysis of the news about the implications of the hurricane on oil and energy production in the region.
For each of the aforesaid cases, we presented to the participants the standard PC visualization of the data as well as the k -generalized version (where k =2).Figure21 shows an example. The participants were asked to respond to a set of questions in which they subjectively evaluated the effectiveness of these PhC approach. ( A. ) This clustering preserves the existing relationships between the terms and the ( B. ) Clustering makes the relationships between the terms and the documents easier
Assertion A is to understand if the users feel that the PhC scheme preserves the main patterns in the data. The second assertion, B , provides feedback on how effective PhC is in visualizing such patterns with respect to the standard PC approach. Again, for both cases (or contexts), the users had to provide a score (from 1 to 5 as described in Table II).

The average user ratings for both assertions are shown in Table VI. These results indicate that, in all cases (from the simplest one with three parallel coordinates to the third case with six) the users were positive (between  X  X aybe yes X  and  X  X  think so X ) in the validities of both assertions. As expected, given that there is some information loss in the generalization process, it is not surprising that the participants were reluctant to give the rating of  X  X es, no doubt about it X . Therefore, in the next study we will assess whether the information loss versus visual clutter reduction results that showed the advantages of PhC over PC were based on valid loss measures.
 In Section 6, we had observed experimentally that PhC was highly effective in reducing visual clutter while minimizing visual loss. These results were, on the other hand, based on loss measures presented in Section 5 and were based on the assumption (common in the literature [Koffka 1999]) that the area between the curves on the display reflected the distance perceived between the curves. In this set of experiments, we validate whether the distances between the curves on the PhC display are indeed correlated to the distances use rs perceive when provided with the original data entries.
In this study, we primarily focused on observing the correlation between: (a) the 21 participants X  assessments of the curve similarities and (b) the similarity assessments of the curves based on the area-based distance measure described in Section 5. We provided each one of the participants the data curve plots shown in Figure 22 and asked them to select the curve (among the black ones) that appears to be more similar to the red curve. The results has shown that similarity assessments were very highly correlated (correlation  X  1 . 0). This supports the appropriateness of the distance (and consequently the loss) measures we used in our experiments.

Secondly, we also considered whether participants X  similarity assessments for the curves are correlated well with their similarity assessments if they are provided the pure data. For this purpose, we also provided them the three data tables, shown in Figure 23, corresponding to the these curves and asked them to pick among the two candidate data entries the one that they think is most similar to the highlighted data entry. In this study, 20 out of the 21 participants made similar similarity selections when given the data tables and when given the corresponding curve plots. Only one participant made a different selection in one of the tables for the two schemes (the participant selected the second row for Table 3 and the curve B for the plot PC 3 ;see Figures 22 and 23). Along with the previous results showing the effectiveness of the curve-based visualizations, this supports the observation that the curves used in PhC have the potential to correctly capture the data similarity/distance judgments of users. In this article, we presented a new visualization mechanism, called Parallel hierarchi-cal coordinates (or PhC) for supporting the visualization and exploration of document collections. At its core, PhC relies on a parallel-coordinates-based approach, where multidimensional vectors are mapped onto a 2D space in such a way that documents with similar term frequencies are represented as similar poly-lines or curves in the visualization space. PhC associates a value-clustering hierarchy to each visualization coordinate (e.g., term provided by a user) and leverages these hierarchies to reduce visual clutter in the visualization with minimal information loss. The user can then interact with the system to selectively roll-up or drill-down along the different visu-alization coordinates to explore the patterns in the document set, without being over-loaded with visual clutter.

In our future works, we will consider the selection of the dimensions to be used for generalization. We also plan to investigate the impact of the number of dimen-sions used for generalization and their ordering on the reduction of visual clutter and information loss and carry out user studies designed to investigate the effectiveness of the proposed approach and its future extensions also in higher-level tasks (e.g., pattern seeking).

