 1. Introduction
The quality of queries submitted to information retrieval (IR) systems directly affects the quality of search would appear perfectly legitimate and understandable to a human being. Unfortunately, in a large number of cases such queries are not handled well by the search engine. While users generally have a model of their infor-mation need, they have little or no knowledge about how the underlying IR system works. This lack of knowl-edge is usually coupled with another unknown: the contents of the collection being searched. A disconnect thus exists between what users enter as queries and the ideal representation required to retrieve the documents they want ( Nordlie, 1999 ).

In this paper we are interested in two types of queries, those that are long and those that are short. Shorter queries are more pervasive than longer ones  X  especially in the web domain. The average query length is around 2.3 words ( Spink, Jansen, Wolfram, &amp; Saracevic, 2002 ), and poses the challenge of understanding the user X  X  information need from such a limited expression of it (Example:  X  X  X urricane Katrina impact  X  ). Longer queries are encouraged by search engines such as Y!Q beta this type of querying (Example:  X  X  X hat was the economic impact of Hurricane Katrina on Mississippi?  X  ) is that longer queries provide more information in the form of context ( Kraft, Chang, Maghoul, &amp; Kumar, 2006 ), and this additional information can be leveraged to provide a better search experience. However, handling such queries is a challenge too. Longer queries contain a number of extraneous terms  X  terms that a user believes are vital to conveying her information need, but in reality hurt retrieval performance due to the way they are handled by the underlying retrieval model.

We are interested in exploring automatic and interactive ways by which we can adapt IR systems to effec-tively and efficiently handle queries, short or long, submitted by users. While a number of techniques for adap-queries input by the user. Modifications to queries, either in the form of expansion or relaxation, have been widely studied and known to contribute to significant improvements in performance. Automatic query expan-sion (AQE) refers to the process of including related terms to the original query, while automatic query relax-ation (AQR) refers to the dropping or down-weighting of terms from the original query. While the former is of query terms obtained by relaxation or expansion, improvements in performance up to 40% and 30% respec-tively are achievable. Targeting this potential improvement we explored automatic techniques (Section 3) for identifying the ideal set of terms for relaxation and expansion. As the results of our experiments in Section 5 show, automatic techniques fail to identify the ideal set of terms for adaptation. To overcome that we explore their utility when used in conjuction with guidance from the user, i.e. adaptation with some simple user inter-action. We refer to the interactive versions of expansion and relaxation as interactive query expansion (IQE) of these techniques in adapting to the user X  X  query.

Analysis of those results reveals that overall (average) improvements in performance are attributable to to improvements only for a subset of queries. Frequently, none of the options selected by the automatic pro-cedures and presented to the user were any better than the original query.

We believe that systems should be able to anticipate and adapt to the situation discussed above, that invok-ing user interaction should be done in a judicious manner. Forcing the user to interact during every query ses-sion irrespective of whether there is utility in doing so can degrade overall user experience, and lead to increased cognitive load ( Bruza, McArthur, &amp; Dennis, 1998 ).

In Section 8 we present an expanded version of our previous work ( Kumaran &amp; Allan, 2007b ) on determin-ing when to interact with the user to obtain explicit feedback. We consider two settings  X  IQR and IQE. We base the decision to interact on the potential for improved performance with user involvement. Our approach is to examine the properties of the set of options presented to the user. We hypothesize that useful sets of options will have distinguishing features from non-useful ones. We also believe that although such a method better search experience.

In this study we motivate the need for and utility of adapting to users X  queries, develop automatic techniques for such adaptation, demonstrate the utility of involving the user in adaptation, and develop techniques to enable systems to identify and adapt to situations where user interaction is futile. 2. Motivation
In this section we provide illustrative examples, 3 one each for relaxation and expansion, to show the utility of Query relaxation : Choosing a good subset (sub-query) from a long query X  X  terms.

Query expansion : Choosing a good subset (expansion subset) of the original set of terms suggested by an automatic query expansion procedure for a short query.

The queries used in the Text REtrieval Conference (TREC) ad hoc tracks consist of title, description and narrative sections, of progressively increasing length. The title, of length ranging from a single term to four the same information need. Almost all research on the TREC ad hoc retrieval track reports results using only we used the description as a surrogate for a long query, and the title as one for a short query. 2.1. Query relaxation Consider the following long (description) query for TREC Topic 324: Define Argentine and British international relations .

When this query was issued to a search engine, the average precision (AP, Section 4) of the results was 0.424. When we selected subsets of terms ( sub-queries ) from the query, and ran them as distinct queries, the performance as shown in Table 1 . It can be observed that there are seven different ways of re-writing the ori-ginal query to attain better performance. The best query, also among the shortest, did not have a natural-lan-guage flavor to it. It however had an effectiveness almost 50% more than the original query, showing the immense potential for query relaxation. 2.2. Query expansion Consider the following short (title) query for TREC Topic 370: Food drug law When we expand this query with 25 terms obtained through pseudo-relevance feedback (PRF) ( Lavrenko &amp; Croft, 2001 ), we obtain an AP of 0.145 compared to an AP of 0.110 when the original query alone was used.
However, if instead of just using all 25 terms, we considered all subsets and ran them as distinct queries, we observed that a large fraction of them performed much better than simple PRF. In Table 2 we can see that certain expansion subsets can lead to a 300% improvement in performance for this query. This motivates an exploration of techniques to automatically identify such expansion subsets. 2.3. Analysis
We analyzed the relationship of terms in the sub-queries and expansion subsets with the original query. We made the following observations that informed techniques to identify good sub-queries and expansion subsets. 1. Terms in the original query that a human would consider vital to convey the type of information desired were missing from the best sub-queries. For example, the best sub-query for the example was Britain Argen-tina , omitting any reference to international relations. This also reveals a mismatch between the user X  X  query and the way terms occurred in the corpus, and suggests that an approximate query could at times be a bet-ter starting point for search. 2. The sub-query would often contain only terms that a human would consider vital to the query while the original query would also (naturally) contain them, albeit weighted lower with respect to other terms. This is a common problem ( Harman &amp; Buckley, 2004 ), and the focus of efforts to isolate the key concept terms in queries ( Buckley, Mitra, Walz, &amp; Cardie, 2000; Allan et al., 1996 ). 3. Good sub-queries were missing many of the noise terms found in the original query. Ideally the retrieval model would weight them lower, but dropping them completely from the query appeared to be more effective. 4. Sub-queries a human would consider as an incomplete expression of information need sometimes per-formed better than the original query. Our example illustrates this point. 5. Smaller-sized expansion subsets led to higher gains in performance. A few key terms were enough to boost performance; more terms only reduced the quality of the query. 6. Terms that would ordinarily be regarded as stop words sometimes proved more useful than content words.
Given the above empirical observations, we explored a variety of procedures to automatically adapt the system to the user X  X  query. We expected that a good query would have the following properties.
A. Minimal Cardinality : Any set that contained more than the minimum number of terms to retrieve rele-
B. Coherency : The terms that constituted the sub-query should be coherent, i.e. they should buttress each 3. Automatic selection techniques Our goal is to identify a subset of the original query (for query relaxation) or expanded set (for expansion). the entire universe of subsets using a measure based on co-occurrence of terms constituting each subset, and selected the one with the best score. We not list the measures based on term co-occurrence we investigated. 3.1. Mutual information Let X and Y be two random variables, with joint distribution P  X  x ; y  X  and marginal distributions P  X  x  X  and P  X  y  X  respectively. The mutual information is then defined as joint entropy. Intuitively, mutual information measures the information about X that is shared by Y .If X and
Y are independent, then X contains no information about Y and vice versa and hence their mutual informa-tion is zero. Mutual Information is attractive because it is not only easy to compute, but also takes into con-sideration corpus statistics and semantics. The mutual information between two terms ( Church &amp; Hanks, 1989 ) can be calculated using Eq. 2.
 where n  X  x ; y  X  is the number of times terms x and y occurred within a term window of 100 terms across the corpus, while n  X  x  X  and n  X  y  X  are the frequencies of x and y in the collection of size N terms.
To tackle the situation where we have an arbitrary number of variables (terms) we extend the two-variable case to the multivariate case. The extension, called multivariate mutual information (MVMI) can be general-ized from Eq. 1 to
The calculation of multivariate information using Eq. 3 was very cumbersome, and we instead worked with the approximation ( Kern, Pattichis, &amp; Stearns, 2003 ) given below.

For the case involving multiple terms, we calculated MVMI as the sum of the pair-wise mutual information for all terms in the candidate sub-query. This can be also viewed as the creation of a completely connected graph G  X  X  V ; E  X  , where the vertices V are the terms and the edges E are weighted using the mutual informa-tion between the vertices they connect.

To select a score representative of the quality of a sub-query or expansion set we considered several options including the sum, average, median and minimum of the edge weights. We performed experiments on a set of candidate queries to determine how well each of these measures tracked AP, and found that the average worked best. We refer to the selection procedure using the average score as Average . 3.2. Maximum spanning tree
It is well-known that an average is easily skewed by outliers. In other words, the existence of one or more terms that have low mutual information with every other term could potentially distort results. This problem could be further compounded by the fact that mutual information measured using Eq. 2 could have a negative value. We attempted to tackle this problem by creating a maximum spanning tree (MaxST) over the fully con-nected graph G , and using the weight of the identified tree as a measure of the candidate query X  X  quality ( van
Rijsbergen, 1979 ). We used Kruskal X  X  minimum spanning tree ( Cormen, Leiserson, Rivest, &amp; Stein, 2001 ) algorithm after negating the edge weights to obtain a MaxST. We refer to the selection procedure using the weight of the maximum spanning tree as MaxST . 3.3. Named entities
Named entities (names of persons, places, organizations, dates, etc.) are known to play an important anchor role in many information retrieval applications. In our example for query relaxation, sub-queries with-out Britain or Argentina will not be effective even though the mutual information score of the other two terms international and relations might indicate otherwise. We experimented with another version of sub-query selec-tion that considered only sub-queries that retained at least one of the named entities from the original query.
We refer to the variants for query relaxation that retained named entities as NE _ Average and NE _ MasT . For query expansion, we did not pursue expansion by only named entities. 4. Experimental setup
We used version 2.5 of the Indri search engine, developed as part of the Lemur network-based retrieval framework of Indri permits the use of structured queries, the use of language model-ing techniques provides better estimates of probabilities for query evaluation. The pseudo-relevance feedback mechanism we used is based on relevance models ( Lavrenko &amp; Croft, 2001 ).

To extract named entities from long queries, we used BBN Identifinder ( Bikel, Schwartz, &amp; Weischedel, 1999 ). The named entities identified were of type Person , Location , Organization , Date , and Time .
We used the TREC Robust 2004, Robust 2005 ( Voorhees, 2006 ), TREC 5 ad hoc ( Voorhees &amp; Harman, 1996 ) and HARD 2003 ( Allan, 2003 ) document collections for our experiments. The 2004 Robust collection contains around half a million documents from the Financial Times, the Federal Register, the LA Times, and
FBIS. The Robust 2005 collection is the one-million document AQUAINT collection. The choice of Robust tracks was motivated by the fact that the associated queries were known to be difficult, and conventional IR techniques were known to fail for a number of them. User interaction held promise for improvement in these collections. The TREC 5 ad hoc collections consists of TREC disks 1 and 2, and presented a standard ad hoc retrieval setting. The HARD 2003 collection, a subset of the AQUAINT corpus and US government corpus containing 372,219 documents in all, was also selected since it was created for a track with focus on user interaction.
 The 50 queries in the Robust 05 data set overlap with those in the Robust 04 data set we used for training.
However, since the collections are different, we do not stand the risk of over-fitting. The HARD data set uses shares neither the queries nor the collection with the Robust 04 data set. We believe that this choice of test data sets will provide a comprehensive validation of our techniques.

We stemmed the collections using the Krovetz stemmer provided as part of Indri, and used a manually-cre-hundred and forty-nine queries from the TREC Robust 2004 track were analyzed to determine and fine tune the procedure we developed to determine the utility of interaction. The remaining 150 queries, 50 each from the three remaining tracks, were used to test the effectiveness of various techniques we present in this paper.

For all systems, we report mean average precision (MAP) and geometric mean average precision (GMAP) (Robertson, 2006 ). MAP is the most widely used measure in Information Retrieval. While precision is the frac-tion of the retrieved documents that are relevant, average precision (AP) is a single value obtained by averag-ing the precision values at each new relevant document observed. MAP is the arithmetic mean of the APs of a set of queries. Similarly, GMAP is the geometric mean of the APs of a set of queries. The GMAP measure is more indicative of performance across an entire set of queries. MAP can be skewed by the presence of a few well-performing queries, and hence is not as good a measure as GMAP from the perspective of measuring comprehensive performance. 5. Automatic adaptation to queries
As mentioned in Section 2, we used the description and title sections of each TREC query as surrogates for the long and short versions of a query respectively. We determined upper bounds on performance on the 249 queries from the TREC 2004 Robust track to provide a target for techniques designed for automatic adapta-tion to queries. 5.1. Upper bounds
To get a sense of the potential utility of query relaxation for long queries and query expansion for short ones, we performed experiments to obtain upper bounds on performance. For query relaxation we ran retrie-val experiments with every combination of query terms from the description portion of 249 Robust 2004 que-ries. For a query of length n , there are an exponential (2 reasons we limited our experiments to combinations of length n 6 12. Table 3 provides an overview of the experimental results. Baseline refers to the situation where we used the description portion of the TREC query without any changes. PRF results in a small improvement over the baseline, i.e. from 0.235 to 0.255. Using the best sub-query of each query (Best Sub-query) resulted in an upper bound in performance almost 40% better than the baseline in terms of MAP, and 100% in GMAP.

Similar experiments for query expansion were performed ( Table 4 ). Our baseline was the PRF-expanded title portion of each TREC query. PRF was performed using the top 25 documents with the number of expan-sion terms set to one hundred. To obtain the upper bound (Best Expansion Subset), we considered all subsets formed experiments by considering all subsets of terms from the top 20 suggested by an automatic query expansion technique. We can observe that selecting the best subset for each query resulted in a MAP improve-ment of 30% and a GMAP improvement of 100%.

As we will notice for other collections too, the potential for improvement in query expansion is less when compared to relaxation. We hypothesize that the reason could be the fact that PRF already provides a com-petitive starting point. 5.2. Automatic adaptation
To evaluate the automatic sub-query or expansion subset selection procedures developed in Section 3 we performed retrieval experiments using the queries selected using them. The results for query relaxation, which are presented in Table 5 , show that the automatic sub-query selection process was just as good as the baseline.
The results of automatic selection were worse than even the baseline, and there was no significant difference between using any of the different sub-query selection procedures. Even in the case of query expansion (Table 6 ), where we used only the MaST technique, the resulting performance was only 2% better than the baseline.

The limited utility of the automatic techniques could be attributed to the fact that we were working with the assumption that a technique designed to favor term co-occurrence could be used to model a user X  X  information performing sub-query from the top ten ranked by each selection procedure ( Table 7 ). While the effectiveness in each case as measured by MAP is not close to the best possible MAP, 0.332, they are all significantly better than the baseline of 0.243. Similarly, in Table 8 we notice a 10% improvement for query expansion. 6. Interactive adaptation to queries
The final results we presented in the last section hinted at a potential for user interaction in the form of IQR and IQE. We envisioned providing the user with a list of the top ten choices made using a good ranking pro-cedure, and asking her to select the option she felt was most appropriate. This additional round of human intervention could potentially compensate for the inability of the selection techniques to select the best sub-query or expansion subset automatically.
 6.1. User interface design Fig. 1 is a screenshot of the interface we provided to annotators to guide the system X  X  adaptation to queries.
For IQR, we displayed the description (the long query) and narrative portion of each TREC query in the inter-face. The narrative was provided to help the participant understand what information the user who issued the designated area. The intention was to provide an example of what would potentially be retrieved with a high rank most general option. A facility to indicate that none of the options were good was also included. 6.2. User interface content issues The two key issues we faced while determining the content of the user interface were
A. Deciding which sub-query selection procedure to use to get the top 10 candidate sub-queries : To determine
B. Displaying context : Simply displaying a list of 10 candidates without any supportive information would 6.3. User study
We conducted an exploratory user study with twelve participants that were a mix of volunteers and paid annotators. The participants were tasked with selecting the best option from a list of ten provided for query relaxation and expansion. They were asked to base their decision on the snippet of text that corresponded to each option. To measure the time it took to complete the task for every query, we instructed the participants to start a timer after they read and understood the query, and just before they started inspecting the options. We used 50 queries from the Robust 2005 track for this study. For query relaxation, we used the description por-tion of each query, and for expansion the title. The baseline for query expansion was a PRF run with number of feedback documents and terms set to fifteen and 20 respectively. 6.4. Query relaxation
Table 11 shows that all participants were able to choose sub-queries that led to an improvement in per-formance over the baseline (description query). This improvement is not only in MAP but also in GMAP, indicating that user interaction helped improve a wider set of queries. Most notable were the improvements in P@5 and P@10. We believe that this was due to the fact that the information participants used for guid-ance was a snippet from the top-ranked document for each sub-query. Selecting an option implied that the participant automatically ensured a relevant document was retrieved in the first position of the ranked list.
The interaction technique we utilized was thus precision-enhancing. Another interesting result, from # sub-queries selected was that participants were able to decide in a large number of cases that re-writing was either not useful for a query, or that none of the options presented to them were better. Showing context appears to have helped. The average time taken by the participants to select an option was a minute and a half. 6.5. Query expansion
Table 12 summarizes the results of our study to evaluate the utility of IQE. While we notice that 50% of participants achieved an improvement in either MAP or GMAP over the baseline expanded title query.
Almost all of them however achieved improvements in P@5 and P@10, a trend noticed in the case of IQR too. This again attested to the fact that the interaction technique we utilized was precision-enhancing. Given that PRF already constitutes a very competitive baseline, and the relatively little room for improvement (see
Upper Bound in each case), we believe it is encouraging that participants registered improved performance over the baseline. We noticed from # sub-queries selected that participants decided in a larger number of cases that expansion was either not useful for a query, or that none of the options presented to them were better. The time taken by the participants to go through the options, read through the snippets and select an option varied from approximately half to one minute. This was much less that the times observed for IQR. The reason was that in the case of IQE a large number of options retrieved the same snippet resulting in the annotators having to read through less text before selecting an option. We hope to use this information to reduce the number of options presented to users as part of future work. 7. Efficiency of interactive adaptation
Our observations from the user study indicate that there were a large number of queries for which users did not select any of the options presented to them. In such cases it might even be preferable to proceed with the original query. Thus, there is clearly a need to develop a procedure to determine the utility of user-guided adaptation. Using such a procedure to determine beforehand the potential utility/futility of invoking user-interaction on a per-query basis will be useful in saving the user time and effort. Determining beforehand if a particular interaction had no potential would also provide a basis for attempting a different interaction mech-anism, if available.
 Fig. 2 sheds light on an interesting aspect of the potential for interactive adaptation to long queries. It shows the distribution of the absolute potential improvements in performance through user interaction across the queries. If one were to consider a minimum improvement of 0.025 to be worth interacting to achieve, then we can see that user interaction for close to 150 queries is unnecessary. The overall improvements in MAP reported in Table 3 masked the minuscule improvements contributed by these queries.

Given this background, we seek to address the question, Given a long query, is it possible to infer the poten-tial utility of invoking user interaction to select a relaxed version of the same query? sion. Analogous to query relaxation, we notice that user interaction for approximately 150 queries is of little utility.
 ing user interaction to select a better set of expansion terms?
We believe that while there is demonstrable gain to be had from involving the user, it is equally important to determine when to bother a user with the request. In following sections we will present techniques we developed towards this goal. We will present the results of simulated user studies. We believe that such studies will enable abstracting away the effects of interface design, experimental methodology, subject experience etc.
We readily acknowledge that these factors might have important ramifications in a deployed system, but believe that their exploration is a natural extension for future work. 8. Adapting to potential interaction utility
There is a large body of previous and related work on procedures to determine the quality of queries ( Zhou &amp; Croft, 2006, Cronen-Townsend, Zhou, &amp; Croft, 2002, 2006 ). The goal of that work was to predict in advance if a query will result in acceptable values of precision, and take appropriate steps if the query was predicted to fail (have a low AP). The procedures were thus tuned to accurately predict MAP. Our goal is different. We wish to determine if the interaction techniques, IQR and IQE, will lead to an improvement in
MAP. From the perspective of a user, expending interaction effort to improve precision from 0.1 to 0.11 is of the same utility as improving precision from 0.8 to 0.81 i.e. little utility. Hence we tuned our procedure to target improvements in MAP, and not just MAP values themselves. 8.1. Predictive features
Our investigation of potential features for predicting improvement was guided by the following hypotheses about potentially good options for interaction. By options, we mean the set of top ten sub-queries or expansion subsets presented to the user. 1. When the original query is very long, a large number of extraneous terms are present that hinder retrieval instead of supporting it. 6 Thus, options that have low average length, or are derived from shorter queries, are potentially better. 2. The average MaST scores of the set of options will be high, indicating a very focused set of queries. 3. The scores of the sub-queries/expansion subsets in the options will be diverse, indicating that they cover different aspects of the query. 8.1.1. Query relaxation
For each query, we started with the top ten sub-queries ranked by NE_MaxST. We used the scores assigned to them by the selection procedure to investigate several features based on measures of central tendency, measures of dispersion, and measures involving query lengths. In this paper we report only those features that had a high coefficient of correlation with MAP.
 Table 13 provides a list of the top four features we found correlating with potential improvements in AP.
The c values in the table are the coefficients of correlation. In this paper, we experiment with using only the first two.

The feature with the highest correlation was original query length (QL). The negative value indicates that high values of initial query length translate to low-quality sub-queries, while lower values of initial query length are predictive of high-quality sub-queries. This is intuitive as identifying all the concepts in longer queries is more difficult. Longer queries also tend to induce more errors into the sub-query ranking procedure. Fig. 4 is a scatter plot of original query size versus potential improvements in AP for the train-ing queries. We can observe a gradual decrease in potential effectiveness as the length of the original query increases. ( He &amp; Ounis, 2004 ) too utilized query length as a feature in their attempts to predict query performance.
 The feature with the second highest correlation was a dimensionless quantity, coefficient of variation (CV): where s x is the standard deviation of a set of samples x likely to contain sub-queries that lead to improvements in AP. This is consistent with our hypothesis that op-tions with varied sub-queries are more likely to cover concepts the user is interested in.

Fig. 5 is a scatter plot of log of coefficient of variation versus potential improvements in AP for all the training queries. We can notice that higher improvements in AP are observed at the higher end of the CV scale. 8.1.2. Query expansion
For each query we considered the top ten expanded queries ranked by MaST. Table 14 lists the features that correlated best with potential improvements. 8.2. Usage scenarios
We believe there is scope for utilizing predictive measures in two different interaction scenarios. The first one is system-centric i.e. the system learns and uses a technique to decide on user interaction each time a user straints on how much interactive effort she is willing to put in. 8.2.1. System-centric
Using training instances we learned a decision function to determine when to interact with a user. The high dispersion observed in Figs. 4 and 5 , and corresponding ones for query expansion (not shown) made the use of machine learning techniques like support vector machines (SVMs) to learn classifiers difficult. We observed that the classifier learned using SVMs used almost every training instance as a support vector i.e. over fitting occurred. With this in view, we decided to apply thresholds to the feature values, and build a simple decision tree. 8.2.1.1. Query relaxation. Table 15 reports the change in potentially achievable MAP as well as the percent-age of queries requiring user interaction when simultaneous threshold sweeps on both features, QL and
CV, were performed. Every MAP value in the table is a statistically significant improvement over the base-line of 0.235. Statistical significance tests were performed using the Wilcoxon signed-rank test, with a set to 0.05.

It is apparent from the table that a wide selection is available for determining appropriate thresholds for the two features. We chose values of 16 for QL, and 2 for CV (see box in Table 15 ). For the training set, it meant obviating interaction for 97 i.e. ((1.0 0.61) * 249) queries in lieu of a 2% reduction in potential MAP improvement. Function 8.1 presents the technique we adopted to determine the utility of interacting with the user in IQR.

Function 8.1 QRPREDICT  X  QL ; CV ; q i  X  8.2.1.2. Query expansion. Table 16 reports the change in potentially achievable MAP and the number of que-ries requiring user interaction as a threshold-sweep is performed on CV. The transition to non-significant improvements over the baseline as the threshold is increased shows the limit to which we can avoid user inter-action without impacting performance seriously.
 Function 8.2 presents the technique we developed to determine the utility of interacting with the user in IQE.

Function 8.2 QEPREDICT  X  CV ; q i  X  8.2.2. User-centric
Consider the scenario where a user presents the system with a set of queries along with a condition that she is willing to only interact say, for x % of the queries. We imagine such a scenario could occur when a time-constrained user is performing exploratory search, for example searching for vacations in Italy , and hence would submit a series of queries to get all the information required. To maximize the benefit from user interaction, it is apt for the system to determine the x % of queries that would have most potential for improvement. We now present some experiments that use the CV values to make those decisions, and study its utility. 8.2.2.1. Query relaxation. We utilized the general trend observed in Fig. 5 to guide the choice of queries to present for interaction. Higher values of potential improvements in AP generally imply higher values of
CV. Our approach was to sort the options in the descending order of CV values, and present them to the user in that order.

Fig. 6 shows the utility of the approach when the user accedes to interact for 10%, 20%, 30% and so on of the query set. The lowest curve shows the gradual improvements with increased user interaction when query subsets are chosen at random for interaction. The highest curve tracks the improvement when the system makes the best choice (highest potential improvement in MAP) on queries for interaction each time. In between the two is the curve that conveys the effect of presenting the options in descending order of CV. While presenting the user with queries in random order. 8.2.2.2. Query expansion. We noticed trends similar to Fig. 5 for query expansion too, and followed the exact same procedure we adopted in the query relaxation case. Fig. 7 depicts similar exploration of the utility of ranking the options by CV before presenting them to the user. 9. Results of selective adaptation
In this section we present results of using our techniques on different sets of queries and collections in the context of IQR and IQE. 9.1. Query relaxation
In Table 17 we provide an overview of results when the system makes a decision to either interact with the user or go with the baseline query. We can see that when selective interaction was performed there was an average drop of 40% in the number of queries the user had to interact with, leading to an average drop in performance of 4.6%. In spite of the reduction, the final MAP was significantly better than the baseline (Wil-coxon text, a = 0.05). However, in the case of Robust 2005 and HARD 2003, there was a significant drop in performance from what would have been achieved if the user interacted with all the queries ( X  X ser Select X ). For a user with only enough time to interact for 60% (or not interact with 40%) of the queries the significant improvement over the baseline is still worth it.

Fig. 8 provides an overview of the performance impact on Robust 2005 as the percentage of interactions is increased. The discrepancy in correspondence between the MAP at 60% interaction in the graph and the value reported in the table is because the latter X  X  ordering of queries involves the second feature QL too. Using the random selection. For the same user with time to spare for 60% of the queries, we can observe that using
CV-based selection helps obtain better performance with the same effort , when compared to randomly select-ing queries. 9.2. Query expansion
The results for our experiments with query expansion are given in Table 18 . Again, we observed statistically significant improvements over the baseline for all three collections. The greatest reduction in the number of queries requiring interaction was for HARD 2003. However the MAP achieved by our system was numerically less than that potentially achieved by interacting with all queries.

Fig. 9 shows the potential gains obtained by increased user interaction on the Robust 2005 corpus. We notice that in the ideal case upper bound performance can be achievable by interacting with only 50% of the queries. In other words there was no utility in interaction for 50% of the queries. This explains the occa-sional  X  X lattening X  of the CV-based selection and Random selection curves. The lower portion of the CV-based selection curve has a higher slope than the upper portion. This indicates that the selection process had done a good job of presenting queries with higher potential ahead of those with less.

For both query relaxation and query expansion for all data sets we can notice small degradations in poten-tial improvements in performance as we avoid interacting with the user for some (percentage of) queries. Thus, there exists a tradeoff that the designer of a system making use of our predictive techniques will need to con-sider. This tradeoff is between the amount of interaction and the degradation in potential improvements in performance, something that the designer needs to tune for based on values of the latter that she considers acceptable. 10. Related work
Our interest in finding a better sub-query or expansion subset that effectively captures the information need is reminiscent of previous work in Buckley et al. (2000) . However, the focus there was more on balancing the effect of query expansion techniques such that different concepts in the query were equally benefited. Previous work ( Shapiro &amp; Taksa, 2003 ) in the web environment attempted to convert a user X  X  natural-language query into one suited for use with web search engines. However, the focus was on merging the results from using different sub-queries, and not selection of a single sub-query. Our approach of re-writing queries could be compared to query reformulation, wherein a user follows up a query with successive reformulations of the original. In the web environment, studies have shown that most users still enter only one or two queries, and conduct limited query reformulation ( Spink et al., 2002 ). We hypothesize that the techniques we have developed will be well-suited for search engines like Ask Jeeves where 50% of the queries are in question for-mat ( Spink &amp; Cenk Ozmultu, 2002 ). More experimentation in the Web domain is required to substantiate this.
Mutual information has been used previously in Church and Hanks (1989) to identify collocations of terms for identifying semantic relationships in text. Experiments were confined to bigrams. The use of MaST over a graph of mutual information values to incorporate the most significant dependencies between terms was first noted in van Rijsbergen (1979) . Extensions can be found in a different field  X  image processing ( Kern et al., 2003 )  X  where multivariate mutual information is frequently used.

Work done by White, Jose, and Ruthven (2005) provided a basis for our decision to show context for user studies. The useful result that top-ranked sentences could be used to guide users towards relevant material helped us design an user interface that the participants found very convenient to use. Borlund and Ingwersen X  X  active information retrieval systems when compared to actual environments influenced the design of our user study.

Determining the quality of queries is a continuing challenge, and especially useful for situations like interactive information retrieval. Cronen-Townsend et al. (2002) developed the clarity measure to serve as a predictive measure for tracking MAP. He and Ounis (2004) and Zhou and Croft (2007) explored a number of pre-retrieval features derived from the query to determine query effectiveness. These include standard devi-ation of the idf scores of the query terms, query length, and query scope. Recent work by Carmel, Yom-Tov, Darlow, and Pelleg (2006) attempts to formalize the query difficulty problem.

Harman (1988) explored the utility of IQE. Her experiments proved that there was utility in interaction, and users found the guidance provided by the system in the form of terms for expansion useful. Magennis and van Rijsbergen (1997) extended these investigations to simulated experiments on a larger scale. The ori-ginal query was expanded using various subsets of predetermined length from the expansion term set.
Through exhaustive experiments, the potential of IQE was determined. This is similar to the upper bound experiments we report in Section 5.1. Ruthven (2003) extended this idea further by examining various query expansion techniques and performing user studies to compare AQE and IQE. His experiments showed that while there is potential for improvement through IQE, realizing the potential in practice is dependent on a number of limiting factors.

Shen and Zhai (2005) presented work that also dealt with the efficiency of user interaction. They per-formed simulated user studies for interaction involving document-level feedback, with the goal of develop-ing procedures that chose the best documents from a pool to present to the user for feedback. The procedures they developed for and results from such active feedback showed that showing users a diverse set of documents was most effective. However unlike our work on query reformulation, they did not extend theirs to determine when to interact with the user, or how to handle a user with time and cognitive load constraints. 11. Conclusions
Our results clearly show there is much to be gained by adapting to users X  queries. While automatic tech-niques to do so are not very effective, involving the user in the process clearly helped. We hypothesize that such adaptation is useful for exploratory search where the user starts off with a more general information need and a looser notion of relevance. Successive rounds of interaction and query modifications are necessary to obtain the information desired. The interactive technique we have presented served as a bridge between the users and the IR system, helping the IR system adapt the users X  queries to the characteristics of the retrieval algorithm and collection. By providing users a preview of the content retrieved by the options, the IR system was able to obtain a sense of the users X  true information need.

We have also discussed an important problem concerning adaptive information retrieval systems. While user interaction is a promising way to improve retrieval effectiveness, its efficiency needs to be considered too. Inefficient interactive systems that force a user to interact on every instance can cause disenchantment. out compromising much on effectiveness. The use of a single feature measuring scatter for both interaction mechanisms implies that interaction mechanisms that provide a wide range of choices have more utility. In other words, showing the user the different parts of the search space her query could lead her to is advantageous. 12. Limitations to our study
We acknowledge several limitations to our study. For instance, the users of our system had to work with pre-have provided greater credence to our conclusions. Our choice of queries, title and description portions of
TREC topics, was motivated by the fact that these were standard test collections with relevance judgments readily available. However, these data sets were not developed with measurement of utility of user interaction navigational and so on extended analysis is not possible. Our users were males and females between the ages of interaction paradigm we have explored involves a single round of interaction. In situations where users did not find any of the options presented to them useful, further rounds of interaction to reformulate or rephrase the query are called for. Exploration of session-based user interaction is planned for future work. 13. Future work
We believe that the work we have presented in this paper could be the starting point for a number of explo-rations. Better techniques to select effective sub-queries and expansion subsets are important. Identifying parts-of-speech in queries and using them preferentially to construct better query alternatives is a current focus. Analyzing the ranking of documents retrieved by different options presented to the user can help deter-mine not only the quality of the options, but also help determine an optimal number of options. We plan to extend the work on selective adaptation to queries towards learning the ideal adaptive technique (expansion or relaxation) in response to a query. We also intend exploring the adaptation techniques we have proposed in Kumaran and Allan (2006) .
 Acknowledgments
We wish to thank the anonymous reviewers of this manuscript for their very helpful comments. This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under Contract Number HR0011-06-C-0023 and in part by NSF Nano
Grant Number DMI-0531171. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.
 References
