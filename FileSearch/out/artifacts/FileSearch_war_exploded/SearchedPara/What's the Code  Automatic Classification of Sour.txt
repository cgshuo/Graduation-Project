
There are various source code archives and open source sites on the World Wide Web. If the programs in such sites are correctly classified and topically components are useful classifications, software reuse would be greatly facilitated. But how are the software programs categorized? In most archives programs are classified according to programming language and application topic. A programmer attempting to organize a collection of programs would most likely categorize resources based on the source code itself, some design specifications and the documentation provided with the program. But to understand which application category the code belongs to, it is very likely the programmer would try to gather natural language resources such as comments and README flies rather than the explicit representation of the algorithm itself. Information in natural language can be extracted from either external documentation such as manuals and specifications or from internal documentation such as comments, function names and variable names. This seems reasonable since algorithms do not clearly reflect human concepts but eomrnents and identifiers do [8]. But to identify the programming language, the programmer or administrator will look at the code itself and distinguish some of the keywords without trying to understand the algorithm. This should be straightforward since almost every language has its own reserved keywords and syntax. However, archives can be large and are rapidly changing, which makes manual categorization of software both costly and time consuming. If our goal is automatic categorization, then we believe it is a good idea to take advantage not only of the natural language information available in documentation but also the code itself. 
Researchers have applied different learning techniques for text categorization: bayesian models, nearest neighbor classifiers, decision trees, support vector machines (SVMs) and neural networks. In text classification, each document in a set is represented as a vector of words. New documents are assigned to predefined categories using textual content. Recently, SVMs have been shown to yield promising results for text categorization [6, 7, 11]. Although programming languages are written in a manner different from natural languages and have some commented information, programming languages have specific keywords and features that can be identified. Using these characteristics we show that text categorization techniques can also be effective for source code classification. To build a classifier our first and maybe most important step is the extraction of features. For programming language classification, our feature set consists of 'tokens' in the source code and/or words in the comments. For the topical classification, we generate our features from words, bigrams, and lexical phrases extracted from comments and README files, and header file names extracted from the source code. We perform feature selection using the expected entropy loss. Each program is then represented as a binary feature vector. For each specific class, we use these vectors to train an SVM classifier. In order to evaluate the effectiveness of our approach, we measure the true-positive and false-positive rates for each class and the overall accuracy. The rest of the paper is organized as follows. Section 2 summarizes background information and related work. In Section 3 we introduce our data set. In section 4 we describe our methodology and algorithm. Results are presented in Section 5 and conclusions are in Section 6. Various software identification and reuse problems explained in the literature vary in terms of the techniques they use and the features of programs they take advantage of. Rosson and Carroll [ 18] examined the reuse of programs for the Smalltalk language and environment. They presented empirical results of the reuse of user interface classes by expert Smalltalk programmers. They observed extensive reuse by users and that the programmers searched implicit specifications for reuse of the target class and evaluated the contextualized information repeatedly. The programmers used and adapted code when the information provided matched their goals. Etzkom and Davis [8] object-oriented software components through understanding comments and identifiers. They found object-oriented code more reusable than functionally-oriented code. Patricia uses a heuristic method deriving information from linguistic aspects of comments and identifiers and from other non-linguistic aspects of object-oriented code such as a class hierarchy. Merkl [16] suggested organizing a library of reusable software components by using self-organizing neural networks. Their approach is based on clustering the software components into groups of semantically similar components. They use keywords automatically extracted from the manual of sottware components. Each component is represented by this set of keywords, which does not include stop word lists. These representations are utilized to build the keywords-components matrix or the vector space model of the data. Each column of the matrix, which corresponds to the software components, is used to train the neural network. Search tools for source code are also impoaant for software reuse. Chen et.al. [4] build a tool called CVSSearch that uses fragments of source code using Concurrent Version Systems (CVS) comments and makes use of the fact that CVS comments describe the lines of code involved. Evaluations of their technique show that CVS comments provide valuable information that complements content based matching. Henninger [10] also studied software e f ----Pr( clf )Ig Pr( C]f )-Pr(C~f )Ig Pr(C~f) ef -= -Pr(CJ, ?)ig Pr(C~j ?)-Pr(C-]f)lg Pr(C-]f) our categories and the number of software programs we used for each from 2 resources. cmcorrs 30 25 DATABASE 31 33 DEVELOPMENT 75 30 GAMES 209 31 GRAPHICS 190 36 NETWORK 270 30 SOUND 222 31 UTILITIES 245 31 
WORD PROCESSORS 11 24 Our system consists of three main components; the feature extractor, vectorizer and the SVM classifier. There are also four supplementary modules, which are necessary for topical classification of programs: the text extractor, filter, phrase extractor and the stemmer. Our system works in two distinct phases: training and testing. Each of the components and phases are explained in the sections below. Examples in each category are considered as the positives and the rest of the examples are counted as negatives. We compute the probabilities for the expected entropy loss of each feature as follows: Pr(~)= 1-Pr(C) Pr~)= l-Pr(f) Pr(C~f)= 1-Pr(~f) Pr(C~f)= 1 -Pr(~f) 
The feature extractor indexes each file and computes the expected entropy loss for each feature. Then the features are sorted by descending expected entropy loss. Some features that appear more frequently in the negative set might also have large expected entropy loss values. We call these features "negative features". 
Thus, a feature that has a higher frequency in the negative set can also be distinguishing for a category. Our feature extractor does not eliminate stop words and can take bigrams into account. It can also consider lexical phrases as features using the output of the phrase extractor module. By default, the feature extractor module data for creating vectors for both the test and the training set. The SVM classifier is trained on vectors generated from the training set in which each document has a class label. It returns the overall accuracy of the classification, which is the percentage of programs that are categorized correctly. We use "LIBSVM -A Library for Support Vector Machines" by Chang and Lin [3], which is integrated software for support vector classification, regression and distribution estimation. LIBSVM is capable of performing cross validation, multi-categorization and using different penalty parameters in the SVM formulation for unbalanced data. LIBSVM uses the "one-against-one" approach [12] for multi-class classification. In the one-against-one approach, k(k-1)/2 classifiers are constructed where k is the number of classes. Each classifier trains data from two different classes. Chang and Lin [3] utilize a voting strategy where each . example is voted against a class or not in each binary classification. At the end the program is assigned to the class with the maximum number of votes. In the case that two classes have the same number of votes, the one with the smaller index is selected. There is also another approach for multi-class classification called "one-against-all". In this technique, the number of SVM models as many as the number of classes are constructed. For each class, the SVM is trained with all the examples in that class as positives and the rest of the examples as negatives. Previous research has shown that one-against-one approach for multi-class categorization outperforms the one-against-all approach[ 17]. 
For programming language classification, we performed our experiments both with the comments included in the code and without comments to ascertain the impact of comments. The programming language class. 
ASP asp, dim, vbscfipt, td, head asp, vbscdpt, dim, td, c\c++ struct, void, sizeof, include, struct, void, ifdef, sizeof, 
FORTRAN subroutine, pgslib, logical, subroutine, logical, pgslib, 
JAVA throws, jboss, java, ejb, lgpl jboss, throws, java, package, LISP defun, lisp, setq, emacs, defun, let, setq, progu, 
MATLAB zeros, -type, denmark, zeros,-name,-type,-string, 
PASCAL unit, sysutils, procedure, implementation, unit, luses, PERL speak, voice, my, said, print my, speak, voice, said, print 
PYTHON def, moinmoin, py, def, moinmoin, py, copying, 
PROLOG pro1og, predicates, diaz, -if, fail, built, bip, atom 
Table 2 lists only the top 5 words when comments are used with the code and when comments are filtered. A minus sign indicates classification, they have a bad effect on identification of Fortran, Matlab, Pascal and Prolog classes. To test our method for topical classification we performed five different experiments on different data sets using combinations of the three types of features: single words, lexical phrases and bigrams. In each experiment, we chose 100 features from each of 11 categories and generated a set of 1100 features. Features were selected according to their expected entropy loss. Table 5 lists the abbreviations of experiments and the feature types used in each experiment. These sets were generated from both the Sourceforge and the Ibiblio archive. EXP. TOP TEN FEATURES EXTRACTED SW Top 100 features from single words 
LP Top I00 features from lexical phrases 20 Top I00 features from bigrams SW2G Top I00 features from single words and bigrams Swl~ Top 100 features from single words and lexical phrases The outputs of the feature extractor were promising for each category. We were able to select the features, from which one can easily guess the corresponding category. For example, Table 6 tabulates the top five words and lexical phrases extracted from the Ibiblio Archive. It is not surprising that we have "calculator" for the mathematics class, "high score" for the games class and "database" for the database class. On the other hand, some of the features are shared among the categories since they have multiple meanings; for example, "play" appears in both the sound and the games classes. Another observation is that the utilities category has more negative features than positive ones. This means that the words like "play" and "client" are unlikely to appear in the utility programs and the "socket.h" library is not included in most of them. To evaluate our classifier and to be able to find the appropriate penalties for training, we first applied 5-fold cross validation to each data set (Sorceforge and Ibiblio) separately and to the combined sets. In 5-fold cross validation, the data is divided into subsets are used for training. We did not use the same archive for both training and testing because the number of examples in some of the categories in an archive were not sufficient. Another factor about our data is that it is unbalanced. For example the number of programs in the word processors category is 11 where it is 270 in the network category. Thus, we used the weighted version of the SVM and changed the penalty parameters (C) for categories. Penalty for each category is computed by multiplying the weights by the specified cost C. We chose the linear kernel function and assigned 100 to C. Table 7 lists the accuracies of the cross validations performed on the Sourceforge, the Ibiblio archive and the on the combined sets for each experiment. We have 2 experiments for the combined sets because one set uses the features extracted from the Ibiblio Linux and the other uses the features extracted from Sourceforge. database, database system, database server, sql statement, method code class library, first item, class hierarchy, global function, header file high score, new game, new level, computer player, map image, independent jpeg, jpeg library, jpeg software, image file plot fimction, radix mode, real numbers, palette change, complex numbers ip address, security fix, error output, backup copy, libc version serial port, modem device, script language, voice modem, incoming data sound driver, cd player, sound card, audio device, track latex command, style sheet, dvi driver, default value, vertical scale the single words. Single words with bigrams are also useful and outperform the other techniques for the last data set. 
Second, we split our combined data set to two subsets and used one subset for training and the other for testing. We used the features extracted from the Linux archive in this experiment. Table 8 shows the true positive and false positive rates and the overall accuracy of the SVM classifier trained by the features from the Ibiblio Archive and tested on the combined set. Similar to the programming language classification, single words when used with bigrams and lexical phrases perform the best on overall. This is also true for each category but the utilities. Between the categories, the database, games, graphics, network and the sound classes performed much better than the other classes. This is again related to the few examples we have in the other classes and the fuzziness of the utilities class. We observe that the utilities class always has a high false positive rate. each experiment using the features from Sourceforge CLASS SW 2G LP SW2G SWLP CIRCUIT 17.86 1.48 25.00 12.8021.43 3.81 32.142.95 25.00 1.84 DATAB. 41.93 0.86 19.35 1.48 16.13 2.83 51.61 1.48 38.71 0.99 DEVEL. 50,94 5.46 26.41 6.98 20.75 6.34 54.71 1.05 37.74 4.95 GAMES 74.17 4.30 39.17 1.32 37.50 12.62 77.504.58 71.67 6.10 GRAPH. 82.30 14.2935.40 8.24 38.05 12.91 70.80 15.8073.45 16.08 MATH 43.33 1.48 36.67 3.33 23.33 1.48 36.67 1.36 43.33 1.23 NET. 80.00 2.60 22.67 1.01 22.00 1.30 46.67 1.44 68.67 2.75 SERIAL. 22.86 1.61 20.00 5,71 22.86 5.46 25.71 1.61 25.71 1.36 SOUND 86.51 4.61 47.62 1.97 57.14 25.59 73.02 7.69 75.40 7.41 UTIL. 38.13 3.70 16.55 2.99 12.95 4.42 23.02 3.28 40.29 4.70 WORD 17.65 1.21 23.53 2.55 23.53 2.67 29.41 2.06 0.00 1.09 P. ACCUR. 64.60% 30.05% 30.88% 52.97% 57.48% In the third step, the classifier was trained with features from the Sourceforge archive and tested on the combined data set. Table 9 shows the accuracy of the SVM classifier for each category. This time experiments on single words have the highest performance. The classes that perform the best do not change for this experiment but the false positive rate of the graphics category is worse than the others. When the two data sets are compared, not surprisingly the accuracy is higher when we train our classifier with features from the Ibiblio archive. Please note that we used the same penalties in each method of the experiments to be able to compare the feature types. However, weights can be different for each feature type to increase the overall accuracy. 
Our experiments show that source code can be accurately classified with respect to programming language and application category. However the accuracy of this classification depends on many factors. The variance of our data, the application categories, the selection of features used, the information retrieval techniques and the programming language all affect performance. 
We demonstrate an SVM based approach to programming language and topic classification of sottware programs. We train our classifier with automatically extracted features from the code, comments and the README files. For programming language classification, these features are tokens in the code and words in the comments. For topical classification, we use words, bigrams and lexical phrases in the comments and README files and header file names in the code as features. We perform feature selection by expected entropy loss values and train SVM classifiers using these features. Though our work shows promising results, there is much to explore, including the choice and number of feature vectors. Using values such as term frequency in the vectors, instead of binaries can improve the performance of our classifier. Our work for programming language classification can also be extended by adding more syntactic features together with the words. We believe that other properties of programrning languages, such as the way comments are included and the tokens used for arithmetic or logical operations, will help in identifying the programming language. 
These results imply that large archive collections of mixed data such as text and source code can effectively be automatically classified and categorized. We feel this will lead to more effective use of code archives and a reduction in duplication of programmer effort. We gratefully acknowledge Gary Flake, Eren Manavoglu and 
Burak Onat for their comments and contributions. [ 1 ] Abramson, N. "Information Theory and Coding." McGraw-[2] Bennett, K. P. and Campbell, C. "Support vector machines: [3] Chang, C. and Lin, C. "LIBSVM: A library for support vector [4] Chen, A., Lee Y. K., Yao A. Y., and Michail A. "Code search conceptual framework for reuse processes, softward technology for adaptable, reliable systems (STARS)" (Technical Report). DARPA, 1992. Intelligent Systems Magazine, Trends and Controversies, 13(4):21-23, 1998. "Inductive learning algorithms and representations for text categorization." Proceedings of the ACM Conference on Information and Knowledge Management, 148-155, 1998. reusable OO legacy code." IEEE Computer, 30(10): 66-71, 1997. Kruger, A., Giles, L. C., and Pennoek, D. M. "Improving category specific web search by learning query modification." 2001), 23-31. San Diego, CA, US: IEEE, 2001. Systems and Software, 30(3): 231-247, 1995. machines." Proceedings of the Tenth European Conference on Machine Learning, 137-142, 1999. learning revisited: a stepwise procedure for building and training a neural network." Neurocomputing: Algorithms, Architectures and Applications. J. Fogelman (Ed.), Springer-Verlag, 1990. Artificial Intelligence, 20, 277-294, 2000. 24(2):131-183, 1992. vector machines." Proc. of the International Conference on Neural Information Processing, 347-351, 1999. organization." Proc. of the IEEE International Conference on Neural Networks, 1086-1091, 1995 margin DAGs for multiclass classification." Advances in Neural Information Processing Systems 12, 547-553. MIT Press, 2000. Smalltalk Programming." ACMTransactions on Computer-Human Interaction, 3(3), 219-253, 1996. selection in text categorization." Proceedings of the (ICML'97), 412-420, 1997. 
