 1. Introduction
Test collections play an important role in the field of information retrieval (IR) because they enable IR researchers to evaluate and compare different retrieval methods in a controlled setting. The construction of these document collections is expensive and time consuming, because of the need to determine the relevance of a large number of documents with respect to a set of search topics. The TREC workshops address this problem by pooling the top 100 documents retrieved by each participating system and then an evaluator deter-mines the relevance of each document in the pool ( Voorhees &amp; Harman, 1999 ). Zobel (1998) has shown that this pooling method leads to reliable results in terms of determining the effectiveness of retrieval systems and their respective rankings, but the relevance determination process is still very resource intensive.
This paper addresses the problem of how to rank retrieval systems in the absence of or without the need for a set of human relevance judgments. The proposed method is easy to compute and utilizes the phenomenon that retrieval systems tend to retrieve similar sets of relevant documents and dissimilar sets of non-relevant documents ( Lee, 1997 ). Specifically, it computes the structure of overlap between the search results of random groupings of five retrieval systems. It will be shown that the percentage of a system X  X  documents that are only found by it and no other system increases as the system retrieval quality decreases, since a poorly performing system tends to find fewer relevant documents, causing its results to overlap less with other systems. This result is then used to demonstrate that the average percentage of a system X  X  documents that are only found by it and no other systems is strongly and negatively correlated with its retrieval effectiveness. Thus, the relative perfor-mance differences between the systems can be inferred based on the percentage of a system X  X  documents that are not found by any of the other systems. An analogy may help to explain the solution presented in this paper: there is a group of professed experts and the task is to rank them based on their long lists of recom-mended documents to a set of information requests without knowing which documents represent relevant rec-ommendations. First, one selects a subset of five experts, compares their list of recommended documents and then counts how many of each expert X  X  documents were only suggested by him or her and nobody else. In order to obtain a representative count, each expert is included an equal number of times in random subsets of five experts. Next, one computes and averages the percentages of an expert X  X  documents that only s/he rec-ommended. Which expert should be  X  X  X rusted X  X  the most and which one the least? This paper proposes that the expert with the lowest percentage of documents that are not recommended by any other expert should com-mand the greatest trust, whereas the expert with the highest percentage of documents only recommended by him or her should be trusted the least. In a way, the proposed method defines and operationalizes  X  X  X uality of expertise X  X  in terms of the degree of consensus or agreement an expert can generate. Since the proposed method does not require knowledge or expertise about the domain being searched or the content of the information being requested, it could be of value to communities that need to identify the best experts or rank them, but do not have the resources to evaluate the experts X  recommendations. This paper will show that only the top 50 documents per information request are needed to infer system rankings of high accuracy in the context of retrieval systems searching the same database.

This paper is organized as follows: first, related work is discussed. Second, the methodology employed is described. Third, the overlap structures between systems of decreasing quality are computed and it is shown that there is a systematic change in the structure of overlap as the retrieval performance of the systems decreases. Fourth, results using data from TREC 3, 6, 7 and 8 are presented that show how the overlap struc-ture between the search results of random groupings of five systems is strongly correlated in a negative way with the relative performance differences between the systems. Finally, the presented work and future research are discussed. 2. Related work
Prior research relevant or related to the work presented falls into two categories: (1) research examining how the overlap between search results is related to the potential relevance of documents; (2) methods pro-posed for ranking retrieval systems in the absence of relevance judgments. 2.1. Identification of relevant documents
Saracevic and Kantor (1988) found that the greater the number of independently created Boolean queries that retrieve the same document, the greater the probability of its relevance. Foltz and Dumais (1992) found similar results when comparing the results obtained by four different methods designed to filter a technical reports database. Belkin, Kantor, Fox, and Shaw (1995) combined different query formulations to create increasingly complex queries that produced a progressively improved retrieval performance. Guided by these results, major data fusion methods use both voting and merging strategies to combine the result sets of differ-ent retrieval systems ( Fox &amp; Shaw, 1994 ). It has been observed that data fusion leads to improved retrieval performance if there is a greater overlap of relevant documents than of non-relevant documents ( Lee, 1997;
McCabe, Chowdhury, Grossman, &amp; Frieder, 1999; Vogt &amp; Cottrell, 1998 ). Specifically, the number of relevant documents found by all systems divided by the number of all relevant documents found needs to be greater than the same ratio for the non-relevant documents ( Lee, 1997 ). Spoerri (2005) has conducted a systematic analysis of the overlap between the results of retrieval systems that participated in TREC 3, 6, 7 and 8.
The analysis showed that the potential relevance of a document increases exponentially as the number of sys-tems retrieving it increases  X  called the Authority Effect. Further, this analysis showed that the higher the posi-tions of a document in the ranked list that contain it and the greater the number of systems that retrieve it, the greater probability of the document being relevant  X  called the Ranking Effect. These two effects suggest that the overlap between search results provides a rich source that can be mined further. A key contribution of this paper is that it shows how the overlap structure  X  the percentage of a system X  X  retrieved documents that are also found by a specific number of other systems  X  can be used to infer the relative performance differences between retrieval systems, significantly improving upon existing methods. 2.2. Ranking retrieval systems
Only a few methods have been proposed to rank retrieval systems without the need for human relevance judgments, because this is a difficult problem, since there does not exist a proven theory for determining exper-tise without explicit relevance or correctness judgments. A contribution of this paper is that it suggests a work-ing hypothesis (and potential theory) for how to rank retrieval systems in the absence of human relevance judgments.

Soboroff, Nicholas, and Cahan (2001) address the problem of how to rank retrieval systems without the need for human relevance judgments by generating a set of pseudo-relevance judgments by randomly selecting and declaring some documents from the pool of top 100 documents as relevant. This set of pseudo-relevance judgments (instead of a set of human relevance judgments) is then used to determine the effectiveness of the retrieval systems. For each topic, the average number of relevant documents in the pool has to be known to decide how many pseudo-relevant documents to select in the pool of top 100 documents. These pseudo-rele-vance judgments are used to estimate the performance of a system, using data from the ad hoc track in TREC 3, 5, 6, 7 and 8. The Kendall X  X  tau correlation with the actual TREC rankings range from 0.37 in TREC-7 to 0.49 in TREC-5, and top-performing systems are ranked together with the poorly performing systems. Sobor-off et al. found that the greatest improvement came from retaining duplicate documents in the pools, which is consistent with the Authority Effect. Aslam and Savell (2003) devised a simple way to measure the similarity between two retrieval systems by computing the ratio of the number of documents in their intersection and union. This measure produced results that were highly correlated with the method using pseudo-relevance judgments, similarly ranking the best systems with poor performers. Aslam and Savell hypothesize that this is caused by a  X  X  X yranny of the masses X  X  effect and that these two related methods are assessing the systems based on  X  X  X opularity X  X  instead of  X  X  X erformance. X  X  The analysis by Spoerri (2005) suggests that the  X  X  X opular-ity X  X  effect is caused by considering all the runs submitted by a retrieval system, instead of only selecting one run per system, which would help to  X  X  X harpen the signal X  X  and make the Authority Effect more dominant.
Wu and Crestani (2001, 2003) have developed two related  X  X  X eference counting X  X  methods to rank retrieval systems without relevance judgments. Both methods assign a score to each retrieved document based on the number of systems that have found it as well as taking into account the rank positions of its duplicates or the scores assigned by the systems. In essence, the  X  X  X eference counting X  X  methods attempt to leverage the Author-ity and Ranking Effects, but they do so only in partial ways, since ad-hoc formulas are used to compute a document X  X  score that not fully reflect the structural properties of these two effects ( Spoerri, 2005 ). Using
TREC 5, 7, 9 and 10 data, the initial method by Wu and Crestani (2001) just counted how many of a system X  X  top documents were also found by a random selection of two to nine other systems. This simple  X  X  X eference count X  X  was used to divide the systems into three categories: good, fair and poor using heuristic thresholds.
The TREC data was used to classify the systems into three categories. If the reference counting approach clas-sified a system as  X  X  X ood X  X  but its TREC category was  X  X  X oor X  X , then this was counted as a  X  X  X ig mistake. X  X  Wu and Crestani ran 50,000 trials using TREC 2001 data, resulting in a mistake rate of 30.4%, and a big mistake rate of 5.2%.

In their second method, Wu and Crestani (2003) developed multiple ways for computing the  X  X  X eference count X  X  to rank retrieval systems and they compared these variations with the ranking method by Soboroff et al., using TREC 3, 5, 6, 7, and 10 data. In Wu and Crestani X  X  implementation of the pseudo-relevance method (PR), 10% of the top 100 documents are declared as relevant, whereas the original method requires the number of relevant documents to be drawn from a normal distribution with a mean and standard devia-tion derived from the actual number of relevant documents per topic in the pool. Further, Wu and Crestani made the distinction between an  X  X  X riginal X  X  document and its duplicates in all other lists, called the  X  X  X eference X  X  documents, when computing a document X  X  score. The basic method (Basic) computes for each  X  X  X riginal X  X  doc-ument how many systems retrieved it. The first variation (V1) assigns different weights to  X  X  X eference X  X  docu-ments based on their ranking positions. The second variation (V2) assigns different weights to the  X  X  X riginal X  X  document based on its ranking position. The third variation (V3) consists of assigning different weights to the  X  X  X eference X  X  documents as well as the  X  X  X riginal X  X  document based on their respective rank positions. The fourth variation (V4) uses the  X  X  X riginal X  X  document X  X  normalized score instead of its ranking position and the  X  X  X ef-erence X  X  documents X  ranking positions to assign the weights. Wu and Crestani also vary the number of systems used to compute the  X  X  X eference count X  X  (ranging from 3 to 20 systems). For each number of systems, they con-struct 20 randomized runs for the 50 search topics, and then average the results. The Spearman rank corre-lation is used to compare their system rankings with the official TREC rankings. If the latter rankings are based on the mean average precision, then the variation V4, which uses the  X  X  X riginal X  X  document X  X  normalized score and the  X  X  X eference X  X  documents X  ranking positions to assign the weights, performed the best among the five methods. However, none of them performed as well as the pseudo-relevance method (PR), which per-formed best for TREC 3 with a Spearman rank correlation of 0.63 and worst for TREC 7 with a correlation value of 0.41.

Wu and Crestani (2003) show that the similarity between the multiple runs submitted by the same retrieval system affect the ranking process. If only one run per systems is selected, then the variation (V3), which takes into account of both the ranking positions of the  X  X  X riginal X  X  and  X  X  X eference X  X  documents, outperforms PR by 45.5% on average for 3 X 9 systems, whereas PR outperforms V3 by 6.5% for 10 and more systems. Further, the accuracy of the ranks assigned to the best performing systems is considerably improved for all methods. This suggests that the similarity between the multiple runs by the same system greatly affects the top end of the rankings. Wu and Crestani find that the  X  X  X eference counting X  X  approach performs best if the inferred system rankings are compared with the TREC rankings based on the systems X  precision@100 measures (followed by
R-Precision, and worst for average mean precision). Further, the Spearman rank correlations of the  X  X  X eference counting X  X  and pseudo-relevance judgments rankings with respect to the official TREC rankings steadily increase as more systems are considered, starting to reach a plateau when random selections of 10 systems are used. 3. Methodology
The TREC workshops provide IR researchers with large document collections, a set of search topics and relevance judgments ( Voorhees &amp; Harman, 1994, 1997, 1998, 1999 ). This makes it possible to compare and analyze the effectiveness of different retrieval methods in a controlled setting. Retrieval systems participating in the ad hoc task search the collections for each of the 50 provided topics, and then submit a ranked list of usually 1000 documents per topic for evaluation (and 50,000 documents in total). The ad hoc task is similar to how a person would search a static and known collection of documents, but the search requests can vary ( Voo-rhees &amp; Harman, 1999 ). As mentioned, for each topic, the top 100 retrieved documents from each run are pooled and then an evaluator determines the relevance of each document in the pool. Each system participat-ing in TREC can submit multiple runs for evaluation. A run can either be automatic or manual . For the for-mer, the query is created without human intervention based on the complete topic statement (called a long run) or only the title and description fields (called a short run). In this paper, the short runs in TREC 3, 6, 7 and 8 are used, because a greater number of systems submitted short runs ( Voorhees &amp; Harman, 1994, 1997, 1998, 1999 ). However, only one short run (i.e., the one with the highest  X  X  X ean average precision X  X , where this per-formance measure will be explained below) is selected for each participating system, because there can be a high degree of similarity between the result sets for runs of the same type and generated by the same system ( Wu &amp; Crestani, 2003 ). This similarity boosts the degree of overlap artificially and thus introduces a source of noise. There are 18 (19), 24, 25 (28) and 35 best short runs for TREC 3, 6, 7 and 8, respectively, that are ranked in this paper. The numbers in brackets indicate the total number of different systems, and some systems were not included in this study because they submitted significantly less than 50,000 documents in total. When sev-eral systems are compared, the overlap between their result sets is computed for each topic. Averaging over all topics, the number of documents found by a specific number of systems is computed. In particular, it is infor-mative to compute the average percentage of a system X  X  documents that are also found by a specific number of multiple systems. For example, if system A retrieves 1000 documents for a topic and this result set is compared with the results of four other systems, then we compute the percentage of A X  X  documents that are found by all five systems, the percentage of A X  X  documents found by four systems and so on, ending with the percentage of A X  X  documents that are only found by the system A itself.

The major measures used to evaluate the performance of the systems participating in TREC are: (1) the mean average precision for all topics, which is equal to the mean of the averages of the precision values after a relevant document is retrieved for each topic; (2) the R-precision , which is the average of the precision values after R documents have been retrieved, where R is the number of relevant documents for each topic; (3) the precision at 1000 , which is the average of precision values at 1000 documents for each topic. These different performance measures tend to be highly correlated. In this paper, the mean average precision and precision at 1000 are used to create the official TREC rankings that are then used to evaluate the effectiveness of the new ranking method presented. 4. Structure of overlap
This paper proposes that the overlap structure between multiple search results can be used to infer the qual-ity of the systems being compared without the need to know which documents are relevant. To better under-stand why the structure of overlap can provide this type of insight, it is instructive to compute the overlap between five  X  X  X onsecutive X  X  systems in TREC 8 that have been ordered based on their mean average precision, starting with the top five systems (1 X 5) and ending with bottom five runs (31 X 35). Fig. 1 (left) shows that on average more than 30% of the documents found by a top five system are also retrieved by all of the top five systems, whereas the bottom five systems find almost no such documents. On average, almost 20% of the doc-uments are retrieved by a single top five system, whereas roughly 70% are found by a single bottom five sys-tem. Similarly, Fig. 1 (middle) shows that on average more than 60% of the relevant documents found by a top five system are also retrieved by all of the top five systems, whereas very few relevant documents are found by all of the bottom five systems. On average, less than 10% of the relevant documents are retrieved by a single top five system, whereas more than 40% are found by a single bottom five system. Finally, Fig. 1 (right) dis-plays the percentage of documents that are relevant and shows that this percentage increases exponentially as the number of systems finding the same document increases.

Fig. 1 shows that the percentages of a system X  X  documents that are found by a specific number of systems changes in a systematic way as the quality of the five systems being compared decreases. Specifically, the greatest percentage of documents is found by all top five systems and then shifts toward the documents retrieved by a single bottom five system as the mean average precision of the systems being compared decreases. This change in percentages is most pronounced for the documents found by a single system or by all systems. This systematic change in the structure of overlap can be used to infer the effectiveness of the methods being fused without the need for relevance judgments. It will be shown that the percentages of a system X  X  documents not found by other systems (Single%) as well as the difference between the percentages of documents found by a single system and all five systems (Single%  X  AllFive%) are highly and negatively correlated with the mean average precision and precision at 1000 scores of the systems.

It has been suggested that different systems searching the same database tend to retrieve similar sets of rel-evant documents but return different sets of non-relevant documents ( Lee, 1997 ). Fig. 1 supports this sugges-tion, because it shows that the degree of overlap decreases as the quality of the systems, and thus the number of relevant documents found, decreases. Fig. 1 shows that the systems ranked 1 X 25 have a large set of relevant documents in common, which only diminishes when the worst performing systems, which find only few rele-vant documents, are compared. 5. Ranking retrieval systems
The question arises whether the systematic change in the overlap structure, which occurs when five  X  X  X on-secutive X  X  systems of increasingly lower retrieval quality are compared, still occurs for random groupings of five systems. First, the overlap structure needs to be computed for a sufficient number of random groupings.
Second, each system needs to be selected an equal number of times. If there are N systems to be ranked, then the randomization process can be constrained so that each system appears five times in the N random group-ings of five systems. Next, for each random grouping and each of the 50 topics, the percentage of documents found by a specific number of systems needs to be computed for each system. These percentage values are then averaged over the 50 topics. Finally, for each system, the N random groupings produce five percentage values (see Fig. 2 ) that can be averaged as well (see Fig. 3 ).

It would seem that the overlap structure should vary greatly if a specific system is compared with different sets of systems, especially if these other systems are mostly top five systems in one comparison, and mostly bottom five systems in another one. In the former, it is expected that a large percentage of a system X  X  docu-ments are found by all five systems, whereas in the latter few documents are expected to be retrieved by all systems. Thus, poorly performing systems can greatly affect the percentage of documents found by all five sys-tems. However, the percentage of documents retrieved by a single system is more robust, because a high per-centage of such documents are not relevant (see Fig. 1 (right)). Fig. 2 (left) displays a scatter plot of the mean average precision scores for the 35 systems in TREC 8 versus all of the five percentage values of a system X  X  documents that are only found by the system itself ( Single% ). This figure shows that there is a strong linear and negative correlation between Single% and mean average precision. Fig. 2 (middle) confirms the expecta-tion that the percentage of a system X  X  documents that are found by all five systems ( AllFive% ) is not such a robust indicator of system quality, because the AllFive% values are very scattered, although an increasing per-centage score is weakly correlated with a system X  X  mean average precision. Fig. 2 (right) displays a scatter plot of the mean average precision scores versus the difference between Single% and AllFive%, and it shows that there is a strong linear and negative correlation, although Single% minus AllFive% is slightly more scattered than Single%.

Next, the multiple percentage values for each system can be averaged. Fig. 3 displays the scatter plots of these average percentages versus the mean average precisions of the 35 TREC 8 systems. The R-squared values for the linear correlations for Single%, AllFive% and Single% minus AllFive% are 0.87, 0.74 and 0.90, respec-tively. The R-squared value for the average of the AllFive% values is greatly improved with respect to its value for all AllFive% values (see Fig. 2 (middle)).

These average percentage values can be used to rank the 35 systems in TREC 8. Fig. 4 displays a scatter plot of the official TREC 8 rankings based on the systems X  mean average precision scores versus the rankings based on the average Single%, AllFive% and Single% minus AllFive%, respectively. A more appropriate mea-sure for comparing the different rankings is the Spearman rank correlation measure, which is 0.89, 0.88 and 0.95 for Single%, AllFive% and Single% minus AllFive%, respectively. Fig. 4 shows that for the TREC 8 sys-tems the difference between Single% and AllFive% produces higher Spearman rank and R-squared linear cor-relation values than if Single% and AllFive% are considering separately. The Single% minus AllFive% values seem to be both sensitive to the number of non-relevant documents, which tend to be retrieved primarily by a single system, and the number of relevant documents, which tend to mainly found by multiple systems. If a system retrieves many relevant documents, then a majority of these documents tend to be found by other sys-tems, reducing Single% and potentially boosting AllFive%. If a systems retrieves few relevant documents, then the great majority of its documents will be found only by the system in question, significantly boosting Single% and AllFive% becomes insignificant.
 In the analysis so far, all of the 1000 documents retrieved per topic by a system are used to rank the systems.
The question arises whether even better results could be obtained if only, for example, the top 50 documents are used to compute the overlap structure. The Ranking Effect suggests that it should be possible to infer the relative retrieval differences using the top 50 or 100 documents, because relevant documents tend to be located higher up in a ranked list, especially if they are found by multiple systems ( Spoerri, 2005 ). For TREC 8, 7, 6 and 3, Fig. 5 (left column) displays the R-squared values if the linear correlation between the average Single% and Single% minus AllFive% values is calculated with respect to the systems X  mean average precision (MAP), and the overlap structure is computed using an increasing number of documents in the ranked lists. The right column shows the correlations with respect to the precision at 1000 scores (P@1000). Fig. 5 also displays the
Spearman rank correlation scores for Single% and Single% minus AllFive%, respectively, as documents increasingly lower in the ranked lists are included in the overlap computation (note that the displays in
Fig. 5 do not all use the same minimum value on their vertical correlation axes to make the differences between the different graphs better visible). For TREC 3, Fig. 5 shows that the rankings based on the top 50 documents provide the best results for MAP, whereas for P@1000, examining all 1000 documents produces the best rank-ings. For TREC 6, rankings based on the overlap between the top 100 documents produce the best or close to the best ranking results for MAP and P@1000, respectively. For TREC 7, the top 50 documents produce the best rankings for both MAP and P@1000. For TREC 8, the rankings based on the top 50 documents are very good, and the correlation values for all rank positions are very high. Table 1 displays the maximum Spearman rank correlation and the resulting correlation if only the top 50 documents are examined to compute the over-lap structure for TREC 3, 6, 7 and 8 data. In summary, Fig. 5 and Table 1 show that if only the top 50 doc-uments are compared, the resulting system rankings produce correlation values with respect to the official
TREC rankings that are close to, if not equal, to maximum correlation values that are obtained if more than the top 50 documents are considered. 6. Discussion and future research
The  X  X  X tructure of overlap X  X  method presented in this paper is simple and effective. As Table 1 shows, the sys-tem rankings produced by this method have a high Spearman rank correlation with the official TREC rankings.
If the percentage of a system X  X  documents that are only found by the system itself (Single%) is used to rank the systems, then the Spearman rank correlation values range from 0.70 to 0.93 when the mean average precision (MAP) scores are used to create the TREC rankings, and from 0.71 to 0.93 if precision at 1000 (P@1000) is used. If the difference between the percentages of a system X  X  document that are found by a single system and all five systems (Single% minus AllFive%) is used to ranks the systems, then the Spearman rank correlation val-ues range from 0.62 to 0.96 for MAP, and from 0.70 to 0.95 for R@1000. The results for Single% and Single% minus AllFive% tend to be very similar, as are the results for MAP and P@1000. These Spearman rank corre-lation scores are significantly better than the rank correlation scores obtained by Wu and Crestani (2003) , who compared several  X  X  X eference counting X  X  methods and the pseudo-relevance method by Soboroff et al. (2001) , where the latter performed best for TREC 3 with a Spearman rank correlation of 0.63 and worst for TREC 7 with a correlation value of 0.41. Further, these methods attain their best results when 10 or more systems are used and many randomized runs need to be conducted. The  X  X  X tructure of overlap X  X  method requires only
N random groupings of five systems to effectively rank N systems. Future research will also investigate the min-imal and optimal number of systems that need to be compared to rank retrieval systems without the need for relevance judgments. Future research will also investigate how well the retrieval systems can be ranked and their relative performance differences inferred if all the systems are compared at once, instead of multiple random subsets of systems. If all systems are compared at once, the issue may arise that similarities between the algo-rithms used by the different systems may affect the overlap structure and it will be investigated if and how retrie-val systems using similar text retrieval and analysis methods could be detected.

Both the ranking methods developed by Soboroff et al. (2001) and Wu and Crestani (2003) produce poor results for the best performing systems because they are ranked together with the poorly performing systems.
As Fig. 4 shows, the  X  X  X tructure of overlap X  X  method ranks the top-performing systems correctly, and performs equally well for the systems in the medium range and the poorly performing systems. There are, however, ranking errors for the top 10 systems with respect to each other, since there are small quality differences between the top ten systems and it is not surprising that some the top five systems are ranked in the top six to ten positions and visa versa.

The question arises whether the  X  X  Overlap Effect  X  X , which refers to the systematic difference in the overlap structure between retrieval systems of varying quality, can also be observed at the level of the individual top-ics, and not just when the results for the 50 topics are averaged. Ongoing research suggests that the Overlap
Effect is present for most individual topics, and that the overlap structure for an individual topic can be used to rank the systems. Thus, the Overlap Effect, which is observed when the results for all the 50 topics are com-bined, is not an artifact of averaging. Now, some topics produce better rankings and/or have more documents that are relevant. Future research will investigatie how to detect the topics that produce robust rankings, while taking into account that a retrieval system does not perform equally well for all topics.

The  X  X  X tructure of overlap X  X  method is straightforward to compute and only the top 50 documents found by a system have to be examined to produce a correlation value with respect to the official TREC rankings that is close to, if not equal, to maximum correlation value that can be obtained if a greater number of documents are considered. In particular, the overlap structure for N random groupings of five systems needs to be computed.
For each random grouping, the degree of overlap between 250 documents has to be determined for each of the 50 topics. For TREC 8, for example, less than half million documents need to be examined. The above dis-cussion regarding the Overlap Effect at the level of the individual topics suggests that not all topics need to be considered. For example, a subset of 25 topics that have varying numbers of relevant documents would be sufficient, reducing the number of documents to be examined in half.

As mentioned, it is critical that only one run by each participating system is included, because otherwise the similarity between the search results is artificially increased. In this paper, the run with the highest mean aver-age precision was selected for each system. Instead of selecting the best run, we could have selected any of the multiple runs submitted by each participating system to compute the overlap structure. Future research will investigate how to detect a high degree of similarity between search results so that systems that tend to use very similar retrieval methods can be identified.

The question arises of how the data and system rankings computed by the  X  X  X tructure of overlap X  X  method can be used to improve existing data fusion methods. For example, fusion methods have been proposed that use linear combination models or apply weights when combining multiple result lists ( Vogt &amp; Cottrell, 1999;
Wu &amp; Crestani, 2001 ). The percentage of a system X  X  documents that are found by no other systems can be used to specify a weight to be applied when combining the different result sets, since this percentage value tends to be strongly correlated in a linear and negative way with a system X  X  mean average precision (see Fig. 3 ).
Both Soboroff et al. (2001) and Wu and Crestani (2003) have suggested that their methods could be employed in the context of the World Wide Web, where the databases used by different Web search engines are tremendously large and change continuously. However, their methods and the approach presented in this paper have been tested with retrieval systems searching the same database, whereas it has been estimated that
Web search engines only index 20% of the Internet and their databases overlap to varying degrees ( Lawrence &amp; Giles, 1999 ). There could be many relevant web pages that are only found by a single Web search engine.
Thus, the Web search engines may tend to retrieve dissimilar sets of relevant documents and dissimilar sets of non-relevant documents. Soboroff et al. noted that their pseudo-relevance method performed better if dupli-cates were not removed. In the case of overlapping databases, there will be fewer duplicates. Similarly, the  X  X  X eference count X  X  approach will have fewer documents that receive high scores, which are critical for inferring relative performance differences between the systems. Future research will address how the  X  X  X tructure of over-lap X  X  approach can be modified so that it can be applied in the context of multiple systems searching overlap-ping, but not identical databases.
The  X  X  X tructure of overlap X  X  method can complement and facilitate the current TREC  X  X  X ooling X  X  method used to identify relevant documents. It can help human evaluators identify highly performing systems, whose documents are more likely to be relevant, especially if these documents have high rank positions and are found by multiple systems. Thus, it can help the evaluators identify relevant documents more quickly. The  X  X  X tructure of overlap X  X  method can also support Interactive Searching and Judging (ISJ), which involves a small of group of evaluators who use a retrieval system to search a TREC database for the documents that are related to the
TREC search topics ( Cormack, Palmer, &amp; Clarke, 1998; Sanderson &amp; Joho, 2004 ). The evaluators then judge the relevance of the documents contained in a result list, starting with the top documents and stopping when the frequency of relevant documents encountered becomes such that continuing appears unproductive. Using
TREC 6 data, Cormack et al. (1998) showed that the ISJ approach can produce a set of found relevant doc-uments that can be used to evaluate the effectiveness of retrieval systems, but requires fewer documents to be judged then the TREC  X  X  X ooling X  X  method. The  X  X  X tructure of overlap X  X  method can be used to help identify effective retrieval systems that can be used by the experts in the ISJ process. 7. Conclusions
Using TREC 3, 6, 7 and 8 data, this paper showed how the overlap structure between the search results of random groupings five systems can be used to rank retrieval systems without the need for human relevance judgments. First, it showed that there is a systematic change in the overlap structure when five  X  X  X onsecutive X  X  systems of increasingly lower retrieval quality are compared. Second, it was demonstrated that the average percentage of a system X  X  documents found only by it and no other system is strongly and negatively correlated with both its mean average precision and precision at 1000, as is the difference between the percentages of doc-uments found by a single system and all five systems being compared. This result was used to develop the  X  X  X tructure of overlap X  X  method, which uses the degree of consensus or agreement a retrieval system can gen-erate to infer its quality and thus the relative performance differences between the systems. Third, this paper showed that if only the top 50 documents are compared, the resulting system rankings produce correlation values with respect to the official TREC rankings that are close to, if not equal, to maximum correlation values that are obtained if a greater number of documents are considered. Fourth, the presented method significantly improves upon previous attempts to rank retrieval systems without relevance judgments. The  X  X  X tructure of overlap X  X  method can be of value to communities that need to identify the best experts or rank them, but do not have the resources to evaluate the experts X  recommendations, since the presented method does not require knowledge about the domain being searched or the content of the information being requested. Acknowledgments
The author would like to thank Nick Belkin and Paul Kantor as well as the reviewers for their valuable feedback. The TREC data used in the research reported in this paper has been provided by NIST and can be downloaded at http:// trec.nist.gov/ . This research was supported by a Rutgers research Council grant. References
