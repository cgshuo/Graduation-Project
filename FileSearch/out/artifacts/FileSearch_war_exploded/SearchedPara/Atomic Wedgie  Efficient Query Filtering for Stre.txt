 In many applications it is desirable to monitor a streaming time series for predefined patterns. In domains as diverse as the monitoring of space telemetry, patient intensive care data, and insect populations, where data streams at a high rate and the number of predefined patterns is large, it may be impossible for the comparison algorithm to keep up. We propose a novel technique that exploits the commonality among the predefined patterns to allow monitoring at higher bandwidths, while maintaining a guarantee of no false dismissals. Our approach is based on the widely used envelope-based lower bounding technique. Extensive experiments demonstrate that our approach achieves tremendous improvements in performance in the offline case, and significant improvements in the fastest possible arrival rate of the data stream that can be processed with guaranteed no false dismissal. streaming time series for a set of predefined patterns. Note that this problem is very different to the classic (and much studied) time series indexing problem [7][10]. It is however a very close analogue to the problem of Query Filtering for discrete valued data (e.g. XML) [3]. As noted in [3],  X  filtering is the inverse problem of querying a database: In a traditional database system, a large set of data is stored persisten tly. Queries, coming one at a time, search the data for results. In a filtering system, a large set of queries is persistently stored. (new data), coming one at a time, drive the matching of the queries.  X  While the need for filtering is well established in discrete domains (XML, Bioinformatics etc), to the best of our knowledge it has not been addressed for time series before. We will therefore take the time to motivate the need for time series filtering in several domains. Electrocardiogram Monitoring : Cardiologists often encounter new interesting ECG patterns. These patterns may be unannotated or explicitly/implicitly annotated (eg. a pattern shows up older patients that were given the drug Terbutaline [1], or a pattern shows up when the Holter electrodes have gotten wet). In either case, once seeing an interesting pattern, a cardiologist will attempt to remember it so that future encounters with similar patterns can benefit from his experience. In our framework, all new interesting patterns are simply saved in the cardiologists  X  X rofile X  and any future occurrences of similar patterns will be automatically flagged. Audio Sensor Monitoring : The damage done by agricultural insect pests costs more than US$300 billion annually [6]. The best way known to mitigate this cost is to monitor insect populations and target harmful species before they can become a major problem. Technological advances and falling prices of hardware have created an explosion of interest in continuous, real-time monitoring of critical pest data by automated ( X  X mart X ) traps in recent years [12]. While it has been shown in the lab that insects can be identified (species and sex) from audio of their wing beats [11], these successes are hard to reproduce in the field because field stations typically have low powered CPUs and the greater variety of possible insects (i.e patterns) encountered. diverse domains. With continuously arriving data and large number of patterns, it may be impossible for the comparison algorithms to keep up. However, in real world, there is likely to be significant commonality among the predefined patterns. Based on this (empirically validated) assumption, we propose a hierarchical wedge-based comparison approach, which merges large number of patterns into a small set of wedges (with similar patterns being merged together) and then compares this set of wedges against the subsequence in the coming data stream. The experimental results show that our approach provides tremendous improvements in performance. 1.1 Related Work been addressed in the literature before. The most similar work is by Gao and Wang [4] , where the authors consider the problem of continuously finding the nearest neighbor to a streaming time series. They assume that the database of predefined patterns is in secondary memory. Thus the problem in question is disk-bound. In contrast, our problem is CPU-bound. We can easily fit our relatively small database of predefined patterns in main memory (indeed, in the insect monitoring problem, there is no secondary storage on the sensors). Furthermore, in Gao and Wang X  X  problem definition, there is always some nearest neighbor to any streaming query. In contrast, we are only interested in finding when a streaming query is within r of anything in our database and we generally expect this to very rarely be the case. dictionary matching problem with errors and don't cares [2]. This problem is defined in [2] as  X  preprocess(ing) a text or collection of strings, so that given a query string p, all matches of p with the text can be reported quickly  X . The crucial difference is that this problem deals with discrete data, and researchers are therefore able to tackle it with an arsenal of tools that are defined only for discrete data, such as suffix trees and lexicographic sorting. Section 2 we review background material. We introduce our algorithms and representations in Section 3. Section 4 sees a comprehensive empirical evaluation and we offer some conclusions in Section 5. material. We begin with a definition of our data type of interest, time series. 
Definition 1 . Time Series : A time series T = t 1 ,...,t is an ordered set of m real-valued variables. properties of a time series; rather, data miners confine their interests to subsequences of the time series. 
Definition 2 . Subsequence : Given a time series T of length m , a subsequence C p of T is a sampling of length w &lt; m contiguous positions from T , that is, C = t p ,...,t p+w-1 for 1 d p d m  X  w + 1. time series and compare them to the target time series. The extraction is achieved by use of a sliding window. 
Definition 3 . Sliding Window : Given a time series T of length m , and a user-defined subsequence of length w , all possible subsequences can be extracted by  X  X liding a window X  across T and extracting subsequence C p .

Definition 4 . Euclidean Distance : Given two time series (or time series subsequences) both of length n , the Euclidean Distance between them is the square root of the sum of the squared differences between each pair of corresponding data points: to discover if they are within a given distance r from each other, we can potentially speed up the calculation by doing early abandoning .

Definition 5 . Early Abandon : During the computation of the Euclidean distance, if we note that the current sum of the squared differences between each pair of corresponding data points exceeds r 2 , we can stop the calculation, secure in the knowledge that the exact Euclidean distance had we calculated it, would exceed r .
 and intuitive [7], it is so critical to our work we illustrate it in Figure 2 and provide pseudocode in Table 1. We call the distance computation of each pair of corresponding data points a step , and we use num_steps to measure the utility of early abandonment. statement of the problem. Assume we are given a set of range r by a user. We want to either: x Search a long batch time series for any subsequences that are within r of any time series in the set C , or x Monitor a time series stream for any subsequences that are within r of any time series in the set C . streaming case we only have a small O( C ) memory buffer; 2) once we are given C and r , we have some reasonable amount of time (say O( C 2 )) to prepare. sequences U and L : U and L stand for Upper and Lower respectively. We can see why in Figure 3. They form the smallest possible bounding envelope that encloses all members of the set C ,.., C k from above and below. More formally: denote it as W = { U , L }. Now we define a lower bounding measure between an arbitrary query Q and the entire set of candidate sequences contained in a wedge W : using a different notation. candidate sequence, LB_Keogh degenerates to the Euclidean distance. More importantly, we can do early abandoning with LB_Keogh, as shown in Table 2. C of length n , and we are given a query sequence Q and asked if one (or both) of the candidate sequences are within r of the query, we naturally wish to minimize the number of steps we must perform ( X  X tep X  was defined in Section 2). We are now in a position to outline two possible approaches to this problem. x We can simply compare the two candidate sequences, 
C 1 and C 2 (in either order) to the query using the early abandon algorithm. We call this algorithm, classic . x We can combine the two candidate sequences into a wedge, and compare the query to the wedge using 
LB_Keogh. If the LB_Keogh function early abandons, we are done. Otherwise, we need to individually compare the two candidate sequences, C 1 and C 2 (in either order) to the query. We call this algorithm, wedgie .
 approach. For classic the worst case is if both candidate sequences are within r of the query, which will require 2 n steps. In the best case, the first point in the query may be radically different to the first point in either of the candidates, allowing immediate early abandonment and giving a total cost of 2 steps. For wedgie , the worst case is also if both candidate sequences are within r of the query. We will waste n steps in the lower bounding test between the query and the wedge, and then 1 n steps for each individual candidate, for a total of 3 n . However the best case, also if the first point in the query is radically different, would allow us to abandon with a total cost of 1 step. x The shape of the candidate sequences. If they are similar, this greatly favors wedgie . x The shape of the query. If the query is truly similar to one (or both) of the candidate sequences, this would greatly favor classic . x The matching distance r . Here the effect is non monotonic and dependent on the two factors above. nesting them. For example, in Figure 4 we have three sequences C 1 , C 2 , and C 3 . A wedge is built from C C , and we denote it as W (1,2) . Again, we can combine W (1,2) and W 3 into a single wedge by finding maximum and minimum values for each i th location, from either wedge. More concretely: now we generalize the wedgie approach. Given a query Q and a wedge W ((1,2),3) , we compare the query to the wedge using LB_Keogh. If it early abandons, we are done -none of the three candidate sequences is within r of the query. Otherwise, we need to recursively compare the two child wedges, W (1,2) and W 3 to the query using LB_Keogh. The procedure continues until we early abandon or reach individual candidate sequence. Because our algorithm works by examining nested wedges until (if necessary) only atomic wedges are left, we call it Atomic Wedgie .
 compared it to classic , using the 3 time series shown in Figure 4. We measured the utility by the number of steps needed by each approach. We found that for reasonable values of r , the type of data we compared it to made little difference: Atomic Wedgie was almost always 3 times faster on average. linear speedup to hold for all possible collections of candidate sequences. This is because the utility of a wedge is strongly correlated with its area. We can get some intuition as to why by visually comparing LB_Keogh( Q , W (1,2) ) with LB_Keogh( Q , W ((1,2),3) Figure 5. Note that the area of W ((1,2),3) is much greater than that of W (1,2) , and that this reduces the value returned by the lower bound function and thus the possibility to early abandon. Atomic Wedgie is dependent on the candidate sequences and the data stream itself. In general, merging similar sequences into a hierarchal wedge is a good idea, but merging dissimilar sequences is a bad idea. Since the meaning of similar / dissimilar is relative to a data stream that by definition we cannot see in advance, it is difficult to predict if Atomic Wedgie will be useful. of Atomic Wedgie . Given a set of k sequences, we can merge them into K hierarchal wedges, where 1 d K d k . This merging forms a partitioning of the data, with each sequence belonging to exactly one wedge. We use W to denote a set of hierarchal wedges: where W set(i) is a (hierarchally nested) subset of the k candidate sequences. Note that we have query. Table 3 formalizes the algorithm. can produce impressive speedup if we make judicious choices in the set of hierarchal wedges that make up W. However, the number of possible ways to arrange the hierarchal wedges is greater than K K , and the vast majority of these arrangements will generally be worse than classic . So specifying a good arrangement of W is critical. similar goals to an ideal wedge-producing algorithm. Hierarchal clustering algorithms attempt to minimize the distances between objects in each subtree, while a wedge-producing algorithm attempts to minimize the area of each wedge. However the area of a wedge is simply the maximum Euclidean distance between any sequences contained therein (i.e Newton-Cotes rule from elementary calculus). This motivates us to derive wedge sets based on the result of a hierarchal clustering algorithm. Figure 7 shows wedge sets W , of every size from 1 to 5, derived from th e dendrogram in Figure 6. wedge sets, all we need to do is to choose the best one. We could attempt to do this by eye, for example in Figure 7 it is clear that any sequence that early abandons on W 3 , will almost certainly also early abandon on both W and W 5 ; similar remarks apply to W 1 and W 4 . At the other extreme, the wedge at K = 1 is so  X  X at X  that it is very likely to have poor pruning power. The set W = { W ((2,5),3) , W (1,4) } is probably the best compromise. However because the set of time series might be very large, visual inspection is not scalable. More generally, we choose the wedge set based on empirical tests. We test all k wedge sets on a sample of data that we believe to be representative of future data and choose the most efficient one. 3.1 A Bound on Atomic Wedgie comparing a set of time series to a large batch dataset. However so far it does not make any contribution to the problem of streaming time series. The reason is that while it is efficient on average , streaming algorithms are limited by their worst case. The worst case is easy to see. Imagine that we might have chosen W with size of K = means that we would do EA_LB_Keogh 2 k -1 times, without early abandoning. This is actually worse than classic , which only requires k complete invocations of EA_LB_Keogh in the worst case. bounds for the worst case of Atomic Wedgie . The intuition is that for realistic values of r and realistic sets of time series, no query Q will be within r of all members of the pattern set. For example, consider the five time series in Figure 7. Clearly any sequence Q that is close to C 1 or C 4 cannot also be close to C 3 , C A more formal explanation is given below. L }, we define the distance between them as: and one from W 2 ), the distance between them is at least d ( W 1 , W 2 ). This is easy to see. For unoverlapped portion, we sum up the distance between the closest edges of the two wedges. Recall that wedge forms the smallest possible bounding envelope that encloses all its members, which means any pair of the time series from W and W 2 cannot be closer than the closest edge pair. For overlapped area, we count the distance as zero. cost of Atomic Wedgie . Say for the five time series shown in Figure 7, we start from the biggest wedge W (((2,5),3),(1,4)) and fail, then we need to test on wedge W is guaranteed that we would not fail both on W (1,4) W ((2,5),3) . Without loss of generality, we can assume that the test fails on W (1,4) , which means d ( Q, W According to triangle inequality, which means testing on W ((2,5),3) would prune the query and we can safely stop that branch there. However  X  if recursively. We illustrate the computation of the cost upper bound in Table 4. between subsequence and interesting patterns, so usually r is a relatively small value, which increases the possibility for two wedges having distance larger than 2 X  r . As the result, the Atomic Wedgie algorithm can skip a lot of computations based on the proof we gave above. As we shall see in Section 4, with reasonable value of r , in the worst case Atomic Wedgie is still three to four times faster than the brute force approach. 3.2 A Final Optimization can do to speed up Atomic Wedgie . Recall that in both Table 1 and Table 2 when we explained early abandoning we assumed that the distance calculation proceeded from left to right (cf. Figure 2). When comparing individual sequences we have no reason to suppose that left to right, right to left, or any other of the w ! possible orders in which we accumulate the error will allow an earlier abandonment. However this order can make a huge difference. It is simply that we cannot know this order in advance. comparing a query to a wedge. In this case we do have an a priori reason to suspect that some orders are better than others. Consider the wedge shown in Figure 3. The left side of this wedge is so  X  X at X , that most query sequences will pass through this part of the wedge, thus contributing nothing to the accumulated error. In contrast, consider the section of the wedge from 10 to 50. Here the wedge is very thin, and there is a much greater chance that we can accumulate error here. such that the loop variable is sorted in ascending order by the value of U i -L i (the local thickness of the wedge). This sorting takes O( w log( w )) for each wedge ( w is the length of the wedge), but it only needs to be done once. As we shall see, this simple optimization speeds up the calculations by an order of magnitude. a comprehensive set of experiments. For each experiment, we compared Atomic Wedgie to three other approaches, brute force , classic , and Atomic Wedgie Random ( AWR) . Among them, brute force is the approach that compares each pattern to the query without early abandoning. AWR is similar to Atomic Wedgie , except that instead of using the wedge sets resulted from the hierarchical clustering algorithm (in this paper we use complete linkage clustering), we randomly merge time series. This modification is essentially a lesion study which helps us separate the effectiveness of Atomic Wedgie from our particular wedge merging strategy.  X  reasonable values of r  X . As the reader may already appreciate, the value of r can make a huge difference to the utility of our work . We want to know the performance of Atomic Wedgie at the values of r which we are likely to encounter in the real world. The two domain experts (cardiology and entomology) that are co-authors of this work independently suggested the following policy. of the patterns of interest. A logical value for r would be the average distance from a pattern to its nearest neighbor. The intuition is that if the patterns seen before tended to be about r apart, then a future query Q that is actually a member of this class will probably also be within r of one (or more) pattern(s) of our dataset. available at the following URL [8]. 4.1 ECG Dataset Arrhythmia Database [5], which contains half an hour X  X  excerpts of two-channel ambulatory ECG recordings. The recordings were digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range. We use signals from one channel as our batch time series, which has 650,000 data points in total. Our pattern set consists of 200 time series, each of length 40. According to the cardiologists X  annotation, they are representative patterns of left bundle branch block beat , right bundle branch block beat , atrial premature beat , and ventricular escape beat . For Atomic Wedgie and AWR , we tested all 200 wedge sets on first 2,000 data points, and chose the most efficient one to use. and illustrate the number of steps needed by each approach in Figure 9 (the precise numbers are recorded in the Appendix). The result shows that our approach is faster than brute force by three orders of magnitude, and faster than classic by two orders of magnitude. Note that AWR does not achieve the same speedup as Atomic Wedgie , suggesting that our wedge building algorithm is effective. We also computed the upper bound of the cost of Atomic Wedgie for ECG dataset, which is 2,120 steps. This is about 4 times faster than the brute force approach, which in the worst case will need to compare the subsequence to all patterns, resulting in 200 * 40 = 8,000 steps. 4.2 Stock Dataset finding interesting patterns in a stock dataset. We tested on a stock time series with 2,119,415 data points. There are 337 time series of length 128 in the pattern set. They represent three types of patterns which where annotated by a technical analyst, with 140 for head and shoulders , 127 for reverse head and shoulders , and 70 for cup and handle . Again, for Atomic Wedgie and AWR , we tested all 337 wedge sets on first 2,000 data points, and used the most efficient one for the rest of the data. needed by each approach is illustrated in Figure 10. The result again indicates impressive speedup of Atomic Wedgie . Atomic Wedgie is faster than brute force by two orders of magnitude, and faster than classic by one order of magnitude. For stock dataset, the cost upper bound of Atomic Wedgie is 18,048, which is about one third to that of the brute force approach (337*128 = 43,136). 4.3 Audio Dataset to monitor the occurrences of some harmful mosquito species. The wave file, at sample rate 11,025HZ, was converted to a 46,143,488 data points X  time series. Here we used a sliding window of size 11,025 data points (1 second X  X  sound) and slid it by 5,512 points (0.5 second) each time. Because insect detection is based on the frequency of wing beat, we applied Fourier transformation on each subsequence and then resampled the time series we got (note that the FFT was performed by specialized hardware directly on the sensor [9] and that the time taken for this is inconsequential compared to the algorithms considered here). We have 68 candidate time series of length 101, which are obtained through the same procedure (FFT plus resampling) from three different species of harmful mosquitoes, Culex quinquefasciatus , Aedes aegypti , and Culiseta spp . For Atomic Wedgie and AWR , we used first three minutes X  sound to decide which wedge set to use. shown in Figure 11. Here the parameter r equals to 4.14. Atomic Wedgie is faster than brute force by two orders of magnitude. Note that here AWR is worse than classic . For audio dataset, the cost upper bound of Atomic Wedgie is 2,929, which is about one third to that of the brute force approach (68*101 = 6,868). 4.4 Speedup by Sorting Atomic Wedgie , where the distance calculation proceeds in the ascending order of the local thickness of the wedge. To demonstrate the effect of this optimization, we compare the wedge in Figure 3 to 64,000 random walk time series, and recorded the total number of steps required with and without sorting. The result, shown in Figure 12, demonstrates that the simple optimization can speed up the calculations by an order of magnitude. series filtering: fast, on-the-fly subsequence matching of streaming time series to a set of predefined patterns. Given the continuously arriving data and the large number of patterns we wish to support, a brute force strategy of comparing each pattern with every time series subsequence does not scale well. We propose a novel filtering approach, which exploits commonality among patterns by merging similar patterns into a wedge such that they can be compared to the time subsequence together. The resulting shared processing provides tremendous improvements in performance. based on the empirical test. For data changing over time (i.e concept drift), dynamically choosing the wedge set will be more useful. We leave such considerations for future work. Acknowledgments : We gratefully acknowledge Dr. Reginald Coler for technical assistance with the insect audio dataset. 
