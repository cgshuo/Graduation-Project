
Causality modeling in time series data has drawn much research attention of late. Given a set of interacting time series, how can we determine if the history of one time series affects the development of another variable? This question about causality plays a fundamental role in economics, health and medical sciences, biology, and the decision-making process, at various domains and levels. For instance , economists are interested in if the domestic demand is a causal factor for China X  X  economic growth [1]; financial analysts want to determine if a company X  X  stock price is causally affected by its inventory turnover ratio [2], and neurobiologists try to understand if the time series of one brain region is a causal factor of another region [3].
Temporal causal modeling tries to recover the causal structure among a group of relevant time series variables [2 ]. A fundamental tool for such inference is the notion of Granger causality [4]. It is derived from the intuition that if one time series is the cause of another time series, then the former can help improve the prediction accuracy of the latte r significantly. More specifically, to determine if time serie s x is Granger-causal for y , we test if the auto-regressive model for y using the past values of both x and y is statistically significantly more accurate than the model using only y  X  X  own past value. The notion of Granger causality has been combined with graphical models to study the interaction between multiple time series [5]. A temporal graphical model is a directed graph where each vertex corresponds to a time series and each edge indicates a direct causality from the starting vertex to the end vertex. The regression coefficient from the auto-regressive model can be assigned t o each corresponding edge to indicate the degree of causation or interaction.

Several methods have been developed to explicitly re-construct temporal causal models [2], [6]. These works typically target a small number of time series variables (on the order of tens). Recently, temporal causal modeling has been extended to study relatively large complex systems, whose dynamics are captured through a set of time series. Typically, each time series measures a basic unit in the system. Basic units may interact with each other for a certai n period of time, and their possible interaction relationshi ps can be summarized through a so-called complex network topology [7]. For instance, in biology, the protein-protei n interaction network specifies which two proteins may inter-act with each other, where the activity of each protein can be measured by the gene-expression time series profiles. Given this, time series x can affect time series y only if an edge ( x, y ) links from x to y in the network. The sparse underlying network topology thus allows efficient computational procedures to recover the causal structure f or a large number of time series.

However, a difficult problem naturally arises as we are able to construct more and more causal graphical models: how can we better understand and conceptualize these complicated causal relationships? Indeed, a causality mod el with only 20 variables can be overwhelming and difficult to interpret at a global level [2]. Clearly, comprehending a much larger causal model with hundreds or even thousands of variables is even more daunting and elusive. Can we simplify the temporal causal graphical model to get a better global view of the interactions among a set of relevant time series? This is the central problem we address in the present work.

To simplify the causal graphical model, we investigate a decomposition approach to cluster time series into groups such that strong interactions appear among the variables within each group and weak (or no) interactions exist for cross-group variable pairs. Clearly, this goal is also con-sistent with the model for complex systems, which tend to be composed of several smaller and relatively independent components. A key thrust here is that the decomposition model is achieved through balancing the prediction power of the causality model with the simplicity of the model. Specifically, the model simplicity is described in terms of both the sparsification of the regression coefficient matrix and an explicit cluster structure of the graphical model. Fr om a different perspective, our approach can also be viewed as a method for clustering a set of interacting time series. What differentiates our work from the existing work on time series clustering [8], [9], [10], [11] is our clustering cri teria, which is derived from temporal graphical modeling and is very challenging to optimize.

In this work, we present a novel and efficient decomposi-tion scheme for a temporal graphical model to cluster a set of interacting time series, with the following contributio ns: 1. We formulate the clustering problem for temporal graphical models as a regression coefficient sparsification problem and define an interesting objective function which balances model prediction power with its cluster structure . 2. We propose an iterative optimization approach utilizing the Quasi-Newton method and generalized ridge regression to minimize the objective function and to produce a clustere d temporal graphical model. 3. We develop a novel optimization procedure using a graph theoretical tool based on the maximum weight inde-pendent set problem to speed up the Quasi-Newton method for a large number of variables. 4. We performed a detailed experimental study on syn-thetic and real datasets to demonstrate the effectiveness a nd efficiency of our approach.
 A. Temporal Graphical Modeling
In the following, we give an overview of temporal graphical modeling for the cause-effect relationships of multivariate time series. Let X i = [ x 0 i , x 1 i , , x L the i -th time series from time point 0 to the end point L . Let X ( j ) = [ x j 1 , x j 2 , , x j N ]  X  be the snapshot vector for the value of each time series at time point j . Let X = [ X 1 , X 2 , , X N ]  X  = [ X (0) , X (1) , , X ( L )] be the matrix for all N time series, where each row ( X i ) corre-sponds to a time series and each column ( X ( j ) ) corresponds to all time series at time point j .

In time series analysis, inference about cause-effect re-lationships is commonly based on the concept of Granger causality [4], which is defined in terms of predictability and exploits the direction of the flow of time to achieve a causal ordering of dependent variables. Simply speaking, given two time series X i and X j , Granger causality tests if time series X i at time point t + 1 , X t +1 i , can be better predicted if we consider both time series X i and X j from time t  X  u to t than if we only consider the time series X itself. When the Granger test is restricted to revealing line ar relationships among different variables, it is closely rel ated to the linear vector autoregressive (VAR) model. Let X V be the submatrix of X which contains only the time series of information set V = { v 1 , . . . , v | V | } . We formally represent time series X j in a VAR model as follows: where each  X  jv ( t  X  u ) is the coefficient indicating the causal influence from X v to X j , and {  X  ( t ) , t  X  Z } is a white noise process with non-singular covariance matrix  X  . In this sense, we say X i is Granger-non-causal for X j with respect to X if and only if  X  ji ( t  X  u ) = 0 for all 1  X  u  X  T .
The path diagram proposed by Eichler [5] combines the notion of (multivariate) Granger causality with a graphica l model, thus forming the basis of temporal graphical model-ing. A path diagram is a directed graph G = ( V, E ) , such that each vertex represents a time series and each edge ( v, v exists if and only if v is a causal factor to v  X  . Path diagrams aid in visualizing the causal relationships among differen t variables. To construct a path diagram, for each vertex (or variable in time series data), we identify which neighbors a re Granger causal for it. Efficient algorithms introduced in [2 ], [7] are able to construct the path diagram and recover the causal relationships. Next, using the path diagram concept , we formally define our problem of studying the global view of interactions among multivariate time series.
 B. Problem Definition
Our goal is to decompose a temporal graphical model into clusters of interacting time series. Our input includes a se t of time series and the path diagram, G = ( V, E ) , which indicates the potential causal relationship between any tw o time series. By selectively removing links of low impor-tance, we seek to break the path diagram into disconnected components. Then, each time series variable is affected by (or interacts with) time series inside its own component but not between components. Dropping the cross-group causal factor should result in a minimal loss of prediction accurac y.
To formalize this requirement, we utilize the vector au-toregressive model. The clustering structure of the tempor al graphical model is represented as the regression coefficien t matrix  X ( u ) having a block diagonal structure: For any vertex pair i , j belonging to different clusters,  X ( u ) ij = 0 , for any u . Each  X  k ( u ) is a square matrix. Since the regression coefficient matrices are used to simplify the path diagram, for each  X  k ( u ) ij 6 = 0 , we need ( i, j )  X  E . We do not introduce any new causal relationship in the VAR model besides those in the path diagram G . Formally, we define the clustered regression coefficient matrices as follows.

Definition 1: (Clustered Regression Coefficient Matri-ces) Let X ( t ) , 1  X  t  X  L , be the N time series and G = ( V, E ) be its path diagram for the causal modeling. Let f be the clustering assignment function, i.e., for each verte x i , f ( i ) is its cluster ID. A clustering coefficient matrix  X ( u ) is referred to as a clustered regression coefficient matrix if it satisfies the following two properties: 1) for any vertice s i and j , f ( i ) 6 = f ( j )  X   X ( u ) ij = 0 ; and 2) for any vertices i i potentially is a causal factor of j in the path diagram.
Basically we require the clustered regression coefficient matrices to comply with the causal prediction described by the path diagram. Note that in general, we can require them to comply with other known knowledge of such causal prediction. For instance, in analysis of complex systems, w e may replace the path diagram with the underlying interactio n relationships (the so-called complex network).

To maximize the predictive accuracy while minimizing its representation cost, we define a cost function for  X ( u ) which is the sum of all residuals (regression errors) plus a regularization penalty: cost = where P ||  X ( u ) || 2 is the L 2 penalty for the regression coefficient, and  X  is the complexity parameter that controls the amount of shrinkage.

Clearly, we would like to minimize the prediction error ( cost ). In other words, we seek the coefficient matrices for the desired clustering f which minimize the clustering cost. However, one problem with this criteria is that if we do not constrain the clustering assignment function, the minimum cost configuration tends to group all the vertices in one cluster and leave other clusters empty. To tackle this problem, we apply a constraint to balance the size of clusters.
 To achieve this, we introduce a cluster membership matrix C with N rows and K columns, where each row corresponds to a vertex and each column corresponds to a cluster. Each entry C ik acts as an indicator variable: C ik = 1 means vertex i belongs to cluster C k , and C ik = 0 means vertex i does not belong to cluster C k . In addition, we have P K k =1 C ik
Utilizing the cluster membership matrix, we can rewrite our optimization problem. For simplicity, we only consider the time window T = 1 here (  X  =  X (1) ). Our framework and algorithm can easily be generalized to T &gt; 1 .
Definition 2: (Optimal Decomposition Problem) The optimal decomposition is to find a cluster membership matrix C and its corresponding regression coefficient matrix  X  , such that cost = is minimized where C ik  X  X  0 , 1 } X  P K k =1 C ik = 1 .
Note that the last term  X  P K k =1 ( P N i =1 C ik ) 2 is our size constraint for balancing the size of clusters. It is not hard to see that P K k =1 ( P N i =1 C ik ) 2 is minimized if and only if P i =1 C ik for every k are equal. That is, P serves as a normalized clusters X  size factor for the cost function.

By using the cluster membership matrix in the cost formula, we cause the regression coefficient matrix  X  to be sparse. This is because the clustering coefficient  X  ij is only useful when C ik = C jk = 1 , i.e., both time series i and j belong to the same cluster. We can see that the decomposition problem is a combined integer (binary membership matrix) and numerical (regression coefficients ) optimization problem. This problem is quite challenging as it contains a large number of ( N 2 + N K ) unknown variables, where the cluster membership matrix contains N K unknown variables and the regression coefficient matrix has N 2 unknown variables.

Our solution to the optimal decomposition problem em-ploys the relaxation strategy, which generalizes the binar y membership matrix C to be a probabilistic membership ma-trix. For each time series i , we relax the membership entry C ik to be the probability of time series i in cluster k , i.e., C relaxation allows us to treat both clustering and regressio n numerically.

Specifically, our optimization procedure will optimize the clustering membership matrix and regression coefficien t matrix in an alternating and iterative fashion (as illustra ted in Figure 1). To begin with, we apply an efficient algorithm developed in [7] to extract the path-diagram from the pro-vided time series data. Given this, two optimization steps are iteratively employed to improve our objective function until cost reaches a local minimum. In the first step, we seek the optimal probabilistic membership matrix [ p ( k | i )] where the regression coefficient matrix  X  = [  X  ij ] is fixed. The traditional Quasi-Newton method can be used to handle it. In the second step, we optimize the regression coefficien t matrix assuming that [ p ( k | i )] is given. We formulate this problem as a generalized ridge regression problem and solve it using existing approaches. Next, we describe these two steps in detail.
 Step 1: Optimizing Probabilistic Membership Matrix . In this step, we assume the regression coefficient matrix is given and try to optimize the probabilistic membership matrix to minimize the cost .

First, we incorporate constraints into the cost formula using the Lagrange multiplier method: where  X  i is Lagrange multiplier for membership constraint P k =1 p ( k | i ) = 1 . Then, we compute its derivatives with respect to each entry p ( r | s ) of the membership matrix as follows: where p ( r | s ) is the probability of vertex s being in cluster r .

It is hard to get a closed form for each optimal p ( r | s ) as there is no easy way to solve a set of quadratic equations (  X  X  ( r | s ) = 0 ). The classical Newton method can handle this type of optimization problem. Let X be the vector of variables (i.e. vector { p ( k | i ) ,  X  i } with N K + N elements in our problem). The typical iterative update scheme is expressed via gradient  X  f ( X ( n ) ) as follows: where X ( n ) is the estimated value of X in the n -th iter-ation, and H ( X ) is the Hessian matrix . Specially, for our optimization problem, H ( X ) is a ( N K + N )  X  ( N K + N ) square matrix.

Clearly, it is too costly to evaluate this Hessian matrix, even if N and K are not very large. To deal with this problem, we employ the Quasi-Newton method [12], which seeks to approximate the Hessian matrix, by avoiding the direct inversion of the Hessian matrix. In this method, we focus on solving the following linear system: If we substitute X with appropriate variables, we can express the linear system of our problem as: Given this, we can apply another formula, such as the Davidon-Fletcher-Powell (DFP) formula [12], to iterative ly update and approximate the Hessian matrix. Thus, the Quasi-Newton method can help construct the probabilistic mem-bership matrix which results in a local minimum of cost . Step 2: Optimizing Regression Coefficient Matrix . In the second step, we assume the probabilistic membership matrix is fixed and try to optimize regression coefficient matrix  X  in order to minimize the overall cost. As we will see, this subproblem corresponds to a generalized ridge regression , so we can obtain the closed form solution to optimize  X  efficiently.

To simplify this optimization problem, we first observe that each row of the regression coefficient matrix  X  T i = (  X  i 1 , ,  X  iN ) can be optimized independently. This is because we can decompose the objective function ( cost ) into several sub-objective functions F i such that cost = P +  X  Each F i is uniquely determined by the corresponding row i , and can be solved independently. Moreover, the global minimum of cost is achieved by each F i obtaining its own minimum.
 Given this, we now focus on how to optimize F i directly. To better understand this problem, we rewrite it in a matrix form. Let y k be the vector ( p ( k | i ) x i (1) , p ( k | i ) x i (2) , , p ( k | i ) x the matrix with L rows and N columns where its entry at t -th row and j -th column is p ( k | i ) p ( k | j ) x j each row of X k corresponds to a different time point, and each column of X k records a different time series. Using these two vectors, we can rewrite F i as follows: F where M i is the last term in Eq. 4, which is constant with regard to  X  . Note that if K = 1 (with only one cluster), then we have the traditional ridge regression problem [13]. Lemma 1: The optimal  X  i that minimizes F i is where I is the identity matrix.
 Proof Sketch: Simply by noting: first derivative to zero, and obtain our results. 2
Finally, we note that the matrix X k of each F i does not need to contain N columns since the causality path-diagram G = ( V, E ) is typically sparse. For each time series i , there is only a small number of other variables which will be its causal factors, and our regression coefficient matrix will only consider those variables. Thus, we only need to find  X  ij for those causal variables. Given this, we can see that each X k typically has only a small number of columns, making our method very efficient for computing the regression coefficient matrix.
 Overall Algorithm: The overall procedure to decompose the path-diagram involving these two steps is sketched in Algorithm 1. We start by initializing the membership matrix P (Line 1 ). The initial membership assignment can be purely random or utilize the knowledge of path-diagram structure (for instance, by a spectral clustering on the path-diagram ). Then, we iteratively invoke the aforementioned two steps fo r optimizing membership matrix P and coefficient matrix  X  (Lines 3 and 4 ). We repeat them until some stop criteria is satisfied, e.g., the improvement of the overall cost is very small or a certain number of iterations is reached (Line 5 ). Following that, hard cluster assignments can be made utilizing the optimal probabilistic membership (Line 6 ). A basic method is to simply assign each time series i to its most probable cluster k , i.e., k = argmax r p ( r | i ) . The key building block of our procedure is the employment of steps 1 and 2 to optimize cost to a local minimum (as formally stated in Theorem 1).

Theorem 1: The cost of path-diagram decomposition con-verges to a local minimum as we iteratively invoke steps 1 and 2 .
 Complexity Analysis: The complexity of our optimization procedure is as follows. In the first step, optimizing the probabilistic membership matrix, computing Eq. 3 and Hes-sian matrix approximation each take O ( N 2 1 ) time, where N 1 = N K + N . If the number of iterations is k , optimizing probabilistic membership can be completed in O ( kN 2 1 ) time. In the second step, optimizing the regression coefficient ma -trix, the highest computational cost is for matrix inversio n. Inversion of a matrix can be computed in O ( R 3 i ) , where R i is the number of causal factors for the i -th time series Algorithm 1 PathDiagramDecomposition( X , G , K ) variable. The overall time complexity of the second step is thus O ( P N i =1 R 3 i ) . Since the number of causal factors for each variable is small, we can treat it as a constant for N is large. Thus, the second step is much more efficient than the first step. In next section, we develop novel methods to speed up the computation process for the first step.
According to the complexity analysis in the previous section, optimizing the probabilistic membership matrix i s the computational bottleneck of our iterative optimizatio n procedure. The main issue is that the Quasi-Newton method is very costly for a large number of variables. In this sectio n, we introduce a strategy which takes the variable dependence relationship into consideration and optimizes each variab le (or a small number of variables) independently, assuming the relationships are fixed. Identifying the subdivision of variables that minimizes the final cost can be formulated as a maximum weight independent set problem.
 A. Covering Structure
Recall that the path diagram G indicates the causal relationship between any two vertices. If ( v i , v j )  X  E , then v is potentially a causal factor of v j . This also corresponds to  X  ij in the regression coefficient matrix  X  being nonzero. In addition, from our objective function, we observe that membership p ( k | i ) relates to only its predecessors, succes-sors and the predecessors of its successors in path diagram G . The predecessors of v i correspond to those vertices in G having an edge pointing to v i , and the successors of v i correspond to those vertices that v i points to. In other words, the predecessors of v i contribute to the prediction of the i -th time series, and v j contributes to the prediction of its successors. In order to describe the set of time series variables (vertices) which v i directly relates to, we introduce the covering structure of v i .

Definition 3: The covering structure of vertex v i is the set of vertices in a path diagram consisting of v i  X  X  predeces-sors, its successors, and the predecessors of its successor s (see Figure 2(a)).

We say v j is independent of v i if v j is not in the covering structure of v i . This independent relationship is symmetric, that is, v j is independent of v i means v i is independent of v j as well. In the following, we will see that if we assume the probabilistic membership for each vertex in the coverin g structure of v i is fixed, then, we can find the optimal probabilistic membership for v i , ( p (1 | i ) , , p ( K | i )) , by solving a set of simple linear equations.
 Optimizing individual membership p ( k | i ) with respect to the covering structure of v i : To see the relationship between the membership function for a vertex v i and its covering structure, we first decompose the cost function into three parts (we omit the shrinkage term as it is a constant during the membership optimization): where F i and F j correspond to the prediction errors for time series variables (vertices) v i and v j , respectively, and suc ( v i ) is the immediate successors of vertex v i in the path diagram. To find the optimal p ( k | i ) , we perform the first order derivative of the cost function.

Let y i be the vector ( x i (1) , x i (2) , , x i ( L )) T . Let x be the vector ( x i (0) , x i (1) , , x i ( L  X  1)) T . Let Z matrix of size L  X  N where its entry at row t and column i follows (the derivative of the third term in Eq. 6 is zero):
Its second order derivative is clearly greater than zero. Note that  X  &gt; 0 because we want to minimize
Now we enhance the objective function to take the prob-ability constraint P k p ( k | i ) = 1 into consideration: The derivative of the new objective function with respect to each probabilistic membership p ( k | i ) , ( 1  X  k  X  K ) is as follows: Note that each of those K equations is linear. If we include the constraint equation P k p ( k | i ) = 1 , we get a linear system with K + 1 equations of K + 1 variables: K variables of p ( k | i ) and one Lagrange multiplier  X  i . Since K is typically very small and each variable is related to only a small number of causal factors (specified in the path diagram), we can solve each such linear system in almost constant time. For all the time series variables, we have O ( N K 3 ) time complexity.

However, we cannot apply these simultaneously as they all assume the probabilistic memberships of vertices in the ir covering structure are fixed. Our strategy is to find a set of vertices which can maximally optimize the overall cost , and then adjust their probabilistic memberships together. We c an repeat this procedure until no improvement can be made. Given this, our problem is to find a set of vertices which maximally optimizes the overall cost , and are independent of each other with respect to the covering structure , such that no vertex appears in the covering structure of others. B. Maximum Weight Independent Set Approach
In the following, we transform the problem of choosing a set of vertices which can maximally optimize the overall cost into a maximum weight independent set problem. The intuition is that if a set of vertices are pairwise independent , then their cost improvements can be simply added together as their memberships do not rely on each other.

Given this, we introduce the cover graph by aggregating all the covering structures together. Specifically, the cover graph G c = ( V, E c ) is an undirected graph, where V is the vertex set in the path diagram, and an edge ( v i , v j ) exists if and only if v j is in the covering structure of v i or vice versa . In other words, v i and v j are not independent. Then, we assign a weight to each vertex v i in cover graph G as: where cost ( v i ) is the minimized cost after we optimize the membership of vertex v i independently and cost is the original one. Thus, we can see the problem of choosing a set of vertices which can maximally optimize the overall cost is an instance of the maximum weight independence set problem. The maximum weight independent set (MWIS) problem is one of the well-known and well-studied problems in combinatorial optimization. While it has been proven to be NP-hard, efficient heuristic algorithms exist [14], [15] . We can apply any of them here.

Putting the probability constraint together with newcost (Eq. 7), we reformulate it as a linear combination of inde-pendent vertices: where V s is a set of independent vertices. Note that because they are independent, their first order derivatives still fo rm a linear system. It consists of | V s | X  ( K + 1) equations for | V s | X  ( K + 1) variables, i.e., | V s | X  K variables of p ( k | i ) and | V s | Lagrange multipliers  X  i for probability con-straints. Thus, we can apply efficient linear solvers, such a s the Cholesky Factorization-based Minimum Degree method [16], to find the optimal membership assignment for all the vertices in V s .

The sketch of our MWIS-based membership optimization scheme is outlined in Algorithm 2. For each vertex in the cover graph, we calculate its cost improvement and take it as vertex weight (Line 2 to 5). Then, a set of independent vertices in terms of covering structure is updated in order to maximally improve the cost objective function (Line 6 to 7). Finally, we repeat this process until the overall cost converges or certain stop criteria are satisfied (Line 8 ). Clearly, the cost function is monotonically reduced by successively invoking membership optimization of the independent set; therefore, it converges to a local minimum . Algorithm 2 MembershipOptimization( G c , P )
In this section, we validate the accuracy and usefulness of our proposed approaches for temporal graphical modeling decomposition. First, we perform this validation on synthe tic data with a known ground truth. Then we apply our ap-proaches to analysis of real-world GDP data.

We apply our two methods for experimental evaluation: 1) iterative optimization based on the Quasi-Newton method ( newton ); 2) iterative optimization based on the MWIS method ( mwis ) where each vertex is updated. For purposes of comparison, two benchmarks are also used. The first benchmark (denoted as Cor Ncut ), uses the Pearson Cor-relation test to generate interaction relationships among dif-ferent variables, then Ncut [17] is employed for clustering ; The second benchmark (denoted as Dcut ), applies directed spectral clustering [18] for path diagram decomposition. W e implemented all algorithms using Matlab. All experiments were performed on an AMD 2.0 GHz dual-core Opteron with 4GB RAM.
 A. Synthetic Data Synthetic data generator: In order to evaluate the decom-position (clustering) accuracy of our approaches, we utili ze the following synthetic data generator which can specify th e ground truth. Basically, the time series data are generated from a approximate block diagonal regression coefficient matrix, in two steps. In the first step, a community-based graph is constructed based on the method described in [19]. This graph is used as the underlying path diagram for time series data generation in the second step. Here, we can specify separate average vertex degrees for intra-communi ty connections (denoted as Z in ) and inter-community con-nections (denoted as Z out ). In the second step, we apply the method introduced in [2] to obtain the time series data. Initially, each edge of the underlying path diagram is assigned a randomly generated weight as its regression coefficient. In addition, we skew the random values so that the regression coefficients for the intra-community pairs a re generally larger than those for the inter-community pairs. Next, we repeatedly apply the path diagram X  X  edge weights to generate time series data. In terms of matrix operations, the next time step X  X  data is obtained by multiplying the regression coefficient matrix with the current time step X  X  d ata vector and then adding a Gaussian noise vector with mean of zero. The process is essentially the same as the vector autoregression process described in Sec II-A, if the histor y length T = 1 .
 Decomposition Accuracy: An accurate decomposition (clustering) is one where the clusters generated by the algorithm closely correspond to the known true clusters. To measure and compare accuracy, we apply a technique developed for cluster ensembles [20]. Let B = ( U, V ) be the complete bipartite graph where each vertex in U corresponds to each cluster generated by a clustering algorithm, and eac h vertex in V corresponds to a true cluster. Moreover, for each edge ( u i , v j ) , we assign a weight, equal to the size of the intersection set for the two clusters corresponding to u i and v j . Thus, the clustering accuracy computation is transformed to finding a maximum bipartite matching for B . We accumulate the sum of weights of all edges in this matching. The ratio of this sum over the total number of variables in the data is the clustering accuracy.
We evaluate the decomposition accuracy of our ap-proaches using two groups of time series datasets. The first group is on a small number of time series variables (on the order of tens). The second group is on a relatively large number of time series variables (on the order of hundreds). Results for a small number of time series variables: The experimental results are shown in Table 1. Each experiment is parameterized by the number of variables for the time series data ( # V ars ) and the number of communities ( K ). For these tests, we varied the average number of inter-community connections ( Z out = 0 . 1  X  # V ars/K , Z out 0 . 2  X  # V ars/K and Z out = 0 . 3  X  # V ars/K ), while fixing the average number of intra-community connections to be Z in = 0 . 5  X  # V ars/K . Note that # V ars/K represents the number of vertices in each community. For each set of Z values, we made five datasets, varying # V ars from 10 to 50 . The vertices in each dataset were decomposed into different numbers of communities, ranging from 2 to 5 communities.

As we can see, newton consistently obtains the best clustering accuracy among all four algorithms. Overall, th e clustering accuracy of newton is better than benchmarks Cor Ncut and Dcut by an average of 27 . 2% and 32% , respectively. In addition, mwis is better than Cor Ncut and Dcut by an average of 23 . 9% and 28 . 8% , respectively. These results show that traditional spectral clustering Dcut cannot accurately decompose even relatively small set of time series. In contrast, both of our methods perform well, with 100% accuracy in several cases.

In addition, Cor Ncut and Dcut , which also employ spec-tral clustering, are faster than others in terms of clusteri ng time, especially in Matlab which has been highly optimized for matrix computation. As for our two algorithms, mwis takes from 2 seconds to 10 minutes for each dataset, while newton needs only 1 to 69 seconds to finish. As expected, newton is the best on datasets with a small number of variables.
 Results for a large number of time series variables: In the second experiment, we generated the times series data with the number of vertices ranging from 100 to 800, while fixing the average number per vertex of intra-community connections Z in = 30 , and the average for inter-community connection Z out = 20 . The community-based path diagrams in these datasets contain from 2 to 8 communities. As we expected, newton was computationally inefficient, even crashing Matlab in some instances.
Figure 3 shows the clustering accuracy of large-scale datasets. Here, the clustering accuracy of mwis is better than that of Cor Ncut by an average of 30 . 1% . From the figure, we can see our mwis is significantly better than the benchmark Dcut , outperforming it by approximately 52 . 5% clustering accuracy. It is interesting to observe that the clustering accuracy of both Cor Ncut and Dcut tend to decrease as the number of variables increases. However, our algorithm mwis maintains good performance on all large-scale datasets. Moreover, mwis was able to achieve their accuracy even with a limited number of iterations. B. Real Data
To validate our approaches in a real-world application, we use global economic data to seek temporal country-country dependencies. Our dataset consists of GDP (gross domestic product) for 192 countries, as collected by the USDA (http://www.ers.usda.gov/Data/Macroeconomics/). The t ime series data for each country is its annual GDP growth rate over the period from 1969 to 2007. We subdivide the time range into four time periods of approximately 10 years each: 1969-1979, 1980-1989, 1990-1999, and 1998-2007. We apply the M W IS -based decomposition approach to top-down hierarchical bipartitioning down to 6 or 7 partitions, to group countries into interdependent groups. Our results exhibit meaningful, sometimes fascinating clusterings.
Most notable is how the grouping of the Soviet Republics changes across the four time periods. In period 1 (1970s), the top partition separates out Russia and 21 other nations, indicating that the most significant division at the global level is to separate out these economies from the rest of the world. Most of the 22 are either Soviet Republics (8), other communist states (5), or received strong Soviet suppo rt (3 -Angola, Uganda, Ethiopia). The remaining countries, in the Middle East or Africa, found the 1970s to be an unsettling time. None of these 22 nations were Western capitalist nations.

In the 1980s, the top partition separates out Russia again, but with only 11 peers, including Soviet-occupied Afghanistan. The smaller size may indicate that some com-munist state economies were beginning to interact more with the rest of the world and less within the communist bloc. After Period 2, this high-level communist bloc is gone. In Period 3, Russia is in a 4 th tier group of 31 mixed nations. In Period 4, it is again in a 4 th tier group, this time with capitalist countries Japan and Australia.

Note that these clusterings are different from what would have been obained if one ignored the temporal dependence and merely tried to match similar patterns of GDP growth. Using ordinary trajectory matching, the top-level Soviet partitions in periods 1 and 2 would not have formed, because these states did not all share the same growth pattern.
Another interesting observation relates to shifting patte rns of dependence and independence among four key entities: The United States, Russia, China, and Western Europe. The U.S. always shows close temporal ties with Canada, and at least part of Europe. It is also always maximally separated from Russia. However, China and Japan change their affiliation with each time period. A final observation is the changing balance of cluster sizes, as indicated by the link thicknesses. This can provide some new insight into shifting balances of power.

Causal modeling or identification of causal relationships have been an area of active scientific research [21], [22]. Traditionally, inference about cause-effect relationshi ps is commonly based on the concept of Granger causality, first proposed by Clive Granger [4] in 1969. Recently, several researchers have combined the notion of Granger causality with graphical models [23], [24] to visualize the cause-effect interactions for multivariate time series data[25] , [5]. However, to the best of our knowledge, no effort has been made to try to simplify and to derive a global view of a temporal causal model. As we argued, this is clearly very important for understanding the interactions among the tim e series variables.

Our work is also related to time series clustering, which has been extensively studied in the data mining and machine learning communities [8], [9], [10], [11]. What differentia tes our work from the existing work is that we focus on the interaction of time series variables. Existing time series clustering methods do not assume that time series interacti on is relevant. Instead, they focus on deriving distance measu res or probabilistic models to capture the similarity between time series. Basically, their goal is to group similar time series into a cluster. However, the goal here is to cluster time series through their causal relationships. As a simple example, two identical time series would not be Granger-causal of one another (adding one time series will not improve the prediction of the time series for itself). Thus, we are not compelling to put them together into the same component.

In summary, we have formulated a novel objective func-tion for the decomposition problem in temporal graphical models. We then introduced an iterative optimization ap-proach utilizing the Quasi-Newton method and generalized ridge regression to minimize the objective function. To improve the efficiency of the Quasi-Newton method on datasets with a large number of variables, we employ a maximum weight independent set -based approach. Our ex-periments on synthetic data demonstrate the effectiveness of our approaches, in terms of clustering accuracy. In additio n, our tests on real GDP data uncover interesting relationship s among countries. In this work, we only consider non-overlapping clusters. However, many real-world datasets have inherently overlapping clusters. We plan to investiga te this problem in the future.

This work is partially supported by NIH 1R01CA141090-0109.

