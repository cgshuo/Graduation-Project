 INEX, the evaluation initiative for content-oriented XML re-trieval, has since its establishment defined the relevance of an element according to two graded dimensions, exhaustiv-ity and specificity. The former measures how exhaustively an XML element discusses the topic of request, whereas specificity measures how focused the element is on the topic of request. However, obtaining relevance assessments is a costly task. In XML retrieval this problem is exacerbated as the elements of the document must also be assessed with respect to the exhaustivity and specificity dimensions. A continuous discussion in INEX has been whether such a sophisticated definition of relevance, and in particular the exhaustivity dimension, was needed. This paper attempts to answer this question through extensive statistical tests to compare the conclusions about system performance that could be made under different assessment scenarios. H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation (efficiency and ef-fectiveness) Measurement, Standardisation XML evaluation, relevance, statistical tests, INEX
In recent years there has been an explosion in the amount of research and development for the retrieval of structured documents. This has been in large part an investigation of XML retrieval systems aimed at supporting content-oriented Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. retrieval. Instead of retrieving whole documents, many XML retrieval systems aim at retrieving document components, i.e. XML elements, of varying granularity that fulfill the user X  X  query. As the number of XML retrieval systems in-creases, so does the need to evaluate their effectiveness. The INitiative for the Evaluation of XML retrieval (INEX) 1 ,es-tablished in 2002, is providing an infrastructure and method-ology to evaluate these XML retrieval systems.

The typical approach to evaluate a system X  X  retrieval ef-fectiveness is with the use of test collections constructed specifically for that purpose. A test collection usually con-sists of a set of documents, topics, and relevance assess-ments. As many related elements within a document may be relevant (and may be retrieved by the XML retrieval sys-tem), the relevance assessments must reflect which elements are better to retrieve than others. To help solve this diffi-culty, INEX defines relevance according to two dimensions, exhaustivity and specificity [6]. Exhaustivity ( e ) measures how exhaustively an element discusses the topic. Specificity ( s ) measures how focused the element is on the topic. That is, it discusses no other, irrelevant topics.

Although there have been arguments against this sepa-ration at the INEX workshops, this solution is believed to provide a more stable measure of relevance than if assessors were asked to rate elements on a single scale. One reason is that assessors are likely to place varying emphasis on these two dimensions when assigning a single relevance value. For example, one assessor might tend to rate highly specific el-ements as more relevant, while another might to be more tolerant of lower specificity and prefer high exhaustivity.
In addition to the dimensions of exhaustivity and speci-ficity, assessments at INEX have historically been done using multiple grades on each dimension. The use of a graded scale was deemed necessary to reflect the relative relevance of an element with respect to its sub-elements. For example, an el-ement may be more exhaustive than any of its sub-elements alone given that it covers all of the aspects discussed in each of the sub-elements. Similarly, sub-elements may be more specific than their parent elements, given that the parent el-ements may cover multiple topics, including irrelevant ones.
However, obtaining relevance assessments is a very tedious and costly task [9]. A continuous discussion in INEX has been whether such a sophisticated definition of relevance, and in particular the exhaustivity dimension, was needed. The ultimate aim of an evaluation is to be able to state http://inex.is.informatik.uni-duisburg.de/ that system A performs consistently better than system B. A simpler definition, e.g. using one dimension, reducing the scale, etc., would be less costly to obtain, and an analysis of the results may arrive at the same conclusion. In particular, it was always felt that having to assess the exhaustivity of an element could be avoided. More precisely, assessors have felt that gauging exhaustivity was a cognitively difficult task to perform, and that the extra burden has led to less consistent assessments [14] compared to those obtained at TREC.
In 2005, as described in Section 2.4, a new assessment procedure was adopted, including a reduced scale for the exhaustivity dimension. The latter was adopted after a thor-ough investigation carried out on the INEX 2004 data, which is reported in detail in Section 4.1. A separate investigation by Piwowarski et al. [10] shows that these changes led to better assessments with respect to the specificity dimension. It is also shown that higher rates of agreement were obtained with the specificity dimension than with the exhaustivity di-mension, even with its reduced scale, thus still questioning the benefit of the exhaustivity dimension.

This paper attempts to provide an answer to the question of whether there is value added by the multi-graded scale for exhaustivity. The analysis in this paper examines this question through the use of quantisation functions to sim-ulate different assessment procedures. These quantization functions map the exhaustivity and specificity dimensions to a single dimension which is used as input to evaluation measures. There is naturally some risk that changing the assessment procedure will change assessor X  X  behavior with respect to the specificity dimension. However, we feel that these simulations enable a more informed decision. This paper presents a thorough statistical analysis of the results when using the different quantisations. From this analysis, we make recommendations about the use of the exhaustivity dimension in XML element retrieval.

We first introduce the evaluation measures and data sets used at INEX (Section 2). Section 3 discusses the statistical significance tests, which we apply to the INEX content ori-ented retrieval tasks of 2004 and 2005 in Section 4. Section 5 summarizes the implications and concludes the paper.
Our investigation of the exhaustivity dimension is based on the INEX 2004 [6] and 2005 [7] data sets and metrics.
In 2004, the document collection consisted of the full-text of 12,107 articles, marked up in XML, from 12 magazines and 6 transactions of the IEEE Computer Society publica-tions, covering the period of 1995-2002, and totaling 494 MB in size, and 8 millions in number of elements. The collection contains scientific articles of varying length. On average, an article contains 1,532 XML nodes, where the average depth of the node is 6.9. The overall structure of a typical arti-cle consists of a frontmatter (containing e.g. title, author, publication information and abstract), a body (consisting of e.g. sections, sub-sections, sub-sub-sections, paragraphs, tables, figures, lists, citations) and a backmatter (including bibliography and author information). In 2005, the collec-tion was extended with total of 4,712 new articles from the IEEE Computer Society, giving a total of 16,819 articles, in size to a total of 764Mb, and around 11 millions elements.
The evaluation in this paper focuses on Content-only (CO) topics, which are traditional IR topics written in natural lan-guage and constrain the content of the desired results, e.g. only specify what an element should be about without spec-ifying what that element is. As in TREC, an INEX CO topic consists of the standard title, description and narra-tive fields. Only the title was used as input to XML systems in both 2004 and 2005. INEX 2004 and 2005 have 29 and 34 topics with relevance assessments, respectively.
In any IR evaluation campaign, participants are asked to submit runs that would satisfy specified retrieval tasks. The retrieval task investigated in this paper is the ad hoc re-trieval for CO topics, which can be described as a simula-tion of how a library might be used. This task involves the searching of a static set of XML documents using a new set of CO topics. It is left to the retrieval system to identify the most appropriate XML elements to return.

In 2005, two CO sub-tasks were investigated, built on as-sumed user behaviors taking into account how the results may be presented. In the focused sub-task the aim was for systems to find the most exhaustive and specific element on a path within a given document and return to the user only this most appropriate unit of retrieval. In the thorough sub-task, the aim was for systems to estimate the relevance of potentially retrievable elements in the collection. This is task that most systems performed up to 2004 in INEX. Due to space constraints, our investigation is based on the thor-ough sub-task (although all conclusions drawn from analysis on this sub-task hold for the focused sub-task).

During INEX 2004 and 2005, participating organizations evaluated each topic set on the document collections in 2004 and 2005, respectively, and produced a list of XML elements as their retrieval results for each topic. The top 1,500 ele-ments returned for each topic X  X  retrieval results were then submitted to INEX. 70 and 55 runs were submitted in 2004 and 2005, respectively, which are used in our investigation.
In INEX 2004, exhaustivity and specificity were both mea-sured on a four-point scale with degrees of highly (3), fairly (2), marginally (1), and not (0) exhaustive/specific. For ex-ample, (2, 3) denotes a fairly exhaustive and highly specific element, which means that it discusses many aspects of the topic of request and the topic of request is the only theme of the component.

Assessors were asked to provide an exhaustivity value and a specificity value for all elements forming the pool to as-sess 2 . To ensure complete assessments, for any element as-sessed as relevant ( e, s &gt; 0), its parent, its children and its sibling also had to be assessed. This assessment procedure led to a very laborious assessment task, with questions re-garding the quality of the assessments; e.g. lower agreement between assessments was found at INEX compared to those reported for TREC [14].

While the definition of the relevance dimensions was the same in INEX 2005, their scales were revised. The scale for exhaustivity was changed to 3 + 1 levels: highly exhaustive
As in TREC, INEX uses a pooling method, to elicitate which elements to assess [9]. (2), somewhat exhaustive (1), not exhaustive (0) and  X  X oo small X  (?). The latter category of  X  X oo small X  was introduced to allow assessors to label elements, which although contain-ing relevant information were too small to sensibly reason about their level of exhaustivity. This change in the exhaus-tivity scale resulted in part from the investigation detailed in Section 4.1. Specificity was measured on a continuous scale with values in [0, 1], where 1 represents a fully specific component (i.e. contains only relevant information). For ex-ample, (2, 0.72) denotes a highly exhaustive element, 72% of which is relevant content.

The assessment procedure used in INEX 2005 was a two-phase process. In the first phase, assessors highlighted text fragments containing only relevant information. The speci-ficity value of an element was calculated as follows: a com-pletely highlighted element had a s value of 1, whereas a non-highlighted element had a s value of 0. For all other elements, s was defined as the ratio (in characters) of the highlighted text (i.e. relevant information) to the element size. In the second phase, assessors assigned one of the 3 + 1 levels of the exhaustivity scale to those elements that intersected with any of the highlighted text fragments.
Although this new assessment procedure reduced the time needed to assess, and higher levels of agreement were ob-tained [8] 3 , it was still felt that similar results in terms of comparing systems effectiveness could be obtained with-out the exhaustivity dimension. This would mean that the assessment procedure would only include the highlighting phase, which is believed would greatly simplify the assess-ment task, both in terms of time and quality.
Given that INEX employs two graded relevance dimen-sions, the evaluation metrics used in INEX (see Section 2.6) require for these two to be combined. The quantization func-tions aim to do just that, by providing a relative ordering of the various combinations of ( e, s ) values and a mapping of these to a single relevance scale in [0 , 1]. Tables 1 and 2 list the quantisation functions used on the INEX 2004 and 2005 data sets, respectively. Some were used officially in INEX as a means to model assumptions regarding the worth of re-trieved elements to users. Others have been defined for this work as a means to test various assessment scenarios.
Strict 4 is used to evaluate retrieval methods with respect to their capability of retrieving highly exhaustive and highly specific elements in INEX 2004. The generalized (Gen 4 )and the specificity-oriented generalized (SOG) functions credit elements according to their degree of relevance, hence al-lowing modeling varying levels of user satisfaction gained from not fully specific and highly exhaustive elements, such as less relevant elements or near-misses. The difference be-tween Gen 4 and SOG is that the former shows slight pref-erence toward the exhaustivity dimension, while the latter assumes that more specific elements are of greater value to the user. The Any Rel quantisation is used to evaluate re-trieval methods that return any relevant element, regardless of their exhaustivity and specificity value (as long as e&gt; 0 and s&gt; 0). This quantisation allows the investigation of an assessment procedure where an assessor is only required to mark which elements contain relevant text.
The agreement level was calculated on very few topics, so any results should be taken as indications.
 Name Function
Strict 5 f ( e, s ):=
Fully Spec f ( e, s ):=
Gen 5 f ( e, s ):=
Gen Lifted f ( e, s ):=
Bin Exh f ( e, s ):=
Both Strict 5 and Gen 5 have the same intent as Strict 4 and Gen 4 , but redefined for the INEX 2005 exhaustivity and specificity scale. Both functions ignore elements assessed as  X  X oo small X . The Gen Lifted quantization function was in-troduced to score too small elements as near-misses. Fully Spec and Bin Exh are quantisation functions that reward elements independently of exhaustivity. They simulate an assessment procedure where all that is required is the high-lighting of relevant text (Section 2.4). Additional judgments on the exhaustivity of the XML elements would not be re-quired to compute these two quantisations, so they are help-ful in the simulation of what conclusions could be made in the absence of the exhaustivity dimension. INEX 2004 and 2005 employ, respectively, inex eval and XCG to measure effectiveness. These measures are used in lieu of the common measures of precision and recall, as they are better suited to handling graded assessments.
This metric [6] applies the measure of precall [11] to ele-ments and computes the probability P ( rel | retr )( x ) that an element is relevant for a given recall level x : where n is sum of the quantisation scores for all relevant ele-ments and NNR x  X  n is the expected number of non relevant elements retrieved until the recall point x is reached. pre-call is designed to handle multiple elements with the same retrieval status value (RSV). We denote the set of elements c at rank i having the same RSV as rank ( i ). The rank at which recall level x is achieved at is: where assess ( c )=( e, s ) is the assessment of the element c and f is one of the quantisation functions in Table 1. That is, r is the minimum rank where the sum of all elements X  quantisation scores up to and including that rank is at least x  X  n . We define nonrel ( i ) as the number of non-relevant elements at rank i: where I is an indicator function which returns 1 when its ar-gument is true and 0 otherwise. We next define the amount of relevance left to be obtained from rank r : NNR x  X  n is estimated as: One can than estimate P ( rel | retr )( x ) for various recall levels and average across queries, as is done for the common eval-uation measure of mean average precision. A main problem with the inex eval metric is that it does not handle overlap-ping elements in the context of XML retrieval evaluation [4], thus a different measure was adopted in 2005 4 .
The XCG measures [5] are extensions of Cumulated Gain [3], which were specifically designed for multi-graded rele-vance. The measures include the user-oriented measures of extended cumulated gain ( nxCG [ i ]) and the system-oriented effort-precision/gain-recall measures ( MAep ).

Given a ranked list of elements 5 , the cumulated gain at rank i , denoted as xCG [ i ], is computed as the sum of the relevance scores up to that rank: For each query, an ideal gain vector, xCI , is derived by filling the rank positions with f ( assess ( c j )) in decreasing order for all assessed elements c j . A retrieval run X  X  xCG vector is compared to this ideal ranking by plotting both the actual and ideal cumulated gain functions against the rank position. Normalised xCG ( nxCG ) is:
The thorough task investigated in this paper does not lever-age ability to penalize overlapping elements of the XCG measures. As such, we present a simplified version of the measure in this paper.
Unlike the inex eval measure, the XCG measures assume a fully ordered list and does not allow ties at a rank. For a given rank i , nxCG [ i ] reflects the relative gain the user accumulated up to that rank, compared to the gain he/she could have attained if the system would have pro-duced the optimum best ranking, where 1 represents ideal performance. In INEX 2005 the officially reported cut-offs for nxCG were i =10, 25, and 50.

The effort-precision ep at a given gain-recall value gr is defined as the number of visited ranks required to reach a given level of gain relative to the total gain that can be obtained. The measure of effort-precision ep is defined as: where i ideal is the rank position at which the cumulated gain of r is reached by the ideal curve and i run is the rank position at which the cumulated gain of r is reached by the system run. A score of 1 reflects ideal performance, i.e. when the user needs to spend the minimum necessary effort to reach a given level of gain. The gain-recall gr is calculated as: where n is the number of elements c where f ( assess ( c )) &gt; 0.
This method follows the same viewpoint as standard pre-cision/recall, where recall (here gain-recall) is the control variable and precision (here effort-precision) is the depen-dent variable. As with standard precision/recall, a non-interpolated mean average effort-precision, denoted MAep , is calculated by averaging the effort-precision values mea-sured at natural recall-point, i.e. whenever a relevant XML element is found in the ranking.
This section reviews related work in statistical significance testing for IR evaluation and presents the approach used in this work. Section 3.1 briefly reviews the two types of er-rors in statistical significance testing. Section 3.2 presents an approach to control the rate of incorrectly identifying statistically significant differences when one makes many comparisons. The bootstrap statistical significance test is presented in Section 3.3. That subsection also presents ex-periments choosing which statistical significance test is most appropriate for the INEX data. Section 3.4 presents and mo-tivates the methodology we use for comparing the effects of using different quantisation functions.

The use of statistical significance testing in IR (Hull [2]) helps researchers make informed decisions about the average performance of two or more systems. For example, we may wish to know whether system A, whose score is 0.34 , is sig-nificantly greater than system B, whose score is 0.28 on the same topic set and corpus. These data are paired; we have scored results for each system on the same topics. As such, an equivalent question is whether the difference between the score of system A and system B is greater than zero.
In this work, we focus on answering this single tailed ques-tion. That is, we consider the hypotheses where  X  AB is the true difference of the mean score between systems A and B. This testing procedure assumes that top-ics are drawn from some probability distribution. The true mean difference is given by this unknown probability dis-tribution. As we do not know  X  AB , statistical significance tests can help us decide whether or not it is appropriate to reject the null hypothesis H 0 and declare (with some level of confidence) that the mean score of system A is greater than the mean score of system B.

One important issue is how one should choose a particular significance test among the myriad of possibilities. Hull [2] outlined a few statistical testing methods for comparing two systems: the paired-t test, the Wilcoxon signed rank test, and the sign test. The paired-t test has the strongest as-sumptions for the test to be valid: the observed differences should be normally distributed. Wilcoxon X  X  signed rank test has the less strong assumption that the differences are sym-metric at the true mean. The sign test makes the weakest assumptions of the tests: on average half of the system dif-ferences should be greater than the true mean.

Generally, with weaker assumptions generally comes lesser power. Power is the probability that the null hypothesis will be rejected. Tests that make stronger assumptions reject the null hypothesis more often. When choosing a significance test, it is desirable to choose the most powerful statistical significance test where the data do not violate the assump-tions of the test. Care must be taken so that assumptions of the test chosen closely match those of the data, otherwise many errors in the statistical analysis may occur.
Statistical significance tests make two kinds of errors. A type I error is a rejection of the null hypothesis when it is in fact true. A type II error results when the null hypothesis is retained when it should be rejected.
 When a statistical test is applied then an acceptable type I error rate is chosen (  X  ) and the test is applied at that level. In other words, the probability of a type I error is less than or equal to  X  . The p-value of a statistical significance test is defined to be the lowest  X  that the test would reject.
Controlling type I error is generally considered more im-portant than type II error, as we do not want to assert that two systems perform differently when in fact they do not. We are generally willing to miss significant differences with the hope that a rejection of the null hypothesis truly iden-tifies a statistically significant difference.

In related work, Sanderson and Zobel [12] observed that there were more type I errors than desirable when comparing many systems and testing at level  X  . They further proposed reducing the type I error rate by dismissing the smaller dif-ferences between systems as not significant when the statis-tical test asserts that the differences are significantly differ-ent. While this will reduce the type I error rate, statistics has provided more principled methods of controlling type I error during statistical significance testing, which we intro-duce in the following subsection.
When comparing only two systems at level  X  , one can be fairly confident that the rejection/retention of the null hy-pothesis was reasonable. However, it also common to com-pare several or many systems in a pairwise manner. When this is done, the probability of observing type I errors is larger. As we wish to control type I error to a reasonable level, we must correct for the fact that we are performing multiple simultaneous hypothesis tests.

A classic way to control this is to control the family wise error rate, which would ensure that the probability of reject-ing any of the null hypothesis falsely is less than or equal to  X  . Hull [2] presented some techniques for testing multiple systems using correction of the family wise error rate. Con-trolling the family wise error rate provides strong assurances about type I error rates. When one compares more than a few systems, controlling the family wise error rate typically results in very few rejections of the null hypothesis; few sys-tem differences are declared significantly different.
To address this problem, one may instead control the false discovery rate, which is the number of type I errors divided by the number of times that the null hypothesis was rejected. Controlling the false discovery rate at level  X  gives us the assurance that no more than  X  times the number of sig-nificant differences identified by the test are type I errors. To gain power over controlling the family wise error rate, we allow that when there are rejections of the null hypoth-esis, we are willing to accept that  X   X  100% of the rejections may be type I errors. For example, in a retrieval experiment where we compare 10 systems and identify 20 statistically significant pair-wise differences at level  X  =0 . 05, we expect that 20  X  0 . 05 = 1 difference will be identified in error.
Benjamini and Yekutieli [1] describes a method to control the false discovery rate. This method operates on the p-values of any statistical significance test. This generality is greatly desirable as we may couple this procedure with the most powerful test that does not violate our assumptions about the data.

Let p (1) ,p (2) ,...,p ( m ) be the p-values resulting from the statistical significance tests in increasing order ( p ( i if i  X  j ). We define where c We then take p ( k ) as the rejection threshold; we reject all null hypotheses where p ( i )  X  p ( k ) . This method for controlling the false discovery rate is powerful when there are many comparisons, as long as the researcher is willing to accept that up to  X  of the differences detected may not be true differences. This is a reasonable trade-off for the greatly increased power over the control for family wise error rate. As the tests statistics when comparing system pairs may be dependent (a particular system may compared many times to other systems), the experiments in this paper use this method under the assumption that the test statistics are not independent.
In this paper we focus our analysis on the outcome of statistical significance tests when comparing all pairs of sys-tems over a variety of quantisation functions. We investigate in this work absolute differences between system scores. In the following sections, we will compare the results of sta-tistical significance tests resulting from the use of different quantisation functions on the INEX retrieval task (Section 2.3). For this work to be as accurate as possible, we wish to choose the statistical significance test that produces the lowest amount of errors. In addition to the tests described by Hull [2], we also consider the bootstrap.
While there are parametric versions of the bootstrap, we will consider the non-parametric bootstrap, as it makes no assumptions about the distribution or continuity of the un-derlying distributions. The bootstrap simulates b repeated observations of the test statistic by sampling with replace-ment from the original data (we sample topics in this work). It is often used to estimate confidence intervals, mean values, and variance [13], but it can also be adapted to perform a statistical significance test. To test the null hypothesis these simulated test statistics  X   X  i AB are sorted in increasing order. Each  X   X  i AB is viewed as an estimate of the test statistic. If  X   X 
AB &gt; 0, then H 0 is rejected and the two systems are de-clared to have different performance. The p-value of the bootstrap test is computed as Another way to think of the p-value for the bootstrap test is that it is the fraction of bootstrap samples where the difference between systems A and B is less than or equal to zero. Generally a large number of simulations should be performed to get reasonable estimates (we take b =10 , 000).
To choose the statistical significance test with the low-est error rate, we examined the error rate of the paired-t, Wilcoxon signed rank, sign, and bootstrap tests on the thorough task of INEX 2005. To estimate the error rates, we perform an experiment similar to that of Sanderson and Zobel [12] and Voorhees and Buckley [15].

As in previous work, we estimated the error rate using 50 repeated splits of the topic set. On each topic set split, we applied each of the statistical significance tests to the system pairs using the first half of the topics. We took and set aside the set of systems where the null hypothesis was rejected (a difference was found). The error rate was defined to be the percentage of systems in this set where the difference between systems X  mean scores on the other half of topics specified by the split was not positive.

When performing this experiment we observed that the bootstrap test made the fewest estimated errors. The error rate for the bootstrap test using the Gen 5 quantisation and the MAep evaluation measure was 8.3% to 14% of error rate observed when using the other significance tests (paired-t, Wilcoxon signed rank, sign). For the Strict 5 quantisation, the error rate of the bootstrap test was 23% to 27% of the other tests. We also observed that the bootstrap test re-jected the null hypothesis on both halves of the topic splits more frequently than the other tests. That is, the bootstrap agrees with itself more than the other tests examined.
What is even more remarkable is that the bootstrap test tended to reject the null hypothesis more often than any of the other tests. For the generalized quantisation and the MAep measure, the bootstrap identified on average 2.1 times more significant differences than the other tests. For the strict quantisation, the bootstrap identified on average 64 times more significant differences. For the generalized quan-tisation applied to the nxCG at 10, 25, and 50 results, the bootstrap test identified on average 20, 8.8, and 3.7 times as many statistically significant differences (respectively) as the other statistical significance tests.

On these measurements of error, it appears that not only does the bootstrap test have a lower type I error rate, it seems to be a more powerful test. As the test seems to reject more of the null hypotheses than the other tests while still maintaining a lower type I error rate we believe that the bootstrap test is retaining the null hypothesis for fewer of the cases when it should truly be rejected. This leads us to believe that the bootstrap test has a lower type II error rate for the INEX data and evaluation measures. Given these observations, we limit the analysis in the rest of this paper to applications of the bootstrap statistical significance test.
Common approaches to compare rankings use measures such as Spearman X  X  and Kendall X  X  rank correlation statis-tics. We avoid using these measures as they can be mis-leading. For example, consider the case where we have two groups of systems of equal size, G 1 and G 2 .Fortwoeval-uation measures produced using quantisation functions Q 1 and Q 2 , all systems perform about the same within either group. However, for both measures all systems in group G 1 perform better than all systems in group G 2 . Suppose that performing statistical significance tests correctly reject the null hypothesis when comparing systems in G 1 to those in G 2 while also correctly retaining the null hypothesis when comparing systems within a group.

If quantisations Q 1 and Q 2 order the systems within each group in reverse order, then any Spearman X  X  rho and Kendall X  X  tau will both be less than 1. For example, if n = 20, then Spearman X  X  rho is 0.5 and Kendall X  X  tau is 0.026. In this case, Spearman X  X  rho does identify a statistically significant correlation between the rankings, while Kendall X  X  tau does not. Similarly, Pearson X  X  correlation would also be less than 1. However, even though Spearman X  X  and the Pearson X  X  tests would probably correctly identify correlation, both tests ig-nore whether two systems can be distinguished from each other or not. A better analysis would conclude that the two measures are equivalent, because the statistical tests made on the pairwise comparisons for either measure would cor-rectly distinguish the groups.

If we directly compare the outcome of statistical signifi-cance tests, we would conclude that the two quantisations are equivalent. It is for this reason we choose to compare the quantisations by examining the results of the statistical tests made when performing pairwise comparisons. Furthermore, comparing the results of the statistical tests will allow us to use easily interpretable statistics, while correlation measures are not generally easy to interpret.

When we compare the statistical decisions made by two different quantisations, we can use the results of the tests performed using one quantisation Q 1 to predict the results of the other quantisation Q 2 . In this case, we treat Q 2  X  X round truth X . We thus can use the standard measures of retrieval performance, where reject ( Q ) is the set of system pairs rejected when applying statistical significance tests to the results when using quantisation Q : Table 3: Quantisation agreement for the INEX 2004 CO Task.
F1( Q 1 ,Q 2 )= 2 These measures give us interpretable measures of correla-tion between the two rankings. When the results of all of the statistical significance tests on both quantisations agree, recall, precision, and F1 will all be one.
This section examines the various quantisation functions used at INEX for the CO retrieval tasks of 2004 and 2005. Section 4.1 reviews previously unpublished analysis of the INEX 2004 data that was used by the organizers of INEX to motivate the reduction of the grades in the exhaustivity dimension in 2005. Section 4.2 presents new results from analyzing the INEX 2005 data.
At the INEX 2004 workshop, assessors commented that multi-graded assessments were time-consuming to perform. There were also concerns that the graded scales may also lead to low inter-assessor agreement. It was generally be-lieved that reducing or eliminating the graded scales would lead to a more consistent and less time consuming assess-ment procedure. In addition, there was also confusion about whether the quantization functions measured different as-pects of the retrieval systems.

In this section we briefly review our motivation for rec-ommending reducing the scale of exhaustivity from 0,1,2,3 used in 2004 to a scale of 0,1,2. We first examine the offi-cial quantisation functions, investigating whether we would draw the same conclusions about systems when using these different quantisation functions (Section 4.1.1). We then in-vestigate whether the 0,1,2,3 exhaustivity/specificity scales were necessary or whether the conclusions could be drawn from 0,1 or 0,1,2 scales (Section 4.1.2). We first review whether the official quantisations used in INEX 2004 identified the same statistically significant differ-ences. From Table 3 we observe that Gen 4 and SOG provide very similar rankings (F1=0.95). There is very high agree-ment about which systems perform statistically significantly differently from each other between these two quantisation functions. Although these two quantisation functions ex-press different user preferences (Section 2.5), they behave very similarly when ranking systems.

However, the Strict 4 quantisation function identifies a quite different set of statistically significantly different system pairs. From Table 3, we see that F1 of the identified system differ-ences of Strict 4 and the Gen 4 and SOG quantisations is 0.75 and 0.73, respectively. This is rather low. Looking at the top part of the table, we see that the Gen 4 and SOG quan-tisations have high recall of Strict 4 system differences (0.96 and 0.92), but that their precision is quite low (0.61 and 0.60). That is, the differences between systems identified when using the Strict 4 quantisation function are roughly a subset of those identified when using either of the Gen 4 SOG quantisation functions.

This is also reflected in the fact that the Strict 4 quantisa-tion function tends to identify fewer statistically significant differences (51% of all possible) than the Gen 4 (79%) and SOG (77%). From this, we observe that the Gen 4 and SOG quantisation functions are better at distinguishing systems than the Strict 4 quantisation function.

In summary, the Gen 4 and SOG quantisation functions behave similarly despite the fact that they emphasize dif-ferent preferences for the exhaustivity and specificity of ele-ments. The Strict 4 quantisation function identifies fewer dif-ferences between systems than the other quantisation func-tions, suggesting that it measures different system behavior. However, the system differences identified using the Strict quantisation function tend to be also identified by the Gen and SOG quantisation functions.
We next investigated whether multiple grades were neces-sary. The Gen 4 and SOG quantisations both rely heavily on the graded scales of exhaustivity and specificity. The Strict quantisation requires knowledge of which components are highly exhaustive and highly specific. Here we wanted to know whether we can predict any of the quantisations used in INEX 2004 without leveraging the exhaustivity and speci-ficity scales.

The Any Rel quantisation function was designed to in-vestigate this question. It gives equal merit to any relevant element, regardless of how exhaustive or how specific each element is. Perhaps surprisingly, we see that the Any Rel quantisation tends to predict the same differences as the Gen 4 and SOG quantisation functions (F1 is 0.97 and 0.94). This suggests that for examining the behavior of systems on the INEX 2004 data, graded exhaustivity and specificity is not necessary to identify the system differences that would be identified by the Gen 4 and SOG quantisation functions.
The analysis on INEX 2004 data resulted in the obser-vation that the Any Rel, Gen 4 , and SOG quantisations all identified a similar set of statistically significantly different system pairs. However, the Strict 4 quantisation identified a smaller set of differences that were typically identified when using the other quantisation functions.

The fact that the Any Rel quantisation function does not need graded assessments has positive implications on the assessment procedure that could be used if the ability to identify the differences found when using the Strict 4 is not important. In such a case, the assessor would only have to mark which elements contain relevant text; no further assessment would be required.

Even with the requirement that the Strict 4 quantisation remain a part of the evaluation, one can reduce the exhaus-tivity and specificity scales. A reduced scale of 0, 1, 2 where 0 corresponds to not exhaustive/specific, 1 to somewhat ex-haustive/specific, and 2 to highly exhaustive/specific would be good enough to identify most of the statistically signif-icant system differences for the Strict 4 ,Gen 4 , and SOG, quantisation functions. Given this analysis, INEX chose to reduce the number of grades in the scale for exhaustivity. No recommendation was made for the specificity scale, as the assessment procedure was changed in INEX 2005 (Sec-tion 2.4). The new highlighting process allowed for a con-tinuous scale of specificity to be calculated automatically.
This section aims to analyze: (1) whether considering el-ements marked as  X  X oo small X  as relevant has a large impact on which systems are identified as statistically significantly different (Section 4.2.1); and (2) what effect further reduc-ing the exhaustiveness scale to a binary scale would have on the conclusions that could be made (Section 4.2.2).
When assessors were judging an article, they could mark all of the remaining un-assessed elements in an article as  X  X oo small X  and continue to the next article. The assessors were trusted to use this feature with care, but this was not always the cases. There were situations where large elements (hundreds of words) were marked as  X  X oo small X . During and after the INEX 2005 workshop, INEX participants expressed concerns that abuse of the  X  X oo small X  may have resulted in poor system rankings.

The reason was that the generalized quantisation (Gen 5 ) assigns a score of zero to these elements marked as  X  X oo small X . To investigate whether this had a large impact on the rankings of systems, the organizers introduced the gen-eralized lifted (Gen Lifted) quantisation, which allowed  X  X oo small X  elements to be considered relevant, but less relevant than those that had an exhaustivity of one or more.
Table 4 shows that there is reasonably high agreement between the Gen 5 and Gen Lifted quantisations as F1 is 0.9 for both nxCG at rank 25 and MAep. In the interests of saving space, we omit results on nxCG at ranks 10 and 50. The results are similar to those of nxCG at a cut-off of 25 elements, although, nxCG at 10 tends to be more variable and less able to distinguish systems whereas nxCG at 50 tends to identify more statistically significant differences. Despite these differences, levels of agreement are similar to those observed for nxCG at 25 and MAep.

It is also illustrative to examine where the disagreements between the quantisation functions occur. Figures 1a and 1b shows this using the MAep evaluation measure. Figure 1a is a scatter plot of the differences identified as signifi-cant using either the Gen 5 or Gen Lifted quantisation. We see that there is high correlation among the differences in MAep, and that when the two quantizations disagree about whether the systems behave differently or not, the differ-ences in MAep tend to be the smaller differences observed on the plot. Figure 1b presents those differences in MAep using a density plot. In the figure the x-axis corresponds to the difference between system pairs using the Gen 5 quan-tisation. The figure clearly demonstrates that most of the errors occur with lower absolute differences. The larger the differences in MAep, the more like each other the two quan-tisations behave. This is encouraging, as the prediction of the Gen Lifted quantisation could be improved with a simple threshold applied to the differences in scores.

In summary, we see that there is reasonably high agree-ment between which system pairs are identified as signifi-cantly different when using the Gen 5 and Gen Lifted quan-tisations. When they do disagree, the differences in scores tend to be smaller than the differences in scores where the quantisations do agree. From this, we conclude that it does not make a large difference in the rankings whether or not the  X  X oo small X  elements are considered relevant.
We next consider whether we can simulate the behavior of either generalized quantisation and the Strict 5 quantisations using only the specificity dimension of relevance. If this were the case, it would eliminate the need for assessors to judge elements with respect to the exhaustivity dimension.
We first ask whether we can simulate the behavior of ei-ther the Gen Lifted or Gen 5 quantisation without the use of the exhaustivity dimension. The quantisation we examine for this is binary exhaustivity (Bin Exh), which simulates an assessment procedure where the user highlights all relevant text (Section 2.5). Table 4 shows that Bin Exh agrees highly with Gen Lifted. F1 is again quite high for both the nxCG at rank 25 and MAep with values of 0.87 and 0.93, respec-tively. Scatter plots and density plots for these comparisons look similar to those in Figures 1a-b, but have been omit-ted to save space. We can conclude that Bin Exh function simulates the behavior of Gen Lifted.

However, Bin Exh is not an ideal predictor of the Gen 5 quantisation function. This could be in part a side effect of treating the  X  X oo small X  elements as relevant. We considered a variant of Bin Exh which did not treat  X  X oo small X  elements as relevant ( f (? ,s ) = 0). With this variant resulted in an F1 of 0.88 and 0.93 for the nxCG at rank 25 and MAep evaluation measures. This is encouraging, and we hypoth-esize that a simple thresholding based on the element size to automatically filter out most  X  X oo small X  elements would gain similar levels of agreement with the Gen 5 quantisation. We leave this to future work.

The ability to reasonably simulate Gen Lifted without the use of the exhaustivity dimension is similar to our findings with the Any Rel quantisation function on the INEX 2004 data (Section 4.1.2). Again we see that a graded scale for exhaustivity is not necessary for simulating rankings that treat somewhat exhaustive elements as relevant.

We now look whether Strict 5 can be simulated. In Sec-tion 4.1.3 we outlined our previous recommendation to pre-serve a graded scale for exhaustivity. Doing so allowed the continued use of a strict quantisation function, which only considers highly exhaustive and highly specific text as rel-evant. We investigate whether the Fully Spec quantisation (Section 2.5), which considers only the highly specific el-ements ( s = 1) as relevant, can be used to simulate the Strict 5 quantisation function. Unfortunately, the Fully Spec is not a good predictor of Strict 5 (Table 4).

The Fully Spec quantisation is able to recall a large por-tion of the differences identified when using the strict quan-tisation, but it also predicts many more differences. Fig-ures 1c-d illustrate where the erroneously predicted differ-ences occur with respect to system differences. In these plots, the Fully Spec quantisation is used to predict the Strict 5 quantisation applied to MAep. We see from these plots that there is a large degree of overlap between the region corresponding to correctly identified differences and those that Fully Spec identified in error. Applying a thresh-old on the differences in score will not improve the predic-tions made. We conclude from this analysis that it is difficult to simulate the behavior of the Strict 5 quantisation without the use of the exhaustivity dimension. We investigated two questions on the INEX 2005 data. We first studied whether considering items assessed as  X  X oo small X  as relevant would greatly change rankings. We found that  X  X oo small X  elements did not have a large impact.
We also investigated the value of the graded exhaustiv-ity scale. We first found that a graded exhaustivity scale is not necessary to simulate the results of the Gen Lifted quantisation. We also found that knowledge of which el-ements are  X  X oo small X  is necessary for good simulation of the Gen 5 quantisation, but we hypothesize this could be ap-proximated with a simple threshold based on the element X  X  length in characters.

As with the Strict 4 quantisation of INEX 2004, we were unable to simulate the Strict 5 quantisation without the use of a graded exhaustivity scale. However, we argue that the Strict 5 quantisation function is not a very useful measure of system performance on the INEX 2005 data. Table 4 shows that the nxCG at rank 25 only identified 15% of all pos-sible system differences as significant, while using MAep, it identified 35% of all possible differences as statistically significant. Both numbers are very low, and very few sys-tems are distinguishable when using the Strict 5 quantisation function. This makes results analysis very difficult, as few conclusions can be drawn with any certainty.

Given that the Strict 5 quantisation proved to be unable to distinguish retrieval systems, we recommend that it be omit-ted from future INEX evaluations. Doing this would open the door to further simplification of the assessment process used at INEX. With only requiring assessors to highlight the most specific text, INEX could make use of the binary exhaustivity (Bin Exh) quantisation function, which only leverages specificity. Since this quantisation function reason-ably simulates the behavior of the Gen Lifted quantisation function currently used at INEX, there would be consistency in the task from INEX 2005 to the coming years.
INEX defines relevance according to a specificity and an exhaustivity dimensions, themselves defined on a graded scale. Obtaining relevance assessments is very costly, in par-ticular in the context of XML retrieval, where elements in addition to documents have to be assessed. For the purpose of comparing retrieval effectiveness, it has been argued in INEX that such a complex definition of relevance was not needed. This paper provides an answer to this argument, through extensive statistical tests.

This paper introduced several statistical tools useful for the evaluation of retrieval systems. In particular, we intro-duced the approach in [1] to control the false discovery rate for multiple test correction. This method is well suited for comparisons of very many retrieval systems.

This paper also introduced a framework for investigat-ing the potential impact of changing an evaluation measure (such as using a different quantisation function) on the sta-tistical conclusions that can be made about system effective-ness. Specifically, we demonstrated the application of this approach to investigating the impact of reducing the grades in a scale and the omission of the exhaustivity scale for XML component retrieval.

Using these tools, we performed analysis of the exhaustiv-ity and specificity dimensions used in INEX 2004 and 2005. In this analysis, we found that the strict quantisations do not distinguish systems as well as the generalized quanti-sations. As such, it is a more difficult evaluation standard. Coupled with our findings that many of the generalized class of quantisations can be simulated without the use of assess-ments of exhaustivity, we feel that it is prudent to drop the strict quantisations from evaluation at INEX.

Another positive side effect of dropping assessment of ex-haustivity would be more topics with assessments. Indeed, INEX has only 34 and 29 topics assessed for the 2004 and 2005 CO tasks, respectively, whereas a typical evaluation at (b) Scaled Density (d) Scaled Density TREC has 50 topics assessed. Voorhees and Buckley [15] and Sanderson and Zobel [12] have observed that more sys-tems are distinguishable when using more topics. Having more topics assessed at INEX would have a similar effect, enabling researchers to draw more reliable conclusions about their research from the evaluation at INEX.

