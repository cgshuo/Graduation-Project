 Fang Han fhan@jhsph.edu Han Liu hanliu@princeton.edu In much of studies on Principal Component Analysis (PCA) it is assumed that the n observations x 1 , . . . , x of a random vector X  X  R d are independent. More-over, in high dimensions, it is commonly assumed that X follows a multivariate Gaussian or sub-Gaussian distribution such that the estimators are consistent. In this paper we focus on a semiparametric method built on the nonparanormal model. A continuous random vector X := ( X 1 , . . . , X d ) T follows a non-paranormal distribution if there exists a set of uni-variate monotone functions f := { f j } d j =1 such that distribution. In this paper we show that the proposed method can loosen both the data independence and Gaussian/sub-Gaussian assumptions.
 Let  X  be the covariance matrix of X . PCA aims at recovering the top m leading vectors u 1 , . . . , u m of  X  . The usual procedures are to estimate the top m lead-ing eigenvectors matrix S . However, there are two drawbacks: (i) when d &gt; n , Johnstone &amp; Lu (2009) show that PCA is incon-sistent. More specifically, let u 1 and eigenvectors of  X  and S . For two vectors v 1 , v 2  X  R d we denote the angle between v 1 and v 2 by  X  ( v 1 , v 2 ). Johnstone &amp; Lu (2009) prove that  X  ( u 1 , converge to 0. (ii) The performances of the estimators rely on independence of the n observations. Their per-formance is unknown if dependence is present among x , . . . , x n .
 A remedy for the inconsistency problem when d &gt; n where card(  X  ) represents the cardinality of a given set. Different sparse PCA procedures have been developed to exploit the sparsity structure: greedy algorithms (d X  X spremont et al., 2008), lasso-type methods includ-ing SCoTLASS (Jolliffe et al., 2003), SPCA (Zou et al., 2006) and sPCA-rSVD (Shen &amp; Huang, 2008), a num-ber of power methods (Journ  X ee et al., 2010; Yuan &amp; Zhang, 2011; Ma, 2011), the biconvex algorithm PMD (Witten et al., 2009) and the semidefinite relaxation DSPCA (d X  X spremont et al., 2004).
 For data dependence, in low dimensions, Skinner et al. (1986) study the behavior of PCA when the selection of a sample of observations depends on a vector of la-tent covariates as, for example, in stratified sampling. Their analysis is based on the normality assumption and that the knowledge of the survey design is known. There are even fewer literatures in high dimensions for the dependent data. Loh &amp; Wainwright (2011) study the high dimensional regression for Gaussian random vectors following a stationary vector regressive pro-cess. Very recently, Fan et al. (2012) analyze the pe-nalized least square estimators, taking a weakly de-pendence structure, called  X  -mixing, of the noisy term into consideration.
 There are several drawbacks of PCA (or sparse PCA): (i) It is not scale-invariant, i.e., changing the measure-ment scale of variables makes the estimates different; (ii) Most estimating procedures require the data to be either Gaussian or sub-Gaussian so that the sam-ple covariance matrix S converges to  X  in a fast rate; (iii) It cannot handle dependent data. Compared with PCA and sparse PCA, Han &amp; Liu (2012) exploit a nonparametric Kendall X  X  tau based regularization pro-cedure, named Copula Component Analysis (COCA), for parameter estimation. They show that COCA is scale-invariant, able to deal with continuous data with arbitrary margins. In this paper, we further generalize their results, showing that COCA can allow weak data dependence. In particular, we provide the generaliza-tion bounds of convergence for both support recovery and parameter estimation for dependent data using our method. We provide explicit sufficient conditions on the the degree of dependence, under which the same parametric rate can be achieved. To our knowledge, this is the first work of analyzing the theoretical perfor-mance of PCA for dependent data in high dimensional settings.
 The rest of the paper is organized as follows. In the next section, we briefly review the nonparanormal fam-ily (Liu et al., 2009; 2012) and the data dependence structure. In Section 3, we introduce the models and rank-based estimators proposed by Han &amp; Liu (2012). We provide our main theoretical analysis of the rank-based estimators for the dependent data in Section 4. In Section 5, we employ the on synthetic data to illus-trate the robustness of COCA to data dependence. We start with notations: Let M = [ M jk ]  X  R d  X  d and indexed by I be denoted by v I . Let M  X  X  submatrix with rows indexed by I and columns indexed by J be denoted by M IJ . Let M I  X  and M  X  J be the submatrix of M with rows in I , and the submatrix of M with columns in J . For 0 &lt; q &lt;  X  , we define the ` q and `  X  vector norms as || v || q := the matrix ` max norm as the elementwise maximum value: || M || max := max {| M ij |} . Let  X  j ( M ) be the j -th largest eigenvalue of M . In particular, are the smallest and largest eigenvalues of M . The vectorized matrix of M , denoted by vec( M ), is defined as: ` 2 sphere. For any two vectors a , b  X  R d and any two squared matrices A , B  X  R d  X  d , denote the inner product of a and b , A and B by The sign d = denotes that the two sides of the equality have the same distributions. For a sequence of random vector { X t }  X  t =  X  X  X  and two integers L &lt; U , we denote X
L := { X t } U t = L . The notation P (  X  ) represents the probability if it is a set inside the brackets and the law (distribution) if it is a random vector inside the brackets. 2.1. The Nonparanormal We first provide the definition of the nonparanormal following Liu et al. (2012).
 Definition 2.1 (The nonparanormal). Let f = { f j } d j =1 be a set of monotone univariate functions. We say that a d dimensional random variable X = ( X 1 , . . . , X d ) T follows a nonparanormal distribution N P N d (  X  , f ) , if We call  X  the latent correlation matrix.
 We next proceed to the invariance property of the rank-based estimator Kendall X  X  tau in the nonpara-normal family. Let X = ( X 1 , . . . , X d ) T be a random vector. Let e X j and e X k be two independent copies of X j and X k . The population version of the Kendall X  X  tau statistic is:  X  ( X j , X k ) := Corr sign( X j  X  e X j ) , sign( X k  X  Let x 1 , . . . , x n  X  R d be n observed data points. The sample version Kendall X  X  tau statistic is defined as: b  X  jk ( x 1 , . . . , x n ) := which is monotone transformation-invariant correla-tion between the empirical realizations of two random variables X j and X k . For x 1 , . . . , x n independent, it is easy to verify that E We denote b R = [ b R jk ]  X  R d  X  d with the Kendall X  X  tau matrix.
 Another interpretation of the Kendall X  X  tau statistic is that it is an association measure based on the notion of concordance. We call two pairs of real numbers disconcordant if ( s  X  t )( u  X  v ) &lt; 0. Kruskal (1958) show that  X  ( X j , X k ) = P ( X j  X  e X j )( X k  X  e X k ) &gt; 0 The following theorem, coming from Kruskal (1958), states the invariance property of the relationship be-tween the population Kendall X  X  tau statistic  X  ( X j , X k and the latent correlation coefficient  X  jk in the non-paranormal family.
 Theorem 2.2. Let X := ( X 1 , . . . , X d ) T  X  N P N d (  X  , f ) . We denote  X  ( X j , X k ) to be the population Kendall X  X  tau statistic between X j and X k . Then  X  jk = sin  X  2  X  ( X j , X k ) .
 Proof. To prove Theorem 2.2, we actually have  X  ( X j , X k ) = P ( X j  X  e X j )( X k  X  e X k ) &gt; 0 = P ( f j ( X j )  X  f j ( e X j ))( f k ( X k )  X  f k ( e = The last equality is coming from Kruskal (1958) X  X  re-sult for Gaussian distribution. 2.2. Mixing Conditions In this section we provide definitions of several mod-els of non-independent data. In particular, we will introduce the notions of strong mixing conditions:  X  -mixing and  X  -mixing. These will be utilized later in analyzing the performance of the proposed method for the dependent data. We first introduce the stationary m-dependence sequences as follows.
 Definition 2.3 (m-dependence). A stationary se-quence X 1 , . . . , X n is said to be m -dependence if and only if (i) X i d = X for i  X  { 1 , . . . , n } and some ran-dom vector X  X  R d ; (ii) For any s, t  X  X  1 , . . . , n } , X is independent of X t whenever | s  X  t | &gt; m . m -dependency is of particular interest in several fields. For example, in genetics and epidemiology, data might contain samples of families and there are correlation between members of the same family. Moreover, m -dependence can be seen as a simplified version of a time series, where X s and X t will often be highly de-pendent if | s  X  t | is small, with decreasing dependence as | s  X  t | increases.
 Next we proceed to some more general weak depen-dency conditions. In particular, we build the depen-dence structure on the mixing sequences. To this end, we first introduce the  X  measure of dependence as fol-lows.
 Definition 2.4 (  X  measure of dependence). Let ( X  , F , P ) be the probability space and A , B  X  X  be two  X  -fields. We define the  X  measures of dependence as:  X  ( A , B ) := sup We then describe the strong mixing conditions. Let X = { X t }  X  t =  X  X  X  be a sequence of random vectors. For  X  X  X  X  L  X  U  X  X  X  , define the  X  -field F U L to be F U L :=  X  ( X i , L  X  i  X  U ). For two probability measures  X  1 ,  X  on the measurable space ( X  , F ), the total variation distance between  X  1 and  X  2 is defined as: With the above notations, the  X  and  X  dependence coefficients are defined as: Definition 2.5. Let X = { X i }  X  i =0 be a sequence of random vectors defined in the probability space ( X  , F , P ) and X j i be the subsequence. For 0  X  L  X  U  X  X  X  , remind that F U L :=  X  ( X i , L  X  i  X  U ) . The  X  and  X  dependence coefficients are defined as: The following lemma connects  X  ij to  X  ( m ). This is coming from Kontorovich &amp; Ramanan (2008) for dis-crete case and a stronger version applicable to contin-uous case can be traced back to Samson (2000). This lemma will play an important role later in analyzing our proposed method. For self-containedness, we in-clude a proof here.
 Lemma 2.6 (Samson (2000)). With the above no-tations, we have  X  ij  X  2  X  ( j  X  i ) .
 Proof. By definition, we have ||
P  X || P ( X n j | X i  X  1 1 = y , X i = x )  X  P ( X n j ) || + || P ( X n j | X i  X  1 1 = y , X i = x 0 )  X  P ( X n j ) ||  X  2  X  ( j  X  i ) .
 Therefore, by the continuity of the probability, we have  X  ij  X  2  X  ( j  X  i ). This completes the proof. In this section, we briefly review the statistical models of Copula Component Analysis (COCA) proposed by Han &amp; Liu (2012). The aim of COCA is to recover the principal components of the latent Gaussian distribu-tion in the nonparanormal family. 3.1. Scale-invariant PCA PCA is not scale invariant, meaning that variables measured in different scales will result in different es-timators (Jolliffe, 2005). To attack this problem, PCA conducted on the sample correlation matrix S 0 instead of the sample covariance matrix S is commonly used. We call the procedure of conducting PCA on S 0 the scale-invariant PCA. It is realized that a large portion of works claiming doing PCA are actually doing the scale-invariant PCA. It is under debate whether PCA or the scale-invariant PCA are preferred in different circumstances and we refer to Jolliffe (2005) for more discussions on it. In the population level, the scale-invariant PCA aims at recovering the leading eigen-vectors of the correlation matrix, which has the same sparsity pattern as the leading eigenvectors of the co-variance matrix. 3.2. Models One of the intuition of PCA is coming from the Gaus-sian distribution. In geometric, the principal compo-nents define the major axes of the contours of con-stant probability for the multivariate Gaussian (Jol-liffe, 2005). However, such a nice interpretation does not exist anymore when the distributions are away from the Gaussian. Balasubramanian &amp; Schwartz (2002) construct examples such that PCA loses in the sense of preserving the structure of the data to the most. However, under the nonparanormal model and considering the monotone transformation f as a type of data contamination, the geometric intuition of PCA comes back.
 In particular, for a positive definite matrix  X  with of  X  and  X  1 , . . . ,  X  d be the corresponding eigenvectors. For 0  X  q  X  1, the ` q ball B q ( R q ) is defined as: when q = 0 , B 0 ( R 0 ):= { v  X  R d :card(supp( v ))  X  R when 0 &lt; q  X  1 , B q ( R q ) := { v  X  R d : || v || q q Accordingly, the model M ( q, R q ,  X  , f ) is considered: The ` q ball induces a (weak) sparsity pattern when 0  X  q  X  1 and has been analyzed in linear regression (Raskutti et al., 2011) and sparse PCA (Vu &amp; Lei, 2012). Moreover, the data are assumed to come from a nonparanormal distribution, which is a strict extension to the Gaussian distribution.
 Inspired by the model M ( q, R q ,  X  , f ), we consider the following global estimator e  X  1 , which maximizes the fol-lowing equation with the constraint that e  X  1  X  B q ( R q for some 0  X  q  X  1: Here b R is the estimated Kendall X  X  tau matrix. The cor-responding estimator e  X  1 can be considered as a non-linear dimensional reduction procedure and has the potential to gain more flexibility compared with PCA, as shown in the analysis of Han &amp; Liu (2012). In this section we provide the theoretical properties of the proposed COCA estimator e  X  1 as obtained in Equa-tion (3.2) for the dependent data. To our knowledge, this is the first work analyzing the theoretical perfor-mance of PCA for the dependent data in high dimen-sions. To provide some insights, we first deliver the rate of e  X  1 converging to  X  1 when the data points are independent from each other. This theorem is coming from Han &amp; Liu (2012).
 Theorem 4.1 (Independence). Let e  X  1 be the global optimum in Equation (3.2) and X  X  M ( q, R q ,  X  , f ) . Let x 1 , . . . , x n be an independent sequence of realiza-tions of X and b R := [sin(  X  2 b  X  jk ( x 1 , . . . , x n as in Equation (2.1) . For any two vectors v 1  X  S d  X  1 and v 2  X  S d  X  1 , let | sin  X  ( v 1 , v 2 ) | = p 1  X  ( v Then we have, with probability at least 1  X  1 /d 2 , sin 2  X  ( e  X  1 ,  X  1 )  X   X  q R 2 q where  X  q = 2  X  I ( q = 1)+4  X  I ( q = 0)+(1+ q &lt; 1) and  X  j =  X  j (  X  ) for j = 1 , 2 . Proof. The key idea of the proof is to utilize the ` max norm convergence result of b R to  X  . See Han &amp; Liu (2012) for a detailed proof.
 It can be observed that the convergence rate of e  X  1 to  X  1 will be faster when  X  1 lies in a more sparse ball. It makes sense because the effect of  X  X he curse of dimen-sionality X  will be decreasing when the parameters are more and more sparse. Generally, when R q and  X  1 ,  X  2 which is the parametric rate Vu &amp; Lei (2012) obtain. In the following, we show that Theorem 4.1 can be applied to derive a support recovery result.
 Corollary 4.2 (Independence). With the settings and notations in Theorem 4.1 held, let If we further have then we have P ( b  X  =  X )  X  1  X  d  X  2 .
 We then generalize Theorem 4.1 to the non-independent cases. Here the notions of m -dependence and  X  -mixing sequences as defined in Section 2.2 are exploited. We first provide an upper bound for the estimator e  X  1 when the data points form an m -dependence sequence.
 Theorem 4.3 ( m -dependence). Let X  X  M ( q, R q ,  X  , f ) and { X t } n t =1 be a m -dependence stationary sequence with X t d = X . Let e  X  1 be the global optimum in Equation (3.2) , where b Equation (2.1) . Let the parameter  X  m,n :=2( m + 1) 2 (1  X  m/n ) + m ( m + 1)(2 m + 1) / (3 n ) represent the effect of dependence on the rate of convergence. Then we have, for any n  X  4 m 2 / (  X  m,n log d ) , with probability at least 1  X  1 /d sin 2  X  ( e  X  1 ,  X  1 )  X   X  q R 2 q where  X  q = 2  X  I ( q = 1)+4  X  I ( q = 0)+(1+ q &lt; 1) and  X  j =  X  j (  X  ) for j = 1 , 2 . Proof. The key is to estimate the convergence rate of b R to  X  in the m -dependence setting. The detailed proof is presented in Han &amp; Liu (2013).
 Remark 4.4. It can be observed in Equation (4.2) and m . Generally, when R q and  X  1 ,  X  2 do not scale is fixed, the rate is optimal.
 Using Theorem 4.3, we provide the support recovery result using a similar technique as the proof of Corol-lary 4.2.
 Corollary 4.5 ( m -dependency). With the settings and notations in Theorem 4.3 held, let . If we further have then we have P ( b  X  =  X )  X  1  X  d  X  2 .
 We next proceed to bound the angle between e  X  1 and  X  1 in a more general setting of data dependence. Here the dependence is quantified by  X  measure as defined in Definitions 2.4 and 2.5.
 Theorem 4.6 (  X  -mixing). Let X  X  M ( q, R q ,  X  , f ) and { X t } n t =1 be a stationary sequence with X t d = X . { 1 , . . . , d } and m  X  N , sup e  X  1 be the global optimum in Equation (3.2) , where b tion (2.1) . Let represent the effect of dependence on the rate of con-vergence. Then we have, supposing with probability at least 1  X  1 /d 2 , sin 2  X  ( e  X  1 ,  X  1 )  X   X  q R 2 q where  X  q = 2  X  I ( q = 1)+4  X  I ( q = 0)+(1+ q &lt; 1) and  X  j =  X  j (  X  ) for j = 1 , 2 . Proof. The main proof is to quantify the bias term, and then utilize Lemma 2.6 to build a bridge between the results in Section 5.1 in Kontorovich (2007) and the desired concentration inequality we need. Detailed proof is presented in Han &amp; Liu (2013).
 Remark 4.7. Sequences satisfying m -dependence is a subset of  X  -mixing sequences. However, Theorem 4.3 provides a faster convergence rate than the re-sult in Theorem 4.6. Moreover, the proof techniques in Theorem 4.3 has interesting points itself. It can be observed that when  X  ( i ) decreases fast (i.e., weak-dependence), the rate is near-optimal. For example, supposing that R q and  X  1 ,  X  2 do not scale with ( n, d ) , when  X  ( i ) = O ( i  X  2 ) , the rate is O P log d n which is optimal. When  X  ( i ) = O ( i  X  1 ) , the rate is O Again, a support recovery result can be provided using a similar technique as the proof of Corollaries 4.2 and 4.5.
 Corollary 4.8 (  X  -mixing). With the settings and notations in Theorem 4.3 held, let . If we further have then we have P ( b  X  =  X )  X  1  X  d  X  2 . In this section we investigate the robustness of COCA to data dependence on the synthetic data. We use the truncated power method proposed by Yuan &amp; Zhang (2011) to approximate the global estimator e  X  1 ob-tained in Equation (3.2). Two procedures are con-sidered here: Pearson : the classic sparse PCA using the Pearson sample correlation matrix; Kendall : the proposed rank-based scale-invariant PCA method using the Kendall X  X  tau correlation matrix. In the simulation study we sample n data points x , . . . , x n from a certain random vector X  X  R d with some type of data dependence. Here we set d = 100. We follow a similar generating scheme as in Han &amp; Liu (2012). A covariance matrix  X  is firstly synthe-sized through the eigenvalue decomposition, where the first two eigenvalues are given and the corresponding eigenvectors are pre-specified to be sparse.
 In detail, we let  X  1  X   X  2  X  . . .  X   X  d be the d eigenval-vectors. Suppose that the first two dominant eigenvec-tors of  X  , u 1 and u 2 , are sparse in the sense that only the first s = 10 entries of u 1 and the second s = 10 entries of u 2 nonzero, i.e., u and  X  1 = 5,  X  2 = 2,  X  3 = . . . =  X  d = 1. The remain-ing eigenvectors are chosen arbitrarily. The correlation matrix  X  0 is accordingly generated and the leading eigenvector of  X  0 is sparse. We aim at recovering the leading eigenvector  X  1 .
 To sample data from the nonparanormal, we also need the transformation functions: f = { f j } d j =1 . Here the following transformation function is consid-ered: There exist five univariate monotone functions h , h 2 , . . . , h 5 : R  X  R and where Here  X  is defined to be the cumulative distribution functions of the standard Gaussian. We then gener-ate n = 100 data points y 1 , . . . , y n such that y i  X  N d ( 0 ,  X  ) where  X  is defined as above. To evaluate the robustness of different methods for dependent data, we suppose that z 1 , . . . , z n follow a stationary vector au-toregressive process as defined in Loh &amp; Wainwright (2011). In detail, we assume that z 1 = y 1 and for some real number 0  X   X   X  1 z Here we have that z i  X  N d ( 0 ,  X  ) forms a dependent random sequence. Finally, we have the data points x , . . . , x n : [Scheme 1] { x i } n i =1 = { z i } n i =1 , with x i  X  N [Scheme 2] { x i } n i =1 = { h ( z i ) } n i =1 where h := { h nonparanormal distribution.
 The final data matrix we obtained is X = ( x 1 , . . . , x n ) T  X  R n  X  d . The truncated power algo-rithm is then employed on X to computer the esti-mated leading eigenvector e  X  1 .
 To evaluate the empirical variable selection property of different methods, we define to be the support sets of the true leading eigenvector  X  1 and the estimated leading eigenvector e  X  1 using the tuning parameter  X  . In this way, the False Positive Number (FPN) and False Negative Number (FNN) of  X  are defined as: FPN(  X  ) := the number of features in b S  X  not in S , FNN(  X  ) := the number of features in S not in b S  X  . Then we can further define the False Positive Rate(FPR) and False Negative Rate (FNR) corre-sponding to the tuning parameter  X  to be FPR(  X  ) := FPN(  X  ) / ( d  X  s ) , FNR(  X  ) := FNN(  X  ) /s. Under the Scheme 1 and Scheme 2 with different levels generate the data matrix X for 1,000 times and com-pute the averaged False Positive Rates and False Neg-ative Rates using a path of tuning parameters  X  . The feature selection performances of different methods are then evaluated by plotting (FPR(  X  ) , 1  X  FNR(  X  )). The corresponding ROC curves are presented in Figure 1. There are several observations we can see from Figure 1: (i) With the increase of the data dependence level, both methods X  performance decreases. (ii) Compared with the Gaussian case (Scheme 1), the difference be-tween Pearson and Kendall are larger when the data are generated from Scheme 2. This coincides with the observations in Han &amp; Liu (2012). (iii) When the data dependence degree  X  increases, Kendall performs better than Pearson in both the Gaussian and Nonparanor-mal cases, meaning that Kendall is more robust to the data dependence than Pearson .
 To explore the empirical performances of difference methods using the truncated power method more, we define an oracle tuning parameter  X   X  to be the  X  with the lowest FPR(  X  ) + FNR(  X  ): In this way, an estimator e  X  1 using the oracle tuning parameter  X   X  can be calculated and we compute the oracle false positive and false negative rates as: We present the means and standard deviations of (FPR  X  , FNR  X  ) in Table 1.
 There are several observations we can see from Ta-ble 1: (i) When  X  is increasing, both methods X  oracle positive and negative rates decrease. (ii) In the per-fect Gaussian case (Scheme 1) where the data points are independent of each other (  X  = 0), there is no statistically significant difference between Kendall and Pearson . (iii) There exist statistically significant dif-ferences between Kendall and Pearson in Scheme 2, no matter how large the degree of data dependence (  X  ) is. (iv) There is a statistically significant difference be-tween Pearson and Kendall for the Gaussian case when  X  = 0 . 4, and Kendall performs constantly better than Pearson when  X  &gt; 0. In all, Kendall is more robust to the data dependence than Pearson . In this paper we analyze both theoretical and empiri-cal performance of a newly proposed high dimensional semiparametric principal component analysis, named Copula Component Analysis (COCA), when the data are dependent. We provide explicit upper bounds of convergence for COCA estimators when the observa-tions are drawn from several different types of non-i.i.d. processes. Our results show that COCA can allow weak dependence. To our knowledge, this is the first work analyzing the theoretical performance of PCA for the dependent data in high dimensions. Our result strictly generalize the analysis in Han &amp; Liu (2012) and the techniques we used have the separate interest for analyzing a variety of other multivariate statistical methods. This research was supported by NSF award IIS-1116730.
 Balasubramanian, M. and Schwartz, E.L. The isomap algorithm and topological stability. Science , 295 (5552):7 X 7, 2002. d X  X spremont, A., El Ghaoui, L., Jordan, M.I., and Lanckriet, G.R.G. A direct formulation for sparse PCA using semidefinite programming . Computer
Science Division, University of California, 2004. d X  X spremont, A., Bach, F., and Ghaoui, L.E. Optimal solutions for sparse principal component analysis.
The Journal of Machine Learning Research , 9:1269 X  1294, 2008.
 Fan, J., Qi, L., and Tong, X. Penalized least squares estimation with weakly dependent data. 2012.
 Han, F. and Liu, H. Semiparametric principal com-ponent analysis. In Advances in Neural Information Processing Systems 25 , pp. 171 X 179, 2012.
 Han, F. and Liu, H. Principal component analysis on high dimensional complex and noisy data. Technical Report , 2013.
 Johnstone, I.M. and Lu, A.Y. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical As-sociation , 104(486):682 X 693, 2009.
 Jolliffe, I. Principal component analysis . Wiley Online Library, 2005.
 Jolliffe, I.T., Trendafilov, N.T., and Uddin, M. A modified principal component technique based on the lasso. Journal of Computational and Graphical Statistics , 12(3):531 X 547, 2003.
 Journ  X ee, M., Nesterov, Y., Richt  X arik, P., and Sepul-chre, R. Generalized power method for sparse prin-cipal component analysis. The Journal of Machine Learning Research , 11:517 X 553, 2010.
 Kontorovich, L. Measure concentration of strongly mixing processes with applications . PhD thesis, Carnegie Mellon University, 2007.
 Kontorovich, L.A. and Ramanan, K. Concentration inequalities for dependent random variables via the martingale method. The Annals of Probability , 36 (6):2126 X 2158, 2008.
 Kruskal, W.H. Ordinal measures of association. Jour-nal of the American Statistical Association , 53(284): 814 X 861, 1958.
 Liu, H., Lafferty, J., and Wasserman, L. The nonpara-normal: Semiparametric estimation of high dimen-sional undirected graphs. The Journal of Machine Learning Research , 10:2295 X 2328, 2009.
 Liu, H., Han, F., Yuan, M., Lafferty, J., and Wasser-man, L. High dimensional semiparametric gaussian copula graphical models. Annals of Statistics , 2012. Loh, P.L. and Wainwright, M.J. High-dimensional regression with noisy and missing data: Prov-able guarantees with non-convexity. Arxiv preprint arXiv:1109.3714 , 2011.
 Ma, Z. Sparse principal component analysis and iter-ative thresholding. Arxiv preprint arXiv:1112.2432 , 2011.
 McDiarmid, C. On the method of bounded differences. Surveys in combinatorics , 141(1):148 X 188, 1989. Raskutti, G., Wainwright, M.J., and Yu, B. Minimax rates of estimation for high-dimensional linear re-gression over ell q -balls. Information Theory, IEEE
Transactions on Information Theory , 57(10):6976 X  6994, 2011.
 Samson, P.M. Concentration of measure inequalities for markov chains and phi -mixing processes. The Annals of Probability , 28(1):416 X 461, 2000.
 Shen, H. and Huang, J.Z. Sparse principal component analysis via regularized low rank matrix approxima-tion. Journal of multivariate analysis , 99(6):1015 X  1034, 2008.
 Skinner, CJ, Holmes, DJ, and Smith, TMF. The effect of sample design on principal component analysis.
Journal of the American Statistical Association , 81 (395):789 X 798, 1986.
 Vu, V.Q. and Lei, J. Minimax rates of estimation for sparse pca in high dimensions. Arxiv preprint arXiv:1202.0786 , 2012.
 Witten, D.M., Tibshirani, R., and Hastie, T. A pe-nalized matrix decomposition, with applications to sparse principal components and canonical correla-tion analysis. Biostatistics , 10(3):515 X 534, 2009. Yuan, X.T. and Zhang, T. Truncated power method for sparse eigenvalue problems. Arxiv preprint arXiv:1112.2679 , 2011.
 Zou, H., Hastie, T., and Tibshirani, R. Sparse prin-cipal component analysis. Journal of computational
