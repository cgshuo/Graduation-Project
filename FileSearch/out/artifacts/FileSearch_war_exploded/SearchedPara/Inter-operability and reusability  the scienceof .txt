 Abstract Annotating linguistic data has become a major field of interest, both for supplying the necessary data for machine learning approaches to NLP applications, and as a research issue in its own right. This comprises issues of technical formats, tools, and methodologies of annotation. We provide a brief overview of these notions and then introduce the papers assembled in this special issue.
 Keywords Linguistic annotation Annotation tools Inter-operability Over the past twenty years, with the surging interest in supervised machine learning techniques, annotating linguistic data has become a central task in natural language processing. Correspondingly, the community has recognized that annotation and its methodological issues is a research area in its own right; this has lead, inter alia ,to the founding of the ACL X  X  special interest group on annotation (SIGANN) 1 and to a series of workshops entitled Linguistic Annotation Workshop (LAW) . The papers presented in this journal volume resulted from selecting high-quality papers that have originally been presented at the third workshop of this series (LAW III), 2 which was held in conjunction with the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL/IJCNLP) in Singapore, in August 2009. Authors had been invited to submit an extended version of their paper to this journal, and the ensuing reviewing process resulted in the set of five papers assembled here. Given the breadth of goals and subfields of Computational Linguistics and Language Technology today, one might wonder whether the widely different annotation tasks involved have enough in common to warrant the recognition of annotation as a research issue of general interest. Even though some may still view annotation as pure utilitarian data preparation for both training and evaluation of NLP tasks, we would like to argue that it plays a crucial role as the most explicit representational result of natural language processing that is both replicable and verifiable. Up to now, annotation offers the only alternative to empirically simulate human linguistic intelligence and ability. In other words, annotation is the science of NLP, the component of NLP research that can be carried out without being linked to a specific application. In fact, this stand-alone potential underlies most of the current issues in the research on annotation, such as the inter-operability of different annotation schemes and the reusability of annotated data.

Yet, the most convincing evidence for the value of an annotation task remains to be its direct contribution to the success of one or more NLP applications. How to manage and optimize both the independence and inter-dependence between annotation and NLP applications will continue to be the central issue in the study of annotation.

For one thing, the issue of technical formats is obviously underlying all annotation efforts, and it would not be wise to start designing XML schemata from scratch whenever some research project embarks on annotating linguistic data for some purpose. Ideally, the project would simply adopt an existing format that has proved its utility for similar efforts already, and that comes with an assortment of tools for processing the data files. Even though the wide range of kinds of data in our field makes it difficult to conceive a single format that would be equally useful for, say, prosodically annotated speech and multilingual, aligned text copora alike, the issue of standardizing formats is highly relevant and has lead to important results already. One important issue is the debate of whether standards are meant to be applied across the board at actual data set level, or are meant to be merely a common exchange platform that divergent data sets can be mapped to. The first view aims to achieve maximal sharability, at the possible expense of losing idiosyncratic information. It seems that the field has converged on the latter approach, which ensures inter-operability while allowing data to be maintained in a distributed way, so that the widest range of linguistic information can be encoded.
In conjunction with data formats, the second issue in reusability concerns the annotation tools that are designed to have human annotators effectively and reliably produce analyzed data. When planning a tool for a specific project, it is not trivial to decide whether it should be tailored to the particular immediate task (thus enabling efficiency) or be generalized toward a family of related tasks (thus enabling reusability, possibly paying a price of reduced effectiveness for the immediate task). By sharing design ideas in the community, interaction with related annotation projects elsewhere can be initiated, the demand for certain types of tools be assessed, and thus the balance between building an idiosyncratic yet effective project-specific tool and a customizable, more generic tool for a wider community of users can be achieved. Furthermore, certain issues of tool usability and interface design are common to many annotation efforts, and sharing experiences on how particular HCI-related choices lead to more or less effective annotation is very valuable.

Besides the technical matters, several aspects of annotation methodology are common to many, if not all, projects that involve human annotation of linguistic data. This concerns primarily the drafting of annotation guidelines, decisions on training, and X  X fter the fact X  X he evaluation of inter-annotator agreement (or, in the social sciences: inter-coder reliability). The discussion on suitable evaluation measures, such as Cohen X  X  kappa or Krippendorf X  X  alpha, is well-known. An issue that probably deserves some more attention, however, is the role of guidelines and training: The decision on how much prescription and detail to state in the guidelines, and on how much training and discussion is done between annotators and  X  X xperts X  clearly has a large influence on the nature of the resulting data, and on agreement values. Consequently, when comparing, say, kappa values that are being reported by different researchers on similar annotations tasks, those factors need to be taken into consideration (as well as issues of data set selection, of course). But, features of guideline formulation and the training process are hard to pin down, let alone to quantify; making progress here toward more meaningful comparability of annota-tions results thus constitutes a major goal for future research.

We wish to mention just one more methodological issue that has gained much attention recently, namely the idea of speeding up corpus creation by carefully deciding which instances of data should be annotated and which should not. For many applications, it is not very useful to have essentially the same phenomenon in the training data annotated many times; instead, it is important to make sure that the interesting, or more difficult, constellations in the data are being annotated, so that insights can be derived. The area of active learning is exploring this from the perspective of making machine learning more effective, but the question arises all the same in many corpus exploration efforts: Should one aim at an, in some specific sense, balanced corpus and annotate it completely, so that distribution studies can be undertaken X  X r should one make sure that the annotated data contains enough  X  X nteresting X  cases rather than very many  X  X oring X  cases, when the research interest is more one of exploring particular linguistic phenomena.

The papers in this volume all address one or more of the aspects we have just introduced. On the side of annotation formats, Nancy Ide and Keith Suderman observe that two popular document processing frameworks, GATE 3 and UIMA, 4 peruse different underlying XML formats, which hinders inter-operability between modules that are embedded in the distinct frameworks. The authors propose that the GrAF format, which was designed as a general XML format for many linguistic annotations, can serve as a  X  X  X ivot X  X  to mediate between processing text with modules residing within GATE and UIMA. A second paper dealing primarily with technical problems is the one by Christian Chiarcos et al . which studies the problem of tokenization X  X .e., a subtask of basically any linguistic processing effort. The authors show that different  X  X  X tandard X  X  tokenization procedures make rather different decisions on certain phenomena where multiple distinct tokenizations are equally plausible, and, similar in spirit to the Ide/Suderman paper, they propose a  X  X  X ivot X  X  format that can mediate between systems operating with conflicting tokenizations of the same data.

Turning to annotation methodology, and in particular to the question of efficiency in annotation, Ines Rehbein et al . investigate the idea of replacing  X  X  X ull X  X  human annotation with an automatic pre-annotation and a subsequent phase of human re-labelling (correction). The authors consider the case of frame-semantic annotation and report on experiments where they varied the amount of automatic pre-annotation, and found different consequences for annotation efficiency and the quality of the resulting data set.

The issue of automatic versus manual annotation is also in the focus of the paper by Marke  X  ta Lopatkova  X  et al . Working with Czech, they observe that the automatic syntactic parsing of complex sentences does not yield sufficient quality for many purposes nowadays, and thus they explore the idea of a two-stage analysis, where the rough breakdown of the sentence in linguistically-motivated segments is computed first, and detailed syntactic analysis then follows on the basis of that segmentation.

In contrast to the well-studied sentence syntax, many phenomena on the discourse level are only beginning to be addressed in corpus annotation. Reporting on a specific discourse annotation project, Stefanie Dipper and Heike Zinsmeister are concerned with abstract anaphora X  X hat is, reference not to simple entities in the context (as via most personal pronouns) but to eventualities or complex configurations thereof, as it can be created with demonstrative pronouns. The authors explain their experiences with applying this problem to German data, and provide results on correlations between certain types of anaphors and their antecedents.

Finally, as the editors of this volume, we wish to thank our colleagues who had helped organizing the LAW III workshop: the members of the SIGANN board, and the members of the program committee, whose help was invaluable for collecting a solid workshop program, which in turn allowed for compiling this issue of follow-up papers.
