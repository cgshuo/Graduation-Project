 With the large number of audio signals such as speech and music data that can be retrieved through int ernet services, automatic cl assification and retrieval methods have received much attention recently from researchers in this field. In a content-based retrieval system for audio signals, audio signal classification normally involves the processing of two main tasks: audio feature extraction and a classification task that uses the extracted features to discriminate the classes. Audio data descriptions used for the similarity measure of audio signals in fea-ture extraction method include Mel-Frequency Cepstrum Coefficients (MFCC), the timbre, the rhythm, pitch-related f eatures, FFT coefficients, Linear Predic-tion (LP) coefficients, MPEG filterbank com ponents, the spectral centroid, the spectral flux, the zero crossing rate, the spectral roll-off, low order statistics, and delta coefficients [1,2]. Among these features, MFCC, which was originally developed for speech recognition systems[3,4], has been widely used in various audio information retrieva l tasks of late. Recent studies indicate that classifica-tion schemes with MFCC features outperform other existing features at a similar time level [5,6].

After the features are determined and extracted from audio data sets, a clas-sifier is employed to classify the genre of a given audio data. In recent years, different clustering methods have been proposed for automatic discrimination of speech and music signals. Among thes e, the Gaussian Mixture Model (GMM) provides notably enhanced performance over conventional models while having a major impact on the computational complexity[1,7]. A GMM is obtained from training data by considering audio signals as mixtures of Gaussian Probability Density Functions (GPDFs). The GPDF model contains a mean vector and a covariance matrix. From the trained GMM, the classification decision is based on the likelihoods of the audio feature vectors and given statistical models.
Among various clustering algorithms, the k-means algorithm[8] is one of the most widely utilized unsupervised learning algorithms. The strengths of the k-means algorithm include its convergence speed and its applicability to large data. Meanwhile, the Self-Organizing Map (SOM)[9], another conventional algorithm, finds the winner neuron, which is the closest to a given datum, and updates the synaptic weights of the winner and its neighbors. The clustering quality of SOM heavily depends on the initial weight values, the sequence of data, the learning coefficient and the number of total iterat ions [10]. Pedro et al. successfully used SOM for a music genre classification problem[11]. Grimaldi et al. [12] used a set of features based on a discrete wavelet packet transform (DWPT) to represent a music track. The classification performance can be evaluated by four alternative classifiers: simple k-nearest neighbor, o ne-against-all, Round-Robin, and feature subspace based ensembles of nearest neighbor classifiers. One of the disadvan-tages of these algorithms is an inability to deal with data in which boundaries are non-linear, in which case the mixtures of the GMM are inefficient.
In order to improve the stability of clustering results from the k-means al-gorithm and SOM, the Centroid Neural Network (CNN) algorithm introduces the concept of reward and punishment to the winner and the loser neurons [10]. Furthermore, in order to obtain a closer solution to the best clustering result, CNN starts by setting the number of groups at 2 and increases the number of groups one by one until reaching the predetermined number of groups. In most experiments, CNN has shown superior performance to conventional algorithms including SOM and Differential Competitive Learning [13].

The approach presented in this paper is based on CNN and the Bhattacharyya kernel[14,15] for clustering GPDF data of a GMM. Since a Gaussian distribution captures only a limited part of the statistics in the empirical distribution of the vectors, those vectors are first mapped in a feature space via the minor kernel. The Bhattacharyya affinity is computed in the feature space, thereby making it possible to capture more information of the empirical distribution [16].
The remainder of this paper is organized as follows: Section 2 briefly sum-marizes the CNN. MFCC, the extracted feat ure vector, is presented in Section 3. A summary of the Bhattacharyya distance and the Bhattacharyya kernel is given in Section 4 and BK-CNN is outlined in Section 5. Section 6 provides the experiments involving practical audio data set and presents the results. Finally, conclusions are given in Section 7. The CNN algorithm [10] is an unsupervised competitive learning algorithm based on the classical k-means clustering algorithm [8]. It finds the centroids of clusters at each presentation of the data vector . The CNN first introduces definitions of the winner neuron and the loser neuron. When a data x i is given to the network at the epoch (k), the winner neuron at the epoch (k) is the neuron with the minimum distance to x i . The loser neuron at the epoch (k) to x i is the neuron that was the winner of x i at the epoch (k-1) but is not the winner of x i at the epoch (k). The CNN updates its weights only when the status of the output neuron for the presenting data has changed when compared to the status from the previous epoch.

When an input vector x is presented to the network at epoch n ,theweight update equations for winner neuron j and loser neuron i in CNN can be sum-marized as follows: where w j ( n )and w i ( n ) represent the weight vectors of the winner neuron and the loser neuron, iteration, respectively.

The CNN has several advantages over conventional algorithms such as SOM or k-means algorithm when used for clustering and unsupervised competitive learning. The CNN requires neither a predetermined schedule for learning gain nor the total number of iterations for clustering. It always converges to sub-optimal solutions while conventional algorithms such as SOM may give unstable results depending on the initial learning gains and the total number of iterations. More detailed description on the CNN can be found in [10,17]. In this work, the problem of automatic classification of audio signals is viewed as a pattern classification problem wher e a music sample, considered as an audio signal, is represented in terms of feature vectors. The aim of feature extraction is to represent audio data in a compact and descriptive manner such that it is efficient to deal with when applying learning algorithms. MFCC has been widely used for speech recognition due to its ability to capture the perceptually most important parts of a spectral envelop e of audio signals[3]. It has been proven that MFCC is an effective tool in automatic speech recognition and in modelling the subjective frequency content of audio signals. The input signal is divided into the number of frames and the MFCC is computed at each frame. In this paper, in order to represent the characteristics of the audio signal, the mean and deviation of the MFCC are adopted as the feature vectors.
 4.1 Bhattacharyya Distance The Bhattacharyya distance has been widely used in the statistical pattern recognition[16]. This measure of divergence, D ( G i ,G j ) is a measure between two Gaussian distributions and it defined as follows: where  X  i and i denote the mean vector and covariance matrix of Gaussian distribution G i , respectively. T denotes the transpose matrix. 4.2 Bhattacharyya Kernel Kernel-based methods are well-established tools that are useful in a variety of contexts including classification, regre ssion, density estimation, and clustering problems. They are known to represent complex decision boundaries very effi-ciently and generalize well to unseen data during the training stage. The Bhat-tacharyya kernel for GPDF data is defined as follows[14]: where BK ( x ( n ) , w j ( n )) is the Bhattacharyya with a kernel distance between two Gaussian distributions, x (n) and w j ( n ).

Each set of vectors was suggested to fit a Gaussian distribution. The kernel value between the two sets of vectors is t hen defined as the Bhattacharyya affinity between the two corresponding Gaussian distributions, which can be computed in a closed form. The energy function with a kernel can be written in the feature space with the mapping function  X  : x i ( j ) denotes the data j in the cluster i.

In order to find the minimum of the objective function with a kernel, the steepest gradient descent algorithm ca n be adopted and the update equations for BK-CNN can be derived. As is the case of CNN, the BK-CNN updates its weights only when the status of the output neuron for the presented data has changed when compared to the status from the previous epoch. When an input vector x is presented to the network at epoch n , the weight update equations for the winner neuron j and the loser neuron i in the BK-CNN can be summarized as follows: where w j ( n )and w i ( n ) represent the weight vectors of the winner neuron and the loser neuron, iteration, respectively.

More detailed information on CNN with the Bhattacharyya Kernel can be found in [18]. For the evaluation of the proposed audio signal classifier based on BK-CNN, two data sets, a speech data set and a music data set, consisting of 1,000 audio signals are collected and used for experiment s. The speech data set, which contains 200 signals of 30 seconds length each, i s collected from BBC Radio News. The speech class includes both male and fema le speakers, recorded both in studio and telephonic environments with a variable amount of background noise. The sampling rate was 44.1k Hz, quantized with 16 bits and only one channel (mono). The second data set consisting of 800 sign als of 4 music genres (jazz, folk, rock, and hip hop), as shown in Fig.(1), was downloaded from an internet site [19]. In order to extract multi-dimensional MFCC feature vectors for audio signals, we use jAudio [20], an application software designed for a variety of music information retrieval tasks.

First, the speech/music classification task was performed. In this case, 200 speech data and 200 randomly selected mu sic data from various music genres were employed. Each of the two data sets is partitioned into two subsets: 150 randomly selected samples are used for the training data set and the remaining 50 samples are used for the test data set. Experiments are performed with 50 different combinations of training and test data sets. The classification accu-racies shown in Table 1 are the means of the obtained accuracies from the 50 different combinations of the training and test data sets. As can be seen from Table 1 for the speech/music classification task, all three algorithms almost per-fectly distinguish speech signals from music signals. However, only the BK-CNN successfully discriminates music signals from speech signals. This implies that the Bhattacharyya kernel for GPDF data is a critical addition to CNN for the speech/music classification task.

Another experiment related to the music genre classification problem was performed. Each of 4 data sets is again partitioned into two subsets: 150 ran-domly selected samples for the trainin gdatasetandtheremaining50samples for the test data set. Experiments are performed with 50 different combinations of training and test data sets. The classi fication accuracies reported in Table 2 and Table 3 are the means of the obtaine d accuracies from the 50 different cases of the training and test data sets. The SOM, CNN, and BK-CNN are evalu-ated through the experiments. As shown in Table 2, overall accuracies of 70.5%, 73.6%, and 87.5% are obtained using the SOM, CNN and BK-CNN, respec-tively. When compared to the speech/music classification task, the music genre classification problem is considerably more complic ated. However, the BK-CNN still outperforms the CNN by 14% in terms of classification accuracy and the usefulness of the Bhattacharyya kernel in the CNN is again demonstrated by this experiment. In this paper, a novel approach for the classification of audio signals using the Bhattacharyya Kernel-based Centroid N eural Network (BK-CNN) is proposed. Experiments were performed for two data sets, a speech data set and a music data set, consisting of 1,000 audio signals. Experiments on the speech/music clas-sification task were first performed. The results show that all three algorithms (SOM, CNN, BK-CNN) almost perfectly identify speech signals from music sig-nals. However, only the BK-CNN successfully discriminates music signals from speech signals. In the subsequent experiments, the music genre classification problem was considered. Four music genres, jazz, folk, rock, and hip hop, were trained and tested using SOM, CNN, and BK-CNN. Results shows that the BK-CNN outperforms the SOM and CNN by 17 % and 14%, respectively, in terms of classification accuracy. The experiments and results clearly show that integration of the CNN and the Bhattacharyya kernel provides an efficient tool for the audio signal classification problem.
 This work was supported by the Korea Science and Engineering Foundation (KOSEF) grant funded by the Korean government (MOST)( Grant No.: R01-2007-000-20330-0).

