 This paper aims to quantify two common assumptions about social tagging: (1) that tags are  X  X eaningful X  and (2) that the tagging process is influenced by tag suggestions. For (1), we analyze the semantic properties of tags and the re-lationship between the tags and the content of the tagged page. Our analysis is based on a corpus of search keywords, contents, titles, and tags applied to several thousand popu-lar Web pages. Among other results, we find that the more popular tags of a page tend to be the more meaningful ones. For (2), we develop a model of how the influence of tag sug-gestions can be measured. From a user study with over 4,000 participants, we conclude that roughly one third of the tag applications may be induced by the suggestions. Our results would be of interest for designers of social tagging systems and are a step towards understanding how to best leverage social tags for applications such as search and information extraction.
 H.4 [ Information Systems Applications ]: Miscellaneous Experimentation, Theory Social tagging, Search
Social tagging has recently received a wide adoption by various Web 2.0 services such as social book-marking and the tagging of blogs, photos, music, and videos. While in many of these applications the primary goal is to serve the needs of search Cambridge.
 individual users (e.g. the organization of personal bookmark collections and their later retrieval) the idea is that the tag s should also help other users to browse, categorize and find items. Furthermore, tags are used for information discovery, sharing, and community ranking. Going beyond that, tags could be useful for tasks such as search, navigation or even information extraction.

However, it is not obvious how tags can be exploited best for these tasks because the nature of tags is not en-tirely clear. Wikipedia, for example, broadly relates tags to the description , classification , and search of information Other descriptions stress the use for personal organization or for re-finding 3 items. Thus, it is not clear what pur-pose the tags actually serve. Given this freedom, users tag according to their own gusto. As a result, the choice of tags varies widely: Sometimes the tags identify an item (like  X  X adonna X ), sometimes they identify the owner of the item, and sometimes they give subjective assessments (like  X  X unny X ) [5]. They may also be purely organizational (like  X  X oread X ) or completely unintelligible to another person (li ke  X ##### X ). Such organizational and personal tag creations would be of less use for semantic applications. These appli-cations would rather require tags that carry  X  X eaning X  in some sense. However, it is not clear what proportion of tags actually falls in this category. In other words, the question is (RQ 1): How  X  X eaningful X  are tags?
To some degree, social tagging environments can steer the tagging process by providing tag suggestions. The sugges-tions guide users based on the tags of other users. Since the suggestions act as a feedback loop, they may be powerful enough to amplify trends or possibly also to distort them. They may boost the number of meaningful tags or they may bring forward less meaningful creations. As the functional-ity of the suggestion mechanism is under the control of the system designer, this opens up the possibility of influencing the resulting tags and their meaningfulness. Thus, the ques-tion is (RQ 2): In what ways are users influenced by tag suggestions? In this paper, we aim to shed light on these issues. http://en.wikipedia.org/wiki/Tag_(metadata) http://del.icio.us/help/tags http://flickr.com/help/tags We first study the semantic properties of tags in detail. Then, we develop a model of how tag suggestions influence the user. This allows us to quantify the influence of tag sug-gestions. It also allows us to study the influence of different interface design choices. Our analyses are based on (1) a corpus of search keywords, contents, titles, and tags applied to several thousand popular Web pages and (2) results of a Web-based user study that we conducted with 4,000+ par-ticipants.
 Our main results are as follows:  X  We found that up to 50% of the tag applications may be  X  X ot meaningful X . This contrasts with much lower pro-portions of non-meaningful terms in document content and queries.  X  Our analysis shows that the more popular a tag is, the more likely it is to be meaningful. In other words, aggre-gating the top tags of a document biases to filtering out the meaningful tags. This is not be a priori clear as some non-meaningful words can be rather common (such as  X  X oread X  or  X  X odo X ).  X  Our analysis also validates that the more users tagged a document, the more meaningful the top popular tags are. However, we show that the meaningfulness increases signif-icantly only if the document is tagged by more than 100 people. This may have consequences for small-scale tagging scenarios (such as enterprize environments).  X  Our analysis indicates that tags applied to a document typically intersect more with the queries and the title than with the content. This suggests that social tags could prove useful for search applications.  X  We propose a novel metric (the imitation rate ) to esti-mate how much the user was influenced by the tag sugges-tions. Note that it is non trivial to estimate this parameter, because the user may apply tags that have been suggested without actually paying attention to the suggestions.  X  Our metric allows us to conclude that up to 1 in 3 tags may be induced purely by presence of tag suggestions. This result implies that the popularity of tags observed in exist-ing systems with tag suggestions may be skewed. On the positive side, it implies that tag suggestions could provide an effective control over the tags.  X  We further evaluated various other factors that affect the tagging process, such as the ordering of the suggested tags, the suggestion set size, and the user interface design.
The rest of the paper is structured as follows: We first give an overview of related work in the domain of social tagging. In Section 2, we present our data sets and methodology. Sec-tion 3 characterizes the semantic properties of tags. Section 4 introduces our probabilistic model of tagging and finally, Section 5 presents the results of our user study.
Social tagging has attracted much attention lately. Golder and Huberman [5] see tags as a flat organizational structure, as opposed to a tree-like categorization structure. They pro-pose categories for the purposes that tags serve and analyze the emergence of consensus among taggers. Chi [2] et al. analyze three wisdom-of-crowds areas, one of which are the tags used in del.icio.us. They analyze the entropy of tags and the relationship between rare and frequent tags. Sen et al. [11] study the tagging process from a user X  X  perspective. They propose a model of components that have an influence on the user and analyze how users react to different types of tag suggestions. However, their analysis remains mainly on the level of 3 classes of tags. Xu et al. [16] introduce the concept of tags as facets , i.e. values for attributes such as ti-tle , composer and artist for a music piece. They propose a taxonomy of tags and derive desiderata for  X  X ood X  tags. Finally, they propose a tag suggestion algorithm that aims at maximizing the proportion of such tags. Santos-Neto et al. [9] study the tagging systems CiteULike and Bibsonomy. They analyze the tagging activity and vocabulary size of users and define a neighborhood of users. They show that the neighborhood is a powerful tool for predicting tag ap-plications. Mika [7] proposes to model a tagging system as a tripartite graph of users, tags and items. He proposes methods to extract ontological knowledge from tags. Sen et al. [10] propose feedback methods by which users can rate the quality of tags. Their user study allows them to derive desiderata for tagging interfaces. In summary, even though many interesting aspects of tagging have been studied, to our knowledge, the semantic properties of tags and the in-fluence of tag suggestions have not been quantified so far.
To assess the semantic properties of tags, we use the se-mantic databases YAGO [13] and WordNet [4]. For our pur-pose, we consider them dictionaries that associate to a word one or multiple meanings. For example, the word  X  X ava X  can mean the programming language, the coffee or the is-land. Beyond that, WordNet also assigns to each word one or multiple word classes. For example, the word  X  X ouse X  belongs to the class of nouns (when used in the sense of  X  X uilding X ) and to the class of verbs (when used in the sense of  X  X o house somebody X ). WordNet contains 155,287 words, while YAGO, partly intersecting with WordNet, contains 2.8 million words. We have combined YAGO and WordNet and refer to this combination as the dictionary . WordNet is de-signed to cover the lexical words of the English language. YAGO builds on Wikipedia and is designed to cover well-known proper names (such as names of cities, famous people and organizations). Hence, we assume that our dictionary covers the most common English words. We obtained a sample of the logs of popular queries to an Internet search engine [15]. The logs associate to each query the first result page that the user clicked on. Our sample contains 1,600 Web pages with their associated queries (data set QUERY). del.icio.us 4 is a popular online service, which allows users to bookmark Web pages and tag them. We use the tagging histories of about 65,000 Web pages in del.icio.us (data set TAG). For these pages, we also downloaded their titles and HTML contents from the Web, where this was possible (data sets TITLE and CONTENT).

DMOZ 5 is a Web page directory that is edited by volun-teers. It classifies Web pages into a tree of categories (with category names like  X  X rts X ,  X  X ports X  or  X  X omputers X ). We http://del.icio.us http://dmoz.org collected for each page the category names on its path to the tree root. Furthermore, DMOZ gives a description for each page. We downloaded the whole directory, yielding cat-egories and descriptions for 3,900,000 Web pages (data sets DMOZCAT and DMOZDES).

We preprocessed all data sets by joining compound words (such as  X  X ar race X ) by help of the dictionary. After that, we eliminated stop-words. Note that our data sets do not necessarily cover the same Web pages.
To study the tagging behavior in a controlled environ-ment, we conducted a Web-based user study. We asked em-ployees of Microsoft Research to participate and we put ad-vertisements for the study on the Microsoft Research Web page. Overall, 4,000 Internet users participated, roughly half of them Microsoft employees. For the study, we pre-pared a pool of 20 predefined Web pages, chosen from lists of popular Web pages. Furthermore, we designed different settings under which a page can be tagged. For example, in one setting the system provides tag suggestions while in another one it does not. The settings differ in the layout of the tagging interface, in the number of suggested tags and in the methods used to suggest tags. When a new participant registers for the study, we generate a sequence of random pages from the pool. We also generate a sequence of ran-dom settings. Then, the participant is asked to tag each page in the sequence under its corresponding setting. We also collected general information on tagging habits. We are well aware that tagging in the context of a user study may differ from tagging in a social system. However, we believe that the insights that we gained from our user study give a valuable hint on the situation in real systems.
Our work analyzes Web documents and associated meta data. These meta data may be tags, but we can also see the categories of DMOZ, the descriptions of DMOZ and the title of the document as meta data. Following [3], even the content of the document can be considered meta data. We also see search keywords that are used in a search engine to find a particular page as meta data attached to the docu-ment. The constituents of these meta data (e.g. the tags) will be called terms . A single occurrence of a term in the meta-data (e.g. one tag applied to one Web document) will be referred to as an application . A set of terms that are applied together (e.g. the tags applied by one user to one document) will be called an event . We represent the meta data of a document as a set of terms, where we associate with each term the number of times it has been applied to the document (its frequency ).
For a given document, the meta data acts as a function from terms to frequencies, a frequency vector . If f is a fre-quency vector, f ( t )  X  R denotes the frequency for term t and f ( t )  X  0 for all t . For a set T of terms, we define the shorthand notation f ( T ) = P t  X  T f ( t ). For a non-trivial frequency vector f for which f ( t ) &gt; 0 for some t , we use terms that are mapped to non-zero frequencies the support all t , we call f a set . We call the sum of all frequencies the mass of f , mass ( f ) = f ( supp ( f )). Let n f denote the number of terms in the support of f . A frequency vector implicitly defines a ranking on its support, i.e. a sequence of terms &lt; t 1 , ...t n f &gt; such that f ( t i )  X  f ( t (where ties are broken arbitrarily). We denote with t f,i i th term in the ranking of f . Often, a frequency vector f ( t ) has to be compared to a  X  X eference frequency vector X  g ( t ). We use the following metrics: To compare the supports of the vectors irrespective of their frequencies, we use the standard measures recall and precision, These measures are useful for comparing two sets. To com-pare the ranking of f to the support of g (e.g.when g is a set), we use the precision at k : The precision at k measures what proportion of the top k terms in the ranking of f is in the support of g . To compare the ranking of f (irrespective of the frequencies) to the frequencies of g , we use the normalized discounted cumu-lative gain (NDCG)[6]: The NDCG assumes that g gives a  X  X ain X  to each term and then measures how much  X  X ain X  the support of f delivers, giving higher weight to higher ranked terms. Furthermore, we define the weighted recall at k : Here, [ ] denotes the Iverson bracket, which evaluates to 1 if the enclosed condition is true and to 0 else. The weighted recall at k measures what proportion of the mass of g is covered by the top k terms in the ranking of f .
 To compare the frequencies of the two vectors, we use the well-known cosine similarity: cos ( f, g ) = X Furthermore, we introduce the fuzzy recall : The fuzzy recall punishes terms to which f assigns less fre-quency than g . If f assigns its maximum value to all terms, the fuzzy recall is 1. If the frequency vectors are sets, the fuzzy recall is identical to the standard recall 6 . All of the above metrics deliver values between 0 and 1. We decided not to use the KL-divergence and the Jenson-Shannon-divergence because they deliver unbounded values that are difficult to interpret. Other metrics that are often
Precision measures can be defined analogously to all recall measures defined here, but are not necessary for our analy-ses. used to compare two rankings are Kendall X  X  Tau, the Kendall tau rank correlation coefficient, the footrule distance and the Spearman coefficient. However, these metrics pay equal at-tention to high-ranked discordant pairs and to low-ranked discordant pairs, which makes them less useful for our anal-yses.
Table 1 gives an overview of our data sets. For TAG, we observe that each page has been tagged on average by 217 users and that each user applied on average 1.95 tags per event (implying that many users tagged the page with just one term). The CONTENT contains on average 366 words per document (of which 223 are distinct). The TITLE has on average 4 words per document. QUERY shows us that each page was searched on average 306 times and that queries were on average 1.47 words long. There are different numbers of pages in DMOZCAT and DMOZDES and 10% of the DMOZ pages have been categorized multiple times. It has been hypothesized [5, 16, 7] that tags are often non-words (like  X  X oread X ) and that polysemy is highly prevalent in tags. To assess these hypotheses, we looked up each tag application in our dictionary and counted how many mean-ings the tag had. As a point of reference, we conducted a similar analysis for CONTENT and QUERY and restricted the analyses to those pages that appear in all three data sets (1521).

As Table 2 shows, more than half of the tag applications use words that are not known to the dictionary. This con-firms that the proportion of personal tag creations (such as  X  X oread X ), misspellings, proper names and possibly also for-eign words is indeed high in tags and may even be larger than 50%. Of course, our dictionary can only be a proxy for proper words. However, the comparison with the other data sets shows that the proportion of non-words is significantly higher in tags than in queries or the Web pages.

As a side-result, our analysis shows that among the words known to the dictionary, most have more than one meaning. Family names, e.g, can refer to dozens of well-known people in our dictionary. Given that a word may have even more meanings than our dictionary knows, this constitutes the proof that polysemy is indeed highly prevalent in del.icio.us . The high proportion of non-meaningful terms and the high polysemy may seem discouraging results for semantic appli-cations. In the following, we show how the meaningful tags can be filtered out.
We considered the top k tags for each page and computed the precision with respect to our dictionary. The average of these values across pages (weighted by their popularity) is shown in Figure 1: The highest precision is found for small k . This shows that the top popular tags for a page are indeed the  X  X eaningful X  ones. Note that it is not a priori clear that this would necessarily hold, as some non-meaningful words may be in rather common use (such as personal organiza-tion tags like  X  X odo X  and  X  X oread X ). A possible reason might be that different taggers use different non-words for their personal organization, but use the same meaningful tags for a general description of the page. This observation is sup-ported by the proportion of DMOZ category terms, which are also proportionally more prevalent in the popular tags. Since our dictionary gives a lower bound on the proportion of English words, we obtain as a side-result that, on aver-age, at least 80% of the top 7 tags are proper English words. This implies that, on average, already the top 7 tags can be of use for semantic applications.

Figure 1: Precision@k for Tags (page average)
The above results suggest that a simple aggregation of the top tags of a document may be effective for extracting the meaningful tags. While this may be effective on average across all the pages, it may fail for pages that were tagged by only few users. We computed for each page the proportion of its top 7 tags that were meaningful in the sense of our dic-tionary. Figure 2 shows the result for the pages, grouped by the total number of tag applications that a page received. Our results suggests that the more popular a page is, the higher its proportion of known words is in the top 7 tags. (We obtain similar results when considering more top tags). Again, the observation is confirmed by the proportion of DMOZ category terms, which are also proportionally more prevalent in popular pages. This means that the top tags of popular pages are even more likely to be meaningful than the top tags of less popular pages. Interestingly, the precision increases only slowly for the first 100 tag applications. This entails that for less popular content or small scale systems such as in enterprise scenarios, simple aggregation of the top tags may not be sufficient to filter out the meaningful tags.
Figure 2: Precision@7 for tags, by page popularity We were interested in the word classes that tags belong to. We distinguish the lexical classes nouns, verbs, adjectives and adverbs. We consider a term a plural noun if it can be stemmed by the PlingStemmer [12] so that the result is a known noun. Furthermore, we consider the class of URLs (as determined by an appropriate regular expression match). A term belongs to the class of categories, if it is used in the DMOZ category system. We also consider the class of proper names. Table 3 gives the percentage of tag applications that fall into these classes (one term can belong to multiple classes).
 Out of the applications known to our dictionary, the applica-tions roughly follow the same distribution as HTML content and queries: Common nouns are most prevalent, followed by proper names. Interestingly, a quarter of tag applications are terms that are also used as categories in DMOZ. This seems to suggest that users often think in terms of categories when they apply their tags.
We wanted to know whether the distribution of tags re-sembles more the distribution of terms in the content or in the queries for a given page. To this end, we computed for CONTENT and QUERY the pages it had in common with the TAG data set (shown in the first line in Table 4). For each of the common pages, we computed the similarity of the frequency vector in the TAG data set to the frequency vector in the other data set, where the latter serves as the reference vector in the sense of Section 2.2.2. If we may see the frequencies as an indicator of how important the term is 7 to summarize the document (CONTENT) or to query for that document (QUERY), then the cosine similarity and the NDCG tell us that tags rather resemble the queries for that document than a summarization. The fuzzy recall shows that the terms that are frequent in the content are rather infrequent as tags, whereas the terms that are frequent in the queries are better covered by the frequent tags.
We were interested in the number of tags that are needed for a document in order to cover a substantial portion of the other meta data. For this purpose, we analyzed for each page the weighted recall of its top k with respect to the other data sets. The weighted recall at k is high if the top k tags cover frequent and thus  X  X mportant X  terms in the reference set. It is low if the tags cover only terms that are rare in the reference set. This measure is meaningful for both sets and distributions. The average of the weighted recall over pages (weighted by their popularity) is shown in Figure 3.
First, the plot confirms that tags cover better the queries for the document than its content. The recall for titles, descriptions and categories cannot be compared easily in
Remember that stop-words have been removed. this way, because these meta data are sets of different sizes. However, the analysis shows that in all of these cases the proportion of covered terms does not increase much beyond k = 20. The top 20 tags cover one third of the terms in the title and the categories and one fifth of the terms used by DMOZ to describe the document. Again, this confirms that most of the  X  X seful X  terms (as identified by the other data sets) appear in the top popular tags of a page.

In summary, our results suggest that tags provide useful summary information for documents that intersect well with the search keywords and titles and thus could be leveraged, e.g., to enhance search applications.
Many existing systems provide tag suggestions. del.icio.us , e.g., suggests the tags that have been most popular so far. On the one hand, tag suggestions may reduce tagging effort (cognitive or typing), thus serving as a participation incen-tive. They may also elicit conformance in vocabulary usage. On the other hand, suggestions may sway a user X  X  tagging behavior and obscure his true preferences. Suggesting tags may also make users more passive and as a result make it harder for him to recall which tags he applied to a previously tagged item.
We consider methods that suggest a set of tags of size k to each user, where k is a system configuration parameter. k is typically much smaller than the total number of distinct tags applicable to an object. We focus on methods that suggest tags for an object based solely on the tags applied by previous users to this document.

In our study, we consider four different suggestion meth-ods, designed to span a broad range in their biasing to suggesting popular tags. First, we consider a method that makes no suggestions at all (NONE), which we use as a ref-erence. Under this method, the tags applied by the users are not biased at all by tag suggestions. Second, we con-sider the standard Top Popular method (TOP), which to any user suggests the set of the k most popular tags. This method aims at best  X  X uessing X  what tags would be applied by the next user. The issue with this scheme, however, is that it may bias the tag popularity ranking if users tend to apply tags from the suggestion set. This implies that the resulting tag frequencies may become distorted relative to the frequencies that would hold if no suggestions were made (a phenomenon called the popularity bias [14]). This com-plicates the learning of a user X  X  true preference of tags for an object.

Following [14], we consider two additional methods, which are designed to mitigate the tag popularity bias by suggest-ing some less popular tags than the top k tags. Frequency Move-to-Set (FMTS) is a method that behaves similarly to TOP, but tends to draw its suggestions from a larger set of popular tags than just the top k . It works as follows: For each document, it maintains a frequency vector of tags. The method will always suggest the top k tags in that vector for the document. After a tagging event, the method incre-ments the frequencies of those tags in the vector that were applied by the user, but did not appear in the suggestion set. This way, the method facilitates the rise of previously unpopular tags while still biasing towards the popular tags in general. The second method, Move-to-Set (MTS), allows even completely unpopular tags to appear in the suggestion set. Like FMTS, it maintains a frequency vector of tags per document. However, it maintains a threshold  X   X  N and never increases the frequency beyond  X . The suggestion set is exactly the set of tags with frequency  X . If a tag is ap-plied that has a frequency smaller than  X , its frequency is incremented. If, this way, its frequency becomes  X , the fre-quency of a random tag in the suggestion set is decremented, so that the overall number of tags in the suggestion set is always k . This way, any tag is catapulted to the suggestion set once it has been applied  X  times. This induces a high rate of perturbance in the suggestion set, while popular tags will still be a little bit more prevalent than less popular ones .
The theoretical properties of these methods are not in the focus of this paper. We refer the reader to [14]. Here, we only constate that TOP, FMTS and MTS cover a broad range in their biasing toward popular tags. Suggested tags may influence the user to a certain degree. This section establishes two measurements for this influence. The first one, the matching rate , serves to prove that users tag differently when suggestions are present. The second measurement, the imitation rate , aims to quantify the ex-tend to which users are influenced by the suggestions.
For the following definitions, we consider a single fixed document D . We will denote a suggestion method by X (with, e.g., X = FMTS). We consider multiple tagging events i = 1 , ..., n by different users on D under the suggestion method X . Let T ( X, i ) and S ( X, i ) be the set of applied tags and the set of suggested tags at the i th event, respec-tively.
The matching rate under the method X is the proportion of the applied tags that appear in the suggested tags, i.e.
If all applied tags appear in the suggestion set, the match-ing rate is 1. The matching rate alone does not tell whether the tag generation was influenced by suggestions; in any case, some portion of the applied tags would appear in the suggestions and we expect schemes such as TOP to have a higher matching rate than schemes such as MTS that sug-gest also some less popular tags. In order to show that the tag generation was influenced by the suggestions of X , we consider a paired setting, in which users tag the document D in n tagging events under the method NONE. The control matching rate is the proportion of the applied tags under the method NONE that appear in the suggested tags of X 8 :
The matching rates can be averaged over tagging events and over documents. If the averaged matching rate and the
Since these definitions are intended for large n and since the tagging events under NONE are independent, the order of tagging events for NONE does not matter. averaged control matching rate differ in a statistically sig-nificant way, this means that users are more likely to choose tags from the suggestion set if the suggestion set is actually displayed. This, in turn, proves that users are influenced by the suggestions.
Note that the matching rate does not tell us what portion of applied tags is a result of the presence of the suggestions  X  even an uninfluenced user may happen to select a tag that was suggested. In order to quantify the extent of this influence, we introduce the the imitation rate for a single set of suggestions, S . Given a method X and a sequence of tagging events i = 1 , ..., n , we consider only the tagging events in which S was suggested. We let prec n ( X, S ) be the portion of applied tags in these events that are in S : We further consider the portion of applied tags that are in S under the suggestion method NONE: Both prec n ( X, S ) and prec n ( N ON E, S ) are standard preci-sion metrics where the suggested tags are considered the X  X el-evant items X  and the applied tags are the  X  X etrieved items X . Equivalently, prec n ( X, S ) and prec n ( N ON E, S ) can be seen as recall metrics. In this case, the applied tags are the rele-vant items (selected by the user) and the suggested tags are the retrieved items (suggested by the system).

The imitation rate is defined by: We first briefly discuss the imitation rate, and then provide its justification. If the tags that the user applied and the suggested tags in the system X are statistically independent, then prec n ( X, S ) and prec n ( N ON E, S ) will converge to the same value as n grows large, and we then have that  X  n ( S ) goes to 0, indicating no imitation. If, on the contrary, tag ap-plications in the system X are biased towards the suggested tags, then prec n ( X, S ) will be larger than prec n ( N ON E, S ), for sufficiently large n , and then a positive  X  n ( S ) will indi-cate imitation. In the extreme case, prec n ( X, S ) will tend to 1 and this will result in  X  n ( S ) tending to 1, indicating full imitation.

We now justify the definition of the imitation rate for-mally. Assume that tags are applied according to the follow-ing probabilistic model: The probability that a user applies a tag t , if S is displayed, is a mixture of two distributions: where with probability  X  S , the user decides to take a tag from the suggestion set. f S ( t ) is the probability that the user chooses the tag t from the suggestion set. Consequently, f ( t ) = 0 for all tags t 6 X  S . With probability 1  X   X  S , the user chooses an arbitrary tag, which may or may not be in S . g ( t ) is the probability distribution over tags for this choice. Following [14], we assume f S ( t ) = g ( t ) /g ( S ), for t  X  S , i.e. in the cases when the sampling is from the distribution f , the user preference over tags is proportional to g but confined to the set S .

We want to estimate the parameter  X  S . It indicates the  X  X ersuasive power X  X f the suggestion set S , i.e. the likelihood that the user decides to make use of the suggestions. We call it the imitation rate for the suggestion set S . We next identify an unbiased estimator of  X  S , which follows from the results of Boes [1]. Let T be the set of possible tags. Let h ( X ) be the portion out of n tag applications that fall in the set of tags  X  by sampling from the mixture distribution Eq. (2). From a corollary of Theorem 1 in [1], we have that the following is an unbiased, minimum-variance estimator of  X  : under g ( T \  X  1 ) &gt; f S ( T \  X  1 ), where  X  1 is a subset of T that has to satisfy some factorization criteria given in Theorem 1 [1]. These factorization conditions hold if we can find two sets  X  i , i = 1 , 2 such that Now, note that this holds for the sets  X  1 := S , and  X  2 S . Indeed, f S ( t ) /g ( t ) = 1 /g ( S ), for t  X   X  1 and f 0, for t  X   X  2 . In this case, we have This can be further simplified by noting that p ( S ) = 1  X  p ( T \ S ) for any probability distribution p on T and that f ( S ) = 1: It remains only to note that prec n ( X, S ) is an estimator of h ( S ) and prec n ( N ON E, S ) is an estimator of g ( t ). We can thus estimate the imitation rate for S as given by Eq. (1).
The previous section established the imitation rate for a single suggestion set. Now, we want to capture the average imitation rate under a suggestion method X . To this end, it is natural to consider the imitation rate for all sets S that X suggests and to compute their weighted average: Here,  X  n ( ) is given by Eq. (3) and  X  n is a probability dis-tribution over suggestion sets. For example,  X  n ( S ) can be the proportion of tagging events that happened when S was displayed. Alternatively,  X  n ( S ) can be the proportion of tag applications that happened when S was displayed. This will give us a  X  X er application X  viewpoint: Using Eq. (5) in Eq. (4), we can interpret  X  n in Eq. (4) as the portion of tag applications that were a result of imitation under the method X .
We now present the results of our user study. Unless oth-erwise indicated, our results for a given tagged Web page under a given setting are based on 100 tagging events by different users for that page under that setting.
In this section, we will determine the influence of the sug-gestions. We will first use the matching rate to validate that, indeed, a user X  X  tag applications are influenced by the suggestions. Then, we turn to estimating the proportion of applied tags that were induced by the suggestions by help of the imitation rate.
We computed the matching rate on 4 different Web pages for the methods TOP, FMTS and MTS. Fig. 4 shows the imitation rates averaged over the pages. As expected, TOP shows a slightly higher matching rate, indicating that users were more likely to chose tags from the suggestion set. To prove that users tag differently when the system provides suggestions, we computed the control matching rate. We observe that the matching rates are roughly twice as large if suggestions are shown. This shows that the tag genera-tion in our user study was indeed influenced strongly by the suggestions.
 Figure 4: Matching Rates and Control Matching Rates for different methods, averaged over 4 pages
Table 5 shows the imitation rates under different sugges-tion methods for 4 pages. The imitation rates are consis-tently between 30% and 40%. This tells us that roughly 1 out of 3 applied tags does not mirror the original user preferences, but was induced purely by the presence of the suggestions.

We further wanted to test whether suggestions that are more popular cause more users to imitate. In other words, the claim is that more plausible suggestions have a higher persuasive effect. Of course, more plausible tags are more likely to be chosen anyway, independent of the suggestions. But the imitation rate allows us to assess whether the very presence of the suggestions further boosts the plausible tags, beyond their actual true popularity. To assess this claim, we conducted a set of experiments for 10 pages with the suggestion sets fixed to either the top popular tags from del.icio.us (High), somewhat less popular tags (Middle) an d even less popular tags (Low).

Fig. 5 shows the imitation rates for the three suggestion types. The plot shows that popular tag suggestions influence the user more than unpopular tag suggestions.

In summary, the results suggest that tag generation can be significantly influenced by suggestions and that this can be even further exacerbated by suggesting popular tags. This raises the issue of whether the tag applications observed in real systems that use TOP-like suggestion methods reflect the users X  true preference or are an artifact of the suggestion mechanism.

To further investigate how distorting the effect of a TOP-like suggestion method can be, we compared the tag fre-quencies under TOP with the final tag frequencies under the method NONE. We understand the final tag frequen-cies of NONE as the users X  true preference distribution over tags. Fig. 6 shows how the aggregated tag frequencies af-ter each event compare to the  X  X rue X  tag frequencies. We show the results of the NDCG metric for two exemplary pages (the cosine similarity yields qualitatively equivale nt results). For the right page, TOP deviates drastically from the true frequencies. This proves that there exist cases in which, after some number of events, the method TOP dis-torts the tag frequencies significantly. This is worrying, as TOP-like methods seem to be the most common suggestion schemes.
 Figure 6: Comparison of the tag frequency vectors
After each tagging event, participants were asked to pro-vide a feedback about the tag suggestions. We presented different predefined options (listed in Table 6) and also a freeform field  X  X ther X , which was rarely used. Recall that the suggestion methods in our study tend to suggest quite different types of tags (TOP suggests the top k popular tags, FMTS tends to suggest a larger set of popular tags and MTS an even larger set; in fact, any tag would have a chance to appear in the suggestion set). Hence, one might expect that users would prefer TOP-like suggestion methods over those that suggest less popular tags. Our results do not support this hypothesis. About 65% of users claimed to pay no at-tention to the suggested tags at all. The remaining feedback data suggests that the users had no particular preference for one suggestion method over another. In particular, users did not favor the method TOP over methods that suggest less popular tags. In fact, the highlighted results in Table 6 in-dicate that with statistical significance (at  X  = 5% ) the portion of users who opted for  X  X hey were generally helpful X  was larger for MTS than for TOP.

These are interesting results as they suggest that the de-sign objective to suggest a few most popular tags (which seems to be standard practice) may not be necessarily the best design choice from the perspective of the users X  judge-ment.
We conducted a number of experiments to evaluate a range of other factors that can influence the tag generation.
We wanted to understand whether users bias to applying tags from particular positions in the list of suggested tags. In our experiments, the tags in the suggestion list were pre-sented in random order. In absence of a position bias, we should thus observe that the frequency of any tag applica-tion does not depend on the position of this tag in the list. It is of interest to understand whether the position bias exists as this can affect designs that do not randomize the order of tags (but, e.g., apply an alphabetical or popularity order instead).

The position bias is well known to feature user interaction with search engine results sets where the few top items in the list get most of the clicks. Our setting differs in that the order of tags in the list is random, the length of the list is typically small, and tags in the list are arranged in a row, not in a column.

Fig. 7 shows that users bias to selecting the leftmost tag in the list. However, this is not very pronounced ( &lt; 15% relative to the second tag from the left in the list). The bias over tags at other positions is statistically insignificant .
In summary, we find that there is a weak position bias, which might make it reasonable to randomize the order of the suggested tags to avoid undesired tag popularity skews.
In our base experiments we fixed the suggestion set size to seven, motivated by existing systems (e.g. del.icio.us) . In practice, this choice may have been motivated by earlier experiments on the human capabilities of information pro-cessing [8]. We wanted to evaluate how the tag generation process is influenced by the number of suggested tags. To this end, we considered 10 Web pages. For each of them, we created 5 experimental settings in which the suggested tags were fixed to the 0, 3, 5, 7, and 20 top popular tags (according to del.icio.us).

We measured the imitation rate for each setting. The results, averaged over the 10 pages, are shown in Fig. 8 (top). One would expect more imitation for larger suggestion sets. However, the estimated imitation rates are non monotonic with respect to the suggestion set size. This indicates a non-trivial relationship between the persuasive power of a suggestion set and its size.
 We also analyzed the number of applied tags per event. We found that the average number of applications per event does not appear to be monotonic with respect to the sugges-tion set size (Fig. 8 (bottom)). In particular, we find that with statistical significance (at  X  = 5%) there exists a sug-gestion set size for which the average number of applications per event is larger than for any other smaller suggestion set size.
These are interesting findings as they suggest that the size of the suggestion set may influence the users X  imitation rate significantly in non-trivial, non-monotonic ways.
We wanted to evaluate whether users would bias to apply-ing tags from the suggestion set if they can be selected by a click . To this end, we considered 10 Web pages and fixed the suggestion sets to the 7 most popular tags (according to del.icio.us). For each such setting, we ran two experiments: (A) with clickable suggestions and (B) with non-clickable suggestions. We found that in case (A) more than 45% of applied suggested tags were selected by clicking on a sug-gested tag, confirming that users were making good use of this feature. At the same time, we found that in both cases the imitation rate was the same.

This suggest that making tags clickable will not bias the tag applications, but benefits those users who prefer clicking over typing. We found that user-generated tags feature substantial se-mantic noise, more than the terms from either page content or search queries. Yet, our analysis reveals that meaningful tags emerge in the more popular tags of a document and that this meaningfulness improves with the number of users that tag the document. We also found that popular tags for a document cover better the terms of the queries than the frequent content terms. The proportion of  X  X seful X  terms (titles, categories, search keywords and descriptions) in the tags increases rapidly among the most popular tags, grows very slowly beyond the top twenty tags and overall attains a moderate recall. Overall, the results are encouraging news, suggesting that popular tags contain useful terms, which could be leveraged for advanced applications.

Our study on the influence of tag suggestions yields that the users X  tendency to bias applied tags towards the sug-gestions could be substantial. We also found evidence that users may tend to bias even more towards the suggestions, if the suggested tags are popular. Interestingly, we found that users did not prefer a suggestion method that suggests a few popular tags over those that tend to cover a larger set, including less popular tags. We also identified and analyzed several other factors that influence the tag generation and derived consequences for the user interface design. In sum-mary, the results raise the question whether the tag applica-tions observed in real systems reflect users X  true preference over tags or are an artifact of suggestions. The observation that popularity of the suggested tags may even further en-courage users to follow suggestions (and thus provide less in-formation about their unbiased inner preference over tags) raises the question whether the standard design choice to suggest a few top popular tags is indeed the best practice. The user feedback indicating indifference over the sugges-tions of varying popularity could be leveraged in the design of advanced suggestion methods.

Future work may investigate the design of suggestion meth-ods that would best support specific users tasks such as search and navigation.
 We would like to thank Misha Bilenko, Silviu-Petru Cucerzan, and Ryen White from Microsoft Research Redmond for pro-viding us with QUERY data set used in this paper. We thank Nick Duffield, John Mulgrew, and Andy Slowey from Microsoft Research Cambridge for providing us with tech-nical assistance in conducting our user study. Last but not least, we thank the participants of our user study. [1] D. C. Boes. On the estimation of mixing distributions. [2] E. H. Chi, A. Kittur, and T. Mytkowicz. Augmented [3] N. Eiron and K. S. McCurley. Analysis of anchor text [4] C. Fellbaum, editor. WordNet: An Electronic Lexical [5] S. Golder and B. A. Huberman. The Structure of [6] K. Jarvelin and J. Kekalainen. Ir evaluation methods [7] P. Mika. Ontologies are us: A unified model of social [8] G. A. Miller. The magical number seven, plus or [9] E. Santos-Neto, M. Ripeanu, and A. Iamnitchi. [10] S. Sen, F. M. Harper, A. LaPitz, and J. Riedl. The [11] S. Sen, S. K. Lam, A. M. Rashid, D. Cosley, and [12] F. M. Suchanek, G. Ifrim, and G. Weikum. Combining [13] F. M. Suchanek, G. Kasneci, and G. Weikum. YAGO: [14] M. Vojnovi  X c, J. Cruise, D. Gunawardena, and [15] R. W. White, M. Bilenko, and S. Cucerzan. Studying [16] Z. Xu, Y. Fu, J. Mao, and D. Su. Towards the
