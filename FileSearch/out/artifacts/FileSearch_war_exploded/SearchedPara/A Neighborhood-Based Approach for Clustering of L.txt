 This paper addresses the problem of automatically structur-ing linked document collections by using clustering. In con-trast to traditional clustering, we study the clustering prob-lem in the light of available link structure information for the data set (e.g., hyperlinks among web documents or co-authorship among bibliographic data entries). Our approach is based on iterative relaxation of cluster assignments, and can be built on top of any clustering algorithm. This tech-nique results in higher cluster purity, better overall accuracy, and make self-organization more robust.
 Categories and Subject Descriptors: H.3.3 Informa-tion Systems: Information Search and Retrieval I.5.3 Pat-tern Recognition: Clustering General Terms: Theory, Algorithms, Reliability Keywords: Clustering, Exploiting Link Structure
The issue of automatically structuring heterogeneous doc-ument collections into thematically coherent subsets is rel-evant for a variety of applications, such as organizing large personal email folders, dividing topics in large web directo-ries into subtopics, structuring large amounts of company and intranet data, etc.

Graph-based clustering is a well established problem in the literature. A detailed overview of existing methods is presented in [6]. Typically, the underlying graph G is con-structed by representing each data point as a node in G and each edge, connecting any two data points, by a weight, in-dicating the distance (dissimilarity) between its end points.
Our approach is orthogonal to the approaches discussed in [6] as we use statistical knowledge about the cluster as-signments of the nodes in the formed neighborhoods in G . Furthermore, the assignment of the edge weights, and thus the type of graphs used by the above approaches, are based on node-node similarity, and it is not clear how to carry this forward to a hyperlinked environment. Closest to the ap-proach presented in this paper is our own recent work on neighborhood-based classification [2].

We propose two approaches that consider the link struc-ture in the test graph.
An intuitive way of combining the content of a document d with the content of its neighbors d  X  N ( d ) is to assign term weights w ( t i ,d ) to all terms t i  X  d while considering in a linear way the term weights t i in d  X  X  neighbors d  X  N The impact of the neighborhood content on the final term weights in document d is controlled by a parameter  X  . The correspondingly adjusted feature vectors can be used as an input to all vector-based clustering algorithms, e.g., the k-means algorithm.
The approach, we propose here, adopts a probabilistic for-mulation of the clustering problem and is based on the so called relaxation labeling technique [2]. We aim to cluster a set of documents D , where each document d  X  X  corre-sponds to a vertex in the graph G and each link between two documents in D corresponds to an edge in G . The clustering algorithm requires as an input the the text of each document d and information about which documents of G constitute its neighborhood, N ( d ) . Let c ( d ) denote the cluster of node d whose validity can be associated with a probability. The content of document d is represented as a set of terms that occur in d and denoted by  X  ( d ) . The output of the algorithm should be an assignment of clusters to the graph nodes such that each document d  X  G belongs to its maximally likely cluster i , selected from a finite set of clusters [1 ..m The intuition behind our approach is sketched in Figure 1. Figure 1 (a) shows the clustering entirely based on the con-tent information about each document. Here document d is assigned to its nearest (most similar) cluster. The simi-larity of document d is measured with respect to the clus-ter centroids, (e.g. produced by a content-based clustering algorithm such as k-means). Figure 1 (b) shows the link structure among the documents. In the content-only world, such link information is completely ignored. Our attempt to make use of it starts with the observation that document d is linked to a significantly higher number of documents from cluster c 2 than from cluster c 1 . Furthermore, the graph structure suggests that documents from cluster c 2 typically link to documents that belong to this very same cluster -c Thus, with high probability a document can be clustered in c if it is linked to many documents that belong to cluster c . In our toy example, this leads to reassigning document d to cluster c 2 . The final clustering after taking both content and link information into account is shown in Figure 1 (c).
Formally, taking into account the underlying link struc-ture and document d  X  X  content-based feature vector, the probability of a document d to be assigned to cluster i is:
Pr [ c ( d )= i |  X , G ]=Pr[ c ( d )= i |  X  ( d ) ,c ( d 1 where d 1 through d l are the documents in D .

For tractability, it makes sense to focus on the strongest dependencies among immediate neighbors. Such a model is called a first-order Markov Random Field or MRF [4, 5]. Computing the parameters of an MRF such that the likelihood of the observed training labels is maximized is a difficult problem that cannot be solved in closed analytic form and is typically addressed by an iteration technique known as relaxation labeling (RL). Our approach builds on this mathematical technique.

In the spirit of emphasizing the influence of the immediate neighbors for each document, N ( d ) , we obtain by  X  i,d . This reflects the MRF assumption that the label of a node is conditionally independent of the labels of other nodes in the graph given the labels of its immediate neigh-bors. We abbreviate Pr [ c ( d )= i |  X  ( d )] , the graph-unaware probability based only on d  X  X  local content, by  X  i,d . Let
N ( d )) denotes the assignment of clusters to the group of neighbors of d , N ( d ) .

Then,  X  i,d can be computed in an iterative RL manner as follows:  X  where r&gt; 1 and i, j  X  [1 ..m ] are cluster assignments.
To avoid the potential increase in the level of noise, we can consider only a subset of  X good X  neighbors. These neigh-bors should be similar enough to the document in question, d . For this purpose, we introduce a similarity threshold, which can be computed as the cosine-similarity between the pair of neighboring documents and which selectively deter-mines the neighborhood of each document d .

However, calculating the sum over all possible cluster as-signments in Equation 1 is hard as we have m | N ( d ) | sum-mands, where m is the number of distinct clusters. To solve this problem we employ two major methods. We approx-imate the sum over all possible cluster assignments of the neighborhood to either its most significant summand, treat-ing it as if it were the true set of clusters, or the most sig-nificant summands and their associated probabilities. The algorithm efficiently re-computes and updates the probabil-ities of particular cluster assignments to the neighborhood N ( d ) after each RL iteration.

To improve the robustness of the algorithm we propose two beneficial extensions. We aim to ignore the unnecessary and most probably noisy information behind all irrelevant links in a neighborhood by assigning to each edge e a weight w e equal to the cosine similarity between the feature vectors of the documents connected by the edge. We also explore further the hypothesis that neighboring documents should receive similar cluster assignments. We introduce a metric over the set of clusters where thematically close clusters are separated by a shorter distance and therefore impose smaller cost for assigning neighbors to similar clusters.
We conducted experiments on three sets of data obtained from the database of scientific papers DBLP, the movie database IMDB, and the online encyclopedia Wikipedia. Full detail of the experiments can be found in [1]. All datasets are available at www.mpi-inf.mpg.de/  X  angelova. We compared the content-based method k -Means [3] -k -Means , and the Content Combination -CComb[  X  ] , with the graph-based clustering method -GC , and its enhanced variant using the edge weighting scheme and the cluster sim-ilarity metric -wmGC . All graph-based methods use the result of the simple content based k -Means in the initializa-tion step. We tested all graph-based methods with different influence of the neighborhood described by  X  which are cor-respondingly shown in squared brackets after the method abbreviation.

We also tested an MST-based graph-cut clustering algo-rithm [7], computing the edge weights as weighted sum of hyperlink based neighborhood and content similarity of the documents, and pruning edges in the corresponding span-ning tree. However in our preliminary experiments, the k-means algorithm (despite of its simplicity) showed superior performance on our data sets. Hence, we did not consider building our algorithm on top of a graph-cut approach.
The outcome of the comparison among the above methods along with the 95% confidence intervals is shown in Table 1.
The graph-based approach significantly outperforms all pure content-based methods. Our experiments show im-provements of up to 9% over the k -Means algorithm as well as significant gains close to 10% over the content combina-tion approach. The performance of the graph-based clus-tering is even better if the content combination technique is used as initialization step for the graph-based methods. Including the cluster distance metric in the computations improves the graph-based clustering by gently imposing con-straints on the pair of cluster assignments for each two neigh-boring documents resulting in up to 6% gain in accuracy in some cases.

The newly proposed graph-based clustering method, espe-cially applied on top of the content combination technique, is very robust and outperforms the previously known state-of-the-art algorithms by a significant margin.
