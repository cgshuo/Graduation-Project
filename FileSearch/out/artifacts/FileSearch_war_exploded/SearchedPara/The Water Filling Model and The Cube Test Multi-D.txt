 Professional search activities such as patent and legal search are often time sensitive and consist of rich information needs with multiple aspects or subtopics. This paper proposes a 3D water filling model to describe this search process, and derives a new evaluation metric, the Cube Test , to encompass the complex nature of professional search. The new metric is compared against state-of-the-art patent search evaluation metrics as well as Web search evaluation metrics over two distinct patent datasets. The experimental results show that the Cube Test metric effectively captures the characteristics and requirements of professional search.
 H.3.4 [ Information Systems ]: Information Storage and Retrieval X  Systems and Software [Performance evaluation] Evaluation, Patent Prior Art Retrieval, Professional Search
Information Retrieval (IR) research has advanced to a stage where searching for complicated information needs is a common practice. Evaluation metrics need to reflect this complexity. Professional search activities such as medical record search [14], patent prior art search [19], and legal dis-covery [3] are examples of complicated information seeking tasks. Most IR research about professional search focuses on query formulation [19] and knowledge discovery [3]. In this paper, we focus on describing the information seeking process and evaluating professional search.

Professional search often involves information needs that consist of multiple aspects or subtopics . For example, a lawyer needs to discover various aspects of a lawsuit to find defensive materials. An ideal professional search evaluation metric should measure how well a search system allows the user to handle the tradeoffs between time taken to search and how well the returned documents cover the different aspects of the information need.

This paper uses patent prior art search as a motivating ex-ample of a complex information seeking task constrained by time. A patent application includes a series of independent and dependent claims , each of which defines some of the scope of the claimed invention. The dependent claims refer to their parent claims, which could be either independent or dependent claims. A patent examiner searches published lit-erature, termed prior art , to determine whether the claimed invention is novel. In some cases, portions of the patent ap-plication document can be used as the input for prior art search rather than a user-issued query. In other cases, prior art search is done using complex Boolean queries whose re-sults are ordered chronologically rather than by relevance.
In the terminology of this paper, each claim corresponds to a different aspect of the full information need. From a patent examiner X  X  perspective, a prior art document is con-sidered a higher quality search result if it can be used to reject a greater number of claims. An evaluation metric to compare ranking algorithms that automatically retrieve relevant prior art needs to consider: (a) the time spent to review the returned results, (b) the number of claims each returned document covers, and (c) how closely the text of the document matches the meaning of the claim in question.
The main metric used in the CLEF-IP 2009-2013 evalu-ations [9] is PRES [7]. PRES is a function of recall, the ranking of retrieved documents, and the maximum number of results to be checked by the user. However, PRES does not capture the embedded subtopics in patent documents and examiners X  desire for wide subtopic/claim coverage in patent prior art search. It also fails to include time to eval-uate the literature.

We propose that professional search can be understood by analogy to water filling a compartmentalized cube (see Fig-ure 1). We also assume that searchers would like the multi-faceted components of the cube filled as quickly as possible. Based on this model we propose a novel IR evaluation metric which we call the Cube Test (CT), which:  X  Covers different aspects or subtopics,  X  Allows for a single document to cover several subtopics,  X  Is time-sensitive, and  X  Expresses the tradeoff between time, quality of documents, and diverse coverage of subtopics.
 We test the new metric using CLEF-IP datasets and patent applications from the U.S. Patent and Trademark Office (USPTO). The results show that our metric effectively cap-tures the characteristics of professional search.
PRES [7] is the main metric for the CLEF-IP 2009-2013 evaluations [9]. PRES [7] was developed to bias towards recall and is defined as: where NRel is the number of relevant documents, p j is the rank position of the j th relevant document, and N max the maximum number of retrieved documents to be checked by the user. PRES is based on normalized recall ( R norm which measures effectiveness by ranking documents relative to the best and worst possible outcomes. The best outcome is defined by retrieval of all relevant documents at the top of the list, and the worst outcome is not retrieving them until the end of the list.

Since many patent searches consist of complex Boolean queries whose results are not relevance ranked, PRES defines a maximum number of documents to be checked by the user ( N max ). If a relevant document is listed after position N max it will have zero recall. Although defining N max is a rather ad-hoc procedure, PRES is among a minority of metrics that discuss stopping criteria in an evaluation metric for search tasks. Others include RBP [8] and expected global utility (EGU) [16].

The Cube Test presented in this paper defines a threshold value at which additional returns for the same concept pro-vide no additional benefit for the user. We consider fulfilling the user X  X  need, rather than stopping after an arbitrary num-ber of documents. Researchers have looked into evaluating subtopic relevance. Zhai et al. [17] introduce subtopic recall, subtopic precision, and weighted subtopic precision, which evaluate a search system X  X  ability to return documents that cover subtopics of a general topic. The TREC 6 interactive track [6] used aspectual recall and aspectual precision to measure how well a system allows a user to find documents which supply mul-tiple instances for an aspect.

The  X  -nDCG metric proposed by Clarke et al. [2] ex-pands upon nDCG by incorporating  X  , the probability that the assessor made an error when judging whether or not a document is relevant.  X  -nDCG is derived from a proba-bilistic model. It models the probability that a document d matches to any subtopic c i in the information need Q as By making a few assumptions, the model simplifies to the gain function for the k th document in a ranked list: where rel ( d k ,c i ) = 1 if document d k contains information the number of documents ranked up to position k  X  1 that contains information for subtopic c i . Notice that when  X  = 0,  X  -nDCG reduces to standard nDCG with the number of matching subtopics used as the graded relevance value.  X  -nDCG has several limitations. Clarke et al. [2] self-acknowledges its limited relevance definition. The model regards the document as irrelevant if it contains a previously reported subtopic. Another limitation is that it assumes that concepts are independent of one another and equally probable to be relevant.
 We argue that these simplifications as demonstrated in Eq. 3 are incorrect. The user may want to find several documents which all relate to the same subtopic, thereby providing a stronger degree of confidence. Our model takes this into account with a decreasing function. Second, af-ter sufficient information has been collected to fulfill the subtopic need, the user does not benefit if further results only provide evidence for the same subtopic. Our model takes this into account by implementing a threshold value, after which further documents provide no gain. Third, we do not assume all subtopics to be independently important and thereby introduce another parameter (  X  ) in our model which measures the relative importance of subtopics with respect to one another.

Other recent metrics that consider subtopic relevance in-clude I-rec [11], nERR-IA [12], and D-nDCG [12]. I -rec @ k is the percentage of matched subtopics at k and is defined set of subtopics for a query Q , d j denotes a document at rank j , and C ( d j ) denotes the set of subtopics to which d is relevant at the cut-off rank k . nERR-IA [12] assumes that, given a query q with sev-eral different subtopics c i , the probability of each subtopic P ( c i | q ) can be estimated and P i P ( i | q ) = 1. It also assumes that document relevance assessments rel ( d,c i ) are available for each subtopic. nERR-IA is defined as: nERR-IA = X where P ( j ) denotes the relevance probability of a docu-ment at rank j . The probability that a user doesn X  X  find the relevant document from document rank 1 to rank j-1 is P probability of a document at rank j in an ideal ranked list
Similar to  X  -nDCG, D-nDCG [12] assumes that a docu-ment is relevant to a query if it is at least relevant to one subtopic. D-nDCG is calculated in the framework of nDCG, with the only change of introducing per subtopic relevance. Instead of a graded judgment for the whole query, D-nDCG uses a weighted combination of subtopic related relevance judgments as the global gain (GG):
Both nERR-IA and D-nDCG employ P ( c i | q ), the prob-ability of the i th subtopic for query q . P ( c i | q ) indicates the importance of the i th subtopic for the entire informa-tion need. The bigger P ( c i | q ) is, the more nERR-IA and D-nDCG favor systems that retrieve relevant documents for the i th subtopic. Saika et al. [12] estimated P ( i | q ) uniformly or non-uniformly. The former assumes uniform distribution of subtopics, which is equivalent to nDCG. The latter esti-mates the i th subtopic probability as 2 n  X  i +1 / P n k =1 when n is reasonably large. Saika et al. reported that both approaches yield similar results.
When evaluating an IR system, regardless of whether the user X  X  focus is precision-or recall-oriented, the time for the user to actually acquire information needs to be considered. Intuitively, documents ranked higher in the returned list save the user time. K  X  aki et al. [5] created a measure termed immediate accuracy that represents how often users found at least one relevant result by the nth result selection. Another metric search speed measures the relevant answers obtained per minute. This has been modeled through metrics such as nDCG [4], Rank Biased Precision (RBP) [8], and Expected Reciprocal Rank (ERR) [1].

The recent time-based-gain measure (TBG) [13] models a gain function that considers factors such as document length and duplicate documents. TBG models the time actually spent, rather than assuming a one-to-one relationship be-tween document rank and time spent. The model considers whether a user spends more time reading a longer docu-ment, or if a summary is read prior to clicking and reading a document. The metric considers the time it takes the user to reach the position at which the document was ranked. Smucker et al. [13] calibrated their model based on a user study. According to their estimation, the time-based gain: where g k is the gain of the k th document in the ranked result list, g k = 0 . 4928 if the k th document is relevant, otherwise 0, and halflife=224 seconds. The expected time for a user to reach a document at rank k is: Time( k ) = P j =1 4 . 4 + (0 . 018 l j + 7 . 8) P ( C = 1 | R = r j ), where l length of the document at rank j , r j is the binary relevance judgment associated with d j . P ( C = 1 | R = r j ) is the con-ditional probability that the user clicks the r th document given document relevance, set to 0.65 if r j = 1, otherwise the probability is set to 0.39.

We argue that TBG oversimplifies the parameters that define the time it takes the user to reach document rank k by keeping these variables constant with respect to time. In our model, we address the tradeoff between search speed and the subtopic relevance and leave for future work a calibration to more accurately model user behavior.
We design a conceptual user utility model called the wa-ter filling model. We form an analogy between professional search and filling water into an empty container which we call a task cube . This model forms the basis of the Cube Test for evaluation.

Figure 1 shows the conceptual model of an empty task cube , which is intended to represent the user X  X  entire in-formation need. The task cube has unit length of 1. The segments of the bottom side of the cube represent subtopics, and the area of each segment represents the importance of Figure 1: An empty task cube with 6 subtopics.
 Figure 3: High scoring re-sult.
 the corresponding subtopic in view of the total informa-tion need. The area values are  X  1 , X  2 , X  3 , X  4 , X  5 , and  X  subtopics c 1 ,c 2 ,c 3 ,c 4 ,c 5 , and c 6 , respectively. Each cuboid, or column, in the cube represents the information need of the corresponding subtopic.

We imagine each retrieved document as water that flows into all relevant cuboids. In some cases, there is a one-to-one mapping between returned documents and subtopics; and in other cases, a document X  X  contents can flow into sev-eral different chambers of the cube if it is relevant to sev-eral different corresponding subtopics. The volume of water added to each cuboid varies depending on its relevance to the subtopic. By filling the cube with  X  X ocument water, X  each cuboid contains water filled to various heights rang-ing from empty to full. As users examine documents they accumulate more information for the total need, but gain-ing information for each subtopic changes at different rates. Figure 2 shows this imaginary  X  X ater filling X  process.
The height of the cube constrains the maximum amount of relevant information that a cuboid can contain, mirror-ing the maximum amount of relevant information that the user cares about for a given subtopic. Finding additional documents that map to a cuboid which is already filled can-not contribute more volume to cuboid and does not provide any additional value to the overall information need of the user. Thus, once any cuboid of the task cube is filled, the corresponding subtopic need is considered satisfied.
The water filling model captures multiple dimensions in the general information seeking process. Recall-oriented IR problems prefer a set of documents that can contribute to-wards filling a greater number of cuboids, while precision-oriented IR problems prefer a few documents to produce high-volume cuboids quickly without requiring many cuboids to contribute. In professional search, we argue that hav-ing more cuboids partially filled is more appropriate than a smaller number of high-volume cuboids. We design the actual evaluation metric to favor a system that produces a filled task cube as in Figure 3 rather than a system that produces a filled task cube as in Figure 4.
The objective function of this water filling process is to fill up the entire cube with  X  X ocument water X  as quick as possible. This equates to the goal of information seeking: to find enough relevant information as soon as possible. This intuition is used as the main optimization objective for the Cube Test to evaluate search systems.

Our model does not assume linear document traversal as most IR evaluation metrics assume. Whether the returned documents are traversed linearly or non-linearly, our model takes into account time taken rather than traversal order. Our model does not require or assume that the returned documents are ranked, and thus can handle the output of unranked Boolean queries or a mix of ranked and unranked results. They can also come from results returned for mul-tiple queries in a session.
Calculation of the Cube Test simulates the water filling model for information seeking. Given an information need Q , we can construct a task cube. Letting D be a set of documents returned by system S , we assume that the docu-ments are ordered by the sequence that they are viewed by the user. This order does not necessarily match their rank-ing orders since the user can view them non-linearly. We calculate the gain for D to fulfill the task cube for Q .
Each document d j  X  D examined contributes some gain of information equal to the volume of relevant  X  X ocument water X  that matches to subtopics in the task cube. It can be calculated as: Eq. 7 calculates the volume of the  X  X ocument water X  in the cuboids that represent the subtopics that d j is relevant to. KeepFilling i is a function specifying whether more  X  X ocu-ment water X  is needed for a subtopic, which depends on the current amount of  X  X ocument water X  in a subtopic cuboid. Using mathematical representation, Eq. 7 becomes: where rel (), a score in [0,1], denotes the relevance between a document and a subtopic.  X  i represents the importance of subtopic c i ; P i  X  i = 1. I is the indicator function. Since the task cube is in unit length, MaxHeight is set to 1. MaxHeight provides a constraint such that once a cuboid is full of  X  X ocument water X , it cannot acquire further gain from any future document.

The discount factor  X  is the novelty discount factor that is related to previous gains for the same subtopic c 1 , 2 ,..,t  X  1  X  can be any formula that models a discounting function. In our experiments, we set where nrel ( c i ,j  X  1) is the number of relevant documents for subtopic c i in the previously examined documents ( d to d j  X  1 ). We study the effect of  X  on the tradeoff between recall vs. precision in the experimental section.

Eq. 8 calculates the gain of document d j as a sum of relevance between d j and each subtopic. When d j is under examination for subtopic c i , if c i has received enough evi-dence from previously examined documents ( d 1 to d j  X  1 contributes nothing to providing evidence to c i and there-fore its contribution is zero. This is captured in the indicator function: if the accumulated gain for concept c i is equal to or more than 1, i.e., the maximum amount of relevance this concept needs, even if document d j is very similar to c i contribution is counted as 0 since it does not add more value in finding useful information for c i .

For a list of documents D , the accumulated gain up to document d j can be calculated as: where Gain ( Q,d j ) is the per-document gain for document d . Note that we do not assume discounted gain accord-ing to document rank as in nDCG and many other met-rics. This is because that we do not assume the result lists are ranked, which is common in professional search environ-ment. It makes our metric general enough to handle both ranked and unranked search results.

The above models the gain that a user gets in the search process. However, this is only one aspect of the objective function. We also need to fill up the cube as quickly as possible. Thus, we model the overall utility and propose a new evaluation metric Cube Test CT as: It is actually an average speed function that measures the volume (gain) change during a period of time. In more gen-eral cases for calculating the speed of how fast we can fill up the task cube at time t , we calculate the speed as the derivative of the gain over time: which can be further written as: where t is the moment examining the t th document, D t is the set of documents examined by the user from the beginning up to the t th document, and Time( D t ) is the amount of time taken to examine the documents up to the t th document.
We compare the Cube Test with a number of IR evaluation metrics, including Recall [18], MAP [18], PRES [7], TBG [13], nDCG [15],  X  -nDCG [2], I-rec [11], nERR-IA [12], and D-nDCG [12]. Among these metrics, Recall, MAP, PRES, TBG and I-rec use binary-relevance, while nDCG,  X  -nDCG, nERR-IA, D-nDCG and CT use graded relevance.
In the experiments, we compare all the metrics on two patent datasets: USPTO and CLEF-IP 2012.
The USPTO 1 dataset is publicly available. It consists of three million patent applications and publications filed from 2001 to 2013 in XML format with images removed. http://www.google.com/googlebooks/uspto-patents-applications-text.html. Some claims in a patent application refer to other claims. We treat the former as dependent claims, and treat other claims as independent claims. We assume that each claim is a subtopic query, and assign an importance weight of  X  ind 1 for all independent claims and  X  dep = 0 . 5 for all dependent claims. Then we normalize the subtopic importance value by dividing it by the sum of all subtopics X  importance values.
Using the Lemur search engine package, 2 we created 33 runs using various state-of-the-art retrieval algorithms, in-cluding tf-idf, language modeling, and Okapi BW25. We also applied several query expansion and refining strategies, including adding titles or important sentences into query keywords, filtering out terms with too high or too low IDF values, and filtering out verbs and adverbs. These runs were created for 49 patent prior art finding tasks. The input query is a complete patent application document. Patents cited by examiners in the office actions are considered relevant prior art. We generated ground truth automatically from the of-fice actions publicly available at USPTO PAIR. 3
Another patent dataset is the European patents provided by CLEF-IP 2012 [9]. These are XML patent documents from the European Patent Office (EPO) prior to the year 2002 as well as over 400,000 documents published by the World Intellectual Property Organization (WIPO). The doc-uments are multilingual, including English, German, and French. We evaluate the 31 official runs from 5 teams that were submitted to CLEF-IP 2012. The ground truth is pro-vided by CLEF-IP.

CLEF-IP X  X  ground truth does not provide graded rele-vance. However, it provides information about which and how many paragraphs in a relevant document are related to one subtopic. We generate the relevance grade by trans-forming the number of relevant paragraphs into a scale of [1  X  4]. Let Rel be the relevant document set, TSet the topic Set, NRelPara ( d,q ) the number of paragraphs in document d relevant to topic q, then document d X  X  relevance grade to-
We experiment with several CT variations and differenti-ate them using subscripts as outlined in Table 2. We employ the measure of discriminative power [10], proposed by Sakai, to evaluate a metric X  X  ability to distinguish IR systems.
In Table 3, we examine the discriminative power for met-rics under comparison at the significance level 0.05, which in-dicates that the metric shows reasonably strong evidence to claim that two runs A and B are different. Our experiments show that when (  X  ind , X  dep ) = (1 , 0 . 5), CT metrics give the best discriminative power. Moreover, there is a large dis-criminative power drop from CT XXc to CT XX  X  c , which leads to the conclusion that CT XXc is a better choice than CT XX  X  c http://www.lemurproject.org/. http://portal.uspto.gov/pair/PublicPair.
 l Subtopic importance is generated by a decay function p Subtopic importance is  X  ind for independent claims, t Assume only the top 10 documents will be examined. a Assume documents will be examined from top down, g Calculate time using Time ( k ) = c Assume the task cube has a top cover and subtopic X  X  Furthermore, we find that in the CLEF-IP dataset, all CT metrics show high discriminative power. For the USPTO dataset, Recall and I-rec show the best discriminative power. CT metrics show good discrimination power. The relative low discriminative powers of the CT metrics may be due to the fact that the ground truth of the USPTO dataset are automatically generated from office actions, which unavoid-ably contains some errors. Nonetheless, the results suggest that the proposed CT metric works well in complex search tasks which demonstrate multiple subtopics.
In this section we demonstrate CT X  X  ability to be able to adjust its bias between recall-oriented tasks and precision-oriented tasks. We generate two artificial ideal runs. The first run is totally biased towards coverage. It arranges rel-evant documents to each subtopic in a round-robin fashion and sorts subtopics by their importance  X  i in descending or-der. The second run is totally biased towards precision, i.e.
Figure 6: CT Score vs.  X  for the single relevance run. single document relevance. It puts all relevant documents by rel ( d,c i ) for a subtopic first, then for the next subtopic. The subtopics are sorted by their importance. We conduct this experiment using CT lgc .

The coefficient  X  in Eq. 9 can be used to adjust CT X  X  de-gree of how much bias towards subtopic coverage. In Figure 5 and Figure 6, the CT metric evaluates the coverage run and the single relevance run with  X  changes in the range of [0.1,0.9]. When  X  is small, the CT metric has a big discount and rewards novelty heavily. It means that a relevant docu-ment will contribute little gain if it is not the first document that covers a subtopic. The CT metric is thus biased towards coverage and rewards more for runs that produce relevant documents that spread across different multiple subtopics. CT therefore shows higher values for the coverage run than the single relevance run. When  X  is big, we observe the oppo-site effect. The CT metric is biased towards precision and rewards more for runs that produce highly relevant docu-ments early. CT therefore shows higher values for the single relevance run than the coverage run. By introducing the tradeoff factor  X  , the proposed CT metric is able to balance the tradeoff between recall and precision. This paper presents a novel evaluation metric  X  the Cube Test (CT), based on a novel utility model  X  the water filling model. CT considers both subtopic relevance and the speed required to fulfill the overall information need. It well cap-tures the features and requirements of complicated search tasks. These features include (1) multiple subtopics in a single document is allowed, which is close to reality; (2) the user X  X  information need on certain subtopic can be ful-filled after collecting enough data; (3) different subtopics may have different importance with respect to each query; and (4) an IR system is considered performing better if it allow users to spend less time to gain more information. Experiments demonstrate that our new metric effectively distinguishes and rates IR systems.
 Acknowledgments: Portions of this work were conducted to explore new concepts under the umbrella of a larger project at the US Patent and Trademark Office. The authors would also like to thank the anonymous reviewers for their valuable comments and suggestions.
