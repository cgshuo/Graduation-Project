 GvLW09]. Recently, Balcan and Blum [BB08] proposed a superv ised model of clustering, where important directions. As a motivating example, consider Go ogle News, where news documents are story. In this case, it is clear to the human eye (the teacher) which group each document should of the teacher.
 median, over the given set of points [KVV00, CGTS99]. These a pproaches work under the implicit assumption that by minimizing a certain objective function one can reach close to the underlying Gaussians [Das99]. However when dealing with web-pages, do cuments etc. it is not very clear if { analyzing clustering problems and algorithms. One such fra mework was proposed by Balcan and Blum [BB08] who, motivated by different models for learning under queries, proposed a model for clustering under queries.
 The model is similar to the Equivalence Query(EQ) model of le arning [Ang98] but with a differ-{ c 1 , c 2 , . . . , c k } For example, the points belonging to the cluster c assume that each point belongs to exactly one of the k clusters. As in the EQ model of learning, the algorithm presents a hypothesis clustering { h some limited feedback. Hence, the model in [BB08] considers the following feedback: If there is a cluster h rithm to split that cluster by issuing the request split ( h the cluster h quest merge ( h One could also imagine applying this split-merge framework to cases where the optimal clustering instances. 1.1 Contributions the class of hyperplanes in d dimensions with known slopes. Balcan and Blum [BB08] also ga ve a new algorithm is much simpler than the one from [BB08]. We stu dy two natural generalization of the original model. In the original model the teacher is only allowed to merge two clusters h h which the teacher can ask the algorithm to merge h allow for the teacher requests to be imperfect.
 algorithms for clustering intervals in both the models.
 We also apply the split-merge framework of [BB08] to dataset s satisfying a spectrum of weak to the strict threshold property (Theorem 6.1). We consider the model proposed by Balcan and Blum [BB08]. The clustering algorithm is given a by interacting with the teacher as follows: 2.1 A generic algorithm for learning any finite concept class We reduce the query complexity of the generic algorithm for l earning any concept class [BB08], new algorithm is described below.
 in V S | R is consistent with h . } . At each step the algorithm outputs clusters as follows: If the teacher says split ( h teacher says merge ( h queries.
 Proof. At each request, if the teacher says split ( h teacher says merge ( h h number of queries will be at most log | V S |  X  log | C | k  X  k log | C | .
 The analysis can be improved if the VC-dimension d of the concept class C is much smaller than number of ways to split m points using concepts in C . Also from Sauer X  X  lemma[Vap98] we know dimensional space, and to hyperplanes in d dimensions with known slopes. 3.1 An algorithm for clustering rectangles Each rectangle c in the target clustering can be described by four points ( a most 2 k points a clusters. The algorithm is sketched below: O (( k log m ) 3 ) queries.
 ( x i , x j ) , ( y i , y j ) complexity to O (( k log m ) 2 ) . So, for rectangles we have the following result 1 . at most O (( k log m ) 2 ) queries.
 get the following result at most O (( kd log m ) d ) queries.
 Corollary 3.4. There is an algorithm which can cluster the class of hyperpla nes in d dimensions having a known set of slopes of size at most s , using at most O (( kds log m ) d ) queries. k the teacher need not provide feedback every time the algorit hm proposes an incorrect clustering. the requests have no noise and the algorithm has access to all the points in X . We now give an algorithm for learning intervals in this model. 4.1 An algorithm for clustering intervals We assume that the space X is discretized into n points. Let us assume that there ex-ist points { a marked/unmarked . When a new interval is created, it is always unmarked . An interval is marked if we know that none of the points( a algorithm is sketched below: first bound the total number of split requests. For every poin t a two variables lef t size ( a a is also a boundary point in the hypothesis clustering ( [ x, a number of points in [ x, a unmarked intervals increase only via split requests. On eve ry merge request either an unmarked is  X  O ( k log n ) .
 k log | C | queries. The previous two models assume that there is no noise in the te acher requests. This is again an unrealistic assumption since we cannot expect the teacher r esponses to be perfect. For example, model [BB08]. As in the original model, the algorithm has m points. At each step, the algorithm proposes a clustering { h the feedback is noisy in the following sense a counter-example.
 Theorem 5.1. Consider m points on a line and k = 2 . Any clustering algorithm must use  X ( m ) queries in the worst case to figure out the target clustering i n the noisy model. Hence, we now consider a relaxed notion of noise. If the teach er says merge ( h Under this model of noise we now give an algorithm for learnin g k -intervals. 5.1 An algorithm for clustering intervals idea is that when the teacher asks to merge two intervals ( a at least  X  fraction of the portion to the left and the right of a as follows Theorem 5.2. The algorithm clusters the class of intervals using at most O ( k (log reduced by at least  X  . Hence, the total number of split + impure merge requests  X  k log We also know that the total number of unmarked intervals  X  k log intervals  X  2 k log To bound the total number of pure merges, notice that every ti me a pure merge is made, the size Hence, the algorithm makes at most O ( k (log d =  X  e 1 , e 2 , . . . , e ( n clustering functions, please see the appendix. 6.1 Threshold Separation We introduce a (strong) property that may be satisfied by d =  X  e is a distance between two points in differing clusters.
 S
TRICT T HRESHOLD S EPARATION . There exists a threshold t &gt; 0 such that all inner edges of  X   X  omitted). outer-cluster distances are increased. tion F , we have F ( d, k ) =  X  .
 of Consistent, k -Rich, and Order-Consistent functions has many members. We now present the algorithm to interact with the teacher.
 which can find the target partitioning for any hypothesis cla ss in O (log( n )) queries the target could be found immediately. By theorem 6.1, we kno w that the target must be exactly done using a binary search on the number of clusters which can vary from 1 to n . We start with find the correct number of clusters in O (log( n )) queries.
 sented in the next section can also be used, giving O (min(log( n ) , k )) queries. Strict Separation: Now we relax strict threshold separation S therefore Theorem 6.1 does not apply. However, [BBV08] prov e the following lemma SL ( d ) .
 the target partitioning for any hypothesis class in O ( k ) queries found.  X  -margin Separation: Margins show up in many learning models, and this is no except ion. A where the points all lie inside the unit ball.  X  With this property, we can prove the following for all hypoth esis classes Theorem 6.5. Given a dataset satisfying  X  -margin Separation, there exists an algorithm which can find the target partitioning for any hypothesis class in O ((  X  d be from the same cluster. We say such a hypercube is  X  X ure X .
 There are at most O ((  X  d at most O ((  X  d Several interesting problems remain [ABD09] M. Ackerman and S. Ben-David. Clusterability: A the oretical study. Proceedings of [AL10] Ben-David S. Ackerman, M. and D. Loker. Characteriza tion of Linkage-based Cluster-[Ang98] D. Angluin. Queries and concept learning. Machine Learning , 2:319 X 342, 1998. [BB08] Maria-Florina Balcan and Avrim Blum. Clustering wit h interactive feedback. In ALT , [BBV08] M.-F. Balcan, A. Blum, and S. Vempala. A discriminat ive framework for clustering [Blu09] Avrim Blum. Thoughts on clustering. In NIPS Workshop on Clustering Theory , 2009. [CGTS99] M. Charikar, S. Guha, E. Tardos, and D. B. Shmoy. A co nstant-factor approximation [Das99] S. Dasgupta. Learning mixtures of gaussians. In Proceedings of the 40th Annual Sym-[GvLW09] I. Guyon, U. von Luxburg, and R.C. Williamson. Clus tering: Science or Art? In NIPS [JS71] N. Jardine and R. Sibson. Mathematical taxonomy. New York , 1971. [Kle03] J. Kleinberg. An impossibility theorem for cluster ing. In Advances in Neural Informa-[KVV00] R. Kannan, S. Vempala, and A. Veta. On clusterings-g ood, bad and spectral. In FOCS [Lit87] Nick Littlestone. Learning quickly when irrelevan t attributes abound: A new linear-[Vap98] V. N. Vapnik. Statistical Learning Theory . John Wiley and Sons Inc., 1998. [ZBD] Reza Bosagh Zadeh and Shai Ben-David. Axiomatic Chara cterizations of Single-[ZBD09] Reza Bosagh Zadeh and Shai Ben-David. A Uniqueness T heorem for Clustering. In
