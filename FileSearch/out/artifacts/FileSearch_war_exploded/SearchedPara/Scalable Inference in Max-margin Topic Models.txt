 Topic models have played a pivotal role in analyzing large collections of complex data. Besides discovering latent se-mantics, supervised topic models (STMs) can make predic-tions on unseen test data. By marrying with advanced learn-ing techniques, the predictive strengths of STMs have been dramatically enhanced, such as max-margin supervised topic models, state-of-the-art methods that integrate max-margin learning with topic models. Though powerful, max-margin STMs have a hard non-smooth learning problem. Existing algorithms rely on solving multiple latent SVM subproblems in an EM-type procedure, which can be too slow to be ap-plicable to large-scale categorization tasks.

In this paper, we present a highly scalable approach to building max-margin supervised topic models. Our approach builds on three key innovations: 1) a new formulation of Gibbs max-margin supervised topic models for both multi-class and multi-label classification; 2) a simple  X  X ugment-and-collapse X  Gibbs sampling algorithm without making re-stricting assumptions on the posterior distributions; 3) an efficient parallel implementation that can easily tackle data sets with hundreds of categories and millions of documents. Furthermore, our algorithm does not need to solve SVM subproblems. Though performing the two tasks of topic dis-covery and learning predictive models jointly, which signifi-cantly improves the classification performance, our methods have comparable scalability as the state-of-the-art parallel algorithms for the standard LDA topic models which per-form the single task of topic discovery only. Finally, an open-source implementation is also provided 1 .
 G.3 [ Probability and Statistics ]: Statistical Computing Algorithms, Experimentation, Performance h ttp://www.ml-thu.net/  X  jun/gibbs-medlda.shtml Inference, Topic Models, Large-scale Systems, Max-margin Learning
Topic models such as latent Dirichlet allocation (LDA) [5] have been successful in discovering the latent factors under-lying observed data. The latent topic representations can be used for many subsequent tasks, such as classification, clus-tering or merely as a tool to structurally browse the data. To handle large-scale applications, scalable inference algo-rithms [16, 19, 1] have been developed, of which the current state-of-the-art approaches can easily tackle hundreds of mil-lions of documents and thousands of topics with hundreds of machines and thousands of CPU cores.

In many cases, we are interested in predictive tasks be-sides discovering latent topic representations. For example, for document data, we may be interested in predicting which categories a new document belongs to [27]; and for social network data, people have been interested in building pre-dictive models that can suggest friends to social network users or recommend products [7, 8]. To improve the predic-tive ability of topic models, people have been interested in learning supervised topic models (STMs) [4, 27] which can perform the two tasks of discovering latent topic structures and learning predictive models jointly.

Max-margin STMs (e.g., maximum entropy discrimina-tion LDA or MedLDA [27]) are the state-of-the-art methods for classification, which integrate the discriminative max-margin learning with topic models and have shown great promise in text categorization and image annotation [25, 23]. Unfortunately, the resulting learning problems normally in-volve a non-smooth objective, to tackle which an EM-type procedure is normally applied in the current variational or Monte Carlo solvers [12]. The EM-type algorithms need to solve many latent SVM subproblems, whose efficiency can be a bottle-neck to make these models unscalable to large scale categorization tasks, such as the PASCAL large-scale hierarchical classification challenge (LSHTC) 2 and the Im-ageNet large scale visual recognition challenge (ILSVRC) 3 which normally involve large data sets consisting of thou-sands of categories and millions of data samples.
To meet the requirements of large-scale document catego-rization as well as topic discovery, in this paper we present to our knowledge the first highly scalable max-margin super-h ttp://lshtc.iit.demokritos.gr/LSHTC2 CFP h ttp://www.image-net.org/challenges/LSVRC/2012/index vi sed topic model. Our method relies on three key innova-tions. First , unlike conventional max-margin STMs [27] that minimize a margin-loss of an expected prediction rule, we present a multi-task Gibbs max-margin STM that optimizes an expected margin loss of many latent predictive rules, each of which is randomly drawn from a posterior distribution. The method is a substantial extension of the recent work on binary classification [28] to the tasks of both single-label multi-class and multi-label [21] classification. Second , we present a simple collapsed Gibbs sampling algorithm with-out making any restricting assumptions on the posterior dis-tributions, by exploring the classical ideas of data augmen-tation in statistics [20, 22] and its recent developments on learning large-margin classifiers [17]. Third , we present a scalable parallel implementation by leveraging the modular-ity property of our algorithm and the recent advances in scalable inference methods for LDA.

We apply our methods to large-scale text categorization data sets. Experimental results demonstrate significant im-provements on classification performance compared to the SVM classifiers built on raw features and on the latent top-ic features discovered by LDA; while the time efficiency is comparable to the state-of-the-art parallel LDA [1]. In sum-mary, our work substantially extends [28] by introducing:
Outline : We introduce the binary Gibbs MedLDA in Sec-tion 2 and present the multi-task formulation in Section 3. We present the parallel implementation in Section 4, and present the large-scale experiments in Section 5. Finally, Section 6 concludes.
We begin by a brief overview of the Gibbs MedLDA for binary classification.
We denote the labeled training set by D = { ( w d ,y d ) } D where the category variable Y takes values from the binary space Y = { X  1 , +1 } . Basically, a Gibbs MedLDA model consists of two parts X  X n LDA model for describing input the words appearing in document d , and a Gibbs classifier for considering the supervising signal y = { y d } D d =1 we introduce each of them in turn.

LDA : LDA is a hierarchical Bayesian model that posit-s each document as an admixture of K topics, where each topic  X  k is a multinomial distribution over a V -word vo-cabulary. For document d , the generating process can be described as 1. draw a topic proportion d  X  Dir( ) 2. for each word n (1  X  n  X  N d ):
A K -d imension binary vector with only one nonzero entry. where Dir(  X  ) is a Dirichlet distribution; Mult(  X  ) is multino-mial; and  X  z dn denotes the topic selected by the non-zero entry of z dn . For Bayesian LDA, the topics are random sam-ples drawn from a Dirichlet prior,  X  k  X  Dir( ).

Given a set of documents W , we let z d = { z dn } N d n =1 the set of topic assignments for document d and let Z = { z d } D d =1 and  X  = { d } D d =1 denote all the topic assignments and mixing proportions for the whole corpus, respectively. Then, LDA infers the posterior distribution using the Bayes X  rule wh ere p 0 (  X  , Z ,  X  ) = and p ( W | Z ,  X  ) = generating process.

An alternative way to understand Bayesian inference is that the posterior distribution by Bayes X  rule is equivalent to the solution of the optimization problem where KL( q || p ) is the Kullback-Leibler divergence and the space of probability distributions. In fact, if we add the constant log p ( W ) to the objective, the problem is the min-imization of KL-divergence KL( q (  X  , Z ,  X  )  X  p (  X  , Z ,  X  whose solution is naturally the desired posterior distribution by the Bayes X  rule.

One advantage of this variational formulation of Bayesian inference is that it can be naturally extended to include some regularization terms on the desired post-data posterior dis-tribution q . This insight has been taken to develop regu-larized Bayesian inference (RegBayes) [29], a computational framework of doing Bayesian inference with posterior reg-ularization. As shown in [12], MedLDA is one example of RegBayes model. Moreover, our Gibbs max-margin topic models follow this similar idea too.

Gibbs Classifier : In learning theory, one approach to building classifiers with a posterior distribution of models is to minimize an expected loss, under the framework known as Gibbs classifiers (or stochastic classifiers) [14, 6, 10] with nice theoretical properties. For our case of inferring the dis-tribution of latent topic assignments Z and the classification model , the expected margin loss is defined as follows. If we have drawn a sample of the topic assignments Z and the prediction model from a posterior distribution q ( , Z ), we can define the linear discriminant function where  X  z is a the average topic assignment vector with each element being  X  z k = 1 N tions using the latent Gibbs rule Let  X  d =  X   X  y d  X   X  z d , where  X  is a positive cost param-eter. Then, the hinge loss of the classifier is R ( , Z ) =  X  d max(0 , X  d ) , a function of the latent variables ( , Z ), and the expected hinge loss is a function of the posterior distribution q ( , Z ). Since for any ( , Z ), the hinge loss R ( , Z ) is an upper bound of the training error of the latent Gibbs classifier (3), i.e., R ( , Z )  X  In other words, R ( q ) is an upper bound of the expected training error of the Gibbs classifier (3). Thus, it is a good surrogate loss for learning a posterior distribution which could lead to a low training error in expectation.
Regularized Bayesian Inference : To integrate the above two components for hybrid learning, Gibbs MedLDA regu-larizes the properties of the topic representations by solving the regularized Bayesian inference (RegBayes) problem where c is a regularization parameter and L ( q ) is the ob-jective of problem (1). Due to the strong coupling between the Gibbs classifier and the LDA model, we can expect to learn a posterior distribution q ( ,  X  , Z ,  X  ) that on one hand describes the observed data and on the other hand predicts as well as possible on training data.
 In [28], extensive comparison with MedLDA was provided. Basically, MedLDA is also a RegBayes model, but it uses a different posterior regularization which is derived from an expected classifier. The effective algorithms for MedLDA are the two-stage approaches [12] that apply Monte Carlo methods as the inner inference engine and iteratively solve latent SVMs to learn the classifier distribution. If we directly solve problem (4), the expected hinge loss R is hard to deal with because of the non-differentiable max function. Fortunately, a simple collapsed Gibbs sampling algorithm can be developed with analytical forms of local conditional distributions, based on a data augmentation for-mulation of the expected hinge-loss.

Let  X  ( y d | z d , ) = exp { X  2 c max(0 , X  d ) } be the unnormal-ized pseudo-likelihood of the response variable for document d . Then, problem (4) can be written as where  X  ( y | Z , ) = can get the normalized posterior distribution wh ere  X  ( y , W ) is the normalization constant. Using the ideas of data augmentation [20, 17], the unnormalized pseudo-likelihood can be expressed as Th is result indicates that the posterior distribution of Gibbs MedLDA, q ( ,  X  , Z ,  X  ), can be expressed as the marginal of a higher dimensional distribution that includes the aug-mented variables = {  X  d } D d =1 . The complete posterior dis-tribution is q ( , ,  X  , Z ,  X  ) = p 0 ( ,  X  , Z ,  X  ) p ( W wh ere the pseudo-joint distribution of y and is  X  ( y , |  X 
Although with the data augmentation formulation we can do Gibbs sampling to infer the complete posterior distribu-tion q ( , ,  X  , Z ,  X  ) and thus q ( ,  X  , Z ,  X  ) by ignoring , the mixing would be slow due to the large sample space of all latent variables. One way to effectively accelerate the mixing is to integrate out the intermediate Dirichlet vari-ables (  X  ,  X  ) and build a Markov chain whose equilibrium distribution is the resulting marginal distribution q ( , , Z ). This idea has been successfully used in LDA [11] and was taken in [28] to develop a collapsed Gibbs sampler for Gibbs MedLDA. With the data augmentation representation, this leads to an  X  X ugment-and-collapse X  sampling algorithm for Gibbs MedLDA, as summarized below.

By integrating out the Dirichlet variables (  X  ,  X  ) in the complete posterior distribution, we get the collapsed poste-rior distribution wh ere  X  ( x ) = is the number of times the term t being assigned to topic k over the whole corpus; C k = { C t k } V t =1 is the set of word counts associated with topic k ; C k d is the number of times that terms being associated with topic k within the d -th document; and C d = { C k d } K k =1 is the set of topic counts for document d . Then, the conditional distributions used in collapsed Gibbs sampling are as follows.

For : For the commonly used isotropic Gaussian dis-tribution p 0 ( ) = parameter, the conditional distribution of given the other variables is also Gaussian: where the posterior mean and the covariance matrix are  X  = W e can easily draw a sample from this K -dimensional mul-tivariate Gaussian distribution. The inverse can be robustly done using Cholesky decomposition, an O ( K 3 ) procedure. Since K is normally not large, the inversion can be done efficiently, especially in the applications where the number of documents is much larger than the number of topics.
For Z : By canceling common factors, the conditional dis-tribution of one variable z dn given others Z  X  is q ( z k dn = 1 | Z  X  , , ,w dn = t )  X  where C  X   X  ,  X  n indicates that term n is excluded from the corresponding document or topic;  X  = 1 N out word n . We can see that the first term on the right hand is from the LDA model for observed word counts and it is the same as that in the collapsed Gibbs sampling method for LDA [11]; while the second term is from the supervised signal y which comes into play through the expected loss in problem (4).

For : Finally, the conditional distribution of the aug-mented variables given the other variables is a generalized inverse Gaussian distribution [9]: where GIG ( x ; p,a,b ) = C ( p,a,b ) x p  X  1 exp(  X  1 2 ( C ( p,a,b ) is a normalization constant. Alternatively,  X  follows an inverse Gaussian distribution where IG ( x ; a,b ) =
With the above conditional distributions, we can construc-t a Markov chain which iteratively draws samples of the clas-sifier weights using Eq. (6), the topic assignments Z using Eq. (7) and the augmented variables using Eq. (8), with an initial condition. To sample from an inverse Gaussian distribution, we apply the efficient transformation method with multiple roots [15]. In our experiments, we initially set = 1 and randomly draw Z from a uniform distribution. In training, we run this Markov chain to finish the burn-in stage with T iterations. Then, we draw a sample  X  as the Gibbs classifier to make predictions on testing data.
To apply the Gibbs classifier  X  , we need to infer the topic assignments for testing document, denoted by w . A fully Bayesian treatment needs to compute an integral in order to get the posterior distribution of the topic assignment given training data D and the testing document content w : p ( z | w , D )  X  where the second equality holds due to the conditional in-dependence assumption of the documents given the topics. Various approximation methods can be applied to compute the integral. Here, we take the approach applied in [27, 12], which uses a point estimate of topics  X  from training data and makes prediction based on them. Specifically, we use the MAP estimate  X   X  (a Dirac measure) to approximate the probability distribution p (  X  |D ). For the collapsed Gibbs sampler, an estimate of  X   X  using the samples is Then, given a testing document w , we infer its latent com-ponents z using  X   X  as where C k  X  n is the times that the terms in this document w are assigned to topic k with the n -th term excluded.
Multi-task learning is a scenario where multiple poten-tially related tasks are learned jointly with the hope that their performance can be boosted by sharing some statistic strengths among these tasks, and it has attracted a lot of research attention. In particular, learning a common rep-resentation shared by all the related tasks has proven to be an effective way to capture task relationships [2, 3, 29]. Here, we take the similar approach to learning multiple pre-dictive models which share the common topic representa-tions. As having been demonstrated in previous work [29] and our own experiments later, one successful application of the multi-task model is to do the single-label multi-class or multi-label [21] classification, where each task correspond-s to a binary classifier to determine whether a data point belongs to a particular category.
We consider the L binary classification tasks and each task i is associated with a classifier with weights i . We assume that all tasks work on the same set of input data W = { w d } D d =1 , but each data d has different binary labels in different tasks. When we have the classifier weights and the topic assignments Z , drawn from a posterior distribution q ( , Z ), we follow the same principle as in Gibbs MedLDA and define the latent Gibbs rule for each task as and the expected hinge loss is For each task i , we can follow the argument as in Gibbs MedLDA to show that the expected loss R i ( q ) is an upper bound of the expected training error the Gibbs classifier (10). Thus, it is a good surrogate loss for learning a posterior distribution which could lead to a low training error in expectation.

Then, following the similar procedure of defining the bi-nary Gibbs MedLDA classifier, we can define the multi-task Gibbs MedLDA model as solving the following regularized Bayesian inference problem where the multi-task expected hinge loss is defined as a sum-mation of the expected hinge loss of all the tasks
Due to the separability of the multi-task expected hinge loss, we can apply the same method as in the binary model to reformulate each task-specific expected hinge loss R i a scale mixture by introducing a set of augmented variables {  X  be the unnormalized pseudo-likelihood of the response vari-able for document d in task i . Then, we have A lgorithm 1 for Multi-task Gibbs MedLDA 1: Ini tialization: set = 1 and randomly draw z dk from 2: for m = 1 to T do 3: for i = 1 to L do 4: draw a classifier i from the normal distribu-5: end for 6: for d = 1 to D do 7: for each word n in document d do 8: draw a topic using distribution (14) 9: end for 10: for i = 1 to L do 11: draw (  X  i d )  X  1 (and thus  X  i d ) from distribution (15). 12: end for 13: end for 14: end for Ob viously, when L = 1, the multi-task model reduces to the binary Gibbs MedLDA.
Similar as in the binary Gibbs MedLDA, we can derive the collapsed Gibbs sampling algorithm, as outlined in Al-gorithm 1. Specifically, let b e the joint pseudo-likelihood of the class labels y i = and the augmentation variables i = {  X  i d } D d =1 . Then, for the multi-task Gibbs MedLDA, we can integrate out the Dirichlet variables ( X ,  X ) and get the collapsed posterior distribution Th en, we can derive the conditional distributions used in collapsed Gibbs sampling are as follows.

For : we also assume its prior is an isotropic Gaussian distribution p 0 ( ) = q ( | Z , ) = where the posterior mean and the covariance matrix are  X  S imilarly, the inverse can be robustly and efficiently done using Cholesky decomposition, an O ( K 3 ) procedure.
For Z : The conditional distribution of Z is q ( Z | , )  X  By canceling common factors, we can derive the conditional distribution of one variable z dn given others Z  X  as: q ( z  X  exp tion value without word n . We can see that the first term is from the LDA model for observed word counts and the second term is from the supervised signal { y i d } from all the multiple tasks.

For : Finally, one can derive that the conditional dis-tribution of the augmented variables is fully factorized, q ( | Z , ) = generalized inverse Gaussian distribution Therefore, (  X  i d )  X  1 follows an inverse Gaussian distribution
One nice property of the above Gibbs sampling algorithm for the multi-task Gibbs MedLDA 5 is that it can be easily parallelized, due to the following observations. The true difficulty is on parallelizing the sampling step of topic assignments, z dn , which depend on the global variables C k (i.e., the topic-word count table) that need to be updat-ed/synchronized during the local sampling process. Fortu-nately, our sampling algorithm is highly modular X  X nce the classifier weights (and the augmented variables ) are giv-en, the sampling of each topic assignment is almost the same as that in the standard LDA, except some additional compu-tation which is carried out locally to each document, as show in Eq. (14). Therefore, we can leverage the recent advances in parallel topic models [1, 19] to solve this problem. Given a multi-core and multi-machine LDA sampler, we can devel-op our parallel sampler using a simple procedure as detailed below.
Th e binary Gibbs MedLDA model is a special case of the multi-task model with L = 1. T able 1: The amount of time (seconds) taken by the step of sampling and network communication on the Wiki data set. K  X  X he number of topics; M  X  X he number of machines; communication includes both reduce and broadcast time. K =5 00, M =10 71 .26 (2.63%) 88.28 (3.25%) 2712.64 K =5 00, M =20 71 .29 (5.01%) 52.90 (3.72%) 1423.53 K =1 000, M =10 14 47.07 (16.99%) 226.47 (2.66%) 8517.97 K =1 000, M =20 14 49.51 (28.73%) 135.44 (2.68%) 5044.61
Let M b e the total number of processes (or machines) and let D m be the data in process m . Then each process m performs the following computations 1. draw topic assignments : use the given LDA sam-2. draw scale parameters : draw the scale parameter 3. compute local statistics : compute the following s-Since  X  m i is symmetric, it suffices to compute only the upper or lower triangle. After process m has finished the local computation, it passes the local statistics m i and  X  m i the master process, which performs the following operations 1. compute  X  : by collecting the message from slaves, it 2 . compute : after obtaining the new  X  i , it updates 3. draw classifier weights : when all local statistic-4. synchronize classifier weights : after sampling , There are indeed more sophisticated synchronization strate-gies that could be applied, however as shown in Table 1, both communication and sampling of take little time compared to the main algorithm. Therefore this treatment is sufficient to achieve high performance. For the LDA sampler, we use the current state-of-the-art method [1].
We run our experiments on a cluster with 20 nodes, where each node is equipped with two 6-core CPUs (2.93GHz).
We present experiments on several public text categoriza-tion data sets, whose statistics are shown in Table 2. The 20Newsgroups (20NG) data set consists of about 20K post-ings within 20 groups and each document has a single cat-egorical label, ranging from 1 to 20; we follow the same Table 2: Statistics of the data sets. N  X  X he number of documents; V  X  X he number of terms; and L  X  X he number of categories. da ta set N -tra in N -test V L t ype set ting as in [27] to build train/test partition and the vocab-ulary. The Wiki data set is built from the large Wikipedia set used in the PASCAL LSHC challenge 2012, and each document has multiple labels. The original data set 6 is ex-tremely imbalanced. We built our data set by selecting the 20 categories that have the largest numbers of documents and keeping all the documents that is labeled by one of the 20 categories. The third data set is the Reuter X  X  Corpus Vol-ume (RCV1-v2) [13], another standard benchmark 7 of which each document has multiple labels. To test the scalability of our method, we have partitioned the data set into training and testing sets with a ratio of 7 : 1.
We first present some empirical results on the singly la-beled 20Newsgroups data set. For the binary Gibbs MedL-DA, one-vs-all is an effective strategy to do multi-class clas-sification [18]. To make the multi-task Gibbs MedLDA (MT-GibbsMedLDA) applicable to the singly labeled data set, we need to transform the true label to get the label for each bi-nary task. Let the label space be Y = { 1 ,...,L } . We define one binary classification task for each category i and the task is to distinguish whether a data example belongs to the class i (with binary label +1) or not (with binary label  X  All the binary tasks share the same topic representations. To apply the model as we have presented in Section 3, we need to determine the true binary label of each document in a task. Given the multi-class label y d of document d , this can be easily done by defining Figure 1 shows the accuracy and training time of the multi-task Gibbs MedLDA, the one-vs-all binary Gibbs MedL-DA [28], the multi-class MedLDA using Gibbs sampling [12] built with an expected classifier, and the two-stage approach of first using Gibbs LDA (gLDA) [11] to learn latent topic features and then building a SVM classifier 8 . We can see that the multi-task formulation of Gibbs MedLDA produces comparable performance as the one-vs-all method; while the two Gibbs MedLDA models slightly outperform MedLDA. Furthermore, the multi-task model is computationally more efficient than the one-vs-all approach due to the less number of topics. A naive parallelization of the one-vs-all approach is to learn the 20 binary classifiers in parallel, which im-proves the efficiency. However, the one-vs-all approach may not be a good choice if we want to get a holistic view of
A vailable at: http://lshtc.iit.demokritos.gr/
Available at: http://jmlr.csail.mit.edu/papers/volume5/ lewis04a/lyrl2004 rcv1 v2 REA DME.htm
The SVM classifier built on raw bag-of-words as well other variants of supervised topic models were outperformed by MedLDA. See [27] for an extensive comparison. Fi gure 1: Classification accuracy and training time of multi-task GibbsMedLDA, GibbsMedLDA with one-vs-all strategy, and the multi-class MedLDA with stage-wise Gibbs sampling. Fi gure 2: The classification accuracy, training ac-curacy and training time of the multi-task Gibbs MedLDA with different burn-in steps. the topic structures of the entire corpus, because it learn-s 20 independent sets of topics which are not easy to be merged. From Figure 2, we can see that the sampling al-gorithm converges quickly to stable performance within 40 iterations. These results demonstrate the effectiveness of the multi-task Gibbs MedLDA. In all the experiments, we fixed = 6 . 4 e , = 0 . 01 e ,  X  = 1 and c = 16. As in [28], Gibbs MedLDA is insensitive to these parameters in wide ranges. We omitted the sensitivity analysis for saving space.
Figure 3 shows the accuracy and training time of the multi-task Gibbs MedLDA with different numbers of ma-chines and different numbers of CPU cores. We can see that the single-machine-multi-core implementation is about 1 or-der of magnitude faster than the single-core version; while using multiple machines can further improve the efficiency dramatically. Meanwhile, the classification accuracy does not sacrifice much in a distributed environment. We now present the experiments of multi-task Gibbs MedL-DA on the two multi-label data sets, where each task is a binary classifier to identify whether a document belongs to a particular category. We use the F-measure, a harmonic mean of precision and recall, to evaluate the performance.
Figure 4 shows the classification F-measure and training time of multi-task Gibbs MedLDA, comparing with the lin-ear SVM classifier built on raw bag-of-words features and the two stage approach, LDA+SVM, which first fits an L-DA model using all the documents and then learn a linear SVM classifier. For Gibbs MedLDA, we report the perfor-mance in the single-machine-multi-core setting as well as the Fi gure 3: The classification accuracy and training time of the multi-task Gibbs MedLDA with different numbers of machines ( M ) and CPU cores ( P ). Fi gure 4: F-measure and training time of various methods on the Wiki data set. setting with all the 20 machines. For LDA+SVM, we use the public Yahoo-LDA on 20 machines (240 CPU cores) 9 . Note that for fair comparison, we use the standard col-lapsed Gibbs sampling for both LDA and Gibbs MedLDA, although Yahoo-LDA has the option to perform fast Gibbs sampling [26]. Developing a fast Gibbs sampling algorithm for Gibbs MedLDA is one of our future work. To learn the SVM classifiers, we use the liblinear package 10 with the one-vs-all strategy and train each binary classifier on one of the 20 machines. We can see that Gibbs MedLDA dramatically improves the classification performance over the two-stage approach of LDA+SVM. Furthermore, we found that the SVM classifier on the raw features doesn X  X  work well, main-ly due to the sparsity issue of the feature space. For training time, the amount of time required by the supervised Gibbs MedLDA is comparable to that by the unsupervised LDA. These results are impressive since Gibbs MedLDA perform-s two jobs of topic discovery and classifier learning jointly, while LDA performs topic discovery only.

Figure 5 presents how the classification performance and training time of the distributed MT-GibbsMedLDA ( M = 20 and P = 240) change with respect to T (i.e., the number of burn-in steps). We can observe that with a number of burn-in steps (e.g., 40, 60 or 80), we can get quite stable prediction performance, which 20 is not sufficiently large; and using a large T generally increases the training time about linearly. We set T = 40 in the experiments.
A vailable at: https://github.com/shravanmn/Yahoo LD A.
Available at: http://www.csie.ntu.edu.tw/  X  cjlin/liblinear/ the training time remains about constant. Here, we use { Fi gure 5: F-measure and training time of MT-GibbsMedLDA ( M = 20 and P = 240 ) with different numbers of burn-in steps on the Wiki data set.
 Figure 6 (Left) shows the scalability analysis for MT-GibbsMedLDA on the Wiki data set. We can draw the fol-lowing conclusions. First, when the computational resources are kept fixed, the amount of time required to process da-ta scales linearly with the amount of data. Second, when the data to machine ratio is kept constant, the amount of time required to process the data is about constant, very close to the ideal line. The tiny performance loss with more machines is mainly due to the latency in network commu-nication. But in general, these observations suggest that the parallel implementation of our sampling algorithms can scale nicely to massive data sets. For example, with 20 ma-chines (240 CPU cores), we can finish the training on 2.8M documents with 20 categories within an hour.
 Figure 6 (Middle) shows the scalability analysis on the RCV data set, when changing the number of labels while the total number of data samples is unchanged. We consider two parallelization strategies: For label partition, it confirms that the amount of time re-mains constant with respect to the number of labels when the label to machine ratio is kept constant, e.g., 5 in our case. While for the document partition strategy, since in-creasing the number of labels doesn X  X  change the amount of data, the running time decreases as more machines are used; furthermore, since more classifier parameters are indeed in-troduced the running time decreases sub-linearly when the label to machine ratio is fixed.

Finally, Figure 6 (Right) shows the amount of time re-quired by the distributed MT-GibbsMedLDA when the num-ber of machines increases, while the number of labels and the number of documents are fixed, on the Wiki data set. We can observed that for the strategy of DP, the amount of time decreases about linearly, i.e., when the number of machines is doubled, the running time decreases to about a half; while LP is slower because each machine in LP needs to process more data and the total number of topics is larger. Also, note that the most right point of LP is in fact the one-vs-all approach with binary GibbsMedLDA, which is much slower than MT-GibbsMedLDA with the DP strategy; this demon-strates the advantages of the multi-task formulation.
We have presented a highly scalable approach to building max-margin supervised topic models for large-scale multi-class and multi-label text categorization. Our Gibbs sam-pling algorithm builds on a novel formulation of multi-task Gibbs max-margin topic models as well as a data augmen-tation formulation. The algorithm is modular and can take a dvantages of recent advances in scalable inference for unsu-pervised topic models. Extensive results on large scale data sets demonstrate that Gibbs max-margin topic models can significantly improve the classification performance while re-quire comparable time as the unsupervised topic models.
Due to the restriction of computational resources, our ex-periments have been carried out on a relatively small cluster with tens of machines. In the future, we plan to carry out careful investigations on large clusters (e.g., with thousand-s of machines) with massive corpora consisting of tens of thousands of categories and millions of data points, as com-monly encountered in PASCAL and ImageNet challenges. Finally, the data augmentation techniques are general and can be applied to improve the inference accuracy of other topic models or latent variable models in general, such as relational topic models [8] for network analysis and matrix factorization [24] for collaborative filtering. This work is supported by National Key Foundation R&amp;D Projects (No.s 2013CB329403, 2012CB316301), Tsinghua Initiative Scientific Research Program No.20121088071, and the 221 Basic Research Plan for Young Faculties at Tsinghua University. [1] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, [2] R. K. Ando and T. Zhang. A framework for learning [3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex [4] D. Blei and J. McAuliffe. Supervised topic models. In [5] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet [6] O. Catoni. PAC-Bayesian supervised classification: [7] J. Chang and D. Blei. Relational topic models for [8] N. Chen, J. Zhu, F. Xia, and B. Zhang. Generalized [9] L. Devroye. Non-uniform random variate generation . [10] P. Germain, A. Lacasse, F. Laviolette, and [11] T. Griffiths and M. Steyvers. Finding scientific topics. [12] Q. Jiang, J. Zhu, M. Sun, and E. Xing. Monte Carlo [13] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A [14] D. McAllester. PAC-Bayesian stochastic model [15] J. Michael, W. Schucany, and R. Haas. Generating [16] D. Newman, A. Asuncion, P. Smyth, and M. Welling. [17] N. Polson and S. Scott. Data augmentation for [18] R. Rifkin and A. Klautau. In defense of one-vs-all [19] A. Smola and S. Narayanamurthy. An architecture for [20] M. Tanner and W.-H. Wong. The calculation of [21] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining [22] D. van Dyk and X. Meng. The art of data [23] Y. Wang and G. Mori. Max-margin latent Dirichlet [24] M. Xu, J. Zhu, and B. Zhang. Fast max-margin matrix [25] S. Yang, J. Bian, and H. Zha. Hybrid [26] L. Yao, D. Mimno, and A. McCallum. Efficient [27] J. Zhu, A. Ahmed, and E. Xing. MedLDA: maximum [28] J. Zhu, N. Chen, H. Perkins, and B. Zhang. Gibbs [29] J. Zhu, N. Chen, and E. Xing. Infinite latent SVM for
