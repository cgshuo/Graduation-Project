 Search engines make use of inverted list caching in RAM and dy-namic pruning schemes to reduce query evaluation times. While only a small portion of lists are processed with dynamic pruning, current systems still store the entire inverted list in cache. In this paper we investigate caching only the pieces of the inverted lists that are actually used to answer a query during dynamic pruning. We examine an LRU cache model, and two recently proposed mod-els. We also introduce a new dynamic pruning scheme for impact-ordered inverted lists.

Using two large web collections and corresponding query logs we show that, using an LRU cache, our new pruning scheme re-duces the number of disk accesses during query processing time by 7% X 15% over the state-of-the-art impact-ordered baseline, without reducing answer quality. Surprisingly, however, we discover that using our new pruning scheme makes little difference to disk traffic when the more sophisticated caching schemes are employed. Categories and Subject Descriptors: H.3.4 [Systems and Soft-ware]: Performance evaluation (efficiency and effectiveness) General Terms: Algorithms, performance, experimentation Keywords: Search engines, efficiency, caching, inverted lists
To allow for scalable and fast search over vast collections, search engines make use of an inverted index data structure [17]. An in-verted index comprises of a vocabulary , which holds the distinct terms that occur in the collection; and inverted lists , where for each term in the vocabulary a posting for every document that contains that term is stored. Inverted (postings) list for small collections may fit in memory, however for larger collections some or all of the lists must reside on disk.

To evaluate ranked queries, the inverted list of each queried term is fetched and each document in that list is assigned a similarity score to the query. Documents with the highest cumulative score from all queried terms are then presented to the user. For large col-lections, evaluating all postings in each query term X  X  list is not only resource demanding, but may hinder simultaneous query evaluation for machines processing queries from multiple sources.

A common technique to reduce the number of documents pro-cessed is index pruning , where inverted lists are organized so that documents with a high probability of being returned as relevant are stored at the front of the list, and only the initial parts of the inverted lists are processed [14]. In static pruning schemes, the number of postings processed is decided independently of the query being evaluated [3, 5, 6]. In dynamic pruning , the number of postings processed is dependent on the query [7, 14].

Anh and Moffat propose impact-ordered inverted lists, and a dy-namic pruning scheme [2]. In this paper we extend Anh and Mof-fat X  X  work, adding a new dynami c pruning scheme. In addition, we investigate caching schemes for inverted lists in search engines. Previous work on caching inverted lists has focused on caching the entire list, or caching of intersections of lists. We experiment with a new scheme that caches only the parts of lists that are actually pro-cessed during query evaluation using aggressive dynamic pruning. We examine the new scheme using the traditional Least Recently Used (LRU) cache eviction strategy, and also using recently pro-posed cost-aware cache eviction policies [4, 7].

Using two large web collections and corresponding logs of one million queries we demonstrate that more than half of the terms in queries can be resolved from cache using unpruned inverted lists. Using pruned inverted lists a further 8% more terms are resolved from cache, that is, a 16% relative improvement. We also demon-strate that recent cost-based cache eviction policies consistently outperform the usual LRU eviction strategy.
In impact-ordered inverted lists [1, 8], postings are stored in sorted blocks of decreasing term-document impact score .Theterm-document impact score is a measure of how well the term conveys the content of the document. Each block of postings has the same impact value, and so, during query processing, the blocks are pro-cessed in an interleaved fashion, where blocks with the highest im-pact scores from any query terms are processed first.

Recently, Anh and Moffat proposed a new scoring scheme tai-lored for impact-ordered inverted indexes [2] loosely based on the known continue and quit methods [13]. Dubbed score-at-a-time, a query is evaluated in four phases: or , and , refine ,and ignore .They then further improve this to get Method B, by removing the refine phase, and only processing 30% of remaining postings in the and phase. Experiments show that by limiting the number of postings processed in the and phase to 30%, results of the same quality as the four-phase process can be achieved.
Prior to impacts, other authors proposed index organizations schemes for the purpose of index pruning. Persin et al. [14], proposed fre-quency ordered inverted lists while Garcia [7] introduces Access-ordered indexes. We focus on improving impact ordered inverted lists with dynamic, Method B pruning, as that combination has been shown to be both efficient and effective [2].
Several papers examine improving search engine throughput by caching inverted lists [7, 11, 15]. With the exception of [15], these approaches either cache full inverted lists, or cache intersected in-verted lists on disk. Saraiva et al. apply pruning to their inverted lists prior to caching, which is similar to our ideas; however we investigate impact pruning and eviction policies other than LRU.
Central to the success of any caching strategy is the effectiveness of its eviction policy. Most search engine caches mentioned in pre-vious work make use of the recency of a cache entry to determine eviction. Alternate approaches consider the size of an entry, the cost of fetching it from disk, the cost of maintaining it in cache, and its access frequency. Garcia [7], proposes a cache eviction scheme in which each item is given a cost value (CPB-L). Once cache is full, items that incur the least amount of cost to fetch are evicted. The cost associated with a cache entry reflects the cost to the search engine to fetch that inverted list from disk. To ensure costly items do not remain in cache forever, the time since last access is included in the cost. The cost C i of an item i using CPB-L is calculated as, Cost i = from disk, t is how long i has been in cache, and s ( i ) in bytes.

However, with this costing method, two items that have an iden-tical cost and both last accessed at the same time will be assigned an equal score, even if one was accessed many more times then the other. The Greedy-Dual size [4] (GDS-F) scheme is a cache evic-tion policy proposed for web proxies that incorporates frequency of access into the cost formula. The original GDS method, as used by the web proxy cache community, calculates the cost of an item as Cost i = been accessed, and L is the recency component. Quantity L is ini-tially set to 0 and after every eviction L = Cost j ,where j is the item removed last.
Method B applies a rigid stoppi ng conditio n so that at least 30% of the total number of documents are processed for every query. Ideally we would like to stop processing when no unprocessed doc-ument can change R , the set of top N most relevant documents. That is, for any document d in the lists of the query terms not yet processed, d should neither be able to make it into the set R , nor update an existing document d  X  R in such a way that the rank of d in R changes. Determining such a complex condition requires prior knowledge of what documents are in each list, and their impact scores, which can only known if lists are processed entirely.
We propose replacing the and phase of Method B with the Grad-ual Stop heuristic, where, every time  X  documents are processed the stability of the set R is checked. The set R is considered stable if, after evaluating  X  documents, the content and ordering within R is not altered, in which case query evaluation can stop. Parame-ter  X  is tuned experimentally for a given collection. The rationale behind Gradual Stop is that, if a large number of documents with high impacts are processed and R does not change, it is unlikely that processing a large number of smaller impact documents will change R . Query processing is also halted once the original 30% limit of Method B is reached.

With our more aggressive pruning strategy, it seems wasteful to store entire inverted lists in cache when a large proportion of them will not be used. Instead we cache blocks of the lists that have equal impact scores. During query evaluation, the search engine first consults the cache to determine if the required block of the inverted list is in memory. Blocks not found in cache are fetched from disk. If the cache is full, items are evicted to make space for new items using one of the three eviction policies discussed in Section 2.2. Though removing cache objects until there is is just enough space to fit the new entry would give the best cache hit performance, once the cache is full, the cost of eviction is added to the cost of every cache miss. A pragmatic solution is to evict a fixed portion or percentage of the cache size every time the cache is filled. We evict the lowest scoring 30% of cache items at once.
To test our methods we made use of the GOV2 , 426 GB containing approximately 25 million web documents from the .gov domain, and WT100g , 100 GB collection contai ning around 18 million web documents, TREC collections [9]. We used two large query logs that correspond to the above collections: the msn and the excite-99 query logs. The msn log contains two million queries submitted to the msn.com search engine between 01-August-2005 and 07-November-2005. The queries had at least one document from the .gov domain in the top three results and the user has clicked on one or more of those documents. The excite-99 query log has about one million queries, and is described elsewhere [16]. The top 20 documents for each query were returned using the Zettair search engine ( http://www.seg.rmit.edu.au/zettair/ ).

We simulated the caching process, reporting term hit rate ,which is the proportion of query terms that required no disk accesses dur-ing query processing, and byte hit rate , which is the ratio of inverted list bytes served from the cache to total bytes required.
Terminating inverted list processing prior to evaluating all post-ings may lead to relevant documents not being retrieved. The effec-tiveness of retrieval is usually measured with precision and recall using a set of human derived relevance judgments, so we may ex-pect a drop in precision and recall for our Gradual Stop method. It is very difficult for us to obtain human relevance judgements for 3 million queries on 500 GB of data, however. Instead, documents returned using the full index are taken as the ground truth, and we compare our pruning system to these results using the following three similarity measures.

Firstly, SymDiff ( R 1 , R 2 ) is the symmetric difference between the two lists R 1 and R 2 , which counts how many items the lists do not share in common. Secondly, we use Kendall X  X  Tau, K measure the difference between two lists [5]. Though K well in measuring similarity between two lists, it does not incorpo-rate the rankings of items when it scores a permutation pair. Since search engine users tend to be interested in the first few results of a list [10], lists where relevant documents are out of order but close to the start of the list should bear less penalty than a list whose rel-evant documents are out of order yet away from start that the user is less likely to see them. To this end, we use the dissim measure introduced by Moffat et al. [12]. Given results sets lists R and R 2 each with k entries, the dissimilarity score dissim is computed as the sum of the reciprocal rank difference between matching documents that occur in both lists.

All scores attained from the above measures are normalised so that they are between 0  X  that the lists being compared are dis-joint/different to 1  X  lists are identical in content and/or order. Figure 1: The affect of varying  X  : mean values over 5,000 msn queries on the GOV2 collection. Similarity is in comparison to results obtained from a full index, while size represents the amount of bytes used as a fraction of the bytes required by a full index.
 Table 1: The number of bytes decoded and postings evaluated when processing one million msn queries.
In order to establish the  X  parameter used by the Gradual Stop method we experimented with values relative to the size of inverted lists being processed starting from the upper limit of 30%, where the gradual stopping c ondition is never checked, down to checking every 2% of the number of postings remaining at the start of the and phase. We randomly sampled 5000 queries from the msn query log and ran them on GOV2 collection. The number of bytes saved, and the similarity with using the full index, is shown in Figure 1. As shown, there is no noticeable effectiveness loss until  X  is 4%. Based on these observations we chose  X  as 6% in all experiments, which preserves effectiveness, and reduces the size of entries processed.
To ensure this parameter value is not obtained as a result of over-fitting for a single query set or a si ngle collection, we sampled 5000 queries from the excite-99 query log and ran them on the WT100g collection. Similar results were obtained, with noticeable effective-ness loss appearing after the 6% mark, but not before.
 We first look at the effectiveness loss incurred by using Gradual Stop compared to Method B and the full index, then turn to caching evaluations.

The number of bytes and the number of postings processed is shown in Table 1. As expected Method B processes around 41% of the number of bytes required by the Full evaluation method and 38% of postings. Using Gradual Stop, a further 7% of processing is saved. This not only translates to less disk traffic, but, during query evaluation, on average 30,000 X 100,000 fewer postings per list have to be decompressed and evaluated. Furthermore, 50% X 57% of the evaluated queries terminated using Gradual Stop only before the 30% limit of Method B was reached. Though Method B has shown that the number of postings processed can be significantly reduced, the above results demonstrate that using a query specific stopping condition reduces the size of inverted lists processing even further.
To measure the effectiveness loss incurred by using Gradual Stop, we compared the top 20 results obtained from each query, using the two pruning methods Method B and Gradual Stop, against those of Table 2: Average effectiveness (standard deviation) of Gradual Stop and Method B compared with the Full index over 5,000 queries.
 Table 3: Term hits and byte hits as a percentage of all accesses using a 512 MB cache for one million msn queries. The values in brackets are the term hit rates if the log-wide query term frequency was used to calculate its cost for eviction. the full index using the three normalized effectiveness metrics dis-sim , K  X  and SymDiff . The results are summarized in Table 2, which shows that the effectiveness loss incurred by Gradual Stop is statis-tically significantly (hardly surprising given n = 5000) more than Method B (t-test, p &lt; 0 . 01), but the effect size is small (Cohen X  X  d &lt; 0 . 15.)
Table 3 summarizes the caching performance on the two query logs and collections for a 512MB cache. Focusing on the term hits, more than a third (37.2%) of queried terms were resolved from cache using the full index and a basic LRU eviction policy. The good performance can be attributed to the constant presence of the frequent terms in cache. Over half of the most popular terms in both logs were found in cache, when the content of cache was investigated at two points, during the simulation, after processing 500,000 queries and at the end of the cache simulation. The miss-ing half constitute terms which occurred 100-200 times in query logs, however with a large average recurrence rate.

Using the slightly more intuitive cache eviction policies, a fur-ther 10% of queried terms were resolved from cache (column X  X  3 and 4 are at least 10 higher than column 2 for term hits.) A charac-teristic of the costing scheme used by CPB-L and GDS-F has pref-erence for small objects. As a result these schemes tend to populate cache with a large number of frequently accessed small inverted lists, therefore yielding better cache hit rate. Unfortunately, this also lowers the byte hit rate of these two eviction policies.
The results also show that using pruned lists yields better cache performance than unpruned lists. Gradual Stop consistently outper-forms Method B and the Full index using the LRU eviction policy. However, the difference in hit rate between pruned and unpruned lists quickly disappears when using the more sophisticated CPB-L and GDS-F eviction policies. This is unexpected, as the reduction in size of cached objects, combined with more efficient eviction policies, was expected to provide improved cache hit rates. There are two main factors contributing to this phenomenon. The first is that the cost-based cache eviction policies keep small, popular ob-jects in cache. As shown in Table 4, the number of terms in a GDS-Figure 2: Cache term hit rate using Gradual Stop pruning and GDS-F eviction policy. Inf(inite) refers to the cache hit rate where the cache can hold the whole index; and Inf-full shows the cache hit rate if a Full index was used with such a cache. F cache for the full index and the pruned indexes are very close. When the LRU method is used, however, the number of items in the cache for both schemes drop dramatically. The cache size is fixed at 512 MB, so the items in the LRU cache must be bigger, on average, than for the GDS-F cache.

The second reason is a quirk of dynamic index pruning. If part of a list is in cache and a longer portion of that list is required to evaluate the current query, fetching the additional portion causes a cache miss as it will incur a disk seek and read. Additional postings required could be as few as a single posting or as many as a few blocks. One way of ge tting around this problem is to try to decide, once such a condition is met, whether is it worthwhile going to disk  X  that is, whether fetching additional blocks is going to improve the set R . This can only be done by se tting an upper limit of how many postings or blocks that can be decoded from each individual list. Our experiments with such an approach (not reported here) have shown that, unless the upper size processed is close to the size of full list, there can be no guarantee of the quality of results.
In the costing schemes reported thus far, a cache entry X  X  cost is determined based on its access recency and frequency of access during its presence in cache. Once it is removed from cache all of the access statistics collected are lost. To ensure that terms which occur frequently but in widely separated clusters are not unfairly disadvantaged, we measured the cache performance where the cost is determined based on log-wide term frequency. As shown by the bracketed figures in Table 3 the use of global term frequency count does improve term hit rate. However to maintain such information, term statistics need to persist in cache, even after the eviction of its inverted list. The trade-off between the additional memory required to maintain persisting terms in memory and the gains brought by the log-wide term frequency may not be worthwhile.

Figure 2 shows the term hit rate if larger caches are used. As expected, increasing the cache size does increase the cache hit rate. Using merely 1 GB of memory results in over 70% of terms being resolved from cache; increasing the memory used to 2 GB results in over 90% of queried terms are resolved from cache. With an in-finite amount of memory the Gradual Stop pruned index gains only a 3-7% increase in term hit rate compared to using 2 GB cache. Caching the full index using infinite memory provides slightly higher term hit rates than using a pruned index, because full lists do not incur more than one disk access on a list, whereas pruned lists may require several accesses to blocks in the list as more of the list is required throughout processing.
In this paper we have proposed a new dynamic pruning scheme for impact-ordered inverted lists, and investigated the interaction of dynamic pruning and inverted list caching. The pruning meth-ods that we used ensure effectiveness that is close to that provided by a full, unpruned index. In addition we made use of cost-aware caching eviction strategies that use attributes besides the recency of a cache element to determine what item is to be evicted. Our results show that caching pruned inverted lists is consistently superior to caching unpruned lists.

Note our experiments ignore any on-board disk caching, or op-erating system buffering, which would increase the performance of our cache.

Finally, though we have performed our experiments assuming a single machine is running the querying process, the proposals could easily be applied in an environment where the index is distributed across several machines. Cachin g and pruning are applied within each machine exploiting the characteristics specific to the inverted list in that machine.
Thanks to Microsoft for providing the msn query log. This re-search was supported by the Australian Research council.
