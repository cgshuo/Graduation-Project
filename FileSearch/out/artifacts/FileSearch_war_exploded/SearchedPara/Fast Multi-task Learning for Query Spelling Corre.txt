 In this paper, we explore the use of a novel online multi-task learning framework for the task of search query spelling cor-rection. In our procedure, correction candidates are initially generated by a ranker-based system and then re-ranked by our multi-task learning algorithm. With the proposed multi-task learning method, we are able to effectively transfer in-formation from different and highly biased training datasets, for improving spelling correction on all datasets. Our ex-periments are conducted on t hree query spelling correc-tion datasets including the well-known TREC benchmark dataset. The experimental results demonstrate that our pro-posed method considerably outperforms the existing base-line systems in terms of accuracy. Importantly, the proposed method is about one order of magnitude faster than baseline systems in terms of training speed. Compared to the com-monly used online learning methods which typically require more than (e.g.,) 60 training passes, our proposed method is able to closely reach the empirical optimum in about 5 passes.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Query Alteration Algorithms, Performance, Experimentation Query Spelling Correction, Multi-task Learning
Search queries present a particular challenge for tradi-tional spelling correction methods, for at least three rea-sons [3]. Firstly, spelling errors are more common in search queries than in regular written texts. For example, [12] showed that roughly 10-15% of queries contain misspelled terms. Secondly, most search queries consist of only a few key words rather than grammatical sentences, making a grammar-based approach inappropriate. Most importantly, many queries contain search terms, such as proper nouns and names, which are not well established in the language. For example, Chen et al. [10] reported that 16.5% of valid search terms do not occur in their spelling lexicon, which contains more than 200,000 entries.

Due to the practical importance, query spelling correc-tion has received much attention and a variety of datasets have been developed (from different domains). As discussed in [17], queries in one specific dataset can be very biased from another dataset, and combining such biased datasets for query spelling correction is difficult. In fact, in this pa-per, we will also demonstrate an empirical phenomenon that simply merging biased datasets for training a unified speller may not bring improvement at all.

The difficulty in effectively combining biased datasets for spelling correction lies in at least three aspects: (i) their error-patterns can be high biased; (ii) their distributions of misspelled queries can be quite different; (iii) their domains can be quite different (hence, a domain adaptation prob-lem exists). Thus, a natural question arises: how can we effectively integrate such highly biased datasets for improv-ing query spelling correction? To the best of our knowledge, there are still no satisfactory solutions in the literature.
Our goal in this paper is to solve this well-known diffi-cult problem with high accuracy as well as high efficiency. The training speed can be crucial because industrial query spelling correction tasks may have to deal with very large-scale training data (e.g., click-through query spelling logs which contain more than millions of entries, see [34]). Our proposed multi-task learning framework for query spelling correction is capable of adaptively integrating highly bi-ased training datasets via automatically learning the task-relationships (data-similarities). This allows us to  X  X oftly X  merge biased datasets for learning a unified speller.
In our procedure, the fist step is to generate a large number of spelling correction candidates using a candidate generation module. Then the top candidates are accurately identified through our novel multi-task learning algorithm. Encouragingly, with the ability to effectively transfer infor-mation among those biased training datasets, our proposed method is capable of improving spelling correction accura-cies on all related datasets. The prior standard practice for combining different datasets is to simply merge them and feed them to a single-task learner. Compared to this baseline, our experimental results will demonstrate that our new approach will result in significantly better performance.
Our major contributions can be summarized as follows:
The rest of this paper is structured as follows. Section 2 describes the architecture of our spelling correction system. Section 3 presents the proposed multi-task learning frame-work for query spelling correction. Section 4 presents the ex-tensive experiments on query spelling correction. Section 5 reviews related work, and Section 6 concludes the paper.
We first describe our procedure for generating correction candidates. Following Ganjisaffar et al. [17], we implement three candidate generators in our system to produce about 2,000 candidates (on average) for each query. Initially, a character-based candidate generator is adopted for produc-ing all possible candidates within an edit distance 1. It con-siders replacing each character with all possible characters in the alphabet, transposing each pair of adjacent characters, deleting each character, inserting all possible characters af-ter each character, and so on.

The AOL query logs showed that 16% of the query corrections are different from the original query only in adding/removing spaces [17]. For example,  X  X bayauction X  is a query which should be corrected to  X  X bay auction X . As a more complicated example, the long query  X  X roccoliand-cheesebake X  actually indicates  X  X roccoli and cheese bake X . In order to handle this class of queries, we implement the word segmentation algorithm based on the Microsoft word breaker 1 , with some minor modifications. For each possible segmentation of the character sequence, we use a language model to compute the probability of the segmentation. The most probable segmentation candidates are then added to the candidate list.

Neither of the above two candidate generators is able to produce candidates which have more than 1 edit distance from the original query (in spite of adding or removing mul-tiple spaces). For example, for the query  X  X ashton univer-sity X , we might want to have  X  X ashington university X  as a candidate, which however can not be produced by the above two candidate generators. To ease this problem, we perform a fuzzy search process based on a lexicon containing most frequent words, to quickly find known unigrams with a small http://web-ngram.research.microsoft.com edit distance to unigrams and bigrams located in the origi-nal query. For the lexicon containing frequent-word, we use the top 100K words based on their frequency on the web. With the three candidate generators, we can generate 2,000 candidates (on average) for each query.
An important factor in selecting and ranking the cor-rection candidates is the prior probability of a correction phrase. It represents our prior belief about how likely a query will be chosen by the user without seeing any input from the user. In this work we make use of the Web n-gram service provided by Microsoft. Web n-gram model intends to model the n-gram probability of English phrases with the parameters estimated from the entire Web data. It also differentiates the data sources to build different language models from the title, anchor text and body of Web pages, as well as the queries from query log. To build our spelling system, we make use of the tri-gram language model. Note that, even though the Web n-gram model contains a large amount of entries, it may still suffer from data sparseness in higher-order language models.
Re-ranking of top search results has shown to improve the quality of rankings in query spelling correction [17, 28]. Since the main focus of spelling correction is on the top ranked list of candidates, we add a re-ranker on top of the naive ranker. This step significantly improved the results of the naive ranker, and the cost is tractable. This is because re-ranking only needs to deal with top-k candidates, and k is typically small (in our case, following [28], we set k = 40). Hence, the re-ranker is solving a much easier problem, com-pared to the original naive ranker which needs to consider about 2,000 candidates for each query.

We use a conditional log-linear method, the Maximum En-tropy model, for re-ranking. The maximum entropy model produces a probability distribution over multiple classes and have the advantage of handling large numbers of overlap-ping features. Assuming a feature function that maps a pair of observation sequence x x x and a classification label y to a feature vector f f f , the probability of y conditioned on the observation sequence x x x is modeled as follows [23]: where w w w is a weight vector. Given a training set consisting of n labeled samples, ( x x x i ,y i ), for i =1 ...n , weights are gained via maximizing the objective function, The first term of this equation represents a conditional log-likelihood of the training data. The second term is a reg-ularizer for reducing overfitting. We employe an L 2 prior. In what follows, we denote the conditional log-likelihood of
Since the evaluation is based on expected F-score, proba-bility information is required in computing expected F-score. The original conditional probability produced by maximum entropy models is not proper for computing expected F-score, because the sum of conditional probabilities of top-k candidates are not 1. To deal with this problem, we re-compute the probability of a candidate as follows: where c is a scalar to control the density of the distribution. Since essentially c has a similar function like the  X  in reg-ularization, to avoid introducing new hyper-parameters, we simply set c based on  X  .
In the re-ranking phase we add features which are ex-tracted from top-k query-candidate pairs. In addition, other than the features which based on transformations between query-candidate pairs, we also design features which con-sider the top candidates of the query (i.e., comparisons with other candidates in the top-k list). In this way, the fea-tures can include valuable information that may help in the ranking process.

We have about 30 feature templates for each query-candidate pair. These features include: error model fea-tures (e.g., edit distance), candidate language model fea-tures, query language model features, surface features cap-turing differences of the query and the candidate, frequency of the query and the candidate, the rank of the candidate in the naive ranking phase, and so on. Although we have used language model scores for naive ranking for producing top candidates, we keep using Web-scale n-gram language model features for re-ranking the top candidates, and we find such Web-scale n-gram language model features are still quite useful in the re-ranking phase.

In the Web-scale n-gram language model features, the log of n-gram language model probabilities of an original query and its candidate corrections are used for re-ranking. In addition, the average, max, min, and standard deviation of the language model probabilities of the top-k candidates are employed as features.
There are two major approaches for training a log-linear model: batch training and online training. Batch train-ing methods include, for example, steepest gradient de-scent, conjugate gradient descent (CG), and limited-memory BFGS (LBFGS) [30]. In such training methods, gradients are computed by using all training instances. Typically, the training process is quite slow in practice.

To speed up the training process, online algorithms have become increasingly popular. A representative online learn-ing method is the stochastic gradient descent (SGD) [7]. The SGD uses a small randomly-drawn subset of the training samples to approximate the gradient of the objective func-tion, which allows one to update the model weights much more frequently, and consequently, to speed up the conver-gence. Suppose  X  S is a randomly drawn subset of the full training set S , the stochastic objective function is then given by The extreme case is a batch size of 1, and it gives the maxi-mum frequency of updates, which we adopt in this work. In this case, |  X  S| =1and |S| = n (suppose the full training set contains n samples). In this case, we have where  X  S = { i } . Model weights are updated like this: where k istheupdatecounter,  X  k is the learning rate.
In this section, we introduce our adaptive (and online) m ulti-t ask l earning framework (MTL below). For every positive integer q , we define N q = { 1 ,...,q } .Let T be the number of tasks which we want to simultaneously learn. For each task t  X  X  T ,thereare n data examples { ( x x x t,i ,y y y N n } available. In practice, the number of examples per task may vary but we keep it constant for simplicity of notation. We use D D D to denote the n  X  T matrix whose t -th column is given by the vector d d d t of data examples. Our goal is to learn the weight vectors w w w 1 ,...,w w w the data D D D . For simplicity of notation, we assume that each of the weight vectors is of the same size f (this is also the feature dimension), and corresponds to the same ordering of features. We use W W W to denote the f  X  T matrix whose t -th column is given by the vector w w w t .Welearn W W W by maximizing the objective function, where Likelihood( W W W,D D D ) is the accumulative likelihood over all interactive tasks, namely, and L ( w w w t ,D D D )isdefinedasfollows:  X  t,t is a real-valued task-similarity ,with  X  t,t =  X  t ,t metric). Intuitively, a task-similarity  X  t,t measures the sim-ilarity of patterns between the t -th task and the t -th task. L ( w w w t ,d d d t )isdefinedasfollows: where P (  X  ) is a prescribed probability function. We can flexibly use any prescribed probability function. This makes our MTL method a flexible and general framework for no matter structured or non-struc tured classification tasks. In this paper, we will adopt the maximum entropy probability function (Eq. 1), which works well for spelling correction.
Finally, R ( W W W ) is a regularization term for dealing with overfitting. In this paper, we simply use L 2 regularization: MTL with fixed task-similarities ( MTL-F ) MTL with unknown task-similarities ( MTL ) Figure 1: MTL algorithms (using batch size of 1). The derivation of 1 n before the regularization term was explained in Eq. 4. Lower-bounding v v v i with  X  is for stability consideration in the online setting. To summarize, the overall objective function is as follows: To simplify denotations, we introduce a T  X  T matrix A A A , such that A A A t,t  X  t,t . We also introduce a T  X  T functional function can be compactly expressed as follows: In the following content, we will first discuss a simple case that the task-similarity matrix A A A is fixed. After that, we will focus on the case that A A A is unknown, because the task-relationships are unknown among the three query spelling correction tasks that we will focus on.
Although the task-similarities are unknown for query spelling correction, we will present a learning algorithm that iteratively reduce the problem to a case of fixed task-similarities. Hence, it is important to discuss the case of fixed task-similarities. With fixed task-similarities, the op-timization problem is as follows: It is clear to see that we can independently optimize w w w w w w t ( t = t ) given fixed task-similarities. Hence, we can independently optimize each column of W W W and derive W W W where  X  ( w w w t ,D D D ) has the form as follows: For high convergence speed, an important issue of MTL-F is to effectively and efficiently approximate the Hessian matrix. Following the work of [20] on single-task learning, we present a simple yet effective method to approximate the eigenvalues of the Jacobian matrix of a fixed point iterative mapping. In MTL-F, the update formula is as follows: The update term g g g t is derived by weighted sampling over different tasks. The weighted sampling is based on fixed task-similarities, A A A  X  . g g g t has a form as follows: where  X   X  t,t = A A A  X  t,t and i t indexes a random sample selected from d d d t . Then, the expectation (over distribution of data) of the update term is as follows: In addition,  X   X   X  t  X  R f + is a positive vector-valued step size and  X   X   X  denotes component-wise (Hadamard) product of two vectors. As presented in [6], the optimal step size is the one that asymptotically approaches to H H H  X  1 t , the inverse Hessian matrix of  X  ( w w w t ,D D D ) in our setting. To avoid actually eval-Following [20], we consider an update iterate as a fixed-point iterative mapping (though a stochastic one) M . Taking par-tial derivative of M with respect to w w w t ,wehave By exploiting this linear relation between Jacobian and Hes-sian, we can obtain approximate eigenvalues of inverse Hes-sian using eigenvalues of Jacobian: In addition, eigen i ( J J J t ) can be asymptotically approximated: When k is sufficiently large,  X  i will be sufficiently close to eigen i ( J J J t ). Therefore, we can asymptotically approximate the inverse of the Hessian matrix via efficient estimation of the Jacobian matrix of fixed-point mapping.

In multi-task setting, this optimization problem is a cost-sensitive optimization problem. To summarize our discus-sion, we present the 2nd-order gradient based MTL algorithm with fixed task-similarities ( MTL-F below) in Figure 1 (up-per). As we can see, the algorithm iteratively approximates the eigenvalues of the inverse Hessian matrix via Eq. 19. This approximation is based on data points that are cost-sensitively sampled from multiple tasks. Then, it updates the vector-valued step size based on the inverse Hessian in-formation, and the multi-task model weights are updated accordingly. This iterative process continues until conver-gence.
For our query spelling correction tasks, the task-similarities are unknown. To solve this problem, we present an algorithm to learn the task-similarities and model weights in an alternating optimization manner. Our alternating learning algorithm with unknown task-similarities, called MTL , is presented in Figure 1 (bottom).

In the MTL learning, the MTL-F algorithm is employed as a subroutine. In the beginning of the MTL, model weights W W W and task-similarities A A A are initialized. W W W is then opti-mized to  X  W W W by using the MTL-F algorithm, based on the fixed A A A . Then, in an alternative way, A A A is updated based on the optimized weights  X  W W W .Afterthat, W W W are optimized based on updated (and fixed) task-similarities. This itera-tive process continues until empirical convergence of A A A and W W W .

In updating task-similarities A A A based on W W W ,anatural idea is to estimate a task-similarity  X  t,t basedonthe similarity between weight vectors, w w w t and w w w t . It is unclear which similarity measure best fits the tasks of query spelling correction. To study this, we propose two candidate simi-larity measures for query spelling correction: Polynomial kernel (Poly) : We can use (normalized) poly-nomial kernel to estimate similarities: where w w w t ,w w w t means inner product between the two vec-tors; d is the degree of the polynomial kernel; || w w w t is the normalizer. C is a real-valued constant for tuning the magnitude of task-similarities. Intuitively, a big value of C will result in  X  X eak multi-tasking X  and a small value of C will make  X  X trong multi-tasking X . For example, when d =1, the normalized kernel has exactly the form 1 C cos  X  ,where  X  is the angle between w w w t and w w w t in the Euclidean space. Correlation (Cor) :Sincethe covariance of task weight vectors is a natural way to estimate inter-task interactions, we consider using covariance information for estimating task-similarities. However, we find directly using a covari-ance matrix (to estimate task-similarities) faces the problem of stability in our online setting. Hence, we use the correla-tion matrix via normalizing the covariance matrix. Table 1: Statistics of the three spelling correction datasets, and the top-1 accuracy of naive ranking (without re-ranking). #Candidates is the number of correction candidates generated for re-ranking. where cov ( w w w t ,w w w t ) is the covariance between w w w std (  X   X   X  )is standard deviation . The MTL learning algorithm can be further accelerated. The naive 2MTL learning algorithm waits for the conver-gence of the model weights W W W (in the MTL-F step) before updating the task-similarities A A A . In practice, we can up-date task-similarities A A A before the convergence of the model weights W W W .

For example, we can update task-similarities A A A after run-ning the MTL-F step over a small number of training passes. We will adopt this accelerated version of the MTL learning for experiments so that the task-similarities will not be re-peatedly updated. In the experiment section, we will com-pare the accelerated MTL method with a variety of strong baseline methods.
We will test the proposed method on three query spelling correction tasks. We will study the MTL method with differ-ent similarity measures for spelling correction, and compare them against a number of strong baselines, including tradi-tional batch and online learning methods. The results have been averaged over 5 runs for random permutations of the training data order, and standard deviations are given. Since the TREC dataset (released by Microsoft Speller Challenge 2011) is a new and well-known benchmark dataset for query spelling correction, we will use this dataset for experiments. The TREC dataset is based on the publicly available TREC queries (2008 Million Query Track). This dataset contains 5,892 queries and corrections annotated by the Speller Challenge organizers. There could be more than one plausible corrections for a query. In this dataset only 5.3% of queries are judged as misspelled. We use the same split of the training and testing data.

To improve the performance of query spelling correction, we use two auxiliary datasets for performing multi-task learning. One auxiliary dataset is the JDB dataset [17] collected from the publicly available AOL query sets, with a total of 12,000 samples, and 2,000 of them (16.7%) are dif-ferent from the original query. The second auxiliary dataset is collected from MSN queries, and for each query there is at most only one correction. In this dataset, about 11% of queries are judged as misspelled. Also, we evenly split the two auxiliary datasets into training and testing data. We show the statistics of the three datasets in Table 1. In the table, we also show the top-1 accuracy (accuracy of the top-1 candidate) of naive ranking using language models (without re-ranking). As we can see, the naive ranking has low accu-racies. This reflects the importance of the re-ranking.
The two auxiliary datasets are biased from the TREC dataset in three aspects. First, the queries in those datasets are from three different domains: TREC, AOL, and MSN domains. Second, the distribution of misspelled queries are very different: 5.3% in TREC dataset, 16.7% in JDB dataset, and 11% in MSN dataset. Finally, the number of correct spelling suggestions are different: For the MSN dataset, for each query there is only one correction. On the other hand, for the TREC and JDB datasets, for each query there can be multiple corrections. Hence, it is expected to be a big challenge to integrate those three highly biased datasets for improving spelling correction.
Four baselines are adopted to make a comparison with the proposed multi-task learning method, including the limited-memory BFGS batch training method [30] with single-task setting (LBFGS-Single), the limited-memory BFGS batch training method with merged setting (LBFGS-Merge), the stochastic gradient descent method with single-task setting (SGD-Single), and the SGD method with merged setting (SGD-Merge). For the single-task setting , it uses only the task X  X  data to train the re-ranker of the speller (i.e., no data from other spelling correction tasks). For the merged setting , it merges all of the training datasets of different tasks to train a unified re-ranker for the speller.

For the multi-task learning method, its hyper-parameters of similarity kernels are tuned in preliminary experiments, and we find using d = 1 worked well. In practice, we break the symmetric setting of C (in similarity kernels) and set different values of C for different tasks. We set  X  =0 . 99 for the proposed MTL method, following the prior work on single-task learning [20]. The proposed method and the four baseline methods use exactly the same features, which is pre-sented before. For the LBFGS method, we use the OWLQN software. The hyper-parame ters of LBFGS were left un-changed from the default settings of the OWLQN software. We use the expected F1 score as the evaluation metric. This is the same evaluation metrics as the Speller Challenge 2011 2 . Following the previous work of [28], top 40 correc-tions are used as the default setting. Using this setting, we can compare our results with the results of [28].
The performance comparisons from 1-pass to 5-pass set-tings are highlighted in Figure 2. In this figure, the MTL represents MTL-Poly; Single represents the SGD-Single baseline. Similarly, Merge represents SGD-Merge .Wecan http://web-ngram.research.microsoft.com/ spellerchallenge/Rules.aspx data order.
 see that the proposed method MTL achieves much better F-score than all of the baseline methods on the 1-pass setting. In a similar way, MTL outperforms all of the baselines on the 2-pass, 3-pass, 4-pass, and 5-pass settings, and most of the differences are statistically significant.

In general, we find SGD-Merge does not have consider-able advantage over the SGD-Single method. This indicates that it is difficult to combine such highly biased datasets for spelling correction, and a simple merge of such biased datasets does not give considerable improvement.
 The experimental results in 5 passes are summarized in Table 2 with more details, including the results of batch training, the training time of different methods, and the comparisons between different similarity kernels. In addi-tion, the results of different methods on their convergence state are shown in Table 3.

As we can see, the batch training (LBFGS) has weak per-formance in 5 passes, compared with SGD and MTL online training methods. The two similarity kernels (polynomial one and correlation one) have very similar performance on the MTL setting. Comparing the 5-pass results of MTL with the baseline results on their convergence, we find the MTL (with 5 passes) outperforms the SGD with 60 passes and LBFGS with 200 passes. In addition, the MTL with 5 passes is quite close to its empirical optimum on conver-gence. Compared with no matter the online or batch base-lines, the MTL method is about one-order magnitude faster in terms of the empirical convergence speed.

In addition, we summarize all F-score curves by changing the number of passes, so that we can check the training process. The curves are shown in Figure 3. We also show the F-score curves based on training time. As we can see, the MTL method can achieve better performance than baselines, with fewer training passes and fewer training time.
Spelling correction for regular written text is a long stand-ing research topic. Previous researches can be roughly grouped into two categories: correcting non-word errors and real-word errors. In non-word error spelling correction, any word that is not found in a pre-compiled lexicon is consid-ered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections.

Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by [11, 22]. During the last two decades, sta-tistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective [21, 37].

Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). Real-word spelling cor-rection tries to detect incorrect usages of a valid word based on its context. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candi-date corrections, then a scoring model, such as a tri-gram language model or naive Bayes classifier, is used to rank the candidates according to their context (e.g., [19, 29]).
When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vo-cabulary (i.e., either a lexicon or a confusion set). How-ever, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent re-search on query spelling correction has focused on exploit-ing noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries.

Cucerzan and Brill [12] discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak [3] propose a method of estimating an error model from query logs using the EM algorithm. Li et al. [25] extend the error model by capturing word-level similarities learned from query logs. Chen et al. [10] sug-gest using web search results to improve spelling correction. Whitelaw et al. [39] present a query speller system in which both the error model and the language model are trained using Web data. Research in this direction also includes more recent work on utilizing large web corpora and query logs [10, 17, 28], employing large-scale n-gram models [17, 28], training phrase-based error model from clickthrough data [34, 18], and so on. Other related work includes [38, 15, 32, 36].
Multi-task learning has been the focus of much interest in machine learning societies over the last decade. Traditional multi-task learning methods include: sharing hidden nodes in neural networks [8]; feature augmentation among inter-active tasks [13]; producing a common prior in hierarchical Bayesian models [42, 43]; sharing parameters or common structures on the learning or predictor space [24, 4]; multi-task feature selection [41]; and matrix regularization based methods [5, 40], among others.

Recent development of multi-task learning is online multi-task learning, started from [14]. [14] assumes the tasks are related by a global loss function and the goal is to reduce the overall loss via online algorithm. With a similar but somewhat different motivation, [1] and [2] studied alternate formulations of online multi-task learning under traditional expert advice models. This is a formulation to exploit low dimensional common representations [16, 31]. Online multi-task learning is also considered via reducing mistake bounds [9], and via perceptron-based online multi-task learning [33]. One of our target is adaptive online multi-task learning. Our adaptive (online) multi-task learning methods not only learn model weights, but also learn task relationships simul-taneously from data [35]. More importantly, the proposed method can effectively estimate 2nd-order information, so that we can achieve very fast convergence of the multi-task learning. Finally, our proposal is a general framework which allows non-structured and structured classification.
In this paper, we proposed an adaptive (and online) multi-task learning method to integrate highly biased datasets for query spelling correction. We performed experiments on three different query spelling correction datasets, including the well-known TREC benchmark dataset. Experimental results demonstrated that the proposed method consider-ably outperformed the existing baseline systems in terms of accuracy.

Importantly, the proposed method was about one-order magnitude faster than baseline systems in terms of training speed. In contrast to the baseline methods which require more than 60 passes in training, the proposed method can approach very close to the empirical optimum in five passes. Since query spelling correction can have very large-scale training data in industrial applications (e.g., query spelling logs which contain more than millions of entries, see [34]), the proposed method X  X  ability to approach empirical opti-mum in 1-pass or a few passes will be of critical importance in such large-scale applications.

The proposed multi-task learning method is a general technique, and it can be easily applied to other classifica-tion tasks. As future work, we plan to apply this method to other large-scale web search and data mining tasks. Also, we will explore the possibility of integrating online learning with modern hashing algorithms [26, 27] to solve extremely large-scale problems. This work is partially supported by NSF (DMS-0808864, SES-1131848), ONR (YIP-N000140910911), and DARPA (FA-8650-11-1-7149). Xu Sun was a Postdoctoral Associate supported by ONR and NSF. Anshumali Shrivastava is a Ph.D. student supported by ONR and NSF. The authors thank Yanen Li for helpful discussions. [1] Abernethy, J., Bartlett, P., and Rakhlin, A.
 [2] Agarwal, A., Rakhlin, A., and Bartlett, P.
 [3] Ahmad, F., and Kondrak, G. Learning a spelling [4] Ando, R. K., and Zhang, T. A framework for [5] Argyriou,A.,Micchelli,C.A.,Pontil,M.,and [6] Benveniste, A., Metivier, M., and Priouret, P. [7] Bottou, L. Online algorithms and stochastic [8] Caruana, R. Multitask learning. Machine Learning [9] Cavallanti, G., Cesa-Bi anchi, N., and Gentile, [10] Chen,Q.,Li,M.,andZhou,M. Improving query [11] Church, K. W., and Gale, W. A. Probability [12] Cucerzan, S., and Brill, E. Spelling correction as [13] Daum  X  e III, H. Frustratingly easy domain adaptation. [14] Dekel, O., Long, P. M., and Singer, Y. Online [15] Duan, H., and Hsu, B.-J. P. Online spelling [16] Evgeniou, T., Micchelli, C. A., and Pontil, M. [17] Ganjisaffar, Y., Zilio, A., Javanmardi, S., [18] Gao, J., Li, X., Micol, D., Quirk, C., and Sun, [19] Golding, A. R., and Roth, D. Applying winnow to [20] Hsu, C.-N., Huang, H.-S., Chang, Y.-M., and [21] Kernighan, M. D., Church, K. W., and Gale, [22] Kukich, K. Techniques for automatically correcting [23] Lafferty, J., McCallum, A., and Pereira, F.
 [24] Lawrence, N. D., and Platt, J. C. Learning to [25] Li, M., Zhu, M., Zhang, Y., and Zhou, M.
 [26] Li, P. and Konig, A.C. Theory and Applications of [28] Li, Y., Duan, H., and Zhai, C. Cloudspeller: [29] Mangu, L., and Brill, E. Automatic rule [30] Nocedal, J., and Wright, S. J. Numerical [31] Rai, P., and III, H. D. Infinite predictor subspace [32] Reynaert, M. Character confusion versus focus [33] Saha, A., Rai, P., Daum  X  e III, H., and [34] Sun, X., Gao, J., Micol, D., and Quirk, C.
 [35] Sun, X., Kashima, H., Tomioka, R., Ueda, N., [36] Sun, X., Shrivastava, A., and Li, P. Query spelling [37] Toutanova, K., and Moore, R. Pronunciation [38] W ang, Z., Xu, G., Li, H., and Zhang, M. Afast [39] Whitelaw, C., Hutchinson, B., Chung, G., and [40] Xue, Y., Dunson, D., and Carin, L. The matrix [41] Yang, H., King, I., and Lyu, M. R. Online learning [42] Yu, K., Tresp, V., and Schwaighofer, A.
 [43] Zhang, J., Ghahramani, Z., and Yang, Y.

