 1. Introduction Block-sorting is an innovative compression mechanism introduced by Burrows and Wheeler (1994) .
Block-sorting compression involves three steps: permuting the input one block at a time through the use of the Burrows X  X heeler transform ( BWT BWT ); applying a recency transformation (usually, but not always, move-to-front, MTF MTF ) to each of the permuted blocks; and then entropy coding the output with a minimum-redundancy (Huffman) or arithmetic coder. Run-length coding transformations are also some-ness of PPM PPM -style mechanisms, and the speed (especially for decoding) of of these alternative compression paradigms, see, for example, Moffat and Turpin (2002) .
Until now, block-sorting implementations have assumed the input message to be a sequence of charac-ters. In this paper we extend the block-sorting mechanism to word-based models. In a word-based model the source message is parsed into words rather than characters, and thereafter all manipulation is on an extended alphabet of thousands of symbols, each of which is defined as a character string in a dictionary constructed as part of the parsing process. The dictionary must also be transmitted as part of the coded message, but overall, fewer bits are likely to be required for the message, since some words appear with considerable frequency, and, once the dictionary is available, can be represented with relatively short codewords.

A number of interesting implementation issues arise with the extension of block-sorting to a word-based model, including that of efficiently calculating an MTF MTF Tarjan (1985) . We also consider other transformations as an alternative to of trees, and a cyclic shuffle of entries, results in improved compression effectiveness compared to pure for reasons that are apparent only in hindsight.
 Section 2 summarizes previous work in the two areas we seek to combine X  X he word-based compression. Section 3 then examines the BWT BWT an input text can be represented as a sequence of spaceless words in which conditioning can be exploited.
Improvements to the basic scheme are then discussed in Section 4, and the improvements evaluated with reference to a set of standard text files drawn from the Calgary and Canterbury corpuses. Section 5 then mechanism. 2. Previous work
Character-based block-sorting has been studied extensively by a number of researchers, including (amongst others) Fenwick (1996a, 1996b) , Arnavut and Magliveras (1997) , Cleary and Teahan (1997) , and exploit the volatile statistical properties so created. An overview of the whole
Section 3.2, and further examples can be found in, for example, Moffat and Turpin (2002) . Seward (2000, 2001) gives details of the algorithmic issues involved, and describes the choices made in the ( Seward &amp; Gailly, 1999 ).
 ley, Sleator, Tarjan, and Wei (1986) , who proposed that a dictionary of words parsed from the text be transformed into MTF MTF numbers and then represented with a static code. Moffat (1989) experimented with those ideas, and showed that for a range of data files the implementation of Moffat, Neal, and Witten (1998) .

In addition, Moffat (1989) investigated first-and second-order word-based models, in which (respec-ling the entropy coding stage. In such a model, common word pairings are exploited, and sequences such as to be independent. In experiments, Moffat (1989) demonstrated that X  X t the cost of considerable additional memory space X  X mproved compression effectiveness could be obtained compared to a zero-order word-based model, with the bulk of the gain accruing from the change from zero-to first-order predictions, Other authors to have made use of word-based models include Horspool and Cormack (1992) , Zobel and Moffat (1995) , and Moffat, Zobel, and Sharman (1997) .

The idea of word-based compression was extended by de Moura, Navarro, Ziviani, and Baeza-Yates (2000) to what they call the spaceless words approach, and in our implementation X  X hich couples the block sorting transformation with word-based modeling X  X e have adopted their style of parsing text, but with a based modeling, but does not give any details of how the necessary structures could be implemented, and reaches no conclusion as to the efficacy of the combination. 3. Four transformations
Our mechanism is based upon four transformations: parsing input text into a sequence of spaceless the BWT BWT to the sequence of integers so produced; applying a recency-rank transformation to the sections describe these transformations. 3.1. Spaceless words
In the word-based approach, the input file is parsed into an alternating sequence of words and non-
Each token is represented by an ordinal integer, assigned in order of first occurrence. In conventional the spaceless-words approach of de Moura et al. (2000) , a single dictionary is used containing both word and non-word tokens, and any non-word tokens that correspond to a single blank character are omitted if they appear between two consecutive word tokens. To avoid ambiguity, if consecutive word tokens genu-zero-length non-word token can be inserted to separate them, to prevent a spurious blank being introduced by the decoder.

The dictionary of strings (sometimes known as the phrasebook) must also be communicated somehow, stream. To achieve the embedding, the composite symbols are numbered starting at 256 rather than 0, and produces as output a single sequence of integers with values between zero and perhaps 100,000 or more.
For example, using  X  X  #  X  X  to represent the blank character, the simple message mary#mary#marry#me,#merry#mary#marry#me. is represented as the integer sequence 109, 97, 114, 121, 256, 109, 97, 114, 114, 121, 32, 109, 101, 44, 32, 109, 101, 114, 114, 121, 256, 257, 258, 46, dimensional stream of 32-bit integers.
 tionary of phrases. When symbols numbers less than 256 are encountered, new phrases are added to the dictionary; whereas symbol numbers greater than 256 refer to already installed phrases. When two consec-gle blank character is re-inserted between them.

Table 1 lists some text files extracted from the Calgary corpus, and the standard and large Canterbury corpora. 1 We focus on text files because of the very nature of word-based compression. In particular, it ing SGML SGML markup ( Harman, 1995 ).
 formed size in thousands of integers; the fraction of the transformed sequence that still corresponds to
ASCII characters (symbol numbers less than 256); and the number of distinct integer values present in ary, and every word is assigned an ordinal number, even those that do not reoccur. On the small files around half of the symbols in the output represent words that have appeared previously, and on the longer files, more than 75% of the emitted tokens are compound. These ratios are especially promising given that each word appearance account for on average 5 X 6 characters in the initial text. 3.2. Block sorting graphic context established by the immediately preceding (or, in some implementations, following) symbols. This mechanism means that symbols immediately after (or preceding) a particular conditioning context of symbols in the input are adjacent in the BWT BWT For example, consider the example string again. After a character-based mmmmmmm#mrrrrrr,.eaaaeeaarryyyryyy###### ple, Moffat and Turpin (2002) .

If the alphabetic ASCII letters are retained to represent themselves, and (for the purposes of the exam-natively presented as mary1marry#me,#merry123.

When applied to this sequence the BWT BWT produces mm#mrrr,eeaayrryy1#1m23.
 In this short example, there are no pairs of repeated words that are clustered by the and made available for subsequent exploitation. 3.3. Recency ranking
The third transformation takes the output from the BWT BWT text of preceding symbols, the number of successor symbols is likely to be considerably restricted. For  X  X  u  X  X  in the BWT BWT output. Similarly, after a word-based corresponds to, for example, the symbol  X  X  President  X  X , is likely to be dominated by the symbol numbers similar words.
 Burrows and Wheeler (1994) suggested the use of the Move-To-Front transformation ( previously applied to unpermuted word tokens by Bentley et al. (1986) .
 ability of a half, and the average information is 1.0 bits per symbol. After an large, the information content per symbol approaches zero.
 In character-based block-sorting a set of 256 source symbols is used, so the items. Moreover, by virtue of the BWT BWT stage, most of the acter-based MTF MTF processing, a simple data structure is perfectly adequate.
 locality is less pronounced. Many words only reappear after hundreds or thousands of other words have intervened, and it is algorithmically indefensible to consider the use of a linear search. Bentley et al. example, Kingston (1990) .
 be directly indexed by symbol number. Each node contains the pointers necessary to manipulate the splay of items in its right subtree within the splay tree. To calculate an smaller timestamps. The complete sequence of steps requires O(log t ) amortized time, where t is the O( t ) time for the same computation. A similar computation allows the reverse transformation X  X rom value to symbol number X  X o also be carried out in O(log t ) amortized operations and time. As will be dem-onstrated shortly, a splay tree allows fast computation of underlying alphabet. 3.4. Entropy coding The final transformation in the four-stage process is entropy coding of the sumed that the sequence of MTF MTF values carries reduced information compared to the original sequence sequence.
 There are two well-known entropy coding mechanisms X  X inimum-redundancy coding (also known as
Huffman coding), and arithmetic coding. Both of these methods are described in most compression texts X  X ee, for example, Moffat and Turpin (2002) . Arithmetic coding is preferable when one symbol has high probability, or when the symbol probabilities are to be calculated adaptively; minimum-redundancy coding is the method of choice for static applications, and when speed considerations are paramount. Both types of coder have been used in this investigation.

To insulate our results from the issues associated with choice of coder, some quantities are described in a source alphabet 1 ... n , then the self-information (in bits) is given by character. That is, the average self-information of a message in bits per symbol is given by
It is also necessary in some coding environments to incur the overhead cost of recording the n values 0 6 f i 6 m , where sum to not more than n + m )is( Moffat &amp; Turpin, 2002 ): and again this quantity might be normalized by either m , the number of symbols in the transformed se-quence, or by the number of characters in the source message. Note that this overhead is applicable whether a semi-static entropy coder or an adaptive entropy coder is used. In the former case, it is the direct cost of transmitting the symbol frequencies so that the decoder can construct the same code; frequencies.
 3.5. Doing the plumbing
Composition of the four transformations allows direct testing of our principal hypothesis X  X hat use of a word-based parsing of input text allows improved BWT BWT claim.

In Table 2 , six different combinations of transformation are evaluated, and the zero-order self-sons to be made. Each column is headed by a string of three bits that represents the use (or not) of, respectively, the word-parsing transformation; the BWT BWT sponds to no word parsing, no BWT BWT , and only the MTF MTF alter the zero-order self-information of a stream, since it only permutes the symbols. That is why  X  X 010 X  X  and  X  X 110 X  X  combinations do not appear in the table X  X hey are identical to  X  X 000 X  X  and  X  X 100 X  X , respectively.

For both characters and words, application of the MTF MTF ler self-information. However, the MTF MTF is certainly beneficial if applied in conjunction with the terms of self-information, the word-parsing step also yields improvements on all of the test files.
The compression rates listed in Table 2 are hypothetical, and in the case of the word-based models, for
For example, when bible.txt is word-parsed the output sequence contains 9195 distinct symbols, each between 0 and n = 13,752 inclusive; and m 971,000 symbols in total. From Eq. (3) the overhead cost of storing the symbol frequencies can be estimated as approximately 112,000 bits, or 0.03 bits per character dropped.)
On the other hand, when processing file cp.html , the overhead cost in the coder of storing the n = 1550 symbol frequencies (of which only 529 are non-zero) might be as large as 0.33 bits per character. For the large files, word-parsing provides a clear benefit, even without any use of the transformation, confirming the results noted by Moffat (1989) . Adding the acter-based compression, the BWT BWT brings the repeated symbols together and provides a basis for the transformation.

Section 5 gives actual compression results when an entropy coder is coupled with the first three trans-tion that, in this application, has unexpected benefits. 4. Improved recency ranking
Fig. 1 shows a different way of computing MTF MTF -like rank values over a large alphabet. A forest of trees is maintained, with tree T i having S i nodes, and with each tree organized by node value in the usual manner rather than by timestamp as was the case in the splay tree in Section 3.3. The nodes within each tree are also forward-and reverse-threaded in insertion order by trees.

Within each tree the nodes are in token order, and the exact rank value assigned to each object in some tree T j is the ordinal position within tree T j of that object, plus the sum of the sizes of trees T object in each intervening tree demoted to the next-highest numbered tree.

The simplest promotion strategy is to move the accessed object from its current tree T
T , and into rank position 1. Under this promotion strategy the rank produced is at most S exact MTF MTF value, and the cost is proportional to O  X  4.1. Full evaluation
The fidelity of the approximation and the running time required to calculate it is governed by the se-cost of accessing an item initially in tree T j is O  X  where t is, as before, the rank generated. ning time of O  X  fast as to be viable.
 The fully-promoting approximate MTF MTF process is referred to as amortized or worst-case behavior can be used. 4.2. Neighbor
An obvious technique to balance effectiveness and efficiency is to only partially promote the item ac-cessed. For example, if the symbol accessed is currently in tree T as the newest item in tree T j 1 , while simultaneously pushing the least recently used object in tree T in a fixed number of the trees in the forest has the same O(log t ) asymptotic cost.
Because each tree is ordered by integer symbol number, promoting an object from T halves its rank if single powers of two are used for the tree sizes S compression effectiveness is comparable to that obtained by fully-promoting approaches. We call this approach the  X  X  X eighbor X  X  strategy, and denote it in the tables and graphs below as second SSEG-N SSEG-N method that promotes by one tree if the accessed item is in T in T j for some j &gt;2. 4.3. Halfway
A more balanced compromise between the slow shuffle of SSEG-N SSEG-N and SSEG-F SSEG-F approaches is to promote each accessed item from its current tree T rank to approximately the square root of what it was prior to being accessed, if single powers of two are used. The least recently accessed symbol in each of the intervening trees is then percolated downward to restore the tree sizes, and so total execution time is O(log SSEG SSEG-F -F , and a practical speedup might result.
 We call this approach the  X  X  X alfway X  X  strategy, and denote it as from zero, this strategy (as does the neighbor heuristic) only allows symbols in tree T and the rank 1 position. Similar variant MTF MTF strategies have also been suggested for character-based
BWT BWT processing, including promotion rules which move the accessed item into the rank 1 position only if it is currently in the rank 2 position, slowing the displacement of dominant symbols out of the rank 1 position ( Balkenhol et al., 1999; Chapin, 2000; Schindler, 1997 ). 4.4. Skipping
The halfway strategy also suggests another, in which the trees between T is promoted into T 0 , the least recent item in T 0 demoted into T either T 2 or T 3 , and so on, following the binary path back down to T being promoted, it is inserted into T 0 , then the oldest symbol in T moved to T 2 , the oldest object in T 2 moved to T 4 , and the oldest symbol in T cyclic update.

Because log 2 S b j /2 c (log 2 S j )/2 when S i =2 i , the total running time for the log two.
 In the experiments described in the next subsection this approach is denoted as the 4.5. Results
Table 3 shows, for each of the eight test files, and each of the recency ranking methods, the zero-order self-information of the ranked sequence, now expressed in terms of bits per symbol, where a symbol is either an ASCII character or a word (or non-word) number in the dictionary.

As can be seen, there is a considerable variation in effectiveness. Overall, the best ranking method is probably SSEG-H SSEG-H , in that when it is not outright superior, it is not far behind the best. Compared to SSEG-H SSEG-H , it appears that promoting symbols all the way to the front ( too dramatic; promoting them by just one tree ( SSEG-N SSEG-N the forest ( SSEG-S SSEG-S ) insufficiently smooth.

Fig. 2 plots compression effectiveness on wsj20 as a function of the cost of the recency transformation for the various mechanisms discussed. The fastest transformation is tree and, at O(log t ) time per update, is asymptotically efficient; the slowest are varying compromises in terms of execution speed, and (measured by the self-information of the streams) do not necessarily trade away compression effectiveness to attain that speed. The best choices are is paramount; SSEG-H SSEG-H , if compression is more important than speed; and between efficiency and effectiveness.

T or T 2 , and by two trees otherwise. For wsj20 the self-information of the ranked stream was slightly lower than shown in Table 3 , at 7:29 bits per symbol, with slightly more time required than
The most interesting aspect of these experiments was that several of the approximate mechanisms con-
The grey bars reflect the probability distribution of symbols for file bible.txt after the word-parsing bars between symbols 128 and 255 arises because the first word number allocated in the spaceless words number (the grey bars for symbols numbered 256 and greater) and frequency of occurrence in the sequence.
 The dotted line in Fig. 3 shows the corresponding probability distribution after the is applied. The rather chaotic distribution of integer values in the smoother X  X nd lower information content X  X equence by the MTF MTF
The solid line in Fig. 3 shows ranks as assessed by SSEG-F SSEG-F arises because of the correlation between word number and rank that was mentioned above. Each cycle of the saw-tooth pattern corresponds to one of the trees in the forest; and because of the symbol-frequency correlation, in each tree low-numbered nodes are preferred to high-numbered nodes. That is, words a minimal-message-length sense, the approximate process generates better symbol ranks than does the pure 5. Entropy coding
The symbol stream produced by the ranking transformation is entropy coded as the final stage of the four-phase mechanism. This section reviews the techniques that have been found to be effective for coding word-oriented BWT BWT techniques. 5.1. Structured arithmetic coding
Early descriptions of BWT BWT -based compression presumed the used of a minimum-redundancy or arithme-pression rate attained should approach, but not exceed, the self-information values in Table 3 .Aswillbe demonstrated shortly, that is indeed the case.

Fenwick (1996b) showed that compression rates in excess of the zero-order self-information can be ob-observation that the stream of integers generated by the BWT BWT correspond to segments in the BWT BWT sequence representing the following symbols of like contexts in the source text. Some of these segments have localized alphabets that are relatively large. Others are very 12 different successors, and of those 12, eleven only appear once. The other 39 appearances of  X  X  Marga-behavior.

On the other hand, the various successors of the token  X  X  Thatcher  X  X  appear together in a later segment abilities without at any stage actually being aware of the words involved, or their meanings.
In a structured arithmetic coder each rank is broken into two components. The first is a selector , which second component of each rank is an offset within the bucket of values corresponding to the selector. In is more conservative, so that accurate statistics are maintained within each bucket.
Another way to understand the benefits of the structured coder is to consider the effect of coding (say) the symbol with rank 100. When the selector that covers the bucket containing rank 100 is used, its prob-rank of 100 can be regarded as evidence that other similar ranks might follow. 5.2. Results
Table 4 summarizes the compression effectiveness of three different entropy coders. Each compresses a sequence of integer values to a bitstream. The SHUFF SHUFF redundancy (Huffman) coder based on the work of Turpin and Moffat (2000) . The other two mechanisms required for, say,  X  X  Margaret  X  X -like symbols and  X  X  Thatcher  X  X -like symbols.
 Program SINT SINT is a structured arithmetic coder derived from buckets, and the buckets are determined by a geometric sequence with radix 1:5. Rounded to integers, this the fifth, and so on.

As expected, both the minimum-redundancy coder SHUFF SHUFF
On the other hand, the structured arithmetic coder undercuts both the self-information and the two uni-occur, and that the changing nature of the probability distribution can again be captured through rapidly changing probability estimates. 5.3. Higher-order probability estimation
Other work has explored the possibility of higher-order correlations between the selector parts of ranked relatively small number of probability estimates being maintained, and results in a very small additional gain in compression effectiveness of approximately 0.01 bits per character ( Isal et al., 2002 ). 5.4. Overall performance
Having constructed a compression system, it is interesting to compare the overall compression effective-ness attained, and the computational and memory resources required to achieve those results, with other compression systems.

Fig. 4 shows, for several different compression tools, the performance achieved on file wsj20 , plotting compression effectiveness in bits per character as a function of the combined CPU time required during encoding and decoding. The systems used as reference points were the 1999 ); and a PPMD PPMD implementation written by the first author.

Three different combinations drawn from the techniques of this paper were also included in the testing: implicit dictionary spaceless words parsing and the BWT BWT phases, followed by MTF+SINT MTF+SINT ; and then the first two phases followed by ing the best compression effectiveness.
 provide faster throughput, but with reduced compression effectiveness.
 Not plotted in Fig. 4 is the PPMZ PPMZ technique of Bloom (1996) . The compression, 2 but is both slow and expensive in terms of its memory requirements. It was not possible to process the whole of wsj20 with PPMZ PPMZ on the 512MB test machine. However, a 5MB prefix of wsj20 took 360 seconds to encode, and consumed approximately 300MB to attain a compression rate of 1.67 bits per character, better than the 1.73 of the SSEG-H+SINT SSEG-H+SINT character.
 Table 5 shows a breakdown of the costs incurred in using the from using the less effective SHUFF SHUFF coder. The two bottleneck operations are the qsort routine. Seward (2000) has considered the problem of block sorting on character inputs, and it may be that the same techniques can be applied to reduce the time required by a word-based
Fig. 2 already explored the cost of the various ranking transformations. With just a single tree to be manipulated, the pure MTF MTF has a distinct throughput advantage compared to the as illustrated in Fig. 4 , is an alternative worth retaining.

The BWT BWT -based schemes presented here have a roughly 60:40 relationship between encoding and decod-
Memory space requirements are also an issue for compression systems, and it is here that the word-based system has a definite advantage compared to other high-performance mechanisms, primarily as a result of the aggregating of characters into larger tokens. For example, a whole-of-wsj20 application of the word-parsing phase requires 22MB of memory, of which 20MB is an input buffer. The ranking transformations there are only 72,000 token numbers assigned by the parsing process, and less than 2MB of memory is re-quired during recency ranking. The entropy coders are just as economical, and even with some sequence buffering included, operate in just a few megabytes of memory.

The only expensive phase is the BWT BWT itself. With approximately 4.7 million symbols in the parsed se-quence ( Table 1 ), a total of 36MB is required while the wsj20 tokens are being permuted. This is rather partially explains why better compression can be attained. On the other hand, the
Fig. 4 built a data structure that occupied 170MB. When that structure was restricted to 40MB, the best compression rate that could achieved X  X sing a fifth order model X  X as 1.75 bits per character (compared to 1.66 bits per character from the seventh order model). As already noted, the than the 512MB available on the test machine, and could not be used to compress file wsj20 . 6. Summary
The implicit dictionary spaceless words preprocessing transformation described in this paper converts a will be able to exploit them.
 In particular, we have chosen to combine the word-parsing transformation with the ing of bigram or trigram frequencies, which means that the memory requirement remains modest compared to (say) explicit context-based methods such as a word-level
We have also explored a variety of approximate ranking methods that offer different tradeoffs between speed of adaptation and computational cost. Using a splay tree, the standard mented in a reasonably efficient manner, even for a large underlying alphabet. Approximate methods that compression gains.
 and 2.14 bits per character. The implementation of Shkarin also has the benefit of appearing to be both significantly faster than PPMZ PPMZ and also more memory-efficient.

Nevertheless, the combination of techniques we have described here provides good compression effec-ration of word-based compression mechanisms.
 Acknowledgement This work was supported by the Australian Research Council and by the QUE Project of the Fasilkom, Universitas Indonesia.
 References
