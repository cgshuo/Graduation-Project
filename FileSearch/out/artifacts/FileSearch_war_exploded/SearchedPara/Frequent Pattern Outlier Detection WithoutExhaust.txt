 Outlier detection consists in detecting anomalous observations from data [ 1 ]. The outlier detection problem has important applications, such as detection of credit card fraud or network intrusions. Recently, outlier detection methods The key idea of such approaches is to consider the number of frequent patterns supported by each data observation. A data observation is unlikely to be an outlier if it supports many frequent patterns since frequent patterns correspond to the  X  X ommon features X  of the dataset. Frequent pattern outlier detection methods first extract all frequent itemsets from the data and then assign an outlier score to each data observation based on the frequent itemsets it contains. These outlier detection methods follow the schema of pattern-based two-step methods.
 Pattern-based two-step methods [ 5 ] aim at exhaustively mining all patterns (first step) in order to build models (second step) like pattern sets (e.g., clas-completeness of pattern mining is often considered as a crucial advantage for con-structing accurate models or measures. However, the completeness of the first step requires to adjust thresholds which is recognized as being very difficult. If the thresholds are too low, the extraction becomes unfeasible. If the thresholds are too high, some essential patterns are missed. Finally, completeness leads to huge pattern volumes without guaranteeing not missing important patterns. For a smaller budget (in time or number of patterns), we claim that non-exhaustive methods can produce collections of patterns better adapted to the task of the second step. Interestingly, a non-exhaustive method can even guarantee a certain quality on the second step.
 (FPOF) based on non-exhaustive methods i.e., by mining a small number of patterns (or zero!). We first propose a method for calculating the exact FPOF of each transaction. Surprisingly, our method is non-enumerative in the sense that no pattern is generated (a fortiori, this is also a non-exhaustive method). For this, we reformulate the FPOF by operating directly on transaction pairs. This method calculates the FPOF in polynomial time on the number of trans-actions and items of the dataset. Experiments show that this method arrives to calculate the exact FPOF where the usual approach fails. We also propose a non-exhaustive approximate method that exploits a pattern sample instead of the complete collection of frequent patterns. Using Bennett X  X  inequality, this method selects the sample size so as to guarantee a maximum error for a given confidence. Experiments show its efficiency with reasonable error. Section 3 introduces the basic definitions about the FPOF and states the prob-lem of its exact and approximate calculation. In Sect. 4 ,weproposeourexact non-enumerative method for calculating FPOF. We introduce our approximate method based on sampling in Sect. 5 . Section 6 provides experimental results. In this paper, we focus on the outlier detection methods based on frequent ing [ 1 ]. Pattern-based methods benefit from the progress of pattern mining made over the past two decades. Such methods have a double interest. On the one hand, they are well suited to handle categorical data unlike most other methods dedicated to numerical data. In addition, they also remain efficient for high-dimensional spaces. The first approach [ 2 ] introduced the frequent pattern out-uses an opposite approach by considering non-frequent itemsets). More recently, [ 4 ] replaces the collection of frequent itemsets by the condensed representation of Non-Derivable Itemsets (NDI) which is more compact and less expensive to mine. We would go further by showing that the frequent pattern outlier factor a small sample.
 methods through pattern sampling [ 8 , 9 ]. Pattern sampling aims at accessing the pattern space L by an efficient sampling procedure simulating a distribution  X  :
L X  [0 , 1] that is defined with respect to some interestingness measure m :  X  ( . )= m ( . ) /Z where Z is a normalizing constant (formal framework and algo-the entire pattern language and with no parameter (except possibly the sample size). Pattern sampling has been introduced to facilitate interactive data explo-an outlier score to each transaction. With a lower (pattern or time) budget than that of an exhaustive method, we obtain a higher quality with a bounded error. 3.1 Basic Definitions Let
I be a set of distinct literals called items , an itemset (or a pattern) is a subset of I . The language of itemsets corresponds to L =2 is a data observation. For instance, Table 1 gives three transactional datasets with 4 or 5 transactions t i described by until 4 items A , B , C and D . Pattern discovery takes advantage of interestingness measures to evaluate the relevancy of a pattern. The support of a pattern X in the dataset portion of transactions covered by X [ 11 ]: supp ( X, D )= A pattern is said to be frequent when its support exceeds a user-specified mini-mal threshold. The set of all frequent patterns for  X  as minimal threshold in is denoted by F  X  ( D ): F  X  ( D )= { X  X  X  : supp ( X, D ) In the following, we manipulate pattern multisets which are collections of patterns admitting several occurrences of the same pattern. The representative-each pattern in P : Supp ( P , D )= X  X  X  supp ( X, D ). The range of Supp ( [0 , |P| ]. Given a cardinality, high representativeness means the multiset contains very common patterns of the dataset. For comparing the content of two pattern multisets, we use the semi-join operator, denoted by P 2 P patterns of P 2 occurring in P 1 : P 2 P 1 = { X  X  X  2 : X { A, AB, A, D } { C, A, B } = { A, A } .
 3.2 Frequent Pattern Outlier Factor Intuitively, a transaction is more representative when it contains many patterns which are very frequent within the dataset. In contrast, an outlier contains only few patterns and these patterns are not very frequent. The frequent pattern outlier factor [ 2 ] formalizes this intuition: Definition 1 (FPOF). The frequent pattern outlier factor of a transaction t in D is defined as follows: representative transaction of the dataset while a value near 0 means that the transaction is an outlier. Other normalizations (denominator) are possible like Supp ( L , D )or t  X  X  Supp (2 t , D ). Whatever the normalization method, two transactions remain ordered in the same way (so it does not affect the Kendall X  X  tau that is used for evaluating our method). Under a certain Markov model, the study the transaction t considering the collection of frequent itemsets [ 12 ]. and, A , B and AB whose support equals to 0 . 75 ( Supp ( while t 4 is only covered by  X  and C ( Supp ( { X  ,C } , D appears to be an outlier. It is easy to see that increasing the frequency of the patterns covering the first transactions (e.g., dataset D t . Similarly, increasing the number of patterns covering the first transactions also decreases the FPOF factor of t 4 (e.g., dataset D ). 3.3 Problem Formulation The outlier detection problem consists in computing the FPOF for each trans-action: Problem 1 (Exact Problem). Given a dataset D , compute the frequent pattern outlier factor of each transaction t  X  X  .
 formed by mining all patterns appearing at least once in the dataset (i.e., with Instead, FPOF is approximated with a collection of frequent patterns i.e., with a higher minimal support threshold: Definition 2 (  X  -Exhaustive FPOF). Given a minimal support threshold  X  , the  X  -exhaustive FPOF of a transaction t in D is defined as follows: The approximation becomes accurate with low support thresholds. Figure 1 (left) plots the Kendall X  X  tau of fpof  X  in comparison with fpof for some bench-marks 1 . When the support threshold becomes very low, the number of patterns and the extraction time explode, see Fig. 1 (right). Furthermore, the approxima-tion error is not estimated. With a smaller budget, we claim that it is possible to approximate more precisely FPOF while having a bound on the error. The Kendall X  X  tau varies significantly depending on the dataset for a same minimal support threshold. This threshold is not easy to set for obtaining a good compromise between efficiency and quality. Therefore, it seems interesting that the user sets the maximum error that he/she tolerates on the result rather than a threshold related to the method: Problem 2 (Approximate Problem). Given a dataset D , two reals  X  and , find a function approximating the frequent pattern outlier factor such that for each transaction t  X  X  , | fpof ( t, D )  X  fpof ( t, D ) | X  with confidence 1 This problem aims at assigning an approximate FPOF to each transaction with the probability 1  X   X  that the error is less than . Problem 1 is a particular case of Problem 2 by fixing =0and  X  =0. This section addresses Problem 1 . To calculate the FPOF of a trans-action t , Definition 1 formulates the problem in terms of frequent pat-dataset D , the FPOF of the first transaction relies on Supp ( which is equal to |{ X  ,A,B,AB,  X  ,A,B,AB,  X  ,A,B,AB,  X  X | / 4. Each subset Algorithm 1. Exact Non-Enumerative Method { X  ,A,B,AB } and { X  X  result from the intersection of patterns covering t those covering another transaction u  X  X  . Thereby, Supp ( |{ the following property: Property 1 (Reformulation). Given a dataset D , the frequent pattern outlier factor can be reformulated as follows for all transaction t Proof. Let D be a dataset. Given u  X  X  , we define  X  ( X, u )=1if X otherwise. For each transaction t  X  X  , we obtain: Injecting this equation into Definition 1 proves that Property 1 is right. FPOF of a transaction is just the sum of its similarity with each of transactions to traditional methods relying on pair-wise distance among data observations. to prove that Problem 1 can be solved in polynomial time, contrary to what was envisaged in the literature: Property 2 (Complexity). The frequent pattern outlier factor of all transactions can be calculated in time O ( |D| 2  X |I| ).
 proposal is the first method to calculate FPOF in polynomial time. Nevertheless, for large datasets with a lot of transactions, this complexity remains high. Then it makes sense to consider an approximate solution obtained much faster. This section addresses Problem 2 by using pattern sampling. First, we propose a method for approximating FPOF from a pattern sample drawn according to frequency. Then we show how to choose the sample size to control the error. 5.1 Pattern Sampling for FPOF to approximate accurately FPOF. The most frequent patterns do not measure the singularity of each transaction that also relies on more specific patterns (whose frequency varies from small to average). Conversely, do not considering frequent patterns would also be a mistake because they contribute significantly to FPOF. A reasonable approach is to select patterns randomly with a probability proportional to their weight in the calculation of FPOF. Typically, in the dataset D of Table 1 , the itemset AB is 3 times more important than itemset C in the calculation of FPOF due to their frequency.
 In recent years pattern sampling techniques have been proposed to randomly draw patterns in proportion to their frequency [ 9 ]. Such approaches are ideal to bring us a well-adapted collection of patterns. Of course, it remains the non-trivial task of approximating FPOF starting from this collection: Definition 3 ( k -Sampling FPOF). Given an integer k&gt; 0 ,a k -sampling frequent pattern outlier factor of a transaction t in D is defined as follows: where S k ( D ) is a sample of k patterns drawn from D according to support: S ( D )  X  supp ( L , D ) .
 It is important to note that | | is used here instead of Supp ( , Definition 1 . As the sampling technique already takes into account the frequency when it draws patterns, it is not necessary to involve the support here. Indeed, the draw is with replacement for the correct approximation of FPOF (without this replacement the most frequent patterns would be disadvantaged). It induces that the same pattern can have multiple occurrences within the sample For the same sample size k and for the same transaction t ,itispossibleto calculate different values of a k -sampling FPOF due to S threshold k , the less the difference between values stemming from two samples is high. Furthermore, the greater the sample size k , the better the approximation: Property 3 (Convergence). Given a dataset D ,a k -sampling FPOF converges to the FPOF for all transaction t  X  X  .
  X 
X  X  X  , lim we obtain that: lim k  X  X  X  |S k ( D ) 2 t | =  X  X  X  2 t supp ( X, injecting this result into Definition 3 , we conclude that Property 3 is right. Algorithm 2. -Approximate Sampling Method far superior to that of the  X  -exhaustive frequent pattern outlier factor as shown in the experimental study (see Sect. 6 ). This speed is accompanied by a good efficiency due to a reasonable complexity of pattern sampling: Property 4 (Complexity). A k -sampling FPOF of all transactions can be calcu-lated in time O ( k  X |I| X |D| ).
 in order to achieve a desired approximation as suggested by Problem 2 . The next section presents an iterative method for fixing this sample size. 5.2 Bounding the Error a k -sampling FPOF satisfying user specified parameters (a maximum error with a given confidence). The idea is to draw a sample and to bound the maximum error of FPOF using a statistical result known as Bennett X  X  inequality. If this error is less than that allowed by the user, the algorithm returns a sampling FPOF based on the current sample. Otherwise, it increases the sample size by drawing more patterns and so on.
 irrespective of the probability distribution. After k independent observations of real-valued random variable r with range [0 , 1], Bennett X  X  inequality ensures that, with confidence 1  X   X  , the true mean of r is at least r are respectively the observed mean and variance of the samples and In our case, the random variable is the average number of patterns within a sample S k  X  supp ( L , D ) that cover the transaction t .Itisdenotedby cov ( t ) and defined as follows: cov S that a k -sampling FPOF factor can be rewritten using cov cov ( t ) / max u  X  X  cov S enables us to bound FPOF: Property 5 (FPOF Bound). Given a dataset D and confidence 1 of a transaction t is bounded as follows: where S k  X  supp ( L , D ), u = arg max v  X  X  cov S ln(1 / X  ) / (3 k ).
 Algorithm 2 returns the approximate FPOF of all transactions by guarantee-until that the maximal error  X  is inferior to the expected bound . Lines 4 X 7 cal-culate the maximal error  X  using Property 5 . If the maximal error is less than , Line 9 returns the k -sampling FPOF with the current sample more pattern is drawn (Line 3).
 As desired by Problem 2 , Algorithm 2 approximates the FPOF: Property 6 (Correctness). Given a dataset D , a confidence 1 Algorithm 2 returns a k -sampling frequent pattern outlier factor of a transaction t in
D approximating FPOF with an error bounded by with confidence 1 This experimental study aims to compare the speed of the non-enumerative method with that of the exact exhaustive method (i.e., 1 / to estimate the error quality of the -approximate sampling method faced to the  X  -exhaustive method. Due to the lack of space, we do not provide new experiments showing the interest of FPOF for detecting outliers as this aspect is already detailed in literature [ 2  X  4 ]. Experiments are conducted on datasets coming from the UCI Machine Learning repository ( archive.ics.uci.edu/ml )and first columns. All experiments are performed on a 2.5 GHz Xeon processor with the Linux operating system and 2 GB of RAM memory. Each reported evaluation measure is the arithmetic mean of 10 repeated measurements (interval confidence are narrow enough to be omitted). 6.1 Exact Non-enumerative Method Table 2 reports the running time required for calculating the exact FPOF using the 1 / |D| -exhaustive and the non-exhaustive methods (respectively the 4th and the 5th column). Note that the exact exhaustive method (as baseline) benefits from lcm which is one of the most recognized frequent itemset mining algorithm. The exact non-enumerative method is effective and rivals the exact exhaustive one. Its main advantage is to calculate the exact FPOF with datasets where the exact exhaustive method fails (e.g., pumsb where the execution was aborted after 5 h).
 6.2 Approximate Sampling Method This section compares our sampling method with the traditional heuristic based on frequent patterns as baseline. For this purpose, we use the Kendall X  X  tau for comparing the ranking stemming from an approximate method f with that stemming from the FPOF (calculated with an exact method):  X  ( f, D )= t  X  X  | f ( t, D )  X  fpof ( t, D ) | / |D| .
 k -sampling FPOF and that of  X  -exhaustive FPOF (when the curve is above 0, it means that the ranking of k -sampling FPOF is better than the ranking of  X  -exhaustive FPOF). The right chart reports the average error of  X  -sampling FPOF divided by that of k -sampling FPOF (when the curve is above 1, it means that the average error of k -sampling FPOF is smaller than that of  X  -exhaustive FPOF). For each point, the minimal support threshold  X  is used as parameter of the  X  -exhaustive FPOF method. At the same time, the sample size k is fixed with the number of patterns mined with the minimal support threshold  X  : k = |F FPOF for a same pattern budget. For some datasets (e.g., chess or sick ), the difference is always greater than 0. For other datasets, as soon as the number of patterns in the sample increases, the difference becomes positive. The average error ratio clearly shows that our method gives a better approximation of FPOF (especially, when the number of patterns is high).
 Figure 3 plots the number of patterns and the average error per transaction of the -approximate sampling method with bound (for  X  =0 . 1). As expected, the smaller the error bound , the greater the number of patterns in the sample. imate sampling method (with =0 . 1) is regularly faster than the exact methods (see Table 2 ). Finally, the real average error per transaction of the approximate sampling method is always much lower than the requested bound (e.g., =0 . 1 inequality that makes no assumption about the distribution. We revisited the FPOF calculation in extracting the least possible patterns or no pattern. Despite this constraint, the proposed exact method has a complexity better suited to certain datasets. Our approximate method using a sampling technique provides additional guarantees on the result with a maximum bound on the error. The experiments have shown the interest of these two approaches in terms of speed and accuracy compared to the usual exhaustive approach where all frequent patterns are mined.
 ods by adding a guarantee on the quality of results without sacrificing speed thanks to sampling techniques. We think it can be generalized to other mea-sures involving patterns or pattern-based models. We would also like to adapt this approach to build anytime algorithms. In the case of FPOF, it consists in extending the pattern sample indefinitely until the end-user wants to stop the process. Then, the algorithm returns the FPOF achieved with the current sample while estimating its error.

