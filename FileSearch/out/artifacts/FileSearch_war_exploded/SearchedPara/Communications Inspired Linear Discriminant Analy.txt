 Minhua Chen minhua.chen@duke.edu William Carson  X  william.carson@paconsulting.com Miguel Rodrigues  X  m.rodrigues@ee.ucl.ac.uk Robert Calderbank robert.calderbank@duke.edu Lawrence Carin lcarin@duke.edu Department of ECE, Duke University, Durham, NC, USA
PA Consulting Group, Cambridge Technology Centre, Melbourn, UK Department of E&amp;EE, University College London, London, UK The analysis of high-dimensional data is of interest in many applications. To reduce the cost of data process-ing, and to increase the interpretability of the data, one typically employs dimensionality reduction as a pre-processing step. It also plays the role of regular-ization for the data. Although nonlinear dimensional-ity reduction methods (Tenenbaum et al., 2000; Song et al., 2007) have become popular recently, linear di-mensionality reduction methods still play an impor-tant role, mainly due to their simplicity. Linear dimen-sionality reduction based on random projections has gained significant attention recently, as a result of suc-cess in compressive sensing (Candes &amp; Wakin, 2008) and other applications (Liu &amp; Fieguth, 2012). How-ever, random projections may not be the best choice if we know the statistical properties of the underlying signal (Duarte-Carvajalino &amp; Sapiro, 2009). Hence, an important question to be answered is how to de-sign the projection matrix so that the measurement is the most informative.
 In this paper we focus on projection design for classi-fication, or supervised dimensionality reduction. Lin-ear Discriminant Analysis (LDA) (Fisher, 1936) is one of the most important supervised dimensionality re-duction methods. The design criterion of LDA max-imizes the between-class scattering while minimizing the within-class scattering of the projected data, with these two criteria addressed simultaneously. It has been proven that under mild conditions this criterion is Bayes optimal (Hamsici &amp; Martinez, 2008). How-ever, this method has two disadvantages. First, the dimensionality of the projected space in LDA can only be less than the number of data classes, which greatly restricts its applicability. Second, LDA only uses first and second order statistics of the data, ignoring higher-order information. To overcome these two disadvan-tages, other criteria have been proposed in the liter-ature (Tao et al., 2009), out of which an important category is the information-theoretic criterion. In the information-theoretic approach, the projection matrix is designed by maximizing the mutual informa-tion (MI) between the projected signal and the class la-bel (Torkkola, 2003; 2001; Nenadic, 2007; Kaski &amp; Pel-tonen, 2003; Hild et al., 2006). Intuitively, the larger the mutual information is, the better it is for the pro-jected signal to recover the label information. Theoret-ically, the Bayes classification error is bounded by the MI (Nenadic, 2007) (based on a Shannon entropy mea-sure). However, the MI is not easy to calculate, posing a significant obstacle to its optimization. Almost all existing information-theoretic-based algorithms seek an approximation to the Shannon MI, hence compro-mising the objective function. For example, in recent studies (Torkkola, 2003; 2001; Hild et al., 2006), the quadratic mutual information (with quadratic R  X enyi entropy) is used instead of the Shannon-based MI; this is because with the use of quadratic R  X enyi en-tropy, the gradient of MI can be calculated analytically under the assumption of a Gaussian mixture model (GMM) signal model. In the work of (Kaski &amp; Pel-tonen, 2003), the Shannon MI is approximated by its empirical estimation on the training data. Using Infor-mation Discriminant Analysis (IDA) (Nenadic, 2007), the entropy of the GMM in the MI calculation, where the higher-order information comes into play, is ap-proximated with the entropy of a global Gaussian dis-tribution, which again loses the higher-order informa-tion. The LDA method, although not proposed under the information-theoretic criterion, can also be viewed as an approximation to the MI objective function. The main contribution of this paper is to show that the use of Shannon MI optimization, for linear feature de-sign in classification, can be solved directly, without compromising or simplifying the objective function. The key tool is a theoretical result that recently ap-peared in the communications literature, which gives an explicit expression for the gradient of Shannon MI with respect to the projection matrix in linear vec-tor Gaussian channels (Palomar &amp; Verdu, 2006). This theorem has found applications in the area of precoder design for communication systems (Xiao et al., 2011; Carson et al., 2012), but is not widely appreciated in the machine learning and signal processing com-munities, except for a few papers on optical imaging system design (Ashok et al., 2008; Baheti &amp; Neifeld, 2009). Our paper is the first to apply this theorem to the supervised dimensionality reduction problem. As a result, we obtain a new explicit expression for the gradient of the Shannon MI objective function for any input signal distribution, which is not achieved in any of the methods mentioned above. Consequently, nu-merical optimization methods ( e.g. , gradient descent) can be applied. Since we make no assumptions on the input signal distribution in each class, our analytical result is very general and can be applied to a broad spectrum of applications. Additionally, we perform a theoretical analysis of this design metric, providing new insights. To connect to the quadratic mutual in-formation approach (Torkkola, 2003; 2001), we adopt the mixture-of-GMMs signal model. Suppose the label and data are generated i.i.d. via the following process: c  X  Mult( c ; 1 , w ); x | c  X  p ( x | c ) where w  X  R M  X  1 is the prior distribution on the M classes, x  X  R p  X  1 is the original signal, and p ( x | c ) is the data distribution for class c . Hence the joint density can be written as Here we make no assumption on the form of p ( x | m ); hence the above signal model is very general. We do assume that w and p ( x | m ) are known or can be esti-mated from training data.
 In supervised dimensionality reduction, we seek a pro-jection matrix  X   X  R d  X  p such that the projected signal is the most informative in identifying the underlying class label c . We assume the measurement noise is Gaussian, i.e., p ( y | x ) = N ( y ;  X  x , R  X  1 ) where R is the known noise precision matrix. We adopt the information-theoretic criterion (Nenadic, 2007) as where C and Y represent c and y as random variables, I ( C ; Y ) denotes the MI, and the orthonormality con-straint is common in the literature (Nenadic, 2007). Intuitively, the larger the MI is, the better it is for the projected signal y to predict the latent class la-bel c . Theoretically, there is also a strong justification for the above criterion. The Bayes classification er-be bounded by I ( C ; Y ) as follows (Hellman &amp; Raviv, 1970; Fano, 1961; Nenadic, 2007) where H ( C | Y ) = H ( C )  X  I ( C ; Y ) and 0  X  H ( P e Hence, the smaller H ( C | Y ) the tighter the bound will be for P e , and minimizing H ( C | Y ) corresponds to maximizing I ( C ; Y ). Note that (4) is based on a Shan-non definition of entropy ( e.g. , not a R  X enyi entropy measure (Torkkola, 2003), with a comparison to re-sults based on R  X enyi entropy discussed below). Unless stated otherwise, all measures of entropy and differen-tial entropy discussed below are based on a Shannon definition (Cover &amp; Thomas, 2006).
 In order to solve the optimization problem in (3), we first introduce a theoretical result that appeared in the communications literature: Theorem 1. (Palomar &amp; Verdu, 2006) Given the measurement model in (2), the gradient of mutual in-formation I ( X ; Y ) with respect to the projection ma-trix  X  can be expressed as where  X  = R p ( y ) R p ( x | y )( x  X  x y )( x  X  x y ) &gt; the MMSE matrix, and x y = R x p ( x | y ) d x is the pos-terior mean.
 This theorem provides a connection between infor-mation theory and estimation theory, by linking the gradient of mutual information to the MMSE ma-trix. It has found applications in precoder design for communications systems (Xiao et al., 2011; Car-son et al., 2012). However, the power of this theorem has not been widely applied in the machine learning and signal processing communities. The only studies we found are (Ashok et al., 2008; Baheti &amp; Neifeld, 2009) which use the above theorem to design optical imaging systems. This paper is the first work to apply and extend the above theorem to the supervised linear-dimensionality-reduction problem. Our main result is summarized in the following new theorem: Theorem 2. Given the measurement model in (2) and the multi-class signal model in (1), the gradient of mu-tual information I ( C ; Y ) with respect to the projection matrix  X  can be expressed as with the equivalent MMSE matrix e  X  expressed as e  X  =  X   X  = where  X  is the global MMSE matrix with input dis-tribution p ( x ) and posterior mean x y , and  X  m is the local MMSE matrix with input distribution p ( x | m ) and posterior mean x y ( m ) .
 Proof. Since I ( C ; Y ) = h ( Y )  X  h ( Y | C ) = I ( X ; Y )  X  to Theorem 1,  X   X  I ( C ; Y ) is equal to  X   X  I ( X ; Y )  X  X  X   X  I ( X ; Y | C )= R  X  (  X   X  where  X  and  X  m are the global and local MMSE ma-trix with input distribution p ( x ) and p ( x | m ) respec-tively. From Bayes rule, p ( x | y ) = = p ( x | y ,m ) = Consequently, we have = =
X since  X  m = R p ( y | m ) R p ( x | y ,m )( x  X  x y ( m ))( x  X  x y ( m )) &gt; d x d y and p ( y ) e w m = w m p ( y | m ). Conse-quently, equation (7) is proved.
 The significance of Theorem 2 is that we obtain an ex-plicit expression for the gradient of the MI objective function in (3) under any input signal distribution (1). Consequently, numerical optimization methods ( e.g. , gradient descent) can be applied to solve (3). The equivalent MMSE matrix in (7) can be computed via Monte Carlo simulation and Bayesian inference (we discuss in Section 4 how we do this in practice). Our analytical result in Theorem 2 is very general, in the sense that we make no assumption on the signal dis-tribution in each class. The algorithm can be summa-rized in the following steps: 1. Obtain the input signal distribution in (1) from 2. Compute the equivalent MMSE matrix in (7) via 3. Compute the gradient in (6) and update the pro-4. If converge, stop. Otherwise, go to step 2. The orthonormal constraint on the projection matrix complicates the theoretical analysis of the optimal de-sign. By relaxing this constraint and instead consid-ering a power constraint we can leverage more results from communications and recent work in image recon-struction (Carson et al., 2012). The relaxed problem is where the trace constraint ensures that the rows of the projection matrix have on average unit-norm.
 The following theorem characterizes the optimal pro-jection matrix for the relaxed problem in terms of the singular value decompositions (SVD) of the noise covariance R  X  1 = U &gt; R D  X  1 R U R and the equivalent MMSE matrix e  X  = U Theorem 3. Given the measurement model in (2) and the multi-class signal model in (1), the projection ma-trix  X  which optimizes the relaxed problem in (9) can be expressed via its SVD as  X  ? = U ?  X  D ?  X  V ? &gt;  X  where D
 X  is a square diagonal matrix of optimal singular values and the orthonormal matrices of optimal singu-lar vectors are U ?  X  = U R and V ?  X  = U optimal permutation matrix  X  ? .
 Proof. From the KKT optimality conditions we know where the Lagrange multiplier  X   X  0, e  X  ? is the equiva-lent MMSE matrix associated with the optimal projec-tion matrix  X  ? and we have used the gradient result in Theorem 2. The optimal projection matrix must therefore also satisfy The left-hand side of this equation is symmetric and is diagonalized by U ? &gt;  X  , which means that matrices R and  X  ?  X  ?  X  ? &gt; commute and are simultaneously diag-onalized by U ? &gt;  X  . We can therefore write without loss of generality the optimal unitary matrices as where D U and D V are diagonal matrices with unit modulus diagonal elements, and  X  ? U and  X  ? V are per-mutation matrices. Noting that the action of the two permutation matrices can be captured by a single per-mutation matrix  X  ? and both mutual information and the MMSE matrix are independent of the unit modu-lus matrices, the result follows.
 The characterization in Theorem 3 of the projection matrix for the relaxed problem provides possible so-lutions to (3). For example, by setting the diagonal matrix D ?  X  to be the identity matrix we satisfy the or-thonormal constraint on the projection matrix. This could be useful in the implementation of the gradient descent algorithm. For example,  X  Theorem 3 takes the form of a fixed-point equa- X  It is not known whether the mutual information is  X  The projection matrix now consists of two rota-A solution to the relaxed problem will necessarily be better than or equal to a solution to (3), since the con-straint in (3) is a subset of that in (9) . Therefore the mutual information can be further improved by opti-mization over the singular values of the projection ma-trix. In the signal reconstruction scenarios in commu-nications and image reconstruction, the mutual infor-mation is known to be concave in the squared singular values of the projection matrix when U ?  X  = U R (Car-son et al., 2012). This property can be used to give guarantees on convergence. However, the mutual in-formation is not concave in this scenario. Nevertheless, by inserting the result for the optimal orthonormal matrices back into (10), the optimal squared singular values of the projection matrix satisfy The equivalent MMSE matrix is a function of the pro-jection matrix and therefore either D 2 ?  X  to satisfy or if no solution exists we choose D 2 ?  X  that from (7) in Theorem 2 we know that the equiva-lent MMSE matrix is positive semi-definite. In this section we focus on a specific signal input distri-bution, the mixture-of-GMMs signal model, in which signal from each class m is modeled as a Gaussian Mixture Model (GMM), i.e. , where O m is the number of Gaussian components for class m . As a result, the density in (1) reduces to which is the mixture-of-GMMs signal model.
 Under this specific signal model, the general Bayesian inference in (8) reduces to the inference of x under GMM priors. According to (Chen et al., 2010), the detailed Bayesian inference can be derived as p ( x | y ) = p ( x | y ,m ) = e  X  p ( y | m ) = e  X  e w The marginal density p ( y ) = P M m 0 =1 w m 0 p ( y | m pressed in the denominator of of GMM. The Matrix Inversion Lemma can be used to expedite the computations. Using the above equa-tions, the equivalent MMSE matrix in (7) can be read-ily computed via Monte Carlo draws from p ( y ), with x y and x y ( m ) provided by the above inference. More-over, the inference naturally induces a Bayes clas-sifier max c p ( c | y ) where p ( c | y ) = this mixture-of-GMMs signal model and the induced Bayesian classifier in the experiments. Information-theoretic supervised dimensionality re-duction was studied in (Torkkola, 2003; 2001). Instead of using Shannon entropy to define the mutual infor-mation, they used quadratic R  X enyi entropy to define a quadratic mutual information as where p ( c ) = w c , and p ( y ) is a mixture-of-GMMs de-fined in the same way as that in Section 4. The main advantage of using quadratic R  X enyi entropy is that the quadratic mutual information and its derivative can be expressed analytically for the GMM signal model without Monte Carlo simulations, due to the following property of Gaussian: Z p ( y | m ) p ( y | c ) d y = =
X In this paper, we will use a similar but different defi-nition of quadratic mutual information where h 2 ( Y ) =  X  log R p ( y ) 2 d y is the quadratic R  X enyi entropy. This definition is more relevant to our Shan-non entropy based approach, since by replacing h 2 ( Y ) with h ( Y ), I 2 ( C ; Y ) reduces to I ( C ; Y ). The opti-mization of I 2 ( C ; Y ) is also straightforward, since the gradient can be expressed analytically due to the above property of Gaussians. We will compare this R  X enyi en-tropy based approach to our Shannon entropy based approach in the experiments. We emphasize that both I ( C ; Y ) and I 2 ( C ; Y ) are approximations to I ( C ; Y ) for the sake of optimization, hence they cannot satisfy the bound in (4).
 The Information Discriminant Analysis (IDA) (Ne-nadic, 2007) and Linear Discriminant Analysis (LDA) (Fisher, 1936) are derived under GMM signal model, which is a simplification and a special case of the mixture-of-GMMs signal model discussed in Section 4. It is interesting to compare our method with these two quantitatively. In the GMM signal model, the signal distribution in each class is mod-eled as a single Gaussian, i.e., O m = 1 in (11) for all m . Hence p ( x | m ) = N ( x ;  X  m ,  X  m ) and p ( x ) = P m =1 w m N ( x ;  X  m ,  X  m ). Thus the Bayesian inference in Section 4 can be further simplified. Under this sim-plified model assumption, the MI objective function in (3) can be expressed as = h ( Y )  X  As illustrated above, p ( y ) is also a GMM whose en-tropy cannot be expressed analytically. To overcome this problem, IDA approximates h ( Y ) with a single Gaussian entropy with the same covariance matrix as the GMM p ( y ), hence the objective function can be expressed as I
IDA ( C ; Y ) = where  X  = P M m =1 w m (  X  m +(  X  m  X   X  )(  X  m  X   X  ) &gt; prior covariance matrix for x and  X  = P M m =1 w m  X  m is the prior mean. Then the optimization of I IDA ( C ; Y ) can be solved via gradient descent (Nenadic, 2007). The LDA method (Fisher, 1936) simultaneously max-imizes the between-class scattering and minimizes the within-class scattering of the projected data. It has been proven that under mild conditions this criterion is Bayes optimal (Hamsici &amp; Martinez, 2008). The LDA criterion can be expressed as I
LDA ( C ; Y ) = An analytical solution can be found for maximizing I
LDA ( C ; Y ), however the solution only permits the number of projections d to be less than the class num-ber M .
 It is easy to prove that I IDA ( C ; Y )  X  I ( C ; Y ) (maximum entropy principle) (Nenadic, 2007) and I IDA ( C ; Y )  X  I LDA ( C ; Y ) (concavity of log det(  X  )). Clearly, only I ( C ; Y ) is the exact information-theoretic principle satisfying the Bayes error bound in (4), while the other two are approximations to the MI objective function. Another advantage of our method is that the higher-order information of the signal distri-bution is preserved in the objective function via h ( Y ), while the other two methods only use first and sec-ond order statistics of the data. Even though I ( C ; Y ) cannot be expressed analytically, the optimization can still be done using the tool developed in Section 2. We test our method on three real datasets: Satellite, Letter and USPS. The first two are used in IDA (Ne-nadic, 2007) and can be downloaded from the UCI Machine Learning Repository. The third one is a stan-dard digit recognition dataset with higher feature di-mensions, which can also be downloaded from the In-ternet. A detailed description of the three datasets is as follows: 1. The 36-dimensional feature vectors in the Satellite 2. The Letter data contains 16-dimensional feature 3. The USPS data contains grey scale images of di-The mixture-of-GMMs signal model is used, and the GMM density for each class is learned on the training data via the EM algorithm. Dirichlet Process (Blei &amp; Jordan, 2006) GMM learning with variational Bayes inference was also tried to infer the mixture-of-GMMs model, yielding similar results. Two settings for the number of Gaussian components ( O m ) are considered: O m = 1 for all m , which reduces to the GMM sig-nal model, and O m = 10 for all m . The noise co-variance matrix R  X  1 in (2) is set to be very small (10  X  6 I d ). Four dimensionality reduction methods are considered: LDA, IDA, the quadratic R  X enyi entropy based method with objective function I 2 ( C ; Y ), and the proposed Shannon entropy based method. For the proposed method, 2000 Monte Carlo particles are sim-ulated to compute the equivalent MMSE matrix, and the step size for the gradient descent is set to be 0 . 01. The Bayes classifier max c p ( c | y ) is employed using the learned signal model. The results are summarized in the following tables.
 We observe that for all cases, the proposed method either gives the best performance, or is very near to the best. The state-of-art performance on the USPS data was obtained in (Tao et al., 2009). By adopt-ing a nearest neighborhood rule-based classifier, they obtained classification accuracies of 0 . 7259, 0 . 8672, 0 . 8991 and 0 . 9182 using 3, 5, 7 and 9 designed projec-tions respectively. Comparing to results in the table, we see that the proposed method is very competitive. Our strong result on this USPS dataset gives confi-dence to our method in general. The reason for our good performance is that we are directly maximizing I ( C ; Y ), which bounds the Bayes classification error P e through (4). The larger I ( C ; Y ) is, the smaller the upper bound of P e will be. All other objective approximations to I ( C ; Y ), hence their performances are generally weaker.
 We also observe that the performance using the mixture-of-GMMs signal model ( O m = 10) is generally better than that of the GMM signal model ( O m = 1), which is most obvious for the Letter dataset. This is because the mixture of GMM can model the data more precisely, which effectively improves the projection de-sign and the Bayes classification.
 The LDA and IDA method assume a GMM signal model ( O m = 1), hence the mixture of GMM signal model ( O m = 10) will not affect the projection design for LDA and IDA. However, as explained earlier, a finer signal model can help improve the classification performance. This is why we often observe a higher classification accuracy in LDA(10) (or IDA(10)) than that in LDA(1) (or IDA(1)), even though the designed projection matrices are exactly the same in the two cases; the brackets (  X  ) indicate the number of GMM mixture components.
 IDA performs poorly on the USPS data. This is be-cause the global Gaussian approximation made in the I
IDA ( C ; Y ) objective function may not be appropri-ate for the USPS data with so much heterogeneity. The performance of the quadratic R  X enyi entropy based method is competitive, especially on the USPS dataset when d  X  4. However, it is generally not as good as the proposed method, for reasons explained earlier. For all three datasets we also considered random projections designed based on draws from N (0 , 1) with orthonor-malization, and those were significantly worse than those of LDA, IDA, Renyi and the proposed method. In summary, the performance of the proposed method is very promising. Its computational load is heavier than LDA and IDA, but the performance gain war-rants the effort. Moreover, the projection design is done offline, so the testing speed will not be affected. By harnessing a recent theoretical result on the gradi-ent of MI with respect to the projection matrix (Palo-mar &amp; Verdu, 2006), we have derived a new counter-part theorem for supervised dimensionality reduction. As a result, the Shannon MI objective function can be optimized directly without any approximation. We compared the proposed method to LDA, IDA and a quadratic R  X enyi entropy based method, both theoret-ically and empirically. Results on real datasets show the advantage of the proposed method. This study can be viewed as an example of how a research prod-uct from one area (communications theory) can benefit research in a seemingly different area (machine learn-ing).
 The research reported here was supported by ARO, NGA, ONR and DARPA (KeCom program).

