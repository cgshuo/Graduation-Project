 Csaba Szepesv  X ari szcsaba@sztaki.hu 1111 Budapest XI, Kende u. 13-17, Hungary.
 William D. Smart wds@cse.wustl.edu Department of Computer Science and Engineering, Since the early days of dynamic programming re-searchers interested in solving problems with large or even infinite state spaces combined function approx-imators and value backups. Reinforcement learning (RL) is sometimes defined as large-scale approximate dynamic programming combined with learning tech-niques. Indeed, RL applied to continuous state spaces has been a long standing challenging problem. In this paper we propose and study methods that al-low Q-learning to work in continuous state spaces, un-der the total expected discounted cost criterion. In its very basic form our method updates the parameters of some function approximator, where the update equa-tions take a slightly modified form of the Q-learning equations. In order to be more specific, let ( X t , A t , R be a controlled Markov process, X t being the state vis-ited, A t being the action chosen ( A t  X  A ) and R t  X  R being the reward observed at time step t . Further, let 0 &lt;  X  &lt; 1 be a discount factor. We assume that X t  X  X , where the state space X is a compact sub-set of a Euclidean space R d ( d  X  1). We also assume that the action space A is finite. Denoting by  X  t the parameters of the function approximator at time step t (  X  t  X  R n , where n is the number of parameters), our basic algorithm takes the following form: Here  X  ti is the learning rate associated with compo-nent i at time t , the factor s ti depends on X t and i and determines how much influance sample X t has on updating component i , F denotes a function approxi-mator with F  X  : X  X A X  R . At the price of a slight abuse of notation we also write F (  X  ) instead of F  X  when we want to emphasize the dependency of F on the parameter vector  X  . In this sense, F also represents a mapping that maps the parameter space R n to real-valued functions defined over X X A . Throughout this article we assume that F is a non-expansion that satis-fies a system of interpolation equations with respect to some fixed set of basis points S of cardinality n . Specif-ically, we assume that if S = { ( x 1 , a 1 ) , . . . , ( x a barycentric interpolator defined over a triangulation induced by the set of basis points { x 1 , . . . , x p } for each a  X  A . We shall assume that the values s ti are defined by the equations where s : X  X A X X  X  R is a bounded measurable spatial smoothing function. Typically s ( x, a, z ) will be smooth and decays to zero as k x  X  z k tends to infin-ity. One possible choice is to use a Gaussian function. Note that Equation (1) allows the update of multiple components of  X  t unlike in standard Q-learning. Our main results will be the following: Initially, as-sume that a stochastic stationary policy is fixed that is used to sample the states ( X t ) and actions ( A t ). Further, assume that this policy is such that ( X t ) is a sufficiently regular Markov process. Then  X  t converges with probability one to some parameter vector  X   X  such that F  X   X  satisfies a fixed point equation of the Bellman type. This result is then extended to methods that add new basis points in an adaptive manner. Finally, we briefly outline a multi-stage method that is claimed to yield estimates that converge to the optimal value function. The same multi-stage method allows one to relax the condition of having to use a fixed sampling policy during the course of learning. We also present some experimental results where we provide a prelim-inary comparision of a particular instantiation of the proposed algorithm and some related algorithms from the literature. We assume that the reader is familiar with the con-cepts of reinforcement learning. Here we introduce only the necessary notation and some well known facts. The sup-norm shall be denoted by k X k . The space of real-valued bounded functions over a set X will be denoted by B ( X ). Except where otherwise noted multiplication, absolute value, equality and inequality of functions are defined componentwise. An operator T : X  X  Y , where X and Y are metric spaces is called a  X  -contraction if k Tf  X  Tg k  X   X  k f  X  g k holds for all f, g  X  X . 1-contracting operators are called non-expansive. The equation Tf = f is called a fixed point equation. Any f satisfying Tf = f is called the fixed point of T . When T : X  X  X is a  X  -contraction with 0 &lt;  X  &lt; 1 and X is a complete metric space then T has a unique fixed point.
 By an MDP we mean a 5-tuple M = ( X , A , P, r,  X  ), where X is a set of states, A is a set of actions, P is a transition probability law, r is a reward function and  X  is a discount factor. A function approximator allows one to use finite di-mensional spaces to represent functions with contin-uous domains. In this sense a function approximator maps the parameter space R n to the space of functions defined over some domain X . Note that at this level of generality we are not interested in how a given pa-rameter  X   X  R n is obtained, but we are only interested in the properties of the mapping that assigns functions to the parameters. The idea of using function approx-imators to solve fixed point equations, first proposed in the RL literature by Gordon (1995) is as follows: Assume that we are interested in the fixed point of T that maps the space of bounded functions into itself. Let P be a mapping that maps functions from B ( X ) into R n . Conversely, let F map points of R n into func-tions of B ( X ). For obvious reasons, we shall call such mappings  X  X unction approximators X . Then define the algorithm Note that if for any V  X  B ( X ), P TV can be computed with a finite amount of work then this algorithm can be implemented using finite resources. Now consider the iteration in the space B ( X ) and define  X  t = P V t . Then we see that  X  t satisfies (3) and iteration (3) can be thought of as a finite dimensional approximation of value it-eration. It is not too hard to show that if T is a contraction and F P is a non-expansion then  X  t con-verges to some limit value  X   X  such that F X  t converges to F X   X  = V  X  , where V  X  is the (unique) fixed point of the composite operator F P T (Gordon, 1995).
 In order to extend the above idea to the learning sce-nario in a rigorous manner we will need some more as-sumptions on F . In particular, the notion of interpola-tive function approximators will be central to our anal-ysis. In order to introduce this concept, we first need to be more specific about the operator P . For this, fix a finite set of basis points S = { x 1 , . . . , x n }  X  X and define P : B ( X )  X  R n by Then P is called the composite point evaluation oper-ator w.r.t. S . Using this notion we can now define the notion of interpolative function approximators. Definition 3.1. Let F : R n  X  B ( X ) be a mapping that maps parameters to functions. Then F is called interpolative w.r.t. the set of basis points S if it holds that for all functions V  X  B ( X ) P F P V = P V , or That is, F is interpolative w.r.t. to the set of basis points S when for all parameters  X  the function F (  X  ) takes on the value  X  i when evaluated at the point x i , i = 1 , . . . , n : Actually, this condition is equivalent to the above def-inition. It should be obvious by now why F is called interpolative.
 Since we need to go in both directions between the spaces R n and B ( X ) in an alternating manner it will be useful to define the composite mapping G : B ( X )  X  B ( X ), G = F P . Note that using G Equa-tion (6) takes the form PG = P . Further G satis-fies property (I): G V = G V 0 whenever V, V 0 are such that P V = P V 0 . Equivalently, one may start with a mapping G : B ( X )  X  B ( X ) that satisfies PG = P and property (I) and define F : R n  X  B ( X ) such that F P V = G V . Then F is well-defined by (I) and since P is surjective. In such a case G is also called an in-terpolative mapping. The following proposition sum-marizes some of the basic properties of interpolative non-expansions: Proposition 3.2 (Basic properties of interpola-tive non-expansion mapping). Let P be the com-posite point evaluation operator over B ( X ) w.r.t. some basis point set S . Let G : B ( X )  X  B ( X ) and F : R n  X  B ( X ) be mappings such that the equations are satisfied, where id R n is the identity operator over R n . Then (i) G is a non-expansion if and only if F is a non-expansion. Further, if F is a non-expansion then (ii): Proof. Assume that G is a non-expansion. Now, let u, v  X  R n be arbitrary. Choose U, V  X  B ( X ) such that P U = u and P V = v and k U  X  V k = k u  X  v k . Then k Fu  X  Fv k = k F P U  X  F P V k = kG U  X  X  V k  X  k U  X  V k = k u  X  v k . This proves that F is a non-expansion. Now, assume that F is a non-expansion. Let U, V  X  B ( X ) be arbitrary. Then kG U  X  X  V k  X  kP U  X  X  V k X k U  X  V k . This finishes the proof of part (i).
 Now, assume that F is a non-expansion. Equations (9) and (10) are trivial. Equation (11) follows since kG U  X  G V k X kP U  X  X  V k (since F is a non-exponasion) and kP U  X  X  V k is equal to kPG U  X  X G V k by (9). Hence kG U  X  X  V k  X  kPG U  X  X G V k  X  kG U  X  X  V k , where the last inequality follows by (10). We start with an extension of a result due to Szepesv  X ari and Littman (1999). The extension con-cerns the convergence of a sequence of random func-tions V t that satisfy the iteration
V Here T t is a sequence of random operators mapping B ( X )  X  B ( X ) into B ( X ), and G : B ( X )  X  B ( X ) is assumed to be a non-expansive interpolative mapping. Intuitively, (12) can be thought of as a randomized ver-sion of approximate value function iteration (4) (i.e. the operators T t are randomized versions of T ). Al-though at a first glance, it looks odd that T t is a two argument mapping and that in iteration (12) both arguments are the same, this special two argument form allows one to reduce the convergence properties of asynchronous algorithms to synchronous once, as we shall see it soon.
 Given the decomposition G = F P , where P is the com-posite point evaluation mapping, one readily derives the parameter space recursion from Equation (12), where V t = F X  t , t = 1 , 2 , . . . and  X  t = P V t , t = 0 , 1 , 2 , . . . . This can be proven e.g. by defining  X  t = P V t . Then by the interpola-tive property of F , we have that  X  t +1 = P V t +1 = P F PT t ( V t , V t ) = PT t ( V t , V t ). Further, V GT for all t = 0 , 1 , 2 , . . . , we also have that V t = F X  for t = 1 , 2 , . . . . Putting the pieces together we arrive at (13). Note that (13) can be a practical algorithm (that can be carried out with finite resources), which is not that obvious and indeed does not hold in general for (12).
 We will be primarily concerned with the convergence of V t (equivalently, with the convergence of  X  t ) and the quality of approximation of the optimal value function by the limit value. Note that if the limit exists then by the continuity of P and since  X  t = P V t ,  X  t will also converge to some limiting value.
 Here is our first result: Theorem 4.1. Let T : B ( X )  X  B ( X ) be a contrac-tion with contraction coefficient 0 &lt;  X  &lt; 1 and let G be a non-expansive interpolative mapping w.r.t. to the composite point evaluation mapping P . Let  X  V  X  be the fixed point of G T . Then, iteration (12) converges to  X  V  X  independently of V 0 provided that the following conditions hold: (i) The sequence U t +1 = GT t ( U t ,  X  converges to  X  V  X  with probability one (w.p.1), indepen-dently of U 0 . (ii) There exists a sequence of random function G t , F t  X  B ( X ) that satisfy 0  X P F t , P G t P F t  X   X  (1  X  P G t ) , and P (lim n  X  X  X  k  X  n t = t 0) = 1 , where t 0  X  0 is an arbitrary natural number, and where T t , F t , G t satisfy the following inequalities componentwise: |T t ( U 1 , V )  X  X  t ( U 2 , V ) |  X  G t | U 1  X  U 2 | , (14) |T t ( U, V 1 )  X  X  t ( U, V 2 ) |  X  F t ( k V 1  X  V 2 k +  X  Here  X  t  X  0 is a random process that converges to zero w.p.1 and U, U 1 , U 2 , V, V 1 , V 2  X  B ( X ) are arbitrary. Proof. The proof follows the steps of the main result of (Szepesv  X ari &amp; Littman, 1999): we compare U t +1 = GT denote the error process. First, note that by (11), This equality plays a key role in proving the conver-gence of V t as it shows that it is sufficient to prove that P  X  t converges to zero w.p.1. This way, the prob-lem is reduced to a finite dimensional problem. Now, by (8) P V t +1 = P F PT t ( V t , V t ) = PT t ( V t , V larly, P U t +1 = PT t ( U t ,  X  V  X  ). Therefore Proceeding formally, using (14) and (15) we get Now, by (16), k  X  t k = kP  X  t k and therefore P  X  t +1  X  ( P G t ) ( P  X  t )+( P F t ) ( kP  X  t k + k U Notice that we have reduced the infinite dimensional error recursion to a finite dimensional one. Now, if P G t , P F t satisfy 0  X  P F t , P G t  X  1, P F t  X   X  (1  X  0 and since k U t  X   X  V  X  k converges to zero, by Lemma 4 of (Szepesv  X ari &amp; Littman, 1999) P  X  t converges to zero w.p.1. Hence, by (16) we also have k  X  t k X  0 w.p.1. Since U t is known to converge to  X  V  X  , it follows that V  X   X  V  X  w.p.1., as t  X  X  X  holds, as well.
 We note in passing that although in this paper we are only concerned with the convergence of Q-learning, the above theorem is rather general and can be used to de-duce the convergence of other reinforcement learning algorithms in continuous spaces when they are com-bined with interpolative function approximators. Similarly to the above analysis, the convergence of U t can be studied by looking at P U t . Namely, if P U t converges to  X   X   X  then by the continuity of F , U t must also converge w.p.1. Further, if  X  V  X  is the fixed point of the operator G T then k U t +1  X   X  V  X  k = kGT t ( U t ,  X  G  X  V  X  k = kP U t +1  X  X   X  V  X  k by (11). This shows that if P U t converges to P  X  V  X  then U t converges to  X  V  X  (and vice versa). This observation will be exploited in our next result where we study the convergence of the basic algorithm (1).
 For the proof of our main result, we will need the fol-lowing assumptions. Let S = { ( x 1 , a 1 ) , . . . , ( x be the basis point set.
 Assumption A1 ( X , A , P, r,  X  ) is a discounted MDP, Assumption A2 A t  X   X  ( a =  X | X t ), where  X  Assumption A3 For all t = 0 , 1 , 2 , . . . and i = Assumption A4 For all t = 0 , 1 , 2 , . . . and i = Note that Assumption A4 is the analogue of the con-dition widely used for finite models that every state is  X  ) s ( x, a, y ) denote the  X   X  -cut X  of s . The following the-orem holds true: Theorem 4.2. Consider the sequence  X  t , gener-ated using (1) and assume that Assumptions A1 X  X 4 hold. Further, assume that F is an interpolative non-expansion w.r.t. the set of basis points S and let G = F P . Then  X  t converges to  X   X  w.p.1 such that  X  Q  X  = F X   X  is the fixed point of the operator GH , where H : B ( X  X A )  X  B ( X  X A ) is given by Here  X  s ( z, a, x ) = s  X  ( z, a, x ) / ( Proof. Let us define the process Q t +1 = GT t ( Q t , Q t where [ T . Note that here  X  t is defined over the whole space X  X  A . However, by the special form of the itera-tion defining Q t , one only needs to define the values of  X  ( x i , a i ). This follows from the identity and since by our previous observations  X  t = P Q t sat-isfies the recursion In particular, Q t = F X  t holds as well and when Q 0 is any function satisfying P Q 0 =  X  0 then Equations (21) and (1) yield the same process  X  t . Now, our goal is to show that Theorem 4.1 can be applied to prove the convergence of the process Q t (and hence that of  X  t ).
 First, let us consider the convergence of the process  X  Q that Q = F X  0 for some  X  0  X  R n . Consider the corresponding parameter space recursion P  X  Q t +1 = PT t ( F P  X  Q t , F X  0 ). This recursion takes a form simi-lar to multi-state Q-learning whose convergence was proven in Theorem 4 of (Szepesv  X ari &amp; Littman, 1999). The only difference is that here s ti is defined with s ti = s ( x i , a i , X t ), where now X t is the element of the not necessarily finite set X . Also, in Theorem 4 it was assumed that X t is stationary. Nevertheless, the same proof applies with some small changes once we show that the conditions hold w.p.1. The rest of the assumptions are readily satisfied. Consequently we will have that P  X  Q t con-verges to PH F X  0 = PH Q , where H is defined by (18) (note that PH F is an operator over a finite dimen-sional vector space). Hence  X  Q t converges to GH Q w.p.1.
 The conditions on the learning rates  X  ti are satis-fied since ( X t ) is positive Harris and therefore for any sion of Theorem 4 of (Szepesv  X ari &amp; Littman, 1999) to non-stationary processes can be obtained thanks to E [ f ( X t ) | H t ]  X  function satisfying f  X  L 1 (  X  ). This convergence holds since ( X t ) is positive Harris and aperiodic (cf. Theo-rem 13.3.3 of (Meyn et al., 1996)). This together with some trivial extensions of Theorem 7 and Lemma 7 of (Szepesv  X ari &amp; Littman, 1999) yield the convergence of P  X  Q t to PH F X  0 . 5 So far we have seen that for Q = F X  0 we have that  X  Q t  X  X H Q . Now let  X   X  be the fixed point of PH F . Then F X   X  is the fixed point of GH . Hence, if one takes  X  0 then by our previous result we get that the first condition of Theorem 4.1 is satisfied. The second con-dition can be checked directly using the definition of T . This shows that Q t converges to the fixed point of GH , and hence also that  X  t converges to some pa-rameter vector  X   X  such that F X   X  is the fixed point of GH .
 Note that by taking | A | = 1, Theorem 4.2 yields the convergence of TD (0) for any fixed positive stochastic stationary policy  X  . In this section we consider several extensions of the basic algorithm. The first extension concerns adaptive basis point construction methods. 5.1. Adaptive Basis Points Often the set of basis points is determined by means of an adaptive process. The underlying assumption is that the function approximator is more accurate where there are more basis points. This is the case when e.g. barycentric (linear) interpolation is used, or more generally for spline-or kernel-based methods. Hence, the goal of the algorithm that determines the location of the basis points is to put more basis points into re-gions where a more accurate representation is required (e.g. (Munos &amp; Moore, 1999)). In the current paper, we are not concerned with the details of such a con-struction, but are interested in the convergence of the resulting algorithm. The only assumption we make on the construction process is that it should depend only on the past observations and that the set of basis points is changed by the process only a finite number of times. Further, we assume that the last time when the set of basis points is updated is bounded with probabil-ity one. We call this assumption (FT). The notion of function approximators need to be extended to cover the case of a variable number of basis points. This is done by assuming that we have a sequence of function where we introduced Z = X  X A . Further, we assume that the following error bounds hold: where Q  X  B ( Z ) is now assumed to be a continuous function living in an appropriate smoothness space L , such as a Lipschitz space, and where the constant C &gt; 0 is independent of S and Q . We shall also assume dens( S ) refers to the density of S . This is defined as max z  X  X  dist( z, S ), where dist( z, S ) = min s  X  S d ( z, s ), and where d is a distance defined over Z . Let n t denote the number of basis points at time t . The following result holds: Proposition 5.1. Assume that the basic algorithm is run parallel to a process that updates the set of basis points based on past observations such that in all steps the maximum number of points added is bounded by some constant. Let S t be the set of basis points at time t . Assume that assumption (FT) holds. Then  X  t will converge w.p.1. to some random vector  X   X  , such that F X   X  is the fixed point of the (random) operator GH , have that lim sup t  X  X  X  dens( S t ) &lt; h 0 holds w.p.1 and if Q  X  , the fixed point of the operator H is sufficiently smooth ( Q  X   X  L ) and if H maps L into itself then k F X   X   X  Q  X  k X  O ( h 0 / (1  X   X  )) .
 Note that smoothness requirements regarding H and its fixed point can be satisfied when one assumes suf-ficient smoothness and regularity of the immediate re-ward function r and the transition probability kernel P , such as if r is bounded and differentiable and P satisfies the so-called Feller property. 5.2. A Multi-stage Process In this section we consider a multi-stage process with the goal of proving convergence to the fixed point of the Bellman operator underlying the MDP. In the pro-posed multi-stage process the parameter update equa-tion is used as a subroutine of an outer cycle. The purpose of the outer cycle is to increase the density of the set of basis points S so that convergence will not be limited by the denseness of this set. Let therefore S t be a (deterministic) sequence of nested sets with dens( S t )  X  0. Assume that the sets are changed at the time steps t 0 , t 1 , . . . , t n , . . . where t n +1 and t n +1  X  t n  X  +  X  at an appropriate rate. Fur-ther, assume that we are also given a sequence of non-expansive operators J t : R | S t |  X  B ( Z ), where J t is in-terpolative with respect to S t . We assume that when at time t j the set S t no information is lost. Specifically, we assume that holds for the sequence J t , as well.
 We shall also change the definition of s ti by letting the spatial smoother shrink with time and by compensat-ing for the effect of the exploration policy  X  . Assuming that  X  X is absolutely continuous w.r.t. the Lebesgue measure d X  , we define g  X  ( x ) = d X  X /d X  as the den-sity of  X  X . We assume again sufficient regularity (e.g. fast mixing, g  X  ( x ) being continuous and bounded away from zero) and introduce  X  t , a density estimator whose purpose is to estimate g  X  ( x ) using the samples ( X t ). We redefine s ti as follows: Here ( x ti , a ti ) are the elements of S t and s t : X  X A X  X X  R is a sequence of functions such that s t ( x, a, y ) =  X  ( k x  X  y k /h t ) for some functions  X  a : R + 0  X  R that are assumed to be continuous and satisfy  X  a ( r )  X  0 as r  X   X  sufficiently fast. We further assume that the smoothing bandwidth h t converges to 0. For the sake of simplicity we assume that h t and  X  t are kept sume that  X  t is constructed such that it converges uni-formly to g  X  w.p.1. Such a density estimator can be constructed using e.g. kernel density estimators (Lieb-scher, 2001). The following proposition holds: Proposition 5.2. Under sufficient regularity assump-tions on the MDP M the estimates  X  t are such that J  X  t converges to the fixed point of the Bellman oper-ator T underlying the MDP M .
 The proof uses uniform bounds on  X  t , a convergence rate estimate of the basic process (1) (where s t is de-fined using a fixed spatial smoother, s ) along the lines of (Even-Dar &amp; Mansour, 2003) and the property that if H s is defined by g operator of the underlying MDP. The proof is omitted due to the lack of space. Note that this result can be easily extended to the case when in each stage a different (but  X  X roper X ) sampling policy is used. In (Gordon, 1995) and indepentently in (Tsitsiklis &amp; Van Roy, 1996) convergence results were derived for approximate dynamic programming when the  X  X alue-fitting operator X  was chosen to be a non-expansion or an  X  X lmost non-expansion X . Both papers assume that the controlled system is known.
 The only work known to us which does need a sam-pling device or the knowledge of a model and which does not build a model is due to Singh et al. (1995). The algorithm introduced by these authors is called  X  X oft-state aggregation X  (SSA) and works by updat-ing the parameters  X  t much like Equation (1) except that neither a spatial smoother, nor a function ap-proximator is used in the update equation and only one component is updated in each time step. For con-venience we assume that components of  X  t are now indexed by pairs of the form ( i, a ), 1  X  i  X  n , a  X  A . Then in step t only component with index ( i t , A t ) is updated with i t  X  P (  X | X t ), where P (  X | X  ) is a param-eter of the algorithm. Also, max b  X  t, ( i the estimate of the value of the  X  X ext state X  in place of max b F  X  action pair is computed by interpolating among the components of  X  t using the probability distribution P : Q ( x, a ) = gorithm can be viewed as a Rao-Blackwellised version of SSA with F  X  s ti = P ( i | X t ). Although, our theoretical results do not apply to this case since F  X  defined this way will not be an interpolative function approximator, intu-ition still says that our algorithm (with some other interpolative non-expansion) should be more efficient than SSA since it avoids the introduction of additional randomness and thus it should yield estimates having smaller variance.
 Another related method is Kernel-based Reinforce-ment Learning (KBRL) introduced by (Ormoneit &amp; Sen, 2002). This algorithm is best viewed as one that uses non-parametric kernel-methods to estimate the model (the reward function and the transition prob-abilities). Since it uses kernels and since it is a non-parametric method at a surface-level it might look sim-ilar to our algorithm, but even for a fixed set of sam-ple points our algorithm converges to a different limit point. Further, KBRL is best viewed as an off-line algorithm, whilst our method is an on-line method. We have run some experiments where we compared our algorithm (henceforth called IFAPPQ for  X  X nter-polative Function Approximator based Q-learning X ) with SSA and KBRL. We have selected the well-known  X  X ountain car X  domain of Singh and Sutton (1996) be-cause it provides a standard test-bed and thus our re-sults can be compared with other results published in the literature.
 For all the three algorithms, samples were generated using the same fixed stochastic stationary policy with each run being started at a uniform random location in the state space. This sampling policy was chosen to be an 0 . 2-greedy policy corresponding to a finely-tuned Q-table. Performance was measured as the L 2 -error of the learned Q-values where the values of the finely-tuned Q-table were taken as the ground truth. The L 2 -error was measured (approximately) only over those parts of the state space that had a subtantial For SSA we used a fixed number of basis points that were sampled uniformly at random. We tried vari-ous number in the range [50 , 400] and finally chose to use 200. This parameter had no substantial influance on the final performance. For IFAPPQ we used the following adaptive basis point construction algorithm: when the closest basis point to a new sample point is farther away than a constant (0.05) then a new ba-sis point is inserted and the corresponding parameter values are set such that the action-values at the in-serted point do not change due to the insertion. At the end of runs we typically ended up with 280 X 290 basis points. All basis points stored the distance to their closest neighbors to improve efficiency. Both s and F  X  use Gaussian kernels whose bandwidth is set such that they evaluate to less than 0 . 01 at their clos-The learning rate  X  ti is set to be a 0 / (1+ n ti /N 0 ), where we used N 0 = 1000 and a 0 = 1. For KBRL we used a fixed bandwidth that matched the bandwidth used with IFAPPQ. We also tried time varying bandwidths but the results did not improve significantly. We decided to compare the algorithms on the num-ber of floating point operations they use. The rea-son is that for the same amount of data KBRL does much more computation than the other algorithms and makes better use of experience initially. However we did not compare the performance of it with that of the other algorithms on the basis of the number of sam-ples observed because of its excessive computational demands.
 Plots of the estimated L 2 -error of the Q -values are shown in Figure 1 obtained by averaging results of 5 independent tests. It can be seen that the performance of KBRL is better than that of the others initially, but as time goes by the error of IFAPPQ becomes lower. For SSA the performance improves initially and then it gets worse again. We are still investigating this. We have derived rigorous convergence results for Q-learning when combined with appropriate function ap-proximators. In addition to being a non-expansion, we require the function approximator to satisfy an in-terpolation property. This result was extended to al-gorithms that construct the set of basis points in an adaptive manner. To our best knowledge this is the first result that concerns the convergence of such pro-cesses. The basic algorithm was also extended to a multi-stage process and it was argued that using the tools available to us this process can be shown to con-verge to the optimal value function of the underlying MDP in probability. In the multi-stage setting it is also possible to remove the assumption that the policy used to sample the MDP must be fixed.
 Even-Dar, E., &amp; Mansour, Y. (2003). Learning rates for Q-learning. Journal of Machine Learning Re-search , 5 , 1 X 25.
 Gordon, G. J. (1995). Stable function approximation in dynamic programming. Proc. of ICML 20 (pp. 261 X 268). Morgan Kaufmann.
 Liebscher, E. (2001). Estimation of the density and the regression function under mixing conditions. Statis-tics &amp; Decisions , 19 , 9 X 26.
 Meyn, S., , &amp; Tweedie, R. (1996). Markov chains and stochastic stability . Springer-Verlag.
 Munos, R., &amp; Moore, A. (1999). Variable resolution discretization for high-accuracy solutions of optimal control problems. Proc. of IJCAI (pp. 1348 X 1355). Ormoneit, D., &amp; Sen, S. (2002). Kernel-based rein-forcement learning. Machine Learning , 49 , 161 X 178. Singh, S., Jaakkola, T., &amp; Jordan, M. (1995). Re-inforcement learning with soft state aggregation. NIPS 7 (pp. 361 X 368). MIT Press.
 Singh, S., &amp; Sutton, R. (1996). Reinforcement learning with replacing eligibility traces. Machine Learning , 32 , 123 X 158.
 Szepesv  X ari, C., &amp; Littman, M. (1999). A unified anal-ysis of value-function-based reinforcement-learning algorithms. Neural Computation , 11 , 2017 X 2059. Tsitsiklis, J. N., &amp; Van Roy, B. (1996). Feature-based methods for large scale dynamic programming. Ma-
