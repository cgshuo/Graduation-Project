 Many real applications in experimental design need to make decisions online. Each decision leads to a stochastic reward with initially unknown distribution. New decisions are made based on the observations of previous rewards. To maximize the total reward, one needs to solve the tradeoff between ex-ploring different strategies and exploiting currently optimal strategies. This kind of tradeoff problems can be formalized as Multi-armed bandit problem. We recommend strategies in series and generate new recommendations based on noisy rewards of previous strategies. When the reward for a strat-egy is difficult to quantify, classical bandit algorithms are no longer optimal. This paper, studies the Multi-armed ban-dit problem with feedback given as a stochastic rank list instead of quantified reward values. We propose an algo-rithm for this new problem and show its optimality. A real application of this algorithm on clinical treatment is helping paralyzed patient to regain the ability to stand on their own feet.
 H.4 [ Information Systems Applications ]: Miscellaneous Clinical Recommendation, Exploration-Exploitation Trade-off, Bandit Problem, Rank-Comparison
Our problem is motivated by clinical research which aims to recover motor function after severe spinal cord injury (SCI). Previous research [4] has shown that electrical stim-ulation applied to the spinal cord via electrodes arrays im-planted in the epidural space over the lumbosacral area en-ables paralyzed patients to achieve full weight-bearing stand-ing, improvements in stepping, and partial recovery of lost autonomic functions. The electrical stimulation must be coupled with physical therapy to realize the best outcome. Figure 1: Clinical Treatment of Spinal Cord Injury
Stimulation consists of electrical pulse trains applied to selected electrodes. However, the optimal stimulus pattern (the choice of active electrodes and their polarity, the pulse amplitude and width, and the pulse train frequency) varies significantly across patients. And even for the same pa-tient, the outcome of the same stimulus varies from trial to trial, and the optimal stimulation varies over time due to spinal cord plasticity. Hence, clinicians must determine the optimal stimulus for each patient under noisy and mildly non-stationary conditions. Currently, the search for the op-timal stimulating parameters is a laborious and somewhat ad-hoc approach which consumes valuable clinician and pa-tient time, and does not currently guarantee an optimal out-come.

Figure 1 shows the clinical treatment procedure for stand-training . During a treatment/optimization session, a new stimulus is recommended by our algorithm. The patient then attempts to stand using the given stimulus, and the observing clinicians then rank the patient X  X  resulting perfor-mance. Using this noisy ranking as feedback, the algorithm continues to explore for the optimal stimulus while also ex-ploiting currently good ones. The algorithm must spend sig-nificant time dwelling on good performing stimuli in order to provide the patient with a good therapeutic experience. Since clinical training has a fixed time horizon, we must also maximize total performance during the limited period within which we can search for the optimal solution.
This paper develops an algorithm to recommend optimal stimuli based on the general setting of multi-armed bandit problem. The classical bandit problem trades off between exploration and multi-armed bandit problem exploitation among a number of different arms, each having a quantifi-able, but stochastic, reward with initially unknown distri-bution. The goal of a bandit algorithm is to maximize the total reward. Since its introduction by Robbins [5], bandit problems have been widely studied in many situations [2]. Many efficient algorithms have been developed based on the work of Auer et al., [1].

However, for our clinical problem, the patient X  X  motor re-sponse to stimulation is hard to quantify. Neither video motion capture nor electromyograhic (EMG) recordings of muscle activity can yet provide a consistent and satisfactory measure of motor skill under stimulation. A good standing performance might map to numerous combinations of mus-cle activities, and it is not a stationary process. While the patient X  X  performance under a specific stimulus is hard to quantify, it can be compared to others. In the clinical set-ting, we can obtain the ranking of a group of stimuli which are performed within the short time period of one training session. The dueling bandit problem [7] formalizes online learning problems with preference feedback instead of abso-lute rewards, and hence it can be used for problems with unquantifiable reward. The algorithm we propose in this paper is a variant of the dueling bandit problem which is dictated by the clinical demands of our application.
At the start of the optimization process, we have little information about the best stimulus for the patient, but we have often have a pool of possibly useful stimuli. Our ap-proach is based on the idea of successively removing subopti-mal arms [3] while keeping the optimal one(s) in the sample space. By setting proper confidence intervals, we can reach the optimal reward within the time horizon.
The classical dueling bandit problem receives feedback in the form of a comparison between a pair of bandits in each unavoidable to carry out a very large number of tests before the algorithm converges to its optimal solution. In some applications like our clinical example, each test is expensive and time consuming. The number of tests -time horizon of an algorithm -is often predetermined by clinical conditions. It is infeasible to apply the dueling bandit algorithm directly.
However, our training and optimization procedure allows for patients to not only compare successive stimulations, but to also rank the performances for a modest-sized group of stimulations (the number which can be tested in one clinical session before the patient fatigues). Thus, feedback con-sists of a ranked list of at most d ( d &lt; K ) chosen arms. More precisely, the feedback for each test consists of a com-bined scoring of 4 different standing criteria by the observing clinicians, and the combined score is used to rank the tests within one session. We show below that this feature helps us to reduce the total number of tests significantly, while also dovetailing well with current clinical practice.

Our procedure can be described as follows. There are K arms { b 1 ,  X  X  X  ,b K } , and a total number of T tests to be per-formed. Each test physically corresponds to a  X  90-second stimulation period with a specific stimulus (arm) chosen from the K arms. T is determined before we run the al-gorithm, and is generally assumed to be an integer multiple of d : T = d  X  G , where G is the number of ranking sessions, with each session producing a noisy ranked list of d arms. Algorithm 1 Rank-Comparisons 1: Input: { b 1 ,...,b K } , d , G // Total tests T = d  X  G 2: Input: c  X  ( n ) = p (1 /n ) log (1 / X  ) 3: Run: [Parameters-Initialization] 4: Run: [Active-Elimination] 5: return b  X  // Optimal arm Algorithm 2 Parameters-Initialization 1: Input: { b 1 ,...,b K } , d , G 2: Input: c  X  ( n ) = p (1 /n ) log (1 / X  ) 3: W 1  X  X  b 1 ,...,b K } // set of active arms 4: `  X  1 // rounds 5:  X  b  X  W ` , n b  X  0 // comparisons 6:  X  b  X  W ` , w b  X  0 // priorities 7:  X  b  X  W ` ,  X  P b  X  w b /n b , or 1/2 if n b = 0 9: c  X   X  c  X  ( n  X  ), or 1 if n  X  = 0 // confidence radius 10: g  X  0 // total number of ranks 11: T  X  d  X  G 12: return all new parameters
We follow the the original notation of the dueling bandit problem [7]. For two arms b i and b j , where i,j  X  X  1 ,  X  X  X  ,K } , we write the comparison factor as: where P ( b i b j ) is the probability that b i dominates b b and b j . We define b i b j  X  ( b i ,b j ) &gt; 0. We use the notation i,j  X  ( b i ,b j ) for convenience. Note that ( b the distribution of reward for each arm is stationary so that all comparison factors converge in [-1/2,1/2]. We also as-sume w.l.o.g. that the bandits are indexed in preferential order b 1 b 2  X  X  X  b K so that there is one preferred arm.
The total reward is defined in terms of regret as in the classical bandit problem setting. In the online setting, let b ( t ) be the arm chosen at test t . We define total regret as follows: The total regret R T = 0 if we constantly choose b ( t ) = b during the experiment. R T =  X ( T ) is linear w.r.t. T if we constantly choose b ( t )  X  X  b 1 ,  X  X  X  ,b K } .

We also inherit two important properties of the compari-son factors from the original dueling bandit problem:
Strong Stochastic Transitivity. For any triplet of arms b i b j b k , we assume i,k  X  max { i,j , j,k } .
Stochastic Triangle Inequality. For any triplet of arms b i b j b k , we assume i,k  X  i,j + j,k . This can be viewed as a diminishing returns property.

An optimal method is proposed for our problem which has a finite-time regret bound of order O ( K d logT ) where T is the time horizon.
Our Rank-Comparison algorithm (Algorithm 1), which is a modified version of  X  X eat-the-Mean X  [8], is based on the idea of successively removing suboptimal arms while keeping Algorithm 3 Active-Elimination 1: Input: { b 1 ,...,b K } , d , G 2: Input: parameters generated in [Parameters-3: while | W ` | &gt; 1 and g  X  G do 4: if | W ` | X  d then 5: select b 0 1 ,...,b 0 d  X  W ` at random with no repeats 6: else 7: r  X  d % | W ` | 8: p  X  ( d  X  r ) / | W ` | 9: select b 0 1 ,...,b 0 r  X  W ` at random with no repeats. In 10: end if 11: test selected arms and get rank of the selection 12: for all commutable pairs ( b 0 i ,b 0 j ) in the selection do 17:  X  b  X  W ` , delete comparisons with b 0 from w b , n b 18: W ` +1  X  W ` \{ b 0 } // update working set 19: `  X  ` + 1 // new round 20: end if 21: end for 22: end while 23: return b  X  = arg max b  X  W `  X  P b the optimal one(s) in the sample space. The inputs to Rank-Comparison are the K arms, the largest group size d , and total number of groups G : T = d  X  G .

Parameters-Initialization (Algorithm 2) defines the set of active arms W ` , whose size shrinks as more tests are com-pleted. For each arm b , let n b be the total number of com-parisons between b and other arms, and let w b be the total number of wins against all other arms. Let  X  P b be the em-pirical average of P ( b b 0 ) for all b 0 in W ` , and let the value of  X  P b after n comparisons between arm b and any other arms. Set the confidence interval of P ( b b 0 ) as: where c  X  ( n ) = p (1 /n ) log (1 / X  ), and  X  is the confidence that P ( b b 0 ) lies in  X  C b,n . The function c  X  ( n ) decreases as the number of comparisons n increases. By properly setting parameter  X  , the optimal reward can be reached within the fixed time horizon.
 Active-Elimination (Algorithm 3) is the key part of Rank-Comparison . For each group of tests, d arms are randomly chosen from W ` with no repeats when d &lt; | W ` | . Otherwise, we pick each arm equally and pick the rest arms randomly according to lines 7-9 in Algorithm 3. The randomized selec-tion method provides low-variance total regret. Each group of tests results in a ranking of d arms, which can be regarded as d ( d  X  1) / 2 comparisons among the d arms. For each arm b , the values of w b , n b and  X  P b are updated, as is the corre-sponding confidence radius c  X  . For any pair of arms b and b , one dominates the other if their confidence intervals do not overlap, and the less superior arm is eliminated from W ` . The algorithm runs until the time horizon T = d  X  G is reached, or only one active arm remains.
The patients can rank performances d stimuli at most. For fixed time horizon T , choose the size of groups equals the maximum group size d . It will maximize the number of total comparisons extracted from the ranks, which is d ( d  X  1) / 2.
Let = 1 , 2 to be the comparison factor between the best and second best arms. Obviously, we have  X  1 ,j for all j . The upper bound of the expected total regret for Rank-Comparison is given in the theorem below.
 Theorem 1. The expected regret generated by running Algorithm 1 is bounded from above by O ( K  X  d logT ) .
As compared to the classical dueling bandit regret bound of O ( K logT ), Rank-Comparison has an extra divisor fac-tor of d . This tighter bound is realized because for each group of d tests, order O ( d 2 ) comparisons are extracted from the ranking test. Recall that R T = 0 if the optimal arm b ( t ) = b 1 is constantly chosen, and R T =  X ( T ) is linear w.r.t. T if we constantly choose b ( t )  X  { b 1 ,  X  X  X  ,b factor O ( K  X  d logT ) lies in the region between 0 and  X ( T ). As T increases, O ( K  X  d logT ) is significantly less than  X ( T ).
By extending Theorem 4 of [7], we can form a lower bound on regret in expectation, as stated in Theorem 2, for any al-gorithm which solves the rank comparison problem. Which means no algorithm can achieve lower regret than Rank-Comparison in expectation.

Theorem 2. Any algorithm for the rank comparison prob-lem has a regret bounded from below by  X ( K  X  d logT ) .
Notice that Theorem 2 lower bounds total regret on the same order as the upper bound in Theorem 1. So we have conclude that total regret is order  X ( K  X  d logT ) for Rank-Comparison .

Theorems 1 and 2, whose detailed proofs can be found in the supplementary [6], show that our algorithm is optimal in terms of the expected total regret.

Unlike the classical multi-armed bandit problem, which only focuses on expected total regret, many applications must constrain the regret X  X  variation. In our context, if a stimulus optimization algorithm provides good results in the majority of patients, but bad results in a few, the variation is large. Such an algorithm is not practically useful, even if total regret is small. By randomizing the choice of arms within each test group, the randomized comparison strategy of Rank-Comparison provides low-variance regret in expec-tation.
We first evaluate the algorithm by simulation. The reward for each arm b i is modeled as a Gaussian distribution with mean  X  i and standard deviation  X  i . All arms are indepen-dent with each other. Obviously, the distributions generated in this way satisfies the Strong Stochastic Transitivity and Stochastic Triangle Inequality. Then we sample the arms for each group and rank them by using the Rank-Comparison algorithm. We calculated the expected regret r t = R t /t (in-stead of total regret R t ) where t is the number of tests. In the simulation, we consider the total number of arms is 10 and we can get rank list with dimension no larger than 5. The reward of each arm b i follows a Gaussian distribution
Figure 2: Mean Regret against Number of Tests with mean  X  i  X  [0 , 1] and standard deviation  X  i = 0 . 2. Set the confidence parameter  X  = 10  X  2 .

Under this setting, the arms are hard to be distinguished from each other due to the large variances. Figure 2 shows the mean regret r t vs. time t for Rank-Comparison (blue curve) and Beat-the-mean [8] (red curve) with fixed hori-zon T = 1000. Blue curve is the mean regret of Rank-Comparison , while the red curve is the mean regret of Beat-the-Mean algorithm. For both algorithms, the mean regret is high during exploration, and then drops quickly after the algorithms converge to the optimum. We can see that Rank-Comparison finds the optimum within 150 tests, and there-after exploits it to reduce the mean regret. However, Beat-the-Mean did not converge to the optimum within the time horizon for the same parameter settings. We hypothesize that Rank-Comparison outperforms Beat-the-Mean because of the utility of finer feedback information.

We have applied Rank-Comparison to a SCI patient im-planted with Medtronic electrode arrays (16 electrodes) driven by a Restore Advanced impulse generator. This system can apply more than 10 9 unique stimuli. Searching through the whole space of possible stimuli is neither feasible nor neces-sary for the clinical experiment.

For the first clinical treatment, the initial space for explo-ration is composed of around 20 stimuli. We have run Rank-Comparison for the stimuli recommendation. The algorithm has not converges to a single arm but has eliminated the ma-jority of them. Since the number of current clinical tests is small, we have not seen the logarithmic convergence of the regret. The reason is that elimination precess is still ongo-ing and we are exploring more for the early experiments. We will keep running the Rank-Comparison algorithm on new clinical treatments.
This paper proposed a Rank-Comparison algorithm to efficiently solve a specific bandit problem using subgroup rank feedback. This optimal strategy (Theorems 1 and 2) provides clinical recommendation which explore for optimal stimuli while exploiting high performing stimuli for SCI ther-apy. The main advantages of Rank-Comparison are:
Rank-Comparison decomposes test group rankings into equally weighted comparisons. One might reasonably as-sume that arms far apart in rank may be more distinguish-able than adjacent ones, and thus employ different confi-dence parameters as appropriate. This feature can reduce total regret under the same problem setting. From the clin-ical point of view, this method avoids the varying effect of human judgement by using robust comparisons instead of volatile quantitative values, which may be non-stationary in our application. However, the time varying characteristics of human motor performance due to fatigue in the short term, and spinal plasticity over the long term, is a real theoretical and clinical issue we must address.

Additionally, the classical bandit problem X  X  assumption of independent arms does not hold for the spinal cord stimu-lation where anatomical principles and electrical properties suggest a coupling occurs. Using a measure of similarity be-tween stimuli based on the physical properties, we can build a prior distribution on unknown arms to guide our search.
This work was supported by the the Helmsley Founda-tion, the Christopher and Dana Reeve Foundation, and the National Institutes of Health (NIH). [1] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time [2] S. Bubeck and N. Cesa-Bianchi. Regret analysis of [3] E. Even-Dar, S. Mannor, and Y. Mansour. Pac bounds [4] S. Harkema, Y. Gerasimenko, J. Hodes, J. Burdick, [5] H. Robbins. Some aspects of the sequential design of [6] Y. Sui and J. Burdick. Bandit problem with subgroup [7] Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The [8] Y. Yue and T. Joachims. Beat the mean bandit. In
