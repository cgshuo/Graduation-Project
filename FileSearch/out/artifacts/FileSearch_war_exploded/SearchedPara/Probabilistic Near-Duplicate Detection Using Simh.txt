 This paper offers a novel look at using a dimensionality-reduction technique called simhash [8] to detect similar doc-ument pairs in large-scale collections. We show that this algorithm produces interesting intermediate data, which is normally discarded, that can be used to predict which of the bits in the final hash are more susceptible to being flipped in similar documents. This paves the way for a probabilistic search technique in the Hamming space of simhashes that can be significantly faster and more space-efficient than the existing simhash approaches. We show that with 95% re-call compared to deterministic search of prior work [16], our method exhibits 4-14 times faster lookup and requires 2-10 timeslessRAMonourcollectionof70Mwebpages.
 H.3.3 [ Information Search and Retrieval ]: Clustering Algorithms Hamming distance, similarity, simhash, clustering
Many clustering problems in data mining involve similar-ity matching , which is a process of finding pairs of documents in collection D that are similar (in some sense) to each other. To decouple similarity from page semantics and ultimately human judgement, it is common to preprocess the data by extracting d -dimensional feature vectors (i.e., syntactic ele-ments perceived to be important) from each page and solving the matching problem on them.

When the number of dimensions d is large, many tradi-tional clustering approaches [4], [12] are a  X ( n 2 )endeavor, where n = |D| , that requires an all-to-all comparison across the entire dataset. Since this is infeasible for collections  X  Supported by NSF grants CNS-0720571 and CNS-1017766. larger than a few thousand pages, another approach [6], [8], [14] is to devise approximate algorithms that can com-pute similarity using much smaller data structures and sub-quadratic overhead. One promising technique in this cate-gory, called simhash [8], [13], [16], relies on replacing feature vectors with b -bit fingerprints that preserve cosine similarity of the vector space.

The main challenge with simhash is to quickly find all pairs of fingerprints within a certain Hamming distance h of each other. For large collections (i.e., billions of pages), the currently fastest and most space-efficient solution [16], which we call Block-Permuted Hamming Search (BPHS), relies on permuting chunks of each hash and performing lookups in multiple similarly-permuted copies of the dataset. However, since D must be replicated at least four times in RAM, BPHS may be unsuitable for certain memory-constrained systems. EvenifenoughRAMexiststohold4 n items, un-less the number of copies is scaled with dataset size, the search complexity of this method is still quadratic in n .
Noticing that simhash is already probabilistic in nature and does not guarantee 100% recall on vector pairs, it makes sense to ask whether by sacrificing a small additional per-centage of recall one can reduce RAM consumption of BPHS, speed up the search, and improve scalability as n  X  X  X  .We investigate this question next.
We first analyze the construction process of simhash and observe that certain bits are much more volatile (i.e., likely to be flipped by modification of features) than others. This leads to a conjecture that the Hamming-distance problem on simhash can be solved efficiently by an iterative process that intelligently searches to distance h within the volatile bits and performs lookups in a single copy of the dataset, keeping RAM overhead at the theoretical minimum.

Thus, the main question with using this approach is how to design an efficient algorithm that decides which bits to flip, in what order, and when to stop (if less than 100% recall is desired). For a given page u ,let p i ( u ) be the probability that bit i of u  X  X  hash is different from that in other pages of the collection and S X  X  1 , 2 ,...,b } be some subset of hash bits. Then, the probability that there exists another simhash that differs from u only in bits contained in S can be estimated as: where independence between the bits is assumed from the simhash algorithm [8].
To maximize useful work and allow the search to stop after k b h attempts, one requires a method for obtain-ing the top-k subsets S (1  X |S| X  h ) in decreasing order of their probability p ( S ),butwithmuchlowercomplexity than exponential needed to compute all possible values (1) and sort them. To solve this problem, we offer a novel al-gorithm we call Volatility Ordered Set Heap (VOSH), which performs the job in the optimal  X ( k log k )timeand X ( k ) space. For the common setting h =3and b = 64 suggested for large datasets [16], we compare VOSH with a matching algorithm that flips bits in random order without repetition. Our results show that for 100% recall, the former requires 61 times fewer attempts than the latter; for 80% recall, 151 times fewer; and for 50% recall, 347 times fewer. We incorporate VOSH in a system we call Probabilistic Simhash Matching (PSM) for finding similar documents in large collections D under two modes of operation  X  online and batch . The former case, assuming that D fits in RAM, aims to minimize the latency of answering queries about incoming pages u . The latter case, assuming that D is kept on disk, aims to maximize the query throughput rate. We compare our method against BPHS [16] in terms query speed and RAM usage using a web collection with 70M documents. For 95% recall, our results show that PSM requires 2  X  10 times less RAM, while being 5  X  14 times faster than BPHS. We also model both approaches and show that with a fixed number of copies of the dataset in RAM, PSM scales as O ( n log h ( n )loglog n ) as opposed to BPHS X  X   X ( n 2 ).
The first approach to matching similar documents is to convert them to some canonical form in which they become exact duplicates. By removing certain tokens perceived dis-pensable (e.g., high/low IDF words [9]) and hashing the re-sulting page to a single fingerprint, similar documents can be found in  X ( n log n ) time using sorting.

Detecting similar pages over a general space of d -dimensional feature vectors is a more challenging task, especially at non-trivialscale.Inthesmallnumberofdimensions d log 2 n , special data structures (e.g., R-tree [12], Kd-tree [4]) can find near neighbors in  X ( n log n ) time; however, their per-formance deteriorates to that of an exhaustive search (or worse) as d increases. Since web-scale collections usually exhibit dictionaries with millions of unique words, each typ-ically mapping to a feature, this approach is generally inap-plicable to such cases.

A more viable solution for large d and n involves approx-imate algorithms that sacrifice some precision and recall in favor of manageable speed. These include Locality-Sensitive Hashing (LSH) [14], Min-Wise Independent Permutations [7], simhash [8], and many variations [2], [3], [6], [5], [10], [13], [16], [18]. This is the direction we pursue below.
We start by motivating the usage of fingerprints, analyze the brute-force approach to finding near duplicates, and for-mulate the problem we aim to solve.
Since our focus is on large web crawls (i.e., billions of pages), simhash  X  X  low space and time complexity [16], as well as high precision [13] compared to other hashing techniques Table1:Extrapolateddelaytocomputecosinesim-ilarity and Hamming distance on all pairs (one core of AMD Phenom II 2.8 GHz). [7], are quite appealing. With this in mind, we next set up the terminology and notation that will be needed later.
Let F = { 1 , 2 ,...,d } be the set of all unique features acrosstheentirecollectionand F ( u )  X  X  be the set of features present on page u  X  X  . The number of features f ( u )= |F ( u ) | can vary from a few hundred to a few thou-sand depending on which features are selected and the spe-cific dataset. Each feature i  X  X  ( u ) is described by a certain real weight w i ( u )  X  R , which measures the importance and contribution of i to u . A combination of F ( u ) and weights { w i ( u ) } i  X  X  ( u ) represents the feature vector of page u . Define V to be the set of all feature vectors produced by D and assume some similarity measure s : V 2  X  [0 , 1] that maps pairs of vectors to real numbers (i.e., values close to 0 indicate dissimilar documents and those close to 1 indicate similar ones). In the context of shingles and min-wise hash-ing [7], s ( u, v )isusuallyJaccard X  X coefficient;however,for simhash and non-integer weights it is more common to uti-lize cosine similarity [8], which we also do below whenever comparison in the vector space is needed.

To understand the scale at which similarity can be sought in an all-to-all search among feature vectors, we have the fol-lowing analysis. Each pair of vectors ( u, v ) requires f ( u )+ f ( v ) operations, some of which can be quite expensive (e.g., multiplication for cosine similarity), for a total overhead of E [ f ( u )] n 2 . Assuming  X  bytes are needed to store each fea-ture index and its weight, the expected space requirement of D is nE [ f ( u )]  X  . Using our dataset of webpages with E [ f ( u )] = 141 and  X  = 8, the left side of Table 1 shows the amount of time and RAM needed to find all duplicate vec-tors under cosine s ( u, v ). Notice in the table that both space and time are prohibitive, even for relatively small datasets.
To make storage and computation more reasonable, a sec-ond level of approximation (i.e., simhash ) is a one-way map-ping V X  X  0 , 1 } b that converts feature vectors to b -bit binary strings. Assuming H isthecollectionofhashesproducedby V , the main similarity metric on H is Hamming distance H ( x, y ) whose benefit over s ( u, v ) lies in the fact that it can be computed with a handful of CPU instructions [20].
Replacing each feature vector with its 64-bit simhash ,the right side of Table 1 shows an improvement in computational speed by a factor of 3800 (i.e., from 70K pairs/sec to 268M pairs/sec)andareductioninspacebyafactorof141,both of which are substantial. While this solves the problem for n up to a few million pages, the 3 months for medium-sized collections and the 68 years for large datasets shown in the table are still undesirable in practice. This explains our in-terest in sub-quadratic techniques for computing Hamming distances over large document sets.
There are two classes of matching problems we consider in this paper. In the first class (e.g., clustering [11]), the Algorithm 1 Simhash ( u ) 1: W  X  array of b zeros 2: for i  X  X  ( u ) do Examine each feature 4: for j =1to b do Iterate through each bit 7: else 9: end if 10: end for 11: end for 12: for j =1to b do Revisit all bits 13: if W [ j ]  X  0 then 14: B [ j ]  X  1 Positive weight, set bit to 1 15: else 16: B [ j ]  X  0 Negative weight, set bit to 0 17: end if 18: end for 19: return array B [1 ...b ] simhash goal is to find all matches for a given page. Knowing pair-wise similarity among pages, one can use separate clustering algorithms, which we do not consider here, to combine the various documents into groups.
 Objective 1. Given x , find all y  X  X  s.t. H ( x, y )  X  h .
In the second class (e.g., duplicate elimination [13], plagia-rism detection [19]), the goal is to determine if there exists at least one similar page in the dataset, without finding all matching documents, which often can save significant pro-cessing overhead and increase performance.
 Objective 2. Given x , find any y  X  X  s.t. H ( x, y )  X  h .
It should be noted that these goals are not specific to what features are being used (e.g., individual words, shin-gles, HTML tags, anchor text, URLs), their weights (e.g., frequency, TF-IDF, HTML highlight), dimensionality of the vectors, or classification purpose.
We next explain the simhash construction algorithm and dissect the properties of its hashes.
Recall that simhash is a feature fingerprinting technique that uses random projections to generate compact represen-tation of high-dimensional vectors. Its main characteristic is that the Hamming distance between two document finger-prints is positively correlated with cosine similarity between the corresponding feature vectors.

Using the notation of the previous section, Algorithm 1 explains the simhash process of [13], [16], which we discuss in more detail next. Denote by W a vector of temporary weights that the simhash function generates. As we will see later, these weights are important elements of the proposed framework. For each feature i in the current document u ,Al-gorithm 1 first computes its uniformly random hash  X  i and then decides to either add or subtract the feature X  X  weight w to/from W j based on whether the j -th bit  X  ij of the uniform hash is zero or one. After all features have been processed, bits of simhash with non-negative W j are set to 1 and the remaining bits are set to 0. Figure 1: Histograms of simhash weights (bin size 1/150, collection of 70M documents).
We are now ready to examine the distribution of weights {
W j } and lay the foundation for understanding which bits are more likely to be different between similar documents.
Personalizing each variable in the algorithm with its page u , it is not difficult to notice that simhash computes a dot-product between a global vector of iid random variables and avectorof u  X  X  specific feature weights: where {  X  ij } ij are iid Bernoulli variables with E [  X  ij This makes each weight W j ( u )zero-mean:
Using normalized TF-IDF weights w i ( u ), Figure 1(a) shows the distribution of W 1 ( u ) from (2) using our collection of 70M documents. We focus only on the first bit since the others produce identical results. While the Central Limit Theorem suggests that constant feature weights (e.g., used in [13]) are expected to converge W j ( u ) to a Gaussian distri-bution, TF-IDF weights interestingly lead to a Laplace-like distribution (log-linear plots of tails are omitted for brevity).
To control the variance and distribution of resulting sums, one can generalize (2) to non-Bernoulli cases. Assume  X  i (  X  1 ,..., X  ib ) is a vector permanently associated with feature i  X  X  whose elements are drawn from a zero-mean symmet-ric distribution. Then, (2) becomes: while the rest of the algorithm remains the same. Assuming  X   X  [0 , X  ] is the angle between two d -dimensional vectors, [8] shows that if  X  ij are zero-mean Gaussian variables, the probability that the corresponding fingerprints collide on a given bit is q =1  X   X / X   X  (cos  X  +1) / 2. While no similar closed-form result is available for (2), intuition suggests that a correlation between cos  X  and q still exists.

Since w i ( u ) X  X  positive contribution to bit j 1 has no bearing on whether its contribution to another bit j 2 will be posi-tive or negative, it follows that bits in the final simhash pairwise independent , which implies that H ( x, y ) is a bino-mial random variable with parameters b and 1  X  q .While a-priori this tells us nothing about which bits are likely to differ in a random pair ( x, y ), we next show that utilizing the knowledge of x  X  X  simhash weights { W j } allows an informed decision.
Suppose page u undergoes a re-write, which includes dele-tion of  X  existing features and addition of  X  new features. This modifies each simhash weight j to: where X  X  j and X  X  j are summations in the form of (2) that correspond to the deleted/added features. Define Y j = X  X  X j to be the random weight change that occurs in response to these modifications. Since we are interested in changes to u that exist in our collection D (as opposed to an infinite number of hypothetical modifications), Y j can be viewed as a random variable with the same distribution as { W j ( u ) W j ( v ) } u,v  X  X  , which is shown in Figure 1(b). The main dif-ference from part (a) is the doubled variance (i.e., Var [ Y more Gaussian shape; however, the log-scale shows that both tails remain exponential rather than Gaussian.
 Conditioned on W j ( u )= c ,itfollowsthat W j ( u )= c  X  which suggests that the difficulty of flipping bit j is directly related to the magnitude of c . Indeed, first notice that Y is zero-mean since E [ X  X  j ]= E [ X  X  j ] = 0, which follows from (3). Second, averaged over all possible pairs (  X ,  X  ), the dis-tribution of Y j is symmetric around 0 since X  X  and X  X  are iid. We can make this argument due to the non-adversarial nature of changes applied to the page. As a result, we have j , one must encounter a modification that satisfies both a) |
Y | &gt; | c | and b) sign( Y j )=sign( c ). The probability of this happening is P ( Y j &gt; | c | )=1  X  F j ( | c | ), where F is the CDF of Y j . Due to the monotonicity of CDFs, this probability monotonically decays with | c | .

This brings us to our main result of the section. Observe that each simhash contains certain bits that are much more volatile than others. Assume page v is similar to u according to cosine similarity at some threshold (e.g., 0.9). Then, if v manages to flip some bits in u  X  X  simhash , the likely order of these flips follows from the smallest | W j ( u ) | to the largest. For example, consider simhash weights (1 . 9 , 0 . 01 ,  X  the first three bits and the distribution of Y j from Figure 1(b). If anything at all is flipped by small changes, it will most likely be a single bit 2. Much more effort is needed to flip bit 3, while bit 1 requires massive summations of added/deleted features in (2) to overpower its weight 1.9.
While it is straightforward to generate lookups for Ham-ming distance h = 1 (i.e., by sorting all bits in the increasing order of | W j ( u ) | ), multi-bit flips are more difficult. For ex-ample, is the two-bit combination with weights (1 . 9 , 0 . 01) more likely than a single-bit combination with weight  X  0 . 5? We study this question next.
This section develops an efficient technique for deciding the order in which bits should be attempted during similarity search on H . that another page in D has flipped bit j in the simhash of a given page u . While obtaining a closed-form model for p j might be possible in future work, an empirical distribution of Y j sampled from pages in D , e.g., as in Figure 1(b), is sufficient for estimating p j ( u )inpractice.
 Let S X  X  1 , 2 ,...,b } be a non-empty subset of hash bits. Then, the probability that there exists a page in D that differs from u only in the bits contained in S is:
To limit the Hamming search to the most likely subsets, one requires an ordering of {S} according to (6). The brute-force approach would be to generate all possible subsets S sort them in the decreasing order of p ( u, S ),andthenselect the top-k elements, where k controls the tradeoff between overhead and recall. The main problem with this method is that it requires  X ( l log l ) operations and  X ( l ) space, where l = h i =1 b i is often quite large. We next offer a better al-gorithm that solves this problem in optimal space and time.
We start by considering the problem at fixed Hamming distance h . We later generalize the solution to all distances up to h . Throughout this section, it is convenient to refer to bits in decreasing order of their probability p j ( u ) rather than their physical position in the hash (i.e., bit 1 is the most volatile and bit b is the least). Similarly, each set is assumed to be sorted in decreasing volatility of its bits (e.g., S =(1 , 3 , 7) refers to the first, third, and seventh most volatile bits of the hash).

Define  X  to be a lexicographical comparison operator on {S} .If S 1  X  X  2 , then there exists an index i&lt;h such that the two sets share the leading i elements, but S 2 is larger in the ( i + 1)-st element. For example, (1 , 3 , 7 , 15) (1 , 3 , 9 , 10). Then, we have the following important result.
Lemma 1. If two sets S 1 , S 2 of the same size h differ in exactly 1 bit and S 1  X  X  2 ,then  X  u : p ( u, S 1 )  X  p ( u,
Proof. Suppose the bits that differ are i in S 1 and j in
S 2 .Since(6)has h  X  1 terms in each product that are common between p ( u, S 1 )and p ( u, S 2 ), we get:
Recalling that S 1  X  X  2 ,noticethat i&lt;j and p i  X  p j , which immediately shows that (7) is lower-bounded by 1.
This result paves the way for an algorithm that sorts bit combinations using a structure we call Volatility Or-dered Set Heap (VOSH). It starts with the optimal set S 0 (1 , 2 ,...,h ) in the root and iteratively generates for each existing node S i two children, each of whom succeeds S i cording to  X  and differ from the parent in exactly one bit. From Lemma 1, observe that each child X  X  p ( u, S )isalways no larger than the parent X  X . Using transitivity of  X  and  X  we obtain that each S i contains bits whose combination is at least as likely as that of any node in its entire subtree.
We next describe how the tree is constructed using Al-gorithm 2. Given node S with h bits, VOSH attempts to generate two children. The left child always increments the last bit of the parent as long as the result does not exceed b (if it does, the left branch stops). The right child scans the parent X  X  bits from right to left until it finds the first gap Algorithm 2 ProduceChildren( S ) 3: else 5: end if 7: gap = S [ j +1]  X  X  [ j ] 8: if gap = 2 then Should increment bit j ? 10: else if gap &gt; 2 then 12: end if 13: end for
Figure 2: Top five levels of three volatility heaps. in bit numbers of size at least 2. If the gap is exactly 2, the bit on the left of the gap is incremented. Otherwise, the right child is omitted. The last nuance is necessary to pre-vent generation of duplicate nodes along different branches of the tree. Figure 2 shows the top five levels of three VOSH heaps that correspond to h =1 , 2 , 3.

Upper-bounding each heap size by k , space and time com-plexity of constructing all VOSH trees to depth h is  X ( k ). We next explain how to use them during Hamming search to decide the order of bit flips.
In this section, we present our algorithm for near-duplicate search in the simhash space. We start with the bit-generation process, verify its effectiveness, and then discuss online/batch-mode operation on large datasets.
While VOSH ensures that each subtree should not be tra-versed before its root, it does not (and cannot) decide which of the two siblings at each level is more optimal. As ex-plained next, we make this decision during run-time using an additional max-heap M that operates on (key, value) pairs, where the key is p ( u, S ) and the value is set S
Given a page u , we first compute its weights { W j ( u ) the corresponding { p j ( u ) } . We then populate M with tu-ples ( p ( u, S ) , S ) generated by h root nodes of VOSH trees, each corresponding to a different number of bits. At each bit-flip, we extract from M the node with the largest p ( u, obtain its children from the corresponding VOSH tree, com-pute their probabilities p ( u, S ), and insert their tuples into M . This guarantees traversal of bit combinations in the de-creasing order of p ( u, S ) and keeps the total complexity of k flips at the optimal  X ( k log k ).

In [16], b =64and h = 3 were tested in an 8B-page col-Figure 3: Bit flips needed in VOSH compared to random (64-bit hashes). lection and found to work well. We later verify that these numbers are also quite appropriate for our dataset; in the meantime, use them as our target combination for all ex-periments and analysis. To understand the performance of VOSH combined with M , we extract from our dataset 8M random pairs of simhashes at Hamming distance h =1 , 2 , 3. We then compare our VOSH-driven approach to random flipping of bits, where the latter considers all possible com-binations of exactly h bits, ensuring there is no repetition.
Figure 3(a) shows the CDF of the number of bit attempts needed to find each of the matches at distance h =1. Ob-serve in the figure that the first VOSH flip finds the match-ing pair in over 30% of the cases and four flips accomplish the same 80% of the time. All pairs at Hamming distance 1 are discovered in 17 or fewer attempts, while the ran-dom approach requires 64 flips to achieve the same 100% recall. For the exact distances h =2and h = 3 (drawn on a log-linear scale), VOSH finds all pairs in 152 and 675 flips, respectively. This compares favorably to 2,016 and 41,664 attempts needed by the random approach.

The difference between VOSH and random flipping be-comes more pronounced as h increases and recall decreases. This is illustrated in Figure 3(d), which plots the ratio of the number of attempts needed between the two methods for 50%, 80%, and 100% recall. At h =1,VOSHis3.7 times faster than random at 100% recall and 16 times faster at 50%. These numbers increase to 13 and 37 respectively for h =2,eventuallybecoming61and347for h =3.

With these encouraging results in mind, we next describe how VOSH can be applied to large-scale datasets in a frame-work we call Probabilistic Simhash Matching (PSM).
In online mode, the existing fingerprints H are stored in memory to support similarity queries about arriving pages. The main performance metric in this setup is delay  X  needed to compare a new document against a set H of n simhashes .
Hash table
To support efficient lookups, assume H is sorted before the algorithm starts and let n =2 d be a power of 2. Using VOSH across all b bits is extremely wasteful as it produces a large number of hashes that do not exist in the collection, which is especially noticeable when n 2 b . Given b = 64, most datasets D fall into this category. It thus makes sense to limit the random lookups to some small range of t  X  d uppermost bits, which we call the header , and perform a linear scan to match the remaining bits.

The matching process may perform a d -step binary search on H or utilize a hash table of size 2 t for efficient header lookups. We use the latter approach in the paper and il-lustrate the resulting system in Figure 4(a). Given a query page u with simhash x , the first lookup is x itself at Ham-ming distance 0. We then use VOSH to generate k new queries x 1 ,...,x k by working with t most-significant bits of x . Ineachlookup,if i bits are flipped in the header, the linear scan flags all pages whose Hamming distance to x in the remaining b  X  t bits is no more than h  X  i .

If only a single-match is desired, the linear scan stops as soon as it identifies the first similar page; otherwise, it continues until a header mismatch. To differentiate these cases, we use PSM F (first) and PSM A (all), respectively. In batch mode, the existing collection H is stored on disk. New fingerprints are accumulated in set Q until it reaches size m .After Q becomes full, PSM scans file H by reading it in chunks of m fingerprints and matching each x  X  X  against the loaded chunk using the online method described above. After Q is processed to completion, it is sorted and appended to the existing file. This removes the need to sort chunks again when they are loaded in RAM.

It should be noted that PSM F canstopreadingthefileas soon as finds at least one near-duplicate for each fingerprint in
Q , which in certain cases can save significant amounts of overhead. In the worst case, however, both PSM F and PSM A read the entire file for each Q . In a multi-core system, the lookups can be easily parallelized by splitting Q across threads. It also makes sense to pre-process Q by identify-ing the volatile bits in each hash and retaining probabilistic information only related to them. This not only saves mem-ory, but also saves time in identifying top bit-combinations every time a new chunk is loaded into memory.
This section models the currently fastest and most space-efficient simhash matching approach [16] and compares its overhead to that of PSM.
Efficient search in the Hamming space is an old prob-lem [17] that has remained difficult to solve at large scale. For small h , [16] offers an algorithm, which we call Block-Permuted Hamming Search (BPHS), for dealing with large collections D . Suppose we are interested in finding all hashes y  X  X  within Hamming distance h  X  1of x . BPHS first splits the b -bit hash x into G  X  h +1 non-overlapping blocks of  X  = b/G consecutive bits. It then selects an integer g be-tween 1 and G  X  h .If H ( x, y )  X  h , then block-by-block comparison between x and y is guaranteed to have at least g exact matches. The goal of BPHS is to group all possi-ble combinations of g blocks at the front of the hash and perform the search only on them. This is shown in Figure 4(b) for G =8and g = 3, which is a combination that can support all h  X  5.

Similar to PSM, define the leading g blocks of the hash as its header . Then, there are T = G g ways of selecting the g blocks for the header. Call each of these selections a block permutation  X  i ( x ) of the original hash. Since the order of blocks neither in the header nor in the rest of the hash matters, there are exactly T unique permutations, which applied to H produce T copies of the dataset H 1 ,..., H T
After sorting the copies (which are called tables in [16]), the lookup proceeds in T iterations. For the i -th step, BPHS uses a binary search to find the numerically smallest hash in H i whose header matches that of  X  i ( x ). Starting with that hash, it scans H i linearly until the first header mismatch. During this scan, it computes the Hamming distance in the lower b  X  ( g X  ) bits between each target hash and x ,withthe rest of the algorithm being similar to PSM. We consider two versions: BPHS F stops after finding the first match, while BPHS A always checks every eligible hash.

For the analysis, we assume a sufficiently large b to ig-nore round-off effects during division and model only the overhead of BPHS A . Define  X  R to be the RAM latency dur-ing random access,  X  P to be the delay needed to permute the blocks of x ,and  X  L to be the per-hash delay needed to perform Hamming-distance calculations during linear scan. Then, define the number of bits that are matched during binary search as: which has a similar meaning to t in PSM. Note that the min function is necessary since the binary search exhausts the entire dataset in d steps. Then, the total the lookup latency of BPHS in T tables is: Thestorageoverhead(inbytes)ofBPHSissimply:
We next analyze the issue of optimally selecting G .Fora fixed g and assuming g X   X  d , (9) breaks into two terms:  X  = ( G both of which monotonically increase in G .For g X  &gt; d ,we have an even simpler situation: which also shows that choosing the smallest possible G leads to best performance. From (10), we can make the same observation about RAM overhead. Since both space and time decrease with G , it follows that its optimal value is its lower bound g + h . Re-writing (9), we have:
Next, notice that increasing g X  beyond d hurts perfor-mance in (13) as it keeps t = d constant, but increases the leading binomial coefficient, which makes the final model:  X  ( d )= g + h where
Determination of optimal g is impossible without knowing the relationship between  X  R and  X  L , as well as the value of b in comparison to d . Assuming 8 GB RAM and the absolute minimum number of tables T = 4 for h =3(i.e., G =4 ,g =1 , X  = 16 bits), BPHS admits datasets up to n =2 28 with an expected length of linear scans 2 28  X  16
For a RAM-restricted system that cannot grow T to infin-ity, notice that  X  ( d ) scales exponentially with d ,orinother words, linearly with set size n . Thus, in such cases, similar-ity search for all document pairs in H requires complexity  X ( n 2 ). We verify this finding in the next section.
BatchmodeinBPHSissimilartothatinPSM,withtwo exceptions explained in [16]. First, matching in BPHS pro-ceeds by checking each fingerprint y from the loaded chunk against T tables built around simhashes in Q . This is neces-sary to prevent repeated construction of T permuted copies of each loaded chunk. One peculiar side-effect of this opti-mization is that BPHS cannot stop when it finds the first match for y since there might be fingerprints x  X  X  that still do not have any matches. Thus, in batch mode, BPHS F is identical to BPHS A . The second difference from PSM is that batches Q do not have to be sorted before being appended to the file since the search runs against Q rather than H
To process a batch of m = |Q| hashes, the I/O delay is the time needed to read the entire file: where D is the read speed of the hard drive. For high-performance RAID-based configurations and overlapped I/O, computation can be executed while the next chunk is being read. In such cases, the bottleneck is in the CPU portion of the overhead, which consists of the time to permute the m incoming hashes T times, sort the corresponding tables, and perform n lookups in them: where  X  ( . ) is given by the online model (14). Finally, the throughput of this system in hashes per second is:
In the experiments below,  X  CPU  X  20  X  disk dominates and rate r is determined solely by the performance of the studied algorithms rather than the disk speed.
Recall that VOSH flips k combinations in the upper t bits of each fingerprint x . PSM then searches for these combina-tions in a single copy of the dataset H in RAM. The latency of each lookup consists of the VOSH overhead  X  V per exam-ined element in the heap (which hash depth log 2 k ), a single visit to the header hash table, and the linear scan: wherewemake k explicitly depend on t . Since increasing t increases k ( t ), the following simple analysis helps choose the optimal t . Recall that each VOSH combination is limited to h bits out of t possible, which means that k ( t ) is upper-bounded by:
Therefore, ignoring the small terms  X  V ( k )and  X  R in (21) and only considering t  X  d , the dominating term of the delay is  X ( t h 2  X  t ), which is minimized when t is maximized. This shows that the optimal choice is t = d and:
To understand the growth of k ( t )with t , we partition our 70M-page web collection into two parts  X  10M random pages are selected to arrive in online mode and the remaining 60M are chosen for the main dataset H . We first pass each online page through BPHS to find all matches in H within Hamming distance h = 3. We then select for each t such k ( t ) that achieve 95% recall in PSM compared to BPHS.
The result is plotted in Figure 5(a). As t grows from 16 bits to 26, the number of combinations required by PSM A increases from 6 to 23 and that for PSM F from 3 to 15. The exact growth rate cannot be readily ascertained over this small range, but it is visibly super-linear. The total run time to verify 10M hashes in online mode is shown in Figure 5(b). As predicted, the delay is minimized when t is at its maximum (i.e., d ), in which case the processing speed reaches 1.6M arriving hashes/sec for PSM F and 765K/sec for PSM A .
Since PSM maintains a hash table with 2 t entries and a single copy of H , it storage requirement for t = d is: which is equivalent to 2 permuted tables in BPHS. For h =3 and the minimum four tables in BPHS, our approach is at least twice as efficient. Furthermore, for a fixed number of additional tables (i.e., just one), PSM X  X  latency (21) scales as  X ( k ( d )log k ( d )), where d =log 2 n .Since k ( d )is O ( d obtain that its overall complexity for processing all hashes in H is O ( n log h ( n )loglog n ). This compares favorably to  X ( n 2 )ofBPHS.
Given a batch Q of new pages, PSM loads chunks of m = |Q| fingerprints from the file, sets up a hash table for each element of the chunk, and performs lookups for all x  X  X  . Note that after the first match, PSM F removes x from Q , which prevents its being checked against subsequent chunks. This allows PSM F to become much faster as it progresses through the file.

In the worst case, PSM X  X  I/O delay is the same as (16), which in our tests is again much smaller than the CPU la-tency, where the latter can be broken down into three parts  X  producing the permutations for m hashes, performing m lookups in each of n/m chunks, and finally sorting the hashes in
Q before writing them to disk: where  X  S is the mean latency of moving hashes while sorting.
As important result of this analysis is how batch size m affects rate r in (18). For a fixed number of tables T , BPHS X  X   X  CPU scales as  X ( m ) and keeps r virtually un-changed. On the other hand, ignoring the VOSH and sort-ing overhead, PSM X  X   X  CPU scales as  X (log h ( m )loglog m ). This increases the overall rate r slightly slower than lin-ear, i.e., as  X ( m/ log h ( m )loglog m ), but nevertheless signif-icantly faster than in BPHS. We re-examine this issue and confirm this result in the next section.
We next describe our dataset D , examine whether simhash indeed approximates cosine similarity on D ,selecttheopti-mal h , and evaluate PSM in comparison to BPHS.
All our experiments involve a set of 100M web pages crawled by IRLbot [15] in April 2008. We process the collection by removing pages that have size less than 5 KB, contain no URLs, have an exact duplicate (identified using a standard hash function), or consist of non-English words, all of which shrinks D to a total of 70M pages.

We parse each remaining page, removing stop-words and stemming all text outside of HTML tags. We then create feature vectors with weights t i ( u ) being the normalize TF-IDFscoreofeachword i on page u . For calculating simhash fingerprints, we use the 64-bit MurmurHash function [1].
While [16] has shown in small-scale manual verification that h =3and b = 64 produce good results on Google X  X  Figure 6: Comparison of simhash against cosine sim-ilarity at different Hamming distances. dataset with 8B pages, we aim to verify that the same pa-rameters work well for our D , but using the similarity mea-sure of the feature-vector space and at a much larger scale. To our knowledge this has not been done before.

We randomly sample 100M document pairs (out of a total of 4.9 quadrillion) and place them into set  X . We then define set N cos to contain all pairs whose cosine similarity is above threshold  X  =0 . 9, i.e., N cos = { ( u, v )  X   X :cos( u, v ) We also define sets N h (for h =1 , 2 ,..., 64) to contain all sampled pairs of documents within Hamming distance h of each other, i.e., N h = { ( x, y )  X   X : H ( x, y )  X  h } .
Then, precision at distance h is defined as the fraction of pages in N h that belong to N cos : and recall at distance h is the fraction of pages in N cos belong to N h :
Figure 6(a) plots both metrics as a function of h .Observe that small h produces a high rate of false-negatives, but keeps the false positive rate low. Large values of h offer the opposite condition  X  many false positives, but few false negatives. Taking the point where both curves intersect, we arrive at h = 3 as a sensible balance for this tradeoff.
Figure 6(b) shows the expected cosine E [cos( u, v )] be-tween document pairs at different Hamming distances. The correlation between the two is very clear, with H ( x, y ) [1 , 3] producing E [cos( u, v )] that ranges from 0.95 down to 0.89, which confirms the applicability of simhash as a sub-stitute for s ( u, v ) on this dataset.
We implemented both PSM and BPHS [16] in Visual Stu-dio C++, using similar optimizations and running them on the same hardware, which consisted of a desktop machine with the AMD Phenom II X6 CPU (2.8 GHz), 16 MB of RAM, 5 TB of disk space, and Windows Server 2008 R2.
Since PSM is an approximation to an exact simhash search, its relative recall R is computed against the matches found by BPHS. Thus, PSM X  X  total recall against cosine similarity is R ( h ) R , where R ( h ) is given in (25) and plotted in Figure 6(a). It should be noted that PSM X  X  relative precision is 100% against BPHS and is not a factor in our comparison.
Relative recall R for our experiments is defined slightly differently for PSM A and PSM F . In the former case, we take the number of matching pairs found by PSM A and normalize Table 2: Comparison of PSM to BPHS (online mode, 10M queries, 60M existing hashes). it by that found by BPHS A . In the latter case, we record the number of simhashes for which PSM F found at least one similar pair and divide it by the same number in BPHS F .
All our experiments use k ( t ) such that R achieves recall 95%, unless otherwise specified. As discussed earlier, we divide the full dataset of 70M fingerprints by random sam-pling into two parts: 10M hashes are used as queries and the remaining 60M are used as the existing collection H .
We experimented with replacing the binary search in BPHS with extrapolation search suggested in [16]. For d = 26, this reduced the number of RAM lookups from 26 to 11, but the overall runtime of the algorithm increased due to the larger number of multiplications/divisions needed to compute each jump. We thus do not include it in our comparison.
In our experiments, we use two most space-efficient BPHS designs suggested in [16], i.e., T =4and T =10tables. To make comparison easier to follow, we convert our RAM overhead to the same notation by dividing  X  by the size of H , i.e., nb/ 8. Thus, PSM X  X  number of tables becomes: By varying t , we can achieve any overhead T  X  [1 , 2]. We study how this selection impacts the result using the total run-time of each algorithm over the entire set of 10M queries. Table 2 shows our results. The top half of the table focuses on finding all matches, where PSM A is 3 . 4  X  5 times faster than BPHS A , while reducing its RAM consumption by a factor of 2-10 depending on the choice of t .Inthelower half, PSM F is even better (i.e., 3 . 7  X  8 . 7 times faster than BPHS F ) with the same savings in RAM.

Another interesting observation in the table is that PSM is relatively insensitive to RAM usage. After dropping T from 2 to 1.06, PSM A losesonly15%inspeedandPSM F only about 9%. Focusing on the ratio of speed (in thousands of queries/sec) to the number of tables used, PSM A peaks at 617 with T =1 . 125 and PSM F at 1367 with T =1 . 06. The best numbers from BPHS are 38 and 46, respectively.
We next focus on the scalability of each method. In the modeling section, we showed that with a fixed number of tables, BPHS X  X  lookup delay for each query scaled linearly with set size n , which is equivalent to an exponential increase when plotted against d =log 2 n .Atthesametime,we Figure 7: Scalability with dataset size (online mode, 10M queries, 60M existing hashes). Figure 8: Relative recall in PSM with a fixed num-ber of bit flips (online mode, 10M queries, 60M ex-isting hashes). showed that PSM X  X  CPU overhead could be crudely upper-bounded by O ( d h ), which at least in theory should be sig-nificantly better.

Figure 7 confirms this result in our implementation. Specif-ically, in part (a) of the figure, we fix T = 4 tables for [16] and keep T = 2 in our method. Notice the aggressive in-crease in delay for both versions of BPHS and an almost lin-ear increase for PSM. In part (b), the BPHS design calls for g =2and G = 5, which means that the exponential term in (14) does not become active until d exceeds gb/ ( b + h )=25 . 6. Thus, most of the visible increase in Figure 7(b) is due to the binary search; however, once the dataset becomes substan-tially larger, these curves will become exponential in d .Note that in part (a) of the figure, the situation was dramatically different because g was 1 and gb/ ( b + h )was16.
We finish this section by keeping the number of attempted combinations k ( t ) constant and examining how recall R changes with dataset size d . This demonstrates the decay rate of recall as a function of |H| , which might be interesting to applications that intend to keep per-query CPU overhead constant as n  X  X  X  . In these experiments, we scale the number of header bits as t = d and plot the result in Figure 8. First, notice that recall of PSM F decreases slightly slower than that of PSM A . Second, observe that with just k =5 combinations of bit-flips, the former method achieves 90% recall for all datasets up to 60M pages. The latter technique canmaintain93%recallwithjust k = 10 flips, which im-plies that by lowering the target R to 90%, our method can become 2 . 5  X  3 times faster than already demonstrated.
In real-time systems with hard memory and performance constraints that are ready to sacrifice a small percentage of recall, these results show that PSM offers a significantly faster and more space-efficient solution than currently avail-able in the literature.
In these experiments, the existing dataset of 60M finger-prints is read from disk and matched against a query batch with m = |Q| = 10M fingerprints stored in memory. Since in our system  X  disk is at least 20 times smaller than  X  CPU overall performance is dominated by the throughput of the studied algorithms. While we are not concerned with RAM as much as before, simple calculations show that PSM still uses less RAM than BPHS.

Figure 9(a) shows the effect of batch size m on the process-ing speed. As discussed earlier, BPHS A and BPHS F are the same method in batch mode, which we plot as a single curve inthefigure. Weusethe4-tabledesignforBPHSsinceit provedfasterthanthe10-tableversionfordatasetsizesbe-low 2 25 (see Figure 7). We now come back to the prediction in the modeling section that BPHS X  X  speed should saturate and remain constant with m , while that of PSM should in-crease sublinearly, but no worse than m/ log h 2 ( m )loglog m .
Figure 9(a) confirms both findings, showing that the pro-cessing rate of BPHS stabilizes at 55K/sec, while our tech-nique scales from 99K/sec to 727K/sec for PSM F and from 70K/sec to 373K/sec for PSM A . At the final batch size (i.e., m = 10M), PSM outperforms BPHS by a factor of 7 . 7  X  14, which is expected to continue increasing as m  X  X  X  .
It should be noted that even for the largest batch m = 10M, both methods in Figure 9(a) are still 50-70% slower than in online mode. For BPHS, this can be explained by the much larger number of binary searches it performs compared to the online version. For PSM, the model predicts that splitting the dataset into small chunks reduces performance since each new hash must be looked up in n/m hash tables.
We finish the paper by examining how both methods be-have when n increases, but m stays fixed. The model shows that the run-time of both techniques should be linear in n , which means that their throughput should be  X (1 /n ). Fig-ure 9(b) confirms this fact and shows that the ratio between the PSM and BPHS curves remains constant at approxi-mately 7 for PSM A and 14 for PSM F . Combining the var-ious observations, we can conclude that as n  X  X  X  ,batch-mode PSM will be able to use larger m and its performance gains over fixed-table BPHS will grow even further.
In this paper, we presented a novel way of utilizing simhash to find near-duplicates in large collections of documents. We showed that by sacrificing a small percentage of recall the proposed approach consistently outperformed [16] in terms of query speed and space consumption, which it was able to simultaneously lower by a factor that ranged from 2 to 14 in various configurations.

Future work involves analysis of feature-selection tech-niques for better clustering, improvement of simhash recall against cosine similarity, and further overhead reduction in our bit-flipping algorithm.
