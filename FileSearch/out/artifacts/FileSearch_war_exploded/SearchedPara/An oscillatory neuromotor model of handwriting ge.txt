 ORIGINAL PAPER Garipelli Gangadhar  X  Denny Joseph  X  V. Srinivasa Chakravarthy Abstract A neuromotor model of handwritten stroke generation, in which stroke velocities are expressed as a Fourier-style decomposition of oscillatory neural activities, is presented. The neural network architecture consists of an input or stroke-selection layer, an oscillatory layer, and the output layer where stroke velocities are estimated. A separate timing network prepares the network X  X  initial state, which is crucial for accurate stroke generation. Neurobiological sig-nificance of this preparation, and a possible mapping of our architecture onto human motor system is suggested. Interaction between timing network and oscillatory layer closely resembles interaction between Basal Ganglia and Supplementary Motor Area in the brain.
 Keywords Handwriting models  X  Oscillatory neural networks 1 Introduction Handwriting is a learned, highly practiced human motor skill that involves control and coordination of several subsystems in our motor system. Production of handwriting requires a hierarchically organized flow of information undergoing a intention to write a message (semantic level), which is transformed into words (lexical and syntactical level). When the individual letters (graphemes) are known, the writer selects specific letter shape variants (allographs). The selec-tion is done according to the formal allograph selection syn-tax, individual preference or just random choice [ 3 ]. Below this level, the allographs are transformed into movement pat-terns, which is the object of focus of the present work.
Models of handwriting We now briefly review some of the key models of handwriting, particularly emphasizing those that inspired development of the proposed handwriting model. Two general methodologies of handwriting model-ing seem to have been adopted by modelers in the past. The first one, dubbed the  X  X ottom-up X  approach, refers to com-putational models which attempt to empirically reproduce features of human writing such as velocity and acceleration profiles etc; they do not claim any fidelity to neuromotor processes underlying handwriting processes [ 4 , 6 , 7 ]. The second methodology of handwriting modeling focuses on psychologically descriptive models [ 8 , 9 ]. These  X  X op-down X  models usually summarize many issues such as, motor learning, movement memory, planning and sequencing, co-articulation and task complexity of strokes, etc. The pres-ent work is closer to the  X  X op-down X  category.

Hollerbach X  X  oscillation theory of handwriting An impor-tant class of handwriting models is centered on the philoso-phy that stroke data can be resolved into certain oscillatory components by a Fourier-style decomposition. The approach was pioneered by Hollerbach [ 6 ] who proposed an insightful model of handwriting generation where the hand-pen system is represented by two orthogonal pairs of opposing springs acting on an inertial load. It was pointed out that the oscilla-tory natural motions of this system resemble real handwriting segments. Anatomical justification of such a simple system has also been explored [ 6 ].
Schomaker X  X  model Schomaker [ 3 ] proposed a neural network model in which a network of oscillators outputs hor-izontal and vertical pen motion. Network training, performed using a variation of delta-rule, led to uncertain results: perfor-mance depended critically on network parameters. In spite of the shortcomings of the performance of the model, Scho-maker X  X  work clearly elucidates certain issues related to any possible handwriting model. Accordingly, the handwriting process X  X nd hence its model X  X ust have four basic events or phases both in chaining and shaping of handwriting [ 3 ]: 1. System configuration This stage is variously known as 2. Start of pattern After configuring the system for the task 3. Execution of Pattern The duration of this phase and 4. End of pattern This stage deals with the termination of Kalveram X  X  model More recently, Kalveram [ 7 ] proposed a model in which stroke data is resolved to its Fourier com-ponents. This simple mathematical operation is described using the metaphor of  X  X entral target pattern generator X . The model, in our view, has several drawbacks. Since a hand-written stroke  X  for that matter any real motor sequence  X  lives for a finite duration, the dynamics of a system that generates it must be appropriately initiated and terminated. Fourier decomposition assumes a set of oscillators whose initial state is accurately prepared with precise phase-rela-tionships among the oscillators. In a large network of oscil-lators this preparation of the initial state can be a challenge in itself, in addition to accurate stroke learning/acquisition and production. These issues are not addressed in [ 7 ] which assumes a prepared initial state. Another drawback is that in [ 7 ] a separate network has to be trained for every stroke.
Plamondon X  X  model Plamondon [ 4 ] presented a bottom-up model using  X  X elta-lognormal synergies X . The name refers to authors X  estimation of the velocity of a muscle synergy as a Gaussian function of the movement parameters that var-ies logarithmically with time. The model therefore produces bell-shaped velocity profiles similar to those seen in simple hand movements. They also demonstrated the  X  X wo-Thirds Power Law X  relation between angular velocity and curvature for a limited range of elliptical movements for which the law accurately describes human writing.

AVITE WRITE model Adaptive VITEWRITE (AVITE) model [ 5 ] is a neural network handwriting learning and generation system that brings together the mechanisms from Bullock X  X  [ 9 ] cortical VITE (Vector Integration to Endpoint) and VITEWRITE trajectory generation models, and the cerebellar spectral timing model of Fiala et al [ 10 ]. This synthesis creates a single system capable of both reactive movements as well as memory based movements based on previous cerebellar movement learning and subsequent read out from long-term memory. AVITEWRITE model success-fully explains the psychophysical and neurobiological data about how synchronous multi-joint reaching trajectories could be generated at variable speeds. The AVITEWRITE model is used to simulate key psychophysical and neural data about learning to make curved movements, including reduction in writing time as learning progresses; generation of unimodal, bell shaped velocity profiles for each movement synergy; size and scaling with preservation of the letter shape and shapes of velocity profiles; an inverse relation between curvature and tangential velocity; and two-thirds power law relation between angular velocity and curvature. Though the model successfully explains several features of handwrit-ing, it may be noted that it does not belong to the family of  X  X scillatory X  models of handwriting. We will argue in this paper that investigating handwriting in terms of its oscillatory components throws up certain important aspects of hand-writing X  X r perhaps all voluntary control X  X ike preparation, motor delay etc. The present model addresses these issues.
A key contribution of the present model of handwriting is its interpretation of motor preparation as the preparation of the initial state of the oscillatory layer. The model presents a network realization of oscillation theory of handwriting [ 6 ], and, unlike Schomaker X  X  model [ 3 ], the proposed model exhibits reliable training performance.

The outline of the paper is as follows. In Sect. 2 , fea-tures of the present model are discussed. A mechanism for preparing the initial state of the network, training, and valida-tion procedures are described. In Sect. 3 , simulation results of learning lower-case, single-stroke English characters are presented. The need for appropriately preparing the network state raises new issues, which are discussed from a biological perspective in the final section. Proposed extensions of the model are also discussed in the same section. 2 The model The essence of our approach is to produce a stable rhythm in a network of oscillators, and resolve the stroke output in a Fourier-like fashion, in terms of the oscillatory activities of the network of oscillators. The architecture of our network that learns to generate strokes has 3 layers: 1. input layer, 2. oscillatory layer, and 3. output layer (Fig. 1 ). Each node in the input layer represents a separate stroke. Under resting condi-tions, all inputs are in a  X  X ow X  (0) state. To generate a stroke, the corresponding input line is taken to a  X  X igh X  ( 1 ) state, and held in that state throughout execution of the stroke. The oscillatory layer has several sublayers. All the neurons in a sublayer have the same oscillation frequency. In each sublay-er, neurons are connected in a ring topology. Our model dif-fers from the model of Schomaker [ 3 ] in this respect: lateral connections are absent in Schomaker X  X  model. 1 Output layer has two outputs representing horizontal and vertical veloci-ties ( U x and U y ) of the pen tip. Each of the outputs is con-nected to all the oscillators in the oscillator layer. The timing network controls the events in the above 3-layered network (Fig. 1 ).

Single oscillator model Dynamics of a single neural oscil-lator used in the oscillatory layer of our network are given as:  X 
V = tanh ( X  x ) (2)  X  where  X  V  X  denotes the oscillatory output, and  X  x  X  and  X  s  X  X re auxiliary, internal variables of the neuron, respectively. Note its  X  x  X . Such excitatory-inhibitory pair is a standard recipe for producing oscillations.

Analysis of Eqs. ( 1 ), ( 2 ) and ( 3 ) shows that, for I s = 0, and  X &gt;&gt; 1, V in Eqs. ( 1 ) and ( 2 ) has two stable states, V  X  X  1. Correspondingly x also has two stable states, a positive and a negative value. Moreover, if x is at negative (positive) stable state, a sufficiently large negative (positive)  X  s  X  X nEq. ( 1 )flips X  x  X  to its positive (negative) stable state. In Eq. ( 3 ),  X  s  X  simply follows  X  x  X  with a delay. Therefore, a persistent value of x induces a change in  X  s  X  such that  X  x  X  X s toggled periodically. Oscillations are produced by the above system (see appendix for a formal proof), but only within cer-tain limits of the external input I (Fig. 2 ). Beyond those limits the neuron has fixed point behavior. Note also Eqs. ( 1 ), ( 2 ) and ( 3 ) have an unstable fixed point at origin ( x = 0, s The average output of the neuron as a function of  X  I  X  has a sigmoidal form (Fig. 2 ).  X  x and  X  s are the time constants of Eqs. ( 1 ) and ( 3 ). The period of oscillation of the oscillator can be varied trivially by scaling these two time constants appro-priately. This method is used to vary the frequency of various sublayers in the oscillatory layer in Sect. 3 , experiment no. 1.
Sublayer model Each sublayer consists of a network of oscillators [see Eqs. ( 6 ), ( 7 ) and ( 8 ) below] connected in a ring topology. By a proper choice of parameters, such a net-work of oscillators can produce a limit cycle, with specific phase relationships among individual oscillators. Odd num-ber of oscillators in ring (sublayer) is preferred for mode locking as even number of oscillators may lead to loss of rhythm stability i.e.,  X  X scillator death. X  X  12 ]. Choice of such special architectures is imperative since it is known that a gen-eral network of nonlinear oscillators is intrinsically chaotic [ 11 ]. A sublayer with ring topology, odd number of oscilla-tors, where each oscillator is coupled with one (right or left only) neighbor with sufficiently strong (negative) coupling strength exhibits mode locking, where each oscillator pro-duces a periodic output and adjacent oscillators differ by a phase difference of  X  =  X  + 2  X  /m ( m is the number of oscillators) [ 12 ].

Preparing the network state This important stage is described variously in literature as system configuration, motor programming, coordinative structure gearing, prepara-tion, planning, schema build-up etc [ 3 ]. We will use the term preparation in this paper. Although the problem of motor preparation has several dimensions, in context of our net-work model we give it a specific meaning. Since the network is a dynamic system, it must be brought to a  X  X tandard state X , V , if possible, from a random, unspecified state, before it can produce a stroke. This preparation is achieved by giving a preparatory pulse (PP) to a specific neuron (chosen to be the first neuron in every sublayer without loss of generality) and waiting for a specific delay interval. The delay must be long enough to allow the oscillatory layer to approach the limit cycle sufficiently closely; beyond this minimum value the delay must be precisely chosen such that the oscillatory layer state is at a predetermined phase in the limit cycle. We refer to this state as the  X  X tandard state X , V s , henceforth. Since the oscillatory layer has a limit cycle attractor, once the steady state is reached, the oscillatory layer, in free-running conditions (no external input), periodically visits every point on the limit cycle. The standard state is chosen to be a point on the limit cycle. We define the standard state, V s ,asthe state reached by the oscillatory layer at the end of a specific preparatory delay (time elapsed after the PP), (=600 time units), and with a specific PP of duration,  X  , (=20 time units) and amplitude, A (=20).

The timing network The timing network controls the tim-ing of various events in the network (Fig. 3 ). The command to execute a stroke corresponding to the j th neuron in the input layer, is received by the timing network at t = 0. At the same time the j th input line in the input layer is set to a  X  X igh X  value. Immediately (at t = 0 + ) the timing network sends PPs (of duration  X  ) to all the sublayers in the oscillatory layer. After a delay, , (i.e., t =  X  + ) the timing network sends an enabling signal, the input gating pulse (IGP) to the input layer so that the input signal, transformed by a weight stage, reaches the oscillatory layer. An output gate pulse (OGP) is also sent to the output layer enabling the output. That is, dur-ing this interval ( t =[ 0 , X  + ] ) the oscillatory layer does not know about the change in the state of the input lines. Imme-diately after ( t &gt; X  + ) , the OGP is given to the output layer, and the stroke velocity information begins streaming out of the output layer. The output gating duration, T i , gen-erally speaking, must be specific to the stroke that is being produced. However we consider a simpler situation where all strokes are of equal duration, which is presently equal to the time period, T f , of the slowest oscillators (those of first sublayer) in the oscillatory layer. Summary of events A, B, C, and D in Fig. 3 : A The input is fed to the network (also to timing network). B This event is the end of PP and start of preparatory delay C Start of IGP and OGP with duration T i , which enable the D The end of IGP and OGP, network output is again dis-Network response Pen-tip velocities ( U x and U y ) estimated by the network are expressed as weighted sum of the outputs of neurons in the oscillatory layer: U x ( t ) = U y ( t ) = where, N s is the number of sublayers in the oscillatory layer and N k is the number of oscillators in k th sublayer, W x W ik are connections from i th oscillator in k th sublayer to output nodes U x and U y , respectively. Output, V ik ,ofthe i th oscillator in the k th sublayer is given by:  X  V  X  where, x ik is the state of i th neuron in k th sublayer, s auxiliary internal variable of the i th oscillator in the k th sub-oscillator in k th sublayer. As described earlier each sublayer is a ring in which oscillators are connected in a unidirectional fashion with negative coupling strengths as follows: W In the simulations of the following section we take  X  = X  0 given by I where,  X  l is l th input in input vector  X  ={  X  1 , X  2 , X   X  n ,  X  1 } and W 1 i th oscillator in k th sublayer. The last component of  X ,  X  is the bias input to the oscillatory layer.

Preparatory pulse (PP) is given as external input, I net ik Eq. ( 6 ). PP is a rectangular pulse of amplitude, A, and dura-tion,  X  , given to the first neuron in each sublayer.
Training Since the time-averaged output of the oscillatory neuron varies in a sigmoid form as a function of external input (see Fig. 2 ) backpropagation (BP) algorithm may be used for training [ 13 ]. Backpropagation with (BP momentum) and without momentum (plain BP) are applied [ 13 ]. BP algorithm is normally used to train a multi-layered perceptron model to map static input/output vector pairs. In the present case, the oscillatory network is trained to produce stroke velocities as follows. To train the network on l th stroke,  X  l ,the l th input component in input vector  X  ={  X  1 , X  2 , X  3 ... X  l ... X  n is set to 1, and all other input components are set to 0. The cor-responding target output is a set of stroke velocities, V V ( t ) . Note that the oscillatory layer is prepared, as described earlier, and brought close to the standard state, before train-ing every stroke. Since time is discretized, when the network is trained to produce a stroke, it is actually trained to map the following sequence of input/output pairs:  X ( t where  X ( t m ) =  X  , (input is constant throughout the stroke) and t m is the m th instant.

Only the first and second stage weights are trained; the lat-eral weights in the oscillatory layer are held constant. Weight update equations are as in [ 13 ]. Comparison of training error corresponding to learning algorithms Plain BP and BP with momentum is shown in Fig. 4 .
 Calculation of mean error The mean error shown in the Fig. 4 is calculated using the formula, E = where, V pq x and U pq x are the q th points in the desired and actual x -velocities of the p th stroke respectively. Similarly subscript  X  y  X  indicates y -velocity. E is the average recon-struction error in stroke velocity, N S is the number of strokes and N L is the number of points in velocity profile of a stroke, which is the same for all strokes. 3 Results Lower case English alphabets are collected using a stylus (electronic pen) connected to a computer. These strokes are represented by pen tip coordinates, x ( t ) and y ( t ) , along x -direction, and y -direction, respectively. The sampling fre-quency of the device is approximately 70 Hz and hence the sampling time (referred as  X  X ime unit X  in the paper) is equal to 1/70 s. The collected strokes are nearly of the same length; points towards the end are dropped to make them all equally long (120 points per stroke). The duration ( T ) of each stroke is therefore 120  X  ( 1 / 70 ) = 1 . 7143 s. These strokes are used to train the oscillator neural network model of handwriting generation. The frequency of sublayer with lowest frequency of oscillations is set to f = 1 / T . In the following simula-tion experiments, the effect of various network parameters on training performance is studied. Training is performed using BP with momentum. Learning rates for the first and second stage weights are 0.000005, and 0.0001, and the momentum factor is 0.7. 3.1 Experiment no. 1 (i) In this experiment, the oscillatory layer is designed with n = 10 sublayers with intrinsic frequencies of the oscillators taken as { n  X  f : n = 1 X 10 } , where f = 1 / T and T is the (common) duration of strokes. The number of oscillators per sublayer is kept constant and is equal to 25.

The results of training of the above network show that the contribution of weights corresponding to oscillators with higher frequencies is not significant (see Fig. 5 b); and the reconstructed strokes have a high-frequency  X  X remor-like X  distortion (see strokes in B of Fig. 5 a). In order to eliminate this high-frequency distortion, we consider the following net-work modification. (ii) In this simulation the oscillatory layer has five sub-layers, and each sublayer has 25 oscillators. The oscillators are assigned frequencies limited to the band f=[f,3f], where the frequency of any oscillator in the k th sublayer is given as f k = f + f  X  ( k  X  1 )/( N s  X  1 ) , and N s is the number of sublayers in the oscillatory layer. On testing, it is observed that reconstructed strokes have substantially lesser high-frequency distortion (see strokes of row B in Fig. 6 a) than those of row C in Fig. 5 b.
 3.2 Experiment no. 2 In this experiment, the number of sublayers is varied from 6 to 1 and the number of oscillators per sublayer is kept con-stant (=25). The frequencies of the oscillators are limited to band f (as discussed in experiment no. 1). The network is trained on 10 strokes (see Fig. 6 a). It is observed that as the number of sublayers increases, the mean reconstruction error decreases (see Fig. 6 b).

The reconstruction of the stroke  X  a  X  by the oscillatory net-work with six sublayers, with 25 oscillators in each sublayer is shown in Fig. 7 . Frequencies of various sublayers are con-fined to the band [ f , 3 f ] as described above. 3.3 Experiment no. 3 Significance of post-preparatory delay (PPD, ): After the preparatory pulse (PP) is given to the oscillatory layer, the layer is allowed to run freely for a post-preparatory delay (PPD) period ( ) before the input is presented (see Fig. 8 ). How does the performance of the network depend on ? Does the performance error decrease gradually with increas-ing PPD since the network gets more time to settle? Simula-tions conducted to answer this question show that the quality of stroke generation depends on PPD in non-intuitive ways. For effective preparation, the timing network should allow enough delay to allow the oscillator layer to settle to a  X  X tan-dard state X . The following studies illustrate the implications of such a delay.

From Fig. 8 it is clear that error in reproduction does not vary monotonically with PPD, but acquires locally minimal values if the stroke onset occurs at discrete, and nearly peri-odic intervals after the preparation. This is because the state of the oscillatory layer (almost) periodically approaches the  X  X tandard state X  in its circling approach to its final limit cycle attractor (see Fig. 9 ). The network is originally trained using a PPD of 600. Reproduction of strokes is legible at PPD values of 240, 360, 480, etc., even though 600 is the PPD value used during training. Therefore, for faster stroke exe-cution, the network may not really need to wait for long peri-ods: what matters is the precise delay after the preparatory pulse. This experiment inspires some clear predictions on human handwriting, or more generally, perhaps on all volun-tary movements. The onset of handwriting probably always occurs only at characteristic, discrete intervals after the com-mand for execution is given. Further, one might surmise that handwriting movements forcibly constrained to commence at other instants should manifest larger errors. 3.4 Experiment no. 4 Origins of motor variability: One of the most commonly seen features in human movement is motor variability. Motor con-trol researchers view it as a window into the central orga-nization of the system that produces voluntary movements [ 14 ]. One of the obvious origins of motor variability is motor redundancy. Motor variability naturally emerges in the present model. We now describe the various sources of motor variability in the model. (i) Variability due to variation of PPD in motor prepara-(ii) Variability due to random initial state also introduces
The network starts from a random state before preparation; preparation ensures that the initial variability in the network state, just before stroke onset, is suppressed. Therefore vari-ability in stroke output due to variability in initial state is not as significant as variability due to PPD (Fig. 11 ). 3.5 Experiment no. 5 Generating a stroke sequence: Natural handwriting involves a smooth, flowing execution of multiple strokes in a desired sequence. So far we have only discussed the execution of sin-gle strokes. Can the model be extended to execute a stroke sequence? How are timing events coordinated when multi-ple strokes are executed in a sequence? Does the network have to be prepared afresh after every stroke? Two options immediately suggest themselves: (1) to prepare the network after every stroke, or, (2) generate multiple strokes with a single preparation before the first stroke. The main issues in the generation of a stroke sequence are: (1) the total time of execution should be small (inter-stroke delays should be minimal), and (2) the generated stroke sequence should be robust and accurate.

Before describing the methods evolved to address the above issues we describe a notation X  X e name it the Event Chain notation X  that simplifies description of the following experiments. Any handwriting sample consists of a sequence of events, some visible (e.g., a stroke), and some invisible (preparatory processes before stroke execution). The Event Chain notation describes the sequence of events in a conve-nient fashion. An event chain is a sequence of events , like, for example:
My_event_chain = [ &lt; event 1 &gt; , &lt; event 2 &gt;,... &lt; n &gt; ]
Description of each event has multiple fields, as, for exam-ple,
My_event = &lt; event_descriptor , duration , param1 , param2 ,... &gt;
In each event, the first field is a textual description of the event, the second denotes event duration, and the subsequent fields are optional, representing other parameters that char-acterize the event. Thus Event Chain notation for a sample sequence is shown below:
My_event_chain =[ &lt; Preparatory Pulse , 20, 20 &lt; Preparatory Delay , 600 &gt;, &lt; S troke  X  X  X , 120 &gt;, &lt; tory Pulse , 20, 20 &gt; , &lt; Preparatory Delay , 600 &gt;, &lt; 120 &gt; ].
 Explanation of events: &lt; Preparatory Pulse , 20, 20 &gt; : A preparatory pulse of duration =20, and amplitude =20 &lt; Preparatory Delay , 600 &gt; : Preparatory Delay of dura-tion 600 &lt; S troke  X  X  X , 120 &gt; : Execution of stroke  X  X  X  of duration 120 (The two subsequent events are as described above.) &lt; S troke  X  X  X , 120 &gt; : Execution of stroke  X  X  X  of duration 120
Thus the above event chain describes an  X  X l X  stroke sequence executed with full preparation before every stroke. With this notation in place, we now proceed to describe devel-opment of methods for executing a sequence of strokes. (a) Stroke sequence production with multiple preparations
Stroke_Sequence_elle =[ &lt; Preparatory Pulse , 20, 20 &gt;, &lt; Preparatory Delay , 600 &gt;, &lt; S troke  X  X  X , 120 &gt;, &lt; tory Pulse , 20, 20 &gt;, &lt; Preparatory Delay , 600 &gt;, &lt; &lt; Preparatory Delay , 600 &gt;, &lt; S troke  X  X  X , 120 &gt; ].
The inter X  X troke delay (duration of preparatory pulse + post-preparatory delay i.e., =  X  p + = 620) for stroke pro-duction by this strategy is much longer than the duration of stroke production ( T = 120) itself. The quality of the stroke sequence  X  X lle X  generated with this method (Fig. 12 a) is robust because the network is fully prepared before every stroke. This is because the network approaches the standard state before execution of every stroke (Fig. 12 c). However it takes 2,960 ( = ( 620 + 120 )  X  4) time units to execute these strokes instead of the ideal 480 ( = 120  X  4 ) time units. By way of reducing total time for stroke production, let us con-sider stroke production without preparation between strokes. In this method there is no preparation between strokes. Event chain description of the  X  X lle X  sequence produced by this method is:
Stroke_Sequence_elle =[ &lt; Preparatory Pulse , 20, 20 &gt;, &lt; Preparatory Delay , 600 &gt;, &lt; S troke  X  X  X , 120 &gt;, &lt; 120 &gt;, &lt; S troke  X  X  X , 120 &gt;, &lt; S troke  X  X  X , 120
Although the sequence now takes only 1,100 time units, production quality after the first stroke is poor (Fig. 12 b). In this case the network approached the standard state before the first stroke. However, its distance from the standard state at the onset of subsequent strokes continued to increase (Fig. 12 d).

Since the network has always been trained such that the oscillatory layer is in the standard state at the onset of every stroke, it is only natural that performance is impaired when the initial state is different from the standard state. How do we ensure that the system returns to the standard state before every stroke with a short inter-stroke delay (preferably much less than stroke duration)? Our present method of preparation involves giving an initial pulse and waiting for the system to arrive at the standard state. Can this waiting be cut short by driving the system to the standard state within a short inter-val?
Active preparation We now present an alternate method of preparation, viz., active preparation (AP), by which the oscillatory layer is driven to the standard state from an arbi-trary initial state. Eqations ( 6 ), ( 7 ) and ( 8 ) are modified as follows for incorporating AP:  X 
V ik = tanh ( X  x ik ) (13)  X 
Note the extra term,  X ( V S ik  X  V ik ) , the drive term, which is ( 13 ) and ( 14 ) above. Equations ( 13 ) and ( 14 ) which are iden-tical to Eqs. ( 7 ) and ( 8 ) are simply reproduced here for clar-ity. In this method, the state of the oscillatory layer is driven towards the standard state, ( V s ) , through some sort of propor-tional control. The drive factor,  X  , is reset to zero (no drive) during stroke generation, but set to a finite value in the inter-stroke interval, during which the network is actively prepared for the subsequent stroke. AP can be performed before the first stroke also, thereby reducing the prolonged preparatory delay. The Event Chain notation for a single event corre-sponding to AP is: &lt; Active Preparation , d , X  &gt; where d is AP duration (APD), and  X  is drive factor of Eq. ( 12 ).

Network Training using AP Using AP mechanism, we now train a network on six letters (strokes)  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X . The oscillatory layer in this case has four sublayers, each sublayer having 25 oscillators. Strokes are presented as sequences of 3. The following sequences are used for train-ing: d X  X  X  X , c X  X  X  X , h X  X  X  X , a X  X  X  X , h X  X  X  X , c X  X  X  X . The network is trained for 5,000 epochs. Learning rates for the first and sec-ond stage weights are 0.000005 and 0.0001, and momentum factor is 0.7. Before presentation of each stroke, the oscilla-tory layer is prepared by AP. For concreteness, we present event chain notation for training of the network for a single two-stroke sequence  X  X  X  X . X  Stroke_sequence_ha = [ &lt; Preparatory Pulse, 20, 20 &gt;, &lt; Preparatory Delay, 100 &gt;, &lt; Active Preparation, 30,  X  = 7 &gt;, &lt; stroke  X  X  X , 120 &gt; , &lt; Active Preparation, 30,  X  = 7 &gt;, &lt; stroke  X  X  X , 120 &gt; ].

A production of stroke sequence  X  X a X  with the above event chain description is shown in Fig. 13 a. Note that due to AP, the state of the oscillatory layer is pushed very close to the standard state by the time of stroke onset. Note also that the preparatory delay is significantly lesser than what was used in passive preparation (100 as opposed to 600). Total time taken in this case is 420 ( = 20 + 100 + 30 + 120 + 30 + 120), which is a significant savings from the previous case of passive prepa-ration. Although the second stroke  X  a  X  is constructed reliably, note that the ligature between strokes is not smooth. In natu-ral handwriting, ligature between two successive strokes has a smooth flowing quality. This is because often the pen tip velocity does not drop to zero at the border of two strokes and merely goes to a minimum. After extensive experimentation, we found that effective ligature handling consists of terminat-ing the preceding stroke at an early stage, and initiating the succeeding stroke at a late stage. These results are explained below.

Ligature handling Early termination of the preceding stroke and late beginning of the succeeding stroke is the recipe used for achieving smooth ligatures. Event chain for executing the stroke sequence  X  X a X  with ligature handling introduced is given below: Stroke_sequence_ha = [ &lt; Preparatory Pulse, 20, 20 &gt;, &lt; Preparatory Delay, 100 &gt;, &lt; Active Preparation, 30,  X  = 7 &gt;, &lt; stroke  X  X  X , 100, 0, 100 &gt;, &lt; Active Preparation, 30,  X  = 7 &gt;, &lt; stroke  X  X  X , 100, 10, 120 &gt; ].

Event description is as follows: &lt; Preparatory Pulse, 20 &gt; : preparatory pulse of duration 20 and amplitude 20. &lt; Preparatory Delay, 100 &gt; : preparatory delay of dura-tion 100 (note the reduction from 600) &lt; Active Preparation, 30,  X  = 7 &gt; : AP of duration 30 and drive factor,  X  ,of7. &lt; stroke  X  X  X , 100, 0, 100 &gt; : stroke  X  X  X  is executed.
The execution, which is of duration 100, is begun at t 1 = and terminated at t 2 = 100, without completing the full 120 time units. This decremented interval at the termination end is called early stopping interval (ESI), which in this case is 120 X 100 = 20. &lt; Active Preparation, 30,  X  = 7 &gt; : AP of duration 30 and drive factor,  X  ,of7. &lt; stroke  X  X  X , 110, 10, 120 &gt; : stroke  X  X  X  is executed. The execution is begun at t 1 = 10, and terminated at t 2 = 120, making the stroke duration only 110 time units. This decrement in the stroke duration in the initiation end is called Late Beginning Interval (LBI).

Production of  X  X a X  obtained by the above method is shown in Fig. 14 . Note the smoother ligature obtained by this method. However, in Fig. 14 we may also note an undesirable descent of the strokes from left to right.

Correcting line orientation The stroke sequence presented in Fig. 14 forms not a horizontal but an oriented line. The cause of this orientation is the discrepancy between the verti-cal positions of the two terminals of individual strokes. Since in the present method, the network estimates stroke veloc-ities only and the actual stroke is constructed by velocity integration, vertical discrepancy between the two ends of the first stroke carries over to the second stroke; the second stroke therefore is drawn at a higher/lower level than the first. This error in orientation can be corrected by adding an appropriate offset in vertical velocity: positive vertical velocity correc-tion must be applied to correct a dip in orientation. This offset is called Vertical Velocity Offset (VYOFF). A reconstruction of the stroke sequence  X  X  X  X  X  of Fig. 14 , drawn with a VY-OFF=0.1 is shown in Fig. 15 .

So far in this section, we have introduced three mecha-nisms for improving the quality of multiple stroke execu-tion: (1) active preparation, (2) ligature handling, and (3) Correcting line orientation X  X o facilitate reliable multiple stroke generation. Each of these processes involves several parameters. AP involves its duration, APD, and drive fac-tor,  X  ; ligature handling involves ESI and LBI; correcting line orientation involves VYOFF. Resetting the magnitude of any of these parameters to zero amounts to withdrawal of the corresponding corrective mechanism. We now demon-strate the significance of presence/absence of each of these mechanisms for effective generation of a sequence of multi-ple strokes (Fig. 16 ).

The parameter values used for execution of sequence  X  X  X  X  X  in Fig. 16 a are as follows: AP duration = 30, drive factor = 7, ESI = 20, LBI = 10, VYOFF = 0.1. Withdrawal of any of the three corrective mechanisms described above produced characteristic distortions in the stroke sequence generated (Fig. 16 b X  X ). With these parameters we now present results related to execution of words  X  2-letter, 3-letter, 4-letter and 5-letter words  X  produced by the network described above. The network is trained on six letters/strokes:  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X , and  X  X  X . Ideally, the above parameters ought to vary depending on the stroke combinations that are produced. But such a characterization requires a much more detailed study and is deferred to future efforts in this direction (Figs. 17 , 18 ). 4 Discussion We present a neural network model of handwritten stroke generation in which stroke velocities are expressed as a Fourier-style decomposition of oscillatory neural activities. Though oscillatory neural models are typically used to model generation of rhythmic behavior like walking, swimming, etc. [ 15 ], they have proved to be useful in non-rhythmic motor function also [ 16 ]. Since Hollerbach X  X  insightful observation on the oscillatory elements in handwriting, neural oscillators have also figured in models of handwriting. An oscillatory neural model of handwriting, for it to be biologically viable, has to address certain fundamental issues.

A key issue addressed in this paper is one of preparing the initial state of the oscillatory network. This question does not seem have received adequate attention in modeling literature [ 3  X  7 ]. Primarily the oscillatory layer must generate a stable rhythm appropriately registered with respect to the time of onset of the stroke. Further there must be a mechanism to switch the network to a different, stable rhythm to produce a different stroke. Even if the network dynamics are stable enough to flow into a stable trajectory on random initiali-zation, the phase of the network X  X  rhythmic state may not be specific enough to produce a desired movement. There must be some level of forgetting of initial conditions, and therefore linear oscillator models are disallowed. Networks of nonlinear oscillators, with their proneness to chaos [ 11 ], must be handled with extreme delicacy to produce stable, specific rhythms.

In the present work, we believe that a reasonable solution that addresses the above issues is provided. Two forms of preparation are described: (1) Passive preparation, (2) Active preparation. In passive preparation, the oscillatory layer is initialized with small random noise, but is immediately given a large PP to specific neurons (the first neuron in each sub-layer). This pushes the evolution of oscillatory layer in a specific direction, which, after a specific delay, , assumes a nearly standard rhythm (in spite of the low-amplitude fluctuation in the initial conditions). This is the form of prep-aration used in Sect. 3 , experiments no. 1 X 4. Passive prepara-tion involves unduly long preparation times, and also yields suboptimal results when a sequence of strokes is executed. Efficient reconstruction of a stroke sequence is achieved by introducing active preparation (in Sect. 3 , experiment no. 5), in which the state of the oscillatory layer is actively driven towards the standard state. With AP, we show that it is possi-ble to robustly produce longer stroke sequences. From purely algorithmic point of view, AP is more efficient (shorter prep-aration times, robust production of stroke sequences) than passive preparation. However, at this point it is not clear which of these faithfully describes the preparatory processes underlying biological motor function.

In the example shown in Sect. 3 , Experiment no. 5, the net-work is trained only on a small number of strokes. To train on larger numbers of strokes, it is desirable to train subsets of strokes on multiple networks and use a mechanism for gating the outputs of the networks in appropriate sequence. For example, if Network-1 is trained on {a,b,c,d,e} and Net-work-2 is trained on {f,g,h,k,l}, then, to execute the stroke sequence  X  X -e-a-l-e-d X , the two networks have to gated in the following sequence {2,1,1,2,1,1}. Such an ensemble of net-works can be trained on a large public online handwriting database and performance can be evaluated.

Mechanisms for ligature handling and orientation cor-rection have been described in the paper. The parameters involved in these mechanisms are optimized by trial and error for a small number of stroke sequences. However, more systematic methods for dynamically controlling these para-meters during execution of arbitrary stroke sequences have to be investigated.

We now go deeper into the neuromotor significance of the proposed model of handwriting generation.

Motor preparation In their classic EEG studies of vol-untary motor action, Kornhuber and Deecke [ 17 ] found slow negative shifts in cortical potential much before the initiation of movement. This potential, termed the Bereitschafts poten-tial (BP), is believed to signify pre-movement preparation of motor cortical areas. Careful current dipole source analysis of BP has identified supplementary motor area (SMA) as a key player [ 18 ]. However, preparatory activity correspond-ing to movement direction has been found in many other brain areas including M1 [ 19 ], premotor cortex [ 20 ], pre-frontal cortex [ 21 ], the parietal cortex [ 22 ], and basal ganglia [ 23 ]. An interesting functional definition of motor prepara-tion emerges out of primate experiments by Churchland et al. (2006). This group hypothesizes that preparation is a pro-cess by which activity of the motor cortical neurons, random and variable in early stages of preparation, is progressively pushed into a limited region of the state space that is specific to a given movement. Data from premotor cortical neurons from primates appears to confirm their hypothesis [ 24 ]. Our model essentially conceives an idealization of this process in which preparation pushes the oscillatory state close to the standard state.

SMA and motor preparation From the above account SMA seems to compete with several other motor areas as a primary source of motor preparatory signals. Single cell recordings in primates revealed more marked preparation-related changes in SMA neurons than in neurons of M1 [ 25 ]. The question can be resolved if it can be shown that preparatory activity in SMA neurons precedes similar activity in M1. It has been shown that SMA neurons exhibiting preparatory activity can be identified to project to M1 [ 26 ]. Contrarily, it was also established that M1 neurons that exhibit preparatory activity receive inputs from SMA and not from thalamus or pari-etal cortex [ 27 ]. Such studies strongly implicate a role to SMA in motor preparation. However, perhaps SMA may not be solely responsible for motor preparation. Its preparatory action might involve interactions among subcortical struc-tures like basal ganglia, which are often implicated in motor timing functions.

Basal ganglia and motor timing Coordinating the relative timing of multiple streams of processing is crucial in both motor performance and sensory perception. Temporal pro-cessing in biological systems occurs over a range of time scales and is broadly classified into three categories: (1) cir-cadian timing, which corresponds to durations of the order of days, and handled by brain structures like suprachiasmatic nuclei, (2) interval timing, which corresponds to durations in the range of seconds to minutes, and coordinated primarily by corticostriatal interactions, and (3) millisecond timing, which obviously corresponds to millisecond durations, con-trolled by the cerebellum [ 28 ].

The role of basal ganglia in  X  X nterval timing X  appears to emerge from the dynamics of thalamo-cortico-striatal loops. In a model X  the striatal beat frequency (SBF) model [ 29 ] X  that highlights the timing function of basal ganglila, the corti-cal oscillators are assumed to increase synchrony just before movement onset and maintain the rhythm throughout the per-formance. The dopaminergic burst at trial onset could trig-ger synchronization of cortical oscillators according to SBF model [ 28 ]. Striatal neurons are tuned to respond to specific patterns of cortical oscillations [ 29 ].

SMA and basal ganglia in sequence generation Interaction between SMA and basal ganglia is believed to play a crucial role in learnt motor sequences [ 30 ]. It has been suggested that phasic activity of basal ganglia may act as a  X  X eset X  sig-nal to the SMA clearing the traces after one movement and preparing it for the consecutive movement [ 31 ].

The above description of cortico-striatal interaction in event timing and sequence generation is much in line with the treatment of these temporal processing mechanisms in our model of handwriting generation. The timing network (basal ganglia) sends a preparatory signal to the oscillatory layer (SMA) so as to induce a stable rhythm in the latter. Once a stroke is executed, the timing network waits for a specific state in the oscillatory layer and initiates execution of the next stroke. We have seen that other ways of determining stroke onset moment yielded suboptimal results. This intricate two-way interaction between the timing network and the oscilla-tory layer is strongly analogous to the above description of the role of basal ganglia in timing and sequence generation.
Handwriting variability Intrinsic variability in handwriting  X  X nd in fact all motor function X  X s a source of difficulty in robust handwritten character recognition. Handwriting vari-ability might seem to be a source of irritation if the goal is handwriting recognition, but one must remember that motor variability is most probably the enabling mechanism by which organisms acquire motor skills [ 32 ]. In the present work, we show that the time of stroke initiation is an important source of variability. Stroke onset must be precisely timed with respect to the evolving rhythm of the oscillatory layer. One might envisage that a lot of variability in real handwrit-ing originates in the variability in the duration between the time of termination of one stroke command , and the time of initiation of the next. However, such assertions stand to be confirmed or rejected by analysis of real handwriting sup-ported by data from underlying neural processes.

We are aware that our model has several simplifying assumptions. The input layer in our model which represents inputs from source areas of handwriting information (proba-bly language areas in parietal cortex or dorsolateral prefrontal cortex if the writing is driven by the contents of working memory) and the model X  X  output layer which represents all motor areas in motor hierarchy below SMA are obviously given a summary treatment. This is because one of the key motivations of the work is to highlight the role of SMA and basal ganglia in sequential behavior, specifically handwrit-ing. The timing network, which represents basal ganglia, is at the moment defined in terms of its inputs and outputs and not implemented as a neural network model. Further, the most important element of basal ganglia is perhaps the dopamine signal, which is thought to contain reward information, is also missing in the model. These necessities provide direc-tion to future extensions of the biological aspect of the present model.

In the applied domain, the potential of the present model to generate synthetic handwriting can probably be exploited as a generator of  X  X andwritten CAPTCHAs X  [ 33 ]. To this end, the present model has to be trained on a large database of online cursive data as described above. The model can also be trained on data from a specific individual. However, more efficient ways of deciding stroke onset and preparing the oscillator layer have yet to be investigated.
 Appendix The proof for the system governed by the equations ( i ), ( ii ) and ( iii ) has a  X  limit cycle  X . d x d t v = tanh ( X  x ) (ii) d s d t We can use Lienard X  X  theorem for existence of limit cycle. Follow the steps given below to convert ( i ), ( ii ) and ( iii )to Lienard X  X  system [ 34 ].

Differentiating ( i )  X  x = X   X  x +  X  sec h 2 ( X  x )  X  x  X   X  s (iv) Substituting ( ii ) and ( iii )in( iv )  X  x = X   X  x +  X  sec h 2 ( X  x )  X  x  X  (  X  s + tanh ( X  x )) (v) Using ( i ) and ( v )  X  x = X   X  x +  X  sec h 2 ( X  x )  X  x On rearranging  X  x +  X  x ( 2  X   X  sec h 2 ( X  x )) + ( x  X  I ) = 0(vi) is similar to Lienard X  X  equation  X  x +  X  xf ( x ) + g ( x f ( x ) = 2  X   X  sec h 2 ( X  x ) , and g ( x ) = x  X  I .
 Checking for the Lienard X  X  conditions: Let us assume I=0.

Both f ( x ) and g ( x ) are continuously differentiable for all x ; g (  X  x ) = X  g ( x ) for all x (i.e., g ( x ) is an odd function); g ( x )&gt; 0for x &gt; 0 ; f (  X  x ) = f ( x ) for all x (i.e., f ( x ) is an even function);
The odd function F ( x ) = exactly one positive zero at x = x o , is negative for 0 &lt; is positive and non decreasing for x &gt; x o , and F ( x as x  X  X  X  . (one can estimate xo from graph of F ( x )) .
So the system has a unique stable limit cycle surrounding the origin in the phase plane.
 References
