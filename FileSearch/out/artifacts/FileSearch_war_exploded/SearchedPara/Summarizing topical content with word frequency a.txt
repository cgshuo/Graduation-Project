 Jonathan M. Bischof jbischof@fas.harvard.edu Edoardo M. Airoldi airoldi@fas.harvard.edu Modern text analysis research has focused on discov-ering latent structure in the content of document col-lections to assist in critical tasks such as topical con-tent exploration, dimensionality reduction, and classi-fication. Most recently, topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have taken a probabilistic approach to this task by view-ing a document X  X  content as arising from a mixture of component distributions. Inferred components, re-ferred to as  X  X opics X , as they often capture thematic structure, characterize content in terms of the rela-tive frequency of within-component word usage (Blei., 2012). While inferred topics have proven to be a useful low-dimensional summary of a corpus X  content, recent work has documented a growing list of interpretability issues: they are often dominated by contentless  X  X top X  words (Wallach et al., 2009), are sometimes incoher-ent or redundant (Mimno et al., 2011; Chang et al., 2009), and typically require post hoc modification to meet human expectations (Hu et al., 2011).
 While most attempts to improve topical summaries to date involve changes to the models used to estimate relative frequency, we propose instead a new defini-tion of topical content that incorporates how words are used differentially across topics. If a word is com-mon in a topic, it is also important to know whether it is common in many topics or relatively exclusive to the topic in question. Both measurements are in-formative: nonexclusive words are less likely to carry topic-specific content, while infrequent words occur too rarely to form the semantic core of a topic. We therefore look for the most frequent words in the cor-pus that are also likely to have been generated from the topic of interest to summarize its content. In this approach we borrow ideas from the statistical litera-ture, in which models of differential word usage have been leveraged for analyzing writing styles in a super-vised setting (Mosteller &amp; Wallace, 1984; Airoldi et al., 2006), and combine them with ideas from the machine learning literature, in which latent variable and mix-ture models based on frequent word usage have been used to infer structure that often captures topical con-tent (McCallum et al., 1998; Blei et al., 2003; Canny, 2004; Ramage et al., 2009).
 Models based on topic-specific distributions over the vocabulary (such as LDA) cannot produce stable es-timates of differential usage since they only model the relative frequency of words within topics. They cannot regularize usage across topics and naively in-fer the greatest differential usage for the rarest fea-tures (Eisenstein et al., 2011). We introduce the gen-erative framework of word rate models that param-eterizes topic-specific word counts as unnormalized count variates whose rates can be regularized across topics as well as within them, making stable infer-ence of both word frequency and exclusivity possi-ble. Word rate models can be seen as a fully gen-erative interpretation of Sparse Topic Coding (Zhu &amp; Xing, 2011) that emphasizes regularization and inter-pretability rather than exact sparsity. We introduce a parallelized Hamiltonian Monte Carlo (HMC) estima-tion strategy that makes full Bayesian inference effi-cient and scalable.
 In this paper we focus on the case of document corpora for which meaningful topical structure is already avail-able, avoiding ambiguities about summarizing a topic space that is not semantically meaningful. We utilize large, annotated collections such as Reuters , New York Times , Wikipedia , and Encyclopedia Britannica for which human coders have already created a hierarchi-cal system of categories for end users. Working within the framework of word rate models, we develop Hierar-chical Poisson Convolution (HPC), a generative model for labeled corpora that exploits the known topic hier-archy in these collections to make focused comparisons of differential use between neighboring topics on the tree and incorporates a sophisticated joint model for topic memberships and labels in the documents. Since HPC is designed to infer an interpretable description of human-generated labels rather than find optimally predictive covariates as with Supervised LDA (Perotte et al., 2012), we restrict the topic components to have a one-to-one correspondence with the human-generated labels. We then infer a clear semantic description of these labels in terms of words that are both frequent and exclusive. The Hierarchical Poisson Convolution model is a gen-erative story for document collections whose topics are organized in a hierarchy. We refer to this structure in-terchangeably as a hierarchy or tree since we assume that each topic has exactly one parent and that no cyclical parental relations are allowed. Each document d  X  X  1 ,...,D } is a record of counts w fd for every fea-ture in the vocabulary, f  X  { 1 ,...,V } . The length of the document is given by L d , which we normalize by the average document length L to get l d  X  1 L L d . Doc-uments have unrestricted membership to any combi-nation of topics k  X  X  1 ,...,K } represented by a vector of labels I d where I dk  X  I { doc d belongs to topic k } . The HPC model leverages the known topic hierarchy by assuming that words are used similarly in neigh-boring topics. Specifically, the log rate for a word across topics follows a Gaussian diffusion down the tree. Consider the topic hierarchy presented in the right panel of Figure 1. At the top level,  X  f, 0 repre-sents the log rate for feature f overall in the corpus. The log rates  X  f, 1 ,..., X  f,J for high level topics are then drawn from a Gaussian centered around the cor-pus rate with dispersion controlled by the variance pa-rameter  X  2 f, 0 . From high level topics, we then draw the log rates for their children from another Gaussian cen-tered around their mean  X  f,j and with variance  X  2 f,j . This process is continued down the tree, with each parent node having a separate variance parameter to control the dispersion of its children.
 The variance parameters  X  2 fp directly control the local differential expression in a branch of the tree. Words with high variance parameters can have rates in the child topics that differ greatly from the parent topic p , allowing the child rates to diverge. Words with low variance parameters will have child rates close to the parent and so will be expressed similarly among the children. If we learn a population distribution for the  X  fp that has low mean and variance, it is equivalent to saying that most features are expressed similarly across topics a priori and that we would need a pre-ponderance of evidence to believe otherwise.
 Documents in the HPC model can contain content from any of the K topics in the hierarchy at varying proportions, with the exact allocation given by the vec-tor  X  d on the K  X  1 simplex. The model assumes that the count for word f contributed by each topic follows a Poisson distribution whose rate is moderated by the document X  X  length and membership to the topic; that is, w fdk  X  Pois( l d  X  dk  X  fk ). The only data we observe is the total word count w fd  X  P K k =1 w fdk , but the in-finite divisibility property of the Poisson distribution gives us that w fd  X  Pois( l d  X  T d  X  f ). These draws are done for every word in the vocabulary (using the same  X  ) to get the content of the document. 1 In labeled document collections, human coders give us an extra piece of information for each document, I d , that indicates the set of topics that contributed its content. As a result, we know  X  dk = 0 for all topics k where I dk = 0 and only have to determine how content is allocated between the set of active topics. The HPC model assumes that these two sources of in-formation for a document are not generated indepen-dently. A document should not have a high probability of being labeled to a topic from which it receives little content and vice versa. Instead, the model posits a la-tent K -dimensional topic affinity vector  X  d  X  X  (  X  ,  X  ) that expresses how strongly the document is associ-ated with each topic. The topic memberships and labels for the document are different manifestations of this affinity. Specifically, each  X  dk is the log odds that topic label k is active in the document, with I dk  X  Bernoulli(logit bels, the topic memberships are the relative sizes of the document X  X  affinity for the active topics and zero stricting each document X  X  membership vectors to the labeled topics is a natural and efficient way to generate sparsity in the mixing parameters, stabilizing inference and reducing the computational burden of posterior simulation.
 We outline the generative process in full detail in Ta-ble 1, which can be summarized in three steps. First, a set of rate and variance parameters are drawn for each feature in the vocabulary. Second, a topic affin-ity vector is drawn for each document in the corpus, which generate topic labels. Finally, both sets of pa-rameters are then used to generate the words in each document. For simplicity of presentation we assume that each non-terminal node has J children and that the tree has only two levels below the corpus level, but the model can accommodate any tree structure. 2.1. Estimands A word X  X  topic-specific frequency,  X  fk  X  exp  X  fk , is directly parameterized in the model and is regular-ized across words (via hyperparameters  X  and  X  2 ) and across topics. A word X  X  exclusivity to a topic,  X  f,k , is its usage rate relative to a set of comparison topics S :  X  f,k =  X  f,k / P j  X  X   X  f,j . A topic X  X  siblings are a natu-ral choice for a comparison set to see which words are overexpressed in the topic compared to a set of sim-ilar topics. While not directly modeled in HPC, the exclusivity parameters are also regularized by the  X  2 fp since if the child rates are forced to be similar then the  X  f,k will be pushed toward a baseline value of 1 / |S| . We explore the regularization structure of the model empirically in Section 4.
 Since both frequency and exclusivity are important factors in determining a word X  X  semantic content, a univariate measure of topical importance is a useful estimand for diverse tasks such as dimensionality re-duction, feature selection, and content discovery. In constructing a composite measure, we do not want a high rank in one dimension to be able to compensate for a low rank in the other since frequency or exclusiv-ity alone are not necessarily useful. We therefore adopt the harmonic mean to pull the  X  X verage X  rank toward the lower score. For word f in topic k , we define the FE fk score as the harmonic mean of the word X  X  rank in the distribution of  X  .,k and  X  .,k : where w is the weight for exclusivity (which we set to 0 . 5 as a default) and ECDF x function applied to the values x over the first index. We use a Gibbs sampler to obtain the posterior ex-pectations of the unknown rate and membership pa-rameters (and associated hyperparameters) given the observed data. Specifically, inference is conditioned on W , a D  X  V matrix of word counts, I , a D  X  K matrix of topic labels, l , a D -vector of document lengths, and T , a tree structure for the topics.
 Creating a scalable inference method is critical since the space of latent variables grows linearly in the num-ber of words and documents, with K ( D + V ) total un-knowns. Our model offers an advantage in that the posterior consists of two groups of parameters whose conditional posterior factors given the other. On one side, the conditional posterior of the rate and vari-ance parameters {  X  f ,  X  2 f } V f =1 factors by word given the membership parameters and the hyperparameters  X  ,  X  2 ,  X  and  X  2 . On the other, the conditional pos-terior of the topic affinity parameters {  X  d } D d =1 factors by document given the hyperparameters  X  and  X  and the rate parameters {  X  f } V f =1 .
 Conditional on the hyperparameters, therefore, we are left with two blocks of draws that can be broken into V or D independent threads. Using parallel computing software such as Message Passing Interface (MPI), the computation time for drawing the parameters in each block is only constrained by resources required for a single draw. The total runtime need not significantly increase with the addition of more documents or words as long as the number of available cores also increases. Both of these conditional distributions are only known up to a constant and can be high dimensional if there are many topics, making direct sampling impossible and random walk Metropolis inefficient. We are able to obtain uncorrelated draws through the use of Hamil-tonian Monte Carlo (HMC) (Neal, 2011), which lever-ages the posterior gradient and Hessian to find a dis-tant point in the parameter space with high probability of acceptance. HMC works well for log densities that are unimodal and have relatively constant curvature. We give step-by-step instructions for our implementa-tion of the algorithm in the Supplemental Material. 2 3.1. Block Gibbs Sampler To set up the block Gibbs sampling algorithm, we derive the relavant conditional posterior distributions and explain how we sample from each. 3.1.1. Updating tree parameters In the first block, the conditional posterior of the tree parameters factors by word: Given the conditional conjugacy of the variance pa-rameters and their strong influence on the curvature of the rate parameter posterior, we sample the two groups conditional on each other to optimize HMC performance. Conditioning on the variance parame-ters, we can write the likelihood of the rate parame-ters as a Poisson regression where the documents are observations, the  X  d ( I d ,  X  d ) are the covariates, and the l serve as exposure weights.
 The prior distribution of the rate parameters is a Gaus-sian graphical model, so a priori the log rates for each word are jointly Gaussian with mean  X  1 and precision matrix  X  (  X  2 ,  X  2 f , T ) which has non-zero entries only for topic pairs that have a direct parent-child relation-ship. 3 The log conditional posterior is: We use HMC to sample from this density.
 We know the conditional distribution of the variance parameters due to the conjugacy of the Inverse- X  2 prior with the normal distribution of the log rates. Specifically, if C ( T ) is the set of child topics of topic k with cardinality J , then 3.1.2. Updating topic affinity parameters In the second block, the conditional posterior of the topic affinity vectors factors by document: p ( {  X  d } D d =1 | W , I , l , {  X  f } V f =1 ,  X  ,  X  )  X 
Y We can write the likelihood of the word counts as a Poisson regression (now with the rates as covariates), with an independent contribution from the labels. The log conditional posterior for one document is: We use HMC to sample from this density. 3.1.3. Updating corpus-level parameters We draw the hyperparameters after each iteration of the block update. We put flat priors on these un-knowns so that we can learn their most likely values from the data.
 The log corpus-level rates  X  f, 0 for each word follow a Gaussian distribution with mean  X  and variance  X  2 . The conditional distribution of these hyperparameters is available in closed form: The variance parameters  X  2 fk independently follow an identical Scaled Inverse- X  2 with convolution parame-ter  X  and scale parameter  X  2 . The exact form of the conditional posterior of these hyperparameters is un-known, so we use HMC to sample from this density. The document-specific topic affinity parameters  X  d fol-low a normal distribution with mean parameter  X  and a covariance matrix parameterized in terms of a scalar,  X  =  X  2 I K . The conditional distribution of these hy-perparameters is available in closed form. For effi-ciency, we choose to put a flat prior on log  X  2 rather than the original scale, which allows us to marginalize out  X  from the conditional posterior of  X  2 : 3.2. Inference for unlabeled documents In order to classify unlabeled documents, we need to find the predictive distribution of the membership vec-tor I  X  d for a new document  X  d . Inference is based on the new document X  X  word counts w  X  d and the unknown pa-rameters, which we hold constant at their posterior expectation. Unfortunately, the predictive distribu-tion of the topic affinities  X   X  d is intractable without conditioning on the label vector since the labels con-trol which topics contribute content. We therefore use a simpler model where the topic proportions depend only on the relative size of the affinity parameters: The predictive distribution of this simpler model fac-tors into tractable components: It is then possible to find the most likely  X   X   X  the evidence from w  X  d alone. We analyze the fit of the HPC model to Reuters Cor-pus Volume I (RCV1), a large collection of newswire stories. First, we demonstrate how the variance pa-rameters  X  2 fp regularize the exclusivity with which words are expressed within topics. Second, we show that regularization of exclusivity has the greatest effect on infrequent words. Third, we explore the joint pos-terior of the topic-specific frequency and exclusivity of words as a summary of topical content, giving special attention to the upper right corner of the plot where words score highly in both dimensions. We compare words that score highly on the FREX metric to top words scored by frequency alone, the current practice in topic modeling. Finally, we compare the classifica-tion performance of HPC to baseline models.
 RCV1 is an archive of 806,791 newswire stories from a twelve-month period in 1996-1997. 4 As described in Lewis et al. (2004), Reuters staffers assigned sto-ries into any subset of 102 hierarchical topic cate-gories. In the original data, assignment to any topic required automatic assignment to all ancestor nodes, but we removed these redundant ancestor labels since they do not allow our model to distinguish intentional assignments to high level categories from assignment to their offspring. We preprocessed document tokens with the Porter stemming algorithm (getting 300,166 unique stems) and chose the most frequent 3% of stems (10,421 unique stems, over 100 million total tokens) for the feature set. 5 The Reuters topic hierarchy has three levels that di-vide the content into finer categories at each cut. At the first level, content is divided between four high level categories: three that focus on business and mar-ket news (Markets, Corporate/Industrial, and Eco-nomics) and one grab bag category that collects all re-maining topics from politics to entertainment (Govern-ment/Social). The second level provides fine-grained divisions of these broad categories and contains the terminal nodes for most branches of the tree. For example, the Markets topic is split between equity, bond, money, and commodity markets at the sec-ond level. The third level offers further subcategories where needed for a small set of second level topics. For example, the Commodity Markets topic is divided between agricultural (soft), metal, and energy com-modities. We present a graphical illustration of the Reuters topic hierarchy in Figure 2. 4.1. How the differential usage parameters A word can only be exclusive to a topic if its expression across the sibling topics is allowed to diverge from the parent rate. Therefore, we would only expect words with high differential usage parameters  X  2 fp at the par-ent level to be candidates for highly exclusive expres-sion  X  fk in any child topic k . Words with child topic rates that cannot vary greatly from the parent should have nearly equal expression in each child k , meaning  X  fk  X  1 C for a branch with C child topics. An im-portant consequence is that, although the  X  fk are not directly modeled in HPC, their distribution is regular-ized by learning a prior distribution on the  X  2 fp . This tight relation can be seen in the HPC fit. Figure 3 shows the joint posterior expectation of the differen-tial usage parameters in a parent topic and exclusivity parameters across the child topics. Specifically, the left panel compares the rate variance of the children of Markets from their parent to exclusivity between the child topics; the right panel does the same with the two children of Performance, a second-level topic under the Corporate category. The plots have similar patterns. For low levels of differential expression, the exclusivity parameters are clustered around the base-line value, 1 C . At high levels of child rate variance, words gain the ability to approach exclusive expres-sion in a single topic. 4.2. How frequency modulates regularization One of the most appealing aspects of regularization in generative models is that it acts most strongly on the parameters for which we have the least information. In the case of the exclusivity parameters in HPC we have the most data for frequent words, so for a given topic the words with low rates should be least able to escape regularization of their exclusivity parameters by our shrinkage prior on the parent X  X   X  2 fp . Figure 4 shows for two topics the joint posterior expectation of each word X  X  frequency in that topic and its exclusivity compared to sibling topics (the FREX plot). The left panel features the Science and Technology topic, a child in the grab bag Govern-ment/Social branch, and the right panel features the Research/Development topic, a child in the Corporate branch. The overall shape of the joint posterior is very similar for both topics. On the left side of the plots, the exclusivity of rare words is unable to sig-nificantly exceed the 1 C baseline. This is because the model does not have much evidence to estimate usage in the topic, so the estimated rate is shrunk heavily toward the parent rate. However, we see that it is pos-sible for rare words to be underexpressed in a topic, which happens if they are frequent and overexpressed in a sibling topic. Even though their rates are similar to the parent in this topic, sibling topics may have a much higher rate and account for most appearances of the word in the comparison group.
 4.3. Frequency and Exclusivity as a two Words in the upper right of the FREX plot X  X hose that are both frequent and highly exclusive X  X re of greatest interest. These are the most common words in the corpus that are also likely to have been generated from the topic of interest (rather than similar topics). These high-scoring words can help to clarify content even for labeled topics. For example, in the Science and Technology topic (shown in detail in Figure 5), we see almost all terms are specific to the American and Russian space programs.
 We also compute the Frequency-Exclusivity (FREX) score for each word-topic pair, a univariate summary of topical content that averages performance in both dimensions. In Table 2 we compare the top FREX words in three topics to a ranking based on frequency alone, which is the current practice in topic modeling. For context, we also show the immediate neighbors of each topic in the tree. The topic being examined is in bolded red, while the borders of the comparison set are solid. The Defense Contracts topic is a special case since it is an only child. In these cases, we use a comparison to the parent topic to calculate exclusivity. By incorporating exclusivity information, FREX-ranked lists include fewer words that are used simi-larly everywhere (such as said and would ) and fewer words that are used similarly in a set of related topics (such as price and market in the Markets branch). One can understand this result by comparing the rankings for known stop words from the SMART list to other words. In Figure 6, we show the maximum ECDF ranking for each word across topics in the distribution of frequency (left panel) and exclusivity (right panel) estimates. One can see that while stop words are more likely to be in the extreme quantiles of frequency, very few of them are among the most exclusive words. This prevents general and context-specific stop words from ranking highly in a FREX-based index.
 4.4. Classification performance We compare the classification performance of HPC with SVM and a LDA+logit classifier, which fits a logistic regression using LDA topic loadings as covari-ates. All methods were trained on a random sam-ple of 15% of the documents using the 3% most fre-quent words in the corpus as features. These fits were used to predict memberships in the withheld docu-ments, an experiment we repeated ten times with a new random sample as the training set. We used a stratified sampling technique to get a balanced sam-ple (across topics) for training, validation, and test partitions with a 15/25/60 split, respectively. We fit the three models to each training set and then used the validation set to calibrate a threshold (except for SVM). Finally, we used the fit from the training set and the threshold from the validation set to predict topic memberships in the test set. Table 3 shows the results of our experiment, using both micro averages (every document weighted equally) and macro aver-ages (every topic weighted equally). HPC performs better than LDA+logit on most metrics but does not dominate SVM, suggesting that there is a tradeoff be-tween predictive performance and interpretability. While HPC was developed for the specific case of hi-erarchically labeled document collections, the frame-work of word rate models can be readily extended to other types of document corpora. For labeled corpora where no hierarchical structure is available, one can use a flat hierarchy that treats all topics as siblings. For corpora where no labeled examples are available, a simple word rate model with a flat hierarchy and dense topic membership structure could be employed to get more informative summaries of inferred topics. In either case, this framework could be combined with non-parameteric Bayesian models that infer hierarchi-cal structure on the topics (Adams et al., 2010). We expect that models based on rates will play an impor-tant role in future work on text summarization. The HPC model can also be leveraged to semi-automate the construction of topic ontologies targeted to specific domains, for instance, when fit to compre-hensive human-annotated corpora such as Wikipedia , The New York Times , Encyclopedia Britannica , or databases such as JSTOR and the ACM repository . By learning a probabilistic representation of high qual-ity topics, HPC output can be used as a gold standard to aid and evaluate other learning methods.
 Targeted ontologies have been a key factor in moni-toring scientific progress in biology (Ashburner et al., 2000; Kanehisa &amp; Goto, 2000). A hierarchical ontol-ogy of topics would lead to new metrics for measuring progress in text analysis. It would enable an evalua-tion of the topical content of any collection of inferred topics, thus finally allowing for a quantitative compar-ison among the output of topic models. Current eval-uations are qualitative, anecdotal and unsatisfactory; for instance, authors argue that lists of most frequent words describing an arbitrary selection of topics in-ferred by a new model make sense intuitively, or that they are better then lists obtained with other models.
