 For text classification or regression task, documents need to be represented by a fix-length feature vector, on which machine learning algorithms can operate. The most naive but commonly used approach is bag-of-words representations, which represent each document with a feature vector by counting its containing unigrams, bigrams or trigrams and then leveraging the vectors based on Tf-idf scheme [ 34 ]. Tf-idf presentations have some appealing features notably its basic identification of sets of words that are discriminative for documents, but also come with severe shortcomings: the simple word-counting statistics are incapable of grasping in-depth information such as word semantics, and usually end up with extremely high dimensional vector representations.
 Distributional semantic models(DSM) [ 44 ] offer a way of document represen-tation by approximating the meaning of words with vectors that keep track of the patterns of co-occurrence of the words in a corpus. It assumes that semanti-cally related words should occur in similar contexts [ 10 ]. Hybrid DSM methods based on traditional topic models are also developed for various tasks [ 14 , 45 ]. However, DSM are psychological models of how we humans acquire and use semantic knowledge. However we can rely not only on linguistic context, but also on our rich perceptual experience [ 21 ].
 let Allocation (LDA) [ 3 ], which represents each document as a probability over a set of components characterized as different distributions over vocabularies. LDA serves as a dimensionality reduction technique. As it first does the word-clustering based on document-level word co-occurrence, it can represent each document with a vector based on more condensed but more meaningful components, called  X  X op-ics X . LDA suffers from the shortcomings like ignorance of word orders, features, and word-similarity. While a bunch of approaches (e.g., [ 31 , 50 ]) have been pro-posed to address the aforementioned shortcomings, the improvements are limited and these approaches often end up with very complicated learning procedure. have been successfully applied to various natural language processing tasks. Such deep architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic advantage of the deep learning framework is that it frees researchers from feature engineering, since its representations are emergent. Furthermore, recent research has begun looking at higher level distributed representations that transcend the token level such as discourse-level [ 12 ].
 so do sentences combine to form the meaning of paragraphs and then docu-ments, in this paper, we propose a supervised framework that learns continuous distributed condensed vector representations at document level for text classifi-cation and regression task. Our approach is hierarchical and is founded on two basic learning structures in deep learning: recursive neural network and recurrent neural network.
 tential compositionality operation is performed relying on sentence parse trees from recursive neural network. The distributed representation for each node in the sentence parse tree is computed in a bottom-up fashion as in [ 42 ] until the root is reached.
 ment which is based on a recurrent neural network architecture that is useful for sequences [ 25 , 43 ]. The distributed vectors for paragraphs are obtained by sub-sequently convoluting its containing sentences with the input from the previous step. Document-level representations are obtained by adding up the vectors of its containing paragraphs in the similar way based on recurrent neural network. resentation in previous work, it is natural to extend it to distributed paragraph and document representation to represent the meaning of texts and benefit other NLP tasks dealing with documents. Recursive neural network requires structured inputs and for text processing, it relies heavily on the parsing results. Document parsing is uncompetitive compared with sentence parsing. So we employ the recursive neural network for sentence level representation but recurrent neural network for paragraph and document level.
 Our approach is a task-specific framework where parameters involved in intra-and inter-sentence compositionality are optimized through the task-specific tun-ing procedure, which can also be treated as a feature selection procedure. We experiment our approach (Compound RNN) on different document-classification tasks, i.e., binary classification, multi-class classification and regression. Exper-imental results illustrate the effectiveness of our model over existing baselines. 2.1 Recursive Neural Networks Recursive neural networks (RNN), as one kind of deep learning frameworks, was first proposed in [ 9 ]. Recursive framework relies and operates on struc-tured inputs (e.g., parse tree) and computes the representation of each par-ent based on its children iteratively in a bottom-up fashion. To tailor different task-specific requirements, some variations of RNN have been proposed such as Recursive Neural Tensor Network [ 42 ] that allows the model to have greater interactions between the input vectors and Matrix-Vector RNN [ 39 ] which rep-resents every word as both a vector and a matrix. There are also some work addressing the feature weight tuning for recursive neural networks [ 17 ]tomake the model emphasize more on important information. Tasks have benefited from recursive framework including parsing [ 19 , 40 ], sentiment analysis [ 42 ], machine translation [ 20 ], textual entailment [ 4 ] and paraphrase detection [ 38 ]. 2.2 Recurrent Neural Networks takes a collection of tokens, phrases or sentences as a sequence and incorporates information from the past (i.e. preceding tokens) to get the current output. Specifically, at each step, recurrent network takes both the output of previous step and the current token as input, convolutes the inputs, and forwards it to the next step. It has been successfully applied to tasks such as language modeling [ 25 ] or spoken language understanding [ 22 ]. Recurrent network does not need external deeper structure (e.g., parse tree) and is able to preserve the embedding dimension when convoluting different number of components. However, in recurrent framework, long unit dependencies might be difficult to capture and the framework suffers from the vanishing gradient problem. 2.3 Distributed Representations Both recurrent and recursive neural networks require vector representations for input tokens. Distributed representations for words were first proposed in [ 33 ] and have been used for statistical language modeling [ 8 ]. Various deep learning architectures have been explored to learn these embeddings in an unsupervised capabilities and able to capture the semantic meanings for specific tasks. These vector representations capture interesting semantic relationships (or to some extent) such as King  X  man  X  Queue  X  woman and have benefited multiple NLP applications such as name entity recognition, tagging or machine translation (e.g., [ 6 , 51 ]).
 beyond sentences. [ 39 ] applied the matrix-vector RNN model to learn composi-tional vector representations for phrases and sentences. But their experiments mainly focused on phrase and sentence level classification like predicting senti-ment distributions of adverb-adjective pairs etc. [ 12 ] extends the representation to sentence-level and discourse level by convoluting representations in a recurrent neural network. But their improvements are trivial. sentations for paragraphs. The basic idea is that the distributed representation of a piece of text should be able to predict the following word. Experiments on various tasks verify the effectiveness of such methods. But they concatenate sentence vectors, even paragraph vectors with word vectors. In this paper, we regard word vectors, sentence vectors and paragraph vectors as different layers in the model. 2.4 Approaches to Classification and Regression As this paper mainly focuses on applying deep architectures to document clas-sification tasks, we just give a brief review of other approaches to classification and regression. Many models can be used for the classification and regression task [ 36 ], such as k -NN, SVM, Naive Bayes, Neural Networks, ect., as long as documents can be represented in appropriate forms. To present documents, pop-ular methods use topic models. Notable ones are pLSI [ 11 ], LDA [ 3 ] and some variations of LDA, such as sLDA [ 2 ], L-LDA [ 32 ]etc. In this section, we describe how we compute the distributed representation for a given sentence based on its parse tree structure and containing words. As the details can be found in a bunch of early work (e.g., [ 42 ]), we try to make this section brief and skip the details for brevity.
 Each token w within the sentence is associated with a specific vector embedding e Here the word embeddings are initialized by using word representations taken from RNNLM [ 24 , 26 ]. The dimension of embeddings is 80. We wish to compute the vector representation h s for current sentence, where h Parse trees are obtained from Stanford Parser 1 . For a given parent p in the tree and its two children c 1 (associated with vector representation h (associated with vector representation h c 2 ), standard recursive network calcu-lates the distributed vector for parent p as follows: where [ h c 1 ,h c 2 ] denotes the concatenating vector for children representations h and h c 2 . W is a K  X  2 K matrix and b is the 1  X  K bias vector. f ( function.
 Standard recurrent framework uses the same (tied) weights W at all nodes to compute the vector. This requires the compositionality function to be extremely powerful as it has to combine phrases with different syntactic roles, which is usu-ally unrealistic. Several approaches have been proposed to address such weakness including Matrix-Vector RNN [ 39 ] or Recursive Neural Tensor Network [ 42 ]. In this work, we adopt a simple alternative where instead of using a single com-positionality matrix W , we associate each of the sentence roles (i.e., VP, NP or NN) with a specific compositionality matrix (i.e, W VP , W and W c 2 denote the matrices associated with children c 1 roles. Then the convolution is given by: where [ W c 1 ,W c 2 ] denotes the K*2K dimensional concatenating matrix for W In this section, we illustrate how we get the distributed vector representation for a given document based on its contained sentences, of which the distributed representations have already been obtained in the Sentence Model Section. Doc-ument d is comprised of a sequence of paragraphs D = { L 1 N denotes the number of paragraphs within the document. Each paragraph is comprised of a sequence of sentences L = { s 1 ,s 2 , .., s number of sentences within the paragraph. To obtain the vector representation h for paragraph L , we turn to recurrent neural network, which successively takes in sentence s t at step i , combines its vector representation h input h t  X  1 L from step i  X  1, calculates the resulting current embedding h passes it to the next step. The convolution can be summarized as follows: where W L and V L are K  X  K matrixes. b L denotes K  X  1 bias vector and f = tanh is a standard element-wise nonlinearity. To note, the calculation for representation at time t = 1 is given by: where h 0 denotes the global sentence starting vector for paragraphs. To note, for documents with only one paragraph, their corresponding distributed vectors are obtained by using the strategy just described and no convolution between paragraphs is needed.
 distributed representations as follows: given the vector presentation for its con-taining paragraphs { h L 1 ,h L 2 , ..., h L N takes as input the vector representation h L t for current paragraph L it with former input h t  X  1 d from step i  X  1, calculates h step. where W d and V d are K  X  K matrixes. b d denotes K  X  1 bias vector. We train our classifier regarding three types of document classification tasks: binary classification, multi-class classification and regression. 5.1 Binary Classification For binary classification, each document d is associated with a 0/1 binary valued variable t d . For classification purpose, given the document distributed vector h we first generate a scalar using linear function U T binary it into [0,1] possibility space using a sigmoid function, as given by: where U binary is a K  X  1 dimensional vector and b binary g (  X  ) denotes the sigmoid function.
 the training set is given by: The regularization part is parameterized by Q that pushes the weights from  X =[ { W tag } , W L , W d , U binary ] to zero. For any parameter  X  to optimize, the derivative of J binary ( d ) with respect to  X  is given by: where  X  X  ( t d =1)  X  X  can be further obtained from the standard back-propagation. 5.2 Multi-Class Classification For multi-class classification task, each document d is associated with a class tag t , which takes value from [1,2,3,...T], where T denotes the number of potential classes. We associate each document d with a T dimensional binary vector R which is the ground truth vector with a 1 at the correct label t entries 0. The prediction task is done through a softmax classifier. U of assigning class i th to the current document.
 For a given set of training document D , the cost function for multi-class classification is given by: Similar to binary classification, Q is the regularization parameter that pushes to optimize, the derivative of J multi ( d ) with respect to  X  is given by: where  X  denotes the Hadamard product between the two vectors. 5.3 Regression For a given document d  X  D associated with regression tag t rating, website popularity), the deep learning framework makes prediction document d as follows: where U regression denotes the K dimensional vector. Parameters are estimated through minimizing the following cost function: 5.4 Optimization The derivative for each parameter can be obtained from standard backpropaga-minibatches, which is widely applied in deep learning literature (e.g.,[ 30 , 38 ]). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, let g i  X  denote the subgradient at time step t for parameter  X  i obtained from backpropagation, the parameter update at time step t is given by: where  X  denotes the learning rate. 5.5 Initialization Many tricks have been reported regarding the initialization of neural networks [ 16 ]. We employed only two of them. The initialization of W were done according to the fan-in of the layer by randomly drawing from uniform distribution [ where = 6 K +2  X  K . For elements involved in recurrent network in paragraph and document compositionality, i.e., W L ,W d , we initialize them by randomly All bias vectors are initialized as 0.
 embeddings using vectors pre-trained from a large unlabeled data corpus instead of random initialization (e.g.,[ 48 ]). We therefore initialize word embeddings using word representations taken from RNNLM [ 24 , 26 ]. The dimension of embed-dings is 80. In this section, we show the experimental results regarding the three aforemen-tioned document classification problems: multi-class classification, binary classi-fication and regression. 6.1 Multi-class Classification We perform multi-class classification task on the 20 Newsgroup dataset data set has a balanced distribution over the 20 categories. The test set is com-prised of 7,505 documents in total, with the smallest category containing 251 documents and the largest category containing 399 documents. The training set has a total number of 11,269 documents, the smallest and the largest categories of which contain 376 and 599 documents respectively. The naive baseline that predicts the most frequent category for all the test documents has the classi-fication accuracy 0.0532. For comparison, we employ the following models as baselines:  X  tf-idf+SVM : Each document is represented as vector of unigram based  X  LDA+SVM : We run variational EM algorithm for LDA using package the 18,828 documents with topic number being set to 110, as suggested in [ 13 , 50 ]. Each document is therefore represented by a 110 dimensional vector.
SVM multi-class takes as input these vectors as training data.  X  sLDA : A supervised version of LDA for multi-class classification [ 46 ]. In addition to the aforementioned baselines, we also implement a simplified ver-sion of our proposed model Simplified which uses a unified convolution matrix when operating on parse tree structure. The prediction accuracy regarding dif-ferent approaches is reported in Table 1 . As bag-of-words models consider nei-ther how each sentence is composed (e.g., word ordering) nor word semantics, it obtained the worst performance. LDA based models can, to some extent, take into consideration the latter (word semantics) by the word pre-clustering but fail to consider the former. The deep learning approaches ( Compound RNN and Simplified ) significantly outperform the other baselines. The original version takes into account different types of compositionality, performing better than the Simplified version, which uses one unified compositionality matrix when operating on sentence parse trees. As Compound RNN consistently outper-forms Simplified version, the results for Simplified version are excluded in the later parts for brevity.
 6.2 Binary Classification News Group Classification: We first perform binary classification evaluation on distinguishing postings of newsgroup alt.atheism and talk.religion.misc from the 20 news groups, as Lacoste-Julien et [ 13 ] did. The training set contains 856 documents with a split of 480/376 over the two categories, and the test set contains 569 documents with a split of 318/251 regarding the two categories. The baselines we explore include tf-idf+SVM, LDA+SVM, and two versions of supervised LDA: the regression version of s-LDA [ 2 ] which uses the binary representation (0/1) of the classes, and uses a threshold 0.5 to make predic-tion, and multi-class sLDA as described in the previous section. For LDA based approaches, topic numbers are set to 30 and 35 as suggested in [ 13 , 50 ]andwe report the better performance.
 based approaches and the native tf-idf approach. To note here, our deep learn-ing approach does not yield significant performance boosting when compared with other sophisticatedly developed LDA-based baselines, such as MedLDA [ 49 ] (reported accuracy around 0.81) and DiscLDA [ 13 ] (reported accuracy around 0.80).
 Truthful vs Deceptive Review Classification: We perform binary classification task on the hotel reviews for 20 Chicago hotels described in [ 28 ]. The dataset contains 400 deceptive fake reviews (positive) solicited from Amazon Mechani-cal Turk and 400 truthful reviews (negative). The algorithm makes prediction regarding whether a given review is deceptive or truthful. We perform 5-fold cross-validation experiments.
 features suggested in [ 28 ]: LIWC+SVM, Unigram+SVM, Bigram+SVM, LIWC+Bigram+SVM and LDA+SVM. LIWC, short for the Linguistic Inquiry and Word, is an automatic analysis tool which counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimen-sions. For LDA+SVM, we run gibbs sampling of LDA on the 800 documents with topic number ranging from 2 to 20 at interval of 2 and report the best performance. We report performance regarding each models in Table 3 . Among the baselines, LIWC+Bigram setting achieve the best performance. The deep learning approach has an absolute improvement of 2 . 6% in terms of accuracy over the LIWC+Bigram setting. 6.3 Regression Review Rating Prediction: For regression evaluation, we first evaluate Compound RNN on the movie review data set introduced in [ 29 ], which contains movie reviews paired with the number of stars given. We treat the rating prediction task as a regression problem and use the same settings as in [ 2 , 49 ]. The evaluation criterion is predictive R 2 , which is defined as one minus the mean squared error divided by the data variance as defined in [ 2 ]. where t d and  X  t d are the true and estimated ratings of document d . mean of review ratings on the whole data set.
 We run experiment with default settings as described in [ 2 ]. We employ the following approaches as baselines:  X  sLDA : the supervised version of LDA introduced in [ 2 ].  X  LDA+SVR : we train the Support Vector Regression (SVR) [ 37 ]onLDA topic representations using the LIBSVM toolkit [ 5 ].  X  L 1 regularized least-squares regression (Lasso) as suggested in [ 2 ], which uses each document X  X  empirical distribution over words as its lasso covariates.
 Topic number is set to 30 for LDA-based approaches. And The pR LDA+SVR, sLDA, Lasso, and Compound RNN are 0.348, 0.502, 0.457 and 0.552 separately. As we can see, the Compound RNN again outperforms the standard baselines. To note, the performance of the Compound RNN is compa-rable to other derivations of LDA such as MedLDA [ 49 ] (reported pR 0.55). In this paper, we propose a deep learning based framework for supervised doc-ument classification and regression. The classification task relies on document distributed representation obtained on the basis of recursive and recurrent neu-ral networks. Our framework is task-specific as it does not aim to learn the general representations for sentences, paragraphs or documents but are based on the task-specific parameters in intra-and inter-sentence convolution which are optimized and guided by the optimization function, which can be viewed as a feature selection process specific for different tasks. Experiments on several text classification tasks such as binary classification, multi-class classification and regression demonstrate that the proposed algorithm is competitive and signifi-cantly outperforms prevailed existing baselines such as LDA and bag-of-words.
