 As information volume in enterprise systems and in the Web grows rapidly, how to accurately retrieve information is an important research area. Several corpus based smoothing techniques have been proposed to address the data spar-sity and synonym problems faced by information retrieval systems. Such smoothing techniques are often unable to discover and utilize the correlations among terms. 
We propose CVS, a Correlation-Verification based Smooth-ing method, that considers co-occurrence information in smooth-ing. Strongly correlated terms in a document are identi-fied by their co-occurrence frequencies in the document. To avoid missing correlated terms with low co-occurrence fre-quencies but specific to the theme of the document, the joint distributions of terms in the document are compared with those in the corpus for statistical significance. 
A common approach to apply corpus based smoothing techniques to information retrieval is by refining the vector representations of documents. This paper investigates the effects of corpus based smoothing on information retrieval by query expansion using term clusters generated from a term clustering process. The results can also be viewed in light of the effects of smoothing on clustering. 
Empirical studies show that our approach outperforms previous corpus based smoothing techniques. It improves retrieval effectiveness by 14.6%. The results demonstrate that corpus based smoothing can be used for query expan-sion by term clustering. H.3.1 [Information Storage and Retrieval]: Content Text mining, smoothing, query expansion, term clustering, information retrieval 
Information volume in enterprise systems and in the Web is growing rapidly. Text mining research facilitates users to effectively mine valuable information from huge amount of data by scoring documents relevant to user queries ([1]). One of the widely used approaches employs a unigram model to represent documents. Other methods include the use of a vector space model, in which a document and a query are modelled as vectors of terms. The dot product of the vectors measures the relevancy of the document to the query. 
These models suffer from the data sparsity problem --the dimension of terms is huge such that the vectors of doc-uments and queries are sparse. The problem is escalated by the synonym problem where documents containing syn-onyms to terms in a query are often not assigned high scores by the dot product approach. 
Smoothing techniques overcome these problems by associ-ating feature terms in a given document with related terms that do not appear in the document. A feature terra of a document is a term with non-zero occurrence frequency and is relevent to the theme of the document. A related terra does not occur in the document but is related to its theme. The term smoothing refers to the adjustment of the maxi-mum likelihood estimator of a language model ([26]). Con-sequently, a document can be represented more accurately with both the feature terms and related terms. This paper proposes CVS, a Correlation-Verification based Smoothing method that is based on corpus statistics. CVS considers joint distributions of terms to identify significant permission and/or a fee. SIGKDD 2002, Edminton, Canada Copyright 2002 ACM ACM 1-58113-567-X/02/0007 ...$5.00. 
Most smoothing techniques rely on corpus statistics, rel-evance feedback and lexical references. Prior corpus based smoothing techniques refine document vectors with the dis-tribution of individual terms in a corpus. CVS uses co-occurrence information of terms to smooth a document. Terms that co-occur frequently in documents are regarded as strongly correlated. To avoid missing terms with low co-occurrence frequencies in a document but related to its theme, the joint distributions of the terms in the document and the corpus are compared for statistical significance. 
We also study how corpus based smoothing techniques can be applied to enhance recall in information retrieval. Infor-mation retrieval can be improved by adjusting weights of related terms in document vectors for more accurate repre-sentation or by expanding a query to include related terms. Query expansion is a more transparent process because it al-lows users to view and modify terms in an expanded query. Smoothing techniques based on lexical references and rele-vance feedback typically lend themselves naturally to query expansion. Prior work on corpus based smoothing meth-ods applies smoothing to refine document vectors. CVS combines corpus based smoothing method with query ex-pansion. 
A class of query expansion methods identify correlated terms by their joint distributions and expand a query di-rectly with correlated terms. These methods do not consider the correlation of term pairs in light of the relationship with other term pairs in the corpus. Our approach augments these methods by selecting better terms for query expan-sion by clustering. As opposed to document clustering ([9, 19]), we consider term clustering ([22, 15, 23, 3]). Term clustering groups related terms into a hierarchy of clusters by minimizing intra-cluster distances and maximizing inter-cluster distances. User queries are expanded by terms in the clusters that contain the terms in the query. A hierar-chy of clusters gives a user guidance in adjusting the degree of query expansion based on depths of clusters. 
Empirical studies show that our approach outperforms two representative corpus based smoothing techniques. It improves retrieval effectiveness by 14.6% as compared to 6.8% and 10.8% by other methods. The studies also demon-strate that corpus based smoothing can be applied to im-prove information retrieval by term clustering and query ex-pansion. 
This paper is organized as follows, Section 2 introduces prior work; Section 3 discusses CVS; Section 4 applies cor-pus based smoothing techniques to query expansion by term clustering; Section 5 presents results of empirical studies; Section 6 comments on CVS method and discusses several variants; and finally, Section 7 concludes the paper. 
Various smoothing techniques have been proposed in the literature. One class of techniques use purely corpus statis-tics to refine term distributions in documents ([26, 6, 10]). Our approach belongs to this category, but differs from them by considering correlations among terms in addition to the distributions of individual terms. Another class of smooth-ing techniques uses lexical references to expand a query with additional, lexically related terms ([24, 16]). A drawback of this approach is that a lexical reference cannot capture idiosyncrasies of a corpus. Prior results of using lexical ref-erences for query expansion are not encouraging. The third class of techniques use relevance feedback to expand a query ([18, 25]). Relevance feedback is effective only if accurate relevance feedback information is available, which requires user intervention. 
Most traditional work on comparing term distributions in 20]). Kilgarriff ([11]) finds that X 2 test identifies too many common terms as distributed differently in the corpora. He proposes to use Mann-Whitney rank test to unveil the sta-tistical significance in term distributions. Such rank tests require sufficiently big corpora. But since most documents are short, 1 rank tests are almost inapplicable without major modifications. 
A class of query expansion techniques directly expand a query with correlated terms ([2, 5, 4, 7]). A drawback is that relationships among other term pairs in the corpus. For ex-ample, the terra "insurance" is strongly correlated with the terms "business directory" and "instant insurance quote", but "business directory" may be correlated with terms not related to "insurance". Expanding "insurance" with "in-stant insurance quote" is better than with "business direc-tory". This paper addresses this shortcoming by using term clusters obtained from term clustering to expand a query. The correlations among all term pairs are captured in the clustering process. 
In this section, we describe two smoothing methods that are representative of corpus based smoothing techniques. We also formulate the proposed method, CVS. 
In this papex, we make the naive Bayes Assumption --a term's occurrence in a document is independent of any other term. We use the multinomial model --a document is represented by its terms and occurrence frequencies ([17]). Statistics of terms in a given document are used to decide whether the document is relevant to a query or whether it belongs to a certain cluster. Therefore, it is critical to calculate such statistics accurately. Smoothing can improve the accuracy on estimating such statistics. 
Let C be a corpus of documents, d a document in the cor-pus, and T the; set of terms selected to model documents in the corpus. Let f(tld ) be the observed occurrence frequency of the term t in document d. 
The conditional probability of having the term t in the document d, denoted as p(tld), is estimated by: The probability of having the term t in the corpus C, de-noted as p(tlC), is given by: We choose to study the Jelinek-Mercer method and the Dirichlet method ([26, 10]) because of their simplicity and yet being representative of various corpus based smoothing techniques. 
The Jelinek-Mercer method adjusts the probability of a term in a document by a linear interpolation on the observed probabilities of the term in the document and in the corpus: where A is a smoothing parameter,/3 is a scaling factor to ensure that all probabilities sum to 1. 2 1The average web page size is only 1-2KB. The Dirichlet method is a general formulation for the 
Laplace method. It adjusts the probability of a term in a document using the multinomial distribution to model a document: where A is a smoothing parameter, ~ is a scaling factor to ensure that all probabilities sum to 1. Previous corpus based methods consider only the distri-butions of individual terms. We propose CVS, Correlation-
Verification Based Smoothing, that considers the co-occurrence information as well as distributions of individual terms. The probability of a term t in the document d can be ad-justed by its correlation to terms observed in the document. 
Figure 1: A cluster hierarchy on Reuters corpus gener-ated by term clustering. An example of clusters of depth 1 is {export, exporter}. s, t in the mega-document C is: sure that all probabilities sum to 1, the predicate square values are based on a continuous distribution, Yate's component is the contribution by smoothing based on cor-relation information. By Bayes rule, we can estimate p(s, tiC ) as: where f(s, rid ) is the co-occurrence frequency of the terms s, t in document d and is defined as min{f(sld), To avoid over-smoothing (terms irrelevant to the theme of a document are assigned non-zero weights), a term pair is used to adjust a document vector or~ if its co-occurrence frequency in the entire corpus exceeds a threshold. While fixing the threshold to a predefined value by trial-and-error can efficiently sift some correlated terms, terms that are strongly correlated to the specific theme of a document but with relatively low co-occurrence frequencies can be missed. 
Therefore, we complement the approach with a statistical test of significance. Following Klas and Fuhr ([14]), we treat a corpus as a mega-document which is a concatenation of all documents. We use the same symbol C to represent the mega-document of the corpus C. We employ X 2 test to mea-sure the statistical difference in joint distributions of terms s, t in the document d and C to identify relevant terms to the document with low frequencies. This provides a sound statistical framework to select a threshold for identifying 
Let Na and Nc be the total frequencies of all terms in the document d and C respectively. Only terms s, t with 
The expected co-occurrence frequency of the terms s, t in expected frequencies before squaring) is applied. Yate's cor-rection may be too conservative in lowering the chi-square values. However, since data with lower confidence could in-troduce noise, we believe that it is better to be conservative. where x, y  X  s, t. Define the null hypothesis as the two distributions being the same. Given a desired confidence level, the critical value can be looked up from the critical value table of X 2 test with degree of freedom equal to 1. 
A term pair with X 2 value higher than the critical value is regarded as strongly correlated in the document. 
In this section, we describe how we can apply corpus based mation retrieval by query expansion and term clustering. 
A similarity measure is first used to measure the correla-
The methodology of the experiment is as follows: (1) Top Reuters collection is mapped to a cluster of depth the cluster hierarchy if the cluster consists of terms of the topic description. A topic may correspond to more than one cluster. (5)A query for a topic consists of terms in the topic description and terms in the clusters to which the topics are mapped. (6) Relevant documents for each topic query are retrieved. (7) Different smoothing techniques are compared using the corresponding best smoothing parameters which are manually fine tuned. condition to select the top 6000 terms for clustering does not significantly increase the number of terms that can be clustered. Many of the 135 topics enlisted are not suitable for evaluation (e.g., the 27 currency codes) or do not have enough sample documents (e.g., the 78 commodity codes). 
Around 35 topics can be mapped to clusters in cluster hi-erarchies. Topics with less than 30 documents are elimi-nated. For fair comparison, only topics that are common to used for evaluation for clusters at depth 1, 2, 3, respectively. 
Nouns / noun phrases can be extracted from 7858 out of the 9603 documents. improvements in average precision for the Dirichlet method, the Jelinek-Mercer method and the CVS method are 10.8%, 6.8% and 14.6% respectively. The results demonstrate the use of corpus based smoothing techniques in improving re-trieval effectiveness. The results can also be viewed in light of the improvements of clustering by smoothing. pect better performance using supervised learning methods. 
If training data are not always available, clustering can be used for query expansion. 
Figure 2: Effects of smoothing on retrieval effectiveness by query expansion and clustering. This table shows average precision with query expanded by clusters of various depths in cluster hierarchies refined by differ-ent smoothing methods X  The CVS method outperforms the Dirichlet method and the Jelinek-Mercer method. than 1 to expand a query. The average precision for clusters of depth 2 and 3 are more similar than those for clusters of depth 1. A cluster contains approximately 4 children clusters in the cluster hierarchies. This observation implies that 4 terms out of 2000 terms are appropriate to augment the description for a topic. ters that are not reflected in the results: While most topics cannot reach 100% recall level, expanding terms with clus-ters of depth 2 and 3 can give 100% recall level for most topics. 
Figure 3: Recall-Precision curve with queries expanded with clusters of depth 1. 
Figure 41 Recall-Precision curve with queries expanded with clusters ,of depth 2. ters (clusters of depth 1) by addressing the data sparsity problem. The similarity measure is refined, which results in more overlapping between distributions of terms. The im-provement in base clusters is propagated to clusters higher up in the hierarchy. the cluster hierarchy. Without smoothing, only 826 terms out of 2000 terms are successfully clustered. Only CVS significantly improves the number of terms clustered. The smoothing parameter A is fine tuned to optimize retrieval effectiveness. There is a potential risk of over-smoothing. 
We conjecture that one can be more liberal in setting the smoothing parameter for a better smoothing method. This significant improvement in CVS further confirms the supe-riority of CVS over the other methods. of CVS. Fast CVS improves the speed whereas Voting CVS and Iterative CVS concern the quality of smoothing. co-occurrence information between term pairs as compared with O([ T l) required by the Dirichlet method and the NoSmooth Dirichlet 
CVS 
Information Retrieval. ACM Press / Addison-Wesley, 1999. 
Information term selection for automatic query expansion. In The Seventh Text REtrieval Conference (TREC-7), pages 308-314. National Institute of 
Standards and Technology (NIST), 1998. http://trec.nist.gov/pubs/trec7/tT_proceedings.html. query, thesaurus, and documents through a common visual representation. In International Conference on 
Research and Development in Information Retrieval (SIGIR 1991), pages 142-151, 1991. correlated features in query expansion. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'98), pages 337-338. 
ACM, August 24-28 1998. automatic query expansion. In Proceedings of the Sixth International Conference on Information and Knowledge Management (CIKM'97), pages 278-284, 
Las Vegas, Nevada, November 10-14 1997. ACM. the estimation of population parameters. In 
Biometrika, number 40 in 3,4, pages 237-264, 1953. 
K. Hashimoto. Trec-7 experiments: Query expansion method based on word contribution. In The Seventh Text REtrieval Conference (TREC-7), pages 373-381. 
National Institute of Standards and Technology (NIST), 1998. http://trec.nist.gov/pubs/trecT/tT_proceedings.html. british and american english. In The Norwegian Computing Center for the Humanities, pages 43-53, 
Norway, 1982. 
WebSOM -self-organizing maps of document collections. In Proceedings of Workshop on Self-Organizing Maps (WSOM97), pages 310-315, 
Espoo. Finland, 1997. markov source parameters from sparse data. In Pattern Recogition in Practice, pages 381-402, North 
Holland, Amsterdam, 1980. corpora: Why chi-square doesn't work, and an improved lob-brown comparison. In ALLC-ACH 
Conference, 1996. ht tp://www.hit .uib.no/allc/kilgarny.pdf. corpus homogeneity and similarity between corpora. 
In Proceedings of 5th ACL workshop on very large corpora, Beijing and Hongkong, August 1997. similarity and homogeneity. In Proceedings of 3rd conference on empirical methods in natural language processing, pages 46-52, 1998. categorizing web documents. In Proceedings of the 22th BCS-IRSG Colloquium on IR Research, 2000. topic words for hierarchical summarization. In International Conference on Research and 
Development in Information Retrieval (SIGIR POOl), pages 349--357, 2001. multiple evidence from different types of thesaurus for query expansion. In Proceedings of the 22nd Annual 
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'99), pages 191-197, Berkeley, CA, USA, 
August 15--19 1999. ACM. models for naive bayes text classification. In AAAI-98 
Workshop on Learning for Text Categorization, pages 41-48, Maxlison, WI, 1998. automatic query expansion. In Proceedings of the 21st Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval (SIGIR1998), pages 206-214, Melbourne, Austrailia, 
August 24--28 1998. self-organizing maps. http://www.ifs.tuwien.ac.at/andi, July 10-16 1999. frequency profiling. In proceedings of the workshop on 
Comparing Corpora, pages 1-6, 2000. corpus. htt p://ab out. reuters, corn/researchandstandards/corpus/. hierarchies from text. In International Conference on 
Research and Development in Information Retrieval (SIGIR 1999), pages 206-213, 1999. collections: the leximancer. In Proceedings of the 5th Australasian Document Computing Symposium, 
Sunshine Coast, Australia, December 1 2000. lexicM-semantic relations. In Proceedings of the 17th Annual International ACM-SIGIR Conference on 
Research and Development in Information Retrieval. (SIGIR'94), pages 61-69, Dublin, Ireland, July 3---6 1994. ACM/Springer. and global document anMysis. In Proceedings of the 19th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval (SIGIR'96), pages 4-11, August 18-22 1996. methods for language models applied to ad hoc iformation retrieval. In Proceedings of the $~th Annual 
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2001), pages 334-342, New Orleans, Louisiana, 
USA, September 9-13 2001. 
