 practical data mining. Interpretable models provide significant in-sights on data and model behaviors and may convince end-users to employ certain models. In return for these advantages, however, representation (e.g., linear, rule-based, etc.) and model complex-stand the results. This paper proposes oblique treed sparse additive models (OT-SpAMs). Our main focus is on developing a mod-el which sacrifices a certain degree of interpretability for accuracy but achieves entirely sufficient accuracy with such fully non-linear models as kernel support vector machines (SVMs). OT-SpAMs are t ure spaces into regions with sparse oblique tree splitting and as-to maintain OT-SpAM interpretability, we have to keep the overall model structure simple, and this produces simultaneous model se-lection issues for sparse oblique region structures and sparse local e xperts. We address this problem by extending factorized asymp-totic Bayesian inference. We demonstrate, on simulation, bench-mark, and real world datasets that, in terms of accuracy, OT-SpAMs outperform state-of-the-art interpretable models and perform com-petitively with kernel SVM s, while still providing results that are h ighly understandable.
 Interpretable Model, Model Selection, Sparseness
Model interpretability has been recognized to play a key role in practical data mining. Interpretable models provide significant in-sights on data and model behaviors and may convince end-users to employ certain models. It is well-recognized that, despite the dramatic evolution of machine learning approaches, such as ker-c ing, medical analytics, and science, for which understanding phe-however, there is generally a sacrifice in accuracy since flexibility of model representation (e.g., linear, rule-based, etc.) and model understand the results.

There are two key concepts in discussions on the issue of model interpretability: 1) model representation and 2) model complexi-g ression tree (CART) [ 4]) may be considered to be the most in-t erpretable. Although the simplicity of their model representations ing a small number of key features makes understanding models a lot easier. Also, whiles deep rule chains for decision trees might structures hard to understand. The trade-off between accuracy and interpretability remains an important issue.
 This paper proposes oblique treed sparse additive models (OT-SpAMs), which provide more flexible representation than linear g ree of interpretability.) While offering the same accuracy as that of such fully non-parametric models as kernel support vector ma-chines (KSVMs), they still maintain easily-interpretable model struc-tures. OT-SpAMs are instances of region-specific predictive model-the specifiers divide feature spaces into disjoint subspaces (region-s), and individual predictors perform predictions in corresponding subspaces ( as we note in Section 2, region-specific predictive mod-e ls unify the above-described two families of interpretable mod-els). OT-SpAMs employ an oblique treed split model as a region s pecifier and sparse additive models as individual region-specific predictors.

As we have noted above , controlling model complexity is an SpAMs, tree structures (the depth of tree, the number of regions, lection for local sparse experts must be determined simultaneously. We address this challenging model selection issue by utilizing fac-E M-like iterative optimization, we are able to automatically obtain compact and interpretable OT-SpAMs. We demonstrate, on simu-lation, benchmark, and real world datasets, that, in terms of accu-racy, OT-SpAMs outperform state-of-the-art interpretable models and perform competitively with kernel SVMs, while still prov iding results that are highly understandable.

The rest of this paper is organized as follows. Section 2 provides l iterature reviews of region-specific predictive models. OT-SpAMs and the proposed learning algorithm are presented in Sections 3 and 4, respectively. Simulation studies (Section 5) and benchmark eval-u ations (Section 6) quantitatively show advantages of OT-SpAMs, a nd we demonstrate results on real world POS (point-of-sales) data in Section 7.
T his section focuses mainly on region-specific predictive mod-els. Table 1 summarizes characteristics of region specific models, w hich are described below. A general and broader survey of inter-p retable models can be found in [ 12 ].
 only one global region and employs a linear prediction model as a rgued that oblique hyperplanes of linear models might be hard to understand for end-users. Feature sparseness is a key concept in trying to mitigate this issue, i.e., selecting a small number of key features makes understanding models a lot easier. To obtain sparse linear models (SLMs), various approaches, including convex meth-h ave been proposed, though their primary focus is on model gener-alization (to mitigate over-fitting), rather than on enhancing model d uce feature-wise nonlinearity to improve accuracy. By restricting tions among features), we can still visualize their feature-wise (but n onlinear) contributions and get insights from SAMs. Variants of S AMs (kernel density logistic regression (DLR) [ 6] and fast flux d iscriminant (FFD) [ 7]) have been proposed as accurate and inter-p retable models in recent KDD conferences, and research in this direction has become a topic of intense interest in the community.
Decision trees, such as CART, have tree-structured region spec-ifiers and performs prediction using constant values in individual t o linear hyperplanes, and Bayesian treed linear models (BTLM-s) [ 8] employ linear hyperplanes for region-specific predictors. Lo-cal supervised learning through space partitioning (LSL-SP) [ 42 ] u tilizes linear hyperplanes for both region-specific predictors and curacy over simple decision trees, their dense linear hyperplanes tion (FAB/HMEs). Using the FAB framework [ 14 ], they enforce s parseness on region-specific linear predictors, which significantly improves interpretability over dense linear predictors, though their all predictive ability. Supersparse linear integer models and their s tructures, which was also presented as a KDD 2014 Industrial and Government Track Invited Talk. A family of locally-linear mod-p oint-specific linear predictors. They do not have explicit regions but, rather, generate linear predictors on the fly. A major drawback of this approach for our purposes is that they can provide model in-formation only with every single test point, which makes it difficult to understand overall prediction behaviors.
This section presents details of OT-SpAMs. We first describe the region specifiers and region-specific predictors for OT-SpAM and then derive the factorized asymptotic Bayesian inference in order t o address the simultaneous model selection challenge.
Our OT-SpAM is a variant of HMEs [ 22 ], which are tree struc-t ured probabilistic mixtures of experts models. In HMEs, region-specific predictors (leaf nodes in trees) are referred to as expert-s . Suppose we have observations { x n ,y n } N X  X  R D is the domain of covariates, Y  X  R (for regression tasks) or { 0 , 1 } (for classification tasks), N is the number of samples, and D is the data dimensionality. Each gate (non-leaf node) in the tree l et z i  X  { 0 , 1 } be the binary variable indicating which branch the instance x should go down (without loss of generality, let represents the instance for going left). OT-SpAMs employ the fol-lowing logistic hyperplane for their oblique region specifiers: p ( z i | x ) = 1 1 + exp(  X  w i x ) where w i is expected to be sparse for maintaining interpretability.
Let  X  n = (  X  n  X  j = 1 G be the index set for the i -th gate, where G i contains the indices for the j -th expert, where E j contains the indices of gates on the hyperplanes { w i } G follows: which the j -th expert belongs at gate i , more specifically: gate.

Let us consider the following SAM: where f jd ( ) is any smooth univariate function and many of them are expected to be zero (i.e., sparse). Notice that, if we set  X  d ard linear model. The generating distributions of y on the expert is given by: for regression, where  X  j = ( f j , X  j ) , and p ( y | x , X  j ) = 1 1 + exp( f for classification, where  X  j = f j .

In summary, the entire likelihood is given by:
In order to learn OT-SpAMs, as well as parameter estimation, we have to address three model selection issues simultaneously: M1: tree structure (the number of gates and experts, etc.). M 3: sparseness of sparse additive experts.
 To accomplish these model selection tasks, we employ FAB infer-ence [ 14 ] for OT-SpAMs. Note that FAB has recently been used t heir framework to the learning of OT-SpAMs.

FAB inference maximizes the following Bayesian marginal log likelihood: where q is an arbitrary distribution on {  X  n } N q is q  X  ( {  X  n } N  X  = [ W ,  X ] where W = [ w 1 ,..., w G ] and  X  = [  X  1 ,..., X  Laplace X  X  method [ 43 ] is then applied to the numerator inside the l og-function in ( 8) as follows: Y where
F  X   X  = [  X  W ,  X   X ] is the maximum complete likelihood estimator and D  X  denotes the dimensionality of  X  .

Although Eto et al. [ 11 ] asymptotically ignore |  X  F using the law of large numbers, this paper considers the following upper bounds to obtain a better approximation, using Hadamard X  X  i nequality [ 30 ]: |  X 
F |  X 
F i nformation criterion (FIC) as follows: where L ( { x ,y } N n =1 ,  X  ,q ) = E q h log p ( { y } N n =1  X 
X  X 
X and || w j || 0 and || f j || 0 are the cardinalities of w i and ber of non-zero w i simplicity, we assume that the data is appropriately scaled in ad-vance such that x n  X  [  X  1 , 1] D .

Our new approximation, ( 12 ) and ( 13 ), results in a key differ-e nce from FIC for HMEs derived by Eto et al. [ 11 ], namely the r egularization terms (wave underline) are adjusted with the factors  X  s istent with that of Eto et al. [ 11 ]). These factors come from the d iagonal elements of  X  F formation matrices and provide natural measurements on the like- X  Algorithm 1 FAB EM optimization for OT-SpAM 1: I nput Data: { ( x n ,y n ) } N 2: Input Parameters: D (maximum depth of the tree),  X  (stopping 3: Initialization: t = 0 , L (0) =  X  X  X  , {  X  n } N 5: M-Step: Update S ( t ) 6 : M-Step: Update w ( t ) 7 : E-Step: Update q ( t +1) (  X  n 8 : Expert Shrinkage: Eliminate  X  X on-effective X  experts us-9 : t = t + 1 . 10: end while 11: Post-processing: E xecute hard-gate post-processing (see [ 11 ] the metric space of p ( { y } N er hand, our regularizers (wave underline) can naturally adjust the effect by taking the metric into account. T o obtain the model which maximizes FIC ( 15 ), FAB employs E M-like alternating optimization on  X  (M-step) and q The overall algorithmic framework is described in Algorithm 1. T he superscription ( t ) represents the t -th EM iteration.
From ( 15 ), we obtain the following update equation: where N j = P N w ith the waved underline. These terms come from the regulariza-e s a shrinkage effect [11 , 14 ] through the EM iteration, i.e, more c omplex and smaller experts are penalized more, and we can safe-ly eliminate  X  X on-effective X  experts from the model using a simple thresholding rule as follows: the  X  X hrinkage X  scheme of OT-SpAM would find the proper size tree structure for capturing the data well. In this way, we have ad-dressed the model selection issue M1 .
We update the i -th gate by solving the following optimization problem (the w i related terms in ( 15 )): Let G iL be the index sets of experts on the left sub-tree of gate and G iR be the index sets of experts on the right sub-tree of gate We can re-write the problem as follows: w where Q ( w i ) = This problem can be seen as a sparsity-regularized generalized l-ogistic regression problem: i) unlike the standard regression, here the response is any number in [0 , 1] ( P j  X  X  iL q (  X  n in this problem) and ii) there is a weight for each instance:
Problem ( 20 ) is non-convex (due to the L 0 -regularization), and we have adopted a greedy strategy [ 38 ] to get an approximate solu-selected features. Also, we denote the maximizer of Q as follows: where solving ( 22 ) is a constrained, weighted logistic regression p roblem. At each iteration, we selected the feature that maximizes the gradient absolute value | X  d Q ( w i ) | , which is where  X  is the Hadamard product, and and then solved the constrained weighted logistic regression prob-lem, until was satisfied, where ( k ) is the iteration index in Algorithm 2. In t his way, we have addressed the model selection issue M2 .
In order to optimize the j -th local expert f j , we introduce the following model: Algorithm 2 Gate Optimization for OT-SpAM 1: f or i = 1 ,...,G do 2: Initialization S ( k ) 3: while TRUE do 4: Select feature d ( k ) = arg max 5 : k = k + 1 . 7: if (24 ) is satisfied t hen 8: k = k  X  1 , and Break . 9: end if 10: end while 12: end for Algorithm 3 Greedy Additive Regression for OT-SpAM 1: I nput Data: { ( x n ,y n ) } N 2: for j = 1 ,...,E do 3: Initialization S ( k ) 4: while TRUE do 5: k = k + 1 . 6: Select feature d ( k ) using ( 27 ). 8 : Update S ( k ) 9: Update residual R ( k ) using ( 29 ) for regression and ( 30 ) 10: if (32 ) is satisfied t hen 11: k = k  X  1 , and Break . 12: end if 13: end while 14: Update  X  f ( t ) 15: end for where g m i s a pre-defined smooth basis function and M is the num-ber of basis functions (in our experiments, we use P-spline func-tions as g m ). Here our parameterization is changed to  X  where  X  j = (  X  1: M pert by solving the following optimization problem: j = arg max  X  0 tice that we can simply ignore  X  2( t  X  1) sification case.

Problem ( 26 ) is reduced to the optimization of weighted GLM u nder group sparsity regularization. This paper adopts the greedy optimization method summarized in Algorithm 3. Note that ex-i sting works on greedy group selection [ 28 ] (or additive forward e lects the feature that maximizes the alignment between residuals and fitted responses. Here we directly select the feature with the maximum gradient norm: This gradient criterion avoids having to fit the model O ( D ) selected feature pool). Rather than this approach, we use a match-a dd the new fitted univariate function without re-fitting the model. The fitting equation (derived by solving a weighted least squares) is described as follows : q and model fitting (but not too much when the basis functions are not highly correlated with each other), the hard-gate post-processing proposed in [ 11 ] (the step 11 of Algorithm 1) makes the final model m ore stable and reliable, as we will see in Section 5.
T he difference between classification and regression is in the up-residual as follows: o f the Newton step as follows: where The stopping condition is defined as follows: In this way, we have addressed the model selection issue M 3 .
This section presents results of simulation studies and demon-strates our FAB-based model selection for OT-SpAMs. In order to make OT-SpAMs interpretable, we proposes a visualization method that employs individual local sparse additive experts.
We generated N = 5000 data points in which each instance is described by D = 15 features, and the features are uniformly original feature and the estimated sparse additive feature. shown in (A). It has 4 experts, each of which uses 2  X  3 features, and the partition nodes use linear functions of 2 features. We generated response Y of the instances, in accord with the ex-perts they belongs to, in the following way:
In this simulation, we set the initial tree-depth to D = 4 the initial number of experts was 16), the shrinkage threshold to oblique region specifiers using many features are hard to interpret, we set the maximum number of features used in each partition node to 3 . Additionally, we employed P-spline functions (a family of B-t he penalty parameter for P-splines as 0.5, the spline degree as 3, the number of knots as 6.
The estimated tree structure is shown in (B). There were 16 ex-perts at the start, irrelevant experts were gradually pruned from the t ures with exactly the same features in each gate (oblique hyper-plane). parison with the functions in (A), since scaling the functions does not change the decision boundary.
Figure 1 shows the estimated additive functions of Expert1. S-ture functions before post-processing (green curves). After post-processing, the true (blue curves) and estimated curves (red curves) are quite consistent. Although we omit results for the other experts, we obtained similarly good estimation results for those as well.
These results empirically demonstrate strong model selection ca-pability in addressing M1 , M2 and M3 simultaneously.
Since each sparse additive feature f jd is (feature-wise) nonlin-sparse additive features. Figure 2 shows the visualization for Ex-easily explain our visualization method). The left-hand figure is a simple line plot of estimated feature functions w.r.t. were selected and overlapped one another in a single plot. To avoid this, we employ a stacked area plot that is constructed as follows. negative sides, as shown in the middle column of Figure 2. The s-t acked area plot was then built by combining positive and negative stacked area plots (the right-hand figure). As is shown, we are able to avoid  X  X gly X  overlapping and can clearly see how each input fea-ture  X  X onlinearly X  contributes to the target signal. We visualize the stacked area plot for each expert, and the combination of standard of nonlinear model behaviors.
We evaluated OT-SpAMs on 24 public benchmark data sets, avail-able from the UCI Machine Learning Repository [ 2], for both re-Figure 2: Stacked area visualization for the learned Expert1 in the simulation study. The horizontal and vertical axes repre-sent, respectively, the original feature and the estimated sparse additive feature. mination conditions as in the simulations and evaluated root mean squared error (RMSE) for regression and accuracy in classification.
For regression tasks, we compared OT-SpAM with the following methods: OLS (ordinary least squares using the full set of the fea-A M (additive models [ 16 ] using the full set of the features), and S VR-RBF [ 36 ]. For classification tasks, we compared OT-SpAM w ith the following methods: LLR (linear logistic regression using S VR-RBF, LSL-SP, LDKL, DLR, and SVM-RBF were optimized on the basis of 10-fold cross validation on training data. Note that we used all features for linear and additive models (OLS, AM, LLR and ALR). The primary focus here was on accuracy evaluation, and they performed better with all features (without sparse regulariza-tion).
 Table 3 and Table 4 report the 10-fold averaged cross validation R MSE and classification accuracy, respectively. From these results, we have the following observations:  X 
For regression tasks, OT-SpAMs achieved the lowest RMSE val-ues in most cases. Both AMs and FAB/HMEs also performed much better than OLS and RegTrees. OT-SpAMs took advan-tages of both methods and performed competitively with SVR-
RBF (or sometimes even outperformed it).  X  For classification tasks, similar observations were obtained, i.e., FAB/HMEs and ALRs performed better than LLRs, and OT-
SpAMs usually outperformed both them and state-of-the-art ad-ditive models (ALRs and DLRs). LDKL also performed well, 2 W e use the built-in RegressionTree class in MATLAB 3 We use the built-in ClassificationTree class in MATLAB 6 F or SVR-RBF and SVM-RBF, we use the LIBSVM package [ 5]. but it is worth noting that LDKL produces a predictor at every s -ingle data point and that no interpretation of regions is provided, as may be seen in Table 1. We observed that OT-SpAMs per-f ormed slightly worse than SVM-RBFs and sacrificed accuracy for interpretability, though, except for D21, the sacrifice was not significant.  X 
On these data sets, OT-SpAM usually output treed models with 5-8 experts, and these models were reasonably interpretable. OT-
SpAM selected different fractions of features, depending on the data sets used.
 In summary, we conclude that OT-SpAMs sacrificed minimum ac-curacy loss for interpretability, w.r.t. fully non-parametric method-s, by maintaining interpretable treed region structures and feature-wise sparse nonlinear expert structures. advanced store management. Let us consider three scenarios: A) store inventory management requires forecasting every 6 hours B) store assortment planning requires forecasting every 1 day for C) production planning requires forecasting every 1 week for 2
We applied OT-SpAM to sales forecasting of sweet bakery prod-ucts in a middle-size supermarket located in a residential area of highlighted in bold and bold italic faces, respectively. ID OLS RegTree FAB/HME AM OT-SpAM SVR-RBF
D1 5.10  X  0 .53 6.67  X  1 .33 3.29  X  0 .32 2.88  X  0 .43 2.79
D2 5.38  X  0 .86 9.06  X  2 .92 3.72  X  0 .96 3.65  X  0 .72 3.41
D3 2.32  X  0 .09 1.99  X  0 .34 2.28  X  0 .39 1.33  X  0 .11 1.02
D4 0.14  X  0 .01 0.28  X  0 .03 0.13  X  0 .01 0.13  X  0 .02 0.12
D5 2.32  X  0 .17 5.57  X  0 .22 2.27  X  0 .15 2.31  X  0 .10 2.22
D6 7.30  X  0 .10 0.90  X  0 .34 4.96  X  0 .57 5.11  X  0 .06 2.99
D7 16.2  X  0 .31 7.57  X  0 .21 5.14  X  0 .62 3.79  X  0 .22 3.26
D8 0.29  X  0 .00 0.40  X  0 .01 0.24  X  0 .01 0.20  X  0 .00 0.19
D9 4.67  X  0 .09 8.44  X  0 .31 4.18  X  0 .21 4.24  X  0 .10 3.31
D10 15.5  X  0 .30 6.73  X  0 .28 5.12  X  0 .40 3.57  X  0 .18 2.79
D11 (2.50  X  2 .2) 10  X  4 (4.27  X  0 .1) 10  X  4 ( 1 .70  X  0.0 )
D12 (7.48  X  0 .1) 10 4 (12.6  X  0 .3) 10 4 (6.82  X  0 .1) 10 4 ( 6 .48  X  0.1 ) 10 4 ( 5 .97  X  0.1 ) 10 4 (11.8  X  are highlighted in bold and bold italic faces, respectively.  X  4 .6 76.2  X  8 .1 67.0  X  7 .1 82.2  X  7 .5 91.3  X  4 .0  X  4 .1 86.5  X  2 .4 86.5  X  3 .5 87.1  X  3 .9 85.5  X  3 .7  X  7 .9 76.5  X  7 .5 76.5  X  6 .7 77.5  X  7 .1 75.8  X  8 .8  X  3 .3 90.0  X  3 .4 75.8  X  4 .7 96.1  X  1 .8 99.8  X  0 .5  X  1 .1 90.7  X  2 .2 90.8  X  2 .0 92.5  X  2 .2 86.1  X  3 .0  X  0 .2 98.9  X  0 .9 88.8  X  1 .4 99.1  X  0 .7 100  X  0 .0  X  0 .7 77.8  X  0 .3 77.6  X  0 .3 78.3  X  0 .5 78.5  X  0 .0  X  1 .1 96.8  X  0 .7 95.5  X  1 .0 97.1  X  0 .6 96.9  X  0 .1  X  0 .0 57.9  X  1 .8 55.1  X  0 .1 59.7  X  2 .3 81.6  X  0 .1  X  0 .8 84.9  X  0 .8 78.6  X  0 .9 85.8  X  1 .1 87.2  X  0 .1  X  1 .2 94.0  X  0 .2 77.6  X  0 .4 94.2  X  0 .2 95.6  X  0 .0  X  1 .7 93.7  X  0 .6 91.1  X  0 .8 95.6  X  1 .9 98.6  X  0 .1 s et the target variable to total sales of sweet bakery products for ples) were used for training, and the other year (365 samples) was used for testing. Table 5 summarizes variables used for the fore-information (x20, x21 and the target variable), we independently collected weather related variables (x2-x19) and also added calen-dar information (x22-x30). All numerical variables, including the target variable, were standardized in advance. Experimental set-tings were the same as those of Sections 5 and 6, except the initial t ree-depth (here we use D = 5 ). Figure 3 shows the forecasting re-s ults for the test period. As can be seen, OT-SpAM achieved fairly good forecasting. http://www.ksp-sp.com .

T he estimated tree structure is shown in (C). The region-specifier and weekday flag (x29). Taking into account the fact that average pressure in Tokyo is relatively high during May to September, the region-specifier identified the following clusters:  X 
Expert1: in-season (high average sales) during early summer to autumn.  X 
Expert2: off-season (low average sales) during early summer to autumn.  X 
Expert3: other season (middle average sales) during early sum-mer to autumn.  X 
Expert4: weekday during autumn to early summer.  X  Expert5: holiday during autumn to early summer.
 Figure 4 provides our stacked area plot for individual experts. W e can characterize the experts as follows:  X 
Expert1: Products in this category (sweets bakery) are sold a lot on Friday. The largest bias value among the experts supports our hypothesis that this cluster corresponds to in-season.  X 
Expert2: The small responses (i.e., the scale of the vertical axis is small) supports the hypothesis that this cluster corresponds to off-season, and the sales are small without relation to weather.  X 
Expert3: The response for daylight (x13) is high in the middle area of the horizontal axis. Since mid-summer daylight hours are long in Tokyo, this result indicates that sunny days tend to have large sales.  X 
Expert4: We can observe a strong response w.r.t the sales of 1 week previous but the peak is somehow shifted to the left-hand side. This might indicate a natural decrease in sales following a promotional campaign. Table 5: List of variables in the sales forecasting dataset. N and B stand for numerical and binary values. weather1 and weather2 are forecasting (1 week ahead) and history (1 week ago), respectively.
  X  E xpert5: The sales are low on Saturdays.

The above observations can be transformed into insights for im-prove store operations, such as: A) store inventory management: In order to avoid excessive in-B) store assortment planning: store managers should consider pos-These insights are still hypotheses and must be evaluated in real stores, but we believe that the above results demonstrate high inter-pretability of OT-SpAMs in the real world applications.
We have proposed oblique treed sparse additive models, novel sparse additive models. We have presented a Bayesian learning al-gorithm which fully automates space partitioning and feature selec-tion, making the proposed approach nearly parameter free. Promis-standing and computational efficiency of OT-SpAMs, as well as extensions to such more general data mining problems as multi-class classification and Poisson regression.
The majority of the work was done during the internship of the first author at NEC Laboratories America, Cupertino, CA. [1] S. Amari and H. Nagaoka. Methods of Information [2] K. Bache and M. Lichman. UCI machine learning [3] L. Breiman. Random forests. Machine Learning , [4] L. Breiman, J. Friedman, R. Olshen, and C. Stone. [5] C.-C. Chang and C.-J. Lin. Libsvm: A library for support [6] W. Chen, Y. Chen, Y. Mao, and B. Guo. Density-based [7] W. Chen, Y. Chen, and K. Q. Weinberger. Fast flux [8] H. A. Chipman, E. I. George, and R. E. Mcculloch. Bayesian [9] C. Cortes and V. Vapnik. Support-vector networks. Machine [10] P. H. C. Eilers and B. D. Marx. Flexible smoothing with [11] R. Eto, R. Fujimaki, S. Morinaga, and H. Tamano. [12] A. A. Freitas. Comprehensible classification models: A [13] J. H. Friedman. Greedy function approximation: A gradient [14] R. Fujimaki and S. Morinaga. Factorized asymptotic [15] Q. Gu and J. Han. Clustered support vector machines. In [16] T. Hastie, R. Tibshirani, and J. H. Friedman. The elements of [17] T. J. Hastie and R. J. Tibshirani. Generalized additive [18] B. Hayete and J. R. Bienkowska. Gotrees: predicting go [19] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning [20] J. Huang, J. L. Horowitz, and F. Wei. Variable selection in [21] J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and [22] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of [23] C. Jose, P. Goyal, P. Aggrwal, and M. Varma. Local deep [24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet [25] L. Ladicky and P. H. S. Torr. Locally linear support vector [26] B. Letham, C. Rudin, T. H. McCormick, and D. Madigan. [27] H. Liu and X. Chen. Nonparametric greedy algorithms for [28] A. C. Lozano, G. Swirszcz, and N. Abe. Grouped orthogonal [29] S. Mallat and Z. Zhang. Matching pursuits with [30] V. G. Maz X  X a and T. O. Shaposhnikova. Theory of [31] P. McCullagh and J. A. Nelder. Generalized linear models [32] L. Meier, S. van de Geer, and P. Buhlmann.
 [33] S. K. Murthy, S. Kasif, and S. Salzberg. A system for [34] P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. Sparse [35] N. Segata and E. Blanzieri. Fast and scalable local kernel [36] A. J. Smola and B. Sch X lkopf. A tutorial on support vector [37] R. Tibshirani. Regression shrinkage and selection via the [38] J. A. Tropp and A. C. Gilbert. Signal recovery from random [39] A. K. Y. Truong. Fast growing and interpretable oblique [40] B. Ustun, S. Trac X , and C. Rudin. Supersparse linear integer [41] V. N. Vapnik. The Nature of Statistical Learning Theory . [42] J. Wang and V. Saligrama. Local supervised learning through [43] R. Wong. Asymptotic approximations of integrals . Computer [44] Z. E. Xu, M. J. Kusner, K. Q. Weinberger, and M. Chen. [45] J. Zhu and T. Hastie. Kernel logistic regression and the
