 ORIGINAL PAPER Philippe Dreuw  X  Georg Heigold  X  Hermann Ney Abstract We present a novel confidence-and margin-based discriminative training approach for model adaptation of a hidden Markov model (HMM)-based handwriting recog-nition system to handle different handwriting styles and their variations. Most current approaches are maximum-likelihood (ML) trained HMM systems and try to adapt their models to different writing styles using writer adap-tive training, unsupervised clustering, or additional writer-specific data. Here, discriminative training based on the maximum mutual information (MMI) and minimum phone error (MPE) criteria are used to train writer-independent handwriting models. For model adaptation during decod-ing, an unsupervised confidence-based discriminative train-ing on a word and frame level within a two-pass decoding process is proposed. The proposed methods are evaluated for closed-vocabulary isolated handwritten word recognition on the IFN/ENIT Arabic handwriting database, where the word error rate is decreased by 33% relative compared to a ML trained baseline system. On the large-vocabulary line rec-ognition task of the IAM English handwriting database, the word error rate is decreased by 25% relative.
 Keywords Handwriting recognition  X  Arabic  X  Discriminative training  X  Maximum mutual information  X  Minimum phone error  X  Margin  X  Confidences  X  Hidden Markov models  X  Model adaptation 1 Introduction Most state-of-the-art single-pass and multi-pass [ 4 , 8 , 11 ] HMM-based handwriting recognition systems are trained using the maximum-likelihood (ML) criterion.

Typical training criteria for string recognition like, for example,minimumphoneerror(MPE)andmaximummutual information (MMI) in speech recognition are based on a (reg-ularized) loss function. In contrast, large margin classifiers X  the de-facto standard in machine learning X  X aximize the separation margin. An additional loss term penalizes mis-classified samples.

The MMI training criterion has been used in [ 30 ]to improve the performance of an HMM-based off-line Thai handwriting recognition system for isolated characters. They propose a feature extraction based on a block-based PCA and composite image features, which are reported to better at dis-criminating Thai confusable characters. In [ 5 ], the authors apply the Minimum Classification Error (MCE) criterion to the problem of recognizing online unconstrained-style char-acters and words and report large improvements on a writer-independent character recognition task when compared to a ML trained baseline system.

Similar to the system presented in [ 29 ], we apply the MMI criterion, but modified by a margin term. This margin term can be interpreted as an additional observation-dependent prior weakening the true prior [ 18 ] and is identical with the SVM optimization problem of log-linear models [ 16 ].
The most common way for unsupervised adaptation is the use of the automatic transcription of a previous recognition pass without the application of confidence scores. Many publications in automatic speech recognition (ASR) have shown that the application of confidence scores for adaptation can improve recognition results. However, only small improvements are reported for confidence-based maximum-likelihood linear regression (MLLR) adaptation [ 12 , 31 , 33 ] or constrained-MLLR adaptation [ 1 ]. In this work, we present a novel unsupervised confidence-based dis-criminative model adaptation approach.

This paper briefly reviews how the MMI/MPE training criteria can be extended to incorporate the margin concept, and that such modified training criteria are smooth approx-imations to support vector machines with the respective loss function [ 16 ]. In addition to the margin concept, the MMI/MPE training criteria are extended by an additional confidence term [ 6 ] to allow for novel unsupervised model adaptation.

The focus of this work shall be on off-line handwriting recognition of closed-vocabulary isolated words and large-vocabulary sentence recognition tasks in combination with m -gram language models. More explicitly, the novelties of our investigation are as follows: 1. Direct evaluation of the utility of the margin term in 2. Direct evaluation of the utility of an additional confi-3. Direct evaluation of the amount of iterations and con-4. Evaluation on state-of-the-art systems. Ideally, we Due to the nature of the novel publicly available RWTH OCR 1 framework and databases, it can be assumed that most results can be transferred to ASR domains. Similar usage of features, lexica, and language models on smaller corpora allow for a detailed analysis of regularization, optimization iterations, as well as impact of confidence thresholds.
The proposed approach takes advantage of the general-ization bounds of large margin classifiers while keeping the efficient framework for conventional discriminative training. This allows us to directly evaluate the utility of the margin term for handwriting recognition. So, our approach combines the advantages of conventional training criteria and of large margin classifiers. 2 System overview In off-line handwriting recognition, we are searching for an unknown word sequence w N 1 := w 1 ,...,w N , for which the sequence of features x T 1 := x 1 ,..., x T fits best to the trained models. We maximize the posterior probability p (w N 1 | x over all possible word sequences w N 1 with unknown number of words N . This is described by the Bayes X  decision rule: x with  X  being a scaling exponent of the language model.
In this work, we use a writing variant model refinement [ 8 ] of our visual model p x T 1 | w N 1 = max with v N 1 a sequence of unknown writing variants,  X  a scaling exponent of the writing variant probability depending on a parameter set v , and  X  a scaling exponent of the visual char-acter model depending on a parameter set e , t for emission and transition model.

Especially in Arabic handwriting with its position-depen-dent shapes [ 23 ], large white-spaces can occur between iso-lated-, beginning-, and end-shaped characters (see Fig. 1 a). As a specific set of characters is only connectable from the right side, such words have to be cut into pieces (Part of Arabic Word (PAW)). Due to ligatures and diacritics in Ara-bic handwriting, the same Arabic word can be written in sev-eral writing variants, depending on the writer X  X  handwriting style.

During training, a corpus and lexicon with supervised writing variants instead of the commonly used unsupervised writing variants can be used, and during decoding, the writ-ing variants can only be used in an unsupervised manner. Obviously, the supervised writing variants in training can lead to better trained character models only if the training corpora have a high annotation quality. Usually, the proba-bility p (v | w) for a variant v of a word w is considered as uniformly distributed [ 7 ]. Here, we use the count statistics as probability p (v | w) = N (v,w) N (w) , where the writing variant counts N (v, w) and the word counts N (w) are estimated from the corresponding training corpora and represent how often these events were observed. Note that v N (v ,w) N (w) = 1. The scaling exponent  X  of the writing variant probability of Eq. 2 can be adapted in the same way as it is done for the language model scale  X  in Eq. 1 . 2.1 Feature extraction The aim of this work is to analyze the effect of discriminative training and the incorporation of a margin and a confidence term into the criteria. Therefore, only few preprocessing steps commonly applied in handwriting recognition will be used: deslanting as well as a size normalization are used to com-pensate for variations in Latin writing style as proposed by [ 20 ], no preprocessing will be used with Arabic handwritten data.

After an optional preprocessing of the input images, the images are scaled down to 16 pixel height while keeping their aspect ratio. We extract simple appearance-based image slice features x t at every time step t = 1 ,..., T which are aug-mented by their spatial derivatives in horizontal direction = x window itself into several sub-windows and extract different features within each of the sub-windows [ 2 , 20 , 30 , 39 ]
In order to incorporate temporal and spatial context into the features, we concatenate seven consecutive features in a sliding window with maximum overlap, which are later reduced by a PCA transformation matrix to a feature vector x of dimension 30 (see Fig. 2 ). 2.2 Visual modeling Our hidden Markov model (HMM)-based handwriting rec-ognition system is Viterbi trained using the maximum-like-lihood (ML) training criterion and a lexicon with multiple writing variants as proposed in [ 7 , 8 ].
 Each character is modeled by a multi-state left-to-right HMM with skip transitions and separate Gaussian mixture models (GMMs). The parameters of all GMMs are estimated with the ML principle using an expectation maximization (EM) algorithm and to increase the number of densities in the mixture densities, successive splitting of the mixture den-sities is applied. Different HMM topologies and transition probabilities are used for character models (cf. Fig. 3 a) and white-space models (cf. Fig. 3 b) in Arabic and Latin hand-writing recognition, where the white-space model itself is always modeled by a single GMM in all systems.
 Arabic handwriting. Depending on the position of the char-acter in an Arabic word, most of the 28 characters can have up to 4 different shapes [ 23 ]. Here, we use position-dependent character models to model the different presentation forms, and due to ligatures, a total of 120 character models and one white-space model have to be estimated in training (see Sect. 4 ). Each character model in our Arabic handwriting recognition base system is modeled by a 3-state left-to-right HMM with three separate GMMs. The position-dependent character-based model of our ML trained baseline system includes 361 mixtures with 36k Gaussian densities (with up to 128 densities per mixture) with globally pooled diagonal variances. Additionally, a large stretching of long drawn-out characters occurs often in Arabic handwriting (see Fig. 1 b). Therefore, we use very low loop penalties but higher skip penalties for our HMM state-transitions (see Fig. 3 a). Latin handwriting. The Latin handwriting is one of the most common handwriting systems worldwide. English handwrit-ing uses the alphabets 26 basic characters. As each letter can be written in lowercase and uppercase, and capitalized or cur-sive writing, and additionally symbols for punctuations are used in the IAM database (see Sect. 4 ), 78 character models and one blank model have to be estimated in our ML trained | | ( a ) (b) baseline system, where each character model is modeled by a 10-state left-to-right HMM with five separate GMMs, result-ing in 391 mixtures with 25k Gaussian densities (with up to 128 densities per mixture) after ML training and globally pooled diagonal variances. 2.3 Discriminative training: Incorporation of the margin In this work, we use a discriminative training approach based on the maximum mutual information (MMI) and minimum phone error (MPE) criteria as presented in [ 15  X  17 ]. In addi-tion to the novel confidence-based extension of the margin-based MMI training presented in [ 6 ], the confidence concept has been incorporated in the margin-based MPE criterion in this work. In the following, we give a brief summary.
The two-dimensional representation of a handwritten image is turned into a string representation X = x 1 ,..., where x t isafixed-lengtharrayassignedtoeachcolumninthe image (see Sect. 2.1 for further details). The word sequence W = w 1 ,...,w N is represented by a character string.
Assume the joint probability p ( X , W ) of the features X and the symbol string W . The model parameters are indicated by . The training set consists of r = 1 ,..., R labeled sen-probability p ( X , W ) induces the posterior p , X  ( W | X ) =
The likelihoods are scaled with some factor  X &gt; 0, which is a common trick in speech recognition to scale them to the  X  X eal X  posteriors [ 17 ]. The approximation level  X  is an addi-tional parameter to control the smoothness of the criterion. Let p ( X , W ) be the joint probability and L [ p ( X r ,  X  ), W senting all possible hypotheses W for a given lexicon, and W r representing the correct transcription of X r . The general optimization problem is now formulated as a minimization of the total loss function:  X  = arg min C ||  X  and includes an 2 regularization term ||  X  0 || 2 2 (i.e., a prior over the model parameters), where the constant C is used to balance the regularization term and the loss term including the log-posteriors. Here, the regularization is replaced by I-smoothing [ 37 ], which is a useful technique to make MMI/MPE training converge without overtraining, and where the parameter prior is centered for initialization at a reasonable ML trained model 0 (see Sect. 2.2 ). 2.3.1 Margin-based maximum mutual information In automatic speech recognition (ASR), maximum mutual information (MMI) commonly refers to the maximum likeli-hood (ML) for the class posteriors. For MMI, the loss func-tion to be minimized is described by: L
MMI ) [ p ( X r ,  X  ), W r ]= X  log p This criterion has proven to perform reasonably as long as the error rate on the training data is not too low, i.e., gener-alization is not an issue.

Themargin-basedMMIlossfunction(M-MMI)tobemin-imized is described by: L  X  [ p ( X r ,  X  ), W r ] which has an additional margin term including the accuracy A (  X  , W W r . Note that the additional term can be interpreted as if we had introduced a new posterior distribution. In a simpli-fied view, we interpret this as a pseudo-posterior probability which is modified by a margin term.

Compared with the true posterior in Eq. 3 ,themargin pseudo-posteriorincludesthemargintermexp (  X   X  A ( V , W r which is based on the string accuracy A ( V , W r ) between the two strings V , W r . The accuracy counts the number of match-ing symbols of V , W r and will be approximated for efficiency reasons (see Sect. 2.3.3 ) by the approximate word accuracy [ 35 ].

As explained in [ 17 ], the accuracy is generally scaled with some  X &gt; 0, and this term weighs up the likelihoods of the competing hypotheses compared with the correct hypothe-sis [ 36 ]. On the contrary, this term can be equally interpreted as a margin term.

This margin term can be interpreted as an additional observation-dependent prior weakening the true prior [ 18 ]. Moreover, this training criterion is identical with the SVM optimization problem for  X   X  X  X  and log-linear mod-els [ 16 ]. Keep in mind that Gaussian HMMs with globally pooled variances are equivalent to a log-linear model with first-order features only [ 14 ]. The loss functions for MMI and M-MMI are compared with the hinge loss function in Fig. 4 . The example is given for a binary classification problem with single observations (i.e., no symbol strings). The loss func-tion is plotted against the log-ratio of the posterior of the correct class W n to the posterior of the competing class d = log p for  X  = 1 , X  = 1, and A ( V , W ) =  X ( V , W ) . MMI and M-MMI differ by an offset d = 1, and M-MMI is a smooth approximation to the hinge loss function (for more details cf. [ 16 , 17 ]). 2.3.2 Margin-based minimum phone error The minimum phone error (MPE) criterion is defined as the (regularized) posterior risk based on the error function E (
V , W ) , which is probably the training criterion of choice in large-vocabulary continuous speech recognition (LVCSR). For MPE, the loss function to be minimized is described by: which is based on the error function E ( V , W ) like, for exam-ple, the approximate phone error [ 35 ]. In OCR, a phoneme unit usually corresponds to a character if words are modeled by character sequences.

Analogously, the margin-based MPE loss function (M-MPE) to be minimized is described by: =
It should be noted that due to the relation E ( W , W r ) = |
W bols in the reference string W r , the error E ( W , W r ) accuracy A ( W , W r ) can be equally used in Eqs. 8 and 9 .The accuracy for MPE and for the margin term do not need to be the same quantity [ 15 ].

Again, the loss functions for MPE and M-MPE are com-pared for a binary classification problem with single observa-tions(for E ( V , W ) = 1  X   X ( V , W ), A ( V , W ) =  X ( V  X  = 1) in Fig. 4 . The illustration shows that M-MPE is a hori-zontally shifted version of MPE, while M-MPE approximat-ing the margin error. Finally, it should be pointed out that other posterior-based training criteria (e.g., MCE as used in [ 5 ]) can be modified in an analogous way to incorporate a margin term (for more details cf. [ 16 , 17 ]). 2.3.3 Optimization In [ 16 ], it is shown that the objective function F ( MMI converges pointwise to the SVM optimization problem using the hinge loss function for  X   X  X  X  , similar to [ 41 ]. In other words, F ( M  X  MMI )  X  () is a smooth approximation to an SVM with hinge loss function that can be iteratively optimized with standard gradient-based optimization techniques like Rprop [ 16 , 41 ].

In this work, the regularization constant C , the approxi-mation level  X  , and the margin scale  X  are chosen beforehand and then kept fixed during the complete optimization. Note that the regularization constant C and the margin scale  X  are not completely independent of each other. Here, we kept the margin scale  X  fixed and tuned the regularization con-stant C (see Table 1 ). Previous experiments in ASR have suggested that the performance is rather insensitive to the specific choice of the margin [ 16 ], and the results in Table 1 furthermore suggest that if the baseline error rate is relatively high,thechoiceoftheI-smoothingconstant C haslessimpact in an Rprop-based optimization than in an Extendend Baum Welch (EBW) environment [ 37 ]. An I-smoothing regulari-zation constant C = 1 . 0 is used in all results presented in Sect. 4 .

Inlarge-vocabularyhandwritingrecognition,wordlattices restricting the search space are used to make the summation over all competing hypotheses (i.e., sums over W ) efficient. The exact accuracy on character or word level cannot be computed efficiently due to the Levenshtein alignments in general, although feasible under certain conditions as shown in [ 15 ]. Thus, the approximate phone/word accuracy known from MPE/MWE [ 35 ] is used for the margin instead. With this choice of accuracy, the margin term can be represented as an additional layer in the common word lattices such that efficient training is possible. More details about the trans-ducer-based implementation used in this work can be found in [ 15 ].

As in ASR, where typically a weak unigram language model is used for discriminative training [ 40 , 41 ], we use a unigram language model in our proposed discriminative training criteria. 2.3.4 Confidences for unsupervised discriminative Sentence or word confidences can be incorporated into the training criterion by simply weighing the segments with the respective confidence. This is, however, not possible for state-based confidences. Instead of rejecting an entire sen-tence or word, the system can use state-confidence scores to select state-dependent data in an unsupervised manner. State-confidencescoresareobtainedfromcomputingarcposteriors from the lattice output from a previous decoder pass.
Rprop is a gradient-based optimization algorithm. The gradient of the training criterion under consideration can be represented in terms of the state posteriors p rt ( s | x posteriors are obtained by marginalization and normaliza-tion of the joint probabilities p ( x T r 1 , s T 1 ,w N r sequences through state s at frame t . These quantities can be calculated efficiently by recursion, cf. forward/backward probabilities. Then, the state-based confidences are incorpo-rated by multiplying the posteriors with the respective con-fidence before the accumulation. In summary, each frame t of state s .

Another way to describe the incorporation of the confi-dence term is from a system point of view. The accumulator acc s of state s can be described by acc s = where the weight  X  r , s , t , which corresponds to  X ( s t ML training i.e., one or zero, is replaced for the proposed M-MMI-conf/M-MPE-conf criteria (with  X  = 0) by the corresponding margin pseudo-posterior. With the additional confidence term for the proposed M-MMI-conf criterion (cf. Eq. 6 ), the new weight can be described as follows:  X  Here, the selector function  X ( c r , s , t  X   X  c ) with the thresh-old parameter  X  c controls the amount of adaptation data. The M-MPE-conf criterion can be defined in a similar manner. Note that due to the quality of the confidence metric, thres-holdingtheconfidencescoresafterfeatureselectioncanoften result in an improved accuracy, as reported in [ 12 ]. On the one hand, the experimental results for word-confidences in Fig. 12 and state-based confidences in Fig. 16 suggest that the confidences are helpful, but on the other hand that the threshold itself has little impact due the proposed M-MMI-conf/M-MPE-conf approaches, which are inherently robust against outliers.

Analogously, the weight  X  r , s , t would correspond to the true posterior (cf. Eq. 3 ) in an MMI-conf/MPE-conf crite-rion. Note that in informal experiments, these criteria lead to no robust improvements, i.e., only the combination of mar-gin and confidences makes the proposed approaches robust against outliers. 3 Decoding architecture The recognition is performed in two passes. System 1 per-forms the initial and independent recognition pass using the discriminatively trained models. The output is required for the unsupervised text-dependent model adaptation in the next step.

For unsupervised adaptation, at test time, the condition-ing state sequence is derived from a prior recognition pass. Although the prior transcript in that case contains errors, adapting on that transcript disregarding that fact generally still results in accuracy improvements [ 12 ].

The model adaptation in the second pass is performed by discriminatively training a System 2 on the text output of the first-pass recognition system. Additionally, the confidence-alignments generated during the first-pass decoding can be used on a sentence-, word-, or state-level to exclude the cor-responding features from the discriminative training process for unsupervised model adaptation.

Out-of-vocabulary (OOV) words are also meant to be harmful for adaptation [ 34 ] but even when a word is wrong, the pronunciation or most of the pronunciation can still be correct, suggesting that a state-based and confidence-based adaptation should be favored in such cases. 3.1 Word confidences As we are dealing with isolated word recognition on the IFN/ENIT database, the sentence and word confidences are identical. The segments to be used in the second-pass system are first thresholded on a word-level by their word confi-dences: only complete word segments aligned with a high confidence by the first-pass system are used for model adap-tation using discriminative training. 3.2 State-confidences Insteadof rejectinganentiresentenceor word, thesystem can use state-confidence scores to select state-dependent data (cf. Sect. 2.3.4 ). State-confidence scores are obtained from com-puting arc posteriors from the lattice output of the decoder. The arc posterior is the fraction of the probability mass of the paths that contain the arc from the mass that is represented by all paths in the lattice. The posterior probabilities can be computed efficiently using the forward-backward algorithm as, for example, described in [ 21 ]. Then, the word frames to be used in the second-pass system are first thresholded on a state-level by their state-confidences: only word frames aligned with a high confidence by the first-pass system is used for model adaptation using discriminative M-MMI-conf/ M-MPE-conf training (see Sect. 2.3 ).

An example for a word-graph and the corresponding 1-best state alignment is given in Fig. 5 : during the decoding, the ten feature frames (the squares) can be aligned to differ-ent words (long arcs) and their states. In this example, the word-confidence of the 1-best alignment is c = 0 . 7 (upper arc). The corresponding state-confidences are calculated by accumulating state-wise over all competing word alignments (lower arcs), i.e., the state-confidence of the 1-best align-ment X  X  fourth state would stay 0 . 7 as this state is skipped in all other competing alignments, all other state-confidences would sum up to 1 . 0. 4 Experimental results The proposed approach is applied to isolated Arabic hand-writingandcontinuousEnglishhandwriting.Theexperiments for isolated word recognition are conducted on the IFN/ENIT database [ 32 ] using a closed lexicon, and experiments for continuous sentence recognition on the IAM database [ 27 ] using a large-vocabulary lexicon and additional external lan-guage model resources as proposed in [ 3 ].

The IFN/ENIT database is divided into four training folds with an additional fold for testing [ 25 ]. The current database version (v2.0p1e) contains a total of 32,492 Arabic words handwritten by about 1,000 writers and has a vocabulary size of 937 Tunisian town names. Here, we follow the same evaluation protocol as for the ICDAR 2005, 2007, and 2009 competitions [ 24 ] (see Fig. 6 ). The corpus statistics for the different folds can be found in Table 2 .

The IAM database was introduced by [ 27 ] in 2002 and contains a total number of 1,539 pages with 5,685 sentences in 9,862 lines. All words are build using only 79 different symbols which consist of both uppercase and lowercase char-acters, punctuation, quotation marks, a special symbol for crossed out words, and a white-space model (cf. Sect. 2.2 ). A comparison of the predefined training, testing, and evalu-ation folds is given in Table 3 . Here, we focus on the large-vocabulary line recognition task, which is one of the four tasks provided with the database. For the large-vocabulary recognition task, we use as proposed in [ 3 ] the three addi-tional text corpora Lancaster-Oslo-Bergen, Brown and Wel-lington (LBW) to estimate our language models and lexica. Note that the IAM validation/test lines were excluded from the Lancaster X  X slo X  X ergen (LOB) corpus. 4.1 First-pass decoding In this section, we compare our ML trained baseline sys-tems (cf. Sect. 2.2 for visual model details) to our discrim-inatively trained systems using the MMI and MPE criteria and their margin-based extensions. The discriminative train-ing is initialized with the respective ML trained baseline model and iteratively optimized using the Rprop algorithm (cf. Sect. 2.3 ).
 Isolated word recognition. For isolated Arabic word rec-ognition, we compare our ML trained baseline system with MMI/M-MMI criteria only.

In general, the number of Rprop iterations and the choice of the regularization constant C have to be chosen carefully (cf. optimization Table 1 in Sect. 2.3 ) and were empirically optimized in informal experiments to 30 Rprop iterations and C = 1 . 0 (cf. detailed Rprop iteration analysis and conver-gence without overtraining in Figs. 8 and 9 ).

The results in Table 4 show that the discriminatively trained models clearly outperform the ML trained baseline models, especially the models trained with the additional margin term. The strong decrease in word error rate (WER) for experiment setup abd-c might be due to the training data being separable for the given configurations, whereas the strong improvement for experiment abcde-e was expected because of the test set e being part of the training data.
In the following experiments, we additionally use a glyph-dependent model length estimation (GDL) as described in [ 7 , 8 ], resulting in an ML trained baseline model with 637 mixtures and 48k densities (cf. Sect. 2.2 ). The necessity of this model length estimation is visualized in Fig. 7 , where we use R-G-B background colors for the 0-1-2 HMM states (also cf. Fig. 3 ), respectively, from right-to-left: the bottom row images visualize an alignment of our baseline system (left) in comparison with the proposed GDL system (right).
By estimating glyph-dependent model lengths, the over-all mean of character length changed from 7.89 pixels (i.e., 2.66 pixels/state) to 6.18 pixels (i.e., 2.06 pixels/state) when downscaling the images to 16 pixels height while keeping theiraspectratio.ThuseverystateofanGDLcharactermodel has to cover less pixels due to the relative reduction of approx. 20% pixels.

In Figs. 8 and 9 , detailed WER and character error rate (CER) plots over M-MMI training iterations are shown, respectively, with Fig. 10 showing a combined WER/CER plot over M-MMI training iterations on the evaluation setup abcd-e . It can be observed that both WER and CER are smoothly and almost continuously decreasing with every Rprop iteration, and that about 30 Rprop iterations are opti-mal for the considered data sets.
 Continuous large-vocabulary line recognition. For the large-vocabulary line recognition task on the IAM database, our system uses a Kneser-Ney smoothed trigram language model [ 22 ] trained on the LBW text corpora (cf. Sect. 2.2 for visual model details and cf. [ 19 ] for a detailed description of the ML baseline system). Note that for discriminative train-ing,aweakenedunigramlanguagemodelisusedasexplained in Sect. 2.3 . The language model weighting factor  X  = 25 (cf. Eq. 1 ) and the word insertion penalty were determined empirically on the validation set using the ML trained mod-els. Again, the discriminative training is initialized with the respective ML trained baseline model and iteratively opti-mized using the Rprop algorithm (cf. Sect. 2.3 ). Results for discriminative training in comparison with our ML trained baseline system are shown in Table 5 . The lexicon size of 50k has been roughly optimized on the ML trained baseline system and used for all further experiments.
The results in Table 5 were obtained after 100 Rprop iter-ations, as shown for M-MMI/M-MPE in Fig. 11 . Note the smooth decrease in both WER and CER after every iteration. Similar figures are obtained with the unmodified MMI/MPE criteria. It can be observed that the margin modified criteria always slightly outperform their corresponding standard cri-teria, and that the MPE-based criteria outperform the MMI-based criteria, especially w.r.t. CER. However, the results in Table 5 support the hypothesis that the effect of the margin on such highly competitive large-vocabulary systems used for discriminative training is sometimes marginal [ 17 ]. 4.2 Second-pass decoding and unsupervised model In this section, we evaluate our discriminative training for unsupervised model adaptation during a second-pass decod-ing step.

In a first experiment, we used the complete first-pass out-put of the M-MMI system for an unsupervised model adap-tation. The results in Table 6 show that the M-MMI-based unsupervisedadaptationwithoutconfidencescannotimprove the system accuracy. With every Rprop iteration, the system is even more biased by the relatively large amount of wrong transcriptions in the adaptation corpus.

The discriminative M-MMI-conf training is initialized with the respective M-MMI trained model and iteratively optimizedusingtheRpropalgorithm(cf. Sect. 2.3 ). Usingthe word-confidences for M-MMI-conf-based model adaptation of our first-pass alignment to reject complete word segments (i.e., feature sequences X T 1 ) from the unsupervised adapta-tion corpus, the results in Table 6 show a slight improve-ment only in comparison with the M-MMI trained system. Figure 12 shows the resulting WER for different confidence threshold values and the corresponding number of rejected segments. For a confidence threshold of c = 0 . 5, more than 60% of the 6,033 segments of set e are rejected from the unsu-pervised adaptation corpus, resulting in a relatively small amount of adaptation data.

Usingthestate-confidencesforM-MMI-conf-basedmodel adaptation of our first-pass alignment to decrease the contri-bution of single frames (i.e., features X t ) during the itera-tive M-MMI-conf optimization process (cf. optimization in Sect. 2.3 ), the number of features for model adaptation is reduced by approximately 5% for a confidence threshold of c from the 6,033 test segments are considered during the opti-mization, and only 20,970 frames are rejected based on con-fidence thresholding (cf. also Fig. 5 ). Note that also the CER is decreased to 6.49%.

Interestingly, the supervised adaptation on test set e , where only the correct transcriptions of set e are used for an adapta-tion of the model trained using set abcd , can again decrease the WER of the system down to 2.06%, which is even better than an M-MMI optimization on the full training set abcde (cf. Table 4 ).
 In Figs. 13 and 14 , detailed WER and CER plots over M-MMI-conf training iterations are shown, respectively, with Fig. 15 showing a combined WER/CER plot over M-MMI-conf training iterations on the evaluation setup abcd-e (cf. initialization plots). In all cases, we estimated the state-confidences on the first-pass output using the M-MMI trained models. It can be observed that both WER and CER are slightly decreasing with every Rprop iteration and that between 10 and 15 Rprop iterations are optimal for the considered small and unsupervised labeled test data sets.

Table 7 shows the final results of our Arabic handwriting recognition system with additional glyph-dependent model length estimation (GDL) as described in [ 6 ]. Again, the WER of the GDL-based system can be decreased by our pro-posed M-MMI training during both decoding passes down to 14.55%.
 Due to the robustness of the confidence-and margin-based M-MMI-conf criterion against outliers, the proposed unsu-pervised and text-dependent model adaptation can be applied in an iterative manner by a re-initialization of the text tran-scriptions. In Fig. 15 , we re-initialize 2 times the model adap-tation process after 15 Rprop iterations. The results in Fig. 15 show the robustness of our approach, leading to an improved WER of 14.39%.
For the confidence-based unsupervised model adaptation approaches on the IAM database, we also measured the per-formance after 15 Rprop iterations. The results in Fig. 16 sug-gest that the often mentioned stronger robustness of the MPE criterion w.r.t. outliers than the MMI criterion [ 17 ] cannot be confirmed for continuous handwriting recognition within the proposed confidence-based M-MMI-conf and M-MPE-conf criteria, as both approaches achieve a similar performance: M-MMI-conf decreases the error rates from 31.63% WER / 11.82% CER down to 29.02% WER / 10.52% CER, i.e., a relative improvement in WER of 8%, whereas M-MPE-conf decreases from 30.07% WER / 10.92% CER down to 29.23% WER / 10.33% CER, i.e., a relative improvement in WER of 2%. Note that in both cases, the best unsu-pervised transcriptions of the unknown validation and test data from the M-MPE model have been used, but that the confidence-based model adaptation has been applied to the corresponding un-adapted models , i.e., M-MMI-conf to adapt the M-MMI trained model, and M-MPE-conf to adapt the M-MPE trained. This might explain the higher relative improvementincaseofM-MMI-confmodeladaptation.Also notethat,asexpected,theCERislowerforM-MPE-confthan for M-MMI-conf.

The number of rejected frames in Fig. 16 is reported in both cases for the testset only, where a confidence-based reduction by approximately 5% of the number of features for model adaptation is again a good choice.

It can be observed that both criteria are robust against out-liers, as the confidence threshold, although helpful for val-ues c threshold  X  0 . 9 (cf. Eq. 10 ), has only a small impact on the overall performance of the model adaptation pro-cedures. Interestingly, and opposed to the results for iso-lated word recognition in Table 6 , the performance is also improved if all data are used in M-MMI-conf/M-MPE-conf for model adaptation. M-MMI-conf in Fig. 16 seems to be less susceptible to unsuitable confidence threshold and can therefore be considered the better unsupervised model adap-tation approach if WER as evaluation criterion is important, otherwise M-MPE-conf might be the method of choice if CER as evaluation criterion is important. In particular, the achieved 29% WER for single and purely HMM-based sys-tem is one of the best known word error rates for this task (cf. Sect. 4.4 ). 4.3 Visual inspections The visualizations in Figs. 17 and 18 show training align-ments of Arabic words to their corresponding HMM states. The upper rows show the alignment to the ML trained model and the lower rows to the M-MMI trained models. We use R-G-B background colors for the 0-1-2 HMM states, respec-tively, from right-to-left. The position-dependent character model names (cf. Sect. 2.2 ) are written in the upper line, where the white-space models are annotated by  X  X i X  for  X  X ilence X ; the state numbers are written in the bottom line. Thus, HMM state-loops and state-transitions are represented by no-color-changes and color-changes, respectively.
It can be observed in Fig. 17 that especially the white-spaces, which can occur between compound words and pieces of Arabic words (PAW) [ 7 ], help in discriminating the isolated-(A), beginning-(B), or end-shaped (E) charac-ters of a word w.r.t. the middle-shaped (M) characters, where usually no white-spaces occur on the left or right side of the character (cf. [ 23 , 32 ] for more details about A/B/M/E shaped characters). The frames corresponding to the white-space part of the words are aligned in a more balanced way in Fig. 17 a, b using the M-MMI modeling (lower rows) opposed to ML modeling (upper rows): the proposed M-MMI mod-els learned that white-spaces help to discriminate different characters. This can even lead to a different writing variant choice without any white-space models [ 7 ] (see Fig. 17 c).
Note that we cannot know in advance in training if a white-space is used or not, and if so, how large it is, as it is not transcribed in the corpora and depends on the writer X  X  hand-writing style (e.g., cursive style used in Fig. 17 a). In Fig. 18 , unsupervised test alignments are compared. The upper rows show incorrectly recognized words by unsupervised alignments to the ML trained model, the lower rows correctly recognized words by unsupervised alignments to the M-MMI trained models. Due to the discriminatively trained character models, the alignment in Fig. 18 atothe M-MMI model is clearly improved over the ML model and the system opts for the correct compound-white-space writing variant [ 7 ]. In Fig. 18 b, again the alignment is improved by the discriminatively trained white-space and character models. Figure 18 c shows a similar alignment to the white-space model, but a clearly improved and cor-rect alignment to the discriminatively trained character models.
 Similar alignment observations can be made for the IAM database, especially for punctuation and white-space symbols. 4.4 Comparisons with other systems IFN/ENIT database and ICDAR Competitions. In Table 8 , we compare or own evaluation results on the ICDAR 2005 [ 25 ] setups (without any tuning on test data as explained in Sect. 4.2 ) and ICDAR 2009 [ 24 ] setups. It should be noted that the result for the abcd-e condition is one of the best known error rates in the literature [ 9 ].

The ICDAR 2009 test data sets that are unknown to all participants were collected for the tests of the ICDAR 2007 competition. The words are from the same lexicon as those of the IFN/ENIT database and written by writers, who did not contribute to the data sets before, and are separated into set f and set s . Our results (externally calculated by TU Braunschweig) in Table 8 ranked third at the ICDAR 2009 competition and are among the best purely HMM-based systems, as the A2iA and MDLSTM systems are hybrid system combinations or full neural network-based systems, respectively. Also note that our single HMM-based system is better than the independent A2iA systems (cf. [ 24 ]for more details), and that the results confirm that our proposed M-MMI-conf approach even generalizes well on the more difficult set s .

Notethe36% relativeimprovement inTable 8 weachieved in the recent ICFHR 2010 Arabic handwriting competition [ 26 ] with the proposed M-MPE training framework but an MLP-based feature extraction (not described here), and again without system-combinations.
 IAM Database. Summarizing results and comparisons of the proposed confidence-based model adaptation methods on the large-vocabulary line recognition task of the IAM database are reported in Table 9 . It can be seen that the per-formance of our ML trained baseline system [ 19 ] is among current state-of-the-art systems [ 3 , 10 ], and that our pro-posed confidence-and margin-based extensions of the dis-criminative MMI/MPE training criteria achieve the currently best known WERs/CERs for a purely HMM-based system using a very simple feature extraction. Even ensemble-based HMM approaches as proposed in [ 3 ] are outperformed by our approaches.
 5 Conclusions We presented a novel confidence-and margin-based dis-criminative training using a MMI/MPE training criterion for model adaptation in off-line handwriting recognition. The advantages of the proposed methods using an HMM-based multi-passdecodingsystemwereshownforArabichandwrit-ing on the IFN/ENIT corpus (isolated word recognition) and for Latin handwriting on the IAM corpus (large-vocabulary, continuous sentence recognition). Both approaches showed their robustness w.r.t. transcription errors and outperformed the maximum-likelihood (ML) trained baseline models.
We discussed an approach how to modify existing training criteria for handwriting recognition like, for example, MMI and MPE to include a margin term. The modified training criterion M-MMI was shown to be closely related to existing large margin classifiers (e.g., SVMs) with the respective loss function. This approach allows for the direct evaluation of the utility of the margin term for handwriting recognition. As expected, the benefit from the additional margin term clearly depends on the training conditions. The proposed discrim-inative training approach could outperform the ML trained system on all tasks.

The impact of different writing styles was dealt with a novel confidence-based discriminative training for model adaptation, where the usage of state-confidences during the iterative optimization process based on the modified M-MMI-conf criterion could decrease the word error rate on the IFN/ENIT database by 33% relative in comparison with a ML trained system.

On the IAM database, similar improvements could be observed for the proposed M-MMI-conf and M-MPE-conf criteria, leading to a WER decrease by 25% relative in comparison with a maximum-likelihood trained system, and representingoneofthebestknown29%WERintheliterature for a single and purely HMM-based system. In supervised training, the M-MPE criterion could outperform the M-MMI approach, whereas in unsupervised and confidence-based model adaptation, the M-MMI-conf approach could clear the initial gap to the M-MPE trained model.
 Interesting for further research will remain hybrid HMM/ ANN approaches [ 10 , 13 ], combining the advantages of large and non-linear context modeling via neural networks while profiting from the Markovian sequence modeling. This is also supported by the 36% relative improvement we could achieve in the ICFHR 2010 Arabic handwriting competition [ 26 ] with the proposed framework but an MLP-based feature extraction.
 References
