 Much work has been done on feature selection. Existing methods are based on document frequency, such as Chi-Square Statistic, Information Gain etc. However, these meth-ods have two shortcomings: one is that they are not reliable for low-frequency terms, and the other is that they only count whether one term occurs in a document and ignore the term frequency. Actually, high-frequency terms within a specific category are often regards as discriminators.
This paper focuses on how to construct the feature selec-tion function based on term frequency, and proposes a new approach based on t -test, which is used to measure the di-versity of the distributions of a term between the specific category and the entire corpus. Extensive comparative ex-periments on two text corpora using three classifiers show that our new approach is comparable to or or slightly better than the state-of-the-art feature selection methods (i.e.,  X  and IG) in terms of macro-F 1 and micro-F 1 .
 H.4 [ Information Systems Applications ]: Miscellaneous feature selection, term frequency, t -test, text classification
Text classification (TC) is to assign new unlabeled natural language documents to predefined thematic categories [13]. Many classification algorithms have been proposed for TC, e.g., k -nearest neighbors [20], centroid-based classifier [7], and support vector machines (SVMs) [3].

Generally, text feature space is often sparse and high-dimensional. For instance, the dimensionality of a moderate-sized text corpus can reach up to tens or hundreds of thou-sands. The high dimensionality of feature space will cause the X  X urse of dimensionality X , increase the training time, and affect the accuracy of classifiers [13, 6, 20]. Therefore, fea-ture selection techniques are proposed to reduce the dimen-sionality under the premise of guaranteeing the performance of classifiers. Existing feature selection methods are based on statistical theory and information theory, such as  X  2 MI, and ECE. The theoretical basis of the four methods is sound, but the performances of these methods on TC tasks are different. Both  X  2 and IG often achieved better accu-racy than MI and document frequency (DF) [20]. However, other authors suspected the performance of IG on skewed text corpora [11].

Besides the classical methods, many improved methods have been proposed. For example, Yang et al. [19] considered the terms whose relative term frequency was larger than a predefined threshold  X  , and then modified the IG formula to select features. Forman [5] proposed the Bi-Normal Separa-tion (BNS) method, which used the standard Normal distri-bution X  X  inverse cumulative probability function to construct feature selection function. Uguz [15] proposed a two-stage feature selection method for TC by combining IG, principal component analysis and genetic algorithm. More and more methods have been generated, such as, mr2PSO [16], and improved TFIDF method [17]. It is worth noting that t -test has been used for gene expression and genotype data [14, 21]. However, the variable in gene expression or genotype data is different from that in te xt data, i.e., the term fre-quency. Thus we try to validate the role of t -test in text feature selection.

From document frequency perspective, the above methods almost use DF sufficiently. However, no efficient method is proposed from term frequency perspective. It inspires our motivation of this paper. Our paper makes the following contributions: (1) Using central limit theorem (CLT), we prove that the frequency distribution of a term within a specific category or within the entire collection will be approximately normally distributed. (2) We model the diversity of the frequency of a term between the specific category and the entire corpus with t -test. It means that if the distribution of one term within the specific category is obviously different with that within the entire corpus, the term can be considered to be feature. (3) We verify our new approach on two common text cor-pora with three well-established classifiers. The experiments show that our approach is comparable to or even slightly better than the state-of-the-art  X  2 and ECE in terms of both macro-F 1 and micro-F 1 , and it outperforms IG and MI methods significantly on unbalanced text corpus. Many feature selection approaches have been proposed in TC tasks, but we only give detailed analysis on four methods because they have been widely used and achieved better per-formance, the formulae can be found in Refs [20, 5, 6]. They are: Chi-Square Statistic (  X  2 ), Information Gain (IG), Mu-tual Information (MI), and Expected Cross-Entropy (ECE).  X  2 was proposed by Pearson early in 1900 [20]. The  X  2 statistic is used to measure the lack of independence between t and C j , and can be regards as the  X  2 distribution with one degree of freedom. In real-world corpus,  X  2 statistic is based, however, on several assumptions that do not hold for most textual analysis [4]. For instance, if term t 1 occurs in 50% documents of a specific category C j and term t 2 occurs in 49% documents, but the frequency of t 2 is much higher than that of t 1 . Experts often think term t 2 should have more discriminating power than t 1 in the specific category C .  X  2 , however, will be prone to select term t 1 as feature, rather than t 2 . The problem is that  X  2 is not reliable for low-frequency terms [4].

The weakness of MI is that the score is strongly influenced by the marginal probabilities of terms, because rare terms will have a higher score than common terms. Therefore, the scores are not comparable across terms of widely differing frequency [20, 9]. Besides, MI gives longer documents higher weights in the estimation of the feature scores.

IG was firstly used as attribute selection measure in deci-sion tree [20]. This measure is from entropy in information theory, which studies the value or  X  X nformation content X  of messages. IG is defined as the difference between the origi-nal information requirement (i.e., based on just the propor-tion of classes) and the new requirement (i.e., obtained after partitioning on term t i ). IG is also called average mutual information. The weakness of IG method is that it prefer-s to select terms distributed in many categories, but these terms have less discriminating power in TC tasks. Differing from IG, Expected Cross-Entropy (ECE) [8] only considers the terms occurred in a document and ignores the absent terms.

As we know, if a term (except s top words) occurs frequent-ly within a specific category, the term should be considered as a feature or discriminator of the category. For example,  X  X omputer X  occurs frequently in the IT category. However, the above methods are all based on document frequency, and ignore the term frequency. In next section, we will propose a new approach based on term frequency, and it can capture the information of high-frequency terms.
The t -test, namely the student t -test, is often used to as-sess whether the means of two classes are statistically dif-ferent from each other by calculating a ratio between the difference of two class means and the variability of the two classes [21]. In this section, we explain why the averaged term frequency within a single category or in the whole cor-pus is approximately normal using Lindeberg-Levy central limit theorems, and then how the t -test is constructed based on the averaged term frequencies.

Let us consider the term frequency in text corpus con-sisting of n documents. Given a vocabulary V, the term frequency ( tf ij )ofaterm t i (1  X  i  X | V | )inthe j th docu-ment (1  X  j  X  N ) can be considered as a random variable, which subjects to some unknown distribution, e.g., multi-nomial model [10]. In the multinomial model, a document is an ordered sequence of word events drawn from the same vocabulary V, and the probability of each word event in a document is independent of the word X  X  context and position in the document. Therefore, each document d j is drawn from a multinomial distribution of words with as many in-dependent trials [10]. That is, the occurrence of one term in each document is dominated by a multinomial function. Then, (1) Let { tf i 1 ,  X  X  X  ,tf iN } be a random sample of size N , where N is the number of documents in the collection, and tf ij (0  X  j  X  N ) is the term frequency of t i in j th doc-ument. That is, a sequence of independent and identically distributed random variables with expected values  X  i = Np and variances  X  2 i = Np i (1  X  p i ), where p i is the distributed probability of term t i in the collection. Each sample belongs to one of K classes 1 , 2 ,  X  X  X  ,K . (2) Let tf i = 1 N ( tf i 1 + tf i 2 +  X  X  X  + tf iN )bethesample average of these random variables in terms of t i . (3) Let tf ki = N j =1 tf ij I ( d j ,C k ) /N k , ( k =1 , the sample average of term t i in category C k ,where I ( d is an indicator to discriminate whether document d j belongs to C k ,and N k is the total samples in class k . According to Lindeberg-Levy central limit theorems (LV CLT) [1], tf i is approximately normal with mean  X  i and variance 1 N  X  2 i , denoted as  X  N (  X  i , 1 N  X  2 i ); And tf imately normal with mean  X  i and variance 1 N k  X  2 i , denoted as  X 
Then we know that tf ki  X  tf i is also approximately normal distributed with mean 0 and variance ( 1 N k  X  1 N )  X  2 i variance (Var) is induced as follows:
Besides, we define the pooled within-class deviation as follows:
According to the definition of the t -test [18], we construct the following formula: where s i is standard deviation, and m k = 1 N k  X  1 N .
The Eq. 3 is used to measure whether the means of the two normal distributions (i.e., tf ki and tf i ) have the statistically significant difference. The bigger the value of t  X  test ( t is, the larger the difference of the means is. For some thresh-old  X  ,ifthe t  X  test ( t i ,C k ) &lt; X  ,itimpliesthattheaveraged frequency of term t i in the specific category C k has the same or similar mean with that in the entire corpus; Otherwise, it implies the averaged frequency of term t i in the specific category C k is significantly different from that in the entire corpus, and the term has more discriminating power for the specific category C k . Compared with the average of term frequency in the entire corpus, the term t i occurred many or few times in C k can be considered as the feature of cate-gory C k .

We combine the category-specific scores of a term into two alternate ways:
Reuters-21578 1 : The Reuters corpus is a widely used benchmark collection [4, 5, 20, 19]. According to the ModApte split, we get a collection of 52 categories (9100 documents) after removing unlabeled documents and documents with more than one class label. Reuters-21578 is a very skewed data set. Altogether 319 stop words, punctuation and num-bers areremoved. All letters are converted into lowercase, and the word stemming is applied. 20Newsgroup 2 : The Newsgroup is also a widely used benchmark [4, 5, 20], and consists of 19,905 documents, which are uniformly distributed in twenty categories. We randomly divide it into training and test sets by 2:1, and only keep  X  X ubject X ,  X  X eyword X  and  X  X ontent X . The stop words list has 823 words, and we filter words containing non-characters. All letters are converted into lowercase and word stemming is applied.

Each document is represented by a vector in the term space, and term weighting is calculated by standard ltc [12], and then the vector is normalized to have one unit length.
In our experiments, we choose three well-established clas-sifiers for the comparison purpose. They are: Support Vec-tor Machines (SVMs) [3], weighted kNN classifier ( k NN) [20], and classic Centroid-based Classifier (CC) [7]. The SVMs implementation we use is LIBSVM [2] with linear kernels. For k NN, we set k = 10 [20]. The similarity measure we use is the cosine function.
We measure the effectiveness of classifiers in terms of F widely used for TC. For multi-class task, F 1 is estimated in two ways, i.e., the macro-averaged F 1 (macro-F 1 )andthe micro-averaged F 1 (micro-F 1 ), as the following: where F 1 ( i )isthe F 1 value of the predicted i th class, and  X  p and  X  r are the precision and recall values across all classes,
Available on http://ronaldo.cs.tcd.ie/esslli07/sw/step01.tgz
Available on http://kdd.ics.uci.edu/databases/20newsgroup. respectively. In general, macro-F 1 gives the same weight to all categories. In contrast, micro-F 1 gives the same weight to each instance, which can be dominated by the performance of common or majority categories.
Firstly, We show one case study of t -test in real-world corpus. Tables 1 lists the scores of seven different feature selection functions for the selected four terms in category  X  X cq X  from the real-life corpus, i.e., Reuters-21578. Based on the literal meaning, the first two terms, i.e.,  X  X cquir X  and  X  X take X , are closely related to the content of category  X  X cq X , while the last two terms, i.e.,  X  X ayout X  and  X  X ividend X , be-long to other category. However, according to the  X  2 ,ECE, and TF methods, we wrongly select  X  X cquir X  and  X  X ividend X  as the features of category  X  X cq X , whereas t -test, IG and MI select the features correctly.

Table 1: The feature values of four terms in  X  X cq X .
Then, we show the performance of t -test on two corpora with three classifiers. For Reuters-21578, the number of fea-ture space is all, 17000, 15000, 13000, 11000, 10000, 8000, 6000, 4000, and 2000, respectively, accounting to ten groups of data sets. On 20 Newsgroup corpus, the original feature space reaches up to 210 thousand and we only select less terms as features to save training time. The dimensional-ity of feature space is all, 2000, 1500, 1000, 500, and 200, respectively, accounting to six groups of data sets.
For  X  2 ,MI,and t -test methods, we tested the two alter-native combinations, i.e., averaged and maximized ways. We observed that the averaged way was always better than the maximized way for multi-classes problem. Thus we only report the best results of three methods.
The macro-F 1 and micro-F 1 of five methods with k NN on imbalanced Reuters-21578 are shown in Fig. 1, Fig. 2, respectively. It is clear that t -test,  X  2 ,andECEachieve evidently better performance than MI and IG in terms of macro-F 1 . However, the diversity among the three methods is small. As shown in Fig. 1, when the number of feature space is larger than 13000,  X  2 , and ECE is a little better than t -test; However, when the number of features falls in [8000, 13000], t -test performs the best macro-F 1 .
The micro-F 1 of five methods increases as the number of features decreases, as shown in Fig. 2. It demonstrates that k NN often obtains better performance with less features. Our t -test method performs consistently the best in distinct feature dimensionality, and the highest micro-F 1 of t -test is 89.8% when the number of features is 4000, which improves up to 4.2% than  X  2 . IG achieves the worst performance in the all experiments on skewed corpus with k NN.

As shown in Fig. 1 and Fig. 2, for unbalanced multi-class tasks, we find IG is inferior to MI in terms of both macro-F Figure 1: The comparative curves of five methods with k NN on Reuters-21578 in terms of macro-F 1 . Figure 2: The comparative curves of five methods with k NN on Reuters-21578 in terms of micro-F 1 . and micro-F 1 , whereas IG is superior to MI for binary clas-sification tasks according to the comparative experiments of Yang et al [20]. The conflict shows that feature selection methods depends on the practical classification problem. Figure 3: The comparative curves of five methods with k NN on 20 Newsgroup in terms of micro-F 1 .
Because macro-F 1 on balanced corpus is close to micro-F , we only show the results of micro-F 1 on 20 Newsgroup. As showninFig.3,themicro-F 1 of both  X  2 and IG are slightly better than our t -test method, and the four methods are obviously better than MI. Especially, the performance of IG is comparable to  X  2 , and ECE on balanced corpus.
Fig. 4 and Fig. 5 depict the macro-F 1 and micro-F 1 of different methods on the Reuters-21578 corpus using SVMs. The t -test,  X  2 , and ECE methods perform similar perfor-mances, which are better than IG and MI methods. Mean-while, the macro-F 1 scores of three methods increase as the number of features reduces. It is worth noting that MI does better than other methods when the number of features is in [15,000, 24,411], and then MI falls dramatically.
The performance of these methods in terms of micro-F 1 on Reuters-21578 corpus is shown in Fig. 5. The micro-F 1 Figure 4: The macro-F 1 of different methods on Reuters-21578 using SVMs. Figure 5: The micro-F 1 of different methods on Reuters-21578 using SVMs. points of different feature selection methods show a tendency to increase as the number of the features decreases. Howev-er, these methods show consistent performance in micro-F and the t -test method is still the best among these methods. Figure 6: The micro-F 1 of different methods on 20 Newsgroup using SVMs.
 Fig. 6 depicts the micro-F 1 of different methods on the 20 Newsgroups using SVM. The trends of the curves are similar to those in Fig. 3. The t -test,  X  2 , IG, and ECE achieve similar performances, which are better than MI. Our t -test is slightly better than others.
For centroid-based classifier, the macro-F 1 of five methods isshowninFig.7.Wecanobservethat  X  2 ,ECE,and t -test do better than MI and IG methods, and  X  2 is slightly better than ECE and t -test. The same conclusion can be done in terms of micro-F 1 , as shown in Fig. 8.

Meanwhile, our t -test is slightly better than  X  2 ,ECE,and Figure 7: The macro-F 1 of five methods on Reuters-21578 using centroid-based classifier. Figure 8: The micro-F 1 of five methods on Reuters-21578 using centroid-based classifier.
 IG methods on 20 Newsgroup corpus. The four methods outperform the MI method significantly. Figure 9: The micro-F 1 of five methods on 20 News-group using centroid-based classifier.
In this paper, we proposed a new feature selection method basedontermfrequencyand t -test. Then we compare our approach with the state-of-the-art methods on two corpo-ra using three classifiers in terms of macro-F 1 and micro-F . Extensive experiments have indicated that our new ap-proach offers comparable performance with  X  2 , and ECE, even slightly better than them. In future work, we will ver-ify our method on more text collections. [1] P. Billingsley. Probability and Measure (Third ed.) . [2] C. Chang and C. Lin. Libsvm: a library for support [3] C. Cortes and V. Vapnik. Support-vector networks. [4] T. Dunning. Accurate methods for the statistics of [5] G. Forman. An extensive empirical study of feature [6] I. Guyon and A. Elisseeff. An introduction to variable [7] E.-H. Han and G. Karypis. Centroid-based document [8] D. Koller and M. Sahami. Hierarchically classifying [9] S. Li, R. Xia, C. Zong, and C. Huang. A framework of [10] A. McCallum and K. Nigam. A comparison of event [11] D. Mladenic and M. Grobelnik. Feature selection for [12] G. Salton and C. Buckley. Term-weighting approaches [13] F. Sebastiani. Machine learning in automated text [14] R. Tibshirani, T. Hastie, B. Narasimhan, and G. Chu. [15] H. Uguz. A two-stage feature selection method for [16] A. Unler, A. Murat, and R. B. Chinnam. mr2pso: A [17] Y.-Q. Wei, P.-Y. Liu, and Z.-F. Zhu. A feature [18] S. William. The probable error of a mean. Biometrika , [19] S.-M. Yang, X. Wu, and Z. Deng. Relative [20] Y.-M. Yang and J.-P. Pedersen. A comparative study [21] N.-N. Zhou and L.-P. Wang. A modified t-test feature
