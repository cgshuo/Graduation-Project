 BBNTechnologies UniversityofIllinoisat Urbana-Champaign MicrosoftResearch performance.
 and achieves the highest F 1 score among 19 participants. 1. Introduction
Semanticparsingofsentencesisbelievedtobeanimportanttaskontheroadtonatural language understanding, and has immediate applications in tasks such as informa-tion extraction and question answering. Semantic Role Labeling (SRL) is a shallow semanticparsingtask,inwhichforeachpredicateinasentence,thegoalistoidentify all constituents that fill a semantic role, and to determine their roles (Agent, Patient,
Instrument,etc.)andtheiradjuncts(Locative,Temporal,Manner,etc.). 2005),whichprovidesalargehuman-annotatedcorpusofverbpredicatesandtheirar-guments,hasenabledresearcherstoapplymachinelearningtechniquestodevelopSRL systems (Gildea and Palmer 2002; Chen and Rambow 2003; Gildea and Hockenmaier 2003;Pradhanetal.2003;Surdeanuetal.2003;Pradhanetal.2004;XueandPalmer2004; Koomen et al. 2005). However, most systems rely heavily on full syntactic parse trees.
Therefore, the overall performance of the system is largely determined by the quality of the automatic syntactic parsers of which the state of the art (Collins 1999; Charniak 2001)isstillfarfromperfect.
 do not provide as much information as a full syntactic parser, have been shown to be more robust in their specific tasks (Li and Roth 2001). This raises the very natural and interesting question of quantifying the importance of full parsing information to semanticparsingandwhetheritispossibletouseonlyshallowsyntacticinformationto buildanoutstandingSRLsystem.
 thePennTreebanksyntacticparsetrees,itisnotclearhowimportantsyntacticparsing is for an SRL system. To the best of our knowledge, this problem was first addressed byGildeaandPalmer(2002).Intheirattempttouselimitedsyntacticinformation,the parser they used was very shallow  X  X lauses were not available and only chunks were used.Moreover,thepruningstagetherewasverystrict X  X nlychunkswereconsidered asargumentcandidates.Thisresultsinover60%oftheactualargumentsbeingignored. Consequently,theoverallrecallintheirapproachwasverylow.
 ignored until the recent CoNLL-2004 shared task competition (Carreras and M ` arquez 2004). In that competition, participants were restricted to using only shallow parsing information,whichincludedpart-of-speechtags,chunks,andclauses(thedefinitionsof chunksandclausescanbefoundinTjongKimSangandBuchholz[2000]andCarreras et al. [2002], respectively). As a result, the performance of the best shallow parsing X  basedsystem(Haciogluetal.2004)inthecompetitionisabout10pointsinF bestsystemthatusesfullparsinginformation(Koomenetal.2005).However,thisisnot the outcome of a true and fair quantitative comparison. The CoNLL-2004 shared task usedonlyasubsetofthedatafortraining,whichpotentiallymakestheproblemharder.
Furthermore, an SRL system is usually complicated and consists of several stages. It wasstillunclearhowmuchsyntacticinformationhelpsandpreciselywhereithelpsthe most.
 system that incorporates a level of global inference on top of the relatively common processing steps. This inference step allows us to incorporate structural and linguistic constraints over the possible outcomes of the argument classifier in an easy way. The inference procedure is formalized via an Integer Linear Programming framework and is shown to yield state-of-the-art results on this task. Second, we provide a fair com-parison between SRL systems that use full parse trees and systems that only use shal-lowsyntacticinformation.Aswithourfullsyntacticparse X  X asedSRLsystem(Koomen etal.2005),ourshallowparsing X  X asedSRLsystemisbasedonthesystemthatachieves very competitive results and was one of the top systems in the CoNLL-2004 shared taskcompetition(CarrerasandM ` arquez2004).Thiscomparisonbringsforwardacare-fulanalysisofthesignificanceoffullparsinginformationintheSRLtask,andprovides anunderstandingofthestagesintheprocessinwhichthisinformationmakesthemost difference. Finally, to relieve the dependency of the SRL system on the quality of 258 automaticparsers,wesuggestawaytoimprovesemanticrolelabelingsignificantlyby developingaglobalinferencealgorithm,whichisusedtocombineseveralSRLsystems basedondifferentstate-of-the-artfullparsers.Thecombinationprocessisdonethrough a joint inference stage, which takes the output of each individual system as input and generatesthebestpredictions,subjecttovariousstructuralandlinguisticconstraints.
Therefore,tomaketheconclusionsofourexperimentalstudyasapplicableaspossible to general SRL systems, the architecture of our SRL system follows the most widely usedtwo-stepdesign.Inthefirststep,thesystemistrainedtoidentifyargumentcandi-datesforagivenverbpredicate.Inthesecondstep,thesystemclassifiestheargument candidatesintotheirtypes.Inaddition,itisalsoasimpleproceduretopruneobvious non-candidates before the first step, and to use post-processing inference to fix incon-sistentpredictionsafterthesecondstep.Thesetwoadditionalstepsarealsoemployed byoursystem.
 bycomparingtheirimpactateachstageoftheprocess.Specifically,ourgoalistoinvesti-gateatwhatstagefullparsinginformationismosthelpfulrelativetoashallowparsing X  basedsystem.Therefore,ourexperimentsweredesignedsothatthecomparedsystems are as similar as possible, and the addition of the full parse tree X  X ased features is the only difference. The most interesting result of this comparison is that although each stepoftheshallowparsinginformation X  X asedsystemexhibitsverygoodperformance, the overall performance is significantly inferior to the system that uses full parsing information. Our explanation is that chaining multiple processing stages to produce of the information passed from one stage to the other is a decisive issue, and it is not necessarily judged simply by considering the F-measure. We conclude that, for the system architecture used in our study, the significance of full parsing information comesintoplaymostlyatthepruningstage,wherethecandidatestobeprocessedlater are determined. In addition, we produce a state-of-the-art SRL system by combining differentSRLsystemsbasedontwoautomaticfullparsers(Collins1999;Charniak2001), which achieves the best result in the CoNLL-2005 shared task (Carreras and M ` arquez 2005).
 semanticrolelabelinginmoredetail.Section3describesthefour-stagearchitectureof our SRL system, which includes pruning, argument identification, argument classifi-cation, and inference. The features used for building the classifiers and the learning algorithm applied are also explained there. Section 4 explains why and where full parsing information contributes to SRL by conducting a series of carefully designed experiments.Inspiredbytheresult,weexaminetheeffectofinferenceinasinglesystem andproposeanapproachthatcombinesdifferentSRLsystemsbasedonjointinference in Section 5. Section 6 presents the empirical evaluation of our system in the CoNLL-2005 shared taskcompetition. Afterthat, wediscuss therelated workinSection7and concludethispaperinSection8. 2. The Semantic Role Labeling (SRL) Task
Thegoalofthesemanticrolelabelingtaskistodiscoverthepredicate X  X rgumentstruc-tureofeachpredicateinagiveninputsentence.Inthiswork,wefocusonlyontheverb predicate.Forexample,givenasentence I left my pearls to my daughter-in-law in my will , thegoalistoidentifythedifferentargumentsoftheverbpredicate left andproducethe output: HereA0representsthe leaver ,A1representsthe thing left ,A2representsthe beneficiary ,
AM-LOC is an adjunct indicating the location of the action, and V determines the boundariesofthepredicate,whichisimportantwhenapredicatecontainsmanywords, forexample,aphrasalverb.Inaddition,eachargumentcanbemappedtoaconstituent initscorrespondingfullsyntacticparsetree.
 therearesixdifferenttypesofargumentslabeledasA0 X  X 5andAA.Theselabelshave different semantics for each verb and each of its senses as specified in the PropBank
Framefiles.Inaddition,therearealso13typesofadjunctslabeledasAM-adj where adj specifiestheadjuncttype.Forsimplicityinourpresentation,wewillalsorefertothese adjuncts as arguments. In some cases, an argument may span over different parts of a sentence; the label C-arg is then used to specify the continuity of the arguments, as showninthisexample:
Insomeothercases,anargumentmightbearelativepronounthatinfactreferstotheac-tualagentoutsidetheclause.Inthiscase,theactualagentislabeledastheappropriate argumenttype, arg ,whiletherelativepronounisinsteadlabeledasR-arg .Forexample, for the same labels, the task of discovering the complete set of semantic roles should involve not only identifying these labels, but also the underlying sense for a given verb. However, as in all current SRL work, this article focuses only on identifying the boundariesandthelabelsofthearguments,andignorestheverbsensedisambiguation problem.
 ofPropBankI,corearguments(A0 X  X 5andAA)occupy71.26%ofthearguments,where the largest parts are A0 (25.39%) and A1 (35.19%). The rest mostly consists of adjunct arguments (24.90%). The continued (C-arg ) and referential (R-arg ) arguments are rela-tivelyfew,occupying1.22%and2.63%,respectively.FormoreinformationonPropBank and the semantic role labeling task, readers can refer to Kingsbury and Palmer (2002) andCarrerasandM ` arquez(2004,2005).
 lapping argumentstobethosethatsharesomeoftheirparts.Anargumentisconsidered embedded inanotherargumentifthesecondargumentcompletelycoversthefirstone.
Argumentsare exclusively overlapping iftheyareoverlappingbutarenotembedded. 3. SRL System Architecture
AdheringtothemostcommonarchitectureforSRLsystems,ourSRLsystemconsistsof fourstages: pruning , argument identification , argument classification ,and inference .
In particular, the goal of pruning and argument identification is to identify argument candidates for a given verb predicate. In the first three stages, however, decisions are independently made for each argument, and information across arguments is not 260 incorporated. The final inference stage allows us to use this type of information along withlinguisticandstructuralconstraintsinordertomakeconsistentglobalpredictions. tanceofsyntacticparsinginSRL,althoughdifferentinformationandfeaturesareused.
Throughout this article, when full parsing information is available, we assume that thesystemispresentedwiththefullphrase-structureparsetreeasdefinedinthePenn
Treebank(Marcus,Marcinkiewicz,andSantorini1993)butwithouttraceandfunctional tags. On the other hand, when only shallow parsing information is available, the full parsetreeisreducedtoonlythechunksandtheclauseconstituents.
 chunks are obtained by projecting the full parse tree onto a flat tree; hence, they are closely related to the base phrases. Chunks were not directly defined as part of the standardannotationofthetreebank,but,rather,theirdefinitionwasintroducedinthe
CoNLL-2000sharedtaskontextchunking(TjongKimSangandBuchholz2000),which aimedtodiscoversuchphrasesinordertofacilitatefullparsing.A clause ,ontheother hand, is the clausal constituent as defined by the treebank standard. An example of chunksandclausesisshowninFigure1. 3.1 Pruning
When the full parse tree of a sentence is available, only the constituents in the parse tree are considered as argument candidates. Our system exploits the heuristic rules introduced by Xue and Palmer (2004) to filter out simple constituents that are very unlikelytobearguments.Thispruningmethodisarecursiveprocessstartingfromthe parent of the verb, and collects the siblings again. The process goes on until it reaches theroot.Inaddition,ifaconstituentisaPP(prepositionalphrase),itschildrenarealso collected.Forexample,inFigure1,ifthepredicate(targetverb)is assume ,thepruning heuristic will output: [ PP by John Smith who has been elected deputy chairman ], [
Smith who has been elected deputy chairman ],[ VB be ],[ MD 3.2 Argument Identification
The argument identification stage utilizes binary classification to identify whether a candidate is an argument or not. When full parsing is available, we train and apply the binary classifiers on the constituents supplied by the pruning stage. When only shallow parsing is available, the system does not have a pruning stage, and also does nothaveconstituentstobeginwith.Therefore,conceptually,thesystemhastoconsider allpossiblesubsequences(i.e.,consecutivewords)inasentenceaspotentialargument candidates.Weavoidthisbyusingalearningschemethatutilizestwoclassifiers,oneto predict the beginnings of possible arguments, and the other the ends. The predictions arecombinedtoformargumentcandidates.However,wecanemployasimpleheuristic to filter out some candidates that are obviously not arguments. The final predication includesthosethatdonotviolatethefollowingconstraints. 1. Argumentscannotoverlapwiththepredicate. 2. Ifapredicateisoutsideaclause,itsargumentscannotbeembeddedin 3. Argumentscannotexclusivelyoverlapwiththeclauses. cannot take itself or any constituents that contain itself as arguments. The other two constraints are due to the fact that a clause can be treated as a unit that has its own verb X  X rgumentstructure.Ifaverbpredicateisoutsideaclause,thenitsargumentcan onlybethewholeclause,butmaynotbeembeddedinorexclusivelyoverlapwiththe clause.
 shallowparsingsettingsareallbinaryfeatures,whicharedescribedsubsequently. 3.2.1 Features Used When Full Parsing is Available. Most of the features used in our system are common features for the SRL task. The creation of PropBank was inspired by the works of Levin (1993) and Levin and Hovav (1996), which discuss the relation between syntactic and semantic information. Following this philosophy, the features aim to indicate the properties of the predicate, the constituent which is an argument candidate, and the relationship between them through the available syntactic infor-mation. We explain these features herein. For further discussion of these features, we 262 refer the readers to the article by Gildea and Jurafsky (2002), which introduced these features.
 stituentsinparsetrees.However,insomecases,weneedtoconsideranargumentthat does not exactly correspond to a constituent, for example, in our experiment in Sec-tion4.2wherethegold-standardboundariesareusedwiththeparsetreesgeneratedby anautomaticparse.Insuchcases,iftheinformationontheconstituent,suchasphrase type,needstobeextracted,thedeepestconstituentthatcoversthewholeargumentwill beused. For example, in Figure 1,the phrase type for by John Smith isPP,anditspath featuretothepredicate assume isPP  X  VP  X  VBN.
 to be useful for the systems by exploiting other information in the absence of the full parse tree information (Punyakanok et al. 2004), and, hence, can be helpful in conjunctionwiththefeaturesextractedfromafullparsetree.Theyalsoaimtoencode thepropertiesofthepredicate,theconstituenttobeclassified,andtheirrelationshipin thesentence.
 264 into a new feature. For example, the conjunction of the predicate and path features for the predicate assume and the constituent [ S who has been elected deputy chairman ]in
Figure1is(S  X  NP  X  PP  X  VP  X  VBN, assume ). 3.2.2 Features Used When Only Shallow Parsing is Available. Most features used here are similartothoseusedbythesystemwithfullparsinginformation.However,forfeatures that need full parse trees in their extraction procedures, we either try to mimic them withsomeheuristicrulesordiscardthem.Thedetailsofthesefeaturesareasfollows. 3.3 Argument Classification This stage assigns labels to the argument candidates identified in the previous stage.
A multi-class classifier is trained to predict the types of the argument candidates. In addition, to reduce the excessive candidates mistakenly output by the previous stage, theclassifiercanalsolabelanargumentas X  X ull X (meaning X  X otanargument X )todis-cardit.
 stage.However,whenfullparsingisavailable,anadditionalfeatureintroducedbyXue andPalmer(2004)isused.

The learning algorithm used for training the argument classifier and argument iden-tifier is a variation of the Winnow update rule incorporated in SNoW (Roth 1998; Carlsonetal.1999),amulti-classclassifierthatistailoredforlargescalelearningtasks.
SNo Wlearns a sparse network of linear functions, in which the targets (argument borderpredictionsorargumenttypepredictions,inthiscase)arerepresentedaslinear functions over a common feature space; multi-class decisions are done via a winner-take-all mechanism. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of separating the data with a large margin separator (Dagan, Karov, and Roth 1997; Grove and Roth 2001; Zhang, Damerau, and
Johnson2002)andvoted(averaged)weightvector(FreundandSchapire1999;Golding andRoth1999). probabilities.Ifthereare n classesandtherawactivationofclass i is act estimationforclass i is didates used to generate the training examples are obtained from the output of the argument identifier, not directly from the gold-standard corpus. In this case, we au-tomaticallyobtainthenecessaryexamplestolearnforclass X  X ull. X  3.4 Inference
Inthepreviousstages,decisionswerealwaysmadeforeachargumentindependently, ignoring the global information across arguments in the final output. The purpose of the inference stage is to incorporate such information, including both linguistic and structural knowledge, such as  X  X rguments do not overlap X  or  X  X ach verb takes at most one argument of each type. X  This knowledge is useful to resolve any incon-sistencies of argument classification in order to generate final legitimate predictions.
We design an inference procedure that is formalized as a constrained optimization problem, represented as an integer linear program (Roth and Yih 2004). It takes as input the argument classifiers X  confidence scores for each type of argument, along with a list of constraints. The output is the optimal solution that maximizes the lin-ear sum of the confidence scores, subject to the constraints that encode the domain knowledge.
 different SRL systems, as we will show in Section 5. In this section we first introduce theconstraintsandformalizetheinferenceproblemforthesemanticrolelabelingtask.
Wethendemonstrate howweapplyintegerlinearprogramming (ILP)togeneratethe globallabelassignment. 3.4.1 Constraints over Argument Labeling. Formally, the argument classifiers attempt to assignlabelstoasetofarguments, S 1: M ,indexedfrom1to M .Eachargument S anylabelfromasetofargumentlabels, P ,andtheindexedsetofargumentscantakea setoflabels, c 1: M  X  P M .Ifweassumethattheclassifiersreturnascorescore( S correspondstothelikelihoodofargument S i beinglabeled c i unalteredinferencetaskissolvedbymaximizingtheoverallscoreofthearguments, turalconsiderations,oursystemseekstooutputa legitimate labelingthatmaximizesthis score.Specifically,itcanbethoughtofasifthesolutionspaceislimitedthroughtheuse of a filter function, F , which eliminates many argument labelings from consideration. 266 Here,weareconcernedwithglobalconstraintsaswellasconstraintsonthearguments.
Therefore,thefinallabelingbecomes
When the confidence scores correspond to the conditional probabilities estimated by the argument classifiers, the value of the objective function represents the expected numberofcorrectargumentpredictions.Hence,thesolutionofEquation(2)istheone thatmaximizesthisexpectedvalueamongalllegitimateoutputs.
 1. Argumentscannotoverlapwiththepredicate. 2. Argumentscannotexclusivelyoverlapwiththeclauses. 3. Ifapredicateisoutsideaclause,itsargumentscannotbeembeddedin 4. Nooverlappingorembeddingarguments.
 5. Noduplicateargumentclassesforcorearguments,suchasA0 X  X 5andAA.
 6. IfthereisanR-arg argument,thentherehastobean arg argument.Thatis, 7. IfthereisaC-arg argument,thentherehastobean arg argument;in 8. Giventhepredicate,someargumentclassesareillegal(e.g.,predicate cationstage(seeSection3.2).Inaddition,theyneedtobeexplicitlyenforcedonlywhen full parsing information is not available because the output of the pruning heuristics neverviolatestheseconstraints.
 reformulating the constraints as linear (in)equalities over the indicator variables that representthetruthvalueofstatementsoftheform[ argument i takes label j ],asdescribed indetailnext. 3.4.2 Using Integer Linear Programming. As discussed previously, a collection of po-tential arguments is not necessarily a valid semantic labeling because it may not satisfy all of the constraints. We enforce a legitimate solution using the following inference algorithm. In our context, inference is the process of finding the best (ac-cording to Equation (1)) valid semantic labels that satisfy all of the specified con-straints. We take a similar approach to the one previously used for entity/relation recognition(RothandYih2004),andmodelthisinferenceprocedureassolvinganILP problem.
 thecostfunctionandthe(in)equalityconstraintsarealllinearintermsofthevariables.
The only difference in an integer linear program is that the variables can only take integers as their values. In our inference problem, the variables are in fact binary. A generalbinaryintegerlinearprogrammingproblemcanbestatedasfollows.
 trices C 1  X  c 1  X  d , C 2  X  c 2  X  d ,where c 1 and c equalityconstraintsand d isthenumberofbinaryvariables,theILPsolution u vectorthatmaximizesthecostfunction, subjectto ables,andthenrepresentthefilterfunction F usinglinearinequalitiesandequalities. bysetting u tobeasetofindicatorvariablesthatcorrespondtothelabelsassignedtoar-guments.Specifically,let u ic = [ S i = c ]betheindicatorvariablethatrepresentswhether 268 ornottheargumenttype c isassignedto S i ,andlet p ic = score( S thenbewrittenasanILPcostfunctionas subjectto whichmeansthateachargumentcantakeonlyonetype.Notethatthisnewconstraint comes from the variable transformation, and is not one of the constraints used in the filterfunction F .
 argumentbasisand,forthesakeofefficiency,argumentsthatviolatetheseconstraints areeliminatedevenbeforebeinggiventotheargumentclassifier.Next,weshowhowto transformtheconstraintsinthefilterfunctionintotheformoflinear(in)equalitiesover u andusetheminthisILPsetting.ForamorecompleteexampleofthisILPformulation, pleaseseeAppendixA.

Constraint 4: No overlapping or embedding. Ifarguments S j inasentence,thenthisconstraintensuresthatatmostoneoftheargumentsisassigned to an argument type. In other words, at least k  X  1 arguments will be the special class null .If the special class null isrepresented bythesymbol  X  , thenforevery set ofsuch arguments,thefollowinglinearequalityrepresentsthisconstraint.
Constraint 5: No duplicate argument classes. Within the same clause, several types of arguments cannot appear more than once. For example, a predicate can only take one A0.Thisconstraintcanberepresentedusingthefollowinginequality.

Constraint 6: R-arg arguments. Suppose the referenced argument type is A0 and the referentialtypeisR-A0.Thelinearinequalitiesthatrepresentthisconstraintare:
Constraint 7: C-arg arguments. This constraint is similar to the reference argument con-straints. The difference is that the continued argument arg has to occur before C-arg .
Assume that the argument pair is A0 and C-A0, and arguments are sorted by their beginningpositions,i.e.,if i &lt; k ,thepositionofthebeginningof S
Constraint 8: Illegal argument types. Given a specific verb, some argument types should never occur. For example, most verbs do not have arguments A5. This constraint is representedbysummingallthecorrespondingindicatorvariablestobe0.
 straintsareverygeneral,andareabletorepresentanyBooleanconstraint(Gu  X  eret,Prins, andSevaux2002).Table1summarizesthetransformationsofcommonconstraints(most areBoolean),whicharerevisedfromGu  X  eret,Prins,andSevaux(2002),andcanbeused forconstructingcomplicatedrules.
 overlapping/embedding constraints (i.e., Constraint 4) when the constraint structure is sequential. However, they are not able to handle more expressive constraints such as those that take long-distance dependencies and counting dependencies into account (Roth and Yih 2005). The ILP approach, on the other hand, is flexible enough tohandlemoreexpressiveandgeneralconstraints.AlthoughsolvinganILPproblemis
NP-hard in the worst case, with the help of today X  X  numerical packages, this problem can usually be solved very quickly in practice. For instance, in our experiments it only took about 10 minutes to solve the inference problem for 4,305 sentences, using  X   X  270
Xpress-MP (2004) running on a Pentium-III 800 MHz machine. Note that ordinary search methods (e.g., beam search) are not necessarily faster than solving an ILP problemanddonotguaranteetheoptimalsolution. 4. The Importance of Syntactic Parsing
We experimentally study the significance of syntactic parsing by observing the effects ofusing full parsing and shallow parsing information ateach stage ofan SRLsystem.
We first describe, in Section 4.1, how we prepare the data. The comparison of full parsingandshallowparsingonthefirstthreestagesoftheprocessispresentedinthe reverse order (Sections 4.2, 4.3, 4.4). Note that in the following sections, in addition to the performance comparison at various stages, we present also the overall system performanceforthedifferentscenarios.Inallcases,theoverallsystemperformanceis derivedaftertheinferencestage. 4.1 Experimental Setting We use PropBank Sections 02 through 21 as training data, Section 23 as testing, and
Section 24 as a validation set when necessary. In order to apply the standard CoNLL sharedtaskevaluationscript,oursystemconformstoboththeinputandoutputformat definedinthesharedtask.
 tionoffullparsinginformationversusshallowparsinginformation(i.e.,usingonlythe part-of-speech tags, chunks, and clauses). In addition, we also compare performance when using the correct (gold-standard) data versus using automatic parse data. The performance is measured in terms of precision , recall ,andthe F all the numbers reported here do not take into account the V arguments as it is quite trivialtopredictVand,hence,thisgivesoveroptimisticoverallperformanceifincluded.
When doing the comparison, we also compute the 95% confidence interval of F ing the bootstrap resampling method (Noreen 1989), and the difference is considered arederivedusingCharniak X  X parser(2001)(version0.4).Inautomaticshallowparsing, theinformationisgeneratedbydifferentstate-of-the-artcomponents,includingaPOS tagger (Even-Zohar and Roth 2001), a chunker (Punyakanok and Roth 2001), and a clauser(Carreras,M ` arquez,andCastro2005). 4.2 Argument Classification
Toevaluatetheperformancegapbetweenfullparsingandshallowparsinginargument classification,weassumetheargumentboundariesareknown,andonlytrainclassifiers to classify the labels of these arguments. In this stage, the only difference between the usesoffullparsingandshallowparsinginformationistheconstruction of phrase type ,
AsdescribedinSection3.2.2,mostofthesefeaturescanbeapproximatedusingchunks and clauses, with the exception of the syntactic frame feature. It is unclear how this feature can be mimicked because it relies on the internal structure of a full parse tree. Therefore,itdoesnothaveacorrespondingfeatureintheshallowparsingcase.
 boundariesareknown.Inthiscase,becausetheargumentclassifierofourSRLsystem doesnotoverpredictormissanyarguments,wedonotneedtotrainwitha null class, andwecansimplymeasuretheperformanceusingaccuracyinsteadofF examples include 90,352 propositions with a total of 332,381 arguments. The test data contain 5,246 propositions and 19,511 arguments. As shown in the table, although the full-parsing features are more helpful than the shallow-parsing features, the perfor-mance gap is quite small (0.75% on gold-standard data and 0.61% with the automatic parsers).
 full parsing and shallow parsing information almost disappears when their output is processedbytheinferencestage.Table3showsthefinalresultsinrecall,precision,and
F ,whentheargumentboundariesareknown.Inallcases,thedifferencesinF the full parsing X  X ased and the shallow parsing X  X ased systems are not statistically significant.

Conclusion. When the argument boundaries are known, the performance of the full parsing X  X asedSRLsystemisaboutthesameastheshallowparsing X  X asedSRLsystem. 4.3 Argument Identification
Argument identification is an important stage that effectively reduces the number of argument candidates after the pruning stage. Given an argument candidate, an argu-mentidentifierisabinaryclassifierthatdecideswhetherornotthecandidateshouldbe consideredasanargument.Toevaluatetheinfluenceoffullparsinginformationinthis stage,thecandidatelistusedhereistheoutputsofthepruningheuristicappliedonthe gold-standardparsetrees.Theheuristicresultsinatotalnumberof323,155positiveand 686,887 negative examples in the training set, and 18,988 positive and 39,585 negative examplesinthetestset.
 parsing X  and shallow parsing X  X ased systems is in the construction of some features. featuresareapproximatedusingchunksandclauseswhenthebinaryclassifieristrained usingshallowparsinginformation.
 the direct predictions of the trained binary classifier. The recall and precision of the 272 fullparsing X  X asedsystemarearound2to3percentagepointshigherthantheshallow parsing X  X asedsystemonthegold-standarddata.Asaresult,theF 1 age points higher. The performance on automatic parse data is unsurprisingly lower butthedifferencebetweenthefullparsing X  X ndtheshallowparsing X  X asedsystemsis asobservedpreviously.Intermsoffilteringefficiency,around25%oftheexamplesare predicted as positive. In other words, both argument identifiers filter out around 75% oftheargumentcandidatesafterpruning.
 recall in argument classification, the threshold that determines when examples are predicted to be positive is usually lowered to allow more positive predictions. That is, a candidate is predicted as positive when its probability estimation is larger than the threshold. Table 5 shows the performance of the argument identifiers when the thresholdis0.1. 2 amorerealisticevaluationmethodistoseehoweachfinalsystemperforms.Usingan inrecall,precision,andF 1 .TheF 1 differenceis1.5pointswhenusingthegold-standard data.However,whenautomaticparsersareused,theshallowparsing X  X asedsystemis, infact,slightlybetter;althoughthedifferenceisnotstatisticallysignificant.Thismaybe duetothefactthatchunkandclausepredictionsareveryimportanthere,andshallow parsersaremoreaccurateinchunkorclausepredictionsthanafullparser(LiandRoth 2001).

Conclusion. Full parsing information helps in argument identification. However, when theautomaticparsersareused,usingthefullparsinginformationmaynothavebetter overallresultscomparedtousingshallowparsing. 4.4 Pruning
Asshownintheprevioustwosections,theoverallperformancegapsoffullparsingand shallowparsingaresmall.Whenautomaticparsersareused,thedifferenceislessthan1 pointinF 1 oraccuracy.Therefore,weconcludethatthemaincontributionoffullparsing is in the pruning stage. Because the shallow parsing system does not have enough in-formationforthepruningheuristics,wetraintwoword-basedclassifierstoreplacethe pruningstage.Oneclassifieristrainedtopredictwhetheragivenwordisthestart(S)of anargument;theotherclassifieristopredicttheend(E)ofanargument.Iftheproduct of probabilities of a pair of S and E predictions is larger than a predefined threshold, then this pair is considered as an argument candidate. The threshold used here was obtained by using the validation set. Both classifiers use very similar features to those usedbytheargumentidentifierasexplainedinSection3.2,treatingthetargetwordas a constituent. Particularly, the features are predicate, POS tag of the predicate, voice, context words, POS tags of the context words, chunk pattern, clause relative position, andshallow-path.TheheadwordanditsPOStagarereplacedbythetargetwordandits POStag.ThecomparisonofusingtheclassifiersandtheheuristicsisshowninTable7. surprisinglybetterthanthepruningheuristics.Usingeitherthegold-standarddataset ortheoutputofautomaticparsers,theclassifiersachievehigherF reason for this phenomenon is that the accuracy of the pruning strategy is limited by the number of agreements between the correct arguments and the constituents of the parse trees. Table 8 summarizes the statistics of the examples seen by both strategies.
Thepruningstrategyneedstodecidewhicharethepotentialargumentsamongallcon-stituents.Thisstrategyisupper-boundedbythenumberofcorrectargumentsthatagree withsomeconstituent.Ontheotherhand,theclassifiersdonothavethislimitation.The numberofexamplestheyobserveisthetotalnumberofwordstobeprocessed,andthe positiveexamplesarethoseargumentsthatareannotatedassuchinthedataset.
 274 once for each verb in the sentence. Therefore, the words and constituents in each sentencearecountedasmanytimesasthenumberofverbstobeprocessed.
 parsing information, we need to see the impact on the overall performance. There-fore, we built two semantic role systems based on full parsing and shallow parsing information. The full parsing X  X ased system follows the pruning, argument identifica-tion,argumentclassification,andinferencestages,asdescribedearlier.Fortheshallow parsing system, the pruning heuristic is replaced by the word-based pruning classi-fiers,andtheremainingstagesaredesignedtouseonlyshallowparsingasdescribedin previoussections.Table9showstheoverallperformanceofthetwoevaluationsystems. basedsystemsenlargestomorethan11pointsonthegold-standarddata.Atfirstglance, this result seems to contradict our conclusion in Section 4.3. After all, if the pruning stage of shallow parsing SRL system performs equally well or even better, the overall performancegapinF 1 shouldbesmall.
 that it filters out easy candidates, and leaves examples that are difficult to the later stages. Specifically, these argument candidates often overlap and differ only in one or twowords.Ontheotherhand,thepruningheuristicbasedonfullparsingneveroutputs overlappingcandidatesandconsequentlyprovidesinputthatiseasierforthenextstage to handle. Indeed, the following argument identification stage turns out to be good in discriminatingthesenon-overlappingcandidates.

Conclusion. The most crucial contribution of full parsing is in the pruning stage. The internaltreestructuresignificantlyhelpsindiscriminatingargumentcandidates,which makestheworkdonebythefollowingstageseasier.
 5. The Effect of Inference
Ourinferenceprocedureplaysanimportantroleinimprovingaccuracywhenthelocal predictions violate the constraints among argument labels. In this section, we first present the overall system performance when most constraints are not used. We then demonstratehowtheinferenceprocedurecanbeusedtocombinetheoutputofseveral systemstoyieldbetterperformance. 5.1 Inference with Limited Constraints
The inference stage in our system architecture provides a principled way to resolve conflictinglocalpredictions.Itisinterestingtoseewhetherthisprocedureimprovesthe performance differently for the full parsing X  vs. the shallow parsing X  X ased system, as wellasgold-standardvs.automaticparsinginput.
 previously, the first three constraints are handled before the argument classification stage. Constraint 4, which forbids overlapping or embedding arguments, is required inordertousetheofficialCoNLL-2005evaluationscriptandisthereforekept.
 constraintsisquiteconsistentoverthefoursettings.Precisionisimprovedby1to2per-centagepointsbutrecallisdecreasedalittle.Asaresult,thegaininF point.Itisnotsurprisingtoseethislowerrecallandhigherprecisionphenomenonafter the constraints described in Section 3.4.1 are examined. Most constraints punish false non-null output,butdonotregulatefalse null predictions.Forexample,anassignment thathastwoA1argumentsclearlyviolatesthenon-duplicationconstraint.However,if anassignmenthasnopredictedargumentsatall,itstillsatisfiesalltheconstraints. 5.2 Joint Inference
The empirical study in Section 4 indicates that the performance of an SRL system primarily depends on the very first stage X  X runing, which is directly derived from is decisive to the quality of the SRL system. To improve semantic role labeling, one possiblewayistocombinedifferentSRLsystemsthroughajointinferencestage,given thatthesystemsarederivedusingdifferentfullparsetrees.
 1999) 3 and Charniak X  X  parser (Charniak 2001), respectively. In fact, these two parsers have noticeably different outputs. Applying the pruning heuristics on the output of
Collins X  X parserproducesalistofcandidateswith81.05%recall.Althoughthisnumber is significantly lower than the 86.08% recall produced by Charniak X  X  parser, the union ofthetwocandidatelistsstillsignificantlyimprovesrecallto91.37%.Weconstructthe twosystemsbyimplementingthefirstthreestages,namely,pruning,argumentidentifi-cation,andargumentclassification.Whenatestsentenceisgiven,ajointinferencestage isusedtoresolvetheinconsistencyoftheoutputofargumentclassificationinthesetwo systems.
 troduced in Section 3.4. Formally speaking, the argument classifiers attempt to assign 276 labelstoasetofarguments, S 1: M ,indexedfrom1to M .Eachargument S label from a set of argument labels, P , and the indexed set of arguments can take a setoflabels, c 1: M  X  P M .Ifweassumethattheargumentclassifierreturnsanestimated conditionalprobabilitydistribution, Prob ( S i = c i ),then,givenasentence,theinference procedureseeksaglobalassignmentthatmaximizestheobjectivefunctiondenotedby
Equation(2),whichcanberewrittenasfollows, wherethelinguisticandstructuralconstraintsarerepresentedbythefilter words,thisobjectivefunctionreflectstheexpectednumberofcorrectargumentpredic-tions,subjecttotheconstraints.
 inferenceprocedurecantaketheoutputestimatedprobabilitiesforallthesecandidates asinput,althoughsomecandidatesmayrefertothesamephrasesinthesentence.For example,Figure3showsthetwocandidatesetsforafragmentofasentence, ..., traders systemBhasthreeargumentcandidates, b 1 = traders , b 2 = the selling panic ,and b both stocks and futures .
 by a system as a possible output. Each possible labeling of the argument is associated withavariablewhichisthenusedtosetuptheinferenceprocedure.However,thefinal predictionwillbelikelydominatedbythesystemthatproducesmorecandidates,which issystemBinthisexample.Thereasonisthatourobjectivefunctionisthesumofthe probabilitiesofallthecandidateassignments.
 hastwocandidates, a 1 and a 4 ,itcanbetreatedasifitalsohastwoadditional phantom candidates, a 2 and a 3 , where a 2 and b 2 refer to the same phrase, and so do a
Similarly,systemBhasaphantomcandidate b 4 thatcorrespondsto a doesnotreallygenerate a 2 and a 3 ,wecanassumethatthesetwophantomcandidatesare predictedbyitas X  X ull X (i.e.,notanargument).Weassignthesamepriordistributionto eachphantomcandidate.Inparticular,theprobabilityofthe X  X ull X  X lassissettobe0.55 basedonempiricaltests,andtheprobabilitiesoftheremainingclassesaresetbasedon theiroccurrencefrequenciesinthetrainingdata.
 estimationbyasystemcanbeviewedasevidenceinthefinalprobabilityestimationand, therefore,wecansimplyaveragetheirestimation.Formally,let S outputbysystem i ,and S = k i = 1 S i bethesetofallargumentswhere k isthenumber ofsystems;let N bethecardinalityof S .Ouraugmentedobjectivefunctionisthen: where S i  X  S ,and where Prob j istheprobabilityoutputbysystem j .
 priors (i.e., weights) on the estimated probabilities of the argument candidates. For example, if the performance of system A is much better than system B, then we may want to trust system A X  X  output more by multiplying the output probabilities by a largerweight.
 parser and Charniak X  X  parser, as well as the joint system, where the two individual systemsareequallyweighted.Thejointsystembasedonthisstraightforwardstrategy significantly improves the performance compared to the two original SRL systems in bothrecallandprecision,andthusachievesamuchhigherF 1 . 6. Empirical Evaluation X  X oNLL Shared Task 2005
In this section, we present the detailed evaluation of our SRL system, in the competi-tion on semantic role labeling X  X he CoNLL-2005 shared task (Carreras and M ` arquez 278 2005). The setting of this shared task is basically the same as it was in 2004, with some extensions. First, it allows much richer syntactic information. In particular, full parse trees generated using Collins X  X  parser (Collins 1999) and Charniak X  X  parser (Charniak 2001) were provided. Second, the full parsing standard partition was used X  the training set was enlarged and covered Sections 02 X 21, the development set was
Section24,andthetestsetwasSection23.Finally,inadditiontothe Wall Street Journal (WSJ) data, three sections of the Brown corpus were used to provide cross-corpora evaluation.
 version of the system described in Sections 3 and 5. The main difference was that the joint-inference stage was extended to combine six basic SRL systems instead of two. Specifically for this implementation, we first trained two SRL systems that use
Collins X  X  parser and Charniak X  X  parser, respectively, because of their noticeably dif-ferent outputs. In evaluation, we ran the system that was trained with Charniak X  X  parser five times, with the top-5 parse trees output by Charniak X  X  parser. Together we have six different outputs per predicate. For each parse tree output, we ran the first three stages, namely, pruning, argument identification, and argument classification.
Then, a joint-inference stage, where each individual system is weighted equally, was used to resolve the inconsistency of the output of argument classification in these systems.
 detailedresultsonWSJsection23areshowninTable13.Table14showstheresultsof individualsystemsandtheimprovementgainedbythejointinferenceprocedureonthe developmentset.
 among the 19 participating teams. After the competition, we improved the system slightlybytuningtheweightsoftheindividualsystemsinthejointinferenceprocedure, wheretheF 1 scoresonWSJtestsectionandtheBrowntestsetare79.59pointsand67.98 points,respectively.
 classifier,thetrainingofeachmodel,excludingfeatureextraction,takes50 X 70minutes using less than 1GB memory on a 2.6GHz AMD machine. On the same machine, the averagetesttimeforeachstage,excludingfeatureextraction,isaround2minutes. 7. Related Work
The pioneering work on building an automatic semantic role labeler was proposed by Gildea and Jurafsky (2002). In their setting, semantic role labeling was treated as a taggingproblemoneachconstituentinaparsetree,solvedbyatwo-stagearchitecture consisting of an argument identifier and an argument classifier. This is similar to our 280 main architecture with the exclusion of the pruning and inference stages. There are two additional key differences between their system and ours. First, their system used a back-off probabilistic model as its main engine. Second, it was trained on
FrameNet(Baker,Fillmore,andLowe1998) X  X nother largecorpus,besidesPropBank, thatcontainsselectedexamplesofsemanticallylabeledsentences.
 (2002). Their system achieved 57.7% precision and 50.0% recall with automatic parse trees, and 71.1% precision and 64.4% recall with gold-standard parse trees. It is worth noticing that at that time the PropBank project was not finished and the data set availablewasonlyafractioninsizeofwhatitistoday.Sincethesepioneeringworks,the taskhasgainedincreasingpopularityandcreatedanewlineofresearch.Thetwo-step constituent-by-constituent architecture becameacommonblueprint formanysystems thatfollowed.
 madeimprovementontheperformanceofautomaticSRLsystemsbyusingnewtech-niquesandnewfeatures.SomeoftheearlysystemsaredescribedinChenandRambow (2003), Gildea and Hockenmaier (2003), and Surdeanu et al. (2003). All are based on a two-stage architecture similar to the one proposed by Gildea and Palmer (2002) with the differences in the machine-learning techniques and the features used. The first breakthrough in terms of performance was due to Pradhan et al. (2003), who first viewed the task as a massive classification problem and applied multiple SVMs to it.
Their final result (after a few more improvements) reported in Pradhan et al. (2004) achieved84%and75%inprecisionandrecall,respectively.
 andPalmer(2004),whointroducedthepruningheuristicstothetwo-stagearchitecture, and remarkably reduced the number of candidate arguments a system needs to con-sider; this approach was adopted by many systems. Another significant advancement wasintherealizationthatglobalinformationcanbeexploitedandbenefitstheresults significantly. Inference based on an integer linear programming technique, which was originally introduced by Roth and Yih (2004) on a relation extraction problem, was first applied to the SRL problem by Punyakanok et al. (2004). It showed that domain knowledgecanbeeasilyencodedandcontributessignificantlythroughinferenceover theoutputofclassifiers.Theideaofexploitingglobalinformation,whichisdetailedin thispaper,waspursuedlaterbyotherresearchers,indifferentforms.
 explored. The alternative frameworks include representing semantic role labeling as a sequence-tagging problem (M ` arquez, Pere Comas, and Catal ` a 2005) and tagging the edgesinthecorrespondingdependencytrees(Hacioglu2004).However,themostpop-ulararchitecturebyfaristheconstituent-by-constituentbasedmulti-stagearchitecture, perhaps due to its conceptual simplicity and its success. In the CoNLL-2005 shared task competition (Carreras and M ` arquez 2005), the majority of the systems followed theconstituent-by-constituentbasedtwo-stagearchitecture,andtheuseofthepruning heuristicswasalsofairlycommon.
 tion,suchasourILPtechniquewhenusedinjointinference,inordertoachievesuperior performance.Thetopfoursystems,whichproducedsignificantlybetterresultsthanthe rest,allusedsomeschemestocombinetheoutputofseveralSRLsystems,rangingfrom usingafixedcombinationfunction(Haghighi,Toutanova,andManning2005;Koomen et al. 2005) to using a machine-learned combination strategy (M ` arquez, Pere Comas, andCatal ` a2005;Pradhan,Hacioglu,Wardetal.2005). tecture of SRL, but was also the first to investigate the interesting question regarding the significance of using full parsing for high quality SRL. They compared their full systemwithanothersystemthatonlyusedchunking,andfoundthatthechunk-based system performed much worse. The precision and recall dropped from 57.7% and 50.0% to 27.6% and 22.0%, respectively. That led to the conclusion that full parsing information is necessary to solving the SRL problem, especially at the stage of argu-ment identification X  X  finding that is quite similar to ours in this article. However, theirchunk-basedapproachwasveryweak X  X nlychunkswereconsideredaspossible candidates;hence,itisnotverysurprisingthattheboundariesoftheargumentscould notbereliablyfound.Incontrast,ourshallowparse X  X asedsystemdoesnothavethese restrictions on the argument boundaries and therefore performs much better at this stage,providingamorefaircomparison.
 et al. (2005) (their earlier version appeared in Pradhan et al. [2003]), which reported the performance on several systems using different information sources and system architectures.Theirshallowparse X  X asedsystemismodeledasasequencetaggingprob-lem while the full system is a constituent-by-constituent based two-stage system. Due to technical difficulties, though, they reported the results of the chunk-based systems onlyonasubsetofthefulldataset.Theirshallowparse X  X asedsystemachieved60.4% precision and 51.4% recall and their full system achieved 80.6% precision and 67.1% recall on the same data set (but 84% precision and 75% recall with the full data set).
Therefore, due to the use of different architectures and data set sizes, the questions of  X  X ow much one can gain from full parsing over shallow parsing when using the full PropBank data set X  and  X  X hat are the sources of the performance gain X  were left open.
 wereaskedtodevelopSRLsystemswiththerestrictionthatonlyshallowparsinginfor-mation(i.e.,chunksandclauses)wereallowed.Theperformanceofthebestsystemwas at72.43% precision and66.77% recall, which wasabout 10pointsinF bestsystembasedonfullparsingintheliterature.However,thetrainingexampleswere derived from only 5 sections and not all the 19 sections usually used in the standard setting.Hence,thequestionwasnotyetfullyanswered.
 byconsideringeachstageinacontrolledmanner,andusingthefulldataset,allowing onetodrawdirectconclusionsregardingtheimpactofthisinformationsource. 8. Conclusion
This paper studies the important task of semantic role labeling. We presented an ap-proachtoSRLandaprincipledandgeneralapproachtoincorporatingglobalinforma-tion in natural language decisions. Beyond presenting this approach which leads to a state-of-the-art SRL system, we focused on investigating the significance of using full parsetreeinformationasinputtoanSRLsystemadheringtothemostcommonsystem architecture,andthestagesintheprocesswherethisinformationhasthemostimpact.
We performed a detailed and fair experimental comparison between shallow and full parsinginformationandconcludedthat,indeed,fullsyntacticinformationcanimprove theperformanceofanSRLsystem.Inparticular,wehaveshownthatthisinformation is most crucial in the pruning stage of the system, and relatively less important in the followingstages. 282 thistask,characterizedbyrichstructuralandlinguisticconstraintsamongthepredicted labels of the arguments. Our integer linear programming X  X ased inference procedure is a powerful and flexible optimization strategy that finds the best solution subject to these constraints. As we have shown, it can be used to resolve conflicting argument predictions in an individual system but can also serve as an effective and simple approach to combining different SRL systems, resulting in a significant improvement inperformance.
 constraints to the inference procedure, an SRL system may be further improved.
Currently, the constraints are provided by human experts in advance. Learning both hard and statistical constraints from the data will be our top priority. Some work on combining statistical and declarative constraints has already started and is reported in Roth and Yih (2005). Another issue we want to address is domain adaptation.
It has been clearly shown in the CoNLL-2005 shared task that the performance of current SRL systems degrades significantly when tested on a corpus different from the one used in training. This may be due to the underlying components, especially the syntactic parsers which are very sensitive to changes in data genre. Developing a better model that more robustly combines these components could be a promising direction. In addition, although the shallow parsing X  X ased system was shown here to be inferior, shallow parsers were shown to be more robust than full parsers (Li and
Roth 2001). Therefore, combining these two systems may bring forward both of their advantages.
 Appendix A: An ILP Formulation for SRL
Inthissection,weshowacompleteexampleoftheILPformulationformulatedtosolve theinferenceproblemasdescribedinSection3.4.

Example. Assume the sentence is four words long with the following argument candidates,andthefollowingillegalargumenttypesforthepredicateofinterest. Sentence: w 1 w 2 w 3 w 4
Candidates: [ S 1 ][ S 2 ][ S 3 ][ S 5 ] Illegalargumenttypes: A3,A4,A5
Indicator Variables and Their Costs. The followings are the indicator variables and their associatedcostssetupfortheexample.

IndicatorVariables:
Costs:
Objective Function. Theobjectivefunctioncanbewrittenasthefollowing. Additional Constraints. Therestoftheconstraintscanbeformulatedasthefollowing. 284 Acknowledgments References 286
