 The proliferation of multilingual and multimodal content online has ensured that multiple views of the same data exist. For example, it is common to find the same article published in multiple lan-guages online in multilingual news articles, multilin-gual wikipedia articles, etc . Such multiple views can even belong to different modalities. For example, images and their textual descriptions are two views of the same entity. Similarly, audio, video and subti-tles of a movie are multiple views of the same entity.
Learning common representations for such mul-tiple views of data will help in several downstream applications. For example, learning a common rep-resentation for images and their textual descriptions could help in finding images which match a given textual description. Further, such common represen-tations can also facilitate transfer learning between views. For example, a document classifier trained on one language (view) can be used to classify doc-uments in another language by representing docu-ments of both languages in a common subspace.
Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Wang et al., 2015) except (Her-mann and Blunsom, 2014b) typically require paral-lel data between all views. However, in many real-world scenarios such parallel data may not be avail-able. For example, while there are many publicly available datasets containing images and their cor-responding English captions, it is very hard to find datasets containing images and their corresponding captions in Russian, Dutch, Hindi, Urdu, etc. In this work, we are interested in addressing such scenar-ios. More specifically, we consider scenarios where we have n different views but parallel data is only available between each of these views, and a pivot view. In particular, there is no parallel data available between the non-pivot views.

To this end, we propose Bridge Correlational Neural Networks (Bridge CorrNets) which learn aligned representations across multiple views using a pivot view. We build on the work of (Chandar et al., 2016) but unlike their model, which only ad-dresses scenarios where direct parallel data is avail-able between two views, our model can work for n (  X  2) views even when no parallel data is avail-able between all of them. Our model only requires parallel data between each of these n views and a pivot view. During training, our model maximizes the correlation between the representations of the pivot view and each of the n views. Intuitively, the pivot view ensures that similar entities across differ-ent views get mapped close to each other since the model would learn to map each of them close to the corresponding entity in the pivot view.

We evaluate our approach using two downstream applications. First, we employ our model to facil-itate transfer learning between multiple languages using English as the pivot language. For this, we do an extensive evaluation using 110 source-target language pairs and clearly show that we out-perform the current state-of-the art approach (Her-mann and Blunsom, 2014b). Second, we em-ploy our model to enable cross modal access be-tween images and French/German captions using English as the pivot view. For this, we created a test dataset consisting of images and their cap-tions in French and German in addition to the En-glish captions which were publicly available. To the best of our knowledge, this task of retrieving im-ages given French/German captions (and vice versa) without direct parallel training data between them has not been addressed in the past. Even on this task we report promising results. Code and data used in this paper can be downloaded from http: //sarathchandar.in/bridge-corrnet . Canonical Correlation Analysis (CCA) and its vari-ants (Hotelling, 1936; Vinod, 1976; Nielsen et al., 1998; Cruz-Cano and Lee, 2014; Akaho, 2001) are the most commonly used methods for learning a common representation for two views. However, most of these models generally work with two views only. Even though there are multi-view generaliza-tions of CCA (Tenenhaus and Tenenhaus, 2011; Luo et al., 2015), their computational complexity makes them unsuitable for larger data sizes.

Another class of algorithms for multiview learn-ing is based on Neural Networks. One of the ear-liest neural network based model for learning com-mon representations was proposed in (Hsieh, 2000). Recently, there has been a renewed interest in this field and several neural network based models have been proposed. For example, Multimodal Autoen-coder (Ngiam et al., 2011), Deep Canonically Cor-related Autoencoder (Wang et al., 2015), Deep CCA (Andrew et al., 2013) and Correlational Neural Net-works (CorrNet) (Chandar et al., 2016). CorrNet performs better than most of the above mentioned methods and we build on their work as discussed in the next section.

One of the tasks that we address in this work is multilingual representation learning where the aim is to learn aligned representations for words across languages. Some notable neural network based ap-proaches here include the works of (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013; Hermann and Blunsom, 2014b; Hermann and Blun-som, 2014a; Chandar et al., 2014; Soyer et al., 2015; Gouws et al., 2015). However, except for (Her-mann and Blunsom, 2014a; Hermann and Blunsom, 2014b), none of these other works handle the case when parallel data is not available between all lan-guages. Our model addresses this issue and outper-forms the model of Hermann and Blunsom (2014b).
The task of cross modal access between images and text addressed in this work comes under Mul-tiModal Representation Learning where each view belongs to a different modality. Ngiam et al. (2011) proposed an autoencoder based solution to learning common representation for audio and video. Srivas-tava and Salakhutdinov (2014) extended this idea to RBMs and learned common representations for im-age and text. Other solutions for image/text rep-resentation learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address multimodal, multilingual learning in situations where parallel data is available only between different views and a pivot view.
In the past, pivot/bridge languages have been used to facilitate MT (for example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009)), transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), transliteration and transliteration mining (Khapra et al., 2010a; Ku-maran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these works use neural networks but it is important to mention them here because they use the concept of a pivot language (view) which is central to our work. In this section, we describe Bridge CorrNet which is an extension of the CorrNet model proposed by (Chandar et al., 2016). They address the problem of learning common representations between two views when parallel data is available between them. We propose an extension to their model which si-multaneously learns a common representation for M views when parallel data is available only between one pivot view and the remaining M  X  1 views.
Let these views be denoted by V 1 ,V 2 ,...,V M and let d 1 ,d 2 ,...,d M be their respective dimensionali-ties. Let the training data be Z = { z i } N i =1 where each training instance contains only two views, i.e. , a pivot view. To be more clear, the training data con-tains N 1 instances for which ( v i 1 ,v i N 2 instances for which ( v i 2 ,v i M ) are available and so on till N M  X  1 instances for which ( v i available (such that N 1 + N 2 + ... + N M  X  1 = N ). We denote each of these disjoint pairwise training sets by Z all these sets.
 As an illustration consider the case when English, French and German texts are the three views of in-terest with English as the pivot view. As training data, we have N 1 instances containing English and their corresponding French texts and N 2 instances containing English and their corresponding German texts. We are then interested in learning a common representation for English, French and German even though we do not have any training instance contain-ing French and their corresponding German texts.
Bridge CorrNet uses an encoder-decoder architec-ture with a correlation based regularizer to achieve this. It contains one encoder-decoder pair for each of the M views. For each view V j , we have, where f is any non-linear function such as sigmoid or tanh, W j  X  R k  X  d j is the encoder matrix for view V , b  X  R k is the common bias shared by all the encoders. We also compute a hidden representation for the concatenated training instance z = ( v j ,v M ) using the following encoder function: In the remainder of this paper, whenever we drop the subscript for the encoder, then the encoder is deter-mined by its argument. For example h ( v j ) means h
Our model also has a decoder corresponding to each view as follows: where p can be any activation function, W 0 j  X  R d j  X  k is the decoder matrix for view V j , c j  X  R d j is the decoder bias for view V j . We also define g ( h ) as simply the concatenation of [ g In effect, h den representation h and then g code/reconstruct v j from this hidden representation h . Note that h can be computed using h ( v j ) or h ( v M ) . The decoder can then be trained to de-code/reconstruct both v j and v M given a hidden rep-resentation computed using any one of them. More formally, we train Bridge CorrNet by minimizing the following objective function: where l ( i ) = j if z i  X  Z corr is defined as follows:
Note that g ( h ( z i )) is the reconstruction of the input z i after passing through the encoder and decoder. L is a loss function which captures the error in this re-construction,  X  is the scaling parameter to scale the last term with respect to the remaining terms, h ( X ) is the mean vector for the hidden representations of the first view and h ( Y ) is the mean vector for the hidden representations of the second view.

We now explain the intuition behind each term in the objective function. The first term captures the er-ror in reconstructing the concatenated input z i from itself. The second term captures the error in recon-The third term captures the error in reconstructing second and third terms ensures that both the views can be predicted from any one view. Finally, the correlation term ensures that the network learns cor-related common representations for all views.
Our model can be viewed as a generalization of the two-view CorrNet model proposed in (Chandar et al., 2016). By learning joint representations for multiple views using disjoint training sets Z Z lel datasets between all views of interest. The pivot view acts as a bridge and ensures that similar enti-ties across different views get mapped close to each other since all of them would be close to the corre-sponding entity in the pivot view.

Note that unlike the objective function of Cor-rNet (Chandar et al., 2016), the objective func-tion of Equation 4, is a dynamic objective func-tion which changes with each training instance. In other words, l ( i )  X  { 1 , 2 ,..,M  X  1 } varies for each i  X  { 1 , 2 ,..,N } . For efficient implementation, we construct mini-batches where each mini-batch will come from only one of the sets Z randomly shuffle these mini-batches and use corre-sponding objective function for each mini-batch.
As a side note, we would like to mention that in addition to Z ditional parallel data is available between some of the non-pivot views then the objective function can be suitably modified to use this parallel data to fur-ther improve the learning. However, this is not the focus of this work and we leave this as a possible future work. In this section, we describe the two datasets that we used for our experiments. 4.1 Multlingual TED corpus Hermann and Blunsom (2014b) provide a multilin-gual corpus based on the TED corpus for IWSLT 2013 (Cettolo et al., 2012). It contains English tran-scriptions of several talks from the TED conference and their translations in multiple languages. We use the parallel data between English and other lan-guages for training Bridge Corrnet (English, thus, acts as the pivot langauge). Hermann and Blunsom (2014b) also propose a multlingual document classi-fication task using this corpus. The idea is to use the keywords associated with each talk (document) as class labels and then train a classifier to predict these classes. There are one or more such keywords asso-ciated with each talk but only the 15 most frequent keywords across all documents are considered as as provided by (Hermann and Blunsom, 2014b). The training corpus consists of a total of 12,078 par-allel documents distributed across 12 language pairs. 4.2 Multilingual Image Caption dataset English captions. On an average there are 5 cap-tions per image. The standard train/valid/test splits for this dataset are also available online. However, the reference captions for the images in the test split are not provided. Since we need such reference cap-tions for evaluations, we create a new train/valid/test of this dataset. Specifically, we take 80K images from the standard train split and 40K images from the standard valid split. We then randomly split the merged 120K images into train(118K), valida-tion (1K) and test set (1K).
We then create a multilingual version of the test data by collecting French and German translations for all the 5 captions for each image in the test set. We use crowdsourcing to do this. We used the CrowdFlower platform and solicited one French and one German translation for each of the 5000 cap-tions using native speakers. We got each transla-tion verified by 3 annotators. We restricted the ge-ographical location of annotators based on the tar-get language. We found that roughly 70% of the French translations and 60% of the German trans-lations were marked as correct by a majority of the verifiers. On further inspection with the help of in-house annotators, we found that the errors were mainly syntactic and the content words are trans-lated correctly in most of the cases. Since none of the approaches described in this work rely on syn-tax, we decided to use all the 5000 translations as test data. This multilingual image caption test data will hopefully assist further research in this area. From the TED corpus described earlier, we consider English transcriptions and their translations in 11 languages, viz. , Arabic, German, Spanish, French, Italian, Dutch, Polish, Portuguese (Brazilian), Ro-man, Russian and Turkish. Following the setup of Hermann and Blunsom (2014b), we consider the task of cross language learning between each of the 11 C 2 non-English language pairs. The task is to clas-sify documents in a language when no labeled train-ing data is available in this language but training data is available in another language. This involves the following steps: 1. Train classifier: Consider one language as the source language and the remaining 10 languages as target languages. Train a document classifier us-ing the labeled data of the source language, where each training document is represented using the hid-den representation computed using a trained Bridge Corrnet model. As in (Hermann and Blunsom, 2014b) we used an averaged perceptron trained for 10 epochs as the classifier for all our experiments. The train split provided by (Hermann and Blunsom, 2014b) is used for training. 2. Cross language classification: For every target language, compute a hidden representation for ev-ery document in its test set using Bridge CorrNet. Now use the classifier trained in the previous step to classify this document. The test split provided by (Hermann and Blunsom, 2014b) is used for testing. 5.1 Training and tuning Bridge Corrnet For the above process to work, we first need to train Bridge Corrnet so that it can then be used for com-puting a common hidden representation for docu-ments in different languages. For training Bridge CorrNet, we treat English as the pivot language (view) and construct parallel training sets Z Every instance in Z bic view of the same talk (document). Similarly, ev-ery instance in Z view of the same talk (document) and so on. For every language, we first construct a vocabulary con-taining all words appearing more than 5 times in the corpus (all talks) of that language. We then use this vocabulary to construct a bag-of-words representa-tion for each document. The size of the vocabulary ( | V | ) for different languages varied from 31213 to 60326 words. To be more clear, v 1 = v arabic  X 
We train our model for 10 epochs using the above training data Z = {Z representations of size D = 128 , as in (Hermann and Blunsom, 2014b). Further, we used stochastic gradient descent with mini-batches of size 20 . Each mini-batch contains data from only one of the Z We get a stochastic estimate for the correlation term in the objective function using this mini-batch. The hyperparameter  X  was tuned to each task using a training/validation split for the source language and using the performance on the validation set of an av-eraged perceptron trained on the training set (notice that this corresponds to a monolingual classification experiment, since the general assumption is that no labeled data is available in the target language). 5.2 Results We now present the results of our cross language classification task in Table 1. Each row corresponds to a source language and each column corresponds to a target language. We report the average F1-scores over all the 15 classes. We compare our re-sults with the best results reported in (Hermann and Blunsom, 2014b) (see Table 2). Out of the 110 experiments, our model outperforms the model of (Hermann and Blunsom, 2014b) in 107 experiments. This suggests that our model efficiently exploits the pivot language to facilitate cross language learning between other languages.

Finally, we present the results for a monolingual classification task in Table 3. The idea here is to see if learning common representations for multiple views can also help in improving the performance of a task involving only one view. Hermann and Blunsom (2014b) argue that a Naive Bayes (NB) classifier trained using a bag-of-words representa-tion of the documents is a very strong baseline. In fact, a classifier trained on document representations learned using their model does not beat a NB classi-fier for the task of monolingual classification. Rows 2 to 5 in Table 3 show the different settings tried by them (we refer the reader to (Hermann and Blunsom, 2014b) for a detailed description of these settings). On the other hand our model is able to beat NB for 5/11 languages. Further, for 4 other languages (Ger-man, French, Romanian, Russian) its performance is only marginally poor than that of NB. In this experiment, we are interested in retrieving images given their captions in French (or German) and vice versa. However, for training we do not have any parallel data containing images and their French (or German) captions. Instead, we have the following datasets: (i) a dataset Z ages and their English captions and (ii) a dataset Z containing English and their parallel French (or Ger-man) documents. For Z of MSCOCO dataset which contains 118K images and their English captions (see Section 4.2). For Z lel documents from the train split of the TED corpus (see Section 4.1). We use English as the pivot lan-guage and train Bridge Corrnet using Z = {Z to learn common representations for images, En-glish text and French (or German) text. For text, we use bag-of-words representation and for image, we use the 4096 (fc6) representation got from a pre-trained ConvNet (BVLC Reference CaffeNet (Jia et al., 2014)). We learn hidden representations of size D = 200 by training Bridge Corrnet for 20 epochs using stochastic gradient descent with mini-batches of size 20 . Each mini-batch contains data from only one of the Z
For the task of retrieving captions given an image, we consider the 1000 images in our test set (see sec-tion 4.2) as queries. The 5000 French (or German) captions corresponding to these images (5 per im-age) are considered as documents. The task is then to retrieve the relevant captions for each image. We represent all the captions and images in the com-mon space as computed using Bridge Corrnet. For a given query, we rank all the captions based on the Euclidean distance between the representation of the image and the caption. For the task of retrieving im-ages given a caption, we simply reverse the role of the captions and images. In other words, each of the 5000 captions is treated as a query and the 1000 im-ages are treated as documents.  X  was tuned to each task using a training/validation split. For the task of retrieving French/German captions given an image,  X  was tuned using the performance on the validation set for retrieving French (or German) sentences for a given English sentence. For the other task,  X  was tuned using the performance on the validation set for retrieving images, given English captions. We do not use any image-French/German parallel data for tuning the hyperparameters.

We use recall@k as the performance metric and compare the following methods in Table 4: 1. En-Image CorrNet : This is the CorrNet model trained using only Z tion. The task is to retrieve English captions for a given image (or vice versa). This gives us an idea about the performance we could expect if direct par-allel data is available between images and their cap-tions in some language. We used the publicly avail-able implementation of CorrNet provided by (Chan-dar et al., 2016). 2. Bridge CorrNet : This is the Bridge CorrNet model trained using Z this section. The task is to retrieve French (or Ger-man) captions for a given image (or vice versa). 3. Bridge MAE : The Multimodal Autoencoder (MAE) proposed by (Ngiam et al., 2011) was the only competing model which was easily extendable to the bridge case. We train their model using Z and Z function. We then use the representations learned to retrieve French (or German) captions for a given image (or vice versa). 4. 2-CorrNet : Here, we train two individual Corr-Nets using Z retrieving images given a French (or German) cap-tion we first find its nearest English caption using the Fr-En (or De-En) CorrNet. We then use this En-glish caption to retrieve images using the En-Image CorrNet. Similarly, for retrieving captions given an image we use the En-Image CorrNet followed by the En-Fr (or En-De) CorrNet. 5. CorrNet + MT : Here, we train an En-Image Cor-rNet using Z Z (or German) caption we translate the caption to En-glish using the MT system. We then use this English caption to retrieve images using the En-Image Cor-rNet. For retrieving captions given images, we first translate all the 5000 French (or Germam) captions to English. We then embed these English transla-tions (documents) and images (queries) in the com-mon space computed using Image-En CorrNet and do a retrieval as explained earlier. 6. Random : A random image is returned for the given caption (and vice versa).

From Table 4, we observe that CorrNet + MT is a very strong competitor and gives the best re-sults. The main reason for this is that over the years MT has matured enough for language pairs such as Fr-En and De-En and it can generate almost per-fect translations for short sentences (such as cap-tions). In fact, the results for this method are almost comparable to what we could have hoped for if we had direct parallel data between Fr-Images and De-Images (as approximated by the first row in the table which reports cross-modal retrieval results between En-Images using direct parallel data between them for training). However, we would like to argue that learning a joint embedding for multiple views in-stead of having multiple pairwise systems is a more elegant solution and definitely merits further atten-tion. Further, a  X  X ranslation system X  may not be available when we are dealing with modalities other than text (for example, there are no audio-to-video translation systems). In such cases, BridgeCorrNet could still be employed. In this context, the perfor-mance of BridgeCorrNet is definitely promising and shows that a model which jointly learns represen-tations for multiple views can perform better than methods which learn pair-wise common representa-tions (2-CorrNet). 6.1 Qualitative Analysis To get a qualitative feel for our model X  X  perfor-mance, we refer the reader to Table 5 and 6. The first row in Table 5 shows an image and its top-5 nearest German captions (based on Euclidean distance be-tween their common representations). As per our parallel image caption test set, only the second and fourth caption actually correspond to this image. However, we observe that the first and fifth cap-tion are also semantically very related to the image. Both these captions talk about horses, grass or wa-ter body (ocean), etc . Similarly the last row in Table 5 shows an image and its top-5 nearest French cap-tions. None of these captions actually correspond to the image as per our parallel image caption test set. However, clearly the first, third and fourth cap-tion are semantically very relevant to this image as all of them talk about baseball. Even the remaining two captions capture the concept of a sport and ra-quet . We can make a similar observation from Table 6 where most of the top-5 retrieved images do not correspond to the French/German caption but they are semantically very similar. It is indeed impressive that the model is able to capture such cross modal semantics between images and French/German even without any direct parallel data between them. In this paper, we propose Bridge Correlational Neu-ral Networks which can learn common representa-tions for multiple views even when parallel data is available only between these views and a pivot view. Our method performs better than the existing state of the art approaches on the cross language clas-sification task and gives very promising results on the cross modal access task. We also release a new multilingual image caption benchmark (MIC bench-mark) which will help in further research in this We thank the reviewers for their useful feedback. We also thank the workers from CrowdFlower for helping us in creating the MIC benchmark. Finally, we thank Amrita Saha (IBM Research India) for helping us in running some of the experiments.
