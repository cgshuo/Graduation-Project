 In the past decades, many different retrieval models have been proposed and successfully applied in document ranking, including vector space models [19], classical probabilistic models [16, 18], and statistical language models [14]. By assuming full independence between terms, most of them represent query and document as bag of words, and mainly exploit term based statistics such as within-document term frequency (tf), inverse document frequency (idf) and document length in ranking. However, the existing methods often lack a way to explicitly model the proximity among query terms within a document, which has been widely recognized to be useful fo r boosting retrieval performance [24]. Recently, many studies have been conducted to capture proximity in retrieval. One is to index phrases instead of terms which can capture proximity indirectly [5]. However, it is shown that the index methods may not perform consistently well across a variety of collections [12]. Another way is the dependency modeling [20, 22, 6, 11, 8], which considers the use of term dependency such as high-order n-grams, and integrates them into the existing retrieval models. The main problem of those dependency models lies in that the parameter space becomes very large, which will lead to difficulty in parameter estimation and sensitivity to data sparse and noise [26].

A simple but effective way is direct proximity modeling. Some early studies on direct proximity modeling try to add proximity factors into probabilistic ranking models in a heuristic manner, but their exp eriment results are not conclusive and show limited success [3, 7, 15, 2]. Recent w ork [24] and [25] obtains a promising performance improvement essentially by a pairwise term distance measure. While considering only word pairs may not be appropriate for long queries, and many redundant proximity information may be involved which may be harmful to the final performance. For example, g iven a user query  X  X econd hand car in Detroit X , word pairs such as  X  X econd car X  or  X  X and car X  can not convey the underlying concept of  X  X econd hand car X  correctly. Therefore, how to model proximity effectively and efficiently rem ains a challenging research problem.
In this paper, we propose a novel proximity statistic named Phrase Frequency ( pf ). It reflects the statistical frequency information of a query phrase provided by a document, beyond the traditional term statistics such as tf and idf. It is defined based on the definition of Span Cover. A Span Cover is a basic document segment that covers each query term at least once within a limited window size, and no overlap between each other. It is different from the existing two span-based definitions: Span (or FullCover) and MinCover [24, 4]. Four proximity-based density functions are considered to produce proximity score of a span cover instance, which is small when query terms within the span cover are far away, and large when query terms are close to each other. The phrase frequency is then computed by accumulating the p roximity scores within a document.
Based on the concept of phrase frequen cy, we propose a proximity-enhanced model referred as BM25PF. Through extensive experiments on different sizes and genres of TREC collections, we show that the BM25PF outperforms the BM25 baseline and is also comparable to the state-of-art approaches in general.
The rest of this paper is organized as follows. In Section 2, we introduce the previous related work. In Section 3, we introduce the concept of pf , correspond-ing calculating algorithm, and the practical application strategy. In Section 4, we propose a proximity-enhanced model named BM25PF based on pf . Then, in Section 5, we test the proposed model BM25PF on four TREC collections, and compare their performance with state-of-the-art approaches. Finally in Section 6, we conclude the paper with a discussion of our work and future work. Term proximity has been previously studied in several approaches. One approach tries to index phrases instead of terms. In such a way, dependence between words can be captured indirectly [5, 12]. However, the index method does not perform consistently well across a variety of collections.

Another way to capture proximity is the dependency modeling. Several work has been conducted to integrate term dependence into the language model. Song et al. propose a general model that combines bigram language model with un-igram language model under several smoothing techniques [20]. Srikanth et al. introduce a biterm language model that takes into account dependency between unordered adjacent word pairs [22]. [13, 6] put forward dependency language models that consider the relation betw een terms existed in a dependency tree. Metzler and Croft propose a general fr amework for model term dependencies based on Markov Random Field [11], in which term structures with different levels of proximity can be defined and captured. He et al. [8] extend the tradi-tional BM25 model to a combination of a series of n-gram BM25 models that will take the proximity betw een query terms into account.

Direct proximity modeling has been co nsidered widely recently. [3] and [7] appear to be the first to evaluate proximity on TREC data sets. Both of them try to measure proximity by using the shortest interval containing a match set. Recent work has attempted to heuristica lly incorporate proximity factor into probabilistic ranking functions [15, 2], but their experiments are not conclusive and show limited success. Song et al. repre sent term proximity by co-occurrences of terms in non-overlapping phrases [21]. Zhao and Yun X  X  work views the query term proximity as the Dirichlet hyper-parameter that weights the parameters of the unigram document language model [26]. Lv and Zhai apply a series of kernel functions to estimate a language model for every position in a document [10]. Overall, direct proximity modeling is an economic and effective way to go beyond the  X  X ag of words X  assumption in retrieval. In our paper, we will also follow the way by introducing a novel proximity statistic.

Density functions based on proximity have been used to propagate term in-fluence [10, 25]. Lv and Zhai [10] incorporate the term proximity by using four kernel functions: Gaussian, Triangle (Linear), Cosine, and Circle. [25] addition-ally introduces three more kernel functions: Quartic, Epanechnikov, Triweight. In our work, we utilize two of them: Gaussian and Linear, and introduce two more kernel functions containing Exponential and Negative Power. In this section, we introduce the concep t of Phrase Frequency and the associated calculating algorithm. Before giving the formal definition of Phrase Frequency, we will first define a new concept: Span Cover , which is different from the pre-vious notions of Span (or FullCover) and MinCover [24, 4]. 3.1 Span Cover The traditional span-based notions: FullCover and MinCover, which can be viewed as two kinds of  X  X oundary X  definitions, while the Span Cover can be viewed as a basic unit between them.
 Definition 1. (Span Cover) Span Cover is defined as a document segment that will have following properties: (1) Starting from a query term and ending with a query term; (2) Covering each query phrase term at least once; (3) Its length is no more than a limited maximal window size, which is usually adaptively set as integral times of the number of unique query terms; (4) Only the shortest of several span covers sharing a common starting points is counted as an instance (5) There is no overlapped terms between any two span cover instances under a certain scanning way; Given a short document d as an example to explain our definitions [24]. Given a query: { t 1 ,t 2 } , and setting the maximal windows size as 4 times the number of query terms, i ts corresponding Span Cover instances would be { t 1 ,t 2 } that, the segment { t 2 ,t 1 } will be also a proper span cover satisfying all the above properties. Usually, we will detect all the Span Cover instances provided by a relevant document in a sequential scanning way. Certainly there may exist some other scanning ways, we will study them in our future work. 3.2 Phrase Frequency The Phrase Frequency is a novel proximity s tatistic which reflects the statistical frequency information of a query phrase within a relevant document. Given a user query phrase that contains at least two terms and a candidate document, there exists several factors that m ay affect the relevance of a document: 1. Density: More tighter or shorter a Span Cover instance, the greater the like-lihood that the document is relevant. 2. Number: The more Span Cover instances provided by a document, the greater the likelihood that the document is relevant. 3. Order: The order in a certain Span Cover instance may also have some influ-ences on query proximity.
 Obviously, the traditional method of proximity modeling based on Mincover, which only considers the Density factor, may not be appropriate. Similarly, the method based on FullCover can not explicitly detect the number of Span Cover instances or the density of a specific inst ance. In fact, the experiment results in [24] also show that they are not effective in general. Meanwhile, many previous work has shown that the order of appearances of query term is not important [11, 8]. Therefore, instead of strict order constraint, rather, we expect the query terms to appear ordered or unordered within a Span Cover instance. And the definition of phrase frequency must systematica lly consider the density and number of Span Cover instances.

Given a query phrase Q with K unique query terms and the allowed maximal windows size as wK ( w usually set as positive integer). Supposing m Span Cover instances have been detected in a candidate document D as follows: { Span Cover 1 ,Span Cover 2 , ..., Span Cover m } Definition 2. The Phrase Frequency ( pf ) of a given query phrase Q in D will be calculated as the following. where Density (  X  ) is a type of proximity-based density function that transforms a Span Cover i instance to a proximity score belonging to (0 , 1], which is notated as pf i . | Span Cover i | is the length of the i th Span Cover: | Span Cover i | = pos end  X  pos start + 1. The more tighter Span Cover i , the higher pf i will be.
While if there does not exist a legal Span Cover instance in document D ,we will assign a uniform minimal value for pf in the following way: Obviously, Density (  X  ) plays an important role in pf . It must follow several prop-erties such as non-negative, monotonic [10, 25]. We will analyze and explore a number of proximity-based density functions in following subsection. 3.3 Proximity-Based Density Functions A major technical challenge in the definition of pf is how to define the density functions Density (  X  ). Following some previous work [24, 10, 25], we study four representative kernel functions: Gaussi an, Linear, Exponential, and Negative Power. They are all non-increasing functions, and represent different ways in which the proximity information ( pf i ) decreases when | Span Cover i | increases. 1. Gaussian Kernel 2. Linear Kernel 3. Exponential Kernel 4. Negative Power Kernel The first two kernels have been used in [10, 25], and the last two kernel functions are firstly introduced in our work. The performance of using all the kernel func-tions will be investigated in the experiments. Obviously, when | Span Cover i | = K , we will get the maximal value: pf i = 1, which means a consecutive occur-rence of a query phrase. a is a normalization parameter to make sure the output belonging to (0 , 1]. It is usually determined by query length and the maximal windows size together. 3.4 Algorithm and Time Complexity
In this section, we present an effectiv e and efficient calculating algorithm of pf by a sequential scanning way. Then we give an analysis of its time complexity. We show that its time complexity is close to linear in terms of N ,whichis the total number of occurrences of all t he query terms within a document. The corresponding calculating algorithm is summarized in Algorithm 1.

Given a query phrase Q = { t 1 ,t 2 , ..., t K } , supposing that a candidate docu-ment D matches the K unique query terms, and the total number of occurrences Algorithm 1. pf Calculating using a Sequential Scanning of these K query terms is N . All the query term occurrences compose a chain the position and the term informati on of every query term occurrence in a Hit set. While scanning, we maintain a list of length K : List [0 , ..., K  X  1], in which we store the temporary position of each seen query term. We set the allowed maximal windows size as w  X  K , w is a positive integer.

The first 5 steps are initialization. Specially we assign every position in List [] a negative value of -1. For steps 7 to 12, we update the corresponding position information of a certain qu ery term when scanning an occurrence sequentially. In steps 13 and 14, we record the start and end position information after every position updating. Then we will judge whether every position in List has been updated in step 15, which corresponds with the second property of Span Cover. In steps 16 and 17, we calculate the length of the segment, and then judge whether it satisfies the lengt h constraint property. In steps 18 and 19, we accumulate the pf score provided by a certain Span Cover instance and set the existpf flag as true. From step 20 to 22, we reinitialize the position information in List [] to make sure there is no overlapping between any two Span Cover instances. Finally we will assign a uniform minimal value for pf from step 26 to 28 if there does not exist a legal Span Cover instance. The algorithm for calculating pf has the time complexity: O ( N  X  K ). Since K is often very small, the time complexity is close to linear in terms of N : O ( N ), and N is the total number of query term occurrence within a document. 3.5 A Practical Application Strategy In practical retrieval environment, ther e exists many long user queries (5 or more terms). A Span Cover instance of a user query req uires to cover each query term at least once within a limited windows size. This condition may be too strong for these long user queries. On the other hand, when user constructs a long query, he will often aggregate additional terms to the concepts which will make them less meaningful [1]. Ideally, we should s elect the meaningful term sequences, and consider the proximity information of them only. Therefore, we naturally consider to combine proximity modeling w ith the query segmentation techniques together. Query segmentation is an important task toward understanding queries accurately[9, 23], which is essential for improving search results in modern in-formation retrieval. It aims to separate query words into segments so that each segment maps to a semantic comp onent or sub-query phrases.

For a user query, if its query length is less than 5 terms, we calculate the pf score as Algorithm 1 normally, or else we need to do a user query structure analysis first by query segmentation tec hniques, and then calculate as follows: where sub i is a sub-query phrase, which contains at least two query terms, wei i is a normalization weight factor assigned for sub i , which can reflect how meaningful the sub-query phrase is. pf ( sub i ,D ) denotes the pf score of a specific sub-query phrase, which will also be calculated by Algorithm 1.

There exists several methods for quer y segmentation, and the result sub-query phrases can be weighted correspondingly. In our paper, we use the method presented in [9], and define the wei i as follows: where connexity ( sub i ) denotes the connexity score of the ith sub-query phrase (or segment)[9], which is defined as a product of the global frequency of the seg-ment ( freq ( sub i )) and the mutual information ( I ) between longest but complete subsequence of sub i . In this section, we will test whether our proposed proximity statistic can re-ally boost the retrieval performance of the existing retrieval models, which are usually derived from bag-of-words assumption (e.g. Okapi BM25, KL-divergence language model). In this paper, we choose to combine the pf score with the rep-resentative state-of-the-art retrieval models: Okapi BM25 model [17, 18]. Previ-ous extensive experiments show that BM25 can provide a robust and effective retrieval performance. The BM25 retriev al model can be expressed as follows:
BM 25( Q, D )= where c ( q i ,d ) is within-document term frequency, and c ( q i ,q ) is within-query term frequency. Additionally, | d | is document length, and avdl is average docu-weight. N is the number of documents within a collection, n is the document frequency of term q i .

And then we can define a new combined proximity-enhanced retrieval model as follows: where  X  is a trade-off parameter reflecting the influence of the proximity statistic. When  X  equals to 1, the BM25PF model degenerates to the basic BM25 model. We conduct a series of experiments on four standard TREC collections: AP88-89, FR88-89, WT10G and ClueWebB. They repr esent different sizes and genres of text collections. AP88-89 and FR88-89 are chosen as small homogeneous collec-tions with little noise. The WT10G is a medium size crawl of Web documents. ClueWeb collection is the largest TREC test collection currently with a very large crawl of the Web. We use the category B of ClueWeb which contains about 50 million English Web pages. Queries are taken from the title field of the TREC topics. The basic statistics of the collections are illustrated in Table 1. We pre-form stemming with the Porter stemmer. And no stop words removing is done in both documents and queries to make sure the minimum preprocessing of them.
On each collection, we conduct a 2-fold cross-validation. The associated test topics are split into two equal subsets, referred as odd-number and even-number topics. In each fold, we use one subset of topics for training, and use the remaining subset for testing. The overall retrieval performance is averaged over the two test subsets of topics. Both top precision and average precision are used to evaluate the experiment result which include MAP/statMAP, P@5, P@10. All statistical tests are based on Wilcoxon matched-pairs signed-rank test at the 0.05 level. 5.1 Effectiveness of BM25PF In this section, we evaluate the effectiv eness of the proximity-enhanced model BM25PF. We use the optimal BM25 as our baseline. The BM25 has three main parameters. We set k 1 =1 . 2and k 3 =1 , 000 suggested in [24] and tune b to be optimal. And the parameter b is set to be 0.3.

The related experimental results are presented in Table 2. We apply four kinds of kernel functions for instantiating BM25PF model, and notate them corre-spondingly as: BM25PF G (BM25PF Gaussian), BM25PF L (BM25PF Linear), BM25PF E (BM25PF Exponential), BM25PF N (BM25PF Negative Power).
 Additionally, the parameter a of them is set as: w  X  K ,  X  1 / (( w +1)  X  K ), w  X  K and 1, respectively. The parameter k in Equation 6 is set as  X  1 (in fact any neg-ative value is allowed ). All results are evaluated in terms of MAP, P@5, P@10. Results with  X   X  indicate the improvement is significant at 0.05 level. The best result obtained on each collection is highlighted.

The results show that the proposed BM25PF model outperforms BM25 on all four data collections. Notablely, the proposed model has significant performance improvement especially on top retrieved documents in terms of P@5, P@10. And each kernel function shows a stable perf ormance improvement especially Gaus-sian kernel, which is also consistent with the conclusion of the previous work [10]. The Gaussian kernel exhibits the following property: the density measurement would drop slowly when the Span Cover length is small, but drop quickly as the length is in a middle range, and then drop slowly again when its length be-come large. Such an  X  X -shape X  trend is reasonable for that: the dependent query terms are not always adjacent in documents, but can be a little far from each other, thus we would not like to make the density measurement of a Span Cover instance so sensitive to the length when its length is small. However, when the Span Cover length is just around the boundary of strong semantic associations, the density measurement should be mor e sensitive to the length change. Then as the Span Cover length increases further, all query terms are presumably only loosely associated, and thus the density measurement again should not be so sensitive to the change of Span Cover length.

We further compare the retrieval performance of the BM25PF with two state-of-the-art approaches proposed in [24] and [25] respectively. For the first approach, to be fair, we use the best retrieval formula suggested in [24] (BM25+MinDist), and tune parameter  X  to be optimal. We label this approach as  X  X M25MD X . For the second approach, the CRTER model proposed in [25] which measures the association of every two query terms, we set their free pa-rameters by using cross-validation, which is also the same way using for our approach. We conduct the performance comparison on all four data collections and report the comparison results in Table 3. Overall, the proposed BM25PF model shows a steady performance improvement on all four data collections. And it is at least comparable to, if no better than, the two state-of-art approaches. 5.2 Parameter Sensitivity of BM25PF The BM25PF model introduces two parameters  X  and w which will affect the retrieval perform ance. The parameter  X  balances the influence of basic BM25 model and the proximity statistic. w is a proximity parameter for modeling different level proximity information. We choose the Gaussian for instantiating the BM25PF model based on the results shown in Table 2.

Figure 1 plot the sensitivity curves over  X  values ranging from 0 to 1 in terms of MAP on ClueWeb data. And a group of value settings of parameter w are to different w values, and  X  =0 . 5 seems to be best.

Figure 2 plot the sensitivity tendency over proximity parameter w on ClueWeb and AP88-89 respectively, in terms of MAP . These two data collections represent different data genres. Additionally, the  X  is fixed at 0.5. From the first subfigure (ClueWeb B), we find that the BM25PF achieves its best performance when w value is around 5, which means the model capturing proximity information at a sentence-level. The second subfigure shows that when w is set a small value such as 1 or 2, the overall performance tend to be best. A larger windows size will bring noise, and the retrieval performance will decrease and be not stable. This may be due to the homogeneous, clean nature of the documents in these small newswire collections. And a strictly tighter matching Span cover instances in the calculating of pf will capture high quality proximity information. While for the Web collections, which is heterogeneous and noisy collections, a larger windows size of sentence-level is appropriate because they can deal better with the noise inherent in Web documents. In this paper, we propose a novel proximity statistic, namely Phrase Frequency ( pf ), to model term proximity for boostin g retrieval performance. It reflects the statistical frequency information of a query phrase within a document, and is approximated by a density function based on the definition of Span Cover. pf systematically considers the density of a span cover instance and the num-bers of span cover within a document. In addition, we also present an efficient calculating algorithm of pf . Based on the concept of pf , we propose a proximity-enhanced retrieval model: BM25PF. Thro ugh extensive experiments on different sizes and genre of TREC coll ections, we show that the BM25PF outperforms the BM25 baseline and is also comparable to the state-of-art approaches in general.
The proposed proximity statistic pf has shown significant influence on re-trieval performance. It is a novel and important proximity statistic as the same as the traditional term statistics such as tf and idf . In the future work, we can study how to eventually obtain a unified retrieval model with the incorporation of pf in a more soundly theoretical way.
 Acknowledgements. This research work was funded by the National Natural Science Foundation of China under Grant No. 60933005, No. 61173008, No. 61003166, 973 Program of Chin a under Grants No. 2012CB316303, and National Key Technology R&amp;D program under Grant No. 2011BAH11B02.

