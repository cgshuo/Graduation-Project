 In text classification, a single or multiple category labels will be automatically assigned to a new text document based on c ategory models created after learning a set of labelled training text documents. Current text classification methods convert a text document into a relational tuple using the popular vector-space model to obtain a list of terms with corresponding frequencies.

Term frequency (TF) has been used to measure the importance levels of terms in a document. Firstly, TF is consider ed as a key component to evaluate term significances in a specific context [11]. The more a term is encountered in a cer-tain context, the more it contributes to the meaning of the context. Secondly, some approaches have combined TF and Inverse Document Frequency (IDF) as a term weighting measure. These approaches outcome the considerable re-sults as applied to text classification tasks [4,16,17]. However, with an abstract and complex corpus such as Ohsumed 1 , the TF-based methods fail to leverage the classification results [4,17]. According to Hassan and Banea [2], TF-based approaches can be effective for capturing the relevance of a term in a local con-text, but they fail to account for the glo bal effects that terms exist on the entire document.

To overcome this shortcoming, the relationships among terms have been in-vestigated to figure out the represent ation of a document. Recent studies have introduced the pre-defined relations among terms. These relations can be ex-tracted from a predefined knowledge source such as Wikipedia Encyclopedia [1,3,12,14], in which the relations (Wikipedia links) are regarded as the main components to represent for the document context. In a case of finding out the methods that can extract the representation of unpredictable text documents, these pre-tagging-based methods are limited to encounter to the variety kinds of document.

The term co-occurrence (TCO) are popularly used to model the relations between terms [2,15]. It is also a model to get over the shortcoming of TF-based methods as well as to deal with the universal kinds of text documents. The idea of taking term co-occurrence as a relation is not only to capture the dependency of terms in local contexts but also to take into account the global effects of terms in the entire document. In order to estimate importance levels of terms, a graph model is used to connect all these relations and a centrality algorithm is used to calculate term weighting values.

Although TCO-based methods give the considerable results in comparison to the TF-based methods when they are used to estimate important terms of a given document for TC tasks [2], some cer tain concerns need to be considered. Firstly, when working in a certain window size as the local context, these meth-ods accept every single pair of terms withi n the window size as a relation. So, the number of relations would be small or even really large depending on the choice of the window size. In those cases, the expectable relations can be eliminated, or the redundancy relations are still retained. Secondly, although the idea of these approaches is to extract important terms in a document context, the way of making pair of relations under a window size does not guaranty for the contri-bution of the relations in weighting important terms. With those explanations, we argue that not only TF-based model but also TCO-based model may not be the best technique to capture those important terms.

In this paper, we propose an alternative method to extract and weighting im-portant terms in a given document. The me thod firstly considers the advantages of the relations among terms to address the shortcoming of TF-based methods. Secondly, under the light of the success of TextRank [9,2], instead of using term co-occurrence as a dependency, we are more concentrating on relations based on grammatical dependency among terms. The choice of the relations not only prevents the issues of window sizes but also discloses more hidden relations as walking along the path connected terms.

As the framework of our method, we start firstly with extracting relations from the given text document. Secondly, the graph model is used to encode contexts of the document, which is constru cted by connecting all these relations. Then, a graph centrality algorithm is applied to calculate scores that represent significant values of terms in the document context. Finally, the top list of high weighted terms will be used as a representation of the given document.
The remaining of this paper is organised as follows. In Section 2, a framework of term weighting approach is presented. Section 3 explains the methodology of extracting grammatical relationships among words. Section 4 shows how to use the graph model to estimate the importance level of terms. How to apply the document representation to text cat egorisation tasks is presented in Sec-tion 5. Section 6 describes experimental s etups, results and discussions. Finally, a conclusion and future work will be discussed in Section 7. The term weighting framework includes the following phases (see Fig. 1): 1. Relation Extraction: Given an input document, the relation extraction phase 2. Term Weighting: A weighted graph is conducted by encoding all the ex-
The proposed framework is capable of extracting more relationships among terms within a document, and the relations include not only grammatical rela-tions, but also hidden relations which are explored by walking along the paths connecting the ideas of each sentence. A g lobal context of the document is cap-tured using a graph model and a centrality ranking algorithm. The graph model is able to inter-connect the major ideas o f the document together, and is a place for the centrality algorithm to estimate the important levels of vertices. The term weighting framework allows the important terms of a document to be  X  X oted X  by other terms in the same document.
 Relation extraction is an important research area in text mining, which aims to extract relationships between entities from text documents. In the framework, a relation is considered as a tuple t =( e i ,r ij ,e j ), where e i and e j are strings denoted as words (terms), and r ij is a string denoted as the relationship between them.

The relation can be extracted based on linguistic analysis, particularly grammatical relations among terms are the key components for extracting information.

For instance, from the following sentence  X  X ntibiotics kill bacteria and are helpful in treating infections caused by these organisms X  , a list of relations ex-tracted includes (Antibiotic, kill, bacteria) , (Antibiotic, treat, infection) , (An-tibiotic, treat infection cause, organism) ,and (infection, cause, organism) .
In order to extract relations, the sentences are identified from the input text document using a sentence detection techni que, a linguistic parser such as Stan-ford parser 2 is then used to analyse each sentence and outputs a graph of link-ages (Fig. 2), and finally a heuristic algorithm is designed to walk along paths from the linkage graph and extract the expectable relations.

The heuristic algorithm firstly scans the parsed sentence and identifies pairs of base terms 3 ( e i ,e j )with i&lt;j . From each pair of base terms, if there is a shortest path connecting from e i to e j , the algorithm will go along the path to identify a sequence of words between e i and e j . These order words is considered are connected direct ly, the connection r ij is regarded as the name of the linkage (label). Finally, if the raw tuples are passed through all given constraints, they will be retained for the next processing step. Constraints to test the raw tuples include:  X  e i and e j have to be base terms with POS filter  X  r ij has to be in a shortest path connecting e i to e j  X  r ij has to contain a verb or a preposition, or it is a grammar connection After extracting the set of raw tuples from each document, components in each tuple should be optimised as follows. All non-essential words such as adverbs, relative clause marker (who, whom, which, that, etc.), and stop-words will be eliminated from all components of tuple. The morphology technique is then used to convert all words to their simple forms, such as converting plural words into singular form and any kinds of verb forms into their  X  X oot X  words [8]. For instance, the noun phrase  X  X eveloping countries X  is converted to  X  X evelop coun-try X  , and the verb phrase  X  X ave been working X  is converted to  X  X ave be work X  . Once all wired tuples are eliminated, the remaining tuples are considered as a set of relations represented the document and is ready to build the graph for selecting term representatives. Graph model is an alternative way to model information to show relation-ships between vertices. It groups relate d information in a certain way that the centrality algorithms can take the best advantages. 4.1 Constructing Graph A graph model is built to connect all ext racted relations. Given a relation t =( e i ,r ij ,e j ), where e i and e j are considered as vertices in the graph and r ij is considered as an edge connecting between e i and e j . The weighting of the edge r ij is calculated based on the redundancy of the tuple t and the relatedness between e i and e i . 4.2 Weighting Graph The weighting of a edge w ( r ij ) is calculated based on two factors. Firstly, it depends on the frequency of the relation t in the document d . The higher redun-dancy of relation t is, the more important it is in the document d . Secondly, the weighting of the edge w ( r ij ) is also based on the redundancy of relation t in the corpus. The redundancy of a tuple determines how valuable of that information from its document [6].
 the graph, the weighting w ( r ij )iscalculatedas: where freq ( t, C ) is the frequency of tuple t in the corpus C , freq ( t, d )isthe frequency of tuple t in the document d ,and rf ( t, d ) is the relation frequency value of the relation t in the document d . 4.3 Ranking Graph Once a document is represented as a weighted graph, a graph ranking algorithm is used to estimate scores of its vertices . According to Sinha and Mihalcea [10], the centrality algorithm PageRank [5] shows its outstanding ability on weighting graph and hence it is adapted in our approach. The basic idea of PageRank algorithm is that a web page will have a high rank if there are many web pages or high ranking web pages pointing to it. Therefore, we treat each node in the term graph as a web page, every undirected edge e =( e i ,w ( r ij ) ,e j ) needs to be converted to two directed edges Then the directed graph is passed through the PageRank as its input and the output is a set of vertices with their r anking scores. Every vertex (term) e i in the graph (document) d has its ranking score pr ( e i ,d ), which is considered as degree of significance of the vertex (term) in the graph (document). Given a text document d from a corpus C , the list of n term representatives of d is defined as where w i is the text value of term i in the document d and pr ( w i ,d )isthe ranking value of term w i of the document d . The list of categories of the corpus C is C = { c 1 ,c 2 ,...,c m } .

In the following section, we proposes a measure that takes into account de-pendencies between terms and classes, which can justify term weighting values to adapt with text classification tasks. 5.1 Proposed Term Class Dependence (TCD) The idea of calculating class dependence value of terms is that terms represented for a document are normally dependent of its categories (classes). Therefore, we propose a measurement tcd , which takes into account the information of cate-gories and denotes the degree of dependence of a term to a particular category. If a word frequently occurs in many documents of one class and infrequently occurs in other classes, it should be consid ered as representative for the class if its ranking value from the document is also comparable. We suggest a measure of degree of dependence of the term w i to the category c i where c k is a categories of the corpus C .

With the purpose of classifying text, we propose a combination of term ranking on documents (pr) and category-based calculation (tcd) , which establishes a formula to weight terms w i from a document d j that belongs to a category c k as follows: and the pr.tcd value of each term will be added to the feature vector for classification task. 5.2 Proposed Hybrid Term Weighting Methods Based on TCD With the purpose of evaluating the effect iveness of term weighting methods based on term frequency, term co-occurrence rel ations, and grammatical relations, we suggest the following combinations to form hybrid term weighting methods for text classification:  X  tf.tcd : a combination of term frequency and term class dependency. The where n is number of terms in the document d j .  X  rw.tcd : a combination term weighti ng method based TextRank (rw) 4 and where rw ( w i ,d j ) is the random-walk weighting value of a term w i in a  X  pr.idf : a combination our term ranking method pr and invert document 6.1 Classifier and Data Sets Support Vector Machine [13] is a state-of-the-art classifier. In our experiments, we used the linear kernel since it was proved to be as powerful as other kernels when tested on data sets for text classification [16].

Our term weighting approach is mainly based on the grammatical relations among terms in English sentences. In order to test its effectiveness in comparison to other methods, we have chosen highly standard English grammar corpora which are Wikipedia XML, Ohsumed, and NSFAwards.
 Wikipedia Corpus: We used the collection Wikipedia XML Corpus compiled by Denoyer &amp; Gallinari [7]. We randomly selected a subset of the English Single-Label Categorization Collection which provides a single category to each doc-ument. After the pre-processing step, we obtained a total of 8502 documents assigned to 53 categories. These docum ents are divided randomly and equally to form a training data set and a test data set including 4251 documents and 53 categories for each set.
 Ohsumed: This corpus contains 50216 abstract documents 5 , we selected the first 10000 for training and the second 10000 for testing. The classification task is to assign the documents to one or multiple categories of the 23 MeSH  X  X iseases X  categories. After pre-processing step, there are 7643 documents in the test set and 6286 documents in the training set.
 NSFAwards: This data set consists of 129000 relatively short abstracts in En-glish, describing awards granted for basic research by the US National Science Foundation during the period 1990-2003. For each abstract, there is a consider-able amount of meta-data available, including the abbreviation code of the NSF division that processed and granted the award in question. We used this NSF division code as the class of each document. The title and the content of the ab-stract were used as the main content of the document for classification tasks. We used part of the corpus for our experiment by selecting 100 different documents for each single category, test set and training set. After the pre-processing step, we obtained 19018 documents for training set, 19072 documents for test set and 199 categories. 6.2 Performance and Discussion To evaluate the classification system we used the traditional accuracy measure defined as the number of correct predictions divided by the number of evaluated examples. Six weighting models that need to be tested are tf.idf , tf.tcd , rw.idf , rw.tcd , pr.idf and pr.tcd . We used the accuracy results from tf.idf ,and rw.idf as the baseline measures.
 GR-based Method versus Baseline Methods. The GR-based method pr.tcd provided outstanding results in comparison to tf.idf and rw.idf .Table1shows the classification results using the SVM classifier. The Ohsumed corpus is one of the challenging text classification datasets when tf.idf and rw.idf achieved under 40% accuracy. However, pr.tcd achieved a considerable result with about 16% accuracy higher than the two previous methods. Wikipedia and NSFAwards also show the similarity trends, yet the gap between pr.tcd and two baseline methods reduced to about 4% from NSFAwards corpus and to about 10% from Wikipedia corpus.

Moreover, from the classification results of six weighting methods in Table 2, it is seen that the GR-based method (pr.tcd) also shows the comparable results to other similarity methods.
 Grammatical Relation versus Term Frequency: The effectiveness of gram-matical relation and term frequency can be measured when ma king comparisons of the accuracy results of two pairs ( tf.idf, pr.idf )and( tf.tcd, pr.tcd ). The chart from Fig. 3 shows that pr always gives better performance than tf when it is combined with tcd in the term weighting methods. However, this trend is not stable when pr goes along with idf .Particularly, pr.idf presents outstanding performance on Wikipedia corpus, but tf.idf shows its strength on the other two corpora Ohsumed and NSFAwards.
 Grammatical Relation versus Term Co-occurrence Relation. The ideas behind term weighting schema based on these methods have some similarity. However, each of them has their strengths and weaknesses. The effectiveness of these approaches can be measured by taking the comparison between two pairs of weighting schemas ( rw.idf, pr.idf )and( rw.tcd, pr.tcd ). The chart from the figure 4 shows that the majority cases term weighting methods based on gram-matical relations ou tperforms to those based on ter m co-occurrence relations. Particularly, the biggest gaps between the classification accuracy between two methodsis2 . 7%, whereas just only 1 out of 6 cases the TCO-based methods show the comparable result to GR-based methods. In the case of NSFAwards corpus, TCO-based methods achieve higher 0 . 1% accuracy results in comparison to GR-based methods.
 Term Class Dependency versus Inverse Document Frequency. The in-formation from the chart of Fig. 5 shows another view of information. It presents the comparison between the contribution of inverse document frequency and term class dependency measures in term weighting schemas. Most of the cases, whenever term impo rtant evaluation (tf, rw, pr) combines with tcd shows the outstanding results in compare with idf . There is just only one cases from the Ohsumed corpus that idf shows better results than tcd .

In the summary, we have presented the experiment results and have made the comparisons related to the strengths and weaknesses of proposed meth-ods. Although some aspects need to be co nsidered, the proposed term weigh-ing approach for text classification using grammatical relations outperforms to other traditional term weighing approaches based on term frequency and term co-occurrence, and the term class dependency measure can be used as the alternative information evaluation instead of inverse document frequency. The paper has presented term weightin g method for text classification based on grammatical relations. With the same datasets, the approach has improved accuracy of text classification in comparison to the traditional term weighting method. The approach overcomes the less of frequency of information by self-creating the frequency based on the gr ammar structure of text content. This approach also raises motivations for our further investigation on the benefits of relations on text classification as well as text mining.

Our approach uses the concept of relations, we still do not take the closed considerations on its seman tic aspect, we have just used the relations as connec-tions between terms as a first attempt for getting more statistical information. For further investigation, we are more focusing on taking the semantic infor-mation from tuples and its connection from the graph to form representations of given documents. Moreover, the grammatical relations is extracted based on the grammar structure of text body, this procedure has consumed much compu-tational processing. Therefore, the need for quick and reliable extraction from input text should be considered as the further investigations.

