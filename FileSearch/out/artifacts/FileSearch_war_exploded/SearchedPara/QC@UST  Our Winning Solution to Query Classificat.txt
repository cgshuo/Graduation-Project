 In this paper, we describe our ensemble-search based approach, Q
C@UST ( http://webproject1.cs.ust.hk/q2c/ ), for the query classification task for the KDDC UP 2005. There are two aspects to the key difficulties of this problem: one is that the meaning of the queries and the semantics of the predefined categories are hard to determine. The other is that there are no training data for this classification problem. We apply a two-phase framework to tackle the above difficulties. Phase I corresponds to the training phase of machine learning research and phase II corresponds to testing phase. In phase I, two kinds of classifiers are developed as the base classifiers. One is synonym-based and the other is statistics based. Phase II consists of two stages. In the first stage, the queries are enriched such that for each query, its related Web pages together with their category information are collected through the use of search engines. In the second stage, the enriched queries are classified th rough the base classifiers trained in phase I. Based on the classification results obtained by the base classifiers, two ensemble classifiers based on two different strategies are proposed. The experimental results on the validation dataset help confirm our conjectures on the performance of the Q2C@UST system. In addition, the evaluation results given by the KDDCUP 2005 organizer confir m the effectiveness of our proposed approaches. The best F1 value of our two solutions is 9.6% higher than the best of all other participants X  solutions. The average F1 value of our two submitted solutions is 94.4% higher than the average F1 value from all other submitted solutions. Query classification, Synonym-based Classifier, ensemble learning, KDDCUP 2005. Historically, search engine technologies and automatic text classification techniques have progressed hand in hand. Ever since the early papers by the pi oneers [14][18][22], people have recognized the possibility of conducting Web search through classification, and vice versa [2][5][6][15]. The KDDCUP 2005 competition made this connection even stronger; the task being to automatically classify 800,000 of the queries on a Web search engine into a set of 67 predetermined categories provided by the organizers. This task deviates from the traditional machine learning and text classification formulation in the following aspects: for training two base classifiers that map a query to the 67 categories provided by the KDDCUP 2005 organizers (KDDCUP categories). In phase II, which corresponds to the  X  X esting phase X  in machine learning research, we process each query through a two-stage framework to handle the problem of query classifications. The overall framework of our approach is summarized in Figure 1 and the detailed illustration of phase II is shown in Figure 2. Phase II consists of two stages performed in a row. The first stage is to enrich the queries by searching the relevant pages which can with different ensemble strategies are employed to classify the queries. The experimental results show that the ensemble classifiers can improve the classification performance significantly. We now discuss phase I of our a pproach: training classifiers for query classification. As noted a bove, a main problem of our task is the lack of training data. This is a new problem in machine learning: without training text doc uments with class labels, many previous methods cannot be used. We tackle this problem by developing two kinds of base classifiers, which we later combine in an ensemble-search based classifier (Section 2.3). The first kind of base classifier is the synonym-based classifiers which uses the category information associated with Web pages collected for each query. The second kind of base classifier is statistical classifiers in which Support Vector Machine (SVM) is employed. We obtain two ensemble classifiers by combining these two kinds of classifiers according to different ensemble strategies, which improves the classification performance significantly more than the base classifiers. 2.1.1 Synonym-based Classifiers As discussed in Section 2.1, afte r the query enrichment stage, we obtain a ranked category list for each query through a search engine. Since the category hierarchies from different search engines are distinct, the categories in different ranked lists vary a lot. In addition, the hierarchies of the search engines differ greatly from that given in the KDDCUP 2005 competition task. For convenience, we call the former the search engine space for a search engine, and the latter the KDDCUP space . For a particular search engine, our objective is to build a mapping function between the two spaces. Using this mapping function, we can classify a query into the 67 KDDCUP categories. The mapping function can be built by keyword matching. We compare the keywords of categories in the KDDCUP space with those in the space of a certain search engine. Consider two categories, c 1 and c 2 , where c 1 is from the space of KDDCUP and c is from the space of a certain search engine. If they share some categories in the space of KDDCUP have two levels, such as,  X  X omputers\Hardware X  and  X  X ntertainment\Other X . The second level specifies a particular field within the first level. For most of the categories in the space of KDDCUP, we only consider the keywords at the second level because they are not confused with other categories. A typical example is  X  X omputers/Internet &amp; Intranet X . Even when we do not consider the first level for all the categories, there are no other categories which can be confused with  X  X nternet &amp; Intranet X . However, there are many categories that are more difficult to deal with. For them we require that the keywords in the first level and the second level should simultaneously match the categories in the space of a certain search engine. Otherwise, we cannot distinguish between two categories that share the same keywords only in the second level, such as  X  X omputers/Hardware X  and  X  X iving/Tools &amp; Hardware X . Although they both have the keywords  X  X ardware X  in the second level, they belong to two different domains  X  X omputers X  and  X  X  iving X  in the first level. We give two examples to illustrate the above two cases: Based on the direct mapping approach, the synonym-based classifiers tend to produce results with high precision but low recall. They produce high precision because the synonym-based classifiers are based on the manually annotated Web pages and can utilize the classification knowledge of editors. Therefore, once a mapping function is constructed, it is highly probable the classification result is correct. For example, we have shown that the categories such as  X .../Computers/..../ Hardware/... X  are mapped to  X  X omputers/Hardware X . Therefore, once a Web page  X  X omputers/Hardware/Storage/Hard_Drives X , we can assign  X  X omputers/Hardware X  to the query with high confidence. They produce low recall because it is hard to find all the mappings from the search-engine categories to the KDDCUP categories by keyword mapping. About 80,000 of the 354,000 categories in the category space of Google are not mapped onto the space of KDDCUP. Therefore, we cannot map all the categories in the category list for a query to the 67 KDDCUP categories and may miss some correct categories for the query. Another reason for the low-recall problem is that a search engine may return only a few or even no Web pages with categories. The synonym-based classifier may fail because of the search results shortage problem. Therefore, we need to construct other classifiers to help handle the low-recall problem which is described in the next section. 2.1.2 Statistical Classifiers As discussed in the previous subsection, the synonym-based classifiers have a low-recall drawback. In order to address this problem, we proposed to use the statistical classifier to help classify queries. The statistical classifier classifies a query based on its semantic content. Thus even if the synonym-based classifier fails, a query can also be classified by a statistical classifier. In this paper, we use the Support Vector Machine (SVM) classifier because of its high generaliza tion performance for document classification tasks and is easy to implement [12][13] . When SVM is used for classification tasks (e.g., document classification), the first step is to train a model using a set of training examples. Then the model can be used to classify other examples. Apparently, it is not st raightforward to apply SVM for a query classification task. There are at least two problems as we have discussed. Firstly, there is no training data available, and we only have the names of the 67 categories. Secondly, it is easy for a human being to capture the semantic meanings of a query and to identify its categories. However, for a statistical classifier, we must put forward a method to represent a query, i.e., to construct a query X  X  features. The above problems are both dealt with in our approach. For the first problem, we collect the training examples for the 67 categories by leveraging the mapping function constructed in the synonym-classifier learning step and the manually labeled Web page directory, such as ODP, etc. For each KDDCUP category, we can find a set of categories from the ODP directory by the mapping function. Thus Web pages contained in those collected ODP categories are used as the training documents for the target KDDCUP category. After the training examples are collected for each KDDCUP directory, the SVM training algorithm can be used to train the classifiers. For the second problem, we use the results returned by a search engine to help represent a query. As discussed in section 2.1, most queries contain only a few terms. Therefore, it is common that a query may be relevant to several relying on a static and out-of-date training set. Instead, we should try to catch its meanings by asking the Internet, retrieving related documents and extracting its semantic features. What is more, in order to obtain a better and unbiased understanding of each query, we should not rely on a single search engine but combine multiple results from different search engines. In our approach, we send each qu ery into Google [10], Looksmart [20] and a search engine developed by ourselves based on Lemur [17]. We chose these three search engines because they can provide both directory search and Web search . Directory search refers to search algorithms that return the related pages of a query together with the pages X  categories. Since these categories of Web pages are labeled by people, it is appropriate to use them as the ground truth to classify the queries. However, not all the pages indexed by the search algorithm contain category information; in this aspect, Web search, which is used to refer to search algorithm as we typically use, can return more related pages that directory search cannot. Based on the content of the pages returned by Web search, we can classify the queries using a text classification algorithm. To enrich a query through search engines, we use the following three steps: Based on the above procedures, there are only 10,000 queries without any pages returned from Google. These queries are inherently noisy and meaningless, such as  X  X ddddfdfdfdfdf X . We do not have to deal with these queries. Note that in the query processing tree shown in Figure 4 , the quality of the retrieved documents at different leave nodes may be different; the closer a node is to the root, the higher the quality of the result. Thus, the results of the 700,000 queries by applying  X  X irectory Search X  and  X  X eb Search X  are of high quality and reflect the true meanings of these queries. However, the results of the other 100,000 may not be reliable because we revise the queries to some extent. We follow the same steps when using Looksmart [20]. Among the 800,000 queries, about 200,000 queries have  X  X irectory We use two approaches to training the ensemble classifiers. The first is to make use of the validation data set which contains 111 pairs of query-to-category mappings provided by the KDDCUP organizers, to adjust the combination weights. The second approach is to ignore this validation data set in order to avoid overfitting. Our primary method in combining di fferent classifier functions is to assign an ensemble weight value to the result returned by each (See Figure 5 ) It is tricky to assign the ensemble weights. Different classifiers introduced in the above sections have different performance. Some may work better than others on certain categories. For example, a classifier may achieve high precision on one category while having high recall on the other category. This indicates that it is not proper to assign a single weight to a classifier. Instead, we should differentiate the weight of a classifier on different classes according to its performance. To determine the weights, we validate each component classifier on the 111 samples given by the KDDCUP organizers. The higher precision a classifier achieves on a category, the higher weight assigned to the classifier on that category. As a result, each classifier may obtain a weight value W ij on a KDDCUP category j. However, the 111 samples may be too small to be a suitable validation data set as it is easy to be overfitting on these samples. Therefore, a conservative way to combine the classifiers is to assign equal weights to them. Both the weight-assignment strategies in ensemble classifier construction are compared in the following experiments. To test our proposed approach, we conduct some experiments on the datasets provided by the KDDCUP 2005 organizer. The experimental results validate the effectiveness of our approach. In addition, we give some analysis of the consistency of the three labelers on their judgment of the performance of the classifiers in this part. The KCCCUP2005 organizer provide s a dataset which contains 111 sample queries together with the manual categorization information. These samples help exemplify the format of the queries and provide the semantics of a tiny number of queries. In fact, since the category informati on of these queries is truthful, they can serve as the ground truth for the test data and validation data for our proposed classifiers. Later, after the competition is finished, to test the solutions submitted by the participants, the organizer provides another data set which contains 800 queries with labels from three human labelers. We denote the three labelers (and sometimes the dataset labeled by them if no confusion is caused) as L1, L2 and L3, respectively. We refer to the former as Sample Dataset and the latter as Testing Dataset in the following sections. The evaluation criteria adopted by the KDDCUP organizer is the standard measures to evaluate the performance of classification in Information Retrieval (IR), incl uding precision, recall and F1-measure [24]. The definitions of pr ecision, recall and F1 in the query classification context are given below: if we include too many pages, noise may be introduced which can reduce the precision. From Figure 6 , we can see that the critical point for most classifiers is 40. Before that point, the performance of the classifiers increases when we use more and more pages, while after that point the performance begins to decrease. Therefore in the following experiments we keep only the top 40 pages for subsequent query classification. Figure 7 shows the performance of different classifiers by varying the number of classified categories. As we expected, when the number of classified categories increases, the precision of all the classifiers decreases while the recall increases significantly. In contrast, the value of F1 increases at the beginning and then subsequently decreases. For most of the classifiers, the maximum values of F1 are achieved when four categories are generated for each query. Although the F1 values are close to those obtained when 5 categories are assigned, the precision values are much lower. Based on the observation of the impact of two parameters, in order to achieve higher precision without sacrificing F1 much, we consider only the top 40 related pages of a query and return 4 categories for each query in most cases. Figure 7. Performance of different classifiers with the number From the above experimental results on the Sample Dataset, we can see that among the four base classifiers, S1 works best while S2 works worst. The reason why S2 does not work well is that for many queries, we cannot obtain enough related pages through the Looksmart search engine. For SVM, we expect that it can solve the low-recall problem caused by the three synonym-based classifiers, as discussed in section 2.1.1. Figure 6 and Figure 7 show that SVM obtains the highest recall in most cases. We also notice that the two ensemble classifiers can achieve better performance in terms of F1 than any other base classifier. For the peak F1 values of the three best classifiers (EN, EV and S1), we can see that, compared with S1, EN and EV improve the F1 by 4.53% and 8.67% respectively. In fact, when we design these two ensemble classifiers, EV is expected to achieve higher precision measure because each component classifier is highly weighted on the classes where it achieves high precision and EN is expected to achieve higher F1 performance since the recall is relatively high. According to our two submitted results, the F1 value of EN (0.444) achieves 4.2% improvement compared with the F1 value of EV (0.426). While the precision value of EV (0.424) improves by 2.3% compared with that of EN (0.414). The effectiveness of our proposed ensemble classifiers can also be validated when compared with the other participants X  solutions on the Testing Dataset. The F1 value of our solution generated by EN is 9.6% higher than the best one among other participants X  results. The averaged F1 value of our submitted results is 94.4% higher than the averaged F1 value of all the others. In this section, we analyze and discuss the consistency between the three labelers on their judgment of the performance of the classifiers. Figure 8 shows the F1 values of the six classifiers developed by us on the testing data labeled by the three labelers. From Figure 8, we can see that the three labelers have a high correlation with respect to the relative performance of the classifiers, especially pages that can well represent the semantics of a query and a category. This is a problem that needs further study. We also tried another method based on a dictionary which does not work well either. We submitted a query q into dictionary software, e.g., WordNet, to get the related words of a query. The result is denoted as R q . The result includes the synonyms or antonyms of the given query, which can be a verb, a noun, or an adjective, among others. Take the query  X  X ar X  as an example, the result contains: car, auto, automobile, machine, motorcar, railcar, railway car, railroad car and cable car, and so on. Similarly, we can get the result of every category c through the same way which is denoted as R c . A similarity between the target query and category is defined as | R q  X  R c | as shown before. We can build a matrix and get the most relevant categories based on similarity for every query. When we test this method on the validation dataset, the F 1 is only about 20%. The main reason for the poor performance is that this method cannot obtain proper results for many queries through WordNet. If more dictionaries like Wikipedia are leveraged, the performance of this kind of method might be improved. In this paper, we presented our approach to solving the query classification problem provided by KDDCUP 2005. Query classification is an important as well as a difficult problem in the field of information retrieval. Once the category information for a query is known, the search engine can be more effective and can return more representative Web pages to the users. However, since the queries usually contain too few words, it is hard to determine their meanings. Another challenge in KDDCUP 2005 is that, no training data are provided for the classification task. What is more, the semantics of the predefined category structure in KDDCUP 2005 are not given explicitly. To solve the task of KDDCUP 2005, we rely on some extra (external) information to overcome these difficulties. Therefore, we proposed an ensemble search based method which consists two phases with the second phase containing two stages. Phase I corresponds to the training phase of machine learning research and phase II corresponds to testing phase. In phase I, two kinds of base classifiers are developed. One is synonym-based and the are enriched in that the related Web pages together with their category information are collected for each query through search engines. In the second stage, the queries are classified through the classifiers trained in phase I. The synonym-based classifiers perform classification using the category information of the related Web pages for each query while the statistical classifier uses the contents of the related pages for each query. The experimental results on the two datasets provided by the KDDCUP 2005 organizer validate the effectiveness of these classifiers. By combining these two kinds of classifiers through two different strategies, we obtained two ensemble classifiers. One is expected to achieve higher precision with the help of the validation dataset and the other is expected to achieve a higher F1 without using the validation dataset. Both our experimental results on the datasets and the evaluation results given by the organizers verified the effectiveness of our approach. Our proposed approach is proved to be very effective for the query classification problem. We have designed a demonstration system called Q 2 C@UST , with a dedicated Web site at [14] K. S. Jones. Automatic Keyword Classification for [15] I.H. Kang, G. Kim, Query type classification for web [16] J. Kittler, M. Hatef, R. P.W. Duin, and J. Matas. On [17] Lemur, http://www.lemurproject.org/ [18] D.D. Lewis, W. A. Gale. A sequential algorithm for training [19] Y.Li, Z.J.Zheng, K.Dai. KDD-CUP 2005. Presentation on [20] Looksmart, http://www.looksmart.com [21] ODP: Open Directory Project, http://dmoz.com [22] L. Page, S. Brin, R. Motwani, and T. Winograd. The [23] J. R. Quinlan. Bagging, boosting and C4.5. In proceedings [24] C.J. van Rijsbergen. Information Retrieval. Second Edition, [25] Wordnet, http://wordnet.princeton.edu/
