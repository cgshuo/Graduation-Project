 With the rapid growth of information on the World Wide Web, automatic clas-sification of Web pages has become important for effective retrieval of Web doc-uments. The common approach to building a Web page classifier is to manually label some set of Web page to pre-defined categories or classes, and then use a learning algorithm to produce a classifier. The main bottleneck of building such a classifier is that a large number of labeled training Web page is needed it is normally easy and inexpensive to co llect positive and unlabeled examples, however, arduous and very time consuming to collect negative training examples and label them by user X  X  own hands.

In this paper, we focus on the problem to classifying Web page with pos-itive and unlabeled data and without la beled negative data. Recently, a few techniques for solving this problem were proposed in the literature. Liu et al. proposed a method (called S-EM) to solve the problem in the text domain [7]. In [8], Yu et al. proposed a technique (called PEBL) to classify Web pages given positive and unlabeled pages. This paper proposes a more effective and robust technique to solve the problem. Experimental results show that the new method outperforms existing methods significantly. Throughout the paper, we call the class of Web page that we are interested in positive and the complement set of samples negative.

The rest of the paper is organized as fo llows: Section 2 presents the concepts paper. Rough set theory is a forma lmathematicaltooltodealwithincompleteor equivalence relation to a tolerance relation, where transitivity property is not required, a generalized tolerance space is introduced below [3],[4],[5],[6].
Let I : U  X  P ( U ) to denote a tolerance relation, if and only if x  X  I ( x ) for x  X  U and y  X  I ( x )  X  x  X  I ( y ) for any x, y  X  U ,where P ( U )aresets of all subsets of U .Thustherelation xIy  X  y  X  I ( x ) is a tolerance relation rough membership function  X  I,V ,as x  X  U, X  X  U , The tolerance rough set for any X  X  U are then defined as
With its ability to deal with vagueness and fuzziness, tolerance rough set seems to be promising tool to model relations between terms and documents. The application of tolerance rough set in classifying Web page using positive and unlabeled examples was proposed a sawaytoenrichfeatureanddocument fication. 2.1 Tolerance Space of Terms in Unlabeled Set this purpose, the tolerance relation is determined as the co-occurrence of terms in all Web pages from U . 2.2 Tolerance Class of Term t occurs. The uncertainty function I with regards to co-occurrence threshold  X  defined as conceptually relate d terms into classes. The degree of correlation of terms in tolerance classes can be controlled by varying the threshold  X  .The membership function  X  for t i  X  T,X  X  T is then defined as: Finally, the lower and upper approximations of any subset X  X  T can be deter-mined with the obtained tolerance relation respectively as [5],[6]: 2.3 Expansion the Web Pages on Tolerance Class of Term In tolerance space of term, an expande d representation of Web document can be acquired by representing Web docum ent as set of tolerance classes of terms it contains. This can be achieved by simply representing Web document with its upper approximation, e.g., the Web page d i  X  U is represented by: The usage of tolerance space and upper approximation to enrich Web page and term relation allows the proposed technique to discover subtle similarities be-set. We use TRS-SVM to denote the proposed classification techniques that employ the method based on tolerance rough set to extract reliable negative set and SVM to build classifier. The TRS-SVM algorithm is composed by following steps:
Step1: Preprocessing of Web page in set P and U .
A preprocessing procedure is done as follows: Remove the HTML tag and extract plain text from each Web page. A ll the extracted words are stemmed. Use a stop list to omit the most common words. Finally, extract term set from positive set P and unlabeled set U respectively, let PT be a term set for P and UT a term set for U .
 Step2: Positive feature selection.

This step builds a positive feature set PF which contains terms that occur in the term set PT more frequently than in the term set UT . The decision threshold in set X .The detail algorithm is given as follows. 1. Generating the set { t 1 ,  X  X  X  ,t n } ,t i  X  UT  X  PT ; 2. PF =  X  ; 3. For i =0to n 4. f i p = freq ( t i ,P ) / | P | , f i u = freq ( t i ,U ) / | U | ; 5. If f i p /f i u &gt; X  then PF = PF  X  X  t i } ; 6. End If 7. End For
Step3: Generating tolerance class of term in unlabeled set and enriching Web page representation.

The goal of this step is to determine for each term in UT , the tolerance class which contains its related terms with regards to the tolerance relation. In our experiment we set  X  = 7 for good result. Then, the Web page in unlabeled set is represented with its upper approximation, e.g. the Web page d  X  U is represented by U R ( d ).
 Step4: Expansion the positive feature set on tolerance class of term.
The tolerance class of term in unlabeled set which contains the positive fea-ture term in PF will be merged with PF .The algorithm is given as follows. 1. For each t i  X  PF  X  UT ; 2. PF = PF  X  I  X  ( t i ); 3. End For Step5: Generating reliable negative set.

This step tries to filter out possible positive Web pages from U . A Web page in U which upper approximation does not have any positive feature in PF is regarded as a reliable negative example. The algorithm is given as follows. 1. RN = U ; 2. For each Web page d  X  U ; 3. If  X  x j freq ( x j ,U R ( d )) &gt; 0and x j  X  PF then RN = RN  X  d ; 4. End If 5. End For Step6: building classifier.

This step builds the final classifier by running SVM iteratively with the sets P and RN . The basic idea is to use each iteration of SVM to extract more possible negative data from U  X  RN and put them in RN .Let Q be the set of remaining unlabeled Web pages, Q = U  X  RN . The algorithm for this step is given as follows. 1. Every Web page in P is assigned the class label +1; 2. Every Web page in RN is assigned the label -1; 3. i =1 ,Pr 0 =0; 4. Loop 5. Use P and RN to train a SVM classifier C i ; 6. Classify Q using C i ; 7. Classify positive set P with C i ; 8. If ( | W | =0 || Pr i &lt;Pr i  X  1 ) 9. else Q = Q  X  W ; 10. End If 11. End Loop
The reason that we run SVM iteratively is that the reliable negative set RN extracted by the method based on toler ance rough set may not be sufficiently more negative Web pages from Q .There is, however, a danger in running SVM iteratively. Since SVM is very sensitive to noise, if some iteration of SVM goes wrong and extracts many positive Web pages from Q and put them in the negative set RN , then the last SVM classifier will be extremely poor. This is the problem with PEBL, which also runs SVM iteratively. In our algorithm, the iteration stops when there is no negat ive Web page that can be extracted from Q or the classification precision decreases which indicates that SVM has gone wrong. 4.1 Experiment Datasets To evaluate the proposed techniques, we use the WebKB data set 1 , which con-tains 8282 Web pages collected from computer science departments of various universities. The pages were manually classified into the following categories: student, faculty, staff, department, course, project, other. In our experiments, we used only the four most common categories: student, faculty, course, other (respectively abbrev iated here as St, Fa, Co, Ot). Each category is employed as us four datasets. Our task is to identify positive Web pages from the unlabeled set. The construction of each dataset for our experiments is done as follows: Firstly, we randomly select 10% of the Web pages from the positive class and the negative class, and put them into test set to evaluate the performance of The rest of the positive Web pages and negative Web pages form the unlabeled set U . Our training set consists of P and U . In our experiments, we range from 10%-70% respectively to create a wide range of settings. 4.2 Performance Measures To analyze the performance of classification, we adopt the popular F1 measure on the positive class. F1 measure is combination of recall (Re) and precision (Pr), F1=2.Re.Pr/(Re+Pr). Precision means the rate of documents classified classified documents among them to be classified correctly. The F1 measure takes into account effects of both quantities. 4.3 Experimental Results and Discussion We now present the experimental results. For comparison, we include the clas-sification results of the naive Bayesian method (NB)[1], S-EM, OSVM [9] and PEBL. Here, NB treats all the Web pages in the unlabeled set as negative. For SVM implementation, we used the LIBSVM 2 . We set Gaussian kernel as default kernel function of SVM because of its high accuracy. PEBL and OSVM also used LIBSVM. We set  X  = 7 for good result in generating tolerance class.
We summarize the average F value results of all a settings in Figure 1. We observe that TRS-SVM outperforms NB, S-EM, OSVM and PEBL. In fact, PEBL performs poorly when the number of positive Web pages is small. When the number of positive Web pages is large, it usually performs well. TRS-SVM performs well consistently. We also ran SVM with positive set and unlabeled set. F values are mostly close to 0) because SVM does not tolerate noise well. Due to space limitations, its results are not listed.

From Figure 1, we can draw the following conclusions: OSVM gives very poor results (in many cases, F value is around 0.3-0.5). PEBL X  X  results are extremely poor when the number of positive Web pages is small. We believe that this is could easily go wrong without sufficient positive data. S-EM X  X  results are worse than TRS-SVM. The reason is that the negative Web pages extracted from U by its spy technique are not reliable. We observe that a single NB slightly outper-forms S-EM. TRS-SVM performs well wit h different numbers of positive Web pages.
 Sensitiveness to co-occurre nce threshold parameter: Co-occurrence threshold parameter  X  is rather important to our TRS-SVM. From definition of tolerance class it is not difficult to get such deduction that inadequate co-on one hand, too small co-occurrence threshold can make too many negative examples be extracted as positive examples, on the other hand, too large co-occurrence threshold can make too little latent positive examples be identified from U , both cases can lead to worse performance.
From Figure 2 we can understand our experimental result corresponds to our deduction: when co-occurrence threshold equals value between 5 and 10, the performance is better, however, when it is out of the interval, the performance is worse (here, a =60% and for other a values, the results are similar). This paper studied the problem of Web page classification with only partial in-formation, i.e., with only one class of labeled Web pages and a set of unlabeled Web pages. An effective technique is proposed to solve the problem. Our algo-rithm first utilizes the method based on tolerance rough set to extract a set of reliable negative Web pages from the unlabeled set, and then builds a SVM clas-sifier iteratively. The experiment we have carried has showed that the method based on tolerance rough set it offers can extract reliable negative examples by discovering subtle information among unlabeled data, which have positive effects on classification quality. Experimental results show that the proposed technique is superior to S-EM and PEBL.
 Acknowledgments. This work was supported by the National Natural Sci-ence Foundation of Chin a (No.60475019) and t he Ph.D. programs Foundation of Ministry of Education of China (No.20060247039).

