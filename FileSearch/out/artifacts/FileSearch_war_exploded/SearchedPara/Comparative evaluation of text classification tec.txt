 Mohammad S. Khorsheed  X  Abdulmohsen O. Al-Thubaity Abstract A vast amount of valuable human knowledge is recorded in documents. The rapid growth in the number of machine-readable documents for public or private access necessitates the use of automatic text classification. While a lot of effort has been put into Western languages X  X ostly English X  X inimal experi-mentation has been done with Arabic. This paper presents, first, an up-to-date review of the work done in the field of Arabic text classification and, second, a large and diverse dataset that can be used for benchmarking Arabic text classification algorithms. The different techniques derived from the literature review are illus-trated by their application to the proposed dataset. The results of various feature selections, weighting methods, and classification algorithms show, on average, the superiority of support vector machine, followed by the decision tree algorithm (C4.5) and Na X   X  ve Bayes. The best classification accuracy was 97 % for the Islamic Topics dataset, and the least accurate was 61 % for the Arabic Poems dataset. Keywords Machine learning  X  Arabic text categorization  X  Arabic text classification 1 Introduction Documents are the primary repositories of knowledge; therefore, documentation is the most effective way to illustrate ideas, thoughts, and expertise. The availability of documents in a machine-readable format and handling them in an intelligent way, such as through text classification, will maximize the benefit of the knowledge they contain. Arabic machine-readable texts are available both on the Internet and within government organizations and private enterprises, and they are rapidly increasing day by day. However, whereas automatic text classification is well known in natural language processing communities, little attention has been given to Arabic texts.
Text classification X  X he assignment of free text documents to one or more predefined categories based on their content X  X s used in various applications, such as e-mail filtering, spam detection, web-page content filtering, automatic message routing, automated indexing of articles, and searching for relevant information on the Web.

There are three main phases involved in building a classification system: (a) compilation of the training dataset, (b) selection of the set of features to represent the defined classes, and (c) training the chosen classification algorithm, followed by testing it using the corpus compiled in the first stage. Automated document classification involves taking a set of pre-classified documents as the training set. The training data is then analyzed in order to derive a classification scheme, which, in turn, often needs to be refined with a testing process. The derived classification scheme is then used for classification of other unknown documents. Further details will be presented in Sect. 2 . The main contribution of this paper is its presentation of a large and diverse benchmarking dataset for Arabic text classification as well as an investigation of different feature selection methods, weighting methods, and text classification techniques using the same datasets.

The rest of the paper is organized as follows. Section 2 presents a brief description of text classification steps with references to some related Arabic text classification literature. In Sect. 3 , the design and the statistics of the benchmarking dataset for Arabic text classification is presented in detail. The illustration of the main functions of a tool incorporated in Arabic text classification is given in Sect. 4 . Sections 5 , 6 , 7 and 8 illustrates detailed experimentation on Arabic text classification using a set of feature selections, weighting methods, and different classifiers. Finally, discussion and some concluding remarks are presented in Sect. 9 . 2 Related works This section summarizes what has been achieved on Arabic text classification from each part is related either to data, features, or classification. Figure 1 depicts nine steps for the problem of text classification. Those steps include data collection, text processing, data division, feature extraction, feature selection, feature representation, machine learning, applying a classification model, and performance evaluation. 2.1 Data collection Collecting data is the first step in text classification studies. The required data are samples of texts that belong to the area of interest. Each sample text must be labeled with one or more tags indicating its  X  X elongingness X  to a certain class. Some horizontal line that can be added in the mid dle of Arabic to certain letters as a form of justification. Most Arabic text classificat ion takes into account the importance of preprocessing eitherfully orpartially, but someresearch does not X  X ee, forexample, Sawaf et al. ( 2001 ) and Thabtah et al. ( 2008 ).

Because of the morphological nature of Arabic, some researchers consider root extraction and word stemming as a part of preprocessing (Kanaan et al. 2005 ; Syiam et al. 2006 ). In our opinion, using the full form of the word, its stem or root, is part of the feature extraction step, which will be discussed in Sect. 2.4 . 2.3 Data division After removing unwanted words and characters, the data are divided into two parts, training data and testing data. Based on training data, the classification algorithm will be trained to produce a classification model. The testing data will be used to assess the performance of the resulting classification model. Since there is no ideal ratio of training data to testing data, different ratios have been used for Arabic text classification research ranging from 25 % for training and 75 % for testing (Kanaan et al. 2005 ) up to 80 % for training and 20 % for testing (Sawaf et al. 2001 ).
The k-fold cross validation is sometimes used where different partitions for training and testing are used to produce k-classification models. The classification performance is the average performance of implemented classification models (see El-Halees 2008 ; Kanaan et al. 2009 ; Al-Saleem 2010 ). 2.4 Feature extraction Texts are characterized by two types of features, external and internal. External features are not related to the content of the text, such as author name, publication date, author gender, and so on. Internal features reflect the text content and are mostly linguistics features, such as lexical items and grammatical categories. Most text classification research concentrates on the simplest of lexical features, the word. Using single words as a representative feature in text classification has proven effective for a number of applications (Diederich et al. 2003 ; Sebastiani 2002 ).
For Arabic text classification, words were treated as a feature on three levels: (1) using words in their orthographic form (Mesleh 2007 ; Thabtah et al. 2009 ); (2) word stems, in which the suffix and prefix were removed from the orthographic form of the word (Syiam et al. 2006 ; Kanaan et al. 2009 ); and (3) the word root, which is the primary lexical unit of a word (Elkourdi et al. 2004 ; Duwairi 2006 ). Whereas the above-mentioned methods focus on words as a way of reflecting meaning, another way is to focus on character n-grams, which usually convey no meaning. In this method, a certain number of consecutive characters are extracted and considered as features (Sawaf et al. 2001 ; Khreisat 2006 ). The output of this step is a list of features and their corresponding frequency in the training dataset. 2.5 Feature selection The output of the feature extraction step is a long list of features, ranging from several thousand to hundreds of thousands. Not all of these features are beneficial for classification for several reasons: (1) The performance of some classification algorithms is negatively affected by the large number of features due to what is called curse of dimensionality. (2) An over-fitting problem may occur when the classification algorithm is trained in all features. (3) A large chunk of these features occur only once or twice in the training data. (4) Finally, some other features are common in all or most of the classes.

To overcome these problems, several methods were proposed to select the most representative features for each class in the training dataset. Feature selection methods statistically rank the features according to their distinctiveness for each class. Features with higher values are selected as the representative features. Different feature selection methods have been used in Arabic text classification. The most frequently used methods have been Chi Squared (CHI) (Syiam et al. 2006 ; Mesleh 2007 ; Thabtah et al. 2009 ; Zahran and Kanaan 2009 ); term frequency (TF), document frequency (DF) and their variations (Elkssssourdi et al. 2004 ; Thabtah et al. 2008 ; Zahran and Kanaan 2009 ); and information gain (IG) (Syiam et al. 2006 ; El-Halees 2008 ). Apart from statistical ranking, word stems or roots were also used as feature selections where words with the same stem or root are considered as one feature, and features with higher frequency are used (Kanaan et al. 2005 ; Duwairi 2006 ; Bawaneh et al. 2008 ; Duwairi et al. 2009 ; Kanaan et al. 2009 ). 2.6 Data representation In this step, the selected features from the previous step are formatted in a stable way to be represented to the classification algorithm. Usually, the data are represented as a matrix with n rows and m columns wherein the rows correspond to the texts in the training data, and the columns correspond to the selected feature. The value of each cell in this matrix represents the weight of the feature in the text. Several methods have been used to assign the proper weight to the feature. The most-used weighting methods have been term frequency inverse document frequency (TFiDF) (Syiam et al. 2006 ; Mesleh 2007 ; Bawaneh et al. 2008 ; Kanaan et al. 2009 ; Zahran and Kanaan 2009 ) and term frequency (TF) (Syiam et al. 2006 ; Kanaan et al. 2009 ). 2.7 Classification algorithm training and testing In this step, the training matrix that contains the selected features and their corre-sponding weights in each text of the training data are used to train the classification algorithm. Classical machine learning algorithms have been the most used in Arabic text classification, such as Na X   X  ve Bayes (NB) (Elkourdi et al. 2004 ; Al-Saleem 2010 ); k-nearest neighbor (KNN) (Syiam et al. 2006 ; Bawaneh et al. 2008 ), and support vector machine (SVM) (Mesleh 2007 ; El-Halees 2008 ).

The training process yields a classification model that will be tested by means of the testing data. The same features that were extracted from the training data and the same weighting methods will be used to test the classification model. 2.8 Classification model evaluation The ability of the classification model to classify texts into the correct classes results from all the previously described steps. A number of methods have been used to assess the performance of the classification model output, such as accuracy (Elkourdi et al. 2004 ; Bawaneh et al. 2008 ), precision and recall (Khreisat 2006 ; Kanaan et al. 2009 ), and f-measure (Syiam et al. 2006 ; Al-Saleem 2010 ).
From the data summarized in Table 1 , it is difficult to suggest which combination of feature selection method, term weighting, and classification algorithm is the optimal solution for Arabic text classification because most of the datasets used are small and are mainly from the news genre. In the following sections, we will present our efforts on Arabic text classification as a follow-up to what we have discussed above. 3 Arabic text classification benchmarking dataset One of the main objectives of this research is to build a benchmarking dataset (corpus) for Arabic text classification that takes into consideration corpus design criteria (Atkins et al. 1992 ; Sinclair 1995 ). The dataset design comprises seven sub-datasets covering different genres and subject domains. Each text in the corpus must be assigned to one of the defined classes. Table 2 illustrates the corpus genres, subject domains/classes, and number of texts for each class.

The datasets were assembled, comprising 17,658 texts, more than 11 million words, and seven different written genres X  X amely, the Saudi Press Agency (SPA), Saudi Newspapers (SNP), Websites, Writers, Forums, Islamic Topics and Arabic Poems. The Internet was the main venue used to collect the texts. A statistical overview of compiled corpora (genres) is shown in Table 3 . Processing the component of this dataset and preparing it for classification algorithms is discussed in the next section. 4 Experiment automation The benchmarking dataset illustrated in Sect. 3 needs to be processed according to text classification steps as mentioned in Sect. 2 and prepared in a suitable format for classification algorithms. A software tool called Arabic Text Classification tool (ATC tool) was developed in Java to handle and process the dataset. The user interface for the ATC tool is shown in Fig. 2 .

The ATC tool incorporates the following main functions: (a) Text preprocessing: This allows the user to remove numbers, punctuations, (b) Data division: This divides the dataset into two sets -one for training and the (c) Feature extraction: This extracts and generates the frequency list of the dataset (d) Feature selection: This calculates the importance of each feature locally (for (e) Data representation This generates the training and testing matrix elements
The resulting matrices are then used in other programs to build the classification model and to evaluate it. Those programs are RapidMiner 4.0 (Mierswa et al. 2006 ) and Clementine. RapidMiner is an open-source software which provides an implementation for all classification algorithms used in our experiments except the C5.0 algorithm. Clementine is a data-mining software from SPSS Inc. which provides an implementation for the C5.0 decision tree algorithm. The classification accuracy in the following experiments is computed by simply dividing the total number of correctly classified samples by the total number of samples in the testing dataset. 5 Assessing classification accuracy versus feature selection This section aims to evaluate our basic classification methodology by employing frequently used classification algorithms: decision tree (C4.5), multilayer perceptron neural networks (MLP), support vector machines (SVM), Na X   X  ve Bayes (NB), and k-nearest neighbor (KNN). We ran the experiments on the SPA corpus which was divided into two distinct sets: training and testing. We selected two simple methods for term selection: TF (term frequency) and DF (document frequency). The top 10, 15, 20, 25, and 30 terms of each class in the corpus were selected as the representative terms, based on their related TF and DF. After we ranked the terms, the data were represented in two forms: Boolean and frequency.

To verify the effect of training data size on classification accuracy, we implemented three scenarios for each set of parameters: 30 % of corpus for training and the remaining 70 % for testing, 50 % of corpus for training and the remaining 50 % for testing and finally, 70 % of corpus for training and the remaining 30 % for testing. The classification accuracy of each scenario is shown in Table 6 .
The NB algorithm shows the highest accuracy among all the five algorithms, 72.69 %. This rate was achieved using the top 30 terms in each class, with 70 % of the corpus used for training and the remaining 30 % for testing; term selection is based on document frequency and Boolean data representation. In all cases the best classification accuracy were achieved when the training data size is larger than testing data size.
 Table 7 ranks the five classification algorithms according to their average accuracies. The next two columns of the table illustrate the highest accuracy rate for that classification algorithm and the equivalent experiment parameters: data representation, trainingset size, feature selection, and number of terms per class. The data illustrates the superiority of NB algorithm followed by SVM with average accuracy of 64.41 and 60.26 respectively. For all classification algorithms, the best classification accuracy achieved when Boolean representation is used except for C4.5 algorithm.
The top three classifiers ranked in Table 7 : NB, SVM, and C4.5 were further evaluated using two more advanced methods for term selection: information gain (IG) and CHI square (CHI). Table 8 shows the classification accuracy of those classifiers using three different term selection methods, and using all the other classification settings that yielded the best accuracy in the previous set of experiments. The IG and CHI weighting formulas were applied on document frequency. The training and testing sets were randomly compiled using the same corpus (SPA). Since the datasets were generated randomly for this experiment, the results of this experiment and of the previous experiment are not directly comparable. The SVM classifier shows the highest accuracy among the three classifiers, 72.15 % when CHI term selection method were used. This accuracy is very close to that NB achieved, 72.69 %, in Table 6 .

Table 8 also ranks the average accuracy for the three classifiers. SVM also achieved the highest average accuracy at 70.91 %. Even though the highest accuracy was achieved using CHI square, the average accuracy of IG is slightly better than that of CHI square (68.93 % compared to 68.79 %). On the other hand, the least accurate results among the group were always associated with the DF term selection method.

We then studied the impact of the data representation schemes on the accuracy of the classification. Seven different representation schemes were used: relative frequency, entropy, LTC, TFC, TFiDF, frequency and Boolean. SVM was again implemented using the datasets used in the best case of Table 8 . The results of this experiment are shown in Table 9 . The best achieved accuracy remains the same as in Table 8 (72.15 %) using the Boolean representation scheme. The LTC scheme achieved an identical accuracy while the accuracy using relative frequency is very close (71.93 %). The least accurate results were with entropy (66.23 %). 6 The impact of training and testing set size on classification accuracy This set of experiments tested the current best settings from Tables 8 , 9 on data from seven different corpora, including the SPA corpus. The classification settings used here are (classifier = SVM, representation = Boolean, training size = 70 and 90 %, term selection = CHI square, and terms = top 30/40/50 terms of each class). We used three stop word lists to filter out very common words from the data. A general stop word list was used with the following corpora (Writers, NP, Poems, and SPA). The forum stop word list was used with both the Web corpus, and the forums corpus. The third stop word list (the Islamic stop word list) was used with the Islamic Topics corpus.

Table 10 shows the results of this experiment using four runs. Run 1 is based on the best settings found in Tables 8 , 9 . The other runs show the effect on classification accuracy when the size of the training data and the number of terms per class increase.
In Run 1, the most accurate results were obtained using the Islamic Topics corpus (86.42 %). The Writers corpus comes next with an accuracy of 75.61 %. The classification accuracy decreased dramatically with the Arabic Poems corpus at 36.42 %. The classification accuracy using the remaining corpora is around 70 %. The average accuracy increased with each run, finally reaching 73.26 % after starting at 68.85 %, but the average in Run 4 showed little improvement over the average in Run 3 (0.14 % improvement). In all of the corpora except for Poems and SPA, individual accuracy improved with each run. The most noticeable result is from the Islamic corpus in Run 4 (accuracy of 95.05 %) and the result for the writers corpus (82.93 %) in the same run. On the other hand, there was an 18.76 % decrease in accuracy for the Poems corpus in Run 4.
 We replicated the same set of runs but this time using C5.0 classifier as shown in Table 10 . In Run 1, the most accurate results were obtained using the Islamic Topics corpus (92.12 %) as well as with the SVM classifier; however, the C5.0 Classifier gives better accuracy. The writers corpus comes next with an accuracy of 86.43 %. The classification accuracy decreased dramatically with the Arabic Poems corpus to reach 49.15 %. The average accuracy of Run 1 is 78.42 %. The average accuracy increased, run after run, reaching 80.51 % after starting at 68.85 %, but the average in Run 4 showed only a small improvement over the average in Run 3 (0.64 % improvement).

In all runs, the results of the C5.0 classifier overcame the results of the SVM classifier, excluding the Islamic corpus in Run 4. It was noticeable that the result for the Islamic corpus (accuracy of 95.05 %) is better than what was achieved with the C5.0 classifier (93.96 %). In addition, it was noticeable that in general the improvement in accuracy is minor over each run.
 The orders of accuracies for both sets of experiments are of the same sort. However, the C5.0 classifier gives better results. This may be due to the splitting technique used with the C5.0 classifier. It works by splitting the sample, based on the field that provides the maximum information gain. Each subsample defined by the first split is then split again, usually based on a different field, and the process repeats until the subsamples cannot be split any further. Finally, the lowest level splits are re-examined, and those that do not contribute significantly to the value of the model are removed or pruned.
 In Table 10 , the Poems corpus yielded the lowest results among all the corpora. This is because of the nature of poetry, in which its quality highly relies on avoiding word repetition which, in turn, has a negative impact on the feature selection. When we excluded the Poems corpus, the average accuracy increased by almost 5 %. 7 Evaluating feature selection and feature representation As previously illustrated, C5.0 and SVM algorithms produced more accurate classi-fications than the NB, C4.5, MLP, and KNN algorithms. A comparison between three term selection methods and seven data representation schemes is also reported. The CHI term selection method outperformed both the IG and DF methods, and the Boolean and LTC representation schemes were the most accurate schemes for classification. Additionally, the results revealed that increasing the number of selected terms improved the accuracy of the output. However, the results that were introduced earlier are based on a relatively small variation of datasets and can be further strengthened if similar experiments are applied on larger variations. Hence, the current experiment was designed to build on the previous experiments and to cover a wide variety of datasets.

In this experiment, classification accuracy was evaluated utilizing nine represen-tation schemes and seven term selection methods, and using TF and DF as two different bases for term selection. Each corpus of the seven corpora was split into a training dataset (70 %) and a testing dataset (30 %). Each training dataset was used to generate 126 training matrices using all combinations of term selection methods and data representation schemes. All term selection methods have been set to select the top 200 terms from each class in the corresponding corpus. A total of 882 matrices were generated using the seven corpora. Common terms and words have all been filtered out using special stop word lists before applying term selection. The main classification algorithm used in this experiment is the SVM algorithm.

Table 11 shows the overall results of this set of experiments where each cell in the table illustrates the average classification accuracy for the seven corpora. The highest average accuracy is 80.53 %, which was achieved using TF as the term selection base with the GSS term selection method and the LTC representation scheme.

The following important findings are supported by the results: (a) The MI and DIA term selection methods produced exactly the same results (b) Except for very few cases, OddsR also produced results similar to those of MI (c) Even though the highest average was achieved using TF, the overall average (d) The top eight most accurate results were achieved using the LTC represen-(e) The top three most accurate results were achieved using the TF term selection (f) The top six most accurate results were achieved using either the GSS, RS, or (g) LTC always produced the highest accuracy with all the term selection methods (h) Entropy seems to work better with DF than TF. (i) Based on the top ten average accuracies extracted from Table 11 , Table 12
Table 13 presents the classification accuracy for each corpus in the set. The numbers are shown in two main columns. The first column shows the accuracy as it occurs in the best overall average (TF-LTC-GSS), while the second column shows the best accuracy of each corpus using different methods. The main aspects of these results are summarized below: (a) Except for the Writers corpus, accuracies in the best overall average are equal (b) The difference between the accuracies in the two columns in the Writers row is (c) Except for the Poems corpus, the accuracy associated with each corpus ranges (d) The best achieved accuracy for the Arabic Poems corpus is 56.84 %, repre-(e) Additionally, poem writing involves a variety of writing techniques, such
SVM is the main classification algorithm in this section. It showed very good results however in order to evaluate other classification algorithms that showed promising results in previous experiments, we ran the same experiment using the NB and C4.5 algorithms and then compared results. The results of this comparison are shown in Table 14 . The SVM algorithm outperformed the other two classi-fication algorithms, with an average improvement of 6.56 % over NB and 31.58 % over C4.5. The SVM results were achieved using TF as the base for term selection, GSS as the term selection method, and LTC as the representation scheme. The TFC and None term selection methods also produced very good results. 8 The impact of number of features on classification accuracy The results from the previous experiments helped us identify the method that gave the best average performance for classification X  X .e., TF-LTC-GSS using the SVM classification algorithm. In this experiment, we tried to determine if there is still room for further improvement by using more terms. Results from previous experiments indicated that the use of more terms will probably improve the performance.
Table 15 presents the results using a different number of terms. Terms were selected as being in the top 1, 2, ... 20 % terms of each class in the related corpus using the GSS feature selection function. The results in general indicate that we obtained better accuracies with higher percentages of the number of terms, but improvements in some cases were not significant. With four of the seven corpora, it was not possible to run the experiment with higher percentages of the number of terms because of memory size limitations. Figure 5 illustrates the results graphically.

The average improvement that occurred when increasing the number of terms from 1 % to at most 20 % was 7.17 %. The greatest improvement in accuracy was recorded for the Poems corpus (49.68 X 60.63 %). In contrast, the Islamic Topics corpus exhibited the least improvement in accuracy (96.12 X 96.72 %). We concluded, therefore, that further improvement in accuracy can be achieved by increasing the number of terms. The factors that govern the choice of more terms involve the available memory resources and speed requirements. If the available memory is limited and classification speed is a concern, then we recommend using fewer terms for the analysis. 9 Conclusion In addition to building large Arabic corpora for text classification, the main contribution of this paper was to investigate a variety of text classification techniques using the same datasets. These techniques include a wide range of classification algorithms, term selection methods, and representation schemes. The classification techniques used in this paper have been widely used by many researchers for the same task. However, to the best of our knowledge, none of the previous works has tried to compare the accuracy of all of these techniques when applied to datasets that belong to a large spectrum of genres, as presented in this paper.

Several classification algorithms were tested in this study (C4.5, C5.0, MLP neural networks, SVM, NB, and KNN algorithms). SVM produced the most accurate classification in the main experiments presented in this paper. The next most noteworthy classification algorithms were C4.5 and NB. However, SVM showed much better results than the other two algorithms, outperforming NB, on average, by 6.56 % and C4.5 by 31.58 %. Some experiments were conducted using the C5.0 decision tree algorithm. In these experiments, C5.0 produced outstanding results that outperformed those from SVM. For term selection, we compared several methods used frequently in the literature. The investigated methods were CHI, DIA, GSS, IG, MI, NGL, None, Odds ratio, and RS. The None method involved using either TF or DF as the only base for term ranking. GSS, None, and RS were the three methods that showed the best results. Our best average result was achieved using the GSS method with TF as the base for calculations.

Several representation schemes, also known as term weighting functions, were evaluated in addition. These included Boolean, frequency, LTC, TFiDF, TFC, entropy, and relative frequency. The experimental results showed that LTC was superior, followed by Boolean and TFC. A related issue in term selection is the proper selection of the required number of terms to achieve good classification accuracy. The results demonstrated that a higher number of terms produced better accuracy, although the improvements saturate after a certain limit. Factors that govern the choice of the number of terms are related to memory and speed X  X .e., how much memory is available and how fast the classification process should be.
The overall results of the different experiments presented in this paper are very good except for the poems corpus. The average would be way better without this one. The best classification accuracy ranges from 60.63 to 96.72 % using seven corpora, representing an average of 85.06 %. The accuracy differs greatly between corpora. The corpus with the most accurate result was the Islamic Topics corpus, while the Arabic Poems corpus yielded the least accurate result. Future work will consider other issues related to Arabic text classification. These include employing linguistic information such as word stems and parts of speech. This approach should be attainable, given the current increased interest in Arabic Natural Language Processing (NLP) in the research community.
 References
