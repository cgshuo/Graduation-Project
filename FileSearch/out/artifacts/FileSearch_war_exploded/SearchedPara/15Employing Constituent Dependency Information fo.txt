 LONGHUA QIAN, GUODONG ZHOU, and QIAOMING ZHU, Soochow University As one of the key tasks in the field of Natural Language Processing (NLP), information extraction (IE) attempts to identify relevant information from a large amount of text documents in the form of natural language and put them in a structured format. The research on IE was first initiated by the Message Understanding Conferences [MUC 1987 X 1998] and then further promoted significantly by the NIST Automatic Context Extraction [ACE 2002 X 2008] program. Of the three subtasks in information extrac-tion defined by the NIST ACE program, namely Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Character-ization (EDC), this article focuses exclusively on the RDC subtask, which detects and classifies semantic relationships between predefined types of entities previously recog-nized by the EDT subtask in the ACE corpus. The RDC subtask is often referred to as semantic relation extraction, or more shortly as relation extraction. For example, the sentence  X  Microsoft Corp. is based in Redmond , WA X  conveys the relation of  X  X PE-AFF.Based X  between  X  Microsoft Corp.  X  with the entity type of  X  X RG X  and  X  Redmond  X  with the entity type of  X  X PE X . In addition to information extraction itself, relation extraction is also very useful in many advanced NLP applications, such as question answering and text summarization. However, due to limited accuracy in the current state-of-the-art syntactic and semantic parsing as well as the complexity and variabil-ity of the semantic relationships, reliably extracting such semantic relationship be-tween named entities from natural language documents is still a difficult, unresolved problem.

In the literature, feature-based methods have ever dominated the research in rela-tion extraction [Jiang and Zhai 2007; Kambhatla 2004; Zhao and Grishman 2005; Zhou et al. 2005], by first transforming relation examples into the corresponding numerical vectors of various syntactic and semantic features and then applying a machine learn-ing approach (such as SVMs or maximum entropy models) to detect and classify them into predefined types of semantic relationships between named entities. The features used include lexical items, phrase and chunk information, syntactic parse trees, deep semantic information and entity-related information. However, the featured-based methods usually fail to effectively capture the critical structural information inherent in the parse trees. Detailed research [Jiang and Zhai 2007; Zhou and Zhang 2007; Zhou et al. 2005] shows that it is quite difficult for the feature-based methods to extract new effective features to further improve the ex traction accuracy. Therefore, researchers turn to kernel-based methods, which attempt to avoid the burden of feature engineer-ing through directly computing the simila rity between any two discrete objects, like parse trees with rich structural information. From prior work [Bunescu and Mooney 2005; Culotta and Sorensen 2004; Zelenko et al. 2003] to current research [Nguyen et al. 2009; Zhang et al. 2006; Zhou et al. 2007, 2009], kernel-based methods have been showing more and more potential in relation extraction due to its effectiveness in modeling discrete objects, for example, capturing the structural information in parse trees.

Among the first to employ the kernel-based methods for relation extraction, Zelenko et al. [2003] achieves surprisingly good performance on two simple tasks using the shallow parse tree. For the more challenging ACE task in relation extraction, kernel-based methods using the dependency tree [Culotta and Sorensen 2004] and the short-est dependency path [Bunescu and Mooney 2005] exhibit relatively high precision. However, they suffer from quite low recall in performance. Thanks to the pioneering work in Semantic Role Labeling (SRL) by Moschitti [2004], convolution tree kernels over syntactic parse trees [Collins and Duffy 2001] are successfully adopted by Zhang et al. [2006] and Zhou et al. [2007] to relation extraction and achieve comparable or even better performance than feature-based ones, largely due to their distinctive merit in effectively capturing the structural information in relation instances. This article focuses on relation extraction using the convolution tree kernels.

Given the convolution tree kernels, the key problem for the kernel-based methods on relation extraction is how to properly represent the structural information inher-ent in relation instances. In this respect, Zhang et al. [2006] investigates five kinds of tree spans and discover that the simple Shortest Path-enclosed Tree (SPT) achieves the best performance. Zhou et al. [2007] further extend the SPT to the more general Context-Sensitive Shortest Path-enclosed Tree (CS-SPT), which dynamically includes necessary predicate-linked path information beyond the SPT. The problem with both SPT and CS-SPT is that they may still contain unnecessary information. For example, inthesentence X ...bought one of town X  X  two meat-packing plants  X , the underlined part in the SPT/CS-SPT is unnecessary for determining the relationship between two enti-ties  X  one  X  X nd X  plants  X . Moreover, a great deal of useful context-sensitive information may be wrongly removed from SPT/CS-SPT, e ven though the CS-SPT already includes some contextual information related to the predicate-linked path. For example, in the samesentence X ...bought one of town  X  X  two meat-packing plants  X , the underlined part, which is removed from the SPT/CS-SPT, is critical for determining the relationship be-tween two entities  X  one  X  X nd X  town  X . Similar phenomena also exist when determining semantic relationships in Chinese narratives.

To address the issues mentioned above, this article proposes a new approach to dynamically determine the tree span for relation extraction by exploiting constituent dependencies to remove the noisy information, as well as to keep the necessary infor-mation in the parse tree. The motivation is to properly utilize linguistic dependency knowledge in constructing a concise and effe ctive tree span, specifically targeted for relation extraction. Moreover, various kinds of entity-related semantic information are explored via a unified parse and semantic tree framework.

The layout of the rest article is organized as follows. Section 2 gives a brief overview of related work. Section 3 proposes the dynamic syntactic parse tree while the unified syntactic and semantic tree with integrated entity-related semantic information is pre-sented in Section 4. Section 5 reports the experimental results on various ACE RDC corpora. Finally, we conclude our work and indicate some future work in Section 6. The task of relation extraction was first envisioned as part of the Template Element task in MUC-6 [Grishman and Sundheim 1996] and formulated as the Template Relation task in MUC-7 [MUC-7 1998], which was further reformulated as the RDC subtask in the NIST ACE program [ACE 2002 X 2008]. Thereafter, many machine-learning methods have been proposed for relation extraction, including supervised learning, semi-supervised learning, and unsupervised learning. As a dominant method, supervised learning can be further classified into feature-based [Jiang and Zhai 2007; Kambhatla 2004; Zhao and Grishman 2005; Zhou and Zhang 2007; Zhou et al. 2005], kernel-based [Bunescu and Mooney 2005; Culotta and Sorensen 2004; Zelenko et al. 2003; Zhang et al. 2006; Zhou et al. 2007], and composite kernel-based [Zhang et al. 2006; Zhou et al. 2007].

The problem related to feature-based methods is that it is quite difficult to further improve the performance of relation extraction through feature engineering, since al-most all the flat features related to lexical, syntactic, and semantic knowledge have been systematically explored. In addition, most of the structural information, such as dependency information and parse tree, shou ld become flattened (e.g., the chain of con-stituents along the path connecting the two involved entities in the parse tree) to be included in feature-based methods. This severely weakens the capability of effectively capturing their inherent structural nature.

As an alternative to feature-based methods, kernel-based methods enjoy the ad-vantage of directly operating on discrete objects, leading to the potential of effec-tively modeling such objects, for example, capturing the structural information in the parse tree. Consequently, it can avoid the burden of feature engineering by directly computing the similarity between any two trees corresponding to their respective re-lation instances.

Zelenko et al. [2003] describe a recursive kernel between shallow parse trees to extract semantic relations, where a relation instance is transformed into the least common sub-tree connecting the two entity nodes. The kernel matches the nodes of two corresponding sub-trees from roots to leaf nodes recursively layer by layer in a top-down manner. Their method shows successful results on two simple extraction tasks. Culotta and Sorensen [2004] propose a slightly generalized version of this ker-nel between dependency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. These strong constraints make their kernel yield high pre-cision but very low recall on the ACE RDC 2003 corpus. Bunescu and Mooney [2005] develop a shortest path dependency tree kernel, which simply counts the number of common word classes at each node in the shortest paths between two entities in de-pendency trees. Similar to Culotta and Sorensen [2004], this method also suffers from low recall, although its precision is promising.

Inspired by the successful application of convolution tree kernels to syntactic pars-ing [Collins and Duffy 2001] and semantic role labeling [Moschitti 2004], Zhang et al. [2006, 2008a] employs a convolution tree kernel to investigate various forms of struc-tural information for relation extraction and find that the Shortest Path-enclosed Tree (SPT) achieves the F -measure of 67.7 on the seven relation types of the ACE RDC 2004 corpus. One problem with SPT is that it loses the contextual information outside SPT, which is usually critical for relation extraction. Zhou et al. [2007] notice the fact that both the SPT and the convolution tree kernel are context-free and thus expand the SPT to the CS-SPT by dynamically including necessary predicate-linked path in-formation and further extend the standard convolution tree kernel to context-sensitive convolution tree kernel, obtaining the F -measure of 73.2 on the seven relation types of the ACE RDC 2004 corpus. However, the CS-SPT only recovers part of contextual information and still contains noisy information as much as the SPT. Another prob-lem related to convolution tree kernels is that entity-related semantic information has not yet been well incorporated into the structural syntactic tree, though they are very helpful to relation extraction.

In order to utilize the advantages of both feature-based methods and kernel-based methods, some researchers resort to composite kernel-based methods. Zhao and Grishman [2005] define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F -measure of 70.4 on the seven relation types in the ACE RDC 2004 corpus. Zhang et al. [2006] design a composite kernel consisting of an entity linear kernel and a standard convolution tree kernel, obtaining the F -measure of 72.1 on the seven relation types in the ACE RDC 2004 corpus. Zhou et al. [2007] further describe a composite kernel to integrate a context-sensitive convolution tree kernel and a state-of-the-art linear kernel, achieving the so far best F -measure of 75.8 on the seven relation types in the ACE RDC 2004 corpus. Nguyen et al. [2009] explore various combinations of convolution kernels on constituent, dependency and sequential structures, and achieve the best performance of 71.5 in F -measure for the task of relation extraction on the seven major relation types in the ACE RDC 2004 corpus. However, they do not consider refining the syntactic structural representation of relation instances using dependency information, which is the focus of this article. For feature-based Chinese relation extraction, please refer to Che et al. [2005b], Dong et al. [2007], Li et al. [2008], Zhang et al. [2009]. We will not detail the corre-sponding related work here due to our focus on kernel-based methods.

For kernel-based Chinese relation extraction, Che et al. [2005a] propose an im-proved edit distance measurement over Chinese strings to extract person-affiliation from Chinese texts. Liu et al. [2007] develop a sequence kernel function over Chinese strings for extracting three major relation types and six relation subtypes from an ACE RDC Chinese corpus. However, they didn X  X  mention the exact ACE RDC Chinese cor-pus they used. Huang et al. [2008] explore the convolution tree kernel and the shortest dependency path kernel for Chinese relation extraction. However, they only achieve the F -measure of 34.13 on the ACE RDC 2007 corpus, which is quite disappointing. Our previous work [Yu et al. 2010] exhibits the effectiveness of convolution tree ker-nel for Chinese relation extraction. With the SPT as the structural representation of relation instances, the F -measure of relation extraction on the major relation types reaches as high as 67.0 on the ACE RDC 2005 Chinese corpus.

In this article, we study how to dynamically determine a concise and effective tree span for a relation instance by exploiting constituent dependencies inherent in the parse tree derivation. Moreover, we attempt to capture both the structural syntactic parse information and entity-related semantic information via a unified syntactic and semantic tree framework. Finally, we evaluate the effectiveness of a composite kernel for relation extraction on both English and Chinese corpora in order to further boost the performance. This section first discusses constituent dependency and its role in relation extraction, and then describes how to generate the dynamic syntactic parse tree by employing various kinds of constituent dependencie s to overcome the problems in the currently widely used tree spans. As described in Section 1, one key problem in kernel-based methods is appropriate representation and construction of the stru ctural information in relation instances. Unlike the shallow parse tree [Zelenko et al. 2003] for the simple extraction tasks, the dependency tree [Culotta and Sorensen 2004], and the shortest dependency path tree [Bunescu and Mooney 2005] suffer from low recall in the ACE task due to strict similarity constraints. As for convolution tree kernels, Zhang et al. [2006] explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT, the small-est common sub-tree including the two entities) achieves the best performance. Zhou et al. [2007] further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. However, the critical problem of how to properly represent the structural syntactic parse tree is only partially resolved. As we indicate in what follows, current tree spans suffer from two disadvantages. (1) Both SPT and CS-SPT still contain unnecessary information. For example, in (2) CS-SPT only captures part of the context-sensitive information related to
Since dependency plays a key role in many NLP problems, such as syntactic parsing, semantic role labeling as well as semantic relation extraction, our motivation is to employ various kinds of dependency knowledge to distinguish the necessary evidence from the unnecessary information in the structural syntactic parse tree. Put in another way, this knowledge can be exploited to construct a proper structural representation for relation instances.

On one hand, lexical or word-word dependency indicates the relationship among words occurring in the same sentence. For exa mple, predicate-argument dependency means that arguments are dependent on their target predicates while modifier-head dependency means that modifiers are dependent on their head words. Such depen-dency relationship offers a condensed representation of the information needed to as-sess the relationship in the form of the dependency tree [Culotta and Sorensen 2004] or the shortest dependency path tree [Bunescu and Mooney 2005] that includes two given entities.

On the other hand, when the syntactic parse tree corresponding to the sentence is derived from the bottom to the top using derivation rules step by step, the word-word dependencies converge upward, making a unique head child containing the head word for every non-terminal constituent. Figure 1 illustrates two example parse trees corre-sponding to an English sentence  X  X e owns rental property in the state X  and an Chinese sentence  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X  (A teenager hijacked a bus), where the arrows at the bottoms denote the dependencies between words (the specific type of word dependency is omitted for simplicity since we do not utilize this kind of information), tree nodes except the terminal nodes and POS nodes are additionally tagged with the associated head words. Take the English sentence as an example,  X  X P-property X  means that the node is tagged with  X  X P X  with its head word being  X  X roperty X , and when  X  X J NN X  is reduced to  X  X P X , the head word of  X  X N X  ( X  X roperty X ) is assigned to their parent node  X  X P X  since the  X  X P X  node is dependent on the  X  X N X  node (this dependency relationship is highlighted using a bold line with an arrow pointing to its head child). Generally, each context-free grammar (CFG) rule has the form of
Where, P is the parent node, H is the head child of the rule, L n ...L 1 and R 1 ...R m are left and right modifiers of H respectively, and both n and m may be zero. In other words, the parent node P (dependent) depends on the head child H (governor), this is what we call constituent dependency . Vice versa, we can also determine the head child of a constituent in terms of constituent dependency. Our hypothesis stipulates that the contribution of the parse tree to establishing a relationship is almost exclusively concentrated in the path connecting the two entities, as well as the head children of constituent nodes along this path. More clearly speaking, only the path nodes as well as the governing nodes they are dependent on are considered necessary, while others are unnecessary for relation instances. Guided by the principle of constituent dependencies, beginning with the Minimum Complete Tree (MCT, the complete sub-tree rooted by the nearest common ancestor of the two entities under consideration) as the structural representation of each relation instance, the head child of every node is found according to the production rule along the path connecting the two entities. Then the path nodes and their head children are kept intact or recovered outside the MCT while any other nodes are gradually removed from the parse tree. Eventually we arrive at a condense tree representation called the Dynamic Syntactic Parse Tree ( DSPT), which is dynamically determined by constituent dependencies and only contains necessary information as expected.
Due to the multitude of production rules with regard to parse trees, there exist a considerable number of constituent dependencies in a context-free grammar as de-scribed by Collins [2003]. However, since our task is to extract the relationship be-tween two named entities, our focus is on how to condense useful constituents re-lated to relation extraction, most of whic h are Noun Phrases (NPs). Therefore con-stituent dependencies can be classified into the following five categories according to constituent types of the CFG rules. Figure 2 and Figure 3 illustrate the application of these constituent dependencies to relation instances in both English and Chinese languages from the ACE RDC 2004/2005 corpora respectively. (1) Modification within base-NPs . Base-NPs mean that they do not directly dominate (2) Modification to NPs . Except base-NPs, other modification to NPs can be classified (3) Arguments/adjuncts to verbs . This type includes the CFG rules in which the left (4) Coordination conjunctions . In coordination constructions, several peer conjuncts (5) Modification to other constituents . Except for the above four major types, other
In fact, the SPT [Zhang et al. 2006] can be constructed by carrying out part of the above removal operations using a single heurist ic rule (i.e., all the constituents outside the shortest path linking the two entities should be removed) and CS-CSPT [Zhou et al. 2007] further recovers part of necessary context-sensitive information outside the SPT. This intuitively justifies why the SPT performs well, and why the CS-SPT outperforms the SPT. This section first explores different represen tations of entity-related features. Then, two schemes that combine structural syntactic information and entity-related infor-mation are discussed. Finally, we present both convolution tree kernels and composite kernels used in our experiments. Entity-related semantic features, such as entity headword, entity type and subtype, etc., impose a strong constraint on relation types according to the definitions by the ACE RDC task. For example,  X  X ER-SOC X  relations describe the relationship between entities of type PER, and no other entity type is allowed as an argument of these re-lations. Therefore, virtually all the relation extraction systems contain entity features in one form or the other. Particularly, experiments by Zhang et al. [2006] show that the linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. Qian et al. [2007] further points out that among these entity features, entity type, subtype, and mention type, as well as the base form of predicate verb nearest to the second entity mention, contribute most while the contribution of other features, such as entity class, headword, and GPE role, can be ignored.

In order to effectively capture various kinds of entity-related semantic features and their combinations as well, particularly bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 4. Take an English sentence  X  they  X  X e here  X  (excerpted from the ACE RDC 2004 corpus) as an example, there exists a relationship  X  X hysical.Located X  between the entities  X  they  X  [PER] and  X  here  X  [GPE.Population-Center]. The features are encoded in the tags as  X  X P X ,  X  X T X ,  X  X T X  and  X  X VB X , which denote type, subtype, mention-type of the two en-tities, and the base form of predicate verb in English if existing (nearest to the second entity along the path connecting the two entities) respectively. For example, the tag  X  X P1 X  represents the entity type of the first entity, and the tag  X  X T2 X  represents the entity subtype of the second entity. Following are the three entity-related semantic tree setups. (a) Bag of Features (BOF, e.g., Figure 4(a)). All feature nodes are uniformly attached (b) Feature-Paired Tree (FPT, e.g., Figure 4( b)). The features of two entities are first (c) Entity-Paired Tree (EPT, e.g., Figure 4(c)). All the features relating to an entity
For the preceding three setups, the predicate feature is always attached under the root node, since it is unrelated to any of the two entities. In summary, the BOF only captures the individual entity features, while the FPT and EPT setups are designed to additionally capture the bi-gram and tri-gram features respectively. After determining the structural representations for both the syntactic parse tree and entity-related semantic tree, the question is how to combine them into an effective form for relation extraction. One common way is via a composite kernel, as explored by Zhang et al. [2006]. While we will discuss the composite kernel in the next section, here we propose a unified syntactic and semantic tree (USST), which integrates the entity-related semantic tree into the dynamic syntactic parse tree. Although the entity features can be attached under the top node, the entity nodes, or directly embedded into the labels of the entity nodes, we only explore to attach the three kinds of entity-related semantic trees (i.e., BOF, FPT, and EPT) under the top node of the dynamic syntactic parse tree right after its original children since detailed evaluation [Qian et al. 2007] indicates that the unified tree achieves the best performance when the feature nodes are attached under the top node. Figure 5 shows an instance of the USST with the entity-related semantic tree  X  X OF X  attached. This instance is directly chosen from the ACE RDC 2004 corpus. Here, we omit its Chinese counterpart because they look almost similar except that the words and the node tags X  names are associated with Chinese. After having rendered relation instances as structured representations X  X SPT and USST, we now turn to discuss how to measure the similarity between two relation instances with a convolution kernel. A convolution kernel [Haussler 1999] aims to capture common structural information between two discrete objects in terms of their sub-structures, such as the convolution tree kernel [Collins and Duffy 2001], the string kernel [Lodhi et al. 2002], and the graph kernel [Suzuki et al. 2003]. Following previ-ous studies in relation extraction [Zhang et al. 2006; Zhou et al. 2007], this article also adopts the convolution tree kernel [Collins and Duffy 2001] to compute the similar-ity between two relation instances due to its effectiveness in capturing the structural information inherent in the parse tree.

For the convolution tree kernel to be incorporated into statistical classifiers such as SVMs, a training or test instance should be ultimately casted as a feature vector, in which features are all of its subtree types and the value of each feature is the number of occurrences of that subtree type in the whole tree. Formally, a tree T can be represented as a vector of integer counts of each subtree type, that is: Where # subtree-type i ( T ) denotes the number of occurrences of the i th subtree type in the tree T . Intuitively, the convolution tree kernel counts the number of common sub-tree types as the structure similarity between two trees T 1 and T 2 . Since the number of subtree types in a tree is exponential with its size, it is infeasible to enumerate all the subtree types and directly use the feature vector  X  ( T ). In order to address this computational problem, Collins and Duffy [2002] propose the following method to implicitly compute the dot product between two high-dimensional vectors  X  ( T 1 )and  X  ( T 2 ): where N 1 and N 2 are the sets of nodes in tree T 1 and T 2 , respectively, and I subtree a binary function whose value is 1 if the subtree-type i occurs with its root at node n or zero otherwise, and ( n 1 , n 2 ) evaluates the number of common subtree types rooted at n 1 and n 2 , 4 that is: ( n 1 , n 2 ) can be computed recursively as follows. 1) If the context-free productions (CFG rules) at n 1 and n 2 are different, then 2) If both n 1 and n 2 are POS tags, ( n 1 , n 2 )=1  X   X  ; otherwise go to Step 3. 3) Calculate ( n 1 , n 2 )recursivelyas: where # ch ( n ) is the number of children of node n , ch ( n , k )isthe k th child of node n and  X  (0 &lt; X &lt; 1) is the decay factor in order to make the kernel less variable with respect to different sub-tree sizes.

This kernel has been successfully applied to many tasks in natural language processing, such as syntactic parsing [Collins and Duffy 2002], semantic role label-ing [Moschitti 2004, 2006; Moschitti et al. 2006, 2008; Zhang et al. 2008b] and relation extraction [Zhang et al. 2006, 2008a; Zhou et al. 2007] as well.

In order to integrate the power of different kernels, composite kernels are usually an effective way to combine several kernel s over different structures. With kernels over the dynamic syntactic parse tree and the entity-related semantic tree (or even a feature-based linear kernel) at hand, we can further consider two kinds of composite kernels as follows: 1) Linear combination 2) Polynomial combination where K P 1 denotes the polynomial expansion of K 1 with the degree d =2,thatis, K P 1 = ( K 1 +1) 2 , the others are the same as those in Equation (5). Since the set of kernels is closed under normalization, linear and polynomial combination [Sch  X  olkopf and Smola 2001], these two composite kernels are also proper kernels. This section will evaluate the effectiveness of the dynamic syntactic parse tree and the contribution of various kinds of entity-related semantic information via the unified syntactic and semantic tree framework. For evaluation, we use the ACE RDC 2004 English corpus and the ACE RDC 2005 Chinese corpus as the benchmark data (hereafter we will refer to them as the Eng-lish corpus and the Chinese corpus respectively). The English corpus was gathered from various sources of newspapers, newswire, and broadcasts. This data set contains 451 documents and 5,702 relation instances. It defines seven entity types, seven ma-jor relation types and 23 relation subtypes. For fair comparison with previous work [Zhang et al. 2006; Zhou et al. 2007], evaluation is done on the same subset of 348 doc-uments (nwire/bnews) and 4,400 relation instances using 5-fold cross-validation. The Chinese corpus contains 633 documents, which were collected from newswire, broad-casts, and weblogs. It defines seven entity types, six major relations types, and 18 relations subtypes. Due to many grammati cally ill-formed sentences in the weblogs and for comparability with the English corpus, evaluation is only done on 533 doc-uments (nwire/bnews) and 7,630 relation instances of the Chinese corpus using the same 5-fold cross-validation scheme.

The English corpus is first parsed using the state-of-the-art Charniak X  X  parser [Charniak 2001] with the boundaries of all the entity mentions kept. Then, relation instances are generated by iterating over al l pairs of entity mentions occurring in the same sentence with the given  X  X rue X  mentions and co-referential information. For fair comparison, various kinds of structured rep resentations of relation instances are gen-erated respectively, such as DSPT described in Section 3, EST in Subsection 3.1, USST in Subsection 4.2 as well as the feature vector used in composite kernels in Table VI and Table VII. This same preprocessing strategy is also applied to the Chinese corpus, except that the Chinese sentences are first segmented by the Stanford Chinese Word Segmenter and then parsed by the Stanford Parser. 5 In our experimentations, SVM light [Joachims 1998] with the tree kernel function SVM-TK toolkits [Moschitti 2004] 6 is selected as our classifier. For efficiency consider-ation, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. For the purposes of meaningful comparison, the training pa-rameters C (SVM) and  X  (tree kernel) are also set to 2.4 and 0.4 respectively, which had been empirically verified to exhibit the best performance in previous relation extrac-tion research. As to the coefficients  X  in the composite kernels, 2-fold cross-validations on the training dataset are used to determine their optimal values. That is, the train-ing dataset is divided into two disjoint and independent folds, one of which is used to induce the classifier models, while the other is tested to find the optimal parame-ter values to attain the best performance score. For reasonable comparison between English and Chinese corpora, the same set of parameter values is applied to both the classifiers. Our experiments are presented in the following order: (1) evaluating the contributions of various kinds of constituent dependencies in DSPT, (2) evaluating the contributions of various kinds of entity-related semantic features in (3) comparing the performance between different entity-related semantic tree setups (4) evaluating the effectiveness of composite kernels by DSPT and EST, (5) comparing with other state-of-the-art relation extraction systems.

To determine whether an improvement is significant or not, we conduct significance of less than 0.01, 0.01 X 0.05, and greater than 0.05, which mean significantly better, moderately better, and slightly better, respectively.

Evaluating the contributions of various kinds of constituent dependencies . Table I evaluates the contributions of different kinds of constituent dependencies on the major relation types of the ACE corpora using the convolution parse tree kernel (with only the entity-type information attached as a child under the tree root node in the BOF manner as illustrated in Figure 4(a)). The MCT with only the entity-type information is first used as the baseline, and various constituent dependencies are then applied sequentially to dynamically reshaping the tree in two different modes.  X  X M1] Individual Mode. Every constituent dependency is individually applied on the
MCT, hence this mode is aimed to assess the separate contribution of each con-stituent dependency. In this mode, significance tests are conducted between each specific dependency type and the MCT baseline.  X  X M2] Accumulative Mode. Every constituen t dependency is incrementally applied on the previously derived tree span, which begins with the MCT and eventually gives rise to a Dynamic Syntactic Parse Tree (DS PT). In this mode, significance tests are conducted between each dependency type and its previously-considered ones.
The table reports that the final DSPT achieves the best performance of 77.4%/65.4%/70.9 for English and 79.1%/56.9%/66.2 for Chinese in precision/recall/ F -measure respectively after applying all the dependencies, with the increase of the F -measure by 8.2 units (English) and 8.4 units (Chinese) over the baseline MCTs. This indicates that reshaping the tree by exploiting constituent dependencies can signifi-cantly improve the extraction accuracy la rgely due to the increase in the recalls. It further suggests that the knowledge on constituent dependencies is very effective and can be fully utilized in tree kernel-based relation extraction. Furthermore, this table also shows the following. (1) Modification within base-NPs significantly (***) improves the accuracy for Eng-(2) Modification to NPs contributes much to the performance improvement (***), (3) Modification of arguments/adjuncts to verbs improves the F -measure by 2.8/2.6 (4) Reduction of coordination conjunctions, in particular noun phrase conjunctions (5) Other modifications show trivial effects on the relation extraction performance,
In summary, the DSPT produced by applying constituent dependencies to a parse tree involving two given entities can dramatically boost the performance of relation extraction for both English and Chinese. Particularly, the modification within base-NPs in English and the modification to NPs in both languages contribute most to the improvement, while other modifications help relatively less. Since most of entity men-tions are encapsulated in noun phrases, this result indicates the locality of semantic relationship occurring in the ACE corpora.

Evaluating the contributions of various ki nds of entity-related semantic features in EST . Ta-ble II further evaluates the contributions of various kinds of entity-related semantic features on the extraction of the major relation types on the ACE RDC corpora using the BOF setup by adding them incrementally in the decreasing order of their potential importance. Note that the plus sign after a specific feature means that this feature is useful and should be included in the next round. This table reports that our system achieves the best performance of 80.2%/69.2%/74.7 in terms of P/R/F (English) and 79.7%/60.9%/69.1 (Chinese) respectively. It X  X  interesting to notice that both English and Chinese exhibit consistent behavior on these features. Specifically, it also shows the following. (1) Both entity subtype and mention level information moderately improve the F -(2) It is a bit surprising, however, to observe that the other four kinds of information, (3) Finally for English, the predicate verb (in its basic form) closest to the second entity
Comparing the performance between d ifferent entity semantic tree setups in USST . We com-pare in Table III the performance of the Unified Syntactic and Semantic Trees with different kinds of Entity Semantic Tree setups described in Section 4, while the SPT and DSPT with only entity-type information are listed for reference. Significant tests are conducted between USSTs and SPT. Note that for the English corpus since only four features, such as entity type, entity subtype, mention level and predicate base, show positive effects from the previous experiments, these features are incorporated into the USSTs as illustrated in Figure 4. The same scheme is also applied to the Chinese corpus, except that the predicate base is unavailable. The table shows the following. (1) All the three setups of USST significantly (***) outperform the DSPT setup, ob-(2) The Unified Syntactic and Semantic Tree with FPT (the Feature-Paired Tree struc-
In Table IV we further summarize the improvements of different tree setups over the SPT for both English and Chinese corpora, namely, CS-SPT, DSPT and USST (FPT). Since these results are produced in a similar setting, that is, extraction of ma-jor relation types on the same corpus with the same convolution parse tree kernel using the same 5-fold validation scheme, they can be compared fairly. It shows that for English our DSPT significantly (***) outperforms SPT by 3.8 units in F -measure, while the CS-SPT outperforms the SPT by 1.3 units in F -measure, and for Chinese our DSPT significantly (***) outperforms SPT by 2.2 in F -measure. This suggests that the DSPT performs best among these tree spans. It also shows that the Unified Syntac-tic and Semantic Tree with FPT (Feature-Paired Tree) performs significantly better than the other two tree setups (i.e., CS-SPT and DSPT) by 6.7/4.2 units in F -measure respectively for English, though the increase is not so distinctive for Chinese. This im-plies that the entity-related semantic information is very useful and contributes much when they are incorporated into the parse tree for relation extraction.

Evaluating the effectiveness of composite kernels by DSPT and EST . These experiments are aimed to evaluate the effectiveness of composite kernels by using either linear combination or polynomial combination, and to compare the difference between the composite kernels and the unified syntactic and semantic tree kernels. Table V re-ports the results in terms of P/R/F corresponding to the composite kernels by the DSPT and three different EST setups, using linear combination (outside parenthe-ses) and polynomial combination (inside parentheses) respectively. These results are obtained on the extraction of the major relation types in the ACE RDC corpora. For reference purposes, the results for the DSPT with only entity-type information are also listed. Significant tests are conducted between composite kernels and the single DPST kernel.

When determining the coefficient for linear interpolation in Equation (5), K 1 rep-resents the EST tree kernel while K 2 represents the DSPT tree kernel. Using 2-fold cross-validation on the training data, the coefficient  X  is fine-tuned to 0.4. For poly-nomial combination in Equation (6), K 1 and K 2 denote the same kernels as for linear combination. Same as the most common setting in the literature [Zhang et al. 2006; Zhou et al. 2007], the polynomial order d is set to 2, while the coefficient  X  is fine-tuned to 0.2 in the same way as for the linear combination. From Table V we can see the following. (1) All the three setups of composite kernels, regardless of their combination methods, (2) Among these composite kernels, for English the linear one with FPT achieves the (3) In contrast to the findings by Zhang et al. [2006] and Zhou et al. [2007] that the
Comparing with other state-of-the -art relation extraction systems . Finally, Table VI compares our system with other state-of-the-art kernel-based systems on the major relation types of the ACE RDC 2004 corpus. It shows that on the ACE RDC 2004 corpus our USST (FPT) outperforms all previous tree setups using one single kernel, even better than two previous composite kernels [Zhang et al. 2006; Zhao and Grishman 2005]. Furthermore, when the USST (FPT) kernel is combined with a linear state-of-the-state feature-based kernel [Zhou et al. 2005] into a composite one polynomial combination in a setting similar to Zhou et al. [2007] (i.e., polyno-mial degree d = 2 and coefficient  X  = 0.3), we get the so far best performance of 77.1 in F -measure for seven major relation types on the ACE RDC 2004 English corpus.

For Chinese, we implemented a feature-based relation extraction system with the similar features as Zhou et al. [2005], including lexical words, entity features, overlapping features and chunking information etc., and achieve the performance of 77.98%/52.34%/62.64 in terms of P/R/F on the ACE RDC 2005 Chinese corpus. Then this linear kernel is further combined with the USST (FPT) kernel for Chinese relation extraction in the same way and using the same parameters as the composite kernel for English. The experiment results are compared in Table VII with other state-of-the-art Chinese relation extraction systems on the ACE RDC 2005 Chinese corpus. However, this comparison is by no means fair due to their distinct evaluation schemes and there-fore only for reference. For example, while we adopt the 5-fold cross validation strategy on the ACE RDC 2005 Chinese corpus, both Li et al. [2008] and Zhang et al. [2009]) use 75% of the total relation instances as the training set and the remaining instances as the test data, and furthermore the total number of relation instances are also dif-ferent due to different preprocessing procedures. Specifically, Li et al. [2008] explore character-based unigram and bi-gram features as well as nine positional structures, plus some correction and inference mechani sms based on the relation hierarchy and co-reference information, while we use convolution tree kernels over syntactic parse trees and entity-related semantic trees. Actually, syntactic parse trees include some kind of positional structures. In Zhang et al. [2009], they combine an entity semantic kernel and a string semantic similarity kern el for Chinese relation extraction, while ignoring the structural information inherent in parse trees. Although comparison of our work with these two works is difficult, nevertheless, one important thing to be sure is that, the USST employing constituent dependencies significantly outperforms that in our previous work [Yu et al. 2010] under the same setting, thus indicating the effectiveness of the convolution tree kernels over syntactic parse trees for extracting Chinese semantic relations as well as for extracting English semantic relations. This article systematically explores the potential of structured syntactic information for tree kernel-based relation extraction on both English and Chinese. In particular, a new approach is proposed to dynamically determine the tree span (DSPT) for relation instances by exploiting constituent dependencies. In addition, we investigate different ways of integrating various kinds of entity-related semantic information via a Unified Syntactic and Semantic Tree (USST). Evaluation on the ACE RDC corpora shows that our DSPT is appropriate for structural syntactic representation of relation instances for both English and Chinese languages. It also shows that, in addition to individual entity features, combined entity features (especially bi-gram) contribute much when they are combined with the DSPT into the USST framework. Finally, this article re-ports the so-far best performance for both English and Chinese corpora when combin-ing the USST-based tree kernel and a state-of-the-art feature-based linear kernel via a composite kernel.

For the future work, on one hand, we will focus on improving the performance of complex structural parse trees, where the path connecting the two entities involved in a relationship is too long for current kernel methods to take effect. Our preliminary experiments of employing a discourse theory exhibit certain positive results. On the other hand, while most recent research on relation extraction focuses on semantic re-lations occurring within one sentence, extracting inter-sentential relations that occur across multiple sentences pose a more challenging problem. It is estimated that 28.5% of MUC6 relations and 9.4% of ACE 2003 relations are inter-sentential [Swampillai and Stevenson 2010], though the ACE project since 2004 limits to annotating relations which are expressed within a single sentence. While some of these inter-sentential relations may be automatically extracted using syntax and co-reference information, others are more difficult and can only be resolved using real-world background knowl-edge. Therefore, further research is necessary to address the issue of inter-sentential relation extraction.

 LONGHUA QIAN, GUODONG ZHOU, and QIAOMING ZHU, Soochow University As one of the key tasks in the field of Natural Language Processing (NLP), information extraction (IE) attempts to identify relevant information from a large amount of text documents in the form of natural language and put them in a structured format. The research on IE was first initiated by the Message Understanding Conferences [MUC 1987 X 1998] and then further promoted significantly by the NIST Automatic Context Extraction [ACE 2002 X 2008] program. Of the three subtasks in information extrac-tion defined by the NIST ACE program, namely Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Character-ization (EDC), this article focuses exclusively on the RDC subtask, which detects and classifies semantic relationships between predefined types of entities previously recog-nized by the EDT subtask in the ACE corpus. The RDC subtask is often referred to as semantic relation extraction, or more shortly as relation extraction. For example, the sentence  X  Microsoft Corp. is based in Redmond , WA X  conveys the relation of  X  X PE-AFF.Based X  between  X  Microsoft Corp.  X  with the entity type of  X  X RG X  and  X  Redmond  X  with the entity type of  X  X PE X . In addition to information extraction itself, relation extraction is also very useful in many advanced NLP applications, such as question answering and text summarization. However, due to limited accuracy in the current state-of-the-art syntactic and semantic parsing as well as the complexity and variabil-ity of the semantic relationships, reliably extracting such semantic relationship be-tween named entities from natural language documents is still a difficult, unresolved problem.

In the literature, feature-based methods have ever dominated the research in rela-tion extraction [Jiang and Zhai 2007; Kambhatla 2004; Zhao and Grishman 2005; Zhou et al. 2005], by first transforming relation examples into the corresponding numerical vectors of various syntactic and semantic features and then applying a machine learn-ing approach (such as SVMs or maximum entropy models) to detect and classify them into predefined types of semantic relationships between named entities. The features used include lexical items, phrase and chunk information, syntactic parse trees, deep semantic information and entity-related information. However, the featured-based methods usually fail to effectively capture the critical structural information inherent in the parse trees. Detailed research [Jiang and Zhai 2007; Zhou and Zhang 2007; Zhou et al. 2005] shows that it is quite difficult for the feature-based methods to extract new effective features to further improve the ex traction accuracy. Therefore, researchers turn to kernel-based methods, which attempt to avoid the burden of feature engineer-ing through directly computing the simila rity between any two discrete objects, like parse trees with rich structural information. From prior work [Bunescu and Mooney 2005; Culotta and Sorensen 2004; Zelenko et al. 2003] to current research [Nguyen et al. 2009; Zhang et al. 2006; Zhou et al. 2007, 2009], kernel-based methods have been showing more and more potential in relation extraction due to its effectiveness in modeling discrete objects, for example, capturing the structural information in parse trees.

Among the first to employ the kernel-based methods for relation extraction, Zelenko et al. [2003] achieves surprisingly good performance on two simple tasks using the shallow parse tree. For the more challenging ACE task in relation extraction, kernel-based methods using the dependency tree [Culotta and Sorensen 2004] and the short-est dependency path [Bunescu and Mooney 2005] exhibit relatively high precision. However, they suffer from quite low recall in performance. Thanks to the pioneering work in Semantic Role Labeling (SRL) by Moschitti [2004], convolution tree kernels over syntactic parse trees [Collins and Duffy 2001] are successfully adopted by Zhang et al. [2006] and Zhou et al. [2007] to relation extraction and achieve comparable or even better performance than feature-based ones, largely due to their distinctive merit in effectively capturing the structural information in relation instances. This article focuses on relation extraction using the convolution tree kernels.

Given the convolution tree kernels, the key problem for the kernel-based methods on relation extraction is how to properly represent the structural information inher-ent in relation instances. In this respect, Zhang et al. [2006] investigates five kinds of tree spans and discover that the simple Shortest Path-enclosed Tree (SPT) achieves the best performance. Zhou et al. [2007] further extend the SPT to the more general Context-Sensitive Shortest Path-enclosed Tree (CS-SPT), which dynamically includes necessary predicate-linked path information beyond the SPT. The problem with both SPT and CS-SPT is that they may still contain unnecessary information. For example, inthesentence X ...bought one of town X  X  two meat-packing plants  X , the underlined part in the SPT/CS-SPT is unnecessary for determining the relationship between two enti-ties  X  one  X  X nd X  plants  X . Moreover, a great deal of useful context-sensitive information may be wrongly removed from SPT/CS-SPT, e ven though the CS-SPT already includes some contextual information related to the predicate-linked path. For example, in the samesentence X ...bought one of town  X  X  two meat-packing plants  X , the underlined part, which is removed from the SPT/CS-SPT, is critical for determining the relationship be-tween two entities  X  one  X  X nd X  town  X . Similar phenomena also exist when determining semantic relationships in Chinese narratives.

To address the issues mentioned above, this article proposes a new approach to dynamically determine the tree span for relation extraction by exploiting constituent dependencies to remove the noisy information, as well as to keep the necessary infor-mation in the parse tree. The motivation is to properly utilize linguistic dependency knowledge in constructing a concise and effe ctive tree span, specifically targeted for relation extraction. Moreover, various kinds of entity-related semantic information are explored via a unified parse and semantic tree framework.

The layout of the rest article is organized as follows. Section 2 gives a brief overview of related work. Section 3 proposes the dynamic syntactic parse tree while the unified syntactic and semantic tree with integrated entity-related semantic information is pre-sented in Section 4. Section 5 reports the experimental results on various ACE RDC corpora. Finally, we conclude our work and indicate some future work in Section 6. The task of relation extraction was first envisioned as part of the Template Element task in MUC-6 [Grishman and Sundheim 1996] and formulated as the Template Relation task in MUC-7 [MUC-7 1998], which was further reformulated as the RDC subtask in the NIST ACE program [ACE 2002 X 2008]. Thereafter, many machine-learning methods have been proposed for relation extraction, including supervised learning, semi-supervised learning, and unsupervised learning. As a dominant method, supervised learning can be further classified into feature-based [Jiang and Zhai 2007; Kambhatla 2004; Zhao and Grishman 2005; Zhou and Zhang 2007; Zhou et al. 2005], kernel-based [Bunescu and Mooney 2005; Culotta and Sorensen 2004; Zelenko et al. 2003; Zhang et al. 2006; Zhou et al. 2007], and composite kernel-based [Zhang et al. 2006; Zhou et al. 2007].

The problem related to feature-based methods is that it is quite difficult to further improve the performance of relation extraction through feature engineering, since al-most all the flat features related to lexical, syntactic, and semantic knowledge have been systematically explored. In addition, most of the structural information, such as dependency information and parse tree, shou ld become flattened (e.g., the chain of con-stituents along the path connecting the two involved entities in the parse tree) to be included in feature-based methods. This severely weakens the capability of effectively capturing their inherent structural nature.

As an alternative to feature-based methods, kernel-based methods enjoy the ad-vantage of directly operating on discrete objects, leading to the potential of effec-tively modeling such objects, for example, capturing the structural information in the parse tree. Consequently, it can avoid the burden of feature engineering by directly computing the similarity between any two trees corresponding to their respective re-lation instances.

Zelenko et al. [2003] describe a recursive kernel between shallow parse trees to extract semantic relations, where a relation instance is transformed into the least common sub-tree connecting the two entity nodes. The kernel matches the nodes of two corresponding sub-trees from roots to leaf nodes recursively layer by layer in a top-down manner. Their method shows successful results on two simple extraction tasks. Culotta and Sorensen [2004] propose a slightly generalized version of this ker-nel between dependency trees, in which a successful match of two relation instances requires the nodes to be at the same layer and in the identical path starting from the roots to the current nodes. These strong constraints make their kernel yield high pre-cision but very low recall on the ACE RDC 2003 corpus. Bunescu and Mooney [2005] develop a shortest path dependency tree kernel, which simply counts the number of common word classes at each node in the shortest paths between two entities in de-pendency trees. Similar to Culotta and Sorensen [2004], this method also suffers from low recall, although its precision is promising.

Inspired by the successful application of convolution tree kernels to syntactic pars-ing [Collins and Duffy 2001] and semantic role labeling [Moschitti 2004], Zhang et al. [2006, 2008a] employs a convolution tree kernel to investigate various forms of struc-tural information for relation extraction and find that the Shortest Path-enclosed Tree (SPT) achieves the F -measure of 67.7 on the seven relation types of the ACE RDC 2004 corpus. One problem with SPT is that it loses the contextual information outside SPT, which is usually critical for relation extraction. Zhou et al. [2007] notice the fact that both the SPT and the convolution tree kernel are context-free and thus expand the SPT to the CS-SPT by dynamically including necessary predicate-linked path in-formation and further extend the standard convolution tree kernel to context-sensitive convolution tree kernel, obtaining the F -measure of 73.2 on the seven relation types of the ACE RDC 2004 corpus. However, the CS-SPT only recovers part of contextual information and still contains noisy information as much as the SPT. Another prob-lem related to convolution tree kernels is that entity-related semantic information has not yet been well incorporated into the structural syntactic tree, though they are very helpful to relation extraction.

In order to utilize the advantages of both feature-based methods and kernel-based methods, some researchers resort to composite kernel-based methods. Zhao and Grishman [2005] define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F -measure of 70.4 on the seven relation types in the ACE RDC 2004 corpus. Zhang et al. [2006] design a composite kernel consisting of an entity linear kernel and a standard convolution tree kernel, obtaining the F -measure of 72.1 on the seven relation types in the ACE RDC 2004 corpus. Zhou et al. [2007] further describe a composite kernel to integrate a context-sensitive convolution tree kernel and a state-of-the-art linear kernel, achieving the so far best F -measure of 75.8 on the seven relation types in the ACE RDC 2004 corpus. Nguyen et al. [2009] explore various combinations of convolution kernels on constituent, dependency and sequential structures, and achieve the best performance of 71.5 in F -measure for the task of relation extraction on the seven major relation types in the ACE RDC 2004 corpus. However, they do not consider refining the syntactic structural representation of relation instances using dependency information, which is the focus of this article. For feature-based Chinese relation extraction, please refer to Che et al. [2005b], Dong et al. [2007], Li et al. [2008], Zhang et al. [2009]. We will not detail the corre-sponding related work here due to our focus on kernel-based methods.

For kernel-based Chinese relation extraction, Che et al. [2005a] propose an im-proved edit distance measurement over Chinese strings to extract person-affiliation from Chinese texts. Liu et al. [2007] develop a sequence kernel function over Chinese strings for extracting three major relation types and six relation subtypes from an ACE RDC Chinese corpus. However, they didn X  X  mention the exact ACE RDC Chinese cor-pus they used. Huang et al. [2008] explore the convolution tree kernel and the shortest dependency path kernel for Chinese relation extraction. However, they only achieve the F -measure of 34.13 on the ACE RDC 2007 corpus, which is quite disappointing. Our previous work [Yu et al. 2010] exhibits the effectiveness of convolution tree ker-nel for Chinese relation extraction. With the SPT as the structural representation of relation instances, the F -measure of relation extraction on the major relation types reaches as high as 67.0 on the ACE RDC 2005 Chinese corpus.

In this article, we study how to dynamically determine a concise and effective tree span for a relation instance by exploiting constituent dependencies inherent in the parse tree derivation. Moreover, we attempt to capture both the structural syntactic parse information and entity-related semantic information via a unified syntactic and semantic tree framework. Finally, we evaluate the effectiveness of a composite kernel for relation extraction on both English and Chinese corpora in order to further boost the performance. This section first discusses constituent dependency and its role in relation extraction, and then describes how to generate the dynamic syntactic parse tree by employing various kinds of constituent dependencie s to overcome the problems in the currently widely used tree spans. As described in Section 1, one key problem in kernel-based methods is appropriate representation and construction of the stru ctural information in relation instances. Unlike the shallow parse tree [Zelenko et al. 2003] for the simple extraction tasks, the dependency tree [Culotta and Sorensen 2004], and the shortest dependency path tree [Bunescu and Mooney 2005] suffer from low recall in the ACE task due to strict similarity constraints. As for convolution tree kernels, Zhang et al. [2006] explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT, the small-est common sub-tree including the two entities) achieves the best performance. Zhou et al. [2007] further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. However, the critical problem of how to properly represent the structural syntactic parse tree is only partially resolved. As we indicate in what follows, current tree spans suffer from two disadvantages. (1) Both SPT and CS-SPT still contain unnecessary information. For example, in (2) CS-SPT only captures part of the context-sensitive information related to
Since dependency plays a key role in many NLP problems, such as syntactic parsing, semantic role labeling as well as semantic relation extraction, our motivation is to employ various kinds of dependency knowledge to distinguish the necessary evidence from the unnecessary information in the structural syntactic parse tree. Put in another way, this knowledge can be exploited to construct a proper structural representation for relation instances.

On one hand, lexical or word-word dependency indicates the relationship among words occurring in the same sentence. For exa mple, predicate-argument dependency means that arguments are dependent on their target predicates while modifier-head dependency means that modifiers are dependent on their head words. Such depen-dency relationship offers a condensed representation of the information needed to as-sess the relationship in the form of the dependency tree [Culotta and Sorensen 2004] or the shortest dependency path tree [Bunescu and Mooney 2005] that includes two given entities.

On the other hand, when the syntactic parse tree corresponding to the sentence is derived from the bottom to the top using derivation rules step by step, the word-word dependencies converge upward, making a unique head child containing the head word for every non-terminal constituent. Figure 1 illustrates two example parse trees corre-sponding to an English sentence  X  X e owns rental property in the state X  and an Chinese sentence  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X  (A teenager hijacked a bus), where the arrows at the bottoms denote the dependencies between words (the specific type of word dependency is omitted for simplicity since we do not utilize this kind of information), tree nodes except the terminal nodes and POS nodes are additionally tagged with the associated head words. Take the English sentence as an example,  X  X P-property X  means that the node is tagged with  X  X P X  with its head word being  X  X roperty X , and when  X  X J NN X  is reduced to  X  X P X , the head word of  X  X N X  ( X  X roperty X ) is assigned to their parent node  X  X P X  since the  X  X P X  node is dependent on the  X  X N X  node (this dependency relationship is highlighted using a bold line with an arrow pointing to its head child). Generally, each context-free grammar (CFG) rule has the form of
Where, P is the parent node, H is the head child of the rule, L n ...L 1 and R 1 ...R m are left and right modifiers of H respectively, and both n and m may be zero. In other words, the parent node P (dependent) depends on the head child H (governor), this is what we call constituent dependency . Vice versa, we can also determine the head child of a constituent in terms of constituent dependency. Our hypothesis stipulates that the contribution of the parse tree to establishing a relationship is almost exclusively concentrated in the path connecting the two entities, as well as the head children of constituent nodes along this path. More clearly speaking, only the path nodes as well as the governing nodes they are dependent on are considered necessary, while others are unnecessary for relation instances. Guided by the principle of constituent dependencies, beginning with the Minimum Complete Tree (MCT, the complete sub-tree rooted by the nearest common ancestor of the two entities under consideration) as the structural representation of each relation instance, the head child of every node is found according to the production rule along the path connecting the two entities. Then the path nodes and their head children are kept intact or recovered outside the MCT while any other nodes are gradually removed from the parse tree. Eventually we arrive at a condense tree representation called the Dynamic Syntactic Parse Tree ( DSPT), which is dynamically determined by constituent dependencies and only contains necessary information as expected.
Due to the multitude of production rules with regard to parse trees, there exist a considerable number of constituent dependencies in a context-free grammar as de-scribed by Collins [2003]. However, since our task is to extract the relationship be-tween two named entities, our focus is on how to condense useful constituents re-lated to relation extraction, most of whic h are Noun Phrases (NPs). Therefore con-stituent dependencies can be classified into the following five categories according to constituent types of the CFG rules. Figure 2 and Figure 3 illustrate the application of these constituent dependencies to relation instances in both English and Chinese languages from the ACE RDC 2004/2005 corpora respectively. (1) Modification within base-NPs . Base-NPs mean that they do not directly dominate (2) Modification to NPs . Except base-NPs, other modification to NPs can be classified (3) Arguments/adjuncts to verbs . This type includes the CFG rules in which the left (4) Coordination conjunctions . In coordination constructions, several peer conjuncts (5) Modification to other constituents . Except for the above four major types, other
In fact, the SPT [Zhang et al. 2006] can be constructed by carrying out part of the above removal operations using a single heurist ic rule (i.e., all the constituents outside the shortest path linking the two entities should be removed) and CS-CSPT [Zhou et al. 2007] further recovers part of necessary context-sensitive information outside the SPT. This intuitively justifies why the SPT performs well, and why the CS-SPT outperforms the SPT. This section first explores different represen tations of entity-related features. Then, two schemes that combine structural syntactic information and entity-related infor-mation are discussed. Finally, we present both convolution tree kernels and composite kernels used in our experiments. Entity-related semantic features, such as entity headword, entity type and subtype, etc., impose a strong constraint on relation types according to the definitions by the ACE RDC task. For example,  X  X ER-SOC X  relations describe the relationship between entities of type PER, and no other entity type is allowed as an argument of these re-lations. Therefore, virtually all the relation extraction systems contain entity features in one form or the other. Particularly, experiments by Zhang et al. [2006] show that the linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. Qian et al. [2007] further points out that among these entity features, entity type, subtype, and mention type, as well as the base form of predicate verb nearest to the second entity mention, contribute most while the contribution of other features, such as entity class, headword, and GPE role, can be ignored.

In order to effectively capture various kinds of entity-related semantic features and their combinations as well, particularly bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 4. Take an English sentence  X  they  X  X e here  X  (excerpted from the ACE RDC 2004 corpus) as an example, there exists a relationship  X  X hysical.Located X  between the entities  X  they  X  [PER] and  X  here  X  [GPE.Population-Center]. The features are encoded in the tags as  X  X P X ,  X  X T X ,  X  X T X  and  X  X VB X , which denote type, subtype, mention-type of the two en-tities, and the base form of predicate verb in English if existing (nearest to the second entity along the path connecting the two entities) respectively. For example, the tag  X  X P1 X  represents the entity type of the first entity, and the tag  X  X T2 X  represents the entity subtype of the second entity. Following are the three entity-related semantic tree setups. (a) Bag of Features (BOF, e.g., Figure 4(a)). All feature nodes are uniformly attached (b) Feature-Paired Tree (FPT, e.g., Figure 4( b)). The features of two entities are first (c) Entity-Paired Tree (EPT, e.g., Figure 4(c)). All the features relating to an entity
For the preceding three setups, the predicate feature is always attached under the root node, since it is unrelated to any of the two entities. In summary, the BOF only captures the individual entity features, while the FPT and EPT setups are designed to additionally capture the bi-gram and tri-gram features respectively. After determining the structural representations for both the syntactic parse tree and entity-related semantic tree, the question is how to combine them into an effective form for relation extraction. One common way is via a composite kernel, as explored by Zhang et al. [2006]. While we will discuss the composite kernel in the next section, here we propose a unified syntactic and semantic tree (USST), which integrates the entity-related semantic tree into the dynamic syntactic parse tree. Although the entity features can be attached under the top node, the entity nodes, or directly embedded into the labels of the entity nodes, we only explore to attach the three kinds of entity-related semantic trees (i.e., BOF, FPT, and EPT) under the top node of the dynamic syntactic parse tree right after its original children since detailed evaluation [Qian et al. 2007] indicates that the unified tree achieves the best performance when the feature nodes are attached under the top node. Figure 5 shows an instance of the USST with the entity-related semantic tree  X  X OF X  attached. This instance is directly chosen from the ACE RDC 2004 corpus. Here, we omit its Chinese counterpart because they look almost similar except that the words and the node tags X  names are associated with Chinese. After having rendered relation instances as structured representations X  X SPT and USST, we now turn to discuss how to measure the similarity between two relation instances with a convolution kernel. A convolution kernel [Haussler 1999] aims to capture common structural information between two discrete objects in terms of their sub-structures, such as the convolution tree kernel [Collins and Duffy 2001], the string kernel [Lodhi et al. 2002], and the graph kernel [Suzuki et al. 2003]. Following previ-ous studies in relation extraction [Zhang et al. 2006; Zhou et al. 2007], this article also adopts the convolution tree kernel [Collins and Duffy 2001] to compute the similar-ity between two relation instances due to its effectiveness in capturing the structural information inherent in the parse tree.

For the convolution tree kernel to be incorporated into statistical classifiers such as SVMs, a training or test instance should be ultimately casted as a feature vector, in which features are all of its subtree types and the value of each feature is the number of occurrences of that subtree type in the whole tree. Formally, a tree T can be represented as a vector of integer counts of each subtree type, that is: Where # subtree-type i ( T ) denotes the number of occurrences of the i th subtree type in the tree T . Intuitively, the convolution tree kernel counts the number of common sub-tree types as the structure similarity between two trees T 1 and T 2 . Since the number of subtree types in a tree is exponential with its size, it is infeasible to enumerate all the subtree types and directly use the feature vector  X  ( T ). In order to address this computational problem, Collins and Duffy [2002] propose the following method to implicitly compute the dot product between two high-dimensional vectors  X  ( T 1 )and  X  ( T 2 ): where N 1 and N 2 are the sets of nodes in tree T 1 and T 2 , respectively, and I subtree a binary function whose value is 1 if the subtree-type i occurs with its root at node n or zero otherwise, and ( n 1 , n 2 ) evaluates the number of common subtree types rooted at n 1 and n 2 , 4 that is: ( n 1 , n 2 ) can be computed recursively as follows. 1) If the context-free productions (CFG rules) at n 1 and n 2 are different, then 2) If both n 1 and n 2 are POS tags, ( n 1 , n 2 )=1  X   X  ; otherwise go to Step 3. 3) Calculate ( n 1 , n 2 )recursivelyas: where # ch ( n ) is the number of children of node n , ch ( n , k )isthe k th child of node n and  X  (0 &lt; X &lt; 1) is the decay factor in order to make the kernel less variable with respect to different sub-tree sizes.

This kernel has been successfully applied to many tasks in natural language processing, such as syntactic parsing [Collins and Duffy 2002], semantic role label-ing [Moschitti 2004, 2006; Moschitti et al. 2006, 2008; Zhang et al. 2008b] and relation extraction [Zhang et al. 2006, 2008a; Zhou et al. 2007] as well.

In order to integrate the power of different kernels, composite kernels are usually an effective way to combine several kernel s over different structures. With kernels over the dynamic syntactic parse tree and the entity-related semantic tree (or even a feature-based linear kernel) at hand, we can further consider two kinds of composite kernels as follows: 1) Linear combination 2) Polynomial combination where K P 1 denotes the polynomial expansion of K 1 with the degree d =2,thatis, K P 1 = ( K 1 +1) 2 , the others are the same as those in Equation (5). Since the set of kernels is closed under normalization, linear and polynomial combination [Sch  X  olkopf and Smola 2001], these two composite kernels are also proper kernels. This section will evaluate the effectiveness of the dynamic syntactic parse tree and the contribution of various kinds of entity-related semantic information via the unified syntactic and semantic tree framework. For evaluation, we use the ACE RDC 2004 English corpus and the ACE RDC 2005 Chinese corpus as the benchmark data (hereafter we will refer to them as the Eng-lish corpus and the Chinese corpus respectively). The English corpus was gathered from various sources of newspapers, newswire, and broadcasts. This data set contains 451 documents and 5,702 relation instances. It defines seven entity types, seven ma-jor relation types and 23 relation subtypes. For fair comparison with previous work [Zhang et al. 2006; Zhou et al. 2007], evaluation is done on the same subset of 348 doc-uments (nwire/bnews) and 4,400 relation instances using 5-fold cross-validation. The Chinese corpus contains 633 documents, which were collected from newswire, broad-casts, and weblogs. It defines seven entity types, six major relations types, and 18 relations subtypes. Due to many grammati cally ill-formed sentences in the weblogs and for comparability with the English corpus, evaluation is only done on 533 doc-uments (nwire/bnews) and 7,630 relation instances of the Chinese corpus using the same 5-fold cross-validation scheme.

The English corpus is first parsed using the state-of-the-art Charniak X  X  parser [Charniak 2001] with the boundaries of all the entity mentions kept. Then, relation instances are generated by iterating over al l pairs of entity mentions occurring in the same sentence with the given  X  X rue X  mentions and co-referential information. For fair comparison, various kinds of structured rep resentations of relation instances are gen-erated respectively, such as DSPT described in Section 3, EST in Subsection 3.1, USST in Subsection 4.2 as well as the feature vector used in composite kernels in Table VI and Table VII. This same preprocessing strategy is also applied to the Chinese corpus, except that the Chinese sentences are first segmented by the Stanford Chinese Word Segmenter and then parsed by the Stanford Parser. 5 In our experimentations, SVM light [Joachims 1998] with the tree kernel function SVM-TK toolkits [Moschitti 2004] 6 is selected as our classifier. For efficiency consider-ation, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. For the purposes of meaningful comparison, the training pa-rameters C (SVM) and  X  (tree kernel) are also set to 2.4 and 0.4 respectively, which had been empirically verified to exhibit the best performance in previous relation extrac-tion research. As to the coefficients  X  in the composite kernels, 2-fold cross-validations on the training dataset are used to determine their optimal values. That is, the train-ing dataset is divided into two disjoint and independent folds, one of which is used to induce the classifier models, while the other is tested to find the optimal parame-ter values to attain the best performance score. For reasonable comparison between English and Chinese corpora, the same set of parameter values is applied to both the classifiers. Our experiments are presented in the following order: (1) evaluating the contributions of various kinds of constituent dependencies in DSPT, (2) evaluating the contributions of various kinds of entity-related semantic features in (3) comparing the performance between different entity-related semantic tree setups (4) evaluating the effectiveness of composite kernels by DSPT and EST, (5) comparing with other state-of-the-art relation extraction systems.

To determine whether an improvement is significant or not, we conduct significance of less than 0.01, 0.01 X 0.05, and greater than 0.05, which mean significantly better, moderately better, and slightly better, respectively.

Evaluating the contributions of various kinds of constituent dependencies . Table I evaluates the contributions of different kinds of constituent dependencies on the major relation types of the ACE corpora using the convolution parse tree kernel (with only the entity-type information attached as a child under the tree root node in the BOF manner as illustrated in Figure 4(a)). The MCT with only the entity-type information is first used as the baseline, and various constituent dependencies are then applied sequentially to dynamically reshaping the tree in two different modes.  X  X M1] Individual Mode. Every constituent dependency is individually applied on the
MCT, hence this mode is aimed to assess the separate contribution of each con-stituent dependency. In this mode, significance tests are conducted between each specific dependency type and the MCT baseline.  X  X M2] Accumulative Mode. Every constituen t dependency is incrementally applied on the previously derived tree span, which begins with the MCT and eventually gives rise to a Dynamic Syntactic Parse Tree (DS PT). In this mode, significance tests are conducted between each dependency type and its previously-considered ones.
The table reports that the final DSPT achieves the best performance of 77.4%/65.4%/70.9 for English and 79.1%/56.9%/66.2 for Chinese in precision/recall/ F -measure respectively after applying all the dependencies, with the increase of the F -measure by 8.2 units (English) and 8.4 units (Chinese) over the baseline MCTs. This indicates that reshaping the tree by exploiting constituent dependencies can signifi-cantly improve the extraction accuracy la rgely due to the increase in the recalls. It further suggests that the knowledge on constituent dependencies is very effective and can be fully utilized in tree kernel-based relation extraction. Furthermore, this table also shows the following. (1) Modification within base-NPs significantly (***) improves the accuracy for Eng-(2) Modification to NPs contributes much to the performance improvement (***), (3) Modification of arguments/adjuncts to verbs improves the F -measure by 2.8/2.6 (4) Reduction of coordination conjunctions, in particular noun phrase conjunctions (5) Other modifications show trivial effects on the relation extraction performance,
In summary, the DSPT produced by applying constituent dependencies to a parse tree involving two given entities can dramatically boost the performance of relation extraction for both English and Chinese. Particularly, the modification within base-NPs in English and the modification to NPs in both languages contribute most to the improvement, while other modifications help relatively less. Since most of entity men-tions are encapsulated in noun phrases, this result indicates the locality of semantic relationship occurring in the ACE corpora.

Evaluating the contributions of various ki nds of entity-related semantic features in EST . Ta-ble II further evaluates the contributions of various kinds of entity-related semantic features on the extraction of the major relation types on the ACE RDC corpora using the BOF setup by adding them incrementally in the decreasing order of their potential importance. Note that the plus sign after a specific feature means that this feature is useful and should be included in the next round. This table reports that our system achieves the best performance of 80.2%/69.2%/74.7 in terms of P/R/F (English) and 79.7%/60.9%/69.1 (Chinese) respectively. It X  X  interesting to notice that both English and Chinese exhibit consistent behavior on these features. Specifically, it also shows the following. (1) Both entity subtype and mention level information moderately improve the F -(2) It is a bit surprising, however, to observe that the other four kinds of information, (3) Finally for English, the predicate verb (in its basic form) closest to the second entity
Comparing the performance between d ifferent entity semantic tree setups in USST . We com-pare in Table III the performance of the Unified Syntactic and Semantic Trees with different kinds of Entity Semantic Tree setups described in Section 4, while the SPT and DSPT with only entity-type information are listed for reference. Significant tests are conducted between USSTs and SPT. Note that for the English corpus since only four features, such as entity type, entity subtype, mention level and predicate base, show positive effects from the previous experiments, these features are incorporated into the USSTs as illustrated in Figure 4. The same scheme is also applied to the Chinese corpus, except that the predicate base is unavailable. The table shows the following. (1) All the three setups of USST significantly (***) outperform the DSPT setup, ob-(2) The Unified Syntactic and Semantic Tree with FPT (the Feature-Paired Tree struc-
In Table IV we further summarize the improvements of different tree setups over the SPT for both English and Chinese corpora, namely, CS-SPT, DSPT and USST (FPT). Since these results are produced in a similar setting, that is, extraction of ma-jor relation types on the same corpus with the same convolution parse tree kernel using the same 5-fold validation scheme, they can be compared fairly. It shows that for English our DSPT significantly (***) outperforms SPT by 3.8 units in F -measure, while the CS-SPT outperforms the SPT by 1.3 units in F -measure, and for Chinese our DSPT significantly (***) outperforms SPT by 2.2 in F -measure. This suggests that the DSPT performs best among these tree spans. It also shows that the Unified Syntac-tic and Semantic Tree with FPT (Feature-Paired Tree) performs significantly better than the other two tree setups (i.e., CS-SPT and DSPT) by 6.7/4.2 units in F -measure respectively for English, though the increase is not so distinctive for Chinese. This im-plies that the entity-related semantic information is very useful and contributes much when they are incorporated into the parse tree for relation extraction.

Evaluating the effectiveness of composite kernels by DSPT and EST . These experiments are aimed to evaluate the effectiveness of composite kernels by using either linear combination or polynomial combination, and to compare the difference between the composite kernels and the unified syntactic and semantic tree kernels. Table V re-ports the results in terms of P/R/F corresponding to the composite kernels by the DSPT and three different EST setups, using linear combination (outside parenthe-ses) and polynomial combination (inside parentheses) respectively. These results are obtained on the extraction of the major relation types in the ACE RDC corpora. For reference purposes, the results for the DSPT with only entity-type information are also listed. Significant tests are conducted between composite kernels and the single DPST kernel.

When determining the coefficient for linear interpolation in Equation (5), K 1 rep-resents the EST tree kernel while K 2 represents the DSPT tree kernel. Using 2-fold cross-validation on the training data, the coefficient  X  is fine-tuned to 0.4. For poly-nomial combination in Equation (6), K 1 and K 2 denote the same kernels as for linear combination. Same as the most common setting in the literature [Zhang et al. 2006; Zhou et al. 2007], the polynomial order d is set to 2, while the coefficient  X  is fine-tuned to 0.2 in the same way as for the linear combination. From Table V we can see the following. (1) All the three setups of composite kernels, regardless of their combination methods, (2) Among these composite kernels, for English the linear one with FPT achieves the (3) In contrast to the findings by Zhang et al. [2006] and Zhou et al. [2007] that the
Comparing with other state-of-the -art relation extraction systems . Finally, Table VI compares our system with other state-of-the-art kernel-based systems on the major relation types of the ACE RDC 2004 corpus. It shows that on the ACE RDC 2004 corpus our USST (FPT) outperforms all previous tree setups using one single kernel, even better than two previous composite kernels [Zhang et al. 2006; Zhao and Grishman 2005]. Furthermore, when the USST (FPT) kernel is combined with a linear state-of-the-state feature-based kernel [Zhou et al. 2005] into a composite one polynomial combination in a setting similar to Zhou et al. [2007] (i.e., polyno-mial degree d = 2 and coefficient  X  = 0.3), we get the so far best performance of 77.1 in F -measure for seven major relation types on the ACE RDC 2004 English corpus.

For Chinese, we implemented a feature-based relation extraction system with the similar features as Zhou et al. [2005], including lexical words, entity features, overlapping features and chunking information etc., and achieve the performance of 77.98%/52.34%/62.64 in terms of P/R/F on the ACE RDC 2005 Chinese corpus. Then this linear kernel is further combined with the USST (FPT) kernel for Chinese relation extraction in the same way and using the same parameters as the composite kernel for English. The experiment results are compared in Table VII with other state-of-the-art Chinese relation extraction systems on the ACE RDC 2005 Chinese corpus. However, this comparison is by no means fair due to their distinct evaluation schemes and there-fore only for reference. For example, while we adopt the 5-fold cross validation strategy on the ACE RDC 2005 Chinese corpus, both Li et al. [2008] and Zhang et al. [2009]) use 75% of the total relation instances as the training set and the remaining instances as the test data, and furthermore the total number of relation instances are also dif-ferent due to different preprocessing procedures. Specifically, Li et al. [2008] explore character-based unigram and bi-gram features as well as nine positional structures, plus some correction and inference mechani sms based on the relation hierarchy and co-reference information, while we use convolution tree kernels over syntactic parse trees and entity-related semantic trees. Actually, syntactic parse trees include some kind of positional structures. In Zhang et al. [2009], they combine an entity semantic kernel and a string semantic similarity kern el for Chinese relation extraction, while ignoring the structural information inherent in parse trees. Although comparison of our work with these two works is difficult, nevertheless, one important thing to be sure is that, the USST employing constituent dependencies significantly outperforms that in our previous work [Yu et al. 2010] under the same setting, thus indicating the effectiveness of the convolution tree kernels over syntactic parse trees for extracting Chinese semantic relations as well as for extracting English semantic relations. This article systematically explores the potential of structured syntactic information for tree kernel-based relation extraction on both English and Chinese. In particular, a new approach is proposed to dynamically determine the tree span (DSPT) for relation instances by exploiting constituent dependencies. In addition, we investigate different ways of integrating various kinds of entity-related semantic information via a Unified Syntactic and Semantic Tree (USST). Evaluation on the ACE RDC corpora shows that our DSPT is appropriate for structural syntactic representation of relation instances for both English and Chinese languages. It also shows that, in addition to individual entity features, combined entity features (especially bi-gram) contribute much when they are combined with the DSPT into the USST framework. Finally, this article re-ports the so-far best performance for both English and Chinese corpora when combin-ing the USST-based tree kernel and a state-of-the-art feature-based linear kernel via a composite kernel.

For the future work, on one hand, we will focus on improving the performance of complex structural parse trees, where the path connecting the two entities involved in a relationship is too long for current kernel methods to take effect. Our preliminary experiments of employing a discourse theory exhibit certain positive results. On the other hand, while most recent research on relation extraction focuses on semantic re-lations occurring within one sentence, extracting inter-sentential relations that occur across multiple sentences pose a more challenging problem. It is estimated that 28.5% of MUC6 relations and 9.4% of ACE 2003 relations are inter-sentential [Swampillai and Stevenson 2010], though the ACE project since 2004 limits to annotating relations which are expressed within a single sentence. While some of these inter-sentential relations may be automatically extracted using syntax and co-reference information, others are more difficult and can only be resolved using real-world background knowl-edge. Therefore, further research is necessary to address the issue of inter-sentential relation extraction.

