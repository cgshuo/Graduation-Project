 We apply language modeling keyword search augmented with Berger and Lafferty X  X  (1999) translation model for query ex-pansion to formulate three query expansion methods using word co-occurrence statistics from a large external corpus and user clickthrough data. We study the performance of these methods on a vertical domain (case law documents) us-ing standard metrics and an evaluation framework designed specifically to measure the performance of query expansion under varying degrees of query-document term mismatch. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance query expansion, translation model, query logs, clickthrough data
The effectiveness of information retrieval systems is lim-ited by the degree of term overlap between queries and rele-vant documents. In the legal domain, attorneys and judges are continuously analyzing, interpreting, and arguing the ap-plication of existing legal principles to new situations, which feeds into the vocabulary mismatch problem.
Our alogrithm, which we will refer to in this paper as TR , combines the keyword search capabilities of the language modeling approach presented by [6, 5, 4] with the inherent query expansion capabilities of [1] X  X  translation approach. Query expansion is built in to our basic retrieval algorithm and occurs at run-time; the selection of query expansion terms is not a separate step in the retrieval process. We rank documents according to equation 1, where q and d indicate individual terms and Q and D denote the entire query and document, respectively:
P ( Q | D ) = Y
In the case that the query term is absent from the doc-ument being scored, the translation portion of equation 1 ( P P ( q | d ) P ( d | D )) is nonetheless able to provide additive evidence to connect the missing query term with terms con-tained in the document being scored. In the case that a query term does appear in a document, the translation com-ponent provides additional evidence for the association be-tween the query term and that document. We bridge the vo-cabulary gap between a query and its relevant documents in the collection by taking our translation probabilities ( P ( q | d )) from domain-specific language resources external to the doc-ument collection.

In our basic formulation ( TR , shown in equation 1), our external corpus consists of 23 million short legal  X  X eadnote X  documents. Headnotes are short summaries of case law documents written by legal experts. Word co-occurrence pairs are unordered. They are constructed by removing stop words, and then pairing each word with all the other words (including itself) within a window of 5 words on either side of it. Using frequency counts for the word pairs in the cor-pus, we are able to compute co-occurrence probabilities. We use these co-occurrence probabilities as a proxy for semantic relatedness, to quantify the degree to which q and d from equation 1 can be  X  X ranslated X  to each other. In our local analysis formulation (which we refer to as TR Local ), the query is first executed and the documents are ranked using keyword-only search. The top 20 documents are assumed relevant, and the top 50 terms are selected from these top 20 documents as candidates for expansion. We then execute the TR model (equation 1), limiting the trans-lation component to those 50 terms.
In this formulation ( TR Qlog ), we use 39 million click-through events from Westlaw.com to estimate the P ( q | d ) values used in equation 1. To compute these conditional co-occurrence probabilities, we establish relationships between query terms and the terms in corresponding clicked docu-ments, following the approach presented in [2] (to which we apply a Bayes X  Rule transformation).
Our test collection is comprised of 20,000 case law doc-uments and 335 queries. Relevance judgments were estab-lished manually by attorneys. We compute Mean Average Precision (MAP) and Recall on the original test collection, and as we increasingly remove query terms from relevant documents, following the evaluation framework presented in [3]. The removal of query terms in this manner allows us to measure the differences in IR system performance with respect to the differing degrees of vocabulary mismatch be-tween queries and relevant documents.
 We compare the performance of 4 retrieval algorithms: 1. Language Modeling ( LM baseline ): This is a language modeling keyword search that functions as our baseline. 2. Language Translation Model for QE ( TR ): This is our basic formulation, as shown in equation 1. 3. Language Translation Model with Local Feedback ( TR Local ): This is the model described in section 2.1. 4. Language Translation Model with Query Logs ( TR Qlog ): This is the model described in section 2.2.

The TR system maintains recall better than the other systems under increasing query-document term mismatch conditions. Recall is of the utmost importance to legal re-searchers when searching for documents. Missing even one relevant document could have significant negative conse-quences to the strength of their case. We view the ability of the TR system to maintain recall as significant, especially within the legal domain.
 Without any term removal, all three TR variants ( TR , TR Local , and TR Qlog ) perform better on MAP than the baseline. As query terms are removed, the TR retrieval variants continue to demonstrate significant robustness to the query-document term mismatch problem. There are 173 queries (of 335 total queries) in this collection that contain 4 or fewer words. With four terms removed, none of these queries has any term overlap with their corresponding rele-vant documents, and the remaining 162 queries are missing their four highest idf terms. Even under these extreme con-ditions, the TR formulation achieved a recall of 79% (Fig-ure 2). Although the resulting ranking is relatively poor when four terms are removed, the precision of the proposed method performs significantly better than the baseline under more realistic conditions of term mismatch.
We incorporate both query expansion and pseudo rele-vance information into a language modeling framework for IR. The formulation extends prior work on machine transla-tion for IR and incorporates three query expansion methods (global, local, and query logs) in a principled way. The ex-perimental results demonstrate that the proposed approaches are significantly more robust to the vocabulary mismatch problem than keyword based systems. [1] Berger, A. and J.D. Lafferty. 1999. Information [2] Cui, Hang, Ji-Rong Wen, Jian-Yun Nie, Wei-Ying Ma. [3] Custis, Tonya and Khalid Al-Kofahi. 2007.  X  X  New [4] Hiemstra, Djoerd. 2001. Using Language Models for [5] Miller, D., T. Leek, and R. Schwartz. 1999.  X  X  Hidden [6] Song, F. and W.B. Croft. 1999. A general language
