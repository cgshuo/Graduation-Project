 As the relevant data sets get large, existing in-memory schemes for tensor decomposition become increasingly in-effective and, instead, memory-independent solutions, such as in-database analytics, are necessitated. In this paper, we present techniques for efficient implementations of in-database tensor decompositions on chunk-based array data stores. The proposed static and incremental in-database ten-sor decomposition operators and their optimizations address the constraints imposed by the main memory limitations when handling large and high-order tensor data. Firstly, we discuss how to implement alternating least squares op-erations efficiently on a chunk-based data storage system. Secondly, we consider scenarios with frequent data updates and show that compressed matrix multiplication techniques can be effective in reducing the incremental tensor decom-position maintenance costs. To the best of our knowledge, this paper presents the first attempt to develop efficient and optimized in-database tensor decomposition operations. We evaluate the proposed algorithms on tensor data sets that do not fit into the available memory and results show that the proposed techniques significantly improve the scalability of this core data analysis.
 H.2.8 [ Database Management ]: Database Applications X  Data mining Tensor Decomposition; In-Database Tensor Decomposition
Thanks to the availability of various mathematical tools (such as tensor decompositions) that support multi-aspect analysis of multi-dimensional data, tensors are increasingly This work is partially funded by NSF grants 116394,  X  X an-Kloud: Data Partitioning and Resource Allocation Strate-gies for Scalable Multimedia and Social Media Analysis X  and 1016921,  X  X ne Size Does Not Fit All: Empowering the User with User-Driven Integration X  being used in many application domains, including scien-tific data management [11, 15, 27, 34], sensor data manage-ment [31, 33], and social network analysis [16, 22, 26].
A key difficulty in tensor decomposition is that the oper-ation results in dense (and hence large) intermediary data, even when the input tensor is sparse (and hence small). This is known as the intermediate memory blow-up problem [16] and renders purely in-memory implementations of tensor-decomposition difficult. While today MATLAB-based in-memory linear algebra operations are widely used for imple-menting tensor decomposition algorithms, these implemen-tations are limited with the amount of memory available to the MATLAB software. Moreover, exporting data from a large database to import into MATLAB is often costly and elimination of this overhead can provide performance gains of several orders of magnitude [13].
Because of the above limitations of in-memory solutions, in this paper, we consider in-database implementations of tensor decomposition operations on disk-resident data sets . In particular, we argue that the ability to implement tensor decomposition operations on disk-resident tensor data can eliminate the challenge posed by the memory-limitations. However, we also recognize that in-database tensor analytics brings its own challenges In this paper, we attempt to address these challenges. In particular, we consider an array model 1 representation of the tensor data, leverage a chunk-based framework to store and retrieve data, extend array operations to tensor operations, and introduce optimization schemes for efficient in-database tensor decompositions. 2 We extend SciDB [1], an open source array-based DBMS. While not discussed in this paper due to space limits, we have also implemented in-database tensor decompositions using SQL (join/aggregate operation) and UDF/UDA ap-proaches over a relational DBMS. These approaches did not scale due to the large intermediate results in tensor decom-position.
In [20], we demonstrate in-database tensor decompositions using TensorDB.
When the data tensors are updated frequently, incremen-tal tensor techniques, such as DTA [31], which dynamically maintain and revise the tensor decomposition are commonly used to avoid the cost of decomposing the data tensor from scratch with each update [31]. However, despite the cost savings they provide, these incremental tensor decomposi-tion techniques still suffer from high memory overheads. In this paper, we show that the cost of this operation can be significantly reduced by leveraging recently introduced com-pressed matrix multiplication techniques, such as [25], in-stead of using traditional matrix multiplication implemen-tations. In particular, we show that the proposed chunk-based operations, complemented with compressive matrix multiplication, can be highly effective in reducing the incre-mental tensor decomposition maintenance costs.
To the best of our knowledge, this paper is the first to study in-database tensor decomposition on a chunk-store. The proposed static and incremental in-database tensor de-composition techniques and the optimizations address the memory limitations when handling large and high-order ten-sor data. The paper is organized as follows: Finally, we conclude the paper in Section 6.
Matrix (i.e., 2-mode tensor) data are often analyzed for its latent semantics and indexed for search using a matrix decomposition operation known as the singular value decom-position (SVD). This operation identifies a transformation which takes data, described in terms of an m dimensional vector space, and maps them into a vector space defined by k  X  m orthogonal basis vectors (also known as latent seman-tics) each with a score denoting its contributions in the given data set. The more general analysis operation which applies to tensors with more than two modes is known as the ten-sor decomposition . CANDECOMP [11] and PARAFAC [15] decompositions (together known as the CP decomposition) decompose a tensor into a sum of rank-1 tensors. Tucker decomposition [34] decomposes a given tensor into a core tensor multiplied by a matrix along each mode.

Many of the algorithms for decomposing tensors are based on an iterative process, such as alternating least squares (ALS), that approximates the best solution until a conver-gence condition is reached [11, 15]. Tensor decomposition is a costly process. In dense tensor representation, the cost increases exponentially with the number of modes of the tensor. While decomposition cost increases more slowly (lin-early with the number of nonzero entries in the tensor) for sparse tensors, the operation can still be very expensive for large data sets. [22] proposed a memory-efficient Tucker (MET) decom-position to address the intermediate blowup problem in Tucker decomposition. [33] proposed a randomized Tucker decomposition algorithm. [27] proposed a modified ALS PARAFAC algorithm called grid PARAFAC for large scale tensor data. Parallelizations in tensor decompositions have been proposed on different platforms, such as CUDA [6] and MapReduce [16]. In [26], sparse parallelizable tensor decom-position algorithm has been proposed. In [17, 18, 19], par-allelized tensor decompositions with relational operations, such as join and union have been proposed. Recently, [28] proposed a fast approach for CP that decomposes an un-folded tensor in lower order, instead of directly factorizing the high order tensor. [23] proposed improving scalability through personalized tensor decomposition operations.
There are also several out-of-core tensor decomposition algorithms, which generally partition the data into blocks, perform decompositions on the resulting blocks, and com-bine the results into a final decomposition [27, 35]. Note that these block-based decompositions also involve partitioning thus will benefit from optimized/compressed chunk-based I/O including the ability to perform chunk -based operations.
The array model [8, 1] is a natural representation to store multidimensional data and facilitate multidimensional data analysis. How arrays are organized and stored depends largely on whether they are dense or sparse. Approaches to represent array based data can be broadly categorized into four types. (a) The first approach is to represent the array in the form of a table. (b) A second approach is to use blob type in a relational database as a storage layer for array data [8]. (c) Sparse matrices can also be represented using a graph-based abstraction [24]. (d) The last approach is to consider a native array model and an array-based storage scheme, such as a chunk-store, as in [1]. However, none of these approaches support tensor decomposition algorithms.
In this paper, we leverage the SciDB [1] native array database and extend it with tensor manipulation and analy-sis. SciDB uses multidimensional arrays as its basic storage and processing unit. Arrays are partitioned into chunks and each chunk is processed in a parallel manner, whenever pos-sible. SciDB also provides various chunk-based array manip-ulation operations, including linear algebra operators. Be-low, we briefly review the relevant SciDB operators:  X 
CREATE ARRAY name &lt;attributes&gt; [dimensions] cre-ates the template for an array with the specified name and schema ( attributes and dimensions ).  X  build(template_array | schema_def.,expression) pro-duces an array with the shape of the given template, with values equal to the given expression.  X  multiply(left_array,right_array) performs matrix multiplication of two input arrays, left_array and right_array , and returns a result array.  X  transpose(array) transposes the given array .  X  store(operator(args),array) updates array with the result of the operation specified in operator(args) . store creates a new version of the destination array (with all pre-(a) A = X (1) ( C B )( C T C  X  B T B )  X  (b) fit computation vious versions also maintained) and utilizes run-length en-coding to compress the array data.  X  reshape(src_array,template_array|schema_def.) reshapes src_array with template_array or schema_definition , which has the same number of cells as the source_array , but a different shape. For example, a 3x4 array is re-shaped into a 6x2 array.  X  redimension(src_array,template_array|schema_def.) re-arranges dimensions of src_array with template_array or schema_definition .
 For more details of the SciDB operators, see [1].

SciDB provides both pipelined and non-pipelined execu-tions. A pipeline execution is the process by which data is passed among the various operators involved in the decom-position process using the main memory buffers; i.e., if the intermediary data fit into main memory, no disk-access is involved. In the non-pipelined execution plan, however, the intermediary data is required to be saved, which involves I/O access. Since, in this paper, our focus is overcoming scalability issues imposed by memory, specifically, address-ing cases where the intermediary data do not fit in the main memory, we do not consider pipelined execution plans 3 .
Currently, most array databases provide limited built-in array operations and leave the responsibility of imple-menting complex operations through user-defined functions (UDF) and aggregates (UDA) [13] to the users. One critical limitation of UDF/UDA-based approaches is that the data, such as coefficient vectors, should comfortably reside in the available memory [13] and this is not always the case, in many tensor operations, such as tensor decomposition. We describe how to extend a native array database, SciDB [1], with tensor manipulation operations; specifically we focus on in-database, chunk-based implementation of the operations needed to achieve tensor decomposition. Nat-urally, there are optimization and scalability issues in in-database implementation of tensor manipulation operations, including how we partition the data into chunks and how we move them in and out of the memory.
In this paper, we consider an alternating least squares (ALS) based implementation of CP decomposition 4 . Our future work includes selective pipelining to minimize I/O overheads
While it is possible to also apply this work to Tucker decom-positions, we omit details in this paper due to page limits.
Let us consider a 3-mode tensor X . CP decomposition involves finding three factor matrices, such that where a k , b k , and c k are the k th column vectors of the factor matrices A , B , and C , respectively and the symbol  X   X   X  represents the vector outer product. Figure 2 shows the outline of the proposed in-database CP decomposition pro-cess, which implements an alternating least squares process to solve this optimization problem; at each step all but one of the factor matrices are fixed and the remaining factor matrix is updated using least square estimation:  X  Initialize factor matrices: Firstly, we create a factor ma-trix U ( d ) for each mode d and initialize these with random data. Tensors and matrices are represented as multidimen-sional arrays in in-database CP and created by a SciDB operation, create array . Random data matrices are ini-tialized by build operation. In our in-database CP imple-mentation, all factor matrices are updated in an iterative manner by a new copyArray operator, described in Sec-tion 3.2.4. This operator performs in place array updates, thus as shown in the experimental evaluations section, sig-nificantly reduces I/O costs.  X  (Iteratively) solve for factor matrices: Next, one mode at a time, we iteratively solve for each factor matrix U example, for a 3-mode tensor X and factor matrices A , B , and C , solving for a factor matrix A can be formulated as where X (1) is mode-1 matricization of X and denotes a Khatri-Rao product. The optimal solution for Equation 2 can be formulated as
A = X (1) [( C B ) T ]  X  = X (1) ( C B )( C T C  X  B T B ) Here M  X  is the Moore-Penrose pseudo inverse of M . Fig-ure 1(a) shows the overall execution plan.  X  Evaluate fit (after each factor matrix computation): The columns of the factor matrix are normalized and each norm is stored in the core  X  . Solving each factor matrix continues repeatedly until a measure of fit (defined as where  X  X is the approximate reconstruction of the X tensor from the current decomposition and k Y k is the Frobenius norm of a tensor Y ) converges or a target maximum num-ber of iterations are exhausted. The reconstruction of is through a series of Khatri-Rao products and a reshape operation (Figure 1(b)).

Note that while some of the operations involved in the pro-cess (such as multiply ) are already implemented in SciDB [1] and other array databases, most of the operations needed to implement Equation 3 are not available in common array databases. Furthermore, as we discuss next, even those ex-isting array operations may require new implementations, more suitable for implementing tensor decomposition op-erations. In the next subsections, we discuss chunk-based implementations of the various operations involved in the process and the proposed optimizations.
In this subsection, we introduce the novel chunk-based tensor operators ( matricization, Khatri-Rao product, ( a) Physical layout of data (b) Mode-1 matricization ( c) Mode-2 matricization (d) Mode-3 matricization (a) Mode-1 (b) Mode-2 (c) Mode-3 Hadamard product, normalization , and copyArray opera-tors) needed for implementing in-database decompositions. 3.2.1 matricize(tensor, m)
Matricization transforms a tensor into a matrix along the given mode, m . More specifically, an element ( i to ( i m ,j ) of mode-m matricization, X ( m ) , such that (assum-ing row-major representation of the result) Note that a tensor can be matricized using different column orderings 5 and, as shown in Figure 3, depending on how the data is physically laid out, different matricizations may involve different amounts of data movements. Therefore, our goal is to reduce this data movement.
 Impact of Data Ordering on Chunk-based Matriciza-tions. One straightforward way to implement matricization would be to use SciDB X  X  reshape and redimension opera-tors as illustrated in the following example (Figure 4).
Example 3.1. Consider a 3-mode tensor T of size 4  X  4  X  2 with chunk size 2  X  2  X  2. We can implement mode-1 matri-cization of T , assuming chunk sizes of 8  X  4, using the re-shape operator, as follows (Figure 4(a)): Mode-2 and mode-3 matricizations of T , on the other hand, can be implemented by first re-arranging the dimensions us-ing the redimension operator followed by the reshape oper-ator (see Figures 4(b) and (c)). For example, for mode-2, followed by the reshape operator as above (Figure 4(b)). The problem with these straightforward implementations is that (as we also see in the experimental evaluations section), in the presence of chunk-based storage, these SciDB opera-tors result in significant amounts of data traffic. Figure 4 vi-sualizes matricizations of a 3-mode tensor using the reshape and redimension operations. As shown in the figure, when accessing data one chunk at a time, matricizations using re-shape and redimension require repeated chunk-swapping in and out of memory to construct the output chunks, resulting in significant I/O overheads. Even in cases where multiple chunks can be stored in the buffer, the movement of data across chunks is costly.
 Chunk-Optimized Matricization. We implement chunk-based mode-m matricization X ( m ) of X  X  R
The same order should be used in all related calcula-tions [21]. In our work, the ordering is aligned with the ordering of the results of Khatri-Rao product (Equation 3). where X ij ( m ) is the ( i,j )-th chunk of X ( m ) and J = I
An element of ( i 1 ,i 2 ,...,i m ,...,i N ) in a chunk of ( c ,c 2 ,...,c m ,...,c N ) of X is mapped to an element of ( i m ,j ) in a chunk of ( c m ,d ) of X ( m ) , such that j = As Figure 5 illustrates, the proposed chunk-based matriciza-tion process does not require repeated chunk-swaps to fill in the result chunks. Furthermore, since the data movement is constrained within individual chunks, the global order in which chunks are considered does not impact performance. Materialization of the Results of the Matricization Operation. Tensor matricization is a costly operation, re-quiring at least one full read-and-write of the tensor data. Moreover, in CP decomposition, matricization of all modes of tensor X are needed in each iteration (see Equation 3). Therefore, one way to minimize the overall matricization overhead, is to materialize the matricization results : more specifically, once a matricization is computed, the result can be materialized on disk and this materialized matricization can be is used in all subsequent iterations. While material-ization of the matricization results introduces additional I/O costs and storage requirements, especially in cases where the number of modes and number of iterations are large, mate-rialization can bring significant savings. 3.2.2 Khatri-Rao(left_matrix, right_matrix) Given a left matrix, A  X  R I  X  K , and a right matrix, B  X  R
J  X  K , their Khatri-Rao product is denoted by A B . The result is a matrix of size ( IJ )  X  K , defined as where a n and b n are columns of A and B , respectively and  X  is the Kronecker product. Note that the Kronecker product, U  X  V , of matrices U  X  R x  X  y and V  X  R w  X  z results in matrix of size ( xw )  X  ( yz ), where Khatri-Rao products of factor matrices generate tall and generally dense matrices, which often do not fit into main memory. This is a well-known bottleneck in CP decom-positions. 6 The proposed chunk-based Khatri-Rao product addresses this problem by dividing the resulting matrix into small enough chunks. We define the chunk-based Khatri-Rao product for matrices A (with chunks A 11 ,...,A IJ ) and B (with chunks B 11 ,...,B IJ ), as follows: Once again, since the data movement is constrained within individual chunks, the order in which chunks are considered does not impact performance.
Matricization times Khatri-Rao product together can be formulated in alternative ways for sparse tensors [7, 16]. In this work, we consider the more general formulation also applicable to dense data. 3.2.3 Hadamard(left_matrix, right_matrix)
The Hadamard product is the elementwise matrix prod-uct; more specifically, given matrices A and B , both of size I  X  J , their Hadamard product, denoted as A  X  B , results in the following size I  X  J matrix:
Given this, we define the chunk-based Hadamard product for matrices A with chunks A 11 ,...,A IJ and B with chunks B 11 ,...,B IJ as follows: Again, the data movement is constrained within individual chunks and, thus, the order in which chunks are considered does not impact performance. 3.2.4 copyArray(operator(args), array)
This operator copies the result of operator(args) to a temporary array, array , and is used for updating the in-termediate results (e.g., in the in-database CP decomposi-tion, for updating the factor matrices, which get updated in each iteration). In contrast, SciDB X  X  analogous opera-tion, store , does not update an existing array but creates a new version of the array (also maintaining the previous versions). Also, unlike store , copyArray does not use run-length encoding/decoding, since frequently updated and rel-atively small factor matrices, do not benefit from run-length encoding/decoding.
In addition to the above, our in-database CP implemen-tation also requires chunk-based implementations of other operators, such as normalize(matrix) , which normalizes the columns of the input matrix , and norm(tensor1, ten-sor2) , which computes the Frobenius norm of the difference between given tensor1 and the decomposed tensor tensor2 for fit computation (see Equation 4).
In addition to the above chunked operators, we also imple-ment two non-chunked operators, pseudoinverse and eigen-decomposition . While these require their inputs to fit into the memory, since (during tensor decomposition) inputs are often relatively small, this rarely constitutes a problem.  X  pseudoinverse(matrix) returns the pseudo-inverse of the input matrix . We implement this operator using a C++ linear algebra library from [30], where SVD is used to solve pseudo-inverse problem. Since during CP decomposition, the input to the pseudo-inverse operation is a matrix of size rank  X  rank , where rank is a relatively small number of target components, this matrix easily fits the main memory and does not require a chunk-based implementation.  X  eigen(matrix, r) returns r leading eigen-vectors of the input matrix . Similar to the pseudoinverse operator, eigen-decomposition is an in-memory operation and we use the eigen-decomposition function in [30] for implementation. In this paper, we use this eigen-decomposition operation to implement incremental tensor decomposition. In particular, we take the leading eigen-vectors of the I d  X  I d covariance matrix to generate factor matrices, where I d is the size of the mode d of the tensor. Note that this matrix is often much smaller than the whole tensor and, thus, we assume that the covariance matrix fits into the main-memory. In cases where this does not hold, it is possible to leverage block decomposition techniques [9] to implement this on chunks.
As described in the introduction, when the data are fre-quently updated, techniques which incrementally maintain tensor decompositions tend to be more efficient than repeat-edly decomposing the whole data tensor with each update.
In our work, we adapt the Dynamic Tensor Analysis (DTA) algorithm [31] for in-database operation. Note that, unlike CP, DTA assumes a dense core matrix as in Tucker decomposition [34]; but, as shown in [10], results of Tucker decompositions can be used as a first step towards boot-strapping CP decomposition.

DTA incrementally maintains covariance matrices for each mode and computes factor matrices by taking the leading eigen-vectors of the covariance matrices. More specifically, Figure 6 provides the pseudo-code for in-database, chunk-based DTA, implemented using the operators described in the previous section. Note that in-database DTA benefits from chunk-based operators in reducing the I/O overhead when dealing with disk-resident, large-scale data. However, as we also experimentally establish in Section 5, a significant portion of the execution cost of the above algorithm is due to the step in which the covariance matrix, C ( d ) , for each mode, d , is computed. Therefore, a key challenge is to reduce the cost of this step. We discuss this next.
As shown in Figure 6, the covariance matrix of a given ten-sor along a given mode, d , is computed by first matricizing the tensor along mode d and then multiplying the matricized tensor with its transpose. Both the matricization operation and the matrix multiplication can be implemented and op-timized using chunk-based techniques (as discussed in the previous section) to reduce I/O costs. However, given two matrix chunks U ij and V kl , which are ( i,j )-th chunk of a ma-trix U and ( k,l )-th chunk of a matrix V respectively, (the first one from the matricized tensor and the second from its transpose) brought into the memory, computation of U ij V is still a costly process. In this paper, we propose to address this by performing, when appropriate, (approximate) com-pressed matrix multiplication, instead of using conventional multiplication operators.
In general, for U ij of size n  X  m and V kl of size m  X  n , the matrix product, U ij V kl , can be obtained as follows: where u 1 ,..., u n are the row vectors of U ij and v 1 ,..., v are the column vectors of V kl . Compressed matrix multi-plication, on the other hand, is a recent technique which leverages compressive sensing to obtain an approximation of the matrix multiplication result, without performing n outer products explicitly [25]. While, the details of this algorithm is outside of the scope of this paper, it is sufficient to note that the algorithm computes a linear count sketch [12] of the entries of each outer product of Equation 6. The algorithm has two key parameters, b and d : b regulates the detail of the count sketches obtained for each column vector of U ij and row vector of V kl ; d , on the other hand, regulates the number of count sketches obtained to improve accuracy. [25] showed that it is possible to approximate a matrix product with high probability if the matrix product is com-pressible , i.e., if the Frobenius norm of the matrix product is dominated by a sparse subset of entries of the product . In this paper, we argue that this condition is often satisfied when computing covariance matrices, as for most data of interest, input matrices (i.e., matricized data tensors) have skewed distributions and, thus, the resulting covariance ma-trices tend to be sparse. Consequently, in most practical cases, approximate compressed matrix multiplication can be applied to obtain accurate estimates of covariance ma-trices. Most importantly, we can decide ahead of the time whether to use regular or compressed matrix multiplication, based on the sparsity of the initial covariance matrix. On the other hand, as we see next, when considering chunk-based in-database implementations, various further optimizations need to be considered.
According to [25], given a parameter pair, b and d (which together govern the number of collected sketches and two square matrices), the cost of multiplying two matrices of sizes n 1  X  n 2 and n 2  X  n 3 , is
Since, when estimating covariance matrices, we multiply chunks of size n  X  m with chunks of size m  X  n (of the transpose matrix), the cost of the operation for each chunk pair can be computed as or equivalently as Since, when chunks are dense, multiplying these two chunks using a straightforward matrix multiplication algorithm would cost O ( mn 2 ), as long as the inequality, is true, compressed matrix multiplication is likely to outper-form exact matrix multiplication.

When the matrices that are multiplied are sparse , how-ever, there are faster matrix multiplication algorithms [3]. Thus, as we experimentally show in Section 5, when the input chunks are sparse, compressed matrix multiplication may not provide significant time gains. Therefore, we utilize compressed matrix multiplication only for pairs of chunks that are both dense; we revert back to the default matrix multiplication algorithm if one of the chunks is sparse.
Chunks used in tensor decomposition are constrained by the amount of buffer available for storing them once they are read from the secondary storage into the main memory; on the other hand, it is possible to use chunk of different shapes, as long as the chunk size fits the allocated memory (i.e., n  X  m  X   X  for some target buffer size,  X  ). However, we see that the cost function (Equation 7) and the associated inequality, together provide additional constraints on n and m . These constraints can help determine the optimal shape of the chunk under a given buffer constraint.

In particular, the cost function implies that, for a given parameter pair, b and d , the running time gets faster when n gets smaller than m , given n  X  m fixed. On the other hand, since when creating count sketches, the column (or row) vectors (of size n ) need to be scanned sequentially, matrices with n &gt; m (when n  X  m is fixed) are likely to be scanned faster. Therefore, in practice, as we see in Section 5.3.3, the best execution times are observed when m  X  n .
In this section, we evaluate the proposed static and dy-namic in-database, chunk-based tensor decomposition oper-ators. We ran experiments on Ubuntu 12.04 64-bit, 7.7 GB RAM, Intel Core i5-2400 CPU @ 3.10GHz  X  4, and 112.6 GB disk. We implemented the proposed tensor manipulation op-erators by extending SciDB 12.12 [1]. In memory baselines are implemented using MATLAB tensor toolbox [7]. Tensor Representations. We use tensors of different densities and different tensor representations: sparse ten-sor representation (shortly referred to as STR), where only non-zero entries are kept, and dense tensor representation (DTR). We consider tensors with different densities, and in each figure we highlight the tensor density along with the tensor representation utilized; e.g., STR:0.001% for sparse representation of a tensor of 0.001% density. Data Sets. In these experiments, in addition to random data sets, we also used real data sets with different charac-teristics: Enron email data set (Enron) [29], MovieLens 1M data sets (Movie) [2], and a face data set (Face) [4] with 5,000 images. Table 1 shows the detail of each data set. In these experiments, we considered a target rank of 10.
Since both in-database CP and in-memory CP are imple-mented based on the same ALS algorithm, only scalability is a relevant comparison criterion.

In Figure 7, we compare the running times of in-memory and in-database CP on 3-mode dense random tensors (DTR: 50%). Here we use the same chunk dimensionality (250) for all tensors. Figure 7 shows that, as expected, when the data fits into the memory, in-memory decomposition is faster than in-database operation; however, the proposed in-database decomposition is able to operate even in situations where the in-memory decomposition is infeasible.

Note that as discussed in Section 2.3, in this paper we do not consider pipelined execution plans and this non-pipelined execution results in slowdowns in the in-database scheme compared against the full in-memory execution for small tensor sizes. We leave selective pipelining to minimize I/O overheads as future work.
We study the cost of in-database CP in detail on random tensors where the computation does not fit into the memory. Cost Breakdown of a Single Iteration. Figure 8(a) provides a high-level breakdown of a single iteration. As we see here, the cost of fit computation step in in-database CP is not negligible and thus the operations involved in this step also need to be carefully optimized. Figure 8(b) confirms that the copyArray operator introduced to efficiently update factor matrices (Section 3.2.4) provides significant savings relative to SciDB X  X  store operator.
 Cost Breakdown of Solving a Factor Matrix. Fig-ure 8(c) focuses on the time needed to solve factor matrices and show that, on dense data sets 7 , matricization (which requires data re-ordering as discussed in Section 3.2.1) is the single costliest operation. Figure 8(d) further analyzes the running times of the remaining operations 8 (i.e., all ex-cept matricization) involved in solving a factor matrix: as expected, multiplication of the result of matricization with the result of Khatri-Rao product consumes the most time. Impact of the matricize Operator. As we also discussed in Section 3.2.1, it is possible to implement matricization us-ing SciDB X  X  redimension and reshape operations, instead of the proposed special matricize operator. Figure 9 shows that the proposed matricize operator is significantly more efficient than SciDB X  X  existing operators for all chunk densi-ties and STR/DTR representations. Note that the execution times reported in Figure 9(b) show the importance of using chunk-based matricize on modes that would otherwise ne-cessitate the use of redimension operator: in this case, the savings in execution time through the use of chunk-based matricize are multiple orders of magnitude .
Matricization is much cheaper on sparse data sets; results are omitted due to space limitations.
While these are negligible for dense input data, for sparse data sets where matricization is fast, these operations, which always operate on dense factor matrices, even when the in-put tensor is sparse, will constitute the dominant cost.
In addition to being useful when solving a factor matrix, as we have seen in Section 3.1, matricization is also useful while computing the degree of fit. Figure 9(c) shows that, also in this case, using the proposed matricize operator helps reduce the execution times.
 Impact of Materialization of Matricization. To fur-ther reduce the cost of matricization, we can also leverage materialization of the matricization results. As seen in Fig-ure 10, the materialization of the matricization can help re-duce running times of in-database CP, especially on higher order input tensors and dense representations.

Note that the cost of materializing matricization gets amortized as the number of iterations increases. Also note that materialization of matricization requires additional storage on the hard disk (equal to the tensor size for each matricized mode). While this is often not an issue, when the storage is a concern, one can selectively materialize the matricization on a subset of modes.
We next present the experiment results for in-database dynamic tensor analysis (DTA) and in-database DTA with compressed matrix multiplication (C-DTA). For implement-ing the FFT process involved in the compressed matrix mul-tiplication algorithm, we used a C subroutine library [14]. In these experiments, we set the ranks to 5 for each mode. Data Sets. For these experiments, in addition to the ran-domly generated data, we used Epinions [32], MovieLens 10M [2], and Aerial views II [5] (Aerial view) data sets. For the Epinions data, we ran the in-database DTA for 4 windows on the input tensor of product ratings (user, prod-uct, category, rating, helpfulness). This tensor is of size 5000  X  5000  X  26  X  5  X  5 (we considered 5000 frequent users and products). For the MovieLens 10M data, we used 5 win-dows of the movie rating data (movie, user, rating) on the input tensor of size 5000  X  5000  X  10 (we considered 5000 fre-quent users and movies). For Epinions and MovieLens 10M data, each entry of the input tensors denotes whether the rating exists (1) or not (0) on the corresponding attributes in the window. For the Aerial view data, in-database DTA is performed on 6 gray-scale image frames (x-coord, y-coord) where each entry represents a gray-scale color (0-255). Ta-ble 2 shows the data sets. Impacts of the Number of Modes and Data Density on DTA. We next evaluate the impact of the number of modes and data density on DTA, using Epinions data and MovieLens 10M data (Figure 11). As we see here, on sparse data, the number of tensor modes is a significant factor and 5-mode Epinions data requires much larger decomposition time than 3-mode MovieLens 10M data. As shown in Fig-ure 11(b), running times get larger for denser data and the largest contributor to the execution time of DTA is the co-variance matrix computation.
The default values of b is set to n/ 2 for n  X  n chunk of the covariance matrix and d is set to 30 (as explained later). Sparse vs. Dense Tensors. As we see in Figure 12, as expected, C-DTA is not advantageous for data with sparse representation (STR). On the other hand, for data with dense representation (DTR), C-DTA provides signifi-cant time gains. These confirm the observations reported in Section 4.2.2. Note that Figure 12 re-confirms that the running times of covariance matrix computation is the most dominant component in DTA and C-DTA. Accuracy of C-DTA. Figure 13 presents the running times and fits of C-DTA vs. DTA on 5 consecutive image frames of the Aerial view data set. As we have already seen in Fig-ure 12(b), on this data set, C-DTA consistently outperforms DTA (  X  3  X  ) and, despite the significant drops in execution time, the fits of C-DTA are close to those of DTA (  X  80% relative fit). Interestingly, while the fit of DTA drops as more update windows are considered, the degree of fit of C-DTA remains mostly consistent.
As we have confirmed above, C-DTA is useful mainly for dense tensors. Thus, here, we focus on dense matrices. Scalability of Compressed Matrix Multiplication. We first compare the running time of compressed matrix multiplication with running time of exact matrix multipli-cation. As we have seen in Section 4.2.2, as the row length of the matrix increases, the time complexity of compressed matrix multiplication increases linearly, whereas the running time of the exact matrix multiplication increases quadrati-cally. This is confirmed in the results in Figure 14(a). Time/Accuracy Trade-Offs for Covariance Compu-tation. Next, we evaluate the time/accuracy trade-offs in computing the covariance matrix with and without com-pressed matrix multiplication in in-database DTA (C-DTA vs. DTA). In particular, we consider different values of the parameter, d , which controls the number of count sketches of the matrix product. In Figures 14(b) and (c), the tensor size is 5000x100x10 and we considered the covariance ma-trix on the first mode (of size 5000x5000). As we see in these figures, as we obtain more count sketches, the accuracy of C-DTA improves, but the execution time gains drop. Based on these results, we choose d = 30 as the default value. Impact of Chunk Density in Compressed Matrix Multiplication. The cost analysis of the compressed ma-trix multiplication in Section 4.2.2 as well as C-DTA vs. DTA experiments reported in Figure 12 implied that com-pressed matrix multiplication is not effective for sparse data. We next evaluate the running times of compressed matrix multiplication on chunks of different densities. Figure 15(a) re-confirms that, as expected, compressed matrix multipli-cation is not advantageous for sparse data, but the execution time gains become significant (e.g., 2.5  X  ) as the chunk size and density increase.
 Impact of Chunk Shape in Compressed Matrix Mul-tiplication. As discussed in Section 4.2.3, shapes of the chunks can impact the performance of the compressed ma-trix multiplication. In Figure 15(b), we evaluate execution times for different chunk shapes (of the same size). The re-sults show that, as the cost analysis in Section 4.2.3 implies, running times are highest when n is largest and running times are smallest when n and m are close to each other.
Lifecycle of most data includes a diverse set of operations, from capture, integration, projection, to data decomposi-tion and analysis. Tensor is a natural representation for multidimensional data due to its simplicity and tensor de-compositions have been used to capture higher-order struc-ture of data. Although tensor decompositions have proven to be useful for multidimensional analysis, the high cost of the operations, due to its high-modality and exponentially increasing complexity in the number of dimensions of the data, makes their applications challenging, especially on disk resident data sets. In this paper, we proposed in-database (static and dynamic) tensor decomposition operations on a chunk-based array to address memory blowup problems when dealing with large, higher-order tensor data.
