 Algorithms based on simulating stochastic flows are a sim-ple and natural solution for the problem of clustering graphs, but their widespread use has been hampered by their lack of scalability and fragmentation of output. In this article we present a multi-level algorithm for graph clustering using flows that delivers significant improvements in both quality and speed. The graph is first successively coarsened to a manageable size, and a small number of iterations of flow simulation is performed on the coarse graph. The graph is then successively refined, with flows from the previous graph used as initializations for brief flow simulations on each of the intermediate graphs. When we reach the final refined graph, the algorithm is run to convergence and the high-flow regions are clustered together, with regions with-out any flow forming the natural boundaries of the clusters. Extensive experimental results on several real and synthetic datasets demonstrate the effectiveness of our approach when compared to state-of-the-art algorithms.
 H.2.8 [ Database Applications ]: Data Mining; G.2.2 [ Graph Theory ]: Graph Algorithms Algorithms,Performance Graphs, Clustering, Communities, Networks
Clustering graphs or discovering communities within net-works is an important problem with many applications in a number of disciplines. Examples abound and range from social network analysis[14] to image segmentation[17] and from analyzing protein interaction networks[2] to the circuit layout problem[6].

Given the importance of the p roblem numero us solutions have been proposed in the literature. Spectral methods that target weighted cuts [17] form an important class of such algorithms and are shown to be very effective for prob-lems such as image segmentation. Multi-level graph parti-tioning algorithms such as Metis[9] are well known to scale well,and have been used in studies of some of the biggest graph datasets[11]. Graclus [4] is another multi-level par-titioning algorithm that optimizes weighted cuts (includ-ing normalized cuts) by optimizing an equivalent weighted kernel K-means loss function. The avoidance of expensive eigenvector computation gives Graclus a big boost in speed while retaining or improving upon the quality of spectral approaches. Divisive/agglomerative approaches have been popular in network analysis[14], but they are expensive and do not scale well[3]. Markov Clustering (MCL)[5], a graph clustering algorithm based on (stochastic) flow simulation, has proved to be highly effective at clustering biological net-works [2, 13].

Here, we focus on the class of flow-based graph clustering algorithms represented by MCL. MCL offers several advan-tages in that it is an elegant approach based on the natural phenomenon of flow, or transition probability, in graphs. It has been shown to be robust to topological noise effects (a desirable property for a number of domains)[2], and while not completely non-parametric, varying a simple parameter can result in clusterings of different granularities. However, inspite of its popularity in the bioinformatics community for the above reasons, MCL has drawn limited attention from the data mining community primarily because it does not scale very well even to moderate sized graphs[3]. Addition-ally, the algorithm tends to fragment communities, a less than desirable feature in many situations.

In this article we seek to develop an algorithm that re-tains the strengths of MCL while redressing its weaknesses. We first analyze the basic MCL algorithm carefully to un-derstand the cause for these two limitations. We then iden-tify a simple regularization step that can help alleviate the fragmentation problem. We call this algorithm Regularized-MCL (R-MCL). Subsequently we realize a scalable multi-level variant of R-MCL. The basic idea of the multi-level procedure is to coarsen the graph (in a manner reminiscent of Metis), run R-MCL on the coarsened graph, and then re-fine the graph in incremental steps. The central intuition is that using flow values derived from simulation on coarser graphs can lead to good initializations of flow in the refined graphs. Key to the refinement operation is a novel way to project flows such that the sanctity of the clustering algo-rithm is maintained. The multi-level approach also allows us to obtain large gains in speed. We refer to this algorithm as Multi-Level Regularized MCL (MLR-MCL).

In our empirical study we compare and contrast R-MCL and MLR-MCL with the original MCL algorithm[5], Gra-clus[4] and Metis[9] along the twin axes of scalability and quality on several real and synthetic datasets. Key high-lights of our study include:
Let G =( V , E ) be our input graph with V and E denoting the node set and edge set respectively. Let A be the |V| X |V| adjacency matrix corresponding to the graph, with A ( i, j ) denoting the weight of the edge between the vertex v i and the vertex v j . This weight can represent the strength of the interaction in the original network -e.g. in an author collaboration network, the edge weight between two authors could be the frequency of their collaboration. If the graph is unweighted, then the weight on each edge is fixed to 1. As many interaction networks are undirected, we also assume that G is undirected, although our method is easy to extend to directed graphs. Therefore A will be a symmetric matrix.
A column-stochastic matrix is simply a matrix where each column sums to 1. A column stochastic (square) matrix M withasmanycolumnsasverticesinagraph G can be interpreted as the matrix of the transition probabilities of a random walk (or a Markov chain) defined on the graph. The i th column of M represents the transition probabilities out of the v i ; therefore M ( j, i ) represents the probability of a transition from vertex v i to v j . We use the terms stochastic matrix and column-stochastic matrix interchangeably.
We also refer to the transition probability from v i to v as the stochastic flow or simply the flow from v i to v j respondingly, a column-stochastic transition matrix of the graph G is also referred to as a flow matrix of G or simply a flow of G . Given a flow matrix M ,the i th column contains the flows out of node v i ,orits out-flows ; correspondingly the i th row contains the in-flows . Note that while all the columns (or out-flows) sum to 1, the rows (or in-flows) are not required to do so.

The most common way of deriving a column-stochastic transition matrix M for a graph is to simply normalize the columns of the adjacency matrix to sum to 1 In matrix notation, M := AD  X  1 ,where D is the diagonal degree matrix of G with D ( i, i )= fer to this particular transition matrix for the graph as the canonical transition matrix M G . However, it is worth keep-ing in mind that one can associate other stochastic matrices with the graph G .

Both MCL and our methods introduced in Section 3 can be thought of as simulating stochastic flows (or simulating random walks) on graphs accordin g to certain rules. For this reason, we refer to these processes as flow simulations .
We next describe the Markov Clustering (MCL) algorithm for clustering graphs, proposed by Stijn van Dongen [5], in some detail as it is relevant to understanding our own method.

The MCL algorithm is an iterative process of applying two operators -expansion and inflation -onaninitialstochastic matrix M , in alternation, until convergence. Both expansion and inflation are operators that map the space of column-stochastic matrices onto itself. Additionally, a prune step is performed at the end of each inflation step in order to save memory. Each of these steps is defined below: Expand : Input M , output M exp .
 The i th column of M exp can be interpreted as the final distri-bution of a random walk of length 2 starting from vertex v with the transition probabilities of the random walk given by M . One can take higher powers of M instead of a square (corresponding to longer random walks), but this gets com-putationally prohibitive very quickly.
 Inflate : Input M and inflation parameter r , output M inf M inf corresponds to raising each entry in the matrix M to the power r and then normalizing the columns to sum to 1. By default r = 2. Because the entries in the matrix are all guaranteed to be less than or equal to 1, this operator has the effect of exaggerating the inhomogeneity in each column (as long as r&gt; 1). In other words, flow is strengthened where it is already strong and weakened where it is weak. Prune : In each column, we remove those entries which have very small values (where  X  X mall X  is defined in relation to the rest of the entries in the column), and the retained entries are rescaled to have the column sum to 1. This step is pri-marily meant to reduce the number of non-zero entries in the matrix and hence save memory.We use the threshold prun-ing heuristic, where we compute a threshold based on the average and maximum values within a column, and prune entries below the threshold. [5]
Pseudo-code for MCL is presented in Algorithm 1. The addition of self-loops to the input graph avoids dependence of the flow distribution on the length of the random walk simulated so far, besides ensuring at least one non-zero entry per column.

Intuitively, the MCL process may be understood as ex-panding and contracting the flow in the graph alternately. The expansion step spreads the flow out of a vertex to po-tentially new vertices and also enhances the flow to those Algorithm 1 MCL vertices which are reachable by multiple paths. This has the effect of enhancing within-cluster flows as there are more paths between two nodes that are in the same cluster than between those in different clusters. However, just applying the expansion step repeatedly will result in all the columns of M becoming equal to the principal eigenvector of the canon-ical transition matrix M G . The inflation step is meant to prevent this from happening by introducing a non-linearity into the process, while also having the effect of strengthen-ing intra-cluster flow and weakening inter-cluster flow. At the start of the process, the distribution of flows out of a node is relatively smooth and uniform; as more iterations are executed, the distribution becomes more and more peaked. Crucially, all the nodes within a tightly-linked group of nodes will start to flow to one node within the group towards the end of the process . This allows us to identify all the vertices that flow to the same  X  X ttractor X  node as belonging to one cluster.
 Interpretation of M as a clustering : As just mentioned, after some number of iterations, most of the nodes will find one  X  X ttractor X  node to which all of their flow is directed i.e. there will be only one non-zero entry per column in the flow matrix M. We declare convergence at this stage, and assign nodes which flow into the same node as belonging to one cluster.
The MCL algorithm is a simple and intuitive algorithm for clustering graphs that takes an approach that is dif-ferent from that of the majority of other approaches to graph clustering such as spectral clustering [17, 4], divi-sive/agglomerative clustering [14], heuristic methods [10] and so on. Further more, it does not require a specifica-tion of the number of clusters to be returned; the coarseness of the clustering can instead be indirectly affected by vary-ing the inflation parameter r , with lower values of r (up to 1) leading to coarser clusterings of the graph. MCL has received a lot of attention in the bioinformatics field, with multiple researchers finding it to be very effective at cluster-ing biological interaction networks ([2, 13]).
 However, there are two major limitations to MCL: Lack of scalability : That MCL is slow has been noted by data mining researchers before ([7, 3]). The Expand step, which involves matrix multiplication, is very time consuming in the first few iterations when many entries in the flow ma-trix have not been pruned out and is the main component of the overall running time. The Expand step in the very first iteration of the algorithm in particular takes O ( operations, where d i is the degree of vertex v i , which is un-acceptably slow for large graphs. Expansion steps in subse-quent iterations take O ( |V| k 2 ) time, where k is the average number of non-zero entries per column. In the first few iter-ations before the flow matrix becomes sparse, k is typically in the range of hundreds or thousands for large graphs, still leading to an unacceptable time complexity.
 Fragmentation of output : MCL tends to produce too many clusters. For example, on the yeast protein-protein interaction network of 4741 nodes, MCL outputs 1416 clus-ters. (We obtained similar results on other datasets, as well as with varying values of the inflation parameter r .) While it still manages to find some significant clusters (as evidenced by its success in bioinformatics [2]), clearly such high frag-mentation is undesirable.
We seek to develop an algorithm for graph clustering that retains the strengths of MCL while alleviating the weak-nesses. We do this by first making a modification to the basic MCL process, resulting in an algorithm we call Regu-larized MCL , and then we embed this latter algorithm in a multi-level framework that further improves both the quality of the output and the speed of the algorithm.
 A weight transformation step : Before discussing the al-gorithm, we first describe a weight transformation step that we apply to the input graphs. This step was suggested by Dhillon et al. [4], who use it as part of the coarsening pro-cess in their multi-level framework. Given the input adja-cency matrix A and the degree diagonal matrix D , define the transformed adjacency matrix A  X  as: The purpose of the above step is to downweight the edges involving high-degree (or hub) nodes, as they can have an undue influence on the clustering process.
Why does MCL output so many clusters? One way of looking at the issue is to understand it as MCL allowing the columns of many pairs of neighboring nodes in the flow ma-trix M to diverge significantly. This happens because the MCL algorithm uses the adjacency structure of the input graph only at the start, to initialize the flow matrix to the canonical transition matrix M G . After that, the algorithm works only with the current flow matrix M ,andthereis nothing in the algorithm that prevents the columns of neigh-boring nodes to differ widely without any penalty. This is what allows MCL to  X  X verfit X  the graph by outputting too many clusters.

We seek to address this issue by regularizing or smoothing the flow distributions out of a node w.r.t. its neighbors. Let q , i =1: k , be the flow distributions of the k neighbors of a given node in the graph. (Each q i is basically a column from the current flow matrix M .) Let w i , i =1: k ,bethe respective normalized edge weights, i.e.
 that since we add self-loops, each node is also one of its own neighbors. We ask the following question: how do we update the flow distribution out of a given node (call it q so that it is, in some sense, the least divergent from the flow distributions of its neighbors? Following [19], we can formalize this requirement for q  X  as: where KL ( p || q ) is the KL divergence between two proba-bility distributions p and q -a commonly used divergence measure for probability distributions -defined as:
There exists a closed form solution for q  X  [19, 15], and can be shown to be:
Hence, we replace the Expand operator in the MCL pro-cess with a new operator which updates the flow distribution of each node according as Equation 1. We call this the Reg-ularize operator, and it can be conveniently expressed in matrix notation as right multiplication with the canonical transition matrix M G of the graph.
 Pseudo-code for Regularized MCL is given in Algorithm 2. The Inflate and Prune steps are the same as for MCL, and the interpretation of M as a clustering is also the same as has been described in Section 2.2.
 Algorithm 2 Regularized MCL
While, as we shall see in Section 4, Regularized MCL does produce fewer clusters of better quality, it still suffers from the scalability issues of the original MCL. We address this issue next and also discuss how the qualitative performance of Regularized MCL can be further improved.
We next explain a multi-level version of Regularized MCL, which we call Multi-level Regularized MCL, or MLR-MCL. The main intuition behind using a multi-level framework in our context is that the flow values resulting from simulation on the coarser graphs can be effectively used to initialize the flows for simulation on the bigger graphs. The algorithm also runs much faster because the initial few iterations, which are also the most time taking, are run on the smallest graphs first. 1
A schematic providing a high-level overview of our multi-level algorithm is given in Figure 1. The pseudo-code is given in Algorithm 3. MLR-MCL operates in three phases: 1. Coarsening : The input graph G is coarsened succes-
We discuss why it is hard to similarly embed MCL as well in a multi-level framework in the extended version of this paper [15].
 Figure 1: A high-level overview of Multi-level Reg-ularized MCL. 2. Curtailed R-MCL along with refinement :Be-3. R-MCL on original graph : With flow values ini-
What is the intuition behind running R-MCL for only a few iterations on the coarse graphs from G l down to G 1 ?We do this as we do not want the flows in a coarse graph to converge; if we run R-MCL until convergence on one of the intermediate graphs, then the same cluster assignments will likely carry over till the original graph, thus not utilizing the additional adjacency information present in the bigger graphs. At the same time we want the flow values to cap-ture some of the high-level cluster structure of the coarser graphs, and also want the flow matrix to be sparse enough to make running R-MCL on the bigger graph computation-ally tractable. So we strike a balance and run R-MCL for a small number of iterations. In practice, we have observed that running R-MCL for 4 to 5 iterations on the intermedi-ate graphs gives good results.
 Algorithm 3 Multi-level Regularized MCL Algorithm 4 ProjectFlow The problem of flow projection : The remaining part of MLR-MCL is the algorithm for projecting flow from a coarse (smaller) graph to a refined (bigger) graph. Projection of flow is concerned with using the flow values of a smaller graph to provide a good initialization of the flow values in the bigger graph. It is not obvious at first sight how this should be done -since there are two nodes in the bigger graph corresponding to each node in the smaller graph, a flow value between two nodes in the smaller graph must be used to derive the flow values between four pairs of nodes in the bigger graph. To look at it another way, if n c is the size of the coarse graph, then the n 2 c entries of the flow matrix of the coarse graph must be used to derive the 4  X  n 2 c entries of the refined graph. How to do this? Our solution : The naive strategy here is to assign the flow between two nodes in the refined graph as the flow between their respective parents in the coarse graph. However, this doubles the number of nodes that any node in the refined graph flows out to. This, combined with the fact that the out-flows of each node sum to 1, results in excessive smooth-ing of the out-flows of each node. (Recall that as the MCL process converges, the out-flow distribution of the nodes gets more and more peaked.) Hence, we instead choose only one child node for each parent node and project all the flow into the parent node to the chosen child node.

However, this raises the question: which child node do we pick in order to assign all the flow into? It turns out that it doesn X  X  matter which child node we pick, as long as for each parent, the choice is consistent. We state this in Theorem 1; the proof is provided in the extended version of this paper [15].

Theorem 1. The MLR-MCL algorithm produces the same final clustering regardless of which child node is picked at each parent node to be assigned all its in-flows.
For this reason, for each node v i in the coarse graph, we arbitrarily pick the first child node NodeMap1( i ) and assign all the flow that was going into v i to NodeMap1( i ).
While we treat the two child nodes asymmetrically when we are assigning the flows into them, the flows out of the two child nodes are assigned the same values. This being the case, can the algorithm treat these two nodes differently? Recall that the Regularize step utilizes the adjacency infor-mation in the graph by assigning a linear combination of the flows of a node X  X  neighbors as the flows of a node. Hence, even if NodeMap1( i ) and NodeMap2( i ) start out with the same flows out of them , they will have different flows out of them after the Regularize step if they have different neigh-bors . This ensures that the additional adjacency information that is present in the refined graph is used to re-adjust the flows of the nodes.
We now discuss the time-complexity of MLR-MCL and the quality of its output.
 Scalability and time-complexity : The main component of the running time of R-MCL is the Regularize step which involves matrix multiplication. The time complexity of the Regularize step in the first iteration is O ( to that for MCL. The subsequent iterations require O ( k |E| where k is the average number of non-zero entries per column in the flow matrix. This rules out the direct use of R-MCL on large g raphs.
The analysis is similar with MLR-MCL, but with a cru-cial difference. The Regularize step of the first iteration is carried out on the coarsest graph, so the time complexity of O ( coarsest graph. As the coarsest graph is small, this is an affordable step. As the algorithm proceeds,we simulate flow on bigger graphs, but at the same time the flow matrix also becomes sparser,enabling the algorithm to scale easily. Em-pirically we observe that after the first Curtailed R-MCL run on the coarsest graph, there are rarely more than a few tens of non-zero entries per column. The overall time complexity of the algorithm is well approximated as O ( k |E| + where the d i s are the degrees of the nodes in the coarsest graph, and k is a small constant, typically in the tens. Quality : Embedding R-MCL in a multi-level framework leads to improvements in quality as well, as we show in Sec-tion 4.2. Coarsening the graph allows the algorithm to uti-lize the global topology of the graph to provide an effective initialization of the flow values for the simulations on the bigger graphs. At the same time because iterations are run on the final graph as well, the algorithm is able to adjust suitably to the local topology. All of this translates into clusters that are of higher quality than those that are pro-duced by either MCL or R-MCL.
We describe extensive evaluation for the methods pro-posed in the paper. We performed experiments on 7 real world datasets; four of these were author collaboration net-works -three from the Physics community (Astro, HepPh and HepTh), and one from the Computer Science commu-nity (DBLP) -, one is a who-trusts-whom network from Epinions.com (Epinions) 2 , one is a paper citation network (Cora) 3 and the last is the Protein-Protein Interaction net-work of yeast (Yeast-PPI) 4 . Details are given in Table 1. We also synthetically generated 5 datasets of increasing size for evaluating scalability, details given in Table 2.
The experiments were performed on a dual core machine (Dual 250 Opteron) with 2.4GHz of processor speed and 8GB of main memory. The software for each of our base-lines was downloaded from the respective author X  X  webpages. Our implementation was in C/C++, as were the implemen-tations of all of our baselines. The matrices were stored using a sparse matrix representation in our implementation.
Except for the Yeast PPI network where we use a domain-specific evaluation, we will use normalized cut or conduc-tance as our measure of cluster quality.

The normalized cut of a cluster C in the graph G is defined as (where A is the adjacency matrix of the graph) The normalized cut of a cluster then, is simply the number of
Astro, HepPh, HepTh and Epin-ions were obtained from http://cs-www.cs.yale.edu/homes/mmahoney/NetworkData/
Obtained from Andrew McCallum X  X  web page: http://www.cs.umass.edu/ mccallum/code-data.html
Obtained from the Database of Interacting Proteins: http://dip.doe-mbi.ucla.edu/dip/Main.cgi Astro-Ph 17903 196972 22.00 Hep-Ph 11204 117619 21.00 Hep-Th 8638 24806 5.78 Epinions 75877 405739 10.69 Yeast-PPI 4741 15148 6.39 edges that are  X  X ut X  when dividing this cluster from the rest of the graph, normalized by the total degree of the cluster.
The normalized cut of a clustering {C 1 , C 2 ,..., C k } sum of the normalized cuts of the individual clusters.
The average normalized cut of a clustering is the average of the normalized cuts of each of the constituent clusters, and lies between 0 and 1. Averaging the N-Cut score allows us to compare clustering arrangements with different numbers of clusters.

The Normalized Cut criterion has been commonly noted as capturing very well the intuitive notion of the  X  X oodness X  of a particular clustering [12, 8, 17]. An additional reason for choosing this criterion is that one of our baselines, Graclus [4] is one of the state-of-the-art algorithms for optimizing precisely this criterion.
In our first set of experiments we compare the performance of R-MCL and MLR-MCL with the baseline MCL algorithm. Table 3 documents results obtained on 6 real datasets. The key trends one can glean from this study are as follows. 5
First, MLR-MCL clearly dominates both R-MCL and MCL in terms of scalability. It is about 2 orders of magnitude faster than MCL and about one order of magnitude faster than R-MCL for most of the datasets. Second, in all cases both MLR-MCL and R-MCL report far fewer clusters than MCL. This trend again serves to highlight the fragmentation problem of MCL. Third, in terms of average normalized cut scores MLR-MCL dominates MCL and also usually outper-forms R-MCL. On two datasets, namely Astro and Hep-Th, we find that R-MCL achieves a marginally better average N-Cut score.
In this experimental study we examine the weak scaling behavior of MLR-MCL and compare it with the state of
In [15], we describe a further experiment where we vary the inflation parameter for MCL and note that the trends do not significantly change. the art Metis algorithm. This experiment evaluates the per-formance of these algorithms as the size and complexity of the dataset is increased. For this experiment we used the synthetic datasets described earlier, ranging from a 10,000 node graph to a 1 million node graph. From Figure 2, we observe that both MLR-MCL and Metis show good scalabil-ity trends. While MLR-MCL is competitive, Metis clearly demonstrates superior performance (by a factor of two typ-ically) on this synthetic dataset. As a counterpoint we also present results on R-MCL which is about 2 orders of mag-nitude slower across the board than MLR-MCL. We do not include results for MCL here since it was found to be ex-tremely slow even on the smaller datasets. Graclus also is not included here since the algorithm was found to suffer from severe memory thrashing effects when attempting to run on the the 500,000 vertex and 1 million vertex datasets.
In the next set of experiments we compare the qualitative performance of MLR-MCL with Graclus and Metis. We de-tail the results here on 6 real datasets. In all experiments MLR-MCL is run with the inflation parameter r =2 . 0. For MLR-MCL we vary the coarseness of the clustering by vary-ing the size of the coarsest graph; we subsequently run Gr-aclus and Metis to output the same number of clusters as has been found by MLR-MCL. We plot the average normal-ized cut of each algorithm as a function of the number of clusters. (It must be kept in mind that with more num-ber of clusters, seemingly small differences in average N-Cut can translate into significant differences in the total N-Cut.) From Figure 4a-f one can easily observe that MLR-MCL is either competitive with Graclus(DBLP and Hep-Ph) or bet-ter (Astro-Ph, Cora, Epinions and Hep-Th) in terms of the normalized cut objective. The improvement in Avg Ncut over Graclus for these latter four datasets is in the range of 10-15%, if we consider the median number of clusters, which is quite significant. Another obvious trend is that both these algorithms outperform Metis, often significantly. Drilling deeper into the data we find that this can be explained by the fact that both Graclus and MLR-MCL admit a more skewed (actually the skew for both is quite similar) cluster-ing arrangement whereas Meti s tends to force a more bal-anced partitioning. Another interesting observation is that when the number of clusters discovered is more, MLR-MCL typically performs better. A third point of note is that the two datasets where Graclus is competitive with MLR-MCL -Dblp and Hep-Th -are the two datasets with the lowest average degree of the 6 datasets (5.56 and 5.78 respectively). Scalability Evaluation: In the next set of experiments we compare and contrast the scalability of MLR-MCL with Metis and Graclus on three of the real datasets, while vary-ing the number of clusters. From Figure 5a for the Epinions dataset we find that MLR-MCL is competitive with Metis, and that both algorithms are faster than Graclus.The trends in Figures 5b and c are consistent in that Metis outperforms MLR-MCL, which in turn outperforms Graclus, especially with increasing number of clusters.
The goal of analyzing protein-protein interaction (PPI) networks is to extract groups of proteins that either take part in the same biological process ( induction of cell death is an example) or perform similar molecular functions (e.g. RNA binding ). This is a challenging problem; it is estimated that the protein function of about one-fourth of the proteins is unknown even for the most well-studied organisms such as yeast.[16].

We use as our dataset the PPI network of S. cervisiae or yeast, which contains 4741 proteins with 15148 known inter-actions. We perform a domain-specific evaluation using The Gene Ontology database [18], which provides three vocab-ularies (or annotations) of known associations  X  Molecular Function, Biological Process and Cellular Component. The first two have functional significance while the last one refers to the localization of proteins within a cell. Researchers have used this ontology in the past to validate the biological significance of clusters. Merely counting the number of pro-teins that share an annotation within each extracted cluster is misleading since the underlying frequency of the annota-tions is not uniform -more proteins are characterized by an annotation at the top of the hierarchy than at the bot-tom. For this reason, p-values are often used to calculate the statistical significance of such clusters [1]. Intuitively these values capture the probability of seeing a particular grouping, or better, by random chance using a background distribution (typically hyper-geometric). Let the total num-ber of proteins be N with a total of M proteins sharing a particular annotation. The p-value of observing m or more proteins that share the same annotation in a cluster of n proteins, using the Hyper-geometric Distribution is: Smaller p-values imply that the grouping is less likely to be random. 6
It is worth remarking before we discuss the results that earlier research has shown that MCL typically outperforms MCODE and several other domain specific community dis-covery algorithms for such biological networks[2]. In our study we compared the performance of MCL, MLR-MCL and Graclus on this domain specific qualitative metric. Metis was found to perform poorly on this metric, primarily be-cause of its tendency to favor balanced clusters.
We set the inflation parameter r to 1.6 for both MCL and MLR-MCL. The size of the coarsest graph was set to 1000 nodes for MLR-MCL. MLR-MCL returned 427 clusters where as MCL returned 1615 clusters. We then ran Graclus to output 427 clusters to keep the comparison fair. Each cluster is associated with the annotation that minimizes the p-value for that cluster, and the corresponding p-value was retained as the p-value to represent that cluster.
Figures 3a and b compare the p-values of the top 100 clus-ters returned by each algorithm, under the Biological Pro-cess (P) and Molecular Function (F) vocabularies respec-tively. The Y-axis represents negative log p-values while the X-axis is simply an ordered list of the top-scoring clus-ters produced by the different graph clustering algorithms. Since better clusters have lower p-values, higher values on the graph represent a higher quality of clustering. As we see from the charts, MLR-MCL clearly outperforms Graclus and MCL among the top set of clusters and is clearly com-petitive or better than either across the board for both the Molecular Function and Biological Process ontologies.
Under the evaluation using Biological Process annotations, the top-scoring cluster returned by MLR-MCL obtained a p-value of 1 . 8 e  X  80, which is significantly better than 2 . 4 e and 1 . 6 e  X  28, the top scoring p-values for Graclus and MCL respectively. In fact MLR-MCL returns 8 clusters which score better p-values than the best p-value scored by Gra-clus. The p-value of 1 . 8 e  X  80 was scored by a cluster of 88 proteins returned by MLR-MCL, out of which 55 are pro-teins currently known to be involved in the process of nuclear mRNA splicing via spliceosome . It is very interesting that the top scoring cluster for MCL was also in fact matched with the same annotation, but MCL managed to retrieve only 25 of the proteins known to be involved in this process. This clearly illustrates that MLR-MCL overcomes the main qualitative limitation of MCL -fragmentation of output. In this work we have presented Regularized MCL and Multi-Level Regularized MCL, two flow based algorithms for graph clustering. Results on several real and synthetic datasets highlight the utility of the approach when compared We used a publicly available package called GO-TermFinder for calculating the p-values. The url is http://search.cpan.org/dist/GO-TermFinder/ with MCL, Metis and Graclus, three state-of-the-art graph clustering algorithms. Specifically, we find that the new al-gorithms are 2-3 orders of magnitude faster than MCL, and improve significantly on the quality of the output clusters. Similarly we find that our approaches outperform Metis and Graclus in terms of quality and are competitive in terms of scalability.

As part of ongoing and future research, we will investi-gate the utility of this approach for analyzing the temporal evolution of networks. Another line of inquiry is to extend it to directed graphs and bipartite graphs. We also believe that this algorithm is amenable to effective parallelization and worth studying.
 Acknowledgments : We thank the reviewers for their thought-ful comments and suggestions. This work is supported in part by the following grants: NSF CAREER IIS-0347662, RI-CNS-0403342, CCF-0702586 and IIS-0742999 Function (F). Higher is better. Dblp (c) Hep-Th
