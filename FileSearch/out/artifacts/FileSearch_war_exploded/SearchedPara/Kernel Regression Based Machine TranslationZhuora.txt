 for statistical machine translation (SMT). A rough linear relation is shown by the co-occurences of phrases in bilingual sentence pairs, which motivates us to introduce a novel study on the SMT task:
If we define the feature space H language X as all its possible phrases (i.e. informa-tive blended word n -grams), and define the mapping  X  x : X  X  H x pressed by its feature vector  X  component of  X  value being the frequency of it in x . The definition of the feature space H be made in a similar way, with corresponding map-ping  X  task, given S = { ( x is the translation of x matrix represented linear operator, such that: to predict the translation y for a new sentence x .
Comparing with traditional methods, this model gives us a theoretical framework to capture higher-dimensional dependencies within the sentences. To solve the multi-output regression problem, we inves-tigate two models, least squares regression (LSR) similar to the technique presented in (Cortes et al., 2005), and maximum margin regression (MMR) in-troduced in (Szedmak et al., 2006).

The rest of the paper is organized as follows. Sec-tion 2 gives a brief review of the regression models. Section 3 details the solution to the pre-image prob-lem. We report the experimental results in Section 4, with discussions in Section 5. 2.1 Kernel Induced Feature Space In the practical learning process, only the inner prod-ucts of the feature vectors are needed (see Section x, z  X  X , a kernel function is defined as: Similarly, a kernel function  X 
In our case, the blended n -spectrum string ker-by counting how many (contiguous) substrings of length from 1 up to n they have in common, is a good choice for the kernel function to induce our feature spaces H in some uninformative features (word n-grams) as well, when compared to our original definition. 2.2 Least Squares Regression A basic method to solve the problem in Eq. 1 is least squares regression that seeks the matrix W mini-mizing the squared loss in H where M [ X  y ( y 1 ) , ...,  X  y ( y m )] nius norm.

Differentiating the expression and setting it to zero gives: where K Gram matrix. 2.3 Maximum Margin Regression 2006), called maximum margin regression. If L2-noted by  X   X  ing optimization: where C &gt; 0 is the regularization coefficient, and  X  are the slack variables. The Lagrange dual form with dual variables  X  min s . t . 0  X   X  i  X  C, i = 1 , . . . , m. (6) where  X   X  tions associated to the respective normalized feature vectors.

This dual problem can be solved efficiently with a perceptron algorithm based on an incremental subgradient method, of which the bounds on the complexity and achievable margin can be found in (Szedmak et al., 2006).
 Then according to Karush-Kuhn-Tucker theory, W is expressed as:
In practice, MMR works better when the distribu-tion of the training points are symmetrical. So we center the data before normalizing them. If  X 
P sentence sample set { x new feature map is given by  X   X  The similar operation is performed on  X  tain  X   X   X   X  y ( ) To find the pre-image sentence y = f  X  1 ( x ) can be achieved by seeking y between its feature vector  X  f ( x ) . That is (Eq. 8: LSR, Eq. 9: MMR): y t = arg min y t = arg min x , and k (  X 
A proper Y ( x ) can be generated according to a lexicon that contains possible translations for every component (word or phrase) in x . But the size of it will grow exponentially with the length of x , which poses implementation problem for a decoding algo-rithm.
In earlier systems, several heuristic search meth-ods were developed, of which a typical example is Koehn (2004) X  X  beam search decoder for phrase-based models. However, in our case, because of the  X  ( y, y ) item in Eq. 8 and the normalization opera-tion in MMR, neither the expression in Eq. 8 nor of subfunctions each involving feature components sentence, which prevents us doing a straightforward beam search similar to (Koehn, 2004).

To simplify the situation, we restrict the reorder-ing (distortion) of phrases that yield the output sen-tences by only allowing adjacent phrases to ex-change their positions. (The discussion of this strat-egy can be found in (Tillmann, 2004).) We use x and y gin with the i th word and end with the j th. Now, if we go back to the implementation of a beam search, each expansion of the search states (hypotheses) we have x or like state (b) in Fig. 2, where l words translated in the source sentence, and l number of words obtained in the translation.
We assume that if y is a good translation of x , then y we can expect that the squared loss k W  X   X  uct h  X   X  large, for the hypothesis yielding a good translation. According to Eq. 8 and Eq. 9, the hypotheses in the search stacks can thus be reranked with the follow-ing score functions (Eq. 10: LSR, Eq. 11: MMR):
Therefore, to solve the pre-image problem, we just employ the same beam search algorithm as (Koehn, 2004), except we limit the derivation of new hypotheses with the distortion restriction mentioned
Figure 2: Search states with the limited distortion. above. However, our score functions will bring more runtime complexities when compared with tra-ditional probabilistic methods. The time complexity of a naive implementation of the blended n -spectrum string kernel between two sentences s tence. So the score function in Eq. 11 results in an average runtime complexity of O ( mnl the average length of the sentences y set. Note here  X   X  for l plexity of the score function in Eq. 10 will be the same if we pre-compute K  X  1 4.1 Resource Description Baseline System To compare with previous work, we take Pharaoh (Koehn, 2004) as a baseline system, beam size 100). We train a trigram language model with the SRILM toolkit (Stocke, 2002). Whilst, the parameters for the maximum entropy model are de-veloped based on the minimum error rate training method (Och, 2003).

In the following experiments, to facilitate com-parison, each time we train our regression models and the language model and translation model for Pharaoh on a common corpus, and use the same phrase translation table as Pharaoh X  X  to decode our systems. According to our preliminary experiments, with the beam size of 100, the search errors of our systems can be limited within 1.5%.
 Corpora To evaluate our models, we randomly take 12,000 sentences from the French-English por-tion of the 1996 X 2003 Europarl corpus (Koehn, 2005) for scaling-up training, 300 for test (Test), and 300 for the development of Pharaoh (Dev). Some 4k 5084 4039 43k 39k 32.25 31.92 6k 6426 5058 64k 59k 30.81 29.03 8k 7377 5716 85k 79k 29.91 28.94 10k 8252 6339 106k 98k 27.55 27.09 12k 9006 6861 127k 118k 27.19 26.41 characteristics of the corpora are summarized in Ta-ble 1. 4.2 Results formance of the blended n -spectrum string kernel in LSR and MMR using BLEU score, with n increas-ing from 2 to 7. Fig. 3 shows the results. It can be found that the performance becomes stable when n spectrum for LSR, and the 5-spectrum for MMR.
Then we scale up the training set, and compare the performance of our models with Pharaoh in Fig. 4. We can see that the LSR model performs almost as well as Pharaoh, whose differences of BLEU score are within 0.5% when the training set is larger than 6k. But MMR model performs worse than the base-line. With the training set of 12k, it is outperformed by Pharaoh by 3.5%. Although at this stage the main contribution is strated. Comparable performance to previous work is achieved by the LSR model.

But a main problem we face is to scale-up the training set, as in practice the training set for SMT will be much larger than several thousand sentences. (Cortes et al., 2005). By approximating the Gram matrix with a n  X  m ( n  X  m ) low-rank matrix, the time complexity of the matrix inversion opera-the space complexity of O ( nm ) in their algorithm is still too expensive for SMT tasks. Subset selection techniques could give a solution to this problem, of which we will leave the further exploration to future work.
 The authors acknowledge the support of the EU un-der the IST project No. FP6-033917.

