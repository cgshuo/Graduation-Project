 In this paper, we propose a novel and effici ent method for outlier detection, which is an important task in data mining and has been applied to many problems such as fraud detection, intrusion detection, data cleaning and so on [7]. The goal of outlier detection is to find an unusual datum ( outlier ) from a given data set. Although many kinds of notion have been proposed to define an outlier, we consider a datum as an outlier if the leave-one-out density is lower than a given threshold for a set of regions around the datum. The leave-one-out density is a ratio of the number of data inside a region to the volume of the region, in which the focused datum is removed from the o riginal data set. Generally, a leave-one-out like method is time consuming because a learning procedure is repeated N -times, where N is the cardinality of a data set. However, the proposed method enables us to evaluate the leave-one-ou t density efficiently without repeating a learning procedure N -times.

We employ the initial region method proposed in [10], in which a data set is encoded into a Boolean formula and represented as a binary decision diagram. Although a one-class classifier is proposed based on the initial region method in [10], it is not applicable to outlier detection, because the classifier is estimated as an over-approximation of the data set and never classify a datum in the data set as an outlier. We extend the work of [10] to outlier detection by introduc-ing the notion of leave-one-out density and developing an efficient algorithm to evaluate it.

The proposed method is compared to oth er well-known outlier detection meth-ods, the one-class support vector machine [13] and the local outlier factor [4], with both synthetic data sets and realistic data sets. The experimental results indicate that the computation time of the proposed method is shorter than those of the other methods, keeping the outlie r detection accuracy comparable to the other methods.
 The outlier detection problem addressed in this paper is formally defined in Sect. 2. We review the initial region met hod in Sect. 3. In Sect. 4, the outline of the proposed method is first stated, and then, its efficient implementation based on binary decision diagrams is proposed. Section 5 shows experimental results. In Sect. 6, we conclude this paper by discussing limitations and future work. 1.1 Related Work Kutsuna [10] proposed a one-class classifier that over-approximates a training data set. The approximation is done quite efficiently by manipulating a binary decision diagram that is obtained by encoding the training data set. The situ-ation considered in [10] is that both a training data set and a test data set are given: A classifier is first learned from the training dataset, then the test data set is classified by the classifier. It may s eem that we can detect outliers within a data set by using the data set as both the training data set and the test data set simultaneously. However, no datum is detected as an outlier in such a setting, because the classifier is estimated as an over-approximation of the training data set. Therefore, the method in [10] canno t be applied to outlier detection directly.
Sch  X  olkopf et al. [13] extended the support vector machine (SVM) to outlier detection, which was originally invented for binary classification. Their method estimates a hyperplane that separates the origin and a data set with maximum margin, in which the hyperplane can be nonlinear by introducing kernel func-tions. The data that are classified to the origin side are detected as outliers. The SVM has an advantage that various nonlinear hyperplanes are estimated by changing kernel parameters. Some heuristics are proposed to tune kernel pa-rameters, such as [6].

Breunig et al. [4] proposed the local outlier factor (LOF) that is calculated based on the distance to the k -nearest neighbor of each datum and has an ad-vantage that it can detect local outliers, that is, data that are outlying relative to their local neighborhoods. The LOF has been shown to perform very well in realistic problems [12]. An efficient calculation of the k -nearest neighbors is essential in the LOF. Some techni ques are proposed to accelerate the k -nearest neighbors calculation, such as [2]. Let D beadatasetthatincludes N data. The i -th datum in D is denoted by x ( i )  X  R u ( i =1 ,...,N ). We assume that there is no missing value in D and all the data in D are unlabeled. In this paper, we regard a datum as an outlier if the leave-one-out density is lower than a threshold for a set of regions around the datum. The leave-one-out density  X  LOO of the i -th datum is defined as: where D is a u -dimensional region such that x ( i )  X  X  ,#(  X  ) is the cardinality of a set and vol (  X  ) is the volume of the region. The outlier score S of the i -th datum is defined as: where  X  D ( i ) is a set of regions around x ( i ) defined as: A datum is detected as an outlier if the outlier score of the datum is less than a a fixed family, which is defined in Sect. 4.1. In the following sections, we propose an efficient algorithm that enables us to evaluate the outlier score in near linear time with respect to N . In this section, we briefly review the initial region method [10] and define nota-tions. Let H be a u -dimensional hypercube defined as H =[0 , 2 m ) u ,where m is an H for every x ( i ) in D .Anexampleof  X  is given in [10] as a simple scaling function. The neighborhood function , which is defined as ( z ):=[ z 1 , z 1 +1)  X  ...  X  [ z u , z u + 1), returns a u -dimensional unit hypercube that subsumes z =  X  ( x ), where  X  is the floor function. The initial region G is a u -dimensional region in-side H that subsumes all the projected data, which is defined as: Forexample,weconsideradatasetin R 2 .Weset m = 3, then the data set is projected into H =[0 , 2 3 ) 2 by  X  . The projected data z are shown as x-marks and the initial region G is shown as the gray region in Fig. 1(a).

The initial region G is expressed as a Boolean function by using the coding function CodeZ . CodeZ first truncates each element of z to an integer, and then, code them into a logical formula in the manner of an unsigned-integer-type coding. The set of Boolean variables B := { b ij | i =1 ,...,u ; j =1 ,...,m } is used to code z ,where b i 1 and b im represent the most and the least significant bit of the i -th element of z , respectively. The initial Boolean function F is given as a disjunction of logical formulas such as: It is shown that the initial Boolean function F is informational equivalent of the initial region G [11]. Let R be a function that decodes a Boolean function defined on B into the corresponding u -dimensional region. In particular, R (1) = H and R ( F )= G . Binary decision diagrams (BDDs) [5] are used to efficiently construct and represent the initial Boolean function F . The order of Boolean variables is square brackets can be in arbitrary order, on constructing F as a BDD. For example, Fig. 1(b) shows a BDD that represents the initial Boolean formula F that is obtained from the data set in Fig. 1(a). In Fig. 1(b), square nodes, ellipsoidal nodes and double-squared nodes are referred to as terminal nodes, variable nodes and function nodes, respectively. Boolean variables that variable nodes represent are on the left side. Solid lines, dashed lines and dotted lines are true edges, false edges and comple ment edges, respectively. A path from a function node to the terminal 1 corresponds to a conjunction of literals. If the path contains an even number of complement edges, the conjunction is included in the function. 4.1 Outline In the proposed method, we calculate the leave-one-out density based on the normalized data z ( i ) =  X  x ( i ) as follows: where C is a region such that z ( i )  X  X  . The outlier score is given as: where  X  C ( i ) is a set of hypercubes defined as: where  X  b i j are assignments of Boolean variables that is obtained by coding z ( i ) with CodeZ . For example, the datum in the region [5 , 6)  X  [2 , 3) in Fig. 1(a) is this datum is derived as which are shown as bold squares in Fig. 2. z
We assume that there is no duplicate in D , that is, at least one of the attributes region G so that each datum in D is allocated in a distinct unit hypercube by setting m large enough and using an appropriate example normalizer. In this case, the number of data inside a regio n can be calculated as the volume of the region unless the boundary of the region goes across a unit hypercube in which a datum exists. Therefore, the leave-one-out density defined as (1) can be calculated based on the initial region as follows for C X   X  C ( i ): where G LOO is the leave-one-out region defined as: For example, Fig. 2 illustrates the calculation of the leave-one-out density for the datum shown as the x-mark.
 4.2 BDD Based Implementation The Number of Minterms Calculation. A minterm is a conjunction of Boolean variables in which each Boolean variable in the domain appears once. Let # A be a function that returns the number of minterms of a Boolean function on the assumption that the domain of the function is A . For example, # a (1) = 2 and # { a,b } (1) = 4 where a and b are Boolean variables.
 Lemma 1. For a Boolean formula A that is defined on B , it holds that: Proof. Since a minterm represents a unit hypercube in the initial region method, the number of minterms equals to the volume of the region that A represents.
Let N +  X  be a Boolean function that node  X  in a BDD represents being con-nected with a non-complement edge. Also, let N  X   X  be a Boolean function being connected with a complement edge. Both of N +  X  and N  X   X  represent regions in-side H . For example, Fig. 3 shows R N  X  E and R N + G where E and G are nodes in Fig. 1(b). It is possible to efficiently calculate the number of minterms of N +  X  and N  X   X  for each node  X  in a BDD in a depth-first manner [15]. For example, Table 1 shows the number of minterms of N +  X  and N  X   X  for each node in Fig. 1(b). We can see that # B N  X  E and # B N + G are equal to the volumes of regions which are shown in Fig. 3. The Leave-One-Out Density Calculation. Because of the fact that z ( i )  X  G and z ( i )  X  X  X   X  C ( i ), it is derived from (4) and (5) that the following equa-tion holds for C X   X  C ( i ):
By replacing C in (6) with R ( L l ( i )), the following equation is derived. From Lemma 1, (7) is transformed as follows: Lemma 2. In a BDD that represents F ,let  X  i,l be the node that can be reached from the function node through the path defined by L l ( i ). Let c be the number of complement edges on the path. Then, it holds that: Proof. F  X L l ( i )meansthat l  X  u Boolean variables that appear in L l ( i )are fixed to specific values in F . On the other hand, N +  X  Boolean variables in L l ( i )smoothed 1 if c is even (odd). Therefore, the number of minterms of N +  X  Theorem 1. The leave-one-out density  X  LOO is evaluated based on a BDD that represents the initial Boolean function F as follows: Proof. It follows from (8) and Lemma 2 immediately.
 It is worth mentioning that we can evaluate the leave-one-out density from the initial Boolean function F straightforwardly without any leave-one-out opera-tion by using Theorem 1. For example, we consider a data set in Fig. 1(a) and the datum in [5 , 6)  X  [2 , 3). The path of the datum is F  X  A  X  B  X  E  X  K  X  Q  X  S  X  1 in Fig. 1(b), where  X  and  X  mean non-complement and complement edges, re-spectively. The leave-one-out density of the datum is calculated from Theorem 1 and Table 1 as follows: We can see that these values are equal to those in Fig. 2. 4.3 The Proposed Algorithm and Computational Complexity We propose Algorithm 1 that calculates the outlier score of each datum in D .In Algorithm 1, the time complexity of constructing the initial Boolean function F is approximately O ( MN ), where M is the number of nodes of the created BDD, because logical operations between BDDs are practically almost linear to the size of the BDDs [3]. The size of the created B DD depends on the characteristics of the data set and can be exponentially large in the worst case, but, it is compact for realistic data sets used in our experime nts. The time complexity of calculating the number of minterms is O ( M ) as mentioned in Sect. 4.2. The time complexity of calculating the outlier score is O ( muN ) because the depth of the BDD is mu . Consequently, the time complexity of Algorithm 1 is O (( M + mu ) N ). Therefore, the proposed method can deal with a large data set efficiently unless the number of Boolean variables and the created BDD are intractably huge.
 Algorithm 1: The outlier score calculation.
 4.4 Dealing with Categorical Attributes The proposed method can be extended in order to deal with a data set that consists of both continuous attributes and categorical attributes. Let y ( i ) be a vector of categorical attributes of the i -th datum. We extend t he leave-one-out density defined as (1) as follows: Then, the outlier score defined as (2) ca n be evaluated very efficiently in the same manner as mentioned in the previous sections. The details are skipped because of the page limit. We compare the proposed method with existing methods, the one-class support vector machine (OCSVM) and the local outlier factor (LOF). The proposed method is referred to as ODBDD. We implemented ODBDD as a C program with the help of CUDD [14]. The ksvm function in the kernlab package [9] is used for OCSVM and the lofactor function in the DMwR package [16] is used for LOF. The parameter m that defines the size of the hypercube H is fixed to m = 16 in ODBDD. In OCSVM, the Gaussian kernel is used and the kernel parameter  X  is set to one of the 10%, 50% and 90% quantiles of the distance parameter is fixed to =0 . 1 in OCSVM. In LOF, the number of neighbors is set to either k =10or k = 50. In OCSVM and LOF, continuous attributes are scaled and categorical attributes are coded by using dummy variables. The accuracy is evaluated in terms of the area under an ROC curve ( AUC )[8].The experiment was performed on a Microsoft Windows 7 machine with an Intel Core i7 CPU (3.20 GHz) and 64 GB RAM. 5.1 Evaluation with Synthetic Data Sets Ten data set is a synthetic data set that consists of two continuous attributes and no categorical attribute. The 95 % data of Ten data set distributes inside the shape  X 10 X  randomly. The remaining 5 % data distributes outside randomly, which are regarded as outliers. The number of data is set to 10 3 ,10 4 ,10 5 or 10 6 . An example of Ten-10 3 data set is shown in the left side of Fig. 4.
Table 2 shows the mean and the standard deviation of computation time over 10 random trials for Ten data set. We can see that the computation time of the proposed method increases moderat ely compared to the other methods.
Table 3 shows the mean and the standard deviation of AUC values over 10 random trials for Ten data set. From Table 3, the accuracy of the proposed method is comparable to those of the other methods. For example, the outliers detected by ODBDD is shown in the right side of Fig. 4, in which the top 5 % of the data are detected as outliers based on the outlier score of ODBDD. 5.2 Evaluation with Realistic Data Sets We use seven data sets from UCI machine learning repository [1] as shown in Table 4, where N a is the size of the original data set. All of these data sets are originally arranged for the classification task. In order to apply these data sets to the evaluation of outlier detection algorithms, we randomly picked out data from each data set to generate a new data set as follows: 1) Pick out all the data whose class are C m where C m is the class of the maximum data size. Let N m be thenumberofdatathatbelongtoclass C m .2)Pickout N o = round (0 . 01 N m ) data randomly from the remaining data set, which are regarded as outliers.
Table 5 shows the mean and the standard deviation of computation time over 10 random trials for UCI data sets. The computation time varies drastically depending on the size of the data set in both OCSVM and LOF. On the other hand, ODBDD works quite fast for all of the data sets.
Table 6 shows the mean and the standard deviation of AUC values over 10 random trials. Although some results of ODBDD are not as good as the best result of the other methods, ODBDD achieves similar accuracy to the others. In this work, we proposed a novel approach for outlier detection. A score of being an outlier is defined based on the leave-one-out density, which is evaluated very efficiently by processing a binary d ecision diagram that represents a data set in a logical formula. The proposed method can deal with a large data set efficiently, because the time complexi ty is near linear unless the created BDD gets intractably huge.

This work can be extended in several ways. First, the region set  X  D ( i )isen-riched by using various normalizers. Th en, the accuracy of ou tlier detection is expected to improve. A simple approach t o generate various normalizers is to incorporate a random rotation into a normalizer. Another extension is to em-ploy nonlinear normalizers. If we use n onlinear normalizers, a hypercube in a projected space corresponds to a nonlinea r region in the original space, which may lead to more precise outlier detection.

The proposed method may suffer from the curse of dimensionality when a data set has many attributes and the number of data is not enough, because the leave-one-out density is zero in almost e very subregion of the whole hypercube in such a situation. A simple solution is to embed some dimension reduction method into a normalizer.

Although we have conducted experiments with several data sets mainly to compare the proposed method to other methods, it is necessary to apply the proposed method to other real world problems in order to examine the practical usefulness of the proposed method and reveal problems to tackle in future.
