 Entity linking links entity mentions in text to the corre-sponding entities in a knowledge base (KB) and has many applications in both open domain and specific domains. For example, in the recruitment domain, linking employer names in job postings or resumes to entities in an employer KB is very important to many business applications. In this paper, we focus on this employer name normalization task, which has several unique challenges: handling employer names from both job postings and resumes, leveraging the correspond-ing location context, and handling name variations, irrele-vant input data, and noises in the KB. We present a sys-tem called CompanyDepot which contains a machine learn-ing based approach CompanyDepot-ML and a heuristic ap-proach CompanyDepot-H to address these challenges in three steps: (1) searching for candidate entities based on a cus-tomized search engine for the KB; (2) ranking the candi-date entities using learning-to-rank methods or heuristics; and (3) validating the top-ranked entity via binary classifi-cation or heuristics. While CompanyDepot-ML shows better extendability and flexibility, CompanyDepot-H serves as a strong baseline and useful way to collect training data for CompanyDepot-ML. The proposed system achieves 2.5%-21.4% higher coverage at the same precision level compared to an existing system used at CareerBuilder over multiple real-world datasets. Applying the system to a similar task of academic institution name normalization further shows the generalization ability of the method.
 employer name normalization; entity linking; named entity disambiguation; learning to rank
Entity linking links entity mentions in text to the corre-sponding entities in a knowledge base (KB) and has many applications such as information extraction and content anal-ysis [22], in both open domain and specific domains. For example, in the recruitment domain, linking the company name on a user X  X  professional profile position to a company entity is very important to many business applications [24].
The mission of CareerBuilder 1 , a company providing hu-man capital solutions globally, is to empower employment. Its online career site contains over 45 million candidate re-sumes and 1.6 million jobs. One of its products is Supply &amp; Demand which performs big data analytics on the supply and demand of jobs. For example, on one hand it computes the demand (i.e., the number of open positions) for specific jobs and the top employers with such demand; on the other hand, it also computes the supply (i.e., the number of candi-dates) for specific jobs and the top places such supply come from. Such analytics are achievable by parsing and analyz-ing a large number of job postings and candidate resumes related to specific jobs. To enable accurate computation, besides a KB of employers [15] we need a system that can do employer name normalization, i.e., linking the employer names in the job postings and the resumes to the employer entities in the KB. In this paper, we focus on this employer name normalization task, which is illustrated in Figure 1.
Some key challenges of the task are summarized below based on the applications at CareerBuilder. An effective employer name normalization system should be able to: 1. Handle employer names from both job postings and re-2. Leverage the location context. Jobs are often associ-3. Handle name variations. An employer entity can have 4. Handle irrelevant or unlinkable input data. There are http://www.careerbuilder.com/ 5. Handle noises in the KB. The employer KB used in
Some of these challenges (3 and 4) also exist in the typ-ical entity linking task [8, 22]. Yet, other challenges (1, 2, and 5) are rather unique in the recruitment domain. This paper describes a system called CompanyDepot which aims to address all these challenges. The system takes an em-ployer name and its associated location from job postings or resumes as input and normalizes it into entities in an employer KB. We develop two approaches, one based on machine learning techniques and the other based on heuris-tics, which address the proposed task in three steps: en-tity retrieval, reranking, and validation. The retrieval step searches for candidate entities based on a customized search engine for the KB, the reranking step ranks the candidate entities based on learning-to-rank methods or heuristics, and the validation step validates the top-ranked entity based on binary classification or heuristics. We experiment with the proposed system over multiple real-world datasets and compare its performance with an existing employer name normalization system used at CareerBuilder. Our system achieves 2.5%-21.4% higher coverage at the same precision level. We also apply it to a similar task of academic institu-tion name normalization and compare its performance with a state-of-the-art method. The experimental results show the effectiveness, robustness, and generalization ability of our system. Its fast query response time allows the system to be applied to online employer name normalization.
The major contributions of this paper are as follows:
The rest of the paper is organized as follows. Section 2 discusses the related work. Next, we provide the problem definition in Section 3 and some preliminaries in Section 4. Then Section 5 describes the system and Section 6 details the experiments. Finally, we conclude the paper in Section 7.
Entity linking [22], also called named entity disambigua-tion (NED) or named entity normalization (NEN), which links entity mentions in text to the corresponding entities in a KB, has attracted much research effort since the avail-ability of large KBs such as Wikipedia 2 and Freebase [2]. It is a key step to understand and annotate the raw and noisy data in many applications.

A comprehensive survey of the issues and methods about entity linking is provided in [22]. As it summarized, a typ-ical entity linking system has three modules: candidate en-tity generation, candidate entity ranking, and unlinkable mention prediction. To generate candidate entities, many methods have been proposed, e.g., building a table of map-pings from surface forms to the possible entities [26, 28], extracted expanded forms for entity mentions from the local document [25, 27, 28], and finding relevant entity pages via search engines (such as Google API [8] and Wikipedia search engine [26]). To rank the candidate entities, many systems used supervised ranking methods and the most popular two are binary classification [24, 26, 27] and learning to rank [8, 21, 25, 27, 28]. To predict unlinkable mentions, many meth-ods used binary classification to decide whether to output the top-ranked entity or NIL (Not-In-Lexicon) as the final result [21, 25, 27, 28], while some methods integrated NIL prediction into the learning-to-rank process [8].

The employer name normalization task proposed in this paper can be viewed as a general entity linking problem, yet it differs from the traditional entity linking task [22] in three aspects: (1) different data sources: entity linking takes an entity mention recognized in text as input while our task takes an employer name parsed from a semi-structured job posting or resume as input; (2) different contexts: in en-tity linking the document where an entity mention appears serves as the context, while in our task the location associ-ated with an employer name extracted from the job posting or the resume is used as the context; (3) different KBs: en-tity linking often focuses on a global KB and multiple types of entities while our task is concerned with a domain-specific KB and a single type of entities.

Because of these differences, the employer name normal-ization task has unique challenges such as handling the lo-cation context and noises in the KB. Our system adapts the three-module framework used in the entity linking systems. https://www.wikipedia.org/ First, we build our own search engine customized for the KB, which enables us to efficiently retrieve relevant enti-ties by tuning the suitable search algorithms. Then we use learning-to-rank methods to rank the candidate entities and a binary classifier to validate the top-ranked entity. Com-pared to the entity linking systems, our system has a novel candidate entity generation process. We also design novel features (such as query complexity and location matching) for learning based entity ranking and validation. Moreover, we develop a heuristic approach which serves as a practical and strong baseline for the machine learning based approach.
Our work is also related to a set of domain-specific name normalization applications. For example, within the same recruitment domain, Yan et al. described how to normalize the company name on a LinkedIn member X  X  profile position using social graphs based on binary classification [24]. Al-though their application is within the same domain as ours, a few differences exist: (1) LinkedIn only considers input from member profiles while we need to normalize company names in both job postings and resumes; (2) LinkedIn has the whole profile as the context which makes the important social features derived from member-to-member links avail-able, while we only have the location corresponding to an employer name as the context; (3) LinkedIn has a Typea-head Assist which allows users to select the correct company entity from a suggested list, thus automatically reducing in-put noises and creating training data, which is unfortunately not available in our application and many other applications.
The problem of academic institution name normalization discussed by Jacob et al. [11] is very similar to ours. Instead of an employer KB used in our work, they used a KB of aca-demic institutions. Their method, named sCooL, consists of two steps: retrieval and reranking. The retrieval step is also based on building a search engine for the KB. However, sCooL treated each mapping from a surface form to an en-tity as a document, while our system treats each entity as a document, which leads to very different indexing structures and search processes. The reranking step of sCooL is based purely on heuristics, while we develop a machine learning based approach besides the heuristic approach.

NEMO [13] addressed a related task of extracting and normalizing organization names from PubMed articles. The method first uses multi-layered rule matching to extract en-tity mentions and then leverages unsupervised clustering to identify entity mentions referring to the same entity.
Besides organization name normalization, there are also many other domain-specific name normalization applications, e.g., product item name normalization [3], gene name nor-malization [23], disease name normalization [16], and person name normalization [18]. The main differences of these ap-plications with ours lie in different data sources, contexts, and KBs, which often bring some different challenges.
The task of employer name normalization depends on an employer KB, and a key step in building such domain-specific KBs is de-duplication [15]. Kardes et al. proposed graph-based blocking and clustering strategies for organization en-tity resolution [14]. McNeill et al. proposed a dynamic blocking method to efficiently deduplicate around 5 billion people records [19]. The book by Christen summarizes more methods for general duplicate detection [6].

This paper focuses on the task of employer name nor-malization, for which we performed simple deduplication of the records in an employer database based on their business names. More complex deduplication will be the future work.
Our task is to link the employer names in job postings or resumes to entities in an employer KB, as illustrated in Fig-ure 1. We next describe the problem more formally. The en-tities in the employer KB are denoted by E = { e 1 , e 2 , ..., e The employer names and the associated location contexts ex-tracted from job postings and resumes 3 are denoted by Q = { q and the associated location. In the rest of the paper, we call n the query name and l i the query location. We represent a query location as a triple: l i = ( City i , State i , Country where the city, state, country information could be empty. The problem of employer name normalization can then be summarized as inferring a mapping function f ( q i )  X  e where q i  X  Q and e j  X  E  X  X  N IL } . Note that NIL (Not-In-Lexicon) means the input query does not refer to any known entity in the KB.

As discussed in Section 2, our task can be viewed as a general entity linking problem, but is different from the tra-ditional entity linking task for documents [22] and other domain-specific name normalization tasks [24] in terms of different data sources, contexts, and KBs.
The employer KB we used in this paper is based on a third-party employer database, which contains about 21 million employer records. Each record represents an employer with a set of attributes like business name, location, industry code, and company size. Branches of a company are represented as different records, often with the same business name. The database has a good coverage of employers in US, however, it is noisy and may contain duplicate records for the same employer entity. For example, it contains different records named  X  X nterprise Rent A Car X ,  X  X nterprise Rentacar X , and  X  X nterprise Rent-A-Car Company X  respectively.

To make an employer KB for the employer name normal-ization system, we performed simple deduplication of the above employer database. We merged all the records with the same business name into a single entity (by assuming that two records with the same business name refer to the same employer entity) and kept a list of all the original loca-tions. We also computed the number of the original records merged into an entity as the entity popularity. This resulted in a set of around 18 million employer entities that serves as our final employer KB. Note that the noises still exist in the KB, which needs to be considered in the employer name normalization system.

Note that in this KB we treat the business name of an entity as its normalized form and there are no surface forms
The extraction of the employer names and the associated locations is done by in-house job parsers and resume parsers at CareerBuilder, which is out of the scope of this paper. Figure 2: Architecture of the CompanyDepot sys-tem. associated with it. Yet, the proposed system can also make use of the available surface forms in other KBs [11, 15].
Our system uses Apache Lucene 4 to index and retrieve the employer entities respectively. Lucene is a high-performance text search engine library. A document in Lucene is a set of fields. Each field has a name and a textual value, with semantics about how it is parsed (e.g., tokenized vs. unto-kenized, stopwords kept vs. removed). Lucene scoring first works on fields and then combines the results to return doc-uments, allowing giving different weights to different fields.
Lucene supports a wide variety of query implementations, which can be combined to provide complex querying capa-bilities. For example, it supports the following searches:
All the above searches are used in our system to retrieve relevant employer entities for a given query.
The architecture of our system is shown in Figure 2. Be-fore taking any queries, the system needs to index the em-ployer KB using a Lucene indexer. Once the index is ready, https://lucene.apache.org/ normalized form International Business Machines calibrated name internationalbusinessmachines Table 1: An example document created for an em-ployer entity. the system can take normalization requests. Each request consists of an employer name and its location context (part of or whole location information could be empty). The sys-tem then uses a Lucene searcher to retrieve a list of N em-ployer entities. We then send these N candidate entities to the reranking step, which first generates a feature vec-tor for each entity and then uses either a machine learn-ing based ranking model or some feature-based heuristics to rank them. Finally, the top-ranked entity is sent to the val-idation step to decide whether it is a correct result for the query using either a binary classifier or some heuristics. If it says yes, we output this entity to the user; otherwise, we output NIL. We will next describe each step in more detail.
The purpose of this step is to index all the entities in the employer KB so that they can be efficiently retrieved in the next step. For each entity, we created a document which contains five fields: id, normalized form, surface forms, cal-ibrated name, and json. An example document is shown in Figure 1. The id field is used to locate an entity. The fields of normalized form and surface forms are used to match the query name in the retrieval step. Distinguishing these two fields allows us to give higher weights to matches in the nor-malized field. The calibrated name field is used to enable better fuzzy match, derived by compact calibration of the normalized form (details in Section 5.1.1). The json field is a stored field, which is used to get all the detailed information about an entity, e.g., locations and popularity, for comput-ing features used for the machine learning techniques.
Then all these documents are sent to the Lucene indexer to build an index, which is then used by the Lucene searcher in the retrieval step.
Proper parsing and pre-processing of text is an impor-tant step in building effective search engines [7]. Inspired by this, we propose to calibrate entity names and query names to make them more comparable. The calibration of an em-ployer name works as follows: 1. Convert the name to lowercase, and replace  X  X  X  X  with 2. Convert all the non-alphanumeric characters to space.
We always include the normalized form of an entity as one of its surface forms. 3. Remove stop-phrases (e.g.,  X  X vt ltd X  and  X  X  l c X ) and 4. Expand commonly used abbreviations, e.g.,  X  X tr X  -&gt; 5. (Optionally) remove all spaces in the name.

Table 2 shows some calibration examples. Considering that a query name is often short while the official name of an entity could be tedious, comparing two calibrated forms often enables more accurate match detection than compar-ing the two original names. The purpose of this step is to efficiently generate a pool of N candidate entities so that the correct result is included. To generate this pool, we first use Lucene X  X  powerful querying capabilities to retrieve a large set of entities that are possibly relevant to the query name, and then apply several filters to this entity set and keep only the most likely correct results in the pool.

Specifically, we first search for the query name against all the entity documents via an aggregated search in Lucene which combines (1) keyword searches in the normalized form and surface forms fields; (2) fuzzy searches in the surface forms and calibrated name fields; and (3) phrase searches in the surface forms field. After obtaining top N N =1000) entities from the Lucene searcher, we then gener-ate the pool of candidate entities as follows:
The resulted pool contains N = N 1 + N 2 + N 3 + N 4 candi-date entities, which will be reranked in the next step. Note that the query location is not used in this step because we believe it is more helpful in ranking rather than generating candidate entities.
The purpose of this step is to effectively rerank the N candidate entities obtained in the previous step so that the correct result is ranked top.

We use supervised learning-to-rank methods to automat-ically build the ranking model based on training data [17]. A key to the success of such methods is the features used, so next we describe the features used in our system.
For each (query, candidate entity) pair, we generate a vec-tor of features, which can be grouped into three categories:
A full list of the features is shown in Table 3. In Section 6, we will do feature ablation tests to see how each feature group contributes to the learning performance.
Based on the above features, our system uses a listwise learning-to-rank method, coordinate ascent [20], to rerank the candidate employer entities. Coordinate ascent can di-rectly optimize any user specified ranking measure, by up-dating one parameter at a time while holding other param-eters fixed. It has been shown to provide superior perfor-mance compared to some other learning-to-rank models [4]. Specifically, we use the coordinate ascent implementation provided in the RankLib 6 library for our experiments.
Before training and testing, we normalize each feature by its mean and standard deviation. Since only the top-ranked entity matters in our task, we choose P@1 to optimize on the training data. To increase the chances of finding a global solution, we use 10 random restarts. For other parameters, the default values provided in RankLib are used. http://sourceforge.net/p/lemur/wiki/RankLib/ The output of the reranking step is a ranked list of the N candidate entities. Yet, only the top-ranked candidate entity will be sent to the next validation step.
The purpose of this step is to validate the top-ranked re-sult so that either a correct result or NIL is sent to the user.
We use a binary classifier to do the validation. The fea-tures used for this classification include the features derived in Section 5.3.1 as well as the score output of the learning-to-rank method. We used LibSVM [5] as our binary classifier. In the future, we could add more features indicating NIL such as how similar the top N candidate entities are.
The supervised learning methods above cannot be applied without training data. For the employer name normaliza-tion task discussed in this paper, there is no off-the-shelf ground-truth data available. Therefore, we also develop a heuristic method within the system framework, which does not require training data.

The heuristic method shares the same indexing and re-trieval steps as the machine learning based approach while having a different process in the reranking and validation steps. In the reranking step, the heuristic method ranks the candidate entities based on the following three feature values (which are also listed in Table 3):
In the validation step, the heuristic method simply out-puts the top-ranked entity and outputs s 1 as its score.
As will be shown in Section 6, this heuristic method is used to collect manual labels as our training data for the machine learning based approach and serves as a strong baseline com-pared to other methods.
The Lucene search engine is also used in sCooL [11] to re-trieve the candidate entities for academic institution name normalization. However, the index structure in our system is different from the one used in sCooL. While sCooL treats each mapping from a surface form to a normalized form as a document, our system treats each entity as a document. This leads to several benefits. First, fewer documents need to be indexed, which often results in more efficient retrieval. Second, the documents returned by the Lucene searcher di-rectly correspond to a set of entities, which does not need an extra step to compute a list of entities from the returned mappings. In addition, more information of an entity (e.g., the normalized form together with all the surface forms) can be used for computing the matching between the entity and the query in the retrieval process.

Ranking candidate entities based on learning to rank and validating the top-ranked entity based on binary classifi-cation have been shown to be effective choices in previ-ous research [21, 25, 27, 28]. We derived similar features (such as name string comparison and entity popularity) that have been shown to be effective, as well as novel features (such as query complexity and location matching) designed for the employer name normalization task. For learning to rank, we choose the coordinate ascent algorithm [20] which can directly optimize the P@1 metric we care most in our task, instead of the more popularly used ranking SVM algo-rithm [12] in previous work [21, 25, 27, 28]. Our preliminary experiments suggest that coordinate ascent gives better per-formance than ranking SVM for our task.
In this section, we conduct experiments to answer the fol-lowing questions: (1) How do the proposed methods com-pare to the existing employer name normalization systems used at CareerBuilder? (2) How do the proposed methods compare to the state-of-the-art methods on similar tasks? (3) What are the effects of different feature groups on the performance of the machine learning based approach?
To evaluate the performance of the proposed employer name normalization system CompanyDepot, we sampled two resume datasets and two job datasets from real applications at CareerBuilder. The two resume datasets (RDB, EDGE) Dataset #Queries %Country %US %State EDGE 1093 97.3% 45.3% 20.8% Table 4: Statistics about the job and resume datasets (%Country means the percentage of queries with country specified, %US means the percentage of queries with country=US when country is spec-ified, %State means the percentage of queries with state specified). are sampled from two resume databases respectively. The two job datasets are sampled from a database of job postings based on a legacy normalization system: JOB1 (JOB2) was sampled from cases for which the legacy system returned some (no) result at some time. The datasets were sam-pled using weighted random sampling techniques [9] so that queries that are more frequent are more likely to be cho-sen. We believe this sampling strategy would enable more realistic evaluation, better reflecting the overall impact on user experience. The statistics of the datasets are shown in Table 4. We can see that different datasets have different characteristics in terms of the percentage of queries with lo-cation specified and the percentage of international queries.
After the query sets were ready, we generated results us-ing three systems: CompanyDepot-H, Legacy, and WSer-vice, which will be described in Section 6.3 in more detail. Each system generated a single result or NIL for each query. Each result has information such as business name, location, company size, and industry.

We then asked a group of raters at CareerBuilder to judge the correctness of the results. We showed each (query, re-sult) pair in a random order to the rater, and asked the rater to choose from the following scoring options:
Based on the above scoring options, we consider a result as a correct result if it gets score &gt; =1.
To understand the generalization ability of CompanyDe-pot, we apply it to a similar task of academic institution name normalization [11]. We used the same five datasets that are used in the previous work: .
To evaluate the performance of different employer name normalization systems, we used the same precision and cov-erage metrics as used in [11, 24]. For a set of queries, suppose a normalization system returns I c correct results, I w wrong results, and I n null results (i.e., NIL), then the following metrics are computed:
Besides the above two metrics, we also add the following two metrics that serve as a combination of precision and coverage to show the overall performance of the system: F 1 = 2  X  P recision  X  Coverage/ ( P recision + Coverage )
The success rate of a system indicates how likely the sys-tem succeeds to generate a correct result, which is important for many applications at CareerBuilder.
The following systems are compared in the experiments:
Note that coverage here is not the same as recall, because the computation of recall requires that all the true answers of a query (which are very hard to obtain) are known, while the computation of coverage only checks whether a non-null result is returned by the system. Table 5: Training and test datasets for CompanyDepot-ML.
Precision Figure 3: Performance comparison on the job and resume datasets. We compared the results of four methods, CompanyDepot-ML, CompanyDepot-H, Legacy, and WService, on the job and resume datasets. For the two CompanyDepot meth-ods (CompanyDepot-H and CompanyDepot-ML), the out-put contains a confidence score along with the result. By varying the threshold on the confidence score, we can plot a precision-coverage curve. For Legacy and WService, how-ever, the confidence score is not available, so we can only derive a single precision and coverage value.

Figure 3 shows the precision-coverage values of the four methods. We can see that CompanyDepot-ML has the best performance over three datasets (RDB, JOB1, and JOB2), while WService performs best on the EDGE dataset. More-over, CompanyDepot-ML achieves 2.5%, 14.2%, and 21.4% higher coverage at the same precision level compared to Legacy over JOB1, RDB, and EDGE respectively. Figure 4: Performance comparison on the EDGE-US dataset (which contains all the US queries in the EDGE dataset).

As shown in Table 4, the EDGE dataset contains a larger percentage of international queries (smaller percentage of US queries, 45.3%) compared to the other datasets. Recall that compared to the KB used by WService, the KB used by CompanyDepot-ML, CompanyDepot-H, and Legacy has a rather poor coverage on international employers. This could be the main reason why these three systems have worse performance than WService on the EDGE dataset. To verify this conjecture, we derived a subset of EDGE called EDGE-US which contains only its US queries. On this subset, we observed that these three systems have better performance than WService, as shown in Figure 4.

The RDB dataset contains a larger percentage of queries with empty country information (smaller percentage of queries with country, 58.5%). Recall that WService requires a coun-try parameter to return any result. This is why WService has a low coverage (55.9%) compared to other systems. The JOB1 dataset contains the cleanest queries, all about US employers that proactively published job postings. There-fore, all the four systems show the best performance on this dataset compared to on other datasets.

In contrast, the JOB2 dataset contains a set of very hard queries. We can see that both Legacy and WService have very bad performance. Legacy has a low coverage (18%) be-cause the dataset was sampled from the cases that Legacy failed to normalize at some time (which may have been cor-rected). WService has a low precision (46%), mostly because it puts too much weight on location match as we found many wrong results returned by WService have the same location as the query location (note that all the queries in this dataset have the state specified).

By comparing the two CompanyDepot methods closely, we can see that CompanyDepot-ML performs better on three datasets: RDB, EDGE, and JOB2. While CompanyDepot-H cannot achieve a precision of 90% on these three datasets, CompanyDepot-ML can achieve a 90% precision at a 40% or higher coverage. The main reason is that CompanyDepot-H mostly depends on a single feature while CompanyDepot-ML is able to use all the available features for entity ranking and validation. As shown in Section 5.5, the final score of CompanyDepot-H is the Jaccard similarity between the 4-gram sets of the compactly calibrated versions of the query name and the entity normalized form. However, even with perfect similarity (i.e., score=1), a result can be wrong. In contrast, CompanyDepot-ML can learn these situations based on many other features, e.g., the popularity of the en-Table 6: Performance comparison on the aca-demic institution datasets (Cov means coverage; Pre means precision; SR means success rate). tity or whether a mapping from the query name to the entity exists in the mapping table. This shows that the machine learning based approach is quite promising. It is also easier to extend and improve, because more features can be easily added and automatically integrated, while the optimization of the heuristic approach depends on pure manual effort. We compared the results of two systems, CompanyDepot-H and sCooL, on the academic institution datasets. The two systems used the same KB derived in [11] for normalization and the same classifier to detect K-12 schools. A new set of stop-words (e.g.,  X  X ollege X ,  X  X niversity X ,  X  X chool X ) is used in CompanyDepot-H for calibration of academic institution names.

Table 6 shows the performance of these two systems. We can see that CompanyDepot-H achieves better success rate and F1 score over all the five datasets. Specifically, it achieves comparable or higher precisions at better coverages. Note that CompanyDepot-ML was not deployed here as our er-ror analysis revealed that the main room to improve is the quality of the underlying KB.
We would like to understand how each of the following feature groups contributes to the learning performance: (1) query features; (2) query-entity features; and (3) entity fea-tures. Figure 5 shows the performance of CompanyDepot-ML over the EDGE dataset based on different feature groups. We can see that the best performance is achieved using all feature groups, which shows that all the feature groups are contributing to the learning process. Considering a single feature group, query-entity features is the most effective one. Without this feature group, the performance is very bad. Next, we compare the effects of the query feature group and the entity feature group. We see that removing the group of entity features (green curve) from all feature groups leads to a higher decrease in performance than removing the group of query features (red curve). This indicates that the entity feature group is more effective than the query feature group for our task, especially considering the noisy KB which con-tains entities with varying degrees of quality.
The job and resume datasets used in this paper were orig-inally built for the purpose of creating benchmark datasets for comparing the performance of CompanyDepot-H, Legacy, and WService. For each query, we received up to three re-Figure 5: Effects of feature groups on performance of CompanyDepot-ML over the EDGE dataset. sults in total, and collected manual judgments about them. With such limited labeled data, the correctness of many re-sults returned by CompanyDepot-ML are unknown, which however were counted as wrong results in our evaluation. Although this setting brings limitations on the training and test data for CompanyDepot-ML, the experimental results still show that the machine learning based approach is quite promising. The performance numbers we computed for it are actually the lower bounds of the performance given bet-ter training and test data. While the machine learning based approach has advantages such as extendability and flexibility, the heuristic approach has its own advantage of being a strong baseline and eliciting ground-truth data for CompanyDepot-ML. Both the two methods have query re-sponse time within a second, showing the feasibility of ap-plying the system to online employer name normalization.
In this paper, we proposed a novel employer name normal-ization task which takes an employer name and its associated location from job postings or resumes as input and normal-izes it into entities in an employer KB. We then presented a system called CompanyDepot which contains a machine learning based approach CompanyDepot-ML and a heuris-tic approach CompanyDepot-H to address the task in three steps: entity retrieval, reranking, and validation. We com-pared its performance with existing employer name normal-ization systems used at CareerBuilder, applied it to a sim-ilar task of academic institution name normalization, and compared its performance with a state-of-the-art method. The experimental results over multiple real-world datasets showed the effectiveness, robustness, and generalization abil-ity of our system.

In our future work, we plan to use more contextual infor-mation (e.g., employer address and website) if available. We also intend to develop more features to handle entity quality and query segmentation. Another branch of potential work is to improve the quality and coverage of the employer KB.
We would like to thank Kyle Diemer for his help with building the benchmark datasets. We would also like to thank Yun Zhu for his helpful discussions about the project. [1] 4ICU. Top universities in the united kingdom. [2] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [3] A. Borkovsky. Item name normalization, Apr. 29 2003. [4] R. Busa-Fekete, G. Szarvas, T.  X  Eltet  X o, and B. K  X egl. [5] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [6] P. Christen. Data Matching: Concepts and Techniques [7] B. Croft, D. Metzler, and T. Strohman. Search [8] M. Dredze, P. McNamee, D. Rao, A. Gerber, and [9] P. S. Efraimidis and P. G. Spirakis. Weighted random [10] Guardian. Universities. http://www.theguardian.com/ [11] F. Jacob, F. Javed, M. Zhao, and M. Mcnair. sCooL: [12] T. Joachims. Optimizing search engines using [13] S. Jonnalagadda and P. Topham. NEMO: Extraction [14] H. Kardes, D. Konidena, S. Agrawal, M. Huff, and [15] M. Kejriwal, Q. Liu, F. Jacob, and F. Javed. A [16] R. Leaman, R. I. Dogan, and Z. Lu. Dnorm: disease [17] T.-Y. Liu. Learning to rank for information retrieval. [18] W. Magdy, K. Darwish, O. Emam, and H. Hassan. [19] W. P. McNeill, H. Kardes, and A. Borthwick.
 [20] D. Metzler and W. Bruce Croft. Linear feature-based [21] L. Ratinov, D. Roth, D. Downey, and M. Anderson. [22] W. Shen, J. Wang, and J. Han. Entity linking with a [23] J. Wermter, K. Tomanek, and U. Hahn.
 [24] B. Yan, L. Bajaj, and A. Bhasin. Entity resolution [25] W. Zhang, Y. C. Sim, J. Su, and C. L. Tan. Entity [26] W. Zhang, J. Su, C. L. Tan, and W. T. Wang. Entity [27] W. Zhang, C. L. Tan, Y. C. Sim, and J. Su. NUS-I2R: [28] Z. Zheng, F. Li, M. Huang, and X. Zhu. Learning to
