 The problem of reconstructing a surface from a set of points frequently arises in computer graphics. Numerous methods of sampling physical surfaces are now available, including laser scanners, optical triangulation systems and mechanical probing methods. Inferring a surface from millions of points sampled with noise is a non-trivial task however, for which a variety of methods have been proposed. The class of implicit or level set surface representations is a rather large one, however other methods have also been suggested  X  for a review see [1]. The implicit surface methods closest to the present work are those that construct the implicit using regularised function approximation [2], such as the  X  X ariational Implicits X  of Turk and O X  X rien [3], which produce excellent results, but at a cubic computational fitting cost in the number of points. The effectiveness of this type of approach is undisputed however, and has led researchers to look for ways to overcome the computational problems. Two main options have emerged.
 The first approach uses compactly supported kernel functions (we define and discuss kernel functions in Section 2), leading to fast algorithms that are easy to implement [4]. Unfortunately however these methods are suitable for benign data sets only. As noted in [5], compactly supported across holes X . A similar conclusion was reached in [6] which states that local processing methods are  X  X ore sensitive to the quality of input data [than] approximation and interpolation techniques based on globally-supported radial basis functions X   X  a conclusion corroborated by the results within a different paper from the same group [7]. The second means of overcoming the aforementioned computational problem does not suffer from these problems however, as demonstrated by the FastRBF TM algorithm [5], which uses the the Fast Multipole Method (FMM) [8] to overcome the computational problems of non-compactly supported kernels. The resulting method is non-trivial to implement however and to date exists only in the proprietary FastRBF TM package.
 We believe that by applying them in a different manner, compactly supported basis functions can lead to high quality results, and the present work is an attempt to bring the reader to the same conclusion. In Section 3 we introduce a new technique for regularising such basis functions which allows high quality, highly scalable algorithms that are relatively easy to implement. We also show that the approximation can be interpreted as a Gaussian process with modified covariance function. Before doing so however, we present in Section 2 the other main contribution of the present work, which is to show how surface normal vectors can be incorporated directly into the regularised regression framework that is typically used for fitting implicit surfaces, thereby avoiding the problematic approach of constructing  X  X ff-surface X  points for the regression problem. To demonstrate the effectiveness of the method we apply it to various problems in Section 4 before summarising in the final Section 5. Here we discuss the use of regularised regression [2] for the problem of implicit surface fitting. In Section 2.1 we motivate and introduce a clean and direct means of making use of normal vectors. The following Section 2.2 extends on the ideas in Section 2.1 by formally generalising the important representer theorem . The final Section 2.3 discusses the choice of regulariser (and associated kernel function), as well as the associated computational problems that we overcome in Section 3. 2.1 Regression Based Approaches and the Use of Normal Vectors Typically implicit surface has been done by solving a regularised regression problem [5, 4] where the y i are some estimate of the signed distance function at the x i , and f is the embedding function which takes on the value zero on the implicit surface. The norm k f k H is a regulariser which takes on larger values for less  X  X mooth X  functions. We take H to be a reproducing kernel Hilbert space (RKHS) with representer of evaluation (kernel function) k (  X  ,  X  ) , so that we have the reproducing property, f ( x ) =  X  f,k ( x ,  X  )  X  H . The solution to this problem has the form Note as a technical aside that the thin-plate kernel case  X  which we will adopt  X  requires a somewhat more technical interpretatiosn, as it is only conditionally positive definite. We discuss the positive definite case for clarity only, as it is simpler and yet sufficient to demonstrate the ideas involved. to prevent contradictory target values (see e.g. [5]). We now propose more direct method, novel in the context of implicit fitting, which avoids these problems. The approach is suggested by the fact that the normal direction of the implicit surface is given by the gradient of the embedding function  X  thus normal vectors can be incorporated by regression with gradient targets. The function that we seek is the minimiser of: the reproducing property we can solve for the optimal f . A detailed derivation of this procedure is given in [1]. Here we provide only the result, which is that we have to solve for m coefficients  X  i as well as a further md coefficients  X  lj to obtain the optimal solution first argument. 1 The coefficients  X  and  X  l of the solution are found by solving the system given by In summary, minimum norm approximation in an RKHS with gradient target values is optimally we can make a more general statement, which we do briefly in the next sub-Section. 2.2 The Representer Theorem with Linear Operators The representer theorem, much celebrated in the Machine Learning community, says that the function minimising an RKHS norm along with some penalties associated with the function value at various points (as in Equation 1 for example) is a sum of kernel functions at those points (as in Equation 2). As we saw in the previous section however, if gradients also appear in the risk function to be minimised, then gradients of the kernel function appear in the optimal solution. We now make a more general statement  X  the case in the previous section corresponds to the following if we choose the linear operators L i (which we define shortly) as either identities or partial derivatives. The theorem is a generalisation of [9] (using the same proof idea) with equivalence if we choose all L i to be identity operators. The case of general linear operators was in fact dealt with already in [2] The following theorem therefore combines the two frameworks: Theorem 1 Denote by X a non-empty set, by k a reproducing kernel with reproducing kernel Hilbert space H , by  X  a strictly monotonic increasing real-valued function on [0 ,  X  ) , by c : R m  X  R  X  X  X  X  an arbitrary cost function, and by L 1 ,...L m a set of linear operators H X  X  . Each minimiser f  X  X  of the regularised risk functional admits the form where k x , k (  X  , x ) and L  X  i denotes the adjoint of L i . i = 1 ...m . Due to the reproducing property we can write, for j = 1 ...m , Thus, the first term in Equation 7 is independent of f  X  . Moreover, it is clear due to orthogonality that if f  X  6 = 0 then so that for any fixed  X  i  X  R , Equation 7 is minimised when f  X  = 0 . 2.3 Thin Plate Regulariser and Associated Kernel As is well known (see e.g. [2]), the choice of regulariser (the function norm in Equation 3) leads to regulariser is the thin-plate energy, which for arbitrary order m and dimension d is given by [2]: where  X  is a regularisation operator taking all partial derivatives of order m , which corresponds to a  X  X adial X  kernel function of the form k ( x , y ) = t ( || x  X  y || ) , where [11] There are a number of good reasons to use this regulariser rather than those leading to compactly supported kernels, as we touched on in the introduction. The main problem with compactly supported kernels is that the corresponding regularisers are somewhat poor for geometrical problems  X  they always draw the function towards some nominal constant as one moves away from the data, thereby implementing the non-intuitive behaviour of regularising the constant function and making interpolation impossible  X  for further discussion see [1] as well as [5, 6, 7]. The scheme we propose in Section 3 solves these problems, previously associated with compactly supported basis functions, by defining and computing the regulariser separately from the function basis. Here we present a fast approximate scheme for solving the problem of the previous Section, in which we restrict the class of functions to the span of a compactly supported, multi-scale basis, as described in Section 3.1, and minimise the thin-plate regulariser within this span as per Section 3.2. 3.1 Restricting the Set of Available Functions Computationally, using the thin-plate spline leads to the problem that the linear system we need to solve (Equations 5 and 6), which is of size m ( d + 1) , is dense in the sense of having almost all non-zero entries. Since solving such a system na  X   X vely has a cubic time complexity in m , we propose forcing f (  X  ) to take the form: For  X  we choose the B 3 -spline function: although this choice is rather inconsequential since, as we shall ensure, the regulariser is unrelated to the function basis  X  any smooth compactly supported basis function could be used. In order to achieve the same interpolating properties as the thin-plate spline, we wish to minimise our regularised risk function given by Equation 3 within the span of Equation 11. The key to doing this is to note that as given before in Equation 9, the regulariser (function norm) can be written as k f k 2 H =  X   X f, X f  X  L for the optimal  X  k (in the sense of minimising Equation 3): where we have defined the following matrices: The computational advantage is that the coefficients that we need are now given by a sparse p -dimensional positive semi-definite linear system, which can be constructed efficiently by simple code that takes advantage of software libraries for fast nearest neighbour type searches (see e.g. [12]). The system can then be solved efficiently using conjugate gradient type methods. In [1] we describe how we construct a basis with p m that results in a highly sparse linear system, but that still contains good solutions. The critical matter of computing K reg is dealt with next. 3.2 Computing the Regularisation Matrix We now come to the crucial point of calculating K reg , which can be thought of as the regularisation matrix. The present Section is highly related to [13], however there numerical methods were resorted to for the calculation of K reg  X  presently we shall derive closed form solutions. Also worth comparing to the present Section is [14], where a prior over the expansion coefficients (here the  X  ) is used to mimic a given regulariser within an arbitrary basis, achieving a similar result but without the computational advantages we are aiming for. As we have already noted we can write k f k 2 H =  X   X f, X f  X  L
To build the sparse matrix K reg , a fast range search library ( e.g. [12]) can be used to identify the to evaluate  X   X f j (  X  ) , X f k (  X  )  X  L function of s i , s j and d . = v i  X  v j : which it turns out is given by
Figure 2: Various values of the regularisation parameters lead to various amounts of  X  X moothing X   X  here we set C 1 = C 2 in Equation 3 to an increasing value from top-left to bottom-right of the figure.
 where j 2 =  X  1 ,  X  = F x [  X  ( x )] , and by F (and F  X  1 ) we mean the Fourier (inverse Fourier) transform operators in the subscripted variable. Computing Fourier transforms in d dimensions the Fourier transform in d dimensions (as well as its inverse) can be computed by the single integral: where J  X  ( r ) is the  X  -th order Bessel function of the first kind.
 Unfortunately the integrals required to attain Equation 14 in closed form cannot be solved for general dimensionality d , regularisation operator  X  and basis function form  X  , however we did manage to solve them for arguably the most useful case: d = 3 with the m = 2 thin plate energy and the B 3 -spline basis function of Equation 12. The resulting expressions are rather unwieldy however, so we give only an implementation in the C language in the Appendix of [1], where we also show that for the cases that cannot be solved analytically the required integral can at worst always be transformed to a two dimensional integral for which one can use numerical methods. 3.3 Interpretation as a Gaussian Process Presently we use ideas from [15] to demonstrate that the approximation described in this Section 3 is equivalent to inference in an exact Gaussian Process with covariance function depending on the choice of function basis. Placing a multivariate Gaussian prior over the coefficients in (11), namely and denoting expectations by E [  X  ] we have for the covariance Now, assuming an iid Gaussian noise model with variance  X  2 and defining K xt etc. similarly to K xz we can immediately write the joint distribution between the observation at a test point t , that is y  X  X  f ( t ) , X  2 and the vector of observations at the x i , namely y x  X  X  f x , X  2 I , which is
The posterior distribution is therefore itself Gaussian, p ( y t | y x )  X  N  X  y can employ a well known expression 2 for the marginals of a multivariate Gaussian followed by the Matrix inversion lemma to derive an expression for the mean of the posterior, Table 1: Timing results with a 2.4GHz AMD Opteron 850 processor, for various 3D data sets. Column one is the number of points, each of which has an associated normal vector, and column two is the number of basis vectors (the p of Section 3.1). The remaining columns are all in units of seconds: column three is the time taken to construct the function basis, columns four and five are the times required to construct the indicated matrices, column six is the time required to multiply the matrices as per Equation 13, column seven is the time required to solve that same equation for  X  and the final column is the total fitting time.
 By comparison with (11) and (13) (but with C 1 = 1 / X  2 , C 2 = 0 and y = 0 ) we can see that the mean of the posterior distribution is identical to our approximate regularised solution based on compactly supported basis functions. For the corresponding posterior variance we have We fit models to 3D data sets of up to 14 million data points  X  timings are given in Table 1, where we also see that good compression ratios are attained, in that relatively few basis functions represent the shapes. Also note that the fitting time scales rather well, from 38 seconds for the Stanford Bunny (35 thousand points with normals) to 4 hours 23 minutes for the Lucy statue (14 million points with normals  X  14  X  10 6  X  (1 value term + 3 gradient terms )  X  56 million  X  X egression targets X ). Taking account of the different hardware the times seem to be similar to those of the FMM approach [5]. Some rendered examples are given in Figures 1 and 3, and the well-behaved nature of the implicit over the entire 3D volume of interest is shown for the Lucy data-set in the accompanying video. In practice the system is extremely robust and produces excellent results without any parameter adjustment  X  smaller values of C 1 and C 2 in Equation 3 simply lead to the smoothing effect shown in Figure 2. The system also handles missing and noisy data gracefully, as demonstrated in [1]. Higher dimensional implicit surfaces are also possible, interesting being a 4D representation (3D +  X  X ime X ) of a moving 3D shape  X  one use for this being the construction of animation sequences from a time series of 3D point cloud data  X  in this case both spatial and temporal information can help to resolve noise or missing data problems within individual scans. We demonstrate this in the accompanying video, which shows that 4D surfaces yield superior 3D animation results in comparison to a sequence of 3D models. Also interesting are interpolations in 4D  X  in the accompanying video we effectively interpolate between two three dimensional shapes. We have presented ideas both theoretically and practically useful for the computer graphics and machine learning communities, demonstrating them within the framework of implicit surface fitting. Many authors have demonstrated fast but limited quality results that occur with compactly supported function bases. The present work differs by precisely minimising a well justified regulariser within the span of such a basis, achieving fast and high quality results. We also showed how normal vectors can be incorporated directly into the usual regression based implicit surface fitting framework, giving a generalisation of the representer theorem. We demonstrated the algorithm on 3D problems of up to 14 million data points and in the accompanying video we showed the advantage of constructing a 4D surface (3D + time) for 3D animation, rather than a sequence of 3D surfaces.
 Figure 4: Reconstruction of the Stanford bunny after adding Gaussian noise with standard deviation, from left to right, 0, 0.6, 1.5 and 3.6 percent of the radius of the smallest enclosing sphere  X  the normal vectors were similarly corrupted assuming they had length equal to this radius. The parameters C 1 and C 2 were chosen automatically using five-fold cross validation.

