 Youlu Xing 1 , Tongyi Cao 2 , Ke Zhou 3 , Furao Shen 1( In the field of unsupervised learning, many algorithms are designed to extract information from the distribution of data. Classic methods include k -means [ 1 ] and Neural Gas [ 2 ], which use fixed number of nodes to get different clusters. Self-Organizing Map [ 3 ] and Topology Representing Networks [ 4 ] represent the distribution and topological structure of the data with some given nodes. Two drawbacks are obvious for these early methods. First, each node stores the mean feature vector of patterns belonged to the node and the metric is Euclidean. Correspondingly, each node is a simple unit with isotropic form and spherical class boundary, and thus has a poor description ability. Second, these methods need a predefined structure or number of nodes that requires additional knowledge of the data which is often hard to know. Also, the fixed structures render them unable to perform incremental learning or to handle the  X  open-ended  X  environment, i.e. data from new distributions may occur during learning. lems, namely matrix learning and incremental networks. For the first problem, in node is preferred, often based on mixture model, including PCASOM [ 5 ], Self-Organizing Mixture Network [ 6 ], localPCASOM [ 7 ], and MatrixNG and Matrix-SOM [ 8 ]. L  X  o pez-Rubio [ 9 ] gave a detailed review about these Mixture Model based Self-Organizing Maps which they called the Probabilistic Self-Organizing Map. For the second problem, many  X  X rowing networks X  or  X  X ncremental net-works X  are proposed. Some grow after a fixed number of inputs learned such as GNG [ 11 ] and GSOM [ 12 ]; some use an adaptive threshold including GWR [ 13 ], Adjusted-SOINN [ 14 ], and TopoART [ 15 ]. Araujo and Rego [ 16 ] gave a detailed review about these incremental Self-Organizing Maps.
 tion information and consider the anisotropy on different basis vectors, they have a common shortcoming -they cannot deal with the Stability-Plasticity Dilemma [ 10 ], i.e. many of these methods cannot learn data from new distri-butions after they are trained on the current data set; the other methods is able to learn data from new distributions but the previous learned knowledge will be forgotten, known as the  X  X atastrophic Forgetting X . Thus, they can only work in a  X  closed  X  environment, with no new distributions occurring during learning. On the other hand, incremental networks process flexible structures that can adapt well for various data and environments, but they lose much use-ful information of the original learning data. Recently, an online Kernel Density Estimator (oKDE) [ 17 ] is proposed to introduce the Kernel Density Estimator to online learning. However, the learning environment ( X  closed  X  X r X  open-ended  X ) must be known in advance to set different parameters.
 combining the advantages of matrix learning and incremental learning, is able to mentally without forgetting previous knowledge. In summary, the characteristics of ILDN are: tion information of the learning data, and adopting statistically supported node merging and denoising criterions, ILDN is able to obtain a precise and concise representation of the learning data, called a relaxation data representation. theoretical supporting, ILDN is able to learn new distributions data effectively without forgetting the previous learnt but still useful knowledge. That is, ILDN can handle the Stability-Plasticity Dilemma effectively. ILDN is an online incremental learning model which combines the advantages of matrix learning and incremental networks. The nodes in the network record not only the weight vector but also the data distribution information around its feature space are connected. The connected nodes will be merged during learning representation.
 In ILDN, each node i is associated with a 4-tuple c i ,M i n are the mean vector, covariance matrix and number of input patterns belonged to node i , it dynamically changes with the learning process. Assume that ILDN receives d -dimensional data x  X  R d , the node can be described as a hyper-ellipsoid region using c i , M i and H i : The entire workflow of ILDN is as follows: when an input pattern comes, ILDN first conducts the Node Activation to find some activated nodes which are recorded in an activating node set S . Then Node Updating is conducted accord-for this input pattern; Else ILDN will find a winner among the activated nodes and update this winner node. Topology Maintaining module will create connec-tions between the nodes in S and record these connections in the connection list set C . After that, ILDN will check the merging condition between the winner node and its neighbor nodes, if the merging condition is satisfied, Node Merging between the winner node and its neighbors will be executed to get a concise local representation. When all the steps above are done, ILDN will process the next input pattern. Denoising is implemented every  X  patterns are learned. When the learning process is finished or users want the learning result, ILDN will Cluster the learned nodes and output the learning result. 2.1 Node Activation When an input pattern x comes, ILDN first calculates the Mahalanobis distance between x and all node i  X  N : where N is the set of nodes, | N | represents the total number of the nodes. If D ( x ) &lt;H i , we say node i is activated. Then we put i in an activating set S and we get: Then set S records all the activated nodes by the input pattern x . 2.2 Node Updating If S =  X  , i.e. no node is activated by x ,itmeans x is a new knowledge. A new node a is created for x as: and  X  is a small positive parameter. This initialization ensures that the covari-decides the initial hyper-ellipsoid size of the new node.  X  control the expansion trend of ellipsoid.  X  2 d,q is a value of  X  details of such parameters will be discussed in Section 3.
 ILDN will find a winner node i  X  from set S :
Then node i  X  : c, M, n, H is updated in a recursive way as: 2.3 Topology Maintaining A topology preserving feature map is determined by a mapping  X  from a man-ifold M onto the vertices (or nodes) i  X  N of a graph (or network) mapping  X  is neighborhood preserving if similar feature vectors are mapped to vertices that are close within the graph. This requires that feature vectors v v j that are neighboring on the feature manifold M are assigned to vertices (or nodes) i and j that are adjacent (or connected) in the graph or network fined structure G such as SOM [ 3 ]. In this paper, the connections between nodes of
G are built according to Hebbian learning rule: If two nodes are activated by one pattern, a connection between the two nodes is created. According to the definition of set S , we know that all nodes in S are activated by the current input pattern x . Thus, if no connection exists between node i and j in S ,ILDN will add a new connection { i, j | i  X  S  X  j  X  S  X  i = j } C . After a period of learning, these connections is able to organize the nodes into groups to represent different topology of the learning data. 2.4 Node Merging As learning continues, there may be some nodes closing to each other and having similar principal components. Such nodes will be merged to obtain a concise local representation. At merging stage, two nodes will merge if the following two conditions are satisfied: two nodes i and j , i.e. If the above conditions are satisfied for node i and j ,wemerge i and j and let m represent the data in i and j collectively: In practice, we only merge the winner node and its neighbors when a pattern is fed into ILDN. After merging, all the connections with original nodes in C are attached to the new node. 2.5 Denoising The data from the learning environment may contain noise. Some nodes may be created by these noise data. Since ILDN records the distribution density of each node, we can use this information to judge whether a node is a noise node. After every  X  patterns learned, ILDN first calculates the mean value of the number of the input patterns belonged to each node as: where | N | represents the total number of the nodes, n i patterns belonged to node i . We assume that the probability density of the noise is lower than the useful data. Based on this assumption, if n a threshold k  X  Mean , we mark node i as a noise node and remove it. Where 0  X  k  X  1 control the intensity of denoising. After denoising, all the connections with deleted nodes in C are also deleted. 2.6 Cluster In [ 2 ], it is proved that the competitive Hebbian rule forms perfect topology preserving map of a manifold if the data is dense enough. Based on this opinion, domains as different clusters. Algorithm 1 shows the details of the clustering method. 2.7 Complete Algorithm of ILDN As a summary for this section, we give the complete algorithm of ILDN (Algo-rithm 2 ).
 Algorithm 1. Cluster Nodes  X  X ath X  exists between the two nodes.
 Algorithm 2. Incremental Local Distribution Network 3.1 The Expansivity of the Nodes As each node records the local regional distribution, we can assume that the pat-terns belonged to one node are generated by a Gaussian distribution where c X is the center and  X  X the covariance matrix. The hyper-ellipsoid bound-ary equation of each node is: Let K = H 2 , then K is a  X  2 distribution with d degrees of freedom. Giving a confidence q , the patterns from random variables X lie in the hyper-ellipsoid drawn by K can be described as: Then K can be solved by K =  X  2 d,q .For K = H 2 , we can get the value of H .In practice, for node i we set H i =  X  n i  X  2 n,q , where  X  n increases. This strategy let the hyper-ellipsoid has a tendency of expansivity at approaches to  X  2 d,q , then the hyper-ellipsoid arrives a stable state. We set  X  2 In ILDN,  X  decides the initial hyper-ellipsoid size of new nodes. It can be understood as the initial size of the window we observe the learning data. Very big  X  may lead to a new node cover several different clusters. Therefore, ILDN prefers small  X  . Though small  X  may make ILDN initially generate many nodes with small hyper-ellipsoid size, ILDN can merge these nodes following the learn-ing process. 3.2 The Relaxation Data Representation With the hyper-ellipsoid defined in formula (1), the volume of node n can be calculated: where M n and H n are the covariance matrix and vigilance parameter of node n . | M n | represents the determinant of M n ,[  X  ] represents the rounding operation. d is the dimension of the sample space. However, | M | in formula ( 12 )maybe very close to 0 in some high dimensional task and thus not suitable to calculate the volume directly with it. To avoid directly calculate the volume with formula m in node merging condition (ii), we do Singular Value Decomposition (SVD) as: where  X  1  X   X  2  X  ...  X   X  d , and we get  X  i 1 ,  X  i 2 ,...,  X  node j and  X  m 1 ,  X  m 2 ,...,  X  m d for node m .
 Then we find a truncated position t of all singular value with a predefined scaling factor  X  : p =argmin a common truncated position t =max( p i ,p j ,p merge ) and calculate  X   X  Substituting these three volumes and after a series of simplification, we get an equivalence merging condition: ( 14 ) is not satisfied, node i and j will remain unchanged.
 At the node merging step, we have two candidate data representation that Assume the domain of distribution Q i ( x )ofnode i is x  X  distribution Q j ( x )ofnode j is x  X  R j . Then the data representation f node merging in domain R i  X  R j is: data in domain R i  X  R j falls in f 2 equals to q (usually q according to node merging condition (ii), the volume of node m is less than the total volume of node i and j . Thus, we get a much concise data representation on domain R i  X  R j .
 the two regions in f 1 overlap each other. Thus, the representation f of the parameters, correspondingly, we call representation f representation. As ILDN aims to combine the advantages of incremental learning and matrix matrix learning and incremental learning. The matrix learning methods include localPCASOM [ 7 ], BatchMatrixNG [ 8 ] and oKDE (oKDE is also an incremental learning method) [ 17 ]. The incremental learning methods include TopoART [ 15 ] and Adjusted-SOINN (ASOINN) [ 14 ]. 4.1 Artificial Data Observe the Periodical Learning Results. We use the artificial data which cluster and generates samples uniformly. Parameters of ILDN are  X  1 (a)). With the learning process continues, some ellipsoids are merged together, as at the 100 patterns stage (Fig. 1 (b)). After 200 patterns, all ellipsoids are original data set very well. ILDN does not need to predetermine the number of the nodes, it automatically generates 2 nodes in this task.
 Work in Complex Environment. In this section, we conduct our experiment 20000 samples in total. Data sets A and B satisfy 2-D Gaussian distribution. C 10% Gaussian noise and random noise to the dataset. Noise is distributed over the entire data set.
 The experiments are conducted in two environments. In the  X  X losed X  envi-ronment, patterns are randomly selected from the whole learning set. In the  X  X pen-ended X  environments, five parts of the data are presented successively. In stage I, i.e. step 1 to 4000, patterns are chosen randomly from data set A. In stage II, i.e. step 4001 to 8000, the environment changes and patterns from B are chosen, etc.
 We set the parameters of localPACSOM as N = 20,  X  =0 . 01, H =1 . 5; Batch-Matrix as N = 20, epoch = 1000; ASOINN as  X  =200, age Top oART as  X  sbm =0 . 32,  X  =0 . 96,  X  =5,  X  =100; oKDE as D 1 for the  X  X losed X  environment and f =0 . 99 for the  X  X pen-ended X  environment. ILDN as  X  2 ( d,q ) =  X  2 (2 , 0 . 90) ,  X  =1 e -5, k =0 . 5,  X  = 1000. Fig. 3 shows the results. localPCASOM and BatchMatrix suffer from the Stability-Plasticity Dilemma. oKDE is vulnerable to noise. ILDN obtains best fitting with least nodes. On one hand, ILDN can merge some local small ellipsoids into a big one, leading to a more concise data representation than other matrix learning methods and can learn incrementally. On the other, ILDN learns the to use a large number of nodes.
 SOM, BatchMatrix, oKDE, and ILDN in the  X  X losed X  environment. The result is shown in Fig. 4 .The relaxation representation can generate a more concise representation than the tight representation on the two Gaussian distributions. For the distribution which cannot be further simplified, the relaxation represen-sinusoidal distribution. This experiment demonstrates that the relaxation data representation gets a much concise representation than the tight data represen-tation while maintaining the same level of details and preciseness. 4.2 Real-World Data In this section, we first do an experiment on the ATT FACE database. There are 40 persons in the database and each person has 10 images differing in lighting conditions, facial expressions and details. The original image (size 92 re-sampled to 23  X  28 image using the nearest neighbor interpolation method. Then Gaussian smoothing is used to smooth the 23  X  28 image with HSIZE = [4; 4],  X  =2.
 The experiments are conducted in the  X  X losed X  and  X  X pen-ended X  environ-ment for oKDE, ASOINN, TopoART and ILDN. In the closed environment, patterns are randomly selected from the whole learning set. In the open-ended environment, from step 1 to 200, patterns are chosen randomly from person 1. From step 201 to 400, the environment changes and patterns from person 2 are chosen, etc. For BatchMatrix and localPCASOM are not incremental method, we only conduct these two methods in the closed environment. Such methods need a predefined node number, to guarantee a good learning result, we set it as 200 which gives it a good initial condition: the initial nodes contain images of all 40 persons. We set the parameters of localPCASOM as N = 200, =0 . 01; BatchMatrix as N = 200, epoch = 200; oKDE as D th =0 . 01, N = 10, f = 1 for the closed environment and f =0 . 99 for the open-ended environment; the parameters of ASOINN is set as  X  =100, age max =50, c =0 . 25; TopoART as  X  =0 . 6,  X  =0 . 96,  X  =3,  X  =100; ILDN as  X  2 ( d,q ) =  X  2  X  =1000.
 We adopt 3 factors to evaluate the learning results: node number, missing person number and recognition ratio. Table 1 gives the learning results of 100 times learning results in both environments. oKDE only gets 3 nodes in the learning result in both environments, losing 38 persons. Though ILDN gets larger node number in the closed environment than TopoART, it does not lose any person. In the open-ended environment, ILDN gets the smallest node number (excluding oKDE) and does not lose any person. On the other hand, the ASOINN  X  X orget X  5.01 persons. Moreover, the proposed ILDN has the least difference of number of node and do not  X  X orget X  any person in the two environments, which means ILDN is a more stable incremental method than others. At last, nearest neighbor is used to classify vectors in the original image vector. We do not test the oKDE because it is unsuitable for this task. Table 1 shows that the recognition accuracy of ILDN is higher than the other methods.
 tle, Webspam and KDD99 which differ in the length, dimensionality as well as the number of classes. The parameters of the comparison methods are set as they suggest. The parameters of ILDN are set as  X  =1 e -3, k =0 . 01,  X  =1000. We define the node number of localPCASOM and BatchMatrix as same as ILDN, marked with  X  in the learning result.
 in all datasets. Compared with three matrix learning methods localPCASOM, BatchMatrix and oKDE, ILDN uses far less nodes in three out of four datasets. Moreover, ILDN can handle very large scale dataset like KDD99, while localP-CASOM, BatchMatrix and oKDE cannot give a learning result within 10 days. We can also find that the incremental (or online) matrix learning method oKDE 10 days. Thus, ILDN is a more practical incremental (or online) matrix learning method than others. This paper presents an incremental local distribution learning network (ILDN). It combines the advantages of matrix learning and incremental learning. The covariant matrix, statistical vigilance parameter and merging mechanism enable it to obtain precise and concise representation of the data and learn incremen-tally. Moreover, the denoising processing based on data density makes it robust idate our claims and show that ILDN is more effective over other matrix learning methods and incremental networks, obtaining higher accuracy and being able to handle large-scale data.
