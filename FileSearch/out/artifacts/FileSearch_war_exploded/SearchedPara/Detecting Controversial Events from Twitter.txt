 Social media provides researchers with continuously updated information about developments of interest to large audi-ences. This paper addresses the task of identifying contro-versial events using Twitter as a starting point: we propose 3 models for this task and report encouraging initial results. I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing Algorithms
The explosion of social media has allowed researchers un-precedented access to data about the opinions of large au-diences regarding political developments or popular culture events. The automatic detection of events which engage large social media audiences is a tempting challenge from both a sociological and a practical perspective: for exam-ple, displaying engaging news and events would allow web content providers to draw more users to their sites.
We present methods for detecting a specific type of en-gaging events -controversial events -using social media as a starting point. Controversial events provoke a public discus-sion in which audience members express opposing opinions, surprise or disbelief. Examples include incidents which vi-olate the public X  X  expectations about a particular entity, or which go against established social norms (see Table 1).
First, we introduce the notion of a Twitter snapshot , i.e. a triple consisting of a target entity (e.g., Barack Obama), a given time period (e.g., 1 day) and a set of tweets about the entity from the target time period. Given a set of Twitter snapshots, controversial event detection can be modeled as follows: (i) assigning a controversy score to each snapshot (ii) ranking the snapshots according to the controversy score. In practice, we focus on controversies involving celebrities (actors, politicians, etc.) from Twitter, but our model can be easily generalized to other types of micro-blogging sites and entity classes (e.g., organizations).

The main contributions of this paper are the following: (a) We formalize the task of  X  X ontroversial event detection X  and introduce 3 regression machine learning models to ad-dress it; (b) We describe a rich feature set for the target task; (c) We report encouraging experimental results: our models register statistically significant performance increases over all baselines, including relevant previous work.

Related Work Our work draws on and advances a rich body of opinion mining and event mining research. The work closest to ours is that of Tsytsarau et al. [7], who pro-pose an unsupervised approach for mining contradictions at different levels of time granularity from postings on a topic. They introduce a new contradiction measure which we in-corporate as a feature; subsequently, we show that our mod-els outperform a supervised version of their (unsupervised) approach. Another relevant paper, [4], studies weblog com-ments and the mining of mixed-sentiment threads -our task and constraints differ, but some features are similar (e.g., polarity information and user engagement features). In re-cent years, event mining has moved from news collections to social streams: e.g., [9] define events as sets of relations between social actors on specific topics over certain time pe-riods -in contrast, we focus on events centered on a given entity in a 1-day period and use a supervised approach. [6] uses community detection methods over a keyword graph to discover events -however, no evaluation is conducted, while we report detailed experimental results. In this work we use the following definitions.
 Event. Given a particular target entity, an event is defined as an activity or action with a clear, finite duration in which the target entity plays a key role.
 Controversial event. An event is controversial if it pro-vokes a public discussion in which audience members express opposing opinions or disbelief (rather than no reaction, or an overwhelmingly positive -or negative -reaction). Snapshot. A snapshot is defined as a triple: s = ( e,  X  t, tweets ) where: e is the target entity, i.e. any type of concept or named entity (e.g.  X  X arack Obama X ) ;  X  t is a time period (e.g. one day); tweets is the set of all tweets from the target time period which refer to the target entity.
 Event snapshot. A snapshot describing one specific event 1 This can be either a controversial-event snapshot , describing a controversial event, or a non-controversial event snapshot , describing a non-controversial event.
 Non-event snapshot. A snapshot which does not describe any event (spam, generic discussions, etc.).
 Controversial event detection. Given a set E of target entities and a set of snapshots S = { ( e,  X  t, tweets ) | e  X  E } , the task is to rank snapshots in S according to a controversy detection function that assigns a controversy score cont ( s ) to each snapshot s in S . The function should assign higher scores to controversial-event snapshots and lower scores to non-controversial-event and non-event snapshots. The task can be decomposed into two steps: Event detection (sepa-rating event snapshots from non-event snapshots) and Con-troversy detection (ranking event snapshots obtained from the previous step according to cont ( s )).
We model the controversial event detection task as a su-pervised machine learning (ML) problem, where each snap-shot s is represented by a feature vector constructed from Twitter and other sources. We operationally define the com-ponents of a snapshot s = ( e,  X  t, tweets ) as follows: e = any entity contained in a list of about 100K celebrities (see Sec-tion 3);  X  t = 1-day period; tweets = the set of tweets in the time period mentioning the entity. We use Gradient Boosted Decision Trees (GBDT, fully described in [2]) as the ML framework. We propose the following models for the task: Direct model , estimating the controversy score cont ( s ) in a single step using a single ML regression model. The man-ually annotated training set is composed of positive exam-ples (controversial-event snapshots), and negative examples (non-controversial-event and non-event snapshots). Two-step pipeline model , using the two-step decompo-sition presented above. The event detection classification model selects event snapshots from the set S . The contro-versy detection regression model assesses the level of contro-versy cont ( s ) for snapshots selected in the first step. Two-step blended model , a soft variant of the pipeline model. The result of the event detection step is used for pro-viding more evidence for the controversy detection regression model. Specifically, the controversy detection model takes as input the full set of snapshots S and employs as an ad-ditional feature the prediction confidence score returned by the event detection model for each snapshot.

We employ a variety of resources to derive a large set of features for our models. A 7,590 word sentiment lexicon includes positive and negative polarity words from Opin-ionFinder 1.5 [8] as well as more informal opinion terms
Snapshots describing multiple events may also exist, though they are very rare.
 Table 4: 10-folds experimental results.  X  indicates statistical significance at 0.95 level wrt base-rnd ;  X  wrt to base-rnd and base-hst ;  X  wrt to all baselines. mined from user reviews (as in [5]); each sentiment word w has an associated polarity strength score sent ( w ). A con-troversy lexicon contains 750 controversial words derived from Wikipedia pages of people mentioned in the Wikipedia controversial topic list. A bad words lexicon of 687 En-glish bad words was downloaded from the Web 2 . Finally, we use an English dictionary of about 100K part-of-speech tagged English words, obtained as output of the Brill tag-ger [1] trained over the Wall Street Journal and Brown cor-pora.
 Table 2 contains our features, organized as follows:
Twitter-based features capture snapshots X  linguistic properties ( tw-ling ), structural and social graph informa-tion ( tw-strc ), the intensity of the discussion about the entity ( tw-buzz ), the distribution of sentiment words in the snapshot ( tw-sent ), and the level of controversy ( tw-cont ). Sentiment features are derived by computing the polarity of each snapshot X  X  tweet t : pol ( t ) = X where sent ( w ) comes from sentiment lexicon.

News buzz features ( ex-buzz ) capture the intuition that if an entity is buzzy in news articles at the same time it is buzzy in a Twitter snapshot, then the snapshot is likely to refer to a real-world event. To compute such features, we align news articles with the set of snapshot tweets: given a snapshot with  X  t , we extract news articles issued in period ( X  t  X  1 ,  X  t + 1) in which the target entity is a salient entity , i.e. it is mentioned in the article headline, or it is one of the 3 most frequently mentioned named entities in the article.
Web and news controversy features ( ex-cont ) as-sess the past and present levels of controversy surrounding the target entity in the snapshot.
We experiment using a set of 104,713  X  X elebrities X  ob-tained as follows : Wikipedia category lists for Actors, Mu-sicians, Politicians and Athletes are scraped and entities whose names have less than 3 characters are removed. Gold standard. We collect 738,045 Twitter snapshots re-ferring to any of the above entities, from a (July 2009 -February 2010) firehose. We remove snapshots which have less than 10 tweets, more than 80% non-English tweets and tweets with more than 80% overlapping tokens 4 , obtaining 73,368 snapshots. The gold standard contains 800 randomly sampled snapshots labeled by two expert editors: 475 are http://urbanoalvarez.es/blog/2008/04/04/bad-words-list/ and http://www.noswearing.com/dictionary.
Our corpus collects articles from Yahoo! News 2009/2010.
We verified by manual inspection on a small set of discarded snapshots, that about 97% of them are indeed irrelevant. non-event snapshots and 325 are event snapshots (kappa-agreement is 1.00). Of the latter, 152 are controversial-event snapshots, and 173 non-controversial-event snapshot (kappa-agreement is 0.89, corresponding to almost perfect agreement ). The final gold standard for our task thus con-tains 152 positive examples (controversial-event snapshots) and 648 negative examples.
 Evaluation metrics. We compare the models on the task of ranking snapshots according to their controversy score. We employ two measures commonly used to evaluate rank-ing quality, average precision (AP) and area under the ROC curve (AROC) [3], in a 10-fold cross-validation setup. The AROC represents the probability that a model will rank a controversial-event snapshot higher than a randomly chosen snapshot. We compare the following systems: base-rnd . A random-rank baseline. base-hst . A ML model using ex-cont-hist as the only feature. base-tsy . A ML model using only the tw-cont-tsy feature from Tsytsarau et al. [7]. direct . Our ML direct model using all our features. pipeline . Our ML two-step pipeline model using all features in both steps. blended . Our ML blended model using all features. 5
Table 4 contains the experimental results: all our models outperform the baselines with statistical significance at the 0.95 level. The blended system performs best, indicating
For all models, GBDT parameters were set on an inde-pendent development set, as follows: number of trees=50, shrinkage= 0.01, max nodes per tree=10, sample rate=0.5 Figure 1: Precision at rank for different models, av-eraged over the 10 folds. that a  X  X oft X  integration between the 1st and 2nd steps (event detection and controversy detection), is preferred to a  X  X ard X  integration ( pipeline model) or a direct approach ( direct model). The lower performance of pipeline is due to the discarding of too many event snapshots in the event detec-tion step. However, the differences in performance between the systems are not statistically significant at the 0.95 level.
The overall AROC results show that our models have good discriminative power -they are able to rank controversial-event snapshots higher than non-controversial-event snap-shots with probability greater than 0.80.

Figure 1 plots the precision-at-rank curves, while Table 3 reports the top-ranked snapshots for the blended model. The blended model has a very high precision at high ranks: precision at rank-1 is 0.90, and at rank-4 is 0.80. Project-ing these numbers to the overall size of the snapshot set (73,368 examples), we estimate that blended extracts the top-900 controversial snapshots with 0.90 precision, and the top-3,700 with 0.80 precision. This is encouraging, especially in view of plugging in the models into a real application.
Feature analysis. The top-10 most discriminative fea-tures 6 for the GBDT blended model are a diverse mix representing various feature families. As expected, event-score is the most relevant feature for the blended model, as it takes care of separating event snapshots from non-event ones. tw-strc-hst ranks second, suggesting that in general hashtags are an important semantic component of tweets: they help identify the topic of a tweet and estimate the top-ical cohesiveness of a set of tweets. External features based on news and the Web are also useful: coupling Twitter in-formation with traditional media helps validate and explain
As ranked by GBDT [2], features with higher importance have a greater contribution in the model building phase. social media reactions. Linguistic, structural and sentiment features are also highly ranked, which indicates that a rich, varied set of features is key for controversy detection.
Error Analysis. Our error analysis found that false pos-itives (i.e. highly but incorrectly ranked non-controversial-event snapshots) are mostly snapshots containing harder-to-interpret, misleading mixtures of positive and negative words (e.g., negative words used in a positive sense, as in  X  X y boy shannon brown is killing it X .) We plan to integrate strategies for ambiguity resolution to deal with such cases.
Causes for false negatives (controversial-event snapshots incorrectly ranked low) include negative terms used in a pos-itive sense (e.g.  X  X oy williams crazy as hell lol X ).
Future work Our future work plans involve improved tweet-level sentiment detection, additional features for con-troversy detection and an automatic analysis of the iden-tified events (e.g., personal vs. professional developments, lasting vs. short-lived controversies). We also plan the in-tegration of our research in applications: detecting recent events about well-known entities can be useful to improve user X  X  search experience and user X  X  engagement on a site. [1] E. Brill. Transformation-based error-driven learning [2] J. H. Friedman. Greedy function approximation: A [3] D. Green and J. Swets. Signal detection theory and [4] G. Mishne and N. Glance. Leave a Reply: an Analysis [5] A.-M. Popescu and O. Etzioni. Extracting product [6] H. Sayyadi, M. Hurst, and A. Maykov. Event Detection [7] M. Tsytsarau, T. Palpanas, and K. Denecke. Scalable [8] J. Wiebe and C. Cardie. Annotating expressions of [9] Q. Zhao, P. Mitra, and B. Chen. Temporal and
