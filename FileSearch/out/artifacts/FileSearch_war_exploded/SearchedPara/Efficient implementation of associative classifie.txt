 1. Introduction
An associative classifier is a classifier using classification rules that are produced through a frequent pattern mining process from a training data collection. This process is the same one used in traditional data mining for history compared to other classifiers such as Na X   X  ve Bayes, k -NN, or SVM. It seems more difficult to find a study in which an associative classifier is applied in the text classification task.
When performing a text classification task in a real world situation, the ability to provide abundant inter-pretation on the classification result is often as important as the ability to classify new documents exactly. ability. The associative classifier is one of the rule-based classifiers. In contrast, some classifiers such as SVM or Neural Network cannot provide this easy interpretation for the classification result, though they may achieve excellent classification accuracy.

We can acquire several additional advantages from using rule-based classifier. One is that since the rules can be expressed in a very intuitive form, humans can easily understand them and can even edit them directly after the rules are produced by some inductive learning process. A human expert could delete the weak rules from the original rule set and add new rules that they carefully handcrafted. This can improve the classifica-tion accuracy remarkably with a little bit of added effort. Another is that the rules can be updated incremen-tally by other machine learning processes later.

Another benefit of the associative classifier is that it can exploit the combined information of multiple fea-tures as well as a single feature, while SVM or k -NN classifiers consider only the effects of each single feature.
This means that in document classification tasks it is possible to use phrase occurrence information as well as word occurrence information.

To apply an associative classifier to the text classification problem in the real world, however, we need to remove several obstacles encountered during the training and testing phase. One of those is a high dimensional feature space. Dataset in the area of text classification, in many cases, has a very large number of features that are distinct lexical words. For example, the 20-newsgroups test collection has more than one hundred thou-sand lexical word features. Most documents of the 20-newsgroups have more than one hundred words; they are sparsely distributed in their word feature space. In associative classification, however, we consider all sub-sets of those words. Therefore, the effective number of features grows exponentially, and we cannot take into account all of them due to computational intractability.

To overcome this problem we adopt a feature selection-based dimensionality reduction technique at the same time maintaining necessary performance in classification. Many well-known methods of dimensionality reduction exist ( Sebastiani, 2002 ). We used the mutual information measure of the information theory. From the training dataset we calculated the mutual information between the word and the class variables. And we selected words that have high mutual information, and used only those in classifying and neglected the others.

Another obstacle in associative text classification is the large number of classification rules that are pro-duced in the training phase. Since using all of them becomes both inefficient computationally and ineffective in classifying, we should select a part of those rules that have high quality. This process has been called Prun-ing in associative classification. Liu, Hsu, and Ma (1998) proposed a pruning by database coverage, which is a kind of validation process using the training set for the purpose of choosing the best classification rules among others. Li, Pei, and Han (2001) refined the concept of the database coverage. In addition, they pro-posed two other pruning methods. One is to prune low-ranked rules in terms of the confidence and support of the rules. The other is to prune the rules in which the correlation between the pattern and the class vari-ables is weak. In this paper, we adopted the pruning methods of Li et al. X  X  and improved them to work for text classification.

Related issue of the rule pruning is the prediction of a new document using classification rules. With a large number of rules, the prediction result of a test document often shows a split decision between different classes.
A method is needed to select one correct class among many in an efficient and effective way. It is not a simple problem because if we extract relatively small portion of the rules to avoid many contradicting rules for a doc-ument, we might lose latent candidate classes that may be the correct answer. To handle this problem, Li et al. (2001) used the weighted chi-square method. We try to resolve this problem by simple efficient voting on the different answer classes.

In Section 2 we introduce the general aspects of the associative classification. In Section 3 we explain the overall architecture of our text classification system using association rules and address the issues such as dimensionality reduction, and rule pruning and prediction from multiple rules. Experimental results and analyses of text classification using a large dataset are presented in Section 4 , and we conclude our works in Section 5 . 2. Associative classification 2.1. Association rule mining
Associative rules are originated from the market basket analysis in which we seek some patterns of purchas-ing. The term Mining indicates that we should apply much effort to searching the log database to acquire valu-able information.

An association rule is a kind of co-occurrence information on items. Consider a transaction log database of a large modern retailing market. We want to extract some pattern of co-purchasing of product items from this database. Let a set of product items be I ={ I 1 , ... , I
The consequent often is restricted to containing a single item ( Webb, 2003 ). The rules typically are displayed with an arrow leading from the antecedent to the consequent: for example, {plums, lettuce, tomatoes} ! {celery}. For an item set A and B , Support ( A ) is defined as the number of t including A divided by N , and Confidence ( A ! B )as Support ( A ! B )/ Support ( A ). A user provides thresholds on the support and confidence of a rule denoted as minsup and minconf , respectively. Definition 2.1 ( Association rule ). Given an item set X and an item Y , let s be Support ( X ! Y ) and c be Confidence ( X ! Y ). Then, the expression X ! Y /( s , c ) is an association rule, if s P minsup and c P minconf .
The two constraints about the support and the confidence of a rule imply that we search some level of  X  X  X re-quent X  X  patterns. In the training phase of the associative classification, the main task is to extract association rules, in other words, frequent pattern mining .
 Unfortunately, as the number of items grows linearly, the number of the antecedents in the left-hand side of
Eq. (1) grows exponentially. Though we can reduce the size of the subset of patterns by the two parameters, minsup and minconf , the search often becomes computationally intractable when we use na X   X  ve methods. Many efficient algorithms were proposed to search frequent patterns more efficiently ( Agrawal &amp; Srikant, 1994; Han,
Pei, &amp; Yin, 2000 ). We modified the algorithm by Han et al. (2000) , the Frequent Pattern tree growth , and applied it when we mined frequent patterns. 2.2. Associative classifier
Consider the association rule in the view of a classification rule. Let A ={ A domains, and a data object obj =( a 1 , ... , a n ) be a sequence of attribute values, i.e. a pattern P  X  a i
P if and only if, for 1 6 j 6 k , obj has value a i
Definition 2.2 ( Associative classifier ). Let C ={ c 1 , ... , c the mapping R from the set of attribute values to a set of class labels
According to Eq. (2) , given a test datum obj =( a 1 , ... , a
Let a pattern variable be P and a class variable c . If we rewrite the rule in the form of R : P ! c and have a mining is not much different from that of general association rule mining. One difference is that, in associative classification rule mining, the information of the distribution of word patterns matching each class is addition-ally maintained.

Now that we have a classification system, it requires a decision on which class to assign a new test docu-ment. First, we search for the rules in which the pattern matches the document. Next, from these rules, we perform a prediction based on some predefined decision criterion. The details are explained in Section 3 . 3. Text classification with associative classifier 3.1. Overall architecture
The overall system architecture for associative classification is shown in Fig. 1 . The left-hand side of the figure denotes the training process and the right-hand side the testing process.

First, raw data for training is processed to fit to an appropriate form for training. This is called Pre-pro-cessing . We index every word of training documents and test it for the quality of its contribution to exactly classifying the given training documents. Each document is converted into a word-vector format and normal-ized to its length.

From the pre-processed database, we mine frequent patterns, i.e. classification rules. Because the initial number of rules is very large, we select a part of them and drop the remaining rules; this process is called Prun-ing . Finally, we construct a classification-rule database with these selected rules.

When a new document comes in to be classified, we convert it into a pattern of words and search the data-base for matching rules. With the rules matched, we decide which class the test document is assigned to. 3.2. Dimensionality reduction by feature selection
Mutual information is defined between the class and the word random variables where we can estimate the degree of contribution that for a word to classify the documents of a given data collection. According to the distribution model of words in the document collection, its calculation may differ slightly ( McCallum &amp;
Nigam, 1998 ). In this paper we adopted as a document event model the multivariate Bernoulli model in which we does not consider the count of word occurrence but its presence in a document. Denote C as the random variable for the class label, and W t as the random variable for the presence or absence of a word w ument. Then the average mutual information of W t with C is defined as ( Cover &amp; Thomas, 1991 ): where H ( C / W t ) is the entropy of C given W t , and f rences of word w t that also appear in documents with class label c , divided by the total number of word occurrences.

Chi-square statistic also provides a measure of dependency between class variable and word variable in the distribution of a document set. But we adopted mutual information not Chi-square statistic. The reason is that since Chi-square statistic judges dependence between a word and a specific class, if we do not have the the word.

We selected M words with the highest average mutual information with the class variable among total N words. In general, we select a value of the parameter M such that M N . Finally, we convert original training documents into the documents of word-vector format that has M dimension. Moreover, since the length of each document has much variation, we should normalize the length of document in order to reduce some biases between the assigned classes as much as possible. In this paper, we introduced a parameter L indicating the maximum length of a document (by the length we mean the count of distinct words in the document). We construct a transaction record with at most L words which are sorted in the order of descending average mutual information. 3.3. Extracting and storing classification rules
Before extracting classification rules, we construct the trees of frequent word patterns. This procedure is the same as the one which is used to construct the frequent pattern mining tree of the previous research ( Agrawal gory is augmented. The information includes the category name, the support and the confidence value of the word pattern. A word pattern tree is depicted in Fig. 2 .
 information for more than one class. For example, the top left node of the tree has the counts of occurrence two and one on the class  X  X  X  and  X  X  X  respectively with respect to the word  X 1 X . From this tree we perform the minimum support and confidence criterion. As stated previously, these extracted rules are produced in exces-sively large numbers.

To overcome this problem, we introduce a new mechanism to store and retrieve these rules efficiently. In this scheme we construct another tree called Classification Rule Tree (CR-tree) apart from the word pattern trees. CR-tree has a similar structure to word pattern tree. The difference is that in CR-tree a node does not have any class distribution information but has only one class information at the last node of a word pat-tern path (see Fig. 3 ).

While executing the frequent pattern mining algorithm ( Han et al., 2000 ), we acquire candidate rules that might be pruned and not be inserted into the CR-tree if those rules do not satisfy some conditions. Except the conventional minimum confidence and the support conditions there is an important criterion of generalized rules . We are required to avoid an overfitting in training phase and thus lessen errors in predicting phase.
So, the condition of the generalized rules is as follows: when we insert a new rule into a CR-tree, every rule in the tree that is subset to the candidate rule must have lower rank than the candidate. If not, the candidate rule becomes more specialized and we cannot avoid the overfitting. In our method, differently from the pre-vious pruning methods, the storing and pruning occurs in one step, hence it dramatically reduces the training time.
 3.4. Pruning rules using CR-tree
It is not always helpful to have a large number of rules when we classify a new test document. There is a greater chance of having more than one rule contradicting each other in the answer class. In addition, the rules may over fit the training document set. We want to have a small number of the most powerful rules. In this pruning process, duplicate rules are eliminated and rules that might produce wrong classification results are removed. We perform two types of rule pruning; the first is pruning by rule ranking and the other is by the Chi-square statistic.

Before we prune the rules by rank, we must first assign a rank to each rule. The rule-ranking criterion is as follows: (1) The rule with a higher confidence has a higher rank than others. (2) If the confidences are the same between two rules, then the one with a higher support has a higher rank than the other. (3) If the supports of the two are the same as well, then the one with the fewer number of words in the left-hand side of the rule has a higher rank. In other words, we prefer  X  X  X hort X  X  rules rather than long ones if other conditions are equal. The short length of the rules means general rules, while long rules are prone to over fit. Therefore, we can reduce the test errors by adopting more general rules.

Assume that the eight rules in Table 1 were found as a result of the frequent pattern mining process. The minsup and minconf of the rules were taken as 3 and 60% respectively. Rule-8 has the highest rank since its confidence is the best. Though rule-6 and rule-7 have the same confidence, rule-6 is ranked higher due to the higher support.

By the pruning criterion of the rule ranking, rule-5 will be pruned because rule-5 is more specific than rule-4 but has a lower confidence. However, rule-2 will not be pruned off because it has a higher confidence than the more general rule-1. We can see that the third ranking criterion reflects the generality.

In previous study, after the whole classification rules were generated, then the pruning on the unnecessary rules was conducted ( Liu et al., 1998 ). In our pruning method utilizing CR-tree, we prune the useless rules at the same time when we insert the rules. Actually we do not prune but we determine whether to insert or not when the rule is newly extracted through the frequent pattern mining process. Different from the pruning in Li et al. (2001) , our method, fortunately, never makes a newly inserted rule prune the existing rules in a CR-tree, which makes the algorithm very simple. The reason for this simplicity is that since the words are sorted in the order of frequently occurring counts and are processed in that order, the frequent patterns are always generated with the longer length and are more specialized.

The detailed pruning process is as follows. Assume some classification rules are already stored in a CR-tree and we extract a new candidate rule:  X  X  X 2, 4, 7} ! c (3, 85%) X  X , of which the support is 3 and the confidence is 85%. We have to determine whether to insert the rule or not. To do this, first we examine whether the subset rules in the CR-tree has a higher rank than the candidate rule. This is accomplished by traversing the CR-tree following the node links of the header elements that are also the elements of the candidate rule. Fig. 3 presents the situation more clearly. Inside a node of Fig. 3 , the first line denotes a word code with a class name and the second line denotes the value of support and confidence respectively. Class information exists only at the last node of a classification rule in the CR-tree.

In a na X   X  ve approach to determine whether a certain subset rule has a higher rank, we examine for all the subset of the antecedent {2, 4, 7} of the candidate rule. This requires to expand the set into its power set, which takes O(2 n ) time if we assume the average number of elements in a rule to be n . However, if we use the CR-tree, we need not expand the set into its power set. Only for those rules in the CR-tree, we examine whether they are subsets of the candidate rule and have higher ranks. This job takes only O( n log n ) time because the length of the path is O(log n ) and we examine n elements in the node-links.

To do this, in Fig. 3 we follow the node links of  X 2 X ,  X 4 X , and  X 7 X . First we follow the node link of  X 7 X , then, following the path from 7 to the root, we examine the rules in the middle of the path that have all their ele-ments also in the candidate rule. This procedure is repeated with respect to the word  X 4 X  and  X 2 X . In Fig. 3 , the rule which is a subset of the candidate rule is  X  X  X 2,4} ! c(7,75%) X  X  and has a lower rank than the candidate rule. Therefore, the candidate rule would be safely inserted into the CR-tree (in other words, it would not be pruned). In Fig. 4 we summarize this pruning procedure into an algorithm.

Another type of pruning utilizes the Chi-square statistic, which provides the correlation information between two random variables. We want to evaluate the quality of a rule by calculating the Chi-square statistic of the pattern and the class label that are the left-hand and the right-hand side of the rule respectively. We can easily calculate the Chi-square statistic of each rule during frequent pattern mining. We denote the word pat-tern of a rule as P and the class label as c . Then, we present the number of the documents of the four possible cases in a box in Table 2 .

A denotes the number of all the documents. B denotes the number of the documents with the class label c . D denotes the number of the documents with matching pattern P , and E denotes the number of the documents labeled with class c and also with matching pattern P . The values of all the other cells can be calculated using these four values. In addition, we need the expected values of the numbers of documents in the four cells located at the center of the table. We can easily calculate these values as well using the ratios of the values of the marginal column and row. Finally, the statistic is calculated as follows: where i denotes the index of four center cells in the table.

Now, we can perform a hypothesis test whether the rule is important by the Chi-square statistic. According to some significance level, we decide whether we select the rule or not. 3.5. Prediction with multiple classification rules
After the training process is finished, we obtain a final set of classification rules. In general, when we predict the class of a test document, we seek the rules matching the document and the system produces more than one rule to classify with. As well as in the phase of the rule extraction, if we utilize the CR-tree in the prediction phase, we can efficiently find out matching rules to a test document.

The procedure of acquiring matching rules is very similar to the rule pruning procedure. This is not different to the task of finding out those rules in a CR-tree that are subsets of the test document. First, for every word element of the test document, we follow the node link of the word in the header table of the CR-tree. Then, starting from the last node of the path, we search the path for the subset rules climbing up to the root. We gather all the subset rules in this manner to determine the category of the test document. Fig. 5 shows the rule matching and classification algorithm.

Generally, the number of the matched classification rules is very large, which may lead to a difficult situ-But if we have many different classes from the matched rules, we need to decide on one rule as the correct one.
For example, assume that from Table 1 we acquired rule-2, rule-4, and rule-6 as matched rules of a test document. We have a split decision between class A and class C . According to the rule ranking criteria, we would select C as an answer class. However, inspecting more deeply, though the confidence of rule-2 is slightly better than the other two, the support values of the two are much higher than that of rule-2. Therefore, we know that we cannot always reliably select rule-2 as a correct answer.

Therefore, it is dangerous that we estimate a class label only by the rule ranking system. So, we adopt an majority-voting method when deciding on a correct class for a test document d rules. Assume that we have K rules which are matched with a test document and have a form of r for 1 6 k 6 K where c j is an element of the set of the possibly correct classes, C
And let S j be the score by which the class c j is estimated to be the correct class. With the majority voting we select the class label c  X  such that: and where R j is the set of the rules of which consequent class is c simplest form of the majority voting, a constant 1 is used as a weight value regardless of the rules. We also tried applying several variations of the majority voting method. We consider a method which adds as a weight the confidence value of the rule instead of the constant 1. This confidence value can be thought of a contri-bution of each classification rule to deciding the correct class. Among the many variations, the one which adds the square of the confidence of each rule showed the best classification performance.

In this scoring scheme, it is very rare for the classes to have the equal scores in Eq. (6) because there are hundreds of thousands rules and each decimal number of the rule X  X  score sums up different total values in most cases. If a tie occurred nevertheless, we would regard all the classes as answers. 4. Experiments and analyses
We performed various experiments on the associative classification using the 20 Newsgroups document col-lection ( Lang, 1995 ). This collection is slightly multi-labeled; 541 documents of the total 19,997 documents are posted to more than one newsgroup.

We pre-processed the raw texts into word vectors. For this purpose, we used the BOW toolkit ( McCallum, 1996 ). We removed general stop words, but did no stemming. During the training process, we included only the body part and the Subject line of the articles because other parts may contain the words that may indicate the answer class directly. We reduced the dimension of word feature space of the original 20 Newsgroups to three thousands, which was originally over one hundred thousand. One fourth of the dataset was used for test-ing and the remainder was used for training.

We implemented with C++ the procedures of our associative classification rule storing, pruning and pre-diction while the frequent pattern mining codes are based on Goethals (2003) . We executed our code on a
Linux machine with a 2.2 GHz CPU and 2 GBytes of memory. Samples of the classification rules are listed in Table 3 , and the best classification results are shown in Table 4 .

The overall performance of the system is a little bit lower than that of the current state-of-the-art research for the same 20 Newsgroups data set ( Bekkerman et al., 2001; Yoon, Lee, &amp; Lee, 2006 ). However, the five of the twenty classes show higher accuracy compared with those of the state-of-the-art systems ( the end of their names in Table 4 ). At the last column of Table 4 , we show the potential accuracy that we have acquired by considering the second and the third majority classes as answers as well as the first one. This fact shows that there is a big room for some improvement on the classification performance in the future.
In addition, since the rules are expressed in the intuitive form of word strings (refer to Eq. (2) ), we can man-ually edit the rules and improve the classification accuracy with little effort. For example, we may add the words listed in Table 3 that could best represent the target class. This is important in a practical application of the classifier since the real performance can be varied with the characteristic of the domain and the test data. Notice that the training time is very short; this is remarkable compared to the case of SVM or even Na X   X  ve X 
Bayes classifiers. Let the maximum length of a document be L , the size of the selected word features M , and the number of the whole words in the training collection N . In general, we take these parameters as L M N . The time complexity in the training using the whole words is O(2 complexity is O( N 2 ). Our training time in this paper is O(2
SVM if we select an appropriate L to be much smaller than M and far smaller than N through the feature selection.

The relation between the maximum length L of the training documents and the classification performance is shown in Table 5 . As the number L increases, we can acquire better classification accuracies while we need more time in the training phase. Since the time increases exponentially, we need to have a bound in acquiring better classification performance. In our experiment the reasonable number of the length L was 25. In the case of the larger numbers we failed to get any classification result within a reasonable time. We expect that we be able to enlarge the number L through improving the algorithms of frequent pattern mining and classification rule pruning and others.

Fig. 6 shows the relation between the number of classification rules and the overall accuracy. The more rules we have the higher accuracy we can achieve. However, as the number of the rules increases, the classi-number of the rules slows the prediction process too due to the longer rule matching time.

If the document length L gets larger, we have more rules to classify with. Fig. 7 represents the number of the rules in relation to the document length L . If we have the more rules, we can achieve the better classification performance. But we cannot use an unlimited number of rules because it takes too much time in the training.
Hence it is very important that we utilize as many effective classification rules as possible by applying efficient rule-pruning methods to diminish the amount of trivial and noisy rules. 5. Conclusion
Associative classification is a new method in the area of document classification. The expression of the clas-sification rule is easy and human-readable. Therefore, it presents an excellent interpretation on the classifica-tion result as well as considerable effectiveness. In addition, the construction of the classification framework is tion is an excessive number of rules produced in the training process. We overcome this by starting with the dimensionality reduction technique using the average mutual information of the word features. The relatively short training time can be achieved by applying our new method of storing and pruning the classification rules using the efficient CR-tree structure. In predicting with new documents, our majority voting method that con-siders the confidence values of the classification rules is very helpful to increase the classification accuracy.
Moreover, by conducting various classification experiments on the large data collection, we showed that this associative classification framework could be well applied to many real world applications.

With these many advantages of the associative classification, there are still some areas for further improve-ment. We plan to study in depth the feature selection method so that we can acquire more satisfactory accu-racy in classification. In addition, to overcome the limit in the number of word features in the training documents, a more efficient frequent pattern mining and rule pruning method should be required. It will also be helpful to improve the classification accuracy.
 Acknowledgement This research was supported by the MIC (Ministry of Information and Communication), Korea, under the
ITRC (Information Technology Research Center) support program supervised by the IITA (Institute of Infor-mation Technology Assessment).
 References
