 With the rise of digital video consumptions, contextual video advertising demands have been increasing in recent years. This paper presents a novel video advertising system that selects relevant text ads for a given video scene by automat-ically identifying the situation of the scene. The situation information of video scenes is inferred from available video scripts. Experimental results show that the use of the situ-ation information enhances the accuracy of ad retrieval for video scenes. The proposed system represents one of the pio-neer video advertising systems using contextual information obtained from video scripts.
 H.3.5 [ Information Storage and Retrieval ]: Online In-formation Services X  Commercial services Algorithms, Experimentation contextual video advertising, scene, script, situation
Over the past years, demands to serve ads on digital videos based on the content displayed to the user have been consis-tently increasing with the growing number of various digital devices and services. However, it is difficult to get direct access to the content of the video with existing visual anal-ysis and speech recognition techniques, because of not only the enormous computational cost they require but their low performance. Other video metadata, such as title, summary, or genre, provides very limited information for dynamically analyzing individual scenes within the video. A reasonable approach would be to utilize available video scripts that contain descriptions of scenes as well as dialogues. This approach is also practically applicable today, because it is easy to obtain such data on the Web especially for popular TV shows or movie contents.

In this paper, we demonstrate the implementation of an intelligent contextual video advertising system that is able to select ads for individual scenes in video contents with the Table 1: Situation categories (in no specific order). use of their scripts. A main difference of our system against a typical contextual advertising system is that, instead of scanning a given text (a scene script in this case) for bid ad-vertising keywords right away, the system examines the sit-uation of individual scenes using descriptions and dialogues in scripts to ensure contextually relevant advertising. For example, if the user is viewing a sports game scene, the user may expect to see ads for sports related topics, such as sportswear or sports equipments. We adopt a learning-based approach to classify individual scenes into (pre-defined) sit-uation categories. The ads are also automatically located in given categories in order to ensure that they would only ap-pear in the appropriate situation context. To the best of our knowledge, this work is one of the first attempts to match video scenes to text ads based on the situation information of the scenes inferred from video scripts.
As in [1], we have pre-defined a set of situation categories as follows. Category names of products and services were collected from various sources, such as shopping websites. For each category name, four volunteers were asked to sug-gest general scenes that the category reminds of. For exam-ple, a category name  X  X ealth X  may infer exercise scenes and hospital scenes. All suggested situations were manually flat clustered into 20 different clusters (categories) as shown in Table 1. Note that these categories do not cover all possi-ble situations that could happen in the real world, but they represent the ones that may often provoke advertising.
Figure 1 illustrates the architecture of the proposed sys-tem. A video is given along with its script as the input. The video is first decomposed into a series of individual scenes according to the video script. For each scene, the system extracts advertising keywords and analyzes the situation in-formation from the corresponding scene text in the script. A candidate list of ads is retrieved from the ad index with regard to the extracted keywords using a standard informa-tion retrieval model. Given the candidate list of ads and the situation information inferred from the script, the system finally re-ranks the list and outputs the top n ads that are determined to be most appropriate for each scene.
 The duty of the preprocessor module includes decomposing a given script input as a series of individual scenes and con-verting each scene text into machine readable format. For the scene decomposition, it uses a collection of hand-crafted rules with several cue expressions that reflect the beginning of a new scene. It also distinguishes dialogues from instruc-tions using pre-defined regular expressions. Some shallow linguistic analyses, such as part-of-speech (POS) tagging, are applied to the text for later use.
 The role of this module is to automatically extract advertis-ing keywords from each scene. Given a scene text, the mod-ule detects noun words and phrases as keyword candidates. For each keyword candidate, several features suggested in earlier advertising keyword extraction studies [1, 4], such as its frequency features, position features, and external adver-tising keyword log features, are extracted. The module com-putes the score of each candidate using a logistic regression model trained in advance with a set of correct and incorrect keyword samples. It finally outputs candidates that receive probability scores higher than a pre-defined threshold value as keyword queries for ad retrieval.
 Given a preprocessed scene text, this module takes a learn-ing based text categorization approach to classify the scene into one or more pre-defined situation categories. Because multiple situations can occur in a scene in general, we built a multi-class logistic regression model in advance with a training set of scene-situation pair samples. All situation categories in which the regression model outputs probabil-ity scores higher than a manually-tuned threshold value are selected. For features, we use the association measures (cal-culated using Pointwise Mutual Information) between all words in a scene text and individual situation categories instead of the traditional tf-idf features. This approach is similar to the one used in [3].
 Given a set of extracted keywords for each scene, the initial retriever module finds a list of top n candidate ads from the ad index using the Indri retrieval model [2] as the basis of textual relevance.
 Given a ranked list of candidate ads and the situation infor-mation for each scene, this module re-ranks the initial list so that ads relevant to the situation context of the scene are ranked higher. Because ads are generally not located in pre-defined situation categories, we built a multi-class logis-tic regression model for ads as for scenes. The regression model uses the same PMI features but is trained with dif-ferent training data, consisting of ad-situation pair samples. Ad candidates are ranked according to their probability of being classified as the situation category of the given scene.
We compare the quality of the final re-ranked list of the proposed system with the initial retrieval list, which rep-resents the result of conventional advertising that do not consider the situation context of individual scenes. We in-dexed 414,419 ad texts collected from various web search engines. For scene data, we randomly chose 75 scene texts from the data set used in [1], which consists of 3406 scene texts from the scripts of popular TV drama shows broad-casted in Korea. From each scene text, the system extracted average 3.25 keywords, which were used as queries for that particular scene. The relevance of the top n retrieved ads were manually judged by undergraduate students in 3-scale (2=relevant, 1=somewhat relevant, 0=non-relevant). Since video advertising systems can display only a few ads due to the limited screen size, we measure the Normalized Dis-counted Cumulative Gain (NDCG) at top k ranks. Table 2 shows the results. It is observed that the proposed system achieves better retrieval performance, which implies a better user experience than conventional advertising.
This work was supported by Samsung Electronics. [1] J.-T. Lee, H. Lee, H.-S. Park, Y.-I. Song, and H.-C. Rim. Finding advertising keywords on video scripts. In
Proc. SIGIR  X 09 , pages 686 X 687, 2009. [2] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Inf.
Process. Manage. , 40(5):735 X 750, 2004. [3] G. Mishne. Experiments with mood classification in blog posts. In Proc. SIGIR  X 05 Style Workshop , 2005. [4] W.-t. Yih, J. Goodman, and V. R. Carvalho. Finding advertising keywords on web pages. In Proc. WWW  X 06 , pages 213 X 222, 2006.
