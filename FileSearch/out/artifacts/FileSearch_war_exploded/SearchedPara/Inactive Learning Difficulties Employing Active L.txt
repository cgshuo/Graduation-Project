 Despite the tremendous level of adoption of machine learn-ing techniques in real-world settings, and the large volume of research on active learning, active learning techniques have been slow to gain substantial traction in practical applica-tions. This reluctance of adoption is contrary to active learn-ing X  X  promise of reduced model-development costs and in-creased performance on a model-development budget. This essay presents several important and under-discussed chal-lenges to using active learning well in practice. We hope this paper can serve as a call to arms for researchers in active learning X  X n encouragement to focus even more attention on how practitioners might actually use active learning. The rich history of predictive modeling has culminated in a diverse set of techniques capable of making accurate pre-dictions on many real-world problems. Many of these tech-niques demand training , whereby a set of instances with ground-truth labels (values of a dependent variable) are ob-served by a model-building process that attempts to cap-ture, at least in part, the relationship between the features of the instances and their labels. The resultant model can be applied to instances for which the label is not known, es-timating or predicting the labels. These predictions depend not only on the functional structure of the model itself, but on the particular data with which the model was trained. In many applications, acquiring a label for a particular in-stance comes at some cost. For example, one may employ human labor to  X  X anually X  examine the instance and record its label. In other applications, costly incentives, interven-tions or experiments may reveal labels. In such cases, simply labeling all available instances may not be practicable, due to budgetary constraints or simply a strong desire to be cost efficient. The dependence of a model X  X  predictive perfor-mance on the selection of data suggests that care should be taken. The importance of selective acquisition is evidenced by the vast number of research papers on active learning  X  using the models learned  X  X o far X  in the selection of subse-quent data for labeling.
 However, while active learning is theoretically appealing, it seems that the techniques have had difficulty gaining trac-tion with practitioners. For example, few papers in the lit-erature report on the use of active learning for  X  X eal X  appli-cations. 1 This essay discusses how the settings typically used in ac-tive learning research papers don X  X  necessarily represent the settings faced by real-world applications. This can result in the literature not providing sufficient guidance for the prac-titioner actually to apply active learning techniques. The purpose of these observations is to serve as a call to arms to active learning research X  X  motivation to focus on develop-ing active learning techniques that can be applied effectively. Consider a typical use case for which active learning may seem appealing: given a particular classification problem with some reasonable loss structure (e.g. is a document relevant to a topic or not?), a pool of unlabeled instances, a labeling workforce or procedure that incurs some cost, and a budget that restricts the number of instances to be labeled to a (small) subset of the available pool, construct a predictive model with the best possible performance for the budget X  X r at least one that is accurate enough to be useful for the practical problem. While this situation may seem extremely simple and an obvious application for active learning, there are several complexities that may stymie the practitioner X  X  application of active learning.
 The first and most obvious difficulty is the selection of the active learning technique itself. This is a non-trivial task: there are hundreds of published papers espousing different techniques for active learning, with neither a clear  X  X inner X  among them, nor an agreed-upon set of rules of thumb for when to use which technique. The quality of the resultant model can rest on the choice: poor selection may yield a model that performs far worse than would be achieved if one were to select instances for labeling at random. We can see examples of this in Figure 1 (discussed below) and in [9; 26]. The typical post-hoc analysis seen in the active learning research literature, comparing learning curves generated by different techniques on a given problem, simply does not apply in this situation. The practitioner does not have the data required.
The notion of  X  X eal X  applications of machine learning and data mining technology can be a touchy subject with re-searchers. Here we simply mean applications with true com-mercial or scientific import, where the labeling actually is done via active learning. This is in contrast to studies with  X  X eal X  data, where researchers apply and compare different active learning strategies.
 Similar choices faced in machine learning are solved by per-forming cross-validation. For instance, one may use cross-validation to decide the optimal value of a hyper-parameter, or to choose the best performing model class for a given data set. This is typically done by building and evaluating can-didate systems on a data set and picking the setting offer-ing the best performance. However, cross-validation is not applicable to selecting an active learning technique. Before choosing how to sample the data to label, there is no labeled data to perform cross-validation.
 One possible solution is to use some  X  X enerally safe X  active learning strategy initially, then using the acquired data for performing a subsequent cross-validation experiment com-paring the induced learning curves to determine the best strategy to use from then on. Such an experimental setup is inadequate. The data sample is biased by the prefer-ences of the initial active learner, and results would be un-reliable. For example, derived estimates of generalization performance could be arbitrarily inaccurate. Furthermore, it is unclear how much of the budget would need to be expended in order to choose the active learning technique (the very purpose of which is to optimize budget alloca-tion). While some hybrid active learning heuristics have been proposed, potentially combining the benefits of their constituent sub-methods [11; 5; 17], these techniques suf-fer from the same failings as their component methods: if the component methods do not work well, or violate the as-sumptions of the hybrid technique on a particular problem, then the greater techniques will fail to perform as promised. Furthermore, these techniques rely on performance estima-tions, which if based on the sample drawn for training via the active learner itself, evoke the same problems discussed above.
 Alternately, one may simply perform random sampling to gather an initial, experimental data set free from the biases of a particular active acquisition strategy. While this would help achieve more reliable learning curves, such a strategy would defeat the intent of performing active learning in the first place by wasting valuable budget on this random ac-quisition. The selected instances are likely to differ substan-tially from those that would be selected by a more intelli-gent selection process, particularly in a large pool. Further-more, again it X  X  unclear how many sample instances would be necessary to produce reliable comparable active learning curves, and even reliable initial learning curves may not be an indicator of the eventual performance of the strategies considered. Some strategies have been observed to gather a degree of knowledge quickly in certain settings, only to taper off without offering exceptional performance for many subsequent selections, other techniques have been seen to excel at refining already  X  X mart X  classifiers, therefore being preferable in the latter stages of active learning [11]. While recent work has examined data acquisition strate-gies purely for the assessment of model performance [25], it remains unclear how to distribute a limited budget be-tween a system intended for self-evaluation, and a system intended for model induction; improved data gathering for model evaluation would only serve to reduce the expenses in-curred by using random sampling for evaluation as described above. Additionally, techniques have been proposed for per-forming unsupervised assessment [24; 17]. While these sur-rogate metrics are convenient and may be useful in certain contexts, the reliance on approximations derived from the currently model may be unreliable. Subsequent decisions based on, for instance, minimizing the pool entropy, may simply strengthen the biases already held by the trained model [2]. Furthermore, these metrics may differ substan-tially from the actual loss describing the problem.
 To our knowledge, the only safe way for a practitioner to proceed is to use the literature to select an active learning strategy that is reasonably stable X  X .e., one that performs reasonably well on a wide variety of tasks. For example, uncertainty sampling (selecting for labeling those instances from the available pool with the least certain predictions [15]) is by far the most widely studied active learning tech-nique. Uncertainty sampling is the typical baseline for stud-ies of more elaborate active learning strategies, and with good implementations is equivalent to selecting the instances closest to the separating hyperplane of a linear classifier like a support-vector machine [31] and to practical implemen-tations of query-by-committee [28]. However, using uncer-tainty sampling leaves the practitioner feeling inadequate; as with other techniques used widely as baselines for re-search papers, uncertainty sampling also is the technique most widely shown to be worse than other strategies! 2 A second obvious question also has a subtle dimension. For most not-yet well-understood predictive modeling problems, practitioners face the question of what base learner should be used. Similarly to the case for selecting an active learning strategy (just discussed), we do not have a set of training data on which to inform the choice of a base learner (e.g., via cross-validation). Settles [27] suggests that in settings where the ideal base learner is unknown, a practitioner may be advised to play it safe and prefer random sampling to an active learning strategy that may result in a poor model: However, when considering active learning in practice, un-der what conditions would one know the best model class and feature set in advance? For most practical applications, that would assume that you already have a large, represen-tative set of labeled training data! Expending some of the budget to randomly (or actively) sample a small data set to choose the base learner is not a satisfactory answer. Perlich et al. [22] show quite clearly that given two popular classi-fier inducers, choosing the learner that performs well for a small data subset often will lead to the wrong choice for a large data set: learning curves often cross. However, to our knowledge there is no good guidance besides experimenta-tion with labeled data to know what exactly  X  X arge X  means in this context for a particular application. There exists a body of work exploring halting heuristics for the active Sometimes including random sampling.
 1 : 1 to 10 , 000 : 1. learning process, for instance when further data acquisition is no longer beneficial [38; 32; 14; 12; 37; 6; 17]. However, this work all assumes that one decides on a particular model (functional) form and learning algorithm prior to perform-ing active learning. Convergence of this particular technique may have resulted from a poor initial choice and may not approach what a better model choice could achieve.
 The lack of data for deciding on a base model seems to have particular import for model-specific active learning tech-niques, for example those designed specifically for support-vector machines [31; 35; 12]. Since any given modeling pro-cedure is better on some domains and worse on others, 3 un-der what conditions would one of these model-specific active learning techniques be justified? Moreover, model-specific techniques aside, there is the issue of the interaction between the choice of active learning strat-egy and the base learner used. If the data to label have been chosen based on one active learning strategy using one base learner, are those good data for use with a different base learner (and possibly a different active learning strategy)? Given that some learners are indeed much better for small amounts of data, and others for larger amounts of data, should active learning strategies be designed appropriately? The literature here has expressed mixed results, with sev-eral papers expressing positive results in this  X  X euse X  setting: transferring a dataset selected my one class of base learn-ers towards the induction of another model class [16; 30]. Other work has observed difficulties transferring actively se-lected data sets amongst heterogeneous base learners [18; 4; 20]. One proposed solution is to perform active learning us-ing ensembles consisting of diverse classes of base learners, potentially alleviating the bias towards a particular type of model [18; 4].
 Returning to model-specific strategies, it may be the case that the best choice for active learning is a base learner that would be suboptimal if one were to have all the data, possi-bly because the combination of this learner and its model-specific active learning technique are better than a generic active learning strategy combined with a would-be better base learner.
 Thus, Settles X  advice amounts to: don X  X  use active learn-ing on real problems where you do not already have a large
For example, in one well-cited comprehensive experimental comparison [8], support-vector machines were not the best model to choose for any data set. amount of labeled training data! Exceptions may include fine-tuning a model that X  X  already known to perform well (possibly due to some previous learning or knowledge en-gineering), and systems where the model form is fixed for other, domain-specific reasons (e.g., a credit-scoring appli-cation may demand a logistic regression model).
 The practice of active learning could benefit from a different sort of research than  X  X y active learning algorithm is better than yours, assuming that I X  X  using learner L . X  Instead (or in addition), it is important to study robust active learning techniques: techniques that (i) have good worst-case perfor-mance across learners and domains X  X s compared to using random sampling with a good learner for that domain X  and (ii) that also often perform significantly better than random sampling. An alternative (and possibly more am-bitious) goal would be (partially) unsupervised methods for estimating the learning technique and active learning tech-nique that in concert would perform well on a given domain, for instance, using a grand expected-utility formulation over the space of learner/active-learner combinations. Practical applications rarely provide us with data that have equal numbers of training instances of all the classes. In many applications, the imbalance in the distribution of nat-urally occurring instances is extreme. For example, when labeling web pages to identify specific content of interest, uninteresting pages may outnumber interesting ones by a million to one or worse (consider identifying web pages con-taining hate speech, in order to keep advertisers off them, cf. [3]).
 Unfortunately, when the data distribution is skewed, active learning strategies can fail completely X  X nd the failure is not simply due to the challenges of learning models with skewed class distributions, which has received a good bit of study [33]. The lack of labeled data compounds the prob-lem, because techniques cannot concentrate on the minority instances, as the techniques are unaware which instances to focus on.
 Figure 1 compares the area under the ROC curve (AUC) of logistic regression text classifiers induced by labeled in-stances selected with uncertainty sampling and with random sampling. The learning task is to differentiate sports web pages from non-sports pages. Depending on the source of the data (e.g., different impression streams from different on-line advertisers), one could see very different degrees of class skew in the population of relevant web pages. The panels in Figure 1, left-to-right, depict increasing amounts of induced class skew. On the far left, we see that for a bal-anced class distribution, uncertainty sampling is indeed bet-ter than random sampling. For a 10:1 distribution, uncer-tainty sampling has some problems very early on, but soon does better than random sampling X  X ven more so than in the balanced case. However, as the skew begins to get large, not only does random sampling start to fail (it finds fewer and fewer minority instances, and its learning suffers), un-certainty sampling does substantially worse than random for a considerable amount labeling expenditure. In the most ex-treme case shown, 4 both random sampling and uncertainty sampling simply fail completely. Random sampling effec-tively does not select any positive examples, and neither does uncertainty sampling. 5 A practitioner well-versed in the active learning literature may decide she should use a method other than uncertainty sampling in such a highly skewed domain. A variety of techniques have been proposed for performing active learn-ing specifically under class imbalance [29; 7; 36; 12; 13], as well as for performing density-sensitive active learning, where the geometry of the problem space is specifically in-cluded when making selections [38; 10; 21; 35; 19]. While initially appealing, these techniques may not provide results better than more traditional active learning techniques X  indeed class skews may be sufficiently high as to thwart these techniques completely [3].
 Attenberg and Provost [3] proposed an alternative way of us-ing human resources to produce labeled training set, specifi-cally tasking people with finding class-specific instances ( X  X uided learning X ) as opposed to labeling specific instances. In some domains, finding such instances may even be cheaper than labeling (per instance). Guided learning can be much more effective per instance acquired; in one of Attenberg and Provost X  X  experiments it outperformed active learning as long as searching for class-specific instances was less than eight times more expensive (per instance) than labeling se-lected instances. The generalization performance of guided learning is shown in Figure 3, discussed below, for the same setting as Figure 1. Even more subtly still, certain problem spaces may not have such an extreme class skew, but may still be particularly dif-ficult because they possess important but very small disjunc-tive sub-concepts, rather than simple continuously dense re-gions of minority and majority instances. Prior research has shown that such  X  X mall disjuncts X  can comprise a large portion of a target class in some domains [34]. For active learning, these small subconcepts act like rare classes: if a learner has seen no instances of the subconcept, how can it  X  X now X  which instances to label? Note that this is not simply a problem of using the wrong loss function: in an 10,000:1  X  still orders of magnitude less skewed than some categories
The curious behavior of AUC &lt; 0 . 5 here is due to overfit-ting. Regularizing the logistic regression  X  X ixes X  the prob-lem, and the curve hovers about 0 . 5. See another ar-ticle in this issue for more insight on models exhibiting AUC &lt; 0 . 5 [23]. active learning setting, the learner does not even know that the instances of the subconcept are misclassified if no in-stances of a subconcept have yet been labeled. Nonetheless, in a research setting (where we know all the labels) using an undiscriminative loss function, such as classification ac-curacy or even the area under the ROC curve (AUC), may result in the researcher not even realizing that an important subconcept has been missed.
 To demonstrate how small disjuncts influence (active) model learning, consider the following text classification problem: separating the science articles from the non-science articles within a subset of the 20 Newsgroups benchmark set (with an induced class skew of 80 to 1). Figure 2 examines graphi-cally the relative positions of the minority instances through the active learning. The black curve shows the AUC (right vertical axis) of the models learned by a logistic regression classifier using uncertainty sampling, rescaled as follows. At each epoch we sort all instances by their predicted proba-bility of membership in the majority class,  X  P ( y = 0 | x ). The blue dots in Figure 2 represent the minority class instances, with the value on the left vertical axis showing their rela-tive position in this sorted list. The x-axis shows the active learning epoch (here each epoch requests 30 new instances from the pool). The blue trajectories mostly show instances X  relative positions changing. Minority instances drop down to the very bottom (certain minority) either because they get chosen for labeling, or because labeling some other in-stance caused the model to  X  X ealize X  that they are minority instances.
 Figure 2: A comparison of the learned model X  X  ordering of the instance pool, along with the quality of the cross-validated AUC.
 We see that, early on, the minority instances are mixed all throughout the range of estimated probabilities, even as the generalization accuracy increases. Then the model becomes good enough that, abruptly, few minority class instances are misclassified (above  X  P = 0 . 5). This is the point where the learning curve levels off for the first time. However, notice that there still are some residual misclassified minority in-stances, and in particular that there is a cluster of them for which the model is relatively certain they are majority instances. It takes many epochs for the active learning to select one of these, at which point the generalization per-formance increases markedly X  X pparently, this was a sub-concept that was strongly misclassified by the model, and so it was not a high priority for exploration by the active learning.
 On the 20 Newsgroups data set we can examine the minor-ity instances for which  X  P decreases the most in that late rise in the AUC curve (roughly, they switch from being mis-classified on the lower plateau to being correctly classified afterward). Recall that the minority (positive) class here is  X  X cience X  newsgroups. It turns out that these late-switching instances are members of the cryptography (sci.crpyt) sub-category. These pages were classified as non-Science pre-sumably because before having seen any positive instances of the subcategory, they looked much more like the many computer-oriented subcategories in the (much more preva-lent) non-Science category. As soon as a few were labeled as Science, the model generalized its notion of Science to include this subcategory (apparently pretty well).
 Density-sensitive active learning techniques did not improve upon uncertainty sampling for this particular domain. This was surprising, given the support we have just provided for our intuition that the concepts are disjunctive. One would expect a density-oriented technique to be appropriate for this domain. Unfortunately in this domain X  X nd we conjec-ture that this is typical of many domains with extreme class imbalance X  X he majority class is even more disjunctive than the minority class. For example, in 20 Newsgroups, Science indeed has four very different subclasses. However, non-Science has 16 (with much more variety). Techniques that (for example) try to find as-of-yet unexplored clusters in the instance space are likely to select from the vast and var-ied majority class. We need more research on dealing with highly disjunctive classes, especially when the less interest-ing 6 class is more varied than the main class of interest. The cold start problem has long been known to be a key diffi-culty in building effective classifiers quickly and cheaply via active learning [38; 11]. Since the quality of data selection directly depends on the understanding of the space provided by the  X  X urrent X  model, early stages of acquisitions can re-sult in a vicious cycle of uninformative selections, leading to poor quality models and therefore additional poor selections. The difficulties posed by the cold start problem can be par-ticularly acute in highly skewed or disjunctive problem spaces; informative instances may be difficult for active learning to find due to their variety or rarity, potentially leading to substantial waste in data selection. Difficulties early in the active learning process can, at least in part, be attributed to the base classifier X  X  poor understanding of the problem space. This cold start problem is particularly acute in oth-erwise difficult domains. Since the value of subsequent label selections depends on base learner X  X  understanding of the problem space, poor selections in the early phases of active learning propagate their harm across the learning curve. In many research papers active learning experiments are  X  X rimed X  with a preselected, often class-balanced training set. As pointed out by [3] if the possibility and procedure exists to procure a class-balanced training set to start the process, maybe the most cost-effective model-development alternative is not to do active learning at all, but to just continue using this procedure. This is exemplified in Fig-ure 3 [3], where the red lines show the effect of investing
How interesting a class is could be measured by its relative misclassification cost, for example. resources to continue to procure a class-balanced, but oth-erwise random, training set (as compared with the active acquisition shown in Figure 1). Active learning as a field has shown tremendous theoretical potential to help us to build predictive models quickly and cheaply. However, slow adoption in practice suggests that practitioners face difficulties realizing this potential. This paper illustrates a surprising array of practical difficulties, including: 1. how to choose (cost-effectively) the active learning tech-2. how to choose (cost-effectively) the base learning tech-3. how to deal with highly skewed class distributions, 4. how to deal with concepts including very small sub-5. how best to address the cold-start problem, and espe-6. whether and what alternatives exist for using human We do not intend this essay to be an indictment of active learning research, a field responsible for substantial strides in understanding the problem of cost-effectively acquiring labeled training data. Rather, we hope that it can serve as a call to arms to the research community. We cannot take the current volume of published papers on active learning as a sign that the problem is  X  X olved. X  As practitioners, we need more research focused on these fundamental questions. It would benefit both the research and practitioner commu-nities if active learning researchers were to view the practi-cal application of active learning techniques as a motivating framework within which to select the important research questions on which to work.
