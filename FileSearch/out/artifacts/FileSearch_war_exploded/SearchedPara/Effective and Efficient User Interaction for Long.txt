 Handling long queries can involve either pruning the query to retain only the important terms (reduction), or expand-ing the query to include related concepts (expansion). Whil e automatic techniques to do so exist, roughly 25% perfor-mance improvements in terms of MAP have been realized in past work through interactive variants. We show that se-lectively reducing or expanding a query leads to an average improvement of 51% in MAP over the baseline for standard TREC test collections. We demonstrate how user interac-tion can be used to achieve this improvement. Most interac-tion techniques present users with a fixed number of options for all queries. We achieve improvements by interacting less with the user, i.e., we present techniques to identify the op -timal number of options to present to users, resulting in an interface with an average of 70% fewer options to con-sider. Previous algorithms supporting interactive reduct ion and expansion are exponential in nature. To extend their utility to operational environments, we present technique s to make the complexity of the algorithms polynomial. We finally present an analysis of long queries that continue to exhibit poor performance in spite of our new techniques. H.3.3 [ Information Search and Retrieval ]: Query for-mulation Algorithms, Experimentation, Performance User Interaction, Interactive Retrieval Efficiency, Query R e-duction, Query Expansion, Query Analysis  X 
This work was done while the author was a graduate stu-dent at University of Massachusetts Amherst
Past work suggests that richer expressions of information need by users can be leveraged to improve search perfor-mance. The richer expression can take the form of longer than usual queries, i.e., more than two to four terms in length [17], inclusion of additional terms the user believe s are related to the query [14], identifying documents contai n-ing similar information [23], identifying topics the query is related to [16] and so on. We refer to all such expressions as long queries . Handling long queries is however difficult as they usually contain a lot of noise. This noise is in the form of extraneous terms that the user believes are important to conveying the information need, but in fact are confusing to automatic systems. Creating a more concise query by identifying and retaining the important terms in the long query (query reduction ) is thus an important and challenging problem that needs to be solved. As opposed to automatic means, interactive query reduction (IQR) [17] is particula rly effective (Section 2) in solving this problem.

Automatic query expansion techniques like pseudo-relevance feedback (PRF) [19] also help improve performance for cer-tain types of long queries. Greater gains can be obtained from interactive query expansion (IQE)(Section 2), which involves asking the user to help remove wrong terms sug-gested by the automatic technique.

User interaction requires cognitive and physical effort fro m the user. Guided by the philosophy that users must get the maximum benefit (effectiveness) for their investment of time and effort, we explore ways to make user interaction for long queries more effective in Section 3. Determining when to interactively reduce or expand a query (Figure 1) can de-liver the sort of improved effectiveness (over 50%) we seek. Automatic techniques to consistently determine whether to expand a query or not have had limited success [7]; we will show we can perform selective interactive reduction and ex-pansion (SIRE) using implicit feedback from the user.
Our past explorations of IQR and IQE techniques [17] in-volved asking a user to select from ten options for each and every query. This can be detrimental to the user experience. Developing techniques to identify and present a minimal set of options to users is thus important. After demonstrating the similarity of the problem with Set Cover , a NP-Complete optimization problem, we utilize a greedy algorithm to pro-vide an approximate solution (Section 4). Additionally, we exploit the presence of redundant information in the inter-face to further prune the set of options presented to the user.
Our previous work on IQR and IQE [17] involved analyz-ing all possible combinations of the terms in the long query ( sub-queries ) or set of terms suggested by PRF ( expan-sion sets ) to determine the set of top options 1 to present to users. Such a technique is difficult to realize in prac-tice due to the exponential number of options that need to be analyzed. In Section 5 we present a technique based on analyzing the properties of ideal queries, and using those observations to prune the option search space.

While our techniques helped improve performance of a significantly larger fraction of long queries compared to au -tomatic techniques, there still remained a few queries that were not amenable to either automatic or interactive han-dling. In Section 7 we analyze such queries and categorize the reasons for their failure. We believe this analysis will be useful not only to determine the categories of problems that have been addressed by our techniques, but also to help plan strategies to tackle those that were not.
In this section we provide an overview of the interaction technique for long queries that we build on in this paper. The right side of Table 1 provides an example of the inter-face provided to users in response to a long query for IQR. The long query in the example is the description portion of TREC Topic 601. Sub-queries are presented to the user along with a corresponding top-ranking snippet of text re-trieved by each of them. Which sub-queries to select for display from the available exponential number of choices is based on a technique described in the next paragraph. The tabbed interface allows the user to click on each sub-query, view the associated snippet, and select the most promising one as their new query. The table also contains various per-formance measures (not shown to the user) that provide an idea of the utility of each sub-query. Notice that simple ad-dition and deletion of terms can produce marked changes in performance. The interface for IQE is similar: the sub-options. queries are replaced by subsets of terms (expansion sets) from a set identified using PRF.

To identify top-ranking options we represented each of the 2 n options as a graph constructed with the constituent terms as vertices, and the mutual information [4] between the terms as edge weights. The maximum spanning tree [5] was identified on each graph, and its weight used to repre-sent the quality of the option. After ranking the entire set of options by the weight of their corresponding maximum spanning trees, the top ten were selected.

In previous work [18] we performed user studies that demon-strated that users could use such an interface to select bett er alternatives, and obtain significant improvements in perfo r-mance. This performance was also better than that acheived by simply using the title portion of the TREC query. Since we are improving on that interaction technique, in this pa-per we will confine ourselves to performing simulated user studies. We hypothesize that improvements such as more efficient background processes, fewer options presented to users, and better quality of options will naturally extend t o improving the interaction experience and performance.
Table 2 shows the best performance that can be achieved under various conditions. All results are reported for 249 TREC description queries from the Robust 2004 track. We will treat the description portion of TREC queries as long queries for our experiments. Baseline refers to a query-likelihood (QL) run using the Indri search engine [24], whil e per Bound X  refers to the situation when the best sub-query and best expansion set was used for query reduction and expansion respectively. In other words, if we had access to an oracle that always provided us the best sub-query and best expansion set for a query, we can obtain the indicated upper bound on performance.  X  X nteraction Upper Bound X  refers to the upper bound on the performance that can be obtained from user interaction, i.e. the user always select s the best option from the ten presented. bounds. We performed comprehensive parameter sweeps to determine the best parameter settings. The performance in practice will be lower as sub-optimal parameters will be learned for each collection by training on other collection s. Table 2: The utility of IQR and IQE. Italicized val-ues indicate that the scores are significantly better than the baseline, while those in bold are signifi-cantly better than PRF. Statistical significance was measured using a paired t-test, with  X  set to 0.05. Figure 1: Difference in MAP due to Selective IQR and IQE.
The results in Table 2 for query reduction and expansion show that user interaction can lead to significant improve-ments in performance for long queries. Further improve-ments can be obtained if we selectively invoke IQR or IQE. Figure 1 shows the ordered distribution of the difference be-tween the potential gains due to IQR and IQE. Some queries are better suited for IQR, while others can be better im-proved through IQE. If we can selectively invoke IQR or IQE for each query we can potentially obtain a 51% (from 0.240 to 0.363, compared to 0.300 and 0.292 for only IQR and only IQE respectively) improvement in MAP over the baseline. Determining when to reduce and when to expand is similar in flavor to the problems of determining when to perform PRF [7] or when to perform stemming [9]: correct answers to either can lead to significant improvements in performance. The tremendous scope for improvement makes the reduce/expand problem worthy of further investigation . We will show in Section 3 that we can address this problem through implicit feedback from the user.

The current interaction paradigm involves always present-ing users with ten options for all queries. There is clearly scope for reducing the number of options presented to users, especially when on average only three out of ten of them are better than the baseline. Figure 2 is a histogram of the number of options better than the baseline for each of the 249 queries we used for training. Clearly, a large fraction Figure 2: Distribution of the number of options in the ten presented to users that are better than the baseline query, for a set of 249 training queries. of the options presented to users have no utility, and can potentially degrade the user experience. In Section 5 we present techniques that enable us to reduce the number of options we present to users significantly, without degradin g performance.

IQR and IQE as reported in past work require a large amount of background processing. For query reduction the top ten options have to be selected from an exponential number of candidates since a query of length n has 2 n sub-queries. Similarly, for query expansion, we need to analyze all 2 n combinations of expansion terms from the n suggested by PRF. Such exhaustive exploration of the sub-query space is infeasible in an operational environment. Also in Sectio n 5 we will present a simple technique based on empirical obser-vations that significantly reduces the search space, withou t sacrificing performance.
Tremendous gains in performance can be obtained by se-lectively expanding or reducing long queries. For each long query, our approach involved selecting the top five sub-quer ies and top five expansion sets and providing the user a merged list for interaction. The downside of this technique was tha t we risked losing potentially useful options ranked between six and ten. However, as Table 3 shows, this risk was in-significant when compared to the potential for improvement through SIRE. By viewing this mix of expansion and reduc-tion options, along with a snippet of text to guide selection , the user can implicitly guide the system towards expansion or reduction of the query.

Table 3 summarizes the improvements in performance that IQE are used with five options, the performance is as de-tailed. However, when the options are combined (SIRE comb technique described later in Section 5 Table 3: The subscript in the system name in-dicates the number of options presented to users. The SIRE comb technique involves merging IQR 5 and IQE 5 . For comparison, performance of IQR and IQE with ten options is also provided. the user can potentially achieve better performance than could be achieved with either IQR or IQE with not only five, but also ten options. If we started with using ten op-tions each from IQR and IQE, we can expect even higher performance improvements. Another interesting aspect of the result is that MAP is significantly improved. IQR is pri-marily a precision enhancing technique, while IQE is both precision as well as recall enhancing. The advantages of eac h technique have thus been carried over to the hybrid SIRE technique in the form of improved MAP.
The technique to analyze the options makes use of co-occurrence information of the constituent terms. While thi s provides a good sense of the cohesiveness of the option, it does not inform the user of the relative utility of an option with respect to the other ones shown. Given that some op-tions differ by just a single term, its quite likely that they might all direct the user to the same search space. In such cases it is wasteful to show similar options, and instead dis -playing a minimal subset of options that covers the original search space(s) might be better. This intuition forms the basis for our technique to prune the original set of options shown to the user. We introduce the Set Cover problem before going into the details of our technique.

The set covering problem [5] is a NP-complete optimiza-tion problem. An instance ( X, F ) of the set covering prob-lem consists of a finite set X and a family F of subsets of X , such that every element of X belongs to at least one subset in F . Mathematically, The subset S is said to cover the elements in X . The goal is to find a minimum-size subset C , C X  X  , whose members cover all of X i.e.

Since finding the exact solution is NP-Complete, we used a greedy set cover algorithm [5] that works by selecting the subset that covers the most number of elements in X in an iterative fashion. The advantage of using the greedy algo-rithm is that it not only runs in time polynomial in | X | and |F| but also returns a (ln( | X | ) + 1) approximation to the solution.
We now show how the problem of finding minimal option sets can be cast as a set cover problem. Each option is used as a query to retrieve a set of ten documents. Let X be the union of sets of ten documents retrieved. The sets of ten documents correspond to the family F of subsets whose union is X . Our goal is to identify a minimal set of subsets C from F that cover X .

Table 4 shows the impact on performance metrics as well as the number of options presented to users due to the var-ious techniques. We can observe in the case of  X  X et-Cover-based Pruning X  for IQR that for an average decrease of two options per query, there is no (statistically) significant d rop in performance. In the case of IQE, it is drastic: an average reduction of six options per query without significant per-formance loss. This result for IQE can be understood by considering the fact that query expansion results in a much longer query than the original, and the subtle differences between options (usually by a term or two) do not lead to radically different sets of documents being retrieved. The results for SIRE show that the pruning strategy works for it too, and performance comparable to IQR is achievable by showing 50% fewer options. In summary, judging by the insignificant drops in performance, we successfully retain ed the useful options and removed the redundant ones.
The user is guided in making a decision on which option to select using a snippet of text. This snippet is extracted from the top-ranking document that is retrieved when the option is used as a query. Frequently, the snippets returned by different options are the same, making the task of se-lecting an option difficult. Retaining a single option from the set that retrieves the same snippet can further decrease the number of options presented to users. The results for  X  X nippet-based Pruning X  in Table 4 show the impact of this pruning strategy. With the exception of P@5 for IQE, this strategy results in an average reduction of approximately one option without significantly impacting performance.
A good query is long enough to describe key concepts but also short enough to avoid containing unnecessary terms. To determine the appropriate length of sub-queries, we plotte d the distribution of the query lengths of the best performing sub-query for each query in our training set. Figure 3 shows the distribution, and compares it with the distribution of the lengths of the original queries. We can observe that the best sub-queries are never more than ten terms in length, with most having six or fewer. This observation informed the decision to restrict analysis of sub-queries to those of length less than or equal to six. Table 5 shows the impact on  X  X nteraction Upper Bound X  performance due to this restric-tion. The restriction not only results in a reduced number of sub-queries analyzed but also maintains the potential fo r improvement through IQR.

In a similar fashion, we analyzed the size of the best ex-pansion subset for our training queries (Figure 4). We can observe that the best expansion sets are frequently between eight to twelve terms in length. This observation again in-formed the decision to restrict analysis of expansion sets t o those of length less than or equal to twelve. The section for IQE in Table 5 conveys that this restriction actually helped users. Figure 3: Distribution of lengths of original and best reduced queries Figure 4: Length distribution of best expansion sets Table 5: Effect of thresholding the lengths of options analyzed. UB refers to Upper Bound. avoid some bad options, and raised the potential for im-provement through IQE.
We used version 2.6 of the Indri search engine, developed variant of statistical language modeling as our baseline, a nd used the PRF mechanism based on relevance models [19] to generate terms for IQE.

As our data sets we used the TREC Robust 2004, Robust 2005 [26], TREC 5 ad-hoc [27] and HARD 2003 [2] document collections. The 2004 Robust collection contains around ha lf a million documents from the Financial Times, the Federal Register, the LA Times, and FBIS. The Robust 2005 col-lection is the one-million document AQUAINT collection. The choice of Robust tracks was motivated by the fact that the associated queries were known to be difficult, and con-ventional IR techniques were known to fail for a number of them. The TREC 5 ad-hoc collection consists of TREC disks 1 and 2, and presented a standard ad-hoc retrieval set-ting. The HARD 2003 collection, a subset of the AQUAINT corpus and US government corpus containing 372,219 doc-uments in all, was also selected since it was created for a track with focus on user interaction. The fifty queries in the Robust 05 data set overlap with those in the Robust 04 data set we used for training. However, since the collection s are different, we do not stand the risk of over-fitting. The HARD data set uses the same collection as the Robust 04 http://www.lemurproject.org data set, but has a different set of fifty queries. Finally, the TREC 5 data set shares neither the queries nor the col-lection with the Robust 04 data set. We believe that this choice of test data sets will provide a comprehensive valida -tion of our techniques. All collections were stemmed using the Krovetz stemmer provided as part of Indri. 249 queries from the TREC Robust 2004 track were used to study the impact of the various techniques presented in this paper, an d to learn parameters used for thresholding. The remaining 150 queries, 50 each from the three remaining tracks, were used to test the generality of our techniques.

Al-Maskari et al. [1] have shown that measures based on cumulative gain [13] and precision correlate well with user s X  satisfaction of the results. For all systems, we report pre-cision at five documents (P@5), precision at ten documents (P@10), normalized discounted cumulative gain at 15 doc-uments (NDCG@15, as defined in [25]), and mean average precision (MAP).
In this section we analyze the effect of using our techniques on different data sets, the results of which are presented in Table 6.

For all collections we notice trends similar to that observe d for the Robust 04 data set namely the higher performance of the SIRE system compared to IQR and IQE with not only five options but also ten. SIRE remains competitive or better even after option pruning: with an average of six options it meets or beats ten-option IQR and IQE.
We now analyze the performance of IQR, IQE, and SIRE with respect to the baseline (automatic) system. Figure 5 shows the scatter plots of the MAP values of the baseline system with respect to each of the interactive techniques, f or 249 Robust 04 queries. The line y = x is included to identify the queries that were improved or hurt by each technique. A point above the y = x line means that performance was improved through interaction, while a point below the line means that interactive retrieval did not help the query. We can observe that a larger fraction of queries was improved by IQR (Figure 5(a)) in comparison to IQE (Figure 5(b)). The plot for IQE (Figure 5(b)) has greater spread, and higher density in the upper left hand corner compared to IQR. This means that when IQE helps, it helps to a greater ex-tent than IQR. However overall improvements are mitigated by the fact that IQE performs worse on already poorly per-forming queries. The SIRE system combines the best of IQR and IQE. Not only are there fewer queries below the y = x line, but the density in the upper left hand corner is greater. These observations mean that SIRE provides a more comprehensive improvement over a set of queries.
We now turn our attention to the lower left hand cor-ner of Figure 5(c) -the area containing the set of queries that were not only poorly-performing to start with, but also were unaffected by IQR, IQE, and SIRE. We define poorly-performing queries as those that had a baseline, IQR, IQE, and SIRE MAP of less than or equal to 0.1. We analyzed each of the 45 such queries, and also the corresponding best reduced and expanded versions that were used to generate the  X  X pper Bound X  scores in Table 2. For situations when the best reduced and expanded queries were themselves low-performing, it was clear that the user would have to enter a completely new query. Table 7 summarizes the failure categories we identified and suggests directions for future Figure 5: Scatter plots of baseline performance (MAP) and performance due to IQR, IQE, and SIRE. IQR and IQE used ten options, while SIRE used a combination of five options from each of IQR and IQE Table 7: Breakdown of the analysis of low-performing queries. By NLP techniques, we refer to identification of phrases in the query and treat-ing them as a unit. work.  X  X ystem failure in identifying sub-query (or expansi on set) X  refers to the situation when a better option was avail-able, but the technique we used to rank the options failed to place it in the top 10. Of these, 3 of the options were of the type that a user with a similar information need could be expected to issue. Another 14 (10+4) of them would have been difficult for a human to come up with without a complete understanding of how the underlying search engine works. For the 4 queries for which NLP techniques would have worked, we expect that identifying noun phrases in the original query would have helped.
Irrespective of the environment, most user studies [17, 14] have reported improvements in performance from user inter-action. Various studies [12, 15] also acknowledge the impor -tance of good interfaces and decision support mechanisms to realize the potential of user interaction. Our work continu es along this line and shows that given the right interactive technique and support mechanism, user interaction can pro-vide great mileage.

An earlier exploration involving the user in IQE was car-ried out by Harman [8]. Positive user experiences were ob-served. Magennis and Van Rijsbergen [20] extended these investigations to simulated experiments on a larger scale. The idea of expanding the original query with sub-sets of predetermined length from the expansion term set is similar to ours, though the motivation was to find an upper bound on performance. Ruthven [21] extended this idea further by examining various query expansion techniques and per-forming user studies to compare IQR and IQE. His experi-ments showed that while there is potential for improvement through IQE, realizing the potential in practice is depen-dent on a number of limiting factors. Salton and Lesk [22] showed that user selection of expansion terms did not do as well as just having the system expand automatically. They reasoned that users did not know enough about how the IR system worked to do effective prediction. This problem can however be solved by showing users a preview of the infor-mation retrieved by each selection, as is the case in the inte r-action technique we developed (Table 1, [18]). We explored the idea of trying to find the appropriate query terms from a long description of a user X  X  information need, and showed that automatic techniques supplemented by user interactio n can deliver significant improvements in performance [17].
Numerous efforts have been made towards finding tech-niques for predicting query quality. Accurately predictin g when a query would fail [7] can be used to attempt an al-ternate technique like PRF. Cronen-Townsend et al. [6] de-veloped the clarity measure to serve as a predictive measure for tracking MAP. He and Ounis [11] explored a number of features derived from the query to determine query ef-fectiveness. Recent work by Carmel et al. [3] attempted to formalize the query difficulty problem. We have tackled a variant of the problem, namely determining if IQR or IQE is better suited for a query.

Harman and Buckley [10] conducted a workshop to iden-tify the reasons why search engines fail. They analyzed the performance and outputs of multiple participating sites an d identified ten reasons for failure of information retrieval sys-tems on TREC description queries. While the categories we report are not identical to theirs, we plan on expanding our current limited analysis to determine the categories of problems they found that our techniques helped solve.
We have presented techniques to improve the effectiveness and efficiency of user interaction for information retrieval us-ing long queries. The SIRE technique has been shown to be an extremely effective way to capitalize on the strengths of the IQR and IQE techniques. Presenting users with the right number of options is an often-ignored aspect of interactive information retrieval. We have developed a sound frame-work for identifying a minimal set of options, and demon-strated that this technique retains good options and remove s the redundant ones. We hypothesize that this technique, which can be used in any interactive environment, will en-hance effectiveness by reducing the cognitive load on users. The exponential-sized analysis of options has been shown to be unnecessary, and reduced to polynomial-sized analy-sis without degrading performance. The ease with which the analysis process can be parallelized (different machine s can analyze options of different lengths and follow up with a merge) and reduction in the complexity can pave the way for live deployment of the effective interaction techniques we have presented in this paper.

Some directions for future work include parsing long querie s using NLP techniques to support user interaction. While the option analysis procedure doesn X  X  involve any querying of the index, the option-pruning procedure requires queryi ng the index to obtain top-ranked documents. A better tech-nique to approximate the top-ranking documents will make the process more efficient. IQR results in a concise version of the long query that is potentially better. In future work we plan to explore using the query identified thorough IQR as a starting point for either automatic or interactive quer y expansion. This two-stage interaction could potentially i m-prove effectiveness even further.

