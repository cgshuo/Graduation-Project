 Learning to Rank ( LtR ) is the machine learning method of choice for producing high quality document ranking func-tions from a ground-truth of training examples. In prac-tice, efficiency and effectiveness are intertwined concepts and trading off effectiveness for meeting efficiency constraints typically existing in large-scale systems is one of the most urgent issues. In this paper we propose a new framework, named CLEaVER , for optimizing machine-learned ranking models based on ensembles of regression trees. The goal is to improve efficiency at document scoring time without af-fecting quality. Since the cost of an ensemble is linear in its size, CLEaVER first removes a subset of the trees in the ensemble, and then fine-tunes the weights of the remaining trees according to any given quality measure. Experiments conducted on two publicly available LtR datasets show that CLEaVER is able to prune up to 80% of the trees and pro-vides an efficiency speed-up up to 2 . 6x without affecting the effectiveness of the model.
 Learning to Rank; Efficiency; Pruning
The problem of ranking objects in response to a user query is of general interest as it is of paramount importance for all kinds of information systems. In Web Search, for example, the problem of ranking documents is particularly challeng-ing due to the large amount of data to manage. Modern Web search engines are expected to return highly relevant results in a fractions of seconds to satisfy hard back-end la-tency requirements. Nowadays, Learning-to-Rank ( LtR ) [6] methodologies are pervasively used as effective solutions to the most difficult ranking problems. LtR methods, which score a set of candidate documents according to their rel-evance to a given user query, use machine learning models trained on ground truth datasets providing an ideal rankings compiled either using human-based or automatic methods. Typically these approaches do not directly take into account efficiency concerns resulting in models that cannot be used in production environments because they do not meet their strict efficiency requirements.

In this paper, we tackle the problem of improving effi-ciency in ( LtR ). We present a framework, named CLEaVER , for the optimization of tree ensemble ranking models after the learning phase has completed. The cost of predicting the relevance for a document by using a tree ensemble model is linear in the number of trees. Larger ensembles, despite be-ing more accurate, come with larger costs thus being very expensive. We study the following novel problem: given a tree ensemble model is it possible to improve its efficiency without affecting its quality? We show that the answer is positive, and we solve the problem by pruning some of the trees in the ensemble and re-weighting the remaining ones.
CLEaVER integrates in a novel way a number of tree pruning strategies with a local optimization method aimed at re-weighing the trees in the pruned ensemble. We ana-lyze several pruning strategies for the identification of the less relevant trees in a given ensemble, and we conduct a comprehensive evaluation of the various strategies on two publicly available datasets. The CLEaVER framework is able to improve the efficiency of a given ranking ensemble up to a 2 . 6x speed-up without affecting the effectiveness of original the model.
 Related Work . Efficiency in Learning to Rank has been in-vestigated in the past from two different research directions. The first includes proposals that trade efficiency off for ef-fectiveness in the learning algorithm. Asadi and Lin observe that compact, shallow, and balanced trees yield faster pre-dictions [1]. They incorporate a notion of execution cost dur-ing training to encourage trees with these topological charac-teristics. Authors propose two strategies for accomplishing this: i) by directly modifying the node splitting criterion during tree induction, and ii) by stage-wise tree pruning. Experiments on a standard learning-to-rank datasets show that the pruning approach is the best. Wang et al. present a unified framework for jointly optimizing effectiveness and ef-ficiency [10]. Authors propose new metrics that capture the trade-off between these two competing forces and devise a strategy for automatically learning models that directly op-timize the trade-off metrics. Experiments show that models learned in this way provide a good balance between retrieval effectiveness and efficiency. Authors also show that their ap-proach naturally leads to a reduction in the variance of query execution times, which is important for query load balancing and user satisfaction.

The second research line considers low-level optimizations to the inference phase by speeding-up the traversal of a given tree ensemble. Asadi et al. [2] propose to rearrange the code visiting the ensemble by transforming control haz-ards into less expensive data hazards , i.e., data dependen-cies introduced when one instruction requires the result of another. Lucchese et al. [7] propose QuickScorer a scoring algorithm, which adopts a new representation of the tree ensemble based on bit-vectors. The tree traversal, aimed to detect the leaves contributing to the final scoring of a doc-ument, is performed feature by feature, over the whole tree ensemble, through efficient logical bitwise operations.
To the best of our knowledge this is the first work focus-ing on post-learning optimization of tree-based LtR models. The two research directions above are orthogonal to ours as we aim at producing a faster model that does not lose its effectiveness and that can by integrated in any scoring algorithm. Moreover, our methodology is totally agnostic w.r.t. both the LtR algorithm. It can be applied to all LtR algorithms producing ensemble models.
Let F = { t 1 ,...,t n } be an additive ensemble of regression trees, composed of n decision trees. Let s i ( q,d ) be the score returned by decision tree t i to document d in response to query q . Moreover, we denote with S ( q,d ) the final score predicted by forest F for query-document pair ( q,d ), ob-tained as a linear combination of tree predictions: where  X  i is the weight associated with tree t i .

We make no assumption on the learning algorithm used to train F and on the loss function optimized during the training. Independently of the algorithm or the loss func-tion adopted, we observe that the cost for computing score able to keep this cost as low as possible, either to comply with time budget constraints or to improve the overall ef-fectiveness of query processing by ranking larger amount of candidate documents returned for a given query [4], we aim at reducing the complexity of a tree-based model by pruning trees. Specifically, given an input forest F providing the de-sired quality, CLEaVER produces a smaller forest F p with at least the same effectiveness as F but with higher effi-ciency. Our solution consists in a combination of pruning strategies and tree weighting optimization. The CLEaVER framework encompasses the two following steps: 1. prune F to select a subset F p of p,p &lt; n trees. We 2. assign new weights to the trees in F p so as to optimize
The resulting forest F p therefore contains a subset of the trees in F but with a different weighting scheme. Since our goal is to produce a more efficient model without affecting its effectiveness, CLEaVER evaluates different pruned model sizes p , and then returns the smallest forest F p providing at least the same quality as F according to the user-defined loss function.

In the following, we first discuss the pruning strategies integrated in CLEaVER , and then discuss how line search is used to improve weights  X  i .
 Ensemble Pruning Strategies . Let p &lt; n be the number of trees in the pruned forest F p . The goal of CLEaVER is to remove n  X  p trees without affecting the predictive power of the initial forest F . To this end, we propose the following strategies:
All the pruning strategies are followed by the line search optimization aimed at tuning the weights of the trees in the pruned forest F p .
 Tree Weighting with Line Search . Let L : R p  X  R be a given loss function, and let  X  = {  X  1 , X  2 ,..., X  current weighting schema for forest F p . Optimizing the tree weighting means finding: Due to the large dimensionality involved and the non differ-entiability of most loss functions, finding  X  is not possible. We thus adopt a heuristic approach to improve the weighting schema  X  obtained after pruning: i) find a descent direction D  X  R p along which L decreases, and ii) compute a step length  X  so as to minimize L ( X  +  X   X  D ). This procedure is iterated as long as the solution improves beyond a given tol-erance. The descent direction D can be chosen with various methods, e . g ., by computing gradient, and the step length  X  can be computed either exactly or approximately. The strategy we use to find  X  is referred to as line search . It is worth noting that the above process performs a local explo-ration of the solution space, without making any assumption on the loss function employed.

In our LtR scenario, we are interested in ranking loss func-tions such as NDCG , for which it is not possible to directly compute the gradient. Therefore, we adopt a greedy variant of the above procedure as in [9]. Given the current solu-tion  X  k at iteration k , we perform a line search along each axis of the weight vector independently, i . e ., find the value d i =  X  i +  X  k i that optimizes the loss function while keep-ing fixed all the other directions. The value of  X  i is found by choosing it among  X  equi-spaced samples in an inter-val [  X   X , +  X  ] around  X  k i , excluding negative values. The re-sulting vector D defines a promising descent direction along which to perform line search. The new weight vector  X  k +1 is chosen by optimizing L ( X  k +  X   X  D ). In this case, the best value for  X  is chosen among  X  equi-spaced samples in the same interval [0 , 1]. The above procedure is iterated until no improvement is measured on a separate validation set.
In order to make faster the above search process, at each iteration the search interval [  X   X , +  X  ] is reduced by a shrink-ing factor  X  . In addition, thread-level parallelism is used in order to evaluate different directions d i and different  X  sam-ples in parallel. Finally, we avoid to visit the whole forest at each step. The ensemble of trees is in fact visited only once and tree predictions s i ( q,d ) for all the documents of the training dataset are stored in a memory data structure.
Experiments were conducted with two publicly available datasets: the MSN-1 1 (Fold 1) dataset and a new dataset provided by Istella 2 (Small) we make publicly available with this work. The MSN-1 dataset consists of 31,351 queries and 136 features extracted from 3,771,125 query-document pairs, while the Istella dataset is composed of 33,018 queries and 220 features extracted from 3,408,630 query-document pairs. The query-document pairs in both the datasets are labeled with relevance judgments ranging from 0 (irrelevant) to 4 (perfectly relevant). Each dataset is split in train, validation and test sets according to a 60%-20%-20% scheme.

Training and validation data were used to learn a full reference model, to which we applied CLEaVER 3 by test-ing different pruning levels in the range 0%-90% with steps of 10%. Eventually, we selected the smallest (and thus most efficient) model generated by CLEaVER still pro-viding greater or equal performance on the validation set w.r.t. the reference un-pruned model, measured in terms of NDCG @10. The parameters adopted for the greedy line search are  X  = 20 equi-spaced samples taken from an inter-val of radius  X  = 2 and adopting a shrinking factor  X  = 0 . 95.
For the sake of fairness, we aimed at building a full refer-ence model that is the best performing state-of-the-art rank-ing ensemble. On both datasets, we trained two different al-gorithms: i) the Gradient Boosting Regression Tree ( GBRT a.k.a., MART ), a point-wise algorithm that uses the root mean squared error as loss function, resulting in a predictor of relevance labels [5], and ii)  X  -MART, a list-wise algorithm that is capable of using NDCG in its loss function, result-ing in a predictor of the ranking [11]. We used the open-source implementation of these algorithms provided by [3]. Both were fine-tuned by sweeping their parameters aiming at maximizing NDCG @10. The maximum number of leaves was tested in the set { 5 , 10 , 25 , 50 } , while the learning rate algorithm to train up to 1 , 500 trees unless the relative im-provement in NDCG @10 on the validation set during the last 50 iterations was smaller than 0 . 5  X  . The resulting forests include 737 and 736 trees for MSN-1 and Istella , respectively, with unitary weighting schema ( i . e .,  X  i = 1 , 1  X  i  X  n ). To assess the validity of the pruning strategies also for smaller forests, we build two more reference models consisting of the first 100 and 500 trees of the full reference . NDCG @10 values for the Random strategy were averaged over 10 runs. We also performed randomization test [8] to assess if the differ-ences in effectiveness between the reference models and the pruned ones are statistically significant.

We found that the largest number of leaves and the smaller learning rate always provide the best results with both LtR algorithms. We highlight that it was not possible, by varying the training parameters, to build more efficient rankers (e.g., with smaller total number of nodes) providing the same level of quality of the reference models. MART resulted the best performing on MSN-1 , and  X  -MART on Istella .
 Results . Table 1 reports the performance of CLEaVER on MSN-1 and Istella datasets. In bold the best performance in terms of number of trees composing the pruned forest. Interestingly, both Random and Skip perform quite well, meaning that trees are somehow redundant by construction. Strategy Last is the worst performing, suggesting that the last trees built by the model still provide a relevant contri-bution. The Low-Weights pruning performs worse than expected, especially with the largest models. We deduce the tree weight cannot be evaluated in isolation from the actual predictions at the tree leaves. The pruning based on Score-Loss exhibits good performance on average, espe-cially on the Istella dataset. We highlight that this strat-egy approximates well the final document score, but small score variations may generate large rank variations, thus re-sulting in a negative impact on the NDCG metric. Finally, Quality-Loss is quite consistently the best pruning strat-egy on both the datasets and for all the model sizes, except for the full  X  -MART model on Istella dataset where Skip and Score-Loss are able to prune 10% more trees. Indeed, Quality-Loss is the only strategy that takes into consider-ation the target quality function NDCG @10.

Pruning the reference models allows to significantly re-duce the number of trees from 50% to 70% thus obtaining a scoring speed-up from 1 . 6x to 2 . 6x while preserving the effectiveness of the original reference model.

We run statistical significance tests, with significance level p = 0 . 05, obtaining a two-fold result. On the one hand, they show a statistical equivalence when dealing with forests of 500 or more trees. On the other hand, they prove that the improvement of the pruned model w.r.t. the reference one when dealing with 100-tree models is statistically significant. Smaller models, whose quality is still far from optimal, can greatly benefit from the line search optimization, which can also better deal with a low-dimensional search space.
To confirm the effectiveness of CLEaVER we also per-formed the following experiment. We allowed pruning as long as no statistically significance difference w.r.t. the ref-erence model is observed. In this setting, CLEaVER pro-vides an even further improvement on the largest models: 70% pruning (221 trees) with Quality-Loss on MSN-1 and 60% (294 trees) with Skip on Istella , corresponding to a scor-ing speed-up of 2 . 9x and 2 . 1x respectively.

An important outcome of this work is that, in order to build a model with a given number of trees, we found to be best performing to use CLEaVER to prune a large model than to exploit line search optimization only. As an example, a  X  -MART model with 100 trees on Istella dataset provides an NDCG @10 of 0 . 6923 and of 0 . 7085 after line search re-weighting. Instead, by pruning a larger model of 500 trees with the Quality-Loss strategy, CLEaVER produces a model providing an outstanding NDCG @10 of 0 . 7329.
We proposed a novel algorithm, named CLEaVER , which is able to improve the efficiency of a given ensemble-based model. CLEaVER , first removes the less relevant elements in the ensemble according to their expected impact, then fine-tunes the weights of the remaining ones through line search. Eventually, CLEaVER provides a smaller and more efficient model with at least the same quality as the origi-nal one. In our experiments, CLEaVER allowed to built models being more than twice as fast. As future work we intend to integrate CLEaVER in several ensemble learning algorithms to systematically prune and improve the mod-els while being generated. We also believe CLEaVER can be successfully extended to non tree-based ensembles and applied to other tasks than document ranking.
 Acknowledgments . This work was partially supported by the EC H2020 Program INFRAIA-1-2014-2015 SoBigData: Social Mining &amp; Big Data Ecosystem (654024). [1] N. Asadi and J. Lin. Training efficient tree-based [2] N. Asadi, J. Lin, and A. P. de Vries. Runtime [3] G. Capannini, C. Lucchese, F. M. Nardini, S. Orlando, [4] V. Dang, M. Bendersky, and W. B. Croft. Two-stage [5] J. H. Friedman. Greedy function approximation: a [6] T.-Y. Liu. Learning to rank for information retrieval. [7] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, [8] M. D. Smucker, J. Allan, and B. Carterette. A [9] M. Taylor, H. Zaragoza, N. Craswell, S. Robertson, [10] L. Wang, J. Lin, and D. Metzler. Learning to [11] Q. Wu, C. Burges, K. Svore, and J. Gao. Adapting
