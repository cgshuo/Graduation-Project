 A Data Warehouse is a repository that integrates information from multiple data sources, which may be heterogeneous, and makes them available for decision support warehouses: First, they isolate decision support process from the traditional, often on-line database transaction systems, which largely alleviates the burden of database systems, since DSS (Decision Support Systems) does not have so strict requirement on response time as database transaction systems. Second, they extract, integrate and store only  X  X elevant X  information from multiple independent information sources. The information is stored in the warehouse ahead of the queries, so user queries are able to original sources for execution. Moreover, when the sources are inaccessible due to locks or regular maintenance, data warehouses can still function normally. 
The form of data stored in data warehouses is the derived views from information resources, usually database tables (base table), called materialized views . Materialized views provide user queries with faster and uniform access to the information integrated from different data sources. Any qu ery that can be rewritten to be answered which often involve large amount of data exchange, the execution time of queries can materialization is considered to be one of the most efficient ways to speed up decision support process and OLAP queries in data warehouse architecture [2]. 
There are dozens of research issues concerning view materialization, such as rewriting user query to transparently direct it from base table to materialized views, or materialized views update when modifications of base table occur, etc. Among most of these issues, to select an appropriate set of materialized views is most fundamental. Given a set of queries at the warehouse, we cannot materialize all possible views, as there are constraints on disk space, computation time, or maintenance cost. Therefore, we need to select a proper set of views to materialize in order to minimize the query response time under certain resource constraint. 
A number of achievements have been made on the problem of materialized view selection. However, most of them focus on centralized data warehouses. There is still no effective solution for the distributed cases other than repeatedly applying selection algorithms for central cases on each distributed nodes. With the development of local business, which need different local data warehouses; there are too huge data to be stored in one single warehouse; different departments of an enterprise may materialized view selection in the distri buted data warehouse context and provides comprehensive solutions for selection with storage cost constraints. 
The rest of the paper is organized as follows: In section2, we introduce distributed view derivation cube to model views and communication network in distributed data warehouse nodes. In section 3, a cost model based on derivation cube is given as quantitative measurement for query response time. Section 4 proposes a greedy-based distributed selection algorithm with a storage cost constraint. A detailed experimental performance analysis is made in section 5 comparing the query response time of our solution with that of simply applying the central methods repeatedly on each warehouse nodes. Finally, we end with related work and some concluding remarks. In this section, we introduce distributed derivation cube model as an abstraction of the relation between views and communication network in distributed data warehouse nodes. We start by defining some preliminary concepts. 2.1 Preliminary Definitions Definition 1: Granularity Definition 2: View A view N v with attributes k1,...,kn is formalized as ) ... , ... (
Obviously, the views in the distributed data warehouse can be essentially represented with their granularities, since those non-orthogonal attributes in the views can be computed from those orthogonal: N v = (G) N .
 Theorem 1: Derivability: 1 1 ) ( N v is derivable from 2 2 ) ( N v if and only if: Proof: First, we define a binary relation  X   X   X : ki  X  kj means grouping attributes kj can be calculated from ki . This binary relation is apparently a partial order, because: 1) it and kj  X  km imply ki  X  km.
 It is easy to see that condition 1 is a basic premise for derivation.  X   X   X  defined above. From definition 2, a view can be characterized by its granularity. So we can see 1 1 ) ( N v can be computed from 2 2 ) ( N v , which proves the proposition. 
By the way, we can see that the derivability relation is also a binary relation derived from v1. 2.2 Distributed View Derivation Cube All possible views of a data warehouse, that is, all combinations of grouping derivation relationship. Within this DAG, every node stands for one view granularity, i.e. different views with the same granularity are grouped into one node, which largely reduces the complexity of DAG. A detailed reasoning for derivability is given in [3] [4]. This DAG is the base for distributed view derivation cube. 
Although the DAG mentioned above can well capture the view derivation semantics, it is not suitable for distributed cases, since there are extra communication and maintenance costs which are not under co nsideration in central data warehouse. Therefore, we propose distributed view derivation cube as an extension to the DAG. 
As shown in Fig 1. Suppose the base table is (A,B,C) and there are three distributed warehouse nodes N1, N2, N3 . So there are 8 possible granularities which are shown as denotes the view at one warehouse node. The directed edges between these sub-nodes represent the communication channels. Edges between cube nodes, set red in Fig 1, v can be deduced by the transitiveness of existing relations. For simplicity, only a small part of the cube is presented in detail, as shown in the rectangle in Fig 1. 
However, since the communication channels between sub-nodes are bidirectional, cycles appear in the resulting cube, which no longer conforms to the characteristics of DAG. In such cases, selection algorithm can not function properly. To solve this respectively. An in-node only have in-edges while an out-node only have out-edges in the sub-graph of a view. It is easy to see that such revision eliminates cycles resulted from communication channels. 
Besides conditions in theorem 1, in the context of derivation cube, v1 has to be an out-node and v2 an in-node if 2 1 v v  X  . Definition 3: Distributed View Derivation Cube containing all granularity combinations, each of which is divided into n sub-node pairs, an in-node and an out-node, with n denoting the number of distributed of different granularities and communication edges Ec between nodes of the same granularity at different warehouse nodes: E={Ed, Ec} . 
The edges of the cube are marked with costs: edges in Ed are marked with computation costs for gathering necessary data while those in Ec are tagged with communication cost for data transfer. The cube in Fig 1 assumes a fully connected communication network while in Fig 2, site 2 and 3 are disconnected. views with maximized benefit under a storage limitation on each warehouse node. 3.1 The Distributed View Selection Problem response time, under the constraint that the total space occupied by M at i th warehouse node is less than Si.

More formally, given a distributed view derivation cube C , a set of queries Q={q1, warehouse node {S1,...,Sn}, let H (q,M) denote the cost of answering a query q using the set M of materialized views in the given cube C . The cost H(q,M) consist of two the communication costs H comm if the query q is issued at a different warehouse node views/nodes M={v1,v2,...,Vm}, that minimized H(Q,M) , where 3.2 Our Cost Model 3.2.1 Cost Function In our approach, we consider a linear cost model and assume that evaluation and communication cost functions are dependent respectively on the cardinality of the weighed by an additional adjusting factor w to tune the ratio of computation to communication costs. So, the overall cost function is: 
The above function is logical because all tuples in v i have to be read and processed where v i is located to the one where the query is initiated. 
The complexity of the view selection problem to compute the optimal solution is mainly based on a greedy approach, which uses a benefit function in order to decide which view to select in each step. 3.2.2 Benefit of a Set of Selected Views Here, we define the notion of a  X  X enefit X  function, which is essential to the development of algorithms presented later. Based on the cost function above, the denoted by B(Vi,M) , and is defined as: 
The benefit of Vi per unit space with respect to M is B(Vi,M)/size(Vi). If there is no space constraint, all possible views will be materialized to minimize query costs, which is not applicable. 3.2.3 Our Monotonic Benefit Model Definition 4: Monotonic Property of B(V,M) A benefit function B , which is used to prioritize views for selection, is said to satisfy the monotonicity property for a set of views M with respect to distinct views V1 and V2 if B({V1,V2},M) is less than (or equal to ) either B(V1,M) or B(V2,M). based approximation. The greedy approach just selecting one single view per iteration may result in an arbitrary bad solution if the monotonicity is not satisfied. Theorem 2: A benefit function B is monotonic if the following holds: 
The proof is provided in [7]. In the case of view selection problem under space condition in theorem 2 as it relates just to the size of evaluated views | Vi |. problem in distributed data warehouse. Algorithm 1 : Greedy Algorithm Input: C , the distributed view derivation cube Output: M , the set of selected views to be materialized BEGIN Endwhile Reture( M ); 
END 
The main idea of algorithm 1 is as follows: the set of selected views to be materialized M is initialized with base table Vo . While there is still storage space left nodes having the same view granularity, we select the view located at the warehouse node with largest storage capacity to add to M . Such iteration continues until no Si&gt;0 . 
The running time of this greedy algorithm is O(mn 2 ) , n being the number of nodes in the derivation cube C and m being the number of views chosen for materialization. In this section, we compare greedy based view materialization selection method for central and distributed data warehouse with an experimental analysis. As illustrated in The ratio of computation cost to communication cost is set to be 1 ( w=1 ). The results of queries from query set Q are assumed to cover every view nodes in the cube evenly. The query frequencies fi are set to be 1. Each node has a storage space of 150. 
We first consider the central cases. In Fig 3 is the view DAG for central data warehouse. The first iteration result of single node greedy algorithm is shown in table two round of iteration, (C) and (B) are selected. The total storage space of these three views equals 66 (1+15+50). As the storage limitation for one warehouse node is 150, no more views can be selected to materialize. 
We repeatedly apply the single node algorithm to each node in distributed data warehouse and we get our M={( )N1, ( )N2 ,, ( )N3 , (C)N1,, (C )N2,, (C)N3,, (B)N1, (B)N2,, (B)N3,}. The total query costs H(Q,M) amount to 147,570 which consists of 5*8,000+50+15+1=40,066 for node N1 and (8,000+3,356+1,480+750+100)*w+ 40,066=53,752 for node N2 and N3 with additional communication costs. 
Now let X  X  look at the distributed algorithm. Suppose the communication channels between the three warehouse nodes are fully connected, derivation cube is as shown in Fig 2. Selection process is shown in table 2. 
The algorithm takes four iterations until the space limitation 150 per node is reached. The M returned by the algorithm is M={( )N1, (C)N2, (B)N3, (A)N1}. The total query costs H(Q,M) amount to 120,358, which means a cost reduction of 14,7570-120,358=27212 (18.44%) is realized. The total storage space used is 1+15+50+100=166 which is much smaller than (1+15+50)*3=198 in the first case. 
Based on the above comparison, it is easy to see that our extended derivation cube costs will be. In the initial research done on the view selection problem in central data warehouses, Harinarayan et al. [8] presented greedy based algorithms for solving view selection special purpose warehouse where there are only queries with aggregations over the distributed cases. 
Negative theoretical results on the view selection problem are also obtained. In the work of Chirkova et al. [10], the complexity of the view selection problem to compute workload queries, while in our approach th e algorithm starts with the derivation cube. 
There are also some jobs done on improving actual selection performance by developing heuristics [13][14][15][16], Most of the work has developed different infrastructure and heuristics algorithms for view selection. However, these approaches are either exhaustive searches or without any performance guarantees on the quality of the solution delivered. 
Another group of work focuses on the maintenance of the materialized views, hoping to minimize view synchronization costs. This problem is more complicated since the benefit function with respect to the maintenance costs exhibits non-monotonic properties. [6] did some sensible work in this context. [17][18][19] apply the idea of caching the result of user queries to speed up similar queries of other users. Determinative algo rithms are developed to decide whether a query result should be put in cache or not. Meanwhile, replacement strategies have to choose those temporarily unused results out of cache in favor of the new result set. 
No proper solutions specially developed for view selection in distributed data warehouse are found in related work. A data warehouse is built for better information integration and decision support process. The selection of views to be materialized is one of the most fundamental issues in designing a data warehouse. While lots of work has been done on the view warehouses. 
In particular, we model the views in distributed warehouse nodes with derivation cube which is a concept extended from the data cube in central data warehouse, and make extensions in order to adapt it to distributed cases. Then, a greedy-based presented. A detailed comparison analysis is made to demonstrate the advantage of our solution over simply applying the central methods repeatedly on each warehouse nodes. Statistical data shows that our approach is far better both in response time and storage space costs. 
There are still a lot of important issues in the context of view selection in distributed data warehouses. Future work will focus on improving our algorithm for time. A more typical and complex performance analysis will be provided to make it more persuasive. Moreover, aggregations queries over more than one base table and queries other than aggregations will be taken into account to make our solution applicable for more common occasions. 
