 Clustering is to partition data points into g roups, which makes dat a points in the same group more similar to one another than to those out of the group. Clustering algorithms are categorized by the data types they fit for. There are roughly two data types including numerical and categorical, and a data set ma y have mixed types. We take the internet traffic data as an example: protocols and operation systems are categorical data, while packet sizes and packet numbers are numerical data. Researchers have designed various clustering algorithms for these two data types. The categorical clustering algorithms are mostly based on pair-wise similarities or information theory etc. such as those proposed in [4,9], most of which suffer from quadratic complexity or combinational explosion problems. Clustering algorithms for numerical data [5,6] are quite different from those for categorical data. In numerical data sp ace, coordinates based linear reference frame is available so that many concepts, like means, and mean square distance etc., could be defined to compress the numerical data and make the clustering algorithms more efficient. Moreover, data visualization, which helps to understand the data intuitively, is easier to be implemented in the numeri cal space than in the categorical space.
Some applications such as internet traf fic data based intrusion detection need an efficient clustering strategy which can handle the mixed data types and integrated with data visualization. Intuitively, mapping the categorical data sets into numerical ones can cover these requirements, since the mixed-typed data are naturally turned into numeri-cal data when their categorical parts beco me numerical ones. It can also employ some efficient numerical clustering algorithms su ch as K-means and can be easily visualized. There X  X e some traditional approaches [8,1 0] that can map data into a new space based on the pair-wise similarity matrix. The most important issue of data mapping is what data property has been preserved during the mapping. Traditional techniques concern different data properties such as the dist ance between each pair of data points [10] or the neighborhood of each data point [8]. These a lgorithms employ complex processes on the similarity matrix to preserve these strict properties.

We hereby propose a model to establish a data mapping for clustering and visual-izing only for categorical data since in the proposed strategy handling the mixed-type data is essentially handling the categorical data. The mapping is based on calculating similarities between data points and a sample set called reference set. Mapping data via calculating similarity is not new and nor is the concept of reference points. Our most significant contribution is that we propose to preserve a more flexible data prop-erty specific to the goal of clustering, i.e. the expectation of intra-cluster similarities is higher than that of inter-cluster similar ities. We find that under some conditions, which are easily to satisfy in practice, this property can be preserved by directly treating the similarities with points in the reference set as the image data. This simplicity makes our mapping algorithm efficient and the preservation of the clustering property makes it effective for clustering categorical data . Because of the existence of the reference set, we X  X l call this mapping framework R-map . 1.1 Related Works Mapping data into a new numerical data space by calculating a similarity matrix 1 is not a new approach. Traditional techniques include Multidimensional Scaling (MDS) [1] and Locally Linear Embedding (LLE) [8] among which Landmark MDS (LMDS) [10] is most closed to the model proposed in this paper. The input of MDS is a distance matrix between objects. MDS produces a c oordinate vector for each object in a new numerical space whose dimension is user-specified. This process is called embedding . The goal of MDS is to minimize a quadratic cost function between the new embedding coordinates and the original coordinates that created the distance matrix. Visualization and clustering can be performed on the resulting embedding coordinates. The original MDS technique is not appropriate for large data sets nowadays. The author of [7] com-pared some scalable MDS approaches and the experimental results show that is more accurate than the others with roughly the same computational cost. LMDS is efficient because it only calculates distances to a set of data points which are called landmarks.
However, we found the operation after generating a reference set can be simpler while keeping the effectiveness. In this paper, we propose a mapping framework that directly treats the similarity matrix as the image data. The key novelty of our framework is that we propose a new data property to preserve and demonstrate the effectiveness of model based on a statistical foundation. Our embedding process preserves some cluster-ing property, which is more flexible than to preserve the distance or neighborhood. This makes our algorithm much simpler and more efficient than the traditional ones while the effectiveness of clustering after the mapping isn X  X  worse but even better under some conditions which will be analyzed in Section 2.1. The main steps of R-map are very simple: Given a cate g orical data set and a similarity meas u re: Step 1: Randomly sample r data points as a reference set.

Step 2: Calc u late the similarity between each data point and reference point to
Step 3: Apply P CA on the similarity matrix (optional), then cl u sterin g and
Some issues should be resolved to accomp lish this algorithm such as what kind of categorical data is appropriate for this framework, how does the measure selection im-pact the framework and what is the detailed sampling strategy? We X  X l discuss these three issues in the following subsection. 2.1 Analysis of the Framework Some notations used in this paper are summarized as follows.

Issue 1: Definition of Categorical Cluster. The R-map framework is based on a specific cluster definition. The clusters C 1 , C 2 ,..., C c in categorical data are defined in a generative way, i.e. we assume the data poi nts in the categorical space are generated by a set of discrete distributions. Formally, let M k ( p k ) denote a discrete distribution Definition 2.1 (Cluster). Acluster C j is generated from a set of discrete distributions: M tion of the whole data space is a mixture of M j : c 1 M 1 + c 2 M 2 + ,..., + c c M c ,where c is the mixture coefficient means the probab ility of a data object belonging to the k-th cluster.
 Note that the discrete distribution is gener al and it can be any specific discrete distri-bution such as multinomial distribution. Mo reover, the distributions on each attribute needn X  X  be independent to each other. Therefor e, most categorical data that have clus-ters satisfy this assumption. Suppose data objects t 1 ,t 2 are generated from M i and t 3 is generated from M j . Following Definition 2.1, on the k -th dimension t 1 k ,t 2 k can be viewed as generated from a subspace cluster M i k ( p i k ) and t 3 k from M j k ( p j k ) . Issue 2: The Property Preserved in the Mapping. As mentioned in Section 1, a data object should be more similar to the objects in the same cluster than to those out of this cluster. We formulate this clustering property as an inequation between the expectation of intra-cluster and inter-cluster similarities (denoted as s (  X  )) .
 Definition 2.2 (Clustering Property) Fo r  X C i , C j , i = j , t 1 ,t 2  X  X  i , and t 3  X  X  j , we have This definition is a reasonable assumption because it is nearly impossible to do dis-tance/similarity based clustering analysis if the intra-cluster and inter-cluster distances/ similarities can not be differentiated. We claim that this property can be preserved by the mapping of our framework, as is formalized in Lemma 1, if we use the Simple-Match (SM) similarity defined as Lemma 1. Suppose o x is a reference object each attribute of which is uniformly and independently generated from the attribute domains respectively. The definition of t 1 , 2 , 3 follows Definition 2.2. Denote random variable X 1 = SM ( t 1 ,o x ) ,X 2 = SM ( t 2 ,o x ) and Y = SM ( t 3 ,o x ) . We have E [( X 1  X  X 2 ) 2 ]  X  E [( X 1  X  Y ) 2 ] ,when i = j . Proof: We omit the proof due to lack of space. 2 .
 Note that the squared Euclidian distance between two objects t i ,t j in IDM is k =1 [ IDM ( i, k )  X  IDM ( j, k )] 2 . Followed by Kolmogorov X  X  Strong law of Large Numbers, it is consistent to E ( X i  X  X j ) 2 as r increases, where X i and X j denote the random variables that generates t i ,t j  X  D . Lemma 1 deems that the objects in the same cluster with high SM similarities in the original data will have small squared Euclid-ian distances in the image data, i.e. the clustering property defined in Definition 2.2 is preserved.
 Issue 3: Sampling Strategy and Similarity Measure. As analyzed above, the key point of the R-map is the inequality between the expectation of intra-cluster similar-ity and inter-cluster similarity and we X  X e demonstrate this inequality will be preserved using a simple distance definition and a reference set uniformly sampled from the back-ground data space. Now we come to discuss the i mpact of different sampling strategies and different similarity measures.

If we directly sample the reference set from the original data points. Such sampling will cause dependence on the attributes of the reference points. But the impact of this dependence is not negative. Actually, under most conditions, the dependence among the attributes makes the SM values between intra-cluster data points smaller and those between inter-cluster data points larger. The inequation in Lemma 1 will not be signifi-cantly impacted. Other similarity measures such as normalized simple match or Jaccard coefficient will not change the distance orders between objects, so the inequality can also be preserved. We have practically examined these discussions and the clustering effectiveness is similar to these different s ettings. We sample the reference set directly from the data points and use the SM measure in our experiments. Other sampling strate-gies and similarity measures are viewed as optional settings of our model. The experiments are performed on a PC platform, with a Pentium(R) 4 3.2GHz CPU, and 1Gb Ram. The procedure of our framework X  X  execution environment is Matlab(R) R14. We use numerous clustering evaluation measures for the clustering results includ-ing Accuracy (AC), Precision (PR), Recall ( RE), Information Gain (IG), Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI). 3.1 Experiments for Effectivenes s Comparison with Related Works In this part we experiment with real life categorical data sets taken from the UCI ma-chine learning repository [3] for a overview of the effectiveness and efficiency compari-son. There are totally 22 categorical data sets in the MLDB of the UCI machine learning repository. We remove some redundant ones and some have more than 1000 data objects for simplicity. On the remaining 14 data sets, we apply K-means clustering after vari-ous data mapping strategies including R-map, LMDS and LLE. We also compare our framework with K-modes clustering algorithm [4] and spectral clustering [2]. Since K-modes algorithm simulates K-means in the cat egorical data and spectral clustering is a clustering strategy based on pairwise distance matrices, these comparisons make sense.
The results of this part are shown in Figure 1 and Figure 2 using scatter plots. Fig-ure 1 reveals the comparison of clustering effectiveness. Each sub-figure reveals the comparison between our framework and a related work (labeled on the Y-axis). Dif-ferent marker types represent different clu stering evaluation measures and each marker type has 14 instances which represent the results on 14 data sets. R-map outperforms LLE, spectral clustering and K-modes on these data sets and obtains similar effective-ness to LMDS. Figure 2 compares the effici ency of algorithms. The values are loga-rithmic because some relate works cost so long time that the scatter points assemble onto the Y-axis. In this scatter figure, differen t marker types represent different related algorithms and each point with the same marker represents a data set. R-map outper-forms all the related algorithms on efficiency. It seems that only LMDS is comparable to R-map on clustering these data sets on effectiveness, so we X  X l pay more attention to this algorithm in the following parts. 3.2 Experiments for Scalability on Synthetic Data Sets Since our model is most related to LMDS and the computational complexity with these two works can not be exactly compared, we empirically compare their scalability using synthetic data. We generate the synthetic data sets by a data generator imple-mented by ourselves, which can generate categorical data sets with optional n , m , d max and c . The generation of clusters follows Definition 2.1. We compare the scalability of the mapping process between R-map and LMDS. Since the synthetic data sets can have optional n , m , d max and c . We fix three of them and increase the remaining one. The average time costs of 50 rounds on each st ep are recorded to exam the scalability of our framework on each data scale. The results are shown in Figure 3, where each sub-figure is the scalability performance on one data scale (labeled on X-axis). The real line represents our framework and the dashed line represents the LMDS as a comparison. We find R-map outperforms LMDS especially on the number of data objects. 3.3 Experiment for Visualization Data visualization uses the first three Principle components of the image data space and the experiment data is the well-known Mushroom data set from MLDB. The left plot of Figure 4 is the visualization of Mushroom data using R-map. We can see there are about 20 clusters from this visualization. This number is exactly the cluster number on which most distance based clustering approaches ge t excellent results. Moreover, the cluster sizes also accord with the results proposed by t hose previous clustering approaches such as reported in [9]. Compared with the right plot of Figure 4 which is visualization based on LMDS, R-map has better cluster aspects. In this paper we introduce a simple and effective framework to map categorical data into a numerical space for visualization and clustering, which has theoretical guaran-tee and empirical demonstration. The key point of our model is a flexible data property which is preserved during the mapping, i.e. the difference between the inter-cluster sim-ilarities and the intra-cluster similarities. The theoretical analysis demonstrates that this property will be preserved if the data is categorical data, the clusters at which can be modeled by a set of discrete distributions.
 Acknowledgments. This work is supported in part by NSFC grants 60673103 and 60421001.

