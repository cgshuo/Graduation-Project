 Distribution channel is a system that partners move prod-ucts from manufacturer to end users. To increase sales, it is quite common for manufacturers to adjust the product prices to partners according to the product volume per deal. However, the price adjustment is like a double-edged sword. It also spurs some partners to form a cheating alliance, where a cheating seller applies for a falsified bigdealwithalow price and then re-sells the products to the cheating buyer-s . Since these cheating behaviors are harmful to a healthy ecosystem of distribution channel, we need the automatic method to guide the tedious audit process.

Thus, in this study we propose the method to rank all partners by the degree of cheating, either as seller or buy-er. It is mainly motivated by the observation that the sales volumes of a cheating seller and its corresponding cheat-ing buyer are often negatively correlated with each other. Specifically, the proposed framework consists of three part-s: 1) an asymmetric correlation measure which is needed to distinguish cheating sellers from cheating buyers; 2) a systematic approach which is needed to remove false posi-tive pairs, i.e., two partners whose sale correlation is purely coincident; 3) finally, a probabilistic model to measure the degree of cheating behaviors for each partner.

Based on the 4-year channel data of an IT company we empirically show how the proposed method outperforms the other baseline ones. It is worth mentioning that with the proposed unsupervised method more than half of the part-ners in the resultant top-30 ranking list are true cheating partners.
 H.2.8 [ Database Applications ]: Data mining Distribution Channel; time series; correlations
Distribution channel, as shown in Figure 1a, is a system of partners to move products from manufacturer to end users . When the number of end customers is large, this indirec-t channel is extremely helpful to improve product revenue and market competitiveness. To maintain the smooth oper-ation of distribution channel, manufacturers need to develop effective channel policies , addressing the following aspects: (i) ensure that end users can purchase products at any time and place, (ii) control product price reasonably, (iii) increase market promotion and market share. Among those goals, price management may be the most important and critical one. To spur the sales enthusiasm of channel partners, man-ufacturer may adjust the product prices according to sales volume per deal. If a partner has an end user who plans to buy plenty of products, she can apply for a low price from the manufacturer. On the contrary, for a normal-sized deal the partner can only get a normal price. Thus, price may differ among partners.

Due to such price differences, cheating in the distribution channel may happen. Hardy and Magrath [6] summarized the types and forms of channel cheating, and then explored the structures, factors and incentives that encourage cheat-ing. Additionally, Narayandas and Rangan [16] showed that to achieve relational benefits and competitive advantages, partners have put emphasis on developing a stable dyadic re-lationship instead of building adversarial relationships with other partners. Thus, cheating becomes the behaviors of the partners X  alliance nowadays.

A typical scenario of cheating alliance is shown in Fig-ure 1b. Partner v 1 pretends to have an end user with a falsified big deal, thus applies for a low price from the manu-facturer. Then, v 1 re-sells part of products to another part-ner v 2 with the price lower than the regular price of the normal deals. In this scenario, we call v 1 and v 2 cheating seller and cheating buyer respectively. However, from the view point of the manufacturer such behaviors, represented by the dash arrows in Figure 1b, are totally unknown .On-ly the sales volume from the manufacturer to the partners, represented by the solid arrows, are known . To cultivate a healthy ecosystem of distribution channel, these unknown cheating behaviors among partners must be detected.
Usually, the audit staff in a company is responsible to detect these cheating behaviors, and their work heavily relies on the process of official examination on the business and financial records. For some severe cheating cases the judicial investigation is also required. It is really a tedious process. Therefore, in this study we aim to provide an automatic method to guide the audit process and greatly reduce the manual efforts needed.

Motivating observation . The only data we can use are the purchase volumes of the partners from the manufacturer. Here, the purchase volume of each partner can be represent-ed as a time series, in which each entry corresponds to the monthly purchase volume. Figure 2 illustrates the time se-ries of purchase volume of two real-world cheating partners, where v 1 and v 2 are the paired cheating seller and cheat-ing buyer, respectively. Usually, the sales volume of these two partners may change collectively on the following two aspects.  X  When v 1 applies for a falsified big deal from the manu-facturer, its purchase volume increases at that month. Mean-while, since v 2 buys some products from v 1 , its purchase volume from the manufacturer will decrease after v 1  X  X  big deal happens. For example, in Figure 2 v 1  X  X  purchase vol-ume increases in the 5 th month while v 2  X  X  purchase volume decreases in the 6 th month.  X  On the contrary, if v 1 does not apply for big deals, it-s purchase volume decreases. At this time, as v 2 can only purchase the products from the manufacturer, its purchase volume will increase (if she has the stable product require-ments). For example, v 1  X  X  purchase volume decreases in the 7 th month while v 2  X  X  purchase volume increases on the 7 th month.

Intuitively, such phenomenon may to some degree be treat-ed as evidence for cheating behavior. The higher the fre-quency of the phenomenon is, the stronger the evidence would be.
Motivated by the above observations, in this study we propose the method to detect the cheating behaviors among the partners. To the best of our knowledge, compared with some previous qualitative works this is the first quantita-tive study in this area. In this problem, we are given the sequences of purchase volume for all the partners, and aim to rank the partners based on the degree of their cheating behaviors. This ranking list can be delivered to the audit staff for further investigation sequentially.

In the following we will present the framework of our method, discuss the challenges in each step of this frame-work, and highlight the technical contributions.
The framework of the solution is shown in Figure 3. It mainly consists of three steps: 1) Building the Partner Correlation Graph ; 2) Partition partners into suspicious sellers and buyers; 3) Ranking the partners by the probabilistic model.
Motivated by the observation shown in Figure 2, we actu-ally need a method to measure the correlation between two sequences. In this study, we adapt the Pearson correlation to depict the collective changes of the purchase volumes be-tween a cheating seller and a cheating buyer. Intuitively, the opposite tendencies of the two sequences yield a strong negative correlation between them. Thus, we believe that if two sequences have strong negative correlation, it is likely that the abnormal deals happen between the corresponding partners.

However, Pearson correlation has the following limitations in this application. First, Pearson correlation is a symmetric measure, and the order of its two inputs is irrelevant to its output. It cannot distinguish cheating sellers from cheating buyers in this application. Thus, an asymmetric measure is needed here. Second, Pearson correlation is computed with the tick-to-tick correspondence on the two sequences. How-ever, an abnormal deal does not necessarily complete during the same month, but with a delay of several months after the cheating seller applies for a big deal from the manufacturer. Thus, we need to consider this time offset in computing the correlation.
 To address these challenges, we leverage the Dynamic Time Warping (DTW) technique to compute a Directed Pearson Correlation (DPC) coefficient. DTW is a well-known method to find an optimal time alignment between two sequences under certain constraints. In an abnormal deal, the cheating seller can only sell products to the cheat-ing buyer in or near after the time when the cheating seller gets a falsified big deal. Thus, the warping direction must be from now to future within a time window.

To this end, we propose an asymmetric measure r dpc for the two sequences x 1 ,x 2 of the two partners v 1 ,v 2 . r measures the correlation when we view v 1 as a cheating seller and v 2 as a cheating buyer. Similarly, r dpc ( x 2 ,x 1 ) measures the correlation when we view v 2 as a cheating seller and v as a cheating buyer. Note that r dpc ( x 1 ,x 2 ) = r dpc The details on computing r dpc will be presented in Section 3.
Therefore, given the n partners with their sequences of purchase volumes { x i | i =1 ,  X  X  X  ,n } ( x i is the sequence of the partner v i ), we can generate a weighted directed graph G =( V,E,w ), where  X  V = { v 1 ,  X  X  X  ,v n } contains the n nodes, where v i corre-sponds to the partner with the sequence x i ; user-specified parameter;  X  The weight w ij ontheedgeof( v i ,v j )issetto  X  r dpc ( x
We call this graph Partner Correlation Graph (PCG for short). Remember that we aim to identify the negative cor-relations among partners. Thus, the weight w ij is set to the opposite value of r dpc ( x i ,x j ), and only the directed edges with the weight values bigger than  X  exist in this graph. Figure 4 gives an example of the generated PCG graph with 5 partners when  X  =0 . 1. We use this graph as a running example in this paper.
In building the PCG graph we compute the DPC values in both the two directions between each pair of partners. In other words, we view each partner as both cheating sell-er and cheating buyer. For example, when we consider the edge from v 5 to v 1 in Figure 4, v 1 acts as a cheating buyer. Meanwhile, on the edge from v 1 to v 4 , v 1 acts as a cheating seller. However, in real world only the partners with abun-dant capital can be cheating sellers since a large amount of investment is needed for big deals. Also, these partners have no incentive to be cheating buyers. On the other hand, the partners with less capital can only be cheating buyers. Therefore, the two roles are exclusive, indicating that each partner can have only one character, i.e. cheating seller or cheating buyer 1 .

To meet this requirement, we need a method to parti-tion the nodes of the graph into two disjoint subsets, i.e. A and B , representing the sets of suspicious cheating sellers and buyers respectively. Also, only the edges going from the source-side A to the sink-side B remain in the graph. Figure 5 gives an example of the resultant bipartite.
To this end, we actually need a cutting method for a weighted directed graph G =( V,E,w ), aiming to maxi-mize certain objective function of the partition ( A, B )of V . In this study, we formulate a new cut problem (called Max-DifCut ), which outputs the partition ( A, B )( A  X  B =  X  ,A  X  B = V ) such that is maximized, where w AB = i  X  A,j  X  B w ij is the weight sum of the edges from A to B ,and w BA is defined similarly.
Section 4 will detail why we formulate this new problem based on the application background. As it is a NP-hard problem, we give the greedy algorithm for it. The experi-mental results will empirically show that the ranking perfor-mance will be significantly improved if the ranking is based on the resultant bipartite graph after removing the noisy edges.
Next, we propose the generative probabilistic model to fit the resultant bipartite with the edge weights. In this probabilistic model, we assume each partner has two laten-
To simply this problem, we also assume that the character of a partner does not change along the time. tvariables  X  and  X  , representing the cheating degree as a cheating seller and a cheating buyer respectively. Specifical-ly, bigger  X  indicates higher motivation of being a cheating seller and smaller  X  suggests higher motivation of being a cheating buyer. We suppose that the cheating probability that partner v 1 sells products to v 2 can be modeled as a Be-ta function with the parameters of the  X  value of the source node and the  X  value of the sink node. The modeling process is to fit the model to the graph so that the cheating probabil-ity that any partner sells products to another approximates as much as possible to the weight on the corresponding edge.
With the model parameters we develop the ranking score for each partner, measuring the degree of cheating either as seller or buyer. All these details will be presented in Section 5.
In this section, we detail the computing of DPC to build the PCG graph. First, we introduce the preliminaries on Pearson correlation and dynamic time warping. Then, we show how to compute DPC with the DTW technique. Here, be the two sequences of purchase volumes from the two part-ners in the past m months, respectively.
In statistics, a measure of correlation is a numerical value which describes the strength of a relationship among vari-ables. Pearson correlation is widely used for analyzing nu-merical variables such as time series. Giving two sequences x and y , Pearson correlation can be computed in Equation (1), where x ( y ) denotes the average value of the entries in x ( y )and  X  x (  X  y ) denotes the standard deviation of x ( y ).
Note that the correlation is computed via the tick-to-tick correspondence by multiplying x ( t )  X  x  X  time stamp t . Moreover, it is symmetric since r ( x,y ) = r holds. Pearson correlation ranges from  X  1to1. r =1 implies that a linear equation describes the relationship be-tween x and y perfectly, with all data points lying on a line for which y increases as x increases. r =  X  1 implies that all data points lying on a line for which y decreases as x increases. r = 0 implies that there is no linear correlation between them.

However, Pearson correlation has the following limitations in this application. First, Pearson correlation is a symmetric measure, and the order of its two inputs is irrelevant to its output. It cannot distinguish cheating sellers from cheating buyers in this application. Thus, an asymmetric measure is needed in this application. Second, Pearson correlation is computed with the tick-to-tick correspondence on the two sequences. However, an abnormal deal does not necessarily complete during the same month, but with a delay of several months after the cheating seller applies a big deal from the manufacturer. Thus, we need to consider this time offset in computing the correlation.
Next, we briefly introduce the technique of dynamic time warping, which can be used to transform Pearson correlation into an asymmetric measure. Dynamic time warping [20] is a transformation that allows sequences to be stretched a-long the time axis to minimize the distance between them. Giving two sequences x =( x (1) , x (2) , ..., x ( m )) and y = noted to represent the distance between ( x ( i ) , y ( j )). Thus, we can obtain a cost matrix C  X  R m  X  m defined by C ( i, j )= c ( x ( i ) , y ( j )). Here, an alignment is a warping path p = ( p [1 , ..., L ] satisfying the following three conditions, The total cost of a warping path p is defined in Equation (2), Here, L is the length of the warping path.

An optimal warping path between x and y is a warping path that have minimal total cost. The DTW distance D ( x, y ) of two sequences x and y is the sum of tick-to-tick distances after the two sequences have been optimal-ly warped to match each other, which is denoted in Equa-tion (3),
To determine the optimal warping path, a dynamic pro-gramming algorithm was proposed to solve it. Let d ( i, j ) denotes the accumulate distance of ( i, j ), then the iterative condition holds as in Equation (4),
As discussed in Sections 2.1 and 3.1 we need an asymmet-ric correlation measure which also considers the time offset from the source sequence to the sink sequence. Thus, we adapt the original Pearson correlation by using the DTW technique. First, we define the following local cost measure as in Equation (5),
Similarly, we can define the total cost as Equation 6, where ( i l ,j l )= p l denotes an element of a warping path p . Then, we can find an optimal alignment p  X  =( p  X  1 , ..., p which has the minimal total cost c p  X  ( x, y ).
To avoid degenerated matching, by which we mean a rel-atively small section of one sequence maps onto a relatively large section of another, the warping path is often limit-ed by global constraints. The warping scope s is the area that the warping path is allowed to visit in warping matrix. The Sakoe-Chiba band [18] is a well-known constraint that restricts the warping path to the range of | i l  X  j l | X  ever, in our application a cheating seller can only sell the products to a cheating buyer in or near after the month. To address this issue, we propose a more restricted constraint, namely 0  X  j l  X  i l  X  s . As illustrated in Figure 6, when s = 2 the black and grey areas denote all probable warp-ing elements between x and y under this constraint, and the black areas denote the optimal warping path.

Thus, the DPC value of two sequences x and y , denoted by r dpc ( x, y ), is given in Equation (7),
Note that it is easy to prove that r dpc ( x, y )  X  r ( x, y )always holds for any two sequences x and y since Pearson correlation is a special case of DPC when s = 0. Also, it is clear that r measure.

Algorithm 1 gives the process of computing r dpc .After the initialization in Lines 1 through 8, Lines 9 through 14 compute the optimal warping value. Line 15 gets the size of optimal warping path. Line 16 computes the value of r dpc Algorithm 1 Computing DPC
With the proposed DPC measure we can exhaustively compute this value on the two directions of each pair of partners, and then build the PCG graph as discussed in Section 2.1. Figure 4 shows an example of this graph. Note that only the edges with strong negative correlations remain in the resultant PCG graph.
In the PCG graph it is possible that a node has both out-edges and in-edges. The nodes with both out-edges and in-edges are considered as both cheating sellers and cheating buyers simultaneously. However, in reality each partner can act as only one character, namely cheating seller or cheating buyer. With this requirement we need a method to partition the nodes of the graph into two disjoint subsets, i.e. A and B , representing the sets of suspicious cheating sellers and buyers respectively. Only the edges going from the source-side A to the sink-side B remain in the graph. In other words, this method is to remove some edges, ensuring that each partner can only have either in-edges or out-edges.
Specifically, the task is to find a cut set in a directed weighted graph such that certain objective function of the partition ( A, B )of V is maximized. It can be used to re-move the noisy edges in the PCG graph. The empirical experiments will show that the ranking performance based on the resultant bipartite is significantly better than that on the PCG graph. All the notations used here are summarized in Table 1.
Most of the previous studies in this area focus on the prob-lem of Max-DiCut [14], which outputs the partition ( A, B ) of V ( A  X  B =  X  ,A  X  B = V ) such that w AB is maximized, where w AB = i  X  A,j  X  B w ij is the weight sum of the edges from A to B .

In this work, we formulate a new graph cut problem, called Max-Difference CUT ( Max-DifCut for short), on the directed weighted graph. In the following we detail how this new problem is motivated by the application background of cheating detection.

Assume we have a partition ( A, B )of V ,where A and B are the sets of cheating sellers and buyers respectively. Based on this partition, we can propose the following measure  X  w to check the degree that node v i is a cheating partner, either as seller or buyer,
In the above equation we consider the two situations, namely v i  X  A (when v i is a cheating seller) and v i  X  B (when v i is a cheating buyer). First, as shown in Figure 7(a), when v i  X  A this measure is defined as w iB  X  w  X  i . Here, w is the weight sum of the edges from node v i to any node in B (the weight sum of all the solid-arrow lines), actually mea-suring the amount of all the possible cheating behaviors if node v i is a cheating seller; w  X  i is the weight sum of the edges from any other node to node v i (the weight sum of all the dash-arrow lines), measuring the amount of noises if node v i is a cheating seller. Thus, the bigger this differ-ence of w iB  X  w  X  i , the more likely that node v i is a cheating seller. Second, as shown in Figure 7(b), when v i  X  B this measure is defined as w Ai  X  w i  X  . Similarly, the weight sum of solid-arrow lines represents w Ak and that of dash-arrow lines represent w i  X  . The bigger this difference of w Ai themorelikelythatnode v i is a cheating buyer.

Therefore, we aim to find the partition in which this mea-sure on each node is as big as possible. By adding these measures on all the nodes we get the objective function as in Formula 9 and can transform it into Formula 10, Note that w G in Formula 10 is a constant for any graph G . Therefore, we aim to find a partition ( A, B )of E such that 3 w
AB  X  w BA is maximized.
Similar to the classical Max-DiCut problem, the hardness of Max-DifCut is also NP-hard [2]. Thus, we need the ap-proximate algorithm to solve it. In this study we develop a greedy algorithm for this problem 2 .
 As shown in Algorithm 2, it performs iteratively. Let A ,B be the sets of cheating sellers and cheating buyers respectively in the current round. At the beginning, A ,B are both empty sets. Then, in each round we select one node which maximizes certain greedy function, and put it into A or B . Based on the problem formulation, Equation (8) can be naturally adapted as the greedy function. Specifically, this greedy function  X  w i is defined as
Here, w iB and w Ai in Equation (8) are substituted with ( w i  X   X  w iA )and( w  X  i  X  w B i ) respectively. It is based on the fact that for the current sets of A ,B ,( w i  X   X  w iA ( w  X  i  X  w B i ) are actually the upper bounds of w iB and w respectively. Take the upper bound of w iB as an example. For v i , the nodes on the out-edges of v i is denoted by V { v j | ( i, j )  X  E } . It can be divided into two parts with the current A ,namely V out i  X  A and V out i  X  A . If we put all the nodes in V out i  X  A into B ,then w iB is equal to ( w i  X  Thus, for the current A ,( w i  X   X  w iA ) is the upper bound of w
Then, with the definition of  X  w i , Lines 3 through 7 in Al-gorithm 2 compute  X  w i of all the nodes, and Line 8 chooses the node that maximizes  X  w i . We use the example in Fig-ure 4 to describe the running process.  X  Round 1. At the beginning with A = {} ,B = {} ,  X  w A 1 reaches the maximal value. Thus, we put node v 1 into A . Meanwhile, in order to make  X  w 1 in the final partition equal to  X  w A 1 , we also put all the nodes on the out-edges of v 1 , namely nodes v 2 and v 4 ,into B . This is conducted by Lines 9 through 16 in Algorithm 2. After this round, A = { v 1 } ,B = { v 2 ,v 4 } .  X  Round 2. Next, for the second round we compute the ure 8 illustrate the process for computing  X  w A 3 ,  X  w B spectively.

As shown in Figure 8(a), we compute  X  w A 3 if v 3 is put into A . Here, v 3 has 4 outedges. However, since currently v  X  A the weight on the edge of ( v 3 ,v 1 ) cannot be consid-ered. Thus, we have
The technique of Semi-Definite Programming (SDP) can also be used here, and will be discussed in Section 7 for the related work.
Similarly, as shown in Figure 8(b), we compute  X  w B 3 if v is put into B . Here, v 3 has 3 inedges. However, since cur-rently v 2  X  B ,v 4  X  B the weights on the edges of ( v 2 and ( v 4 ,v 3 ) cannot be considered. Thus, we have Again, we can get  X  w A 5 =  X  0 . 75,  X  w B 5 =  X  0 . 7. Clearly,  X  w B 3 is the maximal value among  X  w A 3 ,  X  w B 3 ,  X  w Thus, we put node v 3 into B . Then, we consider all the n-odes on the in-edges of node v 3 ,namely v 2 , v 4 , v 5 .Since v and v 4 are already in B , we can only put v 5 into A . Finally, the partition result is as follows, Algorithm 2 The Greedy Algorithm (Max-DifCut)
In this section, we will detail the method of ranking part-ners. We first introduce the basic ranking method which is motivated by partition process. Then we demonstrate a probabilistic ranking model, and give the ranking function based on it.
Here, we discuss the ranking methods using only the edge weights on the resultant bipartite.

Given the partition ( A, B )of V generated by the proposed graph cut method,  X  w i in Equation (8) can be naturally adopted as the ranking score, namely we call this ranking method Cut Rank , since the graph cut method is performed to generate the bipartite firstly.
Next, we propose a probabilistic model to generate the weighted graph G =( V,E,w ), and use the resultant model parameters to rank partners. Note that this probabilistic model can be applied to any weighted graph G ,e.g. the original PCG graph and the bipartite graph after partition-ing.

The modeling process is to fit the model to the graph data so that certain objective function is minimized. Here, the objective function consists of two parts, i.e., data fitting and model complexity. The former one guarantees that the trained model represents the observed data while the second one avoids the over-fitting situation.

Formally, let  X  denotes the set of model parameters, p ( w denotes the probability that w ij is sampled given  X  ,and denotes the model complexity function. Then, the objective function L ( G,  X  )isdefinedas where  X   X  [0 , +  X  ] is a trade-off parameter to adjust the relative importance between the two terms. In the following we give the design of p ( w ij |  X  )and C (  X  ).

The design of p ( w ij |  X  ). In this model we consider the motivation of being a cheating seller and a cheating buyer, separately. Specifically, we assume that each node v i has two latent variables,  X  i and  X  i . The former indicates how much likely the partner will cheat as cheating seller while the latter as cheating buyer. Specifically, bigger  X  indicates higher motivation of being a cheating seller and smaller  X  suggests higher motivation of being a cheating buyer.
Given two nodes v i and v j , the probability of v i selling products to v j , denoted by p ij , is modeled as a Beta distri-bution in Equation (14).
 where  X ( x )=( x  X  1)! is the Gamma function. In other words, p ij , modeling the probability of v i selling products to v , obeys the Beta distribution with the parameter (  X  i , X 
Figure 9 shows the probability density function of p ij for different combinations of (  X  i , X  j )when  X  i &gt; 1,  X  j can see that for large  X  i and small  X  j , p ij is more probable to be close to 1, indicating a high probability of cheating behavior from v i to v j . On the other hand, if  X  i is small and  X  is large, p ij tends to be small, leading to a low probability of cheating from v i to v j . It is intuitively reasonable since cheating is more likely to happen in pairs if one has high probability of being a cheating seller and the other has high probability to be a cheating buyer.
To fit the graph data, we would like to choose the proper latent variables (i.e.  X  and  X  ) for each node so that it is the most probable that w ij can be generated by  X  .Inother words, p ( w ij |  X  ) in Equation (13) can be defined as follows,
The design of C (  X  ). To avoid the over fitting, we con-strain the update of the model parameters from the original ones. Formally, let  X  and  X  denote the initial values for all the nodes before training. This term is thus designed as
It actually limits the KL Divergence between the two Beta functions of final parameter values and the initial ones. It controls the modification on the parameters cannot go far. We can further compute D KL (  X  i , X  i ;  X  , X  ) as follows, where  X ( x ) is the digamma function, B(  X  ) is the beta func-tion and f ( x ;  X ,  X  ) is the probability density function of Beta(  X ,  X  ).

Model learning . To learn the parameters to minimize the objective function in Equation (13) we use the gradient descent method, shown in Algorithm 3. The general idea is to first assign the initial values to the parameters and compute the partial derivatives for them. Then, at each iteration the parameters are updated based on the partial derivatives until convergence. After some empirical studies we find that the change of the initial values  X  and  X  do not change the final ranking order of partners. Thus, we set  X  =10 , X  = 10 in this study.

The partial derivatives is shown as follows, where Algorithm 3 Parameter learning of the probabilistic model 2: repeat 3: for all v k  X  V do 5: end for 9: end for 11: for all v k  X  V do 14: end for 15: until convergence
After we learn the latent variables of all the nodes (i.e.  X  and  X  i for i =1 ,  X  X  X  ,n ), for any edge ( v i ,v j )  X  E we can compute the expected value of p ij as follows,
Here,  X  ij is the expected value of the Beta distribution with the parameters of (  X  i , X  j ).

Ranking on the bipartite with the probabilistic model . Then, after we perform the probabilistic model on the bipartite the new ranking score can be defined as and  X  Ai , X  i  X  are defined similarly.

This ranking score is obtained by replacing w ij in Equa-tion (8) with  X  ij . We call this ranking method Cut ProbRank since both the graph cut method and the probabilistic model are used. The empirical experiments will show that the new ranking method with the support of the probabilistic model significantly outperform the original method based on the edge weights.
In this section, we detail the experimental results on a real-world data set from a world-wide IT company.
The data set for evaluation is from the market channel of a world-wide IT company. We have two kinds of partner-s, namely gold membership partners and silver membership partners . We have totally 104 gold membership partner-s and 424 silver membership partners. For each partner the sequence of its monthly purchase volume is provided. The time interval of the sequence is from January 2009 to December 2012, a total of 48 months. The following experi-ments are conducted on the following two groups of partners, namely gold membership partners (Gold for short), and all the partners (All for short).

Moreover, we have a blacklist of real cheating partners as the ground truth data. There are 17 (85) true cheating part-ners out of the gold (silver) membership partners 3 .Thus, we aim to rank the real cheating partners as high as pos-sible by the proposed method. Thus, the proposed model will provide some guidance for the audit work and greatly reduce the manual efforts for the judicial examination of the business and financial records.

With the blacklist of M true cheating partners, we de-velop the following metrics to measure how high these real cheating partners are ranked by the proposed method. For anumber k , we can count the number of hits on the real cheating partners in the top-k ranking list. Then, we define precision , recall ,and F 1 as follows,
Clearly, the above metrics all depend on k .Thus,we can plot the curves as shown in Figure 10, where the X -axis represents k and the Y -axis shows the metric values. Then, for each of the three curves we can calculate the Area Under Curve (AUC) metric. These three values are denot-ed as AUC p, AUC r, AUC F 1 for the curves of precision, recall and F1 respectively. Note that the higher values of these three metrics indicate better performances of a rank-ing method.
Here, we summarize the experimental results we plan to empirically show and discuss the compared methods. Specif-ically, we will show the following results.
Note this blacklist does not distinguish true cheating sellers from true cheating buyers. 1) Asymmetric correlation measure . First, we will show the proposed asymmetric correlation measure is effec-tive in this ranking task. In other words we will show that the ranking methods based on the asymmetric correlation measure are significantly better than the ones based on the symmetric correlation measure.

For this comparison we propose a ranking method based on the symmetric Pearson correlation. The computing pro-cess is as follows: 1) for each pair of partners we compute the symmetric Pearson correlation between the correspond-ing two sequences of sales volumes; 2) generate an undirect-ed graph of partner correlation, where each node represents individual partner and the edge weight is set to the negative value of the Pearson correlation value between the linked two nodes. A user-specified parameter 0 &lt; X &lt; 1 can also used here to remove the edges with the weights smaller than  X  ; 3) for each node the ranking score is defined as the weight sum of its linked edges. We call this method PearsonRank . 2) Partitioning of partners for edge removing .Sec-ond, we will show that the proposed method of Max-DifCut helps to improve the ranking performance. In other words, we will empirically show that ranking on the resultant bi-partite from Max-DifCut is significantly better than ranking on the original PCG graph.

For this comparison, we propose the following ranking s-core for any node v i on the original directed PCG graph,
It is actually the difference of the weight sum of its out-edges and in-edges. We call this ranking method Noncut Rank since there is no the graph cut method used in advance. 3) Probabilistic model to generate the graph .Third, we will show that the ranking methods based on the learned parameters of the proposed probabilistic model are signifi-cantly better than the ones based on the edge weights. We show this on both the generated bipartite and the original PCG graph.  X  On the generated bipartite we actually have two rank-ing methods, namely Cut Rank (detailed in Section 5.1) and Cut ProbRank (detailed in Section 5.3). We will show that Cut ProbRank is significantly better than Cut Rank in terms of ranking performance.  X  On the original PCG graph we already have the method of Noncut Rank (in Equation (23)) based on the edge weight-s. Similarly, based on the probabilistic model we propose the following ranking score for any node v i on the original directed PCG graph, where  X  i  X  = ( v ranking score is obtained by replacing w ij in Equation (23) with  X  ij . We call this ranking method Noncut ProbRank s-ince the probabilistic model is used here, however, the graph cut method is not used. We will show that Noncut ProbRank is significantly better than Noncut Rank in terms of rank-ing performance.

For easy-reading we summarize all the ranking methods for comparison in Table 2. 4) Different Parameter Settings . Finally, we will also show that the above experimental results are consistent with different parameter settings. In our methods we have the fol-lowing three parameters, i.e. the dynamic warping scope s , the threshold  X  (for removing those edges whose correlations are not negative enough), and the parameter  X  used in the probabilistic model. The ranges for these parameters are discussed as follows.  X  DTW Scope s . Through market investigation, a partner can only store the products at most for 3 months. Thus, we select s from { 1 , 2 , 3 } .  X  Threshold  X  . we set up a threshold  X  to remove those edges whose correlations are not negative enough. For a big  X  , the resultant graph will contain the isolated nodes which do not have any in-edges and out-edges, and without any linked edges the ranking function on these isolated nodes becomes useless. Thus, we need the range of  X  so that the generated graph is a connected one. Table 3 shows this range of  X  for different settings of s on the two data sets. In this study, we select the  X  value by the step of 0 . 1.  X  For probability ranking methods (i.e. Noncut ProbRank and Cut ProbRank ), we set  X  to 0.1 by default. We will al-so empirically show that  X  affects the ranking performance of the probabilistic model.

Recall that the ranges of s and  X  for the two data sets are shown in Table 3. On each setting of s and  X  we can compute the values of AUC p , AUC r ,and AUC F 1for each method. Then, for each method the best value and the average value over all the parameter settings are shown in Tables 4 and 5. For each pair of the ranking methods on the directed graph we also perform the t-test to check whether the performance difference of the corresponding two methods is statistically significant. The p-values for each pair of methods are shown in Table 6. With these three tables we have the following findings:  X  The ranking of the four methods on the directed graph is Cut ProbRank &gt; Cut Rank &gt; Noncut ProbRank &gt; Noncut Rank in terms of the best performance and the av-erage performance. Note that as shown in Table 6 this per-formance ranking is statistically significant when the p-value threshold is set to 0.05. Also, this ranking holds on the two data sets of Gold and All. As a whole, Cut ProbRank with the graph cut method and the probabilistic model outputs the best performance.  X  In terms of both the best performance and the average performance the methods of Cut ProbRank and Cut Rank are better than PearsonRank on both the two data sets of Gold and All 4 . It indicates that the asymmetric correlation measure helps to improve the ranking performance.  X  We can see that: the ranking on the resultant bipartite is always better than the one on the original PCG graph ( Cut ProbRank &gt; Noncut ProbRank and Cut Rank &gt; Noncut Rank ). It indicates that the proposed method of Max-DifCut removes some noisy edges on the original PCG graph, and then helps to improve the ranking performance.  X  Additionally, we can also see that the ranking based on the probabilistic model is always better than the one based on the edge weights ( Cut ProbRank &gt; Cut Rank and Noncut ProbRank &gt; Noncut Rank ). It indicates that the proposed probabilistic model helps to further improve the ranking performance. The new ranking method is based on the values of  X  ij on any edge from v i to v j .Thevalue of  X  ij is actually the adjustment of the corresponding edge weight w ij . These empirical experiments show this adjust-ment significantly improves the ranking performance.  X  For each method with the parameters for the best per-formance we also plot its curves of precision, recall, and F1 along the the increase of k . These curves are shown in Fig-ure 11. Check the Figures 11a, 11b, 11c for the data set All (the figures for the data set Gold show the similar re-sults). We can clearly see that the curves for the method Cut ProbRank are always at the topmost. As shown in Fig-ure 11a, the precision of Cut ProbRank at k = 30 is around 60%. It means that more than half of the partners in its top-30 ranking list are true cheating partners. This is really a great achievement considering that our method is totally unsupervised. For the other methods the precision values are all below 30%.

Moreover, we check how the parameters of s and  X  affec-t the ranking performance of Cut ProbRank method. As shown in Figure 12, the increase of s brings better rank-ing performance. Since bigger s allows larger time delay
Since the method of PearsonRank has only one parameter  X  we do not perform the t-test between this method and the other ones. when computing the DPC value, it indicates that cheating behaviors may happen with three-month time delay. Also, Figure 12 shows that the smaller  X  generates better rank-ing performance. This may indicate that the edges with the small weight values are still important for this ranking task.
Finally, Figure 13 shows how parameter  X  affects the rank-ing performance of Cut ProbRank and Noncut ProbRank , the two probabilistic methods. For Cut ProbRank ,thein-crease of  X  slightly decreases its ranking performance. We explain this observation as follows. Cut ProbRank perform-s the probabilistic model on the bipartite after removing the noisy edges by the graph cut method. Since the edges re-maining in the bipartite are quite clean, completely fitting the data with  X  = 0 outputs the best performance. On the contrary, Noncut ProbRank performs to fit all the edges in the original PCG graph without removing the noisy edges. At this time, the increase of  X  helps to constrain the change of the model parameters, and thus very slightly improve the ranking performance. In this section, we review the related work to this study. First, we list some previous works on applying Pearson cor-relation and dynamic time warping to real world applica-tions. Then, we discuss the previous works on the graph cut methods.

Pearson correlation. Pearson correlation [12] is a well-known measure of linear correlation of two dependent vari-ables. This method was first developed by Karl Pearson and is widely used to measure the similarity between pair-wise time series. Liao [21] used this measure for clustering time series. Papadimitriou et al. [17] proposed a local correlation measure to track the correlation among time-evolving time series. Xiong et al. [22] explored the upper bound of Pear-son correlation to identify strongly correlated pair-wise time sequences. Kawale et al. [7] used the correlation measure to discover dipoles, which represent long distance connections between the pressure anomalies of two distant regions that are negatively correlated with each other. Their contribu-tion is to propose the method to evaluate the significance of the correlations. We also tried this significance measure in our application. However, the experiments show that re-moving the correlations with small significance values does not clearly improve the ranking performance for our task. Note that all the correlation measures used in these studies are symmetric.
Dynamic time warping. Dynamic time warping is widely used in various fields such as bioinfomatics, chem-ical engineering and robotics etc. [1, 11, 15, 19]. Keogh et al. [8] applied DTW technique for exactly indexing time series from large database by proposing a lower bound of DTW distance. In [9] and [10], DTW is modified to ap-proximate high level abstraction of data and track the local accelerations and decelerations in the time axis.
By introducing DTW technique with the forward warping direction to Pearson correlation, we develop the measure of Directed Pearson Correlation. The forward warping direc-tion is motivated by the fact that the cheating deals between cheating sellers and cheating buyers cannot be completed at the same month of the falsified deals, but with a time delay up to 3 months. This directed measure helps to differen-tiate cheating sellers and cheating buyers, and improve the ranking performance as shown by the empirical experiments.
Graph cut methods. In this study, motivated by the application background of cheating detecting we propose a new graph cut problem, called Max-DifCut, with the objec-tive function of 3 w AB  X  w BA (detailed in Equation ( ?? )). Most of the studies in this area focus on the Max-DiCut problem with the objective function of w AB and propose the greedy and SDP solutions [3, 4, 5, 14]. We also tried the SDP solution to the new problem. The SDP solution achieves the bipartite with the bigger value of the objective function. However, the ranking on the bipartite from the SDP solution does not improve clearly. Thus, in this paper we only discuss the greedy solution to this new problem.
In this paper we address the problem of detecting cheat-ing in the distribution channels. Our solution consists of three modules: 1) use the developed directed Pearson cor-relation measure to build the directed Partner Correlation Graph; 2) Partitioning the graph by the Max-DifCut task; 3) Ranking based on the probabilistic model, which fits the bipartite graph (generated by the second step). The exper-iments show that the method Cut ProbRank with all these modules achieves the best performance with the statistical significance in ranking the cheating partners.

The current work only considers the increase or decrease tendency in computing the correlation of two sales-volume sequences. In the future we will consider the quantities each partner buys from the manufacturer to further improve the performance. Also, we will further explore the situation that the character of a partner can change with time. At last, the proposed pairwise correlation measure may not work well when the groups of partners collude in the distribution channel. We will try to combine the Granger Causality [13] with current DPC to address this problem. This work was supported by the National Natural Science Foundation of China (No. 61175052, 61203297, 61035003, 61473274), National High-tech R&amp;D Program of China (863 Program) (No. 2014AA015105, 2013AA01A606, 2012AA011003) and Chongqing Science Foundation (No. CSTC 2010BB2214).
