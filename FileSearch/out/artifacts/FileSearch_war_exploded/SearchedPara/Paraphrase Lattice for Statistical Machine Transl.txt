 Lattice decoding in SMT is useful in speech trans-lation and in the translation of German (Bertoldi et al., 2007; Dyer, 2009). In speech translation, by using lattices that represent not only 1-best re-sult but also other possibilities of speech recogni-tion, we can take into account the ambiguities of speech recognition. Thus, the translation quality for lattice inputs is better than the quality for 1-best inputs.

In this paper, we show that lattice decoding is also useful for handling input variations.  X  X nput variations X  refers to the differences of input texts with the same meaning. For example,  X  X s there a beauty salon? X  and  X  X s there a beauty par-lor? X  have the same meaning with variations in  X  X eauty salon X  and  X  X eauty parlor X  . Since these variations are frequently found in natural language texts, a mismatch of the expressions in source sen-tences and the expressions in training corpus leads to a decrease in translation quality. Therefore, we propose a novel method that can handle in-put variations using paraphrases and lattice decod-ing. In the proposed method, we regard a given source sentence as one of many variations (1-best). Given an input sentence, we build a paraphrase lat-tice which represents paraphrases of the input sen-tence. Then, we give the paraphrase lattice as an input to the Moses decoder (Koehn et al., 2007). Moses selects the best path for decoding. By using paraphrases of source sentences, we can translate expressions which are not found in a training cor-pus on the condition that paraphrases of them are found in the training corpus. Moreover, by using lattice decoding, we can employ the source-side language model as a decoding feature. Since this feature is affected by the source-side context, the decoder can choose a proper paraphrase and trans-late correctly.

This paper is organized as follows: Related works on lattice decoding and paraphrasing are presented in Section 2. The proposed method is described in Section 3. Experimental results for IWSLT and Europarl dataset are presented in Sec-tion 4. Finally, the paper is concluded with a sum-mary and a few directions for future work in Sec-tion 5. Lattice decoding has been used to handle ambigu-ities of preprocessing. Bertoldi et al. (2007) em-ployed a confusion network, which is a kind of lat-tice and represents speech recognition hypotheses in speech translation. Dyer (2009) also employed a segmentation lattice, which represents ambigui-ties of compound word segmentation in German, Hungarian and Turkish translation. However, to the best of our knowledge, there is no work which employed a lattice representing paraphrases of an input sentence.

On the other hand, paraphrasing has been used to enrich the SMT model. Callison-Burch et 1
Figure 1: Overview of the proposed method. al. (2006) and Marton et al. (2009) augmented the translation phrase table with paraphrases to translate unknown phrases. Bond et al. (2008) and Nakov (2008) augmented the training data by paraphrasing. However, there is no work which augments input sentences by paraphrasing and represents them in lattices. Overview of the proposed method is shown in Fig-ure 1. In advance, we automatically acquire a paraphrase list from a parallel corpus. In order to acquire paraphrases of unknown phrases, this par-allel corpus is different from the parallel corpus for training.

Given an input sentence, we build a lattice which represents paraphrases of the input sentence using the paraphrase list. We call this lattice a paraphrase lattice. Then, we give the paraphrase lattice to the lattice decoder. 3.1 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005) X  X  method. Their idea is, if two different phrases e 1 , e 2 in one language are aligned to the same phrase c in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way.

The procedure is as follows: 1. Build a phrase table.
 2. Filter the phrase table by the sigtest-filter. 3. Calculate the paraphrase probability.
 4. Acquire a paraphrase pair.
 3.2 Building paraphrase lattice An input sentence is paraphrased using the para-phrase list and transformed into a paraphrase lat-tice. The paraphrase lattice is a lattice which rep-resents paraphrases of the input sentence. An ex-ample of a paraphrase lattice is shown in Figure 2. In this example, an input sentence is  X  X s there a beauty salon ? X  . This paraphrase lattice contains two paraphrase pairs  X  X eauty salon X  =  X  X eauty parlor X  and  X  X eauty salon X  =  X  X alon X  , and rep-resents following three sentences.  X  is there a beauty salon ?  X  is there a beauty parlor ?  X  is there a salon ?
In the paraphrase lattice, each node consists of a token, the distance to the next node and features for lattice decoding. We use following four fea-tures for lattice decoding.  X  Paraphrase probability (p)  X  Language model score (l) 2  X  Normalized language model score (L)  X  Paraphrase length (d)
The values of these features are calculated only if the node is the first node of the paraphrase, for example the second  X  X eauty X  and  X  X alon X  in line 3 of Figure 2. In other nodes, for example  X  X ar-lor X  in line 4 and original nodes, we use 1 as the values of features.

The features related to the language model, such as (l) and (L), are affected by the context of source sentences even if the same paraphrase pair is ap-plied. As these features can penalize paraphrases which are not appropriate to the context, appropri-ate paraphrases are chosen and appropriate trans-lations are output in lattice decoding. The features related to the sentence length, such as (L) and (d), are added to penalize the language model score in case the paraphrased sentence length is shorter than the original sentence length and the language model score is unreasonably low.

In experiments, we use four combinations of these features, (p), (p, l), (p, L) and (p, l, d). 3.3 Lattice decoding We use Moses (Koehn et al., 2007) as a decoder for lattice decoding. Moses is an open source SMT system which allows lattice decoding. In lattice decoding, Moses selects the best path and the best translation according to features added in each node and other SMT features. These weights are optimized using Minimum Error Rate Training (MERT) (Och, 2003). In order to evaluate the proposed method, we conducted English-to-Japanese and English-to-Chinese translation experiments using IWSLT 2007 (Fordyce, 2007) dataset. This dataset con-tains EJ and EC parallel corpus for the travel domain and consists of 40k sentences for train-ing and about 500 sentences sets (dev1, dev2 and dev3) for development and testing. We used the dev1 set for parameter tuning, the dev2 set for choosing the setting of the proposed method, which is described below, and the dev3 set for test-ing.

The English-English paraphrase list was ac-quired from the EC corpus for EJ translation and 53K pairs were acquired. Similarly, 47K pairs were acquired from the EJ corpus for EC trans-lation. 4.1 Baseline As baselines, we used Moses and Callison-Burch et al. (2006) X  X  method (hereafter CCB). In Moses, we used default settings without paraphrases. In CCB, we paraphrased the phrase table using the automatically acquired paraphrase list. Then, we augmented the phrase table with paraphrased phrases which were not found in the original phrase table. Moreover, we used an additional fea-ture whose value was the paraphrase probability (p) if the entry was generated by paraphrasing and 3 1 if otherwise. Weights of the feature and other features in SMT were optimized using MERT. 4.2 Proposed method In the proposed method, we conducted experi-ments with various settings for paraphrasing and lattice decoding. Then, we chose the best setting according to the result of the dev2 set. 4.2.1 Limitation of paraphrasing As the paraphrase list was automatically ac-quired, there were many erroneous paraphrase pairs. Building paraphrase lattices with all erro-neous paraphrase pairs and decoding these para-phrase lattices caused high computational com-plexity. Therefore, we limited the number of para-phrasing per phrase and per sentence. The number of paraphrasing per phrase was limited to three and the number of paraphrasing per sentence was lim-ited to twice the size of the sentence length.
As a criterion for limiting the number of para-phrasing, we use three features (p), (l) and (L), which are same as the features described in Sub-section 3.2. When building paraphrase lattices, we apply paraphrases in descending order of the value of the criterion. 4.2.2 Finding optimal settings As previously mentioned, we have three choices for the criterion for building paraphrase lattices and four combinations of features for lattice de-coding. Thus, there are 3  X  4 = 12 combinations of these settings. We conducted parameter tuning with the dev1 set for each setting and used as best the setting which got the highest BLEU score for the dev2 set. 4.3 Results The experimental results are shown in Table 1. We used the case-insensitive BLEU metric for eval-uation. In EJ translation, the proposed method obtained the highest score of 40.34%, which achieved an absolute improvement of 1.36 BLEU points over Moses and 1.10 BLEU points over CCB. In EC translation, the proposed method also obtained the highest score of 27.06% and achieved an absolute improvement of 1.95 BLEU points over Moses and 0.92 BLEU points over CCB. As the relation of three systems is Moses &lt; CCB &lt; Proposed Method, paraphrasing is useful for SMT and using paraphrase lattices and lattice decod-ing is especially more useful than augmenting the phrase table. In Proposed Method, the criterion for building paraphrase lattices and the combination of features for lattice decoding were (p) and (p, L) in EJ translation and (L) and (p, l) in EC transla-tion. Since features related to the source-side lan-guage model were chosen in each direction, using the source-side language model is useful for de-coding paraphrase lattices.
 We also tried a combination of Proposed Method and CCB, which is a method of decoding paraphrase lattices with an augmented phrase ta-ble. However, the result showed no significant im-provements. This is because the proposed method includes the effect of augmenting the phrase table.
Moreover, we conducted German-English translation using the Europarl corpus (Koehn, 2005). We used the WMT08 dataset 1 , which consists of 1M sentences for training and 2K sen-tences for development and testing. We acquired 5.3M pairs of German-German paraphrases from a 1M German-Spanish parallel corpus. We con-ducted experiments with various sizes of training corpus, using 10K, 20K, 40K, 80K, 160K and 1M. Figure 3 shows the proposed method consistently get higher score than Moses and CCB. This paper has proposed a novel method for trans-forming a source sentence into a paraphrase lattice and applying lattice decoding. Since our method can employ source-side language models as a de-coding feature, the decoder can choose proper paraphrases and translate properly. The exper-imental results showed significant gains for the IWSLT and Europarl dataset. In IWSLT dataset, we obtained 1.36 BLEU points over Moses in EJ translation and 1.95 BLEU points over Moses in 4 EC translation. In Europarl dataset, the proposed method consistently get higher score than base-lines.

In future work, we plan to apply this method with paraphrases derived from a massive corpus such as the Web corpus and apply this method to a hierarchical phrase based SMT.

