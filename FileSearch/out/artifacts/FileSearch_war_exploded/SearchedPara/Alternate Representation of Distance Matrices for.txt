
The most suitable method for the automated classifica-tion of protein structures remains an open problem in com-putational biology. In order to classify a protein structure with any accuracy, an effective representation must be cho-sen. Here we present two methods of representing protein structure. One involves representing the distances between the C  X  atoms of a protein as a two-dimensional matrix and creating a model of the resulting surface with Zernike poly-nomials. The second uses a wavelet-based approach. We convert the distances between a protein X  X  C  X  atoms into a one-dimensional signal which is then decomposed using a discrete wavelet transformation. Using the Zernike co-efficients and the approximation coefficients of the wavelet decomposition as feature vectors, we test the effectiveness of our representation with two different classifiers on a dataset of more than 600 proteins taken from the 27 most-populated SCOP folds. We find that the wavelet decomposi-tion greatly outperforms the Zernike model.With the wavelet representation, we achieve an accuracy of approximately 56%, roughly 12% higher than results reported on a similar, but less-challenging dataset. In addition, we can couple our structure-based feature vectors with several sequence-based properties to increase accuracy another 5-7%. Finally, we use a multi-stage classification strategy on the combined fea-tures to increase performance to 78%, an improvement in accuracy of more than 15-20% and 34% over the highest reported sequence-based and structure-based classification results, respectively. The structure-based comparison of protein molecules is an underlying component of many applications in computa-tional biology, from molecular alignment and function pre-diction to database clustering and classification. As of May 2005, the Protein Data Bank (PDB) 1 lists almost 31,000 structures, more than double the number of entries from just five years ago. While large, the number of solved protein structures is much less than the number of determined se-quences, with more than 2.1 million entries contained in the PIR-NREF database 2 . Whole-genome shotgun-sequencing experiments have led to an explosion in the number of se-quenced proteins. Currently, the structure of a protein must be determined manually through x-ray crystallography or nuclear magnetic resonance (NMR). However, the expected development of automated, high-throughput crystallization techniques will likely lead to a similar increase in the num-ber of solved structures.

Databases such as CATH [15] and SCOP [14] that provide information on structural classification assign proteins to their respective domains using a variety of semi-supervised techniques. They currently lag the repositories of solved pro-tein structures by several months. An increase in the rate of protein structure determination means that it will become even more difficult to provide timely updates without a more automated process. For any classification process to have a hope of success, an effective representation must be selected. There are almost as many representation techniques as there are proteins, yet no one method stands out above the rest.
In this work, we propose two dimensionality-reducing methods of decomposition that allow us to group proteins sharing certain common structural elements while at the same time distinguishing between those that are structurally dissimilar. Both methods use the distances between the C  X  atoms of a protein as their basis. The first method maps the C  X  distances to a two-dimensional matrix and creates a representational model of that matrix with Zernike poly-nomials. The second starts by treating the protein as a one-dimensional signal, using the distances between C  X  atoms as the amplitude. We apply a wavelet transformation to the signal and use the corresponding coefficients as our repre-sentation.
We test the effectiveness of our representation on a dataset of more than 600 proteins taken from the 27 most-populated SCOP folds. We analyze performance using both a single-and a multi-stage classification strategy. We find while both methods outperform previously published structural classi-fication results [1], the wavelet decomposition greatly out-performs the Zernike model.With the wavelet representation, we achieve an accuracy of approximately 56%, roughly 12% higher than results reported on a similar, but less-challenging dataset. In addition, we can couple our structure-based fea-ture vectors with several sequence-based properties to in-crease accuracy another 5-7%. Finally, we use a multi-stage classification strategy on the combined features to increase performance to 78%, an improvement in accuracy of more than 15-20% and 34% over the highest reported sequence-based and structure-based classification results, respectively. There are a number of public databases that provide informa-tion on protein structure. Among these databases are the Pro-tein Structure Classification database (CATH) [15] and the Structure Classification of Proteins database (SCOP) [14]. In this work, we use the terminology employed by the SCOP database, which arranges proteins into several of hierarchical levels. The first four are: Class , Fold , Superfamily and Fam-ily . Proteins in the same Class share similar secondary struc-ture information, while proteins within the same Fold have similar secondary structures that are arranged in the same topological configuration. Proteins in the same Superfam-ily show clear structural homology and proteins belonging to the same Family exhibit a great deal of sequence similar-ity and are thought to be evolutionarily related. We focus on the task of Fold recognition as there is insufficient training data in most individual Superfamilies and Families. One of the reasons why an effective solution to the problem of au-tomated fold recognition remains elusive is the large number of potential folds. There are 887 Folds listed in the latest version of the SCOP database (May 2005). While this num-ber is relatively small when compared to the overall size of the database, it still presents a formidable classification chal-lenge. Zernike polynomials are a family of circular polynomial functions that are orthogonal in x and y over a normalized unit circle and consist of three elements [19]. The first is a normalization coefficient. The second element is a radial polynomial component and the third, a sinusoidal angular component. The general form for Zernike polynomials is given by: where n is the radial polynomial order and m represents azimuthal frequency. The normalization coefficient is given by the square root term preceding the radial and azimuthal components. The radial component of the Zernike polyno-mial is defined as:
Zernike polynomials have been used within the medical community for a number of different applications, most re-cently for modeling the shape of the cornea [19]. Studies have shown that these models can effectively characterize aberrations that may exist on the corneal surface and that it is also possible to use the polynomial coefficients as a fea-ture vector in order to distinguish between diseased and non-diseased patients [10, 23]. Secondary structure elements of-ten appear as bulges or ridges on a distance matrix, which are similar in appearance to surface aberrations on a cornea. Therefore, we examine whether such a transformation could be applied to the protein domain.

Polynomials that result from fitting the distance matrix with these functions are a collection of orthogonal circular geometric modes. When combined, these modes form a sur-face that represents a protein distance matrix. The coeffi-cients of each mode are proportional to their contribution to the overall topography of the original data.

When discussing our data transformations, we often re-fer to Zernike polynomials of a certain order. Each of these polynomials are comprised of a varying number of coeffi-cients. We provide the number of coefficients for each order tested in Table 1. Since the series are orthogonal, all of the lower order coefficients are contained within the higher order transformations. Wavelets are mathematical functions that, like a Fourier transformation, can be used to approximate a signal based on a superposition of functions. As with Fourier transforma-tions, wavelets approximate the data using functions at vary-ing frequencies. Unlike Fourier transformations, however, the frequency components are examined at different resolu-tions. This allows for the detection of both low-level, global trends, and high-level, local features [3].
There are two types of wavelet transformations: continu-ous ( cwt ) and discrete ( dwt ). For our experiments, we focus on the discrete wavelet transform. Due to space constraints, we do not cover the cwt . For a more in-depth, theoretical dis-cussion on both transformations, the reader is referred to one of the many publications on wavelet analysis [3, 12]. Given a signal of length N ,the dwt consists of at most log N stages. At each stage, two sets of coefficients are produced, the ap-proximation and detail coefficients. The approximation coef-ficients are generated by convolving the original signal with a low-pass filter, the detail coefficients with a high-pass fil-ter. After passing the signal through the filter, the results are downsampled by a factor of two. This step is repeated on the approximation coefficients, producing another smaller set of approximation and detail coefficients. The process contin-ues for log N steps or until a user-specified level is reached, whichever is less.

Wavelets have been used in a number of applications, ranging from signal processing, to image compression, to numerical analysis [3]. They play a large role in the pro-cessing of biomedical instrument data obtained through tech-niques such as ultrasound, magnetic resonance imaging (MRI) and digital mammography [11]. Wavelets have also been used within the protein domain for various applica-tions, though the reported results have been less than im-pressive [18]. In this section, we detail the algorithms used to create our structural representations. We first describe our method of modeling a protein with Zernike polynomials. We follow with the wavelet-based approach. Before we can apply our transformation, we must convert the protein to a distance matrix. First, we use the 3D coor-dinates of the protein (obtained from the PDB) to calculate the distance between the C  X  atom of each residue. These values are placed into an n x n matrix D ,where n represents the number of residues in the protein and D(i,j) represents the distance between the C  X  atoms of residues i and j .Once we have populated the matrix, we divide by the maximum distance to normalize the values between 0 and 1. Figure 1 provides a graphical depiction of this matrix, with higher el-evations (greater distance) having a lighter color.
Our data transformation is based on methods described in detail by Schwiegerling et al. and Iskander et al. [19, 10]. In summary, the data is modeled in an iterative fashion, com-puting a point-by-point representation of the original data at each radial and angular location up to the user-specified limit of polynomial complexity. The Cartesian coordinate values are converted to a polar notation, treating the center of the distance matrix as the origin. The polynomial coefficients are computed by performing a least-squares fit of the model to the original data, using standard matrix inversion [19]. An illustration of a 10th order representation of the matrix shown in Fig. 1 can been seen in Fig. 2. As one can see, the Zernike representation adequately captures the global shape of the matrix, but does not do as well in representing local features. This realization led us to develop our wavelet-based method. The use of wavelets is natural in applications that require a high-degree of compression without a corresponding loss of detail, or where the detection of subtle distortions and dis-continuities is crucial. Their use in classification is less cut and dry, however. Wavelets can be applied to signals of any length 3 , and as such, will produce a varying number of co-efficients. Deciding which coefficients to use in a feature vector becomes difficult. Does one keep the top k coeffi-cients? How does one identify a  X  X op X  coefficient? What if a wavelet transformation yields less than k coefficients? Are we limited by the size of the smallest signal in the dataset? Does one consider the coefficients from every level, or just the last one? Should one ignore the detail coefficients and just consider the approximation coefficients, or vice versa? In order to make a true  X  X pples-to-apples X  comparison, fea-ture i of object x should correspond to the same property in object y . Simply selecting the top k wavelet coefficients does not provide such a guarantee.

In the end, we elect to use the approximation coefficients of the last (highest) level of the wavelet decomposition as our feature vector. In order to guarantee that we are left with an equal number of coefficients per protein, each input signal must be normalized to have the same length. Since proteins belonging to the same SCOP fold have the same SSEs in the same topological order, intuitively this normalization makes sense. Before describing our method of converting a protein into a signal, it should be noted that to normalize the lengths, we will have to downsample, discarding some information. In rare cases when given an extremely short protein, we may have to upsample in order to obtain an adequate number of datapoints.

We convert a protein into a signal in the following man-ner: First, we transform the 3D coordinates into a distance matrix as in the Zernike case. Since the matrix D is sym-metric across the diagonal, we only consider the top half of the matrix. We proceed to convert it to a one-dimensional signal, with D(0,1) of the matrix corresponding to position 0 of the signal, D(1,2) to position 1 andsoforth. D(0,2) will be mapped to position n , D(1,3) to n+1 , and so on, until the matrix has been converted. In this manner, we first add the values closest to the diagonal and gradually move toward the corner. This process is illustrated in the upper right triangle of Fig. 3.

In addition to representing a protein as a distance ma-trix, it is also possible to represent a structure as a  X  X on-tact map, X  which is simply a binary distance matrix, where avalue D(i,j) is set to 1 if the distance between residues i and j is less than a certain threshold and 0 otherwise. In a contact map, secondary structures such as  X  -helices and parallel  X  -sheets appear as thick bands along the main diag-onal, while anti-parallel  X  -sheets appear perpendicular to the diagonal. We felt that our conversion would capture the sec-ondary structures elements that run parallel to the diagonal of the distance matrix. We also want to model those features that are typically found perpendicular to the diagonal. To that effect, we use a second conversion process to create an anti-parallel signal. This process is illustrated in the bottom right triangle of Fig. 3.

The top left image of Fig. 4 shows an example of a con-verted signal (using the matrix values show in Fig. 1). This signal appears to contain a great deal of  X  X oise. X  Therefore, we decided to denoise the signal to determine if it would result in a cleaner representation that in turn would lead to better classification results. The bottom left image of Fig. 4 represents the denoised version of the signal. We apply a dis-crete wavelet transform to these signals, and use the approxi-mation coefficients of the last level as our feature vector. The reconstructed signal using the original and denoised approx-imation coefficients can be seen in the top and bottom right images (respectively) of Fig. 4. These coefficients represent a 5th level Haar wavelet.

As one can see, the approximated signal is not entirely  X  X aithful X  to the original signals. We are not necessarily in-terested in the fidelity of the reconstruction, however. We are merely looking to find a representation method that can be applied to all proteins that will adequately capture underly-ing trends in the data while exposing the differences between proteins of different folds. The Zernike coefficients were calculated in Matlab with a script that implemented the functions given in Section 2.2 and the algorithm described in the previous section. We com-pute the coefficients for a 4th-10th order Zernike polynomial.
All of the wavelet transformations were calculated using functions from the Matlab Wavelet Toolbox (Toolbox Ver-sion 3.0.1). We tested a number of different wavelet families and decomposition levels in our experiments, but finally set-tled on using a 5th level Haar wavelet as our feature vector. The distance matrix elevation values were computed off-line and stored in a file, one for each protein. They were loaded into Matlab and the signal was resampled to 3200 points. This signal was also denoised using the default Matlab pa-rameters. Both the original and denoised resampled signals were then decomposed to generate the approximation coeffi-cients, with a 5th level Haar wavelet yielding 100 values per signal. Before proceeding, it should be noted that we con-sidered including the 5th level detail coefficients as part of the feature vector, but initial tests indicated that they did not provide an improvement in classification accuracy.
Both of the transformation strategies described above will result in a multi-resolution representation of the input data. The difference is that a Zernike polynomial provides a wavefront-based approach applied to the entire distance ma-trix, whereas the wavelet decomposition results in a high-level description created from localized summaries created after converting the 2D matrix to a 1D signal. The idea is that the wavefront-based approach may be able to capture more high-level trends corresponding to long-distance, ter-tiary relationships among structure elements, while wavelets may be more effective at characterizing fine-grained details that are lost in a global model. Further improvement may be achieved by using both representations in classification, therefore capturing both global and local trends. We explore this issue in our experiments. Before describing our validation experiments, we review some of the related work on the problem of protein classi-fication. Many groups have looked into the problem of fold recog-nition, some using a set of sequence-based physio-chemical properties to represent a protein, others using a structure-based description. Here we briefly cover some of the sequence-based techniques, followed by a short discussion of the structure-based methods.

Ding and Dubchak were one of the first groups to report on the problem of fold recognition. They examined a dataset derived from the 27 most-populated SCOP folds, testing sev-eral sequence-based physical-chemical properties as input to a neural network (NN) and a support vector machine (SVM). By combining the feature vectors for several properties, they were able to identify the correct fold with an accuracy of 56% [4]. Several other groups have used the same dataset with various classification methods and input features, but they have failed to achieve much of an improvement in ac-curacy [22, 2]. Most of the structure-based methods of protein analysis are concerned with the problem of pair-wise or multi-way align-ment. These methods, such as DALI [8] and CE [21] (to name just two) can be considered as a type of classifica-tion, but they are primarily focused on providing a measure of similarity between small groups of proteins (usually pair-wise at first, then extended to multi-way).

A number of recent publications have attempted to create a true structure-based classifier. Some, such as SGM [17], use a nearest-neighbor classification strategy, where a pro-tein is assigned the label of the cluster to whose mean rep-resentative it is most similar. Others use a more tradi-tional classification strategy, creating a structure-based  X  X in-gerprint, X  that can be used as a feature vector in classifi-cation [9, 1]. Aung and Tan validate their technique on a dataset that is very similar, though less-challenging (fewer total folds), than the one used by Ding and Dubchak [4]. However, they identify the correct fold only 45% of the time [1]. Many sequence-based classification strategies rely on machine-learning techniques such as support vector ma-chines (SVMs) or neural networks (NNs). These methods are highly-tunable and can provide excellent performance, but they are most effective when dealing with a binary, or two-class decision problem. When faced with a multi-class (or in this case, multi-fold) dataset, a variation of one-vs-others (OvO) or all-vs-all (AvA) classification is often cho-sen.

With an one-vs-others approach, a classifier is con-structed to decide between two classes: the class in question (the  X  X rue X  class) and the rest (the  X  X thers X ). Given k classes, k different classifiers are constructed and an input protein is assigned the label of whatever classifier returns a yes vote. With OvO classification, the number of objects in the true class is often very small compared to the number of others, which can be problematic, as a classifier can completely mis-classify all the objects in the  X  X rue X  class and still report a high accuracy!
Using an all-vs-all strategy and a dataset with k classes, a classifier is constructed for every possible pair of classes, re-sulting in k ( k  X  1) / 2 different classifiers. Given an input ob-ject, it is tested with every classifier, and the class returning the largest number of  X  X es X  votes is assigned to the object. The drawback here is that AvA requires the construction of a large number of classifiers, while at the same time using a smaller number of datapoints in the construction of those classifiers, which can lead to over-fitting.

As an alternative to the above approaches, we recently proposed a multi-stage classification strategy that can be used to significantly improve accuracy [13]. It relies on the fact that proteins can be grouped based on shared structural characteristics. Therefore, proteins within the same Class should be more structurally similar to each other than to pro-teins in other Classes. The same expectation holds through-out the lower levels of the hierarchy. The general idea is to first classify proteins at the class level, grouping them based on global, high-level similarities. Once this partitioning is complete, we subdivide further, classifying each protein by fold. The intent of this step is to improve accuracy by using a increasingly fine-grained classification model, separating the data based on more local, fold-specific features than a typi-cal classifier that is forced to distinguish between all possible classes. Here we provide details on our dataset and give a description of our classification experiments. The dataset for in these experiments is based on the one first described in the work by Ding and Dubchak [4] and is the same as the one used when testing our Multi-Stage classifi-cation strategy [13]. It consists of 653 proteins belonging to the 27 most populated SCOP folds and no two proteins share more than 35% sequence identity for aligned subsequences longer than 80 residues. In order to show the effectiveness of our representation, we need to run a number of tests. To that effect, we conduct the following classification experiments: 1. Single Stage Classification. This experiment is an at-tempt to replicate the earlier work on sequence-based classi-fication [4, 22, 2]. Here, we create a classifier to distinguish among the 27 different folds in our dataset. Unlike previous studies, which typically employ an OvO or AvA strategy, we use a single classifier and attempt to distinguish between all classes at once. 2. Multi-Stage Classification. We test our representations using our recently proposed multi-stage classification strat-egy [13]. For clarity, the implementation details of that method are repeated here. First, we classify each protein based on its SCOP class, using 10-fold cross-validation. Then, for each SCOP class, we take all of the proteins that were correctly classified and construct a classifier that dis-tinguishes between the folds present within that class. For each SCOP class, we take twenty random samples of the data, dividing it into 70%/30% training/testing splits. While a protein can appear in more than one sample, we do not al-low them to appear more than once within a given sample (i.e. they are randomly selected without replacement). Fi-nally, in an attempt to ensure that every protein is adequately represented, we limit the number of samples in which a pro-tein can be present to half the total number of samples. To compute the accuracy of our multi-stage method, we take the number of proteins that were misclassified at the SCOP class level and add to that the average number of misclassi-fications at the fold level. To compute the average, we sum the number of misclassifications for each random sample and divide by the total number of samples. 3. Classification with Sequence-based Properties. A number of previous studies have attempted to classify pro-teins using a series of sequence-based physio-chemical prop-erties [4, 22, 2, 13]. They looked at classification accu-racy using each individual property as a separate feature vector and also at combining them together to improve re-sults. These studies found that using several properties at once does in fact improve classification accuracy. To build on those results, we test whether we can improve classifica-tion accuracy by extending our feature vectors with a dataset of these combined properties.
 The physio-chemical descriptors characterize the follow-ing properties (symbol and number of dimensions given in parentheses): amino acid composition ( c , 20), predicted secondary structure ( s , 21), and hydrophobicity ( h , 21). For a more detailed discussion on the derivation of these properties, the reader is referred to the work by Ding and Dubchak. [4]. To create this combined dataset, we concate-nate the attributes of one descriptor onto the end of another. We refer to this dataset by the symbols of the combined prop-erties: csh . All of our classification experiments were conducted on a PC with a 2.8 GHz Pentium 4 CPU and 1.5 GB RAM, running Debian Linux with a custom 2.6.9 kernel. The classification was done using the WEKA data mining toolkit, version 3.4 4 and Sun Java 1.4.2. Classification accuracy is given as the True Positive Rate (TPR), or the number of correct classi-fications divided by the total. We conduct our single-stage classification experiments using Na  X   X ve Bayes and C4.5 with Adaptive Boosting (boosting) [16, 6]. We also use Boosted C4.5 as the base for our Multi-Stage classifier. For the single-stage tests, all of the proteins were combined into one large dataset and classified using 10-fold cross-validation. In this section we discuss the results of our classifica-tion experiments. We examined both the original wavelet coefficients and the denoised values, but found the de-noised wavelet coefficients performed better than the orig-inal wavelets. As a result, we only present the results of that dataset, which we denote wd . In addition, while we tested the accuracy of a 4th through 10th order Zernike transforma-tion, we found that the 5th order polynomial provided much better results than the others, so to improve the clarity, we only show those values (we refer to this dataset as 5 z ). We also combined the wavelet and Zernike coefficients into a single dataset, which we label wz . Finally, we found that a Boosted C4.5 decision tree greatly outperformed the Na  X   X ve Bayes classifier, so we focus solely on that classifier. The results of our single-stage classification experiments are presented in Figure 5. The values of the structure-based rep-resentations are given in the Structure columns while the results of the structure representation combined with the physio-chemical properties are given in the Structure + csh columns. As we can see when examining the pure struc-ture results, the denoised wavelet dataset ( wd ) clearly out-performs the Zernike coefficients ( 5z ), 57% to 45%. The Zernike coefficients provide roughly the same accuracy as the method proposed by Aung and Tan [1] and the de-noised wavelet coefficients provide a slightly higher accu-racy than the combined physio-chemical property dataset used by Ding and Dubchak [4]. We did not see any increase in accuracy from the combined wavelet-Zernike dataset. When we combine the structural datasets with the physio-chemical values, we can increase the accuracy of all datasets. We improve accuracy to 62% for the Zernike coefficients, 63% for the wavelet coefficients, and unlike the pure struc-ture case, we can improve accuracy to 65% for the wavelet-zernike dataset. All of these values are higher than any re-sult previously seen on this dataset with purely structural or physio-chemical-based features.
 Figure 6 illustrates the benefits of a multi-stage classifica-tion strategy. With the multi-stage classifier, we are able to improve the classification accuracy of the structure-based datasets. The denoised wavelet coefficients again outper-form the Zernike values, 70% to 54%, but there still is an improvement in accuracy in both datasets, 10% and 9%, re-spectively. The accuracy of the wavelet-Zernike dataset is actually worse than the pure-wavelet dataset, but only by 1%, which is not statistically significant. These results give credence to our belief that a multi-level approach to classi-fication can be very beneficial in areas where the data falls within a natural hierarchy.

Even more impressive results are seen when we com-bine the structural datasets with the physio-chemical values. When combined, we can improve the accuracy of the de-noised wavelet and Zernike datasets to 77% and 76%, re-spectively, and we can improve the wavelet-Zernike accu-racy to 78%. These values represent an improvement of 8% over the best structure-based multi-stage value, 15% larger than any single-stage classification result, and a full 34% larger than previously reported structure-based classifica-tion results [1]. In this work, we describe two strategies of representing protein structure, one using Zernike polynomials, the other based on wavelet decompositions. We demonstrate the ef-fectiveness of our approach by using our representations as input features for the classification of a well-known protein dataset. In addition, when combining our methods with ad-ditional physio-chemical descriptors and using a multi-stage classification strategy, we can improve accuracy by almost 20% over existing sequence-based techniques and 34% over published structure-based results.

Most structure-based methods of protein analysis involve some sort of alignment process. Very few strategies exist that allow for the direct comparison of large numbers of pro-tein structures. Our proposed representation yields a com-pact, effective description that can be used as part of any classification strategy. In addition, the coefficients can be combined with other datasets or descriptors to further in-crease performance. The representations described here can be computed quickly and efficiently, and can be tuned to the specific needs of any application. While the Zernike poly-nomials do not provide as high a classification accuracy as the wavelet method, there may be some cases where such a representation would still be useful.

There exist a number of potential avenues for future re-search. In terms of the wavelet representation, one can vary the number of coefficients, the number of points, the wavelet family, as well as the final decomposition level. Rather than treat a protein as a one-dimensional signal, one could com-pute a two-dimensional wavelet decomposition and use those coefficients instead. The same issues about decomposition level, wavelet family and number of points would also need to be explored. The use of intelligent feature selection might also be helpful in improving accuracy. In the future, we hope to extend this representation to other applications and domains, for instance, creating a multi-dimensional, multi-resolution indexing scheme for biomedical datasets. [1] Z. Aung and K-L Tan. Automatic protein strutc-[2] A. Chinnasamy, W. K. Sung, and A. Mittal. Pro-[3] I. Daubechies. Ten Lectures on Wavelets . Soc. Indust. [4] C. H. Q. Ding and I. Dubchak. Multi-class protein fold [5] I. Dubchak, I. Muchnik, S. Holbrook, and S-H Kim. [6] Y. Freund and R.E. Schapire. Experiments with a new [7] M. Gerstein and M. Levitt. Using iterative dynamic [8] L. Holm and C. Sander. Protein structure compari-[9] J. Huan, W. Wang, A. Washington, J. Prins, and [10] DR Iskander, MJ Collins, and B Davis. Optimal model-[11] Andrew F. Laine. Wavelets in temporal and spatial [12] S. Mallat. A Wavelet Tour of Signal Processing . Aca-[13] K. Marsolo, S. Parthasarathy, and C. Ding. A multi-[14] A. G. Murzin, S. E. Brenner, T. Hubbard, and [15] C. A. Orengo, A. D. Michie, S. Jones, D. T. Jones, [16] J.R. Quinlan. C4.5 : programs for machine learn-[17] O. R X gen and B. Fain. Automatic classification of pro-[18] F. Schwarzer and I. Lotan. Approximation of protein [19] J. Schwiegerling, J. E. Greinvenkamp, and J. M. Miller. [20] S. Y. M. Shi, P. N. Suganthan, and K. Deb. Multi-class [21] I. N. Shindyalov and P.E. Bourne. Modern pro-[22] A. C. Tan, D. Gilbert, and Y. Deville. Multi-class pro-[23] MD Twa, S Parthasarathy, TW Raasch, and M Bul-
