 Probabilistic graphical models provide a powerful frame-work to represent rich statistical dependencies among ran-dom variables, hence their broad application to biology, computer vision and robotics. An edge in a graphical model represents a conditional dependence between the two nodes the edge connects. In a Gaussian graphical model (GGM), edges are parameterized by elements of the inverse covariance matrix (precision matrix). Biologists are increasingly interested in understanding how thousands of genes interact, which has stimulated considerable research into structure estimation of high-dimensional GGM. A popular approach to estimating the graph structure of a high-dimensional GGM is the graphical lasso (Friedman et al., 2007) that independently penalizes each off-diagonal element of the inverse covariance matrix with an L 1 norm. However, the independence assumption is unrealistic for many real-world networks that are structured , where edges are not mutually independent. For example, in gene regu-latory networks , genes involved in similar functional mod-ules are more likely to interact with each other. In addi-tion, there are often high-level interactions between func-tional modules, which can be difficult to identify in a stan-dard GGM representation (see Fig. 1(a)). Importantly, how genes are organized into functional modules and how these modules interact with each other are scientifically relevant. In this paper, we propose a general framework to accom-modate the modular nature of many real-world networks. Our approach, called module graphical lasso (MGL), is characterized by the incorporation of latent variables into the GGM. Fig. 1(b) illustrates a toy example where three latent variables L 1 , L 2 and L 3 have mutual dependencies in addition to connections to observed variables by directed edges. Each of L 1 , L 2 and L 3 represents aggregate activity level of specific functional modules as defined by a core of tightly coupled genes. The undirected edges among latent variables determine the dependencies among these func-tional modules. As can be seen in Fig. 1, MGL provides a more compact representation of the conditional inde-pendence relationships compared to the equivalent GGM. By modeling the conditional independence relationships among k latent variables instead of p ( k p ), we show that MGL scales better than standard graphical lasso when p n , enabling us to efficiently learn a GGM with thou-sands of variables. We considered a toy example with 5 latent variables L and 15 observed variables X with the inverse covariance matrix of the latent variables (  X   X  1 lustrated in Fig. 2. Given the same data consisting of 100 observations on X , MGL almost perfectly estimates  X   X  1 X whereas graphical lasso fails to reveal the latent structure among the 5 groups of variables (Fig. 2).
 The rest of the paper is organized as follows. In Sections 2 and 3, we provide the formulation and the learning al-gorithm for MGL. In Sec. 4, we present the results of our experiments on synthetic data and ovarian cancer gene ex-pression data. We conclude with a discussion in Sec. 5. Derivations of the learning algorithms and proofs are avail-able at http://leelab.cs.washington.edu/projects/MGL. 2.1. Preliminaries Assume that we wish to learn the Gaussian graphical model (GGM) with p variables based on n observations ance matrix. It is well known that the sparsity pattern of  X   X  1 represents the conditional independence relationships among the variables (Mardia et al., 1979; Lauritzen, 1996). Specifically, (  X   X  1 ) jj 0 = 0 for some j 6 = j 0 if and only if X j and X j 0 are conditionally independent given X k with k = { 1 ,...,p }\{ j,j 0 } . Hence, the nonzero pattern of  X   X  1 corresponds to the graph structure of a GGM. In or-der to obtain a sparse estimate for the GGM, a number of authors (Yuan &amp; Lin, 2007; Banerjee et al., 2008; Friedman et al., 2007) have considered maximizing the penalized log likelihood, a method called graphical lasso : where S is empirical covariance matrix,  X  is a positive tun-ing parameter, the constraint  X  0 restricts the solution to the space of positive definite matrices of size p  X  p , and the last term is the element-wise L 1 penalty. We denote by  X  the estimate of inverse covariance matrix throughout the paper. When  X  is large, the resulting estimate will be sparse.
 The probabilistic interpretation of the L 1 penalty term de-fines the optimization parameters  X  as random variables rather than fixed parameters. This interpretation requires that we optimize the joint probability density: The use of the Laplacian prior P ( X  j,j 0 ) =  X / 2  X  exp(  X   X  |  X  j,j 0 | ) leads to the optimization problem de-scribed in Eq. 1. The hyperparameter  X  adjusts the sparsity of the optimization variable  X  . 2.2. Module Graphical Lasso Formulation Let L = { L 1 ,...,L k } be a set of latent variables : L  X  N ( 0 ,  X  L ) , where  X  L is a k  X  k covariance matrix. Let X = { X 1 ,...,X p } be a set of observed variables , each having the distribution: X i | L Z i , X  2 Z where Z i refers to the index of the latent variable which X i is associated with. Here, we refer to a set of observed variables that correspond to the same latent variable as a module . As an example, the j th module M j can be de-fined as M j = { X i | Z i = j } for 1  X  j  X  k . Thus, Z = { Z 1 ,...,Z p } defines the module assignment of p variables into k modules.
 Then, the joint probability distribution function P ( X , L , Z ,  X  L ) of the MGL has the following form: MGL can be seen as a generalized k -means clustering that takes into account the Mahalanobis distances between la-tent variables (2nd term in Eq. 3), in addition to the dis-tances between each variable and the corresponding latent variable (1st term in Eq. 3).
 Given n observations x [1] ,..., x [ n ]  X  R p in X , MGL aims to estimate values on the latent variables L , module assign-ment variables Z , and the inverse covariance matrix of the latent variables  X   X  1 L . In order to estimate the inverse co-variance matrix over the observed variables,  X   X  1 X , we can use the relationship between  X   X  1 L and  X   X  1 X , as described in Lemma 3. 2.3. Properties of Module Graphical Lasso Lemma 1. The joint distribution of X = { X 1 ,...,X p } and L = { L 1 ,...,L k } is Gaussian: ( X , L )  X  N ( 0 ,  X  XL ) , where  X  XL is a ( p + k )  X  ( p + k ) covari-ance matrix.
 Lemma 2. The marginal probability distribution of the ob-served variables X = { X 1 ,...,X p } is Gaussian: X  X  N ( 0 ,  X  X ) , where  X  X is a p  X  p covariance matrix. Lemma 3. Let  X  L be a k  X  k covariance matrix of L . The relationship between  X  X and  X  L is as where A is a p  X  p diagonal matrix whose element A ij = element C ij =  X  (1 / X  2 i ) if X j  X  X  i and 0 otherwise; and |M k | meaning the number of X variables in the module k . 2.4. Related Work MGL jointly clusters variables into modules and learns a network among the modules through an iterative proce-dure. This key aspect differentiates MGL from previous approaches that can be organized into four categories: The first category includes latent factor models, such as latent factor analysis or probabilistic PCA (Tipping &amp; Bishop, 1999), which do not learn the network among la-tent factors.
 Second, Toh &amp; Horimoto (2002) clusters variables first and then learns the dependency structure among the cluster cen-troids, instead of jointly clustering and learning the net-work. This method can achieve improved scalability and interpretability; however, we showed through our extensive experiments that MGL outperforms this approach based on all of the evaluation criteria we incorporated.
 Third, He et al. (2012) models each latent variable as a linear combination of variables and estimates the network among k latent variables. Although this approach also learns a network of k latent variables instead of p variables, it does not explicitly cluster variables, which results in a vastly different learning algorithm from MGL. Clustering of variables is a key feature of MGL reducing the number of parameters and increasing the model X  X  interpretability, which enables interesting analyses shown in Sec. 4.2.2. Finally, many authors attempted to incorporate latent vari-ables into GGMs. However, they do not explicitly cluster variables into modules, and require the learning of  X   X  1 variables instead of k latent variables ( k p ), which dras-tically increase the number of parameters. Chandrasekaran et al. (2012) assume that  X   X  1 of observed variables decom-poses into a sparse matrix and a low-rank matrix, and the low-rank matrix represents the effect of unobserved latent variables. They proposed a convex optimization algorithm that utilizes both L 1 and nuclear norm as penalty terms. The SIMoNe (Ambroise et al., 2009) uses an Expectation-Maximization approach (Dempster et al., 1977) for varia-tional estimation of the latent structure while inferring the network among the entire variables. In contrast, MGL per-forms a more aggressive dimensionality reduction by learn-ing a network of k latent variables instead of p observed variables ( k &lt;&lt; p ). Guo &amp; Wang (2010) proposed an al-gorithm consisting of three steps: 1) apply the graphical lasso to compute an adjacency matrix of the variables; 2) partition variables into disjoint clusters; and 3) estimate a sparse  X   X  1 with a modified penalty term such that within-cluster edges are less strongly penalized. Given the mod-ule assignment of variables, Duchi et al. (2008) proposed to penalize the infinity-norm and Schmidt et al. (2009) pro-posed to penalize the two-norm of the inverse covariance matrix block corresponding to each module in the network of the variables. Marlin et al. (2009) and Marlin &amp; Mur-phy (2009) make use of these methods (Duchi et al., 2008; Schmidt et al., 2009), after first identifying the groups of the variables when the modular structure is unknown. 3.1. Overview Here, we present our learning algorithm that optimizes the likelihood function based on the joint distribution described in Eq. 3. Given X (  X  R p  X  n ) that contains n observations x [1] ,..., x [ n ]  X  R p on X , MGL aims to learn the follow-ing: -L (  X  R k  X  n ) containing the values on L in the n observa-tions l [1] ,..., l [ n ]  X  R k ; -Z (  X  { 1 ,...,k } p ) specifying the module assignment of X 1 ,...,X p into k modules; and - X  L (  X  R k  X  k ) denoting the estimate of the inverse co-variance matrix  X   X  1 L . Using the Lemma 3, we can obtain  X  X (  X  R p  X  p ) , the precision matrix estimate of X . We choose to address our learning problem by finding the joint maximum a posteriori (MAP) assignment to all of the optimization variables  X  L , Z , and  X  L . This means that we optimize the following objective function with respect to L , Z , and  X  L ( 0 ): where S L = 1 n LL | is the empirical estimate of the covari-ance matrix of L , X i denotes the i th row of the matrix X , L i denotes the i th row of the matrix L , and  X  is a positive tuning parameter that adjusts the sparsity of  X  L . Throughout this paper, we choose hard assignment of vari-ables to modules to reduce the number of parameters and to increase each module X  X  biological interpretability, where interpretability is a key MGL design feature. Soft assign-ment is a straightforward extension. We also assume a uni-form prior distribution over Z .
 We use a coordinate ascent procedure over the three sets of optimization variables  X  L , Z , and  X  L . We iteratively esti-mate each of the optimization variables until convergence. Since our objective is continuous on a compact level set, based on Thm. 4.1 in Tseng (2001), the solution sequence from MGL is defined and bounded; every coordinate group reached by the iterates is a stationary point of the MGL objective function. And we observed that the value of the objective likelihood function monotonically increases. 3.2. Iterative estimation of L , Z and  X  L To estimate L given Z and  X  L , from Eq. 5, we solve the following problem: Setting the derivative of the objective function in Eq. 6 to zero with respect to L m leads to: where M m means a set of X i that belongs to the m th mod-ule: M m = { X i | Z i = m } , and |M m | means the number of variables that belong to M m . We update L m for each m ( 1  X  m  X  k ), based on the current values of the other latent variables.
 If all elements in  X  L are equal to zero, L m would be set to the centroid of the m th module. This leads to a nice in-terpretation of the MGL learning algorithm with respect to the k -means clustering. The k -means clustering algorithm is the special case of the MGL when no network structure is assumed to exist among the latent variables (cluster cen-troids). More specifically, the MGL is a generalization of the k -means with the distance metric determined by the sparse estimate of the latent structure (  X  L ). In order to estimate Z given L and  X  L , we solve the fol-lowing: which, when  X  1 ,..., X  k = 1 , finds the module for X i that minimizes the Euclidean distance between X i and the la-tent variable. To estimate  X  L given L and Z , we solve the following op-timization problem: max where S L = 1 n LL | is the empirical estimate of the covari-ance matrix of L . Since L is given, the optimization prob-lem in Eq. 9 can be solved by the standard graphical lasso algorithm applied to L . We present our results on synthetic data (Sec. 4.1) and ovar-ian cancer gene expression data (Sec. 4.2). 4.1. Synthetic Data We compared MGL algorithm with four other methods in terms of the performance of learning networks with latent variables: 1) the standard graphical lasso (Glasso) (Fried-man et al., 2007), 2) the method proposed by Toh &amp; Hori-moto (2002) that clusters the variables and learns the net-work of cluster centroids (Toh), 3) the SIMoNe method proposed by Ambroise et al. (2009), and 4) the regularized maximum likelihood decomposition (RMLD) method pro-posed by Chandrasekaran et al. (2012).
 For Glasso, we used CRAN R package QUIC (Hsieh et al., 2011); for SIMoNe, we used CRAN R package simone ; and for RMLD, we used LogdetPPA (Wang et al., 2010), a MATLAB software for log-determinant SDP. We im-plemented MGL in C, and we used the C source code of CRAN R package huge (Zhao et al., 2012) to esti-mate the inverse covariance matrix of the latent variables (Sec. 3.2.3).
 Toh &amp; Horimoto (2002) originally uses hierarchical clus-tering for grouping the variables. In our interpretation of Toh, we used k -means algorithm for clustering the vari-ables due to k -means X  better cluster quality and scalability that we observed for high-dimensional data. Also, we used Glasso to learn the network of cluster centroids for Toh. So, throughout this paper, the method we refer by Toh is k -means followed by Glasso. In terms of module assign-ments and latent variables, k -means and Toh are identical. We used  X  1 ,..., X  k = 1 for MGL throughout Sec. 4, such that we evaluate MGL in the simplest and efficient setting. When  X  1 ,..., X  k = 1 , Eq. 8 is equal to the Euclidean dis-tance objective of the k -means clustering algorithm, which we use as the first step for Toh. We synthetically generated data based on the joint distri-bution described in Eq. 3. We first generate the inverse covariance matrix  X  L  X  1 by creating A as A ii = 0 . 5 and and setting  X  L  X  1 = A + A | . We arranged the parame-ters a and b such that the resulting matrix  X  L  X  1 is posi-tive definite. If it is still not positive definite, which hap-pened only rarely, we regenerated the matrix A . Then, we used Lemma 3 to generate  X  X based on  X  L and  X  . We generated the data for X according to x 1 ,..., x n i . i . d . N ( 0 ,  X  X ) , which results in X  X  R p  X  n .
 In order to evaluate these algorithms in varying degrees of high-dimensionality, we created three settings in terms of ( P , K , N ), where P is the number of variables, K is the number of latent variables, and N is the sample size. Setting I -(100, 10, 10) Setting II -(150, 10, 10): The difference from Setting I is the number of variables P , which increases the dimension-ality of the data by 1.5 times.
 Setting III -(150, 15, 10): We increased the number of latent variables K such that the sample size N is smaller than K .
 By setting a = 0 . 2 and b = 0 . 6 in Eq. 10, we created two different data matrices (training and test datasets) in each of Settings I, II and III. The sparsity (i.e., ratio of the number of nonzero edges to the number of all potential edges) of the resulting data matrices was around 35% . We used one of the two data matrices for training MGL and its competitors, and the other one for testing. We measure the performance of MGL and four compet-ing methods in terms of test log-likelihood using the train-ing/test datasets described above. We chose cross-data test log-likelihood as an evaluation metric because it allows direct comparisons between methods that incorporate la-tent variables and methods that do not (given that each method estimates a p -dimensional precision matrix). Test log-likelihood allows us to evaluate how well the learned models fit unseen data.
 We performed 5 -fold cross validation tests within the train-ing dataset in order to select  X  that gives the best average test log-likelihood for each method. In this cross-validation for choosing  X  , we used a wide range of the  X  values such that the solutions for the inverse covariance matrices range from a full matrix to an empty matrix.
 Fig. 3 shows the difference of the test log-likelihood be-tween each method and the SIMoNe method in Settings I, II and III. For MGL and Toh, we present the results for 3 different k values representing the number of latent vari-ables  X  K/ 2 , K and 2 K , where K means the true number of modules, assuming that the true number of modules ( K ) is unknown by the methods.
 It can be seen that MGL outperforms all of its competitors in all of the three simulation settings we considered. Al-though SIMoNe and RMLD are specific generalizations of Glasso, Ambroise et al. (2009) showed that Glasso outper-forms SIMoNe when p = n and p = 2 n , and Giraud &amp; Tsybakov (2012) argued that RMLD results are valid and meaningful only when p &lt; n , consistently with our results. We note that in Fig. 3a and Fig. 3c, the test log-likelihood was maximized when we used more latent variables than in the generating model. This is a result of the high-dimensionality of the data. But when we increased k fur-ther, test log-likelihood of MGL and Toh decreased, and for k = p , they both became equal to the one of Glasso as expected. 4.2. Cancer Gene Expression Data Ovarian cancer is the 5th leading cause of cancer death among US women and has a 5-year survival rate of 30% (Bast et al., 2009). Learning the gene regulatory network from expression data is an effective strategy to identify novel disease mechanisms (Akavia et al., 2010; TCGA, 2012). Thus, we experimented MGL on three gene ex-pression datasets containing 10404 gene expression lev-els in a total of 909 patients with ovarian serous carci-noma  X  Tothill (269 samples) (Tothill et al., 2008), TCGA (560 samples) (TCGA, 2012), and Denkert (80 samples) (Denkert et al., 2009). We mainly used Tothill for training, and TCGA and Denkert for testing.
 Given the data, MGL estimates Z , L and  X  L (see Eq. 5), which describe a gene module network characterized by the assignments of genes to modules and the latent struc-ture among the modules (Fig. 1(b)). We evaluated MGL based on: 1) how well the learned model fits unseen data (Sec. 4.2.1); 2) how significantly the inferred modules are coherent in terms of gene functions (Sec. 4.2.2); and 3) how well the inferred latent variables are predictive of survival time (Sec. 4.2.3). We also present some of the biologically interesting findings that we obtain from the MGL results (Sec. 4.2.4).
 Since this application requires learning a network with &gt; 10K variables, the methods that attempt to learn the net-work of all individual variables do not scale. Therefore, we compared MGL with only Toh that first clusters the vari-ables and then learns the network of cluster centroids. We applied k -means clustering and used the resulting clus-ters as a starting point for MGL and Toh. We compared be-tween MGL and Toh in terms of the cross-validation (CV) test log-likelihood of the estimated p -dimensional precision matrices. We performed model selection using Bayesian Information Criterion (BIC) for k -means. Cluster count ( k ) was determined as 150 by BIC. Since the data is high-dimensional, we performed 2-fold CV. We used a wide range of the  X  values such that the solutions for the mod-ule precision matrices range from a full matrix to an empty matrix. The results were averaged over 10 iterations due to non-deterministic nature of the k -means. Fig. 4 shows the test log-likelihoods of each method. MGL clearly outper-forms Toh, meaning that the learned model by MGL fits unseen data better than the one by Toh. Moreover, the standard deviation of the test log-likelihoods of the folds is smaller for MGL than Toh, indicating the robustness of MGL. In the subsequent sets of experiments (Sections 4.2.2 and 4.2.3), we use k = 150 (as determined by BIC) and  X  = . 004 (as chosen by CV). Genes assigned to the same module are likely to share simi-lar functions, and those in the connected modules are likely to be involved in similar cellular processes as well. We define a super-module (or a super-cluster ) as the set of genes in two connected modules (or clusters). We com-pared super-clusters from the learned network by Toh to super-modules from the learned network by MGL in terms of functional coherence. For each of the 4722 Curated GeneSets from the Molecular Signatures Database (Liber-zon et al., 2011), we computed the significance of the overlap between the GeneSet and super-modules (or super-clusters). We applied Bonferroni correction to the p -values and only considered the GeneSets with p &lt; 0 . 05 in ei-ther MGL or Toh. We repeated this process 50 times with different random initial points for k -means. As can be seen in Fig. 5(a), for each of 50 runs, there are a larger number of GeneSets that are more significantly overlapped with MGL super-modules than with Toh super-clusters. Thus, MGL improves the initial network of Toh, result-ing in far more shared processes between modules that are connected in the estimated network. Additionally, we ob-served that in each independent run, MGL improves the actual p -values. In Fig. 5(b), for each of 4722 functional categories, the smallest p-value achieved by MGL super-modules is plotted (y-axis) against that achieved by Toh super-clusters (x-axis). The results for all 50 runs were aggregated in Fig. 5(b). Most of the dots in Fig. 5(b) lie above the diagonal, meaning that for most of the func-tional categories, MGL super-modules achieve better en-richment than Toh super-clusters. Moreover, 6 GeneSets were observed to be enriched by MGL super-modules with p -values not only smaller than 10  X  90 , but also smaller than the best p -values for Toh super-clusters ( 10  X  20 ). These GeneSets were related to cell differentiation and increased cell growth, which are core processes relevant to cancer progression. This shows MGL X  X  power to detect core can-cer modules. We also performed the experiment explained above for learned modules without considering the latent network among them, and observed that the functional en-richment results for modules were consistent with the ones for super-modules. As can be seen in Fig. 6(a), for each of 50 runs, there are a larger number of GeneSets that are more significantly overlapped with MGL modules than with Toh clusters. An additional interesting observation is that MGL learns much sparser module networks than Toh for any attempted  X  value in a wide range. For  X  = . 004 and k = 150 , the average number of the edges was 6324 . 7 for the Toh network, and was 4626 . 6 for the MGL network. MGL removes a handful of dependencies from the initial Toh network and adds a number of new dependencies while improving the module assignments meanwhile. Sparsity of MGL networks is plausible in terms of genetic robustness. We compared the enrichment of module pairs whose de-pendencies are removed by MGL to that of module pairs between which new dependencies are added. Interestingly, the former was smaller than the latter for 49 runs out of 50 as displayed in Fig. 6(b). The latent variables could represent activity levels of path-ways relevant to the disease process and clinical outcomes. We evaluated how well the inferred latent variables learned from Tothill are predictive of survival time of ovarian can-cer patients in TCGA and Denkert datasets. After learning Toh and MGL on Tothill dataset, we trained the Cox regres-sion model using the inferred latent variables as features in Tothill dataset, and then tested the model on a separate test dataset. In the test dataset, we computed the concordance index (c-Index) which is considered a standard evaluation metric estimating the accuracy of survival prediction based on the  X  X ensored X  survival data. Fig. 7 shows the c-Index achieved for varying sparsity levels for the Cox regression model (x-axis) on the two training-test dataset pairs. It compares latent variables (modules) from MGL to clusters from Toh and individual genes. In both of the settings, the c-Index values for modules are larger than those for Toh clusters or individual genes, for a wide range of sparsity levels. The maximum c-Index for modules is also higher than those for clusters and individual genes. The c-Index values were averaged over 50 runs for MGL and Toh. A handful of modules identified by MGL are enriched for processes relevant to tumor biology, drug metabolism, and response to drug therapy. Fig. 8 shows a small por-tion of the module network learned on Tothill dataset. It is a network among immune system, cell cycle and drug metabolism processes. Edges between modules 1 through 4 indicate conditional dependencies among cytokines, in-flammation, and immune signaling, which play important roles in tumor biology (Coussens &amp; Werb, 2002). There are suggestive edges between module 4 and modules 7 and 8 (cell cycle modules), since innate immune response can stimulate cell division in neoplastic cells. (Coussens &amp; Werb, 2002). Finally, module 5 is significantly enriched for PDGF for signaling. PDGF receptor agonists, such as the popular drug Gleevec, have succeeded in treating chronic myelogenous leukemia patients (Pietras et al., 2003). We proposed the module graphical lasso , a novel high-dimensional GGM representation of conditional indepen-dencies among tightly coupled sets of variables ( modules ). The MGL algorithm is a novel high-dimensional cluster-ing algorithm that is a generalization of k -means cluster-ing, with Mahalanobis distances between variables. The full joint probability distribution function Eq. 3 defines a non-Euclidean distance metric between the latent variables L based on  X  L .
 There are several possible extensions. First, MGL could be extended to other graphical models, such as Markov random fields, with novel distance metrics and clustering properties. Second, the assumptions about relationships be-tween latent and observed variables could be relaxed. For instance, we could apply soft assignments of variables to modules, and learn sub-networks within modules. Third, we could add learning of module variances (  X  ) as an in-ference step to the MGL algorithm. And finally, we plan to apply MGL to gene expression data across multiple healthy and cancerous tissues to identify conserved and differential latent molecular networks driving tumor biology.
 The authors acknowledge funding from the following sources: American Association of Univ. Women Interna-tional Doctoral Fellowship to SC, NIH T32 HL 007312 to BAL, and Univ. Washington Royalty Research Fund to SL. Akavia, U.D., Litvin, O., Kim, J., Sanchez-Garcia, F., Kotliar, D., Causton, H.C., Pochanard, P., Mozes, E,
Garraway, L.A., and Pe X  X r, D. An integrated approach to uncover drivers of cancer. Cell , 143(6):1005 X 17, 2010. Ambroise, C., Chiquet, J., and Matias, C. Inferring sparse gaussian graphical models with latent structure. Elec-tron. J. Statist. , 3:205 X 238, 2009.
 Banerjee, O., El Ghaoui, L., and d X  X spremont, A. Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data. JMLR , 9:485 X  516, 2008.
 Bast, R.C., Hennessy, B., and Mills, G.B. The biology of ovarian cancer: new opportunities for translation. Nature Reviews Cancer , 9(6):415 X 428, 2009.
 Chandrasekaran, V., Parrilo, P.A., and Willsky, A.S. Latent variable graphical model selection via convex optimiza-tion. The Annals of Statistics , 40:1935 X 1967, 2012. Coussens, L.M. and Werb, Z. Inflammation and cancer. Nature , 420(6917):860 X 867, 2002.
 Dempster, A.P., Laird, N.M., and Rubin, D.B. Maximum likelihood from incomplete data via the em algorithm.
Journal of the Royal Statistical Society, Series B , 39(1): 1 X 38, 1977.
 Denkert, C., J., Budczies, S., Darb-Esfahani, Gyrffy, B., Sehouli, J., Knsgen, D., Zeillinger, R., Weichert, W.,
Noske, A., Buckendahl, A.C., Mller, B.M., Dietel, M., and Lage, H. A prognostic gene expression index in ovarian cancer -validation across different independent data sets. J. Pathol. , 218(2):273 X 80, 2009.
 Duchi, J., Gould, S., and Koller, D. Projected subgradient methods for learning sparse gaussians. UAI , 2008. Friedman, J., Hastie, T., and Tibshirani, R. Sparse inverse covariance estimation with the graphical lasso. Biostatis-tics , 9:432 X 441, 2007.
 Giraud, C. and Tsybakov, A.S. Discussion: Latent variable graphical model selection via convex optimization. The Annals of Statistics , 40(4):1984 X 1988, 2012.
 Guo, J. and Wang, S. Modularized gaussian graphical model. submitted to Computational Statistics and Data Analysis , 2010.
 He, Y., Kavukcuoglu, K., Qi, Y., and Park, H. Structured latent factor analysis. NIPS , 2012.
 Hsieh, Cho-Jui, Sustik, M  X  aty  X  as A., Dhillon, Inderjit S., and
Ravikumar, Pradeep K. Sparse inverse covariance matrix estimation using quadratic approximation. NIPS , 2011. Lauritzen, S.L. Graphical Models . Oxford Science Publi-cations, 1996.
 Liberzon, A., Subramanian, A., Pinchback, R., Thorvalds-dottir, H., Tamayo, P., and Mesirov, J.P. Molecular sig-natures database (msigdb) 3.0. Bioinformatics , 27(12): 1739 X 1740, 2011.
 Mardia, K.V., Kent, J., and Bibby, J.M. Multivariate Anal-ysis . Academic Press, 1979.
 Marlin, B.M. and Murphy, K. Sparse gaussian graphical models with unknown block structure. ICML , 2009. Marlin, B.M., Schmidt, M., and Murphy, K. Group sparse priors for covariance estimation. UAI , 2009.
 Pietras, K., Sjoblom, T., Rubin, K., Heldin, C.H., and Ost-man, A. Pdgf receptors as cancer drug targets. Cancer cell , 3:439 X 444, 2003.
 Schmidt, M., van den Berg, E., Friedlander, M.P., and Mur-phy, K. Optimizing costly functions with simple con-straints: A limited-memory projected quasi-newton al-gorithm. AISTATS , 2009.
 TCGA, Cancer Genome Atlas Research Network. Inte-grated genomic analyses of ovarian carcinoma. Nature , 474(7353):609 X 15, 2012.
 Tipping, M.E. and Bishop, C.M. Probabilistic principal component analysis. Journal of the Royal Statistical So-ciety, Series B , 61(3):611 X 622, 1999.
 Toh, H. and Horimoto, K. Inference of a genetic network by a combined approach of cluster analysis and graphi-cal gaussian modeling. Bioinformatics , 18(2):287 X 297, 2002.
 Tothill, R.W., Tinker, A.V., George, J., Brown, R., Fox,
S.B., Lade, S., Johnson, D.S., Trivett, M.K., Etemad-moghadam, D., Locandro, B., Traficante, N., Fereday,
S., Hung, J.A., Chiew, Y.E., Haviv, I., Group, Aus-tralian Ovarian Cancer Study, Gertig, D., DeFazio, A., and Bowtell, D.D. Novel molecular subtypes of serous and endometrioid ovarian cancer linked to clinical out-come. Clin. Cancer Res. , 14(16):5198 X 208, 2008.
 Tseng, P. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal of
Optimization Theory and Applications , 109(3):475494, 2001.
 Wang, Chengjing, Sun, Defeng, and Toh, Kim-Chuan.
Solving log-determinant optimization problems by a newton-cg primal proximal point algorithm. SIAM J. Optimization , (20):2994 X 3013, 2010.
 Yuan, M. and Lin, Y. Model selection and estimation in the
Gaussian graphical model. Biometrika , 94(10):19 X 35, 2007.
 Zhao, Tuo, Liu, Han, Roeder, Kathryn, Lafferty, John, and Wasserman, Larry. The huge package for high-dimensional undirected graph estimation in r. JMLR ,
