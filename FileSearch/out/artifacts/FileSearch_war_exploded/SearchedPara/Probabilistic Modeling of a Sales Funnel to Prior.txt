 This paper shows how to learn probabilistic classifiers that model how sales prospects proceed through stages from first awareness to final success or failure. 1 Specifically, we present two models, called DQM for direct qualification model and FFM for full funnel model, that can be used to rank initial leads based on their probability of conversion to a sales opportunity, probability of successful sale, and/or expected revenue. Training uses the large amount of histor-ical data collected by customer relationship management or mar-keting automation software. The trained models can replace tra-ditional lead scoring systems, which are hand-tuned and therefore error-prone and not probabilistic. DQM and FFM are designed to overcome the selection bias caused by available data being based on a traditional lead scoring system. Experimental results are shown on real sales data from two companies. Features in the training data include demographic and behavioral information about each lead. For both companies, both methods achieve high AUC scores. For one company, they result in a a 307% increase in number of successful sales, as well as a dramatic increase in total revenue. In addition, we describe the results of the DQM method in actual use. These results show that the method has additional benefits that include decreased time needed to qualify leads, and decreased number of calls placed to schedule a product demo. The proposed methods find high-quality leads earlier in the sales process because they focus on features that measure the fit of potential customers with the product being sold, in addition to their behavior.
Customer relationship management systems and marketing au-tomation software have become popular tools for companies with sales and marketing teams. Because these systems store a large amount of historical sales data, they provide great potential for ma-chine learning algorithms to improve the sales process. In theory, companies can use a predictive sales lead scoring or ranking model to prioritize sales and marketing efforts towards leads that are more Research performed while Brendan Duncan worked at Fliptop Inc., 594 Howard Street, Suite 400, San Francisco, CA 94105. Figure 1: Sales funnel. MQL means marketing-qualified lead, while SQL means sales-qualified lead. Image copyright  X 2015 by Fliptop, Inc. likely to result in successful sales. This paper shows how to put this theory into practice.

Figure 1 shows a typical sales funnel. The different cross sec-tions of the funnel represent different stages as a lead moves for-ward in the sales process, from the top of the funnel to the bot-tom. The decreasing diameter of the funnel represents a smaller and smaller volume of prospects reaching each successive stage.
In Figure 1, a lead is an initial prospective customer that has not been evaluated in any way. For example, when an individual vis-its a website, or exchanges contact information with the marketing team, they will begin to be tracked by marketing automation soft-ware, as a  X  X old lead. X 
As leads are tracked by marketing teams, and by marketing au-tomation software, marketing will qualify leads based on certain criteria, such as the amount of interest they show in the product (be-havioral information) and their demographic fit for purchasing the product; see Section 1.3. Leads that are qualified by marketing will be passed along to the sales team and called  X  X arketing-qualified leads. X 
Once the sales team receives leads from marketing, there is an additional qualification step. So-called teleprospectors or sales de-velopment representatives reach out to the individuals and deter-mine if they meet the minimum criteria for becoming a sales op-portunity. For example, the person must be in the market for the product or service offered by the seller, and must have the author-ity and budget to make a purchase within the sales timeline require-ments. If an individual meets these criteria, he or she is qualified, called a  X  X ales-qualified lead X  (SQL), and becomes a genuine sales opportunity that is assigned to an account executive. This process is called  X  X ead conversion. X  The majority of SQLs will be pursued by sales representatives, and will result in either a successful sale (closed-won) or a failure (closed-lost).
According to the company named SiriusDecisions, it is typical for only 6% of MQLs to convert to closed-won status. A major expense for sales teams is the time wasted on dealing with a large volume of low quality MQLs that will not become sales-qualified. In many cases, there will be more leads than can be targeted by the current sales team. Instead of hiring more teleprospectors, or arbitrarily choosing a subset of leads to pursue, sales teams should prioritize their efforts towards those leads that are most likely to reach the next stage. A predictive model can be employed for this prioritization. It can predict the probability of conversion, the prob-ability of becoming closed-won, and/or the expected revenue from a given lead. The last of these allows a sales team to estimate the amount of sales and marketing budget that should be allocated to deal with particular leads.

The most expensive parts of the funnel are the sales qualification process and the actual sales process, that is sales representatives pursuing opportunities, since these stages require the most human work either by teleprospectors or sales representatives. Therefore, a predictive model can add the most value for these two steps of the funnel. Although this paper focuses on predicting lead conver-sion, the methods proposed are also directly applicable to ranking opportunities at earlier stages of the funnel.

Other reports of data mining techniques applied to sales and mar-keting include [2] and [1], which includes a chapter on identifying prospects using a CRM. Other analyses of using predictive tech-niques to gain insights into consumer behavior and to improve mar-keting operations are given in [11] and [3].
Lead scoring is not new. Many companies currently use a man-ual lead scoring system. Such methods are generally used by the marketing team to identify MQLs. Marketing automation software facilitates the creation of such scoring systems. Although the po-tential benefit of marketing automation has been recognized for at least 25 years [9], according to SiriusDecisions only 40% of sales teams with marketing automation think that their current market-ing automation adds value. These systems still result in low quality MQLs being handed off to sales teams, making the sales qualifi-cation process expensive and time consuming. In this section we discuss these conventional scoring methods.

With a manual lead scoring system, scores are hand-tuned by experienced members of the marketing or sales team. These sys-tems typically use a  X  X corecard X  in which the presence or absence of certain positive or negative customer attributes or behaviors are assigned fixed positive or negative values. These individual values are then summed to determine a final score for the lead. For exam-ple, Table 1 shows some potential values that might be assigned for different behaviors and attributes.

One issue with conventional lead scores is that they fail to cap-ture nonlinear effects. For example, if a user visits many webinars, they will receive a high lead score, since they accumulate 5 points for each webinar. However, there may be diminishing returns for each webinar visit. The highest quality leads may visit, say, be-tween two and four webinars; attending additional webinars past Table 1: An example of a conventional lead scorecard. Values are traditionally hand-selected.
 this may not indicate a significant probability of making a purchase. It may even be the case that visiting many webinars is a negative signal. For example, it could indicate the behavior of a student, or even a competitor, who is researching the marketing functions of the company. In addition, complex interactions of features cannot be represented by scorecard models.

Another issue with conventional lead scoring is that the hand-selection of values is error-prone. In particular, hand-selection is vulnerable to bias from potentially mistaken business logic. This bias is also a problem for predictive methods, and is discussed fur-ther in Section 1.4.

A third disadvantage is that traditional lead scores are unbounded positive or negative values. They do not intuitively map to the probability of lead conversion or opportunity close. Many ma-chine learning methods are probabilistic and therefore can give valid probability scores [13].

A fourth issue is that scorecard systems are often heavily reliant on behavioral data. While such data can be a good indicator of lead interest in the product, it prevents discovering the high quality leads early; they will only be found after enough time has passed for the lead to have taken specific actions. To avoid reliance on behavioral data, one could try to gather additional static features about the customer, but each additional feature adds complexity for hand-selecting an appropriate value.
The criteria for lead qualification vary greatly by seller. Deter-mining that a lead is an MQL is usually based on simple behav-ioral and demographic rules. The demographic rules depend on the product or service being sold, and the behavioral rules depend on lead interaction with marketing materials specific to the com-pany. As discussed above, identifying MQLs is an error-prone pro-cess, and the volume of MQLs is often greater than can be han-dled by the sales team. Even if there is not a great volume of leads, teleprospecting low-quality MQLs results in wasted time, and causes tension between the sales and marketing teams. This tension is the subject of research such as [8].

Most companies identify MQLs based on fixed criteria, usually not more sophisticated than a hand-tuned scorecard. Training a ma-chine learning model could simply learn to reproduce these simple linear criteria, and therefore maintain the bias that is present in the existing, hand-tuned model. For example, if a company has fo-cused its sales efforts on Florida, a machine learning model may learn that a prospect being located outside Florida is a negative sig-nal, which may in fact not be the case. We describe below how our design reduces the effect of selection bias [12].
On the other hand, typically all SQLs are pursued by sales repre-sentatives. Therefore, there is little selection bias in the later stages of the funnel. This is a major reason why predictive models should be trained with information from later stages of the funnel. Another reason is that the ultimate goal of the sales funnel is to close a suc-cessful sale, even if the problem at hand is simply to find leads that are more likely to be qualified by sales.

In the design of the methods described below, we address several major goals: 1. A model should be probabilistic and have a meaningful in-2. A model should not simply relearn an existing conventional 3. A model should be consistent with a separate opportunity 4. The model should be able to find quality leads quickly, with-The design of the models accomplishes goals 1, 2 and 3, while goal 4 is achieved by having good static (non-behavioral) features.
The data in our experiments is provided by Fliptop. It consists of sales and marketing data extracted from Salesforce and Mar-keto systems, to which Fliptop has appended additional proprietary features. As with conventional lead scoring, the type of features present are of broadly two kinds: static (or demographic) features and behavioral (also called activity) features.

The static features are information about either the individual contact or the company for which the individual works. Fliptop ob-tains some of these features directly from fields in Salesforce, and uses individual, company, and domain names from Salesforce to append additional features. These features include company level information such as industry codes, number of employees, market value, income, and company location. They also include company hiring features, such as the number of job openings in marketing, sales, business, and other departments. Fliptop appends binary fea-tures indicating which technologies are employed by the company. Such features include whether a company uses Java, marketing au-tomation software, HTML5, etc. Finally, the contact X  X  job title is appended as a categorical feature. These static features represent the fit of the individual and the product being sold. The majority of static features are binary or categorical values, and the remainder are numerical features.

Behavioral features represent actions taken by an individual, and capture interest in the seller by the potential customer during a spe-cific period of time. These features are all numerical counts repre-senting the number of times a user has performed a specific action that is tracked by marketing automation software. Examples of ac-tions include visiting a product website, opening a marketing email, attending a webinar or trade-show event, and filling out a particular form, such as a product demo request form or an unsubscribe form.
The remainder of this section describes the data available for two sellers called Company A and Company B. This data consists of lead data pulled from CRM and marketing automation software, to which Fliptop then appended additional features. For additional information on data preprocessing, see Section 3.2.

Company A is a SaaS (software as a service) business with around 200 employees and annual revenue around $20 million. The train-ing set for Company A consists of 5925 unconverted leads, 1320 leads that became closed-lost opportunities, and 1469 leads that became closed-won opportunities. For this company, we have 243 static features about leads and their employers, along with 350 be-havioral features. The median closed price of a sale for Company A is $99, while the mean closed price is $9930 The mean is 100 times the median because the pricing varies greatly based on the product type and the number of software licenses sold. The variability in revenue makes identifying the best prospects for Company A espe-cially challenging.

Company B is a software business with over 500 employees and annual revenue around $100 million. Its training set consists of 25904 unconverted leads, 956 leads that became closed-lost oppor-tunities, and 1097 leads that became closed-won opportunities. For this company, we have 242 static features about prospects, along with 20 behavioral features. The median closed price of a sale is $29618, and the mean closed price is $46118.
The DQM (direct qualification model) models a sales funnel us-ing a single multiclass classifier. Leads receive different class la-bels depending on how far along in the sales funnel they progress. We first describe the motivation for this model, then give details on how to construct and label a training set for it, and then describe the predictive algorithm.
Predicting whether a lead will convert is a binary classification problem, and would seem to require only training a binary clas-sifier. There are several reasons why this can be undesirable for lead qualification. The first reason is that it runs the risk of merely learning to reproduce the conventional lead scoring model that the company uses. Since traditional lead scoring models are typically scorecards with linear weights, machine learning models can pre-dict lead conversion with high accuracy. However, this does not provide additional benefit to a sales team.

Another disadvantage of a two-class solution is that a lead that makes it further through the sales funnel is of higher quality than one that does not. Therefore, we would like scores to incorporate information about the chance of a lead to end up as a successful sale. If a lead conversion score incorporates closed-won probability information, it is more likely that the score will be consistent with a separate predictive model that ranks sales opportunities. That is, if lead A has a higher score than lead B, and both leads convert to opportunities A and B, we would like opportunity A to also have a higher score than opportunity B, according to an opportunity scor-ing model.

We address these potential disadvantages by classifying leads into three classes:
For the classes LOST and WON , we include leads that have closed within the last year, so that the model is up-to-date. The numbers given in Section 2 are after performing all the filtering described Figure 2: Leads are sorted by number of activities. The hori-zontal axis value is position in the sort, while the vertical axis value is the corresponding number of activities for that lead. here. For the class NoCON , we use all leads that have not yet con-verted. While this class may contain a small number of leads that will eventually convert, that does not greatly affect the performance of our method. Another option would be to treat the unconverted leads as unlabeled, and use a positive-only learning method [4].
For behavioral features, we ensure that the only the most recent year of values is included; for most leads there is much less data than this. To avoid leakers [7], we only include activities that occur before conversion, and we remove activities that indicate actions taken by the marketing team, including administrative or data man-agement actions, rather than by the actual prospective customer.
For company A, the great majority of unconverted leads have fewer than two activities, and similar features in general, meaning that a model could achieve high accuracy by simply identifying this great majority of unconverted leads. In order to investigate how our methods work well for companies with more variety in the class NoCON , we include all the leads with more than one activity, and a number L 1 of leads with fewer than two activities, such that L is roughly equal to the number of leads with exactly two activities. Although this changes the distribution of leads, and therefore also changes the calibration of probabilities, this filtering of the training set is not unlike the process of clearing unpromising leads out of a leads database. Some companies will be more aggressive with deleting leads, so our method must work with different procedures.
For both Company A and Company B, we use 75% of the data for training and 25% of the data for testing. The training and test split was determined based on the time each lead was added to the lead management system. Leads in the test set were added after those in the training set, to approximate the real-world scenario where training the model occurs before lead scoring.
We use a three-class gradient boosted tree classifier [5, 6]. The experiments in this paper use the implementation from scikit-learn [10] with the default parameters, and with so-called deviance loss in order for predictions to be probabilities.

After training the classifier on the training set, we use it to per-form prediction on a separate test set. For each lead x to be scored in the testing set, the classifier gives three probabilities that sum to one: p 1 ( x ) = P ( l = NoCON | x ) , p 2 ( x ) = P ( l = LOST | x ) , and p ( x ) = P ( l = WON | x ) , where l is a label value conditional on x .
There are several ways to map the three probabilities into a lead score s ( x ) . We consider linear combinations of p 2 and p After a linear combination is chosen, leads are sorted based on their score. As linear combinations we consider (  X , X  ) = (0 , 1) , and (  X , X  ) = (1 , 1) . These correspond respectively to maximizing closed-won probability and to maximizing lead conversion prob-ability.

Although we only consider these two weightings, other weight-ings are possible. Alternative weightings may be desirable as a tradeoff between maximizing conversion of existing leads, which the marketing team is motivated to do, and maximizing closing of opportunities, which the sales team is motivated to do. The weight-ing can be tuned to demonstrate a sufficient benefit to both teams, which is important because companies who purchase predictive lead scoring solutions often split the cost between the marketing and sales budgets.
FFM stands for  X  X ull funnel modeling. X  As a prospect advances in a sales funnel, he or she moves through several stages; see Fig-ure 1. The FFM method uses a separate probabilistic classifier for each stage of the funnel, in whatever way the funnel is defined.
For companies A and B, the transitions we are most interested in are from lead to SQL (conversion beyond MQL) and from SQL to won (successful sale). We represent these transitions using two models: Note that P ( won | x, SQL ) = P ( won | x, lead , SQL ) because SQL being true logically implies  X  X ead" being true. Additionally, we include a third model for the final stage of the funnel, namely the size of the closed deal: In these expressions, x denotes the feature values describing a given potential customer.

The probability that a lead with characteristics x will become a successful sale is The expected revenue from the lead is E ( revenue | x, lead ) = P ( won | x, lead )  X  E ( revenue | x, won ) . Knowing the expected revenue from a prospect x at the stage when x is only a lead allows a sales organization to estimate better how much budget should be invested in pursuing this individual prospect.
A full funnel model can also make predictions involving prospects currently at the SQL stage. For example, the expected revenue from customer x given that x has reached this stage is E ( revenue | x, SQL ) = P ( won | x, SQL )  X  E ( revenue | x, won ) . Separating the conversion classifier and the closed-won classifier results in another advantage of FFM. It is often the case that data about leads data and data about sales opportunities are stored in separate databases. In some cases, missing fields make it difficult to link up a lead with its corresponding opportunity, and vice versa. In such a case, an FFM can be learnt, while a DQM cannot, as we do not know whether to label converted leads as class WON or class LOST .

Filtering and preprocessing the features that describe prospects are done in the same way as described above in Section 3.2, but the training sets and labels differ. In general, FFM requires the construction of a separate training set for each transition that is modeled. Here, we have a training set of leads for modeling the probability P ( SQL | x, lead ) and a training set of opportunities for modeling P ( won | x, SQL ) and a training set of closed-won cus-tomers for modeling E ( revenue | x, won ) .

We use the same classifier learning algorithm and parameters as in the DQM model, but for binary instead of three-class classifica-tion. For regression, we also use gradient boosted trees.
For FFM, we can compute the score s ( x ) of a lead as either s ( x ) = P ( won | x, lead ) or s ( x ) = E ( revenue | x, lead ) . The for-mer definition is analogous to setting (  X , X  ) = (0 , 1) for DQM. Our experiments only consider scoring based on expected revenue of leads.
This section describes empirical results obtained from retrospec-tive analysis of historical data. The next section describes results from actual use in practice of the DQM.

The historical data used for this section is described above. Ex-periments for DQM report two scalar evaluation metrics: AUC the area under the ROC curve (AUC) for classification of non-converted versus converted leads, that is, class NoCON versus class [ WON or LOST ], and AUC 2 , the AUC for the classification of leads that become closed-won opportunities versus those that do not, that is, class [ NoCON or LOST ] versus class WON . These correspond to ranking the leads with (  X , X  ) = (1 , 1) and (  X , X  ) = (0 , 1) , respectively. For FFM we report AUC for the two separate classi-fiers which predict conversion and closed-won. Note that predict-ing conversion is the same binary classification task with the DQN and FFM approaches, so AUC 1 and AUC for FFM conversion are in principle the same. Observed differences are due to randomness.
As another evaluation of score quality, we plot lift curves for each of the experiments that show the ratio of converted or closed-won leads as we increase the proportion of selected leads. We also include lift curves that show the expected revenue as we increase the proportion of selected leads.
Applying the DQM to Company A data results in the AUC met-rics given in Table 2. In order to see how the different types of features contribute to the model, we give AUC metrics for a model Figure 3: Closed-won lift curves for DQM with (  X , X  ) = (0 , 1) . Top: Company A. Bottom: Company B. built with all the features, one built with only behavioral features, and one built with only demographic ( X  X tatic X ) features.

AUC scores for the FFM method are given in Table 3. We show the AUC measures for the two classifiers: for predicting lead to SQL conversion, and predicting SQL to closed-won. To keep the paper shorter and more readable, we do not repeat the comparison of static versus behavioral features for FFM, and all FFM experi-ments use all behavioral and static features.

The AUC 1 scores are high. This is likely because the model can easily learn the existing business rules, that is the linear scorecard for qualifying leads. The way the DQM can add value over existing methods is by using further criteria to prioritize leads, as examined in lift curves for revenue and win rate shown below. A general reason why we are able to achieve high AUCs is that the training data includes all leads tracked in the CRM. Many of these are early stage leads, which are often obviously unlikely to convert.
To visualize the performance of DQM and FFM, we use lift curves. To understand these, note that the criterion for ordering leads on the horizontal axis is in general different from the quan-tity measured on the vertical axis. In particular, the DQM orders leads based on scores s ( x ) corresponding to predicted probability Figur e 4: Conversion and closed-won lift curves for FFM. Top: Company A. Bottom: Company B. of closed-won, using (  X , X  ) = (0 , 1) . With this same ordering, we compute separate curves that track the number of successful sales and the sales revenue. Similarly, experiments with FFM rank leads based on expected revenue, but with this same ordering we again plot lift curves corresponding to number of conversions, suc-cessful sales, and the sales revenue. We use only one ordering for lift curves because this most closely matches the teleprospecting scenario, in which teleprospectors use a single ranking when con-tacting leads.
Figure 3 shows closed-won lift curves for leads prioritized using (  X , X  ) = (0 , 1) . It compares models obtained using all features, using just behavioral features, and using just static features. For both company A and B, we see that using all features performs best, while using behavioral features alone performs worst.
Figure 4 shows conversion and closed-won lift curves when we prioritize leads according to their expected revenue. For company A, the lift is significantly less in the 50% selected to 95% selected range, than it is in the 95% to 100% selected range. Figure 6 shows that the sales in this later range generate low revenue. It is often the case that bigger contracts have a lower chance of successful closing, but still a higher expected revenue overall.
 Figur e 5: Closed-won lift curves, FFM versus DQM. Top: Company A. Bottom: Company B.
 Figure 5 contrasts the closed-won lift curves for FFM and for DQM with (  X , X  ) = (0 , 1) , both trained using all behavioral and static features. The ranking of leads for DQM is based on expected closed-won probability, while the ranking for FFM is based on ex-pected revenue, so the closed-won curves are better for DQM. This is because the win probability for higher revenue deals tends to be lower, but the expected revenue is still higher for these deals. Figure 6 compares revenue for the same models. For company A, DQM performs poorly at achieving lift in revenue. This is be-cause the model focuses on closing the less risky, smaller magni-tude sales. In general, the DQM method is less appropriate if there is high variance in the sales price. Alternatively, separate DQM models could be built for separate products or price ranges.
In Figure 5, the region to the very right of the FFM curve for company A (the straight line region) indicates that this method gives lowest priority to leads that with high confidence result in a low-revenue win. The DQM method achieves high initial closed-won lift for company A. However, the corresponding revenue curve in Figure 6 shows low initial lift, because DQM prioritizes low-revenue deals for company A. These observations suggests that it is easier to predict confidently low-revenue closes for company A. As a final comparison, suppose that the sales teams of company A and B only have enough resources to contact 20% of all leads. Figure 6: Revenue lift curves, FFM versus DQM. Top: Com-pany A. Bottom: Company B.
 Table 4 shows the conversion rates, closed-won rates, and revenue if the companies prioritize leads randomly, using DQM, or using FFM.
Fliptop has deployed DQMs for several companies, and the ini-tial results have shown definite benefits for both marketing and sales. Here we describe results for two sellers, called company C and company D.
The experiments with historical data show that we can best in-crease revenue using a full funnel model. However, the main goal of company C was to improve its marketing team X  X  conversion rate, so a DQM was a good solution for it. Instead of giving explicit ranks, the DQM assigned four grades (A, B, C, D) by splitting the leads into deciles based on their ranking. These scores were then made visible in the sales team X  X  CRM system.

The teleprospectors focused on qualifying A and B leads, and during three months (October 2014 to December 2014), Fliptop collected statistics on how the DQM improved the lead conversion rate and time needed to qualify leads. The conversion rate increased from 8% to 17% during these three months. In addition, the com-pany found another unexpected benefit: the average time needed to qualify a lead went from 20 days to 7 days. This last statistic sug-gests that a three class model that takes into account the class WON produces benefits by accelerating the qualification process as well as by closing more sales.
This company provides a software tool to a variety of differ-ent types of customers. Fliptop built multiple DQM models for company D, with each model corresponding to a different cus-tomer type, or  X  X ertical. X  The company and Fliptop worked to-gether to collect statistics on the performance of DQM for the Den-tal and Lifestyle verticals. For two months (November and Decem-ber 2014), they split the sales team into two groups: a control group that did not see Fliptop grades, and a test group that did see A, B, C, and D grades. The company and Fliptop took care that the two separate teams had similar historical conversion rates and rates at which they succeeded in scheduling demos for leads.

The team that used Fliptop for the dental vertical had a 31% higher conversion rate, and the lifestyle vertical had a 37% higher conversion rate. Additionally, the average number of times the sales team called a lead before conversion was 35% worse in the control group (35 calls versus 26 telephone calls) and the average num-ber of demos scheduled per call placed was 28% better in the test group (3.6% versus 2.8%). These metrics indicate that the high quality leads uncovered by DQM are superior even in metrics that we are not optimizing over. The reduction in calls placed and in days to qualify a lead represent a significant decrease in workload for a sales team. Therefore the DQM has an even greater benefit impact, and potentially a positive impact in revenue, than the met-rics in Section 5 might indicate.
This paper introduces two methods for modeling prospective cus-tomers moving through a sales funnel, called DQM and FFM. We examine how these two models can be used to perform predictive lead scoring. In order to provide benefit to a sales team, we de-sign these models in such a way that they do not simply learn to duplicate a company X  X  existing lead qualification rules, which can be error-prone and often do not take into account enough features. Instead we focus on predicting events further along in the sales process, in particular the likelihood of a successful close and ex-pected revenue. Experiments show that applying these models to actual company data achieves high accuracy, both for classifying lead conversion and for predicting an ultimately successful future sale.

We also demonstrate that the model is predictive whether or not a lead has activity data, which means that high quality leads can be identified even before they take actions that can be tracked by the marketing team. We compare the two methods directly and con-clude that FFM is more advantageous if there is more variance in the average sales price, because it can prioritize based on expected sales price, or if lead and opportunity databases cannot be reliably linked.

Production results show that DQM is successful in deployment, and that it has additional benefits for a sales team, separate from the metrics we are directly optimizing over. These include decreased time needed to qualify leads, and decreased number of phone calls placed by sales representatives in order to qualify leads and move them forward through the sales funnel. [1] M. J. Berry and G. S. Linoff. Data mining techniques for [2] I. Bose and R. K. Mahapatra. Business data mining X  X  [3] G. Cui, M. L. Wong, and H.-K. Lui. Machine learning for [4] C. Elkan and K. Noto. Learning classifiers from only positive [5] J. H. Friedman. Greedy function approximation: A gradient [6] J. H. Friedman. Stochastic gradient boosting. Computational [7] S. Kaufman, S. Rosset, C. Perlich, and O. Stitelman. [8] P. Kotler, N. Rackham, and S. Krishnaswamy. Ending the [9] R. T. Moriarty and G. S. Swartz. Automation to boost sales [10] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, [11] M. J. Shaw, C. Subramaniam, G. W. Tan, and M. E. Welge. [12] A. Smith and C. Elkan. Making generative classifiers robust [13] B. Zadrozny and C. Elkan. Obtaining calibrated probability
