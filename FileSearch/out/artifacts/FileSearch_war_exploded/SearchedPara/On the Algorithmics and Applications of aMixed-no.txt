 In this paper the problem of Multiple Kernel Learning (MKL) is studied where the given kernels are assumed to be grouped into distinct components and each component is crucial for the learning task in hand. The focus of this paper is to study the formalism, algorithmics of a specific mixed-norm regularization based MKL formulation suited for such tasks.
 Majority of existing MKL literature have considered employing a block l 1 norm regularization lead-ing to selection of few of the given kernels [8, 1, 16, 14, 20] . Such formulations tend to select the  X  X est X  among the given kernels and consequently the decision functions tend to depend only on the selected kernel. Recently [17] extended the framework of MKL to the case where kernels are partitioned into groups and introduces a generic mixed-norm regularization based MKL formulation in order to handle groups of kernels. Again the idea is to promote sparsity leading to low number of kernels. This paper differs from [17] by assuming that every component (group of kernels) is highly crucial for success of the learning task. It is well known in optimization literature that l  X  regulariza-tions often promote combinations with equal preferences and l 1 regularizations lead to selections. The proposed MKL formulation hence employs l  X  regularization and promotes combinations of kernels at the component level. Moreover it employs l 1 regularization for promoting sparsity among kernels in each component.
 The formulation studied here is motivated by real-world learning applications like object categoriza-tion where multiple feature representations need to be employed simultaneously for achieving good generalization. Combining feature descriptors using the framework of Multiple Kernel Learning (MKL) [8] for object categorization has been a topic of interest for many recent studies [19, 13]. For e.g., in the case of flower classification feature descriptors for shape, color and texture need to be employed in order to achieve good visual discrimination as well as significant within-class variation [12]. A key finding of [12] is the following: in object categorization tasks, employing few of the feature descriptors or employing a canonical combination of them often leads to sub-optimal solutions. Hence, in the framework of MKL, employing a l 1 regularization, which is equivalent to selecting one of the given kernels, as well as employing a l 2 regularization, which is equivalent to working with a canonical combination of the given kernels, may lead to sub-optimality. This im-portant finding clearly motivates the use of l  X  norm regularization for combining kernels generated from different feature descriptors and l 1 norm regularization for selecting kernels generated from the same feature descriptor. Hence, by grouping kernels generated from the same feature descriptor together and employing the new MKL formulation, classifiers which are potentially well-suited for object categorization tasks can be built.
 Apart from the novel MKL formulation the main contribution of the paper is a highly efficient algorithm for solving it. Since the formulation is an instance of a Second Order Cone Program (SOCP), it can be solved using generic interior point algorithms. However it is impractical to work with such solvers even for moderately large number of data points and kernels. Also the generic wrapper approach proposed in [17] cannot be employed as it solves a non-convex variant of the proposed (convex) formulation. The proposed algorithm employs mirror-descent [3, 2, 9] leading to extremely scalable solutions.
 The feasibility set for the minimization problem tackled by Mirror-Descent (MD) turns out to be direct product of simplexes, which is not a standard set-up discussed in optimization literature. We employ a weighted version of the entropy function as the prox-function in the auxiliary problem solved by MD at each iteration and justify its suitability for the case of direct product of simplexes. The mirror-descent based algorithm presented here is also of independent interest to the MKL com-munity as it can solve the traditional MKL problem; namely the case when the number of groups is unity. Empirically we show that the mirror-descent based algorithm proposed here scales better than the state-of-the-art steepest descent based algorithms [14].
 The remainder of this paper is organized as follows: in section 2, details of the new MKL formulation and its dual are presented. The mirror-descent based algorithm which efficiently solves the dual is presented in section 3. This is followed by a summary of the numerical experiments carried for verifying the major claims of the paper. In particular, the empirical findings are a) the new MKL formulation is well-suited for object categorization tasks b) the MD based algorithm scales better than state-of-the-art gradient descent methods (e.g. simpleMKL ) in solving the special case where number of components (groups) of kernels is unity. This section presents the novel mixed-norm regularization based MKL formulation and its dual. In the following text we concentrate on the case of binary classification. However many of the ideas presented here apply to other learning problems too. Let the training dataset be denoted by point with label y i . Let Y denote the diagonal matrix with entries as y i . Suppose the given ker-nels are divided into n groups (components) and the j th component has n j number of kernels. Let the feature-space mapping generated from the k th kernel of the j th component be  X  jk (  X  ) and the corresponding gram-matrix of training data points be K jk 1 . We are in search of a hyperplane clas-block l  X  regularization over the model parameters w jk associated with distinct components and l 1 regularization for those associated with the same component. Intuitively, such a regularization pro-motes combinations of kernels belonging to different components and selections among kernels of the same component. Following the framework of MKL and the mixed norm regularization detailed here, the following formulation is immediate: Here,  X  i variables measure the slack in correctly classifying the i th training data point and C is the regularization parameter controlling weightage given to the mixed-norm regularization term and the total slack. MKL formulation in (1) is convex and moreover an instance of SOCP. This formulation can also be realized as a limiting case of the generic CAP formulation presented in [17] (with  X  = 1 , X  0  X   X  ). However since the motivation of that work was to perform feature selection, this limiting case was neither theoretically studied nor empirically evaluated. Moreover, the generic wrapper approach of [17] is inappropriate for solving this limiting case as that approach would solve a non-convex variant of this (convex) formulation. In the following text, a dual of (1) is derived. Let a simplex of dimensionality d be represented by  X  d . Following the strategy of [14], one can introduce variables  X  j  X   X  j 1 ... X  jn j &gt;  X   X  n j and re-write (1) as follows: This is because for any vector [ a 1 ...a n ]  X  0 , the following holds: min x ( P i a i ) 2 . Notice that the max over j and min over  X  j can be interchanged. To see that rewrite max j as min t t with constraints min  X  j  X   X  n This problem is feasible in both  X  j s and t and hence we can drop the minimization over individual constraints to obtain an equivalent problem: min  X  j  X   X  n now eliminate t by reintroducing the max j and interchange the min  X  j  X   X  n to obtain: Now one can derive the standard dual of (3) wrt. to the variables w jk ,b, X  i alone, leading to: where  X , X  are Lagrange multipliers, S m ( C )  X  { x  X  R m | 0  X  x  X  C 1 , P m i =1 x i y i = 0 } and Q jk  X  YK jk Y . The following points regarding (4) must to be noted: These facts readily justify the suitability of the particular mixed norm regularization for object cat-egorization. Indeed, in-sync with findings of [12], kernels from different feature descriptors (com-ponents) are combined using non-trivial weights (i.e. 1  X   X  each feature descriptor (component) are utilized by the model. This sparsity feature leads to bet-ter interpretability as well as computational benefits during the prediction stage. In the following section an efficient iterative algorithm for solving the dual (4) is presented. This section presents an efficient algorithm for solving the dual (4). Note that typically in object cat-egorization or other such multi-modal learning tasks, the number of feature descriptors (i.e. number of groups of kernels, n ) is low ( &lt; 10) . However the kernels constructed from each feature descriptor can be very high in number i.e., n j  X  j can be quite high. Also, it is frequent to encounter datasets with huge number of training data points, m . Hence it is desirable to derive algorithms which scale well wrt. m and n j . We assume n is small and almost O (1) . Consider the dual formulation (4). Using the minimax theorem [15], one can interchange the min over  X  j s and max over  X  to obtain: We have restated the maximum over  X  as a minimization problem by introducing a minus sign. The proposed algorithm performs alternate minimization over the variables  X  and (  X  1 ,..., X  n , X  ) . optimized wrt.  X  . This leads to the following optimization problem: where W j =  X  &gt; P n j k =1  X  jk Q jk  X  . This problem has an analytical solution given by: In the subsequent step  X  is assumed to be fixed and (5) is optimized wrt. (  X  1 ,..., X  n , X  ) . For this f (  X  ) needs to be evaluated by solving the corresponding optimization problem (refer (5) for definition of f ). Now, the per-step computational complexity of the iterative algorithm will depend (MD) based algorithm which evaluates f to sufficient accuracy in O (log [max j n j ]) O ( SVM m ) . Here O ( SVM m ) represents the computational complexity of solving an SVM with m training data points. Neglecting the log term, the overall per-step computational effort for the alternate minimization can be assumed to be O ( SVM m ) and hence nearly-independent of the number of kernels. Alternatively, one can employ the strategy of [14] and compute f using projected steepest-descent (SD) methods. The following points highlight the merits and de-merits of these two methods: The MD based algorithm for evaluating f (  X  ) i.e. solving min  X  j  X   X  n super-script  X (t) X . Similar to any gradient-based method, at each step  X  X  X  MD works with a linear rule: ciated with  X  ( x ) , a continuously differentiable strongly convex distance-generating function. s t is a regularization parameter and also determines the step-size. (7) is usually known as the auxiliary problem and needs to be solved at each step. Intuitively (7) minimizes a weighted sum of the local linear approximation of the original objective and a regularization term that penalizes solutions far from the current iterate. It is easy to show that the update rule in (7) leads to the SD technique if  X  ( x ) = 1 2 k x k 2 2 and step-size is chosen using 1-d line search. The key idea in MD is to choose the distance-generating function based on the feasibility set, which in our case is direct product of simplexes, such that (7) is very easy to solve. Note that for SD, with feasibility set as direct product of simplexes, (7) is not easy to solve especially in higher dimensions.
 We choose the distance-generating function as the following modified entropy function:  X  ( x )  X  P objective of SVM with kernel K eff . Since it is assumed that each given kernel is positive definite, the optimal of the SVM is unique and hence gradient of g  X  wrt.  X  exists [5]. Gradient of g  X  can be computed using  X  X   X  SVM with kernel as P n j =1 P update (7) has the following analytical form 4 : The following text discusses the convergence issues with MD. Let the modulus of strong convexity of  X  wrt. k X k  X  k X k 1 be  X  . Also, let the  X  -size of feasibility set be defined as follows:  X   X  our case. The convergence and its efficiency follow from this result [3, 2, 9]: Result 1 With step-sizes: s t = where k X k  X  is the dual norm of the norm wrt. which the modulus of strong convexity was computed (in our case k X k  X  = k X k  X  ) and L k X k ( h ) is Lipschitz constant of function h wrt. norm k X k (in our case k X k = k X k 1 and it can be shown that the Lipschitz constant exists for g  X  ). Substituting the particular values for our case, we obtain and T  X  the number iterations required are O (log [max j n j ]) , which is nearly-independent of the number of kernels. Since the computations in each iteration are dominated by the SVM optimization, the overall complexity of MD is (nearly) O ( SV M m ) . Note that the iterative algorithm can be improved by improving the algorithm for solving the SVM problem. The overall algorithm is summarized in algorithm 1 5 . The MKL formulation presented here exploits the special structure in the kernels and Algorithm 1 : Mirror-descent based alternate minimization algorithm Data : Labels and gram-matrices of training eg., component-id of each kernel, regularization Result : Optimal values of  X , X , X  in (4) begin end leads to non-trivial combinations of the kernels belonging to different components and selections among the kernels of the same component. Moreover the proposed iterative algorithm solves the formulation with a per-step complexity of (almost) O ( SV M m ) , which is the same as that with tra-ditional MKL formulations (which do not exploit this structure). As discussed earlier, this efficiency is an outcome of employing state-of-the-art mirror-descent techniques. The MD based algorithm presented here is of independent interest to the MKL community. This is because, in the special case where number of components is unity (i.e. n = 1 ), the proposed algorithm solves the tradi-tional MKL formulation. And clearly, owing to the merits of MD over SD discussed earlier, the new algorithm can potentially be employed to boost the performance of state-of-the-art MKL algorithms. Our empirical results confirm that the proposed algorithm (with n = 1 ) outperforms simpleMKL in terms of computational efficiency. This section presents results of experiments which empirically verify the major claims of the pa-per: a) The proposed formulation is well-suited for object categorization b) In the case n = 1 , the proposed algorithm outperforms simpleMKL wrt. computational effort. In the following, the ex-periments done on real-world object categorization datasets are summarized. The proposed MKL formulation is compared with state-of-the-art methodology for object categorization [19, 13] that employs a block l 1 regularization based MKL formulation with additional constraints for including prior information regarding weights of kernels. Since such constraints lead to independent improve-ments with all formulations, the experiments here compare the following three MKL formulations without the additional constraints: MixNorm-MKL , the ( l  X  ,l 1 ) mixed-norm based MKL formula-tion studied in this paper; L1-MKL , the block l 1 regularization based MKL formulation [14]; and L2-MKL , which is nothing but an SVM built using the canonical combination of all kernels i.e. K used to solve the formulation. The SVM problem arising at each step of mirror-descent is solved using the libsvm software 6 . L1-MKL is solved using simpleMKL 7 . L2-MKL is solved using libsvm and serves as a baseline for comparison. In all cases, the hyper-parameters of the various formulations were tuned using suitable cross-validation procedures and the accuracies reported de-note testset accuracies achieved by the respective classifiers using the tuned set of hyper-parameters. Figure 1: Plot of average gain (%) in accuracy with MixNorm-MKL on the various real-world datasets.
 The following real-world datasets were used in the experiments: Caltech-5 [6], Caltech-101 [7] and Oxford Flowers [10]. The Caltech datasets contain digital images of various objects like faces, Caltech-101 dataset has 101 categories of objects whereas Caltech-5 dataset is a subset of the Caltech-101 dataset including images of Airplanes, Car sides, Faces, Leopards and Motorbikes alone. Most categories of objects in the Caltech dataset have 50 images. The number of images per category varies from 40 to 800. In the Oxford flowers dataset there are 80 images in each flower category. In order to make the results presented here comparable to others in literature we have followed the usual practice of generating training and test sets using a fixed number of pictures from each object category and repeating the experiments with different random selections of pictures. For the Caltech-5, Caltech-101 and Oxford flowers datasets we have used 50, 15, 60 images per object category as training images and 50, 15, 20 images per object category as testing images respectively. Also, in case of Caltech-5 and Oxford flowers datasets, the accuracies reported are the testset ac-curacies averaged over 10 such randomly sampled training and test datasets. Since the Caltech-101 dataset has large number of classes and the experiments are computationally intensive (100 choose 2 classifiers need to be built in each case), the results are averaged over 3 sets of training and test datasets only. In case of the Caltech datasets, five feature descriptors 8 were employed: SIFT, Op-ponentSIFT, rgSIFT, C-SIFT, Transformed Color SIFT. Whereas in case of Oxford flowers dataset, following strategy of [11, 10], seven feature descriptors 9 were employed. Using each feature de-scriptor, nine kernels were generated by varying the width-parameter of the Gaussian kernel. The kernels can be grouped based on the feature descriptor they were generated from and the proposed formulation can be employed to construct classifiers well-suited for object categorization. For eg. in case of the Caltech datasets, n = 5 and n j = 9  X  j and in case of Oxford flowers dataset, n = 7 and n j = 9  X  j . In all cases, the 1-vs-1 methodology was employed to handle the multi-class problems. The results of the experiments are summarized in figure 1. Each plot shows the % gain in accuracy achieved by MixNorm-MKL over L1-MKL and L2-MKL for each object category. Note that for Figure 2: Scaling plots comparing scalability of mirror-descent based algorithm and simpleMKL . most object categories, the gains are positive and moreover quite high. The best results are seen in case of the Caltech-101 dataset: the peak and avg. gains over L1-MKL are 800% , 37 . 57% re-spectively and over L2-MKL are 600% , 21 . 75% respectively. The gain in terms of numbers for the other two datasets are not as high merely because the baseline accuracies were themselves high. The baseline accuracies i.e., the average accuracy achieved by L2-MKL (over all categories) were 93 . 84% , 34 . 81% and 85 . 97% for the Caltech-5, Caltech-101 and Oxford flowers datasets respec-tively. The figures clearly show that the proposed formulation outperforms state-of-the-art object categorization techniques and is hence highly-suited for such tasks. Another observation was that the average sparsity (% of kernels with zero weightages) with the methods MixNorm-MKL, L1-MKL and L2-MKL is 57% , 96% and 0% respectively. Also, it was observed that L1-MKL almost always selected kernels from one or two components (feature descriptors) only whereas MixNorm-MKL (and ofcourse L2-MKL ) selected kernels from all the components. These observations clearly show that the proposed formulation combines important kernels while eliminating redundant and noisy kernels using the information embedded in the group structure of the kernels.
 In the following, the results of experiments which compare the scalability of simpleMKL and the proposed mirror-descent based algorithm wrt. the number of kernels are presented. Note that in the special case, n = 1 , the proposed formulation is exactly same as the l 1 regularization based formulation. Hence the mirror-descent based iterative algorithm proposed here can also be employed for solving l 1 regularization based MKL. Figure 2 shows plots of the training times as a function of number of kernels with the algorithms on two binary classification problems encountered in the object categorization experiments. The plots clearly show that the proposed algorithm outperforms simpleMKL in terms of computational effort. Interestingly, it was found in our experiments that, in most cases, the major computational effort at every iteration of SimpleMKL was in computing the projection onto the feasible set! On the contrary Mirror descent allows an easily computable closed form solution for the per-step auxiliary problem. We think this is the crucial advantage of the proposed iterative algorithm over the gradient-decent based algorithms which were traditionally employed for solving the MKL formulations. This paper makes two important contributions: a) a specific mixed-norm regularization based MKL formulation which is well-suited for object categorization and multi-modal tasks b) An efficient mirror-descent based algorithm for solving the new formulation. Empirical results on real-world datasets show that the new formulation achieves far better generalization than state-of-the-art ob-state-of-the-art was as high as 37% . The mirror-descent based algorithm presented in the paper not only solves the proposed formulation efficiently but also outperforms simpleMKL in solving the traditional l 1 regularization based MKL. The speed-up was as high as 12 times in some cases. Appli-cation of proposed methodology to various other multi-modal tasks and study of improved variants of mirror-decent algorithm [4] for faster convergence are currently being explored by us. Acknowledgements CB was supported by grants from Yahoo! and IBM. [1] F. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple Kernel Learning, Conic Duality, and [2] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods [3] Aharon Ben-Tal, Tamar Margalit, and Arkadi Nemirovski. The Ordered Subsets Mirror De-[4] Aharon Ben-Tal and Arkadi Nemirovski. Non-euclidean Restricted Memory Level Method for [5] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukerjhee. Choosing multiple parameters for [6] R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-[7] R. Fergus L. Fei-Fei and P. Perona. Learning generative visual models from few training [9] Arkadi Nemirovski. Lectures on modern convex optimization (chp.5.4). Available at www2. [10] M-E. Nilsback and A. Zisserman. A visual vocabulary for flower classification. In Proceedings [11] M-E. Nilsback and A Zisserman. Automated flower classification over a large number of [12] Maria-Elena Nilsback and Andrew Zisserman. A Visual Vocabulary for Flower Classifica-[13] Maria-Elena Nilsback and Andrew Zisserman. Automated Flower Classification over a Large [14] A. Rakotomamonjy, F. Bach, S. Canu, and Y Grandvalet. SimpleMKL. Journal of Machine [15] R. T. Rockafellar. Convex Analysis . Princeton University Press, 1970. [16] Soren Sonnenburg, Gunnar Ratsch, Christin Schafer, and Bernhard Scholkopf. Large Scale [17] M. Szafranski, Y. Grandvalet, and A. Rakotomamonjy. Composite Kernel Learning. In Pro-[18] Vladimir Vapnik. Statistical Learning Theory . Wiley-Interscience, 1998. [19] M. Varma and D. Ray. Learning the Discriminative Power Invariance Trade-off. In Proceedings [20] Zenglin Xu, Rong Jin, Irwin King, and Michael R. Lyu. An Extended Level Method for
