 Taxonomy deduction is an important task to under-stand and manage information. However, building taxonomies manually for specific domains or data sources is time consuming and expensive. Tech-niques to automatically deduce a taxonomy in an unsupervised manner are thus indispensable. Au-tomatic deduction of taxonomies consist of two tasks: extracting relevant terms to represent con-cepts of the taxonomy and discovering relation-ships between concepts. For unstructured text, the extraction of relevant terms relies on information extraction methods (Etzioni et al., 2005).

The relationship extraction task can be classi-fied into two categories. Approaches in the first category use lexical-syntactic formulation to de-fine patterns, either manually (Kozareva et al., 2008) or automatically (Girju et al., 2006), and apply those patterns to mine instances of the pat-terns. Though producing accurate results, these approaches usually have low coverage for many domains and suffer from the problem of incon-sistency between terms when connecting the in-stances as chains to form a taxonomy. The second category of approaches uses clustering to discover terms and the relationships between them (Roy and Subramaniam, 2006), even if those relation-ships do not explicitly appear in the text. Though these methods tackle inconsistency by addressing taxonomy deduction globally, the relationships ex-tracted are often difficult to interpret by humans.
We show that for certain domains, the frequency with which terms appear in a corpus on their own and in conjunction with other terms induces a nat-ural taxonomy. We formally define the concept of a term-frequency-based taxonomy and show its applicability for an example application. We present an unsupervised method to generate such a taxonomy from scratch and outline how domain-specific constraints can easily be integrated into the generation process. An advantage of the new method is that it can also be used to extend an ex-isting taxonomy.

We evaluated our method on a large corpus of real-life addresses. For addresses from emerging geographies no standard postal address scheme exists and our objective was to produce a postal taxonomy that is useful in standardizing addresses (Kothari et al., 2010). Specifically, the experi-ments were designed to investigate the effective-ness of our approach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexi-cal and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), syn-onyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and orga-nize terms. The quality of extraction is often con-trolled using statistical measures (Pantel and Pen-nacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations.

Supervised methods for taxonomy induction provide training instances with global seman-tic information about concepts (Fleischman and Hovy, 2002) and use bootstrapping to induce new seeds to extract further patterns (Cimiano et al., 2005). Semi-supervised approaches start with known terms belonging to a category, construct context vectors of classified terms, and associate categories to previously unclassified terms de-pending on the similarity of their context (Tanev and Magnini, 2006). However, providing train-ing data and hand-crafted patterns can be tedious. Moreover in some domains (such as the one pre-sented in Section 3.2) it is not possible to construct a context vector or determine the replacement fit.
Unsupervised methods use clustering of word-context vectors (Lin, 1998), co-occurrence (Yang and Callan, 2008), and conjunction features (Cara-ballo, 1999) to discover implicit relationships. However, these approaches do not perform well for small corpora. Also, it is difficult to label the obtained clusters which poses challenges for eval-uation. To avoid these problems, incremental clus-tering approaches have been proposed (Yang and Callan, 2009). Recently, lexical entailment has been used where the term is assigned to a cate-gory if its occurrence in the corpus can be replaced by the lexicalization of the category (Giuliano and Gliozzo, 2008). In our method, terms are incre-mentally added to the taxonomy based on their support and context.

Association rule mining (Agrawal and Srikant, 1994) discovers interesting relations between terms, based on the frequency with which terms appear together. However, the amount of patterns generated is often huge and constructing a tax-onomy from all the patterns can be challenging. In our approach, we employ similar concepts but make taxonomy construction part of the relation-ship discovery process. For some application domains, a taxonomy is in-duced by the frequency in which terms appear in a corpus on their own and in combination with other terms. We first introduce the problem formally and then motivate it with an example application. 3.1 Definition Let C be a corpus of records r . Each record is represented as a set of terms t . Let T = { t | t  X  r  X  r  X  C } be the set of all terms of C . Let f ( t ) denote the frequency of term t , that is the number of records in C that contain t . Let F ( t, T + , T  X  ) denote the frequency of term t given a set of must-also-appear terms T + and a set of cannot-also-appear terms T  X  . F ( t, T + , T  X  ) = |{ r  X  C | t  X  r  X   X  t 0  X  T + : t 0  X  r  X   X  t 0  X  T  X  : t 0 /  X  r }| .
A term-frequency-induced taxonomy (TFIT), is an ordered tree over terms in T . For a node n in the tree, n.t is the term at n , A ( n ) the ancestors of n , and P ( n ) the predecessors of n .

A TFIT has a root node with the special term  X  and the conditional frequency  X  . The following condition is true for any other node n : That is, each node X  X  term has the highest condi-tional frequency in the context of the node X  X  an-cestors and predecessors. Only terms with a con-ditional frequency above zero are added to a TFIT.
We show in Section 4 how a TFIT taxonomy can be automatically induced from a given corpus. But before that, we show that TFITs are useful in practice and reflect a natural ordering of terms for application domains where the concept hierarchy is expressed through the frequency in which terms appear. 3.2 Example Domain: Address Data An address taxonomy is a key enabler for address standardization. Figure 1 shows part of such an ad-dress taxonomy where the root contains the most generic term and leaf-level nodes contain the most specific terms. For emerging economies building a standardized address taxonomy is a huge chal-lenge. First, new areas and with it new addresses constantly emerge. Second, there are very limited conventions for specifying an address (Faruquie et al., 2010). However, while many developing coun-tries do not have a postal taxonomy, there is often no lack of address data to learn a taxonomy from. Column 2 of Table 1 shows an example of an Indian address. Although Indian addresses tend to follow the general principal that more specific in-formation is mentioned earlier, there is no fixed or-der for different elements of an address. For exam-ple, the ZIP code of an address may be mentioned before or after the state information and, although ZIP code information is more specific than city in-formation, it is generally mentioned later in the address. Also, while ZIP codes often exist, their use by people is very limited. Instead, people tend to mention copious amounts of landmark informa-tion (see for example rows 4-6 in Table 1).
Taking all this into account, there is often not enough structure available to automatically infer a taxonomy purely based on the structural or seman-tic aspects of an address. However, for address data, the general-to-specific concept hierarchy is reflected in the frequency with which terms appear on their own and together with other terms.
It mostly holds that f ( s ) &gt; f ( d ) &gt; f ( c ) &gt; f ( z ) where s is a state name, d is a district name, c is a city name, and z is a ZIP code. How-ever, sometimes the name of a large city may be more frequent than the name of a small state. For example, in a given corpus, the term  X  X ouston X  (a populous US city) may appear more frequent than the term  X  X ermont X  (a small US state). To avoid that  X  X ouston X  is picked as a node at the first level of the taxonomy (which should only contain states), the conditional-frequency constraint intro-duced in Section 3.1 is enforced for each node in a TFIT.  X  X ouston X  X  state  X  X exas X  (which is more fre-quent) is picked before  X  X ouston X . After  X  X exas X  is picked it appears in the  X  X annot-also-appear X  X  list for all further siblings on the first level, thus giving  X  X ouston X  has a conditional frequency of zero.
We show in Section 5 that an address taxonomy can be inferred by generating a TFIT taxonomy. We describe a basic algorithm to generate a TFIT and then show extensions to adapt to different ap-plication domains. 4.1 Base Algorithm Algorithm 1 Algorithm for generating a TFIT.
To generate a TFIT taxonomy as defined in Sec-tion 3.1 we recursively pick the most frequent term given previously chosen terms. The basic algo-rithm genT F IT is sketched out in Algorithm 1. When genT F IT is called the first time, T + and T  X  are empty and both level l and width w are zero. With each call of genT F IT a new node n in the taxonomy is created with ( t, l, w ) where t is the most frequent term given T + and T  X  and l and w capture the position in the taxonomy. genT F IT is recursively called to generate a child of n and a sibling for n .

The only input parameter required by our al-gorithm is support . Instead of adding all terms with a conditional frequency above zero, we only add terms with a conditional frequency equal to or higher than support . The support parameter con-trols the precision of the resulting TFIT and also the runtime of the algorithm. Increasing support increases the precision but also lowers the recall. 4.2 Integrating Constraints Structural as well as semantic constraints can eas-ily be integrated into the TFIT generation.
We distinguish between taxonomy-level and node-level structural constraints. For example, limiting the depth of the taxonomy by introduc-ing a maxLevel constraint and checking before each recursive call if maxLevel is reached, is a taxonomy-level constraint. A node-level con-straint applies to each node and affects the way the frequency of terms is determined.

For our example application, we introduce the following node-level constraint: at each node we only count terms that appear at specific positions in records with respect to the current level of the node. Specifically, we slide (or incrementally in-crease) a window over the address records start-ing from the end. For example, when picking the term  X  X ashington X  as a state name, occurrences of  X  X ashington X  as city or street name are ignored. Using a window instead of an exact position ac-counts for positional variability. Also, to accom-modate varying amounts of landmark information we length-normalize the position of terms. That is, we divide all positions in an address by the average length of an address (which is 10 for our 40 Mil-lion addresses). Accordingly, we adjust the size of the window and use increments of 0.1 for sliding (or increasing) the window.

In addition to syntactical constraints, semantic constraints can be integrated by classifying terms for use when picking the next frequent term. In our example application, markers tend to appear much more often than any proper noun. For example, the term  X  X oad X  appears in almost all addresses, and might be picked up as the most frequent term very early in the process. Thus, it is beneficial to ignore marker terms during taxonomy generation and adding them as a post-processing step. 4.3 Handling Noise The approach we propose naturally handles noise by ignoring it, unless the noise level exceeds the support threshold. Misspelled terms are generally infrequent and will as such not become part of the taxonomy. The same applies to incorrect ad-dresses. Incomplete addresses partially contribute to the taxonomy and only cause a problem if the same information is missing too often. For ex-ample, if more than support addresses with the city  X  X ouston X  are missing the state  X  X exas X , then  X  X ouston X  may become a node at the first level and appear to be a state. Generally, such cases only ap-pear at the far right of the taxonomy. We present an evaluation of our approach for ad-dress data from an emerging economy. We imple-mented our algorithm in Java and store the records in a DB2 database. We rely on the DB2 optimizer to efficiently retrieve the next frequent term. 5.1 Dataset The results are based on 40 Million Indian ad-dresses. Each address record was given to us as a single string and was first tokenized into a se-quence of terms as shown in Table 1. In a second step, we addressed spelling variations. There is no fixed way of transliterating Indian alphabets to En-glish and most Indian proper nouns have various spellings in English. We used tools to detect syn-onyms with the same context to generate a list of rules to map terms to a standard form (Lin, 1998). For example, in Table 1  X  X aharashtra X  can also be spelled  X  X aharastra X . We also used a list of key-words to classify some terms as markers such as  X  X oad X  and  X  X agar X  shown in Table 1.

Our evaluation consists of two parts. First, we show results for constructing a TFIT from scratch. To evaluate the precision and recall we also re-trieved post office addresses from India Post 1 , cleaned them, and organized them in a tree.
Second, we use our approach to enrich the ex-isting hierarchy created from post office addresses with additional area terms. To validate the result, we also retrieved data about which area names ap-pear within a ZIP code. 2 We also verified whether Google Maps shows an area on its map. 3 5.2 Taxonomy Generation We generated a taxonomy O using all 40 million addresses. We compare the terms assigned to category levels district and taluk 4 in O with the tree P constructed from post office addresses. Each district and taluk has at least one post office. Thus P covers all districts and taluks and allows us to test coverage and precision. We compute the precision and recall for each category level CL as Table 2: Precision and recall for categorizing terms belonging to the state Maharashtra
Table 2 shows precision and recall for district and taluk for the large state Maharashtra. Recall is good for district. For taluk it is lower because a major part of the data belongs to urban areas where taluk information is missing. The precision seems to be low but it has to be noted that in almost 75% of the addresses either district or taluk informa-tion is missing or noisy. Given that, we were able to recover a significant portion of the knowledge structure.

We also examined a branch for a smaller state (Kerala). Again, both districts and taluks appear at the next level of the taxonomy. For a support of 200 there are 19 entries in O of which all but two appear in P as district or taluk. One entry is a taluk that actually belongs to Maharashtra and one entry is a name variation of a taluk in P . There were not enough addresses to get a good coverage of all districts and taluks. 5.3 Taxonomy Augmentation We used P and ran our algorithm for each branch in P to include area information. We focus our evaluation on the city Mumbai. The recall is low because many addresses do not mention a ZIP code or use an incorrect ZIP code. However, the precision is good implying that our approach works even in the presence of large amounts of noise.

Table 3 shows the results for ZIP code 400002 and 400004 for a support of 100. We get simi-lar results for other ZIP codes. For each detected area we compared whether the area is also listed on whereincity.com, part of a post office name (PO), or shown on google maps. All but four areas found are confirmed by at least one of the three external sources. Out of the unconfirmed terms F anaswadi and M arineDrive seem to be genuine area names but we could not confirm DhakurdwarRoad . The term th is due to our Table 3: Areas found for ZIP code 400002 (top) and 400004 (bottom) tokenization process. 16 correct terms out of 18 terms results in a precision of 89%.

We also ran experiments to measure the cov-erage of area detection for Mumbai without us-ing ZIP codes. Initializing our algorithm with M aharshtra and M umbai yielded over 100 ar-eas with a support of 300 and more. However, again the precision is low because quite a few of those areas are actually taluk names.

Using a large number of addresses is necessary to achieve good recall and precision. In this paper, we presented a novel approach to generate a taxonomy for data where terms ex-hibit an inherent frequency-based hierarchy. We showed that term frequency can be used to gener-ate a meaningful taxonomy from address records. The presented approach can also be used to extend an existing taxonomy which is a big advantage for emerging countries where geographical areas evolve continuously.

While we have evaluated our approach on ad-dress data, it is applicable to all data sources where the inherent hierarchical structure is encoded in the frequency with which terms appear on their own and together with other terms. Preliminary experiments on real-time analyst X  X  stock market tips 5 produced a taxonomy of (TV station, An-alyst, Affiliation) with decent precision and recall.
