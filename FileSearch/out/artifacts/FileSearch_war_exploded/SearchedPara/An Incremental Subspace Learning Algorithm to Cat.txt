 In the last decades, the emergence of the daily growth of databases on the Web classi-fication or the face recognition has revived the old problem of incremental and on-line algorithm of subspace learning [5, 14]. Principal Component Analysis (PCA) and algorithms [2, 6, 10-12, 18]. maximal variances. However, it discards the class information which is significant for classification tasks. Through Singular Value Decomposition (SVD)[9], PCA can find and the data dimension. LDA is a supervised subspace learning algorithm. It searches for the projection axes on which the data points of different classes are far from each other. Unlike PCA which encodes information in an orthogonal linear space, LDA necessarily orthogonal. once altogether. However, this type of batch algorithms no longer satisfies the appli-online sensors [13]. Thus, an incremental method is highly desired to compute adap-Analysis (IPCA) [1, 16] are designed for such a purpose and have been studied for a long time. However, IPCA ignores the valuable class label information of the training discriminant ones. The Incremental Support Vector Machine (ISVM) techniques have been developed fleetly. But most of them are approximate and require several passed through the data to reach convergence. Re searchers [3, 4] have proposed incremental stability still remain questionable. 
In this paper, we propose an incremental supervised subspace learning algorithm samples received sequentially and incrementally updates the eigenvectors of the inter-than LDA; and the experimental results on a real text dataset, Reuters Corpus Volume IPCA and Information Gain (IG). 
The rest of the paper is organized as follows. We present the incremental subspace Reuter Corpus Volume 1 in Section 3. We conclude our work in Section 4. As Introduced above, IPCA ignores the class label information and the most represen-Inter-class scatter criterion that aims to make the class centers as far as possible. 
Denote the projection matrix from original space to the low dimensional space 
WR  X   X  . In this work, we propose to incrementally maximize the Inter-class tors of the matrix b S and the column vectors of W are orthogonal to each other. summary are also presented. 2.1 The First Eigenvector Lemma-1 : if lim n Assume that a sample sequence is presented as {()} dimension of transformed data, i.e. the final subspace dimension. be written as below, 
The general eigenvector form is Au u  X  = , where u is eigenvector corresponding to the eigenvalue  X  . By replacing the matrix A with the Inter-class scatter matrix at step we can obtain an approximate iterative eigenvector computation formulation with vu  X  = : formulation: the formula can be rewritten as: where () ()(1) (1) T jj nnvivi  X  = X   X   X  , 1, 2,..., jc = . 
For initialization, we set (0) v as the first sample. Through this way, the subspace di-eigenvectors at time step n-1 and the new arrived data at time step n . 2.2 Higher-Order Eigenvectors Notice that eigenvectors are orthogonal to each other. So, it helps to generate  X  X bser-vations X  only in a complementary space for computation of the higher order eigenvec-mated th j eigenvector from the data, where 1 () () = , 1, 2,..., jk = , and {1, 2,..., } for convenience, we can only update  X  at each iteration step by 
In this way, the time-consuming orthonormalization is avoided and the orthogonal-early stages. 
Through the projection procedure at each step, we can get the eigenvectors of 
S one by one. It is much more efficient compared with the time-consuming orthonor malization process. 2.3 Convergence Proof Lemma-2 : Let () () b An S n = , b AS = , then for any large enough N Lemma-3: () vn is bounded with probability 1. tion to the Ordinary Differential Equation bellow: v almost surely. Note : 
The convergence is a classical result from the theorems of stochastic approxi-mation [7]. From the lemmas and theorem we can draw the conclusion of conver-gence [20]. 2.4 Algorithm Summary Suppose that at Step n , ()  X  ,. () i . () mn is the mean of all samples. K is the dimension of subspace to be found by our n is () j vn , 1, 2,..., jK = . 2.5 Algorithm Property Analysis The time complexity of IIS to train N input samples is sion, which is linear with each factor. Fu rthermore, when handling each input sample, past samples, such as the mean and the counts. Hence, IIS is able to handle large scale and continuous data. 
IIS is also robust since IIS focuses on the mean of each class and all samples. That fact, the robustness is determined by the criterion itself. ments, we used synthetic data that follow the normal distribution to illustrate the subspaces learned by IIS, LDA, and PCA, along with performance in a noise data. formance of our proposed algorithm on large scale text data, in the second set of experiments, we applied several dimension reduction methods on the Reuters Corpus Volume 1 (RCV1) dataset, and then compare the classification perform-ance and the time cost. Reuters Corpus Volume 1 data set [8] contains over 800,000 documents. Moreover, each document is represented by a vector with the dimension about 300,000. 3.1 Synthetic Data We generated a 2-dimension data set of 2 classes. Each class consists of 50 samples We can see that IIS can outperform PCA and yield comparable performance to LDA for classification. 
To demonstrate the robustness of IIS against noise, we generated a 3-d data set of 3 randomly provided several abnormal samp les, and then compared the correlation the more robustness against noise. The results are shown in Table 2. 
We can see from table 1 that IIS is more robust against noises than LDA. With 20  X  are very sensitive to abnormal data. 
Though the IIS has g ood performance on the synthetic data, our motivation to de-scale data sets, we conduct it on the widely used large scale text data RCV1 to intro-duce IIS. 3.2 Real World Data To compare the effectiveness and efficiency of IIS to that of other subspace learning algorithms, we constructed classification experiments on the Reuters Corpus Volume samples with the highest four topic codes (CCAT, ECAT, GCAT, and MCAT) in the  X  X opic Codes X  hierarchy, which contains 789,670 documents. Then we split them into 5 equal-sized subsets, and each time 4 of them are used as the training set and the remaining ones are left as the test set. The experimental results reported in this paper are the average of the five runs. In these experiments, we use a single computer with Pentium(R) 4 CPU 2.80GHz, 1GB of RAM, Microsoft Windows XP Professional Version, to conduct the experiments. The coding language used by us is C++ 7.0. The most widely used performance measurement for text categorization problems are combines recall and precision. We use two different F1 measurements, i.e. micro F1 and macro F1 in our paper. 3.2.1 Experiment Setup The dimensionality reduction algorithms are applies in the following manner: The dimension reduction algorithms applied are: 3.2.2 Effectiveness of IIS The classification performances are summarized in Figure 2 and Figure 3 . From these figures, we can infer that the eigenspace learned by IIS on 100 input samples is sig-input samples (&lt;20%), IIS can generate a comparable eigenspace to the one generated by IG500 in terms of classification performance. Hence, IIS is an effective subspace generated a near optimal eigenspace after just learning 10,000 samples. This indicates that in practice, the convergence speed of IIS is very fast. 
The F1 value of each class is shown in Table 3. The inferior classification perform-ance of ECAT is probably due to the uneven class distribution, as shown in Table 3. 3.2.3 Efficiency of IIS The time spent of each algorithm in dimension reduction and classification training is being near square time complexity of SVM; while the possible reas on for IG3 is that the optimization process of SVM is very slow if the margin between different classes is small which is unfortunately the case in the eigen-space of IG3. In this paper, we proposed an incremental supervised subspace learning algorithm, IIS, this assumption can not always be satisfied; therefore intra-class scatter matrix in LDA is also very important for classification tasks. In the future work, we plan to extend the matrices and we are currently exploring these extensions in theory and practice. The authors would like to thank Ning Liu for the improvement of this paper. 
