 This turorial offers a rich blend of theory and practice re-garding dimensionality reduction methods, to address the in-formation overload problem in recommender systems. This problem affects our everyday experience while searching for knowledge on a topic. Naive Collaborative Filtering can-not deal with challenging issues such as scalability, noise, and sparsity. We can deal with all the aforementioned chal-lenges by applying matrix and tensor decomposition meth-ods. These methods have been proven to be the most accu-rate (i.e., Netflix prize) and efficient for handling big data. For each method (SVD, SVD++, timeSVD++, HOSVD, CUR, etc.) we will provide a detailed theoretical mathe-matical background and a step-by-step analysis, by using an integrated toy example, which runs throughout all parts of the tutorial, helping the audience to understand clearly the differences among factorisation methods.
Representing data in lower dimensional spaces has been extensively used in many disciplines such as natural lan-guage and image processing, data mining and information retrieval. Recommender systems deal with challenging is-sues such as scalability, noise, and sparsity and thus, matrix and tensor factorization techniques appear as an interest-ing tool to be exploited [2, 4]. That is, we can deal with all the aforementioned challenges by applying matrix and tensor decomposition methods (also known as factorization methods).

The rest of this paper is organized as follows. Section 2 provides a detailed outline of the tutorial for matrix decom-position techniques. Section 3 provides a detailed outline of the tutorial for tensor decomposition. Finally, Section 4 concludes this paper.
Matrix Factorization denotes a process, where a matrix is factorized into a product of matrices. Its importance re-
To predict a user X  X  rating over a movie, we can compute the dot product of the movie X  X  and user X  X  [x,y] coordinates on the graph. In addition, Figure 1 shows where movies and users might fall on the basic two dimensions. For example, we would expect User 3 to love  X  X asablanca X  , to hate  X  X he King X  X  Speech X  , and to rate  X  X melie X  about average. Note that some movies (i.e.  X  X aken 3 X  ) and users (i.e. User 4 ) would be characterised as fairly neutral on these two dimen-sions.

In this tutorial, we provide the related work of basic ma-trix decomposition methods. The first method that we dis-cuss is known as Eigenvalue Decomposition, which decom-poses the initial matrix into a canonical form. A second method is the Non-Negative Matrix Factorization (NMF), which factorizes the initial matrix into two smaller matri-ces with the constraint that each element of the factorized matrices should be non-negative. A third method is the Probabilistic Matrix Factorization (PMF), which scales well to large datasets. PMF mathod performs well on very sparse and imbalanced datasets using spherical Gaussian priors. The last but one method is Probabilistic Latent Semantic Analysis (PLSA) which is based on a mixture decomposi-tion derived from a latent class model. The last method is CUR Decomposition, which confronts the problem of density in the factorized matrices (a problem that is faced on SVD method). Moreover, we describe Singular Value Decomposi-tion (SVD) and UV-Decomposition in details. We minimise an objective function, which captures the error between the predicted and the real value of a user X  X  rating. Moreover, an additional constraint of friendship is added in the objective function to leverage the quality of recommendations [1].
Finally, we study the performance of the described SVD and UV-decomposition algorithms, against an improved ver-sion of the original item-based CF algorithm combined with SVD.
Because of the ternary relational nature of data in many cases (e.g., Social Tagging Systems (STSs), Location-based social network (LBSNs) etc.), many recommendation algo-rithms originally designed to operate on matrices cannot be applied. Higher order problems put forward new chal-lenges and opportunities for recommender systems. For ex-ample, the ternary relation of STSs can be represented as dis et al. [3], for example, proposed to interpret the user assignment of a tag on an item, as a binary tensor where 1 indicates observed tag assignments and 0 missing values (see Figure 2):
Tensor factorization techniques can be employed in order to exploit the underlying latent semantic structure in tensor A . The basic idea is to transform the recommendation prob-lem as a third-order tensor completion problem, by trying to predict the non-observed entries in A .

In this tutorial, we provide the related work on tensor de-composition methods. The first method that is discussed is the Tucker Decomposition method (TD), which is the under-lying tensor factorization model of HOSVD. TD decomposes a tensor into a set of matrices and one small core tensor. The
