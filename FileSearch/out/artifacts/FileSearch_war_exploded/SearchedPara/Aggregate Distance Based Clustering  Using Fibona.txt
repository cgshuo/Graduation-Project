 the dataset contains numeric attributes, distance measures such as Euclidean, Manhat-tan and cosine, are effective to evaluate the similarity between objects [1],[2],[3]. However when the dataset contains categori cal (finite and unordered) attributes or a mix of numeric and categorical attributes then such distance measures may not give good clustering results [3]. Comparison of a categorical attribute in two objects would Such similarity measures are defined as overlap measure [4], and mostly suffer from the problem of clustering dissimilar instances together when the number of attributes matched is same, but attributes that are matched are different [5]. Data driven similar-ity measures are becoming a focus of research [5]. Datasets containing a mix of numerical and categorical attributes have become increasingly common in modern real-world applications. In this paper, we present a novel algorithm called as FIBCLUS (Fibonacci based Clustering) that introduces effective similarity measures for numeric, categorical and 30 R. Rawat et al. stance to a global aggregate score, this method reduces the complexity inherent in the clustering process. Moreover, due to the use of Fibonacci numbers to separate the attribute values, this method enables higher intra-cluster similarity and lower inter-cluster similarity and, in hence, better clustering. Experiments with the proposed method are conducted using a total of 9 datasets, containing a mix of numeric, cate-gorical and combinational attributes. The quality of clusters obtained is thoroughly analyzed. Empirical analysis shows that there was an average improvement of 14.6% in the purity values, 28.5% in the entropy values and about 8% in the F-score values of clusters obtained with FIBCLUS method on all the datasets in comparison to clus-tering solutions obtained using the existing methods such as k-means, x-means expected maximization and hierarchical algorithms . 
The contributions of this paper can be summarized as: 1) A novel clustering simi-larity metrics that utilises Fibonacci series to find similarities between numerical, categorical and a mix of both the data types; 2) A global score representation method for these types of attributes; and 3) Enhancing existing clustering algorithms by using FIBCLUS as a similarity metrics. When pure categorical datasets or mixed datasets consisting of both the categorical and between the instances represented by categorical attributes. A similarity measure, over-SN Attribute 1 Card No Numeric/discrete 1-13 1-13 of all cards 2 Colour Categorical 2 Red or Black 3 Category Categorical 4 Hearts, Diamonds, Spade, Clubs 32 R. Rawat et al. 34 R. Rawat et al. ately. If there are m categorical attributes in an instance which have been converted however for two successive attributes, it will always have a minimum values as .  X  search space for each 12 { , ,.., } n XXX X = from m to 1: the golden ratio .  X  Let 12 ,...} m F={F F F be the set of Fibonacci numbers chosen cor-responding to m number of attributes where each successive Fibonacci number 1 j F + maintains the golden ratio  X  with the preceding number . j F In the experiments F1 is where {5,8,13, 21} F = is the set of Fibonacci numbers. In this case, 1 5 F = is used to meric attributes, where kl m += . The score of each instance is determined separately 36 R. Rawat et al. 
Finally, the instance similarity between two instances , ij XX is evaluated based on 
Score(X Score X &lt;= This condition makes sure that the similarity calculation is input to a clustering algorithm. The objective of experiments was to evaluate the quality of clustering results obtained using the proposed FIBCLUS similarity scores, adopted in the different clustering algorithms. Standard evaluation criteria such as Entropy, Purity and F-Score were used to assess the quality. For numeric da tasets FIBCLUS was used with Expectation Minimization (EM), K means (KM) and Extended K means (XM) [6] shown as #1, #2, #3 respectively. For categorical and mix data we used direct, repeated bisec-tion and agglomerative clustering methods im plemented in gcluto [1] and shown as #1, #2 ,#3 in all results table(5,6,7). Correlation coefficient and cosine similarity were taken as similarity evaluation methods and the best results were taken. The test data-A total of 9 datasets, three of each category were used in experiments. These datasets were taken due to clear class definitions of each instance, which could be compared accurately against results of various clustering methods. Creators: Sharon Summers, School of Nursing, University of Kansas Medical Center, Kansas 
City, KS 66160,Linda Woolery, School of Nursing, University of Missouri, Columbia, MO 65211, Donor: Jerzy W. Grzymala-Busse (jerzy@cs.ukans.edu). 38 R. Rawat et al. Overall as can be seen, the performance of all clustering algorithms improves when FIBCLUS based global scores and similarity scores are used. This happens due to the separation ratio that is actively bringing similar instances together (in hence making the intra-cluster similarity larger) and separating dissimilar instances more further from each other (in hence making the inter-cluster similarity lower). Independent of the type of attributes and the clustering process used, FIBCLUS is able to produce clustering solutions of high accuracy. When each cluster is visualized for its purity in figures 3(a)-3(f), the standard EM, KM and XM methods without any space mapping derives clusters with varied purity. For datasets like Iris and Soybean EM performed exceptionally well when compared to distance based algorithm like KM and XM. However when such datasets were used with FIBCLUS in general it was found out that the distance based algorithms like KM and XM performed much better than the density based algorithm like EM. This ob-servation indicates that FIBCLUS has the ability to improve inter and intra cluster distances in any type of clustering method. For numeric datasets FIBCLUS works reasonably well. This is because the a ggregate global score computed by FIBCLUS attribute is well separated by the golden ratio, the overall score of similar instances is more similar. For some datasets like IRIS, unsupervised clustering using FIBCLUS is able to get 96% accuracy which is equal to some supervised learning methods like J48 [19]. This shows that the reduced search map obtained using the global score calcu-lated using Fibonacci numbers is able to decrease the complexity of the grouping process. For the Wine dataset, results are exceptionally well. The performance im-provement in clustering using FIBCLUS (#1, #2, #3) is nearly 50%. For the Liver Fig. 3. (d) Purity FIBCLUS(#1) Fig. 3. (e) Purity FIBCLUS(#2) Fig. 3. (f) Purity FIBCLUS( #3) dataset, results are nearly comparable, however the clustering achieved using it has better clusters which is evident from the purity and entropy measures. 
Overall the average results as percentage of various evaluation metrics are summa-rized in table 8. This paper proposed an innovative clustering method that reduces the search map for clustering to be performed. An aggregate global score is calculated for each instance using the novel idea of Fibonacci series. Similarity functions are proposed by using the aggregate global score for instances with numerical, categorical or mix attributes. The use of Fibonacci numbers is able to separate the instances effectively and, in hence, enables a higher intra-cluster similarity and a lower inter-cluster similarity. The proposed FIBCLUS method is applied on a wide variety of datasets with categorical, numerical and mix attributes. FIBCLUS is compared with the existing algorithms that are widely used to cluster numeric, categorical and mix data types. 
Empirical analysis shows that FIBCLUS is able to produce better clustering solu-tions in terms of entropy, purity, F-score etc in comparison to existing algorithms such as k-means, x-means, expected ma ximization and hierarchical algorithms. However the extra overhead in terms of time and space due to the additional step of calculating the similarity scores between instances in case of instances containing mix or categorical, is compensated by the reduced search map during the clustering process. Moreover, clustering usually is an offline process and is more affected by accuracy than such measures. This research has been funded by CRC (Co-operative Research Centre), Australia and Queensland University of Technology, Brisbane Australia under the CRC Smart Services Project 2009-10. 1. Rasmussen, M., Karypis, G.: Gcluto: An Interactive Clustering, Visualization, and Analy-2. Liao, H., Ng, M.K.: Categorical data clustering with automatic selection of cluster number. 40 R. Rawat et al. 4. Stanfill, C.: Toward memory-based reasoning. Communications of the ACM 29, 1213 X  5. Boriah, S., Chandola, V., Kumar, V.: Similarity measures for categorical data: A compara-7. San, O.M., Huynh, V.N., Nakamori, Y.: An alternative extension of the k-means algorithm 9. Le, S.Q., Ho, T.B.: An association-based dissimilarity measure for categorical data. Pattern 10. Guha, S., Rastogi, R., Shim, K.: Rock: A robust clustering algorithm for categorical attrib-11. Ganti, V., Gehrke, J., Ramakrishnan, R.: CACTUS X  X lustering categorical data using 12. Gibson, D., Kleinberg, J., Raghavan, P.: Clustering categorical data: An approach based on 16. Ichino, M., Yaguchi, H.: Generalized Minkoeski metrics for mixed feature-type data 17. Chandra, P., Weisstein, E.W.: Fibonacci Number. In: MathWorld X  X  Wolfram Web Re-18. Fredman, M.L., Tarjan, R.E.: Fibonacci heaps and their uses in improved network optimi-19. Lacueva-P X rez, F.J.: Supervised Classification Fuzzy Growing Hierarchical SOM. In: Cor-
