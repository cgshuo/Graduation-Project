 In today X  X  rapidly expanding disciplines, scientists and scholars are constantly faced with the daunting task of keeping up with knowledge in their field. In addition, the increasingly interconnected nature of real-world tasks often requires experts in one dis-cipline to rapidly learn about other areas in a short amount of time.

Cross-disciplinary research requires scientists in areas such as linguistics, biology, and sociology to learn about computational approaches and appli-cations, e.g., computational linguistics, biological modeling, social networks. Authors of journal ar-ticles and books must write accurate surveys of pre-vious work, ranging from short summaries of related research to in-depth historical notes.

Interdisciplinary review panels are often called upon to review proposals in a wide range of areas, some of which may be unfamiliar to panelists. Thus, they must learn about a new discipline  X  X n the fly X  in order to relate their own expertise to the proposal.
Our goal is to effectively serve these needs by combining two currently available technologies: (1) bibliometric lexical link mining that exploits the structure of citations and relations among citations; and (2) summarization techniques that exploit the content of the material in both the citing and cited papers.

It is generally agreed upon that manually written abstracts are good summaries of individual papers. More recently, Qazvinian and Radev (2008) argue that citation texts are useful in creating a summary of the important contributions of a research paper. The citation text of a target paper is the set of sen-tences in other technical papers that explicitly refer to it (Elkiss et al., 2008a). However, Teufel (2005) argues that using citation text directly is not suitable for document summarization.

In this paper, we compare and contrast the use-fulness of abstracts and of citation text in automati-cally generating a technical survey on a given topic from multiple research papers. The next section pro-vides the background for this work, including the primary features of a technical survey and also the types of input that are used in our study (full pa-pers, abstracts, and citation texts). Following this, we describe related work and point out the advances of our work over previous work. We then describe how citation texts are used as a new input for multi-document summarization to produce surveys of a given technical area. We apply four different sum-marization techniques to data in the ACL Anthol-ogy and evaluate our results using both automatic (ROUGE) and human-mediated (nugget-based pyra-mid) measures. We observe that, as expected, ab-stracts are useful in survey creation, but, notably, we also conclude that citation texts have crucial survey-worthy information not present in (or at least, not easily extractable from) abstracts. We further dis-cover that abstracts are author-biased and thus com-plementary to the broader perspective inherent in ci-tation texts; these differences enable the use of a range of different levels and types of information in the survey X  X he extent of which is subject to survey length restrictions (if any). Automatically creating technical surveys is sig-nificantly distinct from that of traditional multi-document summarization. Below we describe pri-mary characteristics of a technical survey and we present three types of input texts that we used for the production of surveys. 2.1 Technical Survey In the case of multi-document summarization, the goal is to produce a readable presentation of mul-tiple documents, whereas in the case of technical survey creation, the goal is to convey the key fea-tures of a particular field, basic underpinnings of the field, early and late developments, important con-tributions and findings, contradicting positions that may reverse trends or start new sub-fields, and ba-sic definitions and examples that enable rapid un-derstanding of a field by non-experts.

A prototypical example of a technical survey is that of  X  X hapter notes, X  i.e., short (50 X 500 word) descriptions of sub-areas found at the end of chap-ters of textbook, such as Jurafsky and Martin (2008). One might imagine producing such descriptions au-tomatically, then hand-editing them and refining them for use in an actual textbook.

We conducted a human analysis of these chapter notes that revealed a set of conventions, an outline of which is provided here (with example sentences in italics): 1. Introductory/opening statement: The earliest 2. Definitional follow up: X is def ined as Y. 3. Elaboration of definition (e.g., with an exam-4. Deeper elaboration, e.g., pointing out issues 5. Contrasting definition: Algorithms since then... 6. Introduction of additional specific instances / 7. References to other summaries: R provides a
The notion of text level categories or zoning of technical papers X  X elated to the survey compo-nents enumerated above X  X as been investigated pre-viously in the work of Nanba and Kan (2004b) and Teufel (2002). These earlier works focused on the analysis of scientific papers based on their rhetori-cal structure and on determining the portions of pa-pers that contain new results, comparisons to ear-lier work, etc. The work described in this paper fo-cuses on the synthesis of technical surveys based on knowledge gleaned from rhetorical structure not un-like that of the work of these earlier researchers, but perhaps guided by structural patterns along the lines of the conventions listed above.

Although our current approach to survey creation does not yet incorporate a fully pattern-based com-ponent, our ultimate objective is to apply these pat-terns to guide the creation and refinement of the final output. As a first step toward this goal, we use cita-tion texts (closest in structure to the patterns iden-tified by convention 7 above) to pick out the most important content for survey creation. 2.2 Full papers, abstracts, and citation texts Published research on a particular topic can be sum-marized from two different kinds of sources: (1) where an author describes her own work and (2) where others describe an author X  X  work (usually in relation to their own work). The author X  X  descrip-tion of her own work can be found in her paper. How others perceive her work is spread across other pa-pers that cite her work. We will refer to the set of sentences that explicitly mention a target paper Y as the citation text of Y.
Traditionally, technical survey generation has been tackled by summarizing a set of research pa-pers pertaining to the topic. However, individual re-search papers usually come with manually-created  X  X ummaries X  X  X heir abstracts. The abstract of a pa-per may have sentences that set the context, state the problem statement, mention how the problem is ap-proached, and the bottom-line results X  X ll in 200 to 500 words. Thus, using only the abstracts (instead of full papers) as input to a summarization system is worth exploring.

Whereas the abstract of a paper presents what the authors think to be the important contributions of a paper, the citation text of a paper captures what oth-ers in the field perceive as the contributions of the paper. The two perspectives are expected to have some overlap in their content, but the citation text also contains additional information not found in ab-stracts (Elkiss et al., 2008a). For example, how a particular methodology (described in one paper) was combined with another (described in a different pa-per) to overcome some of the drawbacks of each. A citation text is also an indicator of what contri-butions described in a paper were more influential over time. Another distinguishing feature of citation texts in contrast to abstracts is that a citation text tends to have a certain amount of redundant informa-tion. This is because multiple papers may describe the same contributions of a target paper. This redun-dancy can be exploited to determine the important contributions of the target paper.

Our goal is to test the hypothesis that an ef-fective technical survey will reflect information on research not only from the perspective of its au-thors but also from the perspective of others who use/commend/discredit/add to it. Before describ-ing our experiments with technical papers, abstracts, and citation texts, we first summarize relevant prior work that used these sources of information as input. Previous work has focused on the analysis of cita-tion and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002). Bradshaw (2003) used citation texts to determine the content of articles and improve the results of a search engine. Citation texts have also been used to create summaries of sin-gle scientific articles in Qazvinian and Radev (2008) and Mei and Zhai (2008). However, there is no pre-vious work that uses the text of the citations to pro-duce a multi-document survey of scientific articles. Furthermore, there is no study contrasting the qual-ity of surveys generated from citation summaries X  both automatically and manually produced X  X o sur-veys generated from other forms of input such as the abstracts or full texts of the source articles.
Nanba and Okumura (1999) discuss citation cate-gorization to support a system for writing a survey. Nanba et al. (2004a) automatically categorize cita-tion sentences into three groups using pre-defined phrase-based rules. Based on this categorization a survey generation tool is introduced in Nanba et al. (2004b). They report that co-citation (where both papers are cited by many other papers) implies sim-ilarity by showing that the textual similarity of co-cited papers is proportional to the proximity of their citations in the citing article.

Elkiss et al. (2008b) conducted several exper-iments on a set of 2 , 497 articles from the free PubMed Central (PMC) repository. 1 Results from this experiment confirmed that the cohesion of a ci-tation text of an article is consistently higher than the that of its abstract. They also concluded that ci-tation texts contain additional information are more focused than abstracts.

Nakov et al. (2004) use sentences surrounding ci-tations to create training and testing data for seman-tic analysis, synonym set creation, database cura-tion, document summarization, and information re-trieval. Kan et al. (2002) use annotated bibliogra-phies to cover certain aspects of summarization and suggest using metadata and critical document fea-tures as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization.

Siddharthan and Teufel (2007) describe a new reference task and show high human agreement as well as an improvement in the performance of ar-gumentative zoning (Teufel, 2005). In argumenta-tive zoning X  X  rhetorical classification task X  X even classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences ac-cording to their role in the author X  X  argument.
Our aim is not only to determine the utility of cita-tion texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts X  X omparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Pas-sonneau, 2004; Lin, 2004). We used four summarization systems for our survey-creation approach: Trimmer, LexRank, C-LexRank, and C-RR . Trimmer is a syntactically-motivated parse-and-trim approach. LexRank is a graph-based similarity approach. C-LexRank and C-RR use graph clustering ( X  X  X  stands for clustering). We describe each of these, in turn, below. 4.1 Trimmer Trimmer is a sentence-compression tool that extends the scope of an extractive summarization system by generating multiple alternative sentence compres-sions of the most important sentences in target doc-uments (Zajic et al., 2007). Trimmer compressions are generated by applying linguistically-motivated rules to mask syntactic components of a parse of a source sentence. The rules can be applied iteratively to compress sentences below a configurable length threshold, or can be applied in all combinations to generate the full space of compressions.

Trimmer can leverage the output of any con-stituency parser that uses the Penn Treebank con-ventions. At present, the Stanford Parser (Klein and Manning, 2003) is used. The set of compressions is ranked according to a set of features that may in-clude metadata about the source sentences, details of the compression process that generated the compres-sion, and externally calculated features of the com-pression.

Summaries are constructed from the highest scor-ing compressions, using the metadata and maximal marginal relevance (Carbonell and Goldstein, 1998) to avoid redundancy and over-representation of a single source. 4.2 LexRank We also used LexRank (Erkan and Radev, 2004), a state-of-the-art multidocument summarization sys-tem, to generate summaries. LexRank first builds a graph of all the candidate sentences. Two candidate sentences are connected with an edge if the similar-ity between them is above a threshold. We used co-sine as the similarity metric with a threshold of 0.15. Once the network is built, the system finds the most central sentences by performing a random walk on the graph.

The salience of a node is recursively defined on the salience of adjacent nodes. This is similar to the concept of prestige in social networks, where the prestige of a person is dependent on the prestige of the people he/she knows. However, since random walk may get caught in cycles or in disconnected components, we reserve a low probability to jump to random nodes instead of neighbors (a technique suggested by Langville and Meyer (2006)).

Note also that unlike the original PageRank method, the graph of sentences is undirected. This updated measure of sentence salience is called as LexRank. The sentences with the highest LexRank scores form the summary. 4.3 Cluster Summarizers: C-LexRank, C-RR Two clustering methods proposed by Qazvinian and Radev (2008) X  X -RR and C-LexRank X  X ere used to create summaries. Both create a fully connected network in which nodes are sentences and edges are cosine similarities. A cutoff value of 0.1 is applied to prune the graph and make a binary network. The largest connected component of the network is then extracted and clustered.

Both of the mentioned summarizers cluster the network similarly but use different approaches to se-lect sentences from different communities. In C-RR sentences are picked from different clusters in a round robin (RR) fashion. C-LexRank first calcu-lates LexRank within each cluster to find the most salient sentences of each community. Then it picks the most salient sentence of each cluster, and then the second most salient and so forth until the sum-mary length limit is reached. The ACL Anthology is a collection of papers from the Computational Linguistics journal, and proceed-ings of ACL conferences and workshops. It has almost 11 , 000 papers. To produce the ACL An-thology Network (AAN) , Joseph and Radev (2007) manually parsed the references before automatically compiling the network metadata, and generating ci-tation and author collaboration networks. The AAN includes all citation and collaboration data within the ACL papers, with the citation network consist-ing of 11 , 773 nodes and 38 , 765 directed edges.
Our evaluation experiments are on a set of papers in the research area of Question Answering (QA) and another set of papers on Dependency parsing (DP). The two sets of papers were compiled by se-lecting all the papers in AAN that had the words Question Answering and D ependency Parsing, re-spectively, in the title and the content. There were 10 papers in the QA set and 16 papers in the DP set. We also compiled the citation texts for the 10 QA papers and the citation texts for the 16 DP papers. We automatically generated surveys for both QA and DP from three different types of documents: (1) full papers from the QA and DP sets X  QA and DP full papers (PA) , (2) only the abstracts of the QA and DP papers X  QA and DP abstracts (AB) , and (3) the citation texts corresponding to the QA and DP papers X  QA and DP citations texts (CT) .

We generated twenty four (4x3x2) surveys, each of length 250 words, by applying Trimmer, LexRank, C-LexRank and C-RR on the three data types (citation texts, abstracts, and full papers) for both QA and DP. (Table 1 shows a fragment of one of the surveys automatically generated from QA ci-tation texts.) We created six (3x2) additional 250-word surveys by randomly choosing sentences from the citation texts, abstracts, and full papers of QA and DP. We will refer to them as random surveys . 6.1 Evaluation Our goal was to determine if citation texts do in-deed have useful information that one will want to put in a survey and if so, how much of this infor-mation is not available in the original papers and their abstracts. For this we evaluated each of the automatically generated surveys using two separate approaches: nugget-based pyramid evaluation and ROUGE (described in the two subsections below).
Two sets of gold standard data were manually cre-ated from the QA and DP citation texts and abstracts, respectively: 2 (1) We asked two impartial judges to identify important nuggets of information worth in-cluding in a survey. (2) We asked four fluent speak-ers of English to create 250-word surveys of the datasets. Then we determined how well the differ-ent automatically generated surveys perform against these gold standards. If the citation texts have only redundant information with respect to the abstracts and original papers, then the surveys of citation texts will not perform better than others. 6.1.1 Nugget-Based Pyramid Evaluation
For our first approach we used a nugget-based evaluation methodology (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Hildebrandt et al., 2004; Voorhees, 2003). We asked three impar-tial annotators (knowledgeable in NLP but not affil-iated with the project) to review the citation texts and/or abstract sets for each of the papers in the QA and DP sets and manually extract prioritized lists of 2 X 8  X  X uggets, X  or main contributions, supplied by each paper. Each nugget was assigned a weight based on the frequency with which it was listed by annotators as well as the priority it was assigned in each case. Our automatically generated surveys were then scored based on the number and weight of the nuggets that they covered. This evaluation ap-proach is similar to the one adopted by Qazvinian and Radev (2008), but adapted here for use in the multi-document case.

The annotators had two distinct tasks for the QA set, and one for the DP set: (1) extract nuggets for each of the 10 QA papers, based only on the citation texts for those papers; (2) extract nuggets for each of the 10 QA papers, based only on the abstracts of those papers; and (3) extract nuggets for each of the 16 DP papers, based only on the citation texts for those papers. 3
We obtained a weight for each nugget by revers-ing its priority out of 8 (e.g., a nugget listed with priority 1 was assigned a weight of 8) and summing the weights over each listing of that nugget. 4
To evaluate a given survey, we counted the num-ber and weight of nuggets that it covered. Nuggets were detected via the combined use of annotator-provided regular expressions and careful human re-view. Recall was calculated by dividing the com-bined weight of covered nuggets by the combined weight of all nuggets in the nugget set. Precision was calculated by dividing the number of distinct nuggets covered in a survey by the number of sen-tences constituting that survey, with a cap of 1. F-measure, the weighted harmonic mean of precision and recall, was calculated with a beta value of 3 in order to assign the greatest weight to recall. Recall is favored because it rewards surveys that include highly weighted (important) facts, rather than just a great number of facts.

Table 2 gives the F-measure values of the 250-word surveys manually generated by humans. The surveys were evaluated using the nuggets drawn from the QA citation texts, QA abstracts, and DP ci-tation texts. The average of their scores (listed in the rightmost column) may be considered a good score to aim for by the automatic summarization methods.
Table 3 gives the F-measure values of the surveys generated by the four automatic summarizers, evalu-ated using nuggets drawn from the QA citation texts, QA abstracts, and DP citation texts. The table also includes results for the baseline random summaries.
When we used the nuggets from the abstracts set for evaluation, the surveys created from ab-stracts scored higher than the corresponding surveys created from citation texts and papers. Further, the best surveys generated from citation texts outscored the best surveys generated from papers. When we used the nuggets from citation sets for evaluation, the best automatic surveys generated from citation texts outperform those generated from abstracts and full papers. All these pyramid results demonstrate that citation texts can contain useful information that is not available in the abstracts or the original papers, and that abstracts can contain useful information that is not available in the citation texts or full papers.
Among the various automatic summarizers, Trim-mer performed best at this task, in two cases ex-ceeding the average human performance. Note also that the random summarizer outscored the automatic summarizers in cases where the nuggets were taken from a source different from that used to generate the survey. However, one or two summarizers still tended to do well. This indicates a difficulty in ex-tracting the overlapping survey-worthy information across the two sources. 6.1.2 ROUGE evaluation
Table 4 presents ROUGE scores (Lin, 2004) of each of human-generated 250-word surveys against each other. The average (last column) is what the au-tomatic surveys can aim for. We then evaluated each of the random surveys and those generated by the four summarization systems against the references. Table 5 lists ROUGE scores of surveys when the manually created 250-word survey of the QA cita-tion texts, survey of the QA abstracts, and the survey of the DP citation texts, were used as gold standard.
When we use manually created citation text surveys as reference, then the surveys gener-ated from citation texts obtained significantly bet-ter ROUGE scores than the surveys generated from abstracts and full papers ( p &lt; 0 . 05) [R ESULT 1]. This shows that crucial survey-worthy information present in citation texts is not available, or hard to extract, from abstracts and papers alone. Further, the surveys generated from abstracts performed sig-nificantly better than those generated from the full papers ( p &lt; 0 . 05) [R ESULT 2]. This shows that ab-stracts and citation texts are generally denser in sur-vey worthy information than full papers.

When we use manually created abstract sur-veys as reference, then the surveys generated from abstracts obtained significantly better ROUGE scores than the surveys generated from citation texts and full papers ( p &lt; 0 . 05) [R ESULT 3]. Further, and more importantly, the surveys generated from cita-tion texts performed significantly better than those generated from the full papers ( p &lt; 0 . 05) [R ESULT 4]. Again, this shows that abstracts and citation texts are richer in survey-worthy information. These re-sults also show that abstracts of papers and citation texts have some overlapping information (R ESULT 2 and R ESULT 4), but they also have a signifi-cant amount of unique survey-worthy information (R
Among the automatic summarizers, C-LexRank and LexRank perform best. This is unlike the results found through the nugget-evaluation method, where Trimmer performed best. This suggests that Trim-mer is better at identifying more useful nuggets of information, but C-LexRank and LexRank are bet-ter at producing unigrams and bigrams expected in a survey. To some extent this may be due to the fact that Trimmer uses smaller (trimmed) fragments of source sentences in its summaries. In this paper, we investigated the usefulness of di-rectly summarizing citation texts (sentences that cite other papers) in the automatic creation of technical surveys. We generated surveys of a set of Ques-tion Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation texts us-ing four state-of-the-art summarization systems (C-LexRank, C-RR, LexRank, and Trimmer). We then used two separate approaches, nugget-based pyra-mid and ROUGE, to evaluate the surveys. The re-sults from both approaches and all four summa-rization systems show that both citation texts and abstracts have unique survey-worthy information. These results also demonstrate that, unlike single document summarization (where citing sentences have been suggested to be inappropriate (Teufel et al., 2006)), multidocument summarization X  especially technical survey creation X  X enefits con-siderably from citation texts.

We next plan to generate surveys using both cita-tion texts and abstracts together as input. Given the overlapping content of abstracts and citation texts, discovered in the current study, it is clear that re-dundancy detection will be an integral component of this future work. Creating readily consumable sur-veys is a hard task, especially when using only raw text and simple summarization techniques. There-fore we intend to combine these summarization and bibliometric techniques with suitable visualization methods towards the creation of iterative technical survey tools X  X ystems that present surveys and bib-liometric links in a visually convenient manner and which incorporate user feedback to produce even better surveys.
 This work was supported, in part, by the National Science Foundation under Grant No. IIS-0705832 (iOPENER: Information Organization for PENning Expositions on Research) and Grant No. 0534323 (Collaborative Research: BlogoCenter -Infrastruc-ture for Collecting, Mining and Accessing Blogs), in part, by the Human Language Technology Cen-ter of Excellence, and in part, by the Center for Ad-vanced Study of Language (CASL). Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
