 these bounds in model selection.
 tighter risk bounds, result in averaged statements over the expected true error. is defined independently of the training data).
 of Given a training set S = { z A is described entirely by two complementary sources of information : a subset z obtain a classifier from the compression set z defined by a vector i of indices i def = ( i i whereas z any vector i  X  X  where I denotes the set of the 2 m possible realizations of i . by a compression set z compression set z that can be supplied to R with the compression set z examples, the empirical risk R to the sample compression setting. Pr
S  X  X  m [ S  X  X  ( h, X  )]  X   X  uous w.r.t. P such that  X  = d  X  prior). Then Occam X  X  Hammer [Blanchard and Fleuret, 2007] s tates that: above, define the level function where  X  ( x ) = R x density  X  variables, it holds that where Q is the distribution on H such that d Q S while P is the data-independent prior on H . The subscript S in  X  on the classifier space without reference to the data. consider prior distribution P with probability density P ( z such that: with P compression sets.
 Let P in algorithms that output a posterior Q  X  P sity Q ( z choosing a classifier ( z defined as R ( G risk of the specific classifier ( z the Occam X  X  principle of parsimony.
 Definition 2 Given a distribution Q  X  X  classifier ( z d with a PAC-Bayes bound. 4.1 A PAC-Bayes Bound for randomized SC classifier quantities (for a given i , and hence z Definition 3 Let S  X  D m with D a distribution on X  X  Y , and ( z fewer errors on z  X  and by B That is, for a classifier f : for all R left hand side. Consequently, in the case of sample compress ed classifier,  X  ( z ( X  X Y ) m : Bounding this by  X  yields: by a compression set i and an associated message string  X  . Let  X  hammer defined as: ln(min(  X  X  ( h S )  X  (  X  z m ( i , X  )  X  1 ) , 1)  X  1 ) = ln + where ln regard to the data. Substituting for  X  P posterior distribution Q , then, for  X   X  (0 , 1] and k &gt; 0 , we have: where R examples not in the compression set and R ( z the specific compression set utilized by this classifier. 4.2 A Binomial Tail Inversion Bound for randomized SC classi fier inversion over the distribution of errors. The binomial tail inversion Bin k at most k errors out of m examples: where From this definition, it follows that Bin ( R on a test set of m examples (test set bound): compression set (consistent compression set assumption). Now, let  X  Now, we replace  X  P , X  = 1 ): Hence, we have proved the following: posterior distribution Q , then, for  X   X  (0 , 1] and k &gt; 0 , we have: et al., 2005, Lemma 1]: joint draw of S  X  X  m and i  X  Q
R ( z i , X  )  X  1  X  exp utilize the rescaled posterior over the space of sample comp ressed classifiers. The PAC-Bayes bound of Theorem 4 basically states that where Consequently, Now, bounding the argument of expectancy above using the Mar kov inequality, we get: Now, discretizing the argument over (  X  Taking the union bound over  X  the following holds with probability 1  X   X  : and hence: where: We wish to bound, for the Gibbs classifier, E Now, we have: related by we have: where R Q respectively.
 Hence, with Q  X  = Q and f ( z Further, E over compression set and message strings, for  X   X  (0 , 1] , we have: KL-divergence term, ln( m with the Occam X  X  Hammer criterion. this compares favorably to the existing result of Laviolett e and Marchand [2007]. Acknowledgments The author would like to thank John Langford for interesting discussions. pages 112 X 126, 2007.
 dimension. Machine Learning , 21(3):269 X 304, 1995.
 ing Research , 3:273 X 306, 2005.
 2007.
 University of California Santa Cruz, Santa Cruz, CA, 1986.
 Reasearch , 3:723 X 746, 2002.

