 1. Introduction
It is natural for humans to collaborate on difficult tasks ( Denning &amp; Yaholkovsky, 2008 ), including exploring and retriev-ing information ( Morris, 2008 ). Twidale, Nichols, and Paice (1997) argued that introducing support for collaboration into information retrieval systems would help searchers to learn and use the systems more effectively. They further claimed that a truly user-centered system must acknowledge and support collaborative interactions between people, and showed that searchers often desire to collaborate on search tasks. Based on their extensive study of patent office workers, Hansen and Jarvelin (2005) also concluded that the assumption that information retrieval performance is purely individual needs to be reconsidered. While this issue of collaboration has attained considerable attention lately, the mechanics of how to medi-ate such collaboration remain largely unexplored. Joho, Hannah, and Jose (2009) identified three conceptual approaches to facilitate such collaboration in information searching tasks: models, techniques, and interfaces. In this paper, we propose a new model for addressing role-based IR in collaboration. The model uses existing techniques for merging and splitting re-sults in a unique way.
 We propose to address a core issue of facilitating collaboration between a pair of searchers via algorithmic mediation. More specifically, we describe the design of a system to mediate explicit, synchronous collaboration ( Golovchinsky et al., 2009) between multiple searchers. Mediated collaboration is typically not necessary for simple precision-oriented searches such as fact-finding or for known-item search. However, for tasks that are more recall-oriented, more complex and explor-atory in nature, and for situations in which the information need is not satisfied by a single document, there is value in hav-ing a system that explicitly supports multiple, task-aligned searchers. In addition to this, we wish to incorporate two major elements of algorithmically mediated collaboration as expressed by Foley (2008) : sharing of knowledge, and division of la-bor. The former is achieved by supporting two different roles of the participants, and the latter is done by splitting the re-turned results of a search. search process. Studying the effects of single-round interaction should aid future interactive system design. Focusing on a single round also allows us to specifically study the effects of our algorithmic decisions; future work will address how to incorporate these techniques into an iterative search process.
 in terms of supporting these roles ( Pickens, Golovchinsky, Shah, Qvarfordt, &amp; Back, 2008). In this paper, we define two com-plementary roles  X  Gatherer and Surveyor  X  for our team. The goal of the Gatherer is to scan results of the joint search activ-ity of team members to discover the most immediately relevant information. The goal of the Surveyor is to browse a wider diversity of information to get a better understanding of the nature of the collection being searched, to understand where the current queries might be failing, and to identify potential avenues of exploration. While each person contributes to the que-rying process by expressing their individual understanding of the information need through their queries, the results they see are affected by their roles. The  X  X  X lack box X  facilitates this process by combining the ranked lists from the users X  query formulations, and then by splitting the combined list based on role criteria. The goal of results combination is to improve overall precision by aligning or combining multiple users X  query formulations. The objective of splitting, on the other hand, is to preserve that improvement while simultaneously encouraging exploration and information diversity to avoid getting trapped in a rut.
 rithm. They are merely acting as each other X  X  complements to facilitate algorithmic mediation for role-based collaboration.
Our primary evaluation, thus, will be based on comparing the inputs and outputs of this  X  X  X lack box. X  In addition, we will also analyze merging and splitting phases, testing various retrieval parameters with a set of methods. The main contribution of our work is the demonstration of an effective way of using these two complementary processes to create a system that dis-tributes search results to team members in a role-based collaborative IR session. 2. Background information retrieval. We will first review some of these approaches to put our work in the context. In an IR system, there are various objects and processes that could be combined in  X  X  X ollaborative X  fashion. For instance, Fu, Kelly, and Shah (2007) showed how different queries from a set of users for the same information goal can be combined, calling it  X  X  X ollaborative queries X , for better retrieval performance. Rather than simply combining queries, we first combine and then distribute the rank-lists produced by executing the individual queries.
 categories: content-based, collaborative, and hybrid recommendation approaches (combination of the above two) ( Adom-avicius &amp; Tuzhilin, 2005 ). Content-based recommender systems rely on similarity of content selected by earlier users to make new recommendations. Collaborative recommender systems user other people X  X  opinions or behaviors to inform a new person X  X  search process. Such systems are also called collaborative filtering systems. The majority of collaborative systems are driven by the motivation that prior people X  X  behavior can inform a new user ( Linden, Smith, &amp; York, 2003 ), and that a group of users can tackle a complex problem better than any one of them individually ( Denning, 2007 ). Smyth, Balfe, Briggs, Coyle, and Freyne (2003) argued that one way of making it possible to connect people to information that is difficult to find is to incorporate collaboration in the search phase of an information seeking process.
They showed how collaborative search can act as a front-end for existing search engines and re-rank results based on the learned preferences of a community of users. They demonstrated this concept by implementing I-Spy system ( Freyne,
Smyth, Coyle, Balfe, &amp; Briggs, 2004 ). I-Spy, however, acts more like collaborative filtering than like synchronous collabo-rative searching, which is the focus of our work. HeyStaks 1 supports multiple kinds of interaction, along the lines of Search-
Together ( Morris &amp; Horvitz, 2007 ). Both systems allow users to contribute documents independently, but the system synchronizes the data so that the latest documents are available to all people using a given Stak. While HeyStak may re-or-der search results by promoting previously-selected documents, it does not distinguish between different roles that search-ers may have. It may be thought of as a light-weight recommendation overlay on top of the regular search engine ranking algorithm.
 mon interest user group to collaborate in searching the Web. AntWorld harnesses the expertise of members of a common interest group through their evaluation of documents encountered while searching. It uses judgments about documents made by some people to guide others to pages they may find useful. Once again, this serves more as a collaborative filtering system than a synchronous collaborative search system. While our work incorporates multiple users sharing information and affecting each other by their actions, such sharing and influence are mediated by the system in real time, so that all team members benefit from each other X  X  current (and past) activities. Also, in contrast to AntWorld, the information seen by dif-ferent users of the system is intentionally kept different, allowing the collaborators work on different aspects of the same task.
It is important to note here that in many of these recommmendation-driven applications, a person receiving the recom-mendations may not know the other users in the network personally, and may not share the same information need. Thus, a user is not intentionally and interactively engaged in a true collaboration with other users; he is merely getting filtered con-tent based on other users X  actions on similar information. There have been some applications that exploit more tightly con-nected social networks instead of the entire network to filter and recommend information. For instance, Kautz, Selman, and Shah (1997) described ReferralWeb , which was based on providing recommendations via chains of named entities instead of anonymous users in the network. Even in this case, however, people searching for information could not be certain that oth-ers used the system in sufficiently similar ways for their prior experiences and actions to be useful.
 A common way of providing a collaborative IR solution is to extend a traditional IR model to incorporate multiple people. For instance, Kuhlthau (1991) described the Information Search Process (ISP) model to explain information seeking from a searcher X  X  perspective. She later extended this model to bridge the gap between information seeking and retrieval ( Kuhlthau, 2005 ). This model inspired Hyldeg X rd (2006) to study information seeking and retrieval process in a group-based educa-tional setting. Our work differs in that we are not simply extending a typical IR model to include multiple searchers, but in-stead, we propose a new technique that can leverage the diversity of people X  X  skills and experience when working toward a shared information need.
 A typical use of a collaborative searching or browsing facility is to let a group of people share information. For instance, Root (1988) introduced the idea of social browsing to support distributed cooperative work with unplanned and informal social interaction. This idea was carried over by Donath and Robertson (1994) several years later as The Social Web that al-lows a person to know that others were currently viewing the same web page and communicate with those people. Witt-enburg, Das, Hill, and Stead (1995) came up with the notion of Group Asynchronous Browsing (GAB) to provide tools for people to leverage the information hunting and gathering activities of other people or group of people on the Web. Cabri, Leonardi, and Zambonelli (1999) showed how cooperative Web browsing can be supported using a proxy, and Gerosa, Gior-dani, Ronchetti, Soller, and Stevens (2004) presented a similar idea with the application of e-learning. Systems such as Group-Web ( Greenberg &amp; Roseman, 1996 ), GroupScape ( Graham, 1997 ), and Co-Vitesse ( Laurillau &amp; Nigay, 2002 ) are meant to provide a co-browsing interface to allow users in a collaborative environment to explore the Web together. Keller, Wolfe, Chen, Rabinowitz, and Mathe (1997) described WebTagger , a social bookmarking service similar to del.icio.us. 2
The work presented in this paper differs from other systems listed above in that we provide algorithmic mediation to an explicitly established collaboration among users, rather than only shared awareness. In the literature we find several ap-proaches that entertain such explicit collaboration. For instance, Twidale and Nichols (1996) described the Ariadne system that allowed a user (patron) to collaborate with an information expert (reference librarian) remotely and synchronously. However, the distribution of the information was handled manually, by the users, without much help from the system.
SearchTogether ( Morris &amp; Horvitz, 2007 ) and Shah, Marchionini, and Kelly (2009) allow people to contribute queries and share search results. They are good examples of UI-level mediation for explicit collaborative search ( Golovchinsky et al., xxxx ). Such systems have been used as platforms to investigate various aspects of UI-level mediation, including awareness and sense-making ( Paul &amp; Morris, 2009 ). SearchTogether has also recently introduced algorithmic multi-user results alloca-tion ( X  X  X mart splitting X ) ( Morris, Teevan, &amp; Bush, 2008 ). That work uses personal information such as documents and brows-ing history to cluster search results from single queries for splitting among users; personalization meets division of labor. Searchers get to evaluate the documents they personally are most likely to recognize as useful, as extracted from a single expression of an information need. Our approach differs from this in several ways. We begin by pooling independently-en-tered queries to create a richer set of results. A more important difference is at the second, splitting stage. Rather than split-ting of results based on personal traits of the searchers, we split results based on role-oriented sub-tasks or goals. The focus on (and optimization for) the role, rather than for the person, is an important distinction for future system and algorithm design. Other work on explicit collaborative search includes desktop systems such as F X schl X r-DiamondTouch ( Smeaton, Lee, Foley, McGivney, &amp; Gurrin, 2006 ) which implement symmetric roles: users contribute search terms to a single query, every user sees the same result set at the same time, and each can grab results from that query for analysis.

We are interested in exploring mechanisms for distributing search results to people based on their roles. We are aware of very few systems that experiment with asymmetric roles in information seeking. The work described here builds on our ear-lier work ( Pickens et al., 2008 ) that showed how one user in a collaboration could specialize in issuing queries and the other could specialize in evaluating the results. The work reported here is different in the way we define the roles, and in how we assign documents to each role. Instead of task-based roles, here we have information content-based roles. In other words, instead of letting each person be responsible for a certain kind of task (issuing query vs. looking at the results), we have de-fined the roles such that both people do the same tasks, but are shown information that is based on their roles (Gatherer or Surveyor). This instance of division of labor ( Foley, 2008 ) represents situations in which people have fundamentally similar skill sets, but may benefit from adopting different approaches to exploring the data. For instance, a one searcher (in the Sur-veyor role) may explore the information landscape broadly, looking for serendipitous discovery, while another (in the Gath-erer role) pursues specific highly relevant information from a particular aspect. As the session progresses, they can trade off roles. In this manner, roles offer collaborators a method for solving a potentially tedious task that otherwise would not be possible. different roles is shown in Fig. 1 . The system combines results of queries from two searchers and then redistributes them according to their roles. While there are several ways of defining these roles, as well as many possible algorithms for imple-menting them, in this work we examine the Gatherer , who focuses on quickly finding as much relevant information as pos-sible, and the Surveyor , who will delve into a more diverse set of documents. 3. Role-based algorithmic collaboration mediation three fundamental steps. 1. Two searchers are working together on an explicitly-shared information need. One searcher enters a query, and begins to 2. The system evaluates the second searcher X  X  query and displays it. The system also fuses the results of the two queries to 3. The system then splits the fused results into two sublists, one for each person. These partitioned results are created based second query was available for fusion; the second searcher gets the other list. This assignment strategy should make recom-mendations in line with what has already been seen by the first searcher, making it easier to make sense of the additional suggestions, particularly if there is little overlap among query results. The suggestion process can be iterated as the system receives additional queries.
 will evaluate various aspects of the approach. We note that the main contribution of this paper is not in any particular merg-ing or splitting algorithm, but in the overall conceptual scenario in which these algorithms are being used, i.e., to support intentional, synchronous collaboration of multiple users on a shared information need. 3.1. Merging
We use CombSUM fusion ( Aslam &amp; Montague, 2001 ) to merge the two users X  queries. CombSUM first normalizes the raw scores for each document returned by a query, sums these normalized scores, and then re-ranks documents by the total sum. Documents originally ranked highly in multiple lists will receive a higher score than documents ranked highly in only a sin-gle list, while documents ranked lower, but found in multiple lists can end up ranked ahead of documents that were orig-inally ranked higher, but only in a single list.

For our baseline comparisons we implemented a simple round robin merging approach. Round robin successively adds to the merged list the first two documents from each individual list, then the second two, and so on. If a particular document has already been added to the final merged list (e.g., because it was ranked higher in the other user X  X  list), it is skipped and the next document is added instead. 3.2. Splitting
We split the combined list of documents using k -means clustering based on document content with k equal to the num-ber of collaborators, which, in this case, is two. The idea is that searchers can cover different aspects of the search results, as appropriate to their roles. The Gatherer ensures that the most relevant documents are discovered as quickly as possible. The Surveyor scans other, more diverse documents, trying to understand the collection and to make serendipitous discoveries. In some sense this approach is similar to the scatter phase of Scatter X  X ather, in which one cluster has typically been shown to have good recall and precision ( Hearst &amp; Pedersen, 1996 ).

Starting with the merged ranked list produced by merging query results from the two collaborating searchers, the top 2000 documents are randomly (but with equal probability) assigned to one of two clusters. The Vector Space centroid of each cluster is computed, and then documents are reassigned to each cluster based on nearest proximity to a centroid. Centroid calculation and reassignment is repeated for five iterations. Documents in each cluster are then presented to the two search-ers in the same order they appear in the original merged list. While other clustering algorithms are possible, we believe this approach is representative. 4. Experiments
To create our test collection, we extracted terms from the description field of TREC topics 301 X 450, and ran these terms as queries to identify potential topics. We then selected all topics of moderate difficulty (precision@10 values between 0.1 and 0.5). This yielded 53 topics, from which we randomly removed three to produce five groups of 10 topics. For each group, we generated a paper form with topic descriptions and instructions asking people to write the query they would issue to a typ-ical search engine for the given topic. Fifteen participants were asked to fill forms at their leisure, providing us one query for each of the 10 topics assigned to them, resulting in 150 queries for the 50 topics, or three queries per topic. 4.1. Merging
We used TREC relevance judgments for the selected topics the define the gold standard against which we compared query performance. To simulate single-iteration collaboration in group of searchers, we first ran the three queries (A, B, and C) for each topic individually. Next, we merged the ranked lists two ways (AB, AC, BC) using both round robin and CombSUM fu-sion. We then compared each of these merged lists against the individual query baseline, i.e., AB vs. A, AB vs. B, AC vs. A, AC vs. C, etc. This yielded a total of 300 comparisons from our initial 50 queries. The results are shown in Table 1 . Overall, the round robin approach worked only slightly better than no merging, but the difference is not statistically significant. Comb-SUM, on the other hand, well out-performs the individual queries, and the results are significant.

Next, we compared CombSUM against round robin merging, directly. In this case there are only 150 comparisons to make  X  AB, AC and BC for 50 topics. In line with Shaw and Fox (1994) , these results ( Table 2 ) show that CombSUM fusion far out-performs round robin merging. This suggests that although ranked list fusion is not the only possible foundation for collab-orative algorithms, it is a reasonable starting point. 4.2. Splitting
Now that we have combined the individual queries from multiple users, the next step in a single round of algorithmically mediated collaboration is to redistribute those results back out to the users. The most straightforward method for results vitz, 2007 ): One person would get the documents ranked 1, 3, 5, 7, 9, etc. from the combined list, and the other person would get results 2, 4, 6, 8, 10, etc. A stronger alternative is to use a clustering algorithm described in Section 3.2 . By examining the content of the retrieved results and dividing that content into two clusters, we can present each searcher with a separate cluster, one that is optimized for precision and the other optimized for diversity.
 from which each approach was constructed. For averaging purposes, we group each subset into the  X  X  X est X  and the  X  X  X orst X  subset, as measured by average precision.
 essentially equivalent to the original fused list. On the other hand, the best de-RR cluster is significantly worse. This is to be expected, as there are a lot of relevant documents that will be split evenly between the two collaborators, thus lowering the precision of both. Indeed, when we look at the  X  X  X orst X  retrieval subsets, we can see that the k -means cluster is much worse than the de-RR cluster. This demonstrates that it is possible to re-distribute results in such a manner so as to preserve high precision for one of the users. The role of Gatherer becomes possible.
 rounded view of the entire result set. When we re-examined the two clusters produced by the k -means step, we found that 112 times out of 150 (74%), the cluster with the lowest average precision was also the cluster with the highest average dis-tance (or spread) from the cluster centroid (sign test p 0 : 01). Although not a perfect measure of diversity, this indicates that while the Gatherer retains high average precision (AP), the Surveyor can simultaneously more quickly get a sense of the diversity of the documents in the results set. By way of comparison, the de-RR splitting of the same initial fused list does not exhibit these same role-based properties. With de-RR, we found that only 87 times out of 150 (57%, sign test p &lt; 0 : 10) was the cluster with the lowest AP also the cluster with the highest spread. Thus cluster diversity is relatively evenly spread be-tween the higher and lower precision clusters under a de-RR partitioning scheme.

For an additional comparison, we did one more experiment in which the top 2000 documents from the fused list (the same 2000 that were used to create the k -means clusters) were split evenly into top/bottom halves. The documents that were ranked 1 X 1000 became the first thousand documents in one, obviously higher precision, cluster. The documents that were ranked 1001 X 2000 became the 1st to 1000th documents in a second cluster. And while the precision from the  X  X  X etter X  cluster is (by definition) equivalent to the precision from the initial list from which it was created, we also found that the spread of the bottom,  X  X  X orse X  cluster was larger only 81 times out of 150 (54%, sign test p &gt; 0 : 10). Thus it is not the case that the second search collaborator can simply start further down in the ranked list to get a greater amount of diversity than the first search collaborator. More intelligent algorithmic mediation is necessary to support the team X  X  activities.
These results indicate that while it is possible to create naive partitions from collaboratively created (fused) result sets, neither de-RR splitting nor the top 1000/bottom 1000 splitting will satisfy both Gatherer and Surveyor. De-RR comes close to creating a cluster that fulfills the Surveyor role, but from Table 3 we see that it cannot fulfill the Gatherer role. Top/bottom splitting, on the other hand, can fulfill the Gatherer role, in that one of the clusters is guaranteed to be equivalent to the ini-tial fused list (which has significantly higher AP than either original list). However, it cannot fulfill the Surveyor role.
Our merge-split approach, on the other hand, creates partitions that support both Gatherer (high precision) and Surveyor (high diversity) roles. While diversity, especially in the absence of relevance, is not a typical information retrieval evaluation metric, we argue that this approach lets the search team more readily discover important clues that will guide further search iterations. For example, a more diverse set of results can help a team discover where their search may be failing by being able to see a broader range of information that is being retrieved by the system. The goal is to support the discovery of the kinds of  X  X  X aps in knowledge X  that Marchionini describes ( Marchionini, 2006 ). By looking at a more diverse result set, the Surveyor may identify terms to avoid in future searches, or discover potentially useful documents or search terms. By allowing one team member to focus primarily on that task, rather than mixing different goals, we believe that the team as a whole can work more effectively. Future work, though, will have to address the effect of that diversity on the iterative, interactive nat-ure of the entire search session. We have, at least, shown that with two searchers there is merit to the role-based results redistribution. Overall, one need not sacrifice precision for diversity, as one of the users will still be given access to the pre-cision-optimized cluster.

On the surface, these results appear to be at odds with those reported by Joho et al. (2009) . There are, however, important differences between the two approaches that can account for the apparent discrepancy. Joho et al. clustered 300 documents to simulate assignments to multiple collaborating team members, and did not observe a difference in recall between round-robin and cluster-based assignment of documents to simulated searchers. Their methodology, however, makes that result unsurprising because they appear to be re-pooling results from all k team members after n documents per team member have been examined. Since they are measuring recall at n k documents, and the total number of relevant documents does not change, one would expect little difference in recombined team pools no matter if the initial assignment was clustered or round-robin  X  the team as a whole will get through most if not all of the same best-scoring documents in both cases. In our case, we are considering precision and diversity. The asymmetry of roles (a consideration that is absent in their analysis) makes it unnecessary to recombine the results assigned to each team member because the goals are ultimately different. 5. Future work search. We postulated a collaboration between two people, and modeled one cycle of the collaboration. From this point, sev-eral directions for extension are possible. For instance, our simple clustering approach could be enhanced to support more than one Gatherer. This can be achieved by employing a Scatter X  X ather-style mechanism ( Hearst &amp; Pedersen, 1996 ) to iden-tify more than one high-precision cluster. Other means of determining semantic facets could also be used to determine which documents are shown to whom. In particular, this approach may be useful for mediating collaboration among experts in different domains.

A number of strategies are possible, including the most recent, the ones with the deepest judgments of relevance, or com-binations of the two using a scheme similar to that described by Pickens et al. (2008) that combines relevance with recency.
Gatherer results, the other the Surveyor. We do not believe that this hinders the collaborators X  ability to carry out a success-ful information seeking session. Furthermore, evidence in this paper leads us to believe that both the users and the system should be able to tell the result sets apart after examining a dozen results from each set (after a few documents have been marked relevant). If users wish to swap lists (roles) after at that point, the interface should allow them to. However, we acknowledge that there is a limit to simulation of collaborative information seeking environments and plan to test these ideas in future users studies.
 in the manner described in this paper. These techniques could also be applied to the activity of a single user who  X  X  X ollab-orates with himself X  by combining queries from his search history. In this case, the two clusters can be labeled as different kinds of suggestions. For example, the label  X  X ocused X  can be applied to the set similar to the user X  X  query with the most (po-sitive) relevance judgments, and the label  X  X iverse X  can be applied to the other cluster. 6. Conclusions
Further work is needed to bring this into a full interactive session, but our preliminary results are encouraging. With our experiments on TREC collections, we demonstrated an effective mechanism to redistribute the results for Gatherer and Sur-veyor roles in a collaborative IR situation. We showed that while naive approaches to partitioning data may satisfy one or the other role X  X  requirements, our proposed mechanism satisfies the needs of both roles.
 with identical roles is appropriate, asymmetric roles can be effective in modeling certain kinds of collaboration. Both team members in the same role may get the benefit of division of labor in a collaborative setup, but it is the ability to combine diverse actions and inputs that gives collaboration its real strength ( Shah et al., 2009 ). In other words, it is through such a collaboration with asymmetric roles and responsibilities that we can hope to make the whole greater than the sum of the parts.
 eliminating the possibility of completely different queries for a given topic. This assumption is a driving force for several of the works in collaborative search, such as Smyth, Balfe, Briggs, Coyle, and Freyne (2003) . If, however, the queries are very different with little or no overlap in their result sets, the merge aspect of the algorithm technique may boil down to round robin merging of the two lists, but the splitting aspect will be unaffected. The Gatherer role may not be more precise than either person searching alone, but the Surveyor will still be optimized for diversity. As a degenerate example, the algo-rithm is capable of starting with a single ranked list created by one user X  X  query and split it into Gatherer and Surveyor clusters.
 search results to meet the criterion of asymmetric user-roles in a collaborative IR scenario. We discussed a situation of Gath-erer X  X urveyor and showed how we could facilitate the above requirement with algorithmic mediation. It is possible to de-fine such user roles in other ways. For instance, we could divide the results according to the facets that they cover and present each user with a different facet. Such results can be obtained by performing aspectual or semantic clustering instead of the kind of clustering that we have proposed here. Imagine for a query such as  X  X  X lephants X , one cluster might be  X  X  X frican X , the other  X  X  X sian X . We can then have one user explore one path, the other user explore the other. This is just one of many possible ways to structure the collaborative interaction. Given a situation, one can decide to choose the kind of merging and splitting algorithms that would work the best for that context. We hope this work has shown a promising direction of reaping the benefits of role-based collaborative IR.
 References
