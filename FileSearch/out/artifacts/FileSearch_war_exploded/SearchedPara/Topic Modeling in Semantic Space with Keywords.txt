 A common and convenient approach for user to describe his information need is to provide a set of keywords. Therefore, the technique to understand the need becomes crucial. In this paper, for the information need about a topic or cat-egory, we propose a novel method called TDCS(Topic Dis-tilling with Compressive Sensing) for explicit and accurate modeling the topic implied by several keywords. The task is transformed as a topic reconstruction problem in the seman-tic space with a reasonable intuition that the topic is sparse in the semantic space. The latent semantic space could be mined from documents via unsupervised methods, e.g. LSI. Compressive sensing is leveraged to obtain a sparse repre-sentation from only a few keywords. In order to make the distilled topic more robust, an iterative learning approach is adopted. The experiment results show the effectiveness of our method. Moreover, with only a few semantic concepts remained for the topic, our method is efficient for subsequent text mining tasks.
 I.2.7 [ Arti cial Intelligence ]: Natural Language Process-ing X  Text Analysis ; H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval text mining, semantic space, compressive sensing ing need to get information on some interesting topics or categories. Due to the volume and velocity of Web data, it becomes crucial to provide an efficient and convenient way for users to get the information.
 various users. For example, one may be interested in  X  X ood X , c  X  while the other one is interested in  X  X ports X . It could be dif-ferent in granularity. For example, compared with  X  X occer X ,  X  X ports X  is more coarse and  X  X orld cup X  is more fine. More-over, the topic is often dynamic, which is context dependent. Therefore, it X  X  almost impossible to define topic X  X  categories and organize them in advance. It X  X  essential to provide a flexible approach for user to acquire the information. scribe the topic with several keywords, then an information system tries to understand the request and provide corre-sponding information service. The more accurately users describe the topic, the more precise service the information system could provide. In the usual manner of people X  X  ex-pressions, the system usually requires sufficient information to supply good service. However, in practice, a series of key-words are possible for indicating a topic if the user really has a clear information need. For example, the keywords,  X  X ar, automobile, engine, sedan, bmw X  probably imply an inter-est on the topic  X  X uto X . It X  X  particularly attractive for the prompt need, since humans can quickly give keywords. derstanding and modeling of the topic becomes an impor-tant and essential problem. The problem is not that easy, because sometimes the provided keywords are not sufficient. The complexity of the text representation and the random-ness of provided keywords also increase the difficulty. methods. One is to tackle in the word space, such as bag of words(BOW) based method, e.g. VSM. The other kind of methods is to tackle in the semantic space.
 topic is represented as a bag of words, accounting for the number of occurrences of each term. This method is not sufficient for the problem, since there are many other words also beneficial to the topic.
 mantic concepts or topics behind the documents or words, e.g. LSI, PLSI, LDA. Take example for LSI, through ma-trix decomposition, it builds a semantic space which, by intent, represents the semantic concepts in the dataset, and then projects both documents and words into the seman-tic dimensional space[6]. The most simple and natural ap-proach is to model the topic in the semantic space by a projection from word space. However, it X  X  still not accu-rate enough. Essentially, these methods construct seman-tic concepts by the exploitation of correlations among the co-occurring words. For a word, even it X  X  strongly rele-vant with a topic, it X  X  very likely to appear with the words ma inly about other topics. Thus, the simple projection will inevitably introduce irrelevant semantic concepts which af-fects the precision of the topic.
 semantic space[13][5]. In other words, the topic is sparse in the semantic space, which makes it possible to approximate the real semantic distribution of a topic with a few semantic concepts. In practice, utilizing only a few components of the semantic space to represent a topic could also lead to the computational efficiency in the following tasks, which is very beneficial especially for the big data. This is another important issue to motivate us to build a sparse model. sparse topic in the semantic space with core semantic con-cepts. It X  X  challenging because the number of keywords is too small. To this end, we propose a novel method called TDCS(Topic Distilling with Compressive Sensing), which aims to distill the topic in the semantic space from the key-words. The theory of compressive sensing(CS)[7][8] demon-strates that many natural signals are sparse or compressible in a transformed basis. Thus, we take advantage of CS to establish a sparse solution in the semantic space from only a few keywords. To make the result more robust, an iter-ative learning approach is adopted. Our proposed method is simple yet effective. Moreover, with only a few semantic concepts remained for the topic representation, our method is efficient for subsequent tasks, e.g. text classification. follows: use CS for topic modeling. We propose an effective method to distill the topic from only a few keywords which exploits and utilizes the sparsity of the semantic space. ponents remained for topic representation, it can signifi-cantly reduce computational complexity for subsequent tasks. into other semantic spaces.
 the preliminaries are introduced in section 2. Then, our proposed method will be are described in detail in section 3 and 4. The experiments and results in section 5 demon-strate the effectiveness of our method. Some related works are discussed in section 6. Finally, we conclude our paper in section 7. sive sensing. Though we take LSI as the method for build the semantic space, our method could be easily extended into the semantic space built by other methods. factorization technique, i.e. single value decomposition(SVD) to construct a latent semantic space.
 d -th row X d represents a document d in the corpus D . Sup-posing D is corpus of M documents, indexed by d . There are W distinct terms in the vocabulary.
 U and V are orthogonal, and  X  is diagonal. The values  X  ,  X  2 , . . . ,  X  min { W, M } are the singular values of X . proximation  X  X . This is usually done with a partial SVD using the singular vectors corresponding to the K largest singular values[1]. represented by the K -dimensional vector  X  T v which is the v -th row of the matrix  X  V . Similarly, the document also can be represented by the K -dimensional vector. Thus, LSI projects both terms and documents into a K -dimensional latent semantic space which could be utilized for some tasks, e.g. information retrieval.
 terms, the query could be viewed as a short document and be projected into the latent semantic space using semantic concepts in the dataset. ficiently acquire and reconstruct a signal from a small num-ber of non-adaptive linear measurements. The number of measurements are usually smaller than the dimension of the signal. [7][3].
 natural signals are sparse or compressible. In other words, they have concise representations or structures when ex-pressed in a transformed basis.
 note a target signal in compressible basis  X , and y  X  R n denote n &lt; L measurements of x , where  X   X  R n  X  L is a mea-surement matrix, and we wish to recover the sparse signal x  X  R L such that signal recovery is cast into a linear programming problem: where | . | 1 is the l 1 penalty function.
 LASSO, which aims to solve the following unconstrained l 1 regularized minimization problem by a non-linear procedure: where  X  is a regularization parameter.
 one can reconstruct x by solving the above l 1 minimiza-tion problem[3][8]. The standard CS theory indicates that, an s  X  sparse vector in R L can be recovered efficiently using m = O ( s log( L/s )) measurements[4]. representation upon the low level semantic concepts, and the training data are the keywords and the unlabeled text data. y X  X  X  X   X  model the documents as a mixture of hidden topics/latent semantic concepts, and each topic is usually a probability distribution over the words, w.r.t. p ( w | z ), where z is the given topic, and w is the words in the vocabulary. words upon the low level semantic concepts with the ex-ploitation of semantic sparsity.
 keywords set KS = { keyword 1 , ..., keyword m } , we try to learn a topic T ks , which is a distribution over low level se-mantic concepts/topics z , w.r.t. p ( z | T ks ). T ks is the topic described by the user provided keywords set KS , and z is the lower topics or semantic concepts mined from the doc-uments via a unsupervised approach, e.g. LSI. Under the hypothesis of sparsity, most of the elements of the p ( z are 0. Essentially, we want to refine the semantic concepts for the topic. the semantic space. With the exploitation of the sparsity, we aim to obtain a more concise topic representation. That not only just means a more accurate topic, but also a significant improvement in computation efficiency. The challenging is that the number of keywords is usually very small and ran-dom.
 Sensing) framework for the problem. In the framework, the keywords are taken as the measurements, and the topic is constructed in the transformed semantic space. One-hot representation is used for each keyword X  X  representation in word space, and they are projected into semantic space to build the measurement matrix. The value of the measure-ment is the probability or the confidence about the key-word X  X  relevance with the target topic.
 there still exists the theoretical guideline. Ideally, based on the sparsity hypothesis, if the provided keywords are accu-rate and sufficient enough, the topic could be reconstructed efficiently. In practice, there are two crucial issues needed to care about. Firstly, the provided keywords may not be accurate enough. Secondly, the number of provided key-words may be smaller than the theoretical bound, though the bound is not very tight. Both the two issues could re-sult in not reliable results. Therefore, an iterative learning approach is adopted to make the result more robust in our method.
 ure 2. A unsupervised method e.g. LSI is used to build the original latent semantic space. The keywords are taken as the training data. Firstly, the keywords in word space are projected into semantic space. Compressive sensing is utilized to construct the initial topic representation from them. After the initial topic representation generated, the topic will be reflected into word space in the feedback stage. In feedback stage, we choose the words most likely relevant with the topic to enrich the keywords set, i.e. updating the training data. Then with new keywords set, the learning procedure is conducted once again. The process will be re-peated several times. If the stop criteria is satisfied, the iteration will be terminated.
 our proposed method, we list the detailed notation of each variables in Table 1.
 v ariable de scribtion D th e dataset
X do cument-term representation of the dataset d th e document V oc th e vocabulary of the dataset N t he size of the vocabulary
K S th e set of keywords m n umber of keywords K th e dimension of LSI X  X  semantic space
L th e dimension of original signal space s th e dimension of sparse signal y co rresponding measurement values A th e measurement matrix
T k s th e topic implied by KS p ( z | T k s ) th e topic representation of T ks z th e semantic concepts/topics mined from data mantic space, so we call the method TDCS LS I here. Our method could be easily extended into the semantic space built by other methods. document-term matrix X . Ideally, a larger dataset could obtain a better semantic space. The common used term rep-resentation method is TFIDF[18], however, for some tasks, there are other better methods.
 composed into three components. For matrix V , each di-mension v of V = ( v 1 , ..., v K ) corresponds to a particular semantic concept and they are orthogonal with each other. The dimension v of V could be viewed as the semantic con-cept/topic z described in Section 3.1.
 ory[4], the RIP criteria will be surely satisfied. V will be taken as the semantic basis, which corresponds to matrix  X  in Figure 1.
 for large data set, so we use an approximated algorithm PSVD[1] for LSI, which pursuits the singular vectors corre-sponding to the K largest singular values. K is an empirical value and should be carefully set. Topic section, we will explain in detail how to build the measure-ment matrix through projecting keywords from word space to semantic space.
 used one-hot representation method is utilized. Specifically, for keywrod i , the corresponding representation is where idx i is the index of the keyword i in the vocabulary V oc , and the corresponding element is 1 while others are 0. where m denotes the number of keywords, and each row cor-responds to a keyword.
 tic space will be a is the semantic distribution of keyword i . Since each row of matrix V could be viewed as a words X  semantic distribu-tion in the latent semantic space, the projection essentially selects the corresponding semantic distribution of keyword where each row of A i.e. a i corresponds to a keyword X  X  se-mantic distribution.
 surements, so the matrix e corresponds to  X  in Figure 1 and the matrix A corresponds to the measurement matrix  X  in Figure 1. The measurement values could be written as the vector below where y i = pro r elevance ( keyword i , T ks )  X  (0 , 1], which is the confidence or probability of the relevance between the keyword and the interesting topic.
 each keyword, in practice, we take 1 as the default value which means all the keywords user provided are strongly relevant with the topic.
 e.g. { keyword i , y i } . semantic space. Therefore, we try to find a subspace in V that can best construct and approximate the topic. here we use  X  instead.  X  measures how important the corresponding semantic con-cept v i contributes to the target topic.
 the equation below should be satisfied. to find a  X  , such that, where m &lt;&lt; K .
 matrix A is the measurement matrix, corresponding to  X , and the y is the measurement value.  X  corresponds to the x in Figure 1.
 tion problem will be where y is set as 1 for all keywords in default as we described in last section.  X  is the regularization parameter, which con-trols the degree of the sparsity. A larger  X  always means a more sparse result.
 cause of the l 1 term, and in order to solve it efficiently, the p ackage SLEP 1 is facilitated. The details of the package are described in [16] which implements several state-of-art methods for this kind of optimization problem. sparse models, it X  X  usually tuned in an interval via cross val-idation in the development set. We will give more detailed description about the parameter tuning in the experiments session.
 elements of  X   X  is zero. That X  X  consistent with our previous assumption that only a few latent semantic concepts con-tribute to a topic. in many applications. However, in practice, sometimes, the number of keywords is too small to yield reliable results since the robust reconstruction needs sufficient measurements ac-cording to compressive sensing theory. The quality of the provided keywords is also an important issue for the topic construction.
 pressive sensing algorithm more robust for our task. This is a heuristic method to approximate a more stable topic representation.
 topic is generated through compressive sensing, we reflect the topic into word space in the feedback stage. For the reflection, the equation below is utilized, the parameter  X   X  is the weight of words.
 with the topic as incremental keywords, w.r.t. IKS . The measurement value y j of keyword j in IKS is set as the weight of the keyword j in  X   X  after normalization. pose a new training set.
 where IKS = {{ keyword j , y j } . . . } .
 back, the topic will tend to stable.
 the IKS , w.r.t.  X  . If it X  X  too small, the iteration will be too slow, however, if it X  X  too large, the iteration will shock and lead to the devi-ation of real topic. The size is fine tuned in experiments by cross validation.
 ation. We could calculate the difference value between the last two topic X  X  distribution, if the differ is small than a pre-defined threshold value, a converged solution is obtained. This method is sensitive to the threshold value which should be carefully tuned. An alternative strategy is to measure the difference in the word space, if the IKS no longer brings in new keywords for KS , we could consider to stop the itera-tion. It X  X  usually equal with former one, and more practical, therefore we utilize this strategy in our method. h ttp://yelab.net/software/SLEP/ sentation, i.e.  X   X  from the keywords KS , which captures the core semantic concepts and assign suitable weights(values) to them.
 Algorithm 1.
 Al gorithm 1 The detailed TDCS algorithm 1 : input: 2: document-term matrix X 3: initial keyword list KS 4: output: 5: topic representation:  X   X  6: w of vocabulary 7: % latent semantic space building 8: D is decomposed into U  X  V T 9: repeat 10: for each keyword in KS do 11: generate the corresponding e i 12: end for 13: generate the matrix e 14: % topic projection 15: A = e  X  V ; 16: % topic  X  17: find the  X   X  s.t. min | A X   X  y | 2 2 +  X  |  X  | 1 18: % topic feedback 19: w = V  X   X   X  ; 20: choose the words with highest weight 21: form an incremental keyword list IKS 22: KS  X  = KS  X  IKS ; 23: % update the training data 24: KS = KS  X  25: until IKS  X  KS  X  N U LL learnt from the keywords KS , which could be used for many subsequent text mining tasks with the benefits both in ac-curacy and efficiency, for example, text classification, infor-mation retrieval. In this section, we give an introduction of the detailed usage. category(class) in a dataset. Given a list of keywords to representing different categories, the objective is to learn a model to classify the documents. It becomes the problem called text classification by labeling words [15][11], existing methods are all tackling in the word space.
 g ( f w ( x i )), where and g ( . ) could be 0/1 function, sigmoid function, etc. could be where the L could be square loss, log loss, hinge loss, etc. The R ( w ) is the regularization term. sifiers, which is the product of the parameter w and x i , mea-suring the similarly between an instance and target class. For text classification, usually, x i is the BOW representa-tion of a document d i , the classifiers try to learn a optimal w  X  from the training data for f w ( x i ).
 existing methods, our method could skillfully transform the problem into semantic space. For one class classification, by projecting documents x into a K-dimensional semantic space, w.r.t. x  X  , the w  X  will be learnt in the K-dimensional semantic space. The  X   X  learnt by our method is a very good choice as the w  X   X  for f w  X  ( . ).
 as the query, w.r.t. q , the VSM[23] model will represent the query q and document d in word space, and then calculate the similarity between them with are projected into semantic space w.r.t. q  X  and d  X  following Equation 2, and then sim ( q  X  , d  X  ) is calculated similar with VSM by Equation 15.
 space, a more accurate and concise q  X  X  i.e.  X   X  could be used for the similarity calculation as below. mation retrieval is the product between Q and R ( d ), w.r.t. Q stands for x i in f w ( . ) or d in sim ( q, d ). size of vocabulary. After the projection, N will be reduced to K . With our method, the effective dimension of Q for product calculation will be s , s &lt; K . Usually, s is one order smaller than K . That means a great gain in computational complexity.
 production will decrease from O ( N  X  M ) or O ( K  X  M ) to O ( s  X  M ), where M is the size of dataset. stead, we evaluate it in an indirect way. It will be utilized for concrete text mining tasks, here we choose text classifi-cation as the task and we will verify the effectiveness of our method through the performance of classification. ROC) is adopted to measure the performance[9]. A larger AUC indicates better performance.
 a target category which is similar with information retrieval, our task here is one class text classification, aiming to iden-tity target category documents from unlabeled set. periments setting section. We will learn from each keywords set respectively.
 Intel Xeon E5-2620 @ 2.0GHz CPUs and 8GB memory. Newsgroups 2 , WebKB 3 and Movie Reviews 4 . unigram words, with HTML tags, stop-words and words with length less than three removed.
 processing. For each dataset, we randomly select 1/5 of the dataset as the testing data, and use the rest of the dataset as training set.
 beled training data will be keywords other than documents. Existing standard methods for one class classification, such as one-class SVM[17] and PU learning[12] are not appropri-ate for the task , since they require sufficient labeled docu-ments, so they .
 GE/SWIRL take GE/SWIRL[11] as the method for comparison, which is a state-of-the-art method for learning from features(words). GE/SWIRL is developed from GE(Generalized Expectation) which seeks a distribution that matches a set of given refer-ence distribution over corresponding label if the feature(word) is given and utilizes the sequence information of the words. In this paper, for simplicity, the sequence is not considered. an existing framework for training classifiers with labeled features. The framework is designed for multi-class classifi-cation with multiple keywords sets, for convince, we modify it for one class classification by set the target class value of each keyword as 0.99 while the value of other classes as 0.01. h ttp://people.cs.umass.edu/  X mccallum/data.html http://www.cs.cmu.edu/  X webkb http://www.cs.cornell.edu/people/pabo/movie-review-data/ V SM and LSI category as the relevant documents for the user provided keywords. This task could be viewed as a coarse-granularly information retrieval. Therefore, we also take the VSM and LSI as the methods for comparison.
 where g ( . ) is 0/1 function, and f w ( x ) =  X   X   X  x . time as well as our TDCS method. user provided approach WebKB and Movie Reviews . In [11], the labeled key-words are formatted as list of { keyword,label } pairs. The labels are the categories. We extract the keywords from the list as the user generated keywords. oracle approach topic. In practice, we assume there exists an oracle that can reveal the keywords. We then select keyword according to their predictive power measured by TFIDF score of the given category. This is applied for 20 Newsgroups because there X  X e no public human labeled keywords for this dataset. of the datasets, the keywords are collected with above ap-proaches. During the experiments, the keywords for a topic are randomly selected from the pool. For term weighting method, 20 Newsgroups and WebKB is tfidf , and Movie Reviews is 0/1, i.e. the occurrence of a word or not. There have been some works shown that for sentiment classification, the 0/1 representation is better than tfidf upon most of classification models[14]. erated, LSI is applied to build the semantic space. The dimension of the semantic space, i.e. K is empirically set as 200, which is a commonly used value[18].
 are fine tuned via cross validation. We randomly select 1/10 of the training data as the development set and conduct the parameter tuning in the development set. The parameter  X  is tuned firstly without iteration, and then the  X  is tuned. chosen to build the models with different parameter values. The models will be utilized in the development dataset, af-ter the evaluation of their performance, the parameter value of the best model is set as the final value.
 2 , 1 e  X  1 , 1], and  X  is tuned in the interval [1 , m 2 ing, the value of  X  used in the experiments is 0.01, and the value of  X  is  X  m 2  X  . tions as well as different number of keywords, and give de-tailed analysis of the iterative approach.
 periments 100 times. In order to investigate general influ-ence by the number of keywords, we randomly choose 5 and 10 keywords respectively each time to describe the topic in Movie Reviews. For 20 Newsgroups, we randomly choose 5 keywords as the topic description each time. For WebKB, we randomly choose keywords with variable-length from 5 to 10 each time. Table 3 summarizes the keyword numbers for each dataset.
 sh own in Table 4 5 . The corresponding keywords setup is shown in Table 3. The only difference between Movie Re-views(k1) and Movie Reviews(k2) is the number of keywords provided.
 da taset VS M LS I GE/ SWIRL TDCS Mo vie Rev. (k1) 0. 528 0.5 45 0 .540 0 .596*
Mo vie Rev. (k2) 0. 561 0 .572 0 .584 0 .619* 20 Newsgroups 0. 578 0.9 18 0 .891 0 .944*
W ebKB 0. 705 0 .784 0 .802 0 .825* keyword setups, our TDCS method outperforms the other methods 6 , which demonstrates the effectiveness and robust-ness of our method, and reveals that the distilled topic is more precise for text classification.
 e.g. 5 in Movie Reviews and 20 Newsgroups , the LSI is better than GE/SWIRL. However, GE/SWIRL will be bet-ter when the number increases, e.g. 10 in Movie Reviews and 5-10 in WebKB . A possible reason is that the LSI is superior in improving the recall especially when the num-ber of keywords is too small. However, when the number of keywords increase, the impact of this aspect will be not obvious.
 takes advantage of the semantic space to improve the recall, but also utilizes the semantic sparsity to distill important semantic components from the semantic space which lead to the improvement of precision.
 good compared with the other two datasets, a possible rea-son is that the latent semantic space built by LSI is not ideal for sentiment classification. It X  X  hard to guess what the basis vectors in the LSI space may correspond to. However, from the experiment results, it seems that there should be some
Th e best and second best results are bold in text. * The improvement is statistically significant, since the p value &lt; 0 . 05 according to t test. sem antic concepts highly relevant with sentiments since our method could get better results than others. With a bet-ter semantic space for sentiment analysis, the results of our method will be surely better.
 and our TDCS method all get good results compared with the other datasets. The main reason is the generation ap-proach of the keywords. We choose the keywords with high tfidf values as the topic descriptions, they are representative for the category and discriminative with other categories. find that, roughly, when the number of keywords grows, the performance grows. The average result with 10 keywords is better than that with only 5 keywords. This suggests that for a robust topic construction with CS, sufficient measure-ments are needed.
 our method can get better results with 5 keywords than VSM, LSI and GE/SWIRL with 10 keywords. This shows the advantage of our method especially when training data is small. With less training data, here is the 1/2 of the training data, our method could still get comparable and even better results. This very useful, since people are used to provide as few keywords as possible for a topic. the results in movie data. The detailed topic descriptions are shown in Table 6. These topic descriptions are all gen-erated for the  X  X ositive X  category.

Table 5: results of examples on Movie Reviews
T able 6: keyword examples of the Movie Reviews words provided, even though the number of keywords are the same with each other. For example, the keyword  X  X igh X  in T D 2 is not very good to indicate the positive sentiment, so the result is worse than others. Though T D 3 only has 5 keywords, it could even get the better result than those with 10 keywords. This reveals that the quality of the keywords is also an important factor for our method, i.e. the provided keywords should be representative for the topic.
 method, is better than T D 4 , T D 5 and T D 6 (10 keywords) with SVM or LSI method. This also demonstrates the ef-fectiveness of our method, with 1/2 of training data, our method can get better results than those with full data. ulty X  category. The topic descriptions are given with variable size.
 of keywords. That X  X  reasonable, take example for T D 1 and T D 2 , they are better than T D 3 . The word  X  X epart X  and  X  X mail X  in T D 3 are also possible in the  X  X tudent X  category. This also suggests that good quality of the keywords is es-sential.

Table 7: keyword examples of the WebKB data of our iterative learning approach. The topic description T and T 2 in Table 8 are both generated for the  X  X uto X  category. Figure 3 shows the performance of text classification after each iteration. In general, the performance increases along with the increase of iteration numbers.

Table 8: keyword examples of the 20 Newsgroups are mostly relevant with the topic description T 1 and T 2 spectively in Table 9. The keywords are picked from the final expanded set KS , and for each topic, we choose the top 20 keywords. We can find that if the user really have an interest on this topic, and give accurate descriptions, though the initial given keywords are different, the final expanded relevant keywords are very close. This gives an insight for better keyword recommendation.
 enough, after the iteration, the constructed topic represen-tations in semantic space will tend to be close. In practice, due to the semantic diversity of the keywords provided by different users, the difference between the distilled topic rep-resentations is unavoidable. The iterative learning approach could reduce the difference. Fi gure 3: Classi cation performance along iteration
Table 9: Expanded Keywords on 20 Newsgroups to pic desc. T 1 T 2 rele vant keywords dataset. The time duration is measured in seconds. We can see that with our method, the computation cost is smaller in a order of magnitude.
 ti mizes the matrix computation in memory, and datasets used in our experiments are tiny in comparison with real world datasets, therefore the total computation time is lit-tle. However, for real web data, which contains millions of documents, the difference will absolutely become larger, e.g. 1 hour vs. 10 hours.
 all the data once a time. The frequent data throughput is es-sential, which is usually time-consuming. With the effective dimension reduction through our method, more data could be loaded into the memory for computation each time which is efficient and beneficial for text classification, information retrieval, etc.
 Summary: We have presented comprehensive experiments using three different text collections to evaluate the effective-ness of the proposed TDCS method. In general, with more keywords, the result will be more accurate. The quality of the keywords is also an important issue. Moreover, with the reduction in the topic representation space, the computation complexity significantly reduces. semantic space, in this paper, we take LSI to build the se-mantic space, and surely our method could be extended to other semantic spaces. Meanwhile, there have been arising works on the utilization of semantic sparsity in text mining recently. mantic concepts/topics, for example, PLSA[10] and LDA[2]. They are the typical techniques aiming to find useful latent semantic structure in the unstructured collection. They as-sume that each document exhibits multiple topics, yet ig-nores the correlation between topics. PLSA could be for-mulated a generation process including the document topic distribution and topic word distribution. It X  X  highly sensi-tive to task domain, which is continuously changing in real documents. Different with PLSA, LDA uncovers the under-lying semantic structure based on a hierarchical Bayesian analysis of the original texts. Thus, it becomes computa-tionally very expensive on large data sets.
 distributed representation of words. The neural language model is used to represent the documents, and the learn-ing algorithm is a combination of neural networks and soft-max model[19]. It X  X  a popular word embedding method, and was previously better than LSI for preserving linear regular-ities among words. [21] tells us that word2vec is essentially equivalent with the implicit matrix decomposition with some mathematical explanations.
 into semantic representation space built by other methods. for text analysis, in which, the keyword distribution of each topic is sparse. In [20], they decompose the topic into low-rank principle component and sparse document component. The work is conducted on the assumption that each docu-ment has its own specific keywords which is sparse in com-parison with other documents. In [25], a regularized LSI was proposed to obtain sparse semantic concepts(topics) belong to a document, aiming to scale up LSI for large document sets. In [24], a variation model of PLSI and LDA is pro-posed to fully leverage the sparsity of topics belonging to a document, which is efficient for large scale data.[13] propose a dual-sparse topic model with the hypothesis that in real-world scenarios, individual documents usually concentrate on several salient topics instead of covering a wide variety of topics. A real topic also adopts a narrow range of terms instead of a wide coverage of the vocabulary.
 or topics or words, and building models from the document data set. Our work is different with these works. Firstly, our mission is to construct a more concise topic upon the latent semantic space with the sparsity assumption. Secondly, we need to build the topic from only several keywords rather than documents. the topic from dynamically provided keywords. The spar-sity of the topic is exploited and utilized, and CS is skillfully applied to establish a sparse structure in the semantic space for the topic. With our method, a more precise topic could be obtained. Besides, our method is efficient for subsequent tasks with the significant reduction of computation. strate the effectiveness of our method. Besides text classi-fication, the TDCS method could be used for many other applications, for example, information retrieval, query un-derstanding, keywords recommendation, since which could provide more accurate semantic information.
 direction towards the investigation of sparsity in text analy-sis. We would like to extend our work using other semantic representation methods in the future, and further explore the theoretical perspectives about this method. dation of China(No.61321491, No.61202320), National So-cial Science Foundation of China(No.11AZD121), and Natu-ral Science Foundation of Jiangsu Province(No.BK2012304). It was also partially supported by Collaborative Innovation Center of Novel Software Technology and Industrialization. [1] M. W. Berry. Large scale sparse singular value [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] E. J. Candes, J. Romberg, and T. Tao. Robust [4] E. J. Candes, J. K. Romberg, and T. Tao. Stable [5] X. Chen, Y. Qi, B. Bai, Q. Lin, and J. G. Carbonell. [6] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [7] D. L. Donoho. Compressed sensing. IEEE [8] D. L. Donoho. For most large underdetermined [9] J. A. Hanley and B. J. McNeil. The meaning and use [10] T. Hofmann. Probabilistic latent semantic indexing. In [11] K.-S. Jun, J. Zhu, B. Settles, and T. Rogers. Learning [12] X. Li and B. Liu. Learning to classify texts using [13] T. Lin, W. Tian, Q. Mei, and H. Cheng. The [14] B. Liu. Sentiment analysis and opinion mining. [15] B. Liu, X. Li, W. S. Lee, and P. S. Yu. Text [16] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with [17] L. M. Manevitz and M. Yousef. One-class svms for [18] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [19] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [20] K. Min, Z. Zhang, J. Wright, and Y. Ma.
 [21] Y. G. Omer Levy. Neural word embedding as implicit [22] B. Pang and L. Lee. A sentimental education: [23] G. Salton, A. Wong, and C. S. Yang. A vector space [24] K. Than and T. B. Ho. Fully sparse topic models. In [25] Q. Wang, J. Xu, H. Li, and N. Craswell. Regularized
