 The problem of multimodal data mining in a multimedia database can be addressed as a structured prediction prob-lem where we learn the mapping from an input to the struc-tured and interdependent output variables. In this paper, built upon the existing literature on the max margin based learning, we develop a new max margin learning approach called Enhanced Max Margin Lea rning (EMML) framework. In addition, we apply EMML framework to developing an effective and efficient solution to the multimodal data min-ing problem in a multimedia database. The main contri-butions include: (1) we have developed a new max mar-gin learning approach  X  the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate, which is verified in empirical evaluations; (2) we have applied this EMML approach to developinganeffect iveandefficientsolutiontothemulti-modal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale, allowing facilitating a multimodal data min-ing querying to a very large scale multimedia database, and excelling many existing multimodal data mining methods in the literature that do not scale up at all; this advantage is also supported through the complexity analysis as well as empirical evaluations against a state-of-the-art multimodal data mining method from the literature. While EMML is a general framework, for the evaluation purpose, we apply it to the Berkeley Drosophila embryo image database, and report the performance comparison with a state-of-the-art multimodal data mining method.
 H.2.8 [ Database Management ]: Database Applications X  Data mining,Image databases ; H.3.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval X  Retrieval models ; I.5.1 [ Pattern Recognition ]: Models X  Structural ;J.3[ Computer Applications ]: Life and Med-ical Sciences X  Biology and genetics Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Algorithms, experimentation Multimodal data mining, image annotation, image retrieval, max margin
Multimodal data mining in a multimedia database is a challenging topic in data mining research. Multimedia data may consist of data in different modalities, such as digital images, audio, video, and text data. In this context, a multi-media database refers to a data collection in which there are multiple modalities of data such as text and imagery. In this database system, the data in different modalities are related to each other. For example, the text data are related to im-ages as their annotation data. By multimodal data mining in a multimedia database it is meant that the knowledge discovery to the multimedia database is initiated by a query that may also consist of multiple modalities of data such as text and imagery. In this paper, we focus on a multimedia database as an image database in which each image has a few textual words given as annotation. We then address the problem of multimodal data mining in such an image database as the problem of retrieving similar data and/or inferencing new patterns to a multimodal query from the database.

Specifically, in the context of this paper, multimodal data mining refers to two aspects of activities. The first is the multimodal retrieval. This is the scenario where a mul-timodal query consisting of either textual words alone, or imagery alone, or in any combination is entered and an ex-pected retrieved data modality is specified that can also be text alone, or imagery alone, or in any combination; the re-trieved data based on a pre-defined similarity criterion are returned back to the user. The second is the multimodal in-ferencing. While the retrieval based multimodal data mining has its standard definition in terms of the semantic similarity between the query and the retrieved data from the database, the inferencing based mining depends on the specific applica-tions. In this paper, we focus on the application of the fruit fly image database mining. Consequently, the inferencing based multimodal data mining may include many different scenarios. A typical scenario is the across-stage multimodal inferencing. There are many interesting questions a biologist may want to ask in the fruit fly research given such a mul-timodal mining capability. For example, given an embryo image in stage 5, what is the corresponding image in stage 7 for an image-to-image three-stage inferencing? What is the corresponding annotation for this image in stage 7 for an image-to-word three-stage inferencing? The multimodal mining technique we have developed in this paper also ad-dresses this type of across-stage inferencing capability, in addition to the multimodal retrieval capability.

In the image retrieval research area, one of the notorious bottlenecks is the semantic gap [18]. Recently, it is reported that this bottleneck may be reduced by the multimodal data mining approaches [3, 11] which take advantage of the fact that in many applications image data typically co-exist with other modalities of information such as text. The synergy between different modalities may be exploited to capture the high level conceptual relationships.

To exploit the synergy among the multimodal data, the relationships among these different modalities need to be learned. For an image database, we need to learn the rela-tionship between images and text. The learned relationship between images and text can then be further used in mul-timodal data mining. Without loss of generality, we start with a special case of the multimodal data mining problem  X  image annotation, where the input is an image query and the expected output is the annotation words. We show later that this approach is also valid to the general multimodal data mining problem. The image annotation problem can be formulated as a structured prediction problem where the input (image) x and the output (annotation) y are struc-tures. An image can be partitioned into blocks which form a structure. The word space can be denoted by a vector where each entry represents a word. Under this setting, the learning task is therefore formulated as finding a function f : X X Y X  R such that is the desired output for any input x .

Built upon the existing literature on the max margin learn-ing, we propose a new max margin learning approach on the structured output space to learn the above function. Like the existing max margin learning methods, the image anno-tation problem may be formulated as a quadratic program-ming (QP) problem. The relationship between images and text is discovered once this QP problem is solved. Unlike the existing max margin learning methods, the new max margin learning method is much more efficient with a much faster convergence rate. Consequently, we call this new max margin learning approach as Enhanced Max Margin Learn-ing (EMML). We further apply EMML to solving the mul-timodal data mining problem effectively and efficiently.
Note that the proposed approach is general that can be applied to any structured prediction problems. For the eval-uation purpose, we apply this approach to the Berkeley Drosophila embryo image database. Extensive empirical evaluations against a state-of-the-art method on this database are reported.
Multimodal approaches have recently received the sub-stantial attention since Barnard and Duygulu et al. started their pioneering work on image annotation [3, 10]. Recently there have been many studies [4, 17, 11, 7, 9, 23] on the multimodal approaches.

The learning with structured output variables covers many natural learning tasks including named entity recognition, natural language parsing, and label sequence learning. There have been many studies on the structured model which in-clude conditional random fields [14], maximum entropy model [15], graph model [8], semi-supervised learning [6] and max margin approaches [13, 21, 20, 2]. The challenge of learning with structured output variables is that the number of the structures is exponential in terms of the size of the struc-ture output space. Thus, the problem is intractable if we treat each structure as a separate class. Consequently, the multiclass approach is not well fitted into the learning with structured output variables.

As an effective approach to this problem, the max margin principle has received substantial attention since it was used in the support vector machine (SVM) [22]. In addition, the perceptron algorithm is also used to explore the max margin classification [12]. Taskar et al. [19] reduce the number of the constraints by considering the dual of the loss-augmented problem. However, the number of the constraints in their approach is still large for a large structured output space and a large training set.

For learning with structured output variables, Tsochan-taridis et al. [21] propose a cutting plane algorithm which finds a small set of active constraints. One issue of this al-gorithm is that it needs to compute the most violated con-straint which would involve another optimization problem in the output space. In EMML, instead of selecting the most violated constraint, we arbitrarily select a constraint which violates the optimality condition of the optimization prob-lem. Thus, the selection of the constraints does not involve any optimization problem. Osuna et al. [16] propose the de-composition algorithm for the support vector machine. In EMML, we extend their idea to the scenario of learning with structured output variables.
This work is based on the existing literature on max mar-gin learning, and aims at solving for the problem of multi-modal data mining in a multimedia database defined in this paper. In comparison with the existing literature, the main contributions of this work include: (1) we have developed a new max margin learning approach  X  the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate, which is veri-fied in empirical evaluations; (2) we have applied this EMML approach to developing an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale, allowing facilitating a multimodal data mining querying to a very large scale multimedia database, and excelling many existing multimodal data mining meth-ods in the literature that do not scale up at all; this advan-tage is also supported through the complexity analysis as well as empirical evaluations against a state-of-the-art mul-timodal data mining method from the literature.
Assume that the image database consists of a set of in-stances S = { ( I i ,W i ) } L i =1 , where each instance consists of an image object I i and the corresponding annotation word Figure 1: An illustration of the image partitioning and the structured output word space set W i . First we partition an image into a set of blocks. Thus, an image can be represented by a set of sub-images. The feature vector in the feature space for each block can be computed from the selected feature representation. Con-sequently, an image is represented as a set of feature vectors in the feature space. A clustering algorithm is then applied to the whole feature space to group similar feature vectors together. The centroid of a cluster represents a visual rep-resentative (we refer it to VRep in this paper) in the image space. In Figure 1, there are two VReps, water and duck in the water . The corresponding annotation word set can be easily obtained for each VRep. Consequently, the image database becomes the VRep-word pairs S = { ( x i , y i ) } n where n is the number of the clusters, x i is a VRep ob-ject and y i is the word annotation set corresponding to this VRep object. Another simple method to obtain the VRep-word pairs is that we randomly select some images from the image database and each image is viewed as a VRep.
Suppose that there are W distinct annotation words. An arbitrary subset of annotation words is represented by the binary vector  X  y whose length is W ;the j -th component  X  y j =1ifthe j -th word occurs in this subset, and 0 oth-erwise. All possible binary vectors form the word space Y We use w j to denote the j -th word in the whole word set. We use x to denote an arbitrary vector in the feature space. Figure 1 shows an illustrative example in which the original image is annotated by duck and water which are represented by a binary vector. There are two VReps after the clustering and each has a different annotation. In the word space, a word may be related to other words. For example, duck and water are related to each other because water is more likely to occur when duck is one of the annotation words. Conse-quently, the annotation word space is a structured output space where the elements are interdependent.

The relationship between the input example VRep x and an arbitrary output  X  y is represented as the joint feature mapping  X ( x ,  X  y ),  X  : X X Y X  R d where d is the dimension of the joint feature space. It can be expressed as a linear combination of the joint feature mapping between x and all the unit vectors. That is where e j is the j -th unit vector. The score between x and  X  y can be expressed as a linear combination of each component Then the learning task is to find the optimal weight vector  X  such that the prediction error is minimized for all the training instances. That is where Y i = {  X  y | W j =1  X  y j = W j =1 y ij } .Weuse X  note  X ( x i ,  X  y ). To make the prediction to be the true output y , we must follow where Y i \{ y i } denotes the removal of the element y i the set Y i . In order to accommodate the prediction error on the training examples, we introduce the slack variable  X  The above constraint then becomes We measure the prediction error on the training instances by the loss function which is the distance between the true output y i and the prediction  X  y . The loss function measures the goodness of the learning model. The standard zero-one classification loss is not suitable for the structured output space. We define the loss function l (  X  y , y i )asthenumber of the different entries in these two vectors. We include the loss function in the constraints as is proposed by Taskar et al. [19] We interpret 1  X   X  [ X  i ( y i )  X   X  i (  X  y )] as the margin of y another  X  y  X  X  ( i ) . We then rewrite the above constraint as  X   X  [ X  i ( y i )  X   X  i (  X  y )]  X  1  X  [ l (  X  y , y i ) ing  X  maximizes such margin.

The goal now is to solve the optimization problem where r =1 , 2 corresponds to the linear or quadratic slack variable penalty. In this paper, we use the linear slack vari-able penalty. For r = 2, we obtain similar results. C&gt; 0 is a constant that controls the tradeoff between the training error minimization and the margin maximization.

Note that in the above formulation, we do not introduce the relationships between different words in the word space. However, the relationships between different words are im-plicitly included in the VRep-word pairs because the related words is more likely to occur together. Thus, Eq. (2) is in fact a structured optimization problem.
One can solve the optimization problem Eq. (2) in the primal space  X  the space of the parameters  X  .Infactthis problem is intractable when the structured output space is large because the number of the constraints is exponential in terms of the size of the output space. As in the tradi-tional support vector machine, the solution can be obtained by solving this quadratic optimization problem in the dual space  X  the space of the Lagrange multipliers. Vapnik [22] and Boyd et al. [5] have an excellent review for the related optimization problem.

The dual problem formulation has an important advan-tage over the primal problem: it only depends on the inner products in the joint feature representation defined by  X , allowing the use of a kernel function. We introduce the function K (( x i ,  X  y ) , ( x j ,  X  y )) =  X  i, y i , tives of the Lagrangian over  X  and  X  i should be equal to zero. Substituting these conditions into the Lagrangian, we obtain the following Lagrange dual problem After this dual problem is solved, we have  X  = i,  X  y  X  i,
For each training example, there are a number of con-straints related to it. We use the subscript i to represent the part related to the i -th example in the matrix. For example, let  X  i be the vector with entries  X  i,  X  y . We stack the gether to form the vector  X  .Thatis  X  =[  X  1  X  X  X   X  n ] . Similarly, let S i be the vector with entries l (  X  y , y S i together to form the vector S .Thatis S =[ S 1  X  X  X  S n ] . The lengths of  X  and S are the same. We define A i as the vector which has the same length as that of  X  ,where A Let matrix D represent the kernel matrix where each entry is constant C .

With the above notations we rewrite the Lagrange dual problem as follows where and represent the vector comparison defined as entry-wise less than or equal to and greater than or equal to, respectively.
 Eq. (4) has the same number of the constraints as Eq. (2). However, in Eq. (4) most of the constraints are lower bound constraints (  X  0) which define the feasible region. Other than these lower bound constraints, the rest constraints de-termine the complexity of the optimization problem. There-fore, the number of constraints is considered to be reduced in Eq. (4). However, the challenge still exists to solve it ef-ficiently since the number of the dual variables is still huge. Osuna et al. [16] propose a decomposition algorithm for the support vector machine learning over large data sets. We ex-tend this idea to learning with the structured output space. We decompose the constraints of the optimization problem Eq. (2) into two sets: the working set B and the nonactive set N. The Lagrange multipliers are also correspondingly partitioned into two parts  X  B and  X  N . We are interested in the subproblem defined only for the dual variable set  X  when keeping  X  N =0.

This subproblem is formulated as follows.
It is clearly true that we can move those  X  i,  X  y =0 , X   X 
B to set thermore, we can move those  X  i,  X  y  X   X  N satisfying certain conditions to set  X  B to form a new optimization subprob-lem which yields a strict decrease in the objective function in Eq. (4) when the new subproblem is optimized. This property is guaranteed by the following theorem.

Theorem 1. Given an optimal solution of the subprob-lem defined on  X  B in Eq. (5), if the following conditions hold true: the operation of moving the Lagrange multiplier  X  i,  X  y satisfy-ing Eq. (6) from set  X  N to set  X  B generates a new optimiza-tion subproblem that yields a strict decrease in the objective function in Eq. (4) when the new subproblem in Eq.(5) is optimized.
 Proof. Suppose that the current optimal solution is  X  . Let  X  be a small positive number. Let  X   X  =  X  +  X  e r ,where e is the r -th unit vector and r =( i,  X  y ) denotes the Lagrange multiplier satisfying condition Eq. (6). Thus, the objective function becomes W (  X   X  )= 1 have W (  X   X  ) &lt; W (  X  ). For small enough  X  , the constraints A  X   X  C is also valid. Therefore, when the new optimiza-tion subproblem in Eq. (5) is optimized, there must be an optimal solution no worse than  X   X  .
 In fact, the optimal solution is obtained when there is no Lagrange multiplier satisfying the condition Eq. (6). This is guaranteed by the following theorem.

Theorem 2. The optimal solution of the optimization prob-lem in Eq. (4) is achieved if and only if the condition Eq. (6) does not hold true.

Proof. If the optimal solution  X   X  is achieved, the condi-tion Eq. (6) must not hold true. Otherwise,  X   X  is not op-timal according to the Theorem 1. To prove in the reverse direction, we consider the Karush-Kuhn-Tucker (KKT) con-ditions [5] of the optimization problem Eq. (4).
 For the optimization problem Eq. (4), the KKT conditions provide necessary and sufficient conditions for optimality. One can check that the condition Eq. (6) violates the KKT conditions. On the other hand, one can check that the KKT conditions are satisfied when the condition Eq. (6) does not hold true. Therefore, the optimal solution is achieved when the condition Eq. (6) does not hold true.
 The above theorems suggest the Enhanced Max Margin Learning (EMML) algorithm listed in Algorithm 1. The correctness (convergence) of EMML algorithm is provided by Theorem 3.
 Algorithm 1 EMML Algorithm 1: procedure 2: Arbitrarily decompose  X  into two sets:  X  B and  X  N . 3: Solve the subproblem in Eq. (5) defined by the vari-4: While there exists  X  i,  X  y  X   X  B such that  X  i,  X  y =0, 5: While there exists  X  i,  X  y  X   X  N satisfying condition 6: Goto Step 3. 7: end procedure
Theorem 3. EMML algorithm converges to the global op-timal solution in a finite number of iterations.
 Proof. This is the direct result from Theorems 1 and 2. Step 3 in Algorithm 1 strictly decreases the objective func-tion of Eq. (4) at each iteration and thus the algorithm does not cycle. Since the objective function of Eq. (4) is convex and quadratic, and the feasible solution region is bounded, the objective function is bounded. Therefore, the algorithm must converge to the global optimal solution in a finite num-ber of iterations.
 Note that in Step 5, we only need find one dual variable satisfying Eq. (6). We need examine all the dual variables in the set  X  N only when no dual variable satisfies Eq. (6). It is fast to examine the dual variables in the set  X  N even if the number of the dual variables is large.
In the max margin optimization problem Eq. (2), only some of the constraints determine the optimal solution. We call these constraints active constraints. Other constraints are automatically met as long as these active constraints are valid. EMML algorithm uses this fact to solve the optimiza-tion problem by substantially reducing the number of the dual variables in Eq. (3).

In the recent literature, there are also other methods at-tempting to reduce the number of the constraints. Taskar et al. [19] reduce the number of the constraints by consider-ing the dual of the loss-augmented problem. However, the number of the constraints in their approach is still large for a large structured output space and a large training set. They do not use the fact that only some of the constraints are active in the optimization problem. Tsochantaridis et al. [21] also propose a cutting plane algorithm which finds a small set of active constraints. One issue of this algorithm is that it needs to compute the most violated constraint which would involve another optimization problem in the output space. In EMML, instead of selecting the most violated con-straint, we arbitrarily select a constraint which violates the optimality condition of the optimization problem. Thus, the selection of the constraint does not involve any optimization problem. Therefore, EMML is much more efficient in learn-ing with a much faster convergence rate.
The solution to the Lagrange dual problem makes it pos-sible to capture the semantic relationships among different data modalities. We show that the developed EMML frame-work can be used to solve for the general multimodal data mining problem in all the scenarios. Specifically, given a training data set, we immediately obtain the direct rela-tionship between the VRep space and the word space using the EMML framework in Algorithm 1. Given this obtained direct relationship, we show below that all the multimodal data mining scenarios concerned in this paper can be facili-tated.
Image annotation refers to generating annotation words for a given image. First we partition the test image into blocks and compute the feature vector in the feature space for each block. We then compute the similarity between feature vectors and the VReps in terms of the distance. We return the top n most-relevant VReps. For each VRep, we compute the score between this VRep and each word as the function f in Eq. (1). Thus, for each of the top n most relevant VReps, we have the ranking-list of words in terms ofthescore. Wethenmergethese n ranking-lists and sort them to obtain the overall ranking-list of the whole word space. Finally, we return the top m words as the annotation result.

In this approach, the score between the VReps and the words can be computed in advance. Thus, the computa-tion complexity of image annotation is only related to the number of the VReps. Under the assumption that all the im-ages in the image database follow the same distribution, the number of the VReps is independent of the database scale. Therefore, the computation complexity in this approach is O (1) which is independent of the database scale.
Word query refers to generating corresponding images in response to a query word. For a given word input, we com-pute the score between each VRep and the word as the func-tion f in Eq. (1). Thus, we return the top n most relevant VReps. Since for each VRep, we compute the similarity between this VRep and each image in the image database in terms of the distance, for each of those top n most rele-Figure 2: A pair of embryo images corresponding to the same gene in the two different stages vant VReps, we have the ranking-list of images in terms of the distance. Then we merge these n ranking-lists and sort them to obtain the overall ranking-list in the image space. Finally, we return the top m images as the query result.
For each VRep, the similarity between this VRep and each image in the image database can be computed in advance. Similar to the analysis in Sec. 5.1, the computation com-plexity is only related to the number of the VReps, which is O (1).
Image retrieval refers to generating semantically similar images to a query image. Given a query image, we annotate it using the procedure in Sec. 5.1. In the image database, for each annotation word j there are a subset of images S in which this annotation word appears. We then have the union set S =  X  j S j for all the annotation words of the query image.

On the other hand, for each annotation word j of the query image, the word query procedure in Sec. 5.2 is used to obtain the related sorted image subset T j from the image database. We then merge these subsets T j to form the sorted image set T in terms of their scores. The final image retrieval result is R = S  X  T .

In this approach, the synergy between the image space and the word space is exploited to reduce the semantic gap based on the developed learning approach. Since the complexity of the retrieval methods in Secs. 5.1 and 5.2 are both O (1), and since these retrievals are only returned for the top few items, respectively, finding the intersection or the union is O (1). Consequently, the overall complexity is also O (1).
The general scenario of multimodal image retrieval is a query as a combination of a series of images and a series of words. Clearly, this retrieval is simply a linear combination of the retrievals in Secs. 5.2 and 5.3 by merging the retrievals together based on their corresponding scores. Since each individual retrieval is O (1), the overall retrieval is also O (1).
For a fruit fly embryo image database such as the Berke-ley Drosophila embryo image database which is used for our experimental evaluations, we have embryo images classified in advance into different stages of the embryo development with separate sets of textual words as annotation to those images in each of these stages. In general, images in different stages may or may not have the direct semantic correspon-dence (e.g., they all correspond to the same gene), not even speaking that images in different stages may necessarily ex-hibit any visual similarity. Figure 2 shows an example of a pair of embryo images at stages 9-10 (Figure 2(a)) and stages 13-16 (Figure 2(b)), respectively. They both correspond to Figure 3: An illustrative diagram for image-to-image across two stages inferencing the same gene in the two different stages 1 . However, it is clear that they exhibit a very large visual dissimilarity.
Consequently, it is not appropriate to use any pure visual feature based similarity retrieval method to identify such image-to-image correspondence across stages. Furthermore, we also expect to have the word-to-image and image-to-word inferencing capabilities across different stages, in addition to the image-to-image inferencing.

Given this consideration, this is exactly where the pro-posed approach for multimodal data mining can be applied to complement the existing pure retrieval based methods to identify such correspondence. Typically in such a fruit fly embryo image database, there are textual words for anno-tation to the images in each stage. These annotation words in one stage may or may not have the direct semantic cor-respondence to the images in another stage. However, since the data in all the stages are from the same fruit fly embryo image database, the textual annotation words between two different stages share a semantic relationship which can be obtained by a domain ontology.

In order to apply our approach to this across-stage infer-encing problem, we treat each stage as a separate multime-dia database, and map the across-stage inferencing problem to a retrieval based multimodal data mining problem by ap-plying the approach to the two stages such that we take the multimodal query as the data from one stage and pose the query to the data in the other stage for the retrieval based multimodal data mining. Figure 3 illustrates the diagram of the two stages (state i and state j where i = j ) image-to-image inferencing.

Clearly, in comparison with the retrieval based multi-modal data mining analyzed in the previous sections, the only additional complexity here in across-stage inferencing is the inferencing part using the domain ontology in the word
The Berkeley Drosophila embryo image database is given in such a way that images from several real stages are mixed together to be considered as one  X  X tage X . Thus, stages 9-10 are considered as one stage, and so are stages 13-16. space. Typically this ontology is small in scale. In fact, in our evaluations for the Berkeley Drosophila embryo image database, this ontology is handcrafted and is implemented as a look-up table for word matching through an efficient hashing function. Thus, this part of the computation may be ignored. Consequently, the complexity of the across-stage inferencing based multimodal data mining is the same as that of the retrieval based multimodal data mining which is independent of database scale.
While EMML is a general learning framework, and it can also be applied to solve for a general multimodal data min-ing problem in any application domains, for the evaluation purpose, we apply it to the Berkeley Drosophila embryo im-age database [1] for the multimodal data mining task de-fined in this paper. We evaluate this approach X  X  perfor-mance using this database for both the retrieval based and the across-stage inferencing based multimodal data mining scenarios. We compare this approach with a state-of-the-art multimodal data mining method MBRM [11] for the mining performance.

In this image database, there are in total 16 stages of the embryo images archived in six different folders with each folder containing two to four real stages of the images; there are in total 36,628 images and 227 words in all the six folders; not all the images have annotation words. For the retrieval based multimodal data mining evaluations, we use the fifth folder as the multimedia database, which corresponds to stages 11 and 12. There are about 5,500 images that have annotation words and there are 64 annotation words in this folder. We split the whole folder X  X  images into two parts (one third and two thirds), with the two thirds used in the train-ing and the one third used in the evaluation testing. For the across-stage inferencing based multimodal data mining evaluations, we use the fourth and the fifth folders for the two stages inferencing evaluations, and use the third, the fourth and the fifth folders for the three stages inferencing evaluations. Consequently, each folder here is considered as a  X  X tage X  in the across-stage inferencing based multimodal data mining evaluations. In each of the inferencing scenar-ios, we use the same split as we do in the retrieval based multimodal data mining evaluations for training and test-ing.

In order to facilitate the across-stage inferencing capabil-ities, we handcraft the ontology of the words involved in the evaluations. This is simply implemented as a simple look-up table indexed by an efficient hashing function. For example, cardiac mesoderm primordium in the fourth folder is considered as the same as circulatory system in the fifth folder. With this simple ontology and word matching, the proposed approach may be well applied to this across-stage inferencing problem for the multimodal data mining.
The EMML algorithm is applied to obtain the model pa-rameters. In the figures below, the horizonal axis denotes the number of the top retrieval results. We investigate the performance from top 2 to top 50 retrieval results. Fig-ure 4 reports the precisions and recalls averaged over 1648 queries for image annotation in comparison with MBRM model where the solid lines are for precisions and the dashed lines are for recalls. Similarly, Figure 5 reports the precisions and recalls averaged over 64 queries for word query in com-parison with MBRM model. Figure 6 reports the precisions Figure 4: Precisions and Recalls of image annotation between EMML and MBRM (the solid lines are for precisions and the dashed lines are for recalls) and recalls averaged over 1648 queries for image retrieval in comparison with MBRM model.

For the 2-stage inferencing, Figure 7 reports the precisions and recalls averaged over 1648 queries for image-to-word in-ferencingincomparisonwithMBRMmodel,andFigure8 reports the precisions and recalls averaged over 64 queries for word-to-image inferencing in comparison with MBRM model. Figure 9 reports the precisions and recalls averaged over 1648 queries for image-to-image inferencing in compari-son with MBRM model. Finally, for the 3-stage inferencing, Figure 10 reports precisions and recalls averaged over 1100 queries for image-to-image inferencing in comparison with MBRM model.
 In summary, there is no single winner for all the cases. Overall, EMML outperforms MBRM substantially in the scenarios of word query and image retrieval, and slightly in the scenario of 2-stage word-to-image inferencing and 3-stage image-to-image inferencing. On the other hand, MBRM has a slight better performance than EMML in the scenario of 2-stage image-to-word inferencing. For all other scenarios the two methods have a comparable performance.
In order to demonstrate the strong scalability of EMML approach to multimodal data mining, we take image anno-tation as a case study and compare the scalability between EMML and MBRM. We randomly select three subsets of the embryo image database in different scales (50, 100, 150 im-ages, respectively), and apply both methods to the subsets to measure the query response time. The query response time is obtained by taking the average response time over 1648 queries. Since EMML is implemented in MATLAB environment and MBRM is implemented in C in Linux en-vironment, to ensure a fair com parison, we repo rt the scala-bility as the relative ratio of a response time to the baseline response time for the respective methods. Here the baseline response time is the response time to the smallest scale sub-set (i.e., 50 images). Table 1 documents the scalability com-parison. Clearly, MBRM exhibits a linear scalability w.r.t the database size while that of EMML is constant. This is consistent with the scalability analysis in Sec. 5.
In order to verify the fast learning advantage of EMML in comparison with the existing max margin based learn-ing literature, we have implemented one of the most re-cently proposed max margin learning methods by Taskar et al. [19]. For the reference purpose, in this paper we call this method as TCKG. We have applied both EMML and TCKG to a small data set randomly selected from the whole Berkeley embryo database, consisting of 110 images along with their annotation words. The reason we use this small data set for the comparison is that we have found that in MATLAB platform TCKG immediately runs out of mem-ory when the data set is larger, due to the large number of the constraints, which is typical for the existing max margin learning methods. Under the environment of 2.2GHz CPU and 1GB memory, TCKG takes about 14 hours to complete the learning for such a small data set while EMML only takes about 10 minutes. We have examined the number of the constraints reduced in both methods during their exe-cutions for this data set. EMML has reduced the number of the constraints in a factor of 70 times more than that reduced by TCKG. This explains why EMML is about 70 times faster than TCKG in learning for this data set.
We have developed a new max margin learning framework  X  the enhanced max margin learning (EMML), and applied it to developing an effective and efficient multimodal data mining solution. EMML attempts to find a small set of active constraints, and thus is more efficient in learning than the existing max margin learning literature. Consequently, it has a much faster convergence rate which is verified in empirical evaluations. The multimodal data mining solution based on EMML is highly scalable in the sense that the query response time is independent of the database scale. This advantage is also supported through the complexity analysis as well as empirical evaluations. While EMML is a general learning framework and can be used for general multimodal data mining, for the evaluation purpose, we have applied it to the Berkeley Drosophila embryo image database and have reported the evaluations against a state-of-the-art multimodal data mining method.
This work is supported in part by NSF (IIS-0535162, EF-0331657), AFRL (FA8750-05-2-0284), AFOSR (FA9550-06-1-0327), and a CAREER Award to EPX by the National Science Founda tion under G rant No. DBI-054659. [1] http://www.fruitfly.org/ . [2] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden [3] K.Barnard,P.Duygulu,D.Forsyth,N.deFreitas, Figure 5: Precisions and Recalls of word query be-tween EMML and MBRM Figure 6: Precisions and Recalls of image retrieval between EMML and MBRM Figure 7: Precisions and Recalls of 2-stage image to word inferencing between EMML and MBRM Figure 8: Precisions and Recalls of 2-stage word to image inferencing between EMML and MBRM Figure 9: Precisions and Recalls of 2-stage image to image inferencing between EMML and MBRM Figure 10: Precisions and Recalls of 3-stage image to image inferencing between EMML and MBRM [4] D. Blei and M. Jordan. Modeling annotated data. In [5] S. Boyd and L. Vandenberghe. Convex Optimization . [6] U. Brefeld and T. Scheffer. Semi-supervised learning [7] E. Chang, K. Goh, G. Sychay, and G. Wu. Cbsa: [8] W.Chu,Z.Ghahramani,andD.L.Wild.Agraphical [9] R.Datta,W.Ge,J.Li,andJ.Z.Wang.Toward [10] P. Duygulu, K. Barnard, N. de Freitas, and [11] S. L. Feng, R. Manmatha, and V. Lavrenko. Multiple [12] Y. Freund and R. E. Schapire. Large margin [13] H. D. III and D. Marcu. Learning as search [14] J. Lafferty, A. McCallum, and F. Pereira. Conditional [15] A. McCallum, D. Freitag, and F. Pereira. Maximum [16] E. Osuna, R. Freund, and F. Girosi. An improved [17] J.-Y. Pan, H.-J. Yang, C. Faloutsos, and P. Duygulu. [18] A. W. M. Smeulders, M. Worring, S. Santini, [19] B. Taskar, V. Chatalbashev, D. Koller, and [20] B. Taskar, C. Guestrin, and D. Koller. Max-margin [21] I. Tsochantaridis, T. Hofmann, T. Joachims, and [22] V. N. Vapnik. The nature of statistical learning theory . [23] Y. Wu, E. Y. Chang, and B. L. Tseng. Multimodal
