 Many time series problems involve the task of predicting a sequence of future values using only the values observed in the past. Examples of this task, which is known as multistep-ahead time series prediction [1], include predicting the time series for crop yield, stock prices, traffic volume, and el ectrical power consumption. By knowing the sequence of future values, we may derive interesting properties of the time series such as its projected amplitude, variability, onset period, and frequency of abnormally high or low values. For example, multistep-ahead time series prediction allows us to forecast the growing period of corn for next year, the maximum and minimum temperature for next month, the frequency of El-Nino events in the next decade, etc. 
A typical approach to solve this problem is to construct a single model from historical values of the time series and then applies the model step by step to predict its future values. This approach is known as multi-stage prediction . Since it uses the predicted values from the past, it can be shown empirically that multi-stage prediction is susceptible to the error accumulation problem, i.e., errors committed in the past are propagated into future predictions. 
This paper considers two alternative approaches for multistep-ahead time series prediction. The first approach, known as independent value prediction , builds a approach, known as parameter prediction , fits a parametric function to the time series and builds regression models to predict the parameters of the function. 
We implement all three prediction approaches using multiple linear regression [2], recurrent neural networks [3], and a hybrid of hidden Markov model with multiple linear regression [7] as the underlying regression methods. The advantages and disadvantages of each prediction approach are analyzed in terms of their error accumulation, smoothness of prediction, and learning difficulty. A time series is a sequence of observations in which each observation t x is recorded at future values, h t t X + + 1 , given its p past observations, t p t X 1 +  X  . 2.1 Regression Methods This section presents the regression methods used for modeling the time series. 2.1.1 Multiple Linear Regression (MLR) The MLR model, which is also called the AR model, is given by the equation: the least square method by minimizing the sum of squared error, SSE , of the training window. 2.1.2 Recurrent Neural Networks(RNN) RNN has been successfully applied to noisy an d non-stationary time series prediction. In RNN, the temporal relationship of the time series is explicitly modeled using feedback connections [3] to the internal nodes (known as hidden units). An RNN the Elman back propagation network [4]. The weights of the network are then learning algorithm. 2.1.3 Hybrid HMM/MLR Model Hybrid HMM/MLR model is an extension of traditional hidden Markov model applied to regression analysis [7]. This method is an effective way for modeling piecewise stationary time series, where the observed values are assumed to be value t x depends only on the current state where } ,..., , { We use the regression function produced by MLR in our experiments. The hybrid HMM/MLR model is trained by maximizing the following likelihood function: A brute force method for maximizing the likelihood function requires a complexity of procedure is based on the well-known ex pectation-maximization (EM) algorithm. 2.2 Prediction Approaches the time series using a sliding window of length p + h (see Figure 1). Each instance of corresponds to the remaining h values of the window. For example, the first record of 2.2.1 Multi-stage Prediction this approach, it is sufficient to construct a single model for making the prediction. 2.2.2 Independent Value Prediction model. Given the initial data set shown in Table 1, we first create h training sets, each 2.2.3 Parameter Prediction equivalent problem of predicting ( d +1) parameters. For each record in Table 1, we fit test sequence is reconstructed by substituting the predicted parameters into the of parametric functions, we use polynomial functions in our experiments. 2.3 Model Selection The parameters for our prediction approaches include the order of regression model p , prediction). The size of the prediction window h is domain dependent and depends on the nature of the application. We use Akaike X  X  final prediction error (FPE) [8] as our criterion for determining the right order for p in the MLR model. used in parameter prediction. To determine the correct order for RNN, we employ the method described by Kennel in [5]. Let p X denote as an instance of the training data and ) ( n p X denote its nearest neighbor. The pair is declared as false nearest neighbors if distance between a pair of observations). Our goal is to choose a value for p such that the number of false nearest neighbors is close to zero. and synthetic datasets. The real datasets are obtained from the UCI Machine Learning Repository [9] and the Time Series Data Library [6]. Our experiments were conducted on a Pentium 4 machine with 3GHz CPU and 1GB of RAM. 3.1 Evaluation Metric predicted value, and recorded in our experimental results are obtained using ten-fold cross validation. difference in RMSE to determine whether one approach wins or loses against another determine whether the observed difference in RMSE is statistically significant. To do we compute their T-statistic: degrees of freedom. Under the null hypothesis that the two prediction approaches are comparable in performance, we expect the value of t should be close to zero. From the computed value for t , we estimate the p-value of the difference, which corresponds to the probability of rejecting the null hypothesis. We say the difference in performance 3.2 Error Accumulation decomposition for squared loss functions. Consider a time series generated by the x t + h . Furthermore, let The MSE at each step can be decomposed in to the following three components [10]: 
The first term represents the squared bias of the model; the second term represents the variance of the model; while the third term represents the variability due to noise. The next example illustrates the error accumulation problem for the noise term. coefficients a 1 and a 2 using MLR. For the multi-stage approach, we can show that: The preceding formula shows the accumulation of errors due to noise for multi-stage prediction as the prediction step increases. For independent value prediction: Assuming that the coefficients for x t and x t-1 can be accurately estimated by MLR, the noise terms for multi-stage and independent value prediction are identical.
 The preceding example illustrates that error accumulation due to noise is unavoidable, 50. To ensure there is sufficient bias in the model, we set p = 1. The bias and variance of the models are estimated by generating 500 bootstrap replicates of the training set D the test sequence to obtain 500 estimated values ( empirical bias is computed by taking the average value of the 500 predictions ( illustrate the bias and variance for multi-stage and independent value predictions (using MLR and a hybrid HMM/MLR as the underlying regression methods). Both figures show that the bias and variance for multi-stage prediction grows steadily with increasing time steps, whereas the bias and variance for independent value prediction do not appear to be propagated into future predictions. 
Therefore, error accumulation is a major problem in multi-stage prediction, parameter prediction because the models for predicting different parameters are built independently (similar to independent value prediction). 3.3 Learning Difficulty parameter prediction. Model building is therefore more expensive for independent value and parameter prediction approaches compared to the multi-stage approach. 
The model to be learnt by independen t value prediction also becomes more complex with increasing time steps. To illustrate this, let f denote the true model that generates the data , i.e., ) ( 1  X  input variables and y i = x p+i denote the h output variables: regression method is very flexible, learning the appropriate model for each time step depends on how easy it is to find the appropriate function that fits the output vector. using h=12 and p=12 . For parameter prediction, we use a polynomial function to fit the output vector and vary the degree of the polynomial from 0 to 11. We then employ MLR to predict the parameters of the polynomial. The bottom diagram of Figure 4 shows a comparison between the RMSE of parameter prediction against independent value prediction as the degree of the polynomial function is varied. Observe that the RMSE for parameter prediction drops dramatically when the polynomial degree increases to 3 and decr eases slowly thereafter. This result suggests that it is sufficient to fit a polynomial of degree 4 to the output vector and achieves comparable accuracy as independent value prediction (which must construct 12 regression models). 3.4 Smoothness of Prediction Another factor to consider is the influen ce of noise on the prediction approaches. To smooth out the time series to its mean value after p time steps. Such smoothing effect is not present in independent value prediction, which makes spurious predictions around the mean, because the prediction at each time step is modeled independently. independently, the smoothness of the time series is guaranteed by the parametric function used to fit the output vector. 3.5 A General Comparison Finally, we apply the three prediction approach es to 21 real data sets to compare their relative performance. The RMSE value for each data set is obtained by 10-fold cross validation. The size of the prediction window is set to h=24. Table 2 summarizes the RMSE for the three prediction approaches using MLR as the underlying regression method. Their relative performance is summarized in Table 3 in terms of the number of wins, draws and losses. We also test th e significance of the difference using paired t-significance test. The result shows that the observed difference between the RMSE performance of parameter prediction is significantly worse than independent value function, which have nonlinear relationships with the time series values. milk 0.0733 0.0705 0.0918 Temp. 0.2776 0.2959 0.2936 PET 0.0419 0.0414 0.0619 PREC 0.0310 0.0317 0.0534
Solar 0.1218 0.1236 0.2132 appb 0.2974 0.3804 0.3803 appd 0.2152 0.2395 0.2766 appf 0.3147 0.2445 0.2991 appg 0.8642 0.9343 0.9218 deaths 0.7309 0.5560 0.5633 lead 0.4195 0.4207 0.4206 sales 0.3187 0.3637 0.3637 wine 0.2738 0.2902 0.3209 seriesc 0.9359 0.9845 0.9845 odono 0.4226 0.4731 0.4712 qbirth 0.5450 0.4793 0.5231 Bond2 0.5226 0.5886 0.5884
Daily 0.2006 0.2137 0.2137 food 0.1995 0.1929 0.1950 treerin 0.8929 0.8807 0.8818 pork 0.9462 0.7948 0.7918 Observe that the independent value and parameter prediction approaches perform significantly better than multi-stage prediction (p &lt; 0.05). For multi-stage prediction, the RMSE for RNN is higher than the RMSE of MLR in 10 out of 21 data sets, which suggests the possibility of model overfitting when using a flexible regression method prediction with RNN outperforms all the prediction approaches using MLR and 12 0.01diff 10-6-5 14-2-5 8-12-1 T value 0.1496 0.8761 2.7299 
P value 0.8826 0.3914 0.0129 data sets in which parameter prediction with RNN outperforms all the prediction approaches using MLR. This result suggests that, using nonlinear regression methods such as RNN, the independent value and parameter prediction approaches may achieve better performance than multi-stage prediction. Moreover, for parameter prediction, most of the data sets require d &lt; 5, which makes it more efficient to build compared to independent value prediction (which requires building h = 24 models). In this paper, we conduct an empirical study on three prediction approaches for solving multistep-ahead time series prediction problems. The tradeoffs among these approaches are studied using both real and synthetic data sets. Our experimental results show that multi-stage prediction tends to suffer from error accumulation prediction is less sus ceptible to this problem becau se its predictions are made independently at each time step. However, it has difficulty in learning the true model because the function to be modeled becomes more complex with increasing time entire output sequence while alleviating the error accumulation problem by making prediction when the number of parameters to be fitted is small. However, finding the Finally, we observe successful applications of both independent value and parameter prediction approaches when applied to real data sets using RNN. 
