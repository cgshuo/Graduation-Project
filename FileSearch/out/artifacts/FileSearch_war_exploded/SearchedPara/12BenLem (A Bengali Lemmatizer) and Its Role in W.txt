 Natural language processing (NLP) for inflectional languages has to encounter an additional challenge (as opposed to languages having simpler morphology, e.g., En-glish) related to handling of the morphological variations of root words. Major Indic languages (like Hindi and Bengali) fall under the class of languages that are highly inflected in nature. For example, in Bengali about 70% of words are inflected, and it is found that a particular root (e.g.  X   X /karaa) can produce up to 90 morphological variants [Dash 2015]. In general, 20 or more inflections from a root word are found often in Bengali [Loponen et al. 2013]. The high level of inflections results in problems for developing many NLP tools, for instance, word sense disambiguation (WSD). Im-plementation of traditional dictionary-based WSD approaches (e.g., Lesk [1986] and Kilgarriff and Rosenzweig [2000]) is difficult for any such inflected language particularly because of the presence of word-level inflections. This article addresses this problem, taking Bengali (the second most popular South Asian language) as a reference.
Lemmatization is the process of determining the lemma for a given word. In morphol-ogy and lexicography, a lemma (plural lemmas or lemmata) is the canonical/dictionary form. Surprisingly, research on lemmatization received little attention from the com-putational linguistics community. This may be due to the fact that in English, for which a substantial amount of research has been conducted, the presence of the morpholog-ical variants of root words does not pose much of a problem, and if there is a need of normalizing the word forms, the purpose is well served by using stemming algorithms. Researchers working on Indic languages mostly followed the way in which English is processed, and as a result, no good lemmatization algorithm is available for such lan-guages. Previous studies on Indic languages put stress on developing stemmers, mostly in the context of information retrieval [Majumder et al. 2007; Dolamic and Savoy 2010; Paik and Parui 2011; Paik et al. 2011; Ganguly et al. 2012]. Lemmatization is similar to stemming, but the main difference between them is that a stemmer does not con-sider the context of the target word, and hence cannot distinguish between the cases where a surface word form has different lemmas depending on varying parts of speech in different contexts. The context may be a sentence, a subsentence, a paragraph, and so on. For example, the word  X   X /kara means  X  X and X  when it is used as noun and  X  X o do X  when used as verb. So in the first case,  X   X /kara is the correct lemma, and in the second case, the appropriate lemma is  X   X /karaa. Another difference is that stem is the common portion of all the variant forms and may have no meaning, but a lemma must be a meaningful word and may not be the common portion of all the variant word forms.

Previous works on lemmatizers can be grouped into two classes: rule-based [Koskenniemi 1984; Plisson et al. 2004] and statistical [Wicentowski and Yarowsky 2002; Lind  X  en 2008; Toutanova and Cherry 2009; Loponen and J  X  arvelin 2010; Gesmundo and Samard  X  zi  X  c 2012; M  X  uller et al. 2015]. The rule-based methods need a set of language specific rules to handle the morphological variations of the concerned language. The statistical methods are basically supervised in nature as they make use of a training corpus consisting of morphologically analyzed tokens. For example, the works by Loponen and J  X  arvelin [2010], Gesmundo and Samard  X  zi  X  c [2012], and M  X  uller et al. [2015] used corpora of token-lemma pairs. All these studies were carried out on languages like Finnish, Swedish, and English but not on any South Asian lan-guage, as the required resources (rules or annotated corpus) are not available for any such language. Many researchers attempted rule-based/automata-driven morpholog-ical analysis in Bengali [Bhattacharya et al. 2005; Dasgupta and Ng 2007; Faridee et al. 2009; Sarkar and Bandyopadhyay 2012a, 2012b; Senapati and Garain 2012]. In Faridee et al. [2009], their morphological analyzer, for an input surface word, produces its root along with the other linguistic information. The major disadvantages of their approach are (i) only the standard colloquial Bengali words are supported, (ii) the con-text of the target word is not taken into account, and (iii) lemmas chosen for verbs are not traditional dictionary root words and, therefore, cannot be mapped to standard lexical resources. The work by Loponen et al. [2013] mentions a Bengali lemmatizer (i.e., GRALE) for information retrieval purpose, but concrete algorithmic details are not provided in the paper. Recently, Bhattacharyya et al. [2014] proposed a trie-based lemmatization method for Indic languages. This method initially builds a trie (taking words from WordNet), which is then searched to produce a set of potential root words for a surface word. In cases where there is more than one candidate, some heuristics are used to rank the candidate root words. The method does not consider the context of the target word, and the evaluation method considers that a surface word is correctly lemmatized if the appropriate lemma occurs within the top 10 candidates. Apart from the above-mentioned studies, there is hardly any work found in the literature that directly addresses the problems of lemmatization in South Asian languages including Bengali, indicating a big gap in the area of NLP for Indic languages.
 This article aims to fill up the gap by developing BenLem, a lemmatizer for Bengali. For any language, surface words are formed through some transformations of their respective lemmas. However, these transformations can be hardly defined by a single rule. For instance, a surface word can be formed from its lemma by following one of the four cases: (i) the surface word can be obtained by adding a valid suffix to its lemma (e.g.,  X   X /karaar =  X   X /karaa +  X   X /ra); (ii) subtraction of a valid suffix from the lemma may result in the surface word (e.g.,  X   X /kara =  X   X /karaa - X   X /aa); (iii) unlike any direct addition or subtraction, their combination (at first, subtraction and then addi-tion) may produce the surface word (e.g.,  X   X /karchhen = [ X   X /karaa - X   X /aa] +  X   X /chhen); and (iv) none of the above transformations works (e.g.  X   X /abhyaas  X   X   X /abhyeser,  X   X /paakaa  X   X   X /peke).

In designing a lemmatizer, if the above transformations can be captured in an unam-biguous manner, then lemmas can be found by applying corresponding reverse trans-formations on the surface words. BenLem works on this philosophy and additionally considers contextual information like part of speech and the sense of the surface word. BenLem has been evaluated on a lemma-tagged corpus of 18 news articles (taken from the FIRE Bengali News Corpus [Majumder et al. 2010] 1 ) consisting of 3,342 surface words (this number excludes proper nouns). Next, the role of BenLem has been in-vestigated for Bengali WSD. As there is hardly any work on Bengali WSD, different Lesk-based [Lesk 1986] basic WSD systems are initially developed for Bengali. A sense-tagged corpus is also developed for Bengali. This dataset consists of 667 instances of 10 highly polysemous words in Bengali. 2 Evaluation of the WSD systems on this corpus shows that BenLem improves efficiency of WSD by a statistically significant margin. Let w be a surface word to be lemmatized. Two operations are defined on w as follows:  X  CON ( w ) denotes the bag of words in the current context of w .  X  POS ( w ) denotes the part of speech of the word w in the current context.
BenLem requires two resources: (i) a valid suffix list of the language and (ii) a dictionary.

Bengali Suffix List : For preparing the suffix list in Bengali, we followed the work by Paik and Parui [2011] where the set of all suffixes of length n ( n = 1 , 2 ,... )is generated from the lexicon (of size more than 400K as available from the FIRE Bengali News Corpus) by  X  X rouping words that share the same suffix and the number of words in a group becomes the frequency of the corresponding suffix. A suffix is called a potential suffix if its frequency in the lexicon is larger than a certain cut-off threshold. X  The authors showed that suffixes selected purely based on their frequencies in a lexicon have high recall but very low precision. Although most of the valid suffixes of the language concerned belong to the selected set, several invalid suffixes are also part of this set. So, we put manual effort (one linguist was involved) to remove the invalid suffixes. In our experiment, S contains 264 entries. The reason behind obtaining the list of valid suffixes in this way is that although there may be some isolated lexical resources, having partial lists of Bengali suffixes but no standard as well as complete catalogue is readily available. Traditional grammars of Bengali also does not provide the list of all the potential suffixes needed to care about.

Bengali Dictionary : We used the Bengali dictionary available from the University of Chicago 3 . There are 47,189 distinct headwords in the dictionary. We have orga-nized the dictionary in the following format. Let D ={ d 1 , d 2 ,..., d n } be the set of all headwords present in the dictionary. The structure of each headword d i is as follows: &lt; a particular sense and, hence, treated as a sense bag (i.e., a bag of words representing a particular sense). The following operations are defined on this dictionary.  X  POS dic ( d i ) returns the set of all the parts of speech that d i can have. nary word d i .  X  O v erlapScore ( d i , c j ,w ) gives the number of words common between c j of d i and
CON ( w )if POS ( w )  X  POS sense ( d i , c j ) or 0, otherwise.  X  MaxO v erlapScore ( d i ,w ) is the maximum among the numbers returned by Overlap-Score ( d i , c j ,w )  X  j .

BenLem algorithm makes use of one another important component, namely a dis-tance measure as explained below.

Distance Measure :If w 1 and w 2 are two words, then Dist ( w 1 ,w 2 ) denotes the string distance between w 1 and w 2 and to define the function, Dist , we have selected the dis-tance measure used by Majumder et al. [2007], which rewards long matching prefixes and penalizes an early mismatch as follows. Let the length of two strings X and Y be n + 1 (where n is a positive integer and if strings are of unequal length, null characters are added at the end of the shorter string to make them equal), and let m denote the  X  otherwise. The rationale behind choosing this distance measure is that for suffixing languages like Bengali, morphologically related words typically share a common prefix. So for the two words w 1 and w 2 ,if Dist ( w 1 ,w 2 ) is less than a threshold, then we can say that they may be morphologically related. Input: A surface word w , CON ( w )and POS ( w ).
 Output: The lemma l of w . (1) If  X  d i  X  D , such that w equals to d i and POS ( w )  X  POS dic ( d i ), then w is taken as (3) (a) For each d p  X  D p ,if  X  s x  X  S such that w = d p  X  s x ,then d p is put in a set D p . (4) When none of the previous transformations could produce the lemma, the last step We present examples of four surface words along with their contexts, which are suc-cessfully resolved by different steps of BenLem algorithm, respectively. (1) &lt; Target word:  X   X /chhele; Context:  X  /chhele X  &gt;  X  X   X /chhele is (2) &lt; Target word:  X   X /dale; Context:  X  /dale  X  &gt;  X  X   X /dala is the appropriate (3) &lt; Target word:  X   X /paarlen; Context:  X  /paarlen X  &gt;  X  X nthis (4) &lt; Target word:  X   X /egiYe; Context:  X  /saamne /egiYe /JaaoYaa X  &gt; The experiments consider many different aspects starting from developing an anno-tated dataset for evaluating the lemmatizer to the development of Lesk-based Bengali WSD systems and then preparing a sense annotated dataset for evaluating the WSD systems and finally, investigating the role of the lemmatizer for improving WSD accu-racy. Initially, the lemmatizer is evaluated and the different stages of the lemmatization algorithm (BenLem) are analyzed. BenLem makes use of a parameter called  X  used by the distance function Dist (Step 2 of the algorithm in Section 2.1). At first, we dis-cuss how the value of this parameter is found, and the subsequent sections discuss about the preparation of the datasets, the Bengali WSD systems and evaluation of the lemmatizer and the WSD systems. To determine the value of  X  , the threshold used by the distance function Dist ,weini-tially took the longest suffix from the suffix list. In our suffix list, the longest suffix is  X   X /itechhilaam (this suffix is added to action verbs to denote their past continu-ous forms and only applicable for first person as subject). Then we calculated the value of the Dist function between each surface word ending with  X   X /itechhilaam and its corresponding lemma and took the maximum distance. In this experiment, we found this value to be 7.96875 (between  X   X /khaaitechhilaam and  X   X /khaaoYaa), so we choose 7.97 as the value of  X  for our experiments. This relatively high threshold allows many (noisy) dictionary words but hardly misses a viable candidate. Many of these noisy candidates are later pruned based on the matching of parts of speech (refer to Step 2 of the algorithm in Section 2.1). An effort for optimizing  X  may speed up the lemmatization process. However, the process of empirically tuning the value of  X  can be investigated during the future extension of this study, as it requires a larger amount of annotated data. To evaluate the performance of the proposed lemmatization algorithm, we prepared a lemma-annotated dataset that is built using 18 newspaper articles selected randomly from the FIRE Bengali News corpus. Altogether, there are 6,314 space-separated to-kens, out of which 3,342 are either dictionary words or their morphological variations. The remaining tokens are either proper nouns or punctuation marks that were not con-sidered for lemmatization. All the surface words in this dataset are manually tagged with their parts of speech (in the given context). For the 3,342 tokens, the correspond-ing gold lemmas are also given. As only one linguist was involved for doing this POS and lemma annotation, we have not had any chance to measure the interannotator agreement.

To investigate the role of our lemmatizer for WSD, we have selected 10 highly polyse-mous words from the dictionary. Text pieces are selected from the FIRE Bengali News Corpus and the collection of Tagore X  X  short stories. 4 The chosen words, the number of senses for these words in the dictionary, the number of instances (frequency), sense entropy, and the number of the most frequent sense of each of the 10 words in the WSD test dataset are given in the Table I. The words for which disambiguation is attempted are manually lemmatized and sense tagged in the dataset. The sense entropy of the words is computed as follows. If a word w has n senses defined in the dictionary, then the probability p i of a sense c i of w is calculated as the ratio of the number of times w is used in sense c i to the total number of instances (frequency) of w in the dataset. Next, the summation n i = 1 ( p i  X  log 1 p For disambiguation of the 10 polysemous words as given in the Table I, we have chosen Simplified Lesk (SL) method [Kilgarriff and Rosenzweig 2000] and Lesk X  X  Dictionary-based Disambiguation (LDD) method [Manning and Sch  X  utze 1999]. Note that SL only measures the overlap between the context bag and each sense bag of the word to be disambiguated and returns the sense that has maximum overlap with the context bag, whereas LDD tries to expand the context bag by adding the dictionary definitions of the words present in the context. Next, it finds the intersection between this elabo-rated context bag and each sense bag of the target word and finally selects the sense with the maximum intersection score as the winner sense. To investigate the role of lemmatization in WSD, for each of the 10 selected words, we executed SL and LDD sep-arately once without any lemmatization and then after lemmatizing the words present in the sense bags and in the contexts of the 10 selected words. Furthermore, these experiments have been repeated for each word with the incorporation of the Bengali WordNet 5 to expand the sense definitions of the 10 words by adding the glosses taken from the WordNet. For a word, all of its dictionary senses are often not present in this WordNet. For the 10 words, we manually map the available WordNet senses to the similar senses in the dictionary. When the WordNet is used in WSD, a dictionary sense is expanded by adding the corresponding (if any) gloss from the WordNet. To evaluate the performance of the proposed lemmatization algorithm, we compute direct accuracy by measuring the ratio of the number of surface word tokens for which correct lemmas are produced to the total number of surface word tokens given as input to the lemmatizer. For evaluation of the WSD systems, we measure coverage , precision , recall, and F1-score of the systems [Navigli 2009]. The definitions of the above four measures are given below.

Let W ={ w 1 ,...,w n } be a WSD test set. For each word w i  X  W ,let Sense D ( w i )be the set of all senses of w i defined in the dictionary D . We define an  X  X nswer X  func-tion A gold , which returns the appropriate sense of a word in its present context, that A assigned by an automatic WSD system. 6 Here denotes the case where the automatic WSD system cannot assign a sense. Now, coverage C is defined as follows.
 Precision P of a WSD system is the fraction of correct answers given by the WSD system.

P = Recall R is the fraction of the number of correct answers given by the WSD system over the total number of answers to be given.

R = F1-score is the weighted harmonic mean of precision and recall . It is useful to judge the performance of a WSD system when coverage is less than 100%.
 BenLem has been tested on the POS and lemma-annotated dataset as described in Section 3.2. The container sentence is considered as the context for a word token for which the lemma is to be found. The evaluation result shows that BenLem gives 81 . 95% accuracy for 3,342 word tokens. The algorithm may need minimum one to maximum four steps to process the words for finding out their corresponding lemmas. The percentage of the word tokens resolved by each step of the algorithm are given in Table II. Accuracy of each step in finding the correct lemma is shown in the sec-ond row of the Table II. Initially, all words are passed through Step 1 , which finds the lemmas for 49 . 61% tokens and saves the additional processing effort needed in the subsequent steps for them. It shows that one important characteristic of the lan-guage (more specifically, the characteristics of the test set) is that only searching for a token in the dictionary and matching its POS with that of the dictionary word can decide lemmas for about half of the Bengali words. Since this step resolves the highest percentage of the total number of surface words, accuracy of this step is quite vital for the overall efficiency of the lemmatizer. Experimental result shows that Step 1 is 96 . 99% accurate. The errors in Step 1 occur, in Bengali, when a lemma is suffixed with a valid suffix and the resulting surface word is another lemma. For example,  X   X /kula +  X   X /taa =  X   X /kulataa. Both  X   X /kula and  X   X /kulataa are valid root words and  X   X /taa is a valid suffix in Bengali. If  X   X /kulataa is to be lemmatized by BenLem, it will always return  X   X /kulataa as the lemma that is wrong. We analyzed our dictionary and suffix list and found that there are 4 , 788 &lt; dictionary headword, valid suffix &gt; pairs such that when the dictionary headword is suffixed, another dic-tionary headword is produced. However, the high accuracy of Step 1 reveals that a good dictionary and an efficient POS tagger make the lemmatization task in Bengali easy.

The tokens for which Step 1 does not find any lemma but the set D p (in Step 2) is not empty are passed to the subsequent steps. Step 3(a) carries the next major load and finds lemmas for about 27 . 20% of the total words. This shows that the addition (or dele-tion) of a valid suffix to (from) a dictionary headword generates about one fourth of the inflected words in Bengali. This step produces 16 . 18% errors, which occur due to the sit-uation when two different lemmas are suffixed with two different valid suffixes to pro-duce same surface word. If this situation occurs, BenLem will always select the lemma that has the shorter distance with the surface word according to the distance measure Dist . For example,  X   X /kula +  X   X /taake =  X   X /kulataake =  X   X /kulataa +  X   X /ke. Both  X   X /taake and  X   X /ke are valid suffixes in Bengali. If  X   X /kulataake is given as input to BenLem,  X   X /kulataa will always be returned as the lemma, as it has a shorter distance with  X   X /kulataake than that with  X   X /kula. We searched in the FIRE Bengali corpus and found 5,299 distinct such surface words (about 1%), each of which can be produced by two different dictionary headwords suffixed with two different valid suffixes. When this processing is not adequate to find the lemma, Step 3(b) is invoked that explores whether deletion of a valid suffix (from a dictionary root) followed by addition of another valid suffix would result in the surface word. This step finds lemmas for about 12 . 26% of the total word tokens and produces 30 . 97% er-rors. The errors in this step come from the situation when the deletion of a valid suffix from a root word and then the addition of another valid suffix produce a surface word, but the root is not the appropriate lemma for the produced surface word. For example,  X   X /heraa is a root word and  X   X /heraa - X   X /aa +  X   X /e generates  X   X /here, but for the surface word  X   X /here, the appropriate lemma is  X   X /haaraa. Steps 3(a) and 3(b) give 83 . 82% and 69 . 03% accuracy, respectively, further revealing the importance of the list of valid suffixes for Bengali lemmatization. In cases where more than one headword in D p is found having the shortest distance with the target surface word, the smallest one among them according to the lexicographically sorted order is selected as the lemma.

Step 4 tries to find the lemmas for the remaining words (for which none of the pre-vious three steps could find the lemmas), using the sense of the word in question. About 4 . 39% word tokens are resolved by this step and for such words, the correspond-ing lemmas are found with 6 . 80% accuracy. This shows that finding the lemma of a word by understanding its sense in the context requires further investigation to im-prove the accuracy of Step 4. If there is more than one root word with the highest MaxO v erlapScore and that also has equal distance with the surface word, then the lexicographically smallest one is chosen as the lemma. The  X  X emaining X  column in the Table II gives the statistics of the words for which (i) either D p is empty in Step 2 or (ii) MaxO v erlapScore is zero for all the headwords of D p in Step 4. For those words, BenLem accepts a word form itself as the lemma; 34 . 86% of the lemmas found in this way are correct, which reveals that these correct lemmas are not covered by the dictio-nary used in the experiment, because if they were present in the dictionary, they would have been resolved by Step 1. 4.2.1. Effect of POS Tagger. To study the effect of POS tagging on lemmatization per-formance, BenLem is tested further on the lemmatization dataset (consisting of 18 newspaper articles from the FIRE Bengali News corpus having 6,314 word tokens) where all proper nouns and punctuation symbols have been manually tagged and an automatic POS tagger is used to tag the remaining 3,342 word tokens. The proper names and punctuations are manually tagged to avoid machine errors, as these tokens are not considered for the present evaluation of the lemmatizer. To configure a POS tagger for Bengali, we retrained the Stanford POS tagger 7 using datasets available from the Linguistic Data Consortium (LDC), the University of Pennsylvania; ICON 2007 NLP Tool Contest data for shared task competition on POS tagging and chunk-ing 8 ; and the Indian Institute of Technology, Kharagpur. In these datasets, there are a total 10,416 sentences that have 138,847 word tokens. The POS tagger used in this experiment is tested on the manually POS-annotated lemmatization dataset of total 6,314 word tokens, and it is found to be 79 . 18% accurate on that dataset. For the auto-matically POS-tagged 3,342 word tokens, the overall accuracy of BenLem is found to be 75 . 46%. The percentage of the word tokens resolved by each step of BenLem and step-wise accuracy in finding the lemmas are given in Table III. It is found that the results presented in Tables II and III are highly correlated. We have calculated the Pearson X  X  correlation coefficient between the percentage of word tokens resolved by Step 1 to Step 4 of BenLem using gold and automatic POS tags, and the value is found to be 0 . 9994. For the stepwise accuracy, the value of the above coefficient is 0 . 9844. There is one extra field in the Table III, that is, NNP/SYM. It is to be noted that 0 . 42% of the 3,342 word tokens are wrongly tagged as either proper nouns or punctuation symbols by the automatic POS tagger, and they remain unprocessed by BenLem. It has been noted that for 7 . 14% of these words, the word forms themselves are the lemmas. 4.2.2. Effect of  X  on Lemmatization Performance. A postexperiment analysis has been done to verify whether the current approach taken to determine the value of  X  (described in Section 3.1) is adequate or not. For this, we have conducted a set of experiments (on both the gold and the automatic POS-tagged lemmatization datasets) using different highest accuracy of 82 . 16%, whereas on the automatic POS-tagged dataset, 76 . 21% is the best accuracy obtained at  X  = 3. We further plot the (  X  vs. Accuracy) curves for the experiments on both gold and automatic POS-tagged dataset (given in the Figure 1). In the range of  X  = 1 to 3, both the curves are strictly increasing showing a positive correlation between  X  and the lemmatization accuracy. From  X  = 3to7 . 97, the curves are almost parallel to the x -axis, which indicates that changing the value of  X  affects the accuracy very little. This analysis shows that selecting a relatively higher value of  X  is acceptable. 4.2.3. Speed Analysis. We have analyzed the time requirement for each step of BenLem. Table IV shows the average running time (in milliseconds) for word tokens that are resolved by Steps 1 through 4. The specifications of the system in which the experiment has been carried out are as follows. Random Access Memory (RAM): 4GB; Processor: Intel(R) Core(TM)2 Duo; Frequency: 2.93GHz; Cache Size: 3,072KB. All the units given here express their standard meanings. Table IV shows that Steps 1 and 2 are quite fast, but Steps 3 and 4 are much more time-consuming. Analysis reveals that handling the suffix list is the cause behind the time requirement of Step 3. If the size of the suffix list is increased, more time will be required for completion of that step. However, we did not attempt to do any code optimization at this stage and represented all the dictionary headwords and valid suffixes in list structures but to speed up the algorithm, more efficient data structures can be used (may be trie representations for the dictionary headwords and valid suffixes), which will save the processing time of Steps 1 to 3. As Step 4 attempts to do sense matching, we are not surprised to note that it is taking about 2 seconds. Although it is difficult to reduce the amount of time taken by this step, it does not affect the overall processing time of the algorithm significantly, as only a small part of total word tokens are processed at this step. 4.2.4. Qualitative Error Analysis. The major reasons behind the errors made by BenLem are as follows: (i) in Bengali, there are many compound words like  X   X /aagrahaprakaasher,  X   X /abhaYabaaNiir, and so on, which Ben-Lem fails to handle because the dictionary does not cover all the compound words; (ii) some infrequent but valid suffixes in Bengali are not present in the suffix list (iii) the dictionary does not contain all possible parts of speech for the morphological variants of some headwords. For example, noun is the only possible part of speech of the headword  X   X /aain, but one of its morphologically derived forms  X   X /aaini is used as an adjective, which is not mentioned in the dictionary; and (iv) due to glob-alization, many English words like  X   X /result,  X   X /practice,  X   X /perform, and so on are being used frequently in Bengali and are gradually finding their places in the Bengali vocabulary list. Words that come from different languages cannot be resolved by BenLem (v) examples of  X   X /kulataa,  X   X /kulataake and  X   X /here (discussed in Section 4.2) also point out the reasons for errors in Steps 1, 3, and 3(b) of BenLem. Apart from all of the above reasons, the coverage of the dictionary and the accuracy of the POS tagger used for lemmatization are two very important factors that influence the overall performance of BenLem significantly. In order to explore the role of lemmatization in WSD, we conducted the following eight types of experiments. Two algorithms, namely SL and LDD, are used with or without using WordNet giving four variations. Each variation is further tested with or without using BenLem giving eight different experiments.  X  X D 1.1 and 1.2: SL without and with BenLem (no use of WordNet).  X  X D 2.1 and 2.2: SL, WordNet is used without and with BenLem.  X  X D 3.1 and 3.2: LDD without and with BenLem (no use of WordNet).  X  X D 4.1 and 4.2: LDD, WordNet is used without and with BenLem.

We have used BenLem to lemmatize the sense bags and the context bags of the 10 polysemous words (as in the Table I). For lemmatization, the retrained POS tagger (discussed in Section 4.2.1) is used to POS tag the words present in the sense bags and the context bags. In order to choose the context size for execution of SL and LDD methods, we have taken three-sentences context (one sentence before and after the focus sentence and the focus sentence itself), which is common in WSD. It is to be noted here that for the lemmatization experiment, the container sentence of the word to be lemmatized was considered as the context window. WSD results are presented in the Tables V to XIV. In each of those tables, the rows with IDs X.1 (X varies from 1 to 4) correspond to the WSD systems without BenLem and the rows with IDs X.2 correspond to the WSD systems with BenLem. There are 40 such pairs (four for each of the 10 words). If results in each such pair are compared, we see the usefulness of BenLem. For 22 (out of 40) pairs, BenLem improves coverage (statistically significant at p -value &lt; 0 . 01 in a two-tailed paired t-test); for 23 pairs, the recall is improved (statistically significant at p -value &lt; 0 . 01) upon using BenLem. The highest improve-ment in coverage is from 34 . 80% to 47 . 55% (Row IDs 1.1 vs. 1.2 in the Table V), whereas maximum improvement in recall is from 44 . 11% to 54 . 90% (Row IDs 4.1 vs. 4.2 in the Table V). Note that there are 204 instances of the word  X   X /karaa in the test set. An example is provided here to show why lemmatization is effective for WSD. Consider the context given below and the gold sense of the word  X   X /chalaa for that context.  X  X ontext:  X  , /haYa, thaakbe  X   X  X old Sense:  X  /hate /ghatte /thaakaa |  X   X   X /chaltei is an inflected form of  X   X /chalaa. Before lemmatization, there is no common word present between the context and the gold sense, but after lemmatization, we find three distinct words, namely  X   X /haoYaa,  X   X /ghataa, and  X   X /thaakaa, common between them.  X   X /haoYaa is the lemma of  X   X /haYa and  X   X /hate;  X   X /ghataaisthelemmaof X   X /ghatchhe and  X   X /ghatte; and  X   X /thaakaa is the lemma of  X   X /thaakbe. Hence, the score of the gold sense for the context becomes greater showing the importance of lemmatization for WSD.

In the Table XIV, use of lemmatization does not improve WSD results significantly (BenLem degrades recall in case of Row IDs 2.1 vs. 2.2 and improves same in case of Row IDs 3.1 vs. 3.2), as there are few (20) instances of the word  X   X /dharaa in the dataset, and the word has the highest number of senses (39 different senses) defined in the dictionary. For certain cases, we get 0% precision and recall values indicating that no common word is found between the context bag and the sense bags. This happens especially when the dictionary senses of a word is small in length. This problem is alleviated when we use WordNet, which can provide longer sense bags through glosses. For example, consider the Row IDs 1.1 and 1.2 vs. Row IDs 2.1 and 2.2 in the Table X. Here, when SL was executed without using WordNet, it produced 0% precision and recall. But the same method gives nonzero precision and recall values when Wordnet is used to add the glosses from it.

There are four instances (out of 40), where use of BenLem degrades the WSD per-formance. For Row IDs 2.1 vs. 2.2, and 4.1 vs. 4.2 in Table VII, Row IDs 2.1 vs. 2.2 in Table XII, and Row IDs 2.1 vs. 2.2 in Table XIV, the recall values are degraded because of BenLem. For recall, maximum degradation is from 17 . 07% to 2 . 44% (Row IDs 2.1 vs. 2.2 in Table XII). We have further investigated the reasons for such degradations. Presence of lemmatization errors is one major reason. Combining of the WordNet sense bags with the dictionary sense bags makes the resultant sense bags bigger. The WSD performance is degraded if many of the words in this bigger sense bag (though chances are less) are unfortunately affected (at the same time) by lemmatization errors. We are presenting a particular example below where doing lemmatization hurts WSD perfor-mance. Consider the context given below and the gold sense of the word  X   X /dharaa for this context and one other sense of  X   X /dharaa.  X  X ontext:  X   X  X old Sense:  X  /aatak /karaa |  X   X  X ther Sense:  X  /karaa /neoYaa |  X  Before lemmatization,  X   X /aatak is the only common word between the gold sense and the context, and there is no common word between the other sense and the con-text. So, according to SL method, gold sense is the winner sense as it has greater co-occurrence score with the context. After lemmatization, the common words between the gold sense and the context are  X   X /aatak and  X   X /karaa as  X   X /karaa is the lemma of the context word  X   X /kare. As  X   X /neoYaa is the lemma of  X   X /nen, hence after lemmatization  X   X /neoYaa and  X   X /karaa are the two common words between the context and the other sense of  X   X /dharaa. Thus, after lemmatization, both the gold sense and the other sense have the same number of overlapped words with the context bag resulting the failure of SL method to determine the winner sense.

In Bengali, some of the frequent words are sometimes present in the sense bags and in the context bags in various inflected forms. A few examples of those words and their multiple inflections are &lt; Root:  X   X /karaa; Inflections:  X   X /kare,  X   X /kara, ...&gt; , &lt; Root:  X   X /neoYaa; Inflections:  X   X /nin,  X   X /nen, ...&gt; , &lt; Root:  X   X /JaaoYaa; Inflections:  X   X /JeYe,  X   X /Jete, ...&gt; , and so on. They may or may not bear the principal meaning of the context, and it is very hard to determine from the context when they should be removed and when not. In the WSD algorithms where co-occurrence count between the sense bag and the context bag is considered as the score of a sense, it may happen that two different inflected forms of such words (e.g.,  X   X /neoYaa,  X   X /JaaoYaa) are present in the context but not bearing its principal meaning as well as present in the sense bag of a sense, which is not the gold sense for that context. Before lemmatization, those differently inflected forms will not add value to the score of that sense, but after lemmatization, they will get replaced by their unique lemma and add a count to the score of that sense, raising its chance to be a wrong winner.
Another significant byproduct of this research is the investigation of the role of the newly developed Bengali WordNet for WSD. Despite having a relatively low coverage, WordNet largely improves WSD results over the use of dictionary only. The present version of the WordNet contains 29,882 synsets. For the 10 words for which the current WSD systems have been tested, it is observed that the WordNet contains fewer senses compared to the senses present in the dictionary. For instance, the word  X   X /dharaa has 8 senses in the WordNet out of the 39 senses present in the dictionary. However, the glosses from the WordNet are found to be quite helpful for WSD compared to the sense definitions (and sometimes glosses) present in the dictionary. Therefore, although the current version of the Bengali WordNet is linguistically not very rich, its positive contribution is nicely demonstrated by the WSD results. Therefore, it can be expected that with the addition of more linguistic richness to the WordNet, we would be able to do better WSD in Bengali.

While designing BenLem, we have carefully kept the algorithm less dependent on language-based resources. Very low resource requirements and language-independent features would help the algorithm to be extended for other inflected languages, includ-ing several major Indic langauges. It is to be noted that the qualities of the resources used (e.g., coverage of the dictionary, accuracy of the POS tagger) have significant im-pact on the accuracy of our proposed lemmatization algorithm. Our immediate plan is to extend this lemmatizer for Hindi and thereby investigate the language-independent nature of the present lemmatization algorithm.

The current work does not attempt to lemmatize named entities and out-of-vocabulary words. We agree that lemmatization of proper names is indeed an important aspect and will probably help information retrieval for Indic languages in a significant way, as proper names play vital role in retrieving relevant documents. Therefore, we intend to address this issue (i.e., lemmatization of named entities) in the future ex-tension of this work. Research on lemmatization and WSD would also explore other methods especially a corpus-based unsupervised method like the ones already devel-oped for stemming [Majumder et al. 2007; Paik and Parui 2011; Paik et al. 2011]. Speed optimization issues can be further investigated, as mentioned in Section 4.2.3. Preparations of much larger annotated datasets for evaluating Bengali lemmatizers and WSD systems are also in progress.

