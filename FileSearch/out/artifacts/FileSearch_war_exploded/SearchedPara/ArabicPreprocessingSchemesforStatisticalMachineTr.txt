 Approaches to statistical machine translation (SMT) are rob ust when it comes to the choice of their in-put representation: the only requirement is consis-tenc y between training and evaluation. 1 This lea ves a wide range of possible preprocessing choices, even more so for morphologically rich languages such as Arabic. We use the term  X  X reprocessing X  to de-scribe various input modifications that can be ap-plied to raw training and evaluation texts for SMT to mak e them suitable for model training and decod-ing, including dif ferent kinds of tok enization, stem-ming, part-of-speech (POS) tagging and lemmatiza-tion. We refer to a specific kind of preprocessing as a  X  X cheme X  and dif ferentiate it from the  X  X ech-nique X  used to obtain it. Since we wish to study the effect of word-le vel preprocessing, we do not uti-lize any syntactic information. We define the word (and by extension its morphology) to be limited to written Modern Standard Arabic (MSA) strings sep-arated by white space, punctuation and numbers. Thus, some prepositional particles and conjunctions are considered part of the word morphology .
In this paper , we report on an extensi ve study of the effect on SMT quality of six preprocessing schemes 2 , applied to text disambiguated in three dif-ferent techniques and across a learning curv e. Our results are as follo ws: (a) for lar ge amounts of train-ing data, splitting off only proclitics performs best; (b) for small amount of training data, follo wing an English-lik e tok enization and using part-of-speech tags performs best; (c) suitable choice of preprocess-ing yields a significant increase in BLEU score if there is little training data and/or there is a change in genre between training and test data; (d) sophis-ticated morphological analysis and disambiguation help significantly in the absence of lar ge amounts of data.
 Section 2 presents pre vious rele vant research. Section 3 presents some rele vant background on Arabic linguistics to moti vate the schemes discussed in Section 4. Section 5 presents the tools and data sets used, along with the results of our experiments. Section 6 contains a discussion of the results. The anecdotal intuition in the field is that reduction of word sparsity often impro ves translation quality . This reduction can be achie ved by increasing train-ing data or via morphologically dri ven preprocess-ing (Goldw ater and McClosk y, 2005). Recent publi-cations on the effect of morphology on SMT quality focused on morphologically rich languages such as German (Nie X en and Ne y, 2004); Spanish, Catalan, and Serbian (Popo vi  X  c and Ne y, 2004); and Czech (Goldw ater and McClosk y, 2005). The y all studied the effects of various kinds of tok enization, lemma-tization and POS tagging and sho w a positi ve effect on SMT quality . Specifically considering Arabic, Lee (2004) investigated the use of automatic align-ment of POS tagged English and affix-stem seg-mented Arabic to determine appropriate tok eniza-tions. Her results sho w that morphological prepro-cessing helps, but only for the smaller corpora. As size increases, the benefits diminish. Our results are comparable to hers in terms of BLEU score and consistent in terms of conclusions. We extend on pre vious work by experimenting with a wider range of preprocessing schemes for Arabic, by studying the effect of morphological disambiguation (be yond POS tagging) on preprocessing schemes over learn-ing curv es, and by investigating the effect on dif fer -ent genres. Arabic is a morphologically comple x language with a lar ge set of morphological features. These features are realized using both concatenati ve (af fix es and stems) and templatic (root and patterns) morphology with a variety of morphological and phonological adjustments that appear in word orthography and in-teract with orthographic variations. Certain letters in Arabic script are often spelled inconsistently which leads to an increase in both sparsity (multiple forms of the same word) and ambiguity (same form corre-sponding to multiple words). For example, variants of Hamzated Alif, or are often written without their Hamza ( ): . Another example is the optional-ity of diacritics in Arabic script. We assume all of the text we are using is undiacritized.

Arabic has a set of attachable clitics to be dis-tinguished from inflectional features such as gender , number , person and voice. These clitics are written attached to the word and thus increase its ambiguity . We can classify three degrees of cliticization that are applicable in a strict order to a word base: At the deepest level, the BASE can have a def-inite article (Al+ the ) 3 or a member of the class of pronominal enclitics, +PR ON, (e.g. +hm their/them ). Ne xt comes the class of particle pro-clitics (PAR T+): l+ to/for , b+ by/with , k+ as/suc h and s+ will/futur e . Most shallo w is the class of con-junction proclitics (CONJ+): w+ and and f+ then .
These phenomena highlight two issues related to preprocessing: First, ambiguity in Arabic words is an important issue to address. To determine whether a clitic or feature should be split off or abstracted off requires that we determine that said feature is in-deed present in the word we are considering in con-text  X  not just that it is possible given an analyzer or, worse, because of regular expression matching. Secondly , once a specific analysis is determined, the process of splitting off or abstracting off a feature must be clear on what the form of the resulting word is to be. For example, the word two possible readings (among others) as their writ-ers or I wr ote them . Splitting off the pronominal clitic +hm without normalizing the t to p in the nom-inal reading leads to the coe xistence of two forms of the noun: ktbp and ktbt . This increased sparsity is only worsened by the fact that the second form is also the verbal form (thus increased ambiguity). A scheme is a specification of the form of prepro-cessed output; whereas a technique is the method used to create such output. We examine six dif ferent schemes and three techniques. 4.1 Pr epr ocessing Techniques The dif ferent techniques chosen illustrate three de-grees of linguistic kno wledge dependence. The first is very light and cheap. The second is more expen-sive, requiring the use of a morphological analyzer . And the third is yet more expensi ve than the second; it is a disambiguation system that requires an ana-lyzer and a disambiguated training corpus.
 ply greedy regular expression matching to mod-ify strings and/or split off prefix/suf fix substrings that look lik e clitics indicated by specific schemes. R
EGEX cannot be used with comple x schemes such as EN and MR (see Section 4.2).
 alyzer (Buckw alter , 2002), is used to obtain pos-sible word analyses. Using B AMA pre vents incor -rect greedy R EGEX matches. Since B AMA produces multiple analyses, we always select one in a consis-tent arbitrary manner (first in a sorted list of analy-ses).
 ambiguation for Arabic tool, is an off-the-shelf resource for Arabic disambiguation (Habash and Rambo w, 2005). M ADA selects among B AMA anal-yses using a combination of classifiers for 10 orthog-onal dimensions, including POS, number , gender , and pronominal clitics.

For B AMA and M ADA , applying a preprocess-ing scheme involv es mo ving features (as specified by the scheme) out of the chosen word analysis and regenerating the word without the split off features (Habash, 2004). The regeneration guarantees the normalization of the word form. 4.2 Pr epr ocessing Schemes Table 1 exemplifies the effect of the dif ferent schemes on the same sentence.
 cessing scheme. It is limited to splitting off punc-tuations and numbers from words and remo ving any diacritics that appear in the input. This scheme re-quires no disambiguation.
 off the class of conjunction clitics ( w+ and f+ ). D2 splits off the class of particles ( l+ , k+ , b+ and s+ ) beyond D1. Finally D3 splits off what D2 does in addition to the definite article ( Al+ ) and all pronom-inal clitics.
 into stem and affixi val morphemes.
 minimize dif ferences between Arabic and English. It decliticizes similarly to D3; howe ver, it uses lex-eme and English-lik e POS tags instead of the regen-erated word and it indicates the pro-dropped verb subject explicitly as a separate tok en. We use the phrase-based SMT system, Portage (Sa-dat et al., 2005). For training, Portage uses IBM word alignment models (models 1 and 2) trained in both directions to extract phrase tables. Maxi-mum phrase size used is 8. Trigram language mod-els are implemented using the SRILM toolkit (Stol-cke, 2002). Decoding weights are optimized using Och X  s algorithm (Och, 2003) to set weights for the four components of the log-linear model: language model, phrase translation model, distortion model, and word-length feature. The weights are optimized over the BLEU metric (Papineni et al., 2001). The Portage decoder , Canoe, is a dynamic-programming beam search algorithm, resembling the algorithm described in (K oehn, 2004a).

All of the training data we use is available from the Linguistic Data Consortium (LDC). We use an Arabic-English parallel corpus of about 5 million words for translation model training data. 4 We created the English language model from the En-glish side of the parallel corpus together with 116 million words from the English Giga word Corpus (LDC2005T12) and 128 million words from the En-glish side of the UN Parallel corpus (LDC2004E13). English preprocessing comprised down-casing, sep-arating punctuation from words and splitting off  X  X  X  X . Arabic preprocessing was varied using the pro-posed schemes and techniques. Decoding weight optimization was done on 200 sentences from the 2003 NIST MT evaluation test set. We used two dif-ferent test sets: (a) the 2004 NIST MT evaluation test set (M T 04) and (b) the 2005 NIST MT evalua-tion test set (M T 05). M T 04 is a mix of news, edito-rials and speeches, whereas M T 05, lik e the training data, is purely news. We use the evaluation metric BLEU-4 (Papineni et al., 2001).

We conducted all possible combinations of schemes and techniques discussed in Section 4 with dif ferent training corpus sizes: 1%, 10% and 100%. The results of the experiments are summarized in Table 2. All reported scores must have over 1.1% BLEU-4 dif ference to be significant at the 95% con-fidence level for 1% training. For all other training sizes, the dif ference must be over 1.7% BLEU-4. Er-ror interv als were computed using bootstrap resam-pling (K oehn, 2004b). Across dif ferent schemes, EN performs the best un-der scarce-resource condition; and D2 performs best under lar ge-resource condition. Across techniques and under scarce-resource conditions, M ADA is bet-ter than B AMA which is better than R EGEX . Under lar ge-resource conditions, this dif ference between techniques is statistically insignificant, though it X  X  generally sustained across schemes.

The baseline for M T 05, which is fully in news genre lik e training data, is considerably higher than M
T 04 (mix of genres). To investigate the effect of dif ferent schemes and techniques on dif ferent gen-res, we isolated in M T 04 those sentences that come from the editorial and speech genres. We performed similar experiments as reported abo ve on this subset of M T 04. We found that the effect of the choice of the preprocessing technique+scheme was amplified. For example, M ADA + D2 (with 100% training) on non-ne ws impro ved the system score 12% over the baseline ST (statistically significant) as compared to 2.4% for news only .

Further analysis sho ws that combination of out-put from all six schemes has a lar ge potential im-pro vement over all of the dif ferent systems, suggest-ing a high degree of complementarity . For example, a 19% impro vement in BLEU score (for M T 04 un-der M ADA with 100% training) (from 37.1 in D2 to 44.3) was found from an oracle combination created by selecting for each input sentence the output with the highest sentence-le vel BLEU score. We plan to study additional variants that these re-sults suggest may be helpful. In particular , we plan to include more syntactic kno wledge and investigate combination techniques at the sentence and sub-sentence levels.

