 Transaction data is ubiquitous in data mining applications. 
Examples include market basket data in retail commerce, telephone call records in telecommunications, and Web logs of individual page-requests at Web sites. Profiling consists of using historical transaction data on individuals to construct a model of each individual's behavior. Simple profiling tech-niques such as histograms do not generalize well from spume transaction data. In this paper we investigate the applica-tion of probabilistic mixture models to automatically gener-ate profiles from large volumes of transaction data. In effect, the mixture model represents each individual's behavior as a linear combination of "basis transactions." We evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides im-proved predictive power over simpler histogram-based tech-niques, as well as being relatively scalable, interpretable, and flexible. In addition we point to applications in outlier detection, customer ranking, interactive visualization, and so forth. The paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules. [Methods and Algorithms] 
EM algorithm, mixture models, profiles, transaction data 
Large transaction data sets are common in data mining ap-plications. Typically these data sets involve records of trans-actions by multiple individuals, where a transaction consists of selecting or visiting among a set of items, e.g., a market basket of items purchased or a list of which Web pages an individual visited during a specific session. 
Figure 1: Examples of transactions for several in-dividuals. The rows correspond to market baskets and the columns correspond to particular categories of items. The darker the pixel, the more items were purchased (white means zero). The solid horizontal gray lines do not correspond to transaction data but are introduced in the plot to indicate the boundaries between transactions of different individuals. requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA 
Copyright ACM 2001 1-58113-391-x/01/08...$5.00 We are interested in the problem of making inferences about individual behavior given transaction data from a large set of individuals over a period of time. In particular we focus on techniques for automatically inferring profiles for indi-viduals from the transaction data. In this paper a profile is considered to be a description or a model of an individual's transaction behavior, specifically, the likelihood that indi-vidual i will purchase (or visit) a particular item. Finding profiles is a fundamental problem of increasing interest in data mining, across a range of transaction-related applica-tions: retail cross-selling, Web personalization, forecasting, and so forth. Figure 1 shows a set of transactions for 5 different individu-als where rows correspond to market baskets (transactions) and columns correspond to categories of items (store de-partments in this example). The data set from which these examples are taken involves over 200,000 transactions from 50,000 customers over a two-year period in a set of retail stores. The heterogeneity of purchasing behavior is clear even from this simple plot. Different customers purchase different numbers of items, in different departments, and in different amounts. Our goal in this paper is to investigate parsimonious and accurate models for each individual's pur-chasing behavior, i.e., individual profiles. The paper begins by defining the general problem of profil-ing and the spectrum between sparse individual-specific in-formation and broadly supported global patterns. We then define some general notation for the problem and introduce a mixture model framework for modeling transaction "be-havior" at the individual level. We describe the model and illustrate with some examples. We then conduct a num-ber of experiments on a real-world transaction data set and demonstrate that the proposed approach is feasible on real-world data and provides performance that is interpretable, accurate, and sealable. We briefly sketch how the model can support several data mining tasks, such as exploratory data analysis, ranking of customers, novelty detection, forecast-ing, and so forth and conclude with a discussion of related work. Profiling is essentially the problem of converting transac-tion data (such as that in figure 1) into a model for each individual that can be used to predict their future behav-ior. Clearly human behavior is highly unpredictable and, thus, uncertainty abounds. Nonetheless, there are likely to be regularities in the data that can be leveraged, and that, on average, can lead to a systematic method for making predictions. A facet of many transaction data sets is the fact that the data are quite sparse. For example, for many transaction data sets (including the particular data set corresponding to figure 1), a histogram of "number of transactions" peaks at 1 (i.e., more customers have a single transaction than any other number of transactions) and then decreases expo-nentially quickly. Thus, for many customers there are very few transactions on which to base a profile, while for others there are large numbers of transactions. Assume for example that we model each individual via a sim-ple multinomial probability model to indicate which items are chosen, namely, a vector of probabilities pc, one for each of the C categories of items, with ~=1 Pc = 1). Also assume that this is combined with a histogram that mod-els how many items in total are purchased per visit, and a "rate parameter" A that governs how often the individual conducts transactions per unit time on average (e.g., A --3.5 per month). The crux of the profiling problem is to find a middle-ground between two profiling extremes. At one modeling extreme, we could construct a general model, of the general form de-scribed above, for the whole population and assume that individuals are homogeneous enough in behavior that such a single model is adequate. This is unlikely to be very accurate given the heterogeneity of human behavior (e.g., see figure 1). At the other extreme we could construct a unique model for each individual based only on past trans-action data from that individual, e.g., the multinomial, the histogram, and the rate parameter are all estimated from raw counts for that individual. This will certainly provide individual-specific profile models. However, it suffers from at least two significant problems. Firstly, for individuals with very small amounts of data (such as those with only one item in one transaction) the profiles will be extremely sparse and contain very little information. Secondly, even for individuals with significant amounts of data, the raw counts do not contain any notion of generalization: unless you have purchased a specific item in the past the profile probability for that item is zero, i.e., the model predicts that you will never purchase it. These limitations are well-known and have motivated the development of various techniques for borrowing strength, i.e., making inferences about a specific individual by com-bining both their individual data with what we know about the population as a whole. Collaborative filtering can be thought of as a non-parametric nearest-neighbor technique in this context. Association rule algorithms also try to ad-dress the problem of identifying rules of generalization that allow identification and prediction of novel items that have not been seen in an individual's history before (e.g., Brijs et al., 2000; Lawrence et al., 2001). However, while these methods can support specific infer-ences about other items that an individual is likely to pur-chase, they do not provide an explicit model for an indi-vidual's behavior. Thus, it is difficult to combine these approaches with other traditional forecasting and predic-tion techniques, such as, for example, seasonal modeling techniques that utilize information about annual seasonal shopping patterns. Similarly, it is not clear how covariate information (if available: such as an individual's income, sex, educational background, etc.) could be integrated in a systematic and coherent manner into an association rule framework (for example). In this paper we take a model-based approach to the pro-filing problem, allowing all information about an individual to be integrated within a single framework. Specifically we propose a flexible probabilistic mixture model for the trans-actions, fit this model in an efficient manner, and from the mixture model infer a probabilistic profile for each individ-ual. We compare our approach with baseline models based on raw or adjusted histogram techniques and illustrate how the mixture model allows for more accurate generalization from limited amounts of data per individual. 
We have an observed data set D = {DI,..., DN}, where D~ is the observed data on the ith customer, 1 &lt; i &lt; N. Each individual data set D~ consists of one or more transactions for that customer , i.e., Di = {yil,..., y~j,... ,y~}, where y~j is the jth transaction for customer i and n~ is the total number of transactions observed for customer i. 
An individual transaction y~j consists of a description of the set of products that were purchased at the same time by the same customer. For the purposes of the experiments de-scribed below, each individual transaction yij is represented component ni~c indicates how many items of type c are in transaction ij, 1 &lt; c &lt; C. One can straightforwardly gen-eralize this representation to include (for example) the price for each product, but here we focus just on the number of items (the counts). Equally well the components nijc could indicate the time since the last page request from individual i when they request Web page c during session j. For the purposes of this paper we will ignore any information about the time or sequential order in which items are purchased or in which pages are visited within a particular transaction y, but it should be clear from the discussion below that it is straightforward to generalize the approach if sequential order or timing information is available. We are assuming above that each transaction is "keyed" off a unique identifier for each individual. Examples of such keys can include driver's license numbers or credit card numbers for retail purchasing or login or cookie identifiers for Web visits. There are practical problems associated with such identification, such as data entry errors, missing identifiers, fraudulent or deliberately disguised IDs, multiple individu-als using a single ID, ambiguity in identification on the Web, and so forth. Nonetheless, in an increasing number of trans-action data applications reliable identification is possible, via methodologies such as frequent shopper cards, "opt-in" Web services, and so forth. In the rest of the paper we will assume that this identification problem is not an issue (i.e., either the identification process is inherently reliable, or there are relatively accurate techniques to discern iden-tity). In fact in the real-world transaction data set that we use to illustrate our techniques, ambiguity in the identifica-tion process is not a problem. We propose a simple generative mixture model for the trans-actions, namely that each transaction yij is generated by one of K components in a K-component mixture model. Thus, the kth mixture component, 1 &lt; k &lt; K is a specific model for generating the counts and we can think of each of the K models as "basis functions" describing prototype trans-actions. For example, one might have a mixture component that acts as a prototype for suit-buying behavior, where the expected counts for items such as suits, ties, shirts, etc., il 
Figure 2: An example of 6 "basis" mixture compo-nents fit to the transaction data of Figure 1. given this component, would be relatively higher than for the other items. 
There are several modeling choices for the component trans-action models for generating item counts. In this paper we choose a particularly simple memoryless multinomial model that operates as follows. Conditioned on nij, the total num-ber of items in the basket, each of the individual items is selected in a memoryless fashion by nij draws from a multi-nomial distribution Pk = (gkl,...,0kc) on the C possible items. Other models are possible: for example, one could model the data as coming from C conditionally independent random variables, each taking non-negative integer values. This in general involves more parameters than the multi-nomial model, and allows (for example) the modeling of the purchase of exactly one suit and one pair of shoes in a manner that the multinomial multiple trials model can-not achieve. In this paper, however, we only investigate the multinomial model since it is the simplest to begin with. We can also model the distribution on the typical number of items purchased within a given component, e.g., as a Poisson model, which is entirely reasonable (a gift-buying component model might have a much higher mean number of items than a suit-buying model). These extensions are straightforward and not discussed further in this paper. Figure 2 shows an example of K = 6 such basis mixture components that have been learned from the transaction data of figure 2 (more details on learning will be discussed below). Each window shows a a particular set of multino-mial probabilities that models a specific type of transaction. The components show a striking bimodal pattern in that the multinomial models appear to involve departments that are either above or below department 25, but there is very little probability mass that crosses over. In fact the models are capturing the fact that departments numbered lower than 25 correspond to men's clothing and those above 25 corre-spond to women's clothing. We can see further evidence of this bimodality in the data itself in figure 1 noting that some individuals do in fact cross over and purchase items Figure 3: Histograms indicating which products a particular individual purchased, from both the training data and the test data. Figure 4: Inferred "effective" profiles from global weights, smoothed histograms, and individual-specific weights for the individual whose data was shown in figure 3. from "both sides" depending on the transaction. We further assume that for each individual i there exists a set of K weights, and in the general case these weights are individual-specific, denoted by ai = (~il,... ,(~K), where ~k alk = 1. Weight (~k represents the probability that when individual i enters the store their transactions will be generated by component k. Or, in other words, the (~ik's govern individual i's propensity to engage in "shopping be-havior" k (again, there are numerous possible generaliza-tions such as making the ~ik's have dependence over time, that we will not discuss here). The ~k's are in effect the profile coefficients for individual i, relative to the K compo-nent models. This idea of individual-specific weights (or profiles) is a key component of our proposed approach. The mixture compo-nent models Pk are fixed and shared across all individuals, providing a mechanism for borrowing of strength across in-dividual data. In contrast, the individual weights are in principle allowed to freely vary for each individual within a K-dimensional simplex. In effect the K weights can be thought as basis coefficients that represent the location of individual i within the space spanned by the K basis func-tions (the component Pk multinomials). This approach is quite similar in spirit to the recent probabilistic PCA work of Hofmann (1999) on mixture models for text documents, where he proposes a general mixture model framework that represents documents as existing within a K-dimensional simplex of multinomial component models. Given the assumptions stated so far, the probability of a particular transaction y~j, assuming that it was generated by component k, can now be written as where Okc is the probability that the cth item is purchased given component k and nijc is the number of items of cate-gory c purchased by individual i, during transaction ij. When the component that generated transaction yi~ is not known, we then have a mixture model, where the weights are specific to individual i: An important point in this context is that this probability model is not a multinomial model, i.e., the mixture has richer probabilistic semantics than a simple multinomial. As an example of the application of these ideas, in fig-ure 3 the training data and test data for a particular in-dividual is displayed. Note that there is some predictability from training to test data, although the test data contains (for example) a purchase in department 14 (which was not seen in the training data). Figure 4 plots effective profiles 1 for this particular individual as estimated by three differ-ent schemes in our modeling approach: (1) global weights that result in everyone being assigned the same "generic" profile, (2) a smoothed histogram (maximum a posterior or MAP) technique that smooths each individual's training histogram with a population-based histogram, and (3) indi-vidual weights that are "tuned" to the individual's specific behavior. (Details on each of these methods are provided later in the paper). One can see in figure 4 that the global weight profile re-1We call these "effective profiles" since the predictive model under the mixture assumption is not a multinomial that can be plotted as a bar chart: however, we can approximate it and we are plotting one such approximation here flects broad population-based purchasing patterns and is not representative of this individual. The smoothed his-togram is somewhat better, but the smoothing parameter has "blurred" the individual's focus on departments below 25. The individual-weight profile appears to be a better rep-resentation of this individual's behavior and indeed it does provide the best predictive score on the test data in fig-ure 3. Note that the individual-weights profile in figure 4 "borrows strength" from the purchases of other similar cus-tomers, i.e., it allows for small but non-zero probabilities of the individual making purchases in departments (such as 6 through 9) if he or she has not purchased there in the past. This particular individual's weights, the aik's, are (0.00, 0.47, 0.38, 0.00, 0.00.0.15) corresponding to the 6 com-ponent models shown in figure 2. The most weight is placed on components 2, 3 and 6, which agrees with our intuition given the individual's training data. The unknown parameters in our model consist of both the C, and the individual-specific profile weights ~, 1 &lt; i &lt; N. In learning these parameters from the data we have two main choices: either we treat all of these parameters as unknown and use EM to estimate them, or we can first learn the 
K multinomials using EM, and then determine weights for the N individuals relative to these previously-learned basis functions. The disadvantage of the first approach is that it clearly may overfit the data, since unless we have large numbers of transactions and items per customer, we will end up with as many parameters as observations in the model. 
In the Appendix we outline three different techniques for es-timating individual-specific profile weights (including "full 
EM"), and later in the experimental results section we in-vestigate the relative performance of each. We also include a generic baseline version of the model in our experiments for comparison, namely Global weights where each individ-ual's weights ~ are set to a common set of weights a, where a mixture of K multinomials. Intuitively we expect that these global weights will not be tuned particularly well to any individual's behavior, and thus, should not perform as well as individual weights. Nonetheless, the parsimony of the global weight model may work in its favor when mak-ing out-of-sample predictions, so it is not guaranteed that individual-weight models will beat it, particularly given the relative sparsity of data per individual. 
We also include for baseline comparison a very simple smoothed histogram model defined as a convex combination of the maximum likelihood (relative frequency or histogram) es-timate of each individual's multinomial estimated directly from an individual's past purchases and a population multi-nomial estimated from the pooled population data. The idea is to smooth each individual's histogram to avoid hav-ing zero probability for items that were not purchased in the past. This loosely corresponds to a maximum a posteriori (MAP) strategy. We tuned the relative weighting for each term to maximize the overall logp score on the test data set, which is in effect "cheating" for this method. A limitation of this approach is that the same smoothing is being applied to each individual, irrespective of how much data we have available for them, and this will probably limit the effective-ness of the model. A more sophisticated baseline histogram model can be obtained using a fully hierarchical Bayes ap-proach. In fact all of the methods described in this paper can be formulated within a general hierarchical Bayesian framework: due to space limitations we omit the details of this formulation here. To evaluate our mixture models we used a real-world trans-action data set consisting of approximately 200,000 separate transactions from approximately 50,000 different individu-als. Individuals were identified and matched to a transaction (a basket of items) at the point-of-sale using a card system. The data consists of all identifiable transactions collected at number of retail stores (all part of the same chain). We analyze the transactions here at the store department level (50 departments, or categories of items). The names of in-dividual departments have been replaced by numbers in this paper due to the proprietary nature of the data. We separate our data into two time periods (all transactions are time-stamped), with approximately 70% of the data be-ing in the first time period (the training data) and the re-mainder in the test period data. We train our mixture and weight models on the first period and evaluate our models in terms of their ability to predict transactions that occur in the subsequent out-of-sample test period. In the results re-ported here we limit attention to individuals for who have at least 10 transactions over the entire time-span of the data, a somewhat arbitrary choice, but intended to focus on indi-viduals for whom we have some hope of being able to extract some predictive power. The training data contains data on 4339 individuals, 58,866 transactions, and 164,000 items purchased. The test data consists of 4040 individuals, 25,292 transactions, and 69,103 items purchased. Not all individuals in the test data set appear in the training data set (and vice-versa): individuals in the test data set with no training data are assigned a global population model for scoring purposes. To evaluate the predictive power of each model, we calcu-late the log-probability ("logp scores") of the transactions as predicted by each model. Higher logp scores mean that the model assigned higher probability to events that actu-ally occurred. The log probability of a specific transaction from individual i, yij = (n~jl,... ,nijc) (i.e. a set of counts of items purchased in one basket by individual i), under mixture model M, is defined as 
Note that the mean negative logp score over a set of trans-actions, divided by the total number of items, can be in-terpreted as a predictive entropy term in bits. The lower this entropy term, the less uncertainty in our predictions (bounded below by zero of course, corresponding to zero uncertainty). Figure 5: Predictive entropy on out-of-sample trans-actions for the three different individual weight tech-niques, as a function of K, the number of mixture components. Figure 5 shows the predictive entropy scores for each of the three different individual weighting techniques (described in the Appendix), as a function of K the number of components in the mixture models. From the plot, we see that in general that mixtures (K &gt; 1) perform better than single multino-mial (K --1) models, with an order of 20% reduction in pre-dictive entropy. Furthermore, the simplest weight method ("wtsl") is more accurate than the other two methods, and the worst-performing of the weight methods is the method that allows the individual profile weights to be learned as pa-rameters directly by EM. While this method gave the high-est likelihood on in-sample data (plots not shown) it clearly overfitted, and led to worse performance out-of-sample. The wtsl method performed best in a variety of other experi-ments we conducted, and so, for the remainder of this paper we will focus on this particular weighting technique and refer to the results obtained with this method as simple "Individ-ual weights." Figure 6 compares the out-of-sample predictive entropy scores as a function of number of mixture components K for the Individual weights, the Global weights (where all individu-als are assigned the same marginal mixture weights), and the MAP histogram baseline method (for reference). The MAP method is the solid line: it does better than the de-fault K ---1 multinomial model (leftmost points in the plot) because it is somewhat tuned to individual behavior, but the mixture models quickly overtake it as K increases. The performance of both Individual and Global weight mixtures steadily improves up to about K = 20 and then somewhat flattens out above that, providing about a 15% reduction in predictive uncertainty over the simple MAP approach. The Figure 6: Plot of the negative log probability scores per item (predictive entropy) on out-of-sample transactions, for global and individual weights as a function of the number of mixture components K. Also shown for reference is the score for the non-mixture MAP model. Individual weights are systematically better than the Global weights, with a roughly 3% improvement in predictive accu-racy. Figure 7 shows a more detailed comparison of the difference between Individual and Global weight models. It contains a scatter plot of the out-of-sample total logp scores for spe-cific individuals, for a fixed value of K = 6. We can see that the Global weight model is systematically worse than the Individual weights model (i.e., most points are above the bisecting line). For individuals with the lowest likeli-hood (lower left of the plot) the Individual weight model is consistently better: typically lower weight total likelihood individuals are those with more transactions and items, so the Individual profile weights model is systematically better on individuals for whom we have more data (i.e., who shop more). Figure 7 contains quite a bit of overplotting in the top left-corner. Figure 8 shows an enlargement of a part of this region of the plot. At this level of detail we can now clearly see that at relatively low likelihood values that the Individ-ual profile models are again systematically better, i.e., for most individuals we get better predictions with the Individ-ual weights than with the Global weights. Figure 9 shows a similar focused plot where now we are comparing the scores from Individual weights (y-axis again) with those from the MAP method (x-axis). The story is again quite similar, with the Individual weights systematically providing better predictions. We note, however, that in all of the scatter plots that there are a relatively small number of individuals who get better predictions from the smoother models. We conjecture that this may be due to lack of sufficient regular-ization in the Individual profile method, e.g., these may be individuals who buy a product they have never purchased before and the Individual weights model has in effect over-Figure 7: Scatter plot of the log probability scores for each individual on out-of-sample transactions, plotting individual weights versus global weights. ~" 40 .......... , ...... " ~ ...~: .'~.'.':'". :,. i" i : : : Figure 8: A close up of a portion of the plot in Figure T. Figure 9: Scatter plot of the log probability scores for each individual on out-of-sample transactions, plotting log probability scores for individual weights versus log probability scores for the MAP model. committed to the historical data, whereas the others models hedge their bets by placing probability mass more smoothly over all 50 departments. We conducted some simple experiments to determine how the methodology scales up computationally as a function of model complexity. We recorded CPU time for the EM al-gorithm (for both Global and Individual weights) as a func-tion of the number of components K in the mixture model. The experiments were carried out on a Pentium III Xeon, 500Mhz with 512MB RAM (no paging). For a given number of iterations, the EM algorithm for mix-tures of multinomials is linear in both the number of compo-nents K and the number of total items n. We were interested to see if increasing the model complexity might cause the algorithm to take more iterations to converge, thus causing computation time to increase at a rate faster than linearly with K. Figure 10 shows that this does not happen in prac-tice. It is clear that the time taken to train the models scales roughly linearly with model complexity. Note also that there is effectively no difference in computation time between the Global and Individual weight methods, i.e., the extra com-putation to compute the Individual weights is negligible. There are several direct applications of the model-based ap-proach that we only briefly sketch here due to space lim-itations. In particular, we can use the scores to rank the most predictable customers. Customers with relatively high logp scores per item are the most predictable and this infor-Figure 10: Plot of the CPU time to fit the global and individual weight mixture models, as a function of model complexity (number of components K), with a linear fit. Figure 11: An example of a customer that is au-tomatically detected as having unusual purchasing patterns: population patterns (top), training pur-chases (middle), test period purchases (bottom). marion may be useful for marketing purposes. The profiles themselves can be used as the basis for accurate personaliza-tion. Forecasts of future purchasing behavior can be made on a per-customer basis. We can also use the logp scores to identify interesting and unusual purchasing behavior. Individuals with low per item logp score tend to have very unusual purchases. For exam-ple, one of the lowest ranked customers in terms of this score (in the test period) is customer 2084. He or she made sev-eral purchases in the test period in department 45: this is interesting since there were almost zero purchases by any in-dividual in this department in the training data (Figure 11). This may well indicate some unusual behavior with this indi-vidual (for example the data may be unreliable and the test period data may not really belong to the same customer). Clustering and segmentation of the transaction data may also be performed in the lower dimensional weight-space, which may lead to more stable estimation than performing clustering directly in the original "item-space". We have also developed an interactive model-based trans-action data visualization and exploration tool that uses the mixture models described in this paper as a basic framework for exploring and predicting individual patterns in transac-tion data. The tool allows a user to visualize the raw trans-action data and to interactively explore various aspects of both an individual's past behavior and predicted future be-havior. The user can then analyze the data using a number of different models, including the mixture models described in this paper. The resulting components can be displayed and compared and simple operations such as sorting and ranking of the multinomial probabilities are possible. Fi-nally profiles in the form of expected relative purchasing behavior for individual users can be generated and visual-ized. The tool also allows for interactive simulation of a user profile. This allows a data analyst to add hypothet-ical items to a user's transaction record (e.g., adding sev-eral simulated purchases in the shoe department). The tool then updates a user's profile in real-time to show how this affects the user's probability of purchasing other items. This type of model-based interactive exploration of large trans-action data sets can be viewed as a first-step in allowing a data analyst to gain insight and understanding from a large transaction data set, particularly since such data sets are quite difficult to capture and visualize using conventional multivariate graphical methods. The idea of using mixture models as a flexible framework for modeling discrete and categorical data has been known for many years in the statistical literature, particularly in the social sciences under the rubric of latent class analy-sis (Lazarsfeld and Henry, 1968; Bartholemew and Knott, 1999). Typically these methods are applied to relatively small low-dimensional data sets. More recently there has been a resurgence of interest in mixtures of multinomials and mixtures of conditionally independent Bernoulli models for modeling high-dimensional document-term data in text analysis (e.g., McCallum, 1999; Hoffman, 1999). In the marketing literature there have also been numerous relatively sophisticated applications of mixture models to retail data (see Wedel and Kamakura, 1998, for a review). 
Typically, however, the focus here is on the problem of brand choice, where one develops individual and population-level models for consumer behavior in terms of choosing between a relatively small number of brands (e.g., 10) for a specific product (e.g., coffee). The work of Breese, Heckerman and Kadie (1998) and Heck-erman et al. (2000) on probabilistic model-based collabo-rative filtering is also similar in spirit to the approach de-scribed in this paper except that we focus on explicitly treat-ing the problem of individual profiles (i.e., we have explicit models for each individual in our framework). Our work can be viewed as being an extension of this broad family of probabilistic modeling ideas to the specific case of transaction data, where we deal directly with the problem of making inferences about specific individuals and handling multiple transactions per individual. Other approaches have also been proposed in the data min-ing literature for clustering and exploratory analysis of trans-action data, but typically in a non-probabilistic framework (e.g., Strehl and Ghosh, 2000). Transaction data has al-ways received considerable attention from data mining re-searchers, going back to the original work of Agrawal, Imie-lenski, and Swami (1993) on association rules. Association rules present a very different approach to transaction data analysis, searching for patterns that indicate broad correla-tions (associations) between particular sets of items. Our work here complements that of association rules in that we develop an explicit probabilistic model for the full joint dis-tribution, rather than sets of "disconnected" joint and con-ditional probabilities (one way to think of association rules). Indeed, for forecasting and prediction it can be argued that the model-based approach (such as we propose here) is a more systematic framework: we can in principle integrate time-dependent factors (e.g., seasonality, non-stationarity), covariate measurements on customers (e.g., knowledge of the customer's age, educational-level) and other such infor-mation, all in a relatively systematic fashion. We note also that association rule algorithms depend fairly critically on the data being relatively sparse. In contrast, the model-based approach proposed here should be relatively robust with respect to the degree of sparseness of the data. 
On the other hand, it should be pointed out that in this pa-per we have only demonstrated the utility of the approach on a relatively low-dimensional problem (i.e., 50 departments). As we descend the product hierarchy from departments, to classes of items, all the way down to specific products (the so-called "SKU" level) there are 50,000 different items in the retail transaction database used in this paper. It remains to be seen whether the type of probabilistic model proposed in this paper can computationally be scaled to this level of granularity. We believe that the mixture models proposed here can indeed be extended to model the full product tree, all the way down to the leaves. The sparsity of the data, and the hierarchical nature of the problem, tends to suggest that hierarchical Bayesian approaches will play a natural role here. We leave further discussion of this topic to future work. 
The research described in this paper can be viewed as a first step in the direction of probabilistic modeling of transaction data. Among the numerous extensions and generalizations to explore are: 
To briefly summarize, we have proposed a general proba-bilistic framework for modeling transaction data and illus-trated the feasibility, utility, and accuracy of the approach on a real-world transaction data set. The experimental re-sults indicate that the proposed probabilistic mixture model framework can be a potentially powerful tool for exploration, visualization, profiling, and prediction of transaction data. Consider the log likelihood function for a set of data points D where mixture weights are individual-specific, letting e denote all the mixture model parameters and 0~ denote mix-ture component (multinomial) parameters: This is very similar to the standard mixture model but uses individual-specific weights a~k. The learning problem is to optimize this log-likelihood with respect to parameters O which consists of mixture compo-nent parameters 0k and individualized weights aik subject to a set of N constraints ~"~ aik = 1. In addition, we can define a Global weights model that has an additional con-straint that all the individual-specific weights are equal to a global set of weights a: aik = ak. This particular log-likelihood can be optimized using the Expectation-Maximization (EM) algorithm where the Q func-tion becomes: 45 
In this equation P~jk represents the class-posterior of trans-action ij evaluated using the "old" set of parameters, such that P~jk = Pijk(O'). The Q function is very similar to the 
Q function of the standard mixture model; the only dif-ference is that individualized weights are used in place of global weights and additional constraints exist on each set; of individualized weights. 
Optimizing the Q function with respect to the mixture com-. ponent parameters 0k does not depend on weights aik (it; does depend on the "old" weights through Pijk, though). 
Therefore this optimization is unchanged. Optimization with respect to aik with a set of Lagrange multipliers leads to the following intuitive update equation: which reduces to the standard equation: when global weights are used. Since the models with individualized weights have a very large number of parameters and may overfit the data, we consider several different methodologies for calculating weights: 1. Global weights: this is the standard mixture model 2. wtsl: in this model the weights are constrained to be 3. wts2: this model is similar to wtsl model, but in-4. wts3: in this model a full EM algorithm is used, where Each of the methods as described represent a valid EM al-gorithm since the update equations are always derived from the appropriate Q function. Therefore, the log-likelihood is guaranteed to increase at each iteration and consequently, the algorithms are guaranteed to converge (within the stan-dard EM limitations). The EM algorithm was always started from 10 random ini-tial random starting points for each run and the highest like. lihood solution then chosen. The parameters were initialized by sampling from a Dirichlet distribution using single clus-ter parameters as a hyperprior and an equivalent sample size of 2C = 100. The EM iterations were halted whenever the relative change in log likelihood was less than 0.01% or after 100 iterations. In practice, for the results in this paper, the algorithm typically converged to the 0.01% criterion within 20 to 40 iterations and never exceeded 70 iterations. 
The research described in this paper was supported in part by NSF CAREER award IRI-9703120. The work of IC was supported by a Microsoft Graduate Research Fellowship. 
Agrawal, R., Imielenski, T., and Swami, A. (1993) Min-
Breese J.S., Heckerman D. and Kadie C. (1998) Empiri-Brijs, T., Goethals, B., Swinnen, G., Vanhoof, K., and Bartholomew, D. J., and Knott, M. (1999), Latent Variable Heckerman, D., Chickering, D. M., Meek, C., Rounthwaite, Hoffmann, T. (1999) Probabilistic latent sematic indexing, Lawrence, R.D., Almasi, G.S., Kotlyar, V., Viveros, M.S., Lazarsfeld, P. F. and Henry, N. W. (1968) Latent Structure 
McCallum, A. (1999) Multi-label text classification with a 
Strehl, A. and J. Ghosh (2000) Value-based customer group-
Wedel, M. and Kamakura. W. A. (1998) Market Segmen-
