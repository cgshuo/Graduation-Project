
KNN is one of the most popular data mining methods for classification, but it often fails to work well with inap-propriate choice of distance metric or due to the presence of numerous class-irrelevant features. Linear feature transfor-mation methods have been widely applied to extract class-relevant information to improve kNN classification, which is very limited in many applications. Kernels have also been used to learn powerful non-linear feature transformations, but these methods fail to scale to large datasets. In this paper, we present a scalable non-linear feature mapping method based on a deep neural network pretrained with Restricted Boltzmann Machin es for improving kNN classi-fication in a large-margin framework, which we call DNet-kNN. DNet-kNN can be used for both classification and for supervised dimensionality reduction. The experimental re-sults on two benchmark handwritten digit datasets and one newsgroup text dataset show that DNet-kNN has much bet-ter performance than large-margin kNN using a linear map-ping and kNN based on a deep autoencoder pretrained with Restricted Boltzmann Machines. kNN is one of the most popular classification methods due to its simplicity and reasonable effectiveness: it doesn X  X  require fitting a model and it has been shown to have good performance for classifying many types of data. However, the good classification performance of kNN is highly de-pendent on the metric used for computing pairwise dis-tances between data points. In practice, we often use Eu-clidean distances as similarity metric to calculate k near-est neighbors of data points of interest. To classify high-dimensional data in real appli cations, we often need to learn or choose a good distance metric.

Previous work on metric learning in [22] and [6] learns a global linear transformation matrix in the original feature space of data points to make similar data points stay closer while making dissimilar data points move farther apart us-ing additional similarity or label information. And in [5], a global linear transformation is applied to the original feature space of data points to learn Mahalanobis metrics, which re-quires all data points in the same class collapse to one point. Making data points in the same class collapse is unneces-sary for kNN classification. It may produce poor perfor-mance when data points cannot be essentially squeezed to points, which is often true for some class containing mul-tiple patterns. In [13], a global linear transformation is learned to directly improve kNN classification to achieve the goal of a large margin. This method has been shown to yield significant improve ment over kNN classification, but the linear transformation often fails to give good perfor-mance in high-dimensional space and a pre-processing step of dimensionality reduction by PCA is often required for success.

In many situations, a linear transformation is not pow-erful enough to capture the underlying class-specific data manifold. A locally adaptive distance metric learning is used in [7], but locality is hard to be specified in advance. Therefore, we need to resort to more powerful non-linear transformations, so that each data point will stay closer to its nearest neighbors having the same class as itself than to any other data in the non-linear ly transformed feature space. Kernel tricks can achieve this goal sometimes, and they have been used to kernelize some of the above methods in order to improve kNN classification [5]. The method in [18] extends the work in [13] to perform linear dimensionality reduction to improve large-margin kNN classification and kernelized the method in [13]. However, the kernel-based approaches behave almost like template-based approaches. If the chosen kernel cannot well reflect the true class-related structure of the data, the resulting performance will be bad. Besides, kernel-based appro aches often have difficulty in handling large datasets.

We might want to achieve non-linear mappings by learn-ing a directed multi-layer belief net or a deep autoencoder, and then perform kNN classification using the hidden dis-tributed representations of the original input data. However, a multi-layer belief net often suffers from the  X  X xplaining away X  effect, that is, the top hidden units become depen-dent conditional on the bottom visible units, which makes inference intractable; and l earning a deep autoencoder with back-propagation is almost im possible because the gradient back-propagated to the lower layers from the output often becomes very noisy and meaningless. Fortunately, recent research has shown that training a deep generative model called Deep Belief Net is feasible by pretraining the deep net using a type of undirected graphical model called Re-stricted Boltzmann Machine (RBM) [11]. RBMs produce  X  X omplementary priors X  to ma ke the inference process in a deep belief net much easier, and the deep net can be trained greedily layer by layer using t he simple and efficient learn-ing rule of RBM. The greedy layer-wise pretraining strategy has made learning models with deep architectures possible [14, 1]. Moreover, the greedy pretraining idea has also been successfully applied to initialize the weights of a Deep Au-toencoder (DA) to learn a very powerful non-linear mapping for dimensionality reduction, which is illustrated in Fig. 1a) and 1b). Besides, the idea of deep learning has motivated researchers to use powerful generative models with deep ar-chitectures to learn better discriminative models [21].
In this paper, by combining the idea of deep learning and large-margin discriminative learning, we propose a new kNN classification and supervised dimensionality reduc-tion method called DNet-kNN. It learns a non-linear feature transformation to directly achieve the goal of large-margin kNN classification, which is based on a Deep Encoder Net-work pretrained with RBMs as shown in Fig 2. Our ap-proach is mainly inspired by the work in [13], [18] and [12]. Given the labels of some or all training data, it al-lows us to learn a non-linear feature mapping to minimize the invasions to each data poi nt X  X  genuine neighborhood by other impostor nearest neighbors, which favors kNN classi-fication directly. Previous researchers once used an autoen-coder or a deep autoencoder for non-linear dimensionality reduction to improve kNN [12, 16, 15]. None of these ap-proaches used an objective function as directly as what we use here for improving kNN classification. The approach discussed in [3] uses a convolution net to learn a similar-ity metric discriminatively, but it was handcrafted. Our ap-proach based on general deep neural networks is more flex-ible and the connection weight matrices between layers are automatically learned from data.

We applied DNet-kNN on the USPS and MNIST hand-written digit datasets for classification. The test error we obtained on the MNIST benchmark dataset is 0 . 94% ,which is better than that obtained by deep belief net, deep autoen-coder and SVM [4, 12, 11]. In addition, our fine-tuning process is very fast and converges to a good local minimum within several iterations of conjugate-gradient update. Our experimental results show that: (1) a good generative model can be used as a pretraining stage to improve discriminative learning; (2) pretraining with generative models in a layer-wise greedy way makes it possible to learn a good discrim-inative model with deep archit ecture; (3) pretraining with RBMs makes discriminative learning process much faster than that without pretraining; (4) pretraining helps to find a much better local minimum than without pretraining. These conclusions are consistent with the results of previous re-search trials on deep networks [14, 1, 21, 11, 12].
We organize this paper as follows: in section 2, we in-troduce kNN classification using linear transformations in a large-margin framework. In section 3, we describe previous work on RBM and training models with deep architectures. In section 4, we present DNet-kNN, which trains a Deep Encoder Network for improving large-margin kNN classi-fication. In section 5, we present our experimental results on USPS handwritten digit data, MNIST [20] handwritten digit data, and 20 newsgroup data with binary features. In section 6, we conclude the paper with some discussions and propose possible extensions of our current method.
In this section, we review the large-margin framework of kNN classification using a linear transformation described in [13], which is called LMNN. Given a set of data points D = { x ( i ) ,y ( i ) : i =1 ,...,n } and additional neighbor-hood information  X  ,where x i  X  R D , y ( i )  X  X  1 ,...,c for labeled data points, c is the total number of classes, and  X  tance function d ( i, l ) for pairwise data points i and l such that the given neighborhood information will be preserved in the transformed feature space corresponding to the dis-tance function. If d ( i, l ) is based on Mahanalobis distances, then it admits the following form: where A is a linear transformation matrix. Based on the goal of margin maximization, we learn the parameters of the distance function, A , such that, for each data point i ,the distance between i and each data point j from another class will be at least 1 plus the largest distance between i and its k target neighbors. Using a binary matrix y ij =1 to represent that i and j are in the same class and 0 otherwise for the labeled data points, we can formulate the above problem as an optimization problem: where C is a penalty coefficient penalizing constraint vi-olations, and h (  X  ) is a hinge loss function with h ( z max ( z, 0) .If A is a D  X  D matrix, this problem corre-sponds to the work in [13]; if A is a d  X  D matrix where d&lt;D , this problem corresponds to the work in [18]. When a non-square matrix A is learned for dimensionality reduc-tion, the resulting problem is non-convex, stochastic gradi-ent descent and conjugate gradient descent are often used to solve the problem. When A is constrained to be a full-rank square matrix, we can solve A T A directly and the re-sulting problem is convex. Alternating projection or simple gradient-based methods can be applied here [13].

Figure 1. RBM pretraining and deep autoen-coder
On large datasets, rich information existing in data fea-tures often enables us to build powerful generative mod-els to learn the constraints and the structures underlying the given data. The learned information often reveals the char-acteristics of data points belonging to different classes. In [12], it is shown that a deep belief net composed of stacked Restricted Boltzmann Machines (RBM) can perform hand-written digit classification remarkably well [17]. RBM is an undirected graphical model with one visible layer v and one hidden layer h . There are symmetric connections W between the hidden layer and the visible layer, but there are no within-layer connections. For a RBM with stochastic bi-nary visible units v and stochastic binary hidden units h , the joint probability distribution of a configuration ( v , h of RBM is defined based on its energy as follows:
E ( v , h )=  X  where b and c are biases, and Z is the partition function with Z = u , g exp (  X  E ( u , g )) . The good property due to the structure of RBM is that, g iven the visible states, each hidden unit is conditionally independent, and given the hid-den states, the visible units are conditionally independent. where sigmoid ( z )= 1 1+ exp (  X  z ) . This beneficial property allows us to get unbiased samples from the posterior distri-bution of the hidden units given an input data vector. For a RBM with binary stochastic visible units and Gaussian stochastic hidden units j s with variance  X  j s, the energy function becomes:
E ( v , h )=  X  The conditional probabilities p ( v i =1 | h ) and p ( h j come: where N (  X ,  X  ) is a Gaussian distribution with mean  X  and variance  X  . In practice, we often set all  X  j sto1. Bymin-imizing the negative log-likelihood of the observed input data vectors using gradient descent, the update rule for the weight W turns out to be,  X 
W ij = ( &lt;v i where is learning rate, &lt;  X  &gt; data denotes the expectation with respect to the data distribution and &lt;  X  &gt;  X  denotes the expectation with respect to the model distribution. In practice, we do not have to sample from the equilibrium distribution of the model, and even one-step reconstruction samplesworkverywell[9].
  X 
W ij = ( &lt;v i Although the above update rule does not follow the gradi-ent of the log-likelihood of data exactly, it approximately follows the gradient of another objective function [2]. In [11], it is shown that a deep belief net based on stacked RBMs can be trained greedily layer by layer. Given some observed input data, we train a RBM to get the hidden rep-resentations of the data. We can view the learned hidden representations as new data and train another RBM. We can repeat this procedure many tim es. It is shown that in [11], under this greedy training strategy, we always get a better model p ( h ) for hidden representations of the original input data if the number of features in the added layer does not decrease, and the following varational lower bound of the log-likelihood of the observed input data never decreases. In [12], the greedy training strategy is used to initialize the weights of a deep autoencoder as shown in Fig. 1a) and then back-propagation is used for tuning the weights of the network as shown in Fig 1b). This time the lower-bound guarantee no longer holds, but the greedy pre-training still works very well in practice [12].
The work in [12] and [11] made full use of the capa-bilities of generative models, but label information is only weakly used. In the following, we describe DNet-kNN. DNet-kNN is based on a deep encoder network with 4 hid-den layers as shown in Fig. 2, in which the input layer and the first three hidden layers all have binary stochastic units, and the last hidden layer (feature output layer) has Gaus-sian stochastic units with variance 1. The number of hidden units in each layer is listed on the left in both Fig. 1 and Fig. 2. We use stacked RBMs to initialize the weights of the encoder (we can also optionally further use a deep au-toencoder to find a better initialization). Then we fine-tune the weights of the encoder by minimizing the following ob-jective function: where  X  ij =1 if and only if i is an impostor neighbor of j , which will be discussed in details later. The def-inition of  X  il is the same as discussed before in section 2, and d f ( i, l )= || f ( x ( i ) )  X  f ( x ( l ) ) || 2 f (  X  ): R D  X  R d is a continuous non-linear mapping, and each component of f ( x ) is a continuous function of the in-put vector x , and the parameters W of f are connection weight matrices in a deep neural network. For example, in Fig. 2, W = { W i ,i =1 ,..., 4 } . This equation differs from Eqn 2 in two ways. First, the distance between data points i and j is computed by the Euclidean distance using the feature vectors output by the top layer of the encoder f in Fig. 2. Secondly, the objective function f focuses on maximizing the margin and neglects the term reducing the distance between nearest neighbors. Additionally, un-like Hinton X  X  deep autoencoder, we no longer minimize the reconstruction error, since it was found that this criterion re-duced the ability of the code vectors to accurately describe subtle differences between classes in practice.

To reduce the complexity of the back-propagation train-ing, we use simplified versions of  X  il and  X  ij in the objec-tive function 10, as compared to those described in Section 2. For each index i ,  X  il =1 only if l is one of i  X  X  top k nearest neighbors among the data points having the same class label as i (in-class nearest neighbors). In contrast, for each i ,the j sforwhich  X  ij =1 are selected from the set of impostor nearest neighbors of i , which is the union of the m nearest neighbors from each and every class other than the class of i . For example, in the case of digit recognition with ten classes, there are a total of m  X  9 impostor j sfor each data point i . This method of choosing impostor near-est neighbors is optimal for kNN classification because, by selecting m impostor neighbors from every other class, we help ensure that all potential competitors are removed.
Let y ( i ) = f ( x ( i ) ) be the low dimensional code vector of x ( i ) generated by the Deep Encoder Network. Then the time complexity of computing Eq. 10 is O (( c  X  1) kmn ) , which is a significant improvement over the time complex-ity of Eq. 2, which is O ( kn 2 ) . For the purposes of calculat-ing nearest neighbors and impostor nearest neighbors, we use Euclidean distances in the p ixel space. This means that the  X  il and y ij do not need to be recalculated each time the code vectors are updated. Unfortunately, due to the non-linear mapping, this may mean that ordinary data points in the pixel space may become impostors in the code space and will not be taken into account in the objective function. However, it is likely that the mapping is quasi-linear. There-fore, by taking a large value for m , we find that this captures most of the impostors in the code space, as evidenced by our low kNN classification errors. In our experiments, we use k =5 and m =30 .

To improve the computation time of calculating the ob-jective function gradient, a (( c  X  1) kmn ) x 3 matrix of triples was generated. These triples represent the sets of all allowed indices i , j ,and l in Eq 10 for which  X  il and  X  are non-zero. Therefore, in the triples matrix, the entries in the 2nd column represent the in-class nearest neighbors rel-ative to the first column, and the entries in the 3rd column represent the impostor nearest neighbors relative to the first column. The triples matrix is used in calculating both the gradient of the objective function, and the value of the ob-jective function itself.

The gradient of the objective function, relative to the code vector y ( i ) = f ( x ( i ) ) is given by: where  X  is the flag for margin violations:  X  klj =1 if (1 + d ( k, l )  X  d
While this equation seems to be unwieldy for implemen-tation, the use of the triples matrix makes the computation much easier. In Eq 11, we calculated each sum individu-ally, using the triples matrix to determine the appropriate indices, and then combined them later. For example, to de-termine the value of the first summation term, we simply search through the triples matrix to identify all the triples that yield a margin violation (  X  =1 ). Then, we choose those that have index i in their first column. Thus, this specific set of triples tells us the appropriate indices to use in the first sum. Specifically, the second column of the triples matrix becomes the l index values, and the third col-umn becomes the j index values. Likewise, the same strat-egy is repeated for the second and third summations. After computing  X  f  X  y i , we compute the derivative of the objective function f with respect to the network weight matrix W 4 using the following chain rule: where d is the dimensionality of feature vector y ( i ) and y s is the s -th component of y propagation algorithm referring to the network structure in Fig. 2. The training procedure of DNet-kNN is shown in Algorithm 1 The training procedure of DNet-kNN (the de-scription in [] is optional). 1: Input : training data { x ( i ) ,y ( i ) : i =1 ,...,n 2: pretrain the network in Fig. 2 with RBMs using Eq. 8 3: [Further train a deep autoencoder for T iterations to get 4: calculate each data point i  X  X  k true nearest neighbors in 5: calculate each i  X  X  m  X  ( c  X  1) imposter nearest neigh-6: create triples ( i, l, j ) . 7: set W = W init . 8: while ( &lt; not convergence &gt; ) 9: update W using conjugate gradient based on Eq. 10: Output : W .
 Algorithm 1. The input parameter T and the operations in the square bracket on line 3 are optional. The convergence criteria is achieved when either the training error no longer decreases or the objective function f no longer decreases. After we get the learned network weights W in Fig. 2, we compute test data poi nts X  feature vectors y susing W in a bottom-up pass and predict th eir labels by performing kNN classification in the y space.
We will test our model DNet-kNN for both classifica-tion and dimensionality reduction on two handwritten digit datasets, USPS and MNIST, and one binary newsgroup text dataset. We demonstrate two different types of classifica-tion: standard kNN and minimum energy classification. For standard kNN, after we finish learning the non-linear map-ping by discriminative fine-tuning, we can directly compute pairwise Euclidean distances fo r kNN classification, which is used in DNet-kNN. Alternatively, for minimum energy classification, after we calculate the feature vectors of train-ing data and test data, we can also predict the class label of a test data point by the class to which the test data point is assigned to have the lowest energy defined by Eq. 10. This minimum energy classification is denoted by  X -E X  in the ex-perimental results. In both USPS and MINIST experiments, we set k =5 and m =30 . On the two benchmark datasets, we compare DNet-kNN to some other well-known meth-ods, in which LMNN requires a pre-processing by PCA for its success. As suggested by the authors of LMNN, we re-duced the digit data to 100 dimensions first using PCA and then used LMNN to learn a linear transformation.

The network structure in Fig. 1 for Deep Autoencoder (DA) and the one in Fig. 2 for DNet-kNN (the number of hidden units in each layer is listed on the left) were used because they have good cross-va lidation classification per-formance for handwritten digit classification as discussed in [16]. Another reason for using this architecture is to fa-cilitate method comparisons. It is possible to explore other good network architechures for DNet-kNN. As in [12], in the greedy layer-wise pretraining stage of both DNet-kNN and deep autoencoder based on RBMs, we used mini-batch training with batch size 100, we set learning rate to 0.1 and weight decay coefficient to 2e-4. We set initial momentum to 0.5 in the first 5 iterations and final momentum to 0.9 in later iterations. We performed 50 iterations of pretraining using stacked RBMs on all the three datasets, and we used conjugate gradient method to minimize the margin violation cost in Eq. 10.
We downloaded the USPS di git dataset from a public link 1 . In this dataset, each digi t is represented by a 16 by 16 gray-level image. From this dataset, several different preparations are used. The first preparation is USPS-fixed, which takes the first 800 data points from each of the ten digit classes to create an 8000 point training set. The test set for USPS-fixed then consists of a further 3000 data points, with 300 from each data class.
Second to sixth preparations, called US1 to US5 are then obtained from USPS-fixed by randomly shuffling the data points for each class between tr aining and testing datasets.
Figure 3. Training error on USPS-fixed us-ing two different classification methods: the
Deep Neural Network kNN classifier (DNet-kNN) and the deep autoencoder (DA).

Figure 4. Test error on USPS-fixed using two different classification methods: the Deep
Neural Network kNN classifier (DNet-kNN) and the deep autoencoder (DA).

In Figures 3 and 4, we observe the training errors and test errors for different dimensionality codes. In all cases, the DNet-kNN classification outperforms the deep autoen-coder (DA). Furthermore, as the dimensionality of the codes increases, the classification error decreases. This trend con-inues from d=2 up till d=20, and then levels off. As is dis-cussed in section 4, DNet-kNN is pretrained with RBMs. If there is no pretraining, the error rate on USPS-fixed is 18 . 27% .

Figure 5. Two-dimensional embedding of 3000 USPS-fixed test data using the Deep Neural Network kNN classifier (DNet-kNN).

Fig. 5-7 compares the dimensionality reduction to two dimensions by DNet-kNN to the the ones by the deep au-toencoder and PCA. The DNet-kNN clearly produces su-perior clustering of data point classes in two-dimensional space. There are still some class overlaps, however, be-cause the back-propagation algorithm we use to optimize for kNN classification is not the best choice to improve vi-sualization. This is because the objective function chooses which data points it considers to be in the set of impostor nearest neighbors (allowed j  X  X ) using the pixel space rather than the code space (see Section 4). However, visualiza-tion requires reduction to very low-dimensional spaces, and the mapping from pixel space to code space must become highly non-linear as dimensiona lity is reduced. Therefore, the pixel space becomes a poorer representation of spatial relationships in the code space and the correct choice of im-postor nearest neighbors becomes less reliable during visu-alization.

Table 1. Training error of different methods on 5 random splits of USPS dataset (%). The lowest errors are shown in bold. DNet code dim=30
Figure 6. Two-dimensional embedding of 3000 USPS-fixed test data using the Deep Au-toencoder (DA).

Table 2. Test error of different methods on the same 5 random splits of USPS dataset.  X -
E X  denotes the energy classification method (%). The lowest errors are shown in bold.
 DNet code dim=30
Tables 1 and 2 show respectively the training and test er-ror on multiple USPS-random data sets. DNet-kNN almost consistently outperforms the other methods.
This section deals with the MNIST dataset, which is an-other digit set available online 2 . In MNIST, each digit is
Figure 7. Two-dimensional embedding of 3000 USPS-fixed test data using PCA. represented by a 28 by 28 gray-level image. This dataset contains 60,000 training samples and 10,000 test samples. For the USPS dataset, it was possible to do both the pre-training and the back-propagation on a single batch of data. However, given the size of the MNIST dataset, the training data had to be broken into smaller batches of 10,000 ran-domly selected data points. Then, RBM training and back-propagation could be applied iteratively to each batch. In all our experiments, batch size was set to 10,000.
Fig. 8-10 shows the mapping of the MNIST test data onto a reduced space using the DNet-kNN, the deep au-toencoder and PCA. As with the USPS dataset, DNet-kNN shows a significant improvement over the deep autoencoder and PCA.

Table 3. Test error of different methods on the benchmark MNIST dataset.  X -E X  denotes the energy classification method (%). For differ-ent kNN-based methods, k = 5.
 Methods results DNet-kNN (dim = 30, batch size=1.0e4) 0.94 DNet-kNN-E (dim = 30, batch size=1.0e4) 0.95 Deep Autoencoder (dim = 30, batch size=1.0e4) 2.13 Non-linear NCA based on a Deep Autoencoder ([16] 1.03 Deep Belief Net [11] 1.25
SVM: degree 9 [4] 1.4 kNN (pixel space) 3.05 LMNN 2.62 LMNN-E 1.58 DNet-kNN (dim = 2, batch size=1.0e4) 2.65 DNet-kNN-E (dim = 2, batch size=1.0e4) 2.65 Deep Autoencoder (dim = 2, batch size=1.0e4) 24.7
Table 3 shows the classification error of the DNet-kNN as compared to other common classification techniques on the MNIST dataset. Despite the fact that we must use batches, the DNet-kNN still produces the best classifica-tions. This indicates that the DNet-kNN classifier is highly robust, since it can perform well when limited to seeing only part of the dataset at any one time.

Figure 8. Two-dimensional embedding of 10,000 MNIST test data using the Deep Neural Network kNN classifier (DNet-kNN).

Figure 9. Two-dimensional embedding of 10,000 MNIST test data using the Deep Au-toencoder (DA).

Finally, it is worth noting that, unlike the deep autoen-coder, the fine tuning of the DNet-kNN classifier during back-propagation displays extremely fast convergence. Of-ten, the error reaches a minimum after three to five iter-ations. This is due to the fact that the RBM pretraining
Figure 10. Two-dimensional embedding of 10,000 MNIST test data using PCA. has provided an ideal starting point and also that we are using a supervised learning algorithm, as opposed to an un-supervised algorithm as in the deep autoencoder. For DNet-kNN without pretraining with RBMs, after minimizing the margin violations using conjugate gradient method for one week on a machine with 2.6GHz CPU and 64GB memory, the test error rate is still above 3 . 0% .
Besides handwritten digit recognition, we also applied our method to classify 20 newsgroup text data with binary features, which is publicly available 3 . This dataset con-tains binary occurrences for 100 words across 16242 post-ings. Our task is to use the binary feature vectors to classify these posts into four categories, which are computer, recre-ation, science, and talks. We performed 5-fold cross vali-dation: divide these posts into 5 folds, train on four folds and test on the remaining fold, and cycle this procedure 5 times. The test errors on 5 different test folds for differ-ent methods are shown in Table 4, in which each column corresponds to one test fold. LMNN failed to work on this binary dataset. Table 4 clearly shows that DNet-kNN and DNet-kNN-E performed much better than other methods. On fold 5, DNet-kNN and DNet-kNN -E produced the low-est error rates among all the 5 test folds. Therefore, we used fold 5 as test data and made them embedded into a two di-mensional space. In this case, the test error rate produced by DA is 28 . 4% . Based on the intialization of only stacked RBMs, the error rate produced by DNet-kNN is 24 . 0% , and the error rate produced by DNet-kNN-E is 21 . 5% ,but based on the initialization of both stacked RBMs and DA, the error rate produced by DNet-kNN is 22 . 6% ,andtheer-ror rate produced by DNet-kNN-E is 20 . 6% . These results suggested that the combination of stacked RBMs and DA helped to find better initialization weight matrices for DNet-kNN.

Table 4. Test error of different methods for 5-fold cross validation on binary 20 newsgroup text data.  X -E X  denotes the energy classifica-tion method (%). The lowest errors are shown in bold. DNet code dim=30
In this paper, we have presented a new non-linear fea-ture mapping method called DNet-kNN that uses a deep en-coder network pretrained with RBMs to achieve the goal of large-margin kNN classification. Our experimental re-sults on USPS and MNIST handwritten digits and news-group text data show that, DNet-kNN is powerful in both classification and non-linear embedding. Our results sug-gest that, pretraining with a good generative model is very helpful for learning a good discriminative model, and the pretraining makes discrimina tive learning much faster, and it often helps to find a much better local minimum espe-cially in a deep architecture than without pretraining. Our findings verified the hypothesis discussed in [10].
On huge datasets, the current implementation of our method only works by using mini-batches. We essentially compute the genuine nearest neighbors and impostor near-est neighbors in each mini-bat ch, which might be not opti-mal over the whole dataset. We will develop a dynamic ver-sion of DNet-kNN, in which the mini-batches will change dynamically during training and we dynamically update the true nearest neighbors and impostor nearest neighbors of each data point. And we will e xplore more efficient algo-rithms for computing the derivative in Eq. 11.

The classification performance of DNet-kNN can be pos-sibly further improved by training deep autoencoder first, and then fine-tune network weights by minimizing Eq. 10. Additionally, we plan to use the label information of training data to constrain the distances between pairwise data points in the same class. In specific, we will add a penalty term using supervised stochastic neighbor embed-ding (SNE) [8] or t-SNE [19] to constrain the within-class distances for further improving low dimensional embed-ding.

The method developed in this paper is general and can be readily applied to text data with bag-of-words or TF-IDF representation for document classification and clustering, and it can also be applied to gene expression micro-array data for gene function prediction and gene module identifi-cation.
 We thank Geoff Hinton for his guidance and inspiration. We thank Lee Zamparo for proofreading the manuscript and Ke Jin for drawing Figure 1 and Figure 2. We thank Amir Globerson for reading and discussing the first version of this paper.
 [1] Y. Bengio, P. Lamblin, D. Popovici, and [2] M. A. Carreira-Perpignan and H. G. E. On contrastive [3] S. Chopra, R. Hadsell, and Y. LeCun. Learning a simi-[4] D. DeCoste and B. Sch  X  olkopf. Training invariant sup-[5] A. Globerson and S. Roweis. Metric learning by col-[6] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhut-[7] T. Hastie and R. Tibshirani. Discriminant adaptive [8] G. Hinton and S. Roweis. Stochastic neighbor embed-[9] G. E. Hinton. Training products of experts by [10] G. E. Hinton. To recognize shapes, first learn to gener-[11] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learn-[12] G. E. Hinton and R. R. Salakhutdinov. Reducing the [13] J. B. K. Weinberger and L. Saul. Distance metric [14] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and [15] R. Min. A non-linear dimensionality reduction [16] R. Salakhutdinov and G. Hinton. Learning a nonlinear [17] Y. W. Teh and G. E. Hinton. Rate-coded restricted [18] L. Torresani and K. chih Lee. Large margin compo-[19] L. van der Maaten and G. Hinton. Visualizing data [20] H. Wang and S. Bengio. The mnist database of hand-[21] J. Weston, F. Ratle, and R. Collobert. Deep learning [22] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell.
