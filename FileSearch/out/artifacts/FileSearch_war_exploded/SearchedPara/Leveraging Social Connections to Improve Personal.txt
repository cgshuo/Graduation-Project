 Recommending products to users means estimating their prefer-ences for certain items over others. This can be cast either as a problem of estimating the rating that each user will give to each item, or as a problem of estimating users X  relative preferences in the form of a ranking . Although collaborative-filtering approaches can be used to identify users who rate and rank products similarly, another source of data that informs us about users X  preferences is their set of social connections . Both rating-and ranking-based paradigms are important in real-world recommendation settings, though rankings are especially important in settings where explicit feedback in the form of a numerical rating may not be available. Although many existing works have studied how social connec-tions can be used to build better models for rating prediction, few have used social connections as a means to derive more accurate ranking-based models. Using social connections to better estimate users X  rankings of products is the task we consider in this paper. We develop a model, SBPR (Social Bayesian Personalized Ranking), based on the simple observation that users tend to assign higher ranks to items that their friends prefer. We perform experiments on four real-world recommendation data sets, and show that SBPR outperforms alternatives in ranking prediction both in warm-and cold-start settings.
 H.2.8 [ Database Management ]: Data Mining; J.4 [ Social and Be-havioral Sciences ]: Miscellaneous Algorithms, Experimentation Social Networks; Recommender Systems; Personalized Ranking
Recommending products to users requires not only that we un-derstand their preferences , but also that we understand their inter-actions with others. Just as a person X  X  preferences can be infered from the products they rate, view, and buy, their preferences can also be infered from the products their friends rate, view, and buy.
In fact, feedback from a user X  X  friends may be even more infor-mative than a user X  X  own feedback: by considering all of a user X  X  friends, we can draw upon a much larger volume of data than we can by looking at a user in isolation. This is especially true in  X  X old-start X  settings, i.e., when making recommendations to new users. If a user signs-on to a recommendation service that is connected to a social network, we have access to their social connections X  X nd possibly even their friends X  preferences X  X ven before the user has performed any actions.

The goal of this paper is to leverage such social connections in order to build better models of users X  preferences. Many existing works aim to use social connections in order to predict the ratings that each user will give to each item; a commonly used assumption is that each user X  X  rating behavior should be somehow similar to that of their friends. However, in many real applications, explicit numerical ratings might not be available and one must instead try to model some form of implicit feedback, such as the media they consume, the pages they browse, the music they listen to, or whom they befriend [4, 21]. This setting is called  X  X ne-class X  recommen-dation and a variety of solutions have been proposed to solve it by directly modeling relative preferences, or rankings , of items for personalized recommendation [8, 12, 18, 22].

Although a number of rating estimation methods have been pro-posed that leverage social information, few works have made use of social information for one-class recommendation problems. In [2], Du et al. extend Bayesian Personalized Ranking ( BPR ) [15] by adding a social regularization term. MR-BPR [8], a state-of-the-art method for one-class recommendation, simultaneously mod-els users X  preference on items and their social relations. Pan et al. [13] present GBPR ( X  X roup-based X  BPR), a method that ag-gregates groups of users X  preferences on items to reduce modeling uncertainty and improve recommendation accuracy. Nevertheless, none of these works consider how feedback from users X  friends can be used to model their preference ranking of items.

In this paper, we try to understand the underlying mechanism of how users X  preferences for certain items are revealed by the items chosen by their friends. To motivate this work, we first conduct a simple analysis of preference data from four sources that we will Figure 1: Coverage Analysis. The Y-axis indicates the proba-bility that a user X  X  friends select an item that they have selected. Users are more likely to select items selected by their friends. consider in this paper: Ciao 1 , Delicious 2 , LibraryThing ions 4 . All four sources consist of preference data in addition to explicit social connections. First, Figure 1 shows the probability that an item selected (e.g. purchased/viewed/reviewed) by a user is also selected by their friends. To make the results more clear, we compare the probability with two baseline settings: the probability that an item selected by a user is also selected by randomly sampled users and the probability that an item selected by a user belongs to a randomly sampled item set. In all cases it is clear that the first prob-ability is an order of magnitude higher than in the other two cases. Further to this, Figure 2 shows that the probability that a user se-lects an item increases monotonically as a function of the number of friends who have selected the item, implying a model where each friend X  X  selections contribute independently to the probability that a user selects an item. Based on this idea, we build a model based on a simple assumption about rankings: From this we build a simple Bayesian model of users X  ranked pref-erences over items.

We summarize our contributions as follows: 1. We develop a ranking algorithm, called Social-BPR or SBPR , 2. We evaluate the proposed method on four real-world datasets 3. We report experiments on cold-start recommendation prob-www.ciao.co.uk www.delicious.com www.librarything.com www.epinions.com Figure 2: Users become monotonically more likely to select an item as more of their friends select it.
 Organization. Section 2 formulates the problem; Section 3 in-troduces the datasets used in our study and presents some observa-tions related to the problem we try to solve in this paper; Section 4 describes the proposed model and algorithms; Section 5 shows experimental results and verifies the contribution of our SBPR al-gorithm for one-class recommendation problems; Finally, Section 6 discusses related work and Section 7 concludes the paper.
In this section, we first introduce several concepts and definitions used in the paper and then formally define the problem of social personalized ranking.

Let U denote the user set and I denote the item set. Assume that we observe a social network G = ( U,E ) , where ( u,v )  X  E indicates user u and v are linked, and some  X  X eedback X  F = { ( u,i ) } , where u  X  U and i  X  I , from | U | = M users and | I | = N items. The concepts that will be used in this paper are defined as: Observed items and unobserved items : For each user u , ob-served items O u  X  I are defined as the items for which user u shows an observed rating or preference, and unobserved items O I are the remaining items.

In this work, for each user, we divide the total item set into three parts: positive feedback, social feedback, and negative feedback, defined as follows: Positive feedback : positive feedback P u = { ( u,i ) } is defined as the set of user-item pairs containing user u and his/her observed items i  X  O u . These could be items that u chose, purchased, rated, reviewed etc. depending on the setting and dataset in question. Social feedback : social feedback is defined by the tuple SP { ( u,k ) } where u represents the user, and k  X  O v  X  O u items that user u did not choose but at least one of their friends, v, ( u,v )  X  E selected.
 Negative feedback : negative feedback is defined as the set of user-item pairs N u = { ( u,j ) } , where j  X  O u  X  O v  X  ...  X  O of items that neither user u nor any of their friends, ( u,v ) , ( u,v E , selected. Here negative only means no explicit feedback can be observed from the user and does not represent users X  dislike of the items.

One can easily find that P u  X  SP u  X  N u =  X  and P u  X  SP contains the total item set. Given the definition of Social feedback , we now introduce a social coefficient s uk for ( u,k )  X  SP can be used to describe the preference distance between u  X  X  positive feedback and social feedback. Table 2: Statistics of cold/cool-start users (users who have per-formed fewer than 5 actions).
 Social coefficient : given a particular user u , associated with their social feedback ( u,k )  X  SP u , s uk is a parameter indicating the attitude from u  X  X  social relations towards a particular item k . The value of the social coefficient can be defined in different ways that we will discuss later, but in essence a large value of the social co-efficient indicates that u  X  X  friends show a strong preferenc toward item k from which we can naturally assume u also likes the item although he has not selected it.

Unlike previous works that usually divide the item set into two classes (positive and negative), we introduce a new social feedback class by exploiting users X  social information. Armed with these concepts, we can define the problem of social personalized rank-ing. Our goal is to recommend a (personalized) ranked list of items for each user u . Since the above concepts are defined using both user feedback and social information, the key challenge is now to learn a ranking function that incorporates all of these sources of in-formation. More precisely: Leveraging social connections to improve personalized ranking for collaborative filtering. Given observed feedback F train { U,I } and a social network G from M users and N items, the goal of this paper is to learn a ranking function for each user u f : ( u,F train ,G,P u , SP u ,N u , { s uk } )  X  Ranked _ list ( I ) : where r i ( p ) r i +1 ( q ) encodes that user u shows higher prefer-ence towards item p than item q .
Before we introduce the proposed method, we first discuss the four data sets used in this paper and demonstrate their different features.
The data sets used in this paper are collected from four popular web sites: Ciao, Delicious, LibraryThing and Epinions. Statistics of the four datasets are summarized in Table 1.  X  X eedback X  in these datasets represents whether users purchased an item, or provide a bookmark (in the case of Delicious). All data sets contain such feedback in addition to explicit social network information.
The first two datasets we use are from Ciao, a product review website, and Delicious, a bookmarking website. Both datasets are available online. 5
The remaining two datasets, LibraryThing and Epinions, we in-troduce in this paper. Both are  X  X omplete X  datasets (they include all actions by all users of the website); both were collected from public sources and can be found on the second author X  X  webpage
LibraryThing (Lthing for brevity) is a popular book-reviewing website that allows users to create an online catalog of the books they own or have read. A user can tag and rate all the books she adds to her personal library. The dataset we crawled contains user feedback on items, including both ratings and comments. Since this paper focuses on solving the Top-N recommendation problem, we filter out explicit negative feedback (rating scores below 4 out of 5 stars) and use the remaining instances for model learning. After such preprocessing, the final version of the dataset contains 73,882 users and 337,561 items with 979,053 actions and 120,536 social relations between users.

Epinions is a popular online consumer review website. In to-tal, we collected data from 41,554 users and 112,991 items along with their rating scores, review text and social relations (which in this case indicate trust relationships). Similar to the Lthing dataset, we also remove all negative rating feedback (less than 4 stars) and finally obtain 181,394 positive actions.

Table 1 shows the average amount of positive feedback, social feedback, and the number of social relations. From the table we observe that even when the amount of positive feedback per user is small, the amount of social feedback is relatively much larger. In other words, there is a much larger volume of data to draw upon if we leverage feedback provided by each user X  X  friends.
Cold-(or cool-)start users are those with few observable histori-cal actions in the datasets. In Table 2 we show the statistics of users with fewer than 5 explicit actions. From the table, one can see that different datasets exhibit different properties with regard to cold-start users. In the Ciao dataset, although more than 60% of users have fewer than 5 positive actions, the average number of users X  social relations is quite large. In the Delicious dataset, the number of cold-start users is only 126 and they all have numerous social relations. In the Lthing dataset, we find that cold-start users rarely have social relations with other users, meaning that they are really  X  X old, X  making it difficult to model their preferences. In Epinions, we find that the average amount of social feedback is much smaller than in the other three datasets.

To conclude, we find that even if the amount of positive feed-back for cold-start users is limited, there is often significantly more social feedback available when we consider their social relations. Therefore, how to design an algorithm that properly leverages both limited positive feedback in addition to social feedback becomes a critical task when making recommendations to cold-start users.
In this section, we will detail our model assumption regarding positive, social, and negative feedback and then describe the pro-posed social personalized ranking algorithm.

The problem we study is usually referred to as  X  X ne-class collab-orative filtering X . Most work on this topic focuses on the use of pos-itive versus negative feedback, but ignores the influence of users X  http://www.public.asu.edu/jtang20/ datasetcode/truststudy.htm http://cseweb.ucsd.edu/~jmcauley/ social connections. Moreover, when sampling positive and negative feedback instances for training, prior pairwise ranking methods as-sume that each sampled pair has the same weight and contribution to the model at training time.

Unlike existing methods, we incorporate feedback from a user X  X  social network through a modeling assumption regarding a new class of items that we refer to as  X  X ocial feedback. X  We also in-troduce a coefficient based on the strength between users X  ties that controls how training pairs are sampled.
We first introduce the basic assumption used in prior pairwise methods [8, 15], which can be represented as where x ui represents the preference of user u on item i and the re-lationship x ui x uj says that a user u is likely to prefer a positive item i  X  P u to a negative item j  X  N u . This assumption reflects the differences between the basic ideas of point-wise and pairwise methods. Point-wise methods [5, 12] focus on fitting the numeric values of the data while pairwise methods [15, 16, 18] model the preference-order of the data instead.

Although the pairwise assumption generates better recommenda-tion results than point-wise methods, it suffers from some obvious drawbacks as mentioned in the beginning of this section.
Now we describe our proposed assumption based on two pair-wise preference comparisons: where x ui represents a user u  X  X  preference on positive feedback i , x uk represents the preference on social feedback k , and x sents the preference on negative feedback j . Here, the  X  X nobserved X  feedback is divided into two parts: social feedback and negative feedback. Based on social feedback, we can assume a fine-grained preference order for the  X  X nobserved X  feedback. Our proposed assumption is more general and considers both the influence of a user X  X  social connections as well as their explicit feedback, making it more realistic in real social-recommendation settings. Alternative assumption : Conceivably, one could argue that  X  X o-cial X  feedback should be treated more negatively than items which are not observed at all (i.e., negative feedback). Specifically, so-cial feedback means that a user knows about an item (through their friends), yet has still chosen not to purchase/view/evaluate it. This might be treated as a signal that a user dislikes that item. Since we cannot directly observe which of these two effects is more promi-nent in real data, we test this hypothesis experimentally through the Alternative assumption :
The main difference between Eq. 3 and Eq. 2 is the removal of the preference order between positive feedback and social feed-back. Although the assumptions are different, they can be incor-porated into the same model structure and learning method as de-scribed below. Due to the space limitations, we introduce the for-mulation and learning of the model with the assumption as in Eq. 2 and experimentally compare the two assumptions in Section 5.
Based on our assumption, we propose an optimization criterion for each user. In particular, the inequality in Eq. 2 can be used to maximize the value of the Area Under the ROC Curve (AUC) which is widely used in classification problems. A large AUC value means that the positive feedback is more likely to be ranked higher
Input : Observed feedback F = { ( u,i ) } , where u  X  U and Output : Parameters Initialization:
Initialize for u = 1; u  X  M ; do end
Training: for iterations do end than social feedback and the social feedback is more likely to be ranked higher than negative feedback.

Thus, for each user u , the optimization criterion can be repre-sented as follows: where PSP u = P u  X  SP u , SPN u = SP u  X  N u ;  X  ( u,i,k ) is an indicator function that is equal to 1 if i  X  P u and k  X  SP and 0 otherwise. Similarly,  X  ( u,k,j ) is an indicator function that is equal to 1 if k  X  SP u and j  X  N u , and 0 otherwise. Eq. 4 reflects the main assumption of the paper that for a specific user, (a) her preference due to positive feedback should be larger than that of social feedback, and (b) her preference due to social feedback should be larger than that of negative feedback. Due to the totality and antisymmetry of a pairwise ordering scheme as argued in [15], the above formula can be rewritten as Eq. 5 aiming to maximize the AUC value, When optimizing for the AUC, it is common practice to work with a differentiable function, such as a sigmoid function which has the form  X  ( x ) = 1 1+ e  X  x , to approximate the function P ( . ) , so that the objective function is differentiable. Based on this trick, our goal is to maximize the following objective function,
X where a regularization term is used to avoid overfitting in the learn-ing process. The preference function is modeled by matrix factor-ization, x ui = W T u V i + b i ,x uk = W T u V k + b k and x b where d is the number of latent factors, W  X  R d  X  M R d  X  N ,b  X  R N . We adopt ` 2 -norm regularization terms for model parameters  X  = { W,V,b } .

Unlike other works, we employ a coefficient s uk in Eq. 6 to con-trol the contribution of each sampled training pair to the objective function. Reviewing the definition of s uk , if we assign a fixed value, say, 1 to each social relation, we find that s uk number of user u  X  X  friends who choose the item k when u himself does not. The objective function uses the reciprocal of (1 + s control the preference difference between positive and social feed-back. A large value of s uk indicates that user u might prefer this social feedback (item) over his positive feedback since many of u  X  X  friends show their preference on the item.
We employ the widely used stochastic gradient descent (SGD) algorithm to optimize the objective function in Eq. 6. The main process of SGD is to randomly select a (positive, social) and (so-cial, negative) feedback pair and iteratively update model param-eters based on the sampled feedback pairs. Specifically, for each training instance, we calculate the derivative and update the cor-responding parameters  X  by walking along the ascending gradient direction,
One can find that when users have no social relations, social feedback will vanish and the proposed preference assumption will reduce to the basic assumption of BPR [15], which does not con-sider the influence of social networks on users X  personalized item ranking. The steps to learn the model parameters are depicted in Algorithm 1.
Since the learning method we use in this paper is stochastic gra-dient descent (SGD), the selected training instances might have great impact on the recommendation performance. Therefore, fol-lowing existing works [17, 24], we try different sampling strategies to select the training instances. Specifically, for each user u , the sampling strategies used in this paper are as follows:
In this section, we conduct experiments on the four real-world datasets to evaluate the effectiveness of the proposed method.
In order to demonstrate the performance of our approach, we use four datasets and several metrics to evaluate all compared algo-rithms. Specifically, we split the data into a training part, used for model training, and a test part, used for model evaluation (there are no model hyperparameters so a validation set is not required). For each user, we randomly select 90% of their observed feedback as P u and leave the remainder as T u for testing. Then SP u and N are as defined in Section 2. Grid search is applied to find regular-ization parameters, and we set the values of parameters  X   X  as 0.015, 0.025, and 0.01 respectively.

Our experiments are intended to address the following questions: 1. How does our approach compare with related personalized 2. Can the proposed method solve the cold-start recommenda-3. How quickly does our proposed method converge? 4. How do different definitions of social feedback alter recom-Comparison methods. In order to demonstrate the benefits of our approach, we compare our model with the following methods for item recommendation. Since the problem we solve in this paper is one-class recommendation (without rating scores), it is unsuitable to compare our methods with rating estimation methods. Instead, we consider some state-of-the-art social one-class recommendation methods [8, 13] as baselines. of the proposed method compared with the best baseline method. Dataset Metrics Rand MP MMMF WRMF BPR MR-BPR GBPR SBPR-1 SBPR-2 Improv. Delicious Epinions Many of the above baseline methods can be found in [3].
 Evaluation Metrics. We use three popular metrics, Recall@K (R@K), NDCG (Normalized Discounted Cumulative Gain) and Area under the curve (AUC), to measure the recommendation quality of our proposed approach in comparison to baseline methods. The average AUC statistic is defined as where E ( u ) = { ( i,j ) | ( u,i )  X  T u  X  ( u,j ) /  X  ( P
DCG @ K considers the ranking of the recommended items by discounting the importance and is defined as where rel i represents the relevance score of the item i (we use a binary value for this quantity). NDCG is the ratio of the DCG value to the ideal DCG value for that user. The ideal value of DCG comes from the best ranking function for the user. Recommendation Evaluation Table 3 details the average rec-ommendation performance of different methods. Here we fix the number of latent factors to 10. From the results, we can see that our approach shows significant improvement compared with other algorithms on all four datasets. Since most of the social recom-mendation methods are concerned with rating estimation and can-not handle one-class recommendation problems, here we include comparisons with two state-of-the-art one-class recommendation methods, MR-BPR and GBPR . In particular, when comparing the SBPR models with the two closely related methods, we find that our proposed approach wins in most cases. One possible reason may be that although MR-BPR models users X  social preference and item preference simultaneously making it better than many baseline methods, it fails to model how social relations directly influence users X  preferences on items. As for GBPR , it replaces individual preferences with group preferences . However, the group of users in GBPR is sampled by random selection based only on a simple rule that users in the group have the same positive preference towards an item. We also find that all models show poor performance on the Epinions dataset, the reason we consider is the sparsity of users X  positive feedback (As Table 1 shows, the average number of posi-tive feedback in Epinions is only 4).

In Section 4, we proposed two preference order assumptions based on social feedback. In this section, we compare the perfor-mance of those two assumptions. From the experimental results, we find that the performance of the first assumption always out-performs the other one, which shows that users are more likely to prefer the items selected by their friends to others. These results are also consistent with the observational analysis in Section 1.
Detailed results for Recall@N can be found in Figure 3. Al-though the curves of different methods on Ciao seem to be very close to each other, both of the proposed methods outperform base-lines on different Recall@N measure points. On Epinions, it is much clearer that SBPR -2 improves the recommendation perfor-mance when N is small. On the other two datasets, the proposed methods consistently outperform all baseline methods. Figure 3: Recommendation performance comparisons (Re-call@N) on four datasets.
 Recommendation for Cold-Start Users. In Section 3, we have shown that although cold-start users X  have limited positive feed-back (or none), the introduction of social feedback provides infor-mation that can be used to model their preferences. In this sec-tion, we perform experiments to investigate whether the proposed assumption and algorithms can improve the recommendation accu-racy for such cold-start users. We randomly select 10% of feedback as a test-set and use the remainder for training. Among training feedback, we identify users with fewer than 5 positive actions as cold-start users. The experimental results are shown in Table 5. From these results, we observe that our proposed methods signifi-cantly outperform other baseline methods on all four datasets for cold-start recommendation. When training instances are scarce, social information provides evidence to represent or interpret cold-start users X  preferences. Moreover, when comparing the perfor-mance of SBPR with MR-BPR , the improvement of SBPR indicates that modeling the influence of the social network to cold-start users X  preference ranking is more suitable than explicitly modeling their social relations, since the latter might bring too much bias when users X  available training instances are limited.

Although we have shown that SBPR yields good performance when training instances are scarce, an important case is when there there are no training instances, i.e., for brand-new users of the sys-tem. Such users may have social relationships (e.g. if they signed on to a service using a social media account), but have performed no actions and provided no feedback. This is a genuine cold-(rather than cool-)start setting. To evaluate the performance in this setting we test the recommendation accuracy for users as a function of the number of available training instances (from 0 to 5) and compare the performance of SBPR with MR-BPR on these users. A user with 0 training instances represents the case of a brand-new user of the system. Figure 4 shows results. We can see that on all four datasets, SBPR outperforms the baseline method. The reason is that MR-BPR suffers from modeling biases brought from fitting users X  feedback on social relations when users X  feedback on items is in-sufficient. This improvement verifies that exploiting feedback from users X  social relations to model the ranked list of users X  preferred items is a promising way to solve cold-start recommendation prob-lems. Social Feedback Analysis. In the SBPR model, we define so-cial feedback based on users X  social relations. In order to demon-strate the contribution of this definition, we conduct experiments by comparing several different definitions of social feedback. The definitions compared include random user selection , random item selection and all items . The former two definitions are illustrated in the Introduction, and all items uses all items as social feedback. The results are shown in Figure 6 and we use the AUC value for evaluation. From the results, we can see that only by using users X  explicit social relations to identify social feedback does the pro-posed method achieve optimal performance on all four datasets. This analysis verifies the empirical observation in Section 1. Social Coefficient Analysis. The social coefficient controls the contribution of social feedback. In this section, we study the impact of the social coefficient on recommendation accuracy by measur-ing it in different ways. This coefficient for social feedback can be viewed as the summarization of social strength from related users, indicating the preference distance between positive feedback and social feedback. Social feedback with a large social coefficient im-plies that items have a higher probability of being adopted or pre-ferred by users. Here we choose three definitions to capture the social strength between each linked-user pair: As Figure 5 shows, the common-neighbor setting achieves better performance than the other two settings. In fact, when training data is scarce, the common-neighbor setting captures more social information for social feedback than the common-preference and constant settings.
 Sampling Strategy Analysis. Table 4 demonstrates the AUC values of SBPR with different sampling strategies for training. One can find that static sampling always performs better than other strate-gies. The reason is that the sampling distribution based on the so-cial coefficient provides valuable information for social feedback selection. We also conduct experiments to investigate whether neg-ative feedback should be sampled from a static distribution accord-ing to their global popularity. Unfortunately, the performance is poor; our analysis of the datasets suggests that negative feedback with high global popularity does not indicate that a user dislikes an item. Moreover, both adaptive sampling and dynamic negative sampling strategies show poor performance. One possible reason is that these two strategies select training instances based on inter-mediate estimated parameters. However, since model learning has not yet converged, the intermediate parameters are unreliable for selecting training instances. The uniform sampling strategy shows better results than adaptive and dynamic negative sampling strate-gies but still cannot achieve performance as good as static sam-pling. The only difference between uniform sampling and static sampling in this paper is whether the social coefficient is used to select social feedback for training. By using the social coefficient, the static sampling strategy can leverage users X  social connections to select the most informative feedback pairs for model training. Convergence Analysis. We further investigate the convergence of the SBPR model. Figure 7 shows the convergence analysis of the SBPR model on different datasets. For each iteration, we select  X  within 30 iterations on Ciao and always in fewer than 80 iterations on other datasets. In particular, we also find that when using differ-ent strategies for training instance sampling, our proposed method demonstrates similar convergence performance. This fact indicates that the proposed method can achieve promising convergence re-gardless of how we select training instances. Combined with the above sampling strategy analysis, we conclude that different strate-gies of training instance selection show a great impact on the rec-ommendation accuracy but not on the convergence performance of SBPR .
In this section, we will briefly review related work along two highly related aspects: one-class collaborative filtering and social recommendation.
 One-class Collaborative Filtering Several works have studied one-class collaborative filtering and can be mainly divided into two branches: pointwise methods [5, 12] and pairwise methods [14, 15, 16, 18, 22].

Pointwise methods aim to fit a numeric value associated with each evaluated item. These methods view positive feedback as high preference scores and use several strategies to sample nega-tive feedback as low preference scores. Then existing matrix fac-torization methods can be used to fit the preference scores. Pan et al. [12] solve the one-class recommendation problem in two ways: negative example weighting and negative example sampling. Hu et al. [5] introduce a novel concept, called a  X  X onfidence level, X  asso-ciated with positive and negative feedback, and propose an efficient optimization method for confidence-based matrix factorization.
Different from pointwise methods, pairwise methods focus on modeling the order, or ranking of the feedback. Pairwise methods always consider implicit feedback as relative relationships indicat-ing that users show higher preference on positive feedback than on negative feedback. In [15], Rendle et al. propose a bayesian per-sonalized ranking ( BPR ) framework. Following this, various ideas have been proposed that incorporate different types of contextual information into the BPR framework. [8] extends the BPR frame-work to model both users X  feedback on items and on their social relations. In [18], Rendle et al. extend the BPR framework from matrix factorization to tensor factorization for tag recommenda-tion. Pan et al. [13] aggregate the features of a group of related users to reduce the uncertainty of the selected training instances. Du et al. [2] improve one-class recommendation performance by incorporating a social regularization term into the BPR framework.
Since the learning method used in most related works is stochas-tic gradient descent, how to select the training instances has great impact on model performance. Zhang et al. [24] use current estima-tion results to dynamically select negative feedback. Lee et al. [9] propose a pairwise method by combining rating information and a kernel function to improve recommendation performance. Zhang et al. [24] use the current parameter estimates to dynamically se-lect negative feedback for training. In [17], the authors propose two strategies to sample negative feedback and also design an ef-ficient implementation to improve the convergence of the learning algorithm.

However, the aforementioned works mainly focus on modeling the feedback order by using users X  positive and negative feedback, but do not investigate how the feedback from users X  friends can be used to model users X  preference order on items. Moreover, no existing works aim to leverage social connections to distinguish the different contribution of the sampled training pairs.
 Social Recommendation. Social recommendation methods focus on using the social network to improve rating estimation and item recommendation. SoRec [10] is proposed as a probabilistic matrix factorization framework which incorporates trust network informa-tion into user taste analysis. Ma et al. [11] also propose a matrix factorization framework with social regularization based on the as-sumption that users X  interests should be similar to those of their friends. Based on these works, Yang et al. [21] devise a factor-based random walk model to explain friendship connections, and simultaneously use a coupled latent factor model to uncover inter-est interactions.

Similarly, Jamali et al. [6] propose a matrix factorization method to handle the transitivity of trust relations and trust propagation. Jiang et al. [7] design a matrix factorization framework which ex-hibits the contribution of two important factors: individual prefer-ence and interpersonal influence. Furthermore, In [23], Ye et al. pro-pose a generative model that captures social influence between fri-ends quantitatively and employs social influence to mine the per-sonal preference of users. Shen et al. [19] also propose a joint per-sonal and social latent factor model for social recommendation.
Besides the use of feedback from users and items, recently, other types of contextual information have been incorporated into social recommendation methods for various specific applications. Cho et al. [1] develop a model of human mobility that combines periodic short range movements with travel due to the social network struc-ture for user movement prediction. Zhao et al. [25] use tag informa-tion to incorporate topic mining into social recommender systems. However, most of the existing social recommendation methods fo-cus on rating estimation problems rather than the one-class recom-mendation setting that we consider.
In this paper, we exploit users X  social connections in order to improve the recommendation accuracy on one-class recommen-dation problems. We study four real-world datasets and observe that the probability that a user selects an item increases monotoni-cally as a function of the number of friends who have selected the item. Based on this observation, we design a pairwise algorithm, called SBPR for recommendation. Experiments on four real-world datasets show that SBPR effectively improves the recommendation accuracy in one-class recommendation problems. Besides provid-ing improved ranking accuracy in the general case, the detailed ex-perimental analysis also verifies the contribution of SBPR for solv-ing cold-start recommendation problems.

For future work, we are interested in extending Social-BPR in three ways, (1) Investigating how to incorporate rating information into the SBPR model; (2) Employing an active learning framework to select training pairs more effectively in the Social-BPR model; 3) Exploiting context information to model users X  preference order on items.
The work described in this paper was fully supported by the Na-tional Grand Fundamental Research 973 Program of China (No. 2014CB340405), the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CUHK 413213), and Microsoft Research Asia Regional Seed Fund in Big Data Re-search (Grant No. FY13-RES-SPONSOR-036). [1] E. Cho, S. A. Myers, and J. Leskovec. Friendship and [2] L. Du, X. Li, and Y.-D. Shen. User graph regularized [3] Z. Gantner, S. Rendle, C. Freudenthaler, and [4] L. Hong, R. Bekkerman, J. Adler, and B. D. Davison. [5] Y. Hu, Y. Koren, and C. Volin-sky. Collaborative filtering for [6] M. Jamali and M. Ester. A matrix factorization technique [7] M. Jiang, P. Cui, R. Liu, Q. Yang, F. Wang, W. Zhu, and [8] A. Krohn-Grimberghe, L. Drumond, C. Freudenthaler, and [9] J. Lee, S. Bengio, S. Kim, G. Lebanon, and Y. Singer. Local [10] H. Ma, H. Yang, M. R.Lyu, and I. King. Sorec: Social  X  4 0.0070 0.0071 0.0077 0.0073 0.0090 [11] H. Ma, D. Zhou, C. Liu, M. R.Lyu, and I. King.
 [12] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose, M. Scholz, [13] W. Pan and L. Chen. Gbpr: Group preference based bayesian [14] U. Paquet and N. Koenigstein. One-class collaborative [15] S. Rendle, C. Freuden-thaler, Z. Gantner, and [16] S. Rendle, C. Freuden-thaler, and L. S. Thieme. Factorizing [17] S. Rendle and C. Freudenthaler. Improving pairwise learning [18] S. Rendle and L. Schmidt-Thieme. Pairwise interaction [19] Y. Shen and R. Jin. Learning personal + social latent factor [20] M. Weimer, A. Karatzoglou, and A. Smola. Improving [21] S.-H. Yang, B. Long, A. Smola, N. Sadagopan, Z. Zheng, [22] S.-H. Yang, B. Long, A. J. Smola, H. Zha, and Z. Zheng. [23] M. Ye, X. Liu, and W.-C. Lee. Exploring social influence for [24] W. Zhang, T. Chen, J. Wang, and Y. Yu. Optimizing top-n [25] T. Zhao, C. Li, M. Li, Q. Ding, and L. Li. Social
