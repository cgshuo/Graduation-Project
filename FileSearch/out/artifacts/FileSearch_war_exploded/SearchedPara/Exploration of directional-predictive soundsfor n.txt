 REGULAR PAPER Tatiana G. Evreinova  X  Leena K. Vesterinen  X  Grigori Evreinov  X  Roope Raisamo Abstract Sonification of stylus movements accompanied with kinesthetic feed-back is one of possible techniques to develop cross-modal coordination in the absence of visual information. The investigated problems are the following: how to minimize the number of sounds while increasing the information they contain and how to choose a natural sonification grammar which would not require ex-tra cognitive efforts. We demonstrate two case studies of employing directional-predictive sounds (DPS). Stylus movements were sonified through three sound signals taking into account the exploration behavior and the concept of the cap-ture radius. The performance of eight subjects was evaluated in terms of the stylus deviation in relation to the points of the virtual graph, a length of the scanpaths, and the task completion time. When stylus movements were accompanied with the DPS signals within four capture radiuses, the deviation of the stylus from the graph inspected was always less than one capture radius. The scanpaths were 24 X  40% shorter in length and the task completion times decreased by 20 X 25%. We also demonstrate the game application which was designed to optimize an explo-ration behavior enhanced by the DPS. The results of the proposed sonification technique based on the model of the exploration behavior are discussed. Categories and subject descriptors: H5.2. User interfaces. Input devices and strategies  X  I.3.6 Methodology and techniques. Interaction techniques General terms: Performance  X  Design  X  Experimentation Keywords Haptic-auditory interaction  X  Sonification  X  Cross-modal coordination  X  Nonvisual interaction  X  Directional-predictive sounds  X  Exploratory behavioral patterns  X  Graph access 1 Introduction Over the recent years, there has been an increasing interest into nonvisual tech-niques such as nonspeech audio, touch, and haptics to signify different forms of information. People could have a permanent vision loss or a temporary block when vision is occupied by another task or due to distortion factors which hamper the sight. As additional space for imaging, sounds and sonification have been used to display the huge data arrays in a compact and convenient way because of the tem-poral sound nature [ 13]. Thus, there is a challenge to support a natural integration of the information alternatively encoded in the afferent flow.
 to provide navigational cues [ 2, 9, 16] presenting charts and graphs [ 5, 6]aswellas to produce nonvisual drawings [ 4]. Tran et al. [ 18] evaluated a sonification model, based on the parameters of the acoustic beacons, in navigation tasks in the real and virtual environments. The study showed that the various auditory parameters have an impact on human accuracy in turning toward the direction of the acoustic beacon. The authors revealed that the nonspeech beacons were preferred over the speech beacons and a continuous operation of the sound was favored over a pulsed operation.
 Lindsay [ 19, 20]. Walker and Lindsay concentrated on the usage of the waypoint capture radius of an auditory beacon. They defined the capture radius of an audi-tory beacon as the range at which the system considers a user has to explore the waypoint where beacon is positioned. In practice, as a person approached a way-point (nearer than the capture radius), a sound signal was given as an indication for leading the listener toward the next waypoint in the map. The authors pointed out that the performance in navigation through the given map differed across the capture radius conditions, such as the radius size of the capture.
 in accordance to some data value, but the function of information content. Sound signals support navigation of blind people both indoors and outdoors. To provide a natural and intuitive analysis of the sounds we hear, Yoshikazu Seki has developed a special training system for obstacle detection based on sound field perception [ 16].
 tools with respect to the particular audio-haptic techniques for blind navigation and the indirect inspection of the external objects. Simulation of the white cane through ultrasonic, laser, and IR navigation devices (locators) still leaves a question open: which parameters should be sonified for the blind person to deliver as much useful information as possible and to decrease noise and distraction. For instance, should it be the continued sonification of the distance to a target or the discrete sounds which warn about the change of the distance depending on speed of the person X  X  motion? Due to a narrow haptic (tactile-kinesthetic) perceptual field which is directly involved in interaction through white cane, a blind person could get much more information when a special behavioral strategy and a system of feedback cues are carefully developed. Concerning the usage of the locators for blind navigation [ 1], we believe that the feedback cues should be strictly coordinated both with exploratory movements and parameters being challenge for the research. 2 Background Contour or shape identification is one of the tasks in nonvisual inspection of the graphs. However, when the graphs are exclusively presented through sonification or embossed tactile diagrams, such visualization impoverishes the information the original graph contains. As stated by Jacobson [ 11]:  X  Traditional tactile diagrams are static, unintelligent and inflexible X  X hey can only be read by one person at any time, they cannot be  X  X uestioned X , they cannot be manipulated to change scale or perspective. X  of the relationships between values of the data array which are being ranged and plotted or displayed with a reference to a set of nonvisual cues. The nonvisual physical signals such as sounds, force moments, palpable mechanical vibrations, and electrical pulses can be applied as feedback cues [ 3, 5, 9, 14, 17]. The wider dynamic range of the perceptual cues is the more information concerning data relationships that might be displayed through these signals. When nonvisual cues evoke a diffuse sense, they could be applied to display some generalized features at the comparison of graphs or its segments.
 touchscreen to provide a natural and intuitive interaction for visually impaired users. Cognitive integration of the perceptual flows would also be better performed in a short time of inspection. The less the area used for blind inspection, the easier it is to design an efficient exploration behavior. However, the feedback cues ought to be strictly coordinated with exploratory movements and data visualization of each segment (locus in array) that is being evaluated. Still, the problem is how to minimize the number of sounds and to increase the amount of information they display, as any signal can distract and affect the integrity of the image perception. ments (Fig. 1). The X -axis shows a number of measurements in the sequence and the Y -axis shows a value of the parameter being measured. Visually, we can ob-serve that the measured parameter did not change regularly. We can also find two local maximums, one local minimum and one point of discontinuity. Moving a cursor along the graph, we can receive information concerning each record with a popup label. Gridlines help to interpolate and estimate probable intermediate val-ues. Thus, visual tools of the Microsoft Excel, for instance, provide a qualitative data estimation at a glance.
 5 s with a step of 55 ms gives a general impression regarding two local maximums, one local minimum and one point of discontinuity if the listener can correctly dis-criminate the sonified fragments. There is a special technique to fix attention, in particular, on locations of the virtual sound source to decrease the diffusion area [7]. However, a frequency deviation along the Y -axis could also lead to misrep-resentation of the linear or nonlinear sizes and relationships between the graphic objects due to nonlinear hearing sensitivity and the features of the personal hear-ing experience. As a result, the gap of afferent information in a part of the haptic sensations needed to match virtual locations of the sound source regarding known physical sizes such as the length of the hand, for instance, can lead to an inade-quate perception or a subjective deformation of the sound image. It is noteworthy that the movements of head and eyes play an important role in visual analysis. The kinesthetic feedback of eye position directly follows the motor cortex which is the main integrative structure of the brain. Thus, the direct sonification approach (the entire image-to-sound) can be applied in a case when increment of the array val-ues is big enough and sound mapping allows distinguishing the differences among data groups. While in a sample (Fig. 2) about 90 points were sonified, only five sound fragments could be differentiated with repeated listening. The comprehen-sion of the relationships between graph components still remains vague. More-over, increasing the number of spots does not necessarily lead to increasing the information that has been presented to a listener regarding each sound fragment. signals when their images look like smooth trajectories. Certainly, it is possible to point out (by hand) and to sonify each of the fragments separately using differ-ent cues to display information of the particular segment in relation to the whole graph. Still, the graph has to be sonified in such a way that the sound stream would not have been interrupted with speech cues, such as the simulation of popup labels. vention regarding which method could be used to transform visual (optical) or physical parameters into sound ones. For instance, depending on the image res-olution each pixel or a group of pixels in the working field can be assigned with sound attributes such as volume, frequency, and timbre, which could be modu-lated as a function of geometrical and/or optical (brightness, color) characteristics of the pixels [ 14]. The conversion image-to-sound-to-image itself can be carried out invariantly with a mathematical accuracy. However, this fact does not mean that after a long training and a perceptive experience the person will comprehend the greater part of the information the sound graph does really contain. Anyway, the person is never a passive observer of the static graph. The analysis of the graph includes a dynamic interaction with the data array, and the graphic visualization which in a wide sense is a tool to find the hidden features of the data array. sic model for data transformation (or even metaphoric model) and then they try to generalize the results in order to prove that the proposed particular image-to-sound mapping can be used as a universal approach. However, sometimes an ob-server has to outline parts of the graph to concentrate visual attention or to directly feel some irregularity with a hand (gesture). It is known from practice that hap-tic (tactile-kinesthetic) sense of the surface homogeneity is more obvious than it looks. That is, intuitively people involve other modalities as any previous per-ceptual experience while interacting with novel data. During such an exploratory behavior, the motor component plays an important role in the integration of the perceptual information and extraction of the features in the data array inspected. values when designing new techniques for data visualization. Some kind of user-driven filters synchronized with direct manipulation by stylus or finger could sup-port highly interactive nonvisual inspection of the data array (i.e., alternative vi-sualization). In such a way, suitable and continuous recognition of the abstract features, projected onto the behavioral activity and sensible landmarks could be provided. Being the coordinate system in hand, the horizontal and vertical guide-lines, which interact with graph, could bring much more information than only the values of crossing points do (Fig. 3).
 ities of delivering sound feedback coordinated with exploratory movements of the stylus or finger. If a whole graphic pattern has been presented to the blind person, s/he can never feel the whole image and shall scan it by fingers sequentially, piece by piece. Many researchers concluded that it is not necessary to design a graphi-cal tactile tablet as only a small matrix of the tactile transducers could be placed under the fingertip. A scan process of the graphics could be performed using any pointing input device. A number of devices with built-in tactile transducers have been developed so that people could move their hand along a virtual plane and a limited contact surface would change depending on a device location and graphic features of the inspected plot area [ 17]. Still, this way is unnatural as sliding the fingertip and a direct mechanical contact with the inspected surface brings much more information for a mental reconstruction of the tactile image.
 values but the result of the interaction dynamics with an array. Herewith, the data are explored in relation to the observer gesture that acts as the user-driven filter. In contrast, the exploratory behavior (as a sequence and the system of gestures) acts as the framework which helps and facilitates the cognitive integration of the dis-crete perceptual events. For instance, if the step of inspection would be dependent on the movement speed of the stylus, by manipulating guidelines with a different speed, an observer could increase sound modulation in doubtful positions. rough inspection of the plot area and precise exploration within a specific field. Additional information concerning navigation within the plot area could also be provided via sounds. However, continuous sonification of the distance between the stylus and the graph could be problematic or unavailing. Thus, nonvisual explo-ration and active interaction with graphs could essentially be enhanced by making use of sonification of stylus movements accompanied with kinesthetic feedback. predictive sounds (DPS) to support exploration of the graphs in the absence of visual information. Taking into account the concept of capture radius, stylus move-ments regarding the graph were sonified with three sound signals. The center of the capture radius was associated with the nearest point of the graph regarding the stylus location (Fig. 4). The person hears the crossing sound (CS) when entering or moving inside the capture radius ( R c ). In case the stylus has left the capture radius, the person will hear the backward sound (BS). A normal procedure at this occasion is that the person moves back toward the location where s/he last heard the crossing sound. During the return toward the graph, the person hears the to-ward sound (TS). There is no need to activate BS or TS sounds when a stylus location is far from the graph. Therefore, the distance of four times more than R c was taken as a threshold value after which sound feedback was not given at all. In other words, when the R c is equal to 20 pixels, the BS and TS are produced within the range of 20 X 80 pixels. 3 Method design 3.1 Participants Eight unpaid right-handed sighted students (four females and four males) partic-ipated in the evaluation of using directional-predictive sounds during nonvisual inspection of the graphs. The age of the subjects ranged from 23 to 31 years. They were blindfolded (wore mask) throughout the experiments to avoid visual pre-diction and/or an approximation of the detected positions concerning touchscreen bezel. Graphs were always hidden from the subjects. 3.2 Procedure The study was carried out on an iPAQ Pocket PC 3800 series; the processor speed was 200 MHz. The test program was written in Microsoft eMbedded Visual Basic 3.0. rection of the components) and tracking whole graph under 2-min inspection by relying on sound and kinesthetic feedback. The subjects were told to follow the graph in question several times without needing to identify the graph, however, with their best possible speed and accuracy. Upon completion of the task, the sub-jects were expected to press a button (the down arrow key).
 cation conditions: using the crossing sound alone and three sounds (CS, BS, TS) accordingly. DPS sounds were short sound bursts having a duration of less than 35 ms. The sounds were composed of several sine wave signals with different pitch, timbre, and volume in order to facilitate their perception and discrimination of each other (Fig. 5).
 cation, and interaction techniques. Auditory cues were triggered when a stylus changed position within one or four R c distances from the graph but not more often than once in each four pixels.
 Herewith, the images of the graphs, as presented in Fig. 6, should not be consid-ered as any letters or symbols. We relied on an unpredictable paradigm X  X ot on blocking the visual processing (what is impossible through the mask). To avoid a perceptual transfer, we used ambiguous graphs (Fig. 6, superposition). We did not use geometrical shapes which have often been used in sonification, to avoid a side effect of the visual completion of the amodal percepts due to the cognitive transfer. We did not require identifying the graph explored. The performance of the subjects was estimated based on the objective measures the motion features which were conditioned by sounds. Doing the test under time pressure conditions also stimulated the subjects to choose a right strategy to limit random movements. All points of the graphs were plotted in a square of 220  X  220 pixels. Diameter of the circle, shown in Fig. 6 for a comparison, was equal to 2  X  R c = 40 pixels. The graphs and conditions were changed randomly. Each of the subjects completed in-spection of five graphs in 15 trials in two conditions. Thus, 600 + 600 scan paths were produced and recorded for the statistical analysis. 3.3 Results and discussion Figure 7 shows the movement traces/scanpaths (broken irregular lines) of the sub-jects when the hidden graphs (continuous solid lines) were inspected with the use of the crossing sound alone. These results are representative enough. Some of the subjects could suppose or even recognize a shape of the graphs. Being blindfolded, the person primarily relies on the previous kinesthetic experience as a basic be-havioral pattern and uses it in the next trials. A comparison of the first and second, the third and fourth, and the fifth and sixth tracks proves such a negative transfer effect which impedes forming the universal strategy within the task and inspected field.
 able and often brings an ambiguousness, especially when the graph has branching and crossing points. Certainly, if the system knows a stylus position and the aver-age speed of the motion, it is possible to predict the points or the nearest field of an inspection and to change the capture radius accordingly. However, this feature is still under investigation and outside the work presented in this paper. tracking was accompanied with DPS sounds are shown in Fig. 8. As can be ob-served from the traces, the subjects did inspection accurately and very quickly. During 2 min, they could even successfully complete detection of the same graph several times (a repetition of scanpaths) or accomplish an additional checking of neighbor fields (the third track). Some of the subjects started the inspection from the upper left corner of the touchscreen by doing the movements across or in a diagonal direction. In the beginning of the test, we also observed the tracks when the subjects learned to move stylus and made the choice between three sounds step by step as shown in Fig. 8 (the first behavioral pattern on the left). Other subjects started from the right upper corner or any random position. Therefore, we suggested that the average deviation of the movement traces concerning the nearest graph position could be used as a kind of criterion for an evaluation of the performance of the subjects.
 tion of two hidden graphs. As we could observe from all the data in the case of the use of predictive sounds, the deviation of the stylus from the graph inspected had always a smaller mean. That is certainly encouraging because the subjects tended to minimize the deviation gradually when they became experts in the use of DPS (see trendlines). The deviation was less than a preset capture radius value ( R c = 20 pixels).
 graph than R c , backward sound immediately stimulates the person to move in an opposite direction. Hence, due to BS, we could minimize idle search time and speed up an exploration of the hidden graph.
 ference between the means of the stylus deviation in two conditions of the feed-back provided (CS and DPS), t ( 14 ) = 5 . 217, p &lt; 0 . 0001 for the first sample (left graphinFig. 9); t ( 14 ) = 6 . 7, p &lt; 0 . 0001 for the second sample (right graph in Fig. 9).
 in the case of ambiguous points. However, as we monitored and listened to feed-backs during the experiment, there was another reason why standard deviation of the deviation of the stylus position was higher than it could be expected regarding the capture radius. We can conclude that the subjects may have been following closely to the graph when doing the inspection through a balance between BS and TS, without crossing.
 for each graph throughout the test. All the deviations were averaged and the overall results are shown in Fig. 10 . The length of the scanpaths varied in a number of points sonified from 93  X  23 up to 146  X  19 in the CS condition and from 75  X  8 up to 103  X  11 in the DPS condition. The scanpaths (in pixels) were four times longer, as auditory cues were triggered when a stylus changed position within one or four R c distances from the graph but not often than each four pixels. The average time of the blind inspection with a single sound (CS) was about 54.6 s (SD = 20 . 5 s), but when DPS signals were employed the task completion time decreased to about 45 s on average (SD = 9 . 4s).
 graph inspected by all participants revealed that there was a statistically significant difference between the stylus deviation concerning the graph in two conditions when CS and DPS were used, t ( 4 ) = 5 . 271, p &lt; 0 . 01.
 spected had a smaller mean for all the graphs when predictive sounds were used. By using DPS, all the subjects had the average deviation smaller than a preset capture radius value ( R c = 20 pixels). 4 The DPS in the game application The game is a universal medium for simulation and testing novel metaphors and techniques with a wide contingent of potential users. Audio games (not the sound tricks), which originally were intended for the blind audience, are becoming pop-ular and preferred by all sighted gamers. 1 4 and 5 was the training and improvement of the exploratory behavioral patterns (EBP) coordinated with the capture radius and the directional-predictive sound signals (DPS).
 integration should progress through the game. The cognitive model of the personal exploratory behavior primarily developed within the game field could then be ex-trapolated for outdoor activity (navigation and environment exploration) in the absence of visual cues. However, before training blind children, we had to test the possible benefits and drawbacks of the sonification grammar in the game-based technique. 4.1 Hidden graphs The Hidden Graphs game was developed as a multimodal game for the blind and visually impaired people. The goal of the game is blind exploration of the hidden graphs to capture as many features of the virtual image as possible. The graph does not exist as a drawing at all, and the interaction occurs with the data array. Thus, the player has to explore a  X  X isually empty X  game field.
 The first phase allows a preliminary inspection of the game field. The player has to scan a game field by using a stylus-type input device. To support nonvisual interaction, in each position of the stylus the player receives information regarding a single point (pixel) being inspected through sound parameters. The size of the game field is 250  X  250 pixels and a hidden object comprises only of about 250 pixels, that is, 0.4% of the total number of possible locations. The player has to choose the right behavioral strategy which could allow encoding and integration of the feedback signals and to rebuild the mental model of the virtual image based on discrete sound cues and locations which were inspected.
 predictable, it is quite difficult to complete the game and evaluate the result based on the data collected in the first phase. Therefore, in the second phase, the player has to confirm the detected positions and their sequence suggested. A repetition is also beneficial to fix hidden tracks and optimize audio-kinaesthetic behavior. backs do not sonify the parameters of the pixels belonging to the virtual graph, but they are meant to present the relative distance between a stylus position and the nearest point on the graph. In particular, by changing the distance regarding the graph, the player manages the sound cues. Thus, the task-oriented sonification model is used to guide the player through grasping the graph.
 of the exploratory behavioral patterns. The learning task consisted of how and when it is suitable to apply one or another gesturing (EBP) in dependence on the discovered features and sound feedbacks. As expected, a cross-modal integration of the hearing and haptics should improve the personal audio-kinaesthetic experi-ence in the absence of visual information. 4.2 Directional predictive sounds Directional-predictive sound signals (DPS) used in the game, had the same pa-rameters as described in Sect. 2. The model implies that DPS will guide player actions to help in navigation and grasping features of the hidden graphs. Each of those sounds has their unique denotation as a capturing cue during gameplaying [1, 15].
 the capture radius. In the game application, the capture radius ( R c ) had the same meaning of the restricted area within which the player has reached the target point located on or near the virtual graph curve.
 the player performance and were established in accordance with the different size of the capture radius. The levels of the capture radius were 20, 15, 10, and 5 pixels. Also, 20 pixels was the starting level of the game. 4.3 Exploratory behavioral patterns Other components of the task-oriented interaction model were the exploratory behavioral patterns (EBP). We supposed that EBPs could promote attaining the goal while facilitating cross-modal coordination and integration by demanding less cognitive efforts. Through learning and acquiring experience in the coordi-nation of sound and kinaesthetic feedbacks, personal behavior in the absence of visual information would finally be improved.
 behavioral patterns. To formalize the testing procedure, three types of EBPs were proposed for the players.
 line gestures as the exploratory behavior. The player could change the scale, di-rection, or speed of the gestures during an exploration of the game field in relation to the directional-predictive sound signals.
 ploratory behavior for the graph capturing. The player could change the scale, direction, or speed of those gestures in relation to the DPS signals.
 patterns followed the same rule format. 5 Evaluation of the game When evaluating the game, five two-dimensional arrays were plotted and stored for testing. The subjects were asked to discover the graphs, which were never presented through graphical images. Visualization of the arrays used for testing is showninFig. 11 . Following the previous findings, all the graphs were created with the discreteness index 8 to decrease the number of the array elements (pixels). That is, when R c was equal to 20 pixels in the first level of the game, crossing sound would be activated sequentially in two locations of the stylus movements within R c at inspection of the hidden graph in any direction.
 was specified in relative units. Thus, the game field had a size of 250  X  250 relative units (still later mentioned as pixels). 5.1 Participants Eight students (unpaid volunteers) participated in the evaluation of the game Hid-den Graphs. None of the subjects had prior experience in playing the Hidden Graphs game and were not hired in previous testing of DPS. There were four fe-males and four males, right-handed persons with normal vision and hearing. The ages of the subjects ranged from 20 to 35 years. All used computers on a daily basis, reporting 6 X 8 h of usage per day. 5.2 Apparatus The Hidden Graphs game was written in Microsoft Visual Basic 6.0 under Win-dows 2000. Hardware used in the game testing included the AceCad AceCat Flair USB graphics tablet with an active area of 127 mm  X  96 mm and a standard DELL laptop (Intel Celeron Processor 1.4 GHz, cache 1 Mb) with two external speak-ers. As only the sound bursts were used, there were no particular requirements for sound equipment. The following parameters measured were stored in the log file for further analysis:  X  the number of loaded array (graph);  X  the amount of points to be inspected;  X  the capture radius (game level);  X  the time spent to complete the game;  X  the average distance to the graph and its standard deviation;  X  the number of directional-predictive sound signals (CS, BS, and TS);  X  the amount of points which were captured during the inspection phase and the 5.3 Procedure The evaluation took place in the usability laboratory at the University of Tampere. The subjects were blindfolded (wore a mask) throughout the test to avoid visual prediction and approximation of the detected locations and scanpaths concerning touch tablet features.
 due to high attention level and test duration of about 60 min. Test sessions were carried through without a break, in order to maintain the pace and familiarity with the strategies which were being evaluated. Each subject played 40 games in each session. Thus, each of the five graphs was inspected eight times in a random order using three different strategies. Each game involved playing at the preliminary inspection phase and confirmation phase.
 procedure. Participants were then allowed to familiarize themselves with the pro-cedure and play a  X  X arm-up X  game. 5.4 Results and discussion The data was collected in a total of 960 games from eight players. During the test, we had to restrict the number of games per session. Therefore, all the game lev-els had different relative frequency of appearance. Over 70% of the games were played in levels 2 and 3, where capture radius was set to 15 and 10 pixels, re-spectively. The first and fourth levels were played with almost equal probability of about 15%, with capture radius of 20 and 5 pixels, accordingly.
 difference in the performance. However, lifting the stylus would cause confusion for the player regarding the graph location and the direction of segments which were inspected.
 edly less than the backward sounds (Fig. 12 ). The player followed the directional-predictive sound signals until s/he would lose the track (outside capture radius) and backward sounds would stop the player movement.
 soon after toward sound, stylus would cross the graph (the crossing sound will fol-low immediately). Consequently, the crossing sound was noticeably more frequent than toward or backward sounds as shown in Fig. 12 . There were statistically sig-nificant differences between the number of CS and TS, t ( 3 ) = 3 . 1( p &lt; 0 . 05), between the number of CS and BS , t ( 3 ) = 2 . 7( p &lt; 0 . 05); the correlation was 0.16 and 0.07 accordingly. While there was no statistically significant difference between BS and TS, t ( 3 ) = 3 . 08 ( p &gt; 0 . 05), a similar tendency was found at the confirmation phase for all the subjects when different behavioral strategies were applied (Fig. 13 ). and backward sounds used during the exploration of the hidden graphs at the in-spection and confirmation phase (corr . = 0 . 996). As it is supposed that at the confirmation phase the player has to track the graph discovered during the inspec-tion phase, the ratio value (CS to BS or CS to TS) is higher and follows from the capture radius that defines the difficulty level of the game. The paired-samples t -test result for CS to BS ratio in two phases of the game revealed no statistically significant difference t ( 3 ) = 3 . 1, ( p &gt; 0 . 05).
 the ratio values was decreased. Within each behavioral pattern, BS and TS sounds associated strongly to movement and to each other, resulting in high correlation. The ratio between the number of crossing sounds and backward sounds varied at the confirmation phase depending on some features of the graph and the EBP used (Fig. 14 ).
 of pixels captured within those graphs were the largest. We supposed that the player had larger probability to discover the points that were positioned closer to each other, even by accidental gestures. However, a fixed value of capture radius constrains accuracy and the cognitive performance in grasping continuity of the invisible track. It means that the capture radius ought to be adaptive regarding both the player behavior and the graph features being explored.
 the EBP2  X  X  X -shape and straight-line gestures appeared to be the most efficient for the graph 3, but not for the other graphs. Especially, the EBP2  X  X ell through X  in discovering graph 4 compared to the other graphs. The performance with the EBP1, spiral, and straight-line gestures, appeared to be similar for all the graphs, except for graph 4 (Fig. 14 ). The smaller the capture radius, the longer it took for the player to complete an inspection and confirmation phase. This would lead the player to explore more locations before grasping the virtual object, the whole graph, or a separate segment. graph 4 was explored. The possible reason is the shape of graph 4, in particular, the segments in horizontal direction having a small curvature. While the players were blindfolded, they explored the game field by making exploratory patterns most in the diagonal directions, as these movements could be easily coordinated to the hand position which was almost static. The diagonal gestures have max-imum variations as reported in ref. [ 12]. We did not perform a special analysis for scanpaths of the players, but graph 4 has a lower probability to be crossed by diagonal gestures.
 of directional-predictive sounds. Figure 15 illustrates the average distances to the graph (stylus deviation) and the time spent at the confirmation phase (game phase after the preliminary inspection) The data were averaged over all the recorded cases and strategies and the comparison was made in respect to the game levels (capture radius).
 radius was more or equal to 10 pixels, it was commensurable with the discrete-graphs and allowed the players to efficiently coordinate DSP and exploratory be-havior (kinaesthetic feedbacks). When the capture radius was less than 8 pixels, the probability of losing crossing sounds was higher. The navigation was sup-ported via balance between backward and toward sounds, within a range of four times capture radius. The stylus movements could extend outside this range as well. Nevertheless, when the EBP2 was employed, the average distance to the graph (16.34 pixels, SD = 9 . 67 pixels) within any game level was higher than with other gestural techniques. The composite EBP3 seemed to result in the best performance when a smaller capture radius (5 or 10 pixels) was used. 6 Conclusions In the case of interaction through touchscreen in the absence of visual informa-tion, there are many different possibilities of delivering feedback coordinated with exploratory movements of the stylus or finger. In the presented research, we have investigated the potential of using directional-predictive sounds (DPS) to support nonvisual inspection of virtual graphs. The sound signals were not connected or proportional to visual parameters of the virtual image or an absolute position of the stylus. DPS were used to communicate to the user the appropriate movement direction. That is, sound cues have directed and guided the exploratory behavior. array of alternative signals about the array which is being explored. In contrast, s/he has to understand the direct prompt X  X  X hich direction is the best? X  and to move forward. Such a way of employing sonification showed efficiency and can support natural and intuitive interaction in a much greater degree than the con-ditional mappings used before [5 X 8, 10, 14]. The use of DPS signals facilitates multimodal integration of audio-kinesthetic perceptual signals.
 the stylus movements during nonvisual inspection of hidden graphs based on directional-predictive sounds is reliable and robust enough to be used in mobile devices, such as PDAs. A statistically significant difference was found in the per-formance of the subjects under two sonification conditions: when they used the crossing sound alone and three sounds to signify different movements concern-ing the graph. The deviation of the stylus from the graph inspected had always a smaller mean and it was less than one capture radius. The scanpaths were as much as 24 X 40% shorter in length and the task completion times decreased by 20 X 25% on average.
 havioral strategy can improve the experience to manage audio cues in a more efficient way. The task-oriented sonification technique and parameters of the in-teraction model were designed to augment and coordinate audio and kinaesthetic information. The game was structured with four levels of difficulty in two phases: training (inspection phase) and testing (confirmation phase). Testing phase was used to optimize the learning process as to how and when it is suitable to apply one or the other movement depending on the features discovered.
 formance. The composite exploratory pattern was seen to be the fastest and the most flexible in graph grasping. Still, the exploratory behavior can be improved in accordance with the features of stylus gesturing. A fixed value of capture ra-dius constrains accuracy and the cognitive performance in grasping continuity of the invisible track. This means that the capture radius ought to be adaptive re-garding both the player activity and the explored graph features. In the future, we plan to add an adaptive capture radius which would change as a function of the stylus speed. The timbre of DPS could inform the user about the stylus position concerning both ends of the graph.
 used for training blind children.
 References Author Biographies
