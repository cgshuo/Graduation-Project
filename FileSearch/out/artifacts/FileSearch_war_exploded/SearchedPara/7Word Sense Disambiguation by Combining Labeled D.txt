 Many words have multiple senses that change depending on the context. Recently, it has been confirmed that word sense disambiguation (WSD) improves certain natural language processing (NLP) applications such as parse selection [Fujita et al. 2007] or Machine Translation [Chan et al. 2007]. In international WSD competitions such as SemEval, the many tasks that have been proposed show that WSD is a problem that is attracting a lot of interest. In this article, we experiment with the Japanese WSD task from the most recent competition, SemEval-2010 [Okumura et al. 2010].
Various methods have been proposed for WSD [Navigli 2009]. Unsupervised ap-proaches such as clustering based methods [Pedersen 2006] and extended Lesk [Lesk 1986] have been shown to do well [Baldwin et al. 2010], although in general, they are beaten by supervised approaches if labeled data are provided [Tanaka et al. 2007]. For the Japanese WSD tasks at SENSEVAL-2 and SemEval-2010, supervised approaches achieved the best results [Murata et al. 2003; Okumura et al. 2010]. However, the lack of labeled data is a severe problem with non-English languages. Also in the Japanese WSD task, there are only 50 given labeled instances for each target word and this is insufficient.

Several types of methods have been proposed to compensate for this lack of labeled data. One type uses bilingual corpora and translations [Chan and Ng 2005; Zhong and Ng 2009]. At first, they linked target senses (synsets) to (Chinese) translations. Then they gathered the sentences as labeled data of the sense whose aligned Chinese word matched to the translations. This method is very effective, but its performance depends on the quality of links between target senses and its translations. Zhong and Ng [2009] proposed a method of creating the links automatically, but the performance was sur-passed by manually given translations in Chan and Ng [2005]. One other concern is that it needs a bilingual corpus. Although bilingual corpus size continues to increase, it is still difficult to get one of adequate size for informal data, such as that found at Q&amp;A sites on the World Wide Web (WWW).

Of course, we can gather monolingual corpora more easily, even for informal Web data. Two main types of methods have been proposed that use monolingual corpora to compensate for this lack of labeled data. One type is the semi-supervised learning method [Niu et al. 2005; Pham et al. 2005] or bootstrapping [Mihalcea 2002, 2004; Yarowsky 1995].

These methods use both labeled and unlabeled data from monolingual corpora. This is beneficial because unlabeled training data is easy to obtain. These methods are effective and have high applicability, but there is one problem in that this method cannot obtain labeled data for senses in the lexicon that do not appear in the labeled data (we call this an unseen sense).

The other type of method intended to offset the lack of labeled data is automatic labeled data expansion [Agirre and Martinez 2000; Mihalcea and Moldovan 1999]. They proposed expanding the amount of labeled data through a Web search using monosemous synonyms or unique expressions in definitions from WordNet [Fellbaum 1998]. These methods are also effective, and may be able to obtain labeled data even for unseen senses. But one expected problem is that the performance is influenced by sense bias (i.e., sense frequency) which varies with the corpora [Agirre and Martinez 2004].

Therefore, in this article, we propose a method that synergistically combines auto-matic labeled data expansion and semi-supervised learning. Our proposal takes a two-step approach: in the first step (Step 1), we automatically expand the labeled training data from raw corpora. In this step, we aim to expand the labeled training data even for unseen and infrequent senses.

Then, in the second step (Step 2), we apply a semi-supervised classifier. In this step, we aim to achieve an improvement and adjust the sense bias using easy-to-get unla-beled data. In this step, we also compare the results obtained by using given labeled training data only to show the benefits of our combination method. We also show its effectiveness for unseen senses.

This article is structured as follows. We describe the target task in Section 2. We describe our automatic labeled data expansion method (Step 1) in Section 3, evaluate the data quality using an experiment and human evaluation in Section 4. We then apply a semi-supervised learning method (Step 2) in Section 5. We conduct a further experiment to investigate its effectiveness against unseen senses in Section 6. We conclude the article in Section 7. In this article, we target the SemEval-2010 Japanese WSD task. The sense inven-tory used in this task was the Iwanami Japanese Dictionary ([Nishio et al. 1994]). Iwanami was originally a paper dictionary. We show a typical Iwanami entry in Figure 1. As shown, each Iwanami entry has POS information and definition sen-tences, and most entries have example sentences. Iwanami uses four hierarchical lay-ers to construct word sense descriptions. In this task, senses on the third layer are used in the evaluation phase. For example, in Figure 1, senses 0-0-1-0, 0-0-1-1 and 0-0-1-2 are merged into 0-0-1, therefore 0-0-1 and 0-0-8 are used as correct senses. Iwanami includes 60,321 entries split into 85,870 senses, which are merged into 79,611 senses at the third layer. For this task, 50 words (22 nouns, 23 verbs, and 5 adjectives) are selected as the targets, which are split into 219 senses at the third layer; of these, 144 senses appear in the given labeled training data. On the other hand, 9 senses are un-seen senses that appear in both Iwanami and the test data, but not in the labeled data.
Both the labeled training and test data are part of the Balanced Corpus of Contem-porary Japanese: BCCWJ 1 , which is morphologically analyzed by UniDic corrected. For each target word, 50 instances are provided in both the training and test data. We show an example of the given labeled data in Example (1). The given labeled data are morphologically analyzed, but have no information about the base forms, therefore we added the base forms (lemma ) automatically. The given data are also partly tagged with sense IDs of Iwanami. (1) &lt; mor pos= X   X  X  - X  X   X  rd= X   X  X  X   X  X fm= X   X  X  X   X  sense= X 37713-0-0-l-1 X  lemma= X   X  X  X   X 
One characteristic of this task is that the labeled training and test data come from heterogeneous corpora. The labeled training data include extracts form books and magazines ( PB ), newspaper articles ( PN ), and government white papers ( data also include documents from a Q&amp;A site on the WWW ( the domain from which the training and test sentences were taken. As shown in the table, their distributions are quite different. Of particular interest, of the test data, which is not included in the training data. Another characteristic of this task is that the both training and test data include new senses (called X )that are not defined in the dictionary, Iwanami. However, in this article, we do not focus on domain adaptation or guessing X , because 50 instances are insufficient for both inves-tigating domain adaptation and guessing new senses as reported by Fujita et al. [2010] and Shirai and Nakamura [2010]. However, we target unseen senses (which appear in test data and defined in the dictionary, but do not appear in the labeled training data). In this section, we introduce our automatic labeled data expansion method (Step 1). The main aim of this step is to obtain reliable labeled data even for unseen and infre-quent senses.

As mentioned in Section 1, several labeled data expansion methods have been proposed, such as Mihalcea and Moldovan [1999] and Agirre and Martinez [2000, 2004]. They mainly used WordNet X  X  monosemous synsets (for example,  X  X ecollect X  for remember 1 ) for Web search. This kind of method is effective, and offers the possibility of supplying labeled data for unseen senses. Unfortunately, we cannot obtain monose-mous synonyms because our target task is not tagged with WordNet.

Therefore, in this article, instead of these methods, we propose a method that pro-vides reliable labeled data using example sentences from a dictionary. Such sentences are informative, but in most cases with paper dictionaries such as Iwanami, the exam-ples are fragmentary to save space (in Iwanami, the average fragment is four words long). Therefore, we attempt to extract longer, more natural and high quality labeled data from the raw corpus, under strict conditions using the fragmentary examples.
That is, first, we extract example sentences ( EX ) from Iwanami. Then, we collect sen-tences that include an exact match for Iwanami X  X  example for sense ( s ( h ). Finally, we morphologically analyze the candidate sentences, and if both the base form and the coarse POS correspond to those of h , we tag the words with s the sentences to the labeled data.

For example, we can extract an example sentence as in Example (2), from 37713-0-0-1-2 in Figure 1. In Example (2), the headword is in boldface and is tagged with  X 37713-0-0-1-2 X  (at the third layer,  X 37713-0-0-1 X ). (2)  X   X   X  X  X 
The data used in the Japanese WSD task is part of the BCCWJ corpus. Therefore, we use the remainder of the BCCWJ to extract the labeled data. Note that its morpho-logical information is not hand-corrected. According to the readme file that comes with the BCCWJ, it includes about 43 million words.

We show an example of extracted labeled data in Example (3). The underlined part is exactly the same as the example sentence in Example (2). Therefore we tag  X  X  X  toru  X  X ick up/take X  with 37713-0-0-1-2 and add this entire sentence to the labeled training data. (3)  X  X  X  X   X   X   X   X   X   X  X  X   X   X   X  X  X   X  X  X   X  X  X  X   X 
Because of our strict condition, the size and variation of the extracted labeled data are limited. However, this method gives us longer and more natural reliable labeled data. Given that most languages have dictionaries, and most of these dictionaries in-clude examples, we expect that our method will be applicable to other languages. We show the size of the given ( Trn ) and extracted labeled data ( As shown, the extracted labeled data give less coverage of sense type to the given la-beled data, but provide many more instances. On average, 130 labeled sentences were extracted per example, for 326 out of 1,450 example sentences. Moreover, labeled in-stances for 9 unseen senses were extracted from Iwanami X  X  were extracted from BCCWJ. In Table II, we also show the average number of char-acters per sentence in each case. As shown, the original example sentences are very short, however, the extracted labeled sentences are about eight times longer than the length of original example sentences. In this section, we investigate the reliability and effectiveness of our automatic labeled data expansion. For this purpose, in Section 4.1, we investigate its performance on the Japanese WSD task when we apply the supervised learning approach with and without the extracted labeled data. Then in Section 4.2, we also provide a quantitative analysis of the extracted labeled data. 4.1.1. System Description. Machine Learning Methods . We constructed supervised and semi-supervised WSD classifiers for each target word, based on machine learning methods. The classifiers for a target word were designed to select a sense from a set of pre-defined senses for each instance of the target word.

In Step 1, we employed a Maximum Entropy Model ( MEM ) [Nigam et al. 1999] to design the supervised WSD classifier. Let x denote the feature vector for an instance supervised WSD classifier, for the target words, the conditional probability of s given x is modeled as the following.
 where W = [ w 1 , ... , w k , ... , w K ] is a parameter matrix and w posed vector of w k . We estimated the parameter matrix value by using labeled data.
Features. For each target word w , we used the surface form, the base form, the POS tag, and the coarse POS categories, such as nouns, verbs, and adjectives of w . Then we also used the bag-of-words for the same sentence. Here the target is the i th word, so we also used the same information for the i  X  2, i  X  1, i bigrams, trigrams, and skipbigrams back and forth within three words. We also used domain type, PB , PN , OW ,or OC , as features.

Analytical Setting. One anticipated problem with our expansion method in Step 1 is that the extracted data may have a different sense distribution from that of the test data. Therefore, to investigate trends based on sense distribution, we employed the entropy E ( w ) of the frequency distribution in given labeled data, which is given by the following equation.
 where p ( s k | w ) is the probability that word w has sense s then E ( w ) reflect sense frequency bias. Note that p ( s the probability given each instance of the target word.

Entropy E ( w ) will be lower if one particular sense appears more frequently. There-fore, following the SENSEVAL-2 Japanese WSD task [Shirai 2003], we divided the tar-get words into three classes: difficult ( D diff : E ( w )  X  and easy ( D easy : E ( w )&lt; 0.5). There were nine target words for D 21 for D easy . 4.1.2. Results and Discussion: Step 1. Learning Curves . Because of our strict condition, the variation in the extracted labeled data is limited, so the system may experience over-learning. So first we investigated the learning curves by limiting the number of added labeled instances for each Iwanami example.

Table III shows average accuracies over target words obtained with various number of added labeled instances per example. L # in the table shows the upper bound for adding labeled instances per example. We used all extracted labeled instances when the number was less than # .

We adjusted the parameters based on a five-fold cross-validation of the given la-beled data. The best result (RALI-2, [Brosseau-Villeneuve et al. 2010]) in a formal run of SemEval-2010 is also shown in Table III for reference; the system uses the most frequent sense as a baseline.

Using only the given labeled data (we call this Trn , 77.4%), we achieved an improve-ment over the best published method (76.4%). Even just one labeled instance per ex-ample gave better results (78.5 %) than the given labeled data alone, and 30 labeled instances gave the best result (79.9%), see total 3 .

Table III shows the accuracy per entropy based difficulty band. We found an inter-esting trend, namely that expanding the labeled data tended to degrade the accuracy for easy words ( D easy ), but improved it for middle ( D With easy words, the best result (91.5%) was achieved with the selection of the most frequent sense. On the other hand, especially for difficult words, expansion was very effective, and using all extracted instances gave a + 7.8% ( over Trn . Difficult word means that many more senses appear in the corpus. In other words, more instances are needed to guess the sense correctly, our method is especially effective for such difficult words.

Adding Original Example Sentences. Next, we investigated other conditions. As shown in Section 3, in Iwanami, there are 1,450 original example sentences, such as sentence (2), for the target words. However we could use only 362 example sentences to extract labeled instances, such as in sentence (3). Therefore, 1,088 (= 1,450 example sentences were not used to extract labeled data.

Accordingly, we also added original example sentences in three patterns: that is adding [1] all the original example sentences ( EX ), [2] 30 extracted labeled instances ( L 30 )and EX ,and[3] L 30 and unused example sentences ( EXrL data. These results are also shown at the bottom of Table III
As shown in Table III, the third pattern (+ L 30 + EXrL ) gave the best results (80.2%), and is superior to the patterns using all original examples. The original examples tend to be short, but because shorter examples are easier to match to the raw corpus, we can filter out the examples in EXrL that are too short. We also provide a quantitative analysis of the extracted labeled data. The first five sen-tences extracted from BCCWJ were checked manually. They included 1,038 sentences for 47 words split into 114 senses, of which 979 sentences (94.3%) were considered correct.

Because Iwanami differs from WordNet, it is not possible to make a direct compar-ison with other expansion methods such as Mihalcea and Moldovan [1999]. However, the quality of this manual evaluation result is comparable to that (95.7%) reported in Mihalcea and Moldovan [1999].

The major reason for wrong extractions was idiomatic usage. For example, the tar-get word  X  X  X  jikan  X  X ime X  has a sense  X  X pan from some point to other point X  with an example sentence  X  X  X  X  X   X  jikan no mondai  X  X  matter of time X  which is an idiom. However, it has a literal use in one of the 5 extracted instances, as shown in Exam-ple (4). (4)  X  X  X   X   X   X   X  X  X   X  X  X   X   X  X   X   X  X  X   X   X  [...] To judge whether an expression is an idiom or not is very difficult [Hashimoto and Kawahara 2009], then to filter these wrong instances automatically is difficult.
The second major reason for wrong extraction is bad segmentation. For example, for an example sentence,  X  X  X  X  X  X  ( kazu o toru  X  X ount X , we extracted Example (5). The underlined part in Example (5) matches to the above example, but the segmentation is different:  X  X  X  tensuu  X  X core lit: point count X  should not have been separated. We will be able to filter these wrong instances by using full morphological information about the example sentences. (5)  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X   X  X  X   X   X  X  X   X   X  X  X   X  X  X   X  X  X   X   X   X  In this step, we extracted labeled data automatically using example sentences from Iwanami. This method gave us longer, more natural and higher quality labeled data from the raw corpus, and we could obtain labeled data even for unseen and infrequent senses.

Step 1 provided superior performance (80.2%) to the state-of-the-art result (76.4%), proving the high effectiveness of this method.

However, it may be difficult to achieve any further improvement because the ex-tracted data may have an unnatural sense distribution (sense bias), some errors, and limited variations. Therefore, to realize an improvement, we employ a semi-supervised learning method in Step 2. Step 2 uses a semi-supervised WSD classfier for each target word by using the succes-ful semi-supervised learning method called Maximum Hybrid Log-likelihood Expecta-tion ( MHLE ) [Fujino et al. 2010]. It was reported that the MHLE method was useful for obtaining better classification performance, especially when there is a large difference between the distributions of the labeled and test data. As mentioned in Section 2, the data of the Japanese WSD task ranges across very different types of corpora; rang-ing from formal government articles to rough Web data. That was the reason that we employed the MHLE method. To compare its performance to another method using unlabeled data, we tested the self-training based bootstrapping method.
In this section, we first describe the outline of the MHLE -based semi-supervised WSD classifier (in Section 5.1), introduce our base line system (in Section 5.2), and then we present our method for extracting unlabeled data (in Section 5.3). Finally, we un-dertake an experiment and investigate the effectiveness of our combination of semi-supervised learning and labeled data expansion (in Section 5.4). In the MHLE -based semi-supervised WSD classifier for a target word, the conditional probability, P ( s | x ) ,ofsense s  X  X  s 1 , ... , s k , ... stance is modeled by a combination of discriminative and generative models, P and p g ( x , s ; ) , where W and are the respective parameters of these models. By ap-plying the classifier form and training method presented in Fujino et al. [2010], we define P ( s | x ) as the following.

We also provide objective function J for the parameter estimation of P by using labeled and unlabeled datasets, L ={ ( x n , s n following.
 Here,  X (&gt; 0 ) is a combination weight. W and can be estimated as the values that maximize J for a fixed value of  X  . The local optimal solution of W and around an ini-tial value can be obtained by an iterative process such as the EM algorithm [Dempster et al. 1977]. Namely, the MHLE -based semi-supervised WSD classifier is constructed by combining the discriminative and generative models trained on both labeled and unlabeled samples (See Fujino et al. [2010] for details of the combination and training methods).

We employed a maximum entropy model (multinomial logistic regression model) and a naive Bayes model as P d ( s | x ; W ) and p g ( x , s ; presented in Fujino et al. [2010]. In the naive Bayes model, the probability distri-p ( x | s ; )  X  V p ( x , s ; ) = p g ( x | s ; ) P ( s ) . Here,  X  of feature vector x ,and  X  si is the probability that the i th feature appears in an instance whose sense is s . ={  X  si } s , i is the parameter set of the naive Bayes model. In our experiments, we set P ( s ) = 1 / K . We used Gaussian and Dirichlet priors as p p () , respectively. We tuned the  X  (  X  0.5, 1, 2, 5, 10) value by five-fold cross-validation of the labeled data. In this section, we introduce the general bootstrapping method, we also use it as a baseline in addition to MEM on Trn . According to Mihalcea [2004], the general boot-strapping process is described as shown in Figure 2.

Mihalcea [2004] examined performance with both co-training and self-training, how-ever we used self-training to compare the results under the same features with MHLE , that is we need one supervised classifier C ,soweemploy MEM . As unlabeled data U , we used the same unlabeled data offered to MHLE (See in Section 5.3). We used up to 10,000 unlabeled data ( P ), and tested using G values of 10, 20, 30, ... ,100. As unlabeled data, we extract sentences that include the target words from BCCWJ corpus. We show an example in Example (6), the boldface part indicates the target word. (6)  X  X  X   X   X  X  X   X  X  X 
Because of the looser restriction, we can extract many more sentences than the la-beled data in Section 3. For example, from BCCWJ alone, we can extract more than 10,000 instances for 22 words and more than 1,000 instances for the remaining words except for one wird (i.e.  X  X  hitotsu  X  X ne X ). In this section, we describe an experiment in which we employed a semi-supervised classifier based on MHLE .

This experiment has two purposes: to investigate the effectiveness of (a) MHLE based semi-supervised WSD, and (b) automatically expanded data as labeled data. 5.4.1. Results and Discussion: Step 2. Table IV shows the results we obtained using the unlabeled data extracted from BCCWJ. In this experiment, we limited the unlabeled data to 10, 100, 200, 300, 500, and 1,000 instances. The smaller unlabeled data sets are subsets of the larger unlabeled data. We also use the given labeled data and several types of expanded data as labeled data. In addition, we showed the best bootstrapping results among all settings as a base line.
 To visualize this, we show some learning curves in Figures 3 X 7 which use log scales. In Figures 3 X 6, we compare MHLE with given labeled data to bootstrapping method for MHLE .
 Effectiveness of MHLE . When we use only the given labeled data ( data instances gave the best performance (82.8%), and it achieved a +5.4% ( 77.4) improvement.

In addition, in contrast with the results in Section 4, some improvements were achieved even for easy words ( D easy ). In Section 5.1, it was reported that MHLE was robust even when the labeled and test data were very different, as with the Japanese WSD task, since they came from heterogeneous corpus.
 Next we focus on the bootstrapping method. As shown in Figures 3 X 6, except for D easy , the best results with bootstrapping are always superior to MEM on case of D easy , the result by MEM on Trn was already high, and adding more instances degraded the accuracy. However, in the case of D diff , since the result by MEM on low, bootstrapping was effective, although it started to weaken at the last part. These accuracies are lower than not only MHLE but also the results in Step 1 (for example, we plot the precisions for Trn + L 30 in Figures 3 X 6).

Therefore, we can say that this semi-supervised WSD using a hybrid genera-tive/discriminative approach ( MHLE ) is effective (with respect to purpose (a) above).
Effectiveness of Automatically Expanded Labeled Data. We used several types of ex-panded data as labeled data; that is, Trn + EX ,+ L 1 ,+ L Note that Trn + L 30 + EXrL gave the best result in Step 1.

As shown in Table IV, all of the automatically expanded labeled data provided better results than the given training data alone. Therefore, we can say that these automatically expanded data are better than the given labeled data alone as labeled data (as regards purpose (b) above).

The labeled data Trn + L 30 + EXrL gave the best results in Step 1, but the data that achieved the best result overall was Trn + L 1 . This shows that ultimately the original (fragmentary) example sentences are no match for the real world sentences extracted from raw corpora. By comparing + L 1 with + L 30 , for easy words in particular, + L duced better results than + L 30 probably because of the sense bias. But for difficult words, + L 30 produced better results than + L 1 .

In conclusion, larger labeled data expansion is also effective for our combination method for more difficult words.

Learning Curves for Sample Words. As shown above, MHLE works very effectively, but that is no assurance that larger unlabeled data sets give better results. We show some learning curves for sample target words in D mid ,using data in Figure 8. As this figure shows, the behavior is very different from that of the target words.

For some words, accuracy remains unsaturated (For example,  X  X  yoi  X  X ood X ), and the accuracy of some words actually falls (for example,  X  X  X  motu  X  X ave X ). Therefore, we should decide whether more unlabeled instances should be added or not, by refer-ring to the results of cross-validation over the training data. In future work, we will investigate the causes of improvement or degradation. 5.4.2. Effectiveness for Unseen Senses. One of the advantages of our method is that it can provide training data even for unseen senses (which did not appear in the given training data but were in the dictionary). Therefore, we investigated the accuracy for unseen senses. In the Japanese WSD task, there are 18 instances with 9 unseen senses (See Section 3). Table V shows the results for the 18 instances.

When we use expanded labeled data, the system can sometimes guess the unseen senses. The best performance (9 correct (50.0%)) was achieved with + L labeled data were provided even for unseen senses. With + L correct.
 Of course, these unseen senses have no given labeled data ( 0% on Trn . Since, no method based on given labeled data alone ( senses correctly, this constitutes a significant improvement.
 We mentioned two known problems with WSD, unseen senses and sense bias, in Section 1. In this article, we proposed a combined method of automatic labeled data expansion (Step 1) and semi-supervised learning method (Step 2). Step 1 alone can supply labeled data even for unseen senses but tends to increase the sense bias prob-lem. Step 2 alone can adjust sense bias using unlabeled data but it can not guess unseen senses. The synergism of the Step 1 and Step 2 combination compensates for weak points. That is, our combination method can guess unseen senses and adjust sense bias, if not perfect. 5.4.3. Error Analysis. In this section, we analyze the most frequent errors. We list the test instances which were always wrongly classified, in the all experiments in Section 5.4. Table VI shows the number of listed error instances. As described in Section 2, in this article, we do not focus on guessing new senses ( X )thathaveno labeled data and are not defined in the dictionary. Therefore, nine instances, the first row in Table VI, is impossible from the start. Accordingly, we looked over the remain 173 instances ( = 192  X  9) other than the nine new senses.
 Because the error reasons are unclear, we can not elucidate the reasons correctly. However, we did identify some topical reasons. Of those 173 instances, roughly half were difficult even for humans (some were clearly mislabeled). The remainder were incorrectly classified by our system (some were idiomatic usages). In other words, al-most half of the errors may have some room for improvement.

For example,  X  X  X  X  okii  X  X ig X , whose inflected form is  X  X  X  X  okiku  X  X ig X , appears in both Examples (7) and (8). Both are tagged with  X  exaggeration  X  (5167-0-0-2), and are classified into a sense  X  major  X  (5167-0-0-1) by our system. Actually, we think the sense of Example (7) is  X  major  X  (5167-0-0-1) and the original tag is wrong. However, Example (8) has a correct tag. Correct classifying is difficult because the sentence is very short, but since the usage is idiomatic, even one labeled instance may allow our system to achieve accurate classifying. (7)  X  X  X   X   X  X  X  X  X  X   X   X   X  X   X   X  X  X  X   X   X  X  X  X   X  X  X  X  X  (8)  X  X  X  X   X   X   X   X  5.4.4. Impact of Features Derived from Automatically Extracted Training Data. To analyze the effect of automatically extracted labeled sentences on WSD classifiers based on MEM and MHLE , P ( s k | x ;  X  W ) and P ( s k | x ;  X  W ,  X  , we introduce an entropy score of the weights  X   X  ik estimated for the i th feature as the following. where  X   X  ik =  X  w ik for MEM and  X   X  ik =  X  w ik +  X  log the  X   X  ik value for sense s k is the same as or very similar to the senses s k ( k = k ) . In contrast, small e i values show that the different from the  X   X  ik values for other senses s k ( k values are small work more strongly for discriminating the senses of new samples with trained WSD classifiers than other features. To confirm the effect of extracted labeled sentences, we examined whether the e i values of features included in the extracted labeled sentences were smaller than those of other features.

We focus on  X  Trn + L 1  X  X or MEM ,and X  Trn + U 300 + L 1  X  X or MHLE , because they use minimal numbers of extracted labeled sentences and may make it easier to discern the impact. To find the features derived from just extracted labeled sentences, we com-pared the features of  X  Trn + L 1  X  X nd X  Trn + U 300 + L 1 respectively.

We explain here the actual flow and take  X  Trn + L 1  X  as an example. (1) we sort the features on X  Trn + L 1  X  by entropy e i . (2) we divide the features into 10 groups with bin size of 10%. (3) we find the features different between  X  Trn  X  X nd X  Trn (4) we investigate which of the 10 groups hold the different features. In Table VII, we show the results for different features between  X  then in Table VIII, for  X  Trn + U 300 + L 1  X  X nd X  Trn + U accuracies are 77.4% for  X  Trn  X , 78.5% for  X  Trn + L 1  X , 82.8% for  X  for  X  Trn + U 300 + L 1  X .

As shown in Tables VII and VIII, the features derived from extracted labeled data tend to have smaller entropy, that is, they are more effective in discriminating the senses. In the case of  X  Trn + L 1  X , Table VII, more than 45% of the different features are located in the top three groups. In the case of  X  Trn 40% are located in the top group, and when it comes to D mid group. According to Table IV,  X  Trn + U 300 + L 1  X , yielded the best results overall and in D mid . Therefore, we can say the features derived from extracted labeled data worked effectively, especially in the case of MHLE . We showed the effectiveness of our method for unseen senses in Section 5.4.2. However, the number of unseen senses was very small, so we conducted a more comprehensive experiment. We also compare the quality of original labeled data tagged by humans to that of automatically extracted data, and investigate the commutativity.
In this experiment, first, we delete the original labeled data tagged with a sense for each target word, that is we make pseudo unseen senses. Second, we add original hand-made ( Trn ) or automatically extracted labeled instances ( little by little, and examine the resulting learning curves.
 In this article, we employ two methods to select pseudo unseen senses: [1] we select a sense randomly per word for all 50 words (we call this Rand frequent sense per word for 21 words (we call this Mfs ) 5 The random selection ( Rand ) may look fair, however, the size of balanced. Therefore, in Mfs , to compare the results achieved with the same number of added Trn and Auto , we selected 21 target words which have more than 30 sentences in both Trn and Auto .

We show the data size of selected pseudo unseen senses in Table IX. We add the labeled instances in Trn or Auto tagged with the pseudo unseen senses, then calculated the learning curves for Rand Figures 9 and 11, and for
In Figures 9 X 12, MHLE-Trn and MHLE-Auto show the results applying MHLE with 300 unlabeled instances ( U 300 ), because this gave the best performance in most cases of Section 5.4. We also show the cases applying MEM over the same labeled data ( and MEM-Auto ).

Figures 9 and 10 show the accuracies over the whole test data of the target words.

Figures 11 and 12 show the performance over the pseudo unseen senses. In these fig-ures, we use F measures, because using more labeled data tagged with pseudo unseen senses may tend to mislabel other senses as pseudo unseen senses.

The x-axes in Figures 9 X 12 show the number of added labeled sentences (the max number is limited to 30). However, as shown in Table IX, in the case of senses have less than 30 labeled sentences in Trn and Auto labeled sentences than the number indicated by x-axes, we show the results with all labeled sentences. That is, even if the number on the x-axes are 30, some target senses are added only one or so labeled sentences in Rand .

On the other hand, in the case of Mfs , because the target pseudo unseen senses have more than 30 labeled sentences in both Trn and Auto , we used exactly the same number of sentences as shown by the x-axes. 6.2.1. Comparing Hand-Made and Automatically Extracted Labeled Data. In the cases of ap-plying MHLE , for up to five sentences, adding Auto and Trn accuracy. However, when adding more than 10 sentences, Trn Auto . The reasons might be that Auto includes some errors and it X  X  variation is limited because of the strict condition established at Step 1. In both the learning curve became almost flat in the last half.
 When applying MEM , there is much more difference between reflecting the variation or quality of labeled data, although the line of completely flat even when adding 30 sentences. 6.2.2. Comparison with the Baselines on MEM . In this section, we compare the results by MHLE and MEM . In Figures 9 X 12, MEM-Trn and MEM-Auto are the results of applying MEM over the same labeled data with MHLE-Trn and MHLE-Trn .

In the case of comparing MHLE-Trn and MEM-Trn ,or MHLE-Auto much better accuracies than MEM . The largest difference between is +14.1% in Rand , and +27.1% in Mfs when adding one sentence. This shows that MHLE has very high classification performance even if the labeled data is very small. In other words, in Mfs , MHLE can give almost the same performance with the addition of just one (or five) labeled sentence(s) as MEM ( MEM-Trn beled sentences. Furthermore, MHLE -based results indicate higher performance than MEM regardless of labeled data size. The same trend was seen between MHLE-Auto and MEM-Auto .

Comparing MHLE-Auto to MEM-Trn , MHLE-Auto shows much higher performance when the labeled data is very small, and almost the same performance after 15 sentences. Therefore, MHLE with automatically extracted labeled data is better than MEM with hand-made labeled data. 6.2.3. Performance over Pseudo Unseen Senses. We investigate the performances for the pseudo unseen senses (Figures 11 and 12). As for the whole test data (Figures 9 and 10), the initial parts of MHLE-Trn and MHLE-Auto are much better than MEM-Auto . Increasing the amount of labeled data makes the difference decrease, then MEM-Trn becomes the same as or exceeds MHLE-Auto but not MHLE-Trn
Note that, in the case of no addition, because there is no labeled data for pseudo un-seen senses, MEM , which is a supervised learner, cannot guess pseudo unseen senses ( F measure is 0%). However, MHLE can guess some pseudo unseen senses, its F measure is 19.8% (recall of 11.1%), because, MHLE , which is a semi-supervised learner, assumes that unlabeled data that is very different from labeled data is unseen sense and use them for learning, allowing it to develop classification rules for unseen senses. Fur-thermore, even just one labeled sentence yielded a strong improvement, F measure of 66.3% (recall of 51.8%) in MHLE-Auto .
 The final performance of MHLE-Auto is lower than or equal to that of MEM-Trn , however we can say it is very high, if we consider the fact that these are the results over unseen senses and MHLE-Auto does not need hand-made labeled data. 6.2.4. Effectiveness of Previous Addition of Automatically Extracted Labeled Data. Further experiments were performed using several settings. We described above that the au-tomatically extracted training data ( Auto ) does not yield the same quality as the hand-made training data ( Trn ). However, of course, because it was extracted automatically, if it has no bad influence, we can use the data at the beginning, which may reduce the amount of hand-made training data needed, or we can add training data by hand depending on budget and time.
 Therefore, we investigate the influence by previous addition of 1, 5, 15, and 30 before adding Trn . The learning curves are also shown in Figures 13 X 16, for reference, MHLE-Trn are also shown.

From Figures 13 X 16, the curves converge on the almost same point, therefore, the influence of the automatically extracted data is not bad. In addition, the starting point became much higher, for example only five sentences in Rand and 77.4% of F measure for pseudo unseen senses. As shown in these figures, adding automatically extracted training instances is very beneficial when it is hard to get hand-made training data. Then also we can reduce the amount of needed hand-made labeled data.

Therefore, from the point of view of practical aspect, it is good methods that can automatically and previously create several labeled instances in advance, then make labeled data by hand depending on budget and time. In this article, we proposed a combined WSD method consisting of automatic la-beled data expansion (Step 1) and semi-supervised learning (Step 2). We targeted the SemEval-2010 Japanese WSD task, and showed the effectiveness of our proposed method.

In Step 1, we automatically extract labeled data from raw corpora. We could extract longer, more natural and higher quality labeled data even for unseen senses.
This step by itself yields higher performance (80.2%) than the best result (76.4%) in the formal run of the Japanese WSD task.

Step 2 employs semi-supervised learning. As the semi-supervised learning method, we employ the hybrid generative/discriminative approach ( MHLE ), because this method is known to be robust even when the labeled and test data are very different as in the Japanese WSD task, since they come from heterogeneous corpus.

As a result, in this step MHLE achieved a good performance (82.8%), even when using only the given training data as labeled data. Moreover, we showed the effectiveness of our expanded data as labeled data. We investigated which type of expanded data was the best as labeled data, and showed that adding only one sentence per original example was the best as labeled data (84.2%); it makes it possible for the system to guess even unseen senses (33.3%). In other words, when using labeled data for semi-supervised learning, minimum expansion provides the best performance and protects the system against sense bias.

However, for the words in D mid and D diff , adding 30 extracted data instances per word gave better results than adding one instance; the accuracy became 50.0% for unseen senses. Therefore, in future work, we intend to perform experiments in which we change the amount of expanded labeled data based on Entropy-based difficulties.
In addition, we investigated the commutativity of automatically extracted training data, by deleting a sense per target word and adding original training data and au-tomatically extracted data little by little. As a result, the quality of automatically ex-tracted data does not match that of hand-made training data, but it achieved high accuracy considering that it is for unseen senses. In addition, the preliminary addition of automatically extracted data helps to raise the quality of the starting point. Exper-iments showed that it is of great value when the hand-made training data is hard to obtain.

In future work, we also intend to investigate the reasons for over learning in Step 1 and Step 2, by changing the labeled and unlabeled data.
 We show all target words and the size of data in the following table.

