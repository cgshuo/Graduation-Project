 Uncertain databases have been widely developed to deal with the vast amount of data that contain uncertainty. To extract valuable information from the uncertain databases, several methods of frequent itemset mining, one of the ma-jor data mining techniques, have been proposed. However, their performance is not satisfactory because handling un-certainty incurs high processing costs. In order to address this problem, we utilize GPGPU (General-Purpose compu-tation on GPU). GPGPU implies using a GPU (Graphics Processing Unit), which is originally designed for process-ing graphics, to accelerate general purpose computation. In this paper, we propose a method of frequent itemset mining from uncertain databases using GPGPU. The main idea is to speed up probability computations by making the best use of GPU X  X  high parallelism and low-latency memory. We also employ an algorithm to manipulate a bitstring and data-parallel primitives to improve performance in the other parts of the method. Extensive experiments show that our pro-posed method is up to two orders of magnitude faster than existing methods.
 H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms, Performance GPGPU, Uncertain Databases, Frequent Itemset Mining
The problem of uncertain data management has attracted a great deal of attention due to the inherent uncertainty in the real world. For example, when analyzing purchasing behavior of customers using RFID sensors, there may be incorrect sensor readings due to errors. In addition to this, uncertain data take place in many situations. Major sources of the uncertainty are limitations of equipment, privacy con-cerns, or statistical methods such as forecasting. To deal with such uncertain data, uncertain databases have recently been developed by many researchers [1, 6, 7, 19, 20].
In the area of uncertainty data management, the applica-tion of frequent itemset mining or association rule mining [2] to uncertain databases is one of the major research issues, and there have been a number of algorithms [6, 7, 19, 20]. In fact, frequent itemset mining is a part of association rule mining, and is known to be more computationally intensive than subsequent association rule mining. More than that, frequent itemset mining from uncertain databases is differ-ent from conventional one [2, 3, 10] in the sense that we need to take into account uncertainty. The existing meth-ods suffer from performance problems due to the fact that extra costs for dealing with uncertainty, such as probabil-ity computation, is needed and this another process is often highly time-consuming. It is thus necessary to accelerate this process in order to handle large uncertain databases. To this end, GPGPU (General-Purpose computation on Graphics Processing Unit) is an attractive solution. GPGPU has recently been an interesting research subject in the field of high performance computing as well as many other fields [8, 9, 11, 12, 13, 17, 21]. GPGPU implies using a GPU (Graph-ics Processing Unit), which is a processor originally designed for processing graphics, to accelerate general purpose com-putation. The GPU can simultaneously process many data with high parallelism compared to CPUs because it has many SIMD (Single Instruction, Multiple Data) cores (tens to hundreds). Hence, growing number of researchers are try-ing to exploit the processors for various purposes, such as database processing and data mining. The architecture of these SIMD cores is inherently suitable for processes that employ one instruction on multiple data in parallel. As a result, the GPU has difficulty to deal with a procedure with many conditional branches. Therefore, it is one of typical technical challenges to develop an algorithm to make the best use of the SIMD architecture.

In fact, there have been few works using GPGPU to ac-celerate frequent itemset mining from uncertain databases, other than our previous work [13]. However, the previous approach has some limitations:
In this paper, we propose a method for frequent itemset mining from uncertain databases using GPGPU. Our work aims to overcome the above limitations. To remove the un-necessary computations, we employ a filter primitive [12]. Besides, to further optimize the computations, we use a tree-based approach with low-latency memory on the GPU. We also propose an algorithm to efficiently manipulate a bit-string by using GPU intrinsic functions. In addition, data-parallel primitives are utilized to increase speeds of the other parts of the existing approach. Extensive experiments show that our proposed method is up to two orders of magnitude faster than existing approaches.

The remainder of this paper is organized as follows. Sec-tion 2 defines the problem and describes the pApriori algo-rithm [19]. In Section 3, we briefly introduce GPGPU and data-parallel primitives. In Section 4, we propose a method for processing pApriori using GPGPU. Section 5 evaluates the performance of the proposed scheme. Section 6 shortly reviews related work, and in Section 7 we conclude and men-tion future work.
First, we define the conventional (non-probabilistic) fre-quent itemsets in Section 2.1, and then extend the problem for uncertain databases in Section 2.2. Our definition basi-cally follows [19]. In Section 2.3, we describe the pApriori algorithm, which was proposed by Sun et al. [19].
Let I denote a set of all items. A set of items X I is called an itemset and a k -itemset means an itemset that contains k items. A pair of ID and an itemset is called a transaction T . 1 A transaction database T is defined as a set of transactions. Given a transaction database T of size (the number of transactions) n and an itemset X , the support of X , which is denoted by sup ( X ), is defined as the number of transactions in T that include X . An itemset X is called a frequent itemset , if sup ( X ) minsup , where minsup is a support threshold given by users.
A transaction database T is called an uncertain transac-tion database (UTDB) U , when each transaction in T has an existential probability that indicates the probability that the transaction exists in T . Table 1 shows an example of a UTDB, which represents purchase records of some cus-tomers. For example, game and music is purchased together with the probability 0 : 8.

A UTDB is often interpreted with the possible world se-mantics [19, 20]. Under this notion, a UTDB generates a set of possible worlds , each of which contains a combination of transactions in the UTDB. For instance, the UTDB in Table 1 produces 16 possible worlds such as ; ; f T 1 g ; f T 2
In th is paper, a transaction is used to mean the itemset of the transaction for the set-theoretic inclusion relation. f
T 1 ; T 2 g , etc. Each world is also associated with an existen-tial probability that is the product of the probabilities of the world X  X  transactions. In Table 1, a world f T 1 g has the probability that the transaction T 1 exists and T 2 ; T 3 do not exist, i.e., 0 : 8 (1 0 : 7) (1 0 : 9) (1 0 : 5) = 0 : 012.
In this interpretation, the support of an itemset X varies with each world and becomes a random variable. The proba-bility mass function of the support is called a support proba-bility mass function (SPMF), denoted by f X . In particular, f ( k ) is the probability that sup ( X ) = k , where k is an integer that sup ( X ) can take, i.e., k 2 [0 ; jUj ]. For exam-ple, Figure 1 illustrates f f music g in the UTDB in Table 1. In principle, f f music g ( k ) can be calculated by summing the probabilities of the worlds where the support of f music g equal to k .

An itemset X is called a probabilistic frequent itemset (PFI), if where minprob 2 (0 ; 1] is a probability threshold given by users. In other words, a PFI is an itemset whose support is greater than or equal to minsup with a probability at least minprob . For example, when minsup = 2 and minprob = 0 : 5 with the UTDB in Table 1, the itemset f music g is a PFI, because
P ( sup ( f music g ) minsup ) = f f music g (2) + f f music
Having defined a UTDB and PFI, the problem addressed in this paper is defined as follows. This problem is called Probabilistic Frequent Itemset Mining (PFIM).

Problem. Given a UTDB U , minsup , and minprob , ex-tract all PFIs from U . The Apriori algorithm, which was originally proposed by Agrawal et al. [3], is a well-known algorithm for association rule mining. Sun et al. [19] proposed pApriori that adapts Apri ori to uncertain transaction databases. The pApriori algorithm can be divided into two procedures: 1. Generating a set of candidate k -itemsets C k from a set 2. Extracting a set of k -PFIs L k from C k
The pApriori algorithm continues these procedures alter-nately with incrementing k by one until no additional PFIs are detected. Note that, in the beginning of the algorithm, k  X  X  value is one and each candidate 1-itemset is an itemset whose element is merely one item in U . Finally, the pApriori algorithm returns all the PFIs extracted from U .
The procedure of generating candidate itemsets can be separated into two phases. The first phase is a merging phase and the second phase is a pruning phase.
 In the merging phase, it is verified that two ( k 1)-PFIs, X and Y , are joinable , that is, X and Y satisfy X:item 1 Y:item 1 ^ : : : ^ X:item k 2 = Y:item k 2 ^ X:item k 1 &lt; Y:item k 1 , where X:item i denotes the i th item of X in a certain order. If X and Y are joinable, a candidate k -itemset is created as the union of X and Y , and is stored in a set of candidate k -itemsets C k .

In the pruning phase, the following lemma is used in order to prune the candidates [19].
 Lemma 1 (Anti-monotonicity). If an itemset X is a PFI, then any itemset X 0 X is also a PFI.
 The contraposition of this lemma yields that if any itemset X 0 X is not a PFI, then the itemset X is not a PFI either. Hence a candidate k -itemset can be pruned out when any size-( k 1) subset of the candidate is not contained in a set of ( k 1)-PFIs L k 1 . If the candidate can be pruned out, it is deleted from C k .
In order to reveal whether or not an itemset X is a PFI, the SPMF of X needs to be computed and assigned to Equa-tion 1. Since the number of possible worlds is exponentially large, a na  X   X ve solution to compute SPMFs is considered to be intractable. To address this problem, Sun et al. proposed two algorithms, a dynamic programming approach and a di-vide and conquer approach. Here, we only describe the latter algorithm because it is shown to be more efficient and more suitable for GPGPU.

The algorithm takes a UTDB U and an itemset X as its inputs, and returns the SPMF of X . If U contains only one transaction, it just computes the SPMF and returns the re-sult. If U contains two or more transactions, it horizontally partitions U into two UTDBs ( U 1 and U 2 ), and recursively applies the same algorithm to them. Having finished the computation for U 1 and U 2 , the algorithm combines the re-sulting SPMFs into one SPMF and returns it as output. The point here is that two SPMFs are efficiently combined into one by convolution [19]. Although a na  X   X ve algorithm of the convolution has O ( n 2 ) time complexity, it can be im-proved to O ( n log n ) time complexity with the Fast Fourier Transform (FFT) algorithm.
Owing to the high computational complexity of SPMF computations, it is desirable to prune infrequent itemsets without computing the SPMFs. Let cnt ( X ) be the number of transactions that contain an itemset X in a UTDB U regardless of the existential probabilities. In addition, let esup ( X ) be the expected support of X . With these two values, Sun et al. proved two lemmas [19]. By using these lemmas, candidates can be pruned in O ( jUj ) time.
Lemma 2. If cnt ( X ) &lt; minsup , then an itemset X is not a PFI.
 Lemma 3. Let = esup ( X ) and = ( minsup 1) = . Then, X is not a PFI if,
GP GPU (General-Purpose computation on GPU) implies using a GPU (Graphics Processing Unit), which is a co-processor originally designed for graphics processing, to ac-celerate general-purpose computation [8, 9, 11, 12, 13, 17, 21]. The GPU has recently been paid much attention from numerous researchers and developers as a many-core pro-cessor, because of its high performance, low price, and low power consumption, compared to CPUs. The GPU contains multiple SIMD processors, each of which is called a Stream-ing Multiprocessor (SM), and one SM consists of many pro-cessing units, each of which is called a Streaming Processor (SP).
When developing GPGPU applications, the most common framework is CUDA 2 provided by NVIDIA. CUDA enables developers to write a program with the C programming lan-guage. CUDA provides fine-grained parallelism by spawning tens of thousands of lightweight threads. These threads are grouped together into multiple thread blocks (or blocks in short). Threads within a block are allocated to a same SM and can cooperate, sharing a local memory of the SM. The blocks compose a grid , which is generated for one process on the GPU.

Meanwhile, the GPU has several kinds of memories. The largest memory on the GPU is called global memory . For instance, NVIDIA Tesla C2050 has a global memory of 3GB. This memory can be accessible from all threads of GPU, and has a high bandwidth and a high access latency. In addition, the GPU contains another type of memory called shared memory , which is shared by all threads in a block and can be used to exchange data among the threads. The size of the shared memory is small, while the access latency is much lower than the global memory. To use these memories appropriately is one of the most important points in order to reach GPU X  X  peak performance.
In this subsection, we describe data-parallel primitives on the GPU [11, 12, 23]. These primitives are highly optimized for the GPU, and can be used as building blocks to imple-ment other operations. ht tp://www.nvidia.com/object/cuda_home_new.html
A r educe operation takes an array of n elements [ a 0 ; a a n 1 ] and a binary associative operator , and returns the value a 0 a n 1 . If the binary operator is addition, the reduce operation computes the sum of the array. To implement the reduce primitive, we adopt the tree-based approach described in [23].
A scan , also known as pre x sum , operation takes an ar-ray of n elements [ a 0 ; a 1 ; :::; a n 1 ] and a binary associative operator with identity I , and returns the array [ I; a 0 a ; :::; a 0 a 1 a n 2 ]. This operation is a useful build-ing block for other primitives and many algorithms [12, 24]. We adopt the implementation from GPU Computing SDK 3 .
Although the above mentioned pApriori algorithm achieved an efficient way of probabilistic frequent itemset mining, even better performance is desirable when dealing with huge databases. To this end, we aim at improving performance of the pApriori algorithm exploiting the massive parallelism of a GPU.

We now describe a method for probabilistic frequent item-set mining using GPGPU. Here, we assume that items in transactions are denoted as integer values from zero to j After transferring necessary data from a CPU to the GPU, this method runs entirely on the GPU. In Section 4.1, we ex-plain data layouts on the GPU. Then, Sections 4.2 and 4.3 describe algorithms for generating candidate itemsets and extracting PFIs, respectively.
We represent a UTDB U using two (1-dimensional) arrays on the GPU: an array of itemsets in transactions and an ar-ray of existential probabilities. For the first array, we use the ELL format, which is a format for representing sparse matrices. It is known that this format is well-suited to the GPU X  X  architectures [5]. In the ELL format, an M -by-N sparse matrix with at most K nonzeros per row is stored as a dense M -by-K array of nonzeros and an array of col-umn indexes. When considering itemsets of transactions in a UTDB as a matrix, each element contains merely one or zero. Therefore, we can represent the itemsets by storing only the latter array. In this case, K is the maximum size of transactions.

For example, the UTDB in Table 1 can be represented as arrays shown in Figure 2. In the figure, game, music, and video are denoted as 0, 1, and 2, respectively. Each row of the first array represents the itemset of a transaction. Since the length of each row is the maximum size of transactions, shorter rows are padded with a sentinel denoted as X. We use the number of items j I j as the sentinel in our implemen-tation. The second array just stores existential probabilities corresponding to the transactions. Note that the first array is stored in column-major order, i.e., the array is laid out in linear memory as 0, 1, 0, 0, 1, 2, X, 1, X, and so on. This lay-out enables the GPU to coalesce multiple memory accesses into one transaction when processing inclusion check that is described in Section 4.3.1. ht tp://developer.nvidia.com/gpu-computing-sdk Alg orithm 1 Extracting PFIs. 1: f or all C in C k in parallel do 2: // Section 4.3.1 3: Inclusion check of C 4: end for 5: for each C in C k do 6: // Section 4.3.2 7: Pruning C 8: // Section 4.3.3 9: Filtering transactions 10: // Section 4.3.4 11: Computing the SPMF of C 12: end for
Mea nwhile, we use an array of integers to represent item-sets. A k -itemset is stored in an integer array of k elements. Let now S k be a set of k -itemsets, which corresponds to k -PFIs or candidate k -itemsets. Then, S k is stored in an integer array of k jS k j elements in the following way. The first itemset in S k is stored in the array from 0 to k 1 ele-ments, and the second itemset is stored in the array from k to 2 k 1 elements. In other words, the i th itemset occupies the elements from ( i 1) k to i k 1.
The algorithm to generate candidates is similar to that of pApriori, although the details are different to adapt to GPGPU.

First, it allocates an integer array on the GPU that can store all candidate itemsets, i.e., an array of k ments, where L k 1 is a set of ( k 1)-PFIs. Then, it checks in parallel that pairs in L k 1 are joinable. If X i and X joinable, where X i is the i th PFI in L k 1 , they are combined into a candidate k -itemset, and the candidate stored in the array at an index corresponding to i and j . This index can be computed as ( jL k 1 j 1) i i ( i + 1) = 2 + j 1. If X i and X j are not joinable, 1 is assigned to the array at the index to indicate that the candidate is not a PFI. In the pruning phase, if it is confirmed that L k 1 does not contain any size-( k 1) subset of the candidate, 1 is assigned to the array at the index of the candidate as before.
By allocating the array that stores all the candidate k -itemsets, we can generate the candidate itemsets with a few conditional branches in parallel.
Algorithm 1 shows the procedure to extract PFIs that comprises four steps: inclusion check, pruning, filtering, and SPMF computation. Note that inclusion check is processed in parallel for all candidates and the other steps are taken for each candidate. The following sections describe these four steps one by one.
It is necessary to compute an SPMF in order to judge whether a candidate X is a PFI. To compute the SPMF, we firstly need to check whether each transaction in a UTDB U includes the candidate X . Here, we call this process in-clusion check of the candidate X in the UTDB U . In fact, a straight forward implementation of inclusion check using iterative loops with conditional branching deteriorates the performance, because GPUs cannot execute such codes in parallel due to the architecture X  X  limitation. Although lat-est GPUs support parallel execution of conditional branch-ing, it is more desirable to investigate a method that can be executed efficiently on GPUs.

First, jC k j arrays of jUj elements need to be prepared in order to store the results of inclusion checks, where C k set of candidate k -itemsets. Since the arrays only holds one or zero, it is wasteful to represent the result of each trans-action as one integer element. In addition, as the number of candidates is increasing, the arrays get larger and larger. As a result, the size of the arrays may exceed a capacity of the global memory. Therefore, we store the results to 4-byte integers arrays as bitstrings, thereby reducing the number of necessary elements to djUj = 32 ejC k j .

In the inclusion check, each block on the GPU deals with 32 transactions and each thread in a block checks jC k j =B candidates, where B is the number of threads in one block. In other words, one thread conducts the inclusion checks of jC k j =B candidates with regard to 32 transactions, and stores these 32 binary values into one element of the resul-tant arrays. We allocate one thread to 32 transactions so that the thread can store the results to the arrays without a lock because each element of the resultant arrays is con-sidered as a 4-byte integer. Besides, transactions that are accessed from a block are transferred to the shared mem-ory before the inclusion checks, because the transactions are shared by threads in the block and are read multiple times. Furthermore, since the threads read consecutive addresses on the global memory thanks to the column-major order, these accesses are coalesced into fewer memory reads. and the transfers can be done fast.

For each candidate, each thread judge whether transac-tions include the candidate by linear search. Linear search enables threads in a block to read same data on the shared memory at the same time. These memory accesses are broadcasted to all the threads and thus take only one ac-cess time, resulting in quite fast memory reads. A bit cor-responding to the transaction is set to one, if a transaction includes all items in the candidate; otherwise the bit is set to zero.

Optimization. In the case of k &gt; 1, it is not necessary to scan the whole database by reusing the inclusion check results for k 1. In this case, we borrow the idea from [8]. The inclusion check of a candidate k -itemset X is processed as bitwise AND operations between two ( k 1)-PFIs that are used to create the candidate X . For example, the inclusion check of f game ; music g can be computed as bitwise AND operations between the two arrays of the inclusion checks of f game g and f music g . This method substantially reduces accesses to the global memory and make inclusion checks run much faster.
 Alg orithm 2 Decode a bitstring.
 Input: in teger x 1: n1s = __popc(x) // the number of bits set to one 2: acc = 1 // accumulating the position of the first set 3: for i = 0 to n1s 1 do 4: pos = __ffs(x) 5: acc = acc + pos 6: do some operation using acc as an index 7: x = x &gt;&gt; pos 8: end for
Ha ving finished inclusion check, the next step is to prune the candidates out. As explained in Section 2.3.3, pruning requires computing cnt and esup . cnt ( X ) means the number of transactions which include the candidate X . This value can be computed by summing the numbers of bits set to one in the array of the inclu-sion check of X . For this computation, the reduce primi-tive (Section 3.2.1) is utilized. Meanwhile, esup ( X ) means the expected number of sup ( X ), which can be computed by summing all existential probabilities of transactions that in-clude the candidate X . We also use the reduce primitive for this computation. However, the reduce primitive cannot be applied directly, because the elements in the arrays of inclu-sion checks are bitstrings. It is thus necessary to get indexes of the transactions encoded in the bitstrings.

To this end, we exploit two GPU intrinsic functions, __ffs and __popc [21]. __ffs takes an integer and returns the po-sition of the first (least significant) set bit. __popc takes an integer and returns the number of bits that are set to one in the binary representation. Combining these functions, an algorithm to decode a bitstring can be written as pseudo-code in Algorithm 2. This algorithm enables us to work only with bits set to one. Since __ffs returns the position of the first set bit, an index of the first transaction in a bitstring that contain the candidate is obtained at the first iteration. Then, some operation is done with acc as an index, and x is shifted right by pos bits to make the first set bit dis-appear. The algorithm repeats these operations __popc ( x ) (the number of bits set to one in x ) times.

Actually, the computation of cnt requires only the number of bits that are set to one. Hence, there is no need to use this algorithm and using __popc is enough. In the case of esup , on the other hand, the indexes of the transactions are necessary for addressing existential probabilities. For each bitstring, the existential probabilities corresponding to the transactions in the bitstring are summed at line 6 in Algo-rithm 2. The results of __popc and the summed probabilities are handed to the reduce primitives as input arrays with the addition operator.
 If the candidate is determined to be pruned by evaluating Lemmas 2 and 3, the procedure to extract PFIs continues to a next candidate; otherwise it proceeds to the next step.
To compute the SPMF of a candidate X , it is sufficient to use the transactions that include X , and the other trans-actions can be ignored because these transactions do not contribute to the SPMF. To discard the unnecessary trans-actions, we use the filter primitive that is described in [12].
We firstly decode the array of the inclusion check of X by using Algorithm 2, and store the result to an array of jUj elements. The elements of this array are ags indicating whether transactions include the candidate. Then, the scan operation is applied to the ags array with the addition operator and the result is stored in another array. This array holds indexes of the non-filtered transactions. These ags and indexes arrays are used in the next step to initialize SPMFs.
Since the computation of an SPMF is the most computa-tionally intensive part of the pApriori algorithm, we must carefully parallelize this computation.
 denoted by F X , to store first SPMFs. In other words, the number of elements is 4 multiplied by the next power of two of cnt ( X ). Then, each GPU thread computes the SPMF of X in a UTDB consisting of one transaction, by referring the precomputed ags and indexes arrays in the last step. For the i th transaction of the input UTDB, indexes [i] is the position for storing the SPMF to the output array, if ags [i] is 1. The thread stores the computed SPMF to the array F
X , so that the SPMF occupies four elements of F X . The 1. The number of SPMFs in F X must be the power of 2. Each area of an SPMF must be able to accommodate a 3. Making the size of each SPMF the power of two is
To make the size of SPMFs the power of two and larger than three, zero padding (padding extra elements with zero) is carried out. Then, the convolutions of two adjacent SPMFs on F X are computed, by performing Fast Fourier Transform (FFT), computing the pairwise products of the two trans-formed SPMFs, and performing inverse FFT (IFFT) on the multiplied one.
 For the FFT and IFFT, we use CUFFT, which is the CUDA FFT library [22]. The CUFFT library provide a simple interface for computing parallel FFTs on an NVIDIA GPU. The performance of any transform size that can be factored as 2 a 3 b 5 c 7 d (where a; b; c; and d are non-negative integers) is optimized in the CUFFT library. In particular, the library achieves the best performance when transforming the 2 a size. We can also use batch execution that is a feature of CUFFT, by storing the SPMFs in one array with the same size,
The two operations (zero padding and convolutions) con-tinue until the number of SPMFs becomes one. Finally, the SPMF is stored in the array F X [0] to F X [ cnt ( X )].
Figure 3 illustrates one iteration plus zero padding of the SPMF computation of f f music g in Table 1. f i X denotes the SPMF of f music g in a UTDB consisting of the i th transaction that include f music g . The first element of f is the value of f i f music g (0) and the second element shows f music g (1). The shaded area indicates the elements that are updated with a previous operation. It can be seen that two convoluted SPMFs are obtained after IFFT. Note that Fig ure 3: One iteration of SPMF computation.
 Figu re 4: A tree structure of SPMF computation. IFFT is only performed on half of the SPMFs and the other half is padded with zero, thereby enabling next convolutions to take SPMFs of a power-of-two size.

Optimization. The SPMF computation process is rep-resented as a tree structure in Figure 4. A node means an SPMF and its size indicates the size of the SPMF. In the figure, n denotes the number of first SPMFs, 2 d log 2 j cnt ( X ) and m denotes the height of the tree, log 2 n = d log 2 j
The SPMF computation begins from the leaf nodes. By executing convolutions repeatedly, the sizes of the SPMFs are gradually increasing and the number of SPMFs is de-creasing. That is, in the beginning of the computation, there are many small SPMFs to be convoluted. Since these SPMFs do not get so large in the first a few levels, the con-volutions can be computed on the shared memory of a GPU. Therefore, we consider dividing the SPMF computation into two phases, m 1 level phase and m 2 level phase in Figure 4. In the first phase, the convolutions are directly computed entirely on the shared memory. In the second phase, we use the previously explained algorithm to compute the convolu-tions.
We have implemented our approach using CUDA 4.1 and conducted experiments on a NVIDIA C2050, which has 14 SMs each of which consists of 32 SPs with 1.15 GHz and has a global memory of 3GB. We have also run the pApri-ori algorithm on a Intel Xeon E5620 CPU (2.40 GHz) with memory of 4GB.

We have used four datasets for the experiments. Table 2 summarizes characteristics of the datasets. The density of a dataset is computed as the average length of transactions divided by the number of items.

The first dataset connect and second dataset Accidents are real datasets that are accessible on Frequent Itemset Mining Implementations (FIMI) Repository 4 . connect is the densest one in the four datasets. For connect, existential probabil-ities are given by a uniform distribution between 0 and 1. For Accidents, existential probabilities are randomly drawn from a normal distribution with mean 0.5 and variance 0.02. The default values of minsup are 48% and 35% respectively. The third and fourth datasets, called T40I10D100K and T25I10D500K respectively, are synthetic datasets generated by IBM data generator 5 . T25I10D500K is the sparsest one in the four datasets. Existential probabilities for each dataset are given by a uniform distribution between 0 and 1, and a normal distribution with mean 0.8 and variance 0.01 respec-tively. minsup s are set to be 2.0% and 1.0% for each dataset if not specified.

For all the datasets, the default value of minprob is 0.5.
We have conducted extensive experiments to evaluate sev-eral parts of our GPU-based method. Sections 5.2.1 through 5.2.3 show the results for each part of the GPU-based pApri-ori algorithm, namely, inclusion check, pruning, and SPMF htt p://fimi.cs.helsinki.fi/ http://miles.cnuce.cnr.it/~palmeri/datam/DCI/ datasets.php computation, and Section 5.2.4 shows the result for the whole method. For all the experiments, we omit the results when changing minprob because it does not quite affect running times.
In these experiments, we evaluate the impact of the op-timization scheme (described in Section 4.3.1) for inclusion check on the performance. Figures 5(a), 5(b), 5(c), and 5(d) depict running times with varying minsup , which is the percentage of the size of dataset. In the graphs, GPU w/ opt. plots the running times that are measured with the optimized method, and GPU w/o opt. plots the running times without the optimization. We have not compared our GPU-based method with the CPU-based method, because the CPU-based method carry out inclusion check and prun-ing at the same time and thus it is difficult to compare these two methods directly.

By applying the optimization, we achieve 3 X 19x speedups on all the datasets. As decreasing the value of minsup , the optimized method gains more and more performance im-provements. This behavior is because a small value of min-sup makes the number of candidates large and then inclu-sion check must handle numerous candidates. In particular, when minsup is 1.5 % on T40I10D100K, GPU w/ opt. out-performs GPU w/o opt. by a factor of 19.2.
We also compare the two pruning methods, with and with-out Algorithm 2 (denoted by GPU w/ Alg.2 and GPU w/o Alg.2 respectively). GPU w/o Alg.2 simply loops 32 times to manipulate each bit. Figures 6(a), 6(b), 6(c), and 6(d) il-lustrates running times vs. minsup on the four datasets. We have not compared our GPU-based method with the CPU-based method for the same reason in the previous section.
From the figures, it can be said that if a dataset is dense, then the performance of Algorithm 2 degrades, and if a dataset is sparse, the algorithm is beneficial to performance. For example, GPU w/ Alg.2 is about 1.2 times slower than GPU w/o Alg.2 on connect and Accidents. On the other ha nd, on T40I10D100K and T25I10D500K, GPU w/ Alg.2 achieves better performance. In particular, this method is about 2 times faster than GPU w/o Alg.2 on T25I10D500K. The reason for this behavior can be explained as follows: The efficiency of Algorithm 2 depends on the number of bits set to one in an input integer. If most of the bits are set to on, the algorithm performs a number of wasteful instruc-tions compared to simple loops. In contrast, the algorithm can only process necessary bits with fewer instructions when few bits are set to one.
We have measured running times of SPMF computation on the four datasets. Table 3 summarizes the results that are computed as the average of the measured times for each value of k . The w/o filtering column shows the results without the filtering method described in Section 4.3.3. The m 1 = 0 and m 1 = 8 columns means the results with filtering and setting m 1 in Figure 4 to the respective value. m 1 = 0 indicates the method that uses no shared memory and computes SPMFs entirely on global memory. On the other hand, m 1 = 8 implies exploiting the shared memory. In our preliminary experiments with varying m 1 , m 1 = 8 has achieved the best performance.

The w/o filtering method much improves the perfor-mance on connect and Accidents. By contrast, the per-formance gain on T40I10D100K is modest, and moreover it is slower than the CPU-based method on T25I10D500K. The cause for such results is that the method processes all the transactions that include a large number of unnecessary ones. For instance, T25I10D500K has about 500,000 trans-actions, although the number of essential ones for SPMF computations is nearly 15,000. In fact, by filtering the trans-actions, we obtain substantial speedups as shown in the m 1 = 0 column. In particular, filtering increases the speeds by a factor of 28 on T25I10D500K. Furthermore, apply-ing filtering makes the SPMF computations for larger item-sets faster because the number of discarded transactions in-creases as k gets larger.

In addition, the method to utilize shared memory ( m 1 = 8 column) further increases the speed by a factor of 1.5. Fi-nally, our method achieves about 100x speedups on connect and Accidents, and 25 X 40x speedups on T40I10D100K and T25I10D500K. The reason for the difference between the speedup ratios is considered as follows: The time complex-ity for computation of f X is O ( cnt ( X ) log 2 ( cnt ( X ))), i.e., dependent on cnt ( X ) [19]. Furthermore, the bigger the value of cnt ( X ) is, the larger the size of SPMFs is. In our cases, the value of cnt ( X ) is likely to be high on connect (about 65,000) and Accidents (between 200,000 and 300,000), and low on T40I10D100K and T25I10D500K (between 10,000 and 15,000). Hence, GPU X  X  high parallelism is fully utilized on connect and Accidents to compute large FFTs. On the other hand, the SPMF computation is not a quite computa-tionally intensive task on T40I10D100K and T25I10D500K. Consequently, we achieve the speedups but not to the extent on connect and Accidents.
We compare three methods for probabilistic frequent item-set mining: the pApriori algorithm ( CPU ), our GPU-based method ( GPU ), and a GPU-based method of [13] ( GPU-MUD ). The running times vs. minsup on the four datasets are shown in Figures 7(a), 7(b), 7(c), and 7(d). Breakdowns of our method X  X  running times are illustrated in Figure 8 as well. Figures 7(a) and 7(b) depict the results on connect and Accidents, respectively. GPU is two orders of magnitude faster than CPU , and outperforms GPU-MUD by a factor of 3 to 5. The speedup of PFIM directly reflects that of SPMF com-putation, because the SPMF computations dominate the overall running times as shown in Figure 8.
Figures 7(c) and 7(d) illustrate the results on T40I10D100K and T25I10D500K, respectively. On these two datasets, GPU is up to 35 times faster than CPU , and achieves consider-able speedups compared with GPU-MUD . In particular, when minsup is 2.5% on T40I10D100K, the speedup ratio is 451. This performance gain is caused by the optimization of inclu-sion check and pruning, where GPU-MUD uses na  X   X ve methods. Meanwhile, since the SPMF computation does not domi-nates the running times on these two datasets as shown in Figure 8, the speedup ratios of PFIM are about the middles between those of SPMF computation and inclusion check and/or pruning.
In this section, we briefly review related work on frequent itemset mining, probabilistic frequent itemset mining, and GPGPU (General-Purpose computation on GPU).
The association rule mining problem was firstly intro-duced by Agrawal et al. [2]. Frequent itemset mining can be considered as the first step for association rule mining. To accelerate this step, many efficient algorithms were devel-oped. In such algorithms, there are two major algorithms, Apriori [3] and FP-growth [10].

Parallelization of these algorithms has been widely stud-ied in many literatures [4, 18, 16, 15, 14]. Agrawal et al. [4] presented three parallel algorithms, based upon Apriori on a shared-nothing architecture, that explore several tradeoffs such as computation, communication, and synchronization. Parthasarathy et al. [18] parallelized the Apriori algorithm that uses hash trees. They developed a parallel implemen-tation on a shared-memory multi-processor, taking into ac-count load balancing, data locality, and false sharing. Liu et al. [16] proposed a cache-conscious FP-array to improve the spatial locality of FP-growth, and a lock-free dataset tiling approach to fully utilize a modern multi-core processor. Li et al. [15] designed a parallel algorithm of FP-growth on a massive computing environment using MapReduce. More recently,  X  Ozkural et al. [14] introduced a data distribution scheme that divides the frequent itemset mining task in a top-down manner, and developed two parallel algorithms based on this data distribution scheme.
While the above mentioned parallel algorithms work well for the conventional certain transaction databases, they can-not effectively process frequent itemset mining from uncer-tain databases, which gains increasing importance in order to handle data uncertainty. There is a lot of work for model-ing, querying, and mining such uncertain data (see a survey by Aggarwal et al. [1] for details and more information).
To mine frequent itemsets with taking into account the uncertainty, a number of methods have been proposed [7, 6, 19, 20]. Chui et al. [7] proposed the U-Apriori algo-rithm that computes the expected support of itemsets by summing up all itemset probabilities. Later, Bernecker et al. [6] found that the use of expected support may result in missing important itemsets. Instead, they consider the probability that an itemset is frequent. While [6, 7] deal with the uncertain data model that assume each item in transactions is associated with a confidence value, which is called attribute-uncertainty model , Sun et al. [19] consider the tuple-uncertainty model , where each transaction has an existential probability. Wang et al. [20] proposed a model-based approach to accelerate probability computations for both the attribute-uncertainty model and tuple-uncertainty model.
GPGPU is another hot research topic and has been paid attention from database community as well as high perfor-mance computing researchers. Govindaraju et al. [9] pre-sented GPUTeraSort that used a GPU as a co-processor to sort databases with billions of records. He et al. [11] devel-oped a set of data-parallel primitives, and implemented sev-eral relational join algorithms for GPGPU, using the prim-itives as building blocks. In [12], He et al. extended the primitives, implemented other relational operators, devel-oped cost models for CPU-GPU coprocessing, and finally presented a CPU-GPU hybrid method. Additional infor-mat ion of GPGPU can be found in a survey by Owens et al. [17].

Frequent itemset mining from X  X ertain X  X atabases on GPU was studied by Fang et al. [8]. They proposed two ap-proaches, a GPU-based method and a CPU-GPU hybrid method, for the GPU with massive SIMD parallelism. The GPU-based method utilized bitstrings and bit operations for fast support counting, running entirely on the GPU. The hy-brid method adopted the trie structure on CPU and counted supports on GPU.

Our previous work [13] proposed an approach for prob-abilistic frequent itemset mining using GPGPU. This ap-proach has some limitations and our work aims to overcome them. To this end, we decompose the procedure extracting PFIs into four steps and then optimize each step for GPU. In particular, we employ the ELL format instead of the CSR format, which is used in [13], a bitstring-based method, data-parallel primitives such as reduce and scan, and a tree-based approach.
This paper proposed a method for probabilistic frequent itemset mining using GPGPU based on the pApriori algo-rithm. The main idea is to accelerate SPMF computations by employing a filter primitive and tree-based approach with low-latency memory on the GPU. We also presented an al-gorithm to efficiently manipulate a bitstring and utilized a reduce primitive to achieve high parallelism. We have imple-mented our method using CUDA and compared the perfor-mance with the original pApriori algorithm and our previous GPU-based approach. We observe that our method outper-forms the pApriori algorithm by a factor of two orders of magnitude on a non-sparse dataset, and by a factor of 35 on sparse datasets. In addition, the experiments show that our new GPU-based approach runs considerably faster than the previous one, on sparse datasets in particular.
 As future work, we plan to extend our method to multi-GPU system, which utilizes multiple GPUs. This work is supported in part by JSPS KAKENHI Grant Number 24240015 and the HA-PACS Project for advanced interdisciplinary computational sciences by exa-scale com-puting technology. [1] C. C. Aggarwal and P. S. Yu. A Survey of Uncertain [2] R. Agrawal, T. Imielinski, and A. Swami. Mining [3] R. Agrawal and R. Srikant. Fast Algorithms for [4] R. Agrawal and J. C. Shafer. Parallel Mining of [5] N. Bell and M. Garland. Implementing Sparse [6] T. Bernecker, H.-P. Kriegel, M. Renz, F. Verhein, and [7] C.-K. Chui, B. Kao, and E. Hung. Mining Frequent [8] W. Fang, M. Lu, X. Xiao, B. He, and Q. Luo.
 [9] N. K. Govindaraju, J. Gray, R. Kumar, and D. [10] J. Han, J. Pei, and Y. Yin. Mining Frequent Patterns [11] B. He, K. Yang, M. Lu, N. K. Govindaraju, Q. Luo, [12] B. He, M. Lu, K. Yang, R. Fang, N. K. Govindaraju, [13] Y. Kozawa, T. Amagasa, and H. Kitagawa. Fast [14] E.  X  Ozkural, B. U  X car, and C. Aykanat. Parallel [15] H. Li, Y. Wang, D. Zhang, M. Zhang, and E. Y. [16] L. Liu, E. Li, Y. Zhang, and Z. Tang. Optimization of [17] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, [18] S. Parthasarathy, M. J. Zaki, M. Ogihara, and W. Li. [19] L. Sun, R. Cheng, D. W. Cheung, and J. Cheng. [20] L. Wang, R. Cheng, S. D. Lee, and D. W. Cheung. [21] NVIDIA. CUDA C Programming Guide. http:// [22] NVIDIA. CUFFT Library User Guide. http:// [23] M. Harris. Optimizing Parallel Reduction in CUDA. [24] M. Harris. Parallel Prefix Sum (Scan) with CUDA.
