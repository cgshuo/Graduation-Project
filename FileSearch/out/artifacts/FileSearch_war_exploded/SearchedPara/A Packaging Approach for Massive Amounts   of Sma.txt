 Modern science and technology provide many advantages for obtaining geospatial information, and the development of Internet of Things (IoT) requires massive information management. For example, satellites collect petabytes of geospatial data every day, while remote sensors and urban sensing activities are accumulating data at a comparable faster pace[1]. The massive geospatial files needs the support of corresponding storage technology and information retrieval technology to serve the WebGIS application. The massive network storage system has attracted more attention to solve this problem. Depending on cluster, grid and distributed file system, data storage and information retrieval services with a uniform interface[2]are provided to cooperate the different storage equipments and software together . 
The Hadoop Distributed File System (HDFS) is a distributed file system designed to be deployed on low-cost hardware[3] with high fault-tolerance. HDFS is widely used in research for its open source and advanced architecture. It mainly settles the massive storage problem and data consistency. However, for massive small spatial files, HDFS does not perform well. In addition, lacking of consideration of the spatial are low for spatial data, which cannot satisfy WebGIS. 
In this paper, according to the spatial adjacency between geospatial files, we proposed a method to pack a group of small files into one large logical file, and set up block inner index. The experimentation proved that this method reduces the size of block indices and increases the speed to search and retrieve the massive small spatial files. In distributed file systems, metadata is used to describe the storage file X  X  content and location, which plays an important role for data locating and filename searching. Currently, distributed file systems place the metadata on the metadata server. The typical structure of metadata servers is master/slave, and map-reduce is used as its computing model to locate the file[4]. 
There are two types of file systems used in data intensive applications[7], general designed for High Performance Computing applications which run on large clusters with the needs of high scalability and concurrent storage I/Os. Examples of general parallel file systems include Sun X  X  LustreFS[11] and the open source Parallel Virtual file system (PVFS) [10]. 
Distributed file system is widely used in Internet services. Google File System (GFS) [12], Hadoop Distributed File System(HDFS) [8] and Amazon Simple Storage Service (3S) [16]are typical paragons which support cloud computing environment. processing massive numbers of small files. Whereas, in the cloud environment, lots of enterprises would like to publish their files, most of which are consisted of small files. Dissatisfactory performance of handling small files becomes a bottleneck of GFS, HDFS and 3S, in progress towards cloud applications. 
Researches on small file storage based on HDFS can be classified into two categories: general solutions and special solutions to meet the demand of specific applications. The former includes Hadoop Archives (HAR), SequenceFile and MapFile[8]. A HAR is a file archiving faci lity that packs files into HDFS blocks, which contains metadata. A SequenceFile provides a persistent data structure for binary key-value pairs. It uses file names as the key and file contents as the value to support compressing and decompressing at record level or block level. A MapFile is a type of sorted SequenceFiles with an index to permit lookups by key. It consists of are sorted in key order. As for special solutions, recent papers[13][14][15] are proposed some attracting solutions we present below. Since the massive number of small files makes the metadata table very large, the metadata server X  X  performance may be the system X  X  bottleneck. Therefore, we take analysis of the geospatial information storage and applications, and develops an engine on HDFS for accessing geospatial files. The engine can help us manage the massive amount of small files effectively. 
We present the metadata catalog strategy and design index structure.Firstly, we denotes the file X  X  location in this area. For example, we split the region 1 into 12*12 every file in the group has a unique code. We order them by the Hilbert coding rule. With this method every small file has an order in the group. We store their sizes and the blocks and the storage for these groups are shown in Fig 1. 
After the packing step, we set up the order of the group with hashing method, so second is the files X  order in the group. We only store the group id and block-id in the group index, in order to guarantee that the small files X  index file can be found when needed. In the experiment, we use the hashing algorithm to get the index file, and use the HDFS to store the packed files and index files. We set up a HDFS system and conduct experimentations. There are three DataNodes and one NameNode in our HDFS: A IBM Server (2 Intel CPU 3. 20GHz, 2 GB memory and 1 TB disk), acts as NameNode; DELL Servers (2 Intel CPU 2. 00GHz, 1 GB memory and 500GB disk), act as DataNodes. The hadoop file system X  X  version is 0.20.2,there are 178560 files all together with the total size is 47. 5GB involved in the experiments. 
In our HDFS, every file is stored in one or more blocks. If we did not pack the small files into one logical file, the space of storage would be wasted. In addition, a proper block size should be set to minimize the number of blocks so as to improve the number of files group. The files X  size is 1024K for every group, and the block size is 64M. The ratio of space utilizing is shown in the average value as fig 2. 
As what we can assume from the graph, if the size of group does not reach the size exceeds it, the ratio of space utilization is only affected by the times of the group size dividing the block size. 
We put the files into a list of directories. Each directory is a group of files, which are packed into one block. One group has 12 files or the times of 12 files because of the quad-splitting of spatial index. The time of writing files in HDFS is shown here also includes the packing time. 
As shown in Fig 3, the small files packing method is effective for time saving, especially for the groups with large numbers of small files in one block. We test the time of creating index for every group of files. Result show that it only took about 443 seconds to pack files and create the file index for 17856 files. Obviously, the time is comparatively short for such number of files comparing with the writing time. each group, we use the same strategy as writing. The time of reading the files is shown below. Y-axis is time cost in seconds; X-axis is the number of files. 
As shown in the figure, the time of reading files from HDFS in groups is shorter than that not in groups. Yet it is not obvious when the number of files exceeds 1000. In addition, the group number has little influence for reading. 
Packing the files with spatial adjacency into one logical file can improve the small files accessing efficiency, but the number of small files for one logical file is limited, files descends, and the reading speed for each small file slows down quickly. When with the strategy without group. Therefore, we should select appropriate number of small files and the size of logical file relative to block size. A certain space for some data accessing. In this paper, we introduce the metadata group strategy with HDFS, which uses a two-steps index for geospatial small files metadata management. The experiment shows aspects: Acknowledgment. This research is sponsored by National Basic Research Program of China (973 Program) No. 2011CB302302 
