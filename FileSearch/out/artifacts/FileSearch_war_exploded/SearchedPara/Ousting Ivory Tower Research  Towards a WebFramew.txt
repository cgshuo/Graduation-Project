 With its close ties to the Web, the IR community is destined to leverage the dissemination and collaboration capabilities that the Web provides today. Especially with the advent of the software as a service principle, an IR community is conceivable that publishes experiments executable by anyone over the Web. A review of recent SIGIR papers shows that we are far away from this vision of col-laboration. The benefits of publishing IR experiments as a service are striking for the community as a whole, and include potential to boost research profiles and reput ation. However, the additional work must be kept to a minimum and sensitive data must be kept private for this paradigm to become an accepted practice. To foster experiments as a service in IR, we present a Web framework for experiments that addresses the outlined challenges and possesses a unique set of compelling features in comparison to existing so-lutions. We also describe how our reference implementation is al-ready used officially as an evaluation platform for an established international plagiarism detection competition.
 Categories and Subject Descriptors: H.5.3 [Information Sys-tems]: Information Interfaces and Presentation X  X roup and Orga-nization Interfaces Keywords: Open Evaluation, Experiment Management, Result Dissemination
Within IR research, the integration of previous work in one X  X  ex-periments is significantly simplified if the data and software assets are published together with the papers. In this respect, Armstrong et al. [1] pointed out the verification problems that arise if research assets are not streamlined with the latest publications. In our view, the most convenient way to enhance comparability in experiments is to publish experiments as an online service where researchers can verify experimental results and explore alternate parameter set-tings.

To explore how IR research is published today, we reviewed all 108 full papers from the SIGIR 2011 proceedings concerning ex-periment assets. The results in Figure 1 show the extent to which the authors published their data ,their software , and whether the experiments were hosted as a service . All three aspects need to be addressed to make publishing data and software more widespread. The questions we posed when examining the proceedings and our findings are given as follows.

Data. Are any of the datasets used in the research publicly avail-able? We observe that 51% of the papers use a publicly avail-able dataset for experiments. In this respect, evaluation initiatives such as TREC do a great job in supplying researchers with open datasets. The papers from the Image Search, Indexing, Retrieval Models, Test Collections, and Web Queries sessions all used pub-lic datasets. Other datasets such as search engine query logs re-quire non-disclosure due to commercial and sensitivity reasons, which accounts for much of the una vailability of da ta. Correspond-ingly, only two of eight Query Analysis papers used public datasets, which was second lowest and only ahead of the Summarization pa-pers, where no public datasets were used.

Software. Are any of the software assets used in the research publicly available? We observe that 18% of the papers use shared software contributions, and hence publishing software is currently a minority practice in IR research. 1 Only the Linguistic Analysis papers published software with their papers in all cases. One expla-nation for this low ratio is the lack of acknowledgment researchers receive for publishing software. To foster software release, new incentives must be created that turn invested time in developing reusable software into reputation gain for the researchers.
Service. Are any of the experiments in the research provided as an online service? We observe that providing experiment soft-ware as a service is not practiced at all in the examined papers. The application closest to a web service is the online demonstra-tion of ViewSer [2]. The lack of a compelling web framework that researchers can use to easily transform their experiments into a web service is a plausible explanation.
A further 17% of researchers instead published algorithms in their papers as a compromise.
Motivated by the observations from the paper study above, we propose the development of an online framework to foster publish-able IR experiments. The proposal is based on the needs for local installation, web dissemination, platform independence, result re-trieval, and peer to peer collaboration. Our assessment of existing experimentation frameworks with respect to these goals is depicted in Table 1, which shows that none of the systems fully comply. 1. Local Instantiation. If data must be kept confidential, the framework must be able to reside with the data, hence the frame-work must be locally installable. Unlike centralized experiment platforms like MLComp and myExperiment, local instantiation al-lows experiments on sensitive data to be published as a service from a local host. External researchers can then use the service for com-parison and evaluation of their own research hypotheses, whilst the experiment provider is in full control of the experiment resources. 2. Web Dissemination. URLs are definitive identifiers for digital resources. If all runs of an experiment are accessible over a unique URL, researchers can conveniently link the results in a paper with the experiment service used to produce them. Especially for stan-dard pre-processing tasks or evaluations on private data, such a web service can become a frequently cited resource. In addition, at-tention can be attracted to one X  X  work through integration of the service into home pages and blog articles. To address the issue of digital preservation, URLs should encode all information needed to recompute a resource, such as program and input parameter speci-fications, in case stored data is lost. 3. Platform Independence. The sophisticated and varying soft-ware and hardware requirements of information retrieval experi-ments as well as individual coding preferences of software develop-ers render any development constraints imposed by the web frame-work critical for its success. Ideally, software developers can de-ploy experiments as a service unconstrained by the utilized oper-ating system, parallelization paradigm, programming language, or data formats. Local instantiation is one key to realize this goal. Fur-thermore, the web framework must operate as a layer strictly on top of the experiment software and should use, instead of close intra-process communication such as in TunedIT, standard inter-process communication on the POSIX level and the file system to exchange information. This way, any running software can be deployed as a web service without internal modifications. 4. Result Retrieval. Especially for computationally expensive retrieval tasks, the maintenance of a public result repository can become a valuable asset of a research group. For example, exper-iment services that can index datasets with state-of-the-art natural language processing technology have the potential to raise the com-parability of retrieval model research to a higher level. For cluster-ing and result diversification research, comparability is enhanced by establishing static snapshots of the search results from major search engines regularly. The persistent storage of experiment re-sults by the web framework is key to achieve this goal. Even if the public release of an experiment service is not desired, the frame-work is still useful if it assume s responsibility fo r managing the raw experiment results and making them available across a research team. 5. Peer to Peer Collaboration. Consider a scenario where a con-sortium of service providers become renowned gatekeepers for var-ious streams of research, and maintain the community-wide reposi-tory of state-of-the-art algorithms, datasets, and experiment results on their web site. The gatekeepers drive the standardization of data formats and can, by utilizing the retrieval facility, stage competi-tions in a semi-automated fashion. A mechanism for connecting the local framework instances to a network of experimentation nodes has to be provided to achieve this scenario. Note that currently none of the experimentation platforms implements peer to peer collabo-ration.
The reference implementation 2 of our proposal has been devel-oped as a RESTful J2EE servlet. This implementation is also the official training and evaluation platform for the detailed comparison task of the PAN 2012 plagiari sm detection competition 3 as part of the PAN series [3]. For the training phase, an evaluation service is provided where the participants upload their results and receive the performance score. For the final performance assessment, the par-ticipants submit their detection algorithms as executable software on either a Windows 7 or Linux based virtual machine. All sub-missions are automatically evaluated on the holdout test set with the reference implementation. This method is required since the organizers evaluate the detection approaches using real data that is subject to non-disclosure. The proposed method also allows the runtime of the submitted approaches to be recorded for the first time. The participants also have the possibility to opt-in for a pub-lic release of their plagiarism detection software as a service.
In this paper, we proposed a web framework for IR experiments as a service motivated by low trends in sharing data and software in recent IR research. The fundamental design decisions concern-ing local instantiation, web dissemination, platform independence, result retrieval, and peer to peer collaboration address the specific needs of IR research. A reference implementation has been de-veloped complying with these design rules that has been put to widespread use as part of the PAN competition series in 2012. [1] T. G. Armstrong, A. Moffat, W. Webber, and J. Zobel. Im-[2] D. Lagun and E. Agichtein. ViewSer: Enabling Large-Scale [3] M. Potthast. Technologies for Reusing Text from the Web .PhD
