 Information retrieval systems face a number of challenges, originating mainly from the semantic gap problem. Implicit feedback techniques have been employed in the past to ad-dress many of these issues. Although this was a step to-wards the right direction, a need to personalise and tailor the search experience to the user-specific needs has become evident. In this study we examine ways of personalising af-fective models trained on facial expression data. Using per-sonalised data we adapt these models to individual users and compare their performance to a general model. The main goal is to determine whether the behavioural differences of users have an impact on the models X  ability to determine topical relevance and if, by personalising them, we can im-prove their accuracy. For modelling relevance we extract a set of features from the facial expression data and classify them using Support Vector Machines. Our initial evaluation indicates that accounting for individual differences and ap-plying personalisation introduces, in most cases, a noticeable improvement in the models X  performance.
 H.3.3 [ Information Search and Retrieval ]: Relevance Feedback, Search Process; I.5.1 [ Computing Methodolo-gies ]: Pattern Recognition X  Models Experimentation, Human Factors, Performance
The main challenge information retrieval (IR) systems face nowadays originates from the semantic gap problem: the semantic difference between a user X  X  query representa-tion and the internal representation of an information item  X 
The research leading to this paper was supported by the European commission, under the contract FP6-027122 (SALERO).
 in a collection. Although progress has been made, the effec-tiveness of existing systems is still limited. The gap is further widened when the user is driven by an ill-defined informa-tion need, often the result of an anomaly in his/her current state of knowledge [7]. The formulated search queries, which are used by the retrieval systems to locate potentially rel-evant items, produce results that do not address the users X  true needs.

To deal with information need uncertainty IR systems have employed in the past a range of feedback techniques, which vary from explicit [13, 21] to implicit [1, 6]. The no-tion of explicit feedback was present from the early years of IR, but it soon became apparent that users could not cope with the cognitive burden of explicit relevance judgments. Alternative paths had to be discovered, which led to the un-obtrusive, yet less robust, implicit feedback techniques [12, 15]. Even though this was a step towards the right direction a need to personalise and tailor the search experience to the user-specific needs was progressively made evident.
Personalisation emerged as an appealing technique in deal-ing with the issues caused by the variation of online be-haviour and the individual differences observed in user in-terests, information needs, search goals, difficulties encoun-tered, and other. To apply personalisation an IR system must initially employ a modelling technique that will cap-ture certain user characteristics. At a later stage, infor-mation filtering is performed to refine the aggregated infor-mation and adjust the system X  X  responses to accommodate users X  needs, thus providing a more personalised experience.
Several attempts have been made in the past to develop user models, using implicit feedback. In [16], Oard and Kim define a set of application-specific observable behaviours (ex-amination, retention, etc.) and introduce the concept of learning user interests and building user profiles from im-plicit data. In [20], Puolam  X  aki et al. combine implicit feed-back with explicitly created user profiles. In the latter work, the authors use mixture models to combine different sources of relevance judgments. The implicit feedback information derives from eye-movement data, used in combination with a probabilistic collaborative filtering model.

In [2] the authors make the assumption that, apart from user modelling, query-specific behaviours are also important and should be considered when attempting to predict topical relevance. Following this work, Liu et al. [14] constructed user profiles based on users X  search history and developed al-gorithms that mapped query terms to predefined categories. The latter information was used to extract users X  interests and address issues related to word ambiguity. Teevan et al. showed in [23] that richer representations of the user lead effectively to more accurate relevance predictions. This improvement is achieved by combining different sources of information, such as a search history, webpages visited, doc-uments created and viewed, etc., which is used to re-rank the results obtained by a search system.

However, although the identification of user interests is a definite step, it is important to examine how these interests evolve, interact and lose focus, from a temporal perspective. In [9], Daoud et al. consider in this context the problem of search-session boundary recognition. In the above study the users were represented by long-term interests and short-term contexts, which were both essentially ontologies of seman-tically linked concepts. Their approach to personalisation yielded significant improvements compared to the conven-tional query handling paradigm.

In this paper we examine ways of personalising affective models, trained on facial expression data gathered by many individuals. Our work is limited to that of modelling users X  affective behaviour and does not involve information filtering or adaptation of content. Using personalised data we adapt these models to individuals and compare their performance to a general model. For modelling relevance we extract a set of features from the facial expression data and classify them using Support Vector Machines. Our initial evaluation indi-cates that accounting for individual differences and applying personalisation introduces, in most cases, a noticeable im-provement in the models X  performance. To our knowledge, no prior work has ever applied personalisation on the affec-tive level interaction, in the context of online information seeking.
The major goal of this study is to develop personalised af-fective models, adapted to the individual characteristics of specific users, and compare their ability to discriminate be-tween relevant and irrelevant items against general affective models. To achieve this goal we had to expose our partic-ipants to stimuli of varied intensity. As a result, the infor-mation that we collected covered a much wider spectrum of affective behaviour and allowed the comparability of results with previous work. We, furthermore, explore different ways of combining the personalised data with the general data, to optimise the models X  performance. Overall, we examined the following research hypothesis:
H 1 : By adapting a general affective model with person-alised data, to a specific user, we can improve its accuracy in predicting topical relevance.

H 2 : Merging general with personalised data is more ef-fective personalisation method compare to training separate models and applying information fusion on a decision level.
By definition an experimental study introduces the par-ticipants to an artificial situation that takes place at a lab-oratory setting, therefore lacking the ecological validity of a naturalistic study. In addition, when analysing facial ex-pressions several critical issues arise [22]. Firstly, emotional expressions are highly idiosyncratic in nature and may vary significantly from one individual to another (depending on personal, familial or cultural traits). Secondly, spontaneous expressive behaviour may not be easily elicited, especially when participants are aware of being recorded. Finally, while interacting with researchers and other authorities the participants may intentionally try to mask or control their emotional expressions, in an attempt to act in appropriate ways.

While taking into consideration the above factors we de-vised an experimental setup, similar to the one adopted in [3], that mitigated most of the unwanted effects. In our ap-proach we: (i) employed a facial expression recognition sys-tem of reasonably robust performance and accuracy across all individuals, (ii) applied hidden recording, thus increas-ing the chance of observing spontaneous behaviour, and (iii) made our presence in the laboratory setting as unobtrusive as possible.
This study used a repeated-measures design. There were two independent variables: task difficulty (with three levels:  X  X asy X ,  X  X verage X ,  X  X ifficult X ) and personalisation technique (with two levels:  X  X daptation X  and  X  X eighted voting X ). The task difficulty levels were controlled by re-ranking the re-turned results to include 8 relevant -2 irrelevant, 5 relevant -5 irrelevant, and 2 relevant -8 irrelevant documents, accord-ingly. The set of relevant documents consisted of top-ranked results, while the set of irrelevant documents consisted of bottom-ranked results. This way we improved or decreased the chances of locating relevant items among the results. The personalisation technique was controlled by adopting a different approach (mixing general with personalised data, or using them separately to train different models). The de-pendent variables were: (i) task (difficulty, complexity, etc.), (ii) search process, and (iii) models X  performance, in terms of accuracy.
For our experiment we used one desktop computer, equip-ped with monitor, keyboard, mouse and a web-camera. The computer provided access to a custom-made search inter-face, which allowed the participants to perform their search tasks. The search interface was designed to re-rank the re-sults for each submitted query, according to the level of task difficulty, without the participants being aware of it. In ad-dition, a custom-made script logged participants X  desktop actions, such as starting, finishing and elapsed times for in-teractions, and click-throughs. The web-camera (Creative Live! Cam Optia AF with a 2.0 megapixels sensor) was used in combination with eMotion [24], for the application of real-time facial expression analysis. Finally, we used entry-, post-and exit-questionnaires in each session.
We prepared a number of search tasks that covered a vari-ety of context, from entertainment to health-related issues, in order to capture participants X  interest as best as possible. All tasks were performed manually, prior to the experiment, to ensure the availability of relevant documents. The search tasks were presented using the structural framework of the simulated information need situations [8]. By doing so, we introduced short cover stories that helped us describe to our participants the source of their information need, the envi-ronment of the situation and the problem to be solved. We believe that this way we facilitated a better understanding of the search objective and, in addition, we introduced a layer Topic 1: A task of digging cheesy gossips and scandals.
Topic 2: Formulate an opinion about existing social networking sites.

Topic 3: A task of investigating, obtaining advance knowledge, or doing research on a particular sport.
Topic 4: A task of finding information regarding con-traception methods.

Topic 5: A task of investigating, obtaining new knowl-edge, or doing research on global warming.
 Topic 6: A task of planning your Christmas holidays. of realism, while preserving well-defined relevance criteria. An indicative list of the topics is presented in Table 1.
For the completion of the search tasks we used a custom-made search environment (Zoogle) that was designed to re-semble the basic layout of existing search services, while re-taining a minimum of graphical elements and distractions. Zoogle works on top of Yahoo! API. For every submitted query it returned a list of ten results, stripped of their title, snippet or any other metadata. This layout was intentional to ensure that the participants would not be able to judge the topical relevance of the returned documents, prior to examining them.

Even though this approach introduced our participants into artificial search situations, which differ from real-life experiences, it was a necessary trade-off for capturing af-fective responses exhibited towards the viewed content. In addition, we ensured that the participants viewed an equal number of relevant and irrelevant documents. This allowed us to develop balanced sets of affective data.

Zoogle applies a layered architecture approach, similar to that adopted in [5]. The first layer of the interface is dedi-cated for supporting any interaction that occurs during the early stages of the search process (such as query formula-tion and search execution). Any output generated during this phase is presented in the second layer. From there, the participants can select and preview any of the retrieved doc-uments. The content of an item is shown in a separate panel in the foreground, which constitutes the third layer of our system.

The main purpose of this layered architecture is to iso-late the viewed content from all possible distractions that reside on the desktop screen; therefore, establishing addi-tional ground truth that allowed us to relate participants X  affective responses to the source of stimuli (in our case, the perused documents). This was an important aspect of our experimental methodology, since we were interested in iso-lating content-particular emotions. Upon examining a doc-ument, the participants had the option to either bookmark or ignore it. The first option would classify the document as relevant, while the latter as irrelevant.
The participants completed an Entry Questionnaire at the beginning of the study, which gathered background and de-mographic information, and, furthermore, inquired about previous experience with online searching. A Post-Search Questionnaire was also administered at the end of each task, to elicit subjects viewpoint on certain aspects of the search process. The questions were divided into three sections that covered the search session, the encountered task and the re-turned results.

Finally, an Exit Questionnaire was introduced at the end of the study. The questionnaire gathered information on participants X  views about the importance of affective feed-back, with respect to usability and ethical issues. All of the questions included in the questionnaires were forced-choice type.
Facial expressions have been associated in the past with universally distinguished emotions, such as happiness, sad-ness, anger, fear, disgust, and surprise [11]. Recent find-ings also indicate that emotions are primarily communicated through facial expressions [17] and are generally regarded as essential aspects of human social interaction. The face pro-vides conversational signals, which do not only clarify our current focus of attention [18] but also regulate our interac-tions with the surrounding environment and the organisms that inhabit it.

In this study we applied real-time facial expression analy-sis using eMotion, an automatic facial expression recognition system with emotion-detection capabilities. The process of recognition occurred as follows: initially, eMotion would lo-cate certain facial landmark features (eyebrows, corners of the mouth, etc.) and construct a 3-dimensional wireframe model of the face, consisting of surface patches wrapped around it. After the construction of the model, head motion or any other facial deformation would be tracked and mea-sured in terms of motion-units (MU X  X ), and, finally, classified into one of the seven detectable emotion categories.
Automatic systems are an alternative approach to facial expression analysis [19] and have exhibited performance com-parable to that of trained human recognition (87%). eMo-tion applies a generic classifier that has been trained on a diverse data set, combining data from the Cohn-Kanade database. Its main advantage is its reasonable performance across all individuals, irrespectively of the variation intro-duced from mixed-ethnicity groups. Results of the person-dependent and person-independent tests presented in [24] support our performance-related assumptions. For addi-tional information regarding the advantages and limitations of eMotion the reader is referred to [24, 4]
Sixteen healthy participants of mixed ethnicity and edu-cational background (8 MSc students, 4 BSc. and 4 other) applied for the study through a campus-wide ad. They were all proficient with the English language (1 native, 14 ad-vanced, and 1 intermediate speakers). Out of 16, 7 were male and 9 were female and were between 21-32 years of age ( M =25.83, SD =2.57). They had an average of 7.33 years of online search experience and all claimed to have been us-ing at least one search service in the past (with the most popular being  X  X oogle X  and  X  X ahoo! X ). On average, the participants reported carrying out online searches once or twice a day ( M =5.33, SD =0.84). The frequency was mea-sured using a 6-point scale (1= X  X ever X , 2= X  X nce or twice a year X , 3= X  X nce or twice a month X , 4= X  X nce or twice a week X , 5= X  X nce or twice a day X , 6= X  X ore often X ). The user study was carried out in the following manner. The formal meeting with the participants took place in the laboratory setting. At the beginning of the session the par-ticipants were given an information sheet, which explained the conditions of the experiment. They were then asked to sign a Consent Form and were notified about their right to withdraw at any point during the study, without hav-ing their legal rights or benefits affected. Finally, they were given an Entry Questionnaire to fill in. The session pro-ceeded with a a brief tutorial on the use of the search in-terface, followed by a calibration of the web-camera. The participants X  were told that the web-camera was used for eye-tracking purposes, thus concealing it X  X  true operation.To ensure that their faces would be visible to the camera at all times we encouraged them to keep a proper posture, by indicating the need to stay within the visual field of the eye-tracker.

Each participant completed three search tasks, one for each level of difficulty (see Section  X  2.1). In every task they were handed six topics and were asked to proceed with the one they found most interesting. For each topic the subjects were given 15 minutes, during which they had to locate as many relevant documents as possible. For every submitted query the search interface would return ten results, which they were asked to evaluate one by one. If a document was judged as relevant the participants had the option to bookmark it, or otherwise ignore it and continue with the evaluation of the remaining items. Depending on the level of task difficulty ( X  X asy X ,  X  X verage X ,  X  X ifficult X ) the ratio of relevant-irrelevant documents varied accordingly (the par-ticipants were unaware of this uneven distribution of rel-evant/irrelevant documents). To negate the order effects we counterbalanced the task distribution by using a Latin Squares design. At the end of each task, the participants were asked to complete a Post-Search Questionnaire.
An Exit Questionnaire was administered at the end of each session. The participants were informed about the un-known conditions of the study and were asked to sign a second Consent Form, which was granting us permission to retain the accumulated facial expression data. Finally, the participants were asked to sign a Payment Form, prior to receiving the fee of  X  10.
Out of the 1534 browsing instances that took place during the study, 696 correspond to relevant documents and 838 correspond to irrelevant documents. Overall, we collected 440557 feature vectors, out of which 224165 are associated to relevant documents and 216392 to irrelevant documents. Our main objective was to accumulate a sufficiently rich and balanced set of affective data that would allow us to experiment with different personalisation approaches. The analysis was performed on a frame-basis.
From the output of eMotion we concluded to a subset of 12 features that have directly measured values and were used to train our models. Most of these attributes have been associated in the past with important affective and cognitive processes. Even though eMotion follows the cate-gorical approach (i.e., interprets facial expressions in terms of emotion categories) we did not employ categorical data for the training of our models. Instead, we used the motion-units (MU X  X ) data, which is a low-level category of features very similar to Ekman X  X  action-units (AU X  X ) [10]. MU X  X  mea-sure the intensity of an emotion indirectly, by tracking the presence and degree of changes in all facial regions associ-ated with it. Moreover, MU X  X  allowed us to associate the captured facial expressions with a wider range of affective and cognitive states, which are not accounted for during the meta-classification that eMotion applies.
We shuffled and split the data of each participant into three subsets, two of which were used for training purposes (S 1 &amp; S 2 ) and one for testing (S 3 ). Each time we used an equal number of documents. We also resampled datasets S 1 S and S 3 , based on the participant with the least number of instances. This resulted in three sets with approximately the same number of feature vectors, across all participants. By balancing the training and test sets we prevented over-fitting and, additionally, compensated for the originally uneven size of the datasets. Since eMotion did not pre-processes the data we had to scale them before applying any classification method, to avoid having attributes in greater numeric ranges dominating those in smaller numeric ranges.
We explore the effect of personalisation on the affective models X  performance. The modelling goal is to develop af-fective models that can predict with reasonable accuracy the topical relevance of viewed documents. We employ sensory data that derive from facial expressions as the only implicit feedback information. From the latter signals we extract a set of features, and perform discriminant analysis, using Support Vector Machines (SVM). Additional classification techniques were evaluated in [4], but proved to be less effi-cient. Therefore they were omitted from this study. We do not assume anything about the relationship between these features, which we consider indicative of users X  affective be-haviour and topical relevance. We, rather, follow a straight-forward classification approach, using the ground truth that is associated with our training data.
We used libSVM 1 , an implementation of SVM, to discrim-inate between two classes of documents: (i) relevant, and (ii) irrelevant. Our approach utilises an efficient method that can deal with a difficult, multi-dimensional classifica-tion problem. We trained our models using a radial basis function (RBF) kernel, which, based on previous work [4] that evaluated all basic SVM kernels (linear, polynomial, radial basis function, sigmoid), proved to be the optimal choice. Moreover, the RBF kernel is preferable, since it en-counters less numerical difficulties and has a limited number of hyper-parameters.

To optimise the performance of our SVM model we per-formed a grid-search on the parameters C (cost) and  X  (gamma) using cross-validation, during which we tried ex-ponentially growing sequences of C and  X  . However, since performing a full grid-search can be time consuming, we ini-tially used a coarse grid and then, after identifying a  X  X ood X  region, we performed a finer grid search on that region. The http://www.csie.ntu.edu.tw/  X cjlin/libsvm/ end-purpose was to identify the optimal set of ( C ,  X  ) so that every classifier we trained could achieve the best possible (tuning-wise) accuracy score on in testing data.
We followed two different training approaches: (i) we mer-ged general data, gathered from many individuals, with per-sonalised data from a single participant and trained a single SVM model, and (ii) we used general and personalised data separately, to train two different models and combined their predictions using weighted voting.

In the first approach we used a total of 19157 instances of general data, acquired from [3], in combination with 4300 instances (in three sets of 1430 instances) per participant. For every participant we originally tested the performance of the SVM model (general model) trained on the 19157 instances against S 3 (the predestined test set of the partici-pant). Then we retrained the model using the same general data merged with additional N instances of general data, or N  X  instances of personalised data (where N or N  X  equals 1430 feature vectors), and tested its performance against S Finally, we repeated the same process using N+N instances of general data, or N  X  +N  X  instances of personalised data. This way we were able to examine if by adding personal data we improved the performance of our model more than by adding general data.

In the second approach we examined whether predictions from two different sources (general model and personalised model) could be fused, on a decision level, to determine the topical relevance of a document. For each participant we trained a personalised SVM model using the subsets S 1 and S . A general SVM model was also trained using the same general data as in the previous method. We then used each participant X  X  test set (S 3 ) to acquire the predictions from both classifiers and combine their output using the following formula (each time with a different weighting scheme):
Assume p i gen is the probability estimate of instance i being relevant, as given by the general model, while p i pers denotes the probability of the same instance being relevant, as de-termined by the personalised model. We then calculate the probability p i of the instance i being relevant using Formula 1. Where w gen  X  [0 , 1], is the weight we assign for the pre-diction of the general model. The prediction p i will then be transformed to a binary decision classifying instance i as either relevant or irrelevant, based on a predefined thresh-old value t . The probability estimates of both models were tested for different combinations of weights and threshold, using a step of 0.1.
In this section we present the experimental findings of our study, based on 48 search sessions that were carried out by 16 subjects. Out of the many results, we are reporting those that refer to our models and present only the questionnaire data that refer to the tasks, due to limited space. We mea-sured the performance of all models using the standard met-ric of accuracy. Accuracy was computed as the fraction of items in the test set for which the models X  predictions were correct.
A 5-point Likert scale was used in all questionnaires. Ques-tions that ask for participants X  rating on a bipolar dimension have the positive concept corresponding to the value of 1 (on a scale of 1-5) and the negative concept corresponding to the value of 5. Questions that ask for user rating on a scale of 1-5 represent in our analysis stronger perception with high scores and weaker perception with low scores. Friedman X  X  ANOVA and Pearson X  X  Chi-Square test were used to estab-lish the statistical significance ( p &lt; .05) of the differences observed among the three tasks (T 1 : easy, T 2 : average, and T : difficult). When a difference was found to be significant the Wilcoxon Signed-Ranked Test was applied to isolate the significant pair(s), through multiple pair-wise comparisons. To take an appropriate control of Type I errors the Bonfer-roni correction was applied, and so all effects are reported at a .016 level of significance.

Table 2 shows the means and standard deviations for par-ticipants X  assessment of the performed tasks. With respect to the assessment of the difficulty level it appears that there is a trend, with T 3 considered the most difficult. Friedman X  X  ANOVA was applied to evaluate the significance of this vari-ance. The results indicate that participants X  perception of the task difficulty was significantly different (  X  2 (3, N = 16) = 9.042, p &lt; .05). The post hoc tests show that the differ-ences for the pairs T 1 &amp; T 3 ( Z = -2.434, p &lt; .016) and T &amp; T 3 ( Z = -2.683, p &lt; .016) are statistically significant, but the same condition does not apply for T 1 &amp; T 2 .
This is further supported by participants X  view of the retrieved results. The participants were asked to provide their assessments through the following questions: (i)  X  X ver-all, the results that were presented to you were: (Range: 1-5, Lower = Relevant -Higher = Irrelevant) X , and (ii)  X  X ou feel satisfied with the retrieved results (Range: 1-5, Lower = Agree -Higher = Disagree)  X . For the first ques-tion, the participants considered the retrieved results less relevant for T 3 ( M =2.8125, SD =0.9106), compared to T ( M =2.0625, SD =0.8539) and T 2 ( M =2.1875, SD =0.9811). Furthermore, they were less satisfied with the retrieved re-sults in T 3 ( M =2.8750, SD = 0.9574), than in T 1 ( M =2.0625, SD = 0.9287) or T 2 ( M =2.1250, SD = 0.9574). Table 2 also shows participant X  X  assessment of the ambiguity, complex-ity and interest of the three tasks. Friedman X  X  ANOVA test did not reveal a significant difference for any of the above aspects.
For each personalisation approach we present the perfor-mance of our models in terms of accuracy. The Dependent t -Test was applied, when possible, to determine if the dif-ference between the experimental conditions is statistically significant. The baseline, which represents random choice, is set to 50%, since the class of a document can be either relevant or irrelevant.
The results of the first approach are shown in Figures 1, 2, and 3. Figure 1 illustrates the performance of three different classifiers, per participant: (i) a classifier trained exclusively on general data, (ii) a classifier trained using general data merged with the personalised dataset S 1 , and (iii) a classifier trained using general data merged with the datasets S 1 and S . For every participant we tested these three combinations against the corresponding S 3 . Only for this case, the subsets S -S 3 were not balanced. Therefore, the contribution of per-sonalised data by each participant varied. The progression of the columns in Figure 1 suggests that, in most cases, an improvement was achieved by introducing personalised data to the training set, reaching classification rates that exceed 70%.

Figures 2 and 3 show the results of the same personalisa-tion approach, as described above, with the exception that this time we used balanced sets of personalised data (we re-sampled the datasets S 1 , S 2 and S 3 to ensure that each participant contributed the same number of instances). This was a necessary step to allow for testing the significance of the variation introduced in the models X  performance. Fig-ure 2 shows the performance of a classifier trained using the original set of general data, merged with an additional N instances of general data, and a classifier trained using the same general set of data, merged with an additional N instances of personalised data. On average, the second clas-sifier ( M =52.74, SD =3.31) performed slightly better than the first classifier ( M =51.23, SD =4.33). Therefore, we can argue that by adding N number of personalised data we achieved a slightly better performance, compared to adding the same number of general data. However, the post-hoc tests did not reveal a significant difference.

Figure 3 illustrates the performance of the two classifiers after adding N+N general data, or N  X  +N  X  of personalised data, to the original training set and tested against the cor-responding test set S 3 , for each participant. The results show that the second classifier attained a significantly higher performance ( M = 55.94, SD =6.62) than the first classifier ( M =50.83, SD =4.34), t (15)=-3.848, p &lt; .01. In this graph the enhancement of the model X  X  performance, due to the integration of additional personalised data, is much more evident.
The results of the second approach are presented in Figure 4. In this approach we used the general and the personalised data separately, to train two classifiers and combine their predictions using weighted voting. The graph illustrates the performance of the classifiers for different thresholds. Each line in the graph is a different weight combination, e.g., for w gen =0.0 and w pers =1.0 we see the progression of the performance, between thresholds 0.0 to 1.0. The graph indicates that, on average, the best performance was held by the classifier with combination of weights w gen =0.3 and w pers =0.7, for threshold t =0.3. This suggests that the vot-ing scheme worked better when more emphasis was put on the personalised model. However, the contribution of the general model was equally important, to keep the classifica-tion rates optimal. When higher weights were given to the general model the performance dropped considerably, which supports further the positive effect of personalisation on the models X  performance.
In this paper we explored two different approaches to per-sonalising affective models that are capable of discriminat-ing between two categories of documents: relevant and ir-relevant. We devised an experimental setup that exposed our participants to search tasks of varying difficulty, which was achieved through the re-ranking of the return docu-ments. This manipulation of task difficulty resulted in a much wider spectrum of affective reactions, thus making the accumulated affective data not only more authentic but also comparable to data gathered from previous studies. Our analysis also indicates that this variation was perceived by the participants, as it was found statistically significant.
For modelling relevance we extracted from facial expres-sion data a set of features and classified them using Support Vector Machines. In the first approach we adapted a gen-eral model to the behavioural characteristics of a number Figure 2: Performance of general model after adding N general or N  X  personalised data of participants, using personalised data, and established its performance against a model trained exclusively on general data. In the second approach we trained a general and a personalised model separately and combined their predic-tions using a combination of weighting schemes. We, finally, examined the effect of personalisation on the model X  X  per-formance and tested it X  X  significance.

One facet of affect recognition is developed here for the first time: the personalisation of affective models, trained on facial expression data, for the prediction of topical rele-vance. Our experimental evidence supports our first hypoth-esis, namely that by adapting a general affective model to a specific user we introduce a noticeable improvement in its discriminating ability. Our best performing model attained an accuracy of 72.52%, which is substantially better than the baseline or any other performance presented in [4].This difference was found to be highly statistically significant, which is an encouraging finding.

Using weighted voting we provided additional evidence in favour of accounting for the behavioural differences of users. Our analysis indicates that by fusing, on a decision level, the output of both general and personalised classifiers (with the emphasis on the latter) we can attain the optimal per-formance. Regarding our second hypothesis, we cannot sug-gest which approach was more effective, since our findings did not favour one method over the other. Clearly, there is more than one alternative to personalising user models, especially those built on affective data. Additional work is necessary before we determine if these two approaches per-form equally well under different experimental conditions.
Finally, the evidence accumulated from both approaches suggests that personalisation works better for some users, than others. We speculate that the variation in the mod-els X  performance might be correlated with the ability of the participants to behave naturally and be expressive in a lab-oratory setting, as in their home environment. However, the choice of setting was a necessity, guided by the need to allow for comparability between data from previous studies.
In conclusion, we feel that the quality of our results is good enough to indicate that personalisation of affective feedback Figure 3: Performance of general model after adding N+N general or N  X  +N  X  personalised data is a promising area of research and that it can potentially in-fluence other aspects of the search process, such as relevance feedback, ranking, recommendation techniques, as well as of-fer new insight to the semantic gap problem. Finally, since there are no other systems available for direct comparison, our system holds the best accuracy achieved, so far, in the deduction of topical relevance using affective information. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [3] I. Arapakis, J. M. Jose, and P. D. Gray. Affective [4] I. Arapakis, I. Konstas, and M. Jose, J. Using facial [5] I. Arapakis, Y. Moshfeghi, H. Joho, R. Ren, [6] R. Badi, S. Bae, J. M. Moore, K. Meintanis, [7] N. J. Belkin. Anomalous states of knowledge as a basis [8] P. Borlund. Experimental components for the [9] M. Daoud, L. Tamine-Lechani, and M. Boughanem. [10] P. Ekman. Facial Expressions , chapter 16, pages [11] P. Ekman. Emotions Revealed: Recognizing Faces and [12] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [13] J. Koenemann and N. J. Belkin. A case for interaction: [14] F. Liu, C. Yu, and W. Meng. Personalized web search [15] M. Morita and Y. Shinoda. Information filtering based [16] D. W. Oard and J. Kim. Modeling information [17] M. Pantic and L. Rothkrantz. Expert system for [18] M. Pantic and L. J. M. Rothkrantz. Toward an [19] M. Pantic, N. Sebe, C. J. F., and T. Huang. Affective [20] K. Puolam  X  aki, J. Saloj  X  arvi, E. Savia, J. Simola, and [21] Y. Rui and T. Huang. Optimizing learning in image [22] N. Sebe, M. S. Lew, Y. Sun, I. Cohen, T. Gevers, and [23] J. Teevan, S. T. Dumais, and E. Horvitz.
 [24] R. Valenti, N. Sebe, and T. Gevers. Facial expression
