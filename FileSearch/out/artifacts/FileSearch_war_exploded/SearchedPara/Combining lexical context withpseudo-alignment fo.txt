 Bilingual dictionaries are bridges between two different languages and im portant resources for empirical multilingual natural language processing tasks such as cross -lingual information retrieval ( CLIR ) [ 1] and statistical machine translation [ 13 ] . With the high -speed development of international communication, the bilingual dictionary demand grows highly accordingly. Hand -coded dictionaries are of high quality, but it is expensive to build and researchers have tried, since the end of the 1980s, to aut o-matically extract bilingual lexicons from parallel corpora [ 7 , 1 0 , 1 1 , 17 ] . Parallel co r-pora are however difficult and time consuming to get in several domains and the m a-jority of bilingual collect ions are comparable and not par allel. Due to their more abundant, less expensive and low cost of acquisition via web , v arious methods have been previously proposed to extract bilingual lexicons from comparable corpora [ 3, 4, 5 , 6 , 8 , 12 , 15 , 19 ] .

Most work s in bilingual lexicon extraction follow the assumption that words in translation should have similar context in both languages. Based on this assumption, a standard approach usually builds context vectors for each word of the source and target languages. T he candidate translations for a particular word are obtained by com paring the translated source context vector with the target context vector using a general bilingual dictionary [ 8 ] , and it has been proved to get a good performance in previous works .

In addition to context information, heuristics are often used to improve the general accuracy of the context -vector approach, like orthographic similarities between the source and the target terms [ 1 6]. Cognate -based techniques are popular in bilingual term spotting, in particular for specific domains. It can be explained by the large amount of transliteration even in unrelated languages. Also, related languages like Latin languages can share similarities between a term and its translation, like identical lem mas.

As far as we could tell , only a few studies have paid attention to the co -occurrence information between words in aligned docu ments [ 14 , 18 ] . Those approaches did not ta ke lexical context into account, resulting in poor performance compared to work in the same vain . M oreover , comparable corpora in those studies were divided in to strict 1 -1 aligned document pairs which, however, did not exist broadly in comparable co r-pora . For instance, [14] aimed to enhance the performance of lexicon extraction for those rare words. They did make use of the alignment information in a machine lear n-ing manner , which however relied on strictly 1 -1 alignments and expensive training process . Moreover, by extracting aligned pairs, they had ac tually reduced a lot the size o f original corpora and suffered from great information loss .

However, f actors af fecting the meaning of a word are far more than this, let alone the task is dealing with two languages of different cultural background. Besides , d ue to polysemy and homonymy , it is hard to identify precise translations of a word a c-cording to a single type of feature, a conclusion that can be drawn from poor perfo r-mance in previous studies [ 4 ]. A natural solution to this problem is to resort to co m-prehensive features, especiall y those reflecting different aspects of a word. Therefore, in this paper , we propose a com prehensive approach considering the lexical context together with the co -occurrence feature in loose aligned document pairs .

The rest of the paper is organized as follows: Section 2 reviews related works on some popular methods in lexicon extraction from comparable corpora. Section 3 pr e-sents the combined model we proposed: we firstly present the method to calculate similarity u sing context information and then dis cuss the principle and method to build word co -occurrence. Section 4 shows the experimental setup and evaluation of our method on three groups of corpora . Conclusions will be drawn in section 5 . Several research work s ha ve been done on the task of extracting bilingual l exicon from comparable corpora . Most approaches are based on the same intuitive assum p-t ion of word distribution, that is words appear in context of the same form or semantic are attend ed to be translation pairs. The starting point of the strategy is as follows: for each word w, the context information of w is described in a certain way, such as a vector, and then we can get the translation candidates of w by ranking the context similarity between w and any target word . This method was often been conducted with a n existing bilingual resources [ 6 ] or parser tool [ 5 ] .

E xisting research works are trying to improve performance through two way s : one is trying to find another way to describe the context which contains more context information ; the other is to find a more effective way to measure the similarity b e-context, and test s several di ff erent models in bilingual lexicon extraction from para l-lel or comparable corpora in specialized domains. It show s that the combination of multi models signi fi cantly improves results, and that the use of the thesaurus UMLS/MeSH is of primary importance. B ut [ 5 ] i mprove s the context information by using dependency parsing . It use s contexts derived from head -words linked by d e-pendency trees instead of the immediate adjacent lexical words. With the deep sema n-tic information, it gain s signi fi cant improvement compared to approaches solely rel y-ing on the lexical context. [ 6 ] present s a geometric view on bilingual lexicon extra c-tion from comparable corpora, which can interpret all the context vector approaches in a uniform framework. It u ses singular value decomposition ( SVD) to map the ori g-inal context vector to another space in which synonyms dictionary entries are close to each other, while polysemous ones still select di ff erent neighbors. T he precision is proved to be improved. [1 2 ] tri es to extract French -Japanese ter minologies from co m-parable corpora, considering both single and multi -word terms. They show the fact that the quality of comparable corpora is more important than the quantity and that this ensures the quality of acquired terminological resources. It also conclude s that the quality of co -occurrence vectors can be substantially improved by ensuring domain and discourse comparability of the corpora from which co -occurrences are obtained. [ 18 ] introduce s a new way to align two document collections in different languages, and test the effectiveness of several combined CLIR approaches based on comparable corpora, dictionary -based query translation, and pseudo -relevance feedback. But i t did not take the co -occurrence information of words in document pairs into acc ount . [ 14 ] incorporates the lexical con text similari ty with the co -occurrence model between words in strict aligned documents to extract bilingual lexicon s, which mainly tries to solve the problem of data sparseness of rare words . Furthermore , c omparable corpora in [ 14, 18 ] were divided into strict 1 -1 aligned document pairs that do not exist broa d-ly in comparable corpo ra . In this case, the size of original corpora was greatly re duced while retaining the aligned docu ment pairs, le a d ing to great i nformation loss . In add i-tion , a large volume of training data was needed, which makes the method infeasible in general extraction task.

I t has been proved in most previous studies that lexical context is important for b i-lingual lexicon extraction. The pseudo -alignment s in comparable corpora are another feature we deem to be important. W e thus plan to combine these two features to form a comprehensive model to extract bilingual lexicon from comparable corpora. 3.1 Lexical context s imilarity We implemented the context -vector similarity in a way similar to Pascale Fung [ 9 ] , using context information to extract bilingual lexicon. It assumes that the words in the source and target language are likely to be mutual translations if their context is sim i-lar. Based on the assumption, the standard approach builds a context vector respe c-tively for the source and target word. Then the context vector of the source word is translated to the target language, so that we can co mpare the source context vector with the target context vector and a similarity between them is also calculated. The context vector is built as follows :
F or an English word , we collect its context words in the entire English corpus, number of entries the lexicon has , the value of the i -th dimension of  X   X   X  is : w here rep resents the number of times the i -th word in the dictionary appears in the context of . w here is the maximum frequency of any word in the corpus , is the total num ber of occurrences of word in the corpus .

For a French word , we acquire a context word vector  X   X   X   X  in a simil ar manner as the English word .

Once the context vector of each word has been built, the problem is to measure the similarity between two words. In the cross -language settings, one need s to compare two vectors in different languages, i.e. one vector in the source language needs to be mulating the contributions from words in  X   X   X  . Here we calculate the contributions by the Cosine similarity for context -vector comparisons , which has often been shown to achieve superior results in comparative studies. With the cross -language vector ma p-ping and the cosine formula , t he similarity between words and is computed as: w here is the translation set of in the bilingual dictionary . 3.2 Pseudo -alignment similarity According to the characteristics of the comparable corpus , Comparable documents are not strictly paralle l, but have overlapping information. Therefore, words occur in a pair of comparable documents tends to have more probability of be ing translation candidates of each other. So i n this work , first, we get a loose alignment be tween the two corpora, which has great differences with strict 1 -1 alignment in other works and it is more suitable for the reality . Then, we propose a q uantity which is large if and appear in many document pairs with a high comparability score, and small otherwise.

In order to establish loose alignments among documents , one in fact needs to measure the similarity of each document pair consisting of documents in two la n-guages. We directly use here the measure ) proposed by [ 9 ]. The measure is light -weighted and does not depend on complex resources like the machine translation system. Let us assume we have an English document and a French document , then ) measures the proportion of the English and French words for which a transla tion can be found in the document pair , that is: w here (resp. ) is the set of all words which appear in the English(resp. French) corpus . (resp . ) is the English (resp. French) part of a given, indepen d-ent bilingual dictionary. is a function indicating whether a translation from the translation set of is found in the French word set , that is:
In order to measure the co -occurrence feature of and , f irst , we define the joint probability of and as ( 6 ), which is in direct proportion to the number of comparable document pairs they occur in , that is: w here (resp . ) is the set of documents containing word (resp. ) . is defined as: w here is the threshold to judge the comparability of two corpus or documents. Here we can conclude that the co -occurrence probability of and is in direct propo r-tion to the number of comparable document pairs they occur in . According to equ a-tion ( 6 ), the marginal proba bility of is: w here is the set of all documents in French corpus. Our corpora is large enough to assume that all in have almost the same vocabulary size and all have the same number of comparable counterparts in , then . Point -wise mut u-al information ( PMI) is a measure of association widely used in information theory and statistics. PMI is fi rst proposed by [ 2 ] as equation ( 9 ), Here we use the o judge how relevant the source word is to the target word : Then , we proposed a quantity , which is in direct proportion to the PMI of w hich is easy to compute and has the desired property: it is reasonable to measure the co -occurrence feature of and .

The lexic al context is a classic feature that has been proved to be efficient in lex i-con extraction . The pseudo -alignment in comparable corpora is another feature we deem to be important. Those two features can be combined to form a comprehensive measure and we thus obtain: This is simply the product of two similarities from different aspects. 4.1 Experimental setup We perform our lexicon extraction on English and French comparable corpora which w ere used in the multilingual track of CLEF ( http://www.clefcampaign.org), inclu d-ing the Los Angeles Times ( LAT94, English), Glasgow Herald ( GH95 , English ), Le Monde ( MON94, French), SDA French 94(SDA94, French) and SDA French 95(SDA95, French). To gain the di versity of the corpora, two monolingual corpora from the Wikipedia dump 1 were built. English articles are retained below the root category Society and French articles are extracted from the Soci X t X  category . A dump contains text, but also some special data and syntax ( images, internal links, etc.) which are not interesting for our experiments. We remov e all tags in the collection. A consists of 533k documents and the French one consists of 508k documents. Table 1 contains the details about the comparable corpora in which documents containing less than 30 words have been deleted .

Standard preprocessing steps: tokenization, POS -tagging by Tree tagger and le m-matization a re performed on all the linguistic resources. We will directly work on lemmatized forms of content words (nouns, verbs, adjectives, adverbs).
 The seed lexicon used in our experiments is constructed from an online dictionary. It consists of 33 k distinct English words and 2 8k distinct French words, which const i-tutes 7 6k translation pairs. 
In order to measure the performance of the bilingual lexicon extraction method pr e-sented above, we divided the original seed dictionary into 2 parts: 10% of the Eng lish words ( about 3 k words) together with their translations are randomly chosen and used as the evaluation set, th e remaining words being used to compute context vectors and similarity between them. 4.2 Experiment groups While getting the POS information of the corpus, we divided the entire corpus to three parts which were the three groups of corpora using in the experiments. Then we ra n-domly pick some files from them to do the experiments, documents in each language have the same number. T he source and size of the final corpus is listed in Table 2. group Data Language Source S ize 1 CLEF B ase corpus English GH95 19 K 2 CLEF B ase corpus + 3 CLEF B ase corpus+
While building the context vector of the source and target corpora, s yntactic co n-texts are considered to be less ambiguous and more sense -sensitive than contexts defined as windows of size N for the reason of no structural damage to a complete sentence. But in our work, the Wikipedia texts are not so standard in grammar b e-cause of thei r voluntary editor . Therefore, we use the period concluding a sentence combined with a context widow of size 10 to define the range of context. 4.3 Results and analysis After calcul ating similarities b ased on the baseline method and the one developed in this paper , for each word in the test set , we list their French translation candidates which are ranked by the two method s of calculating similarity respectively . In order to test the performance of the wo rds in the evaluation lists, we get the top N ranked translate candidates of each word, then measure the precision rate and recall rate. In addition, several studies have proved that it is easier to find the correct translati ons for frequent words than for infrequent words, to take this fact into account, we disti n-guished different frequency ranges to assess the validity of our approach for all fr e-quency ranges, Words with frequency less than 100 are defined as low -frequency w ords ( W L ), while words with frequency larger than 400 are high -frequency words ( W H ), and words with frequency in between are medium -frequency words ( W M ). The r esults obtained when using the lexical context information alone (baseline) and the refined combined model proposed in this paper (new) were displayed in Table 3. The relative improvement is further shown in Table 4 . Gi X  ( i=1, 2, 3) stands for the result of group Gi  X  while using the improved method.

Table 3 shows the overall results on three groups of test corpora obtained with our approach as well as the baseline method. One can find that performance of lexicon extraction, in terms of both precision and recall, has been enhanced on all groups, although the improvement is more remarkable on G2 and G3. I t is a tough task to improve the performance of lexicon extraction when considering target words distri b-uted in all frequency ranges[ 6, 9 ], compare to those studies only focusing on words of high frequency [ 4, 12 ] . Accord ing to the experiment results obtained in previous work for the same task [9], our results here should be considered as important, although the increase in terms of precision is not that much. We also notice from table 3 that the performance does not chang e much on group G1 with our approach . The reason is that corpora in group G1 are of small size where context information and alignments of high quali ty do not exist in large scale. We can draw a conclusion out of these r e-sults: the size of corpus influ ences the quality of bilingual lexicon s extrac ted with the method proposed in this paper .

We will give here more comments on translation performance for words distributed in different frequency ranges, since it is much easier to translate words of higher fr e-quency [ 4, 12 ]. The improvement on group G1 is not that significant and we only focus on those on groups G2 and G3 and list detailed results in table 4. One could find a consistent improvement for words in all the frequency ranges . W hen using the i m-proved approach, the precision of low -frequency words is strongly improved from 0.167 to 0.175 (corresponding to a relative in crease of 4.8%) on group G2, from 0.206 to 0.221 (corresponding to a relative increase of 7.3% ) on group G 3. F or middle fr e-quency words, the precision is relatively increased by 5.9 % on group G 2 and 7.7 % on group G 3. Lastly, for high frequency words , the performance is also significantly improved: from 56.1 % to 59.9 % (corresponding to a relative increase of 6.8 %) on group G 2 and from 63.2 % to 65.7 % (corresponding to a relative increase of 4 %) on group G 3. W e can thus conclude that our approach performs consistently on all fr e-quency ranges . E specially for those low frequency words of which the performance is rather difficult t o improve, our approach has shown a consist ent and satisfactory e n-hancement.
 The a verage p recision of three groups on different approaches is displayed in Fig 1. From it , one can see that the overall average precision is further improved by 1.3% compared with the baseline. For N from 1 to 20, there is always a significant i m-provement i n the precision and recall rate. The most fastest rising appears in top 1 to top 5, this is mainly because correct translations can be easily found in top 5 with only little random error, with the increase of number of candidates, the accuracy does not increase so sharply means the candidates among top 10 to 20 are usually words relates to the true translations , so one can refine the model eventually to get better precision . W e have proposed in this paper a combined model to improve the efficiency of b ili n-gual l exicon e xtraction from c omparable c orpora . This model combines the traditional lexical context information with the word co -occurrence in loose ly aligned docu ments to compute the similarity between two words . It has been proved to be effective mai n-ly be cause the novel model has tak en into account various characteristics that could reflect word meaning from a comprehensive perspective . We have first established in our approach loosely aligned document pairs relying on a light -weighted comparabi l-ity measure proposed in [9] . Contrary to previous studies, t he pseudo -alignment in our pract ice . The co -occurrence information of wor ds in aligned documents is then inco r-porated into the traditional model solely relying on lexical context similarity to form a comprehensive model . Experiments have shown that translation precision can be i m-proved significantly on all test groups , i.e. a relative improvement of 6.2 % on group G 3 from 32.5% to 34.5% . In addition , we have noticed much mor e improvement on those corpora of larger size where alignment of higher quality could be found easier . In future work, we will try to discover and incorporate more influential factors to measure the similari ty of words in two languages .
 This work was supported by t he Major Project of National Social Science Fund ( No. 1 2&amp;2D223), the Natural Sci ence Foundation of China (No. 61300144 ) , the Natural Science Foundation of Hubei Province (No.2011CDA034) , the Major Project of State Language Commission in the Twelfth Five -year Plan Period (No.ZDI125 -1) , the Pr o-ject in the National Science &amp;Technology Pillar Program in the Twelfth Five -year Plan Period (No.2012BAK24B01) , the Program of Introducing Talents of Discipline to Universities (No.B07042), and the self -determined research funds of CCNU from the colleges X  basic research and operation of MOE(No. C CNU13A05014, No. CCNU13C01001 , No. CCNU13F010 ) .

