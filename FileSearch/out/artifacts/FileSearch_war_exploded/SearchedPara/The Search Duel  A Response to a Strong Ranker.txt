 How can a search engine with a relatively weak relevance ranking function compete with a search engine that has a much stronger ranking function? This dual challenge, which to the best of our knowledge has not been addressed in pre-vious work, entails an interesting bi-modal utility function for the weak search engine. That is, the goal is to produce in response to a query a document result list whose effec-tiveness does not fall much behind that of the strong search engine; and, which is quite different than that of the strong engine. We present a per-query algorithmic approach that leverages fundamental retrieval principles such as pseudo-feedback-based relevance modeling. We demonstrate the merits of our approach using TREC data.

We revisit the classic ad hoc relevance ranking problem from a competition perspective. Rather than addressing a single ranking system, we consider a duel in which a search problem  X  i.e., a query representing an information need  X  is presented to two players. One of the players has a rel-evance ranking function that is considerably  X  X eaker X  (i.e., less effective) than that of the other player. Yet, his goal is to produce a ranking that is competitive with that created by the player with the stronger ranking function.

On the theory side, this type of interaction is modeled in the context of the recently introduced dueling algorithms [9]. Our goal is to introduce a pragmatic manifestation in the context of an adversarial retrieval setting (e.g., the Web).
The ranking functions employed by leading Web search engines are remarkably effective. However, there is still much room for improving retrieval effectiveness due to var-ious reasons. Many of these are related to the adversarial nature of the retrieval setting (e.g., search engine optimiza-tion efforts). Furthermore, it is impossible for a search en-gine to index all possible documents in a large-scale and dynamically changing collection such as the Web. This real-ity provides some hope for a search engine with a relatively weak ranking function to compete with a search engine that has a much stronger ranking function.

A potential approach to addressing the duel challenge is to try to explicitly learn the ranking function of the strong search engine. However, this approach falls short in our competitive setting. That is, some of the most important information types utilized by the strong ranker may not be available to the weak ranker; e.g., those based on user en-gagement information such as clickthrough data.

We propose a per-query competitive approach. We let the weak search engine observe the output (i.e., the result list of the most highly ranked documents) of the strong search en-gine for a query. Using this list, which is treated as a pseudo feedback set, we induce a relevance model [10]. The model is used to modify the ranking of the weak search engine. The modification is based on a bi-modal criteria: retrieval effectiveness (in terms of relevance) and diversification with respect to the results presented by the strong search engine. The motivation for diversification is based on the following realization. Users of the strong search engine would have no incentive to switch to (or consider) the weak engine if they are presented with the same results.

Empirical evaluation performed with TREC data attests to the effectiveness of our approach. For example, we show that the approach can be used to boost the retrieval effec-tiveness of weak rankers to a level competitive with that of strong rankers, while maintaining relatively low overlap with the strong rankers X  result lists. The approach also sub-stantially outperforms a highly effective fusion method that merges the results of the strong and weak search engines.
This paper describes a preliminary, and the first (to the best of our knowledge), attempt to address the interesting and practical challenge of a search engine duel. Naturally, an abundance of research challenges, in addition to those we address here, arise. We discuss some of these in Section 5.
As mentioned, from a theory perspective, the work on dueling algorithms [9] deals with a setting similar to ours. Although the emphasis is on computing minimax strategies, the basic building block is computing the response of one agent to another. However, the model is stylized and does n ot refer to the realistic search duel we discuss here.
There is a large body of work on merging document lists that were retrieved in response to a query from the same corpus [4] or from different corpora [1]. Our use of a rel-evance model induced from one list to re-rank another list is conceptually reminiscent of work on using inter-document similarities between two lists for re-ranking [11]. However, in contrast to all these results merging approaches which aim to maximize only relevance, our methods are designed to also minimize overlap with the strong engine X  X  result list.
There is work on training a ranker for one domain (lan-guage) and applying it to another [7, 8]. In contrast, we do not assume different domains or languages and we do not train a ranker but rather use a pseudo-feedback-based relevance model.

We note that existing methods for diversifying search re-sults (e.g., [2, 12]) focus on a single retrieved list and on a notion of diversification  X  i.e., coverage of query aspects  X  which is different than that we address here; that is, the overlap with another result list.
Let q be a query which is fixed here and after. Let C be a corpus of documents upon which two search engines perform a search in response to q . One of the search engines, henceforth referred to as strong , is assumed to have a more effective relevance ranking function than that of the other  X  the so called weak engine. Specifically, the ranking induced by the strong engine in response to q is assumed to be of higher effectiveness than that induced by the weak engine. uments that are the most highly ranked by the strong and weak engines, respectively. The goal we pursue is devising an effective response strategy for the weak engine, given that it has access to the list L [ n ] strong of the strong engine. composed of n documents, that will replace the original list, L
A key question is what makes a response  X  X ffective X . Ob-fectiveness, preferably, not significantly lower than that of L strong . Supposedly, then, a highly effective response is set-gine replicate the result list produced by the strong engine. However, assuming that the strong search engine already has a well established, and wide, base of users, such a re-sponse is not likely to result in any incentive for users to switch engines. Therefore, the second criterion we set for an effective response is that the overlap, in terms of shared imal; i.e., the goal is to differentiate the weak engine from the strong engine.

In summary, the weak engine should produce a result list that is as diverse as possible, and competitive in terms of effectiveness, with respect to the result list produced by the
I n practical settings, the weak engine can potentially record, from time to time, the results produced by the strong search engine, specifically, in response to common queries. strong engine. In doing so, the weak engine can use its original list L [ n ] weak and that of the strong engine, L
The basic assumption underlying the strategies that we employ below is that there are relevant documents in L [ n ] that are not in L [ n ] strong . The reason could be, for exam-ple, a different coverage of the indexes of the two engines. Yet, it is not necessarily the case that these documents are ranked high enough due to the relatively weak relevance ranking function of the weak engine. Thus, we use a rel-evance language model R [10] induced from L [ k ] strong  X  the k (  X  n ) documents that are the highest ranked in L [ n ] strong list. As R reflects a model of relevance of the strong engine with respect to the query q , we assume that the ranking of L weak ; re  X  rank is of higher effectiveness than that of L We provide empirical support to this assumption in Section 4. The re-ranking of L [ n ] weak using R is based on the cross en-tropy between R and the language models induced from the documents. Details regarding (relevance) language model induction are provided in Section 4.1.

In what follows we present several strategies of producing
The first response strategy, WeakReRank , is simply us-sity with the strong list L [ n ] strong is the assumed existence sumably improved effectiveness of L [ n ] weak ; re the way it was created; that is, re-ranking L [ n ] weak using a relevance model induced from L [ n ] strong .

The second response strategy is a probabilistic round robin procedure, henceforth referred to as ProbRR . We create L and L [ n ] strong also top down. The next document selected bility p and from L [ n ] strong with probability 1  X  p . (Documents that were already selected are skipped.) Smaller values of p result in more documents taken from L [ n ] strong . Accordingly, can increase and the effectiveness is potentially maintained at the same level as that of L [ n ] strong . Hence, ProbRR en-ables a certain degree of control over the effectiveness and ing different values of p . However, L [ n ] weak ; re documents that are also in L [ n ] strong . Thus, higher values of p do not necessarily directly translate to increased diversity
To directly control the level of diversity of L [ n ] weak ; response with respect to L [ n ] strong , we examine a variant of ProbRR termed ProbResRR  X  the third response strategy we pro-L L the ranking of the residual documents. We then apply the same procedure described above for ProbRR to create the fi-nal result list L [ n ] weak ; response . Using larger values of p results in increased diversity with respect to L [ n ] strong . We used the Web tracks of TREC 2009 X 2011, henceforth TREC-2009, TREC-2010, and TREC-2011, to create the strong vs. weak search engine setting. We focused on runs submitted for the ClueWeb09 category B collection which is composed of around 50 million documents.

We randomly selected 30 pairs of runs from all those sub-mitted and which contain at least 1000 documents as results for each query in the track. In each pair of runs, the result lists of the run whose MAP (@1000) is higher serve for the lists of the strong engine, L [ n ] strong , and those of the run with the lower MAP serve for the lists of the weak engine, L [ n ] Each result list contains n = 1000 documents. We report average performance over the 30 samples. Thus, here, the strong and weak engines are represented by  X  X verages X  over runs of which one is on average more effective than the other.
Titles of TREC topics serve for queries. Stopwords on the INQUERY list were removed from queries but not from documents. The Indri toolkit (www.lemurproject.org/indri) was used for experiments.

To evaluate retrieval performance, we use MAP (@1000) and NDCG (@20). Statistically significant differences of per-formance, computed over the 30 pairs of runs, were deter-mined using the two-tailed paired t-test at a 95% confidence level. To measure the diversity of the result list of the weak engine with respect to that of the strong engine, we use the overlap (i.e., number of shared documents) at the top ten (OV@10) and twenty (OV@20) ranks of the two lists.
We use Dirichlet-smoothed document language models with the smoothing parameter set to 1000. For the relevance model R , we use the rank-based RM3 model [5] which is con-structed from the top k (= 10) documents in the strong en-gine X  X  result list. (We use ranks rather than retrieval scores as the latter are not assumed to be known.) The number of terms used by RM3, and its query anchoring parameter, are set to the default values of 50 and 0 . 5, respectively. Using this (under optimized) default parameter setting allows to demonstrate the potential of using the relevance modeling idea as a basis for producing responses.

As a reference comparison response strategy we use the highly effective CombMNZ fusion method [6] to merge the result lists of the weak and strong engines. Document scores induced from ranks, as suggested in [3], are used in CombMNZ. As all other fusion methods, CombMNZ ad-dresses (explicitly) only one of the two criteria for effective response strategy  X  i.e., retrieval effectiveness. The performance numbers of all methods are presented in Table 1. We use ProbRR( p ) and ProbResRR( p ) to indicate that the ProbRR and ProbResRR response strategies were used with the probability parameter p .
 Table 1 shows that Strong is much more effective than Weak for both MAP and NDCG; the differences are substan-tial and statistically significant for all experimental settings. Furthermore, the overlap between Strong and Weak, as mea-sured by OV@10 and OV@20, is low. Thus, the experimen-tal setting we used adheres to the problem definition: (i) the strong search engine is (much) stronger in terms of retrieval effectiveness, and (ii) the overlap between the result lists of the strong and weak search engines is not high.

We also see in Table 1 that WeakReRank is quite an ef-fective response strategy; specifically, in comparison to a highly effective fusion method (CombMNZ) in terms of both retrieval effectiveness and overlap at top ranks with Strong. WeakReRank X  X  retrieval effectiveness can be statistically sig-nificantly worse than that of Strong. However, WeakR-eRank outperforms Weak for MAP and NDCG, with all the improvements being statistically significant. Although the overlap at top ranks of WeakReRank with Strong is larger than that of Weak, it is still quite low with respect to that of the other strategies considered. These findings attest to the effectiveness of using relevance modeling based on the result list of the strong search engine so as to re-rank the result list of the weak search engine.

Table 1 also shows that ProbRR is a highly effective re-sponse strategy in many cases. Evidently, the balance be-tween retrieval effectiveness and overlap with Strong can be effectively controlled via the parameter p . Although in terms of overlap with Strong, ProbRR is somewhat less effective than WeakReRank and CombMNZ, in terms of retrieval ef-fectiveness it is substantially better than the two and often better than Strong.

It is also evident in Table 1 that ProbResRR is less ef-fective than ProbRR for MAP and NDCG. However, the overlap numbers for ProbResRR are lower than those for ProbRR. This finding is not surprising because ProbRR uses the re-ranked list of the weak engine while ProbResRR uses the residual documents in the same list that remain after the removal of documents that also appear in the result list of the strong engine.
 effect of the parameter p used in ProbRR and ProbResRR on the tradeoff between the retrieval effectiveness of the re-sponse list of the weak search engine, as measured using MAP, and its overlap with the result list produced by the strong search engine, measured using OV@10.

Figure 1 presents the results of setting p to values in { 0 , 0 . 1 , . . . , 1 } ; p = 0 amounts to using only the result list of the strong search engine in ProbRR and ProbResRR, while p = 1 amounts to using the result list L [ n ] weak ; re
We can see that for ProbResRR, MAP decreases with in-creasing values of p . The reason is that fewer documents that appear in the result list of the strong engine are used. Fur-thermore, ProbRR outperforms ProbResRR for all p &gt; 0. In addition, we see that ProbRR can attain its optimal per-formance for 0 &lt; p &lt; 1 (i.e., outperform both Strong and WeakReRank) which echoes findings in work on fusion [4].
For both ProbRR and ProbResRR, OV@10 decreases with increasing values of p , as fewer documents are selected from the result list of the strong engine. As expected, for the same value of p ( &gt; 0), the OV@10 of ProbRR is higher than that of ProbResRR. Yet, for the same value of overlap, the MAP of ProbRR is higher than that of ProbResRR. Strong and Weak, respectively. of OV@10. The y-axis on the left of a figure is the range of MAP.
Thus, we arrive to the conclusion that tuning the parame-ter p in ProbRR and ProbResRR helps to effectively control the balance between retrieval effectiveness and overlap (di-versity) with the strong engine. Furthermore, ProbRR is a more effective response strategy than ProbResRR as it al-lows to attain improved level of retrieval effectiveness for the same level of overlap with the strong search engine.
We presented the first (preliminary) attempt to address the search engine duel problem; namely, how can a search engine with a relatively weak relevance ranking function compete with a search engine with a much stronger rele-vance ranking function? We devised an algorithmic response framework which consists of several strategies that can be used by the weak search engine. We consider a response to be effective if it results in improved search effectiveness and the search results are different than those presented by the strong engine. Empirical evaluation demonstrated the merits of our response strategies and shed some light on the (relevance) effectiveness-diversity tradeoff embodied in our bi-modal criteria for response effectiveness.

Devising additional criteria for response effectiveness, along with developing additional corresponding response strate-gies, is the first future venue we intend to explore. We also plan on devising response strategies for the strong engine. Acknowledgments We thank the reviewers for their com-ments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Cen-ter. This work has also been supported in part by Microsoft Research through its Ph.D. Scholarship Program.
