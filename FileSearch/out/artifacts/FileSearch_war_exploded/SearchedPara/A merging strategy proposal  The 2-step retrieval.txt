 Fernando Mart  X  inez-Santiago  X  L. Alfonso Ure  X  na-L  X  opez  X  Maite Mart  X  in-Valdivia Abstract A usual strategy to implement CLIR (Cross-Language Information Retrieval) sys-present in the multilingual collection in order to compute an independent monolingual infor-mation retrieval process per language. Thus, this approach divides documents according to language. In this way, we obtain as many different collections as languages. After searching in these corpora and obtaining a result list per language, we must merge them in order to provide a single list of retrieved articles.
 In this paper, we propose an approach to obtain a single list of relevant documents for CLIR systems driven by query translation. This approach, which we call 2-step RSV (RSV: Retrieval Status Value), is based on the re-indexing of the retrieval documents according to the query vocabulary, and it performs noticeably better than traditional methods.
The proposed method requires query vocabulary alignment: given a word for a given query, we must know the translation or translations to the other languages. Because this is not always possible, we have researched on a mixed model. This mixed model is applied in order to deal with queries with partial word-level alignment. The results prove that even in this scenario, 2-step RSV performs better than traditional merging methods.
 Keywords CLIR . Merging strategies . Pseudo-relevance feedback . 2-step RSV . Mixed 2-step RSV 1. Introduction The typical CLIR requirement is for the user to input a free form query, usually a brief description of a topic, into a search or retrieval engine which returns a list, in ranked or-der, of documents or web pages that are relevant to the topic. The search engine matches target documents. Unlike monolingual information retrieval, CLIR requires query terms translate query terms into index terms, how to eliminate alternative translations (e.g. to de-cide that French  X  X raitement X  in a query means  X  X reatment X  and not  X  X alary X ), and how to rank or weight translation alternatives that are retained (e.g. how to order the French terms  X  X venture X ,  X  X usiness X ,  X  X ffaire X , and  X  X iaison X  as relevant translations of English  X  X ffair X ) (Grefenstette 1998).

A new issue arises as to whether queries are translated into each language present in the multilingual document collection. The user query is translated into each language present in the multilingual collection in order to compute a different monolingual information re-trieval process per language. Thus, this approach divides document sources according to language. In this way, we obtain as many different collections as languages. Searching in
CLIR systems. The second step must merge monolingual result lists in order to provide users with a single list of retrieved documents. Obtaining an optimal multilingual list by means of monolingual list is not an easy problem, since the score assigned to each document (the so called Retrieval Status Value -RSV) is calculated not only according to the relevance of the document and the IR model, but also the rest of monolingual corpus is determinant (Dumais 1994).

The rest of the paper is organized as follows. Firstly, we present a brief revision of the most well-used methods for merging strategies. Section 3 and 4 describe our proposed method. In
Section 5, we detail the experiments carried out and the results obtained. Finally, we present our conclusions and future lines of work. 2. Traditional merging strategies
There are various approaches in order to carry out the merging of monolingual collections, collection, between 20% and 40%) (Savoy 2002). Perhaps for this reason, CLIR systems based on document translation tend to obtain results noticeably better than system driven by query translation. The most popular approaches are briefly depicted below: 1. Round-Robin fashion. The documents are interleaved according to rank obtained for each document by means of monolingual information retrieval processing. Thus, given a multilingual collection and N languages, the first document for each monolingual retrieval list will constitute M first documents, the second document of each list will constitute the next M documents, and so on. In this case, the hypothesis is the homogeneous distribution of relevant documents across the collections. This merging process decreases precision about 40% because of the merging process (Callan et al. 1995, Voorhees et al. 1995a). 2. Raw-scoring. This method produces a final list sorted by document score computed in-dependently for each monolingual collection. This method works well whether each are distributed homogeneously over all the monolingual collections. Heterogenous term distribution will mean that query weights may vary widely among collections (Du-mais 1994), and therefore this phenomenon may invalidate the raw-score merging hypothesis. 3. Normalized scoring. An attempt to make document scores comparable is by normalizing in some way the document score reached for each document: 4. Perhaps the most original approach is by creating a single index of all the documents without taking into account the multilingual nature of the collection (Gey et al. 2000).
In this way, a single and multilingual index is obtained with all the documents of every language. Given a user query, a single multilingual query is obtained in the same way as a single term index was obtained. That is, the query must be translated into each language present in the multilingual collection. A query for each translation is not generated but all the translations are joined thereby forming a composite query. Finally, this composite query is searched across the entire multilingual term index. In the same way as the approach based on document translation, this method will return a single list of documents for each query. But the background problem is not eliminated. Although a single index is generated, the vocabulary of each language is practically exclusive. Two different languages rarely share terms (a noticeable exception are proper nouns). For this reason, the weight obtained by each term will depend on the language. Therefore, the rank and scoring will be fully comparable between documents expressed in the same language. However, the obtained results with this method are disappointing (Nie and Jin 2002, McNamee and Mayfield 2002).

Note that CLIR merging problem is very similar to the collection fusion problem (Voorhees et al. 1995a) of Distributed Information Retrieval (DIR), although DIR environments tend to be monolingual and uncooperative (information about resources such as size of the collection, documental frequency of the terms are not detailed). Thus, some collection fusion techniques have been applied to CLIR with several degrees of success: 5. Towell et al. (1995),Voorhees et al. (1995b) apply several learning algorithms in order collection. One such learning algorithm is based on neural networks. A similar approach, has been applied to CLIR environments with good results (Mart  X   X n et al. 2003). The main difference between both works is the neural network topology (LVQ neural networks instead of feed-forward neural networks). In addition, Voorhees et al. (1995b) create only network per collection with a single output according to the document relevance. 6. One of the most extended DIR models is CORI (Callan et al. 1995). Some of the CORI metodology was applied in Savoy (2002), Moulinier and Molina-Salgado (2003) with poor results. However, recent variations of the CORI model have obtained better results applied to CLIR (Savoy 2004). 7. Finally, Calv  X  e and Savoy (2000), Savoy (2003a) propose a merging approach based on probability of a binary outcome variable according to a set of independent explanatory variables. The probability of relevance to the corresponding document D i will be estimated according to both the original score and logarithm of the ranking. Based on these estimated probabilities of relevance, the monolingual list of documents will be interleaved forming a single list:
Prob[ D i is rel | rank i , rsv i ] =
The coefficients  X  ,  X  1 and  X  2 are unknown parameters of the model. The usual methods when fitting the model tend to be maximum likelihood or iteratively re-weighted least squares methods.

Because this approach requires fitting the underlying model, the training set (topics and their relevance assessments) must be available for each monolingual collection (in the same way as approaches based on neural networks). Relevance assessments are usually available only for academic collections such as TREC or CLEF 1 campaigns. 3. The 2-step retrieval status value method
The basic 2-step RSV idea is straightforward: given a query term and its translations into the other languages, their document frequencies are grouped together (Mart  X  inez-Santiago et al. 2003). In this way, the method requires recalculating the document score by changing the document frequency of each query term. Given a query term, the new document frequency will be calculated by means of the sum of the monolingual document frequency of the term and their translations. Because re-indexing all the documents in the multilingual collection could be computationally expensive, given a query only the retrieved documents for each monolingual collection are re-indexed. These two steps are: 1. The document pre-selection phase consists of translating and searching the query on  X  The translation to the rest of languages for each term from the original query as result  X  A single multilingual collection of preselected documents ( D collection) as result of the 2. The re-indexing phase consists of re-indexing the multilingual collection D , but con-
The hypothesis of this method is as follows. Given two documents, the score of both documents will be comparable when the document frequency is the same for each meaningful term query and their translations. By grouping together the document frequency for each term and its own translations, we ensure compliance with the hypothesis. 3.1. A more formal definition of the approach A large number of retrieval methods are based on this structure (Sheridan et al. 1997): &lt;
T ,, D ; ff , df &gt; where:
D es is the document collection to be indexed.
A token  X  is a specific occurrence of a term in a document. Let T be the set of all tokens representing an occurrence of a term  X  i  X  in a document d j  X  D .

Thus, the function  X  : T  X  ,  X   X   X  (  X  ) simple process such as removing accents or another more complex such as root extraction (stemming), lemmatization ... In addition, stopwords will be removed (  X  (  X  ) =  X , if  X  belongs to the stopwords set.) ff is the feature frequency and denotes the number of occurrences of  X  i in a document ff (  X  i , d j ): =| {  X   X  T |  X  (  X  ) =  X  i  X  d (  X  ) = d j }| where d is the function that makes each token  X  correspond to its document: d : T  X  D , X   X  d (  X  ) df is the document frequency and denotes the number of documents containing the feature  X  at least once: df (  X  i ): =| { d j  X  D | X   X   X  T :  X  (  X  ) =  X  i  X  d (  X  ) = d j }|
For each monolingual collection we begin with the already-known structure: &lt; T , i , D i , ff , df &gt;, 1 &lt; = i &lt; = N Where N is the number of languages present in the multilingual collection to be indexed. Let
Q to the other languages, in such a way that Q i is the query expressed in the same language as &lt; T , i , D i , ff , df &gt; , it is possible to obtain a new and single structure: &lt; T , , D , D , ff , df &gt; where: D is the complete multilingual document collection: D ={ D i , 1 &lt; = i &lt; = N } . D is the set of retrieved multilingual documents as consequence of running the query Q . T is the set of concepts  X  j , and denotes the vocabulary of the D collection. Since each query Q i is a translation of another, it is possible to align queries at term level 3 .  X  : ={  X  ij  X  Q i , 1 &lt; = i &lt; = N } , 1 &lt; = j = M , M =| Q | where  X  ij represents all the retained translations (usually synonymous) of the term j of the query Q to the language i and M indicates the query length. Thus,  X  j denotes the concept j of the query Q independently of the language. is a new vocabulary to be indexed, such that each  X  j  X  is generated as follows:  X  : ={  X  (  X  ij ) , 1 &lt; = i &lt; = N } , 1 &lt; = j &lt; = M The ff function and df function are interpreted as usual:  X  ff is the number of occurrences of the concept j , expressed in the i language, in the k document. ff (  X  j , d k ): = ff (  X  ij , d k ) , d k  X  D  X   X  ij is the index token j of the translation to the language i of the query Q .  X  df is the number of documents with the concept j in the D collection. That is, the sum Given this structure, a new index is generated in search time, but only taking into account the documents that are found in D . The df function operates on the whole collection D , not only on the retrieved documents in the first phase, D , because of we found empirically that the obtained results are slightly better when the whole collection is considered in order to calculate the new document frequency. This is not surprising since D is made up by searching documents with the T vocabulary and differences between document frequency of concepts will be artificially lower than by taking into account D instead of D . Note that 2-step RSV approach is quite different from the computation performed by Pirkola in his experiments based on synonym operator (Pirkola 1998). The model proposed by Pirkola treated the translations of a query term as if they were synonyms, by using InQuery synonym operator # syn , which means grouping target words derived from the same source word into the same facet. The main difference between this approach and 2-step RSV is that 2-step RSV treats both the original term and the translated terms as synonymous. On the other hand, Pirkola X  X  approach only applies the synonym operator between translated terms. Another important difference is that 2-step RSV calculus only operates on a small subset of documents ( D ) rather than on the whole set of documents, which is much faster than the InQuery synonym. Finally, Pirkola and others (Sperer and Oard 2000, Airio et al. 2003) have applied the InQuery synonym operator on bilingual experiments instead of on full multilingual environments.

In some way, this method shares some ideas with the CLIR systems based on corpus translation, but instead of translating the complete corpus, it only translates non-empty words appearing in the query in the retrieved documents. These two simplifications allow the de-ployment of the system in search time since the necessary re-indexing process in the second phase is computationally possible due to small size of D collection and to the scarce vo-cabulary T (approximately, non-empty query terms multiplied by the average number of retained translations by term and by the number of languages present in D ). 4. Mixed 2-step RSV and not aligned words with the rest of its translations. However, this information is not always available:
Several translation techniques such as Machine Translation make word-level alignment of the queries difficult.

The second step of the proposed method does not make use of automatic query expansion techniques such as relevance feedback (RF) or pseudo-relevance feedback (PRF) applied to monolingual queries. Since RF and PRF extend every monolingual query with collection-dependent words, the reindexing process (second step of 2-step RSV) will not take into account all of these words. Because such words are not the same for each monolingual ignores these new terms for the second step. However, the overall performance will also improve since PRF and RF improve on monolingual experiments and usually some ex-tended terms coincide with terms of the original query, and such terms will be aligned. Sometimes, a word is translated as two or more words (i.e., a multi word expression). For example, the word  X  X ope is translated to Russian by  X  X imskij papa (Cyrillic alphabet has been transliterated with ASCII characters, following the standard Library of Congress transliteration scheme). In this case, word and translation are not aligned. Nevertheless, if a good multiword expressions list is available, then the most frequent multiword expressions are mapped as a single token (Rimskij papa  X  Rimskij Papa). Thus,  X  X ope is successfully aligned with  X  X imskij Papa. If this is not possible, the word and that translation remain unaligned. We have used multiword expressions list only for some European agglutinative languages: Dutch, Finnish and German (Mart  X  inez-Santiago et al. 2004).

As a way to deal with partially aligned queries, we propose four approaches by mixing evidence from aligned and not aligned terms: Raw mixed 2-step RSV method: A straightforward and effective way to partially solve this problem is by taking non-aligned words into account locally, just as terms of a given monolingual collection. Thus, given a document, the weight of a non-aligned term is the initial weight calculated in the first step of the method.
 In this way, the second step of the 2-step RSV method manages two vocabularies for each language: the concept dictionary T , and the new local term vocabulary T i . T i contains every unaligned query-term expressed in the i language. Thus, for a given  X  ij , term j into the monolingual collection i , the document frequency value will be:  X  df (  X  i ), if  X  (  X  ij ) belongs to the concept  X  i . In other words,  X  ij is aligned. unknown.
 Thus, the score for a given document d i will be calculated in a mixed way by means of the weight of local terms and global concepts present in the query: 2-step RSV method depicts. On the other hand, RSV nonalign i is calculated locally. Finally,  X  is a constant (usually fixed to  X  = 0 . 75 because we have found empirically that this value obtains the best results for many queries. Nevertheless, in spite of values behind 0.6 and 0.8 obtain very similar results, the  X  value should be fixed for each particular multilingual system whether relevance assessments are available ).
 Normalized mixed 2-step RSV method: Since the weights of the aligned and non-aligned words are not comparable, the idea for the raw mixed 2-step RSV seems counterintuitive. In an attempt to make comparable RSV align and RSV nonalign , we are able to normalize those values, as is shown in Formula 1: RSV i =  X   X 
Learning-based algorithms (logistic regression and neural networks). In the same way that the score and ln( rank ) evidence was integrated by using logistic regression (Formula 3), we are able to integrate ln( rank ), RSV align and RSV nonalign : reached by D i at the end of the first step.

Again, training data must be available in order to fit the model. This a serious drawback, the original rank of the document. In addition this approach can be applied with fully-aligned queries ( rs v nonalign i = 0) in a way to improve the original 2-step RSV by using extra information extracted from the first step: the rank of the document obtained through the monolingual searching process.

In addiction, we also used neural networks to integrate aligned and non-aligned queries
Quantization (LVQ) algorithm. LVQ is supervised competitive learning which needs a training data set to adjust the model. 4.1. 2-step RSV and machine translation In this study, machine translation is perceived as a black box which receives English phrases and generates translations of theses phrases to the other languages. We have developed a straightforward and quite effective algorithm in order to align phrases and their translations. In order to explain how it works, suppose the phrase  X  X esticides in baby food X  is translated to Spanish as  X  X esticidas en alimentos para ni  X  nos X . The question is what English word is translated by what Spanish word?. The algorithm works as follows: 1. Let the original phrase P en ,  X  X esticides in baby food X . 2. Translate P en + Unigrams P  X  EXP en = { Pesticides in baby food }{ Pesticides,baby, food }{ Pesticides baby,baby food }  X  P sp = { Pesticidas alimento ni  X  nos }  X  Unigrams  X  Bigrams word sp i is translation of word en i ,  X  word sp i  X  Unigrams P bigram sp i is translation of bigram en i ,  X  bigram sp i  X  Bigrams P 3. For each word sp i  X  Unigrams P 4. For each bigram bigram sp i  X  Bigrams P
This algorithm fails if there are bigrams without any aligned term. For example, bigram words. In order to improve the matching process, words are stemmed by removing at least genre and number.

Finally, agglutinative languages such as German usually translate (adjective, noun)  X  X   X  auglingsnahrung X  instead of  X  X   X  augling Nahrung X  (Babelfish translation). We decompound compound words if possible by using the algorithm depicted in Mart  X  inez-Santiago et al. (2004).

We have tested the proposed algorithm with the CLEF query set (Title + Description) of the last three years. It aligns about 85 X 90% of non-empty words (Table 1). 5. Experiments and results The experiments have been carried out for eight languages: English, Spanish, German, French, Italian, Swedish, Dutch and Finnish. CLEF 2001, 2002 and 2003 collection data and relevance assessments have been used in our experiments. The Cross-Language Evaluation Forum (CLEF) supports global digital library applications by (i) developing an infrastructure for the testing, tuning and evaluation of information retrieval systems operating on European languages in both monolingual and cross-language contexts, and (ii) creating test-suites of reusable data which can be employed by system developers for benchmarking purposes. 6 A brief description of test collections and structure of queries is depicted as follows: CLEF 2001 and CLEF 2002 editions have the same test collection for the multilingual task. This collection is made up by news published for 1994 in Agencia EFE (Spanish), Der Spiegel (German), Frankfurter Rundschau (German), La Stampa (Italian), Los Angeles
Times (English), Le Monde (French) and SDA (French, German and Italian). CLEF 2003 edition has two different multilingual tasks: the main multilingual task CLEF 2003-8 is made up by news published for 1994 and 1995 in eight different European languages.
CLEF 2003-8 is made up by the whole of CLEF 2001, 2002 sources (extended to 1995 year) and news from Algemeen Dagblad (Dutch), Aamulehti (Finnish), Glasgow Herald (English), Handelsblad (Dutch) and Tidningarnas Telegrambyr X  a (Swedish). Finally, the other CLEF 2003 multilingual task, CLEF 2003-4, is a subset of CLEF 2003-8 task, since CLEF 2003, 4 collection set is limited to four languages (English, French, German and
Spanish) (see Table 3). CLEF queries have three sections: title, description and narrative. The title is a very sort phrase (4 X 5 words). The description is a slightly longer phrase (15 X 20 words). Finally the narrative section is a more detailed paragraph about the topic of the query (see Table 2 and Table 4). Note that CLEF 2003-4 query set is the same as the CLEF 2003-8 one but only for four languages.

Every collection has been pre-processed as usual, using stopword lists and stemming algorithms available across the Web. 7 Stopword lists have been increased with terms such as  X  X etrieval X ,  X  X ocuments X ,  X  X elevant X  ... Due to the German, Dutch Swedish and Finnish morphological complexity, compound words have been reduced to simple words by using a simple probabilistic procedure (Mart  X  inez-Santiago et al. 2004). Once collections have been pre-processed, they are indexed with the Zprise IR system, using the OKAPI probabilistic model (fixed at b = 0 . 75 and k 1 = 1 . 2) Robertson et al. (2000). The OKAPI model has also been used for the on-line re-indexing process required by the calculation of 2-step RSV. 5.1. Translation strategies and bilingual results We have used several translation approaches. For each query we have taken into account only Title and Description query fields.

Machine Readable Dictionary (MDR, Babylon 8 ) has been used to translate the query word for word. This bilingual dictionary may suggest not only one, but several terms for the translation of each word. In our experiments, we decide to pick the first translation label  X  X abylon 2 X ). Since Babylon 1 and Babylon 2 are word for word translations, such translations are fully-aligned. Thus, both translation approaches are used to test original 2-step RSV.

Machine Translation (MT, Babelfish) translates phrases better than words, and then word level alignment is not possible at all. We propose a simple and quite effective approach in order to align phrase translations at word level (Section 4.1). Partially aligned phrases are used to test mixed 2-step RSV.
 Mixed MT and MDR. This third approach translates every phrase by taking together Babelfish and Babylon 1 translations.
 The rest of this section consists of bilingual experiments and multilingual experiments driven by query-translation with fully and partially aligned queries.

Tables 5 and 6 show the bilingual precision obtained re-indexing by means of several translation approaches. Babelfish+Babylon 1 bilingual experiments are noted by the  X  X B X  label.
 Babylon 1 + Babelfish approach performs slightly better than the rest. On the other hand, Babylon 1 slightly outperforms Babylon 2. Since Babylon 2 keeps two translations per word, precision could be worse because of bad translations kept by Babylon 2. Babelfish obtains results between Babylon 1 + Babelfish and Babylon. These results are according to other reports over the same query set (Savoy 2002 2003b). Better improvements are usually reached by means of mixing several MT and MDR resources. The aim of this work is not to obtain the best translation possible but to experiment using the 2-step RSV technique with several translation approaches.

Expansion queries were carried out by means of pseudo-relevance feedback (blind ex-pansion). In this study, we adopted Robertson-Croft X  X  approach (Harman 1992) where the system expands the original query generally by 10 X 15 search keywords, extracted from the 10-best ranked documents. We have choosen this configuration because empirically we have obtained better results than with other configurations available at ZPrise system. 5.2. Multilingual results with fully aligned queries at term level with a single list of retrieved documents. In this section, we study the second step in which query and translation are fully aligned. This scenario is not possible with machine translation applied for each language collection, then those new terms added to the original query will be not aligned since such terms are language-dependent. Thus, in order to study original 2-step RSV, we have used Babylon 1 and Babylon 2 query set without any expansion query techniques. In this way, the queries are fully aligned at term level.

The merging approach has been made up by using several approaches: round-robin, raw scoring, normalized score (Formula 1), variation of normalized score (Formula 2), logistic regression (Formula 3), neural networks and 2-step RSV approach. In addition, theoretical optimal performance has been calculated by using the procedure proposed in Chen (2003) (label  X  X ptimal performance X  in Table 7). This procedure computes the optimal performance that could possibly be achieved by a CLIR system by merging bilingual and monolingual list is preserved. The relevances of documents must be known previously, thus it is not useful to predict ranks of documents in the multilingual list of documents. The procedure obtains the upper-bound performance for a set of ranked list of documents, and this information is useful in measuring the performance of several merging strategies. Note that 2-step RSV calculus does not ensure the preservation of the relative ranking of documents, the upper-bound performance calculated by such procedure could be overcome, at least theoretically.
The 2-step RSV merging approach improves on all the other approaches, reaching about 85% of theoretical optimal performance (Table 7). On the other hand, traditional methods perform at about 70%.

Logistic regression and neural networks obtain the second best result, but both approaches require data training (we have used CLEF-2001 queries and relevance assessments).
We have implemented the logistic regression model with the R package. The model is adjusted by an iterative re-weighted least squares algorithm (it is part of the R package).
We have carried out the experiments with the LVQ algorithm by using the implementation described in LVQ PAK documentation with default parameters.

Babylon 1 and Babylon 2 obtain a very similar precision. Since bilingual experiments by using Babylon 2 translation approach are a little worse than Babylon 1 (see Table 5), Babylon 2.

Finally, we have carried out several experiments with CLEF2003-4 and CLEF2003-8 tasks in order to evaluate several merging approaches with four and eight languages. The previous results, or is even increased (Table 8).
 5.3. Analysis of failures
Sometimes, 2-step RSV technique works worse than traditional merging strategies. For ex-ample, Table 9 shows three such queries, translated by using Babylon 1 query set.
Maybe that the most representative case of error is the query 50. The lost of precision is over 50%. Thus, whether we examine this query more carefully we obtain the data summarized in Table 10.
 relevant documents and 228 are written in Spanish (38.5%). This percentage is too high the language with more documents retrieved (81.1% of retrieved documents are written in
Spanish) and more relevant documents retrieved (203 of 341, 59.5%). In other words, there are too many documents from the Spanish collection for this query. The question is why this happens and how it affects the 2-step RSV approach.

Table 11 shows document frequency for each concept of the query. To ensure clarity we represent each concept by the corresponding English term. The 2-step RSV approach uses the global document frequency (grouping together the document frequencies of aligned query terms) re-indexing with the global document frequency. The concepts  X  X hiapas X  and  X  X prising X  are the most meaningful concepts since they have the lowest document frequency. The concept  X  X prising X  is smoothly distributed among the German, French and Spanish collections. On the other hand, 62.9% of documents containing the concept  X  X hiapas X  are of retrieved documents coming from the Spanish data collection. In other words, 2-step RSV and the proportion of relevant documents for that collection is inferior to the proportion of meaningful terms that the collection contains. Other approaches such as round-robin or normalized score could present less sensibility to this situation because the score of each document is discarded (round-robin sort by ranking only) or is calculated and normalized locally. 5.4. Multilingual results with partially aligned queries at term level In this section we study the mixed 2-step RSV merging strategy by means of queries partially aligned at term level. Given a language and a query, the query usually contains non-aligned words when the translation approach works at phrase level better than word level, or expan-sion query techniques such as blind-feedback are applied. The pseudo-relevance feedback technique expands the original query by adding search keywords extracted from the first N documents ranked. Since some of these keywords are new terms (not appearing in the original query), these terms are not aligned. 5.4.1. Experiments based on MDR translation approach Tables 12 and 13 show results obtained with MDR translations and CLEF 2001 X 2002 and 2003 corpora. Pseudo-relevance feedback is applied for each query and language. Again, the merging approach has been formed by using several approaches: round-robin, raw scoring, normalized score (Formula 1), variation of normalized score (Formula 5), logistic regres-sion (Formula 3) 2-step RSV, and mixed 2-step RSV approach (raw, normalized and neural network and logistic regression).

The proposed 2-step RSV merging approach improves on all the other approaches. Raw mixed 2-step RSV and normalized mixed 2-step RSV have been calculated by means of Formula 4 and Formula 5, with  X  = 0 . 75. These values have been fixed because empirically we have found good results. Mixed 2-step by means of logistic regression is implemented as shown in Formula 6. Thus, the unknown parameters  X  ,  X  1 ,  X  2 and  X  3 must be estimated in the same way as shown in Equation 6 by using iteratively re-weighted least squares method. The best result is obtained with LVQ neural network mixed 2-step RSV approach and Babylon-1 translation. Unfortunately, learning-based algorithms are not applied to CLEF2003-4 and CLEF2003-8 tasks because training data is not currently available.

Perhaps the most surprising result is the good performance of raw-mixed 2-step RSV, even overcoming the normalized version of the approach, and obtaining a result very near counterintuitive since the method adds two values which are not directly comparable: the score obtained by both aligned and non-aligned terms. Some of the reasons for this good result are:  X  parameter of Formula 4 limits the weight of the unaligned factor.
 Not all the terms to be added to the original query are new terms since some terms obtained aligned terms. In the same way this explains the good performance of 2-step RSV original method with expanded queries.

CLEF uses comparable document collections (news stories from the same period). The results might be different if collections have vastly different sizes and/or properties.
In order to study the real impact of non aligned words in the final performance of the system, we have carried out an experiment by using original expanded queries and expanded queries without non aligned terms (non aligned terms have been removed from the original expanded query) . The results are summarized in Table 14.This table shows that the improvement is moderate, but this improvement holds when using mixed 2-step RSV approach.

Another interesting result is that the performance obtained by raw mixed 2-step is about 85% of the theoretical optimal performance. This percentage is very similar to the percentage obtained with the original 2-step RSV method (Table 7).

In short, 2-step RSV approach seems to work well with non aligned terms, when the proportion of such terms is not too large. 5.4.2. Experiments based on MT translation approach In order to evaluate the proposed approach with other translation approaches, we have carried out several experiments with the CLEF 2001 X 2002 test collection and CLEF2001+CLEF 2002 + CLEF2003 query set (160 queries, five languages, EN, SP, DE, FR, IT) by using MT (Babelfish label) and MT + MDR (Babelfish + Babylon 1 label) with and without pseudo-relevance feedback (Table 16). Since the CLEF2003 collection is a superset of previous CLEF collections, and we are using the CLEF2001 X 2002 collection test, we have removed from the relevance assessments the documents belonging to the CLEF 2003 document collection.
Since MT does not obtain fully aligned queries (see Table 1) 2-step RSV method is not directly applicable, so we have used a raw-scoring 2-step RSV variant with  X  = 0 . 65. The most interesting result is that MT and MT+MDR approaches are near 90% of optimal performance.

Again, results by merging with raw-scoring are noteworthy. CLEF corpora are comparable contributes to make the document score comparable to a certain extent.

Babelfish + Babylon 1 obtains noticeably better results when no expanded queries are used. On the other hand, the best translation approach is not clear when PRF is taken into account.
 6. Conclusion and future work of merging relevant documents in CLIR systems. This approach has performed noticeably collections, languages or translation resources. The proposed method reaches about 85 X 90% of the theoretical optimal performance (traditional merging strategies obtain about 65 X 70%). In order to achieve this performance, queries must be aligned at term level. In addition, we suspect that the best results are obtained when meaningful terms are distributed throughout document collections approximately in the same proportion as relevant documents.
On the other hand, a drawback for the proposed method is that, given a query, every word must be aligned with the other words, for every language. Thus, we study four approaches for the integration of aligned and non-aligned terms in the same query. The best results are obtained re-indexing by means of logistic regression and neural networks, but this approach depends on data training (mainly assessments of relevance) for each collection, usually scarce. Good results are obtained with both raw and normalized mixed 2-step RSV approaches. Our future efforts are directed towards the following aspects:
Dealing with translation probabilities. The original term and translations are treated in exactly the same way in the proposed model. When translation probabilities are available, the calculation of the document frequency and term frequency for a given concept should be reconsidered by means of the translation probability. This can be modelled as follows: ff (  X  j , d k ): = ff (  X  ij , d k )  X  w (  X  ij ) ,  X   X  ij  X   X  j , to language i .
 Testing the method with other translation strategies such as the Multilingual Similarity Thesaurus.
 Index terms used in the reported experiments are basically obtained by means of stemming. We are very interested in the application of the proposed approach to n-grams indexing. n -grams cannot be assimilated directly as concepts since an n-gram is usually contained within several unrelated terms. In addition, we have carried out preliminary experiments, and the obtained results suggest that an n-gram is not a direct representation of a concept.
Finally, we will continue studying strategies in order to deal with aligned and non-aligned term queries: the integration of both sorts of terms by means of bayesian networks (although this structure requires data training) and the development of global rather than local pseudo-relevance feedback constitute interesting areas to explore.
 References
