 As a powerful way to discover the hidden semantics of the document collec-tion, topic models are extensively studied and successfully applied to many text mining tasks, such as information retrieval [ 1 ] and document clustering [ 2 ]. Tra-ditional topic models assume that the documents are independent and do not consider the correlation among them. With the flourish of Web application, tex-tual documents such as papers, blogs and product reviews, are not only getting richer, but also interconnecting with other objects like users in various ways; and therefore form text related heterogeneous information networks [ 3 ]. Take the bib-liographic data shown in Figure 1 as an example. There are three types of objects, papers, authors, and venues in such a heterogeneous network. These objects form two types of relationships: the author-write-paper relationship between authors and papers, and the venue-publish-paper relationship between venues and papers. It is challenging for traditional topic models to capture the rich information, especially the link information in such a text related heterogeneous information network.
 Traditional topic models, such as latent semantic analysis (LSA) [ 4 ], proba-bilistic latent semantic analysis (PLSA) [ 5 ], and latent dirichlet allocation (LDA) [ 6 ] focus on purely utilizing the textual information to discover topics with the assumption that the documents are i.i.d. (independent and identical distributed). With the explosive of interconnected textual contents with rich link informa-tion, the assumption may not hold and traditional models become less effective. Although some attempts, such as LaplacianPLSI [ 7 ], NetPLSA [ 8 ], and iTopic-model [ 9 ] have been conducted to combine topic modeling with link information in a homogeneous network, how to integrate various types of links associated to different types of objects into a unified topic model is still less studied. Although the links among documents as well as other types of objects might be helpful for analyzing text, it is non-trivial to handle the rich heterogeneous information in a unified framework. First, it is challenging to model the seman-tic information of links such as the author-write-paper and venue-publish-paper relationships. Different from text, link structure is a totally different type of information and can not be easily added to traditional topic models in a straight-forward manner. Second, there are usually several types of different objects in a heterogeneous information network. Different types of objects may have their own inherent information and should be treated differently. How to use the dif-ferent types of objects and integrate them in a unified way with the textual information also makes the studied problem challenging.
 rating the heterogeneous link information into topic modeling. cluTM learns a latent semantic space by jointly factorizing the document-phrase matrix and the link matrices with latent semantic analysis. The basic idea is that the textual documents and link information in the heterogeneous information networks have similar latent semantic features. For example, in the bibliographic data, a paper contains several topics. Likewise, the researchers and venues also have their pre-ferred research topics associated to related papers. The inherent connections between contents and links can be therefore constructed by assuming that the text matrix and link matrices share the same latent semantic features. With such an assumption, all the objects in the heterogeneous information network are projected into a unified latent semantic space based on the common latent semantic features. In the unified latent semantic space, each object is represented as a vector. Topics of documents and clusters of other types of objects can be easily obtained by calculating the similarity of the vectors.
  X  study the novel problem of topic mining and multi-objects clustering simul- X  propose a unified topic model to seamlessly integrate the content of textual  X  extensive experiments on DBLP dataset show the effectiveness of the pro-concepts. We elaborate cluTM in Section 3. Section 4 presents the extensive experiment results. We discuss the related work in section 5 and finally conclude our work in section 6. In this section, we formally introduce several related concepts and notations to help us state the problem.
 Definition 1. Information Network [ 3 ]. Given a set of objects from K types X = { X k } K k G = &lt;V,E&gt; is called an information network on objects E is a binary relation on V . Specifically, we call such an information network heterogeneous information network when K  X  2 .
 Definition 2. Text Information Network. An information network G = &lt; exists at least one type of text object in the network, i.e. of X k is text. Specifically, we call a text information network text information network when K  X  2 .
 DBLP Bibliographic Network Example. We use the DBLP bibliographic network as an example to illustrate the heterogeneous text information network. As shown in Figure 1, there are three types of objects, i.e., authors A , venues VE and papers D , and two types of links among papers, authors, and venues. The type of paper object is text. The bibliographic network can be denoted as G =( D  X  A  X  VE,E ), where E is a set of edges that describe the relationships between papers D = { d 1 , ..., d n } , authors A = { a 1 VE = { ve 1 , ..., ve o } .
 In our model, each topic can be represented as a set of meaningful frequent phrases [ 10 ], definited as follows.
 Definition 3. Meaningful Frequent Phrases Meaningful frequent phrases are defined as the phrases that capture the main themes of the document col-lection. Meaningful frequent phrases lay a foundation for the readability of the discovered topics. They can be represented as MFP = { mf p where mf p m denoting the m th meaningful frequent phrase. We will first revisit the classic LSA model that is widely used to discover topics of document by matrix factorization. Motivated by LSA model, we next intro-duce how to conduct the matrices factorization on the document-author matrix and document-venue matrix. Finally, we elaborate how to combine the content and link information by joint matrix factorization with a assumption that these matrices share the same latent semantic space. 3.1 LSA on Document-Phrase Matrix We use the classic LSA model to discover the latent topics of documents. The key idea of LSA model is to project documents as well as terms into a relatively low dimensional vector space, namely the latent semantic space, and produce a set of topics associated with documents [ 4 ].
 In our model, documents are represented as a bag of meaningful frequent phrases. Consider the analysis of document-phrase matrix M and it is a sparse matrix whose rows represent documents, and columns represent phrases, where n is the number of documents and m is the number of meaningful frequent phrases. Singular vector decomposition [ 12 ] is performed on matrix M where U D  X  MFP and V D  X  MFP are orthogonal singular matrices U U is a diagonal matrix containing the singular values of M D  X  MFP k singular values to zero,  X  D  X  MFP  X  R k  X  k . The matrix Y = U ( Y  X  R n  X  k ) defines a new representation of documents that each column cor-responds to a topic and each row is a k  X  dimensional vector representing the weights of a document in the k topics. Therefore, the LSA approximation of M where ||  X  || F is the Frobenius norm,  X  1 is the parameter,  X  regularization term to improve the robustness. The i  X  th row vector of Y can be considered as the latent semantic feature vector of document d 3.2 Link Matrices Factorization Taking the bibliographic network in Figure 1 as an example again, the relation-ships between papers and authors as well as papers and venues can be repre-the number of authors and o is the number of venues. In LSA model, a document contains several topics with each topic associated with a set of frequently used terms. Likewise, an author also has several preferred research topics with each research topic associated with a set of related papers. If we consider the authors and papers as documents and words respectively, we can use the similar idea to LSA to analyze the latent semantic of the author  X  paper link. Motivated by above idea, the link matrices M D  X  A can also be factorized by SVD as follows, and the diagonal matrix  X  D  X  A contains the singular values of M topics. The latent topics of a venue preferring can be obtained by factorizing the document-venue matrix using SVD as follows, where U D  X  VE and V D  X  VE are orthogonal matrices U V
D  X  VE V D  X  VE = I and the diagonal matrix  X  D  X  VE contains the singular values set the other singular values to zero. For the matrix M D  X  A sent the document respectively. Thus the matrices M D  X  A represented as follows where V D  X  A is a l  X  k matrix, V D  X  VE is a o  X  k matrix, and Y latent semantic feature matrices. Each column of Y D  X  A and Y a topic and each row is k  X  dimensional vector representing the weights of a document in the k topics. Therefore, Y D  X  A and Y D  X  VE matrix Y . In our model, to combine the content of textual documents and link information in the heterogeneous text information network, we assume that they share the latent semantic feature Y , i.e. Y D  X  A = Y D  X  VE 3.3 Combing Content and Link by Joint Matrix Factorization Based on the assumption discussed above, the document-phrase matrix M
D  X  MFP and link matrices M D  X  A , M D  X  VE are connected by the latent seman-tic feature Y , that is, the latent feature for content is tied to the latent feature for links. Our model aims to find a latent semantic feature Y that best explains thermore, different types of objects and links reflect distinctive semantics of a heterogeneous text information network, so they should be treated differently. To achieve these goals, we propose a joint matrix factorization framework to fuse them into such an unified optimization problem, where  X  ,  X  and  X  (  X &gt; 0,  X &gt; 0,  X &gt; 0) are parameters to balance the rela-tive importance of document-phrase matrix M D  X  MFP and link matrices M M
D  X  VE . We set a constraint  X  +  X  +  X  =1.  X  1 ,  X  2 and  X  parameters that improve the robustness. V D  X  MFP , V D  X  A l  X  k ,and o  X  k matrix respectively. Y is a n  X  k matrix. Note that if  X  =0,  X  =0,thus  X  = 1, the unified topic model boils down to the LSA model on document-phrase matrix.
 The optimization problem aims to simultaneously approximate M M low-dimensional matrices with regularizations. The joint optimization illustrated in Eq.7 can be solved by using the standard Conjugate Gradient (CG) method. The gradients for the object function J are computed as follows: phrase matrix M D  X  MFP and the link matrices M D  X  A , M geneous text information network.
 optimal latent semantic feature Y. All the objects in the heterogeneous infor-mation network are projected into the unified latent semantic space in which each paper, meaningful frequent phrase, author and venue is represented by a k -dimensional vector. According to the similarity calculation of vectors, we can get the topics. Analogously, the author clusters and venue clusters also can be obtained by similarity calculation. In this section, we evaluate cluTM on the real dataset. First, we introduce the experiment setup, including the dataset and evaluation metric. Then we show the experimental results from the following three aspects: case study, parameters analysis, and quantitive comparison with baselines. 4.1 Dataset and Metric We evaluate cluTM on the Digital Bibliography and Library Project (DBLP) dataset. In our experiments, we select papers from DBLP of four research areas, i.e. database (DB), data mining (DM), information retrieval (IR) and artificial intelligence (AI). The selected dataset contains 1200 papers, 1576 authors and 8 conferences. We extract 1660 meaningful frequent phrases from these papers. The heterogeneous text information network of this dataset contains three types of objects: papers, authors and venues, and two types of links: paper-author link and paper-venue link. There are 3139 paper-author links and 1200 paper-venue links in total. Link matrices M D  X  A , M D  X  VE are constructed from the heterogeneous text information network, and the element value in matrix M by using the tf  X  idf weight of the phrases. As we select the papers from four research areas, we set the number of topics k to be 4.
 iments, there are four topic clusters. For each topic cluster, we calculate the Pre-cision and Recall with regard to each given category. Specifically, for the obtained cluster label j and the true cluster label i , the precision can be calculated by P recision ( i, j )= n ij n where n ij is the number of members of category i in cluster j , n of members in the given category i ,and n j is the number of members in clus-ter j . Based on precision and recall, the F1-measure of cluster j and i can be calculated by The F1-measure of the whole clustering results is defined as a weighted sum over all the categories as follows: F 1= i n i n max j F 1( i, j ). 4.2 Experimental Results We first analyze the topic modeling results with case studies. Then we discuss the effect of parameters on performance. Finally, experiments are conducted to compare the performance of object clustering with different models. Topic Analysis with Case Study. In our model, we set parameters  X  =0 . 6,  X  =0 . 3, and  X  =0 . 1 due to the better performance based on our empirical experiment results. The topic modeling results are shown in Table 1. Each dis-covered topic is represented as a set of meaningful frequent phrases. sentative terms generated by PLSA and TMBP-Regu on the DBLP dataset are shown in Table 2. Compared with the results in Table 2, the results shown in Table 1 is easier to understand the meanings of the four topics by meaningful fre-quent phrases, i.e.,  X  X atabase systems X ,  X  X ata mining X ,  X  X nformation retrieval X , and  X  X rtificial intelligence X . cluTM and TMBP-Regu achieve better performance than PLSA by considering the heterogeneous text information network. different algorithms select different terms, all these terms can reveal the topics to some extent. For Topic 4, the topics such as X  X rtificial intelligence X , derived from cluTM is obviously better than the terms  X  X roblem, algorithm, paper X  derived by PLSA and  X  X earning, based, knowledge X  derived by TMBP-Regu. Therefore, from the view of readability of the topics, cluTM is better than PLSA and TMBP-Regu by representing the topics by a set of meaningful frequent phrases. Parameter Analysis. In our model, there are three essential parameters,  X  ,  X  and  X  in joint matrix factorization. In this section, we study the effect of these parameters on the performance of the proposed cluTM.
 tive importance of document-phrase matrix M D  X  MFP , link matrices M M
D  X  VE . When  X  =0,  X  = 0, the joint regularization framework boils down to the LSA model. Since  X  +  X  +  X  =1,wevary  X  from 0 to 1 by step 0.2 and  X  from 0 to 1 by step 0.1 respectively. Tables 3 report the results with the varied parameter values.
 Tables 3 show that the best performance is obtained with  X  =0 . 6,  X  =0 . 3, thus  X  =0 . 1. When  X &lt; 1, the joint regularization framework takes into account both textual documents and links in the heterogeneous text information network. We observe that the performance is improved over the LSA model (  X  = 1) when incorporating link information. One can also observe that the document matrix M D  X  A is more important than the document  X  venue matrix M in the joint regularization framework by the different values of  X  ,  X  . Note that with the decrease of  X  , the performance becomes worse and even worse than the standard LSA. This is mainly because cluTM relies more on the topic consistency between the content of textual documents and links while ignores the intrinsic topic of the textual documents. Due to the superior performance, we empirically set  X  =0 . 6,  X  =0 . 3,  X  =0 . 1 in the following experiments.
 Clustering Performance Comparison of Objects. We apply cluTM on the task of object clustering. The discovered topics can also be regarded as clusters. We can obtain the clustering results of other objects similarly.
 The proposed cluTM is compared with the following two state-of-the-art baselines: latent semantic analysis (LSA), and LSA-PTM [ 10 ]. Table 4 reports the clustering performance comparison on different methods.
 For the DBLP data, cluTM and LSA-PTM cluster all types of objects in different groups by considering both the textual documents and the link infor-mation. As one can see, both cluTM and LSA-PTM achieve better performance than LSA. This shows that integrating the heterogeneous network structures into topic modeling does help us better cluster the objects. Meanwhile, com-pared with LSA-PTM, cluTM is consistently better on all the three types of objects. This is mainly because LSA-PTM combines the textual content and heterogeneous network structures as two independent stages, while cluTM com-bines the textual documents and the heterogeneous network structures into a joint regularization framework such that they can mutually enhance each other. Topic modeling is an unsupervised approach to automatically discover the latent semantic of document collections. It has attracted a lot of attention in multiple types of text mining tasks, such as information retrieval [ 1 ], geographical topic discovery [ 13 ], topic level information diffusion modeling in social media [ 18 ]. tic latent semantic analysis (PLSA) [ 5 ] and Latent Dirichlet Allocation (LDA) [ 6 ] have been successfully applied or extended to many data analysis problems, including document clustering and classification [ 7 , 14 ], author-topic modeling [ 15 , 16 ]. However, most of these models merely consider the textual documents while ignore the network structures. Several proposed topic models, such as LaplacianPLSI [ 7 ], NetPLSA [ 8 ] and iTopicmodel [ 9 ] have combined topic mod-eling and network structures, but they only emphasize on the homogeneous net-works, such as document network and co-authorship network. Recent study [ 10 ] integrates the heterogeneous network structures into topic modeling, however, it combines the textual documents and the heterogeneous network structures as two independent stages. Our model combines the textual documents and het-erogeneous network structures into a joint regularization framework in which the textual content analysis and heterogeneous network analysis can mutually enhance each other. Experimental results prove the effectiveness of our model. and HITS. Many techniques have been proposed to analyze the heterogeneous networks. For example, [ 17 ] proposed a Co-HITS algorithm for bipartite graph analysis. Graph-based methods have been widely and successfully applied in data mining and information retrieval, such as text classification [ 14 ], and document re-ranking [ 19 ]. However, most of existing work treats different objects uniformly. Our work is different from them, as we focus on heterogeneous information net-works and propose a joint regularization framework, in which different types of objects are treated in a different way. In this paper, we proposed a unified topic model cluTM to effectively discover topics of documents and cluster objects of various types simultaneously on het-erogeneous text information networks. cluTM first conducted latent semantic analysis on the content of textual documents and factorized the link matrices of objects by SVD separately; then fused all the matrices into a single, compact feature representation by joint matrix factorization to find the common latent feature. By projecting all the objects in the heterogeneous text information net-works into the unified latent semantic space, topics of documents and clusters of other objects could be finally obtained by calculating their similarity. We evaluated cluTM on DBLP bibliographic dataset against several state-of-the-art baselines. Experimental results showed the effectiveness of cluTM.
