 An attacker can draw attention to items that don X  X  deserve that attention by manipulating recommender systems. We describe an influence-limiting algorithm that can turn exis t-ing recommender systems into manipulation-resistant sys-tems. Honest reporting is the optimal strategy for raters who wish to maximize their influence. If an attacker can create only a bounded number of shills, the attacker can mislead only a small amount. However, the system even-tually makes full use of information from honest, informa-tive raters. We describe both the influence limits and the information loss incurred due to those limits in terms of information-theoretic concepts of loss functions and entr opies. I.2.6 [ Computing Methodologies ]: Artificial Intelligence X  Learning Algorithms, Reliability Recommender systems, manipulation-resistance, shilling
Content posted on the Internet is not of uniform qual-ity, nor is it equally interesting to different audiences. Re c-ommender systems guide people to items they are likely to like, based on their own and other people X  X  subjective re-actions. We will refer to people X  X  opinions generically as ratings, whether users explicitly enter them in the form of ratings or tags, or whether the system infers them from im-plicit behavioral indicators such as purchases, read times , bookmarks, or links.

Authors and other parties often want to direct attention to particular items. Google, Yahoo!, and others channel thi s Copyright 2007 ACM 978-1-59593-730-8/07/0010 ... $ 5.00. into a multi-billion dollar advertising marketplace. But t o the extent that people rely on recommender systems of var-ious kinds to guide their attention, there are also natural i n-centives for promoters to manipulate the recommendations. An attacker may rate strategically rather than honestly and may introduce multiple entities, sometimes called sybils , to rate on behalf of the attacker.
 We offer a manipulation-resistance algorithm, called the Influence Limiter, that can be overlaid on existing recom-mender algorithms. Consider the predictions about whether a particular target person will like various items. Each rat er begins with a very low but non-zero reputation score. The current reputation limits the influence she 1 can have on the prediction for the next item. Eventually, the target person indicates whether he likes the item and the raters who con-tributed to predicting whether the target would like it gain or lose reputation. The more that a rating implies a change in the prediction for the target, the greater the potential change in the rater X  X  reputation score. A rater who simply goes along with the previous change will have no impact and thus get no change in her reputation.
 The Influence Limiter has several desirable properties. First, in order to maximize the expected reputation score of a single rater endowed with some information about the target X  X  likely response to the items, the optimal strategy is to induce predictions that accurately reveal that rater X  s information about the items. If the underlying recommen-dation algorithm is making optimal use of ratings, this im-plies that entering honest ratings is optimal. An important special case is that a rater who has not interacted with an item, and therefore has no information about the target X  X  likely response to it, can only lose reputation in expectati on by giving a rating.

Second, the actual reputation score of any rater is always positive and is bounded above by an information-theoretic measure of the actual improvement that rater has made in the predictions for the target. It is not possible to prevent all manipulations of the predictions for particular items X  a rater who provides good information on all other items but strategically provides bad information on one item is indis -tinguishable from a rater who simply has an unusual opinion on the item in question. Our algorithm does, however, en-sure that no rater can negatively impact the overall set of recommendations for a target by more than a tiny amount. Moreover, if the item being manipulated is unlikely to be of interest to the target, later raters may provide informatio n
We refer generically to raters as female and the target for predictions as male. that corrects the prediction on that item before the target is affected by the recommendation.

Finally, our algorithm limits the amount of damage that can be done with sybils. For example, if one rater provides bad information about an item in order to increase the repu-tation of another rater who later corrects that bad informa-tion, the expected sum of the reputations of the two raters does not increase. Thus, while it may be possible to trans-fer reputation among sybils, it is not possible to increase the total reputation of the raters that a person controls. We presume that the recommender system imposes some mini-mal cost (or inconvenience) on the creation of rater entitie s. Thus, there is some bound (say, 1,000) on the number of sybils one person can create without it being too costly and without being detected. The initial reputation of each rate r is set low enough that the total reputation of this bounded number of raters is still relatively small.

To further motivate our manipulation-resistance algorith m, consider some approaches to manipulating conventional rec -ommender systems. One threat is a cloning attack. For example, in a recommender system that asks each rater to report movie ratings on a 1-5 scale, the attacker simply re-ports the same ratings as some other rater, except for a single item to be manipulated. Most recommender systems do not take into account the order of ratings, and thus the attacker will have just as much effect on predictions for the last item as the rater who was copied. In a nearest-neighbor recommender algorithm, the attacker can even just clone the ratings of the target; the attacker will then be the near-est neighbor of the target. The cloning attack can be made more difficult by hiding the actual rating vectors of raters, but significant information about others X  ratings will leak out in the content of the recommendations, and sophisti-cated attackers will be able to create influential rating pro -files through approximate cloning. Our approach thwarts the cloning attack by adding reputation only when a rater improves the prediction made for some target. Unless the rater moves the recommendation from where it was before the rater provided its information, the rater can neither ga in nor lose reputation.

A second threat to conventional recommender systems comes from random profile flooding . An attacker creates a large number of sybils that provide random reports except on the item or items to be manipulated. By chance, some sybils may appear to have provided useful information in their random reports. These sybils are used to impact the prediction for an item being manipulated. Our algorithm thwarts this attack by making the probability of gaining sufficient credibility through random reports very low, so low that an attacker gains less influence in expectation from its sybils making random guesses than from simply transfer-ring the initial credibility of all its sybils to a main ident ity. Moreover, any sybil profile that does happen to gain a high credibility score has, by chance, moved the predictions for the target in a useful way, thus compensating for the lost utility from the subsequent manipulation.

The paper begins with an exploration of related work in section 1.1. Section 2 presents a model of the recommending process. Section 3 presents our algorithm. Section 4 pro-vides formal statements of its manipulation-resistance pr op-erties. Section 5 presents information-theoretic bounds o n the information loss due to influence limits. Section 6 dis-cusses limitations and possible extensions.
The possibility of sybil attacks on recommendation sys-tems has been noted by O X  X ahony et al. [22] and Lam and Riedl [15], who use the term  X  X hilling attack X . Through simulation, Lam and Riedl study versions of the cloning and random profile attacks on different recommender algorithms, and note that the effectiveness of the attack varies depend-ing on the algorithm used. However, they do not address the development of a provably attack-resistant algorithm.
Several authors have suggested using statistical metrics on ratings to distinguish  X  X ttack X  identities from  X  X egula r X  identities, and eliminate the former [7, 23, 18]. Mobasher et al. [20] survey this literature and classify attack strategies . This approach is likely to lead to an arms-race where shiller s employ increasingly sophisticated patterns of attack. To avoid this, our approach does not rely on identifying partic u-lar attack identities or specific attack strategies. O X  X ono van and Smyth [21] suggest using accuracy information from multiple targets to judge credibility; it would be interest -ing to see if our scheme can be extended in this way.
Dellarocas [8] provides an algorithm that bounds the dam-age that attackers can do when they collectively provide les s than half the ratings in the system and the honest ratings are normally distributed. Our approach succeeds much more generally, even in situations where only a tiny fraction of t he ratings are honest, at the expense of greater information lo ss during the startup phase when raters are not yet credible.
Herlocker et al. [13] study a modification of a nearest-neighbor recommender algorithm that does not count a rater as a near neighbor until it has rated sufficiently many items in common with a target rater. This is similar in spirit to our influence-limiting approach, but we provide a limiting process that is grounded in information theory and provably resistant to manipulation.

We use proper scoring rules to elicit honest ratings. These were pioneered in the context of forecasting objective even ts like weather patterns [4]; Miller et al. [19] noted that scor-ing rules could be adapted to the recommendation setting by treating the target X  X  rating as an objective outcome. Han-son [11] developed the market scoring rule as a mechanism for information markets. In this mechanism, a trader is re-warded with the score difference between her prediction and the previous prediction. This relative scoring rewards the first provider of information, and forms an essential compo-nent of our approach. We develop additional machinery to handle strategies involving sybils and bankruptcy.
Bhattarcharjee and Goel [3] suggest sharing the revenue of a ranking system with the raters. Using techniques similar to market scoring rules to determine the revenue shares, they argue that an attack on the system would be costly. In contrast, we do not require any real money transactions, and prove bounds on the damage that sybils can do.
We use the error score change as a natural measure of per-formance of the system and damage to the system. Rashid et al. [25] propose several other algorithm-independent mea-sures of rater influence; unlike error score change, their me a-sures do not consider the dynamic order of ratings. Sepa-rately, Rashid et al. [24] use entropy to analyze a different problem: choosing which items to ask new users to rate.
The literature on bounded-regret online learning deals with combining predictions from multiple forecasters and proving worst-case bounds on the error relative to the best predictor that could be chosen in hindsight (see Cesa-Bianc hi and Lugosi [5] and references therein). Many online learnin g algorithms can also be viewed as schemes for betting on a se-quence of events [26]. The influence-limiting algorithm can be interpreted as an online learning algorithm: our damage bound is a form of relative error bound. The online learning literature typically does not take the order of forecasts on a single item into account; our algorithm is tailored to this critical feature of the recommender setting.

Awerbuch et al. [2] study manipulation in a different model of the recommendation process: a user samples items and recommendations until he likes an item, at which point he recommends that single item to others. They present a sam-pling algorithm and prove bounds on the number of samples required, even in the presence of adversaries. Awerbuch and Kleinberg [1] describe an online learning scheme that is pro v-ably good for a generalization of this problem that incorpo-rates time-varying preferences and recommendations.
Sybilproofness has been studied in the context of repu-tation systems by Cheng and Friedman [6]. Their mecha-nisms begin with a trust graph: expressions of trust by rater s about other raters . Advogato [16] and Eigentrust [14] also attempt to address manipulation in a trust-graph model. Massa and Bhattacharjee [17] show how external trust re-lationships can be used to improve recommender systems. These techniques are not directly usable in our setting, be-cause raters may not know anything about the other raters in the system. In fact, our algorithm can be viewed as a way to securely derive a trust (or credibility) graph from ratin gs on inanimate objects.
We now present a formal model of the recommending pro-cess, illustrated in Figure 1. Let N denote the total number of identities in the system, and N denote the set of identi-ties. We use M to denote an upper bound on the number of items that are available for rating, so that each item can be assigned an index in { 1 , , M } .

There is a space L of possible labels, which describes the classification of items by the target for whom the system makes predictions. The prediction space X is a set of prob-ability distributions over the label space; a prediction q  X  X  is thus a distribution over labels. Prior to the target provi d-ing a label, a sequence of raters provide ratings r 1 , r that change the recommender system X  X  assessment of the target X  X  probability distribution over labels.

We describe the process assuming that all raters enter ratings into the same underlying recommender algorithm, which combines them with previous ratings to generate pre-dictions; this matches the standard mode of operation of current recommender systems. The formal analysis extends to more general settings: raters could directly report prob a-bilities, incorporating past information in any way they li ke. In practice, the recommender algorithm would also use each incoming rating to update predictions about other items and for more than just one target person. It is sufficient for our purposes to describe the predictions for a single target.
Existing recommenders typically predict a single number, which corresponds to the mean of the distribution over la-bels on a numeric scale (e.g., 3.8 on a 1-5 scale). To function in our scheme, such recommenders could be extended to re-port entire probability distributions (e.g., 30% chance of 5; 20% chance of 4; 50% chance of 3). Alternatively, interme-diate labels and point predictions can both be interpreted as a mixture of ratings over the extreme points ( e.g. , 3.8 is a 70% chance of 5 and 30% chance of 1). However, in the spe-cial case where there are only two possible labels, the mean (e.g., .7 on a 0-1 scale) completely determines the entire di s-tribution. In the rest of this paper, we shall assume that the label space is simply { HI, LO } , and that a prediction expresses the mean, the probability of a HI label.
In this section, we describe the error measures we use for a single prediction. Later, in section 5.1, we will see that these error measures lead naturally to information-theore tic measures of a rater X  X  expected contribution.
 We begin by assigning a value to good recommendations. We do this by postulating that the target has a loss function L ( l, q ), where q  X  [0 , 1] is the recommendation (predicted probability of rating HI ) that the target was given, and l  X  X  HI, LO } is the target X  X  ultimate label. One common choice is the log-loss function: Although typically logarithms to base 2 are used, so as to measure entropy in bits, we use natural logarithms (loga-rithms to base e ) throughout this paper. Note that the loss is 0 when the prediction is completely accurate (i.e., when there is no error in the prediction).

One drawback of the log-loss function is that it is un-bounded as q tends to 0 or 1. We need to avoid the possibil-ity of infinite (positive or negative) scores. One alternati ve is the Quadratic loss function/scoring rule: This corresponds to using the expected squared error (the variance) as a measure of uncertainty, which has a long his-tory in statistics. It also corresponds to the use of mean squared error as a measure of prediction accuracy in recom-mender systems that make predictions on a 0-1 scale. The quadratic loss function is clearly bounded by [0 , 1].
Figure 2 depicts an influence-limited recommender sys-tem. There are two key additional features. First, the prediction q j output by the recommender algorithm passes through an influence-limiting process to produce a modified prediction  X  q j . The influence-limiting process generates  X  q a weighted average of the prediction  X  q j  X  1 prior to incorpo-rating j  X  X  rating and the prediction q j using j  X  X  rating. The weighting of the two terms depends on the reputation R j that j has accumulated with respect to the target. When R j  X  1, all weight is on q j , i.e. , j has full credibility.
The second feature is a scoring function that assigns repu-tation to j based on whether or not the target actually likes the item. The amount of reputation that can be gained is again limited by the current reputation, and is selected so that the reputation will always be positive. It is also tuned so that honest raters reach full credibility after O (log n ) rat-ings, to limit the amount information that is discarded in th e influence-limiting step before a rater reaches full credibi lity. If a rater X  X  entire reputation was risked on each rating, the probability of getting to full credibility from a sequence o f ratings the target agrees with would be too small and the expected time to full credibility would be too high. The rate limiter  X  j threads the needle, allowing the rater to risk more as her reputation rises, but not risk too much, as we prove in section 5. 2
Figure 3 presents the algorithm more formally. Each rater begins with a tiny reputation e  X   X  , small enough so that even a large number of raters would have total reputation much less than 1.  X  j = min(1 , R j ) determines j  X  X  influence limit; if R j is above 1, j can move the prediction  X  q j  X  1 to q R j is below 1 she can only move it partway there. The score for rater j on the current item is a fraction  X  j times the market scoring rule, the difference between the loss functio n for the prediction  X  q j  X  1 and the loss function for q that she is scored on the prediction she would have liked to make, q j , even if the rate limiter only permitted her to move the prediction to  X  q j .

The log loss and quadratic loss functions are proper scor-ing rules [4, 9]. That means that its expected value is maxi-mized when q j matches the true probability that the target likes the item. This eliminates any incentive for rater j to lie about her rating, if she wants to maximize her expected reputation score, a property that we prove more formally in section 4. Note that honest reporting only maximizes the score in expectation; given the target X  X  true opinion of the item, the actual loss is minimized with a prediction of that outcome with certainty.
Our main non-manipulation result shows that, for any attack strategy involving up to n sybils, the net negative impact (damage) due to the attacker is bounded by a small amount. We first state two important assumptions we make about the attack strategy, and then prove the results on damage limits. We also show that a user seeking to maxi-mize her influence has a strict incentive to rate honestly. Assumptions : One important assumption we make is that there is a number n that bounds the maximum number of fake identities a single attacker can create. As stated in se c-tion 1, we intend n to be a fairly large number, perhaps in the thousands or even millions. It might appear that limit-ing a user to a million identities is qualitatively as difficul t as limiting her to one, but there is an important difference in practice. For example, many sites require new users to solve CAPTCHAs [27], which are puzzles that require a few seconds of human attention. While this is a mild incon-venience for most users who create only one account (or
By analogy, consider a sequence of coin flips where you double your money on heads and give back all but $1 on tails. The expected time to reach a bankroll of $ x would be quite long, even if heads occurred with probability 0 . 75. By contrast, if tails only required giving back half your money , the expected time would be 2 log x .
 Figure 2: An influence-limited recommender system
ComputeReputations(  X  ) Figure 3: Algorithm to compute reputations and limited predictions. Loss function L can be any proper scoring rule bounded by [0 , 1] . a few accounts), it can make creating millions of accounts prohibitively expensive. On the other hand, the only real-istic way to limit a user to only one account is to use and verify personally identifying information for each accoun t, which is a major privacy threat, and can be costly and time-consuming. Note that we do not assume that a constant fraction of the identities in the system are real users; the s et of identities can be dominated by an attacker X  X  sybils.
Secondly, we restrict our attention throughout this paper to myopic attack strategies. In particular, we do not con-sider reputation-building strategies in which the attacke r enters poor ratings that mislead later raters into amplify-ing her misinformation, and then later corrects it with an additional rating from another sybil. This is a nontrivial assumption: In many cases, the optimal prediction after honest ratings from agents 1 , 2 , .., j will be sensitive to each of their inputs and might amplify the change in predictions by earlier raters.
These non-myopic strategies, if they exist, would be equiv-alent to manipulative strategies in information markets, a nal-ogous to initiating a buying frenzy for a particular stock an d then selling to the very buyers who entered the market be-cause of your actions. While there are market settings in which non-myopic strategies are theoretically possible, s ev-eral studies have shown that such manipulation attempts are not very successful in practice (see, e.g. [12]). Non-myopic manipulative strategies, if they exist, are also likely to b e quite complex for an attacker to carry out, as they would require delicate calculations about other agents X  inferen ce and learning methods. In this paper, we restrict our at-tention to preventing the simpler (but still very powerful) myopic attacks. Research on non-myopic manipulation in information markets, and methods to guard against them, will have direct relevance to our mechanism.
 Impact of rater j : We now introduce a measure of rater j  X  X  impact on the recommendation quality. j  X  X  rating q j an item i has an immediate impact on the recommendations: The recommendation  X  q j  X  1 is changed to the recommenda-tion  X  q j . We define the myopic impact attributed to j for item i ,  X  i j , by: (Here, l i is the target X  X  rating on item i .) In other words,  X  j measures the reduction in prediction loss that resulted from the immediate changes in recommendation following j  X  X  rating.

Note that myopic impacts are additive. The total impact of a sequence of raters is the reduction in error from the initial default prediction to the final prediction  X  q J be expressed as the sum of the impacts attributed to the individual raters because the terms telescope:
X Finally, we define the net impact of rater j by the sum of her myopic impacts on all items i :  X  j = P i  X  i j .
We now prove our non-manipulation results in this con-text. First, we make a straightforward but important obser-vation about the reputations calculated by the algorithm: Observation: R j  X  0 This is true initially, and it is clearly maintained in step 2 .f of the algorithm because L () is bounded between 0 and 1.
Lemma 1. (Honest Reporting) For any entity j , if j be-lieves (at the time of rating) that the probability of the ite m being labelled HI is q , j optimizes her expected reputation gain on this item by putting in a rating such that q j = q .
Proof. From line 2.d, it can be seen that the change  X  R j in j  X  X  reputation is proportional to L ( l i ,  X  q j  X  1 Agent j cannot control the first term. As the scoring rule is proper, the second term is minimized in expectation by reporting q j = q .
 Remark: In itself, this is an incentive for honest rating only if users care about their reputations or influence. Inducing users to care about reputations is an orthogonal issue; this might be done through status rewards for raters with high reputation, or by adding a little noise to the recommenda-tions for users with low reputation.
 Remark: If the recommender algorithm that generates the predictions q j from the ratings r 1 , , r j is imperfect, rater j may benefit by compensating for the algorithm: reporting a value r j different from what she truly perceived in order to improve the prediction.

The following critical lemma relates the influence an agent garners to her measured impact on prediction error.
Lemma 2. (Reward-performance inequality) The reputa-tion increase  X  R j of player j on an item i is no more than the impact  X  i j of entity j on this item X  X  recommendation.
Proof. Let  X  j denote the influence limit calculated in step 2.d of the algorithm and l denote the eventual label on this item. Then, the reputation earned by player j for this item is The reduction in prediction error, on the other hand, is give n by (Note that  X  R j and  X  i j may be negative.) For both scoring rules we consider, log and quadratic, the loss functions are convex, and so we have:
L ( l, (1  X   X  j )  X  q j  X  1 +  X  j q j )  X  (1  X   X  j ) L ( l,  X  q Thus, substituting in the expression for  X  i j , we get
Corollary 3. At any point of time, the total myopic im-pact  X  j of all ratings by entity j satisfies  X  j  X  X  X  e  X   X 
Proof. From the algorithm, it is clear that  X  j and R j are never negative. Thus, the net decrease in reputation of entity j is at most e  X   X  . By lemma 2, the net increase in prediction error  X  j = P  X  i j due to j  X  X  ratings is no more than this.

We can now state our main manipulation resistance prop-erty:
Theorem 4. (Limited Damage) Consider any strategy for the attacker k ; the strategy involves the creation of up to n sybils K = { k 1 , k 2 , . . . , k n } . Then, the total myopic impact of all of k  X  X  sybils is bounded by
Proof. It follows from Corollary 3 by simply summing over all sybil identities.
 Remark: Setting  X  = log( cn ) for some parameter c , Theo-rem 4 shows that an attacker cannot cause a total reduction in the system performance of more than 1 /c . This damage bound does not require any assumptions about rationality, prior probabilities, or honest ratings.

Note, however, that we are relying on the assumption that strategies that involve misleading other raters are imposs ible to carry out: otherwise,  X  j would not completely capture the effect j has had on the recommendations. If one sybil can induce subsequent raters to amplify its effect on the pre-dicted probability, then a later sybil may be able to secure an inflated  X  score, by correcting the amplified misinforma-tion. The sum of all impacts still correctly measure the tota l reduction in prediction but the attacker would be stealing some impact score from the other raters who were misled by the initial sybil.
Manipulation-resistance in itself is not very hard to achie ve: simply ignoring all user ratings would result in nonmanip-ulable recommendations. It is therefore necessary to show that the recommendation scheme is still informative . In par-ticular, the influence-limited operation does not make full use of the information from a user with low reputation. In this section, we show that users will in expectation require O (  X  ) = O (log n ) ratings to build up their influence to an adequate level. We use this to bound the total information loss. We work primarily with the quadratic loss function, but the results should generalize to other loss functions wi th different constants.
In order to analyze the performance of the system in terms of reducing uncertainty, we need a formal model that cap-tures the uncertainty of rating, raters X  partial informati on signals, and raters X  ability to learn from (and copy) previo us ratings. We use a standard model of information partitions, which is described below. This model is used purely to ana-lyze information loss; the operation of the Influence Limite r algorithm does not depend on it.

There is a space  X  of possible states (item types). For example, each state in  X  might correspond to a particu-lar combination of a large number of movie attributes. For each  X   X   X , there is a corresponding value l (  X  )  X  X  HI, LO } that describes whether the target will like items in that state ( i.e. , with that combination of attributes). Further, we assume there is a common prior probability distribution p :  X   X  [0 , 1] that defines the relative likelihood of different states in the absence of additional information. We use p to denote P p ( l (  X  ) = HI ), the prior probability, before tak-ing into account any ratings, that a randomly chosen item will be labelled HI by the target. We assume that the state space is finite.

When a rater acquires a signal about an item, we model her as eliminating some of the possible states, but not chang -ing the relative likelihood of the remaining states. For ex-ample, she might be able to tell if the movie is violent or not, but not be able to discern some other characteristic, such as humor, that would distinguish among violent movies. More formally, all information acquisition is modelled in terms of identifying a component of a partition  X  of the state space  X . A rater j has a partition  X  j = {  X  1 j , ,  X  t j } . This is a partition, so  X  s j  X   X  k j =  X  for s 6 = k , and  X  s  X  s terpretation is that after acquiring a signal about the item , e.g., by watching the movie, j will know which component  X  j of her partition the true state belongs in, but will not be able to distinguish two states in the same partition.
Given the loss function L , a natural way to frame the objective of the recommender system is to attempt to min-imize the total error in the predictions, as measured by the sum of the losses. If no partial information from raters was available, the best prediction the system (or the target her -self) could make would be to say q = p 0 = P p ( l (  X  ) = HI ). This value of q can be shown to minimize the expected loss in the absence of any other information; the expected loss on each item is then given by the entropy H(l) : H ( l ) =  X  p 0 L ( HI, p 0 ) + (1  X  p 0 ) L ( LO, (1  X  p 0 )).

The log-loss function (or logarithmic scoring rule ) allows us to frame our results in terms of standard information-theoretic entropy. Other loss functions can be used as scor-ing rules in our algorithm, and would correspond to other measures of entropy, relative entropy, etc. (see the articl e by Grunwald and Dawid [10] for more information). The entropy measures thus derived are conceptually similar to the standard entropy.

Now, consider a sequence of raters 1 , 2 , , j ; each rating identifies a component of that rater X  X  partition. Then, the prediction made after j  X  X  rating will depend on j  X  X  rating as well as all previous ratings. The information available to t he recommender algorithm can be represented by a partition  X   X  j that is a refinement of  X  j , reflecting the fact that it can distinguish between items which j rated differently as well as possibly some items which were in the same component of  X  j , but different components of the partition of some  X  for k &lt; j . An ideal recommender algorithm then makes the best possible prediction q j given this information.
A sequence of ratings by raters 1 , 2 , . . . , j thus defines a sequence of partitions  X   X  1 , ,  X   X  j  X  1 ,  X   X  j . Generally,  X   X  refinement of  X   X  j  X  1 : no component of  X   X  j overlaps multiple components of  X   X  j  X  1 . 3
We can now define the innate informativeness of player j as the expected reduction in loss due to player j  X  X  par-ticipation as the jth rater in the sequence, before we learn what any of the realized ratings are. For any component s  X   X   X  j , define s j  X  1  X   X   X  j  X  1 as the component of  X   X  taining s . We can then define q j ( s ) = P ( l (  X  ) = HI |  X   X  s ) and q j  X  1 ( s ) = P ( l (  X  ) = HI |  X   X  s j  X  1 ). That is, q posterior probability the target will like the item if the st ate is in the partition s , and q j  X  1 ( s ) is the predicted probability after the rating sequence that would occur when the state is in s , but before taking into account j  X  X  rating. The informativeness ( i.e. , expected error reduction) tion is calculated as: L ( LO, q )] is the relative entropy , a central construct in in-formation theory. We can formally extend this definition to define I ( q || u ) for any functions q (  X  ) and u (  X  ) that are constant on components s , such as the influence-limited pre-dictions  X  q j .

This I ( X   X  j |  X   X  j  X  1 ) is our measure of the incremental value of j  X  X  information given the ordering 1 , 2 , , j . If j rated earlier in the sequence, she might look more informative, as her ratings might be less redundant with the information provided by previous raters.
In order to simplify the analysis, we assume a fixed rating order 1 , 2 , . . . , j, . . . on all items. We also assume that the ratings are reported without error and the underlying rec-ommender algorithm correctly processes them to determine q values. These assumptions allow us to present a concise
It is possible that  X   X  j  X  1 has two or more components in which the expected value of l is exactly the same, and hence, j  X  1 rates in exactly the same way. However, as q j  X  1 takes the same value on all these components of  X   X  j  X  1 , we can think of them as one component without altering the analysis. information-theoretic bound on the information loss due to influence limiting . Note that there may be other sources of information loss in practice ( e.g. , rating errors or rec-ommender imperfections). As before, we assume that the ratings on different items are independent.

Given the order in which users rate, we want to relate the influence j earns to the informativeness I ( X   X  j ||  X   X  j  X  X  private information. From step 2.f of the algorithm, the expected growth in reputation of a rater j with reputation R j  X  1, whose rating identifies a component of  X   X  j on which the prediction changes from u to q , is: q [ L ( HI, u )  X  L ( HI, q )] + (1  X  q )[ L ( LO, u )  X  L ( LO, q )] This is just the relative entropy, D ( q || u ).

The players X  reputations are initially very small. Initial ly, then, the amount that a reputation grows is scaled by R j We now show that a reputation builds exponentially in the number of ratings: The expected logarithm of the rater X  X  reputation grows at a rate that depends on the rater X  X  in-formativeness but not on the reputation.

Suppose a player with reputation R j &lt; 1 moves a predic-tion (on some item i ) from u to q . With probability q , this item will eventually be labelled HI , and her reputation will be adjusted to R  X  j = R j + R j ( L ( HI, u )  X  L ( HI, q )). Simi-larly, with probability 1  X  q , her reputation will be changed to R  X  j = R j + R j ( L ( LO, u )  X  L ( LO, q )). Then, the expected value of log R  X  j is given by:
E (log R  X  j ) = q log[ R j (1 + L ( HI, u )  X  L ( HI, q ))] where the growth factor GF ( q || u ) is defined as
GF ( q || u ) def = q log(1 + L ( HI, u )  X  L ( HI, q ))
The growth factor determines the expected rate of growth of j  X  X  reputation a single component of  X   X  j that j  X  rating identifies. We define an expected growth factor measure EGF ( X   X  j ||  X   X  j  X  1 ) by averaging over all possible components that j  X  X  rating could identify: As in the case of the informativeness measure, we can ex-tend this definition naturally to define EGF ( q || u ) for any functions q (  X  ) and u (  X  ) that are constant on components of  X   X  j and  X   X  j  X  1 respectively.

Lemma 5 shows that the growth factor on any single component of  X   X  j is close to the relative entropy on that component. Lemma 6 extends this to show that the ex-pected growth factor is close to rater j  X  X  informativeness limited prediction  X  q j  X  1 instead of the previous accurate pre-diction q j  X  1 . Proofs of these results and theorem 7 are omit-ted from the paper due to space restrictions; they can be found in an Appendix which has been archived online at http://hdl.handle.net/2027.42/55415 .

Lemma 5. For the quadratic scoring rule (MSE) loss, for all q, u  X  [0 , 1] , GF ( q || u )  X  D ( q || u ) 2 .
Lemma 6. Suppose  X   X  j and  X   X  j  X  1 are two partitions such that  X   X  j is a refinement of  X   X  j  X  1 . For each state  X  , let q E ( l (  X  ) |  X   X  j ) be the optimal prediction function given partition  X   X  . Let u (  X  ) be any function that is constant on each com-ponent of  X   X  j  X  1 . Then, EGF ( q j || u )  X  I ( X   X  j ||  X   X  quadratic loss model.

Theorem 7. Suppose rater j has rated m items, and sup-pose the informativeness of rater j is I ( q j || q j  X  1 for all m  X  (2  X  + 1) /h , rater i  X  X  expected reputation (with the quadratic scoring rule) is bounded below by The intuition behind this bound is simple: While j  X  X  rep-utation is below 1, log R j increases by at least h/ 2 in ex-pectation; thus, after 2  X /h ratings, log R j will be at least 0 ( i.e. , R j will be at least 1) in expectation. After this, j is not influence limited, and she will earn (an expected amount of) h in reputation in each subsequent round. Thus, in each of the last ( m  X  2  X /h ) rounds, she gains reputation h in expectation, for a total of about ( mh  X  2  X  ).
 An important consequence of Theorem 7 is that, by the Reward-Performance inequality (Lemma 2), the expected impact of player j will be greater than her reputation. In other words, we can expect to discard at most about 2  X  units of information from each rater in total.

Note that this information loss bound employs a very con-servative accounting scheme. Without the Influence Limiter , rater j would have contributed h bits on item i , but the in-fluence limits reduces the effect of j  X  X  rating, we account for this as lost bits. However, if there is a subsequent rater j +1 with sufficient reputation, the eventual prediction may be the same as without influence limits. Rater j + 1 might have been redundant in the absence of influence limits, but contributes valuable information to compensate for the los s of j  X  X  information when influence limits are present. Thus, the information loss bounds are based on a worst-case sce-nario.
We have presented an architecture and algorithm to limit the influence of individual raters that is provably manipu-lation-resistant (even when an attacker can create a large but bounded number of sybils), yet discards only a bounded amount of information from each rater. The smaller the size of the sybil attack that must be prevented, the less in-formation that will be discarded from honest raters as they build up their credibility. There are two key ingredients to our system. First, we use the same metric L to measure the prediction performance of the system and to calculate raters X  contributions (reputations). Second, the bootstr ap-ping scheme is carefully balanced: we tie the influence a rater can have to her accumulated reputation in a way that enables informative raters to ramp up exponentially fast, while limiting the total damage at any point to the amount of positive impact already created by each rater.

Our results suggest several questions for future work. We have presented the algorithm as if one item is under con-sideration at any point of time, or equivalently, as if we get feedback about the true label as soon as the recommenda-tion is used. In practice, several recommendations may be used before we get feedback about any of them; our algo-rithm will need to be modified slightly to manage  X  X redit X  in this scenario.

Another complication that we have not yet addressed is the target X  X  selection bias in labeling items. The target is more likely to view items with high recommendations, and as a result, items with low recommendation values might be less likely to be labelled; this could influence the raters  X  incentives to provide effort. Modified scoring schemes that correct for this bias would be very interesting.

Our manipulation resistance result assumes that an at-tacker cannot mislead future raters into amplifying the ef-fects of their ratings in a way that boosts the attackers X  own credibility. Another area of interest for future work is to analyze and prevent such non-myopic strategies.

Although we have proven theoretical bounds on informa-tion loss, any amount of discarded information during the initiation period is still of concern, particularly in sett ings in which each individual rater has low incremental informa-tiveness h or will rate only a few items. The algorithm will need to be carefully tuned to work well in such settings. A lower bound on the minimum information loss that any manipulation-resistant algorithm must incur would also be useful for comparison.
 This material is based upon work supported by the National Science Foundation under Grant No. IIS 0308006. [1] B. Awerbuch and R. D. Kleinberg. Competitive [2] B. Awerbuch, B. Patt-Shamir, D. Peleg, and M. R. [3] R. Bhattacharjee and A. Goel. Algorithms and [4] G. Brier. Verification of forecasts expressed in terms of [5] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, [6] A. Cheng and E. Friedman. Sybilproof reputation [7] P.-A. Chirita, W. Nejdl, and C. Zamfir. Preventing [8] C. Dellarocas. Building trust online: The design of [9] I. J. Good. Rational decisions. Journal of the Royal [10] P. Grunwald and A. Dawid. Game theory, maximum [11] R. Hanson. Combinatorial information market design. [12] R. Hanson, R. Oprea, and D. Porter. Information [13] J. Herlocker, J. Konstan, and J. Riedl. An empirical [14] S. D. Kamvar, M. T. Schlosser, and H. Garcia-Molina. [15] S. K. Lam and J. Riedl. Shilling recommender systems [16] R. Levien. Attack-Resistant Trust Metrics . PhD thesis, [17] P. Massa and B. Bhattacharjee. Using trust in [18] B. Mehta, T. Hoffman, and P. Fankhauser. Lies and [19] N. Miller, P. Resnick, and R. Zeckhauser. Eliciting [20] B. Mobasher, R. Burke, R. Bhaumik, and [21] J. O X  X onovan and B. Smyth. Trust no one: [22] M. O X  X ahony, N. Hurley, and G. Silvestre. Promoting [23] M. P. O X  X ahony, N. J. Hurley, and G. C. M. Silvestre. [24] A. M. Rashid, I. Albert, D. Cosley, S. K. Lam, S. M. [25] A. M. Rashid, G. Karypis, and J. Riedl. Influence in [26] G. Shafer and V. Vovk. Probability and Finance: It X  X  [27] L. von Ahn, M. Blum, N. Hopper, and J. Langford.
