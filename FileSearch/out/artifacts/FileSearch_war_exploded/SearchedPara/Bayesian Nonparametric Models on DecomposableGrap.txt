 Institut de Math  X  ematiques de Bordeaux 10]. We consider here the case where the data we are interested in are  X  X ocally X  dependent; these dependencies being represented by a known graph G where each data point/object is associated to a vertex. These local dependencies can correspond to any conceptual or real (e.g. space, time) metric. For example, in the context of clustering, we might want to propose a prior distribution on Similarly, in the context of latent feature models, we might be interested in a prior distribution on The  X  X tandard X  CRP and IBP correspond to the case where the graph G is complete; that is it is fully connected. In this paper, we generalize the CRP and IBP to decomposable graphs. The resulting generalized versions of the CRP and IBP enjoy attractive properties. Each clique of the graph follows marginally a CRP or an IBP process and explicit expressions for the joint prior distribution on the graph is available. It makes it easy to learn those models using straightforward generalizations of Markov chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) algorithms proposed to perform inference for the CRP and IBP [5, 10, 14].
 The rest of the paper is organized as follows. In Section 2, we review the popular Dirichlet multi-nomial allocation model and the Dirichlet Process (DP) partition distribution. We propose an exten-sion of these two models to decomposable graphical models. In Section 3 we discuss nonparametric latent feature models, reviewing briefly the construction in [5] and extending it to decomposable graphs. We demonstrate these models in Section 4 on two applications: an alternative to the hierar-chical DP model [12] and a time-varying matrix factorization problem. Assume we have n observations. When performing clustering, we associate to each of this observa- X 
A j = [ n ] and n ( X  n ) is the number of subsets for partition  X  n . We also denote by P n be the to be known. In the standard case where the graph G is complete, we first review briefly here two decomposable graphs; see [2, 8] for an introduction to decomposable graphs. Finally we briefly discuss the directed case. Note that the models proposed here are completely different from the hyper multinomial-Dirichlet in [2] and its recent DP extension [6]. 2.1 Dirichlet multinomial allocation model and DP partition distribution Assume for the time being that K is finite. When the graph is complete, a popular choice for the allocation variables is to consider a Dirichlet multinomial allocation model [11] Dirichlet multinomial prior distribution and then, using the straightforward equality Pr( X  n ) = K ! ( K  X  n ( X  P K where P K = {  X  n  X  X  n | n ( X  n )  X  K } , we obtain DP may be seen as a generalization of the Dirichlet multinomial model when the number of com-given by [11] and forms a new cluster with probability  X / ( n  X  1 +  X  ) . This property is the basis of the CRP. decomposable graphs. 2.2 Markov combination of Dirichlet multinomial and DP partition distributions Let G be a decomposable undirected graph, C = { C 1 , . . . , C p } a perfect ordering of the cliques they yield the same distribution (2) over the separators. Therefore, the unique Markov distribution over G with Dirichlet multinomial distribution over the cliques is defined by [8] where for each complete set B  X  G , we have Pr( z B ) given by (2). It follows that we have for any  X  variables define a partition distributed according to the Dirichlet-multinomial distribution.  X  n such that the distribution of  X  B over each complete set B of the graph is given by (4) with any partition distributed according to (4) still follows (4) [7].
 any ( i, j )  X  A  X  B , i  X  j  X  X  X  k  X  A  X  B such that k  X  i  X  j . As K  X  X  X  , the prior distribution over partitions (6) is given for each  X  n  X  X  G n by where n ( X  B ) is the number of clusters in the complete set B .
 Proof . From (6), we have Thus when K  X   X  , we obtain (7) if n ( X  n ) = We have n ( X  n )  X  n ( X  n ) = Example . Let the notation i  X  j (resp. i j ) indicates an edge (resp. no edge) between two sites. Let n = 3 and G be the decomposable graph defined by the relations 1  X  2 , 2  X  3 and 1 3 . and the separator is S 2 = { 2 } . The distribution is given by Pr( X  3 ) = Pr( X  C 1 ) Pr( X  C 2 ) Pr( X  and Pr( { 1 } , { 2 } , { 3 } ) =  X  2 (  X  + 1)  X  2 .  X  Let now define the full conditional distributions. Based on (7) the conditional assignment of an item k is proportional to the conditional over the cliques divided by the conditional over the separators. where j = 1 , . . . , n ( X   X  k ) with probability proportional to generally to species sampling models.
 Example (continuing). Given  X   X  2 = { A 1 = { 1 } , A 2 = { 3 }} , we have item 2 is assigned to A 1 with probability 1 .  X  Figure 1: Chinese wedding party. Consider a group of n guests attending a wedding party. Each everybody knows everybody. The belonging of each guest to the different cliques is represented by color patches on the figures, and the graphical representation of the relationship between the guests is represented by the graphical model (e). (a) Suppose that the guests are already seated such that two guests cannot be together at the same table is they are not part of the same clique, or if there mine  X ). (b) The guest number k leaves his table and either (c) joins a table where there are guests from the same clique as him, with probability proportional to the product of the number of guests from each clique over the product of the number of guests belonging to several cliques on that table or (d) he joins a new table with probability proportional to  X  . 2.3 Monte Carlo inference 2.3.1 MCMC algorithm Using the full conditionals, a single site Gibbs sampler can easily be designed to approximate the  X  j = 1 , . . . , n ( X   X  k ) , with probability proportional to and use p auxiliary variables x 1 , . . . , x p . The procedure is as follows. 2.3.2 Sequential Monte Carlo late a sequential updating rule for the corresponding perfect directed version D of G . Indeed, let ( a Then the vertex a k joins the set j with probability n j,pa ( a cluster with probability  X / One can then design a particle filter/SMC method in a similar fashion as [4]. Consider a set of to cluster j . The weight associated to e  X  ( i,j ) k is given by Assume we have n objects; each of these objects being associated to the vertex of a graph G . To z n,i = 1 if object n possesses feature i and z n,i = 0 otherwise. These vectors z t form a binary n  X  K matrix denoted Z 1: n . We denote by  X  1: n the associated equivalence class of left-ordered matrices and let E K be the set of left-ordered matrices with at most K features. models to undirected decomposable graphs. This can be used for example to define a time-varying IBP as illustrated in Section 4. 3.1 Beta-Bernoulli and IBP distributions The Beta-Bernoulli distribution over the allocation Z 1: n is where n j is the number of objects having feature j . It follows that where K h is the number of features possessing the history h (see [5] for details). The nonparametric model is obtained by taking the limit when K  X  X  X  where K + is the total number of features and H n = 3.2 Markov combination of Beta-Bernoulli and IBP distributions are consistent as they yield the same distribution (11) over the separators. Therefore, the unique Markov distribution over G with Beta-Bernoulli distribution over the cliques is defined by [8] where Pr( Z B ) given by (11) for each complete set B  X  G . The prior over  X  1: n is thus given, for  X  the set B and n B is the whole set of objects in set B . Taking the limit when K  X   X  , we obtain after a few calculations possessed by objects in B .
  X   X  k . For each feature j = 1 , . . . , K +  X  k , if  X   X  k  X  X  where b is the appropriate normalizing constant then the customer k tries Poisson new dishes. We can easily generalize this construction to a directed version D of G using arguments factorization. 4.1 Sharing clusters among relative groups: An alternative to HDP notation  X  i,j = U z model for sharing clusters among related groups. It is based on a hierarchy of DPs Under conjugacy assumptions, G 0 , G j and U can be integrated out and we can approximate the franchise to sample from the full conditional p ( z i,j | z  X  X  i,j } , y ) .
  X  decomposable graphical model whose separator is given by the elements of group 0. We can rewrite the model in a way quite similar to HDP conjugacy conditions, we can integrate out G 0 , G j and U and approximate the marginal posterior distribution over the partition using the Chinese wedding party process defined in Section 2. Note HDP, multiple layers can be added to the model. Figures 2 (a) and (b) resp. give the graphical DP alternative to HDP and 2-layer HDP. Figure 2: Hierarchical Graphs of dependency with (a) one layer and (b) two layers of hierarchy. If N = 0 , then G j  X  DP (  X , H ) for all j and this is equivalent to setting  X   X  X  X  in HDP. If N  X  X  X  then G j = G 0 for all j , G 0  X  DP (  X , H ) . This is equivalent to setting  X   X   X  in the HDP. One the usual DP) with the size of each group, whereas it scales doubly logarithmically in HDP. Contrary to HDP, there are at most N clusters shared between different groups. Our model is in that sense reminiscent of [9] where only a limited number of clusters can be shared. Note however that contrary to [9] we have a simple CRP-like process. The proposed methodology can be straightforwardly extended to the infinite HMM [12].
 Another issue is that to achieve high correlation, we need a large number of auxiliary variables. Nonetheless, the computational time used to sample from auxiliary variables is negligible compared model proposed offers a far richer framework and ensures that at each level of the tree, the marginal distribution of the partition is given by a DP partition model. 4.2 Time-varying matrix factorization Let X 1: n be an observed matrix of dimension n  X  D . We want to find a representation of this matrix in terms of two latent matrices Z 1: n of dimension n  X  K and Y of dimension K  X  D . Here Z 1: n is a binary matrix whereas Y is a matrix of latent features. By assuming that Y  X  X  and we obtain Z [5] or SMC [14].
 We consider here a different model where the object X t is assumed to arrive at time index t and we features. To achieve this, we consider the simple directed graphical model D of Fig. 3 where the site IBP to sample from p ( Z 1: n ) associated to this directed graph follows from (16) and proceeds as follows.
 At time t = 1  X  Sample K new 1  X  Poisson (  X  ) , set z 1 ,i = 1 for i = 1 , ..., K new 1 and set K + 1 = K new . At times t = 2 , . . . , r At times t = r + 1 , . . . , n can be sampled using the time-varying IBP described above, we can easily design an SMC method does not require inverting a matrix whose dimension grows linearly with the size of the data but only a matrix of dimension r  X  r . In order to illustrate the model and SMC algorithm, we create 200 6  X  6 images using a ground truth Y consisting of 4 different 6  X  6 latent images. The 200  X  4 binary  X  order of the model is set to r = 50 . The feature occurences Z 1: n and true features Y and their estimates are represented in Figure 4. Two spurious features are detected by the model (features 2 varying prior occurences of the features over time.
 Figure 4: (a) True features, (b) True features occurences, (c) MAP estimate Z MAP and (d) associated E [ Y | Z MAP ] The fixed-lag version of the time-varying DP of Caron et al. [1] is a special case of the proposed model when G is given by Fig. 3. The bivariate DP of Walker and Muliere [13] is also a special case when G has only two cliques. In this paper, we have assumed that the structure of the graph was known beforehand and we have shown that many flexible models arise from this framework. It would be interesting in the future to investigate the case where the graphical structure is unknown and must be estimated from the data.
 The authors thank the reviewers for their comments that helped to improve the writing of the paper. [1] F. Caron, M. Davy, and A. Doucet. Generalized Polya urn for time-varying Dirichlet process [2] A.P. Dawid and S.L. Lauritzen. Hyper Markov laws in the statistical analysis of decomposable [3] M.D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal [4] P. Fearnhead. Particle filters for mixture models with an unknown number of components. [5] T.L. Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process. [6] D. Heinz. Building hyper dirichlet processes for graphical models. Electonic Journal of Statis-[7] J.F.C. Kingman. Random partitions in population genetics. Proceedings of the Royal Society [8] S.L. Lauritzen. Graphical Models . Oxford University Press, 1996. [9] P. M  X  uller, F. Quintana, and G. Rosner. A method for combining inference across related non-[10] R.M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of [11] J. Pitman. Exchangeable and partially exchangeable random partitions. Probability theory and [12] Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. Hierarchical Dirichlet processes. Journal of [13] S. Walker and P. Muliere. A bivariate Dirichlet process. Statistics and Probability Letters , [14] F. Wood and T.L. Griffiths. Particle filtering for nonparametric Bayesian matrix factorization.
