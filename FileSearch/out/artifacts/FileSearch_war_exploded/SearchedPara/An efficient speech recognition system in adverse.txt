 1. Introduction
The main objective of Automatic Speech Recognition (ASR) is to map between the acoustic micro-structure of the speech signal and the phonetic macro-structure. To achieve this, it is required to suitably describe the phonetic macro-structure that is usually hidden behind general knowledge of phonetic science. One of the interesting challenges that ASR is facing is to build language-independent systems. Thus, it is necessary to unify acoustic processing and to adapt the architecture of the ASR system to cover the broadest range of languages and situations like real-life environments.
ASR systems are usually based on the stochastic approach using the Hidden Markov Model (HMM) that provides a mathematically rigorous approach to the development of statis-tical speech models ( Rabiner, 1989 ; Jelinek, 1997 ). The HMM-based methods are suitable for acoustic modelling but suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption and the complexity to estimate those parameters.
A promising technique for speech recognition is the hybrid-based approach, which combines the function capabilities of
Artificial Neural Networks (ANNs) ( Lippman, 1989 ; Haykin, 1999 ), with the modelling power of HMM. ANNs have been integrated into hybrid HMM/ANN models to compute emission posterior state probabilities. The best known approach is the one proposed by Bourlard ( Bourlard and Morgan,1994,1996 ; Renals et al.,1998 ).
Rigoll ( Rigoll and Neukirchen, 1996 ) has also proposed another hybrid approach where the ANN has been used as a vector quantizer for discrete HMM. If for continuous speech recognition these methods were essential, for the isolated word recognition (e.g., digits which represent shorter units) other directions need to be explored. For this purpose, neural networks, which have great discrimination ability, can be particularly adapted to spoken words recognition.

Speech recognition modelling by ANNs does not require a prior knowledge of the speech process. Neural networks, such as the Multilayer Feed-forward Networks (MLPs) or the Recurrent Neural
Networks (RNN) can be trained to associate unknown input data to learned words. As recognizers, ANNs have been shown to yield better performance than HMM on short isolated speech units (Bourlard and Morgan, 1996).

The neural network recognizer based on a static network, such as MLP, and a dynamic network, as RNN ( Alotaibi, 2005 ) or Time
Delay Neural Network (TDNN) ( Waibel et al., 1989 ), use parametric representation of the activation function. This assumption can be relaxed by introducing a nonparametric technique. Nonparametric techniques in pattern recognition can be used when no functional form for the density function is assumed. The density of estimates is driven by the data without making any assumption on the form of the distribution ( Cacoulos, 1966). An adaptation scheme based on nonparametric regression using GRNN has been proposed by Specht (1991, 1996) . GRNN has been used in a variety of applications, mapping problems ( Rutkowski, 2004 ). Some comparative studies have been also published to demonstrate the modelling capability of the GRNN with respect to the other types of neural networks. Works have been reported in the speech field ( Chung and Tognieri, 1996 ;
Hoya and Constantinides, 1998 ), on the detection of the human emotioninspeech( Bhatti et al., 2004 ), and on the speech music classification ( Bolat and Kucuk, 2004 ). Moreover, GRNNs have been not extensively used in speech recognition, particularly for Arabic language.

The major works related to speech recognition in Arabic language deal with the morphological structure ( Datta et al., 2005 ; Kirschhoff et al., 2003 ) or the phonetic features in order to identify the particular Arabic phonemes (pharyngeal, geminate and emphatic consonants) ( Selouani and Caelen, 1999 ; Debyeche et al., 2006 ) and discusses their further implication in a larger vocabulary speech system. This opened a very interesting way for researchers, but the applications in term of implementation of recognition system dedicated to spoken isolated words or continuous speech are not exten-sively conducted and only few examples have be discussed.
Shoaib et al. (2004) has developed a derivative scheme, named the Concurrent GRNN, implemented for accurate Arabic phonemes identification in order to automate the intensity-and formants-based feature extraction. The validation tests expressed in terms of recognition rate obtained with clean speech were up to 93.37%. Alotaibi (2005) has developed an isolated word speech recognizer using the RNN. The word accuracy is over 94.5% in term of recognition rate in speaker-independent mode and 99.5% in speaker-dependent mode. The spoken data set used is limited to 17 speakers for both training and testing process. Saeed and Nammous (2005) have been proposed a heuristic method of Arabic digit recognition, using the Probabilistic Neural Network (PNN). The recognition rate, obtained in speaker-dependent mode with 20 people, is over 98%. A hybrid method has been applied to Arabic digits recognition by Lazli and Sellami (2003) . The Fuzzy C-Means method has been added to the traditional ANN/HMM speech recognizer using RASTA-PLP features vectors. The Word Error
Rate(WER)isover14.4%.Withthesameapproach,amethod using data fusion gave a WER of 0.8%. However, this method was tested only on one personal corpus and the authors indicated that the obtained improvement needed the use of three neural networks working in parallel. Thus, the recogni-tion time is more longer compared to the traditional ANN/
HMM method. Another alternative hybrid method has been proposed by Bourouba et al. (2006) where the Support Vector
Machine (SVM) and the K nearest neighbor (KNN) were substituted to the ANN in the traditional hybrid system. The training phase were made by only 10 persons by gender and the best results, expressed in term of recognition rate, did not exceed 92.72% for KNN/HMM and 90.62% for SVM/HMM.

Speech recognition systems using these techniques have limited performance, despite the fact they operate in quiet environment. Thus, the use of a neural network recognizer, with a nonparametric activation function, constitutes a promising solution to increase the performances of speech recognition systems, particularly in the case of Arabic language. In a previous work, we have demonstrated the advantages of the GRNN speech recognizer over the MLP and the HMM in quiet environment (Amro uche and Rouvaen, 2003, 2004 ).

Compared to other languages, Arabic digits are much more elongated. They include two to four syllables, while French, English and Mandarin digits are single or double syllables. Arabic digits can be considered as representative elements of language, because more than half of the phonemes of the Arabic language are included in the 10 digits. The fricative and plosive consonants are more dominants and characterized by the presence of noise in the high-frequency band spectrum. In fact, these consonants are easily corrupted by noise sources making. Therefore, speech recognition systems usually fall to identify them in adverse conditions.
On the other hand, one of the most critical problems in speech recognition is the inter speakers variability, which made the digits pronounced in different manners, around the pattern roots according to the regional and ethnic origin. The presence of the same word in various forms makes the recognition task extremely difficult, particularly for the longest digits.

To overcome these problems and improve the robustness of the speech recognition in adverse conditions for Arabic language, we introduced the nonparametric methods in neural network-based approach. Indeed, nonparametric methods do not require prior knowledge of speech production process, so the neural network outputs can be calculated by estimating the probability densities of the acoustic input vectors. In this work, the Nadaraya X  X atson regression, based on Parzen estimators extended to multidimen-sional case, is used in the GRNN-based recognizer. Thus, an isolated speech recognition system based on GRNN recognizer was developed and compared with the discrete HMM, the MLP and the RNN recognizers-based systems in different noisy environments.
This paper is organised as follows: in Section 2, a brief description is presented. In Section 3, the basic concept of the nonparametric regression and the neural network implementation are recalled. Section 4 describes our proposed adaptation scheme based on
GRNN. The experimental results obtained in quiet and adverse conditions are presented in Section 5, and discussed in Section 6.
Finally, Section 7 draws conclusion on the approach presented here. 2. Arabic language
Arabic is currently one of the most widely spoken languages in the world with an estimated number of 350 millions speakers covering a large geographical area. This Semitic language is characterized by the presence of specific consonants like phar-yngeal, glottal and emphatic consonants. Furthermore, it presents some phonetics and morpho-syntactic particularities. The mor-pho-syntactic structure built, around pattern roots (CVCVCV,
CVCCVC, etc.) and generative rules, makes the Arabic language more appropriate to natural language automatic processing.
As mentioned above, the Arabic digits include more than half of the phonemes of this language. They are usually polysyllabic words whose mostly contain fricative and plosive sounds.
Fig. 1 a X  X  shows spectrograms of /sitta/ or digit 6 (bi-syllabic), / y ala: y a/ or digit 3 (tri-syllabic) and / y ama:nija/ or digits 8 (quadric-syllabic), respectively. These spectrograms show that the fricative consonants are characterized by the presence of noise in the high-frequency band spectrum, while the plosive consonants are characterized by the presence of  X  X  X urst X  X , a short time friction noise. In adverse conditions, these consonants are easily corrupted by noise sources, making them difficult to be identified.
Also, the Arabic digits are pronounced in different manners, around their pattern roots according to the regional and ethnic origins of speakers. Table 1 reports the different forms of the ten Arabic digits pronounced by Algerian speakers. 3. Nonparametric regression 3.1. Theoretical foundations
Let f ( x , y ) be the joint continuous probability density function be a particular measured value of the vector random variable x of conditional expectation of y on X expressed as Ey 9 X  X 
In nonparametric density estimation, no fixed parametrically defined shape for the estimated density is assumed. Then, the probability density function must be estimated empirically from a sample of observations (data points) of x and y . The general form of the estimator is given by the following ( Cacoulos, 1966 ): f  X  x  X  X  where the x i are independent, identically distributed random variables with absolutely continuous distribution function.
One useful shape of the weighting function f is the Kernel density function (Gaussian). Parzen has shown that these estimators are consistent ( Cacoulos, 1966 ). They asymptotically converge to the underlying distribution at the sample point when it is smooth and continuous. Parzen X  X  results have also been extended to the multivariate distribution case ( Cacoulos, 1966 ;
Specht, 1991 ). Based upon sample values X i and Y i of the random has been showed by Specht (1991, 1996) is given by ^ f  X  X ; ^ Y  X  X  1 where p is the dimension of the vector variable x , n the number of observations (pattern sample), s the smoothing factor (spread) of the estimating kernel factor, and Y i the desired scalar output given the observed input X i .
 Let us define the scalar function D i 2 as
D  X  X  X X i  X  T  X  X X i  X  X  4  X 
Combining Eqs. (3) and (4) and interchanging the order of integration and summation, yields the desired conditional mean, expressed as ^ Y  X  X  X  X 
The resulting regression, (5), known also as Nadaraya X  X atson kernel regression estimator, is directly applicable to problems involving numerical data. The estimate ^ Y  X  X  X  can be considered as a weighted average of all the observed values, Y i , where each observed value is weighted exponentially according to its Euclidean distance from X .

Freq. (KHz)
Freq. (KHz)
Freq. (KHz) 3.2. Neural implementation
General regression neural network implementation was firstly proposed by Specht (1991, 1996) . Let w ij be the target output corresponding to the input training vector x i and the j th output.
Eq. (5) can be expressed as y  X  with h i  X  exp D 2 i 2 s 2
According to Eqs. (6) and (7), the topology of a GRNN described in Fig. 2 consists of
An input layer (input cells), which is fully connected to the pattern layer.

A pattern layer which contains one neuron for each pattern. It computes the pattern functions h i  X  s ; C i  X  expressed in (7) using the centres C i .

The summation layer which has two units N and D . The first unit, which has input weights equal to X i , computes the numerator N by summing the exponential terms multiplied by the Y i associated with X i . The second unit has input weights equal to 1. Thus, the denominator D is the summation of the exponential terms only.

Finally, the output unit divides N by D to provide the prediction result.
 The choice of the smoothing factor is very important. When s is small, only few samples play a role. If s is large, even distant neighbours can affect the estimate at X . 4. GRNN-based system for speech recognition
The general scheme of the proposed speech recognizer is and the recognition step based on the learning and recognition tasks. 4.1. Feature extraction
The speech signal was firstly digitized and end-pointed. In order to flatten the signal, the digitized speech signal, pre-emphasised by a first-order digital filter, is given as ^ s  X  n  X  X  s  X  n  X  m : s  X  n 1  X  X  8  X  where the pre-emphasis parameter m is equal to 0.96, and where emphasis, respectively.

Pre-emphasis ensures that all the formants of the speech signal have similar amplitudes in the frequency domain, so that they get equal importance in subsequent processing stages.
 Then, the signal was fragmented into frames by using a
Hamming window (256 points with half covering). In order to reduce the among of the information in the speech signal, the frame features were extracted using the Mel Frequency Cepstrum
Coefficients ( MFCC ), the most popular features for ASR. For each frame, a set of Cepstrum Coefficients were then calculated as follows: The Mel Frequency Cepstrum Coefficients (MFCC).
 The first-and second-order derivatives of MFCC.

The MFCC take into account the human ear sensitivity. The bands are linear up to 1 kHz and logarithmic at higher frequencies. The relation between the Mel frequency scale and the linear frequency scale is given by Mel  X  f  X  X  2595log 10 1  X  f  X  Hz  X  700  X  9  X 
To calculate the filter bank coefficients, the magnitude coefficients of spectrum were first accumulated after windowing.
Thus, triangular filters were spread over the whole frequency range from zero up to the Nyquist frequency. In this work, 24 filter banks were chosen. The discrete cosine transform or DCT (a version of the Fourier transform using only cosine basis functions) converts the set of log energies to asset of cepstral coefficients:
MFCC  X  i  X  X  where ( m j ) represent the log filter bank amplitudes, N the number of filter bank channels  X  X et to 24 X ) and P the number of cepstral coefficients (set to 12).
The first ( D )-and second ( DD )-order derivatives of MFCC are approximated and given by D MFCC l  X  i  X  X  G DD MFCC l  X  i  X  X  X  D MFCC l  X  1  X  i  X  D MFCC l 1  X  i  X   X  12  X  where k and l are frames indexes, i the MFCC component and G a gain factor selected as 0.375.

Hence, the j th frame of the word W i is represented by an acoustic vector S ij S  X f MFCC ; D MFCC ; D  X  D MFCC  X  ; log  X  Energy  X g  X  13  X  4.2. Recognition step 4.2.1. Learning task
The features vectors represent the inputs of the GRNN used as recognizer, as shown earlier in Fig. 2 . The input vector corre-sponding to the first word in the learning set was used to compute pattern was memorized. For the following words in the learning phase, which is a sequentially process, only the new patterns were memorized. 4.2.2. Recognition task
When presented with features of unknown word, the distance between the unknown word and each pattern memorized in the hidden layer was computed and passed through a kernel function. The output of kernel function is an estimate of how likely the unknown pattern of a word belongs to the pattern distribution stepped in the hidden layer. 5. Experimental results 5.1. Speech database
The used spoken words are the Arabic digits which are polysyllabic, excepted the 0 ( /sifr/ ) . They are more lengthened than the English or French digits and they have two to four syllables.

The database used for training and testing the recognition system was a locally Arabic speech database collected from 150 Algerian natives aged from 18 to 50. This database, named ARADIGITS, was recorded in a large and very quiet auditory room, at 22.050 kHz and down-sampled at 16 kHz.

In the training phase, a total of 1800 utterances pronounced by 90 speakers of both sex (equally distributed) were used. In the testing phase, 1000 utterances pronounced by 50 others speakers of both gender (25 males and 25 females) were used. The experiments were conducted in speaker-independent mode; therefore, the data in the testing set did not intersect with those in the training set. 5.2. Experimental framework
A set of experiments was conducted to test the accuracy by measuring the ASR performance. All recognition results are given in term of WER defined as WER  X  N R N 100 %  X  14  X  where N is the total number of words in the test set and R the total numbers of words correctly recognized in the test set.
The clean speech material was used for training the speech recognizer system. In order to compare the performance of the
GRNN speech recognizer, we interchanged the GRNN classifier by the MLP, the Elman Recurrent Neural Network (RNN) and the HMM.
The HMM baseline system achieved was the five states left-to-right model. The recognition experiments were conducted in clean speech of the recognizer. The optimization of the smoothing factor was critical to the GRNN performance and was usually found through iterative adjustment and cross-validation procedure. In previous works, we have shown that the suitable interval for speech recognition is 14 o s o 20, and that s =15 is a convenient value (Amrouche and Rouvaen, 2003, 2004 ). The results obtained in quiet environment are reported on Table 2 for both genders.
Using the GRNN speech recognizer, the digits 1, 4, 5 and 9 were correctly recognized ( WER =0%), the digits 2, 6 and 7 were recognized with 1% WER while the digits 0, 3 and 8 presented a
WER of 4%, 4% and 3%, respectively. The global WER is 1.4% for both genders, 0.8% for female speakers and 2% for male speakers.
For the MLP, the spoken digits 5, 6, 7 and 9 were correctly recognized and the digits 1 and 4 were recognized with a WER of 1%. For the remainders, WER ranged between 4% and 8%. The global performance is 2.7% WER,1.6% for female speakers and 3.8% for male speakers.

For the RNN, only the spoken digits 6 and 7 were recognized without error. The digit 1 was recognized with 1% WER, the digits 2, 4, 5 and 8 were recognized with a 3% WER, whereas 0 had 7%
WER and 3 had 9% WER. The global performance is 2.9% WER for the RNN, 1.4% for female speakers and 4.4% for male speakers.
For the HMM, only the digit 6 was recognized with 2% WER, whereas for the other digits the error rate varied from 5% to 44%.
For the HMM-based speech recognizer, the global performance is 16.6% WER,15.4% for female speakers and 17.8% for male speakers. 5.3. Sensitivity to noise
For real-word applications, a speech recognition system must operate in situations where it is not possible to control the acoustic environments. This may result in a serious mismatch between the training and test conditions. Differences in the acoustic environments may result from additive noise (back-ground noise: car noise, babble, etc.), convolutive distortion (such as transmission channel distortion, room reverberation, microphone distortion, etc.) or any combination of them. Both classes of noise have been found to degrade seriously the speech recognition performance. The problem of minimizing the degra-dation in performance is the problem of robustness. The approaches that may be used to enhance the robustness of an
ASR system are usually classified into two types: the acoustic model adaptation techniques ( Kermorvant and Morris, 1999 ) and the noise reduction techniques ( Yapanel and Hansen, 2003 ; Cui and Alwan, 2005 ). The problem discussed here is the sensitivity to noise of the speech recognizers, or in other words, the intrinsic robustness to noise (inherently robustness).

The observed signal corrupted by additive noise can be represented as x  X  t  X  X  s  X  t  X  X  n  X  t  X  X  15  X  where x(t) , s(t) and n(t) denote observed noisy speech, clean speech signal and additive signal noise, respectively.
The additive noise is classified into two types, i.e., the stationary noise that has time constant characteristics of spectral features and signal energy, and the non-stationary noise that has time variable characteristics of spectral features and signal energy. An example of the degradation of the original signal by a non-stationary noise is given in Fig. 4 b. Fig. 4 a shows the also spectrogram in Fig. 1 a) and Fig. 4 b shows the temporal form of the same spoken word corrupted by a noise signal recorded in a factory (car production hall) at SNR=5 dB. The transitions from silence to speech did not only vary but also some speech parts might be masked by noise. It is noted that the stopped plosive /t/ and the fricative /s/ are completely masked by the noise signal.
To evaluate the robustness of the speech recognizer in various kinds of noise, stationary as well as non-stationary noises, issued from the NOISEX-92 database, were added to the testing database in a range of SNRs from 0 to 20 dB, step size 5 dB. We were particularly interested by four kinds of noises: two stationary and two non-stationary. For the stationary noise we considered the military vehicle noise acquired by recording noise signal from the leopard vehicle moving at 70 km/h, and the cockpit noise acquired from the fighter jet (buccaneer moving at a speed of 450 knots).
For the non-stationary noise, we considered the speech babble acquired from 100 people speaking in canteen, and the factory floor noise is recorded in a car production hall. The spectrogram of each noise signal is presented in Figs. 5 X 8 .

We will illustrate the difficulty faced by ASR systems in noisy spectrograms of these two signals recorded in quiet environment.
Spectrograms illustrating the degradation caused to the original signal by the fighter jet noise at 5 dB SNR are also displayed and that show pronounced spectral similarities of the two words, making their distinction difficult during the recognition phase.
The comparative performance of the four types of recognizers observation is that the speech recognizers alone are not able to adapt themselves with environmental noisy conditions if the SNR is less than 15 dB, because the rise of WER curves increase strongly after this threshold level. The spectral features of the background noise are a significant element for the speech recognition, much more than the SNR level. For example, the degradation caused by the fighter jet noise, characterized by a broadband spectrum, at 20 dB SNR exceeds the degradations caused by the military vehicle noise, located in the low-frequency narrowband, at 0 dB SNR level. Figs. 11 X 14 show that the less harmful is the vehicle noise, which have a narrowband spectrum.
The most significant degradations are caused by the fighter jet noise whose spectral broadband can cover the whole -1.5 -0.5 0.5 1.5
Amplitude -0.5 0.5 1.5 Amplitude 
Freq. (KHz) 0 2 4 6 8
Freq. (KHz) 0 2 4 6 8 speech spectrum, masking completely the speech, even on high level SNR.

As shown in Fig. 11 , the transition from a quiet condition to a noisy environment, with a speech babble background at SNR=15 dB, is followed by a loss of accuracy of 13.6% for the GRNN, 15.3% for the MLP, 20.1% for the RNN and more than 28% for the HMM. At 10 dB SNR, the accuracy drops by 22.6%, 23.3%, and 34.1% and more than 49%, respectively. Fig. 11 shows that the GRNN is the least sensitive to the environmental condition degradation; the MLP exhibits a performance close to the GRNN and better than of the RNN. Finally, the HMM has the most degraded performance with this type of noise.

For the noise factory, also a non-stationary noise, the GRNN still gives the best results as shown in Fig. 12 . The loss of effectiveness when one passes from quiet environment into a factory environment producing a noise at 15 dB SNR, is 11.6% for the GRNN, 14.3% for the MLP, 14.1% for the RNN and 41% for the
HMM. This loss of effectiveness reaches, respectively, 19.6%, 25.3%, 25.1% and 48% for the four speech recognizers, if one passes from quiet environment into a factory environment producing noise at 10 dB SNR. It is noticed that the ANN-based recognizer working in adverse conditions caused by factory noise, in particular the
GRNN, exhibits a performance at SNR=15 dB similar to the HMM-based recognizer working in quiet environment.

Fig. 13 shows that the fighter jet noise, which is a stationary type, is the most harmful of the four noise conditions studied in this paper, because of its spectral broadband. Indeed, the loss of effectiveness at SNR=15 dB reaches to 35.6% for the GRNN, 41.3% for the MLP, 48.1% for the RNN and 66% for the HMM. The degradation is very significant, even if the GRNN present the best results.

Finally, the military vehicle noise, which is a stationary noise with a low-frequency limited bandwidth, did not affect seriously the speech signal, particularly the fricative and pharyngealized phonemes, which are the most present consonants in Arabic digits (Fig. 14 ). 6. Discussion
A discussion should be conducted in two directions: the effectiveness of the speech recognizers in quiet environment and the intrinsic robustness (inherently robustness) of these speech recognizers in adverse conditions.

Freq. (KHz)
Freq. (KHz)
Freq. (KHz)
Freq. (KHz) Word Error Rate (%)
The experimental results show that the global performance of the GRNN-based speech recognizer is 1.4% WER. This error rate is lower than that obtained by the MLP-, the RNN-and the HMM-based speech recognizers. In fact, the improvement is 1.3%, 1.5% and 15.2%, respectively, compared with the MLP (WER 2.7%), the RNN (WER 2.9%) and the HMM (WER 16.6%).

It appears clearly that the GRNN-based speech recognizer has better performance than the other neural networks-based recognizers. This improvement of the effectiveness results from the use of a statistical nonparametric function in the GRNN.
Furthermore, this results exceed those obtained with the HMM baseline system. The discrete HMM is unable to adapt itself to the variability of the words, in particular the longest words like the spoken digits 2, 3, 7 and 8, which undergo a syntactic modification according to the geographical origin of the speakers.
Indeed, the word / y ama:nija/ may be pronounced in at least five different ways and the word /i y na:n/ in more than six ways, through omissions, additions, or substitutions phonemes. The plosive consonant /t/ is often substituted by the / y / consonant.
This involves a real alignment problem between states model and the frames of the same word. Thus, the alignment frame/state or the state probability estimate and the transitions between states are made very complex. This was confirmed, for the HMMs by the high values of the error rates on these digits: respectively, 44%, 24%, 24% and 27% (see Table 2 ).

The improvement in the recognition performance, obtained with the GRNN recognizer, is due to the fact that this network utilizes a nonparametric regression to estimate the probability densities, in order to decide if the acoustic vector elements of an unknown word belong to one of the classes of words learned during the training phase. Thus, the GRNN memorizes more learning examples, because the Kernel Parzen estimated functions are selective, depending on the smoothing parameter (spread factor).

In an other hand, the intrinsic robustness of the speech recognizer was studied in four different noisy conditions, using additive stationary and non-stationary noises. The spectral features, particularly the spectral broadband of the background noise, are a significant element of the performance degradation.
The spectrogram in Fig. 5 shows that the noise produced by the military vehicle (tank leopard) is characterized by a narrow band localized in the lower spectrum band (0 X 1 kHz). It then can corrupt the low band of the speech signal, without affecting higher bands. Therefore only the intelligibility of speech is affected, unless the SNR is low.

The spectrogram in Fig. 6 shows the noise collected in the by a power spectral density uniformly distributed over the whole spectrum of speech. It can thus affect the entire spectral band of the speech signal, covering all the phonemes of the language.
Fig. 7 shows the spectrogram of the babble noise collected in a canteen which 100 persons of both sexes. The sound intensity level during recording was 88 dB (A). It is a characteristic of a speech signal with a very high power spectral density up to 1 kHz.
Indeed, the F1 formants of vowels, and voiced consonants, are concerned by this spectral band. The power spectrum remains high between 1 and 2.8 kHz area where particular formants of vowels such as F2 and F3 formants exist, then it decreases up to 4 kHz. Beyond this value, the spectrum loses intensity since fricatives, consonants involved in the upper speech spectrum, are generally of low energy. This kind of noise can corrupt all the phonemes, particularly for low SNR values.

The noise collected in factory (car production hall), given by the spectrogram in Fig. 8 , is characterized by a very intense power spectrum up to 1 kHz thus decreasing towards high frequencies.
For instance, the weakest performance was encountered with the fighter jet noise, which has a large spectral broadband. We can also deduce that the GRNN-based speech recognizer is the least sensitive to the background noise present in adverse conditions.
For the factory noise, also a non-stationary noise, the GRNN gave even better results, as shown in Fig. 12 . The loss of effectiveness for a SNR of 15 dB, was around 11.6% for the GRNN, 14.3% for the MLP, 14.1% for the RNN and 41% for the HMM. For a 10 dB SNR, this efficiency loss reached 19.6%, 25.3%, 25.1% and 48%, respectively. Note that, for neural networks recognizers working in a noisy environment caused by factory noise at a level 20 40 60 80 100 Word Error Rate (%) 20 40 60 80 100 Word Error Rate (%) 20 40 60 80 100 Word Error Rate (%) of 15 dB SNR, particularly the GRNN, the results were comparable to HMM recognizer operating in quiet environment.
 Fig. 13 shows that the jet fighter noise is the most harmful. Indeed, efficiency loss caused by this noise at a level of 15 dB SNR reached 35.6% for the GRNN, 41.3% for the MLP, 48.1% for the RNN and more than 66% for the HMM. This kind of noise seriously degrades the recognition performance, even if, once again the GRNN recognizer gave the best results.

The first observation is that the used classifiers are not able to adapt themselves to noise conditions if the SNR falls below 15 dB, because the error rate increases after this threshold. Another interesting observation is that the spectral structure of the additive noise significantly affects the recognizer performances. For example, the degradation caused by the jet fighter noise at an SNR of 20 dB is greater than the degradation caused by the vehicle explanation is that the aircraft noise, characterized by a wide band spectrum covers the entire speech spectrum, while the vehicle noise, located in lower frequencies, affects only plosives and voiced sounds.

The choice of the spread value is also a very important aspect, because a low spread value makes the neural network unable to generalize. On the other hand, a high spread value makes the learning set be integrally stored, requiring a huge memory size. An appropriate spread value enables the activation function, a Kernel Parzen function, to efficiently select the output value, compared to MLP and RNN, which have sigmoid activation function. 7. Conclusion
In this work we proposed a GRNN adaptation scheme for spoken word recognition. The efficiency of our approach was demonstrated through a comparative study with the MLP, the RNN and the discrete HMM speech recognizers. The use of a nonparametric density estimator with an appropriate smoothing factor improved the generalization capability of the neural network. The improvement in performance results from the use of a statistical nonparametric function in the GRNN. In fact, nonparametric statistics based on Parzen estimators, achieve a better identification of the recognition problem.

Experimental results obtained with large corpora confirmed that the proposed model present several advantageous character-fast learning capability, (iii) the flexibility network size and (iv) the ability to adapt to speaker variability.

Furthermore, experiments simulating the operation in noisy environment show that the classifiers are more sensitive to noise with broadband spectrum, particularly for SNR levels below 15 dB. In all studied examples studied, the recognition system based on GRNN suffered less damage compared to other neural-based systems reported in this paper. The HMM-based recognition system on the recognition results, we can conclude that the proposed recognition system, built around the GRNN, exhibits a better intrinsic robustness t o noise than systems built on static neural networks like MLP, dynamic networks like RNN or the HMMs.
Thus, the GRNN speech recognizer gave the best results in free noisy or quiet environment. The inherently robustness of the GRNN adapted scheme could improve significantly the recogni-tion accuracy in adverse environments, including stationary and non-stationary noises. The ANN-based speech recognizers con-firmed their discrimination capacity and remain thus, a serious alternative to HMM for isolated word recognition. GRNN is a successful alternative vs. other neural networks and discrete HMM. It is therefore suitable to be applied in ASR systems. References
