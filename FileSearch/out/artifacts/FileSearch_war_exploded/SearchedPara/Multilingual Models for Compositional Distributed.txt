 Distributed representations of words provide the basis for many state-of-the-art approaches to var-ious problems in natural language processing to-day. Such word embeddings are naturally richer representations than those of symbolic or discrete models, and have been shown to be able to capture both syntactic and semantic information. Success-ful applications of such models include language modelling (Bengio et al., 2003), paraphrase detec-tion (Erk and Pad  X  o, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013).

Within a monolingual context, the distributional hypothesis (Firth, 1957) forms the basis of most approaches for learning word representations. In this work, we extend this hypothesis to multilin-gual data and joint-space embeddings. We present a novel unsupervised technique for learning se-mantic representations that leverages parallel cor-pora and employs semantic transfer through com-positional representations. Unlike most methods for learning word representations, which are re-stricted to a single language, our approach learns to represent meaning across languages in a shared multilingual semantic space.

We present experiments on two corpora. First, we show that for cross-lingual document clas-sification on the Reuters RCV1/RCV2 corpora (Lewis et al., 2004), we outperform the prior state of the art (Klementiev et al., 2012). Second, we also present classification results on a mas-sively multilingual corpus which we derive from the TED corpus (Cettolo et al., 2012). The re-sults on this task, in comparison with a number of strong baselines, further demonstrate the relevance of our approach and the success of our method in learning multilingual semantic representations over a wide range of languages. Distributed representation learning describes the task of learning continuous representations for dis-crete objects. Here, we focus on learning seman-tic representations and investigate how the use of multilingual data can improve learning such rep-resentations at the word and higher level. We present a model that learns to represent each word in a lexicon by a continuous vector in R d . Such distributed representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and mor-phological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia ).

We describe a multilingual objective function that uses a noise-contrastive update between se-mantic representations of different languages to learn these word embeddings. As part of this, we use a compositional vector model (CVM, hence-forth) to compute semantic representations of sen-tences and documents. A CVM learns seman-tic representations of larger syntactic units given the semantic representations of their constituents (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Her-mann and Blunsom, 2013, inter alia ).

A key difference between our approach and those listed above is that we only require sentence-aligned parallel data in our otherwise unsuper-vised learning function. This removes a number of constraints that normally come with CVM mod-els, such as the need for syntactic parse trees, word alignment or annotated data as a training signal. At the same time, by using multiple CVMs to transfer information between languages, we en-able our models to capture a broader semantic con-text than would otherwise be possible.

The idea of extracting semantics from multilin-gual data stems from prior work in the field of semantic grounding. Language acquisition in hu-mans is widely seen as grounded in sensory-motor experience (Bloom, 2001; Roy, 2003). Based on this idea, there have been some attempts at using multi-modal data for learning better vec-tor representations of words (e.g. Srivastava and Salakhutdinov (2012)). Such methods, however, are not easily scalable across languages or to large amounts of data for which no secondary or tertiary representation might exist.

Parallel data in multiple languages provides an alternative to such secondary representations, as parallel texts share their semantics, and thus one language can be used to ground the other. Some work has exploited this idea for transferring lin-guistic knowledge into low-resource languages or to learn distributed representations at the word level (Klementiev et al., 2012; Zou et al., 2013; Lauly et al., 2013, inter alia ). So far almost all of this work has been focused on learning multi-lingual representations at the word level. As dis-tributed representations of larger expressions have been shown to be highly useful for a number of tasks, it seems to be a natural next step to attempt to induce these, too, cross-lingually. Most prior work on learning compositional se-mantic representations employs parse trees on their training data to structure their composition functions (Socher et al., 2012; Hermann and Blun-som, 2013, inter alia ). Further, these approaches typically depend on specific semantic signals such as sentiment-or topic-labels for their objective functions. While these methods have been shown to work in some cases, the need for parse trees and annotated data limits such approaches to resource-fortunate languages. Our novel method for learn-ing compositional vectors removes these require-ments, and as such can more easily be applied to low-resource languages.

Specifically, we attempt to learn semantics from multilingual data. The idea is that, given enough parallel data, a shared representation of two paral-lel sentences would be forced to capture the com-mon elements between these two sentences. What parallel sentences share, of course, are their se-mantics. Naturally, different languages express meaning in different ways. We utilise this di-versity to abstract further from mono-lingual sur-face realisations to deeper semantic representa-tions. We exploit this semantic similarity across languages by defining a bilingual (and trivially multilingual) energy as follows.

Assume two functions f : X  X  R d and g : Y  X  R d , which map sentences from lan-guages x and y onto distributed semantic representations in R d . Given a parallel corpus C , we then define the energy of the model given two sentences ( a,b )  X  C as: We want to minimize E bi for all semantically equivalent sentences in the corpus. In order to prevent the model from degenerating, we fur-ther introduce a noise-constrastive large-margin update which ensures that the representations of non-aligned sentences observe a certain margin from each other. For every pair of parallel sen-tences ( a,b ) we sample a number of additional sentence pairs (  X  ,n )  X  C , where n  X  X ith high probability X  X s not semantically equivalent to a . We use these noise samples as follows:
E hl ( a,b,n ) = [ m + E bi ( a,b )  X  E bi ( a,n )] where [ x ] + = max ( x, 0) denotes the standard hinge loss and m is the margin. This results in the following objective function: J (  X  ) = where  X  is the set of all model variables. 3.1 Two Composition Models The objective function in Equation 2 could be cou-pled with any two given vector composition func-tions f,g from the literature. As we aim to apply our approach to a wide range of languages, we fo-cus on composition functions that do not require any syntactic information. We evaluate the follow-ing two composition functions.

The first model, A DD , represents a sentence by the sum of its word vectors. This is a distributed bag-of-words approach as sentence ordering is not taken into account by the model.

Second, the B I model is designed to capture bi-gram information, using a non-linearity over bi-gram pairs in its composition function: The use of a non-linearity enables the model to learn interesting interactions between words in a document, which the bag-of-words approach of A
DD is not capable of learning. We use the hy-perbolic tangent as activation function. 3.2 Document-level Semantics For a number of tasks, such as topic modelling, representations of objects beyond the sentence level are required. While most approaches to com-positional distributed semantics end at the word level, our model extends to document-level learn-ing quite naturally, by recursively applying the composition and objective function (Equation 2) to compose sentences into documents. This is achieved by first computing semantic representa-tions for each sentence in a document. Next, these representations are used as inputs in a higher-level CVM, computing a semantic representation of a document (Figure 2).

This recursive approach integrates document-level representations into the learning process. We can thus use corpora of parallel documents X  regardless of whether they are sentence aligned or not X  X o propagate a semantic signal back to the individual words. If sentence alignment is avail-able, of course, the document-signal can simply be combined with the sentence-signal, as we did with the experiments described in  X  5.3.

This concept of learning compositional repre-sentations for documents contrasts with prior work (Socher et al., 2011; Klementiev et al., 2012, inter alia ) who rely on summing or averaging sentence-vectors if representations beyond the sentence-level are required for a particular task.

We evaluate the models presented in this paper both with and without the document-level signal. We refer to the individual models used as A DD and B
I if used without, and as D OC /A DD and D OC /B I is used with the additional document composition function and error signal. We use two corpora for learning semantic rep-resentations and performing the experiments de-scribed in this paper. used during initial development and testing of our approach, as well as to learn the representa-tions used for the Cross-Lingual Document Clas-sification task described in  X  5.2. We considered the English-German and English-French language pairs from this corpus. From each pair the final 100,000 sentences were reserved for development.
Second, we developed a massively multilin-2013 (Cettolo et al., 2012). This corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED confer-ence. While the corpus is aimed at machine trans-lation tasks, we use the keywords associated with each talk to build a subsidiary corpus for multilin-The development sections provided with the IWSLT 2013 corpus were again reserved for de-velopment. We removed approximately 10 per-cent of the training data in each language to cre-ate a test corpus (all talks with id  X  1 , 400 ). The new training corpus consists of a total of 12,078 parallel documents distributed across 12 language English sentences (the number of unique English sentences is smaller as many documents are trans-lated into multiple languages and thus appear re-peatedly in the corpus). Each document (talk) con-tains one or several keywords. We used the 15 most frequent keywords for the topic classification experiments described in section  X  5.3.

Both corpora were pre-processed using the set ercasing the data. Further, all empty sentences and their translations were removed from the corpus. We report results on two experiments. First, we replicate the cross-lingual document classification task of Klementiev et al. (2012), learning dis-tributed representations on the Europarl corpus and evaluating on documents from the Reuters RCV1/RCV2 corpora. Subsequently, we design a multi-label classification task using the TED cor-pus, both for training and evaluating. The use of a wider range of languages in the second experi-ments allows us to better evaluate our models X  ca-pabilities in learning a shared multilingual seman-tic representation. We also investigate the learned embeddings from a qualitative perspective in  X  5.4. 5.1 Learning All model weights were randomly initialised us-ing a Gaussian distribution (  X  =0 , X  2 =0 . 1 ). We used the available development data to set our model parameters. For each positive sample we used a number of noise samples ( k  X  X  1 , 10 , 50 } ), randomly drawn from the corpus at each training epoch. All our embeddings have dimensionality d =128 , with the margin set to m = d . 6 Further, we use L2 regularization with  X  =1 and step-size in { 0 . 01 , 0 . 05 } . We use 100 iterations for the RCV task, 500 for the TED single and 5 for the joint corpora. We use the adaptive gradient method, AdaGrad (Duchi et al., 2011), for updating the weights of our models, in a mini-batch setting ( b  X  { 10 , 50 } ). All settings, our model implementation and scripts to replicate our experiments are avail-able at http://www.karlmoritz.com/ . 5.2 RCV1/RCV2 Document Classification We evaluate our models on the cross-lingual doc-ument classification (CLDC, henceforth) task first described in Klementiev et al. (2012). This task in-volves learning language independent embeddings which are then used for document classification across the English-German language pair. For this, CLDC employs a particular kind of supervision, namely using supervised training data in one lan-guage and evaluating without further supervision in another. Thus, CLDC can be used to establish whether our learned representations are semanti-cally useful across multiple languages.
 We follow the experimental setup described in Klementiev et al. (2012), with the exception that we learn our embeddings using solely the Europarl data and use the Reuters corpora only during for classifier training and testing. Each document in the classification task is represented by the aver-age of the d -dimensional representations of all its sentences. We train the multiclass classifier using an averaged perceptron (Collins, 2002) with the same settings as in Klementiev et al. (2012).
We present results from four models. The A DD model is trained on 500k sentence pairs of the English-German parallel section of the Europarl corpus. The A DD + model uses an additional 500k parallel sentences from the English-French cor-pus, resulting in one million English sentences, each paired up with either a German or a French sentence, with B I and B I + trained accordingly. The motivation behind A DD + and B I + is to inves-tigate whether we can learn better embeddings by introducing additional data from other languages. A similar idea exists in machine translation where English is frequently used to pivot between other languages (Cohn and Lapata, 2007).

The actual CLDC experiments are performed by training on English and testing on German doc-uments and vice versa. Following prior work, we use varying sizes between 100 and 10,000 docu-ments when training the multiclass classifier. The results of this task across training sizes are in Fig-ure 3. Table 1 shows the results for training on 1,000 documents compared with the results pub-lished in Klementiev et al. (2012). Our models outperform the prior state of the art, with the B I models performing slightly better than the A DD models. As the relative results indicate, the addi-tion of a second language improves model perfor-mance. It it interesting to note that results improve in both directions of the task, even though no addi-tional German data was used for the  X + X  models. 5.3 TED Corpus Experiments Here we describe our experiments on the TED cor-pus, which enables us to scale up to multilingual learning. Consisting of a large number of rela-tively short and parallel documents, this corpus al-lows us to evaluate the performance of the D OC model described in  X  3.2.

We use the training data of the corpus to learn distributed representations across 12 languages. Training is performed in two settings. In the sin-gle mode, vectors are learnt from a single lan-guage pair (en-X), while in the joint mode vector-learning is performed on all parallel sub-corpora simultaneously. This setting causes words from all languages to be embedded in a single semantic space.

First, we evaluate the effect of the document-level error signal (D OC , described in  X  3.2), as well as whether our multilingual learning method can extend to a larger variety of languages. We train D
OC models, using both A DD and B I as CVM (D OC /A DD , D OC /B I ), both in the single and joint mode. For comparison, we also train A DD and D
OC models without the document-level error sig-nal. The resulting document-level representations are used to train classifiers (system and settings as in  X  5.2) for each language, which are then evalu-ated in the paired language. In the English case we train twelve individual classifiers, each using the training data of a single language pair only. As described in  X  4, we use 15 keywords for the classification task. Due to space limitations, we report cumulative results in the form of F1-scores throughout this paper.
 MT System We develop a machine translation baseline as follows. We train a machine translation tool on the parallel training data, using the devel-opment data of each language pair to optimize the translation system. We use the cdec decoder (Dyer et al., 2010) with default settings for this purpose. With this system we translate the test data, and then use a Na  X   X ve Bayes classifier 7 for the actual experiments. To exemplify, this means the de  X  ar result is produced by training a translation system from Arabic to German. The Arabic test set is translated into German. A classifier is then trained result, underline best result amongst the vector-based systems. languages that do not share any parallel data. We train a D 2013). Classification Accuracy (%) 100 200 500 1000 5000 10 k 50 60 70 80 90 on the German training data and evaluated on the translated Arabic. While we developed this system as a baseline, it must be noted that the classifier of this system has access to significantly more infor-mation (all words in the document) as opposed to our models (one embedding per document), and we do not expect to necessarily beat this system. The results of this experiment are in Table 2. When comparing the results between the A DD model and the models trained using the document-level error signal, the benefit of this additional sig-nal becomes clear. The joint training mode leads to a relative improvement when training on En-glish data and evaluating in a second language. This suggests that the joint mode improves the quality of the English embeddings more than it affects the L2-embeddings. More surprising, per-haps, is the relative performance between the A DD and B I composition functions, especially when compared to the results in  X  5.2, where the B I mod-els relatively consistently performed better. We suspect that the better performance of the additive composition function on this task is related to the smaller amount of training data available which could cause sparsity issues for the bigram model.
As expected, the MT system slightly outper-forms our models on most language pairs. How-ever, the overall performance of the models is comparable to that of the MT system. Consider-ing the relative amount of information available during the classifier training phase, this indicates that our learned representations are semantically useful, capturing almost the same amount of infor-mation as available to the Na  X   X ve Bayes classifier.
We next investigate linguistic transfer across languages. We re-use the embeddings learned with the D OC /A DD joint model from the previ-ous experiment for this purpose, and train clas-sifiers on all non-English languages using those embeddings. Subsequently, we evaluate their per-formance in classifying documents in the remain-ing languages. Results for this task are in Table 3. While the results across language-pairs might not be very insightful, the overall good performance compared with the results in Table 2 implies that we learnt semantically meaningful vectors and in fact a joint embedding space across thirteen lan-guages.

In a third evaluation (Table 4), we apply the em-beddings learnt with out models to a monolingual classification task, enabling us to compare with prior work on distributed representation learning. In this experiment a classifier is trained in one lan-guage and then evaluated in the same. We again use a Na  X   X ve Bayes classifier on the raw data to es-tablish a reasonable upper bound.

We compare our embeddings with the SENNA embeddings, which achieve state of the art per-formance on a number of tasks (Collobert et al., 2011). Additionally, we use the Polyglot embed-dings of Al-Rfou X  et al. (2013), who published word embeddings across 100 languages, including all languages considered in this paper. We repre-sent each document by the mean of its word vec-tors and then apply the same classifier training and testing regime as with our models. Even though both of these sets of embeddings were trained on much larger datasets than ours, our models outper-form these baselines on all languages X  X ven out-performing the Na  X   X ve Bayes system on on several languages. While this may partly be attributed to the fact that our vectors were learned on in-domain data, this is still a very positive outcome. 5.4 Linguistic Analysis While the classification experiments focused on establishing the semantic content of the sentence level representations, we also want to briefly in-vestigate the induced word embeddings. We use the B I + model trained on the Europarl corpus for this purpose. Figure 4 shows the t-SNE projec-tions for a number of English, French and German words. Even though the model did not use any par-allel French-German data during training, it still managed to learn semantic word-word similarity across these two languages.

Going one step further, Figure 5 shows t-SNE projections for a number of short phrases in these three languages. We use the English the presi-dent and gender-specific expressions Mr President and Madam President as well as gender-specific equivalents in French and German. The projec-tion demonstrates a number of interesting results: First, the model correctly clusters the words into three groups, corresponding to the three English forms and their associated translations. Second, a separation between genders can be observed, with male forms on the bottom half of the chart and fe-male forms on the top, with the neutral the presi-dent in the vertical middle. Finally, if we assume a horizontal line going through the president , this line could be interpreted as a  X  X ender divide X , with male and female versions of one expression mir-roring each other on that line. In the case of the president and its translations, this effect becomes even clearer, with the neutral English expression being projected close to the mid-point between each other language X  X  gender-specific versions.
These results further support our hypothesis that the bilingual contrastive error function can learn semantically plausible embeddings and further-more, that it can abstract away from mono-lingual surface realisations into a shared semantic space across languages. Distributed Representations Distributed repre-sentations can be learned through a number of ap-proaches. In their simplest form, distributional in-formation from large corpora can be used to learn embeddings, where the words appearing within a certain window of the target word are used to com-pute that word X  X  embedding. This is related to topic-modelling techniques such as LSA (Dumais et al., 1988), LSI, and LDA (Blei et al., 2003), but these methods use a document-level context, and tend to capture the topics a word is used in rather than its more immediate syntactic context.

Neural language models are another popular ap-proach for inducing distributed word representa-tions (Bengio et al., 2003). They have received a lot of attention in recent years (Collobert and We-ston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010, inter alia ) and have achieved state of the art performance in language modelling. Collobert et al. (2011) further popularised using neural net-work architectures for learning word embeddings from large amounts of largely unlabelled data by showing the embeddings can then be used to im-prove standard supervised tasks.
Unsupervised word representations can easily be plugged into a variety of NLP related tasks. Tasks, where the use of distributed representations has resulted in improvements include topic mod-elling (Blei et al., 2003) or named entity recogni-tion (Turian et al., 2010; Collobert et al., 2011). Compositional Vector Models For a number of important problems, semantic representations of individual words do not suffice, but instead a se-mantic representation of a larger structure X  X .g. a phrase or a sentence X  X s required. Self-evidently, sparsity prevents the learning of such representa-tions using the same collocational methods as ap-plied to the word level. Most literature instead fo-cuses on learning composition functions that rep-resent the semantics of a larger structure as a func-tion of the representations of its parts.

Very simple composition functions have been shown to suffice for tasks such as judging bi-gram semantic similarity (Mitchell and Lapata, 2008). More complex composition functions us-ing matrix-vector composition, convolutional neu-ral networks or tensor composition have proved useful in tasks such as sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), rela-tional similarity (Turney, 2012) or dialogue analy-sis (Kalchbrenner and Blunsom, 2013).
 Multilingual Representation Learning Most research on distributed representation induction has focused on single languages. English, with its large number of annotated resources, has enjoyed most attention. However, there exists a corpus of prior work on learning multilingual embeddings or on using parallel data to transfer linguistic in-formation across languages. One has to differen-tiate between approaches such as Al-Rfou X  et al. (2013), that learn embeddings across a large va-riety of languages and models such as ours, that learn joint embeddings, that is a projection into a shared semantic space across multiple languages. Related to our work, Yih et al. (2011) proposed S2Nets to learn joint embeddings of tf-idf vectors for comparable documents. Their architecture op-timises the cosine similarity of documents, using relative semantic similarity scores during learn-ing. More recently, Lauly et al. (2013) proposed a bag-of-words autoencoder model, where the bag-of-words representation in one language is used to train the embeddings in another. By placing their vocabulary in a binary branching tree, the prob-abilistic setup of this model is similar to that of Mnih and Hinton (2009). Similarly, Sarath Chan-dar et al. (2013) train a cross-lingual encoder, where an autoencoder is used to recreate words in two languages in parallel. This is effectively the linguistic extension of Ngiam et al. (2011), who used a similar method for audio and video data. Hermann and Blunsom (2014) propose a large-margin learner for multilingual word representa-tions, similar to the basic additive model proposed here, which, like the approaches above, relies on a bag-of-words model for sentence representations.
Klementiev et al. (2012), our baseline in  X  5.2, use a form of multi-agent learning on word-aligned parallel data to transfer embeddings from one language to another. Earlier work, Haghighi et al. (2008), proposed a method for inducing bilingual lexica using monolingual feature repre-sentations and a small initial lexicon to bootstrap with. This approach has recently been extended by Mikolov et al. (2013a), Mikolov et al. (2013b), who developed a method for learning transforma-tion matrices to convert semantic vectors of one language into those of another. Is was demon-strated that this approach can be applied to im-prove tasks related to machine translation. Their CBOW model is also worth noting for its sim-ilarities to the A DD composition function used here. Using a slightly different approach, Zou et al. (2013), also learned bilingual embeddings for machine translation. To summarize, we have presented a novel method for learning multilingual word embeddings using parallel data in conjunction with a multilingual ob-jective function for compositional vector models. This approach extends the distributional hypoth-esis to multilingual joint-space representations. Coupled with very simple composition functions, vectors learned with this method outperform the state of the art on the task of cross-lingual docu-ment classification. Further experiments and anal-ysis support our hypothesis that bilingual signals are a useful tool for learning distributed represen-tations by enabling models to abstract away from mono-lingual surface realisations into a deeper se-mantic space.
 This work was supported by a Xerox Foundation Award and EPSRC grant number EP/K036580/1.
