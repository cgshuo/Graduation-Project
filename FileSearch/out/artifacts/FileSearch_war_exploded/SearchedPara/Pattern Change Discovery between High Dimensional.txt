 This paper investigates the general problem of pattern change discovery between high-dimensional data sets. Current meth-ods either mainly focus on magnitude change detection of low-dimensional data sets or are under supervised frame-works. In this paper, the notion of the principal angles be-tween the subspaces is introduced to measure the subspace difference between two high-dimensional data sets. Princi-pal angles bear a property to isolate subspace change from the magnitude change. To address the challenge of directly computing the principal angles, we elect to use matrix fac-torization to serve as a statistical framework and develop the principle of the dominant subspace mapping to transfer the principal angle based detection to a matrix factoriza-tion problem. We show how matrix factorization can be naturally embedded into the likelihood ratio test based on the linear models. The proposed method is of an unsuper-vised nature and addresses the statistical significance of the pattern changes between high-dimensional data sets. We have showcased the different applications of this solution in several specific real-world applications to demonstrate the power and effectiveness of this method.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Measurements Unsupervised Learning, Pattern Change Detection, Princi-pal Angles, Principle of Dominant Subspace Mapping, Ma-trix Factorization Figure 1: The Euclidean metric fails to differentiate the length difference from the direction difference
High dimensional data exist everywhere in our life and in all the sectors of our society in every modality of the data we live with today, including text, imagery, audio, video, and graphics. Pattern change discovery from high dimen-sional data sets is a general problem that arises in almost every application in the real-world; examples of such ap-plications include concept drift mining in text data, event discovery in surveillance video data, event discovery in news data, hot topic discovery in the literature, image pattern change detection, as well as genome sequence change detec-tion in bioinformatics, to just name a few.

In each of the above applications, we formulate the prob-lem as follows. Given two typically high-dimensional data sets, we intend to determine whether there is a significant pattern change between the two data sets. In different ap-plications, the physical interpretation of the two data sets may be different. For example, in detecting any topic change between two text documents, the two high-dimensional data sets may be the two text documents; in detecting any con-cept drift among a text stream, any pair of two neighboring snapshots of the text collections in the timeline may be con-sidered as the two high-dimensional data sets; in detecting any pattern change between two images or two collections of images, the two high dimensional data sets may be the two corresponding images or the two collections of the im-ages; in detecting any event occurred in a surveillance video camera, the two high-dimensional data sets may be any pair of two neighboring video frames or groups of video frames in the video stream; in detecting any hot topics in a news data stream, the two high-dimensional data sets may be two neighboring sample windows of the news text data within the stream.

One may wonder what makes high-dimensional data dif-ferent when it comes to change detection. For almost all the magnitude change detection methods, an invisible pit-fall arises with the increase of data X  X  dimensionality. The tricky conflict between Euclidean distance and dimensional-ity is illustrated in Fig 1. Here we use Euclidean distance because it is the most intuitive and popular metric. More over, many commonly used metrics, such as L-norms, K-L divergence, or more generally, Bregman divergence, are de-fined based on the Euclidean distance. Fig 1 gives two pairs of vectors ( v 1 ,v 2 )and( v 1 ,v 2 ), and the angles,  X  ,  X  between each pair, respectively. Under Euclidean distance, v 1  X  v and v 1  X  v 2 are the same. In other words, Euclidean dis-tance fails to detect  X  =  X  , and therefore, is unable to dif-ferentiate the length difference from the direction difference introduced by the dimensionality.

In fact in quite a few real-world applications, high dimen-sional data per se do not contribute to the data vectors X  mag-nitude change, but to a new combination of a certain subset of the features. For example, we do not intend to conclude that the difference between a human baby and an adult is the same as that between the baby and a little monkey; a banker is not interested in the volume of the financial news but the newly emerged key words; to examine the mutation of a DNA sequence, a biologist needs to find the new com-bination of Adenine and Guanine instead of the DNA data size change. In these cases, the change of feature subspace should not be confused with the change of data X  X  magnitude. One may argue that we still could round up all the vectors into the same length and then apply the Euclidean distance to avoid the confusion with the magnitude. Such a manipu-lation theoretically works only when the subspace dimension spanned by data is one (to compare only two vectors). More-over, the round-up errors and the change of the original data structure may lead to unmanageable consequences.

Based on the above fact, our first motivation is to find a metric that is invariant under data X  X  magnitude change and only characterize the subspace change introduced by dimen-sionality. Further, we require that such a metric is in a form suitable for computation and manipulation. We would like to clarify that, it is not our intention to underestimate the significance of detecting data X  X  magnitude change. Our standpoint is that to detect the subspace change between high-dimensional data sets through a magnitude-based met-ric is inaccurate and conceptually confusing. In the rest of the paper, when we say pattern change between high-dimensional data sets, we refer to the subspace change, not the magnitude change.

In order to identify the appropriate subspace for discover-ing the pattern change between the data sets, we introduce the concept of dominant subspace based on the principal angles [7]. The notion of principal angels between two sub-space has a nice property of invariant under an isomorphism, thus is independent of data X  X  magnitude change. The chal-lenge then is to compute the principal angles. To address this challenge, we elect to use matrix factorization to serve as a statistical framework for computing the principal an-gles. We develop the principle of dominant subspace map-ping and show how matrix factorization can be naturally embedded into the likelihood ratio test based on the princi-ple. The proposed method is of an unsupervised nature and addresses the statistical significance of the pattern changes between high-dimensional data sets.
 The contribution of this work is highlighted as follows. First, we have studied the very general problem of pattern change discovery among different high dimensional data sets. Second, we have introduced the notion of principal angles between subspaces as a metric for pattern change. Third, we have introduced the principle of the dominant subspace mapping to transfer the principal angle based detection to a matrix factorization problem. Fourth, we have showcased the different applications of this solution in several specific real-world applications to demonstrate the power and effec-tiveness of this method.
The classic paradigm for magnitude-based change detec-tion between two data sets is through parameter estimations based on established distribution models. More recent work in this direction [25, 12, 27] attempts to avoid the paramet-ric dependency and to define alternative distance measures between the two distributions. In [25], Song et al. developed a Monte-Carlo framework to detect distribution changes for low dimensional data. In [12], Kifer et al. defined the A-distance to measure the non-parametric distribution change. In [27], Leeuwen and Siebes described the data distributions using their compressed code table and defined the Code-Table-Difference to capture the distribution difference be-tween data sets. The limitation of the low-dimensional dis-tribution models, as Vapnik pointed out at the beginning of his book [28], is that they do not reflect the singularities of the high-dimensional cases, and consequently cannot grasp the change of the subspace.

Based on Vapnik X  X  statistic supervised learning theory, the pattern change detection problem, also called concept drift in several specific applications, has been attracted great ef-fort [26, 31, 30, 29, 21]. Classifiers are trained to capture the subspace structures of the high-dimensional data sets via support vectors. The pattern changes can be indirectly reflected through evaluating t he classification errors on the data sets. We refer to the survey by Tsymbal [26] for an overview of the important literature on this topic. The main categories of the methods to address the concept drift analy-sis problem include the instance selection and weighting [13, 11], the ensemble learning [30, 31, 2], and the two-samples hypothesis test [5, 10, 21]. Although supervised learning techniques have the capacity to detect structural changes between high-dimensional data sets, they require labels to train and validate the classifiers. Most of the real-world data sets, however, typically lack sufficient labels that can be used to train the classifiers. In [5], Dries and R  X  uckert proposed a trade-off strategy. Without using real labels, they constructed two virtual classifiers by giving two differ-ent types of the labels to the two data sets, respectively, and then proposed three two-sample test methods based on the quality of the classifiers; a good quality indicates a concept drift between the two data sets. Using one classifier to de-scribe the whole dataset, however, oversimplifies the mixture structures of the data sets, and the detection performance is expected to be impaired (see Sec. 5).

As an unsupervised paradigm, matrix factorization is re-cently considered for subspace analysis of high-dimensional data sets. The theory and applications of matrix factoriza-tion have been intensively developed during the last decade. In [16], Lee and Seung developed the breakthrough of the multiplicative updating rules for solving matrix factoriza-tion, extending the classical vector quantization and princi-pal components analysis to a new horizon. In [8], Gordon unified the matrix factorization literature with the general-ized linear models, strengthening the statistical foundation for matrix factorization. As for the applications, Ding et al. [4] applied non-negative matrix factorization to spectral clustering, graph matching, and clique finding. Long et al. [19, 18] used matrix factorization for relational clustering. Miettinen [20] developed factorization algorithms for binary data sets. In this paper, we use matrix factorization and the notion of the principal angles between subspaces to cap-ture the structural difference between the high-dimensional data sets. We address the statistical significance of the dif-ference through a likelihood hypothesis test based on the linear model. To the best of our knowledge, this is the first time when matrix factorization is used to develop a statisti-cal framework for the pattern change detection.

Other recent efforts on the magnitude-based change de-tection for specific applications include the event detection from time series data [15, 22] focusing on discovering a sig-nificant magnitude change and its duration on a particular feature, word bursts tracking[6, 9], and trend analysis in blogosphere by tracking singular values [3].
In this paper, a matrix is denoted as a capital letter in boldface such as X . X ij is the entry in the i th row and the j th column. X i  X  stands for the i th row of X and X stands for the j th column of X . A vector is a lowercase letter in boldface such as x . A scalar variable is denoted as a lowercase letter such as x . U T stands for the transpose of the matrix U . X m  X  n stands for a matrix X  X  R m  X  n span ( A ) stands for the subspace spanned by the column vectors of the matrix A .  X  by default is the Frobenius norm for a matrix;  X  2 is the 2-norm [7] for a matrix. diag ( { x i } ) stands for a diagonal matrix with x i as its i th diagonal entry.
In this section, we introduce the principal angels between subspaces to measure the subspace difference between data sets of high dimensions. We have already addressed the pit-fall of the popular distance metrics and now we explain why the principal angles can avoid it. We start with the same example in Fig 1. We have already shown that Euclidean distance fails to detect  X  =  X  , and therefore, is unable to dif-ferentiate the length difference from the direction difference introduced by the dimensionality. On the other hand, in this specific example, the principal angle between span ( v and span ( v 2 ) is actually  X  , and that between span ( v span ( v 2 )is  X  . Onemaynoticethatwehereuse span ( v ) instead of just v . This indicates that  X  and  X  are invariant under the length shrinking or stretching for the correspond-ing vectors. Now one can reasonably understand the notion of principal angles between two subspaces as a generaliza-tion of an angle between two vectors as the dimensionality goes from one (as for span ( v 1 )) to n where n  X  1. The principal angles have a very im portant property that all the Euclidean-based metrics do not have  X  Invariant under an isomorphism and thus independent of the magnitude change (e.g., invariant under scalar multiplication when the dimen-sionality is one).

Without loss of generality, assume two vector sets { x i } m and { y i } l i =1 , x i , y i  X  R n . Golub and Loan proposed in [7] the definition of principal angles as to measure the structural difference between the two subspaces S 1 = span ( { x i } m and S 2 = span ( { y i } l i =1 ): An increasing sequence of prin-cipal angles {  X  k } q k =1 is defined between two arbitrary sub-spaces S 1 and S 2 using their orthonormal basis: ([7] Page 602):
Definition 1. Let S 1 and S 2 be subspaces in R n whose dimensions satisfy The principal angles  X  k  X  [0 , X / 2] , k =1 ,...,q , between S and S 2 are defined recursively as when k =1 , u 1 = v 1 =1; when k  X  2 , u k = v k = 1; u T k u i =0; v T k v i =0 where i =1 ,...,k  X  1 .
In this definition, vectors { u k } q k =1 and { v k } q k ally part of the orthonormal basis for S 1 and S 2 ; the inner products of each pair u k and v k form a unique increasing sequence of angles. These angles explicitly give the differ-ence of the subspace structure between S 1 and S 2 .The algorithm given in [7] to compute the principal angles takes O (4 n ( q 2 +2 p 2 )+2 pq ( n + q )+12 q 3 ) in time complexity.
The leading largest principal angles depict the most no-ticeable structural difference between S 1 and S 2 .Thecor-responding dimensions responsible for the largest principal angles are of great interest as they reflect the major pattern change. We call the subspace formed by these dimensions the dominant subspace . Now in order to measure the struc-tural difference between { x i } m i =1 and { y i } l i =1 to directly computing the principal angles between S 1 and S , and then obtain the dominant subspace based on the largest principal angles. In practice, however, this is not an optimal solution. First, the values of n, p, and q in Defini-tion 1 can be very large in real-world data set s, resulting in a high complexity to compute the principal angles. Sec-ond, since the real-world data sets typically contain noise and outliers, the principal angles directly computed from the raw data may not reflect the true situation. Third, in many applications, given a large amount of samples, one is only interested in the most frequent pattern changes in the majority of the data set and does not care of the principal angles for all the samples. All these issues require to de-veloping an alternative solution to directly computing the principal angles. On the other hand, matrix factorization [16, 1, 4, 24] has been used extensively for reducing dimen-sionality and extracting collective patterns from noisy data in a form of a linear model. In the next section, we develop the principle of dominant subspace mapping through ma-trix factorization as the alternative to obtain the dominant subspace.
Given two data sets X = { x i } n i =1 and X = { x i } n i present a model to detect the pattern changes between X and X . Instead of computing the principal angles directly, we provide a more practical strategy involving three steps: To establish a null-hypothesis on pattern change, to extract a set of basis vectors from span ( { x i } n i =1 ) under a null and its alternative hypothesis, and a statistical test to confirm these changes. We will show how principal angles, matrix factorization and linear models work together to serve the purpose.
Learning mixture patterns from data can be formulated as generalized 2 linear 2 models [8, 24] using the following matrix factorization term: where the matrix X m  X  n =[ x 1 , x 2  X  X  X  x n ] , x  X  R m of n data samples represented as the n column vectors. Matrices P m  X  k and S n  X  k ,k min( m, n ), are two lower-dimension factors whose product approximates the original data set X .The k column vectors of P are prototype pat-terns learned from X ;the i th row of S is a soft indicator using k prototypes to restore the i th sample. Thus, the columns of P can also be considered as an approximate gen-erating set for the subspace containing samples { x i } n this modeling, we concentrate on P , the prototype patterns, and its changing behavior. S describes how the k prototypes are distributed among the n samples and may also contains useful information to characterize the dataset; yet in this paper we do not discuss its behavior and leave it as an open question.

Another advantage for matrix factorization is its form as a linear model under which a hypothesis test can be developed. More specifically, given a linear model with additive Gaus-sian noise G : X = PS T +  X  ,where  X   X  j  X  N m  X  1 ( 0 , X  our strategy is to check the pattern change in P by prop-erly constructing a hypothesis on P and then applying the standard likelihood ratio test.
In order to extract the plausible pattern changes, instead of directly computing the principal angles we develop the principle of dominant subspace mapping through construct-ing a hypothesis testing as follows. We first establish a hypothesis on the pattern matrix P . Assuming P and P from the two data sets X and X , since the principal angles {  X  i } k i =1 between span ( P )and span ( P ) indicate the scale of pattern changes, it is straightforward to set up the hy-pothesis on the principal angles to begin with. Now we have two options for the null-hypothesis: To assume no pat-tern change or to assume an obvious pattern change. If we choose the former, there a re two concerns. First, the possibility that two data sets obtained from different times or locations share the same subspace is almost zero, result-ing in a hypothesis on an almost impossible event. Sec-ond, as shown in definition 1, the principal angles are com-puted via cos ; the null-hypothesis of no pattern change gives H o : diag ( { cos  X  i } k i =1 ) = k , indicating that every princi-pal angle is zero; such a setting is vulnerable due to differ-ent k value in different applications and we have no prior knowledge about the specific value of k . On the other hand, if we set the hypothesis as an obvious pattern change, it serves both purposes of detecting pattern change and a con-venient form of H o : diag ( { cos  X  i } k i =1 ) =0. Ifthehy-pothesis is true, the values of {  X  i } k i =1 are large, indicating the large pattern change between span ( P )  X  X  x i } n i =1 span ( P )  X  X  x i } n i =1 . More importantly, this hypothesis is independent of the value k , making the detection more ro-bust despite the possible information loss caused by matrix factorization.

While the hypothesis H o : diag ( { cos  X  i } k i =1 ) =0is straightforward, in order to construct a simple statistic test, we elect to use a hypothesis cast directly on P and P .For this purpose, we first introduce the following lemma:
Lemma 1. Given that P  X  R m  X  p and P  X  R m  X  q ,each with linearly independent columns, and that each column is normalized into the same 2-norm length L , and further given the QR factorizations P = QR and P = Q R ,the princi-pal angles { cos  X  i } k i =1 between span ( P ) and span ( P ) satisfy inequality: where a  X  pq is a constant, and  X  1 and  X  2 are the smallest eigenvalues of R and R , respectively.

The proof of Lemma 1 is in Appendix A. Lemma 1 gives the upper and lower bounds of diag ( { cos  X  i } k i =1 ) in terms of P T P . More importantly, due to the Sandwich The-orem, P T P and diag ( { cos  X  i } k i =1 ) are asymptotically equivalent as diag ( { cos  X  i } k i =1 ) is close to zero. Therefore, we establish a hypothesis using P and P directly, as shown in the following corollary: Corollary 1. The null-hypothesis has its equivalent form of Proof. It is a direct result due to Lemma 1 and the Sandwich Theorem.
Given the simple form of null-hypothesis H o : P T P = 0 on linear model G : X = PS T +  X  ,where  X   X  j  X  N m  X  1 ( 0 , X  one can use the standard likelihood ratio test for verification ([23] Page 98): First, estimate P only based on linear model G . Second, estimate P under constraint H o . Finally, com-pute the likelihood ratio between the two cases.
 To estimate P only based on the given linear model G : X = PS T +  X  ,where  X   X  j  X  N m  X  1 ( 0 , X  2 I m  X  m ), the likelihood function for G is [23] Maximizing the likelihood function (4) is equivalent to es-timating the factors P and S that minimize X  X  PS T 2 . This normal-distribution-based matrix factorization can be efficiently solved via the multiplicative iteration algorithm proposed by Lee and Seung in [16, 17]: The proof of the convergence of the updating rule can be found in [17]. This updating rule generates the estimation  X  P and  X  S . Algorithm 1 LRatio Input: data sets X , X ,,andthreshold h .
 Output: Feature basis P H , indicator matrix S H , the like-lihood ratio test statistic  X , and the testing result. Method: 1: Initialize P , S ,  X  P ,  X  S ,  X  P H and  X  S H ,and  X  randomly. 2: Iteratively update P and S using (5) until convergence 3: Iteratively update  X  P and  X  S using (5) until convergence 4: Iteratively update  X  P H and  X  S H using (7) until conver-5: Compute  X  using (8) 6: Reject H o if  X  is smaller than h .

Finding the maximum likelihood estimates subject to the constraint (3) gives the following log likelihood function ([23] Page 98):
L ( P , S )=  X  log L ( P , S )+  X  P T P 2 which is equivalent to minimizing: where  X &gt; 0 is the Lagrange multiplier. To solve this con-strained optimization problem, we give the non-increasing updating rule through the following Lemma:
Lemma 2. The loss function (6) is non-increasing under the updating rule: The loss function (6) is invariant under this rule if and only if P and S are at a stationary point of the loss function.
The proof of the lemma is in Appendix B. Using updating rule (7), we obtain estimation  X  P H and  X  S H under the null-hypothesis. The time complexity of the updating rule (7) for each iteration is O ( mnk + k 2 m ), where m, n, and k are defined at the beginning of Section 4.1.
 After we obtain the estimation of  X  P ,  X  S and  X  P H ,  X  likelihood ratio statistic is given by ([23] Page 99): According to the likelihood principle, a small  X  indicates a bad estimation of  X  P H and  X  S H ,andwethenreject H o . On the other hand, a large value of  X  suggests a pattern change detected in  X  P H . The algorithm, called LRatio, is summarized in Algorithm 1.

The time complexity of the updating rule (5) is O ( mnk ) in each iteration, and that of (7) is O ( mnk + k 2 m )ineach iteration; the complexity to compute  X  is O ( mnk ), where m, n ,and k are defined in Section 4.1. Thus, the total time complexity is O ( mnk + k 2 m ) for each iteration, which is much lower than that of directly computing the principal angles between X and X .

Table 1: Configuration of the pattern change data sets.
In order to demonstrate the power and promise of LRatio as well as its superiority to the existing literature in discov-ering significant pattern changes in different applications in the real-world, we have applied LRatio to several different real-world problems in comparison with the existing meth-ods in the related literature in these different applications.
The goal of the first application is to verify the perfor-mance of LRatio test using collections of text documents. In this application, we use the standard 20-newsgroup data sets [14], the dimension scale of which is of thousands. As listed in Table 1, we construct eight scenarios using different topic combinations. For each scenario, two parts are set up (Part 1 and Part 2 ). Each part contains articles evenly mixed from one or more topics. Under each scenario, if the two data sets are sampled from the same part, they should bear sim-ilar subspace structure; while if the two data sets are from different parts, their subspace structures are different and LRatio test should be able to reflect this difference through the testing statistic. These eight scenarios are constructed to showcase data sets with different structural complexities and/or pattern change strengths. The first four scenarios intend to imitate moderate pattern change by electing sim-ilar topics between the two parts. The next four scenarios imitate strong pattern change by setting different topics be-tween the two parts. We compare the performance of LRatio with the following methods.

Baseline . We apply the standard K-means to each of the two data sets to obtain the data matrices composed of the K centroids, respectively, and then compute the subspaces distance between the pair of the K centroids based on Def-inition (1). Intuitively, a pattern change shall results in a large distance. We use this distance as a statistic to indicate the pattern change, and compare its sensitivity with LRatio test. For the reference purpose, we call this baseline method as KM. between W and B indicates a better performance.

Peers . We elect to use three different concept drift de-tection methods in the recent literature [5] by Dries and R  X  uckert for a peer comparison. They are PCA-Bayesian Margin Test and two other error rate based test methods. For the reference purpose, we call them SVM-margin, SVM-sigmoid, and SVM-(0,1), respectively. The reasons why we select these comparing methods are the following. First, un-der the framework of support vector machine (SVM), their methods are suitable for high-dimensional data sets. Sec-ond, although based on supervised techniques, the model does not require real labels and therefore can be used in un-supervised applications. Third, these methods are in a sim-ilar two-sample statistical test framework to that of LRatio, resulting in a fair comparison environment.

In order to verify the detection sensitivity, we compare the testing statistics of the data set pair having no pattern change in between with the testing statistics of the data set pair involving a pattern change in between. The evaluation protocol is defined as follows.

For each scenario, 1. Obtain testing statistic from data set pair with no pattern change in between: i). Constructing two data sets by randomly sampling 200 articles, each dataset with 100 samples, only from Part 1 (or Part 2). ii). Applying LRatio and the four comparison methods on the two data sets. iii). Repeating i) and ii) 20 times. 2. Obtain testing statistic from data set pair with pattern changes in between: i). Constructing the first data set by randomly sampling 100 articles form Part 1; constructing the second data set by randomly sampling 100 articles from Part 2. ii). Applying LRatio and the four comparison methods on the two data sets. iii). Repeating i) and ii) 20 times. 3. For each method, normalize the 40 testing statistics to the range of [0 , 1] for easy comparison.

Ideally, there should be a big gap between the first 20 testing statistics and the last 20 testing statistics, because the first 20 tests are from the dataset pair that has no pat-tern change, and the last 20 tests are from the data set pair with pattern changes. Fig. 2 documents all the re-sults of this experiment, where a boxplot is used to rep-resent the numerical distribution of the statistics obtained from the sampling within each part (red boxplots) and sam-pling between two parts (blue boxplots). In each boxplot, the median(in ), the 25th percentile(in bars), the 75th percentiles(in whiskers) and the outliers(in  X  ) of the distri-bution are drawn. Consequently, for each method and for each of the eight collections, there is a corresponding pair of boxplots representing the statistic distributions for sampling within each part (red boxplot labeled with letter W) and for sampling between two parts (blue boxplot labeled with letter B), respectively. Clearly, more overlap between the pair of boxplots indicates the worse performance in discovering the pattern change for the method. From the figure, all the four comparing methods have the overlaps in the majority of the eight collections; in comparison, LRatio is the only method that has no overlap at all for all the eight collections; fur-ther, for the first four scenarios where there expects to be only moderate topic changes between the two parts, LRatio still clearly stands out with no overlap at all between the two boxplots. This demonstrates that LRatio is not only pow-erful in discovering pattern changes, but also very sensitive to the pattern changes.
While the 20 newsgroups data experiment is for system-atic evaluations of the pattern change discovery capabilities and sensitivities, the next experiment is an application sce-nario of event detection in a news stream data set. We have manually collected Google news data everyday from 23. Oct. to 22. Nov. for the year of 2008 for four specific tracks: po-litical news, financial news, sports news, and entertainment news. To form the news stream data for each of the four tracks, we group the news documents and time-stamp these documents in a unit of every three neighboring days. Since all the five methods we used in the previous experiments are for pattern change discovery between two data sets, we apply each of them to each pair of the neighboring units of the news stream in each track to obtain the statistic value. Consequently, for the whole month news data in each track, each method generates a statistic sequence, which is called thetestsequenceforthetrackofthenewsforthecorre-sponding method.
Fig. 3 documents the political news test sequences within the window between October 23, 2008, and November 22, 2008 for LRatio, KM, and SVM-margin. Since the three methods from [5] are very close in performance, for the clar-ity purpose in the figure, we only show the test sequence of SVM-margin in this figure. Presumably in the figure for each method a significant peak in the test sequence means a significant pattern change, indicating that significant news events are detected by this method on that day. We manu-ally examined everyday X  X  news data within this whole month to provide the ground truth regarding whether there are any significant news events on everyday of the month, and an-notated the specific events.

Since both LRatio and KM conduct the pattern change discovery through the clustering manner, they are both fur-ther able to detect the specific events through key words in each cluster, and are able to rank the  X  X ignificance X  of each detected event based on the number of samples in each cluster. Fig. 4 documents the top five detected significant events on Nov. 7, Nov. 13, and Nov. 19 for both meth-ods, respectively, where each event is represented using a bar with the length proportional to the significance of the event, and the same event is ground-truthed with the same color. From the figure, there are three observations. First, for each of the three days, all the five detected events by LRatio are unique and distinct, while there are many du-plications of the five top events detected by KM when com-pared with the ground truth; this is particularly true for Nov. 7 where all the five top events detected by KM are about the Election Campaign . Second, as is also observed in Fig. 3, for many events KM is unable to detect them  X  X n time X  but rather with a delay; in other words, for many of the news events KM detected are actually old news events. For example, the event that Obama was elected as the US President occurred on Nov. 7 (which LRatio corrected de-tected in time) is declared as the number one detected event for KM both on Nov. 13 and on Nov. 19, but not on Nov. 7. In fact, for all the three days X  top five events detected by KM, only the event Senator Clinton Became Secretary of State on Nov. 13 and the event North Korea Nuclear Crisis on Nov. 19 are caught by KM in time, whereas all others are actually old news events. Third, the specific event data reported in Fig. 4 coincide with the holistic event detection results reported in Fig. 3 very well. Of all the three days, KM essentially only detected old news events, and that is why in Fig. 3 on these three days there is no peak in the KM test sequence curve, indicating KM fails to detect any significant events for these three days. While SVM-margin does not have the capability to do the clustering analysis to report the specific events detected on each day as LRatio and KM do, it detects the events based on a holistic analysis reported in the test sequence shown in Fig. 3, from which it is clear that SVM-margin still fails to detect any significant events on Nov. 13 and Nov. 19 with an exception on Nov. 7becausetheevent Obama Became US President was such an obvious significant event that SVM-margin did not miss.
The difference in performance between LRatio and the comparing methods is obvious. LRatio aims at discovering pattern changes regardless of whether the pattern changes come from a completely new topic or a new direction of an existing topic. KM, on the other hand, aims at discovering major clusters from the data; thus, new topics need time to  X  X ccumulate X  to form clusters in order to become significant
Figure 3: Test sequences for Google political news stream topics, while new directions of an existing topic are likely to be absorbed into the clusters and would never show up un-til they eventually dominate the clusters. That is why KM always misses many significant events and often detects an event with a delay in time. For SVM-margin, SVM-(0,1), and SVM-(sigmoid), although they also aim at discovering pattern changes, they work well only when the data have a simple structure and the majority of the samples bear a similar pattern change, which also explains why they only provide a holistic statistic on event detection with no capa-bility for the specific pattern changes.
Finally, we also showcase the application of LRatio to the surveillance video stream data to detect events. In this con-text, each frame of the video stream is considered as a sample vector. Like the news data stream in the previous experi-ment, here again we apply LRatio to each pair of neighboring video segments (each segment has 100 frames) to see whether there is any event occurred. To demonstrate the power of the surveillance event detection capability, we apply LRa-tio to several different video surveillance data sets collected at different specific surveillance applications. Figs. 5 to 7 showcase three different tests of using LRatio for surveil-lance event detection, where in each of the figures the left panel is a snapshot of the surveillance video stream and the right panel indicates the test sequence of LRatio along the timeline.
We have studied the very general problem of pattern change discovery among different high dimensional data sets which exist everywhere in almost every application in the real-world, and have identified an approach based on the prin-cipal angles to discover the pattern change. We have intro-duced the principle of the dominant subspace mapping to transfer the principal angle based detection to a matrix fac-torization problem through a hypothesis testing. Finally, we have showcased the different applications of this solution in several specific real-world applications to demonstrate the power and effectiveness of this method.
 This work is supported in part by NSF through grants IIS-0503162, IIS-0812114, IIS-0905215, and CCF-1017828. Bryan Bernard, Kevin Hannon, Daniel McFadden, and Greg Stod-Figure 4: Pattern changes detected by LRatio v.s. Clusters found by KM.  X   X   X  marks the old news topics that have been detected in the previous days. Boxes with the same colors are related to the same news topics dard helped in data collection in this work under an NSF REU grant. [1] A. Banerjee, S. Merugu, I.S.Dhillon, and J.Ghosh. [2] A. Bifet, G. Holmes, B. Pfahringer, R. Kirkby, and [3] Y. Chi, B. Tseng, and J. Tatemura. Eigen-trend: [4] C. Ding, T. Li, and M. Jordan. Nonnegative matrix [5] A. Dries and U. R  X  uckert. Adaptive concept drift [6] R.Ge,M.Ester,B.Gao,Z.Hu,B.Bhattacharya,and Figure 5: detection result for video 1; (a) rank 1 event: a man is running towards a cart; (b) the test sequence; the star marks the time when this man begins running. Figure 6: detection result for video 2; (a) rank 1 event: an earthquake occurs and people are running out; (b) the test sequence; the star marks the time when the earthquake occurs. [7] G. H. Golub and C. F. V. Loan. Matrix computation . [8] G. Gordon. Generalized 2 linear 2 models. In NIPS , [9] D. He and D.Parker. Topic dynamics: An alternative [10] S. Hido, T. Ide, H. Kashima, H. Kubo, and [11] G. Hulten, L. Spencer, and P. Domingos. Mining [12] D. Kifer, S. Ben-David, and J. Gehrke. Detecting [13] R. Klinkenberg. Learning drifting concepts: Example [14] K. Lang. [15] L.Chen and A.Roy. Event detection from flickr data Figure 7: detection result for video 3; (a) rank 2 event: the car collision occurs;(b) rank 1 event: a man is running towards the accident scene (c) the test sequence; the star marks the time of the collision. [16] D. Lee and H. Seung. Learning the parts of objects by [17] D. Lee and H. Seung. Algorithms for non-negative [18] B. Long, Z. Zhang, and P. Yu. Co-clustering by block [19] B. Long, Z. Zhang, and P. Yu. Unsupervised learning [20] P. Miettinen. Matrix Decomposition Methods for Data [21] K. Nishida and K. Yamauchi. Detecting concept drift [22] D. Preston, P. Protopapas, and C. Brodley. Event [23] G. Seber and A. Lee. Linear Regression Analysis . [24] A. P. Singh and G. Gordon. A unified view of matrix [25] X. Song, M. Wu, C. Jermaine, and S. Ranka.
 [26] A. Tsymbal. The problem of concept drift: Definitions [27] M. van Leeuwen and A. Siebes. Streamkrimp: [28] V. N. Vapnik. Statistical Learning Theory . John Wiley [29] J. Vreeken, M. Leeuwen, and A. Siebes. Characterising [30] H. Wang, W. Fan, P. Yu, and J. Han. Mining [31] P. Zhang, X. Zhu, and Y. Shi. Categorizing and The proof of Lemma 1 uses the method in [7] for computing the principal angles. We first give this method as follows.
Given A  X  R m  X  p and B  X  R m  X  q ( p  X  q ), each with linearly independent columns, the principal angles between subspaces span ( A )and span ( B ) can be computed as fol-lows . First, compute the QR factorizations for A and B , respectively Then, let C = Q A T Q B and compute the SVD (singular value decomposition) of C such that Y T CZ = diag (cos  X  ), where diag (cos  X  ) is short for the diagonal matrix with the cosines of the principal angles { cos  X  1 , cos  X  2 ... cos  X  diagonal elements.

Proof. Since where diag (cos  X  )= Y ( Q T Q ) Z is the SVD of Q T Q .The inequality we are to prove can now be re-written as: For the left hand side inequality, since P = QR = R = pL and similarly P = R = qL ,wehave For the right hand side inequality, Since R and R are upper triangular, the inverses R  X  1 and R  X  1 are also upper triangular. Therefore, the eigenvalues of R are { ( R ) ii | i =1 ,...,p } , the diagonal entries of R . Hence, the eigenvalues of R  X  1 are { 1 ( R ) ii } ,theinverseof the diagonal entries of R . The same conclusion also holds true for R  X  1 .Thus,wehave Combining (10) and (11) we obtain where a  X  pq .
 This completes the proof of the Lemma.
 We first prove the convergence of the updating rules for P and S , then determin the value of  X  .To prove the updating rules for P and S , we make use of an auxiliary function simi-lar to that used in the Expectation-Maximization algorithm [17].
Definition 2. G ( u, u ) is an auxiliary function for F ( u ) if the conditions are satisfied.

The auxiliary function is a useful concept due to the fol-lowing lemma:
Lemma 3. If G is an auxiliary function, then F is non-increasing under the update:
Proof. F ( u t +1 )  X  G ( u t +1 ,u t )  X  G ( u t ,u t )= F ( u
We show that by defining the appropriate auxiliary func-tion G ( u, u t ) for (6), the update rule in Lemma 2 easily follows from (13). Now let u = P T i  X  , u = P T i  X  . Lemma 4. Function is an auxiliary function for where K ( u t ) is a diagonal matrix defined as
Proof. Since G ( u, u )= F ( u ) is obvious, we only need to show that G ( u, u t )  X  F ( u ). To do this, we compare with (14) to find that G ( u, u t )  X  F ( u )isequivalentto To prove the positive semidefiniteness, consider the matrix which is a rescaling of the components of K ( u t )  X  ( S  X u T u I ). Then, K ( u t )  X  ( S T S +  X u T u I ) is positive semi-definite if and only if M is, and v T Mv =
Now we are ready for the proof of the updating rule for P in Lemma 2.

Proof. Applying update rule (13) to the auxiliary func-tion (14) results in: Lemma 3 guarantees that F is non-increasing under this up-date rule. Writing the component of this equation explicitly, we obtain
The proof of update rule for S is the same as that in [17].
 The approach to determine  X  is more straightforward. Since  X  only controls the convergence rate of P and S ,the-oretically its value does not influence the final convergence of P and S . Wesetthecoursetoregularize  X  by using standard Lagrange multiplier procedure. Let entry of P and S may have different gradient speed, we therefore set  X  to be the average as given in Lemma 2
