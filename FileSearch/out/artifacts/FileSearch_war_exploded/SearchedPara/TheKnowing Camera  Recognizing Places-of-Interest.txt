 This paper presents a framework called Knowing Camera for real-time recognizing places-of-interest in smartphone pho-tos, with the availability of online geotagged images of such places. We propose a probabilistic field-of-view model which captures the uncertainty in camera sensor data. This model can be used to retrieve a set of candidate images. The vi-sual similarity computation of the candidate images relies on the sparse coding technique. We also propose an ANN filtering technique to speedup the sparse coding. The final ranking combines an uncertain geometric relevance with the visual similarity. Our preliminary experiments conducted in an urban area of a large city show promising results. The most distinguishing feature of our framework is its ability to perform well in contaminated, real-world online image database. Besides, our framework is highly scalable as it does not incur any complex data structure.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval places-of-interest, image retrieval, bag-of-visual-words
The global population of smartphone users has hit one billion in the past year. Every day and night, the world is being captured through millions of phone cameras, and then displayed in images via Internet social applications (e.g. Facebook and Flickr) to an enormous audience. These im-ages, ever increasing in numbers at an unprecedented rate, comprise a rich and useful data source for the physical world. Think of a tourist visiting an unfamiliar city holding a GPS-equipped smartphone. Some of the greatest conve-niences she could have are: (1) Upon taking a photo of a place (e.g. a tower or a coffee shop) using her mobile phone, the gadget displays in seconds a pop-up, showing the name of the place, probably with a URL directing to further in-formation about it; (2) Afterwards, her friends would be able to watch her online photos, automatically annotated with respective place names, without requesting for time-consuming and error-prone tagging. This paper reports our recent work on the Knowing Camera (KC) project, which aims at developing a system for recognizing outdoor Places-of-Interest (POIs) captured in smartphone photos, relying on geotagged photo sharing Web services (e.g. Flickr).
Specifically, the key technical problem of the system can be defined as follows. Given (1) a database of POIs denoted by P , where each POI has a name and geo-location, (2) a set of geotagged images S , and (3) a query photo with its GPS location and camera geometries, find the most prominent POI (target POI) captured in that photo.

Generally, there exist two approaches to this problem, namely the spatial approach and the visual one. The spa-tial approach usually takes the query location and camera geometries (also known as the eld-of-view or FOV) to find the closest and most visible POIs in P . Unfortunately, such solution fails to discriminate POIs in the save FOV, not to mention the errors prevalent in the geotags and camera sen-sors.

The visual approach requires that each photo in S is as-sociated with one or more POIs. By computing the visual similarity between the query image and those in S , it is possible to find the best matching photos, and subsequently retrieve the target POI which receives the most contribu-tions from the matching photos.

However, the online geotagged photos are known to be both visually and semantically noisy. The  X  X ruly X  visually similar ones (judged by human perception) comprise only a small percentage, as the photos of the same semantics (a POI) could have very diversified visual appearances. In addition, each POI might be associated with other photos which are just semantically irrelevant, probably due to (i) users X  mistakes, (ii) the physical distances between the cam-eras and the objects, and (iii) humans or events recorded in the photos.

We propose novel techniques to address the above prob-lems. Our method relies on the following observations: First, the camera geometries (FOV) are easy to obtain when a photo is shot by a smartphone. However, those of the on-line geotagged photos are typically unavailable. Second, the sensors in cameras are erroneous. The sensory readings of phone direction, viewing angles, and camera location may all contain uncertainty. Third, as online photos associated wit h each POI contain high impurity, simply computing the visual similarity between the query and POI-tagged photos leads to indiscriminating results  X  similarity values which are indistinct among several POIs.

Our first and second observations motivate the design of a probabilistic FOV model. Based on this model, all POIs relevant to the FOV are given a likelihood of being captured by the camera. Our third observation leads to a solution of visual similarity comparison based on sparse coding , a tech-nique rooted in the signal processing domain. This technique allows a query image to be given as a linear function of the  X  X ases X , which stand for a few representatives in the origi-nal candidate images. The noisy images are not accounted in this case, making the resultant similarity (which we call SC-similarity) reasonably discriminative. As a result, our scheme can tolerate impure images containing multiple hu-man faces and bodies, in contrast to most of the existing works [1, 4, 3] which require clean and uncontaminated im-age database.

Our method only extracts  X  X ag-of-visual-words X  features from each image, and does not require any additional vi-sual data structure. The only structure being used is a conventional spatial index (an R-tree in our implementa-tion), which accesses the visual feature vectors of the pho-tos by their geotag locations. This makes our scheme highly scalable, efficient, and easily adaptable to the existing geo-tagged photo sharing services. Another desirable feature of the scheme is the possibility to balance the local recognition accuracy and visual similarity computation cost, by varying the range of the probabilistic FOV.
 Summary of contributions:
Our POI recognition framework consists of (1) a POI database P , where each point p  X  P has a geo-location p.loc , and (2) an image database S , where each photo s  X  S has a geotag location s.loc and is also associated with a POI in P . To expedite spatial accesses, we also generate an R-tree which indexes all images in S by their locations s.loc .
Generally, a query q contains a query image q.img , and its camera geometries stored in q.f ov , including its GPS lo-cation, the angles of view, the maximum visible distance, and the direction of the camera. Each query is processed in three consecutive phases, namely the spatial phase , the visual phase , and the ranking phase , as described in the fol-lowing.
The spatial phase takes the camera location as the center and makes a 2D square range query on the R-tree to retrieve images which are geotagged in the query box. These images are called  X  X ocal images X . Fi gure 1: The probabilistic FOV model consisting of 4 pa-rameters, namely the camera location q , the camera direc-tion  X  X  , the camera viewing angle  X  , and the maximum visible distance R .

Next, the POIs associated with the local images undergo a probabilistic FOV culling procedure, which removes the POIs that are geometrically impossible to appear in the query image (detailed in Section 2.1.2). Images associated with any culled POI are deleted from the local image set. The remaining local images now become the candidate im-ages , and their associated POIs are considered the candidate POIs .

Subsequently, we compute for each candidate POI p a geometric-relevance value (denoted by geo-relevance ), which indicates the geographical probability that p appears in the query pFOV (detailed in Section 2.1.3).
Given a query q , figure 1 illustrates its probabilistic FOV (pFOV) model, which is derived from the conventional FOV (field-of-view). The conventional FOV model consists of 4 parameters, namely the camera location q , the camera di-rection  X  X  , the viewing angle  X  , and the maximum visible distance R . The first three quantities can be obtained in real-time from the sensory readings while the last parame-ter R is determined by the camera specification.

It is important to note that the sensory readings may con-tain an abundance of uncertainty due to device errors. Using the GPS data of the camera location as an example, the ac-tual camera location might be a hundred meters away from the GPS readings. Thus, we use a 2D Gaussian distribution centered at the current location reading q to model the ac-tual camera location. The shaded circle in Figure 1 indicates the probable camera locations, with an uncertain radius of r . Likewise, the direction  X  X  , and the viewing angle  X  are all uncertain variables with uncertainty margins. For clarity of illustration we do not plot these margins in the Figure. Such probability distribution of FOV parameters define the probabilistic FOV of the camera. It can be seen that for any POI p appearing in the query FOV, there must exist a probabilistic FOV instance (at q i which contains p . In other words, the union of all pFOV instances must cover all possible candidate POIs. This ob-servation leads to a pFOV culling algorithm, which relies on the following Lemma.
 Lemma 1: All candidate POIs whose locations are outside the union of all pFOV instances can be discarded.
A sample process of pFOV culling is given in Figure 2, F igure 2: Illustration of pFOV culling and candidate im-ages/POIs. The blue markers indicate the geotagged loca-tions of the images { s i } , while the red ones { p i } indicate their respective POIs. The circular sectors centered at q , q and q  X  X  illustrate the pFOV instances of q . where POIs located in the shaded region can be culled. Thus, POIs p e , p f can be culled, because their actual FOVs cannot be any pFOV instance for q . However, p d cannot be culled because it is within the reach of a possible pFOV instance ( q  X  ), even when it is outside the query box.
Given the current camera parameters the pFOV culling algorithm can quickly determine the candidate POIs which may possibly appear in the image. As a result, the cost of subsequent evaluation of geo-relevance and SC-similarity can be reduced significantly.
Now let us evaluate the probability that a POI p is cap-tured by an exact FOV. As shown in Figure 1, given an exact FOV at q and a point p whose distance to q is d and its viewing angle (the angle between  X  X  and to see that p has a higher probability to be captured by the camera if d is smaller or angle  X  is smaller. Specifically, we model the probability distribution function of POI p being captured by an exact FOV at q as the following function: where d =  X  pq  X  .  X  1 and  X  2 are parameters which ensure that P (  X , d ) is a negligible value (  X  0) if p is outside the FOV region. Thus, the probability of a POI p captured by a query FOV is a cumulative distribution function given by where Q is the circular region for the Gaussian distribution of the camera location with radius r , c &gt; 0 is a factor simu-lating the 2D uncertainty of the angular parameters, namely  X  and  X  d . In practice, the geo-relevance is evaluated in a dis-cretized form of Equation 2. This is omitted to save space.
Given a list of candidate images C , we consider each of their respective visual feature vectors as a basis signal, and employ the sparse coding technique to obtain a linear com-bination of the basis vectors, which maximally reproduces the original signal (the feature vector of the original image).
To compute the visual similarity, each candidate image is represented as a bag-of-visual-words column vector. Let D be the matrix where each column is the vector for each can-didate image in C . Then the problem can be described as: Given a query image x , can we represent it as a linear com-bination of other candidate images(columns in D ). This is a typical problem of sparse coding and the objection function is given by where  X  controls the sparsity in w and  X  w  X  1 is the l 1 of the parameter vector. Furthermore, the original query image x can be reconstructed by multiplying D by the sparse code w .

This optimization problem is called Lasso and can be solved by the least angle regression(LAR) approach [2]. The optimization result is a sparse code as w = (  X  0 ,  X  1 , ...,  X  where  X  i indicates the contribution of the i th candidate image to reproduce the query image. Weight value  X  i is then used as the visual similarity between candidate image i and the query. Finally, these weight values are aggregated by different POIs, so that each POI receives the sum of all the weights from its associated candidate images. The aggregated weights are called sparse coding similarity (SC-similarity) for each POI.

So far, we have proposed the method of acquiring visual similarity between a query image and a certain candidate image via sparse coding (SC). However, the solution to SC is typically time-consuming when the number of columns of D is large.

To improve the efficiency without compromising the SC effect, we conduct an Approximate Nearest Neighbor (ANN) filtering procedure on the candidate columns before conduct-ing sparse coding. ANN is able to retrieve the top-k approx-imate nearest-neighbours at significantly reduced cost com-pared to exact NN search in high dimensions. Since the SC process aims at reconstructing the query image with similar ones in D , columns which are dissimilar to the query mostly do not contribute in the reconstruction. Therefore, the rea-son for using ANN is to discard the dissimilar columns before invoking the expensive SC process.
Finally, for each candidate POI, we compute a vote score as a linear combination of the geo-relevance and SC-similarity of each candidate POI score =  X   X  geo -relevance + (1  X   X  )  X  SC -similarity (5) where 0  X   X   X  1 is a balancing factor. The POI with the top ranking score is taken as the final result. Image Dataset Given the same availability of image data, our framework is expected to perform better in sparsely populated regions, as the geo-relevance and visual similarity could be more discriminative. Therefore, we test the frame-work for urban area only. We download from Flickr 151,193 F igure 3: Noisy dataset downloaded from Flickr. The red frames indicate the  X  X oisy X  images, which do not show out-door appearance of the places.
 g eotagged images containing 2,256 distinct POIs in a densely populated area of a large city. The POI information are ac-quired from other sources. Although the dataset has limited size, it can sufficiently simulate the density of photos in ur-ban areas of the world. A larger dataset (spanned over a greater region) is not expected to impact the recognition performance, due to the locality introduced by the spatial range query. It is also important to note that the majority of the dataset are noisy images which do not capture the outdoor appearances of the POIs, as depicted in Figure 3. Query Set We develop an Android APP to capture query photos of the POIs in the dataset. The APP records the camera FOV parameters at the shooting time. The true POI of each query image (ground truth) is determined by the user when the photo is captured. We employ 4 users who shoot totally 170 POIs, including 48 landmarks and 122 non-landmarks. Each POI is captured for 4 times by different users. The results presented below are averaged among the users.
 Baseline Approach Besides our proposed framework KC , we also implement a state-of-the-art approach bag-of-visual-words( BOW ) as a baseline.
 Performance Results
Table 1 shows the results of computational costs with pFOV culling and ANN filtering enabled or disabled. It can be seen that pFOV culls a significant number of images from the original query box. ANN can further reduce the number of candidate images (columns) by almost an order of magnitude. Therefore, these two techniques can effectively reduce the query processing cost.

The POI recognition accuracy of the proposed approach is illustrated in Figure 4, where we vary the spatial query box size and plot the results of different  X  values. When  X  = 1, the KC scheme degenerates to a pure spatial method. For comparison, we also plot the accuracy of the BOW. It can be seen that (i) The pure spatial method alone does not per-form well; (ii) As the query box expands, the accuracy of all schemes is significantly improved (when box size &lt; 120m) because more relevant photos are added into the candidate image set. However, as the query box size increases further, the accuracy of pure SC-similarity (  X  = 0) declines consider-ably. This is a clear indication that the recognition problem becomes harder as the search space increases. A more ap-parent decline can be observed for the BOW method, due to the same reason. Fortunately, the problem in SC-similarity can be compensated by the geo-relevance in our scheme, as when  X  = 0 . 5, the accuracy is very stable and remains to be above 90%. Such results confirm the effectiveness of our spatio-visual ranking score.
We presented the Knowing Camera framework for real-time recognizing places-of-interest in smartphone photos. The framework captured the device uncertainty in a proba-bilistic field-of-view model, which could be used to cull can-didate images from the database. The visual similarity com-putation of the candidate images relied on the sparse coding technique. We also performed an ANN filtering technique to speedup the sparse coding. The final ranking score com-bined the geometric relevance and the visual similarity. Our experiments in an urban area showed promising results. The work is supported by the National Science Foundation of China (GrantNo. 61170034). [1] Y. S. Avrithis, Y. Kalantidis, G. Tolias, and E. Spyrou. [2] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. [3] X. Li, C. Wu, C. Zach, S. Lazebnik, and J.-M. Frahm. [4] M. Z. Zheng, Yan-Tao, Y. Song, H. Adam,
