 Convolutional neural networks ( CNN s) have achieved significant improvements for query classi-fication. CNN s capture the correlations of spatial or temporal structures with different resolutions using their temporal convolution operators. A pooling strategy on these local correlations extracts invariant regularities.

However, CNN s use simple linear operations on n -gram vectors that are formed by concatenating word vectors. The linear operation together with the con-catenation may not be sufficient to model the non-consecutive dependency and interaction within the n -grams. For example, in the query  X  X ot a total loss X , nonconsecutive dependency  X  X ot loss X  is the key information that is not well addressed by the lin-ear operation with simple concatenation.

In this paper, we propose to use deep long-short-term-memory ( DLSTM ) based feature mapping to capture high order nonlinear feature representations. LSTM (Hochreiter and Schmidhuber, 1997) is one type of recurrent neural networks ( RNN s) that have achieved remarkable performance in natural lan-guage processing and speech recognition (Sutskever et al., 2014; Graves et al., 2013).

The DLSTM is a stack of LSTM units where dif-ferent order of nonlinear feature representation is captured by LSTM units at different depth. The bot-tom LSTM unit extracts the first order feature repre-sentation from current word. The LSTM unit at the higher position captures the higher order feature rep-resentation relying on the outputs from LSTM units at lower position, specifically, the memory cell from lower LSTM unit at previous word position and the hidden output from lower LSTM unit at current word position. Using DLSTM , linear feature mapping in traditional CNN can be obviously extended to non-linear feature mapping. Moreover, the memory cell together with different gates in LSTM unit are able to model the nonconsecutive feature interaction and information decaying based on context. For exam-ple, in the query  X  X ot so good X , the proposed DL -STM is expected to keep the information of  X  X ot X  and  X  X ood X  in the memory, and to decay the information about  X  X o X  via the forget gates.

Similar to CNN s where multiple convolution oper-ations are used, we propose to stack different DLSTM feature mappings together to model multiple level nonlinear feature representations. The bottom DL -STM layer takes the original word sequence as input. The DLSTM layer at lower position fed its output to the adjacent higher DLSTM layer. In the proposed models, the concatenation of the multiple level fea-ture representations are further reduced by the pool-ing operation. The prediction output is finally made based on the reduced feature representations.
We evaluated the proposed method on three benchmark data sets: Standford Sentiment Treebank dataset (Socher et al., 2013), TREC (Text Retrieval Conference) question type classification data set (Li and Roth, 2002) and ATIS (Airline Travel Informa-tion Systems) dataset (Hemphill et al., 1990). On Standford Sentiment Treebank dataset, our model obtains 51 . 9% accuracy on fine-grained classifica-tion and 88 . 7% accuracy on binary classification. The SVM based method uses a large amount of engi-neered features, and it outperforms LSTM and RNN based methods on TREC question type classification dataset. The DLSTM outperforms other neural net-work based methods without using engineered fea-tures. On ATIS data, DLSTM achieves 97 . 9% F1 score, which is better than the previous best F1 score of 95 . 6% using the same data settings. Deep neural networks (Bengio, 2009; Deng and Yu, 2014; Hinton et al., 2006) dominates natural lan-guage processing (Socher, 2012; Collobert et al., 2011; Gao et al., 2014). They have achieved cutting-edge performance in various tasks such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), machine translation (Bahdanau et al., 2014; Cho et al., 2014; Jean et al., 2015), slot fill-ing (Yao et al., 2014a; Shi et al., 2015a) and syn-tactic parsing (Wang et al., 2015; Collobert et al., 2011). For query classifications, recurrent neural networks ( RNN s) and convolutional neural networks (
CNN s) have emerged as top performing architec-tures (Zhang and Wallace, 2016; Kim, 2014; Kalch-brenner et al., 2014; Ravuri and Stolcke, 2015a).
Due to its superior ability to memorize long dis-tance dependencies, LSTM s have been applied to extract the sentence-level continuous representation (Ravuri and Stolcke, 2015a; Tang et al., 2015; Tai et al., 2015). When the LSTM is applied to model a sentence, memory cell from the ending word in the sentence carries the information of the whole sen-tence. The LSTM hidden vector from the ending word is directly used as sentence feature represen-tation in (Ravuri and Stolcke, 2015a). Alternatively, a sentence is represented by the average of LSTM hidden vectors from its words (Tang et al., 2015). Inspired from recursive neural networks (Socher et al., 2011a), LSTM is further combined with a tree structure to model sentence representation (Tai et al., 2015).

CNN s have been originally developed for image processing (Lecun et al., 1998). They are firstly ap-plied by Collobert et al. (2008; 2011) for natural lan-guage processing tasks using max-over-time pool-ing method to aggregate convolution layer vectors. CNN s have also been applied to spoken language un-derstanding (Shi et al., 2015b), information retrieval (Shen et al., 2014) and semantic parsing (Yih et al., 2015). Kalchbrenner et al. (2014) proposed to ex-tend CNN s max-over-time pooling to k -max pooling for sentence modeling. Remarkable query classifi-cation performance on different benchmark datasets have been achieved by integrating CNN s with differ-ent feature mapping channels and pre-trained word vectors (Zhang and Wallace, 2015; Kim, 2014). Re-cently, Mou et al. (2015) proposed to model sen-tences by tree structured CNN s.

CNN s and LSTM s are complementary in their modeling capabilities; CNN s are good at capturing local invariant regularities and LSTM s are good at modeling temporal features. The combination of CNN s and LSTM s achieves improved performances in speech recognition (Sainath et al., 2015) and query classification (Tang et al., 2015; Zhou et al., 2015). In these models, the basic architecture is the LSTM that models sequence representation from lo-cal features captured by CNN s.

Different from the above methods, our method use LSTM units to model the nonlinear and non-consecutive local features. CNN s are placed on top of these local features for query classification. Our motivation is to use LSTM replace the linear feature mapping in convolution operation where the feature mapping is a multiplication of the word vectors with a filter matrix. So our proposed model is still CNN based model but using DLSTM as feature mapping for convolution operation.

Our work is closely related to tensor product based CNN s (Lei et al., 2015) that expand CNN fea-ture representation capacity with non-consecutive n -grams. They improve the query modeling from two aspects. Firstly, tensor products enable the non-linear feature vector interactions between adjacent words. Secondly, an exponentially decaying weight is applied to represent non-consecutive n -gram fea-tures. Instead of using tensor products as feature mapping, we propose to apply DLSTM to address these two aspects. Nonlinear feature mapping can be achieved by the DLSTM that equipped with nonlin-ear activation function. The nonconsecutive feature interaction is well addressed by the memory cell and different gates in LSTM unit. In particular, the for-get gate is able to decay the information according to the context rather than a fixed decaying weight in tensor product based CNN s. 3.1 Linear Feature Mapping in CNN Let k -dimensional vector x t  X  R k be the continu-ous feature representation of the t th word in a sen-tence. A sentence with l words is represented by x word vectors. The traditional CNN (Collobert et al., 2011; Kim, 2014) takes such sentence feature vector as input.
 volution operation to map each n -gram feature vec-where b j is the bias in filter j .

The resulting feature vector c t , j are often passed through non-linear element-wise transformations (e.g. the hyperbolic tangent and rectifier linear unit) as well as pooling operations. After aggregation or reduction by different pooling operations such as the max-over-time pooling (Collobert et al., 2011; Kim, 2014) and the average pooling (Lei et al., 2015), a constant dimensional feature vector is generated for sentences with various lengths.

In traditional CNN s, the concatenated word vec-tors are mapped linearly to feature coordinates as shown in Equation (1). Such linear feature mapping can be improved from the following two aspects, one is to extend linear mapping to nonlinear mapping. The other one is to improve the consecutive feature mapping to nonconsecutive feature mapping. For example, in the query  X  X ot a total loss X ,  X  X ot loss X  is the key sentiment. By using nonconsecutive fea-ture representation, the information about  X  X ot loss X  could be addressed. Lei et al. (2015) extends the linear feature mapping to tensor based feature map-ping. To model the nonconsecutive n -grams, a de-caying weight is applied to control the information carryover. In this paper, we propose to replace the linear feature mapping using DLSTM that captures the nonlinear and nonconsecutive feature interaction within n -grams. Rather than setting a fixed decaying weight, the proposed architecture is able to control the information decaying according to the context information. 3.2 Feature Mapping Based on Deep Long Figure 1 gives the basic architecture of a three-order nonlinear feature mapping in DLSTM . The bottom LSTM 0 extract the first order information from word input vector x t . It is equipped with input gate and output gate. The input gate automatically controls the information saving in memory cell that will be passed to higher order LSTM unit. The output gate modifies the information from the memory cell to represent current word.
On top of the bottom LSTM 0 unit, we analogously stack two LSTM units LSTM 1 and LSTM 2 to extract nonlinear feature representations from bigram and trigram, respectively. The LSTM j is formulated as follows: i
Due to the effect from different gates that con-trols the information saving, expressing and decay-ing, LSTM 1 and LSTM 2 are able to model the non-consecutive interaction in n -grams. Take  X  X ot so good X  as a example. LSTM 0 extract the nonlinear feature mapping from word  X  X ood X  as h 0 , 2 . The LSTM 1 takes c 0 , 1 (carries the information from word  X  X o X ) and h 0 , 2 as input. Due to the effect of forget gate, we expect the output h 1 , 2 from LSTM 1 to ad-dress more on word  X  X ood X  rather than  X  X o X . By further stacking LSTM 2 , information about the word  X  X ot X  and  X  X ood X  should be emphasized by the pro-Note the sum of the resulting outputs from these LSTM units is used as the high order feature rep-resentation of a n -gram ending with word x t . So the original sequence input x 0: l  X  1 is mapped to a sequence of feature vector z 0: l  X  1 = [ z 0 , z 1 ,..., z
The proposed DLSTM architecture is character-ized by the following two features: 1. Weight Sharing: LSTM 1 and LSTM 2 are iden-2. Memory Cell Interaction: To model the nonlin-To stack the LSTM unit deeper, the depth-gated LSTM (Yao et al., 2015) and the highway network (Srivastava et al., 2015; Zhang et al., 2015) also al-low the memory cell flow across LSTM units at dif-ferent depth. There are three basic differences be-tween these architectures with the proposed DLSTM . Firstly, in their architectures, LSTM units at different depth are different LSTM s that have different weight matrices. In our model, the LSTM units in DLSTM share weight matrices with each other. Secondly, in their proposed architecture, the memory cell is car-ried over to higher LSTM unit for facilitating model training. Because the networking training becomes more difficult with increasing model depth. In our DLSTM , the LSTM unit at higher position takes the memory cell from lower LSTM unit mainly for fea-ture interaction in n -grams. Finally, an additional  X  X epth X  gate is applied in their architecture to con-trol the information flow across different layers. In our model, the input gate in higher LSTM unit con-trols the interaction between the memory cells ex-tracted from previous word and current word. 3.3 The Architecture Figure 3 gives the whole architecture of the pro-posed query classification system. A DLSTM layer first maps the input sequence to a sequence of high order nonlinear feature representations z 0 . Instead of being directly used for query classification, the fea-ture representation z 0 is further processed by a stack of DLSTM layers illustrated in previous section. In such stacked DLSTM layers, the output z i of the i th DLSTM layer, is used as the input for the i + 1th DL -STM layer parameterized by a different set of weight matrices. As shown in Figure 3, the resulting fea-ture representations z 0 , z 1 ,..., z d of all these layers are concatenated. Finally, an average pooling is ap-plied to reduce the sentence feature representation to a fixed dimensional vector that is further fed to a softmax function to obtain the prediction output. 3.4 Learning and Regularization In the classification layer, the prediction output is obtained by the following softmax function. where y is a m -dimensional vector. The model is trained by minimizing cross-entropy on the given training data set. To avoid overfitting during train-ing, L 2 regularization and dropout (Hinton et al., 2012) are used. The L 2 regularization is applied to constrain all weight matrices using the same regu-larization weight. The dropout is only applied to the output of each DLSTM layer.

In the training, the model weights are updated using mini-batch stochastic gradient descent ( SGD ). We adapt a per-feature learning rate control method (AdaGrad) (Duchi et al., 2011) to dynamically tune the learning rate as follows: where  X  t , i is the learning rate for weight i at epoch t .  X  j = 1 g j , i sums all the historical gradients of weight i . A small positive  X  is applied to make the AdaGrad robust.  X  is usually set to 1 e  X  5. 4.1 Datasets We evaluate the proposed query classification mod-els on sentence sentiment classification, question type categorization and query intent detection tasks.
For sentence sentiment classification, the Stan-ford Sentiment Treebank (Socher et al., 2013) is used. In this dataset, 11855 English sentences are annotated at both sentence level and phrases level with fine-grained labels (very positive, positive, neu-tral, negative and very negative). We use the pro-vided data split, which has 8544 sentences for train-ing, 1101 sentences for developing and 2210 sen-tences for testing. This dataset also provides a bi-nary classification variant that ignores the neutral sentences. The binary classification task in this dataset has 6920 sentences for training, 872 sen-tences for developing and 1821 sentences for test-ing. There are in total 17835 unique running words for fine-grained dataset and 16185 for binary version dataset.

For query intent detection, ATIS (airline travel in-formation system) dataset (Hemphill et al., 1990; Yao et al., 2014b) is used. This dataset is mainly about the air travel domain with 26 different intents such as  X  X light X ,  X  ground s ervice  X  and  X  X ity X . There are 893 utterances for testing (ATIS-III, Nov93 and Dec94), and 4978 utterances for training (rest of ATIS-III and ATIS-II). There are 899 unique run-ning words and 22 intents in the training data.
The question type classification task is to clas-sify a question into a specific type, which is a very important step in question answering system. In TREC (Text Retrieval Conference) data (Li and Roth, 2002), all the questions are divided into 6 categories, including  X  X uman X ,  X  X ntity X ,  X  X ocation X ,  X  X escription X ,  X  X bbreviation X  and  X  X umeric X . The dataset in total has 5952 questions, 5452 of them for training, the rest for testing. The vocabulary size of TREC dataset is 9592.

Following previous work (Iyyer et al., 2015; Tai et al., 2015; Lei et al., 2015), we used word vectors pre-trained on large unannotated corpora to achieve better generalization capability. In this paper, we used a publicly available 300 dimensional GloVe word vectors that are trained using Common Crawl with 840B tokens and 2.2M vocabulary size. 4.2 Settings We implemented our model based on Theano library (Bastien et al., 2012). All our models are trained on Nvidia Tesla K40m.

We performed extensive hyperparameter selection based on Stanford Sentiment Treebank Binary ver-sion of validation data. The selected hyperparame-ters were directly used for all datasets. To investi-gate the robustness of the proposed method, we ran each configuration 10 times using different random initialization (random seed ranges from 1 to 10).
For final models, we set the initial learning rate to 0 . 1, L 2 regularization weight to 1 e  X  5, the dropout probability to 0.5 and mini-batch size to 64. We use hidden layer size 256 for all the models described in model Fine Binary SVM (Lei et al., 2015) 38.3 81.3 Nbow(Lei et al., 2015) 44.5 82.0 Para-vec(Le and Mikolov, 2014) 48.7 87.8 DAN(Iyyer et al., 2015) 48.2 86.8 RAE(Socher et al., 2011b) 43.2 82.4 MVRNN(Socher et al., 2012) 44.4 82.9 RNTN(Socher et al., 2013) 45.7 85.4 DRNN(Irsoy and Cardie, 2014) 49.8 86.8 RLSTM(Tai et al., 2015) 51.0 88.0 CLSTM(Zhou et al., 2015) 49.2 87.8 DCNN(Kalchbrenner et al., 2014) 48.5 86.9 CNN-MC(Kim, 2014) 47.4 88.1 CNN-nostatic(Kim, 2014) 48.0 87.2 TCNN (Lei et al., 2015) 50.6 87.0
TCNN+phrases(Lei et al., 2015) 51.2 88.6 ours 49.2 87.2 ours+phrases 51.9 88.7 the experiments. The number of the DLSTM layers and the number of the LSTM units in each DLSTM are both set to 3. So basically there are 9 LSTM units are used for each word position.

For all models, we set maximum iteration num-ber 100 to terminate the training process. For sen-timent classification task, during the training, the model with the best classification accuracy on val-idation data was used as final model for testing. For question type classification and query intent detec-tion, there wasn X  X  validation data. So we simply use the model trained at the 100th iteration as the final model for testing. 4.3 Results on Stanford Sentiment Treebank
Table 1 lists results for sentiment classification. There are four blocks in the table. The bottom block gives the results from our model. The third blocks are methods related to CNN s. The second block shows the results from recursive neural net-work based approaches. The other baseline methods are listed in the top block.

The top block shows that the traditional methods such as SVM using ngram features and neural net-work using bag-of-words features ( Nbow ) perform much worse than Para-vec and DAN using word vectors that are pre-trained on large amount of un-labeled data. Para-vec builds a logistic regression on top of paragraph vectors. DAN is a deep neural network takes the average of word vectors as input.
In addition to pre-trained word vectors, syntac-tic compositional information can be used to im-prove the sentiment classification accuracy. RAE is a tree structured Antoencoder model based on pre-trained word vectors from Wikipedia. MVRNN fur-ther improves the recursive neural network by as-signing each node with a matrix to learn the meaning change of neighboring words and phrases. To ad-dress large amount of different vectors and matrices involved in MVRNN , RNTN proposed to use one single tensor based function to model all nodes. By making the tree-structured recursive neural networks deeper, significant improvement has been achieved by DRNN . According to our knowledge, the best compositional information based model is achieved by RLSTM that combines LSTM unit with tree-structure.

By comparing the classification accuracy between second blocks and third blocks, we see that CNN based models in general perform better than recur-sive neural network based methods. Another advan-tage of CNN based methods is that they can be gen-eralized to any language without dependency over compositional information. DCNN uses a dynamic k -max pooling operator function in CNN . To explore the task specific word vectors and the general word vectors pre-trained on large News dataset, CNN-MC equips CNN with two feature mapping channels. CNN-nostatic gives the results by only making use of general word vectors. The best published classi-fication results are achieved by TCNN that is tensor based CNN .

In this paper, the proposed method is closely re-lated to TCNN . Instead of using tensor products to replace linear convolution operation, our method ex-ploits the nonlinear feature mapping through DL -STM . Rather than setting specific decaying weight to model non-consecutive n -gram features in tensor based CNN , the different gates automatically adjust the information storing, removing and outputting ac-cording to context.

Following the work of TCNN , to leverage the phrases level annotation in Standford Sentiment Treebank, all phrases and their corresponding labels are added to training data as additional sequences. The bottom line of Table 1 shows that our models achieved the state-of-the-art performance on senti-ment classification task.

For the best settings described above, we ran each model 10 times with different random initialization. The average and standard deviation for fine-grained classification are 50 . 7% and 1 . 04%, for binary clas-sification 88% and 0 . 41%. Comparing with TCNN , our model is more sensitive to the parameter random initialization. In the future, some efforts should be used to analyze and address this issue. 4.4 Results on ATIS ATIS dataset is widely used to test spoken language understanding system. As shown in Table 2, SVM using n -grams performs better than simple RNN and CNN based approach. joint-RNN is a query classi-fication and slot filling joint training model where CNN is applied on top of slot tagging RNN for query classification. In this way, joint-RNN actually im-plicitly makes use of slot tag information for query classification. However, joint-RNN doesn X  X  take ad-vantage of word vectors trained on large amount of unlabeled data. Based on pre-trained word vectors, our models obtain more than 2% absolute classifica-tion accuracy improvement over the published best model.

In ATIS data, about 70% of queries is categorized to  X  X light X  intent. Recent work using RNN for ut-terance classification (Ravuri and Stolcke, 2015b; Ravuri and Stolcke, 2015a) simplifies it to a  X  X light X  VS  X  X thers X  binary classification task. In their paper, using word based LSTM , they achieve 97 . 55% clas-sification accuracy. By using extra name entity fea-tures, word based gated RNN obtains 98 . 42% classi-fication accuracy. 4.5 Results on TREC Question Type Table 3 gives the TREC question type classification accuracy of our models with other baseline mod-els. Different from the sentiment classification task, the shallow models using diverse engineered feature performs better than CNN and LSTM based models. Previous best classification results on TREC data is achieved by SVM using unigrams, bigrams, wh-word, head word, POS tags, hypernyms, WordNet synsets and a bunch of hand-coded rules.

AdaSent is a self adaptive hierarchical sentence model based on gating networks with level pooling. As shown in Table 3, CNN and LSTM achieve similar performances on question type classification. Re-cently CLSTM achieves substantial improvement over previous neural network based methods. In CLSTM , CNN is used to extract high level phrase representation. Such local segment representation is fed into LSTM to model whole sequence representa-tion. Different with CLSTM that is an LSTM based sequence model with CNN for local feature extrac-tion, our model is CNN based model using DLSTM for non-linear feature mapping. Our model outper-forms previous neural network based models with-out relying on task specific feature engineering. 4.6 Deep Architecture One critical hyperparameter in the proposed method is the number of DLSTM layers. On sentiment bi-nary classification task, we run our model 10 times by keeping all the hyperparameters the same except the number of DLSTM layers using different random initialization. As observed from Figure 3, the bet-ter performance is achieved by deeper architecture. Our model achieves the best classification result by stacking 3 DLSTM layers that actually leverages 9 different LSTM units to extract the nonlinear feature from n -grams. 4.7 Examples Figure 4 demonstrates some examples and their sen-timents predicted by our model trained on fine-grained classification data. In order to see how the nonlinear feature mapping captures the sentiment at each word position in the query, we follow the strategy used in (Lei et al., 2015) where the soft-max function is directly applied on the concatenated feature mapping without passing through the aver-age pooling layer. So the sentiment distribution p t at t th word is computed as p t = W T [ z 0 t , z 1 t The expected value over the probability distribution  X  s =  X  2 s . p t is used as the sentiment score that is plot-ted in Figure 4. In the figure, the sentiment score ranges from  X  2 to 2, where  X  2 means very nega-tive, 2 mean very positive and 0 means neutral.
Five examples are illustrated in the figure where the first row gives the synthetic examples to show that our model is able to model the nonconsecutive interaction within n -grams. For example, in query  X  X ardly to be bad X , even though word  X  X ardly X  is not directly modifying word  X  X ad X , our model still be able to capture such sentiment changes.

The second row of the figure shows the examples from fine-grained classification testing data. Both the example show that our model to some degree can capture sentiment of the satire. Especially the last example, our model actually gives negative pre-diction, even no word in the query really means neg-ative. We have proposed a deep long-short-term-memory (
DLSTM ) nonlinear nonconsecutive feature mapping architecture to replace traditional linear mapping in the convolutional neural network based query clas-sification. Each LSTM unit in the DLSTM is respon-sible for capturing different order feature represen-tation from word segments. The bottom LSTM unit equipped with input gate and output gate, extracts the nonlinear feature from unigram. The higher LSTM unit in the DLSTM takes the outputs from lower LSTM units as input. In such way, the higher LSTM unit is able to capture nonlinear feature rep-resentation from higher order n -grams. The sum of different LSTM units is used as the output of the DL -STM layer. The DLSTM output rather than being di-rectly used as input to convolutional neural network for query classification, is passed through a stacked DLSTM layers. The query is finally represented by the concatenation of the outputs from the stacked
We evaluated the proposed models on three benchmark datasets X  X tanford Sentiment Treebank dataset, TREC dataset and ATIS dataset. On both sentiment classification dataset and ATIS dataset, our model achieved the state-of-the-art performance. On TREC question type classification, SVM based model using extra engineered features still per-formed better than our model. But we noticed that the proposed method outperformed all the other neu-ral network based approaches.

