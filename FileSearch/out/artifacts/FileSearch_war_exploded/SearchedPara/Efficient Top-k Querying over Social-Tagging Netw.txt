 Online communities hav e become popular for publishing and search-ing content, as well as for finding and connecting to other users. User-generated content includes, for example, personal blogs, book-marks, and digital photos. These items can be annotated and rated by different users, and these social tags and derived user-specific scores can be leveraged for searching relevant content and discov-ering subjectively interesting items. Moreover, the relationships among users can also be taken into consideration for ranking search results, the intuition being that you trust the recommendations of your close friends more than those of your casual acquaintances.
Queries for tag or keyword combinations that compute and rank the top-k results thus face a large variety of options that complicate the query processing and pose efficiency challenges. This paper ad-dresses these issues by developing an incremental top-k algorithm with two-dimensional expansions: social expansion considers the strength of relations among users, and semantic expansion consid-ers the relatedness of different tags. It presents a new algorithm, based on principles of threshold algorithms, by folding friends and related tags into the search space in an incremental on-demand manner. The excellent performance of the method is demonstrated by an experimental evaluation on three real-world datasets, crawled from deli.cio.us, Flickr, and LibraryThing.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.3 [ Information Search and Retrieval ]: Search Process Performance, Experimentation social networks, scoring and ranking, top-k query processing
The advent of Web 2.0 has changed the way users interact with the Internet. Publishing content has never been easier, and new on-line services offer not only the possibility to store personal content, but also to share it with other people and to explore other users X  contents. Examples of social-community platforms are del.icio.us, Flickr, LibraryThing, LinkedIn, MySpace, Facebook, and YouTube.
While differing in the type of content that they focus on (e.g., blog entries, photos, videos, books, bookmarks), most community platforms follow a common pattern. Users must register in order to join the community. Then they produce information, ideally by publishing their own documents 1 and by adding tags or ratings or comments to other content already available in the community. The platforms also offer a way to maintain a list of friends and means to keep friends informed about your latest content items. The size of your friend network is often considered as an indication of high reputation in the network. Many users initially populate their lists of friends with people they already know from the offline world or other online communities, but over time they typically identify previously unknown users that they share common interests with and also add those users to their friends lists.

A key feature of social communities is the widely used opportu-nity to attach manually generated annotations, so-called tags ,to content items. Tags can provide precise descriptions of content items, flavored with the respective p ersonal interest of the user who generates the tag.

Social tagging offers an opportunity to exploit the  X  X isdom of the crowds X  by identifying valuable content that is recommended by friends -either directly or indirectly (i.e., via friends of friends) and explicitly (e.g., ratings) or implicitly (e.g., by intensive tag-ging). To this end, the relationships among users should be taken into account for ranking, the intu ition being that you trust the rec-ommendations of your close friends more than those of your casual acquaintances. This situation resembles the paradigm of collab-orative recommendation [19, 25], which applies data mining on customer-product and similar usage data to predict items that users are likely interested in. However, the fast growth of communities and the very high rate of content production and tagging efforts calls for highly efficient and scalable methods. Existing algorithms for fast Web search do not consider user relationships and the as-sets from social tagging, and prior methods for collaborative rec-ommendation do not provide t he throughput scal ability that one needs for running millions of daily ad-hoc queries in social com-munities such as Flickr -with more than 2 billion content items, many million users, a nd high dynamics.

This paper presents a solution to the above problem by develop-ing an efficient top-k algorithm for social search and ranking. The method is based on principles of top-k threshold algorithms over inverted lists of different types, with various novel techniques for the specific setting of large online communities. For leveraging the  X  X ocial wisdom X , the algorithm employs a new form of two-
For the remainder of this paper, we will use the familiar IR-jargon term  X  X ocument X , while actually referring to a more general notion of content items. dimensional expansions: social expansion considers the strength of relations among users, and semantic expansion considers the relat-edness of different tags. For efficiency, these expansions are per-formed incrementally on demand, by dynamically folding friends and related tags into the search space. The excellent performance of the method is demonstrated by an experimental evaluation on three real-world datasets, crawled from deli.cio.us, Flickr, and Li-braryThing.

The rest of the paper is organized as follows. Section 2 reviews related work and puts it into context. Section 3 introduces a graph model to represent the entities in a social community and their re-lationships. Section 4 presents ways of quantifying the strengths of different relationships, which in turn feeds into a model for com-puting scores of user-specific search results and recommendations. Section 5 explains the algorithm for efficiently processing queries in social networks. Section 6 presents experiments with three real-world datasets to study retrieval/recommendation effectiveness and, most notably, demonstrate the efficiency gains by the new query processing algorithm.
Social networks of the Web 2.0 style have received major at-tention in the recent literature, with focus on applying data min-ing methods on social relations and, most prominently, relations among tags. [16] provides an empirical study of the tagging behav-ior and tag usage in online comm unities. [ 21, 31] discuss methods for generating taxonomy-like relations among tags, so-called  X  X olk-sonomies X , based on statistical measures. Similar approaches have been applied to query-and-click logs, e.g., in [5], but none of this work considers social relations between different users. Identify-ing important and emergent tags and visualizing them in so-called  X  X ag clouds X  (and corresponding time series) has been extensively explored; recent work along these lines includes [14, 17, 32]. The dynamics of social relations among users (e.g., the rate of making friends) has been studied, for example, in [2, 24, 33].
As for the exploitation of social tags for information retrieval, [3] discusses the challenges of searching and ranking in social com-munities. Various forms of community-aware ranking methods have been developed, mostly inspired by the well-known PageR-ank method [10] for web link analysis. [22] proposes FolkRank for identifying important users, data items, and tags. [36] compares different methods for identifying authoritative users with high ex-pertise. [6] introduces SocialPageRank , to measure page author-ity based on its annotations, and SocialSimRank for the similarity of tags. [35] further extends this work by augmenting language models with tag similarities. [13] shows that explicit user tagging can help to improve precision of queries for Intranet search. The very recent work of [20] provides an empirical analysis of how so-cial bookmarking can influence Web search, with both positive and negative insights. None of this prior work considers the impact of user-friendship strengths on the scoring of search results, and the problem of efficient query processing in the presence of such  X  X o-cial wisdom X .

Aspects of user communities have also been considered for peer-to-peer search, most notably, for establishing  X  X ocial ties X  between peers and routing queries based on corresponding similarity mea-sures (e.g., similarities of queries issued by different peers). [8] has studied  X  X ocial X  query routing strategies based on explicit friend-ship relationships and behavioral affinity. [27] has developed an ar-chitecture and methods for  X  X ocial X  overlay networks that connect  X  X aste buddies X  with each other. [26] has proposed a community-enhanced Web search engine that takes into account prior clicks by community members. [11] has proposed the notion of Peer-Sensitive ObjectRank, where peers receive resources from their friends and rank them using peer-specific trust values.
There is ample literature on collaborative filtering for recom-mender systems (e.g., [1, 18, 29, 30]), for example, to predict movies or e-commerce items that customers are likely to buy or to identify news that news-feed subscribers are likely interested in. In a nutshell, these methods aim to learn user preferences from the collective behavior -like purchases or tagging -of an entire com-munity. Typically, statistical analysis and machine learning tech-niques are used offline for precomputations, and the actual run-time recommendations have limite d flexibility and cannot easily cope with high dynamics and ad-hoc interests of individual users (as expressed by an ad-hoc query). One of the notable exceptions is the recent work by [12] which addresses scalability issues when the number of users and items in a recommender system grows to many millions and both undergo fast changes. However, in con-trast to our  X  X ocial search X  theme, this prior work considers only the space of user-item pairs and there is no notion of user-specific tags or annotations on items. Thus , our setting requires search over a three-dimensional user-tag-item space, as opposed to the two-dimensional user-item space of the previous work on collaborative filtering.
The entities that occur in social networks can be cast into a com-mon graph model, representing the different elements of a social network and their mutual relationships. Figure 1 illustrates the graph, which forms the basis of our scoring model and query pro-cessing algorithm.

A node in the graph can be of one of the following types: it can either represent a user ,a tag ,ora document (data item). Addition-ally, social networks exhibit various relationships, both among the nodes of the same type and between nodes of different types; these are represented by edges in the graph and described below. Edges can be weighted in different ways, according to the applications built on top of the model. The following relations exist between nodes of the same type: Friendship(User1, User2, FriendshipStrength): Social networks allow users to maintain an explicit list of friends. The trust of one user in another user (as far as potential recommendations are concerned) is reflected in the FriendshipStrength. (The quantita-tive measures to this end are discussed in Section 4.) Additional means of establishing implicit friendships include, for example, a user subscribing to another user X  X  content (e.g., adding the other user X  X  books to your set of InterestingLibraries in LibraryThing) or subscribing to the same user group as another user expressing high overlap in thematic interests. Regardless of how direct friends are defined, we may consider the transitive closure of the Friend-ship relation or a bounded set of transitive connections (up to some distance).
 TagSimilarity(Tag1, Tag2, TagSim): As tags are freely selected annotations and there is often a natural diversity in the tag wordings used by different users in a social network, different tag words may express (near-)synonyms (e.g.,  X  X eline X  and  X  X at X ,  X  X eb_2.0 X  and  X  X ocial_Web X , etc.) or other kinds of semantically related con-cepts (e.g., hyponyms such as  X  X ogs X  and  X  X erman_shepherd X ,  X  X earch_engine X  and  X  X oogle X , etc.). The tag-usage statistics in the social network can be harnessed to derive TagSim similarities between tags (see Section 4 for quantitative measures). Linkage(Document1, Document2, Weight): In some applications, documents (data items) also exhibit relations among themselves. In the case of web pages, this linkage is obvious and given by the hyperlink graph, with weights often chosen proportionally to the outdegree of the pages. For other types of documents, different no-tions of links and weights need to be defined; conceivable options could include, for example, the geographic proximity of different photos when GPS information is available. The following relations exist between different nodes types: DocContent(Document, Tag, ContentScore): By annotating a document with a tag, users associate the two entities so that the tag should be viewed as an indicator of the document X  X  content. We consider a ContentScore associated with a document-tag pair to reflect how well that tag describes the document.
 Tagging(User, Tag, TagScore): Each tag is associated with each user who used it on at least one document; this captures the topics that the user is interested in. The TagScore reflects how intensively a tag has been used by one user.
 Rating(User, Document, RatingScore): in many social commu-nities, users can explicitly rate documents, which is captured by a rating score. Another naive instantiation of Rating is authorship of a content item, which (e.g., in the case of bookmarks) can be seen as an endorsement for the document.

Additional aspects can be considered as attributes associated with edges. Our model aims to capture all relationships that occur in social networks, but some of the above relationships may be unde-fined for specific networks. In del.icio.us, for instance, user inter-actions are mainly through bookmarking and tagging, whereas in Flickr, the vast majority of users has authored content (their pho-tos).

For the purpose of search result ranking and top-k query pro-cessing, we will mainly focus on the Friendship and Tag Similarity scores, as discussed next.
In line with the free-text tagging of the social communities, we consider a query Q ( u, q 1 ...q n ), issued by a query initiator u ,as a set of tags (keywords) q 1 ...q n , possibly with weights for each tag (which will be disregarded for the sake of presentation simplic-ity). Result documents should contain at least one of the query terms and be ranked according to a query-specific document score . In contrast to standard IR query models, our document scores also contain a social component: the content-based score of a document is additionally user-specific , i.e., it depends on the social context of the query initiator. This resembles the personalization approaches offered by some search engines, but social networks have the addi-tional asset of knowing friendship relations and the friends X  tagging behavior and are thus the most natural habitat to further explore and improve this idea.

Our social scoring model extends the traditional IR scoring mod-els (tf-idf-based, p robabilistic IR, language models) to search in social networks, with the following ingredients: (1) a measure for the importance of users, relative to the querying user, (2) a context-specific tag frequency relative to the querying user that reflects the relative importance of users which used a tag, and, optionally, (3) the expansion of query tags with related ( X  X emantically similar X ) tags. In the following, we denote by U the set of users, by D the set of documents, and by T the set of tags.

Friendship Similarity. The importance of a user, relative to the querying user, is quantified by the friendship similarity func-tion F u ( u ) .Wedefine F u ( u )=0 (as a user is not interested in getting recommendations to he r  X  X wn X  documents/bookmarks which she knows anyway), and we normalize the F values by set-ting u  X  U F u ( u )=1 for all users u  X  U . F u ( u ) may be viewed as the probability that a r andom document that was tagged by u will be interesting to u . The friendship similarity may be a combination of syntactic measures (like overlap of tag usage), social measures (like distance in the friendship graph), and global measures (like a PageRank-style importance of users computed on the friendship graph). In our implementation, we first compute an overlap-based similarity O ( u, u ) for directly connected users in the friendship graph ( direct friends) as the Dice coefficient of the sets of tags u and u used: We then extend this to users u and u that are indirectly connected by one or more friendship paths by aggregating similarities along each path and picking the path with highest similarity. To this end, similarities can be averaged, multiplied, or multiplied weighted by (linear or dampened) distance, etc. In the implementation, we use which favors users at small distances. Other types of direct friend-ship strength and aggregation over paths can be easily plugged into the model and our implementation.

As not all users are connected in the friendship graph, we addi-tionally use a uniform background model to assign a small, con-stant similarity also to unconnected users; so the final definition of friendship similarity is:
Social Frequency. To reflect the similarity of the users who tagged a document that may be of interest to the querying user, we introduce the notion of social frequency , which replaces the standard term frequency (tf) and considers friendship similarities. More formally, denoting by tf u ( d, t ) the number of times user u used tag t for document d ,wedefinethe social frequency sf for a tag t and a document d , relative to a user u ,as Note that tf u ( d, t ) is typically 1 (tagged once) or 0 (not tagged at all) in most of today X  X  social-tagging platforms, but it is conceiv-able that quantitative ratings are factored into this measure or user feedback leads to non-binary sf values. By plugging the definition of F u ( u ) into the above formula, we obtain sf u ( d, t )= showing us that the social frequency can be split into a global part u  X  U  X tf u ( d, t )=  X T F ( d, t ) that is independent of the querying user and corresponds to a weighted global term frequency TF ( d, t ) , and a user-specific frequency (the second sum) that de-pends on the friendship strengths of the querying user. We will make use of this decomposition for more efficient query processing in Section 5.

Social Score for Tags. To compute the score s u ( d, t ) of a doc-ument d with respect to a single tag t relative to the querying user u , we use a scoring function in the form of a simplified BM25 [28] score: where k 1 is a tunable coefficient (just like in standard BM25) and idf ( t ) is the inverse document frequency of tag t , instantiated as with df ( t ) denoting the number of documents that were tagged with t by at least one user. Unlike the original BM25 formula, our model has no notion of document lengths; the number of tags assigned to a document does not vary as much as the length of text documents.
Plugging the definitions of sf u ( d, t ) and F u ( u ) into the for-mula, we obtain s u ( d, t )= ( k 1 +1)(  X T F ( d, t )+(1  X   X  ) u  X  U | U | P u ( u ) tf u ( d, t )) k 1 +  X T F ( d, t )+(1  X   X  ) u  X  U | U | P u ( u ) tf u ( d, t ) As a special case, we can emulate socially agnostic global scoring, with just considering global tag frequencies TF ( d, t ) and idf ( t ) , by setting  X  =1 .

Tag Expansion. Even though related users are likely to have tagged related documents, they may have used different tags to de-scribe them. It is therefore essential to allow for an expansion of query tags to  X  X emantically X  related tags. A simple way to account for this would be to statically expand the query with a fixed num-ber of similar tags; however, experiments on text IR have shown that this can lead to topic drift and search results that are infe-rior to those of the unexpanded, original query [9]. Our scoring model therefore adopts the careful expansion approach proposed in [34] that considers, for the score of a document, only the best expansion of a query tag, not all of them. More formally, we in-troduce the tag similarity tsim ( t 1 ,t 2 ) for a pair of tags t 0  X  tsim ( t 1 ,t 2 )  X  1 . The final score s  X  with respect to a tag t and relative to a querying user u , considering tag expansion, is then defined as In our current implementation, the similarity between two tags is determined by the co-occurrence of the tags in the entire document collection by estimating conditional probabilities: where df ( t  X  t ) is the number of documents that have been tagged by both tags (but possibly by different users). Other measures such as SocialSimRank from [6] could be easily incorporated as well.
Social Score for Queries. Finally, the score for an entire query with multiple tags q 1 ...q i is the sum of the per-tag scores: Note that this score assumes a non-conjunctive query evaluation. However, it can easily be extended to conjunctive evaluation by setting s  X  u ( d, q 1 ...q n )=0 when at least one of the s
This section introduces the C ONTEXT M ERGE algorithm to ef-ficiently evaluate the top-k matches for a query, using the social-context score introduced in the previous section. This algorithm generally falls into the well-established framework of threshold al-gorithms over impact-ordered inverted lists [15, 4] for efficient top-k query processing. However, as the social score depends on the user who submits a query, it is impossible to precompute per-tag scores for each document and each user. Standard algorithms that rely on scanning inverted lists cannot be applied here. Instead, C
ONTEXT M ERGE makes use of information that is available in so-cial tagging systems anyway, namely lists of documents tagged by a user and numbers of documents tagged with tags. It incremen-tally builds social frequencies by considering users that are related to the querying user in descending order of friendship similarity, computes upper and lower bounds for the social score from these frequencies, and stops the execution as soon as it can be guaranteed that the best k documents have been identified.
C ONTEXT M ERGE makes use of four different kinds of prepro-cessed index lists that are built at indexing time and accessed mostly sequentially in descending order of scores at querying time. Addi-tionally, random accesses to look up the value of an item in a list are possible, but more expensive than sequential accesses in terms of access cost.
Figure 2 shows the general structure of our query processing framework. To compute the top-k results for a query q 1 ...q mitted by a user u ,C ONTEXT M ERGE sequentially scans, for all query tags, the DOCS lists and the USERDOCS lists of the friends of u in an interleaved way, maintains a list of candidate documents seen during the scans and a list of current top-k candidates, and terminates as soon as none of the candidates can move to the top-k . To improve efficiency, C ONTEXT M ERGE can additionally perform random accesses to the index lists to lookup the values for selected documents. Note that the algorithm can be further optimized if  X  is set to extreme values: For  X  =0 , no DOCS lists need to be opened as the execution can be limited to the context of u ; analogously, if  X  =1 , there is no need to consider any lists of friends, so just the DOCS lists are read and C ONTEXT M ERGE behaves like a standard top-k algorithm. However, the interesting case is for  X  between 0 and 1.
 Sequential Scans. To limit the number of disk accesses, the USERDOCS lists are opened only on demand, namely when the procedure C ONTEXT M ERGE (user u , query q 1 ...q n ,  X  ) end procedure score that can be read from a USERDOCS list is greater than the score from any DOCS list. The algorithm additionally scans, for each query term, the list FRIENDS(u) of u  X  X  friends to determine an upper bound for the score that can be read from the USER-DOCS list of the next friend for the query term. In each iteration of the main loop, C ONTEXT M ERGE does a batch of batchsize list accesses, where in each access a number of entries is read from the list that can contribute the highest score. As we do not keep precomputed scores in the list, but just frequencies, we need to compute the score contributions from each list at run-time. To do this, the algorithm maintains the last value high [ i ] read from each DOCS[i] list (which is set to the top value in the list upon initialization and updated when t uples are read from the list) and the last value highF [ i ] read from the FRIENDS list in each query dimension (i.e., original query tag). The upper bound for the next score read from the DOCS[i] list can be computed by evaluating s ( d, t ) with TF ( d, t )= high [ i ] and setting the user-specific part to 0. Analogously, the upper bound for the next USERDOCS list in dimension i (which could be read if the USERDOCS list for the next user in that dimension were opened) is computed as maximal term frequency of any user and any document for tag q The idf values needed to compute these scores are fetched once during the initialization of the execution.

The C HOOSE N EXT L IST method of the algorithm then greed-ily selects the list which has the highest expected score. If this is a FRIENDS list, the corresponding USERDOCS list is read com-pletely and the FRIENDS list in that dimension is advanced by one. Note that in most of today X  X  social-tagging applications, all docu-ments in the USERDOCS list will have the same tf value of 1, so all of them are read in one shot. If the next list is a DOCS list, a configurable number of entries is read from it (as DOCS lists are usually much longer than USERDOCS lists).

Candidate Management and Termination Test. When scan-ning the index lists, C ONTEXT M ERGE collects candidates for the query result and maintains them in two disjoint priority queues, one for the current top-k items and another one for all other candidates that could still make it into the final top-k . For each candidate d the algorithm maintains the following information: To compute the worstscore of a candidate, s u ( d j ,q ) is evaluated by setting TF ( d j ,q i )=0 for i  X  E ( d j ) and using uf ( d instead of the summation u  X  U P u ( u )  X  tf u ( d j ,q i in conjunctive evaluation, the worstscore of a candidate remains 0 until the document has, for each query tag q i , been seen in DOCS(i) (by sequential or random access) or in one of the USERDOCS lists read for q i .

To compute d j  X  X  bestscore, we assume TF ( d j ,q i )= high [ i ] for i  X  E ( d j ) (the current upper bound of the corresponding DOCS list). Additionally, we need to estimate the contribution C from users that have not yet been seen, and use the sum uf ( d in the social-context part of s u ( d j ,q ) . As the algorithm considers users in descending order of similarity, we know that for any user u that has not yet been considered for q i , P u ( u )  X  highF [ i ] . of the similarities of users already considered for q i and by maxtf the maximal tag frequency of any user for any document and tag in the collection (which is usually 1), we can estimate the remaining contribution as C =(1  X  mass i ) maxtf , because the P u values are normalized to a sum of 1. Moreover, if we additionally know the maximal number maxu [ i ] of users who tagged any document with q i , the maximal contribution from unseen users for d can be C =( maxu [ i ]  X  r ( d j ,q i ))  X  maxtf ( maxu [ i ] can be read during the initialization of the algorithm; the initial high value of the corresponding DOCS list is an upper bound for maxu [ i ] which is tight if maxtf =1 ).
 The bestscore estimation can be made more precise if TF ( d j ,q i ) is known, because we can replace maxu [ i ] in the for-mula by TF ( d j ,q i ) which often is much smaller. If  X &gt; 0 , i.e., if the algorithm scans the DOCS lists, we can replace maxu [ i ] by high [ i ] if TF ( d j ,q i ) is not yet known (if it was higher than high [ i ] , it would have already been read during the scan of the cor-responding DOCS list).

In addition, the following information is derived at each step: The algorithm can safely terminate, yielding the correct top-k re-sults, when the maximum bestscore of the candidate queue and the best score of any unseen document is not larger than min -k .Ad-ditionally, whenever a candidate in the queue has a bestscore that is not higher than min -k , this candidate can be pruned from the queue (which helps to keep the memory footprint of the execution low as unneeded candidates can be removed early in the process). To further limit the CPU overhead of testing the candidates, this test is only performed after a full batch of scan steps, and only en-abled after the bestscore of the unseen document d v is not greater than min -k .

Random Accesses. In addition to sequential accesses (SA) to the index lists, C ONTEXT M ERGE can also perform random accesses (RA) to the index lists to look up missing scores of candidates. However, it is not feasible to check all USERDOCS lists of not yet seen users for a document, as this would require to explore the full range of (potentially many thousands of transitive) friends. There-fore, the only type of RA applied by C ONTEXT M ERGE is RA to DOCS lists to look up the global term frequency of a document for a tag. This serves two purposes: first, it can reduce the gap between the document X  X  worstscore and bestscore because TF is then known exactly for one more tag; second, the estimation of the score contribution from the remaining friends gets more precise (as TF ( d, t ) can be used in the estimation instead of maxtf ( t ) ).
As RA are much more expensive than SA (in the order of 100 to 1,000 times for real systems), they have to be carefully selected and scheduled to avoid any unnecessary work. Our scheduling for RA follows the LAST heuristics from [7]: our algorithm performs only SA until the estimated cost to perform all RA to remaining candidates is at most as high as the cost for all SA done so far. We estimate the number of RA by summing up, for all candidates, the number of query dimensions (i.e., original query tags) that have not yet been evaluated.
Tag expansion adds another dimension that C ONTEXT M ERGE needs to combine with the user-expansion dimension. Concep-tually, we add, for each similar tag t ij of a query tag q DOCS [ i ][ j ] list (which corresponds to DOCS ( t ij ) and a list FRIENDS [ i ][ j ] of similar users. The score for these lists is com-puted like the score for lists wit hout tag expansion, but additionally multiplied with tsim ( q i ,t ij ) . Candidate documents now maintain, for each query tag q i , the information shown above not only for q itself but also for each similar tag t ij . Following the max-semantics of our tag-expansion score, we can now estimate worstscores and bestscores of candidates as the maximum worstscore and bestscore for each similar tag, again weighted by tsim ( q i ,t ij ) .
However, it would be very inefficient to directly include the lists of all similar tags in the processing. Instead, C ONTEXT M limits the number of expansion tags per original query tag (e.g., by a limit of 10) and incrementally adds lists for similar tags to the processing on the fly. To do this, the algorithm maintains, for each query tag q i , the list SIMTAGS ( q i ) of tags similar to q cludes these lists in the list selection process (C HOOSE To compute the score bounds of a list SIMTAGS ( q i ) ,C ON TEXT M ERGE first reads the entry t ij ,tsim ( q i ,t ij ) from the top of the list without actually removing it from the list and looks up idf ( t ij ) . The score of the SIMTAGS ( q i ) list is then the maxi-mum of the scores that the DOCS and FRIENDS lists for t had if they were opened for scanning. If C HOOSE N EXT L IST chooses the SIMTAGS ( q i ) list, it adds DOCS [ i ][ j ] and FRIENDS [ i ][ j ] to the processing.

In this dynamic handling of tag expansions, the computation of worstscores for candidates must take into account that the actual score of a document for a query tag q i is the maximum over all similar tags t ij whose lists are open. In addition, the bestscore computation needs to consider that not all lists for expanded tags may have been opened already. For each query tag q i where the tag expansion limit has not yet been reached, it therefore computes the maximal score that any document can get for the top tag t in SIMTAGS ( q i ) , and the bestscore of a document is then the maximum bestscore it can obtain in all open lists and the next ex-pansion. Note that this bound is only correct because the entries in SIMTAGS ( q i ) are sorted by tsim ( q i ,t ij  X  idf ( t ij
We evaluate the effectiveness of our scoring model and the effi-ciency of the C ONTEXT M ERGE algorithm on three different datasets, crawled from social network websites of different kinds:
Finding a good set of queries and relevant results for them is not an easy task. Even though there has been some work on evaluating queries in social networks, most notably by Bao et al. [6] who use DMOZ categories as global ground truth, such methods don X  X  fit our user-centric search task. Here, it is not sufficient to consider global relevance measures, as the notion of relevance is highly sub-jective and dependent on the initiator of the query and her personal context. For example, a picture of a person may only be relevant if she is known to the query initiator. However, when we execute our queries in the context of a user taken from our collections, it is not possible to ask this user to assess the subjective relevance of a result item.

To solve this problem, we propose two independent evaluation methods, a user-specific ground truth and a user study with man-ual relevance assessments for selected queries. For a given query Q ( u, q 1 ...q n ) , we consider as user-specific ground truth the union of all documents which were tagged with all query tags by u or any of her direct friends. Our decision to consider also friend X  X  doc-uments as part of the ground truth is based on the fact that these are also documents that the user has direct access to and it is likely that the user has seen, and agreed with, the tags assigned. Since documents on the ground truth set contain tags from the query, we evaluate the queries on a residual collection where query tags by u and her friends are removed as they are known to lead towards relevant results. Note that this introduces a penalty for our method as the transitive friends with highest scores cannot contribute rel-evant results by definition. Queries for the ground truth were ran-domly selected from tag pairs with medium frequency in the cor-pus (between 1,000 and 2,000), the query initiator was chosen ran-domly among users that have previously used the query tags and Table 2: Precision@10 for varying  X  values, ground truth ex-periments have at least one friend in the collection. This process yielded 150 queries for delicious and 184 queries for LibraryThing; note that this method cannot be applied to the Flickr data because there is almost no overlap in the pictures users tag.

For the user study on the LibraryThing data, we asked five col-leagues to register with LibraryThing, tag at least 20 books there, and choose some friends among other users. They then suggested 28 queries related to the books they tagged. For Flickr, we collected 40 queries from other colleagues and randomly selected a (fictious) query initiator among the users in our Flickr crawl who used all query tags at least once on the same picture. We then ran the queries with different configurations of our algorithm and pooled the re-sults. In the assessment phase, a volunteer (which was the query initiator for LibraryThing) was shown, in addition to the results for the query from the pool, the documents (i.e., pictures, bookmarks, or books) from the query initiator t hat contain at least one of the query tags, in order to understand the personal context of the query initiator. This way, we try to overcome the aforementioned problem of subjectively assessing result qualities with the eyes of the query initiator. The participant then marks each result as highly relevant, relevant, or nonrelevant in the context of the query initiator (with-out knowing which configuration generated the result).
We evaluate the effectiveness of our method by computing pre-cision and normalized discounted cumulative gain (NDCG) [23] at certain cutoff levels, where relevant documents were determined either by human assessments or by the ground truth.

Results for the User Studies. The results from the user study are shown in Table 1. For both the Flickr and the LibraryThing data set theNDCGimprovedbyincreasing  X  , but dropped of w hen setting  X  to 1. This shows that while the semantic aspect is more important than social aspect for these specific datasets, the social component helps improving the search result, in particular for Flickr. This can also be seen with the precision@10, which for LibraryThing starts at 0 . 50 for  X  =0 . 0 , increases to 0 . 53 for  X  =0 . 9 and drops to 0 . 40 for  X  =1 . 0 .

Results for the User-Specific Ground Truth. The ground truth based experiments (Table 2 show very similar results. Again, re-sult quality improves then increasing  X  , but drops when ignoring the social aspect and setting  X  =1 . For Delicious the social as-pect seems to be more important than for the other data sets, here the optimal value is  X  =0 . 8 . Overall search effectiveness clearly benefits from integrating social scores.
 Figure 3: Average Execution Cost without tag expansion
Our main concern in this paper has beeen the efficiency and scal-ability of the query processing. To assess the efficiency of the C TEXT M ERGE algorithm, we evaluated the two sets of queries used for the ground-truth based evaluation on the Librarything and de-licious data sets, and in addition a set of 164 queries on the Flickr data set that were computed similarly to the others. The algorithm was implemented in Java, with the index lists stored in an Oracle 10g database. The experiments were done on an Windows-based server machine with four Opteron CPUs and 16GB of memory. We measured wall-clock runtimes and abstract cost in terms of disk ac-cesses, where the cost for a random access was 100 and the cost for a sequential access 1. We compared our algorithm with a standard join-then-sort algorithm that reads all index lists involved in the query execution into memory, uses an in-memory hash join to com-bine entries for the same document, and finally does an in-memory sort of the candidate set to compute the top-k results.
For each collection, we performed a large variety of experiments to explore the space of alphas, thresholds for maximal tag expan-sion, and conjunctive vs. disjunctive evaluation. For space restric-tions, we limit the discussion to selected results with conjunctive evaluation, results with disjunctive evaluation generally followed the same trends. Figure 3 depicts the average abstract cost per query for the three collections and selected values of  X  , evaluated with C ONTEXT M ERGE and the baseline without tag expansion. It is evident that our highly efficient top-k algorithm has at most half the cost of the baseline algorithm for most values of  X  ,andshows even higher savings for the LibraryThing collection. Table 3 shows some additional details for the experiments on LibraryThing; it is Table 3: Efficiency details for LibraryThing with conjunctive evaluation evident from the table that wall-clock runtime of our algorithm is also at least 50% better than that of the baseline (which is also the case for the other two collections).

Figure 4 shows abstract cost for the same setup, but now with tag expansion up to a limit of 10 similar tags. Note that the bars for the LibraryThing baseline experiment have been cut at 350,000. Here, the effectiveness of our dynamic tag expansion is clearly evident, as it saves factors of 3-5 compared to the baseline method which needs to completely scan the lists of all 10 related tags for each query tag. Table 3 again shows details for some experiments on LibraryThing; here, our highly efficient method manages to reduce runtime by up to an order of magnitude over the baseline. The column avg.#exp shows the average number of similar tags considered per query. Whereas the baseline methods needs to consider all tags, our self-throttling expansion technique requires only very few similar tags.
Social search is a promising direction to increase user-perceived query result quality. This paper developed an effective scoring model for user-centric searches in social networks, and introduced the C ONTEXT M ERGE algorithm to efficiently evaluate queries in such networks, dynamically including related users in the execu-tion. Combining a top-k algorithm with dynamic tag expansion and dynamic expansion to similar users, C ONTEXT M ERGE is up to an order of magnitude faster in terms of measured runtime and cheaper in terms of abstract cost than the standard baseline method of processing inverted lists. [1] G. Adomavicius and A. Tuzhilin. Toward the next generation [2] Y.-Y. Ahn et al. Analysis of topological characteristics of [3] S. Amer-Yahia et al. Challenges in searching online [4] V. N. Anh and A. Moffat. Pruned query evaluation using [5] R. A. Baeza-Yates and A. Tiberi. Extracting semantic [6] S. Bao et al. Optimizing web search using social annotations. [7] H. Bast et al. IO-Top-k: Index-access optimized top-k query [8] M. Bender et al. Peer-to-peer information search: Semantic, [9] B. Billerbeck and J. Zobel. Questioning query expansion: An [10] S. Brin and L. Page. The anatomy of a large-scale [11] A. Damian et al. Peer-sensitive objectrank -valuing [12] A. Das et al. Google news personalization: scalable online [13] P. A. Dmitriev et al. Using annotations in enterprise search. [14] M. Dubinko et al. Visualizing tags over time. ACM [15] R. Fagin et al. Optimal aggregation algorithms for [16] S. Golder and B. A. Huberman. Usage patterns of [17] H. Halpin et al. The complex dynamics of collaborative [18] D. Heckerman et al. Dependency networks for inference, [19] J. L. Herlocker et al. Evaluating collaborative filtering [20] P. Heymann et al. Can social bookmarking improve web [21] P. Heymann and H. Garcia-Molina. Collaborative creation of [22] A. Hotho et al. Information retrieval in folksonomies: Search [23] K. J X rvelin and J. Kek X l X inen. IR evaluation methods for [24] R. Kumar et al. Structure and evolution of online social [25] G. Linden et al. Amazon.com recommendations: [26] A. Mislove et al. Exploiting social networks for internet [27] J. Pouwelse et al. Tribler: A social-based peer-to-peer [28] S. E. Robertson and S. Walker. Some simple effective [29] B. M. Sarwar et al. Item-based collaborative filtering [30] J. B. Schafer et al. Collaborative filtering recommender [31] C. Schmitz et al. Mining association rules in folksonomies. [32] S. Sen et al. Tagging, communities, vocabulary, evolution. In [33] C. Tantipathananandh et al. A framework for community [34] M. Theobald et al. Efficient and self-tuning incremental [35] S. Xu et al. Using social annotations to improve language [36] J. Zhang et al. Expertise networks in online communities:
