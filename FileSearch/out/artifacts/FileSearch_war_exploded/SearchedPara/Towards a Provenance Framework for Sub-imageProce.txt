 While there has been advances in observational equipment that generate huge high quality images, the processing of these images remains a major bottleneck. We show that provenance data collected during the processing of data can be reused to perform selective processing of data and sup-port network collaboration without clogging distribution net-works. We introduce the idea of sub-image processing (SIMP) in the context of processing a subset of pixels of an image and the use of provenance data to assemble pipelines and to select processing metadata for SIMP. We describe an imple-mentationofSIMPinAstro-WISE 1 .
 H.3.4 [ Systems and Software ]: Distributed Systems; H.4 [ Information Systems Applications ]: Workflow Man-agement; J.2 [ Physcial Sciences and Engineering ]: As-tronomy Design, Experimentation
Much of modern and scientific research such as virtual astronomical observations, bioinformatics and high-energy physics involves the accumulation of huge amount of digi-tized data and requires global participation for processing and visualization. Such observations provide image data with catalogues of millions of objects, each object with hun-dreds of associated parameters. Raw data is often collected from shared observational instruments, while end-users of www.astro-wise.org this data perform analysis and/or visualization at local sta-tions in their research centers. Because of the distributed nature of data, people and the processes; processing of these huge datasets is becoming problematic [9].

End-users can no longer visualize and manipulate these large data on their local workstations and over networks in real time because of the high pixel images coupled with the narrow network bandwidth. The processing is also increas-ing in complexity requiring laboriously sophisticated tech-niques and high end distributed computing resources. Such processing requires trying different datasets and processing techniques, tweaking parameters, verifying data quality, re-peating this data derivation and customization of the re-sults. Although most of the processes are run in a separate dataflow, they have a certain amount of overlap. (e.g. share some input and sometimes intermediate data). Its clearly wasteful to run the same processes repeatedly without ref-erence to (or the use of ) already existing processed data.
Astronomical systems have been built to work with and process data based on the telescope X  X  detector or chip (full-image) [2, 6]. All metadata and processing parameters are basedonaninstrumentwithfixeddetectorproperties(e.g., image size, calibration frames, overscan regions, etc.). An image from an observation may have a catalogue of millions of sources, each source with hundreds of associated param-eters. By source we mean a celestial objects such as stars, planets, comets, nebulae, star clusters and galaxies. How-ever, most often users are interested in a source that lies on a few pixels of an image. The current approach allows the processing of a full-image even if the source of interest exists on a few pixels of an image. Accordingly, out of millions of images in a survey, it is nearly impossible and wasteful to process the whole data volume. Instead of processing the whole dataset, a user should only select, retrieve and pro-cess only relevant pixels on an image where the source exists. These pixels are extracted and processed as a sub-image.
Based on these observations, we motivate the need to record fine-grained provenance and also the need to use provenance to enable sub-image processing (SIMP). Data provenance in scientific experiments has been used to under-stand results, the relationships between data products and the programs that were used during processing by examin-ing the sequence of steps that led to a result [3, 4]. By using provenance, we can simply the processing of data by exam-ining previous reruns and relating them to current runs [8]. In this context, since all pipelines for astronomical image processing have been written for full-images, and all meta-data and processing parameters are based on an instrument (or full-image), we use provenance data to match and re-trieve existing pre-processed information in the system from which we build pipelines and select input data for the SIMP. In the same line, we enable operations that are very difficult (or impossible) to execute at sub-image level.

This paper presents our approach to providing these fun-damental services in a distributed computing environment. This papers focus on the use of provenance to support SIMP. We specifically show how we trace fine-grained provenance (at pixel level) and how we use this provenance data to per-form SIMP. With SIMP the user controls the downloading process by selecting, extracting and processing pixels with their objects of interest. This schemes drastically reduces the amount of data transferred over wide-area-networks.
To the best of our knowledge, this is the first work that leverages provenance to support SIMP. The rest of the pa-per is organized as follows; In Section 2, we briefly describe our provenance model in Astro-WISE. We present the SIMP framework in Section 3 and we review related work in Sec-tion 5. We conclude in Section 6 where we outline directions for future work.
In our previous work [7], we proposed a model that uni-formly captures provenance during the course of data pro-cessing. From that model, we explicitly assume that every output depends on every input and parameters passed to the function. Therefore, such proveance accounts for an output product produced during the course of a dataflow execution by displaying a connected graph of input, intermediate, out-put data and also parameters and attributes used during the processing. This level of granularity is in-sufficient for sub-image processing. We need to trace fine-grained lineage to include transformations at pixel/byte level. The challenge is to determine which pixels of which input images were used in the construction of the composite pixels in the output image.
We define F : V X  X  as a function on the space of astro-nomical image processing, and  X  : V X V X  F as a function that takes an input pixels p a and outputs p b , then for brevity let  X  ab ( p a )= p b .Ifthereexists  X  ba such that  X  ba  X  where e is an identity matrix, then there exists a 1-1 map-ping between pixels of the iinput image to pixels to an output image. Then it is possible to a find a mapping between pixels of related images or connect all pixels from raw images inter-mediate to the final objects. However, we can achieve this if our sequence of operations consists of invertible atomic operations. Specifically, suppose  X  ab = f n  X  X  X  X  X  X  f 1 then each f must have a well-defined inverse. For example, if f i is the operation of applying a distortion correction then, f  X  1 is the operation of removing the distortion correction.
Because of the nature of the source data as well as the nature of the image processing and regridding algorithms, invertibility is a very hard problem or impossible. For such non-reversible transformation, pixel lineage is computed us-ing numerical techniques. Thus pixel lineage becomes prob-abilistic and different precision levels can be achieved . We have implemented and tested such numerical techniques for Figure 1: A tree view is given of the dependencies This tree view gives an overview of the target de-pendencies. Green dependencies are up-to-date, red dependencies are out-of-date and for orange depen-dencies indicate a newer version exists. . tracing pixel lineage. Early results are promising. A posi-tional accuracy of approximately 5 orders of magnitude can be achieved. The algorithm can be tuned to achieve any precision, but its a trade-off between performance and accu-racy. With this framework, we can trace and link all pixels that have been processed in the system.
The ability to store and mine provenance data is required to enable SIMP. This is because pipelines have been writ-ten for full-images and designed for instruments with fixed detector properties. No metadata and processing parame-ters currently exist for SIMP. These parameters have to be copied from provenance data, modified and new pipelines for SIMP assembled. SIMP involves 4 basic steps;
The basic idea behind our algorithm is to search for a graph representation of the dataflow that was used in ear-lier runs to process target. The starting point is a target and a set of preconditions. E.g. consider the basic query find the derivation path for a source  X  X  X  The answer to this query may consists of the several paths rooted at the same Raw-Data. It is possible that the same object can be computed using different variations of m ethod or parameters. To select the required pipeline, we add more constraints to the query. Using these constraints, the system matches and selects the pipeline from all candidate pipelines by pairing nodes and computing similarity between two adjacent nodes. From the selected pipeline, the system builds a directed graph repre-senting the data dependencies with nodes representing ob-jects and edges representing all dependencies attached to object (Refer to Fig. 1). The graph begins with the top-most node, which is the target to be made. New edges are added starting at this trigger and expanding outward, using the dependency logic derived from provenance data. The dependency graph is built and checked recursively till the last dependency (in this case raw data from the telescope).
In this step we retrieve input data, processing parameters and intermediate data products from the provenance store basedontheobjectsinthegraphgeneratedinsection3.0.1. Each node in the graph is associated to an object. Each object is identified with a unique ID which is used when searching the database for provenance data related to a par-ticular object. Part of the provenance includes input and output data, processing parameters and attributes, meth-ods/modules that were used to process the object. If part of provenance is an image, a cutout is made of the pixels of interest from this image and used as input to the module. The pixels extracted as a cut out are determined through pixel lineage. We also identified basic operations that are useful for common querying tasks over provenance store that simplify the query syntax. Some examples are listed below;
In this frame-work there are two types of changes. Firstly, pipelines must be modified to process sub-images and sec-ondly a user might need to change processing parameters or to switch to a different algorithm to improve a given result while carrying out a detailed analysis or a compu-tation to a specific region on an image. In any case we build on already existing data from previous runs to enable such changes. All changes required for this framework are included in the system, as a new module, attribute or pa-rameter without changing the overall data model. These are automatically loaded whenever a user requests to process a sub-image. However, the complete account on what changes are made to the pipelines is beyond the scope of this paper.
Using the dependency graph generated in section 3.0.1, we apply all changes as identified in section 3.0.3 and at-tributes and parameters from section 3.0.2, and we build the pipeline for SIMP. We apply all the changes (or additions) and analyze the dependency graph. Only the modules, in-termediate data or a dependency affected by the change will be processed. If new versions exist of the classes that cre-ated the dependencies, then these modules will also be rerun using the new versions of the classes (refer to the caption of Fig 1).

After assembling the pipeline and collecting all necessary input data, processing of sub-image is then done. Source extraction is then run on the sub-image resulting in a new Figure 2: A plot showing the time it takes to process sub-images of different sizes catalog of sky positions, and/or any other user specific pro-cessing is done on the sources extracted.
Initially images are reduced, processed and source extrac-tion carried out. After source extraction, it happens that sources were detected in some frames(images) derived from data taken in certain filters, we call  X  X aster filters X , and not detected in other frames derived from data taken in some other filters, we call  X  X rop-out filters X . The task is to find drop-out candidates, that is, those sources that were de-tected in frames observed using the master filters, but not appearing in the frames observed using dropout filters. Such sources might be extremely high-redshift quasars. Precise fluxes need to be determined in the master filters and flux upper-limits in the drop-out filters. After identifying the drop-out candidates, all the images need to be reprocessed using new source parameters and new detection thresholds. Rather than re-processing full-images, we extract and pro-cess only the pixels surrounding the dropout candidates. By doing this we save on computation and data transfer time. Below, we present two tests to to demonstrate the power of SIMP.
 Processing Time : We create the experiment as explained above. We process a full image using new source parame-ters and new detection thresholds. We repeat the same test using sub-images of various sizes. The results of this pro-cessing are shown in Fig.2. The time it took to process each different sized cutout was reco rded. Notice from Fig.2 that the processing time increases as the size of the cutout in-creases. This therefore shows that there is significant saving in computation time (and resources) while processing sub-images compared to processing full-images.
 File Retrievals ; One common characteristic of all dataflow programming frameworks is the requirement of locally staged data for processing. In this test, we estimate how much time is required to download 24 sub-images of 544k from a data server to local PC compared to downloading the 24 full-images of 384MB. we run this test over two links of different capacities. The table below shows the results; Link No of Image Time (s) Transfer
Size Images Size(KB) Rate 1Gbit/s 24 544 0.4 1.22 MB/s 6Mbit/s 24 544 1.3 423 KB/s The time in seconds is the total time that the actual trans-fer took, from the first byte to the last byte of each file. We notice a significant difference when transferring sub-images compared to full-images on a 1Gbit/s link. As you notice downloading the sub-images on a 6Mbit/s takes almost the same amount required as on a 1Gbit/s. However, the results for transfer full-images 6Mbit/s are shocking and might ren-der the system unusable.
We could not find anywhere in literature where SIMP has been done and therefore we have no comparative analysis to evaluate if there could be advantages or disadvantages of our approach compared to any other approach. How-ever, distributed data sharing systems e.g. grid systems [10] have been developed to address performance and dataset concerns. Such systems like [2], provide a scalable infras-tructure for running image pipelines in a distributed way. However even with such grid system, transferring of the data to the processing node still suffers from delays due to con-gestion on WAN links.

Provenance-aware scientific workflow systems [5] have also been considered as the paradigm for representing and man-aging complex distributed scientific computations. Systems such as those surveyed in [4] have enabled scientists to carry out complex scientific computations while capturing prove-nance. Despite these developments, little or no support ex-ists in current systems to recored provenance data and at pixel level and most models do not allow end-users to use lineage data in scientifically meaningful way in particular to improve scientific processes. Our provenance model intro-duced in this paper does trace lineage at the finest detail (e.g., a pixel transformation process). This lineage captured is then used for various scientific processes but most specif-ically as introduced in this paper, this lineage at pixel level has been used for SIMP.

The use of provenance we describe in this paper is anal-ogous to how other authors have used provenance to solve some use-cases. For example in Kepler [1] provenance has been used to enable smart  X  X eruns X . Kepler takes data de-pendencies into account and only execute those parts of the workflow affected by the parameter change. In [8] prove-nance has been used for interactive design of workflows.
We have described a framework that leverages provenance to aid in selective retrieval and processing of data. All of the discussed functionality has been implemented, however due to the page size limitation, most of the interesting features have not been mentioned. We plan to continue working on the data models and make it available as a web-service in the near future. This paper does not focus on the changes made to the modules/pipelines process sub-images, but rather how provenance can be used to support SIMP.

We can safely say that we have accomplished our design goals of supporting network collaboration because users at remote research centers could comfortably run and process data, without limitations of huge data transfers and limi-tations of resources on local clients. However our approach is not foolproof, and there are cases where it may fail to produce the results a user expects. For example, if a user applies the methodology to a processing that involves neigh-boring pixels to determine a result of a pixel, the pipelines is likely to fail. However, when such a processing fails the user can initially process the full image and extract all parame-ters/needed to aid in processing of other sub-images derived from the same image. The effect shall be slower performance for the start, which improves significantly while processing other sub-images.

Although we reduced the domain from a full frame to a sub-image, we intend to further transform scientific systems to process pixels rather than images. We are currently inves-tigating how we can use databases to aid in such processing. [1] I. Altintas, B. Ludaescher, S. Klasky, and M. A. Vouk. [2] K.G.Begeman,A.N.Belikov,D.R.Boxhoorn, [3] J. Freire, D. Koop, a nd L. Moreau, editors. [4] J.Freire,D.Koop,E.Santos,andC.T.Silva.
 [5] Y. Gil, E. Deelman, M. Ellisman, T. Fahringer, [6] P. Greenfield. Reaching for the stars with python. [7] J. Mwebaze, D. Boxhoorn, and E. Valentijn.
 [8] C. Scheidegger, H. Vo, D. Koop, J. Freire, and [9] A. S. Szalay. The sloan digital sky survey and beyond. [10] J. Yu and R. Buyya. A taxonomy of scientific
