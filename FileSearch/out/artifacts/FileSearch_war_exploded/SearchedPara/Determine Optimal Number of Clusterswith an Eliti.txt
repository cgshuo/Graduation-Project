 Clustering is a challenging research area in data mining. A common form of clus-tering is partitioning the data set into homogeneous clusters such that members of the same cluster are similar and members of distinct clusters are dissimilar. Determining the optimal clusters number is one of the most difficult issues in clustering data. In this work, we deal with the clustering problem without prior knowledge on the appropriate clusters number and we try to propose the global optimal or near-optimal cluster seeds. Clustering algorithms can be broadly clas-sified into two groups: hierarchical and partitional [1]. Hierarchical algorithms recursively find nested clusters either in a divisive or agglomerative method. In contrast, partitional algorithms find all the clusters simultaneously as a partition of the data and do not impose a hierarchical structure. Common formulation of the clustering problem is assuming S is the given data set include n data points: integer k . The goal of clustering is to determine a set of k clusters C 1 ,C 2 , ..., C k such that the points belonging to the same cluster are similar, while the points belonging to different clusters are dissimilar in the sense of the given metric. The problem of finding an optimal solution to the partition of n data into k clusters is NP  X  complete , and heuristic methods are widely effective on NP  X  complete global optimization problems and they can provide good sub-optimal solutions in reasonable time. We propose a clustering algorithm that can detect compact and hyperspherical clusters that are well separated using an Euclidean distance. To detect hyperellipsoidal clusters, we can use a more general distance function such as the Mahalanobis distance for examp le [2]. Some recent research has shown that the problem of searching efficient initialization methods for k-means clus-tering algorithm is a great challenge. Numerous initialization methods have been proposed to address this problem. Celebi and al. [3] present an overview of these methods with an emphasis on their computational efficiency. In their study, they investigate some of the most popular initialization methods developed for the k-means algorithm. They describe and compare initialization methods that can be used to initialize other partitional clustering algorithms such as fuzzy c-means and its variants and expectation maximization, and they conclude that most of these methods can be used independently of k-means as standalone clustering algorithms. Some others methods are proposed, based on metaheuristics such as simulated annealing [4] and genetic algorithms [5]. These algorithms start from a random initial configuration (population) and use k-means to evaluate their so-lutions in each iteration (generation). There are two main drawbacks associated with these methods. First, they involve numerous parameters that are difficult to tune [6]. Second, due to the large search space, they often require a large number of iterations, which renders them computationally prohibitive for most of the data sets. This paper presents a method that proposes, in the same time, the optimal cluster number with the initial clusters seeds. We don X  X  use k-means to evaluate our solution because it depe nds on several parameters itself (k, ini-tial seeds, . . . ), or any other clustering algorithm, so, our method can be used as standalone clustering algorithms without a prior knowledge of cluster num-ber. Generally metaheuristics approache s involve numerous parameters that are difficult to tune, to deal with this problem, we propose an Elitist Evolutionary Approach that involves numerous evolutionary algorithms (EAs). The difference between them is implemented by the parameters and we select only the best concurrent solution. The remainder of this paper is organized as follows: Section 2 describes the related work on initialization and elitist methods. Section 3 gives the details of the new proposed algorithm, including the algorithm description, motivation of population initialization, as well as the mutation process based on neighborhood search. Section 4 shows the experimental results on data sets clustering. Finally, we draw the conclusions. In this section, we briefly review some of the commonly used initialization meth-ods and elitist methods. 2.1 Initialization Methods Celebi and al. [3] investigate some of the most popular initialization methods developed for the k-means algorithm. Their motivation is threefold. First, a large number of initialization methods have been proposed in the literature. Second, these initialization methods can be used to initialize other partitional clustering algorithms such as fuzzy c-means and its variants and expectation maximiza-tion. Third, most of these initialization methods can be used independently of k-means as standalone clustering algorithms. They review some of the com-monly used initialization methods with an emphasis on their time complexity. They conclude that the super linear methods often have more elaborate designs when compared to linear ones. An interest ing feature of the super linear methods is that they are often deterministic, which can be considered as an advantage especially when dealing with large data sets. In contrast, linear methods are of-ten non-deterministic and/or order-sensitive. A frequently cited advantage of the more elaborate methods is that they ofte n lead to faster k-means convergence, i.e. require fewer iterations, and as a result the time gained during the cluster-ing phase can offset the time lost during the initialization phase. This may be true when a standard implementation of k-means is used. However, convergence speed may not be as important when a fast k-means variant is used as such methods often require significantly less time compared to a standard k-means implementation. Some other initialization methods such as the binary-splitting method [7] takes the mean of data a s the first center. In iteration t ,eachofthe existing 2 t  X  1 centers is split into two new centers by subtracting and adding a fixed perturbation vector. These 2 t new centers are then refined using k-means. There are two main disadvantages associated with this method. First, there is no guidance on the selection of a proper valu e for the vector, which determines the direction of the split [8]. Second, the method is computationally demanding since after each iteration k-means has to be run for the entire data set. Some other methods based on metaheuristics such as simulated annealing [4] and genetic algorithms [5]. These algorithms start from a random initial configuration and use classical algorithm such as k-means to evaluate their solutions in each itera-tion. There are two main disadvantages associated to these methods. First, they involve numerous parameters that are difficult to tune (initial temperature, cool-ing schedule, population size, crossover/mutation probability, etc.) [6]. Second, due to the large search space, they often require a large number of iterations, which renders them computationally prohibitive for all but the smallest datasets. Interestingly, with the recent developme nts in combinatorial optimization algo-rithms, it is now feasible to obtain globally minimum clusterings solution for small data sets without resorting to metaheuristics [9]. 2.2 Elitist Methods Prevent promising individuals from being eliminated from the population during the application of genetic operators is a very important task. To ensure that the best chromosome is preserved, elitist methods copy the best individual found so far into the new population. Different EAs variants achieve this goal of pre-serving the best solution in different ways. However, elitist strategies tend to make the search more exploitative rather than explorative and may not work for problems in which one is required to find multiple optimal solutions [10]. Elitist methods are widely applied on different domains, Qasem and Shamsuddin [11] developed a Mimetic Elitist Pareto Differential Evolution algorithm, in order to deal with the hybrid learning problem (unsupervised and supervised learning), they use the multi-elitist approach to help the learning algorithm to get out of local minimum, therefore improving the accuracy of the proposed learning model. Das and al. [12] proposed a method based on a modified version of classi-cal Particle Swarm Optimization algorithm, known as the Multi-Elitist Particle Swarm Optimization model. The proposed algorithm has been shown to meet or beat the other state of the art clustering algorithms in a statistically meaningful way over several benchmark datasets. The unique disadvantage of this algorithm is choosing the best suited parameters to find optimal solution. Gou and al. [13] apply Multi Elitist approach on quantum clustering problems. They use these methods to avoid getting stuck in local extremes. They used the mechanism of cluster center updating with a property of k-means clustering, that can influ-ence the clustering results. This is one of disadvantages of this method, adding parameters that the method is based on. According to elitist strategy that work for problems in which one is required to find multiple optimal solutions, we in-troduce a new approach where multi evolutionary algorithms run together at the same time to compare their proposed solutions, and we select only the best one which is the optimal or nearest optimal solution. We propose an Elitist Evolutionary Clustering Algorithm (EECA) (figure 1) that combine different techniques such as evolutionary algorithm with elitist approach and local search approach. This approach allows us to determine the optimal cluster number as well as finding cluster seeds. 3.1 Evolutionary Algorithm In this section, we try to explain succinctly the evolutionary algorithm which is shown in right of the figure 1.
 Gene Representation. A potential solution of our problem is a combination of potential optimal seeds, we try to find optimal number of these seeds. According to this, we consider a genetic individual (chromosome) as a combination of k max potential optimal seeds, k max being parameter of the algorithm. Each gene in genetic individual is an integer number, which takes values from { 1 , 2 , .., n } .This value indicates the data point identification. The gene with value 0 indicates that there is no point selected as center. The n umber of genes in the optimal solution different from 0 represents the optimal cluster number. Each data point is a d  X  dimensions vector containing the d real values. We want to diversify the population, for this, we associate a frequency rate at each data point, each new genetic individual will be composed by the data point had not been used before, that have frequencies equal or close to zero.
 Population Initialization. The population initialization is created by nb pop genetic individuals with nb pop given before. Each gene in the genetic individual is selected randomly on : { 0 , 1 , 2 , ..., n } data set identification. Each genetic in-dividual corresponds to a specific clustering solution in terms of cluster seeds. We impose in the genetic individual to have a different gene to obtain specific and different clusters seeds. We also impose to have an initial population with-out redundant genetic individuals. Studying individual is applied in each genetic individual which is created. And then, we verify that is no identical genetic individual, in the population which has the same gene. At this step, the first population is ready. Once the population is evaluated and sorted according to the fitness function we operate the genetic operator described bellow. Genetic Operators. The crossover operation produces new offspring genetic individuals from parent individual. Two new genetic individuals are created by exchanging genes from two parent chromosomes. This exchange may start from one or several positions in the chromosomes called cut point. We can use two types of crossover: a randomly determined cut point or an optimized cut point. In the latter case, we determine the best point before the cut, this implies an evaluation of each possible cut for the individual. For our case, we first use a ran-domly chosen cut point which is modified to obtain optimized cut point, which implies evaluation of each new created individual. The first child resulting from by repeated randomly crossover which is better than initial child will determine the optimal cut point. The mutation make s gene inversion in the genetic indi-vidual. This genetic operator is used to avoid the degeneration of the population in order to prevent a too fast convergence of the genetic algorithm. If imple-mented appropriately, the operator can make the algorithm able to leave from local optimum. The mutation is usually applied with a small probability ( prob m , algorithm parameter). The mutated gene is chosen according to neighborhood search (section 3.2), we should verify if a new gene value is not in neighborhood of other genes composing the genetic individual. It is evident that we study the composition of each creation of new individual to have a different gene in a specific genetic individual.
 Evaluation Criteria. The first objective is to find the optimal number of clus-ters, where clusters are compact and sep arated between them. Several measures have been proposed, Milligan and Cooper [14] presented a survey and compari-son of 30 internal validity indexes for clustering algorithms and out-perform that CH ( k ) [15] is one of the best solutions.

This index represents a ratio of the sum of between-cluster and the sum of within-cluster, which is expressed as follows: Where n is the number of data points, k is clusters number. With | C i | is the number of assigned objects to the cluster C i ( i =1 ,...,k ); C i is a center of class C i and x = 1 n optimal clusters number is found by maximizing Calinski and Harabasz index ( CH  X  index ).

The second objective is to find clusters that are compact and separate, so we introduce a measure that minimizes the overlapping between clusters, this criteria is defined by :
We compare our method with two fitness functions, first test series focus on maximizing CH ( k )ineach EAi and in the elite population, where second test series focus on minimizing OP in each EAi and sort the elite population with CH ( k ). 3.2 Neighborhood Search When we operate the mutation we should firstly verify if all genes are different; secondly, according to the ob tained potential clusters seeds, we introduce a new verification process where we impose that a new gene must not be in the neigh-borhood of others genes composing the genetic individual. For this, we introduce an automatic method to detect the limit of the cluster.
 Cluster limit detection. In order to obtain the neighborhood seed, we search the data points contained in the cluster, we select the ones close enough to the seed. We choose the threshold  X  (algorithm parameter) by computing the distance between all the data points and the cluster seed and ordering them from the closest object to the farthest. We then tr y to find an abrupt increasing of distance that will indicate the cluster limit. We choose to use the peak detection method presented by Palshikar [16] applied on diff erential distances. Other cluster limit detection might be used, but this one is fast and gives the algorithm a complexity of O ( n  X  d  X  ( p + g )), where n is the number of data points, d the number of dimensions, p the population size and g the number of generations.

Then, we operate mutation by changing gene value by new value which is not in the neighborhood of other genes composing the genetic individual, and if we don X  X  find this new gene, we change the value by 0 (no center selected). 3.3 Elite Population Generally, the goal of the adaptive elitist-population search method is to adap-tively adjust the population size according to the features of our technique to achieve, firstly, a single elitist individual searching for each solution; and sec-ondly, all the individuals in the population searching for different solutions in parallel. For satisfying multi modal optimization search, we define the elitist in-dividuals in the population as the individual with the best fitness on different solutions of the multiple domain. These elitist individuals define the elite popu-lation. Then we propose the elitist genetic operators that can maintain and even improve the diversity of the population and performing different evolutionary al-gorithms. A major advantage of using EAs over traditional learning algorithms is the ability to escape from local minimum using genetic operators [17]. The evolutionary algorithm depend on 4 parameters : nb pop , prob m , nb iter and  X  .We perform as much as possible many evolutionary algorithms according to different values of these parameters and then we se lect the best solution, which compose elite population, without focusing on setting parameters. From this elite popu-lation we select the best solution (elite individual), which is the optimal solution for our problem. To evaluate the performance of the pro posed method, we proceed several ex-periments on data sets from University of California at Irvine (UCI) machine learning benchmark repository [18]. Data sets information are summarized in Table 1.

In our experiments, we perform an Elitist Algorithm with varying different parameters values in each Evolutionary Algorithm. We vary Evolutionary Algo-rithm parameters nb pop , prob m , nb iter and  X  as follow :  X  nb pop  X  X  50 , 100 , 150 }  X  prob m  X  X  0 . 1 , 0 . 2 , 0 . 3 }  X  nb iter  X  X  200 , 300 , 500 }  X   X   X  X  1 , 2 , 3 } For each data set we perform the elitist algorithm with 18 EAs and k max = 10. The results of finding optimal k are illustrated in table 2. We compare our method with the two fitness functions presented before. First test series focus on maximizing CH ( k )ineach EAi and in the elite population ( EACH corre-sponding column in table 2). The second test series focus on minimizing OP in each EAi and sort the elite population with CH ( k )( EECA corresponding column in table 2).

As we can see in the table 2, we find exactly the same number of clusters as in real data sets using overlapping fitness function combined with CH ( k ) index. When only CH ( k ) index is used, we always find k = 2, (except for 2 data sets, Ecoli and Synthetic data sets). This result can be explained by the formula of CH ( k ) index, when we try to maximize only this index, we converge to small number of clusters. To find the data set partitions, we use a cluster limit detection method based on hypers pherical cluster forms. For Synthetic data set which presents prefect hyperspherical clusters, the two fitness functions performs perfectly. But in other data s ets it seems important to consider the overlapping fitness in the first, and then to select the better solution according to the CH ( k ) index. To confirm our results, we visualize some data sets using scatter plot methods [19] which represent all 2D projection of the data set. The figure 2 represents the projection of iri s data set, and the figure 3 represents the projection of Haberman data set, as we can see in the figures the data points colors (different forms) represent the r eal clusters, and in red (square form) we can see the clusters seeds that are det ected by our method. We can note that each of them corresponds to the real clust ers, and can be considered as the center of the different clusters or as initial seed for any clustering algorithm.
These results and the visualizations sh owed the effectiveness of our methods on data sets that have compactness clusters structures. As we can see in the figure 4, that represent a synthetic data set, composed by five clusters, with Gaussian distribution. We can easily adapt our method with changing and adapting dis-tance measures to extract other cluster structures. In fact, for this we need to build a good benchmark data bases. We can also improve our method with al-lowing an overlapping degree between different extracted clusters. This approach allows us to apply our method on more different data sets structures that can be added on the benchmark. This article proposes a new method that finds at the same time the optimal clus-ter number and proposes the initial clusters seeds without a prior knowledge of cluster number. We don X  X  use any other clustering algorithm to evaluate our solu-tion, so, our method can be used as standalone clustering algorithms dealing with hyperspherical clusters. Unlike other algorithms such as k-means, our approach do not need to fix a priori the clusters number. We propose a new mutation pro-cess using neighborhood search and we use, for this, an automatic cluster limit detection method. We also introduce a new combined fitness function to evaluate our solution. To deal with the problem of involving numerous parameters, we propose an Elitist Evolutionary Approach that involve numerous evolutionary algorithms (EAs). The difference between them is implemented through param-eters and we select only the best concurre nt solution. This proposition can also address the problem of exploration of large search space. The initial popula-tion of numerous evolutionary algorithms (EAs) is substantially different, so, we can deal with the large data sets in few nu mber of iterations. First results are encouraging, as meaningful clusters seeds have been found from different data sets. Results showed the effectiveness of our methods on data sets that have compactness clusters structures. We can easily adapt our method with changing and adapting distance measures to extract other cluster structure. Concerning further work, we plan to test our approach to different benchmark data bases to detect hyperellipsoidal clusters or other data sets structures and we think improving our methods with allowing an overlapping degree between different extracted clusters. This approach allows us to apply our method on more dif-ferent data sets structures that can be added on the benchmark. We want also to apply our approach on different subspaces, we think that the optimal cluster-ing can be different according to the subspace data projection. And then, our method can be applied on multiview clustering or on subspace clustering.
