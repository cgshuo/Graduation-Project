 Instead of a standard support vector machine (SVM) that pushed apart as far as possible. This formulation, which gram that require considerably longer computational time. Computational results on publicly available datasets indi-cate that the proposed proximal SVM classifier has compa-an order of magnitude faster. The linear proximal SVM can All computational results ave based on 6 lines of MATLAB code. data classification, tions 1. INTRODUCTION 
Standard support vector machines (SVMs) [36, 6, 3, 5, In contrast we propose here a proximal SVM (PSVM) which requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA Copyright ACM 2001 1-58113-391-x/01/08...$5.00 frequently used in the SVM literature: 
The identity matrix of arbitrary dimension will be denoted by I. 2. THE LINEAR PROXIMAL SUPPORT VEC-by the m  X  n matrix A, according to membership of each diagonal matrix D with plus ones or minus ones along its quadratic program with parameter ~ &gt; 0: planes: that bound most of the sets A+ and A-respectively. The the class A-points as follows: Consequently, the plane: midway between the bounding planes'(3), is a separating only approximately as depicted in Figure 1. The quadratic the "margin". Maximizing the margin enhances the gener-a "soft margin" (i.e. bound approximately with some error) determined by the nonnegative error variable y, that is: xtw-~ 0, thenxEA-, (~,~,~)eR~+'+~ (8) (~,~,~)en-+~+~ (9) Figure 1: The Standard Support Vector Machine Classifier in the w-space of _Rn: The approximately bounding planes of equation (3) with a soft (i.e. with (5) approximately separating A+ from A-. Figure 2: The Proximal Support Vector Machine Classifier in the (w,3')-space of R~+I: The planes x~w -3' = +1 around which points of the sets A+ and A-cluster and which are pushed apart by the optimization problem (9). 
We note that our formulation (9) can be also interpreted standard SVM formulation (2) can be interpreted, by using linear programming perturbation theory [21], as a least 2-norm approximate solution to the system of linear inequal-however, is based on the idea of maximizing the margin, feature of support vector machines [36, 6, 20]. 
The Karush-Kuhn-~cker (KKT) necessary and sufficient u = (I + D(AA' + ee')D)-'e = (I + gH,)_le, (13) and compute u by (15) for some positive v. Typically is chosen by means of a tuning (validating) set. which the Lagrange multipliers are nonzero because, solving planes x'w =  X 1, the concept of support vectors needs to points are needed to determine the basic nonzero compo-small number of data points can characterize any specific proximal SVM problem (9) with these data points and a using the entire dataset. for i = 1,... ,m: hi -DiHHi'D~u~ = DiHHC Di(I -Hi( I + Hi'Hi)-lHi')e. of D. 
We extend now some of the above results to nonlinear proximal support vector machines. 3. NONLINEAR PROXIMAL SUPPORT VEC-
To obtain our nonlinear proximal classifier we modify our w = A'Du from (12) to obtain: min 5[t [[ + vl y 2  X (u,u+~2) 
Figure 3: The spiral dataset consisting of 97 black points and 97 white points intertwined as two spirals in 2-dimensional space. PSVM with a Gaussian ker-nel generated a sharp nonlinear spiral-shaped sepa-rating surface. 
CODE 4.1. PSVM MATLAB Code function [w,gamma] = psvm(A,D,nu) ~, PSVM:linear and nonlinear classification Z INPUT: A, D, nu. OUTPUT: w, gamma Y, [w, gamma] = psvm(A,D,nu); fro, n] =size (A) ; e=ones (m, i) ;H=D* [A -e] ; r=sum(H) ' ; Y,r=H'*e; r=(speye(n+l)/nu+H'*H)\r; ~,solvs (I/nu+H'*H)r=H'*e u=nu* (I-(H,r)) ; s=D*u; w= (s'*A) ' ; Zw=A'*D*u gamma=-sum (s) ; Y, gamma=-e ' *D*u 
Note that the command line in the MATLAB code above: 
H'H)-IH'e of (15). This is much more economical and sta-consuming. Instead, we calculate r=sum(H) ' and w=(s'*A) ' respectively, the transposes of these vectors. 
We further note that the MATLAB code above not only is then given by (27) as: associated with the reduced matrix .4. A final note regarding a further simplification of PSVM. where E = DH and hence H = DE. Thus: E=DH=[A -e], and H=DE=D[A -el. (30) This direct explicit solution of our PSVM problem (9) can be written as the following single line of MATLAB code, which also does not perform the explicit matrix inversion code: Here, according to MATLAB commands, diag(D) is an rn  X  1 vector generated from the diagonal of the matrix D. Com-putational testing results using this one-line MATLAB code the problem and solving the resulting unconstrained mini-computations. ing: 
We outline our computational results now in five groups as follows. 1. Table 1: Comparison of seven different meth-of RAM, also under Windows NT 4 and using Visual C++ 5.0. The SVM light experiments were run on the same hardware as that for SOR, but under the So-laris 5.6 operating system. Bold type indicates the best result and a dash (-) indicates that the results were not available from [22]. Although the timing com-parisons are approximate because of the different ma-chines used, they do indicate that PSVM has a distinct edge in speed, e.g. solving the largest problem in 7.4 seconds, which is much faster than any other method. Times and ten-fold testing correctness are shown in Table 1. Times are for the ten-folds. 2. Table 4: Comparative performances of LSVM [24] and PSVM on a large dataset Two large datasets consisting of 2 million points and 10 attributes were created using the NDC Data Gen-erator [29]. One of them is called NDC-easy because it is highly linearly separable (around 90%). The other one is called NDC-hard since it has linear separability of around 70%. As is shown in Table 4 the linear clas-sifters obtained using both methods performed almost identically. Despite the 2 million size of the datasets, PSVM solved the problems in about 20 seconds each compared to LSVM's times of over 650 seconds. In contrast, SVM l~gh~ [16] failed on this problem [24]. 3. Table 3: Comparison of PSVM~ SSVM and LSVM and SVM light, using a Linear Classifier In this experiment we compared four methods: PSVM, SSVM, LSVM and SVM light on seven publicly avail-able datasets from UCI Machine Learning Repository [28] and [30]. As shown in Table 3, the correctness of the four methods were v~ry similar but the execution time including ten-fold cross validation for PSVM was smaller by as much as one order of magnitude or more than the other three methods tested. Since LSVM, SSVM and PSVM are all based on similar formula-tions of the classification problem the same value of v was used for all of them. For SVM light the trade-off between trading error and margin is represented by a parameter C. The value of C was chosen by tuning. A paired t-test [27] at 95% confidence level was per-formed to compare the performance of PSVM and the other algorithms tested. The p-values obtained show that there is no significant difference between PSVM and the other methods tested. We used a Gaussian kernel in order to classify the spi-ral dataset. This dataset consisting of 194 black and white points intertwined in the shape of a spiral is a synthetic dataset [37]. However, it apparently is a diffi-cult test case for data mining algorithms and is known to give neural networks severe problems [15]. In con-trast, a sharp separation was obtained using PSVM as can be seen in Figure 3. 5. Table 2: Nonlinear Classifier Comparison using PSVM, SSVM and LSVM For this experiment we chose four datasets from the UCI Machine Learning Repository for which it is known that a nonlinear classifier performs significantly better 5, CONCLUSION AND FUTURE WORK 
We have proposed an extremely simple procedure for gen-sible. This procedure, a proximal support vector machine (PSVM), requires nothing more sophisticated than solving subject of future work. Our computational results demon-less. 
Another avenue for future research is that of incremen-that can be updated incrementally as new data points come streaming in. 
To sum up, the principal contribution of this work, is a sifter. 00-02, February 2001, was supported by National Science J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen. LAPACK User's Guide. SIAM, Philadelphia, 
Pennsylvania, third edition, 1999. http://www.netlib.org/lapack/. programming discrimination of two linearly inseparable sets. Optimization Methods and Software, 1:23-34, 1992. discrimination via linear support vector machines. 
Optimization Methods and Software, 13:1-10, 2000. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-03.ps. from: www.sgi.com/Technology/mlc/db/. machines for pattern recognition. Data Mining and 
Knowledge Discovery, 2(2):121-167, 1998. Concepts, Theory and Methods. John Wiley &amp; Sons, 
New York, 1998. 1992. networks and support vector machines. Advances in Computational Mathematics, 13:1-50, 2000. networks and support vector machines. In A. Smola, P. Bartlett, B. Sch51kopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 171-203, 
Cambridge, MA, 2000. MIT Press. for massive support vector machines. Technical Report 00-05, Computer Sciences Department, University of 
Wisconsin, Madison, Wisconsin, May 2000. ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/00-05.ps. support vector machine classification. In R. Ramakrishnan and S. Stolfo, editors, Proceedings August 20-23, 2000, Boston, MA, pages 64-70, New 
York, 2000. Asscociation for Computing Machinery. ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/00-02.ps. [13] D. Gale. The Theory of Linear Economic Models. [14] G. H. Golub and C. F. Van Loan. Matrix [15] J. Gracke, M. Griebel, and M. Thess. Data mining [16] T. Joachims. Making large-scale support vector [17] Y.-J. Lee and O. L. Mangasarian. RSVM: Reduced [18] Yuh-Jye Lee and O. L. Mangasarian. SSVM: A smooth [19] O. L. Mangasarian. Nonlinear Programming. SIAM, [20] O. L. Mangasarian. Generalized support vector [21] O. L. Mangasarian and R. R. Meyer. Nonlinear [22] O. L. Mangasarian and D. R. Musicant. Successive [23] O. L. Mangasarian and D. R. Musicant. Active [24] O. L. Mangasarian and D. R. Musicant. Lagrangian Wisconsin, June 2000. Journal of Machine Learning 
Research, to appear. ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/00-06.ps. continuity of solutions of linear inequalities, programs and complementarity problems. SIAM Journal on Control and Optimization, 25(3):583-595, May 1987. 
Natick, MA 01760, 1994-2001. http://www.mathworks.com. 
Boston, 1997. machine learning databases, 1992. www.ics.uci.edu/~mlearn/MLRepository.html. datasets, 1998. www.cs.wisc.edu/~-.musicant/data/ndc/. 
R. Humphreys, and W. Zumach. Automated star/galaxy discrimination with neural networks. 
Astronomical Journal, 103(1):318-331, 1992. algorithm for training support vec X or machines. In 
B. SchSlkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods -Support Vector 
Learning, pages 185-208. MIT Press, 1999. http://www.research.microsoft.com/~jplatt/smo.html. J. Schiirmann (editors). Advances in Large Margin 
Classifiers. MIT Press, Cambridge, MA, 2000. support vector machine classifiers. Neural Processing 
Letters, 9(3):293-300, 1999. 
Ill-Posed Problems. John Wiley ~: Sons, New York, 1977. 
Theory. Springer, New York, 1995. Theory. Springer, New York, second edition, 2000. cgi.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/neural/bench/cmu/0.html. Method p-value * p-value* 0.71 0.71 75.8% 73.7"% 20.65 0.97 98.4% 395.30 
I 0.09 0.79 p-value * p-value* p-vMue* 70.8% 68.5% 0.17 1 94.3 % 88.7 % 1.23 0.55 87.3% 86.2% 0.7 0.91 78.2% 77.6% 0.78 0.95 70.2% 70.0% 0.78 0.84 95.0% 95.0% 5.21 4 x 10 -4 0.49 0.49 0.48' 
