 Jason Wolfe jawolfe@cs.berkeley.edu Aria Haghighi aria42@cs.berkeley.edu Dan Klein klein@cs.berkeley.edu With dramatic recent increases in both data scale and multi-core environments, it has become increasingly important to understand how machine learning algo-rithms can be efficiently parallelized. Many computa-tions, such as the calculation of expectations in the E-step of the EM algorithm, decompose in obvious ways, allowing subsets of data to be processed independently. In some such cases, the MapReduce framework (Dean &amp; Ghemawat, 2004) is appropriate and sufficient (Chu et al., 2006). Specifically, MapReduce is suitable when its centralized reduce operation can be carried out ef-ficiently. However, this is not always the case. For ex-ample, in modern machine translation systems, many millions of words of example translations are aligned using unsupervised models trained with EM (Brown et al., 1994). In this case, one quickly gets to the point where no single compute node can store the model pa-rameters (expectations over word pairs in this case) for all of the data at once, and communication required for a centralized reduce operation dominates computa-tion time. The common solutions in practice are either to limit the total training data or to process manage-able chunks independently. Either way, the complete training set is not fully exploited.
 In this paper, we propose a general framework for dis-tributing EM and related algorithms in which not only is the computation distributed, as in the map and reduce phases of MapReduce, but the storage of pa-rameters and expected sufficient statistics is also fully distributed and maximally localized. No single node needs to store or manipulate all of the data or all of the parameters. We describe a range of network topologies and discuss the tradeoffs between commu-nication bandwidth, communication latency, and per-node memory requirements. In addition to a general presentation of the framework, a primary focus of this paper is the presentation of experiments in two ap-plication cases: word alignment for machine transla-tion (using standard EM) and topic modeling with LDA (using variational EM). We show empirical re-sults on the scale-up of our method for both applica-tions, across several topologies.
 Previous related work in the sensor network literature has discussed distributing estimation of Gaussian mix-tures using a tree-structured topology (Nowak, 2003); this can be seen as a special case of the present ap-proach. Paskin et al. (2004) present an approxi-mate message passing scheme that uses a junction tree topology in a related way, but for a different purpose. In addition, Newman et al. (2008) present an asyn-chronous sampling algorithm for LDA; we discuss this work further, below. None of these papers have dis-cussed the general case of distributing and decoupling parameters in M-step calculations, the main contribu-tion of the current work. Although our framework is more broadly applicable, we focus on the EM algorithm (Dempster et al., 1977), a technique for finding maximum likelihood param-eters of a probabilistic model with latent or hidden variables. In this setting, each datum d i consists of a pair ( x i ,h i ) where x i is the set of observed variables and h i are unobserved. We assume a joint model over P ( x i ,h i |  X  ) with parameters  X  . Our goal is to find a  X  that maximizes the marginal observed log-likelihood P i =1 log P ( x i |  X  ). Each iteration consists of two steps: where the expectation in the M-Step is taken with re-spect to the distribution q (  X  ) over the latent variables found in the E-Step. When P (  X |  X  ) is a member of the exponential family, the M-Step reduces to solving a set of equations involving expected sufficient statistics under the distribution. Thus, the E-Step consists of collecting expected sufficient statistics  X  = E  X  P (  X  | X ) with respect to q i for each datum x i . We briefly present two EM applications we use for experiments. 2.1. Word Alignment Word alignment is the task of linking words in a cor-pora of parallel sentences. Each parallel sentence pair consists of a source sentence S and its translation T into a target language. 1 The model we present here is known as IBM Model 1 (Brown et al., 1994). 2 In this model, each word of T is generated from some word of S or from a null word  X  prepended to each source sentence. The null word allows words to appear in the target sentence without any evidence in the source. Model 1 is a mixture model, in which each mixture component indicates which source word is responsible for generating the target word (see figure 1). The formal generative model is as follows: (1) Select a length n for the translation T based upon | S | = m (typically uniform over a large range). (2) For each j = 1 ,...,n , uniformly choose some source alignment position a j  X  X  0 , 1 ,...,m } . (3) For each j = 1 ,...,n , choose target word t j based on source word s a j with probability  X  s a In the data, the alignment variables a are unobserved, and the parameters are the multinomial distributions  X  s  X  for each source word s . The expected sufficient statistics are expected alignment counts between each source and target word that appear in a parallel sen-tence pair. These expectations can be obtained from the posterior probability of each alignment, The E-Step computes the above posterior for each alignment variable; these values are added to the cur-rent expected counts of ( s,t ) pairings, denoted by  X  st . The M-Step consists of the following update:  X  model on a data set with more than 243 million pa-rameters (i.e., distinct co-occurring word pairs). 2.2. Topic Modeling We present experiments in topic modeling via the La-tent Dirichlet Allocation (Blei et al., 2003) topic model (see figure 2). In LDA, we fix a finite number of topics T and assume a closed vocabulary of size V . We as-sume that each topic t has a multinomial distribution  X  t  X   X  Dirichlet(Unif( V ) , X  ). Each document draws a topic distribution  X   X  Dirichlet(Unif( T ) , X  ). For each word position in a document, we draw an unobserved topic index z from  X  and then draw a word from  X  z  X  . Our goal is to find the MAP estimate of  X  for the observed likelihood where the latent topic indicators and document topic distributions  X  have been inte-grated out. In this setting, we can not perform an exact E-Step because of the coupling of latent vari-ables through the integral over parameters. Instead, we use a variational approximation of the posterior as outlined in Blei et al. (2003), where all parameters and latent variables are marginally independent. The relevant expected sufficient statistics for  X  are the ex-pected counts  X  tw over topic t and word w pairings under the approximate variational distribution. The M-Step, as in the case of our word alignment model in section 2.1, consists of normalizing these counts:  X  model. We note that the number of parameters in this model is a linear function of the number of topics T . Given the amount of data and number of parameters in many EM applications, it is worthwhile to distribute the algorithm across many machines. We will consider the setting in which our data set D has been divided into k splits {D 1 ,..., D k } . 3.1. Distributing the E-Step Distributing the E-Step is relatively straightforward, since the expected sufficient statistics for each datum can be computed independently given a current esti-mate of the parameters. Each of k nodes computes expected sufficient statistics for one split of the data, where we use the superscript ( i ) to emphasize that these counts are partial and reflect only the contribu-tions from split D i and not contributions from other partitions. We will also write  X  i for the set of suffi-cient statistic indices that have nonzero count in  X  ( i ) and use  X  [  X  i ] to indicate the projection of  X  onto the subspace consisting of just those statistics in  X  i . In order to complete the E-Step, we must aggregate expected counts from all partitions in order to re-estimate parameters. This step involves distributed communication of a potentially large number of statis-tics. We name this phase the C-Step and will examine how to efficiently perform it in section 4. For the mo-ment, we assume that there is a single computing node which accumulates all partial sufficient statistics, where we write  X  ( i ) [  X  i ] to indicate that we only com-municate non-zero counts. This is a simple and effec-tive way to achieve near-linear speedup in the E-Step; previous work has utilized it effectively (Blei et al., 2003; Chu et al., 2006; Nowak, 2003). 3.2. Distributing the M-Step A further possibility, which to our knowledge has not been fully exploited, is distributing the M-Step. Often in EM, it is the case that only a subset of parameters may ever be relevant to a split D i of the data. For instance, in the word alignment model of section 2.1, if a word pairing ( s,t ) is not observed in some D i , node i will never need the parameter  X  st . For our full word alignment data set, when k = 20, less than 30 million of the 243 million total parameters are relevant to each node.
 We will use  X  i to refer to the subset of parameter in-dices relevant for D i . In order to distribute the M-Step, each node must receive all expected counts nec-essary to re-estimate all relevant parameters  X  [  X  i ]. In section 4, we develop different schemes for how nodes should communicate their partial expected counts, and show that this choice of C-Step topology can dramat-ically affect the efficiency of distributed EM. One difficulty in distributing the M-Step lies in the fact that re-estimating  X  [  X  i ] may require counts not found in  X  [  X  i ]. In the case of the word alignment model,  X  st requires the counts  X  st 0 for all t 0 appearing with s in a sentence pair, even if t 0 did not occur in D i . Often these non-local statistics enter the computation only via normalization terms. This is the case for the word alignment and LDA models explored here. This obser-vation suggests an easy way to get around the problem presented above in the case of discrete latent variables: we simply augment the set of sufficient statistics  X  with a set of redundant sum terms that provide the missing information needed to normalize parameter estimates. For the word alignment model, we would include a suf-ficient statistic  X  s  X  to represent the sum P t :( s,t )  X  X  Then the re-estimated value of  X  st would simply be requires only  X  st and  X  s  X  for all ( s,t )  X  D i . It might seem counterintuitive, but adding these extra statis-tics actually decreases the total necessary amount of communication, by trading a large number of sparse statistics for a few dense ones. This section will consider techniques for performing the C-Step of distributed EM, in which a node i ob-tains the necessary sufficient statistics  X  [  X  i ] to esti-mate parameters  X  [  X  i ]. We assume that the sets of relevant count indices  X  i have been augmented as dis-cussed at the end of section 3 so that  X  [  X  i ] is sufficient to re-estimate  X  [  X  i ].
 4.1. MapReduce Topology A straightforward way to implement the C-Step is to have each node send its non-zero partial counts  X  ( i ) [  X  to a central  X  X educe X  node for accumulation into  X  . This central node then returns only the relevant com-pleted counts  X  [  X  i ] to the nodes so that they can inde-pendently perform their local M-Steps. This approach, depicted in figure 3(a), is roughly analogous to the topology used in the MapReduce framework (Dean &amp; Ghemawat, 2004). When parameters are numerous, this will already be more bandwidth-efficient than a naive MapReduce approach, in which the Reduce node would perform a global M-Step and then send all of the new parameters  X  back to all nodes for the next iter-ation. To enable sending only relevant counts  X  [  X  i ], the actual iterations are preceded by a setup phase in which each node constructs an array of relevant count indices  X  i and sends this to the Reduce node. This array also fixes an ordering on relevant statistics, so that later messages of counts can be densely encoded. This MapReduce topology 3 may be a good choice for the C-Step when nodes share most of the same statistics. On the other hand, if sufficient statistics are sparse and numerous, the central reduce node can be a significant bandwidth and memory bottleneck in the distributed EM algorithm. Indeed, in practice, with either Model 1 or LDA, available amounts of train-ing data can and do easily cause the sufficient statis-tics vectors to exceed the memory of any single node. The MapReduce topology for estimation of LDA has been discussed in related work, notably Newman et al. (2008), though they do not consider the sparse distri-bution of the M-step, which is necessary for very large data sets. 4.2. AllPairs Topology MapReduce takes a completely centralized approach to implementing the C-Step, in which the accumula-tion of  X  at the Reduce node can be slow or even infea-sible. This suggests a decentralized approach, in which nodes directly pass relevant counts to one another and no single node need store all of  X  or  X  . This section describes one such approach, AllPairs , which in a sense represents the opposite extreme from MapRe-duce . In AllPairs , the network graph is a clique on the k nodes, and each node i passes a message m ij =  X  ( i ) [  X  i  X   X  j ] to each other node j containing precisely the statistics j needs and nothing more (see figure 3(b)). Each node j then computes its completed set of sufficient statistics with a simple summation: AllPairs requires a more complicated setup phase, where each node i calculates, for roughly half of the other nodes, the intersection  X  i  X   X  j of its parame-ters with the other node j  X  X . 4 Node i then sends the contents of this intersection to j .
 In each iteration, message passing proceeds asyn-chronously, and each node begins its local M-Step as soon as it has finished sending and receiving the neces-sary counts. An important point is that, to avoid dou-ble counting, a received count cannot be folded into a node X  X  local statistics until the local copy of that count has been incorporated into all outgoing messages. AllPairs is attractive because it lacks the bandwidth bottleneck of MapReduce , all paths of communica-tion are only one hop long, and each node need only be concerned with precisely those statistics relevant for its local E-and M-steps. On the down side, AllPairs needs a full crossbar connection between nodes, and requires unnecessarily high bandwidth for dense suffi-cient statistics that are relevant to datums on many nodes. In particular, a statistic that is relevant to k 0 nodes must be passed k 0 ( k 0  X  1) times, as compared to an optimal value of 2( k 0  X  1) (see section 4.3). 4.3. JunctionTree Topology A tree-based topology related to the junction tree ap-proach used for belief propagation in graphical models (Pearl, 1988) can avoid the bandwidth bottleneck of MapReduce and the bandwidth explosion of All-Pairs . In this approach, the k nodes are embedded in an arbitrary tree structure T , and messages are passed along the edges in both directions (see figure 3(c)). We are certainly not the first to exploit such structures for distributing computation; see particularly Paskin et al. (2004), who use it for inference rather than estimation. We first describe the most bandwidth-efficient method for communicating partial results about a single statis-tic, and then show how this can be extended to pro-duce an algorithm that works for the entire C-Step. Consider a single sufficient statistic  X  x (e.g., some  X  st for Model 1) which is only relevant to E-and M-Steps on some subset of machines S . Before the C-Step, each node has  X  ( i ) x , and after communication each node should have  X  x = P i  X  S  X  ( i ) x . We cannot hope to ac-complish this goal by passing fewer than 2( | S |  X  1) pairwise messages; clearly, it must take at least | S | X  1 messages before any node completes its counts, and then another | S |  X  1 messages for each of the other | S | X  1 nodes to complete theirs too. This is fewer mes-sages than either MapReduce or AllPairs passes. This theoretical minimum bandwidth can be achieved by embedding the nodes of S in a tree. After desig-nating an arbitrary node as the root, each node accu-mulates a partial sum from its subtree and then passes it up towards the root. Once the root has accumu-lated the completed sum  X  x , it is recursively passed back down the tree until all nodes have received the completed count, for a total of 2( | S | X  1) messages. Of course, each node must obtain a set of complete relevant statistics  X  [  X  i ] rather than a single statistic  X  . One possibility is to pass messages for each suffi-cient statistic on a separate tree; while this represents the bandwidth-optimal solution for the entire C-step, in practice the overhead of managing 240 million different message trees would likely outweigh the benefits. Instead, we can simply force all statistics to share the same global tree T . In each iteration we proceed much as before, designating an arbitrary root node and pass-ing messages up and then down, except that now the message m ij from node i to j conveys the intersec-tion of their relevant statistics  X  i  X   X  j rather than a single number. For this to work properly, we require that T has the following running intersection property: for each sufficient statistic, all concerned nodes form a connected subtree of T . In other words, for all triples of nodes ( i,x,j ) where x is on the path from i to j , we must have (  X  i  X   X  j )  X   X  x . We can assume that this property holds, by augmenting sets of statistics at interior nodes if necessary.
 When the running intersection property holds, the message contents can be expressed as where T i is used to represent the subtree rooted at i , and  X  ( T i ) is the sum of statistics from nodes in this subtree. Thus, the single global message passing phase can be thought of as |  X  | separate single-statistic mes-sage passing operations proceeding in parallel, where the root of each such sub-phase is the node in its sub-tree closest to the global root, and irrelevant opera-tions involving other nodes and statistics can be ig-nored. In our actual implementation, we instead use an asynchronous message-passing protocol common in probabilistic reasoning systems (Pearl, 1988), which avoids the need to designate a root node in advance. The setup phase for JunctionTree proceeds as fol-lows: (1) All pairwise intersections of statistics are computed and saved to shared disk. (2) An arbitrary node chooses and broadcasts a directed, rooted tree T on the nodes which optimizes some criterion. (3) Each node (except the root) constructs the set of statistics that must lie on its incoming edge, by taking the union of the intersections of statistics (which can be reread from disk) for all pairs of nodes on opposite sides of the edge. 5 (4) Each node passes the constructed edge set along its incoming edge, fixing future message struc-tures in the process. (5) Each node augments its  X  i to include all statistics in local outgoing messages, thus enforcing the running intersection property. To choose a heuristically good topology, we use the maximum spanning tree (MST) with edge weights equal to the sizes of the intersections |  X  i  X   X  j | , so that nodes with more shared statistics tend to be closer to-gether. This heuristic has been successfully used in the graphical models literature (Pearl, 1988) to construct junction trees. However, in general one can imagine much better heuristics that also consider, e.g., max degree, tree diameter or underlying network structure. If statistics tend to be well-clustered within and be-tween nodes, we can expect this MST to require less bandwidth than either alternate topology, and (like AllPairs ) there should be no central bandwidth bot-tleneck. On the other hand, if statistics tend to be shared between only a few nodes and this sharing is not appropriately clustered, bandwidth and memory may increase because many statistics will have to be added to enforce the running intersection property. 6 Furthermore, if the diameter of the tree is large, la-tency may become an issue as many sequential message sending and incorporation steps will have to be per-formed. Finally, the setup phase takes longer because choosing the tree topology and enforcing the running intersection property may be expensive. Despite these potential drawbacks, we will see that MST generally performs best of the three topologies investigated here in terms of both bandwidth and total running time. As a final note, if T is a  X  X ub and spoke X  graph, and the hub X  X  statistics are augmented to contain all of  X  , a MapReduce variant is recovered as a special case of JunctionTree . This is the version of MapReduce we actually implemented; it differs from the version described in section 4.1 only in that the role of reduce node is assigned to one of the workers rather than a separate node, which reduces bandwidth usage. We performed experiments using the word alignment model from section 2.1 and the LDA topic model from section 2.2. For each of these models, we com-pared the network topologies used to perform the C-Step and how they affect the overall efficiency of EM. We implemented the following topologies (described in section 4): MapReduce , AllPairs , and Junc-tionTree . Although our implementation was done in Java, every reasonable care was taken to be time and memory efficient in our choice of data structures and in network socket communication. All experiments were performed on a cluster of identical, load-free 3.0 GHz 32-bit Intel machines. Running times per iteration represent the median over 10 runs of the maximum time on any node. We also examine the bandwidth of each topology, measured by the number of counts communicated across the network per iteration. 5.1. Word Alignment Results We performed Model 1 (see section 2.1) experiments on the UN Arabic English Parallel Text TIDES Ver-sion 2 corpus, which consists of about 3 million sen-tences of translated UN proceedings from 1994 until 2001. 7 For the full data set, there are more than 243 million distinct parameters.
 In table 1(a), we present results where the number of sentence-pair datums per node is held constant at 145K and the number of nodes (and thus total training data) is varied. For 10 or more nodes, the MapRe-duce topology runs out of memory due to the num-ber of statistics that must be stored in memory at the Reduce node. 8 In contrast, both AllPairs and JunctionTree complete training for the full data set distributed on 20 nodes.
 We also experimented with the setting where we fix the total amount of data at 200K sentences, but add more nodes to distribute the work. Figure 4 gives iteration times for all three topologies broken down according to E-, C-, and M-Steps. The MapReduce graph (fig-ure 4(a)) shows that the C-Step begins dominating run time as the number of nodes increases. This effect reduces the benefit from distributing EM for larger numbers of nodes. Both AllPairs and Junction-Tree have substantially smaller C-Steps, which con-tributes to much faster per-iteration times and also allows larger numbers of nodes to be effective. On the full dataset, JunctionTree outperforms All-Pairs , but not by a substantial margin. Although the two topologies have roughly comparable running times, they have different network behaviors. Figure 5, which compares bandwidth usage in billions of counts transferred over the network per iteration, shows that AllPairs uses substantially more bandwidth than ei-ther MapReduce or JunctionTree . This is due to the O ( k 2 ) number of messages sent per iteration. In contrast, JunctionTree typically has a higher la-tency due to the fact that nodes must wait to receive messages before they can send their own. AllPairs and JunctionTree with the MST heuristic represent a bandwidth and latency tradeoff, and the choice of which to use depends on the properties of the partic-ular network. 5.2. Topic Modeling Results We present results for the variational EM LDA topic model presented in section 2.2. Our results are on the Reuters Corpus Volume 1 (Lewis et al., 2004). This corpus consists of 804,414 newswire documents, where all tokens have been stemmed and stopwords removed. 9 There are approximately 116,000 unique word types after pre-processing. The number of pa-rameters of interest is therefore 116,000 T , where T is the number of topics that we specify.
 We experimented with this model on the entire corpus and varied the number of topics. The largest num-ber of topics we used was T = 1,000, which yields 116 million unique parameters. Our results on iteration time are presented in figure 6. Note that the number of parameters depends linearly on the number of top-ics, which can roughly be seen in figure 6. This figure demonstrates that the efficiency of the AllPairs and JunctionTree topologies as the number of parame-ters increases. We see that JunctionTree edges out AllPairs for a larger number of topics.
 Table 1(b) shows detailed results for the experiment depicted in figure 6. Besides the difference in itera-tion times for the three algorithms as the number of topics (and statistics) grows, there are at least two other salient points. First, while the number of to-tal statistics grows similarly to in the word alignment experiments, here the number of unique statistics is significantly smaller (i.e., each statistic, on average, is relevant to more nodes). This leads to significantly worse performance, especially in terms of bandwidth, for AllPairs . A second point is that setup times are much lower than for word alignment, because sets of relevant words can be determined first, and only then expanded to ( word,topic ) pairs.
 We note that the total bandwidth is actually lower for MapReduce than JunctionTree since the MST only heuristically minimizes the number of discon-nected statistic components, rather than the true cost of enforcing the running intersection property. Despite this, the bandwidth bottleneck for JunctionTree is still much lower than for MapReduce . We have demonstrated theoretically and empirically that a distributed EM system can function success-fully, allowing for both significant speedup and scaling up to computations that would be too large to fit in the memory of a single machine. Future work will con-sider applications to other machine learning methods, alternative junction tree heuristics, and more general graph topologies.
 The authors of this work were supported (respectively) by DARPA IPTO contract FA8750-05-2-0249, a Mi-crosoft Research Fellowship, and a Microsoft Research New Faculty Fellowship.

