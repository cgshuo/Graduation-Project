 1. Introduction
With the evolution of information and communication technologies in recent years, the need to store and data types include free text, images, audio, video, voice, genomic and biometric data, and so forth. There is therefore an increasing need for new database technologies designed to manage such repositories.
A prominent research challenge posed by these new technologies is efficient search. Search operations in traditional database systems have been commonly applied to structured data, that is, numerical or alphabet-and keys, traditional search methods can not be applied to these data types. More general models and algo-rithms are therefore required.

A unifying and general searching approach that emerged in recent years is that of similarity search , where tance function. Let U be a universe of objects, and d : U U ! R among them. This distance function satisfies the three axioms that make the pair M  X  X  U ; d  X  a metric space: suitable for numerous search problems in unstructured data applications, such as: retrieval by contents from multimedia databases (storing images, audio, video, etc.); biometric identification systems (based on finger-cost model commonly used defines CPU costs as dominated by distance computations (since these computa-tions are assumed to be highly expensive for most unstructured data applications), and I/O costs as dominated by disk accesses.

Substantial work has been done on indexing (or access) methods for one-dimensional and multi-dimen-domination of I/O costs over other costs  X  all of which are generally inappropriate for unstructured data applications.

To overcome these restrictions, a different class of indexing methods, called metric access methods , was to organize and partition the search space. Although shown effective, the applicability of these methods to designed for static data and do not allow dynamic operations. Those that do permit dynamic operations com-monly support only insertions, and most of them require costly reorganizations to prevent performance reduc-tion. Most of the existing methods are main memory based, thus limiting their scalability. Furthermore, most of them focus on reducing the number of distance evaluations (or at most the total CPU cost), but neglect the
I/O costs. Last but not least, existing dynamic methods do not sufficiently prevent the development over time of large empty indexed volumes (i.e. volumes that do not contain actual data objects) and large overlaps between volumes indexed by the tree nodes, which were observed (for instance in [14,34] ) to significantly increase query processing costs, as more paths should be traversed for answering a query. Existing attempts to alleviate these impediments are based on costly post-construction procedures.

In this article we present a new dynamic metric access method, named CM-tree (Clustered Metric tree), which overcomes the problems listed above of existing metric access methods, and yields significantly improved search performance. It reduces simultaneously both CPU costs (i.e. distance computations) and methods, the CM-tree is especially designed to minimize the empty indexed volumes and the overlapping vol-umes in the tree via its primary construction algorithms (instead of costly post-processing). This enables to maintain a highly well-clustered structure and yield significantly improved search performance (which is dem-clustering based node split algorithm; and an improved sub-tree pruning method used during search. To facil-itate these methods the pairwise distances between the objects of a node are maintained within each node.
Results from an extensive experimental study show that the CM-tree significantly outperforms the M-tree [14] and the Slim-tree [34] , improving search performance by up to 312% for I/O costs and 303% for CPU costs, as well as demonstrate its robustness to the data sets X  dimensionality and scale.
This article is organized as follows: In Section 2 we briefly review related work. In Section 3 we introduce the CM-tree X  X  definition and structure. In Sections 4and5 the CM-tree X  X  search algorithms for range and nearest neighbor queries correspondingly are presented. In Section 6 the CM-tree X  X  construction algorithms sented, and then issues and sub-algorithms of interest are detailed. The proofs of all lemmas, theorems and corollaries are presented in Appendix . In Section 7 we report experimental results. Finally, in Section 8 we conclude our work and discuss future directions. 2. Related work
Multi-dimensional access methods have been considered to index metric databases, although they have sig-include hash based methods and methods using space filling curves. An extensive survey on this topic appears in [21] .

Another approach to index a metric space is to map it into a vector space, such that the distances and the overall structure of the data set are preserved, and then use some multi-dimensional access method for index-ing. Methods taking this approach are FastMap [19] and BoostMap [2] .

Single-dimensional access methods were also suggested for metric space indexing. The iDistance method reference points for each partition. The points in each partition are then transformed into single dimension values based on their proximity to their reference point.

Most of the work on indexing metric spaces is focused on metric access methods. The majority of these these pivots. Given a query object, its distances to the pivots are computed, and the triangle inequality is the distances to them. One slice of the space corresponds to each sub-tree. At each sub-tree, new pivots are between the objects in the database. The iAESA method [20] improves on AESA by choosing more effective pivots. Methods for selection of effective pivots are presented in [9] .

Compaction based methods partition the space into regions as compactly as possible, typically recursively,
Methods that use this criterion include the gh-tree [35] and GNAT [7] . The second criterion is the covering radius, which is the maximum distance between the representative object of a region and the objects in its of each cluster. The M-tree [14] is a paged and balanced metric tree allowing dynamic operations and reducing CPU and I/O costs. Data objects are stored in or pointed by leaf nodes. Internal nodes store representative that minimizes the required extension of its covering radius. Overflowing nodes are handled by a split method in [15] for the M-tree. The Slim-tree [34] improves on the M-tree X  X  construction performance by using a min-imal spanning tree based splitting algorithm, and improves query performance by using an insertion strategy based on node occupancy and applying a post-processing algorithm called Slim-down to reduce the overlap-ping volumes in the tree. The PM-tree [33] augments the M-tree with hyper-rings related with a fixed set of global pivots, so that a region associated with a node is formed by an intersection of its hyper-sphere and the hyper-rings, consequently producing tighter regions.

Another approach is searching by spatial approximation. The idea is to start with some object in the space a k-NN graph is a weighted directed graph connecting each element to its k nearest neighbors. Extensive sur-veys on metric access methods appear in [11,24] . 3. The CM-tree: definition and structure
The CM-tree is a dynamic, paged and balanced metric tree. The data in the tree is organized in fixed size directory node by c L and c D correspondingly.
 The tree nodes are organized in a hierarchical structure. Each node is associated with a region of the space.
The region associated with each node N completely encloses the regions associated with all the nodes refer-
Each node N , except for the root, is associated with a navigation object O porated into its parent node, such that the region associated with N is the hyper-sphere centered on O radius R ( O n ). An entry of a navigation object, Entry  X  O object X  X  properties necessary for distance calculations S ( O the root node of its referenced sub-tree T ( O n ). All the objects stored in its referenced sub-tree T ( O a distance no larger than R ( O n )to O n , i.e. 8 O j 2 T  X  O entry, Entry  X  O l  X  X  X  S  X  O l  X  ; I  X  O l  X  , is stored in a leaf node, where S ( O the object itself. The CM-tree can be also used for primary data organization, thus replacing I ( O
Each CM-tree node N (leaf or directory) is structured as follows: Structure  X  N  X  X   X  P  X  N  X  ; Y  X  N  X  ; E  X  N  X  ; DT  X  N  X  ; Entry  X  O 1  X  ; ... ; Entry  X  O
Fig. 1 illustrates the structure of the CM-tree, for five database objects v , w , x , y ,and z . 4. Range search algorithm
The CM-tree X  X  RangeSearch algorithm, given in Fig. 2 , follows all the paths in the tree that can not be
The following lemmas show that the algorithm is correct, namely it does not discard paths that lead to objects satisfying the query condition (proofs appear in Appendix ). Consider a point in time where the algo-rithm accesses node N . To determine which of the sub-trees pointed by node N can be safely pruned from the search, the query object Q and the radius R ( Q ) are compared to the objects { O
Lemma 1. If d  X  Q ; O i  X  &gt; R  X  Q  X  X  R  X  O i  X  , then for each object O
Corollary 1. If d  X  Q ; O i  X  &gt; R  X  Q  X  X  R  X  O i  X  , then T(O be used to avoid this computation. Consider that we have already computed the distance between the query object Q and some object O 0 in the node.

Lemma 2. If j d  X  Q ; O 0  X  d  X  O i ; O 0  X j &gt; R  X  Q  X  X  R  X  O
Corollary 2. If j d  X  Q ; O 0  X  d  X  O i ; O 0  X j &gt; R  X  Q  X  X  R  X  O
For a given object O i , it issufficient that the condition of Lemma 2 holdsfor a single O
O  X  X sub-treefromthesearch.TheCM-tree X  X  RangeSearch algorithm isdifferentfromexistingalgorithmsbyitsabil-enables all the CM-tree algorithms to achieve very high performance (shown experimentally later). 5. Nearest neighbor search algorithm
The CM-tree X  X  k-NN algorithm, NearestNeighborSearch , is based on a priority backtracking approach, any given moment the sub-trees to be traversed ordered by their priorities (defined shortly). When the most promising sub-tree is extracted from the queue, its referenced sub-trees may be added to the queue (unless
An object or sub-tree is inserted into the list if its distance to Q is smaller than d in the list and possibly reducing d k . Smaller values of d
Two bounds, d L and d U , for a given sub-tree T ( O i ) referenced by O correspondingly, on the distance between Q and its closest object contained in T ( O d  X  T  X  O i  X  X  X  d L  X  Q ; T  X  O i  X  X  X  MAX f d  X  Q ; O i  X  R  X  O rectness is shown in Lemmas 3 and 4 .

Lemma 3. Given a query object Q and a sub-tree T(O i ), for each object O d  X  T  X  O i  X  X  6 d  X  Q ; O j  X  .

Lemma 4. Given a query object Q and a sub-tree T(O i ), let O Then d U  X  T  X  O i  X  X  P d  X  Q ; O 0  X  .
 The sub-trees X  priorities are based on the combination of d tree, its d U value is used to consider its insertion into the list and possible reduction of d used for pruning considerations according to Lemmas 5 and 6 and Corollaries 3 and 4 .
A key difference of the proposed algorithm is a pruning method based on Lemma 6 and on the pairwise of the algorithm.
 larger than d k .
 Corollary 3. Any sub-tree T(O i ) for which d L  X  T  X  O i
Lemma 6. Given a query object Q, a sub-tree T(O i ) and an object O d(O i ,O 0 ) are known, if j d  X  Q ; O 0  X  d  X  O i ; O 0  X j P d
Corollary 4. Given a query object Q and an object O 0 , any sub-tree T(O d  X  R  X  O i  X  can be safely pruned from the search.
The proposed algorithm is correct, i.e. it does not discard any sub-trees that contain objects satisfying the query condition, because it prunes sub-trees according to Corollaries 3 and 4 , which are proven to prune safely, and since d k , which is used by these corollaries, is not too low, according to Lemma 5 . 6. Constructing the CM-tree 6.1. Inserting objects into the CM-tree The insertion algorithm proposed in this section specifies the method for inserting a single object into a
CM-tree. This algorithm is especially designed to minimize the empty indexed volumes (i.e. volumes cov-ered by the tree nodes which do not contain any objects) and minimize the overlapping volumes between the tree nodes. Reduction of the empty indexed volumes decreases the volumes covered by navigation objects and shortens their covering radii. This leads to improved pruning ability during search and to reduction in the overlapping volumes. This in turn decreases the number of paths that should be traversed and the number of objects that should be inspected during search, thus significantly improving search performance.

The insertion algorithm operates as follows: It descends down the tree to locate the most suitable leaf node ity, or, uniquely to the CM-tree, if the new object entails structural changes in an extent that exceeds some are specified in the next sub-sections. 6.2. Finding an accommodating leaf node
The algorithm FindAccommodatingLeaf , proposed in this section, finds the most suitable leaf node to store a new object O n . The algorithm follows a path down the tree, descending in each step to the most promising by traversing only a single path. It minimizes CPU costs by using a new pruning method, based on Lemma 7 efficiency.

Lemma 7. Consider the procedure where the algorithm FindAccommodatingLeaf evaluates the sub-trees referenced by navigation objects in a node on a path to a leaf node. Let T(O ones considered until this moment; let T(O i ) be the next sub-tree to be evaluated; and let T(O was already evaluated and for which the value d(O n ,O 0 ) was computed. Note that the distance d(O stored in the pairwise distance table. If one of the following conditions is satisfied then T(O from the search without computing d(O n ,O i ). Condition 1: d  X  O j d  X  O n ; O 0  X  d  X  O i ; O 0  X j &gt; R  X  O i  X  . Condition 2: d  X  O
R  X  O i  X  &lt; R  X  O c  X  and j d  X  O n ; O 0  X  d  X  O i ; O d  X  O i ; O 0  X j &gt; d  X  O n ; O c  X  . 6.3. Choosing and modifying navigation objects
In this section we define an invariant property maintained uniquely by the CM-tree X  X  navigation objects. In general, a CM-tree navigation object is defined as the object which minimizes the covering radius required to cover all of the objects in its referenced node.

The motivation for maintaining this property is hereby explained. Given a set of objects, we can theo-retically choose any of the objects in the set as its associated navigation object. It is clear however that space indexed by this navigation object, which may in addition increase the overlapping volumes in the tree.
In other words, the length of the covering radius associated with the chosen navigation object can be regarded as a measure of the empty space indexed by this navigation object. Therefore, minimizing the cov-igation object, and hence reducing the overlapping volumes in the tree. The following definitions underlie the discussion ahead.

Definition 1. For a given object in a leaf or directory node O
PCR ( O i ), is defined as PCR  X  O i  X  X  MAX O associated with O j , and R ( O j )=0if N is a leaf node.

For a node N and its chosen navigation object O i 2 N , PCR ( O
O , required to cover all the objects of the sub-tree rooted by N .
 Definition 2. For a node N (leaf or directory), its central object , denoted as CO ( N ), is defined as
CO  X  N  X  X  ARGMIN O Definition 3. Let N be a node in a CM-tree. Its navigation object, denoted as NO ( N ), always holds The entry associated with NO  X  N  X  therefore holds Entry  X  NO  X  N  X  X  X  Entry  X  CO  X  N  X  X  X  X  S  X  CO  X  N  X  X  ; PCR  X  CO  X  N  X  X  ; N .

Definition 3 defines the invariant property maintained by the CM-tree X  X  navigation objects, and from which many of the CM-tree X  X  benefits stem. By choosing the central object and its promotion covering radius to be the navigation object of a node N , we minimize the empty space indexed by N , and potentially reduce N  X  X  overlapping volumes with its siblings. Computing the central object and promotion covering radius is easily done by using N  X  X  pairwise distance table (with no additional distance computations required). This property creates a dependency between the navigation objects and the objects contained in their referenced sub-trees. may also require modification. Hence, the proposed algorithm UpdateNavigationObjects , given in Fig. 4 , improving its efficiency. 6.4. Properties of the central object in future work (proofs are given in Appendix ). These theorems analyze the effects of adding and removing an associated with an object in an object set ( Theorems 3 and 4 ) on the central object and promotion covering volumes and on the overlapping volumes in the index structure, as well as their scope of effect on the paths from the modified nodes to the root of the tree. Since various CM-tree algorithms may apply these operations on the index structure, these theorems can be used to reduce the empty indexed volumes and the overlapping volumes, and to reduce the number of nodes and objects which the algorithms process.
Theorem 1. Consider a set of objects S  X f O i g m i  X  1 (possibly contained in a CM-tree node), each of which associated with a covering radius CR  X  O i  X  P 0 . Let O c promotion covering radius. Suppose that a new object O n with an associated covering radius CR  X  O to the set, such that S 0  X  S [f O n g . Let the central object of S associated promotion covering radius be PCR 0  X  O c 0  X  . 1. If (1) d  X  O c ; O n  X  X  CR  X  O n  X  6 PCR  X  O c  X  then PCR and then PCR 0  X  O c 0  X  X  PCR  X  O c  X  ; or alternatively O 2. If (2) d  X  O c ; O n  X  X  CR  X  O n  X  &gt; PCR  X  O c  X  then d  X  O case, either O c 0  X  O c and then d  X  O c ; O n  X  X  CR  X  O d  X  O c ; O n  X  X  CR  X  O n  X  P PCR 0  X  O c 0  X  P PCR  X  O c Fig. 5 illustrates the two cases.

Theorem 2. Consider a set of objects S  X f O i g m i  X  1 (possibly contained in a CM-tree node), each of which associated with a covering radius CR  X  O i  X  P 0 . Let O c promotion covering radius. Suppose that an object O d 2 S with CR  X  O
S  X  S f O d g . Let the new central object of S 0 be O c 0 (which is possibly equal to O covering radius be PCR 0  X  O c 0  X  . 1. If (1) O d 5 O c then PCR 0  X  O c 0  X  6 PCR  X  O c  X  . 2. If (2) O d =O c then: 2.1. If there are no objects in S whose farthest object was O 2.2. If there is at least one object in S whose farthest object was O Fig. 6 illustrates the two cases.

Theorem 3. Consider a set of objects S  X f O i g m i  X  1 (possibly contained in a CM-tree node), each of which associated with a covering radius CR  X  O i  X  P 0 . Let O c promotion covering radius. Suppose that the covering radius associated with an object O larger value CR 0  X  O a  X  , i.e. (1) CR 0  X  O a  X  &gt; CR  X  O
O ) and let its associated promotion covering radius be PCR 1. If (2) PCR  X  O c  X  P d  X  O c ; O a  X  X  CR 0  X  O a  X  then O 2. If (3) PCR  X  O c  X  &lt; d  X  O c ; O a  X  X  CR 0  X  O a  X  then either O PCR 0  X  O c 0  X  P PCR  X  O c  X  .
 Fig. 7 illustrates the two cases.

Theorem 4. Consider a set of objects S  X f O i g m i  X  1 (possibly contained in a CM-tree node), each of which associated with a covering radius CR  X  O i  X  P 0 . Let O c promotion covering radius. Suppose that the covering radius associated with an object O value CR 0  X  O a  X  which is not larger than CR  X  O a  X  , i.e. (1) CR this replacement), let O c 0 be the new central object of S promotion covering radius be PCR 0  X  O c 0  X  . Then PCR 0
Fig. 8 illustrates this case. 6.5. Splitting nodes
In this section we discuss the motivations for splitting CM-tree nodes within the insertion algorithm, and propose novel algorithms for node splitting. The CM-tree maintains an invariant property on its nodes, by nodes are assumed.
 Definition 4. The maximal capacity of a CM-tree node is the maximal number of entries it can physically hold.
Definition 5. Let N be a CM-tree node. Let T be the maximum number of navigation object entries that can be added to a directory node during a single object operation on the tree (elaborated shortly). The net maximal minus T ,if N is a directory node.

The maximal capacity invariant property of the CM-tree nodes states that the contents of a node can not exceed its net maximal capacity (between dynamic operations). Within a dynamic operation, a node that ing a boolean value.

Unlike the split algorithms of existing paged and balanced trees, the proposed split algorithm is aimed not the tree, leading to reduction in the empty indexed volumes and the overlapping volumes and to better per-formance. The following properties distinguish the proposed split algorithm: (1) It may be triggered also by objects of a node into result nodes. This algorithm, denoted as Partition , is designed to reduce the empty indexed volumes and the overlapping volumes in the tree. (3) It may split a node into up to T result nodes (rather than 2), according to both clustering and storage considerations.

The parameter T is an external parameter controlling the freedom of the proposed insertion algorithm to allocate new nodes, and therefore dominating the following trade-off: As T increases the nodes of the tree lapping volumes in the tree, yielding better query performance. However larger T values increase the storage required by the tree for storing the same amount of data and decrease the storage utilization. Furthermore, given in Fig. 9 . 6.6. Algorithm for partitioning the objects of a node
In this section we propose a new algorithm for partitioning the objects of a node within the CM-tree X  X  split algorithm. The proposed algorithm is based on an agglomerative clustering approach [17,23,27,28] , and is especially designed to reduce the empty indexed volumes and the overlapping volumes in the tree, with min-imized impact on storage utilization. The input of the algorithm is a pairwise distance table of a node (thus requiring no distance computations), and upper bounds on the size of a set and on the number of sets in tition of the node X  X  objects into k (2 6 k 6 T ) sets.

The proposed algorithm, denoted as Partition , produces an output partition whose empty indexed space is low compared to other partitions. This is achieved by minimizing the sum of promotion covering radii asso-ing radii and lower empty indexed space is explained in Section 6.3 .
 covering radius CR  X  O i  X  P 0. Let P k j  X f S ja  X f O jai where k 6 m . For a sub-set S ja 2 P k j , let CO  X  S ja  X  and PCR  X  CO  X  S is defined as SPCR  X  P k j  X  X  covering radius CR  X  O i  X  P 0. Let P k  X f P k j g n j  X  1 is the size of P k . The minimal covering radii partition of size k , denoted as P ing 8 P k j 2 P k ; SPCR  X  P k min  X  6 SPCR  X  P k j  X  . The minimal covering radii partition of sizes k
P min , is defined as the one satisfying 8 k 2f k 1 ... k l defined similarly.

The proposed algorithm outputs a partition, denoted by P k larger k value allows a higher degree of freedom in reducing the SPCR value. On the other hand, since k 1 best point on this trade-off, minimizing both the SPCR value and the size of P which is defined next.
 covering radius CR  X  O i  X  P 0. Let P k j  X f S ja  X  t f O where k 6 m . Let T be the maximal size of a candidate partition, l (2 6 l 6 T ) be the minimal size of a candidate partition, and W (0 6 W 6 1) be a weight parameter. b. Let GSIZE  X  P k j ; 2 ; T  X  X  T k T l be a function grading P and size, ranging from 0 for the worst partition, to 1 for the best partition. Two alternatives are defined:  X  GRADE avg  X  GSPCR  X  P k j  X  ; GSIZE  X  P k j  X  X  X  W GSPCR  X  P k  X  GRADE dist  X  GSPCR  X  P k j  X  ; GSIZE  X  P k j  X  X  X  1
The algorithm Partition operates as follows: It employs a bottom-up strategy to gradually cluster the objects together. Starting from a state in which each object is considered as a cluster, the algorithm fuses together in each step individual objects or clusters of objects, which are closest under a distance measure defined next, and whose unified size does not exceed the given upper bound. Using this method the algorithm produces a series of candidate partitions of the given objects, f P one that maximizes the value of GRADE as the output partition.

Definition 9. Let S 1  X f O i g m 1 i  X  1 and S 2  X f O j g following measure is defined as the distance between these two sets: MAX DIST  X  S f d  X  O i ; O j  X g .

The distance measure defined in Definition 9 is known from agglomerative clustering theory as the com-plete-link method. This method is most suitable for the Partition algorithm because of the following reasons.
The complete-link method produces tightly-bound (or compact) clusters with relatively small diameters, since context of the Partition algorithm, tightly-bound clusters lead to minimal SPCR values  X  the complete-link method enables the algorithm to produce partitions whose SPCR values are close to the minimal among the partitions of similar size. We have compared experimentally several additional distance measures and val-idated that the complete-link method is the most suitable for our partitioning algorithm. The Partition algo-rithm is given in Fig. 10 .
 6.7. Clustering criteria for triggering a split of a node
A key idea differentiating the CM-tree from existing access methods is that the split algorithm is also trig-increase in the node X  X  empty indexed volume and overlapping volumes. In order to reduce these effects, such a case causes the proposed insertion algorithm to split that node. The split algorithm partitions the node X  X  objects into at least two sets, one of which containing the new object. (If the new object lies exceptionally far it may be the sole occupant of its set). The new partition minimizes the empty volumes indexed by the gered by clustering considerations is applicable both to leaf and directory nodes (except for the root of the tree).
 that the covering radius associated with the node should be extended if no split occurs. We propose the fol-that node will cause significant increase in the node X  X  empty indexed volume or overlapping volumes. If this comparison yields a positive answer (say based on some threshold), the node is split. We propose several such criteria [1] , and give here a concise sketch.

One criterion compares the distance between the new object and the node X  X  navigation object to the dis-compute this probability for a new object. Another possibility would be to compare the distance of the new object to a threshold based on the average and standard deviation of the existing distances; or alternatively measure the increase in the average of these distances caused by the new object. Another criterion com-pares the average distance between the new object and the node X  X  existing objects to the average distances of the node X  X  existing objects. The comparison may be done using similar methods to those described for the previous criterion. An additional criterion compares the density of objects around the new object with the densities associated with the node X  X  existing objects. The density around an object is defined as the number of objects lying in the hyper-sphere centered on that object with a predefined radius (similarly to the notion of density in [18] ). The higher the density around an object the higher is its probability of belonging to the node X  X  cluster. Here also the comparison may be done using methods similar to those described earlier. 6.8. Deleting objects from the CM-tree
Dynamic operations are generally overlooked by existing metric access methods. Most of them do not allow any dynamic operations. Among the few methods that do permit dynamic operations, most support only single object insertions, and some require costly reorganizations to prevent performance reduction.
An additional key differentiator of the CM-tree is its full support of efficient dynamic deletion as well as algorithm we propose is designed to minimize the empty indexed volumes and the overlapping volumes in the utilization of the tree. For this we propose a new node merging algorithm supporting the deletion algorithm. These algorithms are detailed in [1] , and we give here a brief sketch.

The deletion algorithm descends recursively down the tree to locate the object to be deleted (inside a leaf siderations, thus emptying that node and eliminating it. If the leaf node is indeed merged then the merging for the leaf node (to reflect its new center), and the selection process continues up the tree as required. 7. Experimental results
In this section we report experimental results on the performance of the CM-tree in processing similarity queries and in construction. We have implemented the CM-tree in C++ (complied and tested on Solaris,
Linux and Windows machines). The experiments were performed on a Sun Blade 1000 UltraSPARC-III dual processor machine running Solaris 5.8. The objectives of the experimental study are as follows: to give evi-dence on the efficiency and practicality of the CM-tree; to demonstrate the better performance of the CM-tree compared to existing methods (namely M-tree and Slim-tree); and to study the empirical effects of data set properties and CM-tree parameters on its performance. 7.1. Experimental framework
The experiments were based on real and synthetic data sets. The real data sets are extracted from the  X  X orel has 9 dimensions and consists of color moments data, and the second data set has 16 dimensions and consists of co-occurrence texture data. The synthetic data sets were generated using a program we have implemented.
Each data set consists of 10 normally distributed clusters, with standard deviation of 0.05, and centers uni-formly distributed in the unit hyper-cube. These data sets are of varying number of dimensions  X  2, 5, 10, 20, 30, 40, 50; and of varying sizes  X  (1, 2, 4, 6, 8, 10)  X  10 of dimensions varies, unless stated otherwise, the size of the data sets is 10 dimensional data sets used in the experiments. The distance function used for all the experiments is the L features data sets.

In accordance with the common framework of performance evaluations of metric access methods, we mea-sured and compared the quality of performance as follows: CPU costs were measured by the number of dis-tance computations performed during a given task. I/O costs were measured by the number of unique disk age costs were measured using the storage size required by a given task, and by the storage utilization rate.
Performance evaluations for a given experiment are generally reported in the following facets: construction neighbor) 100 queries were performed on the constructed index structure using pre-sampled query objects. For each data set, 100 query objects of each type were randomly sampled in advance from the data set, and were therefore fixed for that data set. Range queries request objects lying in a query ball whose volume is 1/10 the unit hyper-cube volume, thus having a query radius of nearest objects to the query object. 7.2. Data set dimensionality and node size
In this experiment we examine the effect of the data sets X  dimensionality (i.e. number of dimensions) and the effect of the node size on the CM-tree X  X  performance. The dimensionalities considered range between 2 and 50 dimensions; and the node sizes range between 8 KB and 64 KB.

Figs. 12 X 15 demonstrate that the CM-tree performs well for high as well as low dimensionalities, and is moderate decrease in CPU costs of construction as dimensionality increases (with few exceptions). This is because nodes with smaller capacities have smaller pairwise distance tables, and therefore fewer operations prune entries during query processing.

As for the considered node sizes, Figs. 12 and 13 (a) show that larger node sizes increase CPU and storage costs of construction and reduce I/O costs of construction. The main reasons are that larger node capacities that larger node sizes reduce both I/O (more significantly) and CPU costs of query processing, especially for the capacities of larger node sizes, thus increasing this difference with increasing dimensionality. For our experiments, we chose a node size of 16 KB, which seems as a good point on the trade-off. 7.3. CM-tree versus M-tree and Slim-tree
In this experiment we compare the performance of the CM-tree (denoted as CMT) to that of the M-tree [14] and the Slim-tree [34] , in a controlled environment. All access methods were constructed based on the same data sets, queried with the same queries, and measured using the same performance measures. The M-tree and the Slim-tree were chosen for comparative analysis because they are well-established metric access meth-ods which, like the CM-tree, reduce both I/O and CPU costs while providing dynamic capabilities, thus having a solid ground for comparison. We used the M-tree implementation of its authors [13] and our own implemen-tation of the Slim-tree.

Two M-tree configurations were used in this experiment, which differ in their promotion policy. The pro-motion policy determines the method for selecting representative objects for the nodes resulting from a split operation [14] . The first configuration (denoted as MT_RND) uses the random policy, which selects two rep-resentative objects at random. The second configuration (denoted as MT_MINRD) uses the minimum sum of and was shown in [14] to produce the best query performance. The Slim-tree configuration used in this exper-iment (denoted as SLT) employs the minimum sum of radii policy (shown in [34] to be the most efficient con-sidering query processing), and employs the Slim-down algorithm [34] as a post-processing step (considering the costs of this algorithm as an addition to the costs of construction).

Figs. 16 and 17 show that the CM-tree significantly and consistently outperforms the M-tree and the Slim-tree considering all costs of query processing. The CM-tree reduces I/O costs by an average of 68% compared to the M-tree (both variants) and 57% compared to the Slim-tree, and it reduces CPU costs by an average of 56% compared to the M-tree (both variants) and 49% compared to the Slim-tree, for both types of queries and all dimensionalities. The CPU costs are measured by the distance computations ratio, which is defined as the ratio between the number of distance computations performed during a given task and the number of data due to its better resilience to higher dimensionalities.

Fig. 18 shows that the CM-tree generally requires more CPU and storage costs for construction compared to the other methods. For lower dimensionalities however the CM-tree shows better construction CPU costs.
The CM-tree requires on average additional construction CPU costs of 53% compared to the M-tree (MT_MINRD) and 18% compared to the Slim-tree, and additional storage of 194% compared to the other methods. The primary reason for this is the additional storage and distance computations required during con-struction by the CM-tree X  X  pairwise distance tables stored in its nodes. 7.4. CM-tree versus M-tree and Slim-tree using image data sets
In this experiment we compare the performance of the CM-tree (denoted as CMT) to that of the M-tree and the Slim-tree, based on the Corel image features data sets (described in Section 7.1 ). We used here the same setting as in the previous experiment. The M-tree configuration used is MT_MINRD (described in the previ-ous section).

Tables 1 and 2 specify the results based on the co-occurrence texture data set and the color moments data set correspondingly. The rows correspond to the different access methods, and the columns are described as follows: Query distance comps ratio is the ratio between the average number of distance computations per-formed during a query and the number of data objects in the data set; Query number of I/Os is the average number of I/O accesses performed during a query; Construction number of distance comps is the number of distance computations performed during the construction of the index structure; Construction allocated size nearest neighbor (NN) queries separately.

Both tables show a clear superiority of the CM-tree with regard to query processing, while its storage costs are higher than those of the other methods. The CM-tree reduces CPU costs by an average of 67% compared to both the M-tree and the Slim-tree, and it reduces I/O costs by an average of 55% compared to the M-tree and 47% compared to the Slim-tree, for both data sets and both types of queries. Note that the CM-tree X  X  moments data set (further demonstrating the CM-tree X  X  advantage in higher dimensionalities). 7.5. Distance computations pruning
A significant tool we propose for boosting query performance is a set of novel pruning techniques of dis-tance computations, which are used by the CM-tree X  X  algorithms during search based on its pairwise distance tables. In this experiment we demonstrate the effect of these techniques on the CPU costs during both con-struction and query processing. In order to measure this effect, CPU costs were measured with these tech-niques enabled and disabled.

Figs. 19 and 20 show that these pruning techniques have a clear reducing effect on the CPU costs of both construction and query processing, which is more significant in lower dimensionalities (where the nodes X  capacities are larger thus having more entries available for pruning). For query processing these techniques save up to 90.6% of the CPU costs, averaging to 29.2% for all dimensionalities, and to 76.9% for dimensio-posed pruning techniques are very effective, especially for query processing. 7.6. Scalability of the CM-tree
In this experiment we examine the scalability of the CM-tree X  X  performance with regard to the sizes of the input data sets. The sizes considered range from 10 4 to 10 onalities, which are 2, 5, 20 and 40.

Fig. 21 exhibits a moderate increase in the average CPU and I/O costs of construction as the data set size CM-tree with regard to the costs of construction.

Fig. 22 demonstrates fixed average storage costs (for insertion of 10 storage requirements and increase its utilization.

Figs. 23 and 24 demonstrate fixed CPU costs and relatively fixed I/O costs per query for growing data set sizes. Here also, increased node capacities for lower dimensionalities decrease both CPU and I/O costs of ratio. Summarizing, these results exhibit very good scalability of the CM-tree. 8. Conclusion
We have introduced the CM-tree, a new dynamic access method for similarity search in metric data sets, which overcomes problems of existing metric access methods, and yields considerably improved search per-formance. The CM-tree is structured as a paged and balanced tree, reducing both CPU and I/O costs. It fully supports dynamic capabilities of insertions and deletions. Empty indexed volumes and overlapping volumes in the tree are minimized via its primary construction algorithms, thus maintaining a highly well-clustered structure and yielding significantly improved search performance. Several new methods were gering a node split (in addition to storage criteria); a clustering based node split algorithm; and an improved sub-tree pruning method for search. To facilitate these methods the pairwise distances between the objects of a node are maintained within each node. We have presented the CM-tree X  X  structure and invariant properties, search algorithms and construction algorithms of a single object, along with formal analysis of their correctness and properties. Results from an extensive experimental study show that the
CM-tree significantly outperforms the M-tree and the Slim-tree, improving search performance by up to 312% for I/O costs and 303% for CPU costs, as well as demonstrate its robustness to the data sets X  dimen-sionality and scale.

We are further studying additional problems, related to both the proposed CM-tree access method as well as metric access methods in general. These problems include: bulk processing; optimization of metric access methods; extensions and applications of the CM-tree to various practical areas.
 Acknowledgements We thank the anonymous referees for their valuable comments helping us to improve this article. Appendix
Proof of Lemma 1. According to the lemma X  X  assumption we have: (1) d  X  Q ; O d  X  O i ; O j  X  6 R  X  O i  X  . According to the triangle inequality we obtain: (3) d  X  O  X  4  X  d  X  Q ; O j  X  P d  X  O i ; Q  X  d  X  O i ; O j  X  . Applying (2) to (4) we get: (5) d  X  Q ; O (5) we get: (6) d  X  Q ; O j  X  &gt; R  X  Q  X  . h
Proof of Lemma 2. According to the triangle inequality we have: (1) d  X  Q ; O d  X  Q ; O i  X  P d  X  Q ; O 0  X  d  X  O i ; O 0  X  . And we also have: (2) d  X  O d  X  O i ; O 0  X  d  X  Q ; O 0  X  . Combining (1) and (2) yields: (3) d  X  Q ; O assumption of the lemma on (3) we get: (4) d  X  Q ; O i  X  &gt; R  X  Q  X  X  R  X  O
Proof of Corollary 2. According to the corollary X  X  assumption we have: (1) j d  X  Q ; O  X  R  X  O i  X  . Applying Lemma 2 on (1) we get: (2) d  X  Q ; O (3) 8 O j 2 T  X  O i  X  , d  X  Q ; O j  X  &gt; R  X  Q  X  . h
Proof of Lemma 3. Assuming falsely that there is an object O f d  X  Q ; O i  X  R  X  O i  X  ; 0 g , there are two cases. If (2) MAX f d  X  Q ; O hand (4) MAX f d  X  Q ; O i  X  R  X  O i  X  ; 0 g X  d  X  Q ; O
R  X  O i  X () R  X  O i  X  &lt; d  X  Q ; O i  X  d  X  Q ; O 0  X  . From the triangle inequality we get (6) d  X  Q ; O  X  O 0 ; O i  X  , and therefore (7) R  X  O i  X  &lt; d  X  O 0 ; O i
Proof of Lemma 4. Assuming falsely that (1) d  X  Q ; O 0  X  &gt; d  X  Q ; O contradiction to the assumption that O 0 is the closest object to Q from T ( O signifies an object O i , then according to the list X  X  definition it holds that (1) d  X  O a sub-tree, then according to the list X  X  definition we know that (2) d we get that the closest object O 0 2 T  X  O i  X  to Q holds (3) d  X  Q ; O there are at least k objects f O j g k j  X  1 for each of which (4) d  X  O
Proof of Corollary 3. From Lemma 3 we get that each object O distance to Q is not larger than d k , then searching the sub-tree T ( O closer to Q than the objects currently known. h
Proof of Lemma 6. According to Lemma 2 it holds that (1) d  X  Q ; O d  X  Q ; O i  X  R  X  O i  X  P d k , yielding (3) d L  X  Q ; T  X  O Proof of Corollary 4. This immediately results from Lemma 6 and Corollary 3 . h
Proof of Lemma 7. In Proof of Lemma 2 we showed that (1) j d  X  O j d  X  O n ; O 0  X  d  X  O i ; O 0  X j is a lower bound on the distance d  X  O ferred sub-tree does not entail any radius extension, i.e. (2) d  X  O j d  X  O n ; O 0  X  d  X  O i ; O 0  X j &gt; R  X  O i  X  , then using (1) we obtain that (4) d  X  O safely pruned from the search. Condition 2: suppose, on the other hand, that the currently preferred sub-tree does entail an extension of its radius, equal to the value of (5) d  X  O
R  X  O i  X  &lt; R  X  O c  X  and (7) j d  X  O n ; O 0  X  d  X  O entails a positive radius extension larger than the one entailed by choosing T ( O pruned from the search. h
Proof of Theorem 1. Since a new object was added to S , and due to the PCR definition it holds that (3) 8 i  X  1 ... m ; PCR 0  X  O i  X  P PCR  X  O i  X  .

Section 1: Due to (1) and the definition of PCR , it is clear that the farthest object from O same object as it was in S , therefore (4) PCR 0  X  O c  X  X  PCR  X  O 8 i  X  1 ... m ; PCR 0  X  O i  X  P PCR 0  X  O c  X  .

Case 1: In this case (6) PCR 0  X  O n  X  &gt; PCR 0  X  O c  X  and thus 8 i  X  1 ... m  X  1 ; PCR the central object of S 0 is identical to the one of S , i.e. (7) O
Case 2: In this case (9) PCR 0  X  O n  X  6 PCR 0  X  O c  X  and thus 8 i  X  1 ... m  X  1 ; PCR
Therefore the central object of S 0 may be either O n or O we obtain (11) PCR 0  X  O c 0  X  X 
Combining the results of both cases (8) and (11) yields PCR
Section 2: Due to (2) and the definition of PCR , it is clear that the farthest object from O therefore (12) PCR 0  X  O c  X  X  d  X  O c ; O n  X  X  CR  X  O n  X  &gt; PCR  X  O
Case 1: In this case (13) 8 i  X  1 ... m ; PCR 0  X  O i  X  P PCR to the one of S , i.e. (14) O c 0  X  O c . Thus using (12) and (14) yields (15) d  X  O PCR  X  O c  X  .

Case 2: In this case (16) 9 c 02 1 ... m ; c 0 6  X  c ; 8 i  X  1 ... m ; PCR of S 0 and S are different, i.e. (17) O c 0 6  X  O c . Due to (12) and (16) we obtain (18) PCR d  X  O c ; O n  X  X  CR  X  O n  X  . Due to (3) we get (19) PCR  X  O
PCR  X  O c  X  . Combining (20) and (19) we obtain (21) PCR  X  O using also (18), that (22) d  X  O c ; O n  X  X  CR  X  O n  X  P
Combining the results of both cases (15) and (22) yields d  X  O which concludes section 2. h
Proof of Theorem 2. Since an existing object O d 2 S was removed from S , and due to the definition of PCR it holds that (3) 8 i  X  1 ... m ; i 6  X  d ; PCR 0  X  O i  X  6 PCR Section 1:
Case 1: In this case (4) 8 i  X  1 ... m ; i 6  X  d ; PCR 0  X  O identical to the one of S , i.e. (5) O c 0  X  O c . Combining (3) and (5) we obtain (6) PCR PCR  X  O c  X  .

Case 2: In this case (7) 9 c 0 2 1 ... m ; c 0 6  X  c ; d ; 8 i  X  1 ... m ; PCR objects of S 0 and S are different, i.e. (8) O c 0 6  X  O
Combining the results of both cases (6) and (9) yields PCR
Section 2: Since O c was removed from S it is clear that (10) 9 c
P PCR 0  X  O c 0  X  , and thus (11) O c 0 6  X  O c . Due to the definition of PCR it is known that (12) PCR  X  O Section 2.1: Since the assumption in this section is that there are no objects in S whose farthest object was
O , and considering that by the central object definition O then the removal of O c from S could not have affected the PCR values of the other members of S , i.e. (13) 8 i  X  1 ... m ; i 6  X  d ; PCR 0  X  O i  X  X  PCR  X  O i  X  . Combining (12) and (13) we obtain (14) PCR
O . Let S 1 be the subset of objects from S whose farthest object was O of the central object is it clear that (15) 8 O i 2 S 1 ; PCR  X  O
PCR  X  O j  X  . The removal of O c from S could have affected the PCR values of only the members of S we obtain (17) 8 O i 2 S 1 ; PCR 0  X  O i  X  6 PCR  X  O i  X  , and (18) 8 O and (18), we obtain (19) 8 O i 2 S 1 ; 8 O j 2 S 2 ; PCR
Therefore it is clear that (20) O c 0 2 S 1 . Applying (20) on (17) and (15), we obtain (21) PCR
Proof of Theorem 3. Due to (1) the distances of all the objects in S from O
CR 0  X  O a  X  CR  X  O a  X  . Therefore (4) 8 O i 2 S ; PCR  X  O object in S whose farthest object was O a , their PCR values will increase by exactly CR denote this set of objects. There may be objects in S for which O increase, however it became their farthest object after the increase. For these objects the increase in their
PCR values will be no larger than CR 0  X  O a  X  CR  X  O a  X  . Let S in S which do not belong to any of these two sets, their PCR values are not affected by this increase. Let S denote this set of objects.

Section 1: Due to (2) it is clear that O c 2 S indep , i.e. (5) PCR object we know that (6) 8 O i 2 S ; PCR  X  O c  X  6 PCR  X  O 8 O i 2 S ; PCR 0  X  O c  X  X  we get (9) PCR 0  X  O c 0  X  X  PCR  X  O c  X  , which concludes section 1.

Section 2: Due to (3) and based on the definition of the PCR it is clear that either O i.e. (10) PCR  X  O c  X  X  CR 0  X  O a  X  CR  X  O a  X  P PCR 0  X  O from S will become the new central object. It is also possible however that O S .
 Case 1: We assume that (11) O c 0  X  O c . Therefore combining (11) with (10) we obtain PCR 0  X  O c 0  X  X  PCR 0  X  O c  X  &gt; PCR  X  O c  X  .

Case 2: We assume that (12) O c 0 6  X  O c . Let us falsely assume that (13) PCR (13) we obtain (14) PCR  X  O c 0  X  6 the central object of S before the change. Therefore (13) is false and we obtain (15) PCR which concludes section 2. h
Proof of Theorem 4. Due to (1) the distances of all the objects in S from O
CR  X  O a  X  . Therefore (2) 8 O i 2 S ; PCR  X  O i  X  X  CR  X  O a whose farthest object was O a , their PCR values will decrease by at most CR  X  O set of objects. For the other objects in S which do not belong to S increase. Let S indep denote this set of objects. Regardless if the PCR value of O some other object of S may sustain a decrease in its PCR value so that it becomes O
Case 1: We assume that (3) O c 0  X  O c . Using (2) and (3) we obtain (4) PCR PCR 0  X  O c 0  X  6 PCR 0  X  O c  X  6 PCR  X  O c  X  .

Combining the two cases yields (7) PCR 0  X  O c 0  X  6 PCR  X  O
References
