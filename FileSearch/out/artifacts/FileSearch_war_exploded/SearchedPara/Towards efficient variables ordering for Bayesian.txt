 1. Introduction
Traditionally, the task of learning Bayesian Networks (BNs) from data has been treated as a NP-Hard search problem [13] . To overcome such difficulty in terms of computational complexity, several approxima-tions have been designed, such as imposing a previous ordering on the domain attributes that restrict the num-ber of Bayesian structures to be learned [51] or using other approaches [42,41,40,27,26] trying to reduce the state space of this problem. When a BN is built for classification purposes, it is possible to impose additional constraints, from which extra computational efficiency gains can be derived. Under this perspective, a prom-ising alternative to get more efficient classifiers involves the identification of a suitable ordering on the attri-butes based on the class attribute information.

The Variables Ordering (VO) definition can be done by a human expert. But it is not usual to have a human expert nearby when performing data mining and machine learning tasks. In this manner, BN learning from data algorithms are often applied using random variables ordering or variables ordering given by the datasets [55] , and it can lead to poor results. Feature Ranking (FR) algorithms can define the relevance of an attribute compared to others, thus these algorithms can be useful when learning a Baysian Network Classifier (BNC) from data. In this paper, we propose a simple method, based on feature ranking algorithms, to define an attri-bute ordering suitable in the BN Classifier learning context. Since we are referring to different domains: feature subset selection and Bayesian Networks; it is important to state that, in this work, as in the work of Guyon and Elisseeff [31] , we employ the terms variable (raw inputs), attribute and feature (constructed inputs) without distinction.

The remainder of this work is structured as follows: Sections 1.1 X 1.4 point out to theoretical foundations on feature selection and Bayesian Networks. Section 2 describes the Feature Ranking Bayesian Classifiers (FRa-BayCla) learning method proposed by the authors. The simulations performed to establish the soundness of
FRaBayCla and their results are presented in Section 3 . Section 4 brings the conclusions and points out future works. 1.1. Feature ranking
Feature Ranking can be defined as a category of feature selection methods. Feature selection has become the focus of research work in areas where datasets with tens or hundreds of thousands of variables are avail-able [31] . While, in a theoretical sense, having more features should only give us more discriminating power, the real-world provides us with many reasons why this is not generally the case [35] . Reunanen [48] observes that there can be many reasons for selecting only a subset of features: (i) it is cheaper to measure only a subset of features; (ii) prediction accuracy might be improved through exclusion of irrelevant features; (iii) the pre-dictor to be built is usually simpler and potentially faster when less input features are used; and (iv) knowing which features are relevant can give insight into the nature of the prediction problem at hand. Therefore, the problem of focusing on the most relevant information has become increasingly important for machine learn-ing and data mining procedures [6] .

When considering the method X  X  output, feature selection methods can be grouped in two categories:  X  X  X ea-ture Ranking X  X  and  X  X  X inimum Subset Selection X  X  algorithms [39] . A Feature Ranking algorithm defines a score to express the relevance of a feature; a Minimal Subset Selection algorithm tries to identify a subset of relevant features. In this work, we are more interested in Feature Ranking algorithms. These methods require the evaluation of each feature using a specific distance metric, for identifying its Degree of Relevance (DR). The DR is then used to sort the features into a list called  X  X  X anked List of Features X  X . Fig. 1 shows the algorithm in a nutshell.

One can use the Feature Ranking algorithm with many different distance measures. In this work, we use the  X  X  X nformation Gain X  X  and the  X  X  X hi-squared X  X  distance measures implemented in the WEKA System [55] and apply the Ranked List of Features to define the Variables Ordering in a Bayesian Network Classifier learning process.
 In Section 2 , we argue that, in the context of learning a Bayesian Network Classifier from data, Feature Ranking techniques can play an interesting role.
 1.2. Bayesian networks
A Bayesian network is a directed acyclic graph in which the nodes represent the variables and the arcs rep-resent a relationship among the connected variables. A conditional probability table gives the strength of such relationship. The variables that are not connected by an arc can be considered as having no direct influence on and p Xi the set of parents of xi ). As it is described in [44] , the conditional independence assumption (Markov condition) allows one to calculate the joint distribution of all variables as where BK represents Background Knowledge. Therefore, a Bayesian network can be used as a knowledge rep-resentation that allows inferences.

Learning a Bayesian network from data became an effervescent research topic in the last decade [25] , and there are two main classes of methods to perform this task: methods based on heuristic searching [2,10,12,14,36] and methods based on the conditional independence (CI) definition [18,15,54,52] to generate the network structure. Besides, there are also methods that combine these two strategies [4,3,16,49] .

In a process of BN learning from data, the variables of the BN represent the dataset attributes. When using algorithms based on heuristic search, the initial order of the dataset attributes may be considered an important issue. Some of these algorithms depend on this ordering to determine the arcs direction such that an earlier attribute (in an ordered list) is a possible parent only of the later ones. On the other hand, conditional independence methods try to find the arcs direction without the need of ordering the attri-butes. However, even for the CI methods, when the ordering is known the algorithms can be improved [51] . 1.3. K2 algorithm
K2 algorithm [14] learns a Bayesian network from data using a heuristic search. It assumes that the attri-butes are discrete; the dataset is complete and has only independent cases; and all the attributes must be pre-ordered. Considering these assumptions, the algorithm looks for a Bayesian Network structure that best represents the dataset.

The algorithm uses an ordered list (containing all the attributes), which asserts that only the attributes posi-tioned before a given attribute A may be parent of A. Hence, the first attribute in the list has no parent (it is a root node in the Bayesian network).

The network construction uses a greedy method to search for the best structure. It begins as if every node had no parent. Then, beginning with the second attribute (the first one is the class) from the ordered list, the possible parents are tested and the ones that maximize the whole probability structure are added to the net-work. This process is repeated to each attribute until the last one, thus the best structure is achieved. It is done by maximizing the results of the following equation: where each attribute x i ( i =1, ... , n ) has r i possible values  X  v i structure containing the attributes to be represented. Each attribute x i within Bs has a set of parents p i . w ij ber of objects in D in which x i has value v ik and p i is instantiated as w ij  X  N ij = ability constant P (Bs) to each Bs.

With the best structure already defined, the network conditional probabilities must be determined. It is done using a Bayesian estimation of the (predefined) network structure probability. The Bayesian estimation is adopted in other learning Bayesian methods as in Spiegelhalter and Lauritzen [50] , but there are other ways to compute this probability (i.e. the variance analysis). 1.4. Learning classifiers from data Instead of encoding a joint probability distribution over a set of random variables, a Bayesian Network
Classifier (BNC) aims at correctly predicting the value of a designated discrete class variable given a vector of attributes (predictors). Learning Bayesian Networks methods (as K2) may be used to induce BNC, in this sense, traditional learning unrestricted Bayesian Networks algorithms (Tabu, CI and B-Hill, for instance) can be used. But, the score functions used in most of the unrestricted Bayesian Network learning algorithms do not attempt to optimize the conditional likelihood of the class given the other variable. To be more precise, they try to optimize the likelihood of the entire data. Thus, when using these algorithms in classification tasks, the results may not be optimal. This characteristic motivated the development of algo-rithms specific to classification (Na X   X  ve Bayes and TAN, for instance). To give support to our empirical sim-ulations (showed in Section 3 ) a very brief overview of five Bayesian Network Classifiers (TAN, Na X   X  ve,
Tabu, CI and B-Hill) is given in the sequel. More details about these well-known algorithms can be found in the given references.  X  Na X   X  ve Bayes . The Na X   X  ve Bayes Classifier idea comes originally from a pattern recognition approach given in [21] . This classifier works with a network structure where all the features are connected with the class fea-ture (the class feature is parent of all other features) and no other connection is allowed. In this sense, all the features (except the class) are considered conditionally independent from each other (given the class fea-ture). A good overview of this classifier can be found in [37] .  X  TAN . As described in [23] , this classifier is an extension of the Na X   X  ve Bayes classifier. The Tree Augmented
Na X   X  ve Bayes (TAN) removes the Na X   X  ve Bayes assumption that all the features are independent. It begins the network structure as a Na X   X  ve Bayes Network, then, it finds correlations among the features and connects them in the network structure learning process.  X  Tabu . The basic concept of Tabu Search as described by Glover [28] is  X  X  X  meta-heuristic superimposed on another heuristic X  X . In the used Bayes Network learning process, the tabu search is used for finding a well scoring Bayes network structure. It performs a hill climbing until an optimum is reached. The fol-lowing step is the  X  X  X east bad X  X  possible step. The last X steps are kept in a list called Tabu List and none of the steps in this list is considered in taking the next step. The best network found in this traversal is returned.  X  CI . The Conditional Independence (CI) algorithm uses conditional independence tests to find a network skeleton, finds V-nodes and applies a set of rules to find the directions of the remaining arrows [53] . Thus, this algorithm does not depend on a predefined variable ordering to generate the network structure.  X  B-Hill . This Bayes Network learning algorithm uses a hill climbing algorithm adding, deleting and reversing arcs. Thus, as in the CI algorithm, in this method the search is not restricted by an order on the variables (unlike K2) [8] .

Besides the aforementioned BNCs, our simulations (Section 3 ) also use two classifiers based on other the-ories than Bayesian Network: the instance based method IB1 and the decision tree classifier J48. As these clas-sifiers are also considered popular, only a brief overview of each applied classifier is given in the sequel. More details about each method can be found in the indicated references.  X  IB1 . Different from other learning algorithms described in this paper, the IB1 [5] does not generate a description of the target function. Thus, the training process is not based on the construction of a model that represents the main characteristics of a concept to be learned. Instead of it, in the IB1 algorithm, the classification procedure is the following: considering q a query (instance to be classified),
T = h t 1, t 2, ... , tn i a set of training instances (where each ti is a set of feature-value pairs), the objective of the classifier is to predict the class of q . The prediction is based on the identification of the training instance (ti) closer to q .  X  J48 . it is the Weka X  X  implementation of the popular C4.5 decision tree learner [46] . In fact, J48 is the later and slightly improved version, called C4.5 revision 8, which was the last public version of this family of algorithms before the commercial C5.0 was released [47] .
 2. Feature ranking bayesian network classifiers learning (FRaBayCla)  X  K2 v 2 and K2IG
The use of K2 for learning BNs from data is often motivated by its ability to find the network structure efficiently [33,11] given that a reasonable variable ordering (VO) is provided. If the VO is not adequate, the quality of the learned structure may be low. Thus, searching for a good VO is relevant to the K2 learning pro-cess and it can be performed by: (a) using the knowledge of a human expert to help the definition of such ordering; (b) performing an exhaustive search; and (c) using heuristic search methods.

In data mining applications, human experts are not always available to define the Variable Ordering, and the exhaustive search approach is usually not computationally feasible. Di Zio et al. [20] propose to order the variables according to their reliability. They consider more reliable those variables with a lower percentage of missing items, higher accuracy, and availability of external sources. The authors observe that reliable variables are frequently available when samples are drawn from a sampling frame where social or demographic vari-ables are already known. However, such a scenario is not common in data mining applications, and the assess-ment of the reliability of the variables may be hard to perform automatically. Therefore, the adopted strategy usually involves performing a heuristic search. It is particularly desirable to employ an efficient heuristic search for the K2 algorithm. Although it is known that the search for an adequate ordering is computationally less expensive than the search for the BN structure [24] , if the complexity of the VO search procedure is too high, one of the main characteristics of K2 (i.e. its fast performance in the learning process) may be affected and, in this case, other BN learning algorithms may be more indicated. For this reason, in this work we apply a heu-ristic based on the v 2 (chi-squared) statistical test [19] and the Information Gain [46] metric to define an appro-priate VO before using K2.

The K2 algorithm was designed to learn unrestricted Bayesian networks from data. However, it can be adapted to construct Bayesian networks specifically designed for classification problems. This approach is adopted in our work, in this sense, FRaBayCla has been specially conceived to learn a classifier from data.
The idea of inducing classifiers using the K2 score (Eq. (2) ), instead of using a simpler Bayesian Classifier (as the Na X   X  ve Bayes, for instance), is aimed at reducing the bias introduced by the independence assumptions embedded in the Na X   X  ve Bayes classifier, thus improving estimates of class probabilities. Estimates of class probabilities are crucial for decision-making. As stated in [29] , a classifier is often only one part of a larger decision process, for which accurate class probability estimates provide additional utility. For instance, know-ing the class probability may give much information about costs of incorrect predictions (even if associated costs are not precisely defined) [45] .
 As stated in [51] , v 2 statistical test allows finding conditional independence relationships among attributes. Such relationships can be used as constraints to construct a BN [9] . The PC algorithm [51] , which is a classical BN learning algorithm that does not depend on an initial VO, uses the v 2 statistical test to measure the effec-tiveness of the relationships between pairs of attributes. Following this concept, we have applied the v 2 statis-tical test to optimize the attribute ordering in the structure learning phase of K2. Therefore, our BNC learning method is called K2 v 2 . In order to define the VO, the v 2 statistical test is performed in each attribute jointly with the class attribute (for this reason, K2 v 2 can only be applied in a classification context, where there is a distinguished variable, namely class variable). Thus, the strength of the dependence relationship between each attribute and the class attribute can be measured. Subsequently, the attributes are decreasingly ordered according to the v 2 scores. The first attribute in the ordered list has the highest v 2 score, i.e. it is the most dependent upon the class attribute. Obviously, the relation between the v 2 statistical test and the best VO may not hold strictly, but a previous work [32] , as well as our current paper, show that good results can be achieved using this heuristic. Also, in the present work we investigate another Feature Ranking algorithm, the Information Gain, using the same aforementioned methodology, in this version, our learning Classifier is called K2IG.

Taking on account the relevance of the variables ordering, and considering the predictable absence of experts, relevant research has been done on automated methods to define appropriate variables ordering for Bayesian network learning from data. Significant results are presented in [1,22,30,38,7,17] , but these meth-ods need a high computational effort as they suggest search mechanisms to identify an appropriate variables ordering. In [32] , the authors suggest that the v 2 feature ranking algorithm can help in the optimization of learning a BN from data, but those results are incipient.
As Feature Ranking algorithms define the relevance of a feature compared to others (Section 1.2 ), they seem to be appropriate to define variables ordering in a BNC learning task. In this paper we propose a simple method based on feature ranking algorithms which has low computational complexity (O( n 2), where n is the number of variables) and produces good results. We empirically demonstrate that feature ranking algorithms (namely, Chi-Squared and Information Gain) can be used to define efficient variables ordering in the BNC learning context. The proposed method can bring improvements, when using the K2 algorithm [14] , to learn a BNC from data.

The methodology is quite simple: before applying the learning algorithm to generate a BNC from data, one must run the feature ranking algorithm to each feature in the dataset and the class feature, as seen in Section 1.2 it will result in a ranked list. The ranked list is then considered as the VO; in other words, the BNC learning algorithm must use the ranked list as the variables ordering definition. Fig. 2 shows the Feature Ranking Bayesian Network Classifier (FRaBayCla) learning algorithm in a nutshell.

Observing Fig. 2 , one can see that the method allows the use of any distance measure (for the feature rank-ing step) and any BN learning algorithm (for the learning step). In the simulations performed in this work (described in the next section) are employed the Chi-squared and the Information gain distance measures [39] , and the K2 learning algorithm.

The extra complexity imposed by FRaBayCla is generated only by the Feature_Ranking algorithm which run time complexity is O( n 2) [39] (where n is the number of features in the dataset). 3. Simulations
Trying to identify the consistency of the proposed method, simulations were performed with two classes of datasets. The first class includes datasets with few features, and the second class includes datasets with more features than the former. The datasets are described in Table 1 .

The aim of the performed simulations is to verify the soundness of the FRaBayCla in the context of a clas-sification task. Thus, we consider that the higher Average Correct Classification Rate (ACCR) obtained the classification (using a 10-fold stratified cross-validation strategy) identifies the best Variables Ordering. It is important to say that the same training and test datasets (for each dataset) were used for all learning tasks in all the performed simulations.

The methodology of the simulation performed using the datasets from class1 is different from the one per-formed using datasets from class 2. To classify the datasets as  X  X  X lass 1 X  X  or  X  X  X lass 2 X  X  we used the following rule of thumb: considering M the number of features that forms a dataset D ,if( M &gt; 5) then  X  X  X lass 2 X  X , else  X  X  X lass 1 X  X . In this sense, there are M ! possible variables orderings in such a dataset. When M &gt; 5, the number of pos-sible VO becomes too high and to minimize the computational effort, only 35 randomly selected VO are used.
Both methodologies are described in more details in the next subsections. 3.1. Class 1 datasets simulations
As the datasets from class 1 have few features, all the possible variables orderings were generated for each dataset from this class. Afterwards, each variables ordering of each dataset was used with K2 BN learning algorithm. Latter, using the feature ranking algorithms Chi-Squared and Information Gain, two ranked lists were generated and FRaBayCla applied. Subsequently, the Average Correct Classification Rates (ACCRs) are compared with all the possible orderings of each dataset. This process is summarized in Fig. 3 , and the results are described in Table 2 .

As described in Fig. 3 , all the possible variables orderings were used in a classification task (using a 10-fold stratified cross-validation strategy) to identify the best average correct classification rate. Considering that the best ACCR is given by the best variables ordering in the classification context, Table 2 shows that, for the three datasets, the ACCRs obtained, when using the ranked lists defined by the Chi-Squared and Information Gain distance measure, are the same got when using the best Variables Ordering. In other words, it means that for the three datasets, the ranked list of variables defined by the Chi-squared Feature Ranking algorithm is the same defined by the Information Gain Feature Ranking algorithm, and both ranked lists are also the same given as the best VO in the classification context.

It is worth to say that in these simulations, the proposed method defined the best VO for the three dataset tested, but it does not mean that FRaBayCla always find the best Variables Ordering. The aim of this paper is to show that on average, the proposed method can bring a better VO than a randomly defined one. To identify the accuracy of our proposed method, simulations with datasets containing more features were performed and are described in the following section.
 3.2. Class 2 datasets simulations
As described in Table 1 , datasets from class 2 have more features than datasets from class 1 and the number of possible VOs can be huge (for instance, the congress voting dataset has 20,922,789,888,000 (16!) possible orderings). Consequently, it is not reasonable to perform an exhaustive search to identify the best Variables
Ordering (as done with the Class 1 datasets). It explains why the simulations using the class 2 datasets are different from the ones performed with the class 1 datasets.

As stated in [19] , an approximate confidence interval for a population mean can be constructed for random variables that are not normally distributed in the population, relying on the central limit theorem, if the sam-ple sizes and counts are big enough to compute a consistent confidence interval. In this sense, it is desirable to work with samples having more than 30 objects. For this reason, to verify the soundness of the ranked lists (Chi-Squared and Information Gain) generated for the datasets from class 2, we applied the following proce-dure: 35 variables orderings were randomly generated for each dataset, and a 10-fold cross-validation classi-fication was performed to each one. Then, the 95% and 99% confidence intervals on the ACCR mean are computed based on the ACCRs mean and ACCRs standard deviation [19] . Afterwards, a 10-fold cross-vali-dation classification task is performed using the K2 v 2 and the K2IG, and the obtained ACCRs are compared to the 95% and 99% confidence intervals on the ACCRs mean. The procedure is described in an algorithmic fashion in Fig. 4 , and the obtained results are presented in Table 3 .
 Results presented in Table 3 reveal that in all employed datasets (Nursery, Wisconsin Breast Cancer, Adult,
Congressional Voting Records and Mushroom), the ranked lists defined by the Feature Ranking algorithms (Chi-Squared and Information Gain  X  third and fourth table lines, respectively), produced better ACCRs than the 99% confidence interval ACCRs definition. It means that 99% of the possible Variables Orderings would result in lower ACCRs than the ACCRs got with the ranked lists. In a Bayesian interpretation, one can say that we are 99% sure that using a random sample will bring lower ACCRs than the ACCRs got using K2 v 2 and K2IG.

Considering the difficulty of identifying a good VO, some authors [55] suggest that K2 should be run several times with different, randomly generated, VO. In this sense, FRaBayCla may be seen as a good method to learn BNC using K2 as the learning algorithm. As FRaBayCla needs to run only once the learning algorithm (K2), this method overcomes the high computational effort of running several times the learning process. In addition, results showed in Table 3 reveal that the obtained ACCRs can be considered good.

The last line in Table 3 describes the performance of the best ACCR, for each dataset, in the 10-fold cross-validation classification task (among the 35 randomly generated VOs). It reveals that only in the Congressional Voting Records, the ACCRs obtained when using the ranked lists are lower than the best ACCRs among all the 35 randomly generated VOs. Even in this case, one can see that the ACCRs obtained using the FRaBayCla are competitive. However, it is worth to restate that the proposed method does not aim to find always the best Variables Ordering, but an efficient one.
 Another interesting result revealed in Tables 2 and 3 is that with these datasets both, Chi-squared and Information Gain, feature ranking methods tend to produce similar and consistent Variables Ordering to be applied in BNC Learning tasks. 3.3. Other learning algorithms
As well as comparing the behavior of the proposed method (FRaBayCla) with the classic K2 (using differ-ent Variable Orderings), it is important to verify the accuracy of the FRaBayCla when compared to other clas-sifiers. In this sense, five different Bayesian Network Classifiers (TAN, Na X   X  ve, Tabu, CI and B-Hill), one instance based method (IB1) and one decision tree classifier (J48) were used in the performed simulations. These classifiers were briefly described in Section 1.4 and their implementation are popular in the data mining community, and make part of the WEKA System [55] , which was used to perform our simulations, using its default parameters. More implementation details can be obtained in [8] .

Looking for a comparative accuracy analysis, the  X  X  X lass 2 X  X  datasets (described in Table 1 ) were classified in a 10-fold cross-validation strategy with each one of the aforementioned classifiers. The obtained ACCRs are described in Table 4 .
 The bold ACCRs in Table 4 reveals the best result for each dataset (considering) all the applied classifiers.
The last line (Average) describes the average ACCRs for each dataset also considering all the applied classi-fiers. The results presented in Table 4 show that the FRaBayCla brought the best ACCRs in three datasets (namely, Wisconsin Breast Cancer, Adult and Mushroom) and the second best ACCRs in the two remaining datasets (Nursery and Congressional Voting Records). Another interesting point is that the FRaBayCla brought results much better than the average of all the employed classifiers. 4. Conclusions and future work
Defining a suitable variables ordering (VO) is a very important issue when learning a Bayesian network from data [1] . In this paper, we propose a simple method to learn BNC based on Feature Ranking algorithms which has low computational complexity and produces efficient results. Traditionally, one of the main draw-backs of find a good VO is the computational effort that is added to the classification task. The proposed method defines a suitable VO to a Bayesian Network Classifier with an asymptotical computational complex-ity of O( n 2). We demonstrate empirically that feature ranking algorithms (namely, Chi-Squared and Informa-tion Gain) can be used to define the Variables Ordering and optimize a BNC learning procedure. FRaBayCla (Feature Ranking Bayesian Networks Classifier) can bring improvements, when using the K2 algorithm, to learn a Bayesian Network Classifier from data.
 When having datasets with few features (class 1 datasets), the performed simulations showed that the
FRaBayCla found the best Variables Ordering for the three tested datasets. In the simulations performed with datasets from class 2, the results revealed that the FRaBayCla defines VOs very close to the best pos-sible ones.
 This research brought to notice some interesting future works. The first is to explore the efficiency of FRa-
BayCla when using BN learning algorithms based on the conditional independence theory. The second one is to make a comparative analysis of the FRaBayCla Variables Orderings quality and the variable orderings given by other traditional method. Third, we intent to apply the FRaBayCla method in real datasets to verify its behavior.

The identification of the best variable ordering can be done using other context than the classification task results (ACCRs). Thus, another interesting future work concerns the identification of suitable Variables Orderings based in other metrics such as the BN structure, for example.
 Acknowledgements This research is partially supported by CNPq, FAPESP and FAPERJ.

References
