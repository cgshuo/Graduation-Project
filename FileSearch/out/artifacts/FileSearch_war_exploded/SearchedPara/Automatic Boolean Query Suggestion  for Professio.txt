 In professional search environments, such as patent search or legal search, search tasks have unique characteristics: 1) users interactively issue several queries for a topic, and 2) users are willing to examine many retrieval results, i.e., there is typically an emphasis on recall. Recent surv eys have also verified that professional searchers continue to have a strong preference for Boolean queries because they provide a record of what documents were searched. To support this type of professional search, we propose a novel Boolean query sugge stion technique. Specifically, we generate Boolean queries by exploiting decision trees learned from pseudo-labeled documents a nd rank the suggested queries using query quality predictors. We evaluate our algorithm in simulated patent and medical search environments. Compared with a recent effective query ge neration system, we demonstrate that our technique is effective and general. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Query Formulation, Search Process.
 Algorithms, Experimentation. Boolean query suggestion, prior-ar t search, patentability search. Query suggestion is an effective and practical way to help users formulate queries [15, 16]. While there have been many studies on how to provide alternative queries in general web search [15, 16], little work has been done about suggestion for domain-specific search, e.g., patent retrieval, legal search, and medical information search. Many of the users in such domains are search professionals, e.g., patent examiner s and information specialists in companies and law firms, who perform specialized search tasks such as prior-art search and legal discovery. Query suggestion techniques should be designed for the unique search characteristics of these domains. For example, professional search is typically more recall-oriented than consumer search. In the patent validity task, for example, patent examiners formulate search queries from a ne w patent to validate its patentability, and generally spend about 12 hours to complete a single task by examining approximately about 100 patent docume nts retrieved by 15 different queries on average [1]. Another typical characteristic of professional search is the need to document the searches that are carried out. For a number of reasons, both hist oric and technical, Boolean queries are particularly common in professional search. For example, in patent search, recent surveys [1, 2] revealed that the use of Boolean operators is one of the most important features to formulate effective queries from the perspective of patent professionals. Also, according to [2], most patent professionals who participated in the survey did not regard query term weighting and query expansion as important whereas 96.3% of participants agreed that Boolean operators are necessary. This is not because Boolean queries are the most effective. In fact, a number of studies over the years (e.g ., [5, 6, 7, 9, 11]) have shown that  X  X eyword X  queries are often significantly more effective. Boolean queries, however, are easy for information professionals to manipulate and are essentially self-documenting in that they define precisely the set of doc uments that are retrieved. Despite the importance of Boolean queries in professional search, there has not been much research on helping information professionals formulate those queries. Tseng and Wu [3] indicated that the provision of suggested search vocabulary would be helpful in patent search. Other studies on prior-art search that automatically generate queries from patent text (e.g., [6, 7]) did not investigate Boolean query s uggestion. Current government or commercial patent search systems 1 used by information professionals all support Boolean que ries but not query suggestion. In this paper, we propose a method to suggest Boolean queries for professional search. We define a Boolean query as the sequence of terms associated by conjunction (AND) where each term can be prefixed by negation (NOT). Although the OR operator is often used by professionals to indicate synonym groups, the retrieval evidence shows that AND and NOT have much more impact on effectiveness in domains such as patent search with very detailed documents (e.g., [4]). Adding synonym structure is left for future work. Although the suggested Boolean queries can be generated and used with any search engine, we use a simple statistical Boolean retrieval model for our experiments (explained in Section 5). We do not adopt any additional query processing and term 
PATENT SCOPE (http://www.wipo.int/patentscope/), PatFT (http://patft.uspto.gov/), DELPHI ON (http://www.delphion.com/) weighting because those features are not generally preferred by professionals and not supported by commercial search systems. In order to suggest Boolean queri es, we first focus on generating Boolean queries that describe th e content of an initial set of retrieved documents. We then rank the generated queries to place effective (in terms of finding more relevant documents) queries at higher ranks. In other words, our system performs two sub-tasks: 1) Boolean Query Generation, and 2) Boolean Query Ranking. In the first task, we extract queries composed of Boolean operators and various terms, representing a pseudo-relevant document set, i.e., the top-k documents retrieved by a baseline system. To do this, we learn a decision tree from the pseudo-relevant documents so that the decision tree can determine whether a document is pseudo-relevant or not. Afterwards , each positive decision rule (i.e., a path from the root to a positive leaf node indicating pseudo-relevance) formulates a Boolean query. Our Boolean query generation process base d on decision trees has two advantages: i) a (binary) decisi on tree can be equivalent to a Boolean function in terms of its expressiveness [20], and ii) decision trees naturally determ ine the number of query terms. In the next step, among the many generated Boolean queries, we select the effective ones by ranki ng them. This is consistent with typical query suggestion techniques used in web search, where the  X  X est X  suggestions are presented to the user. We learn to rank the generated queries using various quality predictors proposed for adhoc retrieval [21, 22, 23, 24, 25] and several new features that consider unique properties of Boolean queries. We show the effectiveness of our Boolean query suggestion system by verifying that the system is capable of generating effective Boolean queries at high ranks. Although our focus is on patent search, we show the gene rality of our approach through additional experiments with a medical literature database. The rest of this paper is organized as follows. In Section 2, we outline previous work in query suggestion and generation, and describe the limitations of previ ous work. Section 3 defines the task of Boolean query suggestion for professional search, and we present the methods we used in Section 4. Section 5 contains the experimental results and discussi on. Finally, we summarize the contributions of our research an d future work in Section 6. In this section, we explore prev ious work for query suggestion for web search and query generation. Automatic query expansion [13] has been intensively researched to bridge the gap between users X  queries and relevant documents. In particular, pseudo-relevance f eedback [12] is known as one of the most effective techniques. Although many successful query expansion techniques have been propos ed (e.g., [13, 14]), most of them are not easily applicable to our tasks because they are not able to generate Boolean querie s required for professional search environments. However, the query expansion method proposed by Mitra et al. [13] is strongly re lated to our work because they addressed the effectiveness of Boolean filters to improve precision of automatic query expansion. Specifically, they manually formulate fuzzy Boolean operators (conjunction and disjunction) and select expanded terms from a set of pseudo-relevant documents refined by the Boolean filters. However, their work is limited in that the Boolean filters are manually constructed while we focus on automatic formulation. Moreover, they did not consider Boolean queries. Jones et al. [17] proposed a que ry substitution system that suggests strongly related queries identified from user query sessions. In query reformulation, Wang and Zhai [18] discovered associated terms from query logs to substitute the original query terms or add new terms into an original query. Also, query recommendation techniques proposed in [16] provide alternatives by clustering related queries in query logs, and White et al. [15] studied types of query suggestions for web search preferred by users via a user study. While the query logs and session information that most previous techniques depend on are readily available for web search, such resources are mostly not available in domain-specific search environments that we address. In patent search, although automatic query generation is crucial, most previous work has focuse d on improving a retrieval model. The participants in the patent retrieval task of NTCIR-6 [8] used terms from claim sections without term selection. Mase and Iwayama [9] added terms from abstra ct sections into claim-based queries and weighted the query te rms by their TF-IDF scores in the query patent. Xue and Croft [6, 7] described a query generation technique for patent search. In [6], they generated a patent search query by selecting an effective section and extracting the top-ranked words from the section using TF-IDF weights. Their finding is that the  X  X rief summary X  section of patents can produce the most effective queries, while weighting query words by term frequencies is more effective than TF-IDF weighting. They also expanded search queries to incorporate noun phrases, but the improvement was not significant. In [7], they improved retrieval performance of queries generated from [6] by using a learning-to-rank model a nd various features. The queries generated by this approach, howev er, contain many terms that are weighted and have other constrai nts, making them unsuitable for query suggestion. Similar to the NTCIR workshop, TREC recently proposed the Chemical track. A sub-task of the track addresses search for chemical pa tents [10]. Though Gobeill et al. [11] considered query expansion and showed the best performance among the participants, their expansion technique is somewhat limited in that it depends on a concept identifier and external resources (PubChem 2 ) which would not be applied to general patents. The Boolean query suggestion approach is based on our previous work with professional searchers and studies such as Bache and Azzopardi [4]. They pointed out that patent searchers are in favor of exact-match models, i.e., trad itional Boolean retrieval, because Boolean models can improve the retrievability of target documents. Accordingly, they propos ed hybrid retrieval models in which target patents are ranked by TF-IDF or Okapi BM25 models and then filtered by conjunctive and disjunctive Boolean operators (AND and OR). Though their models are empirically effective, those are limited in using only two-word queries, i.e., two words are associated by conjunction or disjunction. In the medical domain, Hashmi et al. [19] developed a system to generate context-specific queries from clinical guidelines, but they also used an external knowledge base to create query terms. In this section, we formulate the Boolean query suggestion problem for professional search and define associated terms. The database of compounds structure and description for chemical molecules (http: //pubchem.ncbi.nlm.nih.gov/) Definition 1 (Professional Search): Professional search is interactive information retrieval performed by professionals in a specific domain such as the patent or medical domain. The main difference of this task from adhoc retrieval (e.g., web search) is that professional search tends to be recall-oriented. The search behavior of professionals is quite different from that of general users. For example, since professionals tend to prefer finding more relevant documents to find ing a small number of relevant documents at the top ranks, they examine more retrieval results than web search users. Accordingly, search processes are often long and repeatable, e.g., a patent examiner issues several queries until the obtained results are satisfactory. Definition 2 (Topic): A topic is a subject for which queries against search engines are formed and the retrieval results are examined by professionals. For example, in prior-art search, a new patent to be validated can be a topic. In the medical domain, a description of patient symptoms can be a topic. Definition 3 (Boolean Query): A Boolean query is a sequence of query terms all of which are conn ected by conjunction and each of which can be prefixed by negation, e.g.,  X  X  X  X  X  X  X  X   X   X  X  X  X  X  X  X   X   X  X  X  X   X  X  X  X  X  X  X  . In this work, as query term candidates, we consider bigrams as well as unigrams. Definition 4 (Pseudo-Relevant Documents): Pseudo-relevant documents are the top k documents retrieved by a baseline system. The baseline system can handle a weighted or expanded query using a state-of-the-art retrieva l model. We exploit the pseudo-relevant documents to formul ate Boolean queries that are suggested to users. Definition 5 (Boolean Query Generation): Boolean query generation is formulating Boolean queries from a set of query term candidates. Using terms appear ing in a set of pseudo-relevant documents for topic, we formulate Boolean queries that consist of effective terms and Boolean op erators (AND and NOT), where query term candidates can be uni grams or bigrams extracted from the pseudo-relevant documents.
 Definition 6 (Boolean Query Ranking): Boolean query ranking is determining a preference among ge nerated Boolean queries for a topic with respect to a recall me tric, e.g., recall at 100 (R@100). This is necessary for suggesting a reasonable number of effective Boolean queries (e.g., 5~10) to users because many queries can be generated in the Boolean quer y generation phase. We produce a ranked list of generated Boolean queries where an effective Boolean query should be placed w ithin the high ranks (e.g., top 10). In this section, we first propos e a decision tree-based method for Boolean Query Generation, and th en describe a Boolean Query Ranking model using various quer y quality predictors. Figure 1 demonstrates the overall process of our system. In Boolean Query Generation, we train decision tr ees using the baseline retrieval result (containing the top-k pseudo-relevant documents and beyond-k non-relevant documents) and formulate corresponding Boolean queries (BQs) (details in Section 4.1). In Boolean Query Ranking, the ranking model trained from sorted lists of BQs with respect to R@100 can rank the generated BQs by query quality predictors (details in Section 4.2). The top ranked queries are presented as suggestions. Binary decision trees are equivalent representations of Boolean functions [20]. If we could train a decision tree where a node corresponds to a term appearing in training documents in order to determine whether a document is relevant to a topic, the learned decision tree could imply a Bool ean query representing a set of relevant documents. In addition, the length and query terms of a Boolean query are naturally determined by the depth and the nodes of the tree with reasonabl e accuracy. A problem, however, is that we do not have training data to learn a tree which can be generalized for every query because each query is associated with a different set of terms. So, instea d of relevant documents, we use pseudo-relevant documents defined in Section 3 as training data. In other words, we learn a de cision tree by using the top-k documents as positive exampl es. As negative examples, presumably non-relevant documents (ranked beyond-k in the baseline retrieval result) are used. Accordingly, Boolean queries generated from the positive nodes of the learned decision tree are expected to be as effective as the baseline query because the decision tree is learned from the pseudo-relevant documents. Once we learn a decision tree from a topic, we identify a single path from a root to a positive l eaf node in the decision tree and convert the rule (path) into a Boolean query. Accordingly, a decision tree produces as many Boolean queries as the number of positive leaf nodes. Figure 2 depicts how to generate Boolean queries from an example decision tree whose attributes (query term candidates) are alloy , wheel , and steel , and True/False values of each leaf node denotes a positive/negative decision for input documents. For exam ple, a document including alloy and wheel is classified as True (or relevant) because a number of pseudo-relevant examples used fo r training include the two terms. That is, the path from alloy to the first True leaf can formulate query Q 1 , which is expected to retr ieve documents containing alloy and wheel . Since we concentrate on conjunction and negation, we generate two queries, Q 1 and Q 2 , rather than a single unified query such as  X   X  X  X  X  X   X   X  X  X  X  X   X   X   X   X  X  X  X  X  X   X   X  X  X  X  X   X  Note that AND and NOT have more imp act on the effectiveness and Q or Q 2 is empirically better than the unified query w.r.t R@100 that we use to evaluate a Boolean query. We describe the Boolean Query Generation algorithm used for the example in the following. Figure 2 shows the process of generating Boolean queries from several sets of query term candidates (i.e., attributes), a se t of pseudo-relevant documents (the top-k baseline retrieval results ) and a set of non-relevant documents (the beyond-k baseline retrieval results). To produce a sufficient number of Boolean qu eries (among which we can find many effective Boolean queries), for each topic, we train several decision trees with different attributes, while all the trees are trained by the same training set. In this, the training set includes the k positive (pseudo-relevant) documents and an equal number of negative instances (non-relevant) for a topic. To obtain N sets of attributes, various approaches (e.g., thesaurus and concept identification) can be used. In th is paper, we change the number of query term candidates in each set to get multiple attribute sets. For the patent search experiments, in this paper, we select query term candidates from a unigram language model estimated from a topic patent (i.e., new pa tent) or its pseudo-rele vant patent set. We first select unigrams which are likely to be generated from the following language models, assuming that terms are effective for retrieving pseudo-relevant patents if the terms frequently occur in the topic patent or ps eudo-relevant patents. where  X  is topic patent,  X  X  X  X  is the set of pseudo-relevant patents document  X   X  , and |  X   X  | denotes the length of  X   X  . Stop-words 3 are removed. Using term probabilities, we can rank terms in a topic document or pseudo-relevant documents. We select the top-m terms as attributes for decision trees. In particular, to obtain multiple sets of attributes, we consider 20 different m  X  X  (in {5,10,...,100}) , each of which makes an attribute set.
 In addition, from an observation that patents contain many compound words that may help to improve the retrievability, we can add an equal number of bigrams into each set of selected unigrams. To rank bigram s, we estimate smoothed bigram language models for the topic pa tent and the pseudo-relevant patent set as follows: Stop-words contain articles, prepositions, acronyms (e.g., fig .), (relative) pronouns, and general nouns (e.g., method , figure , and apparatus ), frequently appeared in pa tent documents (available at http://www.cs.uma ss.edu/~yhkim/). ALGORITHM Boolean Query Generation 
INPUT : OUTPUT : A set of Boolean queries, S
PROCESS : where  X  is a bias to unigrams. We set  X  to 0.7 and eliminate bigrams which contain any stop-word. For the medical search experiments, we use the same setting except that we use only pseudo-re levant documents because topic documents are not usually given. In addition, since long Boolean queries may not return any retrieval results, we eliminate Boolean queries which contain more than 15 terms. To select a reasonable number of effective queries from a pool of generated Boolean queries, we propose a Boolean query ranking model and introduce features for the model. In order to rank generated Boolean queries, we learn a ranking function which predicts the preference between Boolean queries. That is, given a topic and genera ted Boolean queries, our ranking model produces a ranked list of th e Boolean queries in descending order of recall at 100 (R@100). To do this, we use ranks by R@100 X  X  of the Boolean queries generated for each topic as target values to be predicted. The form al definition of this model is given as follows. the number of ranks, and we can order the ranks  X   X   X  X   X  where  X  indicates the preference between two ranks. For training, a set of topics  X  X   X   X   X   X ,  X   X ,...,  X   X  is given and each topic  X  associated with  X  X  X   X   X  X  X  X  X   X  X   X  X ,  X  X   X  X ,...,  X  X   X   X  queries, where n  X   X   X   X  means the number of generated Boolean queries for  X   X  and a list of labels  X   X   X  X  X  X   X  X   X ,  X  X   X ,..., which  X   X  X  X   X  X  indicates the rank of each Boolean query, BQ  X  . A feature vector  X   X  X  X   X  X  X  X  X   X   X  X ,  X  X  X  X X  X  is generated from each topic and Boolean query pair. We can represent a set of training examples as  X  X   X   X   X   X   X  X  X ,  X   X ,  X   X   X   X  X  X  X   X  . 
Figure 2: Decision Tree-based Boolean Query Generation. A ranking function X X  X : X  maps a feature vector associated with a Boolean query to a score for the query. Specifically, this model generates a permutati on of integers spanned in  X  1,n  X   X  for a topic, the corresponding Boolean query list, and the ranking function f . The permutation  X   X   X   X   X  X  X ,  X   X  is defined as a bijection integer of  X  1,n  X   X   X   X   X  and  X   X   X   X  denotes the position of  X  X  model is learned to minimize a loss function which is defined by the disagreements between permutation  X   X   X   X   X  X  X ,  X   X  and rank list y for every training topic. For learning, we use Ranking SVM [26]. In contrast to Boolean Query Generation where only pseudo-relevanc e is considered, we use real relevance judgments to compute R@100 of training examples for Boolean Query Ranking. This is because Boolean Query Ranking uses generalizable features while Boolean Query Generation uses terms which strongly depends on topics. In order to compose a feature vector for our query ranking model, we leverage features from previ ous studies for predicting query performance [21, 22, 23]. Previous studies (e.g., [24, 25]) proved that query quality predictors are effective for ranking sub-queries. Since generated Boolean queries al so consist of subsets of terms in topic documents (e.g., query patents), we can expect those quality predictors also help to r ecognize effective Boolean queries. However, we additionally use more features specialized for our task because we observed that Boolean queries often show different characteristics from adhoc queries. Accordingly, we categorize our features into tw o groups, General Query Quality Predictor and Boolean Query Quality Predictor. Table 1 summarizes the featur es in each group. General Query Qua lity Predictors contain features proposed by previous studies for quality predic tion of adhoc queries. As shown in Table 1, these features includ e QCS, QS, SOQ, SCQ, IDF, and ICTF. Since Boolean queries show different aspects from adhoc queries for which those features ha ve been proposed, we need to adjust the way these features are computed. For example, since adhoc queries do not contain negation (e.g.,  X  X  X  X  X  X  X  ) in contrast to a Boolean query, we consider terms associated only with conjunctions. SOQ measures cosine similarity between a Boolean query and the baseline query while QS is computed only within pseudo-relevant documents, not within the whole collection because we aim to generate Boolean queries to retrieve pseudo-relevant documents. For IDF, IC TF, and SCQ, as [24] did, we calculate the sum, the standard deviation, the ratio of the maximum to the minimum, the maximum, the arithmetic mean, the geometric mean, the harmonic mean, and the coefficient of variation of each value of a query term. These modified rules are applied to both unigrams and bigrams. Boolean Query Qua lity Predictors are features with the purpose of estimating Boolean query quality. All these features except BQTF are related to the retrieval results of a Boolean query because comparing a Boolean query result with the baseline result is a simple and effective way to predict Boolean query quality. BQCB is the ratio of the number of documents retrieved by both a Boolean query and the baseline system to the number of documents retrieved by the baseline system. This feature denotes how many of the documents retrieved by the baseline system can be found by a Boolean query. BQS is a measure of the number of pseudo-relevant documents retrie ved by a Boolean query relative QCS Query Clarity Score [21] QS Query Scope [23] in pseudo-relevant patents SOQ Similarity to Original Query [24] SCQ Similarity Collection Query [22] IDF Inverse Document Frequency ICTF Inverse Collection Term Frequency [24] BQCB Boolean Query result Coverage of Baseline retrieval results BQS Boolean Query Scope in pseudo-relevance LBQR Length of Boolean Query Result 
BQTF Boolean Query Term Frequency in pseudo-relevant documents to the whole size of pseudo -relevant documents, i.e., k . This feature helps to assure the effectiveness of a Boolean query. LBQR measures the number of re trieved documents for a Boolean query. Since we found that an ef fective Boolean query sometimes returns a shorter result list cont aining highly relevant documents than the baseline result, we consider this feature as a signal to find such Boolean queries. BQTF counts the frequency of a conjunctive query term in pseudo-relevant documents, assuming that a frequent term in pseudo-relevant documents might be effective for retrieving the documents. Note that we do not consider negation terms because th ey rarely appear in pseudo-relevant documents. Besides, fo r BQTF, the same statistics as used for IDF are calculated. Overall, a feature vector contains 37 different feature values (from 10 different types). We evaluate our Boolean query suggestion system by simulating professional search. We first desc ribe how to set up experiments to simulate search processes and then provide experimental results and discussion with Boolean query examples. To perform decision tree learning, the C4.5 algorithm with pruning turned on to obtain more accurate trees. For Boolean Query Ranking, SVM rank 5 is used as a learning-to-rank algorithm, and 10-fold cross-validation is performed. Queries and documents are stemmed by the Krovetz stemmer . More details are provided as follows. We setup experiments for two diffe rent search tasks considering two domains of interest: the patent and medical domains. Our task for the patent domain is patentability search which aims to find prior patents which may conflict with a new patent. On the other hand, the search task for the medical domain is reference retrieval http://en.wikipedia.org/wiki/C4.5_algorithm http://www.cs.cornell.edu/Peop le/tj/svm_light/svm_rank.html upon the request of disease information that the patient is undertaking. Both tasks are known to be recall-oriented. We suggest Boolean queries to help professionals formulate queries. In order to demonstrate the practical effectiveness of our system, we test our system under the following assumptions. First, we assume that professionals directly use su ggested Boolean queries without reformulation becau se we want to show the lower bound of performance that our sy stem can achieve. In real environments, professionals may use our suggestions or formulate new Boolean queries based on the suggestions. Second, we assume that the searchers will examine a maximum of the top 100 of every Boolean query result since 100 patents on average are examined in real examination processes as reported in [1]. Experiments are conducted on two different collections, patents and medical documents. As a patent corpus, we use USPTO (United States Patent and Tradem ark Office) patents provided by NTCIR-6 [8]. The collection contains 981,948 patents published from 1993 to 2000. To develop topics (new patents), we randomly selected 100 patents published in 2000, ensuring that their citations list more than 20 patents and at least 90% of them are included in the test collection. As done in the TREC chemical track [10] and NTCIR-6 [8], we considered patents cited in each topic patent as  X  X elevant X . We call this collection USPAT. For a medical corpus, we used the OHSUMED collection [27] which consists of 348,566 medical re ferences (documents) and 106 queries (topics). This test collec tion contains relevance judgments manually annotated using th ree relevance levels ( definitely relevant , possibly relevant , and not relevant ). We consider definitely and possibly relevant as  X  X elevant. X  For collecting pseudo-relevant documents as well as comparing performance, we employ baseline systems. As a baseline system for the USPAT, we consider the prior-art query generation method proposed in [6]. We generate a prior-art query which includes 100 unigrams ranked by TF-IDF from the  X  X rief summary X  section of each topic patent. To formulate the queries, we used both unigrams and bigrams, with 100 unigrams showed the best performance in R@100. Each query te rm is weighted by its TF in the topic patent. In addition, as [6] used, we employ the Indri search engine [28] to run each baseline query. For example, a baseline query is formed as  X # weight (15 glyph 20 character ... ) X . We consider the top 100 retrieved patents as pseudo-relevant. For OHSUMED, we consider all terms from the  X  X atient Information X  and  X  X nformation Request X  sections of each topic as a baseline query. The top 100 documents returned by the Indri search engine are regarded as pseudo-relevant. To run Boolean queries, we use a statistical Boolean retrieval model. For each topic, we firs t find all documents satisfying the given Boolean function (i.e., Boolean query) and rank the documents by the generative probability of the query: where D is a target document satisfying a Boolean query BQ , q is the query term not associated with negation in BQ ,  X  X  collection C, and  X  is the Dirichlet smoothing parameter [29] set to 2000. Note that we do not use any query processing including query term weighting in this Boolean retrieval model. Since many current patent search systems (e.g., Patent Scope) are also based on these simple term statistics , query evaluation using this statistical Boolean retrieval mode l would be more practical and similar to real search environm ents than using other enhanced retrieval techniques (e.g., learni ng-to-rank or a dependency model) that are hard to integrate into current patent search systems. We use several metrics for evaluatio n. Since professional search is different from adhoc retrieval, and we are evaluating Boolean Query Suggestion, we employ new metrics in addition to conventional IR evaluation metrics. Failure Rate measures the percentile ratio of  X  X ailure X  Boolean queries to all generated ones fo r each topic. Boolean queries which failed to retrieve any target documents are considered as a  X  X ailure X . Success Rate measures the percentile ratio of  X  X ffective X  Boolean queries to the all generated ones, where  X  X ffective X  means a Boolean query performing identical to or better than the baseline query with regard to R@100. This metric denotes how many Boolean queries achieve the baseline performance. Recall is a traditional IR evaluation metric; we use R@100 because every retrieval result is truncated at rank 100 according to the assumption for simulation. In order to evaluate our query ranking model, we use the best recall scores of the top-n Boolean queries for each topic because if the ranking model can place effective queries within top-n suggestions, they will be available to searchers. F1 and F2 indicate weighted F-scores , both harmonic means of precision and recall, where F2 denotes double the weight on recall than F1. In particular, we compute F1@100 and F2@100 of the best performing query (with respect to R@100) from the top-n ranked queries. These metrics reflect the practical effectiveness of Boolean queries. Sometimes, the best Boolean query returns a short result list where many documents are relevant, which results in higher precision than the baseline results and expedites examination processes by professionals. F1 and F2 capture both recall and precision simultaneousl y, and help to measure search efficiency. The first experiment is conducted to verify the effectiveness of Boolean Query Generation. We use the results of our baseline system as specified above and docu ments retrieved at ranks higher than 100 as non-relevant. In the USPAT, we genera te 4 types of attribute sets; unigrams and unigr ams+bigrams from a topic patent, and unigrams and unigrams+bigr ams from the pseudo-relevant documents. For OHSUMED, onl y 2 types, unigrams and unigrams+bigrams from the pseudo-re levant set, are used because there is no topic document. Ta ble 2 shows the performance of Boolean query generation. We report the average of each evaluation metric over all topics. topic documents and a set of pseudo-relevant documents, respectively. Metric USPTO  X  0.1421  X  8.15% 9.60% 4.85% 5.21% * 0.1686 5.89% 7.65% 7.94% 7.42%  X  * 0.1721 5.58% 7.26% 8.21% 7.69%  X  * 0.1724 5.60% 7.20% 8.56% 7.69%  X  * 0.1745 5.53% 7.06% 8.79% 7.84%  X  * 0.1808 5.58% 6.98% 9.13% 8.46%  X  * 0.1818 5.42% 6.88% 9.32% 8.66%  X  * 0.1833 5.36% 6.83% 9.76% 8.91% OHSUMED  X  0.2710  X  15.02% 14.07% 6.20% 6.28% 4  X  inrush  X   X  metallic  X  inverter  X  compressor  X  relay As shown in Table 2, our decision tree-based generation algorithm generates sufficiently many distin ct Boolean queries. About 200 queries are generated for each topic, of which 6 ~ 9% fail to retrieve any target documents. In USPAT, pseudo-relevant documents are more reliable re sources to generate Boolean queries than topic patents because of the smaller failure rate on average. Also, adding bigrams can le ad decision trees to generate more queries, and the relative failure rate could drop. However, bigrams seems to be harmful in terms of the success rate. In addition, considering the number of  X  X ffective X  Boolean queries (the number of successes), about 7% of queries show better or equal performance to the baseline system. Although this percentage may look low, we intend to obtain many effective queries via this generation proce ss. Indeed, as you see from the number of successes, more than 10 effective queries are generated for each topic. If we can place these effective queries at top ranks using our Boolean query ranking method, professionals who examine these suggesti ons will find the effective queries. We address the performance of the query ranking technique in the following section. In the next series of experiments, we evaluate the effectiveness of Boolean Query Ranking by investigating if it succeeds in placing effective Boolean queries at hi gh ranks, i.e., top 1 to 10. In training, generated queries for each topic are ordered by their R@100 scores. Also, in order to generate ground-truth ranked lists for training, we used Boolean queries from the topic-patent and pseudo-relevant patents in the U SPTO, i.e., a ranke d list contains unigram Boolean queries from the topic patent and pseudo-relevant patens and the other list includes unigram+bigram Boolean queries from the topic patent and pseudo-relevant patens. For the OHSUMED, we contain tw o ranked lists: unigram and unigram+bigram Boolean queries from pseudo-relevant documents. Creating a long ranked list by unifying unigram and unigram+bigram results is possible, but our scheme (handling unigram and unigram+bigram B oolean queries separately) empirically showed better performance. Overall, the training sets for the USPAT include 30,561 unigram and 38,034 unigram+bigram examples, and for OHSUMED contain 19,967 unigram and 23,122 unigram+bigram instances. Prior to evaluating the performance of Boolean Query Ranking, we optimize the Ranking SVM by selecting optimal features. In order to obtain an optimal set, we tested several combinations of the proposed features (Section 4.2.2) using 10-fold cross-validation. To identify a better combination, we use the best recall scores of the top-1 to 10 ranked Boolean queries for each combination. We examine what features can help to produce better rankings. Figure 4 depicts the average recall of the best Boolean queries over all topics of unigram training set in the USPAT. Gen shows consistently the worst performance, and when combined with Bool , the performance of Bool is also degraded. The best combination (QCS, SCQ, BQCB, and LBQR) found via trials of several possible combinations that can outperform Bool only. This is consistent with what [25] found, i.e., SCQ and QCS are the most effe ctive features for sub-query ranking. We additionally recognized that combining them with BQCB and LBQR is more effective because BQCB ensures that the Boolean query performs at least as well as the baseline and LBQR benefits effective Boolean queries returning a short result list. Experiments using the OHSUMED collection or unigram+bigram query terms s howed similar tendencies. With the optimal feature set, we now verify the effectiveness of our Boolean Query Ranking model by comparing with the baseline system. We calculate R@100, F1@100, and F2@100 of the best-performing query among the top-1 to 10 ranked queries. Table 3 shows the retrieval results within the top-1 to 10 ranked Boolean queries by 10-fold cross validation. From the table, we can identify how many top-n Boolean queries need to be examined to find an  X  X ffective X  one (i.e., performing as well as the baseline). In other words, results of the top-n queries which are not significantly different from the baseline result show that at least one effective Boolean query can be within the top-n.
 In Table 3, we see that effect ive Boolean queries can be found within the top 2 or 4 suggestions in each corpus. In USPAT, an effective Boolean query is observed within the top 2 ranks in both unigram and unigram+bigram cases. Furthermore, in the unigram case, significantly improved resu lts in terms of R@100 can be obtained by examining 7 or more Boolean queries. This is surprising to us because we expected Boolean queries to perform similar to the baseline. However, the result is a good indication that our system provides effective suggestions. In terms of F1 and F2, the top-5 unigram queries contain queries that outperform the baseline. These suggested queries retrieve about the same number of relevant documents as the ba seline result, but with higher precision. That is, these Boolean queries may be more efficient in that they can allow professionals to examine fewer documents. On the o ther hand, effective queries ar e not successfully generated in case of unigram+bigram. For exam ple, the number of generated effective queries in the unigram+big ram case is smaller than in the unigram case as seen in Table 2. Furthermore, many unigram results show statistically significant improvements over the unigram+bigram results, when comparing query performance at the same top-n . In OHSUMED, more queries need to be examined to find effective Boolean queries compared to USPAT. For example, four query suggestions should be examin ed in the unigram case in the OHSUMED, while only two querie s are needed in the USPAT. Furthermore, even more queries should be examined in the unigram+bigram case. In add ition, we could not obtain significantly better Boolean queries with regard to R@100 in this domain. For F-scores, however, we also identify more efficient Figure 4: Recall of the best query within Top-n Boolean queries (unigram case in USPAT).  X  X en X  and  X  X ool X  mean general query quality predictors and Boolean query quality predictors, respectively.  X  X en+Bool X  uses all features, and  X  X est X  uses only selective features (QCS, SCQ, BQCB, and LBQR) achieving the best performance. Boolean queries by examining the top 6 or 7 queries. A critical difference between OHSUMED and U SPTO is that there is little distinction between uni gram and unigram+bigram results in the OHSUMED while unigram queries ar e consistently better than unigram+bigram queries in the USPTO. Overall, our ranking model is effective in placi ng  X  X ffective X  Boolean query suggestions within th e top 2 to 5 ranks. In order to see how many new re levant documents (which were not retrieved by the baseline) are discovered and how many old relevant documents (which were retrieved by the baseline) are missed, by the best Boolean query within the top-n , we measure the ratios of the numbers of the new and missed old relevant documents to the number of all relevant documents (see  X  X issed Old Rel X  and  X  X ew Rel X  columns in Table 3). Generally, the more effective Boolean queries identified, the more new relevant documents are discovered while th e less old relevant documents are missed. Specifically, for the best query in the top-10 suggestions, the number of newly discovered relevant documents is greater than that of missed old relevant documents. In addition, the difference between unigram and unigram+bigram queries is not significant. Figures 5 and 6 depict scatter plots for the top-10 unigram and unigram+bigram Boolean queries in USPAT and OHSUMED, respectively. In Figure 5, the poi nts on or above the diagonal line indicate effective Boolean queries (performing identical to or better than the baseline). The section from 0.0 to 0.1 in the x-axis contains many queries perfor ming poorly. However, effective Boolean queries are also identified and some of them are superior to the low baseline. Also, there are many effective Boolean queries over the baseline performing moderately (0.1 ~ 0.7). In Figure 6, there are fewer poorly-performing queries (0.0 ~ 0.1), and most points are distributed between 0.1 and 0.9 in the x-axis. Along the diagonal line, effectiv e queries in Figure 6 are more spread out than those of Figure 5. Although many Boolean queries underperform the baselines in both plots, our system is promising because effective queries can be identified by searchers in real environments. Figure 7 depicts the accumulated length of the top-n query results which means the number of unique documents in all the results by the top-n queries. This is related to the efficiency of our system in the investigation, i.e., how long it takes to complete the task. Patent searchers generally exam ine maximally 600 patents to accomplish a single task (i.e., topic) [1]. In Figure 7, less than 500 unique documents accrue in the top-10 suggestions, which means that our suggestions can return a practical number of documents. Besides, since unigram+bigram queries return fewer documents than unigram queries, the covera ge by unigram+bigram queries appears narrower . We now provide a qualitative anal ysis of our system via real examples. Table 4 shows the top 5 Boolean queries suggested by our system, for a sample topic in USPAT. In this topic, the baseline system shows moderate performance (0.30 for R@100, 0.10 for F1@100), and some suggestions outperform the baseline. Many Boolean queries retrieve less than 100 documents, and some long suggestions (e.g.,  X  power unit  X  air conditioner  X  output  X  inverter  X  circuit) can precisely retrieve relevant documents in the short result lis ts. Several suggestions return significantly more re levant documents. Th e suggested Boolean queries can provide reasonable query contexts. For example,  X  X ompressor X  is often combined with  X  X nverter X ,  X  X upply X ,  X  X ircuit X  in Table 4 because compressor driving apparatus can include power supply, inverter drivers and storage circuits. Also, Figure 7: Average number of accumulated unique documents over the top-n Boolean queries.  X  X  X  and  X  X  X  indicate the results of unigram training ex amples and of bigram training examples, respectively.  X  X AT X  and  X  X ED X  denote the USPAT and the OHSUMED collections, respectively. Figure 5: Scatter plot of the top-10 Boolean queries in USPAT. The x-axis represents R@100 of the baseline queries.
 A circle and a cross mean a unigram query and a unigram+bigram query, respectively. Figure 6: Scatter plot of the top-10 Boolean queries in OHSUMED. The x-axis represents R@100 of the baseline queries. A circle and a cross mean a unigram query and a unigram+bigram query, respectively. looking at the negated terms, prof essional searchers can recognize where negation is applied in the provided context. For example,  X  X ower unit X  is negated when it comes with  X  X ir conditioner X ,  X  X utput X ,  X  X nverter X , and  X  X ircuit X  in Table 4. Since we found that past cited patents are dealing with inverters or circuits for air conditioners, power supplies can be considered less important. We proposed a framework to au tomatically suggest Boolean queries to assist professional searchers. We defined professional search as recall-oriented interactive tasks performed by information professionals. In order to provide reasonable suggestions in this context, we first generate a sufficient number of Boolean queries by exploiting decision tree learning and pseudo-relevant documents. To provide a reasonable number of suggestions, we rank the generate d queries by a query ranking model using query quality predic tors. In experiments, we simulated professional search pro cesses in the patent and medical domains with our suggestion syst em. We found that our system can not only generate many eff ective Boolean queries but also select highly effective queries for suggestion. For future work, we plan to conduct experiments on the legal domain (e.g., finding relevant cases). Also, we will consider adding synonym structure into the current suggestion framework. This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF CLUE IIS-0844226. Any opinions, findings and conc lusions or recommendations expressed in this material are th e authors' and do not necessarily reflect those of the sponsor. [1] H. Joho, L. Azzopardi, and W. Vanderbauwhede. A Survey of [2] L. Azzopardi, W. Vanderbauwhede, and H. Joho. Search [3] Y. Tseng and Y. Wu. A study of search tactics for patentability [4] R. Bache and L. Azzopardi. Improving access to large patent [5] H. Turtle. Natural language vs. Boolean query evaluation: a [6] X. Xue and W. B. Croft. Transforming patents into prior-art [7] X. Xue and W. B. Croft. Automatic query generation for [8] A. Fujii, M. Iwayama, and N. Ka ndo. Overview of the patent [9] H. Mase and M. Iwayama. NTCIR-6 Patent Retrieval [10] M. Lupu, F. Piroi, X. Huang, J. Zhu, and J. Tait. Overview of [11] J. Gobeill, D. Teodor, E. Patsche, and P. Ruch. Report on the [12] J. Rocchio. Relevance feedback in information retrieval. In [13] M. Mitra, A. Singhal, and C. Buckley. Improving automatic [14] J. Xu. Improving the effectiven ess of information retrieval [15] R. White, M. Bilenko, and S. Cucerzan. Studying the use of [16] R. A. Baeza-Yates, C. A. Hurtado, and M. Mendoza. Query [17] R. Jones, B. Rey, O. Madani , and W. Greiner. Generating [18] X. Wang and C. Zhai. Mining term association patterns from [19] Z. Hashmi, T. Zrimec, and A. Hopkins. Automatic query [20] S. Russell and P. Norvig. Artificial Intelligence: A Modern [21] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting [22] Y. Zhao, F. Scholer, and Y. Tsegay. Effective Pre-retrieval [23] B. He and I. Ounis. Inferring query performance using pre-[24] G. Kumaran and V. Carvalho. Reducing Long Queries Using [25] V. Dang, M. Bendersky and W. B. Croft. Learning to Rank [26] T. Joachims, Optimizing Search Engines Using Clickthrough [27] W. R. Hersh, C. Buckley, T. J. Leone, and D. H. Hickam. [28] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: a [29] C. Zhai and J. Lafferty. A study of smoothing methods for 
