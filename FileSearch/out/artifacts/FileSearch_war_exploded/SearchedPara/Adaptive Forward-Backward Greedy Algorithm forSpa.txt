 Consider a set of input vectors x 1 , . . . , x n  X  R d , with corresponding desired output variables y , . . . , y n . The task of supervised learning is to estimate the functional relationship y  X  f ( x ) The quality of prediction is often measured through a loss function  X  ( f ( x ) , y ) . In this paper, we consider linear prediction model f ( x ) = w T x . As in boosting or kernel methods, nonlinearity can be introduced by including nonlinear features in x .
 We are interested in the scenario that d n . That is, there are many more features than the number of samples. In this case, an unconstrained empirical risk minimization is inadequate because the solution overfits the data. The standard remedy for this problem is to impose a constraint on w to obtain a regularized problem. An important target constraint is sparsity, which corresponds to the (non-convex) L 0 regularization, where we define k w k 0 = |{ j : w j 6 = 0 }| = k . If we know the sparsity parameter k , a good learning method is L 0 regularization: If k is not known, then one may regard k as a tuning parameter, which can be selected through cross-validation. This method is often referred to as subset selection in the literature. Sparse learning is an essential topic in machine learning, which has attracted considerable interests recently. Generally speaking, one is interested in two closely related themes: feature selection, or identifying the basis functions with non-zero coefficients; estimation accuracy, or reconstructing the target function from noisy observations. It can be shown that the solution of the L 0 regularization problem in (1) achieves good prediction accuracy if the target function can be approximated by a sparse  X  w . It can also solve the feature selection problem under extra identifiability assumptions. However, a fundamental difficulty with this method is the computational cost, because the number of subsets of { 1 , . . . , d } of cardinality k (corresponding to the nonzero components of w ) is exponential in k . There are no efficient algorithms to solve the subset selection formulation (1). Due to the computational difficult, in practice, there are three standard methods for learning sparse representations by solving approximations of (1). The first approach is L 1 -regularization (Lasso). The idea is to replace the L 0 regularization in (1) by L 1 regularization. It is the closest convex approximation to (1). It is known that L 1 regularization often leads to sparse solutions. Its perfor-mance has been theoretically analyzed recently. For example, if the target is truly sparse, then it was shown in [10] that under some restrictive conditions referred to as irrepresentable conditions , L 1 regularization solves the feature selection problem. The prediction performance of this method has been considered in [6, 2, 1, 9]. Despite its popularity, there are several problems with L 1 regu-larization: first, the sparsity is not explicitly controlled, and good feature selection property requires strong assumptions; second, in order to obtain very sparse solution, one has to use a large regulariza-tion parameter that leads to suboptimal prediction accuracy because the L 1 penalty not only shrinks irrelevant features to zero, but also shrinks relevant features to zero. A sub-optimal remedy is to threshold the resulting coefficients; however this requires additional tuning parameters, making the resulting procedures more complex and less robust. The second approach to approximately solve the subset selection problem is forward greedy algorithm , which we will describe in details in Sec-tion 2. The method has been widely used by practitioners. The third approach is backward greedy algorithm . Although this method is widely used by practitioners, there isn X  X  any theoretical analysis when n d (which is the case we are interested in here). The reason will be discussed later. In this paper, we are particularly interested in greedy algorithms because they have been widely used but the effectiveness has not been well analyzed. As we shall explain later, neither the standard forward greedy idea nor th standard backward greedy idea is adequate for our purpose. However, the flaws of these methods can be fixed by a simple combination of the two ideas. This leads to a novel adaptive forward-backward greedy algorithm which we present in Section 3. The general idea works for all loss functions. For least squares loss, we obtain strong theoretical results showing that the method can solve the feature selection problem under moderate conditions.
 For clarity, this paper only considers the fixed design formulation. To simplify notations in our description, we will replace the optimization problem in (1) with a more general formulation. In-stead of working with n input data vectors x i  X  R d , we work with d feature vectors f j  X  R n ( j = 1 , . . . , d ), and y  X  R n . Each f j corresponds to the j -th feature component of x i for i = 1 , . . . , n . That is, f j,i = x i,j . Using this notation, we can generally rewrite (1) with in the form  X  w = arg min w  X  R d R ( w ) subject to k w k 0  X  k , where weight w = [ w 1 , . . . , w d ]  X  R d , and R ( w ) is a real-valued cost function which we are interested in optimization. For least squares regression, we have R ( w ) = n  X  1 k P j w j f j  X  y k 2 2 . In the following, we also let e j  X  R d be the vector of zeros, except for the j -component which is one. For convenience, we also introduce the following notations.
 Definition 1.1 Define supp( w ) = { j : w j 6 = 0 } as the set of nonzero coefficients of a vector w = [ w 1 , . . . , w d ]  X  R d . For a weight vector w  X  R d , we define mapping f : R d  X  R n as: f k 2 subject to supp( w )  X  F , and let  X  w ( F ) =  X  w ( F, y ) be the solution of the least squares problem using features F . Forward greedy algorithms have been widely used in applications. The basic algorithm is presented in Figure 1. Although a number of variations exist, they all share the basic form of greedily picking an additional feature at every step to aggressively reduce the cost function. The intention is to make most significant progress at each step in order to achieve sparsity. In this regard, the method can be considered as an approximation algorithm for solving (1).
 A major flaw of this method is that it can never correct mistakes made in earlier steps. As an illustration, we consider the situation plotted in Figure 2 with least squares regression. In the figure, y can be expressed as a linear combination of f 1 and f 2 but f 3 is closer to y . Therefore using the forward greedy algorithm, we will find f 3 first, then f 1 and f 2 . At this point, we have already found all good features as y can be expressed by f 1 and f 2 , but we are not able to remove f 3 selected in the first step. The above argument implies that forward greedy method is inadequate for feature selection. The method only works when small subsets of the basis functions { f j } are near orthogonal [7]. In general, Figure 2 (which is the case we are interested in in this paper) shows that forward greedy algorithm will make errors that are not corrected later on.
 In order to remedy the problem, the so-called backward greedy algorithm has been widely used by practitioners. The idea is to train a full model with all the features, and greedily remove one feature (with the smallest increase of cost function) at a time. Although at the first sight, backward greedy method appears to be a reasonable idea that addresses the problem of forward greedy algorithm, it is computationally very costly because it starts with a full model with all features. Moreover, there are no theoretical results showing that this procedure is effective. In fact, under our setting, the method may only work when d n (see, for example, [3]), which is not the case we are interested in. In the case d n , during the first step, we start with a model with all features, which can immediately overfit the data with perfect prediction. In this case, the method has no ability to tell which feature is irrelevant and which feature is relevant because removing any feature still completely overfits the data. Therefore the method will completely fail when d n , which explains why there is no theoretical result for this method. The main strength of forward greedy algorithm is that it always works with a sparse solution ex-plicitly, and thus computationally efficient. Moreover, it does not significantly overfit the data due to the explicit sparsity. However, a major problem is its inability to correct any error made by the algorithm. On the other hand, backward greedy steps can potentially correct such an error, but need to start with a good model that does not completely overfit the data  X  it can only correct errors with a small amount of overfitting. Therefore a combination of the two can solve the fundamental flaws of both methods. However, a key design issue is how to implement a backward greedy strategy that is provably effective. Some heuristics exist in the literature, although without any effectiveness proof. For example, the standard heuristics, described in [5] and implemented in SAS, includes another threshold 0 in addition to : a feature is deleted if the cost-function increase by performing the deletion is no more than 0 . Unfortunately we cannot provide an effectiveness proof for this heuristics: if the threshold 0 is too small, then it cannot delete any spurious features introduced in the forward steps; if it is too large, then one cannot make progress because good features are also deleted. In practice it can be hard to pick a good 0 , and even the best choice may be ineffective. This paper takes a more principled approach, where we specifically design a forward-backward greedy procedure with adaptive backward steps that are carried out automatically. The procedure has provably good performance and fixes the drawbacks of forward greedy algorithm illustrated in Figure 2. There are two main considerations in our approach: we want to take reasonably aggressive backward steps to remove any errors caused by earlier forward steps, and to avoid maintaining a large number of basis functions; we want to take backward step adaptively and make sure that any backward greedy step does not erase the gain made in the forward steps. Our algorithm, which we refer to as FoBa , is listed in Figure 3. It is designed to balance the above two aspects. Note that we only take a backward step when the increase of cost function is no more than half of the decrease of cost function in earlier forward steps. This implies that if we take ` forward steps, then no matter how many backward steps are performed, the cost function is decreased by at least an amount of `/ 2 . It follows that if R ( w )  X  0 for all w  X  R d , then the algorithm terminates after no more than 2 R (0) / steps. This means that the procedure is computationally efficient.
 Now, consider an application of FoBa to the example in Figure 2. Again, in the first three forward steps, we will be able to pick f 3 , followed by f 1 and f 2 . After the third step, since we are able to express y using f 1 and f 2 only, by removing f 3 in the backward step, we do not increase the cost. Therefore at this stage, we are able to successfully remove the incorrect basis f 3 while keeping the good features f 1 and f 2 . This simple illustration demonstrates the effectiveness of FoBa. In the following, we formally characterize this intuitive example, and prove the effectiveness of FoBa for feature selection as well as parameter estimation. Our analysis assumes the least squares loss. However, it is possible to handle more general loss functions with a more complicated derivation. We introduce the following definition, which characterizes how linearly independent small subsets of { f j } of size k are. For k n , the number  X  ( k ) can be bounded away from zero even when d n . For example, for random basis functions f j , we may take ln d = O ( n/k ) and still have  X  ( k ) to be bounded away from zero. This quantity is the smallest eigenvalue of the k  X  k diagonal blocks methods such as in [2, 8], etc. We shall refer it to as the sparse eigenvalue condition . This condition is the least restrictive condition when compared to other conditions in the literature [1]. Definition 3.1 Define for all 1  X  k  X  d :  X  ( k ) = inf 1 n k f ( w ) k 2 2 / k w k 2 2 : k w k 0  X  k . Assumption 3.1 Consider least squares loss R ( w ) = 1 n k f ( w ( k ) )  X  y k 2 2 . Assume that the basis are independent (but not necessarily identically distributed) sub-Gaussians: there exists  X   X  0 such Both Gaussian and bounded random variables are sub-Gaussian using the above definition. For example, we have the following Hoeffding X  X  inequality. If a random variable  X   X  [ a, b ] , then E The following theorem is stated with an explicit for convenience. In applications, one can always run the algorithm with a smaller and use cross-validation to determine the optimal stopping point. Theorem 3.1 Consider the FoBa algorithm in Figure 3, where Assumption 3.1 holds. Assume also that the target is sparse: there exists  X  w  X  R d such that  X  w T x i = E y i for i = 1 , . . . , n , and  X  F = supp(  X  w ) . Let  X  k = |  X  F | , and &gt; 0 be the stopping criterion in Figure 3. Let s  X  d be an  X  ( s )  X  2 , and for some  X   X  (0 , 1 / 3) ,  X  64  X  ( s )  X  2  X  2 ln(2 d/ X  ) /n , then with probability larger than 1  X  3  X  , when the algorithm terminates, we have F ( k ) =  X  F and k w ( k )  X   X  w k 2  X   X  p  X  k/ ( n X  (  X  k )) h 1 + p 20 ln(1 / X  ) i .
 The result shows that one can identify the correct set of features  X  F as long as the weights  X  w j are not close to zero when j  X   X  F . This condition is necessary for all feature selection algorithms including previous analysis of Lasso. The theorem can be applied as long as eigenvalues of small s  X  s diagonal blocks of the design matrix [ f T i f j ] i,j =1 ,...,d are bounded away from zero (i.e., sparse eigenvalue condition). This is the situation under which the forward greedy step can make mistakes, but such mistakes can be corrected using FoBa. Because the conditions of the theorem do not prevent forward steps from making errors, the example described in Figure 2 indicates that it is not possible to prove a similar result for the forward greedy algorithm. The result we proved is also better than that of Lasso, which can successfully select features under irrepresentable conditions of [10]. It is known that the sparse eigenvalue condition considered here is generally weaker [8, 1].
 order to select features effectively. If any nonzero weight is below the noise level, then no algorithm can distinguish it from zero with large probability. That is, in this case, one cannot reliably perform feature selection due to the noise. Therefore FoBa is near optimal in term of its ability to perform reliable feature selection, except for the constant hiding in O (  X  ) . For target that is not truly sparse, similar results can be obtained. In this case, it is not possible to correctly identify all the features with large probability. However, we can show that FoBa can still select part of the features reliably, with good parameter estimation accuracy. Such results can be found in the full version of the paper, available from the author X  X  website. We compare FoBa described in Section 3) to forward-greedy and L 1 -regularization on artificial and real data. They show that in practice, FoBa is closer to subset selection than the other two approaches, in the sense that FoBa achieves smaller training error given any sparsity level. In oder to compare with Lasso, we use the LARS [4] package in R, which generates a path of actions for adding and deleting features, along the L 1 solution path. For example, a path of { 1 , 3 , 5 ,  X  3 , . . . } means that in the fist three steps, feature 1 , 3 , 5 are added; and the next step removes feature 3 . Using such a solution path, we can compare Lasso to Forward-greedy and FoBa under the same framework. Similar to the Lasso path, FoBa also generates a path with both addition and deletion operations, while forward-greedy algorithm only adds features without deletion. Our experiments compare the performance of the three algorithms using the corresponding feature addition/deletion paths. We are interested in features selected by the three algorithms at any sparsity level k , where k is the desired number of features presented in the final solution. Given a path, we can keep an active feature set by adding or deleting features along the path. For example, for path { 1 , 3 , 5 ,  X  3 } , we have two potential active feature sets of size k = 2 : { 1 , 3 } (after two steps) and { 1 , 5 } (after four steps). We then define the k best features as the active feature set of size k with the smallest least squares error because this is the best approximation to subset selection (along the path generated by the algorithm). From the above discussion, we do not have to set explicitly in the FoBa procedure. Instead, we just generate a solution path which is five times as long as the maximum desired sparsity k , and then generate the best k features for any sparsity level using the above described procedure. 4.1 Simulation Data Since for real data, we do not know the true feature set  X  F , simulation is needed to compare feature selection performance. We generate n = 100 data points of dimension d = 500 . The target vector  X  w is truly sparse with  X  k = 5 nonzero coefficients generated uniformly from 0 to 10 . The noise level is  X  2 = 0 . 1 . The basis functions f j are randomly generated with moderate correlation: that is, some basis functions are correlated to the basis functions spanning the true target. Note that if there is no correlation (i.e., f j are independent random vectors), then both forward-greedy and L -regularization work well because the basis functions are near orthogonal (this is the well-known case considered in the compressed sensing literature). Therefore in this experiment, we generate moderate correlation so that the performance of the three methods can be differentiated. Such mod-erate correlation does not violate the sparse eigenvalue condition in our analysis, but violates the more restrictive conditions for forward-greedy method and Lasso.
 Table 1 shows the performance of the three methods (including two versions of FoBa), where we repeat the experiments 50 times, and report the average  X  standard-deviation. We use the three methods to select five best features, using the procedure described above. We report three metrics. Training error is the squared error of the least squares solution with the selected five features. Pa-rameter estimation error is the 2-norm of the estimated parameter (with the five features) minus the true parameter. Feature selection error is the number of incorrectly selected features. It is clear from the table that for this data, FoBa achieves significantly smaller training error than the other two methods, which implies that it is closest to subset selection. Moreover, the parameter estimation performance and feature selection performance are also better. The two versions of FoBa perform very similarly for this data. 4.2 Real Data Instead of listing results for many datasets without gaining much insights, we present a more detailed study on a typical dataset, which reflect typical behaviors of the algorithms. Our study shows that FoBa does what it is designed to do well: that is, it gives a better approximation to subset selection than either forward-greedy or L 1 regularization. Moreover, the difference between aggressive FoBa and conservative FoBa become more significant.
 In this study, we use the standard Boston Housing data, which is the housing data for 506 cen-sus tracts of Boston from the 1970 census, available from the UCI Machine Learning Database Repository : http://archive.ics.uci.edu/ml/ . Each census tract is a data-point, with 13 features (we add a constant offset one as the 14th feature), and the desired output is the housing price. In the experiment, we randomly partition the data into 50 training plus 456 test points. We perform the experiments 50 times, and for each sparsity level from 1 to 10, we report the average training and test squared error. The results are plotted in Figure 4. From the results, we can see that FoBa achieves better training error for any given sparsity, which is consistent with the theory and the design goal of FoBa. Moreover, it achieves better test accuracy with small sparsity level (corresponding to a more sparse solution). With large sparsity level (corresponding to a less sparse solution), the test error increase more quickly with FoBa. This is because it searches a larger space by more aggressively mimic subset selection, which makes it more prone to overfitting. However, at the best sparsity level of 2 or 3 (for aggressive and conservative FoBa, respectively), FoBa achieves significantly better test error. Moreover, we can observe with small sparsity level (a more sparse solution), L 1 regularization performs poorly, due to the bias caused by using a large L 1 -penalty. Figure 4: Performance of the algorithms on Boston Housing data Left: average training squared error versus sparsity; Right: average test squared error versus sparsity For completeness, we also compare FoBa to the backward-greedy algorithm and the classical heuris-tic forward-backward greedy algorithm as implemented in SAS (see its description at the beginning of Section 3). We still use the Boston Housing data, but plot the results separately, in order to avoid cluttering. As we have pointed out, there is no theory for the SAS version of forward-backward greedy algorithm. It is difficult to select an appropriate backward threshold 0 : a too small value leads to few backward steps, and a too large value leads to overly aggressive deletion, and the pro-cedure terminates very early. In this experiment, we pick a value of 10, because it is a reasonably large quantity that does not lead to an extremely quick termination of the procedure. The perfor-mance of the algorithms are reported in Figure 5. From the results, we can see that backward greedy algorithm performs reasonably well on this problem. Note that for this data, d n , which is the scenario that backward does not start with a completely overfitted full model. Still, it is inferior to FoBa at small sparsity level, which means that some degree of overfitting still occurs. Note that backward-greedy algorithm cannot be applied in our simulation data experiment, because d n which causes immediate overfitting. From the graph, we also see that FoBa is more effective than the SAS implementation of forward-backward greedy algorithm. The latter does not perform signif-icant better than the forward-greedy algorithm with our choice of 0 . Unfortunately, using a larger backward threshold 0 will lead to an undesirable early termination of the algorithm. This is why the provably effective adaptive backward strategies introduced in this paper are superior. This paper investigates the problem of learning sparse representations using greedy algorithms. We showed that neither forward greedy nor backward greedy algorithms are adequate by themselves. However, through a novel combination of the two ideas, we showed that an adaptive forward-back greedy algorithm, referred to as FoBa, can effectively solve the problem under reasonable condi-tions. FoBa is designed to be a better approximation to subset selection. Under the sparse eigenvalue condition, we obtained strong performance bounds for FoBa for feature selection and parameter es-timation. In fact, to the author X  X  knowledge, in terms of sparsity, the bounds developed for FoBa in this paper are superior to all earlier results in the literature for other methods. Figure 5: Performance of greedy algorithms on Boston Housing data. Left: average training squared error versus sparsity; Right: average test squared error versus sparsity Our experiments also showed that FoBa achieves its design goal: that is, it gives smaller training error than either forward-greedy or L 1 regularization for any given level of sparsity. Therefore the experiments are consistent with our theory. In real data, better sparsity helps on some data such as Boston Housing . However, we shall point out that while FoBa always achieves better training error for a given sparsity in our experiments on other datasets (thus it achieves our design goal), L 1 -regularization some times achieves better test performance. This is not surprising because sparsity is not always the best complexity measure for all problems. In particular, the prior knowledge of using small weights, which is encoded in the L 1 regularization formulation but not in greedy algorithms, can lead to better generalization performance on some data (when such a prior is appropriate). [1] Peter Bickel, Yaacov Ritov, and Alexandre Tsybakov. Simultaneous analysis of Lasso and [2] Florentina Bunea, Alexandre Tsybakov, and Marten H. Wegkamp. Sparsity oracle inequalities [3] Christophe Couvreur and Yoram Bresler. On the optimality of the backward greedy algorithm [4] Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. [5] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning . Springer, [6] Vladimir Koltchinskii. Sparsity in penalized empirical risk minimization. Annales de l X  X nstitut [7] Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Info. [8] Cun-Hui Zhang and Jian Huang. Model-selection consistency of the Lasso in high-dimensional [9] Tong Zhang. Some sharp performance bounds for least squares regression with L 1 regulariza-[10] Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning
