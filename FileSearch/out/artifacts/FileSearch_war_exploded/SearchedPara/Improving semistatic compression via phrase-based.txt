 1. Introduction
A lossless compressor is evaluated by the amount of space reduction it achieves, as well as by its compression and decom-pression efficiency. Decompression is usually more important since in many cases the file is compressed once and decom-pressed many times. When using a compressor within the framework of a text database, however, more factors come into pressed text without decompressing it, and (ii) local decompression (random access) of any portion of the compressed file without the need of decompressing it from the beginning.

Statistical compressors replace the most frequent source symbols by shorter codewords. Therefore, they need a model of
Once the model is built, an encoding scheme assigns each source symbol a codeword. Then, the compressor performs a second pass over the original file replacing each source symbol by the corresponding codeword. Finally, to inform the decompressor of the correspondence between codewords and source symbols, a prelude is stored along with the compressed text.  X 
Classical statistical compressors are not well suited for text databases since they use character-based modelers, which even using a Huffman coding ( Huffman, 1952 ) obtain poor compression ratios
Bentley, Sleator, Tarjan, and Wei (1986) used words as source symbols instead. This might seem problematic, as the size using a word-based modeler can reach compression ratios around 25% ( Moffat, 1989 ).

Byte-oriented variants of Huffman codes allow for faster decompression in exchange for higher compression ratios. These aimed at allowing fast direct searches on the compressed text, have been superseded by the more recent Dense Codes family pressor using a semistatic zero-order word-based modeler. pler to program.

One way to improve compression ratios beyond the 30% achievable with byte-oriented compression is to use higher-order modeling, that is, to capture the dependencies between consecutive words in the text. For example, in a newspaper was indeed mentioned in the foundational article of Bentley et al. (1986) , but not explored further.
The idea of capturing the co-occurrence of sequences is behind high-order statistical compressors and repetition-based well in practice, but it is particularly unfriendly to random access and direct searches.
 press static databases to be stored in CD or DVDs. Some afford several passes over the original text and/or a high memory consumption to achieve high compression, yet decompression is fast and memory-efficient.

By using an algorithm for finding the shortest path in a network, Katajainen et al. (1989) gave a procedure that, given a
Smyth, 2002 ) uses Crochemore X  X  algorithm ( Crochemore, 1981 ) to create a list of phrases that are used to encode the text as the compressor traverses the original text.

In this paper we improve the compression ratio of ETDC by coupling it with an offline modeler that detects promising sequences of words and regards them as a single token. As a result, compression ratios become competitive with the best compressors (near 25%). Compression speed is reasonable, while decompression, random access, and direct search speeds stay very appealing for compressed text database scenarios.

More precisely, we introduce two new semistatic modelers that, coupled with the encoding scheme of ETDC, produce two new compressors. The first one is called Pair-Based End-Tagged Dense Code (PETDC) , with a compression ratio around 28 X 29%
Phrase-Based End-Tagged Dense Code (PhETDC) goes one step further by building a model composed of words and sequences of words (phrases). Its compression ratio improves up to around 23%.

The key point is how the pairs and phrases are selected. For PETDC, we cannot add all the possible pairs because the size of the prelude would spoil the compression. We have devised a method to select the pairs of words that most improve the compression, taking into account the cost in space of adding such a pair to the prelude. To construct phrases, PhETDC uses the replacement algorithm of Re-Pair, which is preempted to speed up compression. As the resulting sequence of phrases is still compressible, PhETDC applies a dense coding scheme over it. 2. End-Tagged Dense Code
In general, ETDC can be defined over symbols of b bits, although in this paper we focus on the byte-oriented version where b = 8. Given source symbols with decreasing probabilities { p
Dense Code is formed by a sequence of symbols of b bits, all of them representing digits in base 2 2 b 1 1) except the last one, which has a value between 2 b 1
That is, the first word is encoded as 1 0000000, the second as 1 0000001, until the 128th as 1 1111111. The 129th word is coded as 0 0000000:1 0000000, the 130th as 0 0000000:1 0000001 and so on until the (128 prelude only includes the sorted vocabulary (list of source symbols).

However, not only is the sequential procedure available to assign codewords to the words. There are simple encode and decode procedures that can be efficiently implemented, because the codeword corresponding to the symbol in position i is obtained as the number x written in base 2 b 1 , where x  X  i 2 digit.

Function encode obtains the codeword C i encode ( i ) for the word at the i th position in the ranked vocabulary. Function decode gets the position i decode ( C i ) in the vocabulary for a given codeword C l = O (log( i )/ b ) is the length in digits of the codeword C and masking.

A deeper description of ETDC, including analytical and empirical studies, can be found in Brisaboa et al. (2007) . 3. Pair-Based End-Tagged Dense Code the different pairs of words that occur adjacent in the source text and counts their number of occurrences. PETDC aims at taking advantage of the co-occurrence of words in the text by including some pairs in the vocabulary, thus comprising both codeword assigned by the encoding scheme. Therefore, replacing two source words by just one codeword during the second pass may need fewer bytes than replacing two single words by two codewords.

Example 1. Consider a text containing the following one-character words ADCBACDCCDABABACBB. Being only four source of the compressed file (compressed text plus the prelude) is 18 + 4 = 22 bytes. Let us add the most frequent pair of words and  X  X  X . Now the compressed text occupies 15 bytes and the prelude needs 6 bytes (the new pair is added as two pointers to the positions of the plain words forming the pair). Therefore, the compressed file requires 15 + 6 = 21 bytes. 3.1. Deciding which pairs should be added to the vocabulary
Adding all the different pairs to the vocabulary does not usually improve the compression ratio because the prelude text and the size of the prelude) depending on the number of pairs added. The process starts adding the most frequent pairs ness greatly. However, at some point, the gain obtained by replacing two words by a unique codeword does not compensate the growth of the prelude.
Fig. 1 b shows that the curve has multiple local minima. This fact prevents us from breaking the addition of pairs to the vocabulary when the addition of a new pair worsens the compression. Instead of that, as in Apostolico and Lonardi (2000) ,
PETDC uses a gain function that determines which ones have to be added. 3.1.1. Gain function as the number of occurrences of a word or pair x . Let us also define C the occurrences of a and b in two cases: 1. The pair is skipped ( skip bytes ). 2. The pair is added to the vocabulary ( add bytes ).

Once those values are computed, the pair a b is added to the vocabulary if skip skip bytes and add bytes are given by the two following expressions: number of occurrences are f a f a b and f b f a b , respectively. The term K is an estimation of the number of bytes needed to store any pair into the vocabulary. In our implementation, K = 5 bytes.
 included later. This happens because, once we choose a pair, we do not access the text to replace it by another symbol in a those flags before adding a new pair to the vocabulary. 3.2. Data structures
The data structures used by the compressor are sketched in Fig. 2 . There are two well-defined parts: 1. Data structures that make up the vocabulary. 2. Data structures needed to hold the candidate pairs.

The vocabulary of the compressor consists of: a hash table used to locate a word or pair quickly ( hashSymb ) and two vec-tors: symbVect and topVect . The hash table hashSymb contains eight fields: 1. type indicates if an entry is either a word w or a pair p , 2. word stores the original word in plain text form, if type is set to w , 3. freq keeps the number of occurrences of the entry, 4. e 1 and e 2 flag if the word is enabled to be the first or second component of a pair, respectively, 5. w 1 and w 2 store, for an entry of type p , pointers to the words that form the pair, and ally be replaced by calls to the function encode(i).

The vector symbVect maintains the vocabulary sorted by frequency. Its first slot points to the entry of hashSymb where the and so on. Assuming that symbVect is sorted by decreasing frequency, topVect [ f whose frequency is f i . If there are no symbols of frequency f the ascending sort of the vocabulary such that f i &lt; f
The offline compressors that use a gain function need to calculate the frequency of the phrases and an estimation of the space consumed by their compressed version. This may result in a bottleneck that our compressors should alleviate in order to obtain reasonable compression times. Recall that in ETDC, we do not need the actual frequency of the source symbols to compute their codewords, we only need their position in the ordered vocabulary. Maintaining the vocabulary ordered upon insertions and deletions would be too expensive. Therefore, to estimate the positions of the source symbols, we only main-tain topVect updated. Then, being x an entry with frequency f imate value, computing this estimation is much cheaper than computing the actual codeword sizes.
Finally, managing the candidate pairs to be added to the vocabulary includes the use of two auxiliary data structures: 1. A hash table hashPairs, with fields freq , w 1 , and w 2. A vector pairsVector , which maintains all the candidate pairs sorted, in the same way as symbVect does. 3.3. Compression, decompression, and search procedures
Compression consists of five main phases: 1. First pass along the text . As shown, during this pass, PETDC obtains the different didate pairs that appear in the text. At the same time, the number of occurrences of all those elements is obtained. The are sorted by decreasing frequency. Finally, topVect is initialized: consider m the maximum frequency value of the phase is O ( n + v log v + p log p + m ). We show empirical evidences that n p and n m , and typically p &gt; the cost is O ( n ). Fig. 2 a shows the state of the structures after this phase. added to the vocabulary or discarded by applying the gain function explained in Section 3.1.1 . To compute the gain func-tion, as seen, using topVect we compute the current positions and therefore it is the fourth most frequent element of the vocabulary. However, observe that topVect is updated since, for example, the seventh entry is now empty due to lack of vocabulary entries with frequency 7. Now, topVect contains the pointers to a hypothetic updated version of symbVect , which no longer exists. The overall cost of this phase is O ( p p )= O ( p a m ), being p a the number of pairs added to the vocabulary. 3. Code Generation Phase . The only data structures needed in this phase are depicted in Fig. 2 c. The vocabulary (with v 0 entries) is ordered by frequency and the encoding scheme of ETDC is used. Encoding takes O (
Symb will contain the mapping entry i ? code i " i 2 1 ... v 0 . The cost of this phase is O ( 4. Second pass . The text is again traversed reading two words at a time and the source words are replaced by codewords. If the read pair a b is in hashSymb , then the codeword C a b and only the following word c is read to form a new pair b c . This phase takes O ( n ) time. is used to save the type of entry. Then the v 0 entries of the vocabulary follow that bitmask. A normal entry stores plain the-fly C i encode ( i ) function in order to save space). Finally, the whole prelude is encoded with character-based Huffman.

Considering the cost of each phase, the overall cost of the whole process is O ( n + p we obtain that the overall cost is O( n + p a m ). Note that since p p a n .

Decompression starts by loading the prelude into a vector that keeps both words and pairs. For each codeword C the compressed file, i decode ( C i ) is used to obtain the entry i containing either the word or the pair associated to C i .

When searching text compressed with PETDC, a searched word a can occur alone or as a part of one or more pairs a b , ca , ... Therefore, we have to use a multi-pattern matching algorithm. When we load the vocabulary, for each single-word both all those codewords and codeword C a . 4. Phrase-Based End-Tagged Dense Code
PhETDC goes one step beyond with regard to PETDC, since it represents phrases, with an undetermined number of words, using just one codeword.

However, the prelude issue gets more complicated because the number of possible entries in it is much higher. Therefore, the main problem is again how to determine which phrases should be introduced in the vocabulary to improve the compres-sion ratio.

PhETDC is an evolution of PETDC based on the Re-Pair compression algorithm ( Larsson &amp; Moffat, 1999 ). PhETDC performs placed word in the hash table hashSymb .

Again, single words are immediately added to the vocabulary, but the pairs of words are kept in a list of candidate pairs, which may be added later to the vocabulary.

The second phase of the compression process begins by adding the most frequent pair a b of the list of candidate pairs to by the new integer pointing to the entry in hashSymb that stores the new pair. This produces a reduction in the number of are: 1. The pair c i a formed by the previous word ( c i ) and the first word of the added pair ( a ). 2. The pair bd i formed by the second word of the added pair ( b ) and the following word ( d In addition, new candidate pairs appear.

For each occurrence of the added pair ( a b ? k ), new candidate pairs/phrases appear. They are formed by: 1. The previous word and the added pair ( c i k ). 2. The new pair and the next word ( kd i ).

Example 2. Let us consider the text CDBCDCABCDBCACD. The most frequent pair is CD with four occurrences, and the second one is BC with 3. Then, we add the new element CD to the vocabulary and we replace all the occurrences of CD by the symbol E. The new text is EBECABEBCAE .

Now, the pair BC has only one occurrence, whereas two new pairs (BE, EC) appear. As explained, BE and EC are pairs that the text BCD .

Once this replacement is finished, the processed pair is removed from the list of candidate pairs and the process contin-that point on, two words, two pairs, or one word and a pair can form pairs.
 Once the replacement process is finished, we use the ETDC encoding scheme to encode the resulting vector of integers. point to original words). Fig. 3 shows the main steps of the compression process. 4.1. Deciding which phrases should be added to the vocabulary
Fig. 4 shows how the compression process, explained in the previous section, evolves as we add all the candidate pairs in (in this experiment, this point was reached after the addition of 6550 pairs and when the file occupied 816,757 bytes). We studied two methods to avoid adding pairs that worsen the compression: using a finishing condition and using a gain function.

Stopping the addition of phrases when their number of occurrences was under a given threshold value worked well. How-ever, we obtained better results when that finishing condition was removed and we processed all the candidate pairs, choos-ing, by means of a gain function, those pairs that improved the compression.

In addition, we also studied if choosing the most frequent pair yields the best ordering to add pairs. Several strategies were tested, each one with different motivations: ends when there is no pair in the list of candidates increasing the entropy. The purpose in this approach is to produce symbols carrying as much information as possible in order to reduce the number of symbols in the output. the compressed file. It only adds pairs that reduce the compressed file in at least K = 5 bytes, and stops adding new pairs when the list of candidate pairs becomes empty.
 vocabulary provided that such an addition reduces the compressed file by at least K = 5 bytes; otherwise it is discarded. The process finishes when the list of candidate pairs becomes empty.

Table 1 shows the results of our study. In addition to obtaining the best compression ratio, the third alternative (to pre-strategy used in the empirical results.

We note that the chosen strategy is very similar to Re-Pair on words ( Wan, 2003 ). The main differences are (1) that we a complex technique while we compress it with a simple bit-oriented Huffman; and (3) that they compress the final se-quence with bit-oriented Huffman while we use ETDC. Those differences are important with respect to random access (as we can uncompress an area with a fast Huffman decompression of the prelude, which is usually a significant part of the out-put) and direct search (as ETDC can be efficiently searched, while bit-oriented Huffman cannot). 4.2. Data structures
PhETDC uses the same structures of PETDC, with some modifications in the hash table hashSymb . In addition, it uses a new vector of integers ( T ids ), which follows the ideas in ( Wan, 2003 ). T rence, the presence of the other id . 2. Two pointers to the previous word/pair and the next word/pair in the text. With the replacement of two consecutive ids ers: one points to the previous valid entry and the other to the next valid slot.

The hash table hashSymb has the same fields as in the case of PETDC, except the e not invalidate possible pairs with the addition of one pair. Furthermore, each entry has a new pointer to one of the occur-rences of its word/pair in T ids . In this way, we have access to the linked list of occurrences of such a word/pair.
Observe that now the pointers to the two members of a pair/phrase ( w unlike PETDC, where the two pointers of a pair pointed to plain words.

Fig. 5 sketches the structures of the compressor in a point of the compression procedure. The structures are shown after the addition of the phrase  X  X  X he room X  X . The hash table hashPairs includes only the fields freq , p text version of the pair/phrase (with grey stripes) is included only for illustration purposes.
The new phrase ( the room ) is inserted in the 5th entry of hashSymb . Then, when the ids 3 and 6 (representing the plain room , the next word/pair pointer skips the next slot (formerly used by the id 6, that is, by the word room ). 4.3. Compression, decompression and search procedures
Compression consists of six main phases: 1. First pass along the text . This phase is the same as in the case of PETDC. Recall that it costs O ( n ). illustration purposes, but this phase can be done in conjunction with the previous one with no additional analytical cost. DC, the estimation of the new size of the compressed text costs O ( m ) time, where m is the maximum frequency value of a word/pair. Once the new pair a b is added to hashSymb , we access the list of occurrences of a . We traverse this list and rences of a is n / v . In conjunction with the previous process, for each occurrence of a b : c word/pair and d j is the next word/pair: (a) the number of occurrences in hashSymb of the pairs c i (c) the list of occurrences of a and b and their frequency in hashSymb are updated.

Finally, the vector topVect is sorted. The overall cost of this phase is O ( p vocabulary. 4. Code Generation Phase . This phase is the same as in PETDC. The cost of this phase is O ( put. This phase takes O ( n ) time. 6. Storing the prelude . It works as in the case of PETDC.

Considering the cost of each phase, the overall cost of the whole process is O ( n + p a pair occurring x times is replaced we shorten Tids length by x elements ( n n x ), the term p it also holds that v 0 n , we obtain that the cost is O( n + p
Decompression starts by loading the prelude into a vocabulary vector. For each codeword C used to obtain the entry i that contains either the word or the pair associated to C form a phrase.

As in the case of PETDC, a word being searched for can be found alone or as a part of one or more pairs. Therefore, we need to gather all the codewords corresponding to pairs that include a given searched pattern. This task is supported by keeping two lists for each word/pair d : contained _ by ( d ) stores pointers to all the pairs which directly contain d . codewords ( d ) stores the codewords that should be sought in case of searching for d .
Searches start by loading the prelude that keeps the vocabulary of words/phrases. For each pair k formed by /h , a pointer
Let us consider that contained _ by ( a ) includes pointers to q , s , and r . In such a case, the codewords C codewords ( a ) are searched for. 5. Empirical results
We used some large text collections from TREC-2 , 4 namely AP Newswire 1988 (AP) and Ziff Data 1989-1990 (ZIFF), as well as one from TREC-4 : Congressional Record 1993 (CR). As a small corpus, we used the Calgary corpus. just encode the word, otherwise both the word and the separator are encoded.
 Our machine is an Intel Core2Duo E6420@2.13 GHz, with 32KB + 32KB L1 Cache, 4MB L2 Cache, and 4GB of DDR2-800
RAM. It runs Ubuntu 7.04 (kernel 2.6.20-15-generic). We compiled with gcc version 4.1.2 and the options -m32 -09. 5.1. Compression ratio, compression time, and decompression time
We compared PETDC and PhETDC against two semistatic word-based byte-oriented compressors, namely Plain Huffman (PH) ( Moura et al., 2000 ) and ETDC, and against a Re-Pair four well-known general purpose compressors: Gnu gzip, 8 a Ziv-Lempel-based compressor; Seward X  X  bzip2, based on the Burrows-Wheeler transform; p7zip 10 that is a LZMA compressor with a dictionary of up to 4 GB; and ppmdj1 , a compressor based on an arithmetic encoder coupled with a k -order modeler. We set ppmdj1 compression options to  X  X 12  X  m256  X  X 1 (using a 12-order modeler, up to 256MB of memory, and rebuilding the model  X  rather than discarding it  X  when memory is exhausted). These options provided the best compression in most files. The other two parameterizable compressors ( gzip and bzip2 ) were run with default options, aiming at providing the best compression/speed trade-off.
In any case, compression times consider that the compressors are fed with the text in plain form. In the same way, decom-pression times include the complete process of recovering the original text. Finally, we recall that our compression ratios cludes the size of the prelude.

Table 2 shows the compression ratios achieved by the compressors. PH is the optimal semistatic zero-order word-based byte-oriented statistical compressor, yet PETDC beats it by around 1 X 3 percentage points and PhETDC by around 4 X 8 per-centage points.

As the word-based statistical methods, PETDC and PhETDC perform worse in small collections due to the need of storing a large prelude. Yet, if the text is large enough, they can even compete with a powerful compressor such as p7zip . presses only 1.3 percentage points worse than bzip2 , whereas in medium size collections, the gap is around 3 X 4 percentage points. Finally, PETDC obtains compression ratios around 6 X 12 percentage points worse than those of p7zip , around 7 X 12 percentage points worse than those of Re-Pair, and it is overcome by ppmdj1 by around 12 X 16 percentage points.
PhETDC is still unable of beating gzip in the smallest collection. Yet, when the texts are large enough, PhETDC compresses with p7zip , which overcomes PhETDC by less than 1 percentage point.

With compression ratios around 3 X 7 and 5 X 13 percentage points better respectively, Re-Pair and ppmdj1 remain as the where searching for a word is simpler, still the need to decompress the prelude and the slowness of processing bit-oriented
Huffman codes hamper random access and direct search capabilities, as explained. Therefore, as we will see, the key reasons that make our compression ratios inferior to the classical word-based Re-Pair are the same for our high decompression and search efficiency.

We performed an additional experiment to check the compression ratios of Re-Pair using an ETDC encoder as a final com-pression step, instead of a bit-oriented Huffman encoder. The values were the following: 35.15% compressing Calgary, 22.54% compressing CR, 22.31% compressing ZIFF, and 17.14% compressing AP. This is still better than PhETDC due to the compres-acters or words) much slower at decompression, particularly making infeasible direct searches on it.
Table 3 shows that PETDC and PhETDC pay the extra cost of managing pairs or phrases during compression. PETDC is tween 4.5 X 5 times slower than ETDC and PH, around 1.5 X 2 times slower than gzip , and it is on a par with bzip2 . Yet, with similar compression ratios, it is around 2.5 X 7 times faster than p7zip . The comparison against ppmdj1 shows that PhETDC but at the expense of poor compression times, in part due to a greedy use of memory. Yet, we have to take into account that we did not use the block-wise version of Re-Pair, which reduces the memory consumption in exchange for poorer compres-sion ratios. We used the normal version of Re-Pair to have a fair comparison with our non-block-wise PETDC and PhETDC. and ppmdj1 , the gaps are considerable, being PETDC around 3 times faster than p7zip , between 3 and 7 times faster than bzip2 , and between 13 and 75 times faster than ppmdj1 .

In the case of PhETDC, the extra cost of the recursive rebuilding of the phrases implies a slowdown. Now, PhETDC is on a par with ETDC and PH, it is around 17 X 22% faster than gzip , except in the smallest collection, and between 1 and 3 times faster than p7zip . bzip2 and ppmdj1 are again the slowest techniques, being PhETDC between 3 and 6 times faster than bzip2 and between 13 and 55 times faster than ppmdj1 .

As expected, Re-Pair is not so time-consuming at decompression. Indeed, in the small collection Re-Pair performed sim-ilarly to PETDC and PhETDC, but in the rest of collections, it was around 2.5 X 4.5 times slower than PETDC and PhETDC. We also compared the decompression performance of Re-Pair using an ETDC encoder instead of a bit-oriented Huffman. Firstly, we compared the decompression of the sequence of integers generated by the Re-Pair process, and found that using
ETDC is twice as fast as using bit-oriented Huffman. Then we compared the overall Re-Pair decompression time. In this case, using ETDC rather than Huffman reduces the overall decompression times by around 3 X 9%.

As a conclusion, if one is interested in compression ratio, PETDC and PhETDC can now compete with bzip2 and p7zip ,at the cost of losing some compression speed compared with the classical PH and ETDC. Yet, as we will see in the next section, PETDC and PhETDC are efficiently searchable, which is a remarkable feature.
 5.2. Search speed We performed single and multi-pattern searches over corpus AP. To choose our search patterns, we followed the model in
Moura et al. (2000) where each vocabulary word is sought with uniform probability, and extracted patterns chosen at ran-dom over the vocabulary of corpus AP. We classified the generated patterns depending on their frequency and length as shown below.

We carried out two different experiments. The first one compares searches over text compressed with PETDC and PhETDC versus searching the uncompressed text. This shows how much search time is gained or lost due to having the text in com-pressed form. The second kind of comparison is against ETDC. This shows the loss in search speed with respect to ETDC, due to the management of pairs and phrases.

To search text compressed with ETDC, we use our own implementations of Horspool and Set-Horspool algorithms spool even for single-pattern searches.
 On the other hand, three different algorithms were tested to search the uncompressed text: our own implementations of the pattern occurrences from the original text (AP corpus). Then we ran Agrep -s over T 0 and we scaled the resulting times
To choose the search patterns, we considered the vocabulary of corpus AP, we extracted sets of patterns with K words of of running the searchers over 10 sets for each combination of L and K .

PETDC single-pattern search (although it is actually a multi-pattern search) is around 2.7 times slower than that of ETDC the plain text searchers.

PhETDC is around 1.5 X 2.25 times slower than PETDC. With patterns of length 5, it is on a par with Agrep single-pattern faster than plain text searchers. However, when the patterns are long, it is 16 X 45% slower.
As a summary, PETDC and PhETDC perform better in multi-pattern searches than in single-pattern searches, whereas plain text searchers perform better when the patterns are long. Two factors determine this situation. The first one is that longer than compressed patterns, since compressed patterns are only between 1 and 3 characters long (the usual size of
However, in any multi-pattern Boyer X  X oore-type search, when both longer and shorter patterns are sought, the most effi-
This harms the advantage of the plain text searchers, particularly when the plain patterns are not very long.
An interesting question is how PETDC and PhETDC behave when the frequency of the searched words increases, since it is expected that the more frequent a word is, the more the codewords (from its pairs) that will represent such a word, and in most multi-pattern searches, faster than Horspool over plain text. 6. Conclusions and future work We have introduced two new semistatic modelers and tested them in two compressors called Pair-Based End-Tagged
Dense Code (PETDC) and Phrase-Based End-Tagged Dense Code (PhETDC) . They take advantage of using pairs of words or phrases (exploiting the co-occurrence of words) to improve the compression obtained by word-based semistatic techniques such as PH or ETDC.

In essence, the goal is to use offline techniques to improve the ETDC compression ratio, which is its weakest point if we compare ETDC with PPM-based compressors, offline compressors, p7zip , and bzip2 . At the cost of a slowdown in compression erful PPM-based compressor such as ppmdj1 shows that the worse compression of PETDC and PhETDC is compensated by their faster compression and decompression processes. We showed also that PETDC and PhETDC cannot compete with
Re-Pair in compression ratio, but they still obtain competitive values with remarkable better compression time, which is the main disadvantage of Re-Pair, and an efficiently searchable compressed text.

As explained, the improvement in compression ratio has a price. PETDC is around 3 times slower than ETDC in compres-par in decompression speed.
 Fig. 8 shows trade-offs between compression ratio and both compression and decompression times. We used the corpus
CR, where Re-Pair can run without swapping in our machine. In this way the figure reflects the actual differences better. As expected, the improvement in compression ratio implies also a slowdown in the search procedure with respect to
ETDC. PETDC is between 2.1 X 2.6 times slower than plain text Horspool single-pattern searches. Yet, in multi-pattern plain text searchers in multi-pattern searches for average length words, but it is beaten in searches for long patterns. Fig. 9 displays trade-offs between compression ratio and search time.

As a conclusion, there is an interesting trade-off between speed and compression ratio. Those who are more interested in speed can use the classical ETDC, which compresses to around 31%. With a reasonable slowdown, PETDC obtains 27 X 28% compression ratio. Finally, users most interested in compression ratio can opt for PhETDC, which achieves up to 23%, at the cost of a more significant loss of speed.

As semistatic compressors, PETDC and PhETDC have to keep in memory the vocabulary (that is, the model). In the case of the word-based compressors, this consumes a considerable amount of memory, which in the case of PETDC and PhETDC is even higher, since they do not only deal with words, but also with pairs of words or phrases. In addition, PETDC and PhETDC use structures that are more complex and PhETDC requires recursive processes. Table 5 shows the peak memory consump-tion of PETDC and PhETDC compared with ETDC and Re-Pair, which are compressors that also have to store a large model.
From the point of view of a practitioner, on the one hand, we remark that the implementation of PETDC is quite similar to 5% larger than that of ETDC. On the other hand, it can be seen that the implementation of PhETDC requires much more effort.
We show that the executable file of the PhETDC compressor is around 36% and 29% larger than those of ETDC and PETDC respectively.
 As a future work, to reduce the memory usage of our compressors, we intend to develop block-wise versions of PETDC and
PhETDC following the ideas in Wan and Moffat (2007) . Block-wise compressors compute the model of a chunk of the text and use it to compress such a chunk. For the next chunk of text, the model is computed again. By using small chunks, models are shorter and then the memory consumption decreases. As shown in Wan and Moffat (2007) , combining block-wise compres-sion with a technique called Re-Merge, the overhead achieved by repeating portions of the vocabulary in the prelude of each chunk can be reduced drastically, so the compression ratio is not damaged.

Another solution to save memory is to compute the pairs/phrases to be added in a small portion of the text and use the resulting vocabulary to compress the complete text.

Finally, the compression obtained by our new techniques can still be improved even further on several ways. For example, points in compression ratio (and a marginal loss of speed). Also, studying new alternatives to compress the hierarchy of phrases obtained by PhETDC would be of interest.
 Acknowledgments
This work was founded in part by Ministerio de Educaci X n y Ciencia [TIN2009-14560-C03-02] and [TIN2010-21246-C02-01], Ministerio de Ciencia e Innovaci X n [CDTI CEN-20091048], and Xunta de Galicia Grant 2010/17 (for the Spanish group), and Fondecyt Grant 1-080019 (third author).

We want also to thank  X ngel Y X  X ez Miragaya and Yolanda Varela Sobrino for their help in the implementation of PETDC and PhETDC.
 References
