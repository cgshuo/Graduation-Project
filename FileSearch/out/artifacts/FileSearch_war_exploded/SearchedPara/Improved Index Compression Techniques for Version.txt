 Current Information Retrieval systems use inverted index structures for efficient query processing. Due to the ex-tremely large size of many data sets, these index structures are usually kept in compressed form, and many techniques for optimizing compressed size and query processing speed have been proposed. In this paper, we focus on versioned document collections, that is, collections where each docu-ment is modified over time, resulting in multiple versions of the document. Consecutive versions of the same document are often similar, and several researchers have explored ideas for exploiting this similarity to decrease index size.
We propose new index compression techniques for ver-sioned document collections that achieve reductions in in-dex size over previous methods. In particular, we first pro-pose several bitwise compression techniques that achieve a compact index structure but that are too slow for most ap-plications. Based on the lessons learned, we then propose additional techniques that come close to the sizes of the bit-wise technique while also improving on the speed of the best previous methods.
 H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval.
 Algorithms, Performance.
 Inverted index, index compression, versioned documents.
Over the last few years, web search engines and other in-formation retrieval tools have become the primary means of finding relevant informatio n for millions of users. The largest search engines now have to answer tens of thousands of queries per second over billions of web pages. To do so, current search engines rely on a data structure called an in-verted index , which allows efficient retrieval of all documents containing a particular term or set of terms. A lot of research over the last three decades has focused on how to efficiently build, compress, and access inverted index structures, result-ing in the highly optimized implementations deployed in the current generation of search engines.

In this paper, we focus on an important special case of the text indexing problem that has received much less attention, the case of a versioned document collection, i.e., a collec-tion where each document is represented by multiple versions. Important examples of such collections are web archives con-taining many past versions of web pages, the page history in Wikipedia, or revision control systems storing all past ver-sions of program source code. Our goal is to build a concise and efficient inverted index structure for versioned collec-tions, such that queries can be evaluated over all versions of all documents. A trivial way to build such an index would treat each version as a separate document. However, this is wasteful, since for a collection with 50 versions per document we obtain an index with 50 times the size of a single-version index, even if many versions are almost identical.
Thus, the main challenge in indexing versioned document collections is how to exploit the significant similarity that often exists between versions of the same document, in or-der to avoid a blowup in index size. Ideally, the resulting index should have a size that is proportional to the amount of change between the versions, rather than the total collec-tion size. This problem has received some attention in the research community [2, 8, 4, 13, 31, 11], but we believe there is still room for improvements.

We are motivated by two important search applications, search in the Internet Archive and in Wikipedia, and our experiments use data from these collections. The Internet Archive ( www.archive.org ) is a non-profit organization that has collected more than 150 billion web pages since 1996 in an attempt to archive the World Wide Web. While current commercial search engines provide only access to the most recent snapshot of the evolving web, and simply replace old versions of pages with the most recently crawled version, the Internet Archive aims to also provide access to all previous versions. However, indexing all these versions is very expen-sive (and not currently done by the Archive), especially for a non-profit without the deep pockets of the major search en-gine companies. Our second application is Wikipedia, which keeps a complete history of all past versions of every arti-cle, and where it would be desirable to be able to search across the different versions. Other applications are search in revision control and document management systems, and indexing support for versioning file systems.
Our contributions in this paper are improved techniques for organizing and compressing inverted index structures for ver-sioned document collections. In particular, we analyze typ-ical properties of versioned document collections that make them highly compressible, and provide simple combinatorial upper and lower bounds for index size. Our main result is a set of index organization and compression schemes, building on previous work in [2, 13, 11, 1], that achieve improvements in both index size and query processing speed over all previ-ous approaches.

The remainder of this paper is organized as follows. Next, we provide some technical background and discuss previous work. Section 3 discusses some typical properties of versioned document collections that help compression, and Section 4 establishes combinatorial upper and lower bounds. Section 5 provides improved practical schemes and evaluates them on data sets from Wikipedia and the Internet Archive. Finally, Section 6 provides some concluding remarks.
In this section, we first give some background on versioned document collections, inverted indexes, and index compres-sion techniques, and then discuss related work.
Most current search engines use an inverted index struc-ture to support efficient keyword queries [32]. An inverted index for a collection of documents is a structure that stores, for each term (word) occurring in the collection, information about all locations where it occurs. For each term t , the index contains an inverted list I t consisting of a number of index postings . Each posting in I t contains information about the occurrences of t in one particular document d , usually the ID of the document (the docID), the number of occurrences of t in d (the frequency), and possibly other information such as the locations within the document. In this paper, we assume that postings have docIDs (or version IDs) and frequencies, but we do not consider the case of a positional index where within-document positions are stored in the postings. Index Compression: The postings in each list are usually sorted by docID and then compressed using any of a number of techniques from the literature [32]. Most techniques first replace each docID (except the first in the list) by the differ-ence between it and the preceding docID, called a d-gap ,and then encode the d-gaps using some suitable integer compres-sion algorithm. Using d-gaps instead of docIDs decreases the average value that needs to be compressed, resulting in bet-ter compression. These values have to be summed up again during decompression, but this can be done quite efficiently. The d-gaps and frequencies are often stored separately, and thus we compress sequences of d-gaps and frequencies.
A large number of inverted index compression techniques have been proposed; see [32] for an overview and [30] for a recent experimental evaluation of state-of-the-art methods. In our work here, we utilize two techniques that have been previously applied to versioned document collections [11], In-terpolative Coding (IPC) and PForDelta (PFD).

Interpolative Coding is a technique introduced in [17] that was designed for the case of clustered or bursty term oc-currences, such as those encountered in longer linear texts (e.g., books). IPC achieves a very small index size in many scenarios, but it is not very fast in terms of decompression speed. PForDelta is a family of compression schemes first introduced in [12, 33], and further optimized in [30, 29], that allows extremely fast decompression (beyond a billion inte-gers per second per core) while also achieving a fairly small compressed size. Here, we use a version of PFD called OPT-PFD described in [29] that achieves very good compression on clustered data, in some cases coming close to IPC in size.
Inverted lists are often organized into blocks that can be independently accessed, thus enabling forwards skips during query processing. We use such blocked indexes throughout this paper, with a block size of 128 postings.
 Versioned Collections and Two-Level Indexes: We de-fine a versioned document collection D as a set of documents d (In some cases, it is convenient to define d 0 i as the empty document.) We assume a linear history, and do not try to model the case of branches (forks) in the revision history, though we believe our ideas could be adapted to this case.
We define the first-level index of a versioned document collection D as an inverted index where the inverted list for aterm t contains a posting for document d i if at least one version d j i of d i contains t .Foraterm t that occurs in at least one version of d i , we define the bit vector of t and d i array of m i bits such that the j -th bit is set to 1 iff version d i contains t . Similarly, we define a frequency vector that contains in its j -th entry the frequency of t in d j i .(Whenthe bit vector is already known, it may be convenient to think about the frequency vector as having entries only for those versions that contain t.)
A two-level index structure for a versioned document col-lection consists of the first-level index, and a second level storing all the bit and frequency vectors. All our construc-tions in this paper use the two-level approach, which was first proposed in [1] in the context of non-versioned collec-tions, and then adapted to versioned collections in [11]. This approach allows efficient query processing by first running a query on the small first-level index, and then fetching and decompressing any necessary bit and frequency vectors.
The first-level index contains only docIDs, and no fre-quency values. It is compressed using standard index com-pression techniques such as IPC or PFD, since there is no ob-vious structure in the data that distinguishes this case from that of a non-versioned collection. The main challenge is the modeling and compression of the bit and frequency vectors in the second level, and as we will show, these vectors have a wealth of interesting structure that can be exploited.
We now discuss previous related research, including work on indexing and querying versioned document collections, on indexing collections with similar documents, and on storage and transmission of similar and versioned documents. Positional Indexing of Versioned Collections: Previ-ous work on indexing versioned text collections falls into two classes, work on positional indexes [31] and work on non-positional indexes [2, 8, 4, 13, 11]. For positional indexes, [31] uses content-dependent string partitioning techniques [22, 14, 27] to split each document into fragments, and then removes duplicate fragments from the collection; this results in signif-icant reductions in index size and also supports very efficient updates as new versions are added to the collection.
We note here a basic difference between positional and non-positional indexes. In the former case, similarity is based on common substrings; this way a change in position informa-tion due to an insertion or deletion before the start of the substring can be efficiently handled by changing the offset of the substring in the document. In the latter case, we have a set-or bag-oriented model where similarity is based on common subsets of terms; for non-positional indexes this performs much better than a substring-oriented approach, and thus techniques for positional indexes are of limited rel-evance. However, improved techniques for positional indexes are an interesting open problem.
 Non-Positional Indexing of Versioned Collections: The approaches in [2, 8, 4, 13, 11] consider non-positional indexes, our focus in this paper. The earliest work we are aware of is that in [2], which proposes to index the differences between the versions rather than the versions themselves. In particu-lar, [2] indexes the last version, and then adds delta postings to store changes from the last to previous versions. We will refer to this basic approach as DIFF (for difference).
In more recent work in [8], the set of terms of a document is organized into a tree structure, where each node has some private and some shared terms, and each node inherits its ancestors X  shared terms. The versions are then expressed in terms of these sets of terms, and the sets of terms are indexed. Followup work in [13] identifies subsets of terms that are contained in a range of consecutive versions. Such subsets are then treated as virtual documents and indexed, and the main challenge is to minimize the number and size of virtual documents. This can be done by reordering the versions in an optimal manner, resulting in an instance of the NP-Complete Multiple Sequence Alignment (MSA) problem. However, in most cases a simple ordering of versions by time appears to achieve very good results, and this is the approach we use later in our constructions. We nonetheless refer to this general approach as MSA.

Another approach in [4] is based on the idea of coalesc-ing index postings corresponding to consecutive versions if they contribute almost the same score to the overall ranking function. This is a lossy compression technique that uses an index with precomputed quantized scores, and thus it is not easily comparable to other approaches.

Recent work in [11] compares the DIFF and MSA ap-proaches to two additional techniques. One technique, called Sorted, simply assigns docIDs to the versions of a document in the natural order, and then relies on standard index com-pression techniques to exploit the resulting clustered nature of the index data. This can be seen as a non-lossy variant of the approach in [4], and it is shown to do much better than the trivial baseline, but not as well as either DIFF or MSA. Another method, called HUFF, uses the two-level ap-proach in [1], and compresses the bit and frequency vectors in the second level using a simple hierarchical Huffman cod-ing scheme adapted from [10]. This method outperforms all other methods in terms of size, but results in slightly slower query processing due to the use of Huffman coding.
Thus, the work in [11] provides a recent comparison of ex-isting methods that we use as a baseline for our results. We also use the same Wikipedia and Internet Archive data sets as [11], allowing a direct comparison of the numbers. Over-all, we will show significant improvement in both index size and access speed over previous methods.
 Query Processing in Versioned Collections: Another related problem is how to perform query processing in ver-sioned document collections, and in particular what types of operations should be supported. This is a non-trivial prob-lem that has received only limited study. The simplest type of query treats all versions as separate documents, and thus would return the most relevant versions over the entire his-tory of the collection (probably with additional filtering to remove multiple versions of the same document). It is likely that in many scenarios, users would also like to be able to restrict a query to a limited time interval (e.g., the most rel-evant pages during 1998); this could be achieved either by post-filtering, or through specialized index structures that support more efficient range queries over time. Recent work in [16] proposes durable search , where the goal is to return documents consistently ranked high over a range of time, and describes a number of algorithms for this problem.
In general, in this paper we are trying to sidestep the ques-tion of what the right set of query operations is. We evaluate our index structures by running queries over all versions, with no temporal restrictions, but we believe that our techniques are fast enough to allow efficient filtering and aggregation on top in order to implement other operations. Of course, specialized index organizations for range search and durable search should outperform more general index layouts, and how to optimize versioned index structures for these cases is an interesting problem for future work.
 Succinct Indexing of Similar Documents: There are several techniques that exploit similarities between distinct documents (i.e., not different versions of the same document) for better index compression. This problem is closely related to that of indexing versioned collections, but also differs from it in important ways: First, similarities between distinct doc-uments are typically smaller, leading to more limited gains, and second, an important part of the problem is to identify which documents are similar, while in versioned collections this information is more or less implied.

The most common approach to indexing similar documents is based on reordering the documents in the inverted lists, by assigning consecutive or close-by docIDs to documents with high similarity [7, 24, 26, 6, 25, 29]. This results in a more skewed d-gap distribution in the inverted lists, with many more small d-gaps and a few larger ones, leading to a reduction in index size under common index compression schemes. The Sorted method in [11] is an application of this idea to versioned collections.

Another approach was studied in [1], where similar docu-ments are clustered into disjoint groups and then indexed by a two-level index structure. While in [1] this does not reduce index size, it leads to an increase in query processing speed. The HUFF approach in [11] and our techniques here adapt the two-level approach to versioned collections, where it re-sults in improvements i n both size and speed.
 Storing Redundant Document Collections: There has been a significant amount of research on the problem of re-ducing space or network transmission costs in storage systems by exploiting redundancy in the data. This includes the Low Bandwidth File System [18], various storage and backup sys-tems [9, 5, 15, 21, 27], and remote file synchronization tech-niques [28, 23, 19]. This work differs from our work in that the goal is to reduce the size of the collection rather than the size of the index. However, some of the techniques are also relevant to our work, and we will adapt ideas from [19] based on Communication Complexity for our analysis in Section 4.
In this section, we describe the data sets that we use, and then perform a preliminary analysis of the data in order to detect patterns that can be expl oited for better compression.
Data sets: In our experiments, we use versioned col-lections obtained from Wikipedia (Wiki) and the Internet Archive (Ireland). The Wikipedia data set consists of 0 . 24 million distinct documents with 35 versions per document on average, resulting in slightly more than 8 million pages. This is about 10% of the English version of Wikipedia. The Inter-net Archive data set consists of 1 . 06 million documents from the Irish web domain, collected between 1996 and 2006, with 15 versions per document on average. The data sets were used in [11], thus allowing a direct comparison of results.
Data analysis: Next, we perform a simple analysis of the data in order to identify four properties that lead to good data compression. For each property, we also point out which previous methods take advantage of it.

Property 1: Most changes are small. Figure 1 shows the distribution of the sizes of the changes between versions. Here, the size of the change is the number of unique terms that are either added or removed between versions. As we see, at least half of all versions in Wiki and Ireland differ by less than 5 terms that are added or removed. (Note that we are looking at a set-based model  X  a change size of 0 does not mean that two consecutive versions are identical, but only that they are based on the same set of terms. All iden-tical versions were removed from the data.) This is of course not surprising, and the main reason why a versioned index structure should be much smaller than a standard structure that treats each version as a separate document. All existing techniques for versioned collections exploit this property. Figure 1: Cumulative distribution of change size for Wiki
Property 2: Terms changes are bursty. Figure 2 shows the distribution of the change distance, defined as the number of versions between two changes in a term bit vec-tor. From the figure, we see that the longer a term stays unchanged, the more unlikely it is to change in the next step. Conversely, terms that have just been added or re-moved are more likely to disappear or reappear again in the next few steps. (To limit the impact or the truncated his-tory, we removed terms that stay until the last version.) We note that all existing compression techniques also exploit this property; in the case of DIFF, MSA, and Sorted this happens through use of compression methods such as IPC and PFD that exploit data clustering, while in HUFF the Huffman ta-ble catches on to this pattern.

Property 3: Change size is bursty. Figure 3 shows the cumulative size of the amount of change when we sort versions from largest to smallest change. This is the flip side of Property 1: while most changes are small, less than 10% of all versions are responsible for more than 50% (Wiki) and 70% (Ireland) of the total change. So, large changes are rare, but account for much of the total change. (However, the distribution of change does not seem to follow power law.)
We note that none of the existing techniques really exploits this property, but that it is potentially very useful. If we have a term that exists in the current version, then knowing, say, that the next 3 versions have very little change, but that the fourth version has a large amount of change, would lead us to guess that the term is most likely to change in this fourth version than in the others. In fact, in the next section, we show that even a simple combinatorial technique that uses only this property, and that knows the change size for each version, outperforms all previous methods in terms of index size. Most of the improvements in this paper are based at least indirectly on exploiting this property.
 Figure 2: Distribution of change distance for Wiki (left) Figure 3: Cumulative distribution of change size for Wiki
Property 4: Terms are dependent. One more prop-erty of the collections is that terms often come and leave together. In particular, we found that a pair of terms in Wiki that exists in a given version has a 48 . 8% change of being deleted at the same time if it was also added at the same time, while the chance was only 30 . 5% if the words were added at different times. For Ireland, the numbers were 55% and 34 . 6%. This is of course not really surprising and can be explained in several ways. Terms that are added in the same version are often next to or close to each other, so that a future change often also impacts both terms. Or, this could be mainly a result of Property 3 since many pairs of terms will be added in a large update and then removed to-gether in the next large update. Or it could be a consequence of Property 2. We note that this property is only exploited in the MSA approach [13], and is an important reason for its good performance compared to DIFF.
In this section, we discuss the index compression problem from a combinatorial perspective, and derive worst-case lower bounds and upper bounds for the problem. The techniques we use are fairly simple, and motivated by previous work in Communication Complexity [19], but we are not aware of any previous formal analysis of versioned index compression. We note that the algorithms in this section are not directly useful in practice, as their bitwise approach to compression is too slow for a state-of-the-art query processor. However, we believe the results are of interest on their own, and they also inform the practical approaches in subsequent sections.
In the following, we assume the two-level approach to in-dexing as described earlier. Thus, we have a small first-level index that specifies for each term which documents contain the term in at least one version, and a second-level index containing compressed bit vectors specifying which versions contain the term. (We focus on the case of docIDs only, but the approach can be extended to indexes with frequency val-ues.) The first-level index is compressed using standard tech-niques, as the problem of compressing this level is similar to the standard non-versioned index compression problem. Our focus here is on the second level, and the modeling and com-pression of the bit vectors, which encapsulates the essence of the versioned index compression problem.

We start out with an analysis of a simple model based on set difference in Subsection 4.1, where we establish upper and lower bounds in this model. Subsection 4.2 extends the results to models that use term appearances, term deletions, and term reappearance, and provides some experimental re-sults on real data. In Subsection 4.3, we provide another upper bound that uses additional features to obtain better results, and finally we discuss conclusions from this section.
Let D = { d 0 ,d 1 ,...,d n  X  1 } be a collection of versioned documents. Consider one document d i  X  D consisting of m i versions d the empty document. The size of d i , s ( d i ), is the number of terms that appear in at least one version of d .Foreach d define ch ( d j i ), the change in d j i , as the size of the symmetric difference between d j i and d j  X  1 i , that is, the number of terms added or removed between versions d j  X  1 i and d j i .
As discussed before, we expect that the index size of a versioned document collection should depend on the amount of change between versions. In the following, we show that this can be formalized, by deriving suitable lower and upper bounds for index size based on change. In particular, we can show the following simple information-theoretic lower bound:
Theorem: For any numbers c 1 ,c 2 ,...,c m and s ,andany index compression scheme, there exists a document d with s ( d )= s and m versions such that ch ( d j )= c j for all j ,that requires an index size of at least .
 Proof Sketch: Given a set of s terms, there are at least distinct documents d that satisfy the stated constraints on change between versions. An index needs to be able to dis-tinguish between all such documents, and thus there must exist a document with index size at least log 2 ( X )bits. End.
We note that this is not just a bound on index size, but also a bound on the amount of bits needed to store the versions themselves (modeled as sets rather than sequences of terms). More precisely, it is a lower bound on the second level of the index, that it, it holds even if we already know the informa-tion in the first level of the index. The theorem is stated for one document, but generalizes to collections by summing up over all documents, with each document having its own con-straints on the change between versions. The result and its proof are inspired by previous work on the communication complexity of exchanging similar documents in [19], which we apply to a set-based model with multiple versions.
Next, we will derive an upper bound on second-level index size based on the amount of change between versions, and relate it to the lower bound. The second level consists of bit vectors that we need to represent in a concise manner. Consider a bit vector x of length m for a term t that occurs in at least one version of an m -version document d .Itiswell known that compression is related to prediction, and one way to compress such a bit vector is to derive a probability for the next bit to be 0 or 1, given the previous bits and the bounds c on change between versions. These probabilities can then be used to drive a binary coder matching the entropy bound (e.g., binary arithmetic coding) to code the bit vector.
The main insight here is that the values c j are very useful in predicting the next bit of the bit vector. Assume that version d j  X  1 of d contained a term t , and we would like to guess how likely it is that t will also be present in version d This should depend on ch ( d j ), the amount of change between the two versions, and if we assume that terms affected by change (terms that disappear or appear in the next version) are selected at random from all eligible terms, than t has a chance of ch ( d j ) /s ( d ) of disappearing in version d will code the j -th bit of the bit vector by using the probability ch ( d j ) /s ( d ) in the binary coder. Overall, we use the following probabilities to encode a bit vector x = x 1 x 2 ...x m : We now analyze this approach. For simplicity, assume that ch ( d j )  X  s ( d ) / 2 for all j . (Otherwise, we can simply use the inverse of d j .) We also assume that bit vectors are com-pressed optimally and in a continuous manner  X  a practical arithmetic coder would have some additional waste in bits, and we would also need to periodically align the data with bit or byte boundaries in order to allow retrieval of particular bit vectors, but this is harder to model. Then we can show:
Theorem: The number of bits used by the above algo-rithm for a document d is
The bound follows from the fact that when we look at the j -th bit of all the s ( d ) bit vectors for d ,thereare ch ( d wherewehavetoencodean x j = x j  X  1 ,and s ( d )  X  ch ( d entropy then gives the expression in the first line, which can be easily bounded by the second line.
 Thus, at least under the idealized assumptions about coding and data alignment, the algorithm achieves a size within a factor of 2 of our lower bound. We note that the factor of 2 can be improved for values of ch ( d j ) bounded away from s ( d ) / 2. (However, we were unable to come up with a useful form for such an improved bound.) As shown in our later experiments, the actual gap on real data is much smaller.
In general, there are two challenges to designing even tighter upper bounds. First, our lower bound is based on the size of a non-trivial combinatorial space (the set of documents satisfying the constraints) that is not easy to map to a dense set of bit representations. Second, an upper bound for in-dex structures needs to be able to separately decode the bit vectors for a particular term, and simply labeling the space of possible documents in an optimized way would not assure this. Our approach based on probabilities clearly allows de-compression on a per-term basis, but at some inefficiency in representing the document space.
The results in the previous section are based on only the information about the total amount of change between ver-sions. We now improve these bounds using additional con-straints, and state upper and lower bounds for each case.
In the first case, assume that rather than just knowing the change ch ( d j ) between d j and d j  X  1 , we know the number of newly added terms ins ( d j ) (insertions) and the number of removed terms del ( d j ) (deletions). Given this, we can further restrict the combinatorial space, resulting in the following worst-case lower bound on the number of bits: where s ( d j ) is the size (number of distinct terms) of ver-sion d j . (Note also that del ( d 1 ) = 0 by definition.) Fur-thermore, if we divide insertions into first occurrences fo ( d (where a term occurs for the first time) and reappearances re ( d j ) (where a term reappears after having previously been deleted), we obtain the following worst-case lower bound on the number of bits: We can also derive upper bounds for these two cases, by as-suming that in each step, terms affected by a change (inser-tion, deletion, first occurrence, or reoccurrence) are selected at random from all eligible terms. This gives us the following probabilities to drive a binary coder: and for the second case: Next, we apply the three different types of upper and lower bounds to the Wiki and Ireland data sets described in Section 3. The results are shown in Table 1.
 Table 1: CompressedsizeinMBforWikiandIreland,
To implement the upper bound for Case 1, we need for each version d j the value of ch ( d j ); this can be stored at a cost of about 1 byte per version. For Case 2, we need ins ( d del ( d j ), and s ( d j ). Note that s ( d j ) is probably stored anyway as it is needed, e.g., for basic ranking under BM25 or cosine, and that del ( d j )and ins ( d j ) can be computed from ch ( d and s ( d j ); in this case the extra space is the same as for Case 1. Finally, for Case 3, we need fo ( d j )and re ( d j )instead of just ins ( d j ), resulting in an additional about 1 byte per version in the collection.

Looking at the results, we see that the upper and lower bounds are quite close, and that adding additional constraints results in visible improvements, particularly when moving from Case 1 to Case 2. The most surprising result for us, however, was that even the simple upper bound in Case 1 is much better than the best bound from previous work, the Huffman-based bit vector encoding in [11], which achieves 140 MB on Wiki and 304 MB on Ireland.
Results in the previous subsection showed that a simple coding method that focuses on exploiting the amount of change in each version beats all previous published methods in terms of index size. The previous methods exploit various other patterns in the data set, but none of them uses the amount of change in each version in a principled way. This raises the question of whether we can improve the upper bound by using the amount of change as well as other patterns.
We again take a bitwise approach where we model the probability of the next bit in the bit vector being 0 or 1. However, in addition to the amount of change, we want to use various other features that might lead to a better prediction. In particular, from Section 3 we know that the likelihood of a term disappearing in the next version depends on how long the term has been in the document; recently added terms are more likely to disappear again. It is difficult to derive a good explicit formula for the probabilities without a better model for generating versioned documents (i.e., better than our ear-lier model where affected terms are selected at random), and thus we take a simple statistical approach where we analyze the complete data to derive a table of precomputed probabil-ities. We use the following features to predict the next bit:
We implement this model using a k -D tree [3], which is a data structure for organizing k -dimensional points. In partic-ular, our goal is to partition our 8-dimensional feature space into regions such that in each region the probability of the next bit being 1 is roughly uniform. We store this probability with the region, so we use it to arithmetic-code the next bit.
To construct the k -D tree, we go over all bit vectors, and obtain the set of corresponding 8-dimensional points, with each point having a label of 0 or 1 corresponding to the next bit. We recursively partition the space by looking at possible cuts along each axis, for the cut that achieves the largest decrease in overall entropy. (This is similar to the MHIST algorithm for constructing multi-dimensional histograms in [20].) We recursively apply cuts until the entropy is below a selected threshold. A lower threshold will result in a better prediction, but also in a larger tree stored as meta data.
We see from Figure 4 that approaches achieve moderate ad-ditional improvements over the formula-based approach from the previous subsection. The best results we get are 77 . 5MB for Wiki and 209 MB for Ireland for the second level of total index. There is a trade-off between tree size and prediction quality. Note that using a k -D tree may appear cumbersome and impractical, but our goal here was to see how far we can reduce the index size by using many available features, and to use this as guidance for more practical methods in subsequent sections.
We now briefly discuss the results from this section. Our main contribution was a set of upper and lower bounds for versioned indexing inspired by results in communication com-plexity. The most useful take-away from this was that a com-binatorial approach that just exploits information about the amount of change in each version can beat all previous meth-ods, which fail to exploit this information. Moreover, adding additional features can result in modest further gains.
However, the upper bounds in this section are not directly applicable in practice because of their bitwise nature. That is, for every bit in the bit vector, we need to call an adap-tive arithmetic coder using the derived probabilities. While such coders can decode millions of bits per second on current CPUs, they are still by one to two orders of magnitude slower than index compression methods such as PFD or even the slower IPC. Thus, a query processor based on these methods would be too slow for most practical applications. Our chal-lenge in the next sections is to design methods that exploit information about the amount of change without a bitwise approach that looks at one version at a time. We will show that this can be achieved through appropriate reorderings of the bit vectors, resulting in methods with both smaller index size and faster index access than previous results.
The approaches from the previous section achieve very good compression but are not practical due to their use of a bitwise arithmetic coder that would make decompression and thus query processing very slow. In this section, we propose alternative methods based on modifying the DIFF and MSA Figure 4: Compressed second-level index size for Wiki approaches that are also able to exploit knowledge about the amount of change between different versions. The resulting index sizes are close to the best from the previous section, with query processing speeds that are faster than that of all previous methods in the literature.
 The new algorithms are based on three modifications to DIFF and MSA described in the following. In particular, we convert DIFF and MSA into two-level methods [1], apply a reordering operation on the versions (i.e. the bits of the bit vectors), and propose a hybrid between DIFF and MSA.
In a nutshell, both DIFF and MSA work by first creating artificial virtual documents from versions, and then index-ing these virtual documents instead. In DIFF, we create one virtual document for each version, consisting of the symmet-ric difference between this and the previous version (i.e., the terms that were either added or removed). In MSA, each virtual document represents a range of versions; thus, one virtual document may correspond to versions 3 to 8 and con-tain all terms inserted in version 3 and deleted in version 9. This means that DIFF results in more postings than MSA, since it needs one posting to signal the arrival of a term, and one posting to signal departure. MSA uses only one posting for this, but creates more virtual documents, thus resulting in a larger docID space and larger d-gaps.
 DIFF and MSA can be adapted to two levels as follows. The first-level index is always the same for all methods, and stores which documents have at least one version contain-ing a particular term. For the second level, DIFF now looks at the relevant bit vector corresponding to the virtual docu-ments rather than versions, and compresses this bit vector by storing the gaps between 1 values in the bit vector. This is in fact very similar to standard DIFF, except that gaps are now only created within a single bit vector and not across documents. We then compress all these gap values using ei-ther IPC or PFD with a block size of 128 values. In addition, we also need to store an indicator bit for every gap to signal when a gap is the last gap in the current bit vector.
For MSA, the construction is similar. In this case, we have a bit vector with one bit for each virtual document, and store the gaps between 1 values in this bit vector. As with DIFF, we compress these gaps using IPC or PFD, and also add an extra bit for each gap to indicate when a gap is the last one. As we show in our experiments further below, this change of DIFF and MSA into two-level methods results in reductions in index size and query processing costs. In the following, we refer to these methods as 2-DIFF and 2-MSA.
The two-level variants of DIFF and MSA still do not ex-ploit the information about the amount of change between versions that turned out to be so valuable in the previous section. While it is conceptually simple to use this infor-mation in bitwise methods, where we only need to take the amount of change of the next version into account, this is much harder for methods such as DIFF and MSA that store longer gaps. Intuitively, to better model these longer gaps we would have to look at the change information not just of the next version, but of many future versions.

We now describe a simple reordering trick that achieves a similar result in an indirect way. Consider again the virtual documents created by DIFF, and suppose that after creating thesevirtualdocumentswesortthembysizeindecreasing order. This means that the largest virtual document now corresponds to the first bit of each bit vector, and that, when we look at all bit vectors for a document, the first bit has the largest number of 1 values, the second bit the second largest, and so on. In other words, we have transformed the bit vectors into  X  X ront heavy X  bit vectors where the initial bits are more likely to be set to 1. This induces a clustering effect in the resulting gaps that is then automatically exploited by IPC, PFD, or possibly other methods. We refer to this method as 2R-DIFF (where R stands for reordering).
We note here that it is not clear that sorting by virtual document size is the best rule for reordering. In fact, it can be argued that it is desirable to place as close to each other those virtual documents tha t have a lot of terms in common. This can be modeled by building a graph on the virtual doc-uments, with edges weighted by the size of the intersection, and then running a TSP-like computation on this graph. We experimented with this approach, but were unable to obtain any benefits over simple sorting, and thus we use sorting in all subsequent results.
 The same reordering trick can also be applied to two-level MSA, by sorting its virtual documents by size; we call the resulting method 2R-MSA. We note that this reordering ap-proach for MSA is very different from the reordering con-sidered in the original MSA paper [13], in that we reorder virtual documents while [13] reorders the original versions.
Another algorithm can be obtained by again reordering the bit vectors for DIFF, and then using the hierarchical Huffman-based coding scheme from the HUFF method in [11] directly on the bit vectors, rather than coding gaps using IPC or PFD. We call this method 2R-HUFF. Finally, these reorderings have to be inverted during query processing.
Our third optimization involves a hybrid between DIFF and MSA. Recall that MSA decreases the number of postings compared to DIFF, at a blow-up in the docID space. MSA performs best when a few virtual documents contain most of the postings, and does less well when there are a large num-ber of small but non-empty virtual documents. This suggests a hybrid approach where we pick all the large virtual docu-ments in MSA, remove the corresponding postings from the versions, and use DIFF to finish up the rest of the postings.
We refer to this method as 2-Hybrid for the basic two-level method, and 2R-Hybrid when used with reordering. One question is to decide how many of the large virtual docu-ments we should pick using MSA, and when we should stop and mop up the remaining postings using DIFF. We found that selecting all virtual documents above a fixed size works reasonably well.
All the described methods can be extended to store fre-quency values, using ideas similar to those in [11] for the ba-sic versions of DIFF and MSA. During the reordering step, we now sort by a slightly different weight function that takes into account that the virtual documents are bags (multi sets) rather than sets. In the case of 2R-HUFF, as in the HUFF method in [11], we have a choice between keeping docIDs and frequencies separate and integrating them into one vec-tor for better compression. Details are omitted due to space constraints, but we provide experimental results for the fre-quency case further below.
We now outline the changes in query processing that are required when using our new techniques. We assume here a standard DAAT type query processor that implements ranked queries on top of a Boolean filter such as an AND or OR of the query terms. Here, we focus on the case of AND. Of course, real queries on archival collections may involve addi-tional features such as restrictions to certain time ranges.
For the two baseline methods, 2-DIFF and 2-MSA, we first traverse the first-level index, and any data from the second-level is only fetched once the Boolean filter has found a docID in the intersection of the first-level inverted lists of the query terms. Recall that the second-level postings are blocked, such that each block of 128 integers can be independently accessed and decompressed. To retrieve the second-level data for a particular document and term, we may need to decompress more than one block if the data goes across block boundaries.
Bit vectors are recreated from compressed data by initial-izing a vector to zero, and then setting corresponding bits as we decompress the gap values. Afterwards, the reordering needs to be inverted in the cases of 2R-DIFF, or 2R-Hybrid. Finally, we process the information in the recovered bit vec-tor as required in DIFF, or the hybrid. In the case of DIFF we need to loop over the bit vector and apply any changes (recall that DIFF indexes these changes); we found that this step can be accelerated significantly by using a lookup table of size 512 that converts one byte of the bit vector at a time. We now present our experimental results on the Wiki and Ireland data sets.

One versus two levels: In Table 2 we show the sizes of the first-level indexes for Wiki and Ireland under IPC and PFD. As we see, IPC performs better than PFD on this part of the index. Next, in Table 3 we compare the one-level and basic two-level methods for DIFF and MSA in terms of index size. For the first level in the two-level methods, we use IPC, while for the second level, we show results for IPC and PFD. We see that the two-level methods consistently outperform their one-level counterparts in terms of index size, with particularly large improvements when using PFD.
Thus, even without the reordering technique, we see decent improvements over the one-level versions of DIFF and MSA evaluated in [11]. To directly compare the results to those for the combinatorial approach in Table 1, we need to deduct the size of the first-level structure (73 MB for Wiki and 273 MB for Ireland) from the numbers in Table 3; we see that there is still a gap between the methods.
 Table 3: Compressed index sizes for the one-level and .

Results for docID indexes: InTable4,welookatthe compressed size for index structures with docIDs but no fre-quency values. First, we see improvements in index size over the results in Table 3 from use of the reordering technique. After accounting for the size of the first-level index, many of the numbers are now in the range of numbers that we saw from the bitwise approaches in the previous section. The best result on Wiki is obtained by 2R-HUFF, while for Ire-land 2R-Hybrid-IPC performs best. Note that, for 2R-DIFF, 2R-HUFF, and 2R-Hybrid, the size of a global table to re-verse the bitvector is included in the compressed size.
In general, improvements are more limited for Ireland, and this is probably due to the smaller number of versions per document: Basically, shorter bit vectors offer less structure that can be exploited for better compression.
 Table 4: CompressedindexsizeinMBformethodswith .

Results for docIDs and frequencies: In Table 5 we show results for index structures that contain docIDs as well as frequencies. This means that for MSA and DIFF we also change the way virtual documents are defined, resulting in increases in the size of the docID data itself. For 2R-HUFF, we have a choice between keeping docIDs and frequencies separate, or integrating them into one vector for better com-pression. We again see very significant improvements over the best previous approach, the HUFF method from [11].
Tuning the hybrid: In Figure 5 we show the compressed size of a docID-only index as we vary the cut-off between using MSA and DIFF. We see that for Wiki, it is best to use all virtual documents in MSA with size at least 20, while for Ireland, virtual documents with sizes as small as 10 should be selected. We note that slight additional improvements might be possible by further tuning of this policy.
 Table 5: Compressed sizes of indexes with docID and Figure 5: Selecting the best cut-off point between MSA
Query processing performance: We have observed great improvements in index size, but these improvements would be futile if query processing performance is degraded due to the various complicated ordering and hybrid strategies. We now show that this is not a problem, and that in fact our methods are even faster than th e fastest previous method, the Sorted algorithm in [11].

In Table 6, we show query processing cost in milliseconds per query on the Wiki data for a few of our new two-level techniques and for the Sorted algorithm from [11] that was previously the fastest method. For each technique we show the query processing speed, index size, and number of de-coded postings per query, using IPC, PFD, and a mixed in-dex with IPC on the first level and PFD on the second level.
Our query processor assumes that the entire index is kept in main memory, and thus there are no disk access costs. Following the approach in [11], we randomly selected 10000 queries from a large trace of AOL queries, considering all queries that resulted in visits to Wikipedia. As we see from Table 6, our new two-level methods are in fact all faster than the fastest previous method! The fastest method is 2-DIFF, which runs in 0.66 millisecond per query using PFD. Adding the reordering only increases the time to 0 . 83 mil-liseconds. Note that for 2R-MSA, the query processing time is the same as 2-MSA. Because there is no reversed opera-tion during query processing. For 2R-Hybrid, we were un-able to get results in time for the final version, but we expect running times very close to the methods without reordering. 2R-HUFF, on the other hand (not shown), runs slower than the other methods, due to the use of Huffman coding, and takes more than 2ms.

We also note that using IPC instead of PFD in the first level of the index, but keeping PFD in the second level, only marginally increases query processing costs, while giving a decent reduction in index size. The new two-level methods obtain much of their speed gains by decoding many fewer postings than Sorted. In fact, this is not unexpected and was the main motivation for the introduction of two-level methods in [1]. In summary, our results show decent and si-multaneous improvements in index size and query processing speed over all previous approaches. Table 6: Query processing cost, index size, and number
In this paper, we have studied index organization and com-pression techniques for versioned document collections. In particular, we analyzed typical properties of versioned docu-ment collections that lead to succinct index structures, and then derived combinatorial upper and lower bounds for index size. Our main contribution are new index organization and compression schemes based on the DIFF [2] and MSA [13] approaches that achieve significant improvements in both in-dex size and query processing speed.

Our improvements are based on exploiting the properties of the bit vectors in the second level of the index. The prob-lem of better modeling these bit vectors is closely related to text evolution and user edit behavior in versioned collections. It would be very interesting to come up with simple mathe-matical models for these bit vectors. In particular, it would be nice to have generative models for user behavior that ex-plain the patterns we observe in these bit vectors. We would expect such models to be different in multi-author environ-ments such as Wikipedia where multiple authors edit the same document, and collections such as the Internet Archive where most pages are maintained by a single person or orga-nization. While there is a lot of recent work on the structure of Wikipedia, we have not seen a good model to explain the overall evolution of text and thus the coming and going of terms in documents.

Other interesting research problems are improved posi-tional index structures for versioned collections, and opti-mizing query processing performance in versioned collections for different classes of queries.
 This research was supported by NSF Grant IIS-0803605,  X  X f-ficient and Effective Search Services over Archival Webs X , and by a grant from Google. We also thank the Internet Archive for providing access to the Ireland data set.
