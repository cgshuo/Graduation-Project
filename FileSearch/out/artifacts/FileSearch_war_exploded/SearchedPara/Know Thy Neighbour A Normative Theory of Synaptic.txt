 Far from being static relays, synapses are complex dynamical elements. The effect of a spike from a presynaptic neuron on its postsynaptic partner depends on the history of the activity of both pre-and postsynaptic neurons, and thus the efficacy of a synapse undergoes perpetual modification. These changes in efficacy can last from hundreds of milliseconds or minutes (short-term plasticity) to hours or months (long-term plasticity). Short-term plasticity typically only depends on the firing pattern of the presynaptic cell [1]; short term depression gradually diminishes the postsynaptic effects of presynaptic spikes that arrive in quick succession (Fig. 1A). Given the prominence and ubiquity of synaptic depression in cortical (and subcortical) synapses [2], it is pressing to identify its computa-tional role(s).
 There have thus been various important suggestions for the functional significance of synaptic de-pression, including  X  just to name a few  X  low-pass filtering of inputs [3], rendering postsynaptic responses insensitive to the absolute intensity of presynaptic activity [4, 5], and decorrelating input spike sequences [6]. However, important though they must be for select neural systems, these sug-gestions have a piecemeal flavor  X  for instance, chaining together stages of low-pass filtering would lead to trivial responding.
 Here, we propose a theory according which synaptic depression solves a computational problem that is faced by any neural population in which neurons represent and compute with analog quantities, but communicate with discrete spikes. For convenience, we assume this analog quantity to be the membrane potential, but, via a non-linear transformation [7], it could equally well be an analog firing rate. That is, we assume that network computations require the evolution of the membrane potential of a neuron to be a function of the membrane potentials of its presynaptic partners. However, such a neuron does not have (at least not directly, see [8] for an example of indirect interaction) access to these membrane potentials, but rather only to the spikes to which they lead, and so it faces a key estimation problem.
 logically broken down into three concurrent processes, each running in its dedicated functional compartment: 1) the neuron X  X  afferent synapses (e.g. spines) estimate the membrane potential of its presynaptic partners, scaled according to the rules of the network computation; 2) the neuron X  X  soma-dendritic compartment follows the membrane potential-dependent dynamics and post-synaptic in-tegration also determined by the computation; and 3) its axon generates action potentials that are broadcasted to its efferent synapses (and possibly back to the other compartments, eg. for long-term plasticity). It is in the indispensable first estimation step that we suggest synaptic depression to be involved.
 In Section 2 we formalise the problem of estimating presynaptic membrane potentials as an instance of Bayesian inference, and derive an online recursive estimator for it. Given suitable assumptions about presynaptic membrane potential dynamics and spike generation, this optimal estimator can be written in closed form exactly [9, 10]. In Section 3, we introduce a canonical model of postsynap-tic membrane potential and synaptic depression dynamics, and show how it relates to the optimal estimator derived earlier. In Section 4, we present results from numerical simulations showing the quality with which synaptic depression can approximate the performance of the optimal estimator, and how much is gained relative to a static synapse without synaptic depression. Finally, in Section 5, we sum up, suggest experimentally testable predictions, and discuss possible extensions of this work, eg. to incorporate other forms of short-term synaptic plasticity. The Bayesian estimation problem that needs to be solved by a synapse involves inferring the poste-time), given the spikes seen from the presynaptic cell up to that time step, s 1 ..t . We first define a statistical (generative) model of presynaptic membrane potential fluctuations and spiking, and then derive the estimator that is appropriate for it.
 The generative model involves two simplifying assumptions (Fig. 1B). First we assume that presy-naptic membrane potential dynamics are Markovian In particular, we assume that the presynaptic membrane potential evolves as an Ornstein-Uhlenbeck (OU) process, given (again, in discretized time) by Figure 1: A. Synaptic depression: postsynaptic responses to a train of presynaptic action poten-tials (not shown) at 40 Hz . (Reproduced from [11], adapted from [12].) B. Graphical model of the process generating presynaptic subthreshold membrane potential fluctuations, u , and spikes, s . The membrane potential evolves according to a first-order Markov process, the Ornstein-Uhlenbeck (OU) process (Eqs. 1-2). The probability of generating a spike at time t ( s t = 1 ) depends only on the current membrane potential, u t , and is determined by a non-linear Poisson (NP) model (Eqs. 3-5). C. Sample membrane potential trace ( red line ) and spike timings ( vertical black dotted lines ) generated by the OU-NP process; with u r = 0 mV ,  X   X  1 = 100 ms ,  X  2 W = 0 . 02 mV 2 / ms  X 
OU = 1 mV where 1 / X  is the time constant with which the membrane potential decays back to its resting value, u , and  X  t is the size of the discretized time bins. Because both  X  and  X  W are assumed to be constant, the variance of the presynaptic membrane potential,  X  2 OU =  X  2 W / 2  X  , is stationary. The second assumption is that spiking activity at any time only depends on the membrane potential at that time: In particular, we assume that the spike generating mechanism is an inhomogeneous Poisson process (Fig. 1C). Thus, at time step t , the neuron emits a spike ( s t = 1 ) with probability g ( u t ) X  t , and therefore the spiking probability p ( s t | u t ) given the membrane potential can be written as: We further assume that the transfer function, g ( u ) , is exponential 1 : where  X  determines the stochasticity of spiking. In the limit  X   X   X  the spiking process is deter-the neuron does not fire.
 Estimating on-line the membrane potential of the presynaptic cell from its spiking history amounts to computing the posterior probability distribution, p ( u t | s 1 ..t ) . Since equations 1 and 3 define a hidden Markov model, the posterior can be written in a recursive form: even though inference can be performed recursively, and the hidden dynamics is linear-Gaussian (Eq. 2), the (extended) Kalman filter cannot be used here for inference because the measurement does not involve additive Gaussian noise, but rather comes from the stochasticity of the spiking process (Eqs. 4-5). Performing recursive inference (filtering), as described by equation 6, under the generative model de-(see Supplementary Information). The mean and variance of this Gaussian evolve (in continuous time, by taking the limit  X  t  X  0 ) as: with the normalisation factor given by where S ( t ) is the spike train of the presynaptic cell (represented as a sum of Dirac delta functions). (A similar, but not identical, derivation can be found in [9]).
 Equation 7 indicates that each time a spike is observed, the estimated membrane potential should increase proportionally to the uncertainty (variance) about the current estimate. This estimation uncertainty then decreases each time a spike is observed (Eqs. 8-9). As Fig. 2A shows, the higher the presynaptic membrane potential is, the more spikes are emitted (because the instantaneous firing rate is a monotonic function of membrane potential, see Eq. 5), and therefore the smaller the posterior variance becomes. Therefore the estimation error is smaller for higher membrane potential (see Fig. 2B). Conversely, in the absence of spikes, the estimated membrane potential decreases while the variance increases back to its asymptotic value. Fig. 2C shows that the representation of uncertainty about the membrane potential by  X  2 is self-consistent because it is predictive of the error of the mean estimator,  X  .
 The first term on the r.h.s of equation 7 comes from the prior knowledge about the membrane poten-tial dynamics. The second term comes from the likelihood of the spiking observations. Those two contributions can be isolated independently by taking two different limits that we will consider in the next two subsections. 2.1 Small noise limit In the limit of small variance of the noise driving the OU process, i.e.,  X  2 W =  X  2 W the asymptotic uncertainty  X  2  X  scales with :  X  2  X  =  X  2 W dynamics of  X  becomes driven only by the prior mean membrane potential u r : and so the asymptotic estimated membrane potential will tend to the prior mean membrane potential. This is reasonable since in the small noise limit, the true membrane potential u t will effectively be very close to u r . Furthermore the convergence time constant of the estimated membrane potential should be matched to the time constant  X   X  1 of the OU process and this is indeed the case in Eq. 10. 2.2 Slow dynamics limit A second interesting limit is where the time constant of the OU process becomes small, i.e.,  X  =  X  0 with  X  0 . In this case, the variance of the noise in the OU process must also scale with , i.e W =  X  2 W 0 , to prevent the process from being unbounded. The variance  X  2 OU =  X  2 W 0 / 2  X  0 of the OU process is therefore independent of . In this case, the asymptotic value of the posterior variance becomes  X  2  X  = scales with whereas the second term with Because the time constant  X   X  1 of the OU process is slow, the driving force that pulls the membrane potential back to its mean value u r is weak. Therefore the membrane potential estimation dynamics should rely on the observed spikes rather than on the prior information u r . This is apparent in Eq. 11. Furthermore, the time constant  X  = p  X // X  W 0 is not fixed but is a function of the mean estimated Figure 2: The performance of the optimal on-line estimator. A. Red line: presynaptic membrane potential, u , as a function of time, vertical dotted lines: spikes emitted. Dot-dashed black line: on-line estimator  X  given by Eq. (7), gray shading:  X   X   X  , with  X  given by Eq. (8). B. Estimation error (  X   X  u ) 2 as a function of the membrane potential u of the OU process. Black dots: estimation error and true membrane potential in individual time steps, red line: third order polynomial fit. C Black bars : histogram of normalized estimation error z = (  X   X  u ) / X  . Red line : normal distribution N ( z ; 0 , 1) . Parameters were as in Fig. 1, except for  X   X  1 = 0 . 5 mV . will be small and hence the time constant  X  will be small as well. As a consequence, each spike will greatly increase the estimate and therefore speed up the approach of this estimate to the true value. As  X  gets closer to the true membrane potential, the time constant increases, leading to an appropriately accurate estimate of the membrane potential. This dynamical time constant therefore helps the estimation avoid the traditional speed vs accuracy trade-off (short time constant are fast but give a noisy estimation; longer time constant are slow but yield a more accurate estimation), by combining the best of the two worlds. In section 2 we have shown that presynaptic spikes have a varying, context-dependent effect on the optimal on-line estimator of presynaptic membrane potential. In this section we will show that the variability that synaptic depression introduces in postsynaptic responses closely resembles the variability of the optimal estimator.
 A simple way to study the similarity between the optimal estimator and short-term plasticity is to consider their steady state filtering properties. As we saw above, according to the optimal estimator, increment due to subsequent spikes should decrease. This is consistent with depressing synapses for which the amount of excitatory postsynaptic current (EPSC) decreases when the stimulation frequency is increased (see Fig. 3). Figure 3: A. Steady-state spiking increment  X  X  2 of the optimal estimator as a function of r =  X  S  X  (Eq. 8). B. Synaptic depression in the climbing fibre to Purkinje cell synapse: average (  X  s.e.m.) normalised  X  X teady-state X  magnitude of EPSCs as a function of stimulation frequency. Reproduced from [3].
 Importantly, the similarity between the optimal membrane potential estimator and short-term plas-ticity is not limited to stationary properties. Indeed, the actual dynamics of the optimal estimator (Eqs. 7-9) can be well approximated by the dynamics of synaptic depression. In a canonical model of short-term depression [14], the postsynaptic membrane potential, v , changes as where J and Y are constants (synaptic weight and utilisation fraction), and x is a time varying  X  X e-source X  variable (e.g. the fraction of presynaptic vesicles ready to fuse to the membrane). Thus, v is increased by each presynaptic spike, and in the absence of spikes it decays to its resting value, v 0 , with membrane time constant  X  . However, the effect of each spike on v is scaled by x which itself is decreased after each spike and increases between spikes back towards one with time constant  X  D . Thus, the postsynaptic potential, v , behaves much like the posterior mean of the optimal estimator,  X  , while the dynamics of the synaptic resource variable, x , closely resemble that of the posterior variance of the optimal estimator,  X  2 . This qualitative similarity can be made more formal under appropriate assumptions, for details see section 3 of supplementary information. Indeed, the ca-pacity of a depressing synapse (with appropriate parameters) to estimate the presynaptic membrane potential can be nearly as good as that of the optimal estimator (Fig. 4, top). Interestingly, although the scaled variance  X  2 / X  2  X  does not follow the resource variable dynamics x perfectly just after a spike, these two quantities are virtually identical at the time of the next spike, i.e. when they are used by the membrane potential estimators (Fig. 4, bottom). In order to quantify how well synaptic dynamics with depression perform in estimating presynap-tic membrane potentials, we measure performance by the mean-squared error (MSE) between the true membrane potential u and the estimated membrane potential, and compare the MSE of three alternatives estimators.
 The simplest model we consider is a static (non-depressing) synapse, in which v is given by Eq. 12 with constant x = 1 . This estimator has only 3 tuneable parameters:  X  , v 0 and J ( Y = 1 is fixed without loss of generality). The second estimator we consider includes synaptic depression, i.e. x is also allowed to vary (Eq. 12). This estimator contains 5 tuneable parameters ( v 0 ,  X  , Y , J ,  X  D ). Finally, we consider the optimal estimator (Eqs. 7-9). This estimator has no tunable parameters. Once the parameters of presynaptic membrane potential dynamics (  X  W ,  X  , u r ) and spiking (  X  , g 0 ) are fixed, the optimal estimator is entirely determined. The comparison of the performance of these three estimators is displayed on Fig. 5. The optimal estimator (black circles) is obviously a lower bound on any type of estimator. For a wide range of parameter values, the depressing synapse performs almost as well as the optimal estimator, and both perform better than the static synapse. Figure 4: Depressing synapses implement near-optimal estimation of presynaptic membrane poten-tials. Top. Red line , and vertical dotted lines: membrane potential, u , and spikes, S , generated by a simulated presynaptic cell (with parameters as in Fig. 1). Blue line: postsynaptic potential, v , in a depressing synapse (Eq. 12) with all 5 parameters ( J = 4 . 82 ,  X  = 60 . 6 ms, v 0 =  X  0 . 59 mV,  X  d = 64 ms, Y = 0.17) tuned to minimize the mean squared estimation error, ( u  X  v ) 2 . Black line: Posterior mean of the optimal on-line estimator,  X  (Eq. 7). Bottom. Black: resource variable, x , in the depressing synapse (Eq. 12). Blue: posterior variance of the optimal estimator,  X  2 (Eq. 8). In the slow dynamics limit (  X  0 , see section 2.2), the estimation error of the optimal estimator can even be approximated analytically (see Supplementary Information). In this limit, the error scales with expression is consistent with the simulations. Synapses are a cornerstone of computation in networks, and are highly complex dynamical systems involving more than a thousand different types of protein. One prominent feature of their dynamics is significant short-term changes in efficacy; these belie the sort of single fixed, or slowly changing, weights popular in most neural models. We interpreted short-term synaptic depression, a key feature of synaptic dynamics, as solving the fundamental computational task of estimating the analog mem-brane potential of the presynaptic cell from observed spikes. Steady-state and dynamical properties of a Bayes-optimal estimator are well-matched by a canonical model of depression; using a fixed synaptic efficacy instead leads to a highly suboptimal estimator.
 Our theory is readily testable, since it suggests a precise relationship between quantities that have been subject to extensive, separate, empirical study  X  namely the statistics of a neuron X  X  membrane potential dynamics (captured by the parameters of Eq. (2)), the form of its spiking non-linearity (described by Eq. (5)), and the synaptic depression it expresses in its efferent synapses. Accounting of short-term synaptic plasticity [15] remains a challenge; one obvious possibility is that different synapses are estimating different aspects or functions of the membrane potential.
 Our approach is almost dual to that explored in [16]. For that model, the spike generation mechanism of the presynaptic neuron was modified such that even a simple read-out mechanism with fixed efficacies could correctly decode the analogue quantity encoded presynaptically. By contrast, we considered a standard model of spiking [17], and thereby derived an explanation for the evident fact that synapses are not in fact fixed.
A B Figure 5: A. Comparing the estimation error for different membrane potential estimators as a func-tion of . (  X  =  X  0 ,  X  2 W =  X  2 W synapse with its 5 tuneable parameters (see text) being optimised for each value of . Red: static synapse with its 3 tuneable parameters (see text) being optimised. Total simulated time was 5 min. Horizontal dot-dashed line : upper bound on the estimation error given by  X  OU =  X  W / B. Analysing the estimation error of the optimal estimator in the slow dynamics limit (  X  0 ). Solid line: analytical approximation (Eq. 31 in the Supplementary Information), circles: simulation, horizontal dot-dashed line: as in A .
 There are several avenues to extend the present analysis. For example, it would be important to un-derstand in more quantitative detail the mapping between the parameters of the process generating the presynaptic membrane potential and spikes, and the parameters of synaptic depression that will best realize the corresponding optimal estimator. We present some preliminary derivations in the supplementary material that seem to yield at least the right ball-park values for optimal synaptic dy-namics. This should also enable us to explore the particular parameter regimes in which depressing synapses have the most (or least) advantage over static synapses in terms of estimation performance, as in Fig. 5. We should also consider a meta-plasticity rule that suitably adapts the parameters of the short-term dynamics in the light of the statistics of spiking.
 Our assumption about the prior distribution of presynaptic membrane potential dynamics is highly restrictive. A broader scheme that has previously been explored is that it follow a Gaussian process model [18, 19] with a more general covariance function. Recursive estimation is often a reasonable approximation in such cases, even for those covariance functions, for instance enforcing smooth-ness, for which it cannot be exact. One interesting property of smooth trajectories is that a couple of spikes arriving in quick succession may be diagnostic of an upward-going trend in membrane po-tential which is best decoded with increasing, i.e., facilitating, rather than decreasing, postsynaptic responses. Thus it may be possible to encompass other forms of short term plasticity within our scheme.
 The spike generation process can also be extended to incorporate refractoriness, bursting, and other forms of non-Poisson behaviour, eg. as in [20]. Similarly, synaptic failures could also be considered. We hope through our theory to be able to provide a teleological account of the rich complexities of real synaptic inconstancy.
 Acknowledgements Funding was from the Gatsby Charitable Foundation (PD) and the Wellcome Trust (JPP, ML and PD). [1] Abbott, L.F. &amp; Regehr, W.G. Synaptic computation. Nature 431 , 796 X 803 (2004). [2] Zucker, R. &amp; Regehr, W. Short-term synaptic plasticity. Annual Review of Physiology 64 , [3] Dittman, J., Kreitzer, A. &amp; Regehr, W. Interplay between facilitation, depression, and residual [5] Cook, D., Schwindt, P., Grande, L. &amp; Spain, W. Synaptic depression in the localization of [6] Goldman, M., Maldonado, P. &amp; Abbott, L. Redundancy reduction and sustained firing with [7] Ermentrout, B. Neural networks as spatio-temporal pattern-forming systems. Reports on [8] Shu, Y., Hasenstaub, A., Duque, A., Yu, Y. &amp; McCormick, D. Modulation of intracortical [9] Eden, U., Frank, L., Barbieri, R., Solo, V. &amp; Brown, E. Dynamic analysis of neural encoding [10] Bobrowski, O., Meir, R. &amp; Eldar, Y. Bayesian filtering in spiking neural networks: Noise, [11] Dayan, P. &amp; Abbott, L.F. Theoretical Neuroscience (MIT Press, Cambridge, 2001). [12] Markram, H. &amp; Tsodyks, M. Redistribution of synaptic efficacy between neocortical pyramidal [13] Jolivet, R., Rauch, A., L  X  uscher, H.R. &amp; Gerstner, W. Predicting spike timing of neocortical [14] Mongillo, G., Barak, O. &amp; Tsodyks, M. Synaptic theory of working memory. Science 319 , [15] Markram, H., Wu, Y. &amp; Tosdyks, M. Differential signaling via the same axon of neocortical [16] Deneve, S. Bayesian spiking neurons I: inference. Neural Computation 20 , 91 X 117 (2008). [17] Gerstner, W. &amp; Kistler, W.K. Spiking Neuron Models (Cambridge University Press, Cambridge [18] Cunningham, J., Yu, B., Shenoy, K. &amp; Sahani, M. Inferring neural firing rates from spike trains [19] Huys, Q., Zemel, R., Natarajan, R. &amp; Dayan, P. Fast population coding. Neural Computation [20] Pillow, J. et al. Spatio-temporal correlations and visual signalling in a complete neuronal
