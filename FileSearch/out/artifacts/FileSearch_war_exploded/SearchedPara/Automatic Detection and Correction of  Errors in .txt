 Treebanks and other annotated corpora have be -come essential for almost all NLP applications. Pa -pers about corpora like the Penn Treebank [1] have thousands of citations, since most of the algorithms profit from annotated data during the development and testing and thus are widely used in the field. Treebanks are therefore expected to be of a very high quality in order to guarantee reliability for their theoretical and practical uses. The construc -tion of an annotated corpus involves a lot of work performed by large groups. However, despite the fact that a lot of human post-editing and automatic quality assurance is done, errors can not be avoided completely [5]. ing and correcting errors in dependency treebanks. We apply our method to the English dependency corpus  X  conversion of the Penn Treebank to the dependency format done by Richard Johansson and Mihai Surdeanu [2] for the CoNLL shared tasks [3]. This is probably the most used dependency corpus, since English is the most popular language among the researchers. Still we are able to find a considerable amount of errors in it. Additionally, we compare our method with an interesting ap -proach developed by a different group of research -ers (see section 2). They are able to find a similar number of errors in different corpora, however, as our investigation shows, the overlap between our results is quite small and the approaches are rather complementary. Surprisingly, we were not able to find a lot of work on the topic of error detection in treebanks. Some organisers of shared tasks usually try to guarantee a certain quality of the used data, but the quality control is usually performed manually. E.g. in the already mentioned CoNLL task the organisers ana -lysed a large amount of dependency treebanks for different languages [4], described problems they have encountered and forwarded them to the de -velopers of the corresponding corpora. The only work, that we were able to find, which involved automatic quality control, was done by the already mentioned group around Detmar Meurers. This work includes numerous publications concerning finding errors in phrase structures [5] as well as in dependency treebanks [6]. The approach is based on the concept of  X  X ariation detection X , first intro -duced in [7]. Additionally, [5] presents a good method for evaluating the automatic error detec -tion. We will perform a similar evaluation for the precision of our approach. We will compare our outcomes with the results that can be found with the approach of  X  X ariation detection X  proposed by Meurers et al. For space reasons, we will not be able to elaborately present this method and advise to read the referred work, However, we think that we should at least briefly explain its idea.
 strings, which occur multiple times in the corpus, but which have varying annotations. This can obvi -ously have only two reasons: either the strings are ambiguous and can have different structures, de -pending on the meaning, or the annotation is erro -neous in at least one of the cases. The idea can be adapted to dependency structures as well, by ana -lysing the possible dependency relations between same words. Again different dependencies can be either the result of ambiguity or errors. We propose a different approach. We take the Eng -lish dependency treebank and train models with two different state of the art parsers: the graph-based MSTParser [9] and the transition-based MaltParser [10]. We then parse the data, which we have used for training, with both parsers. The idea behind this step is that we basically try to repro -duce the gold standard, since parsing the data seen during the training is very easy (a similar idea in the area of POS tagging is very broadly described in [8]). Indeed both parsers achieve accuracies between 98% and 99% UAS (Unlabeled Attach -ment Score), which is defined as the proportion of correctly identified dependency relations. The reas -on why the parsers are not able to achieve 100% is on the one hand the fact that some of the phenom -ena are too rare and are not captured by their mod -els. On the other hand, in many other cases parsers do make correct predictions, but the gold standard they are evaluated against is wrong.
 when both parsers predict dependencies different from the gold standard (we do not consider the cor -rectness of the dependency label). Since MSTPars -er and MaltParser are based on completely differ -ent parsing approaches they also tend to make dif -ferent mistakes [11]. Additionally, considering the accuracies of 98-99% the chance that both parsers, which have different foundations, make an erro -neous decision simultaneously is very small and therefore these cases are the most likely candidates when looking for errors. In this section we propose our algorithm for auto -matic correction of errors, which consists out of the following steps: identified 6743 error candidates, which is about 0.7% of all tokens in the corpus.
 situte the gold standard by MSTParser and not MaltParser in order not to give an advantage to a parser with similar basics (both MDParser and MDParser are transition-based). the result of MDParser significantly improves: it is able to correctly recgonize 3535 more dependen -cies than before the substitution of the gold stand -ard. 2077 annotations remain wrong independently of the changes in the gold standard. 1131 of the re -lations become wrong with the changed gold standard, whereas they were correct with the old unchanged version. We then undo the changes to the gold standard when the wrong cases remained wrong and when the correct cases became wrong. We suggest that the 3535 dependencies which be -came correct after the change in gold standard are errors, since a) two state of the art parsers deliver a result which differs from the gold standard and b) a third parser confirms that by delivering exactly the s ame result as the proposed change. However, the exact precision of the approach can probably be computed only by manual investigation of all cor -rected dependencies. The previous section tries to evaluate the precision of the approach for the identified error candidates. However, it remains unclear how many of the er -rors are found and how many errors can be still ex -pected in the corpus. Therefore in this section we will describe our attempt to evaluate the recall of the proposed method.
 which can be found with our method, we have de -signed the following experiment. We have taken sentences of different lengths from the corpus and provided them with a  X  X old standard X  annotation which was completely (=100%) erroneous. We have achieved that by substituting the original an -notation by the annotation of a different sentence of the same length from the corpus, which did not contain dependency edges which would overlap with the original annotation. E.g consider the fol -lowing sentence in the (slightly simplified) CoNLL format: 1 Not RB 6 SBJ 2 all PDT 1 NMOD 3 those DT 1 NMOD 4 who WP 5 SBJ 5 wrote VBD 1 NMOD 6 oppose VBP 0 ROOT 7 the DT 8 NMOD 8 changes NNS 6 OBJ 9 . . 6 P We would substitute its annotation by an annota -tion chosen from a different sentence of the same length: 1 Not RB 3 SBJ 2 all PDT 3 NMOD 3 those DT 0 NMOD 4 who WP 3 SBJ 5 wrote VBD 4 NMOD 6 oppose VBP 5 ROOT 7 the DT 6 NMOD 8 changes NNS 7 OBJ 9 . . 3 P well-formed dependency tree (since its annotation belonged to a different tree before) to the corpus and the exact number of errors (since randomly correct dependencies are impossible). In case of our example 9 errors are introduced to the corpus. tences of different lengths with overall 1350 tokens. We have then retrained the models for MSTParser and MaltParser and have applied our methodology to the data with these errors. We have then counted how many of these 1350 errors could be found. Our result is that 619 tokens (45.9%) were different from the erroneous gold-standard. That means that despite the fact that the training data contained some incorrectly annotated tokens, the parsers were able to annotate them dif -ferently. Therefore we suggest that the recall of our method is close to the value of 0.459. However, of course we do not know whether the randomly in -troduced errors in our experiment are similar to those which occur in real treebanks. The interesting question which naturally arises at this point is whether the errors we find are the same as those found by the method of variation de -tection. Therefore we have performed the follow -ing experiment: We have counted the numbers of occurrences for the dependencies B  X  A (the word B is the head of the word A) and C  X  A (the word C is the head of the word A), where
B  X  A is the dependency proposed by the pars -ers and C  X  A is the dependency proposed by the gold standard. In order for variation detection to be applicable the frequency counts for both rela -tions must be available and the counts for the de -pendency proposed by the parsers should ideally greatly outweigh the frequency of the gold stand -ard, which would be a great indication of an error. For the 3535 dependencies that we classify as er -rors the variation detection method works only 934 times (39.5%). These are the cases when the gold standard is obviously wrong and occurs only few times, most often -once, whereas the parsers pro -pose much more frequent dependencies. In all oth -er cases the counts suggest that the variation detec -tion would not work, since both dependencies have frequent counts or the correct dependency is even outweighed by the incorrect one. We will provide some of the example errors, which we are able to find with our approach. Therefore we will provide the sentence strings and briefly compare the gold standard dependency annotation of a certain dependency within these sentences. takeover stock traders, and caused a 7.3% drop in the DOW Jones Transportation Average, second in size only to the stock-market crash of Oct. 19 1987.
 dependency relation market  X  the , whereas the parsers correctly recognise the dependency crash  X  the . Both dependencies have very high counts and therefore the variation detection would not work well in this scenario.
 time.
 points  X  at , whereas the parsers predict was  X  at . The gold standard suggestion occurs only once whereas the temporal dependency was  X  at occurs 11 times in the corpus. This is an example of an error which could be found with the variation detection as well.
 CenTrust's cash  X  plus a $ 1.2 million commission  X  for  X  X ortrait of a Man as Mars X .
 dependency relation $  X  a , whereas the parsers correctly recognise the dependency commission  X  a . The interesting fact is that the relation $  X  a is actually much more fre -quent than commission  X  a , e.g. as in the sen -tence he cought up an additional $1 billion or so . ( $  X  an ) So the variation detection alone would not suffice in this case. The quality of treebanks is of an extreme import -ance for the community. Nevertheless, errors can be found even in the most popular and widely-used resources. In this paper we have presented an ap -proach for automatic detection and correction of errors and compared it to the only other work we have found in this field. Our results show that both approaches are rather complementary and find dif -ferent types of errors. modifier annotation of the dependency relations in the English dependency treebank. However, the same methodology can easily be applied to detect irregularities in any kind of annotations, e.g. labels, POS tags etc. In fact, in the area of POS tagging a similar strategy of using the same data for training and testing in order to detect inconsistencies has proven to be very efficient [8]. However, the meth -od lacked means for automatic correction of the possibly inconsistent annotations. Additionally, the method off course can as well be applied to differ -ent corpora in different languages. though we could not compute the exact value, since it would require an expert to go through a large number of cases. It is even more difficult to estimate the recall of our method, since the overall number of errors in a corpus is unknown. We have described an experiment which to our mind is a proach. On the one hand the recall we have achieved in this experiment is rather low (0.459), which means that our method would definitely not guarantee to find all errors in a corpus. On the oth -er hand it has a very high precision and thus is in any case beneficial, since the quality of the tree -banks increases with the removal of errors. Addi -tionally, the low recall suggests that treebanks con -tain an even larger number of errors, which could not be found. The overall number of errors thus seems to be over 1% of the total size of a corpus, which is expected to be of a very high quality. A fact that one has to be aware of when working with annotated resources and which we would like to emphasize with our paper. The presented work was partially supported by a grant from the German Federal Ministry of Eco -nomics and Technology (BMWi) to the DFKI Theseus project TechWatch X  X rdo (FKZ: 01M -Q07016). 
