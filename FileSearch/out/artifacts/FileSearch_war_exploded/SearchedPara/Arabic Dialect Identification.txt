 Microsoft Research University of Pennsylvania
The written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-trivial manner from the various spoken regional dialects of Arabic X  X he true  X  X ative languages X  of Arabic speakers. Those dialects, in turn, differ quite a bit from each other. However, due to MSA X  X  prevalence in written form, almost all Arabic data sets have predominantly MSA content. have created a large monolingual data set rich in dialectal Arabic content called the Arabic On-line Commentary Data set (Zaidan and Callison-Burch 2011). We describe our annotation (like over-identification of one X  X  own dialect). Using this new annotated data set, we consider the task of Arabic dialect identification: Given the word sequence forming an Arabic sentence, determine the variety of Arabic in which it is written. We use the data to train and evaluate significantly and dramatically outperform baselines that use MSA-only data, achieving near-a large Web crawl consisting of 3.5 million pages mined from on-line Arabic newspapers. 1. Introduction The Arabic language is a loose term that refers to the many existing varieties of Arabic.
Those varieties include one  X  X ritten X  form, Modern Standard Arabic (MSA), and many  X  X poken X  forms, each of which is a regional dialect. MSA is the only variety that communication and formal venues. The regional dialects, used primarily for day-to-day dealings and spoken communication, remain somewhat absent from written com-munication compared with MSA. That said, it is certainly possible to produce dialectal
Arabic text, by using the same letters used in MSA and the same (mostly phonetic) spelling rules of MSA. commonly used is the on-line domain: Dialectal Arabic has a strong presence in blogs, forums, chatrooms, and user/reader commentary. Harvesting data from such sources statistical learning setups. However, because all Arabic varieties use the same character set, and furthermore much of the vocabulary is shared among different varieties, it is not a trivial matter to distinguish and separate the dialects from each other. a large data set that we created by harvesting a large amount of reader commentary on on-line newspaper content, and describe our annotation effort on a subset of the harvested data. We crowdsourced an annotation task to obtain sentence-level labels indicating what proportion of the sentence is dialectal, and which dialect the sentence patterns and biases, and the data are used to train and evaluate automatic classifiers for dialect detection and identification. Our approach, which relies on training language models for the different Arabic varieties, greatly outperforms baselines that use (much more) MSA-only data: On one of the classification tasks we considered, where human annotators achieve 88.0% classification accuracy, our approach achieves 85.7% accuracy, compared with only 66.6% accuracy by a system using MSA-only data.
 the various Arabic varieties and corresponding data resources. In Section 3, we intro-duce the dialect identification problem for Arabic, discussing what makes it a difficult problem, and what applications would benefit from it. Section 4 provides details about our annotation set-up, which relied on crowdsourcing the annotation to workers on
Amazon X  X  Mechanical Turk. By examining the collected labels and their distribution, we characterize annotator behavior and observe several types of human annotator biases. We introduce our technique for automatic dialect identification in Section 5.
The technique relies on training separate language models for the different Arabic varieties, and scoring sentences using these models. In Section 6, we report on a large-scale Web crawl that we performed to gather a large amount of Arabic text from on-line newspapers, and apply our classifier on the gathered data. Before concluding, we give an overview of related work in Section 7. 2. Background: The MSA/Dialect Distinction in Arabic
Although the Arabic language has an official status in over 20 countries and is spoken different varieties of the language. Arabic is characterized by an interesting linguistic dichotomy: the written form of the language, MSA, differs in a non-trivial fashion from lit.  X  X ccent X ; also darjah , lit.  X  X odern X ). MSA is the only variety that is standardized, communication in formal venues. 1 The regional dialects, used primarily for day-to-day dealings and spoken communication, are not taught formally in schools, and remain somewhat absent from traditional, and certainly official, written communication. 172 grammatical and ungrammatical . 2 Furthermore, even though they are  X  X poken X  varieties, it is certainly possible to produce dialectal Arabic text , by spelling out words using the same spelling rules used in MSA, which are mostly phonetic. extent to which a particular individual is able to understand other dialects depends heavily on that person X  X  own dialect and their exposure to Arab culture and literature from outside of their own country. For example, the typical Arabic speaker has little trouble understanding the Egyptian dialect, thanks in no small part to Egypt X  X  history in movie-making and television show production, and their popularity across the Arab world. On the other hand, the Moroccan dialect, especially in its spoken form, is quite view, the dialects can be considered separate languages in their own right, much like
North Germanic languages (Norwegian/Swedish/Danish) and West Slavic languages (Czech/Slovak/Polish). 4 2.1 The Dialectal Varieties of Arabic One possible breakdown of regional dialects into main groups is as follows (see Figure 1): dialects. Some of those differences do not appear in written form if they are on the level of short vowels, which are omitted in Arabic text anyway. That said, many differences manifest themselves textually as well: 174 sentence composition as well. For instance, all varieties of Arabic, MSA, and otherwise, allow both SVO and VSO word orders, but MSA has a higher incidence of VSO sen-tences than dialects do (Aoun, Benmamoun, and Sportiche 1994; Shlonsky 1997). 2.2 Existing Arabic Data Sources
Despite the fact that speakers are usually less comfortable communicating in MSA than in their own dialect, MSA content significantly dominates dialectal content, as MSA is the variant of choice for formal and official communication. Relatively little printed material exists in local dialects, such as folkloric literature and some modern poetry, but the vast majority of published Arabic is in MSA. As a result, MSA X  X  dominance is also apparent in data sets available for linguistic research. The problem is somewhat mitigated in the speech domain, since dialectal data exists in the form of phone conver-sations and television program recordings, but, in general, dialectal Arabic data sets are hard to come by.
 ods applied to Arabic, but only the MSA variant of it. For example, a state-of-the-art Arabic-to-English machine translation system performs quite well when translating
MSA source sentences, but often produces incomprehensible output when the input is dialectal. For example, most words of the dialectal sentence shown in Figure 2 are transliterated, whereas an equivalent MSA sentence is handled quite well. The high transliteration rate is somewhat alarming, as the first two words of the dialectal sentence are relatively frequent function words: Aymt  X  y means  X  X hen X  and rH corresponds to the modal  X  X ill X .
 the system to produce a poor translation even for frequent words. Case in point, the system is unable to consistently handle any of Ayh ( X  X hat X ), Ally (the conjunction  X  X hat X ), or dh ( X  X his X ). Granted, it is conceivable that processing dialectal content is more difficult than MSA, but the main problem is the lack of dialectal training data. a large enough extent to warrant treating them as more or less different languages. The behavior of machine translation systems translating dialectal Arabic when the system 176 has been trained exclusively on MSA data is similar to the behavior of a Spanish -to-English MT system when a user inputs a Portuguese sentence. Figure 4 illustrates how MT systems behave (the analogy is not intended to draw a parallel between the linguistic differences MSA-dialect and Spanish-Portuguese). The MT system X  X  behavior is similar to the Arabic example, in that words that are shared in common between
Spanish and Portuguese are translated, while the Portuguese words that were never observed in the Spanish training data are left untranslated.
 dialectal content properly. A similar scenario would arise with many other NLP tasks, such as parsing or speech recognition, where dialectal content would be needed in large quantities for adequate training. A robust dialect identifier could sift through immense volumes of Arabic text, and separate out dialectal content from MSA content. 2.3 Harvesting Dialect Data from On-line Social Media
One domain of written communication in which MSA and dialectal Arabic are both commonly used is the on-line domain, because it is more individual-driven and less institutionalized than other venues. This makes a dialect much more likely to be the user X  X  language of choice, and dialectal Arabic has a strong presence in blogs, fo-rums, chatrooms, and user/reader commentary. Therefore, on-line data is a valuable resource of dialectal Arabic text, and harvesting this data is a viable option for com-learning.

Burch 2011) a 52M-word monolingual data set by harvesting reader commentary from the on-line versions of three Arabic newspapers (Table 2). The data is characterized by the prevalence of dialectal Arabic, alongside MSA, mainly in Levantine, Gulf, and
Egyptian. These correspond to the countries in which the three newspapers are pub-lished: Al-Ghad is from Jordan, Al-Riyadh is from Saudi Arabia, and Al-Youm Al-Sabe X  is from Egypt. 7 large portion of it that is in MSA. (Later analysis in Section 4.2.1 shows dialectal content is roughly 40%.) In order to take full advantage of the AOC (and other Arabic data sets with at least some dialectal content), it is desirable to separate dialectal content from non-dialectal content automatically. The task of dialect identification (and its automa-tion) is the focus for the remainder of this article. We next present the task of Arabic dialect identification, and discuss our effort to create a data set of Arabic sentences with their dialectal labels. Our annotation effort relied on crowdsourcing the annotation task to Arabic-speakers on Amazon X  X  Mechanical Turk service (Section 3). 3. Arabic Dialect Identification
The discussion of the varieties of Arabic and the differences between them gives rise to the task of automatic dialect identification (DID). In its simplest form, the task is to build a learner that can, given an Arabic sentence S , determine whether or not S contains dialectal content. Another form of the task would be to determine in which dialect S was written, which requires identification at a more fine-grained level.
 identification is often considered to be a  X  X olved problem, X  DID is most similar to a particularly difficult case of language ID, where it is applied to a group of closely related languages that share a common character set. Given the parallels between DID and language identification, we investigate standard statistical methods to establish how difficult the task is. We discuss prior efforts for Arabic DID in Section 7. 3.1 The Difficulty of Arabic DID
Despite the differences illustrated in the previous section, in which we justify treating distinguish and separate the dialects from each other. Because all Arabic varieties use the same character set, and because much of the vocabulary is shared among compiling a dialectal dictionary and detecting whether or not a given sentence contains dialectal words.
 178 as those of Figure 5, dialectal, even when none of the individual words are. The answer lies in the structure of such sentences and the particular word order within them, rather than the individual words themselves taken in isolation. Figure 6 shows MSA sentences that express the same meaning as the dialectal sentences from Figure 5. As one could see, the two versions of any given sentence could share much of the vocabulary, but in ways that are noticeably different to an Arabic speaker. Furthermore, the differences would be starker still if the MSA sentences were composed from scratch, rather than by modifying the dialectal sentences, since the tone might differ substantially when composing sentences in MSA.
 3.2 Applications of Dialect Identification
Being able to perform automatic DID is interesting from a purely linguistic and experi-mental point of view. In addition, automatic DID has several useful applications: 180 4. Crowdsourcing Arabic Dialect Annotation
In this section, we discuss crowdsourcing Arabic dialect annotation. We discuss how we built a data set of Arabic sentences, each of which is labeled with whether or not dialectal content (i.e., how much dialect there is), and of which type of dialect it is. The sentences themselves are sampled from the AOC data set, and we observe that about 40% of sentences contain dialectal content, with that percentage varying between 37% and 48%, depending on the news source.
 quality control (Callison-Burch and Dredze 2010). We present the annotation interface and discuss an effective way for quality control that can detect spamming behavior. We then examine the collected data itself, analyzing annotator behavior, measuring agree-ment among annotators, and identifying interesting biases exhibited by the annotators.
In Section 5, we use the collected data to train and evaluate statistical models for several dialect identification tasks. 4.1 Annotation Interface
The annotation interface displayed a group of Arabic sentences, randomly selected from the AOC. For each sentence, the annotator was instructed to examine the sentence and make two judgments about its dialectal content: the level of dialectal content, and its type , if any. The instructions were kept short and simple: the dialect breakdown. Figure 7 shows the annotator interface populated with some actual examples, with labeling in progress. We also collected self-reported information such as native Arabic dialect and age (or number of years speaking Arabic for non-native speakers). The interface also had built-in functionality to detect each annotator X  X  geographic location based on their IP address. 110,000 sentences to be annotated for dialect.
 the segment is written in and the level of dialect in the segment. The dialect labels were
Egyptian, Gulf, Iraqi, Levantine, Maghrebi, other dialect, general dialect (for segments that could be classified as multiple dialects), dialect but unfamiliar (for sentences that are clearly dialect, but are written in a dialect that the annotator is not familiar with), no dialect (for MSA), or not Arabic (for segments written in English or other languages).
Options for the level of dialect included no dialect (for MSA), a small amount of dialect, finer-grained labels into an  X  X nnotator rationales X  model (Zaidan, Eisner, and Piatko 2007).

Workers performed our task, they were shown the 10 sentences of a randomly selected set on a single HTML page. As a result, each screen contained a mix of sentences across the three newspapers presented in random order. As control items, each screen had two additional sentences that were randomly sampled from the article bodies . Such sentences are almost always in MSA Arabic, and so their expected label is MSA. Any worker who frequently mislabeled the control sentences with a non-MSA label was considered a spammer, and their work was rejected. Hence, each screen had twelve sentences in total. 182 redundantly completed by three distinct Workers. The data collection lasted about 4.5 months, during which 33,093 Human Intelligence Task (HIT) Assignments were completed, corresponding to 330,930 collected labels (excluding control items). The total cost of annotation was $3,050.52 ($2,773.20 for rewards, and $277.32 for Amazon X  X  commission). 4.2 Annotator Behavior
With the aid of the embedded control segments (taken from article bodies) and expected dialect label distribution, it was possible to spot spamming behavior and reject it. Table 3 shows three examples of workers whose work was rejected on this basis, having clearly demonstrated they are unable or unwilling to perform the task faithfully. In total, 11.4% of the assignments were rejected on this basis. In the approved assignments, the embedded MSA control sentence was annotated with the MSA label 94.4% of the time. In the remainder of this article, we analyze only data from the approved assignments.
 was clearly problematic, opting to approve assignments from workers mentioned later in
Section 4.2.3, who exhibit systematic biases in their labels. Although these annotators X  behavior is non-ideal, we cannot assume that they are not working faithfully, and therefore rejecting their work might not be fully justified. Furthermore, such behavior might be quite common, and it is worth investigating these biases to benefit future research.
 4.2.1 Label Distribution. Overall, 454 annotators participated in the task, 138 of whom completed at least 10 HITs. Upon examination of the provided labels for the com-mentary sentences, 40.7% of them indicate some level of dialect, and 57.1% indicate being non-Arabic, non-textual, or as being left unanswered. The label breakdown is a commentary contains dialectal content. 10 classification accuracy rates that assume the presence of gold-standard class labels.
Unless otherwise noted, this majority-vote label set is used as the gold-standard in such experiments. 184
EGY instead of simply dialect ), we assign to the dialectal sentence the label corre-sponding to the news source X  X  country of publication. That is, dialectal sentences in the Jordanian (respectively, Saudi, Egyptian) are given the label LEV (respectively, GLF , EGY ).
We could have used dialect labels provided by the annotators, but chose to override those using the likely dialect of the newspaper instead. It turns out that sentences with an EGY majority, for instance, are extremely unlikely to appear in either the Jordanian or
Saudi newspaper X  X nly around 1% of those sentences have an EGY majority. In the case of the Saudi newspaper, 9% of all dialectal sentences were originally annotated as LEV but were transformed to GLF . Our rationales for performing the transformation is that no context was given for the sentences when they were annotated, and annotators had a bias towards their own dialect. We provide the original annotations for other researchers to re-analyze if they wish. the news source X  X  primary dialect, inspection of such sentences reveals that the classification was usually unjustified, and reflected a bias towards the annotator X  X  native dialect. Case in point: Gulf-speaking annotators were in relatively short supply, whereas a plurality of annotators spoke Levantine (see Table 4). Later in Section 4.2.3, we point out that annotators have a native-dialect bias, whereby they are likely to label a sentence with their native dialect even when the sentence has no evidence of being written in that particular dialect. This explains why a non-trivial number of LEV labels were given by annotators to sentences from the Saudi newspaper (Figure 8). In reality, most of these labels were given by Levantine speakers over-identifying their own dialect. Even if we were to assign dialect labels based on the (Levantine-biased) majority votes, Levantine would only cover 3.6% of the sentences from the Saudi newspaper. 12 dialect corresponding to the sentence X  X  news source, without having to inspect the specific dialect labels provided by the annotators. This not only serves to simplify our experimental set-up, but also contributes to partially reversing the native dialect bias that we observed. 4.2.2 Annotator Agreement and Performance. The annotators exhibit a decent level of agreement with regard to whether a segment is dialectal or not, with full agreement (i.e., across all three annotators) on 72.2% of the segments regarding this binary dialect / MSA [1971] for multi-rater scenarios), indicating very high agreement. percentage decreases to 56.2% when expanding the classification from a binary decision to a fine-grained scale that includes individual dialect labels as well. This is still quite a reasonable result, since the criterion is somewhat strict: It does not include a segment labeled, say, { Levantine , Levantine , General } , though there is good reason to consider that annotators are in  X  X greement X  in such a case. 186 number of sentences labeled as being dialectal (MSA), over the total number of sen-tences that have dialectal (MSA) labels based on the majority vote. Overall, human ure 9, causing some accuracy rates to drop as low as 80% or 75%. Of the annota-tors performing at least five HITs, 89.4% have accuracy rates greater than or equal to 80%.
 of them achieving at least 80% in both MSA and dialect recall. Combined with the general agreement rate measure, this is indicative that the task is well-defined X  X t is unlikely that many people would agree on something that is incorrect.
 annotators X  accuracy rate, by virtue of the construction of the gold labels. Because the correct labels are based on a majority vote of the annotators X  labels themselves, the two sets are not independent, and an annotator is inherently likely to be correct. A more informative accuracy rate disregards the case where only two of the three annotators agreed and the annotator whose accuracy was being evaluated contributed one of those two votes. In other words, an annotator X  X  label would be judged against a majority vote that is independent of that annotator X  X  label. Under this evaluation set-up, the human accuracy rate slightly decreases, to 88.0%. 4.2.3 Annotator Bias Types. Examining the submitted labels of individual workers reveals interesting annotation patterns, and indicates that annotators are quite diverse in their behavior. An annotator can be observed to have one or more of the following bias types: 14 188 5. Automatic Dialect Identification
From a computational point of view, we can think of dialect identification as language identification, though with finer-grained distinctions that make it more difficult than typical language ID. Even languages that share a common character set can be distin-guished from each other at high accuracy rates using methods as simple as examining character histograms (Cavnar and Trenkle 1994; Dunning 1994; Souter et al. 1994), and, as a largely solved problem, the one challenge becomes whether languages can be identified for very short segments.
 ing on character histograms alone is ineffective (see Section 5.3.1), and more context is needed. We will explore higher-order letter models as well as word models, and determine what factors determine which model is best. 5.1 Smoothed n -Gram Models
Given a sentence S to classify into one of k classes C 1 , C with the maximum conditional probability: classes, which is estimated from the training set. The training set is also used to train probabilistic models to estimate the probability of S given a particular class. We rely on training n -gram language models to compute such probabilities, and apply Kneser-
Ney smoothing to these probabilities and also use that technique to assign probability mass to unseen or OOV items (Chen and Goodman 1998). In language model scoring, a sentence is typically split into words. We will also consider letter -based models, where the sentence is split into sequences of characters. Note that letter-based models would be able to take advantage of clues in the sentence that are not complete words, such as prefixes or suffixes. This would be useful if the amount of training data is very small, or if we expect a large domain shift between training and testing, in which case content words indicative of MSA or dialect might not still be valuable in the new domain. is thus relatively simple, it is nevertheless very effective. Experimental results in Sec-only slightly behind the human accuracy rate of 88.0% reported in Section 4.2.2. 5.2 Baselines
To properly evaluate classification performance trained on dialectal data, we compare the language-model classifiers to two baselines that do not use the newly collected data.
Rather, they use available MSA-only data and attempt to determine how MSA-like a sentence is.
 a higher percentage of  X  X on-MSA X  words that cannot be found in a large MSA corpus.
To this end, we extracted a vocabulary list from the Arabic Gigaword Corpus, producing a list of 2.9M word types. Each sentence is given a score that equals the OOV percentage, and if this percentage exceeds a certain threshold, the sentence is classified as being dialectal. For each of the cross validation runs in Section 5.3.1, we use the threshold much a boost as possible). In our experiments, we found this threshold to be usually around 10%.
 using MSA-only data, and use it to score a test sentence. Again, if the perplexity exceeds a certain threshold, the sentence is classified as being dialectal. To take advantage of domain knowledge, we train this MSA model on the sentences extracted from the article bodies of the AOC, which corresponds to 43M words of highly relevant content. 5.3 Experimental Results
DID systems, and show that they outperform other baselines that do not utilize the annotated data. 5.3.1 Two-Way, MSA vs. Dialect Classification. We measure classification accuracy at vari-ous training set sizes, using 10-fold cross validation, for several classification tasks. We examine the task both as a general MSA vs. dialect task, as well as when restricted within a particular news source. We train unigram, bigram, and trigram (word-based) models, as well as unigraph, trigraph, and 5-graph (letter-based) models. Table 6 sum-marizes the accuracy rates for these models, and includes rates for the baselines that do not utilize the dialect-annotated data.
 slightly behind (Figure 11). Bigram and trigram word models seem to suffer from the sparseness of the data and lag behind, given the large number of parameters they 190 would need to estimate (and instead resort to smoothing heavily). The letter-based models, with a significantly smaller vocabulary size, do not suffer from this problem, and perform well. This is a double-edged sword though, especially for the trigraph model, as it means the model is less expressive and converges faster.
 be it word-or letter-based, over baselines that use existing MSA-only data. Whichever model we choose (with the exception of the unigraph model), the obtained accuracy rates show a significant dominance over the baselines.
 2009), which points out that the only  X  X nteresting X  aspect of the problem is performance on short segments. The same is true in the case of dialect identification: a short sentence that contains even a single misleading feature is prone to misclassification, whereas a long sentence is likely to have other features that help identify the correct class label. task in the Saudi newspaper than in the Jordanian paper, which in turn is harder than in the Egyptian newspaper. This might be considered evidence that the Gulf dialect is the closest of the dialects to MSA, and Egyptian is the farthest, in agreement with sentences tend to be significantly shorter X  X he ease of distinguishing Egyptian holds even at higher sentence lengths, as shown by Figure 12. 5.3.2 Multi-Way, Fine-Grained Classification. The experiments reported earlier focused on distinguishing MSA from dialect when the news source is known, making it straightforward to determine which of the Arabic dialects a sentence is written in (once 192 the sentence is determined to be dialectal). If the news source is not known, we do not important to evaluate our approach in a multi-way classifiation scenario, where the class set is expanded from { MSA , dialect } to { MSA , LEV , GLF , EGY 81.0%. 16 The drop in performance is not at all surprising, since four-way classification is inherently more difficult than two-way classification. (Note that the classifier is trained on exactly the same training data in both scenarios, but with more fine-grained dialectal labels in the four-way set-up.) when the classifier tends to make mistakes. We note here that most classification errors on dialectal sentences occur when these sentences are mislabeled as being MSA, not when they are misidentified as being in some other incorrect dialect. In other words, dialect  X  dialect confusion constitutes a smaller proportion of errors than dialect confusion. Indeed, if we consider a three-way classification setup on dialectal sentences alone ( LEV vs. GLF vs. EGY ), the classifier X  X  accuracy rate shoots up to 88.4%. This is a higher accuracy rate than for the general two-way MSA vs. dialect classification (85.7%), despite involving more classes (three instead of two), and being trained on less data (0.77M words instead of 1.78M words). This indicates that the dialects deviate from
MSA in various ways, and therefore distinguishing dialects from each other can be done even more effectively than distinguishing dialect from MSA. 5.3.3 Word and Letter Dialectness. Examining the letter and word distribution in the corpus provides valuable insight into what features of a sentence are most dialectal. Let DF ( w ) denote the dialectness factor of a word w , defined as: where count D ( w ) (respectively, count MSA ( w )) is the number of times w appeared in the dialectal (respectively, MSA) sentences, and count D ( . ) is the total number of words in those sentences. Hence, DF ( w ) is simply a ratio measuring how much more likely w is to appear in a dialectal sentence than in an MSA sentence. Note that the dialectness factor can be easily computed for letters as well, and can be computed for bigrams/bigraphs, trigrams/trigraphs, and so forth.
 dialectness factor. The most dialectal words tend to be function words, and they also tend to be strong indicators of dialect, judging by their very high DF . On the other hand, the MSA word group contains several content words, relating mainly to politics and religion.
 relative frequencies of dialect/MSA, but does not capture how often the word occurs in the first place. Figure 14 plots both measures for the words of Al-Ghad newspaper. The 194 farthest away from the point of origin, along both dimensions.
 essentially the same words important to the unigram word model. The letter-based models are, however, able to capture some linguistic phenomenon that the word model and the prefixes H +( will in Egyptian), bt + (present tense conjugation in Levantine and Egyptian), and y + (present tense conjugation in Gulf).
 when using dialect. Namely, there is closer attention to following hamza rules (distin-guishing A ,  X  A ,and  X  A from each other, rather than mapping them all to A ), and better adherence to (properly) using + instead of + h at the end of many words. There is also a higher tendency to use words containing the letters that are most susceptible to being transformed when pronounced dialectally:  X  (usually pronounced as z ), as D ), and  X  (pronounced as t ).
 text before training language models might enhance coverage and therefore improve performance. For instance, would it help to map all forms of the alef hamza to a single letter, and all instances of to h , and so on? Our pilot experiments indicated that such normalization tends to slightly but consistently hurt performance, so we opted to leave the Arabic text as is. The only type of preprocessing we performed was more on the  X  X leanup X  side of things rather than computationally motivated normalization, such as proper conversion of HTML entities (e.g., &amp;quot; to " ) and mapping Eastern Arabic numerals to their European equivalents. 6. Applying DID to a Large-Scale Arabic Web Crawl
We conducted a large-scale Web crawl to gather Arabic text from the on-line versions of newspapers from various Arabic-speaking countries. The first batch contained 319 on-line Arabic-language newspapers published in 24 countries. This list was compiled from http://newspapermap.com/ and http://www.onlinenewspapers.com/ , which are Web sites that show the location and language of newspapers published around the world.
The list contained 55 newspapers from Lebanon, 42 from Egypt, 40 from Saudi Arabia, 26 from Yemen, 26 from Iraq, 18 from Kuwait, 17 from Morocco, 15 from Algeria, 12 from Jordan, and 10 from Syria. The data were gathered from July X  X ept 2011.
 and directories. We identified 3,485,241 files that were likely to contain text by selecting the extensions htm, html, cmff, asp, pdf, rtf, doc, and docx. We converted these files to text using xpdf X  X  pdftotext for PDFs and Apple X  X  textutil for HTML and Doc files.
When concatenated together, the text files contained 438,940,861 lines (3,452,404,197 words). We performed de-duplication to remove identical lines, after which 18,219,348 lines (1,393,010,506 words) remained.

Arabic varieties ( MSA , LEV , GLF , EGY ), as described in the previous section. We used these models to classify the crawled data, assigning a given sentence the label corresponding to the language model under which that sentence received the highest score. Table 8 gives the resulting label breakdown. We see that the overwhelming majority of the sentences are classified as MSA, which comes as no surprise, given the prevalence of MSA in the newspaper genre. Figure 16 shows some sentences that were given non-
MSA labels by our classifier. 7. Related Work
They present pilot annotation results on a small set of around 1,600 Arabic sentences (19k words), with both sentence-and word-level dialectness annotations. 196 large-scale effort to create dialectal Arabic resources (and tools). They too focus on on-line sources such as blogs and forums, and use information retrieval tasks to measure their ability to properly process dialectal Arabic content. The COLABA project demon-strates the importance of using dialectal content when training and designing tools that deal with dialectal Arabic, and deal quite extensively with resource creation and data harvesting for dialectal Arabic.
 using any significant amount of dialectal data. They utilize an available Levantine X  X SA lexicon, but no parses of Levantine sentences. Their work illustrates the difficulty of adapting MSA resources for use in a dialectal domain.
 machine translation system vastly improves the quality of the translation of dialect sentences when compared to a system trained solely on an MSA-English parallel cor-pus. When translating Egyptian and Levantine test sets, a dialect Arabic MT system outperforms a Modern Standard Arabic MT system trained on a 150 million word
Arabic X  X nglish parallel corpus X  X ver 100 times the amount of data as their dialect parallel corpus.
 bic text. However, Lei and Hansen (2011) and Biadsy, Hirschberg, and Habash (2009) in-vestigate Arabic dialect identification in the speech domain. Lei and Hansen (2011) build
Gaussian mixture models to identify the same three dialects we consider, and are able to achieve an accuracy rate of 71.7% using about 10 hours of speech data for training. speech data) and take a phone recognition and language modeling approach (Zissman 1996). In a four-way classification task (with Iraqi as a fourth dialect), they achieve a 78.5% accuracy rate. It must be noted that both works use speech data, and that dialect identification is done on the speaker level, not the sentence level as we do. 8. Conclusion Social media, like reader commentary on on-line newspapers, is a rich source of dialectal
Arabic that has previously not been studied in detail. We have harvested this type of resource to create a large data set of informal Arabic that is rich in dialectal content. We selected a large subset of this data set, and had the sentences in it manually annotated for dialect. We used the collected labels to train and evaluate automatic classifiers for dialect identification, and observed interesting linguistic aspects about the task and annotators X  behavior. Using an approach based on language model scoring, we develop classifiers that significantly outperform baselines that use large amounts of MSA data, and we approach the accuracy rates exhibited by human annotators.
 features of the Arabic text, by incorporating analyses given by automatic analyzers such as BAMA (Buckwalter 2004), MAGEAD (Habash and Rambow 2006), ADAM (Salloum and Habash 2011), or CALIMA (Habash, Eskander, and Hawwari 2012). Although the difference between our presented approach and human annotators was found to be relatively small, incorporating additional linguistically motivated features might be pivotal in bridging that final gap.
 content, such as specific annotation for why a certain sentence is dialectal and not MSA:
Is it due to structural differences, dialectal terms, and so forth? We also hope to expand beyond the three dialects discussed in this article, by including sources from a larger number of countries.
 dialectal Arabic are Twitter posts (e.g., with the #Egypt tag) and discussions on various political Facebook groups. Here again, given the topic at hand and the individualistic nature of the posts, they are very likely to contain a high degree of dialectal data. 198 Appendix A
The Arabic transliteration scheme used in the article is the Habash-Soudi-Buckwalter transliteration (HSBT) mapping (Habash, Soudi, and Buckwalter 2007), which extends the scheme designed by Buckwalter in the 1990s (Buckwalter 2002). Buckwalter X  X  origi-nal scheme represents Arabic orthography by designating a single, distinct ASCII char-acter for each Arabic letter. HSBT uses some non-ASCII characters for better readibility, but maintains the distinct 1-to-1 mapping.
 sections: vowels, forms of the hamzah (glottal stop), consonants, and pharyngealized  X  is a pharyngealized glottal stop, which is supported by Gairdner (1925), Al-Ani (1970), consonants. Pharyngealized consonants are  X  X hickened X  versions of other, more familiar consonants, voiced such that the pharynx or epiglottis is constricted during the articula-tion of the sound. Those consonants are present in very few languages and are therefore likely to be unfamiliar to most readers, which is why we place them in a separate section X  X here is no real distinction in Arabic between them and other consonants. because those diacritics are only rarely expressed in written (and typed) form, we omit them for brevity.
 Acknowledgments References 200
