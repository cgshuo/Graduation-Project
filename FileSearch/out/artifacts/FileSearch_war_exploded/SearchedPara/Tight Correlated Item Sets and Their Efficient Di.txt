 Frequent item sets and association rule mining will find co-occurrence relationship of frequent items, while correlation mining will find correlated patterns not only and customer segmentation, climate studies, gene expression analysis, and so on. However, most existing algorithms are too expensive in run time cost to be practical difficult for user to capture the relationship of these rules or patterns. 
In order to overcome the shortcomings of classical association and correlation algorithm that will be practical for applications. For example, in market-basket data set, correlated item set, items are correlated each other. Thus, each tight correlated item set includes several correlated items that have common correlated patterns. The classical association rule framework use a minimum support threshold  X  to control the minimum number of data cases that a rule must cover, and a minimum confidence threshold  X  to control the predictive strength of the rule. Item sets with support above  X  are called frequent item sets and will be kept in the mining process, while others are those frequent item sets. The first problem for theses settings is the rare item dilemma [2] that is a low minimum support will typically generate many rules of no interest, but that some association rules with confidence above  X  are actually misleading [3]. For an Noticed that SUP(AB)/SUP(A ) is probability of A in condition B, SUP(B) is probability of B, if SUP(AB)/SUP(A)&lt;SUP(B) , rule A  X  B is wrong. 
Recently, efforts have been devoted to address these problems. Correlation mining methods are developed to overcome difficulties of association mining. Instead of dependency between items. Mining correlations does not rely on the support measure confidence measure and two other claimed correlation measures: lift and  X  2 . Xiong et correlations based on information theory. In general, our research focuses on deve loping techniques of correlation mining. We propose tight correlated item set to re present items that share common correlated efficiently from large data sets. probability, and explore Cosine  X  X  other properties. 
The second aspect is about algorithm X  X  efficiency. Suppose we have a database with m items (columns) and n transactions (rows), there are computing correlations becomes expensive. So we explore the relationship between Cosine and support of items, and prove a support boundary for correlated item pairs. ranges. Thus, we develop a support regional query technique to optimize the algorithm of computing correlation. 
The third aspect is about tight correlated item sets. If we get these correlated item pairs, it X  X  easy to know the correlated neighbors for each individual item. But can we get the tight correlated item sets from correlated item pairs easily? Unfortunately, the complexity of algorithm to get the whole tight correlated item sets is NP-hard. However, for many applications, we don X  X  need to get the whole tight correlated item correlated pairs by increasing correlation threshold  X  . The algorithm is fast enough to be applicable to applications with a great number of items. Its price is that the results explain that any item A and its correlated members form an upper bound of the tight correlated item sets containing item A . 
In summary, our work has the following contributions. 1. This paper proposes a method to mine tight correlated item sets from binary patterns. Items in a tight correlated item set are tightly correlated each other. Mining tight correlated item sets is useful for a lot of applications. 2. In order to improve algorithm X  X  efficiency, we prove Lemma 1 and Lemma 2 in used to prune unrelated item pairs in correlation computation. Lemma 2 is about transitivity of correlations, which can be used to get tight correlated item sets directly and provide an upper bound of the complete results. 3. Utilizing Lemma 1 and Lemma 2, we design RSC algorithm. Analysis and experiments are also performed to examine the efficiency and effects of RSC algorithm. some basic terminologies and concepts, and explains the reason that we choose Cosine as the correlation measure. Section 3 deeply examines the properties of Cosine section, we summarize our work. In this section, we present formal definitions and terminology about correlation measure Cosine and tight correlated item sets, and describe the concepts briefly. transaction data and its corresponding matrix are illustrated as the follow (Fig. 1.(a)). Definition 1. Cosine coefficient between items defined as: binary variables. 
SUP(A) is the support of item A , SUP(B) is the support of item B , and SUP(AB) is ( A under condition A . 0  X  Cos(A, B)  X  1. When Cos(A, B) is small, items A and B have little chance to occur more chance to occur together, and their correlation is strong. Thus, we chose Cosine as measure of correlation between items. specified threshold  X  . Definition 2.  X  -Correlated-Items for item J ( CI  X  (J) ) 
Given item J  X  I , CI  X  (i s ) ={ X | Cos(X, J)  X   X  , X  X  I }. Definition 3.  X  -Correlated-Item-Pair ( CIP  X  ) Definition 4.  X  -Tight-Correlated-Item-Set ( TCIS  X  )  X  -Tight-Correlated-Item-Set ( TCIS  X  ). 
From definition 2, 3, 4, we can directly get properties as the follow. (2) A CIP  X  is a 2-items TCIS  X  , and any item pairs from a TCIS  X  is a TCIS  X  . 
Our target is to mine tight correlated item sets TCIS  X  from database, such that items in a TCIS  X  are correlated each other for threshold  X  . In this section, we first introduce the correlation measure Cosine and examine its two discover tight correlated item sets. 3.1 Correlation Properties the ratio of the base to the hypotenuse. In data mining area, Cosine often acts as the measure of similarity between vectors. According to the Cosine rule, big Cosine they have. In order to apply Cosine to binary variables for correlation mining, we first explore the relationship between Cosine and support . Lemma 1. Correlation X  X  support boundary value SUP(B) for item B , we have Proof: (omitted) We can utilize Lemma 1 to reduce the search space when computing correlations for expressed as Cos(A, B)  X  with support below  X  2 *SUP(A) are unrelated with item A. When computing correlated items for item A, we need only to test those items whose support measures are in range of [ SUP(A) ,  X  2 *SUP(A) ]. In a naive algorithm, the searching range is [ SUP(A) , 0 ]. Lemma 2. Correlation X  X  attenuated transitivity Cos(B, C)  X  2  X  2 -1 (4) Proof: (omitted) a tight correlated item set in threshold 2  X  2 -1 . Proof: (omitted) item set I . Proof: (omitted) Using these properties, we design the Regional-Searching-Correlations ( RSC ) algorithm. 3.2 RSC Algorithm RSC algorithm first scans database to compute support for every item, and then sorts the items by their support in a descending order ( i 1 , i 2 , ..., i m ). 
In the next steps, RSC algorithm computes correlated item pairs for items from i 1 to i m-1 . According to Lemma 1, correlated item pair has a support boundary. For item i  X  *SUP(i 2 ) ], ..., and [ SUP(i m-1 ) ,  X  2 *SUP(i m-1 ) ]. item X  X  support and sort items by their support in a descending order. The sorted items boundary is [ 7/8 , 7/16 ]; the last item to be examined is item 3. Thus, we only compute results and the pruned search space are illustrated in Fig. 2. our skill is increasing threshold from  X  to  X  = { i upper bound of TCIS  X  . Running RSC algorithm with original threshold  X  , we can get the upper bound of TCIS  X  . Specifically, we will 1) compare the performance of RSC algorithm, TAPER regional searching technique; 3) test the scalability of RSC algorithm. 
In our experiments, we use synthetic data sets and real data sets. Synthetic data set benchmark for evaluating the performance of association rule algorithms on dense data sets. Pumsb data set has 2113 items and 49046 records. Pumsb* data set does not contain items with support greater than 80%, and it has 2089 items and 49046 records. 
We implement RSC algorithm using Microsoft visual c++ 6.0, and run the program on the windows XP platform with a 1.7 GHz CPU and 512 MB main memory. First, we compare the relative computation performance of RSC algorithm with ( m =5000, n =50000). As can be seen in Fig. 3, the na X ve algorithm X  X  run time doesn X  X  The run time cost of RSC and TAPER decreases as we increase the threshold  X  . Both of RSC and TAPER are faster than the na X ve approach, and the performance of RSC is much better that of TAPER. n =50000). The pruning ration is defined to be: 2 understand the reason that RSC algorithm has high performance. number of items at a high minimum correlation threshold. RSC algorithm has a good scalability for high dimensional database. efficiency of algorithm. 
Correlation X  X  support boundary and Correlation X  X  attenuated transitivity are two interesting properties of the selected Cosine function, which can be utilized to design the algorithm. Using Correlation X  X  support boundary , we can determine the unrelated item pairs according to item X  X  support, and then exclude them from computation. complete result. 
Generally, mining tight correlated item sets will find correlated patterns where helpful for a wide range of applications. This paper provides a method to be practical for such applications. 
