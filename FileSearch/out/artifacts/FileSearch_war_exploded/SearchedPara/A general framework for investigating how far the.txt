 An ultimate goal of neuroscience is to elucidate how informa tion is encoded and decoded by neural amount of data and many complex computations are needed for o ptimal decoding, the assumption of an optimal decoder in the brain is doubtful.
 mal decoder is that we are completely ignorant of how informa tion is decoded in the brain. Thus, we simply evaluate the maximal amount of information that ca n be extracted from neural activities by calculating the mutual information. To address this lack of knowledge, we can ask a different question:  X  X ow much information can be obtained by a decoder that has partial knowledge of the encoding process? X  [10, 14] We call this type of a decoder  X  X i mplified decoder X  or a  X  X ismatched that maximum likelihood estimation can be implemented by a b iologically plausible network [2, 4]. dependent decoder, we can say that the brain may function in a manner similar to the independent decoder. In this context, Nirenberg et al. computed the amount of information obtained by the in-cells showed a loss of information greater than 11%. Because only pairs of cells were considered activities.
 When population activities are analyzed, we have to deal with not only second-order correlations how much information is obtained by the simplified decoders, we investigate how many orders of rived by Merhav et al. [8]. Information for mismatched decoders previously propo sed by Nirenberg and Latham is the lower bound on the correct information [5, 1 1]. Because this lower bound can be in the paper, we need to accurately evaluate the information obtained by mismatched decoders. The plan of the paper is as follows. In Section 2, we describe a way of computing the information that can be extracted from neural activities by mismatched d ecoders using the information derived by Merhav et al. . Using analytical computation, we demonstrate how informa tion for mismatched decoders previously proposed by Nirenberg and Latham diffe rs from the correct information derived constructing simplified decoders by using the maximum entro py principle [12]. We then compute the information obtained with the simplified decoders. We find th at more than 90% of the information are ignored in decoding. We also describe the problem of prev ious studies [10, 12] in which the assumption. Let us consider how much information about stimuli can be ext racted from neural responses. We information obtained with the optimal decoder can be evalua ted by using the mutual information: where p ( r ) = P encoding model p ( r | s ) by using an equation derived by Merhav et al. [8]: call a decoder using the mismatched decoding model a  X  X ismat ched decoder X . Figure 1: Comparison between correct information I  X  derived by Merhav et al. and Nirenberg-Gaussian model where correlations and derivatives of mean fi ring rates are uniform. Correlation parameter c = 0 . 01 . B: Difference between I  X  data in Figure 3A are used. For this spike data and other spike data analyzed, Nirenberg-Latham information provides a tight lower bound on the correct info rmation, possibly because the number of cells is small.
 Previously, Nirenberg and Latham proposed that the informa tion obtained by mismatched decoders can be evaluated by using [11] We call their proposed information  X  X irenberg-Latham info rmation X . If we set  X  = 1 in Eq. 2, we obtain Nirenberg-Latham information, I  X  (1) = I NL . Thus, Nirenberg-Latham information mation, I  X  (  X  ) , which is the maximum value with respect to  X  [5, 8]. The lower bound provided by Nirenberg-Latham information can be very loose and the Ni renberg-Latham information can be negative when many cells are analyzed.
 Theoretical evaluation of information I , I  X  , and I NL We consider the problem where mutual information is compute d when stimulus s , which is a single to as mismatched decoders and the Nirenberg-Latham information I NL can be written as quantity for mismatched decoders. Let us consider the case in which the encoding model p ( r | s ) obeys the Gaussian distribution where C 0 . If the Gaussian integral is performed for Eqs. 4-5, I , I  X  , and I NL can be written as The correct information obtained by the independent decode r for the Gaussian model (Eq. 10) is inversely proportional to the decoding error of s when the independent decoder is applied, which was computed from the generalized Cram  X  er Rao bound by Wu et al. [14].
 As a simple example, we consider a uniform correlation model [1, 14] in which covariance matrix C is given by C that is f  X  information I  X  and Nirenberg-Latham information I NL is very large when the number of cells N is large. When N &gt; c +1 many cells are analyzed. 3.1 Methods We analyzed the data obtained when N = 7 retinal ganglion cells were simultaneously recorded using a multielectrode array. The stimulus was a natural mov ie, which was 200 s long and repeated 45 times. We divided the movie into many short natural movies and considered them as stimuli over not in each time bin with a binary variable:  X  an N -letter binary word,  X  = {  X  mation of spike trains into binary words. the conditional probability distribution p probabilities, we evaluated the information contained in N -letter binary words  X  . Generally, the joint probability of N binary variables can be written as [9]  X  , we can determine the values of parameters so that the log-li near model p empirical probability distribution p To compute the information for mismatched decoders, we cons truct simplified models of neural responses that partially match the empirical distribution , p pendent model X  p h  X  straints. In accordance with the maximum entropy principle [12], we choose the one that maximizes entropy H , H =  X  P corresponds to a log-linear model in which all orders of corr elation parameters {  X  are omitted. If we perform maximum likelihood estimation of model parameters  X  (1) in the log-linear model, the result is that the average  X  found in the data: that is, h  X  the maximum entropy model. Generally, the maximum entropy m ethod is equivalent to maximum likelihood fitting of a log-linear model [6].
 Similarly, we can consider a  X  X econd-order correlation mod el X  p only the averages of  X  entropy with constraints h  X  described above can also be used to construct a  X  K th-order correlation model X  p tute the simplified models of neural responses p 100 -ms-long natural movie Eq. 2, we can compute the amount of information that can be obt ained when more than K th-order correlations are ignored in the decoding, By evaluating the ratio of information, I  X  be taken into account to extract enough information. 3.2 Results First, we investigated how the ratio of information obtaine d by an independent model, I  X  obtained by a second-order correlation model, I  X  number of cells simultaneously recorded. We computed the av erage value of I  X  all possible combinations of cells. Figure 3A shows that I  X  when the number of cells was increased. A comparison between the correct information, I  X  Nirenberg-Latham information, I NL only two cells were considered, I  X  [10]. However, when all cells ( N = 7 ) were used in the analysis, I  X  Thus, correlation seems to be much more important for decodi ng when population activities are things occur when large populations of cells are analyzed, a s Schneidman et al. pointed out [12]. during each stimulus. By stationarity we mean that we assume d spikes are generated by a single change much more rapidly and our visual system has much highe r time resolution than 10 s [13], we also considered shorter stimuli. In Figure 3B, we compute d I  X  the length of each stimulus was 100 ms, no spikes occurred whi le some stimuli were presented. We information obtained by independent model I  X  were considered. Although 100 ms may still be too long to be co nsidered as a single process, the that reflected in Figure 3A. Figure 4A shows the dependence of information obtained by simplified ms to 10 s and computed I  X  experimental data obtained when N = 6 retinal ganglion cells were simultaneously recorded from  X  Figure 4: Dependence of amount of information obtained by si mplified decoders on length of stim-different salamander were used in panels A and B. A: Seven sim ultaneously recorded ganglion cells B: Six simultaneously recorded ganglion cells C: Artificial spike data generated according to the firing rates shown in Figure 5A  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  line) when stimulus was 500 ms long. another salamander retina. The same 200-s-long natural mov ie was used as a stimulus for Figure 4B can clearly see the same tendency as shown in Figures 4A and B: the amount of information decoded by the simplified decoders monotonically increased as the le ngth of the stimulus was shortened. the toy model shown in Figure 5. We considered the case in whic h two cells fire independently in accordance with a Poisson process and performed an analys is similar to the one we did for the We divided the 2-s-long stimulus into two 1-s-long stimulus , s Then, we computed mutual information I and the information obtained by independent model I  X  over s dynamically changing stimulus. The pseudo correlation was high for s that  X  X orrelation X  plays an important role in discriminati ng two stimuli, s mean firing rates of the two cells during each stimulus were eq ual for s stimulus is 1 s long, we cannot discriminate two stimuli by us ing the independent model, that is, I 1 = 0 the information. The dependence of I  X  it may simply be caused by meaningless pseudo correlation. T o assess the role of correlation in stimuli generated by a single process. We described a general framework for investigating how far t he decoding process in the brain can be simplified. We computed the amount of information that can be extracted by using simplified that more than 90% of the information encoded in retinal gang lion cells activities can be decoded by using an independent model that ignores correlation. Our results imply that the brain uses a simplified decoding strategy in which correlation is ignore d.
 When we computed the information obtained by the independent model, we regarded a 100-ms-long natural movie as one stimulus. However, when we considered s timuli that were long compared with durations. The human visual system can process visual infor mation in less than 150 ms [13]. We into account.
 Our results do not imply that any kind of correlation does not carry much information because we investigations are needed for these types of correlation. O ur approach of comparing the mutual other types of correlations.
 [1] Abbott, L. F., &amp; Dayan, P. (1999). Neural Comput. , 11, 91-101. [2] Deneve, S., Latham, P. E., &amp; Pouget, A. (1999). Nature Neurosci. , 2, 740-745. [3] Gollish, S., &amp; Meister, M. (2008). Science , 319, 1108-1111. [4] Jazayeri, M. &amp; Movshon, J. A. (2006). Nature Neurosci. , 9, 690-696. [5] Latham, P. E., &amp; Nirenberg, S. (2005). J. Neurosci. , 25, 5195-5206. [6] MacKay, D. (2003). Information Theory, Inference and Learning Algorithms (Cambridge Univ. [7] Meister, M., &amp; Berry, M. J. II (1999). Neuron , 22, 435-450. [9] Nakahara, H., &amp; Amari, S. (2002). Neural Comput. , 14, 2269-2316. [11] Nirenberg, S., &amp; Latham, P. (2003). Proc. Natl. Acad. Sci. USA , 100, 7348-7353. [13] Thorpe, S., Fize, D., &amp; Marlot, C. (1996). Nature , 381, 520-522. [14] Wu, S., Nakahara, H., &amp; Amari, S. (2001). Neural Comput. , 13, 775-797.
