 Analyzing three-way data has attracted a lot of attention recently due to the intrinsic rich structures in real-world datasets. The PAR-ATUCKER model has been proposed to combine the axis capa-bilities of the Parafac model and the structural generality of the Tucker model. Since no algorithms have been developed for fitting the PARATUCKER model, in this paper, we propose TANPT al-gorithm to solve it, and apply it to temporal relation co-clustering on author-topic evolution. Experiments on DBLP datasets demon-strate the effectiveness of our proposed algorithm.
 Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering-Algorithms General Terms: Algorithms, Experimentation, Performance Keywords: Tensor, Three-way data, Author-Topic Evolution
Three-way data naturally appear in many applications. For ex-ample, in document clustering, the data can be represented as a three-way dataset as author  X  terms  X  time , and in email commu-nications, the data can be represented as sender  X  receiver  X  time . features, occasions. The c -th frontal slice of the three-way data is X way data can be matricized in the n -th mode to form a flattened matrix X ( n ) . The sum-up matrix of three-way data X is defined as X =  X  Three-way models such as the well-known Parafac model and Tucker3 model [5] were proposed to specifically analyze three-way data. Recently, PARATUCKER model, a general parallel factor model that combines the axis capabilities of the Parafac model with some structural generality of the Tucker model, is proposed [5]. Two-way PARATUCKER model can be seen as a 3-factor matrix decomposition that X ARB T [9]. This model provides a good framework for simultaneously co-clustering the rows and columns of X . The three-way PARATUCKER model extends the two-way PARATUCKER by considering the third mode such as time.
Although the PARATUCKER model has been proposed, no al-gorithms have been developed for fitting the PARATUCKER model. In this paper, we propose TANPT , a three-way alternating algo-rithm to solve the PARATUCKER model. We also apply the algo-rithm for author-topic evolution analysis. Experiments on DBLP datasets demonstrate the effectiveness of our proposed algorithm.
Update  X  D and  X  D : The Newton X  X  method is adopted to update each frontal slice of multi-array  X  D and  X  D . To derive the gradients, the objective function is first transformed into Then the gradients of the objective function on  X  D and  X  D can be derived. To update  X  D , according to Newton X  X  method, where  X  D c and  X  D c are gradients, H c is the inverse of Hessian and calculated using BFGS in each step.  X  c is the step size chosen to satisfy the Wolf conditions. We use a standard projected gradient descent method [3] to project  X  D c and  X  D c into positive domain. The updating procedure of  X  D can easily refer to  X  D .
 TANPT starts with positive random initializations of A and B . Each  X  D c and  X  D c are initialized to identity matrices. The matrix R is computed first. Then A , B ,  X  D ,and  X  D are updated one by one.
In this section, we apply TANPT on the DBLP datasets to iden-tify the author-topic relations evolution along the time. The datasets are extracted from the DBLP computer science bibliography that can be downloaded at http://www.informatik.uni-trier.de/  X  ley/db/ . The data are three-way arrays with the author, term, and year modes. We conduct experiments on two such three-way datasets, one of which is DBLP1000 (1000 authors  X  1000 terms  X  20 years), the other of which is DBLP100 (100 authors  X  200 terms  X  20 years). We use Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and Accuracy (ACC) as our performance measures. Generally, the larger the values of these measures, the better the clustering performance.

The clustering performance of TANPT is compared with a wide range of clustering algorithms. Tensor-factorization methods: (1) Parafac [7]; (2) Tucker-3 [7]; Two-way data clustering methods: (3) KMeans(sum) : K-Means on the sum-up matrix (authors  X  terms); (4) KMeans(ext) : K-Means on the unfolded matrix in the first mode of the three-way array; (5) KMeans(pca) : Perform PCA first on the unfolded matrix in the first mode and then use K-Means algorithm; (6) InfoCo : Run information theoretic co-clustering algorithm [4] on the sum-up matrix; (7) EuclCo : Run Euclidean co-clustering algorithm [2] on the sum-up matrix; (8) MinSqCo : Performs min-imum squared residue co-clustering algorithm [2] on the sum-up matrix. (9) ClusterAgg : Run K-Means clustering on each frontal slice of the three-way array, and combine them using clustering ag-gregation algorithm [8]. In the experiments, the clustering results are computed by averaging ten runs. The experimental results are presented in Table 1. The values of the best clustering performance based on each measure are highlighted with the bold numbers. We
