 1. Introduction
Online reviews play an important role in online advertising and marketing ( Barton, 2006 ). Online reviews have been rec-ognized as a type of word-of-mouth that helps managers and manufacturers in brand building, product development, and quality assurance. According to recent social commerce statistics from Kelton Research / Bazaarvoice , 83% of consumers believed that it is important to read user-generated content before making a decision about banking or other financial ser-vices. As the volume and velocity of online reviews rapidly increases, it becomes ever more challenging for customers to read through entire reviews. Aggregated ratings alone do not address the above challenge for two main reasons: (1) customers really care about aspects (i.e., parts and attributes) of a product, and (2) customers have their own preferences for aspects reviews is instrumental for leveraging the online word-of-mouth for individual and business decision making.
Extracting product aspects from online consumer reviews remains a difficult task due to some unique characteristics of online reviews such as unstructured text, and the colloquial and casual style of Internet language ( Pollach, 2005 ). Product reviews are commonly written by consumers, not paid professionals. The extant methods for product aspect extraction from online reviews can be classified into four main categories ( Liu, 2012, chap. 5 ): (1) extraction based on frequent nouns and  X 
Huang, &amp; Zhou, 2007 ). The first type of method (e.g., Apriori) is focused on those aspects that occur frequently, which depends on pruning methods to improve the relevance of the extraction results. The second type (e.g., double propagation) relies on the dependency relationship to propagate information between aspects and opinions. Although such a method is capable of dealing with infrequent aspects, noise as well as information could be propagated. The third type is most com-monly used, which generally outperforms its unsupervised counterpart. Nevertheless, it requires training data, which is an inherent limitation of supervised methods. Annotating online reviews with product aspects is both labor-intensive and time-consuming, particularly in light of the high variety and volume of products and high dimensions of product aspects.
The last type of method is aimed at discovering the main themes that pervade a large and otherwise unstructured collection of documents by building topic models ( Blei, 2012 ). The topics discovered by such a method generally contain both aspects and opinions about product entities, so additional work is required to separate the two types of information. In this research, we propose a method for product aspect extraction by augmenting frequency-based extraction with PMI-
IR. PMI-IR ( Turney, 2001 ) is used to measure the semantic similarity between aspect candidates and product entities. Com-pared with previous frequency-based methods, our proposed method has several advantages. First, it leverages a universal search engine rather than a static collection of online reviews to estimate the similarities between candidate aspects and an entity. A universal search engine routinely maintains the coverage and freshness of its content, which enables more updated and complete estimates of similarities with no extra effort. Second, it prunes frequent aspect candidates based on the PMI-IR score between a candidate aspect and the target entity under review instead of between the candidate and multiple discrim-inator phrases, which improves the efficiency of aspect extraction that uses PMI. Third, it only requires small datasets for threshold learning, which can easily scale up. Our experiment results show that the proposed methods outperform the state-of-the-art frequency-based method.

The rest of this paper is organized as follows. In Section 2 , we provide background and review related work on aspect extraction. In Section 3 , we introduce the experiment design in detail, followed by results and discussion in Section 4 .We conclude the paper with Section 5 . 2. Background and related work
The process of product review mining mainly consists of the following steps ( Popescu et al., 2005 ): identify product aspects, identify opinions regarding product aspects, determine the polarity of opinions, and rank opinions based on their strength. Thus, product aspect extraction is fundamental to review mining. Before providing a systematic review of aspect extraction methods, we define some key concepts based on the previous literature ( Hu &amp; Liu, 2004a, 2004b; Liu, 2012, chap. 5; Liu, Wu, &amp; Yao, 2006 ).
 in an online review. Entity e can be represented as a two-element vector: e =( P , AT ), where P denotes a hierarchy of parts which is organized based on semantic relations, and AT denotes a set of attributes of e .
 Definition (aspect): Aspect a 2 (P [ AT) is described as a part or an attribute of entity e .

For example, cell phone is the entity that a review targets, and both screen and weight are discussed in the review. Since term aspect to avoid confusion with the term feature used in machine learning literature ( Liu, 2012, chap. 5 ), and to include those aspects that not have associated opinions. Aspect is commonly represented as a noun or noun phrase. There are primarily four types of methods for extracting product aspects from online reviews ( Liu, 2012, chap. 5 ). 2.1. Extraction by frequent nouns and noun phrases
Hu and Liu (2004a, 2004b) laid the groundwork for applying Apriori algorithm of association rule mining to aspect extrac-tion by treating frequent nouns and noun phrases as aspect candidates. The algorithm achieved a precision of 80% and a recall of 72%. Scaffidi et al. (2007) improved the above method by introducing pruning methods to filter those English words that are unlikely to be aspects, such as those words that are more likely to be a verb than a noun. In addition, they pruned some compound words based on the difference in frequency distribution between online reviews and generic text. These pruning strategies improved the precision of aspect extraction to the range of 85 X 90%. Nevertheless, they show two major limitations: (1) the pruning strategies were developed based on the morphological forms of English words, which are diffi-cult to be extended to other languages like Chinese; and (2) they chose corpora of spoken and written conversations as gen-eric text, and difference in conversational structure would have impact on the performance of text classification ( Tavafi,
Mehdad, Joty, Carenini, &amp; Ng, 2013 ). Li, Ye, Li, and Law (2009) extended the method of Hu and Liu (2004a) to Chinese by introducing additional pruning steps aspect extraction from Chinese reviews. Xu, Huang, and Wang (2013) treated frequent sets of skip-bigrams, which are word pairs that allow skips between words, as candidate aspects. They used skip-bigrams to mitigate the negative impact of errors in Chinese part-of-speech tagging, which led to increased recall but reduced precision (below 0.5). Ferreira et al. (2008) found that Hu and Liu X  X  approach is effective for text that mainly consists of on-topic content. Following Hu and Liu (2004a), Blair-Goldensohn et al. (2008) extracted aspects based on syntactic patterns, relative word frequency, and a sentiment lexicon.

Popescu, Nguyen, and Etzioni (2005) employed PMI-IR (pointwise mutual information X  X nformation retrieval) ( Turney, 2001 ) to estimate the semantic associations between aspect candidates and discriminator phrases (i.e., part-of and is-a rela-tions). Their method is based on the KnowItAll, a web-based, domain-independent information extraction system ( Etzioni, Cafarella, Downey, et al., 2005 ). The PMI scores of each noun or noun phrase were then converted into a binary value using a Naive Bayes classifier. Compared with Hu and Liu X  X  method (2004a) , the PMI-IR method was empirically found to improve the precision of aspect extraction by an average of 22% at a small cost of recall for about 3%. Despite the promising results, this approach exposes several limitations. First, the identification of part-of and is-a relations relies on English lexical resources such as WordNet, which may not be readily available in other languages. The performance for automatic extraction of the above two types of patterns remains low for languages such as Chinese ( Cao, Cao, &amp; Wu, 2013 ). Second, of content in online reviews. Third, the Na X ve Bayes classifier is a supervised method, which by nature requires a set of positive and negative reviews for each type of products as seeds in training models separately. Moreover, their selection of seed reviews, particularly negative seeds, was not random, in order to prevent the PMI scores to become zero. Further, the performance of the method largely depends on the aspects that have been selected into seeds. Additionally, choosing aspects that are semantically similar brings up the question of how to calibrate the semantic similarity. Fourth, for each can-didate aspect, the approach queries a search engine in relation to each discriminator phrase separately, which is inefficient. Our proposed method has close resemblance to that of Popescu et al. (2005) , but addresses the above limitations. 2.2. Extraction by opinions and aspects relation
This category of methods leverages the relationship between aspects and opinions, because opinions are generally expressed on some aspects in online reviews ( Liu, 2012, chap. 5 ). Such dependency relationship was first used to find the nearest nouns and noun phrases of a set of known opinion words. As a result, this method is capable to identify infrequent aspects missed by the Apriori algorithm. Shi and Chang (2006) developed an  X  X  X pinion first and aspect second X  X  approach for Chinese reviews, which extracted implicit aspects using an opinion lexicon pertaining to a product genre that was compiled manually.

Qiu et al. (2009), Qiu, Liu, Bu, and Chen (2011) proposed double propagation that simultaneously extracts aspects and expands opinion lexicons by moving forward and backward between opinion mining and aspect extraction. The method also agation improved the F -score of aspect extraction by 10% over that of Hu and Liu (2004a) . Zhang, Liu, Lim, and O X  X rien-Strain (2010) employed the part-whole pattern and no pattern to increase the recall and ranked candidate aspects by importance to increase the precision of double propagation. However, the approach was not found to scale well ( Zhang, Liu, Lim, &amp; O X  X rien-Strain, 2010 ) as the precision dropped as the size of the corpora was increased. 2.3. Extraction by supervised methods
This is by far the most dominant method for aspect extraction. Most of the supervised methods for aspect extraction uti-lized machine learning techniques such as Hidden Markov Model, Support Vector Machines, and Conditional Random Fields aspects from labeled training data. Some of them also require lexicons, domain and/or linguistic knowledge base methods have distinct advantages and disadvantages. On one hand, these methods generally perform well. On the other hand, they require review data with proper annotation to build classification models. 2.4. Extraction by topic modeling
Topic modeling ( Blei, 2012 ) has been extended to aspect extraction under the assumption that: (1) reviews have latent structure of topics; (2) topics can be inferred from word-review co-occurrences; and (3) words are related to topics and so are topics to reviews. Based on the mathematical framework, there are two main categories of statistical topic models: pLSA Compared with pLSA, LDA does not require training data to estimate model parameters and thus is more scalable. Titov and McDonald (2008a, 2008b) proposed two static methods for labeling sentences with topics or aspects, one is based on LDA and the other on multi-grain topic models (MaxEnt-LDA Hybrid model), which could achieve accuracy comparable to that of a standard supervised model. Jo and Oh (2011) proposed Sentence-LDA (SLDA) and extended SLDA to build aspect and sentiment unification model. Like other probabilistic generative models such as pLSA and LDA, SLDA mines both aspects and sentiments simultaneously. Unlike other models, however, SLDA assumes that all words in a single sentence are gener-ated from the same aspect, and the extended SLDA has relaxed this assumption. Si et al. (2013) applied a continuous Dirichlet Process Mixture model to learn a set of daily topics from Twitter for stock market prediction, where the topic extraction has resemblance to aspect extraction. Although these topic modeling methods can take advantage of the big data of online reviews for parameter estimation, they are unable to distinguish between aspects and opinions but treat both of them as topics ( Liu, 2012, chap. 5 ).

In this research, we propose a method that extracts aspects based on frequent nouns and noun phrases. It adapts PMI-IR to the problem of aspects extraction and addresses the limitations of previous methods to improve the performance and gen-erality of aspect extraction. 3. Method design
In this section, we propose a method for product aspect extraction from online reviews. The method consists of three components: frequency based mining and pruning, order-based filtering, similarity-based filtering, as shown in Fig. 1 . The third component proceeds in two sub-steps. In the first step, we extended PMI-IR to measure the semantic association between frequent aspects and the corresponding product entity. In the second step, we developed a threshold learning method for selecting the final set of aspects. 3.1. Frequency based mining and pruning
Following Hu and Liu (2004a) , we performed frequent aspect mining using the Apriori algorithm to generate candidates lection of data. Another critical step is to prune candidate aspects. To this end, we applied compactness rules and redundancy rules ( Hu &amp; Liu, 2004a ):
Compactness rules: we designed these rules to filter some higher-order candidate aspects that do not make sense. For example, both game and CPU are candidate aspects of mobile phone, and they often co-occur in a sentence. Thus, game
CPU is a higher-order candidate aspect, but it is not an actual aspect of the mobile phone. This problem is attributed to the Apriori algorithm X  X  ignorance of the relative positions of its component nouns in generating higher-order frequent aspects. By applying the compactness rules, those candidate aspects in which lower-order nouns or noun phrases are located distant from each other (e.g., game CPU ) would be eliminated from the consideration of actual aspects.
Redundancy rules: we used these rules to prune those single-word candidate aspects that seldom appear alone, but often along with other candidate aspects. For example, time and standby time are both candidate aspects of a mobile phone.
Based on the occurrence frequency of time (i.e., the single-word candidate aspect alone) and its co-occurrence frequency with standby (e.g., another candidate aspect), time was considered as redundant and pruneda accordingly.

In addition, we extended general concepts and domain-specific common concepts strategies ( Li et al., 2009 ) to filter the following types of candidate aspects:
General concepts: The intuition behind this strategy was that an aspect should be expressed in specific rather than general terms. There are some frequent nouns or noun phrases that are generic enough to be used to reference any type of entities such as people , place , thing , and event .

Domain-specific common concepts: Some nouns or noun phrases are unlikely to express aspects such as my opinion , expec-tation , uses , and aspects despite of their high occurrence frequencies in online reviews. This type of nouns or noun phrases was treated as domain-specific common words or stop words in this research.
 3.2. Order based filtering An aspect, particularly those related to entity parts and part attributes, are generally expressed words in certain order. However, the basic Apriori algorithm also ignores the word order in combining more than one candidate aspect. For instance, battery life and screen size describe aspects of camera, but their reverse orders do not (without changing the syntactic struc-method for word order acquisition is briefly described as the following.
 (1) ):
The word order in f will be determined based on their corresponding Loc ( t i ) scores sorted in a descending order. For exam-also appears in three other sentences, among which two have t 1 (speed) appearing before t 2 (options), and one in the oppo-site order, then Loc ( t 1 )&lt; Loc ( t 2 ), and accordingly the aspect would be extracted as speed options . 3.3. Similarity-based filtering We extended PMI-IR to estimate the semantic similarity between a frequent aspect and a product entity, using Eq. (2) :
Here, hits ( ) denotes the number of pages returned from a search engine for a query consisting of the phrase ( ). The higher the PMI-IR score, the stronger the semantic association between the frequent aspect and the entity, and accordingly order of their PMI-IR scores, we selected those aspects whose scores are above certain threshold.

To determine the threshold, we adopted RCut ( Yang, 2001 ). The RCut strategy was originally applied in text classification not), we set t to 1 in the current study.

Among the various criteria for selecting a thresholding method, we focused on its generality. More strictly, generality implies that certain threshold values can be extended to different types of products. This is important for aspect extraction, particularly in view of the potentially large number of product types. To address this issue, we aimed to determine a single threshold for a collection of different types of products. Moreover, we validated the generality of RCut based on whether its effectiveness is indifferent to the types of products under review. Furthermore, a classification method belongs to the par-adigm of supervised machine learning, which is subject to the availability of training data. Methods that require smaller data by experiment.

Taking a set of frequent aspects extracted by the previous components as the input, the threshold learning and testing goes through the following steps: Step 1: Randomly split reviews into training data and testing data.
 Step 2: Compute the PMI-IR value for each frequent aspect extracted from the training data.
 Step 3: Select the PMI-IR value that optimizes the performance of aspect extraction (i.e., F-score ).
 Step 4: Repeat the above steps for each of the products in the training data.
 Step 5: Determine the threshold a as the average PMI-IR value of all of the selected products.

Step 6: Evaluate the performance of a on the test data by choosing frequent aspects with PMI-IR scores greater than a . 4. Experiment
We evaluated the proposed method for aspect extraction by conducting experiments. 4.1. Data collection and preparation
Previous research on aspect extraction has predominantly studied English reviews, partly because there is a convenient sample of annotated dataset of online reviews in English (e.g., Hu &amp; Liu, 2004a ). We chose to conduct the experiment with Chinese reviews.
Given the lack of any publicized dataset in Chinese, we built a dataset from scratch. Following the composition of the Eng-types of electronic products, including mobile phone, digital camera, MP3 player, and LED monitor, and eight specific prod-ucts. These reviews were downloaded from IT168 ( http://www.it168.com ), one of the most reputable online communities of digital products in China. To test the generality of the threshold determination method, a fifth distinct product type, namely book, was also included. Specifically, the reviews of one of the most popular titles of the year were downloaded from Ama-zon.cn ( http://www.amanzon.cn ), one of the largest online bookstores in China. The PMI-IR was computed using Baidu ( http://www.baidu.com ), which had the largest share in the Chinese search engine market.

We randomly selected 120 reviews for each of the above nine products. These reviews went through a cleansing process, which involves removing reviews that contain less than three words or more punctuation marks than words, and removing punctuation marks and hyperlinks. We then proceeded with manual annotation of the remaining reviews, accounting for anywhere between 60 and 100 for the different products (see Table 1 ).

Four human coders were recruited to extract product aspects from the cleansed online reviews independently. The coders were first given detailed instructions and opportunities to clarify any questions they may have about the task; and then they were asked to identify all aspects of the product mentioned in each review. A comparison between four coding results showed that the average percentage of pairwise agreement across all the products was 92.5%. The average pairwise agree-ments range between 88.9% and 93.6% across product types. Only those features that were unanimously identified by all the coders were selected as the ground truth for testing aspect extraction methods. In other words, any inconsistent aspects between any pairs of the coding results were dropped.

The descriptive statistics of the finalized dataset is reported in Table 1 , which describes the number of reviews, the num-ber of aspects per review, the length of reviews, and the number of reviews per aspect. 4.2. Evaluation metrics and experiment settings
Precision and Recall were selected as the evaluation metrics, as defined in Eqs. (3) and (4) . In addition, F -score was also included to address a possible tradeoff between the first two metrics, as defined in Eq. (5) .
Li et al. (2009) was selected as the baseline method for comparison for two reasons: (1) the method is based on frequent nouns and noun phrases, which falls into the same category as our proposed method; and (2) it represents the state-of-the-art research in Chinese review mining, which has demonstrated superior performance to that of Hu and Liu (2004a) .
To determine the threshold for PMI-IR scores and to test the performance of the proposed aspect extraction method, a subset of four products, including one from each type of the electronic products) was randomly selected as the training, and the remaining was held out as the test data. To test the generality of the method, we also conducted additional exper-iments that included book reviews into the training and the test data separately. 5. Results and discussion
The performance of aspect extraction is reported in Table 2 . It is shown from the table that the proposed method outper-formed the baseline method in terms of precision and F -score. We conducted paired sample T -test to assess the significance drop in recall by 2.3% ( p &gt; .05).

The results of PMI-IR threshold learning are reported in Table 3 . The average PMI-IR score of the four products from the training data, 25.18, was used as the threshold value. It is shown from the table that there is little variation in the thresh-olds across different products, providing preliminary evidence for the generality of our threshold learning method and for the validity of using one threshold value aggregated from various products.

We evaluated the sensitivity of the PMI-IR threshold values to the size of the training data in terms of the number of prod-vide strong evidence for the robustness of the proposed methods for aspect extraction in that it requires little training or supervision.

To test the generality of the proposed method, we included book reviews into the training data (with five unique product types for training) and test data (with five unique product types for testing), separately. The results after including book reviews are reported in Table 5 , which confirmed the superior performance of the proposed method. Specifically, compared with the baseline method, the proposed methods increased the precision of aspect extraction by 12.2% and F -measure by 5.2% despite a drop in recall (2.9%), when the book reviews were include into the training data; and the methods increased the precision of aspect extraction by 11% and F -measure by 4.7% despite a drop in recall (3%), when the book reviews were include into the test data. Moreover, the average F -scores with book reviews included are comparable to that without (i.e., 73.3%). Further, the results of threshold learning after incorporating book reviews are also reported in Table 5 . A comparison electronic products. These results demonstrate the generality of the proposed methods for aspect extraction.
Compared with the previous frequency-based extraction methods, our proposed method demonstrates the following advantages. First, the current method employs a universal search engine instead of a static local database or dataset. In view that both data coverage and freshness are major design factors of a universal search engine, the proposed method is capable to keep up with evolving products and product aspects. In addition, search engines have become a ubiquitous information source in our daily use of the web ( Levene, 2010 ), making the proposed method easy to use. Second, the computation of the PMI-IR score for each candidate aspect in the current method involves querying the search engine only once instead of multi-ple times (e.g., one for each discriminator phrase separately), and thus is more efficient than a previous PMI-IR based method ( Popescu et al., 2005 ). Third, the proposed method for threshold learning quickly stabilizes at its near-optimal performance, and the learnt threshold values are insensitive to product types, which highlight the robustness of the proposed method.
The methods proposed in this study have limitations, which warrantee future research. First, like other frequency-based methods for aspect extraction, including most cited ones, the current method does not explicitly handle infrequent aspects. Some methods have been suggested to address infrequent aspects. For instance, Hu and Liu (2004a, 2004b) used adjectives to find the nearest nouns as the infrequent aspects in English reviews. However, this approach did not show promising results on Chinese reviews ( Li &amp; Lu, 2011 ). To extract infrequent aspects and/or frequent aspects that do not often co-occur with product entities, we plan to develop subjective patterns in future. The subjective patterns generally express more intensive associated nouns or noun phrases from Chinese reviews as infrequent aspects. Second, the performance improvement of the proposed aspect extraction method would achieve higher levels of statistical significance as we further increase the number of products in the dataset. Third, the aspects that consumers care about are not limited to product functions but include non-functional ones such as price and customer service as well. Although online reviews are unlikely to fully reflect consum-ers X  actual experiences, service quality is of great importance to consumer satisfaction ( Yang &amp; Peterson, 2002; Yang,
Peterson, &amp; Huang, 2001 ). Therefore, future work will expand the scope of extraction to include non-functional aspects. 6. Conclusion
In this study, we proposed a method to improve the performance of aspect extraction from online consumer reviews. The method augments frequency-based extraction with PMI-IR. In addition, it extends RCut to learn thresholds for selecting can-didate aspects. Experiment results show that our proposed method not only outperforms the state-of-the-art method for aspect extraction but also demonstrates the potential to generalize to varying data sizes and different product domains.
Our future work will focus on extracting infrequent aspects to improve the recall of aspect extraction and evaluating the gen-eralizability of the proposed method with a broad range of products and services.
 Acknowledgements DL11BB25), the National Natural Science Foundation of China (Grant No. 71001023), and National Science Foundation (IIS-1250395).
 References
