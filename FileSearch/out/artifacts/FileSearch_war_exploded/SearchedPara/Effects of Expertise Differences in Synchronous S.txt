 Synchronous social question-and-answer (Q&amp;A) systems match askers to answerers and support real-time dialog between them to resolve questions. These systems typically find answerers based on the degree of expertise match with the asker X  X  initial question. However, since synchronous social Q&amp;A involves a dialog be-tween asker and answerer, differences in expertise may also mat-ter (e.g., extreme novices and experts may have difficulty estab-lishing common ground). In this poster we use data from a live social Q&amp;A system to explore the impact of expertise differences on answer quality and aspects of the dialog itself. The findings of our study suggest that synchronous social Q&amp;A systems should consider the relative expertise of candidate answerers with respect to the asker, and offer interactive dialog support to help establish common ground between askers and answerers. H.3.3 [ Information Storage and Retrieval ]  X  Information Search and Retrieval: search process , selection process . Synchronous social Q&amp;A; Expert finding; Expertise location. Synchronous social question answering (Q&amp;A) systems match askers to answers in real time and facilitate dialog between them, usually via instant messaging (IM) [3][9]. These systems typically use expert finding methods (e.g., [1]) to identify candidate an-swerers based on the match between their profile and the initial question. However, since synchronous social Q&amp;A systems initi-ate direct dialog between askers and answerers, the ability of ask-ers and answerers to converse with each other effectively may also affect the outcome. Indeed, psychologists and human-computer interaction researchers have shown that expertise differ-ences can hinder dialog between domain novices and experts [4][6] and that IM is a difficult medium for establishing the com-mon ground that is important in successful dialog [5]. Understand-ing the effect of expertise differences on synchronous Q&amp;A can help in designing better social Q&amp;A systems, perhaps by includ-ing relative expertise when selecting candidate answerers [7], or providing support to help ground the Q&amp;A dialog. In this poster we present a study of expertise differences in syn-chronous social Q&amp;A. We use data from a live synchronous social Q&amp;A system, IM-an-Expert , deployed to a community of over two thousand users [9]. IM-an-Expert receives questions via IM, identifies candidate answerers by ranking all users according to the match between their expertise and the question, routes ques-tions only to those available to answer, and mediates the dialog between the asker and answerer. In IM-an-Expert, all users ask or answer questions; to ask, users must be willing to answer. We use the data from the system to study the effect of expertise differ-ences on answer ratings (assigned by askers and indicating answer quality) and the dialog. The asker poses the question to IM-an-Expert via IM. The system selects (using BM25 [8]) users who seem to be most likely able to answer a question using profile information from three sources: (i) self-reported knowledge: a Web interface lets users create and update a profile comprising keywords and URLs of pages about them or their interests; (ii) email sent to mailing lists: archives of email sent to internal mailing lists, available on a range of topics. Email is preprocessed to exclude headers and quoted text so that each profile contains only the user X  X  authored text, and; (iii) histo-ry : each user is associated with questions they answered, allowing the system to improve over time. Other factors such as elapsed time since a user was last asked a question are also considered. A small group of experts who are currently available (not busy, away, etc. via presence data from the IM client) are contacted via IM three at a time, in descending order of expertise, to determine whether they can help answer the question. If and when an an-swerer accepts, other requests are canceled. If a candidate an-swerer does not respond in time or rejects the question, the service asks others. Once an answerer accepts, IM-an-Expert mediates the conversation. When the conversation ends, the asker can optional-ly rate answer quality from one (not helpful) to five (very helpful). IM-an-Expert is deployed within Microsoft and thousands of em-ployees use it to find answers to their questions. We use a set of 1,725 questions from 937 users. 1,144 (66%) of the questions were answered, of which 908 (79%) were rated. 527 users asked at least one question and 573 users answered at least one question. For each question we have the text, the asker and answerer identi-ty, and the full-text of the IM dialog. We also have the profiles of askers and answerers, not including questions that they answered well ((iii) above), which varied based on when the question was asked and excluding it simplified our analysis. On average, users provided 8.0 unique keywords and 2.7 unique URLs. The average profile length was 8,930 words (after removing HTML tags). To study the effect of differences in the expertise levels on the question outcomes and the dialog itself, we devised a way to esti-mate the expertise of the asker and the answerer with respect to the question. Following removal of stopwords from the question and from user profiles, we represent each as term vectors with term frequency counts. We then compute the cosine similarity between the question and the profile. We do this on a per-question basis since expertise may vary by question topic. Cosine was used since it gave us a normalized measure of expertise, in the range [0,1], that could be computed for both askers and answerers and easily compared to estimate expertise differences. It also has the advantage of estimating expertise relative to the question at hand, rather than using measures such as reputation or answer history, which are question independent. We use the difference ( ) be-tween the similarities of the asker and the answer as a proxy for differences in expertise. Reassuringly, the results show that for 76% of questions, the answerer is at least as expert as the asker and in most cases (58%), is more expert. It is worth noting that using the cosine similarity as the measure of expertise penalizes people whose profiles span multiple areas of expertise. Since such profiles may indicate less expertise in the question than someone whose profile focuses only on that topic, it is unknown whether this is a concern. We could also have focused on asynchronous media such as stackoverflow.com or quora.com. However, target-ing synchronous Q&amp;A better aligned with our research focus and let us study dialog dynamics. We focused our analysis of the effect of expertise differences on two important aspects of the synchronous social Q&amp;A process: (i) outcome (answer rating) and (ii) dialog (balance of conversation). We were interested in the relationship between the difference in expertise and the quality of the answer. Answer quality is a meas-ure of the benefit to the asker of the dialog. The top row of Table 1 presents the average answer rating depending on whether the answerer was less, equally, or more expert than the asker. Table 1 shows differences in the answer rating as answerers be-come more expert than askers. All differences in ratings are statis-tically significant with an independent measures analysis of vari-ance (ANOVA), ( (2,905) = 4.63, = .01) and post-hoc testing as appropriate. Figure 1 shows a more granular breakdown of rating by . Error bars denote standard error of the mean (SEM). Figure 1 shows that there is a slight increase in answer rating as answerers become increasingly more expert than askers. Howev-er, we can make more interesting observations about the figure: 1. It is not until the answerer is significantly less expert than the asker (  X   X  0.8) that the outcome suffers significantly. The rat-ing is pretty robust to expertise differences (ranging from 3.7 at that the ability to clarify and discuss the problem via IM may lessen the importance of the expertise differences. 2. The asker just needs to have a level of expertise equivalent to the answerer to get an average answer rating of four or more. 3. More expertise is not always valuable. There is no additional asker benefit (at least in terms of answer rating) after  X  0.4. 
One possible explanation for this is that askers with little or no expertise on their question topic (required for such high values of d ) may be easily satisfied with any help received, regardless of how beneficial the help was. Since our measure of answer quality is subjectively provided by the asker, we cannot distin-guish between an asker receiving genuinely better answers vs. an asker who is more easily satisfied with mediocre answers. The third observation in particular motivated us to explore the Q&amp;A process itself for an alternative objective measure. We focus on the dialog balance, representing the relative fraction of the dialog that askers and answerers perform. This can be viewed as a measure of the fraction of the work (the relative cost) to each party of engaging in the conversation. To measure the effect of expertise differences on the balance of the conversation, we computed the number of messages from each user for each IM dialog. The average percentage of each dialog coming from the asker, given differences in expertise levels, is shown in Table 1. Large differences are observed when answerers have more topic expertise than the asker ( (2,1141) = 6.24, &lt; .01, and &lt; .02 with Tukey post-hoc tests). One possible explana-tion for this phenomenon is that the asker is spending more time clarifying their question. Possible reasons for this include differ-ences in vocabulary between novices and experts [2] and the ten-dency of novices to underspecify their goals in dialog with experts [6], both of which may result in more messages. In this case, it may be beneficial if the Q&amp;A system was a more active interme-diary and suggested strategies to establish common frames of reference, something that is important in successful novice-expert dialog [4]. Alternatively, it may be the case that the answerer, by being much more expert, is able to rapidly answer the question. We have demonstrated the effect of expertise differences on the outcomes and dialog in synchronous social Q&amp;A. As expected, we showed that when askers engage in dialog with those who are more expert on the question topic, they are more satisfied with the answer they receive. However, the benefit does not seem to in-crease beyond a certain expertise difference. In fact, beyond a difference of 0.4, the average answer rating no longer increases yet the dialog continues to become more and more imbalanced. Synchronous social Q&amp;A systems should consider both asker and answerer topic knowledge during expert finding. In these systems it may be worth finding answerers slightly more knowledge than the asker, and reserve the most expert answerers for the most knowledgeable askers. There are opportunities for future work in better understanding the underlying causes of the observed dialog imbalance, in determining whether the imbalance is positive or negative, and in developing mitigation strategies if negative. [1] Balog, K. et al . (2006). Formal models for expert finding in [2] Falzon, P. (1991). Cooperative dialogues. In Rasmussen, J., [3] Horowitz, D. and Kamvar, S.D. (2010). The anatomy of a [4] Issacs, E.A. and Clark, H. (1987). References in conversations [5] McCarthy, J.C et al . (1991). An experimental study of com-[6] Pollack, M.E. (1985). Information sought and information [7] Smirnova, E. and Balog, K. (2011). A user-oriented model for [8] Sp X rck-Jones, K. et al . (2000). A probabilistic model of in-[9] White, R.W. et al . (2011). Effects of community size and 
