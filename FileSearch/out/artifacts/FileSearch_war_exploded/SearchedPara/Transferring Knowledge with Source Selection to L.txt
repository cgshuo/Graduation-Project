 We investigate the problem of learning an IR function on a collection without relevance judgements (called target col-lection) by transferring knowledge from a selected source collection with relevance judgements. To do so, we first construct, for each query in the target collection, relative relevance judgment pairs using information from the source collection closest to the query (selection and transfer steps), and then learn an IR function from the obtained pairs in the target collection (self-learning step). For the transfer step, the relevance information in the source collection is summarized as a grid that provides, for each term frequency and document frequency values of a word in a document, an empirical estimate of the relevance of the document. The self-learning step iteratively assigns pairwise preferences to documents in the target collection using the scores of the former learned function. We show the effectiveness of our approach through a series of extensive experiments on CLEF-3 and several collections from TREC used either as target or source datasets. Our experiments show the importance of selecting the source collection prior to transfer information to the target collection, and demonstrate that the proposed approach yields results consistently and significantly above state-of-the-art IR functions.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Search process ; I.2 [ Artificial In-telligence ]: Natural Language Processing -text analysis Learning -Parameter learning ; H.1 [ Models and Princi-ples ]: Miscellaneous Domain adaptation, knowledge transfer, source selection, learning to rank, transductive learning
I n the following sections, we first present the framework and then our transfer learning approach for ranking.
In a typical transfer scenario for IR, relying on the same source collection for all the target queries prevents one from taking into account the fact that queries are usually dif-ferent from each other and that different collections gener-ally display different query types. We consider here a set of source collections {C s 1 ,  X   X   X  , C s L } composed of a set of ments for each query in Q s i , 1  X  i  X  L . These relevance judgements are assumed to be binary, which is the most common situation. We also consider a target collection C t , composed of a set of documents D t and a set of m queries Q goal here is to transfer relevance information to C t from all the interesting sources in {C s 1 ,  X   X   X  , C s L } and then learn a ranking function on C t . To do so, we propose (a) to induce relative relevance judgements between documents in C t by selecting for each target query the closest source collection (in a sense defined below) and transferring relevance infor-mation from it, and (b) to learn a ranking function in C t from the relative relevance judgements obtained previously. As in traditional learning to rank approaches for IR, each query-document pair in the target collection ( q t , d )  X  Q t  X  D t is represented as a K -dimensional feature vector f . The vec-tor attributes, considered in this work are standard features used in document retrieval as well as three state-of-the-art IR scoring functions. These features are presented in more details in section 3. For each query q t in the target collec-tion, the transfer of relevance information from the source collection to the target one results a set of relative judgement pairs for documents in D t , of the form d  X  q t d  X  , where  X  q t denotes a preference relationship and means more relevant to query q t than . From these sets, one can then construct a ranking function h : R K  X  R that assigns a score to docu-ments in D t for each query q t  X  Q t . Similarly to previous learning to rank studies, we focus here on linear ranking functions: where h ., . i stands for an inner product and the weight vec-tor w represents the model parameters. Table 1 gives the notations used in the paper.

A word w can be characterized by two quantities which constitute the basis of all IR scoring functions: its normal-ized document frequency, DF ( w ) = N w ( C ) N ( C ) , and its normal-ized number of occurrences in any document d of the col-lection considered, which is set, following [1] and [6], to: ument d in which w occurs, the contribution of w to the relevance of d to q can be estimated through the propor-tion of relevant documents (to any query q s ) in a source collection C s that have the same ( DF, T F ) values as DF ( w ) and T F ( w, d ). However, as typical IR collections only con-tain few queries, very few words will have exactly the same ( DF, T F ) values. One way to avoid this problem is to con-sider regions in the ( DF, T F ) space into which the different values are considered equivalent. This amounts to discretize the DF and T F values, e.g. by defining intervals on each value range. The probability that d is relevant to q knowing w and a source collection C s ,  X  P ( d  X  R ( q ) | w ; C s ), can then be written as:
As one can note from Figure 1, the grids obtained from the three collections differ on several regions, as for example the peak exhibited on the CLEF-3 grid around the ( DF, T F ) values of 0 . 1 and 6 . 5 (pointed to by arrows in Figure 1). Relevance preferences extracted from the grid from TREC-3 will miss the behavior of CLEF-3 query-document pairs in this particular region. This is related to the fact, mentioned above, that different collections tend to use different types of queries, which motivates our will to select, for each target query, a source collection that is appropriate for transfer.
Let q t be a target query, w a word in q t and T F 1 ,  X   X   X  , T F n the different discrete T F values. As before, T F dis ( w, d ) denotes the discrete T F value associated to T F ( w, d ) and DF dis ( w ) the discrete DF value associated to DF ( w ). For any collection C  X  C t  X  {C s 1 ,  X   X   X  , C s L } , let us consider the n -dimensional vector x ( w , C ), the coordinates of which cor-respond to the normalized number of documents in C that contain w with a specific term frequency: x ( w, C ) i = x ( w , C ) thus indicates how w is distributed in the documents of C , an information that can be summarized through the skewness value of w ( C ). Indeed, the skewness measures the asymmetry of an empirical distribution and aims at assessing whether the mass of a distribution is concentrated on the right or the left tail; the more similar two distributions are, the closer their skewness values will be. For each word w in q t , one can thus see which source collection is closest by comparing the skewness values of w ( C t ) and w ( C s i ) (1  X  i  X  L ) (i.e. sk ( x ( w , C t )) and sk ( x ( w , C ))). The score for the query q t is then simply defined as the average score (here equivalent to the sum) over the query words. The source collection used to build pairwise relevance judgements for q t is thus defined as: C q t corresponds to the source collection that displays, in av-erage, the closest ( DF, T F ) distribution to the words con-tained in q t . 4. For each query q t , in the target domain, its most simi-
We conducted a number of experiments aimed at eval-uating to which extend the knowledge transfer presented above can help to learn an efficient ranking function on the target domain. We used nine standard IR collections from TREC and CLEF 2 evaluation campaigns. Simple statistics of these collections are shown in table 2. Among these data sets, TREC-6 , TREC-7 and TREC-8 use the same document sets ( TREC disks 4 and 5) but different query sets, whereas rest use unique document sets and unique query sets. We appended TREC-9 and TREC-10 Web tracks to experiment with WT10G , and TREC-2004 and TREC-2005 Terabyte tracks for experimenting with GOV2 3 . Our preprocessing steps in creating an index uses Porter stemmer and stop-words re-moval using the stopword list provided by Terrier [13]. Collection, C N ( C ) l a vg ( C ) Index size | Q |
I n our transductive transfer learning setting, we use all the unlabeled set of queries and their associated retrieved document lists in the target collection for training. For eval-uation, we use the true relevance judgements provided with the collections. In order to compare the performance of the algorithms we computed the mean average precision ( MAP ) and the average of precision at 10 documents ( P@10 ) across queries. Finally, from now on, we designate the transfer from a source collection C s to a target collection C t using the grid built over the source collection, G C s , by: C s y G h ttp://www.clef-campaign.org Empty documents were not indexed.
 1. X 3 . X 5 . X 7 . BM25 ( q, d ) 8. LM ( q, d ) 9. LGD ( q, d ) Table 3: Features in the vector representation of ( q , d ) , see table 1 for notations. [7] using different but fixed source collections to get a first in-sight into the effectiveness of cross-domain knowledge trans-fer for learning to rank, and moreover the necessity of source selection. Finally, we analyze the effects of the number of sources on the performance of the learned function.
Table 4, shows MAP results on TREC-3 and TREC-4 taken as target collections using a fixed source collection for knowl-edge transfer. For LRT d , we took named page finding (NP) adaptation to topic distillation (TD) in the same years (2003 or 2004) and from NP 2003 to TD 2004. For TLR ss we per-formed knowledge transfer from CLEF-3 , TREC-3 , TREC-6 and TREC-7 collections to respectively TREC-3 and TREC-4 . Table 4: M AP measures on TREC-3 and TREC-4 taken as target collections and when only one fixed source is used. Best results are shown in bold.

From these results, it can be seen that the performance of both transfer learning algorithms, on a given target col-lection, may vary significantly depending on the source col-lection in use. For example, on the same target TREC-4 , the MAP performance of TLR ss varies about 2% depending whether TREC-6 or TREC-3 is used as source collection, while the MAP performance of LRT d differs about 1% on the same target collection. These results also suggest that the effi-ciency of transfer learning may highly depend on the ad-equacy of the source collection with respect to the target one. We now study the behaviour of TLR ss when there are different sources available for knowledge transfer.
Using TREC-3 , TREC-4 , TREC-5 , TREC-6 and CLEF-3 as source collections, we measured the MAP and P@10 of all the models (except LRT d for which the source selection step is not trivial to carry out) on the remaining data sets. Table 5 summarizes these results.

From these results it becomes clear that the transfer rank-ing algorithm TLR ss consistently and significantly improves over other IR models on MAP and P@10 in most cases. Fur-threshold of 0 . 05 (respectively 0 . 01 ). to each query in the target dataset and to derive, from ab-solute relevance judgements available in the selected source collection, relative relevance judgements in a target collec-tion. This derivation relies on a grid that associates to each ( DF, T F ) value of a term in a query-document pair a rele-vance score, which is then combined over all query terms. A ranking SVM system is then deployed on the obtained rel-ative relevance judgements, and further improved through a self-learning mechanism. The experiments we have con-ducted show that the ranking function obtained in this way consistently and significantly outperfoms state-of-the-art IR ranking functions in the majority of cases on 4 large collec-tions from TREC with respect to the MAP the P@10 measures.
Our approach directly learns a ranking function on the target collection (as opposed to previous approaches devel-oped in the same setting and which learned the ranking func-tion on re-weighted version of the source collection), which allows one to develop a simple source selection procedure. This is, to our knowledge, the first time that a source selec-tion procedure for transfer learning for IR is proposed and validated on a whole range of IR collections, including large scale collections as WT10G and GOV2 .

The consistent improvements obtained with TLR ss show that it is possible to learn an efficient combination of state-of-the-art IR scoring functions from the relevance judge-ments provided by a selected source collection. These find-ings go one step further than results presented table 4, indi-cating that source selection using the grid information (sec-tion 2.2) may be effective for learning a domain adaptive ranking function.As depicted in table 6, it can be seen that sources are not uniformly selected for each target collection suggesting which of these source datasets are more related to each of the latter according to our selecting scheme. Finally, the information provided by the grid and the state-of-the-art IR scoring functions allowing to learn this function (Eq. 3) is also complimentary. [1] G. Amati and C. J. V. Rijsbergen. Probabilistic [2] M. R. Amini and P. Gallinari. The use of unlabeled [3] P. Cai, W. Gao, A. Zhou, and K.-F. Wong. Query
