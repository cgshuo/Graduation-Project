 In this paper, we study the emerging Information Retrieval (IR) task of local event retrieval using sensor metadata streams. Sensor metadata streams include information such as the crowd density from video processing, audio classifications, and social media activity. We propose to use these metadata streams to identify the topics of local events within a city, where each event topic corresponds to a set of terms repre-senting a type of events such as a concert or a protest . We develop a supervised approach that is capable of mapping sensor metadata observations to an event topic. In addition to using a variety of sensor metadata observations about the current status of the environment as learning features, our approach incorporates additional background features to model cyclic event patterns. Through experimentation with data collected from two locations in a major Spanish city, we show that our approach markedly outperforms an alter-native baseline. We also show that modelling background information improves event topic identification.
Local search is increasingly attracting more demand, where-by the users are interested to find out about places or events in their local vicinity [11]. Local event retrieval is an exam-ple of local search where users can retrieve a ranked list of lo-cal events of interest, such as music concerts, entertainment events or even protests. Recent work has addressed local event retrieval by using social media activity as a sensor to detect and rank events [1, 13]. However, social media may only cover very popular events as users may not necessarily comment on all events taking place in the city. Therefore, physical sensors that record observations about the status of the environment can provide additional evidence about the events taking place in the city. These sensors can take the form of visual sensors such as CCTV cameras, acoustic sen-sors such as microphones or possibly environmental sensors.
There is a wealth of research on identifying low-level hu-man activities from acoustic and visual sensors. Often, this involves sensor signal processing to extract sensor features c  X  2015 ACM. ISBN 978-1-4503-3621-5/15/08...$15.00. for modelling human activities. For example, Atrey et al. [2] developed a Gaussian Mixture Model using a variety of fea-tures derived from audio signal processing to classify human activities into vocal classes, such as talking and shouting, and non-vocal classes, such as walking and running. Sim-ilar approaches also used audio signal features to identify low-level human activities that are related to security inci-dents, such as breaking glass or explosions [7]. In addition to using acoustic sensors, several studies have been con-ducted to identify low-level human activities from videos. Since its introduction in 2002, the TRECVID evaluation campaign [9] has tackled a variety of content-based retrieval tasks from video recordings to support video search and nav-igation. This includes the semantic indexing of video seg-ments, whereby videos are mapped to concepts, which can be certain objects or human activities [9]. Another related task is multimedia event detection, where the aim is to iden-tify predefined classes of events in the videos. In this task, the existing effective approaches employ classifiers trained with motion features from the videos [10]. Moreover, classi-fying human interactions identified in video recordings has been studied to detect surveillance-related incidents [5].
Although the aforementioned approaches derive useful se-mantics about the multimedia content, they only consider low-level human activities. In other words, they provide sensor metadata describing low-level human activities in the physical environment. However, to the best of our knowl-edge, no previous work has investigated combining these sensor metadata to detect and retrieve higher level complex events taking place in the city, such as music concerts or entertainment shows, which may involve several lower level human actions. In this paper, we propose an approach for combining sensor metadata streams to support local event retrieval. Our major contribution in this paper is devising a supervised machine learning approach that combines sensor metadata to identify the topic of a potential event happen-ing at a particular time in a certain location of the city. The topic corresponds to a set of terms representing a type of events, such as a concert or a protest. Our approach uses features from acoustic, visual and social sensor metadata. We also incorporate background features from past obser-vations to model events that exhibit cyclic patterns such as traffic jams at peak times. To develop our supervised ap-proach, we perform two necessary steps. First, we obtain event annotations on a pool of candidate video and audio recordings of two vibrant locations in the centre of a major Spanish city over a period of two weeks. Second, we use the obtained annotations to map typical events taking place in those locations into coherent topics using a topic mod-elling technique. Through experimentation with the data collected, we evaluate the accuracy of our event topic iden-tification approach and shows that it markedly outperforms an alternative baseline. The results also show the effective-ness of the background features in improving the accuracy of event topic identification.
In this paper, we tackle the problem of event topic identi-fication. The aim is to combine the sensor metadata obser-vations captured at different locations in a city to identify topics of potential high-level events taking place within cer-tain locations. Formally, for a location l i in a city, we de-note the sensor metadata observations captured at time t j in that location l i by the vector data observations may include the crowd density identified from captured videos in the location, low-level audio events identified from the acoustic sensors installed in the location or social media activities, such as tweets posted by people at the location. The problem of event topic identification is to use the vector location  X  l i , t j  X  to a certain topic p x  X  X  described by a set of terms T x ; where P is a set of predefined topics.
In a previous work [1], the textual content of public tweets has been used as the only source of sensor metadata obser-vation to identify topics of local events. Although this has worked well on popular events that attract social media ac-tivities, it did not work as well on more localised events that may not attract coverage on the social media. To alle-viate this shortcoming, we introduce physical sensor meta-data streams that can provide an additional evidence for the topic of an event, namely video and audio metadata obser-vations. However, this requires understanding the semantics of visual scenes or audio recordings, which remains an open challenge. Indeed, there is no known taxonomy that maps sensor metadata to topics of high level events. To address this challenge, we propose to learn the topic associated with a tuple  X  l i , t j  X  from labelled training data using features ex-tracted from the sensor observations
For this purpose, and to collect labelled training data, we obtain event annotations on a pool of videos that are identi-fied as potential candidates to contain events . Furthermore, to extract a predefined set of coherent event topics, we apply topic modelling on the descriptions of the annotated events. We detail the event annotation and the topic extraction in Section 4. Next, we describe the sensor data collection.
Our study considers two locations in the city centre of San-tander in Spain . The first location is the geographical and business heart of the city; it is a major square opposite to the municipality building. The second location is a popular open market in the city, where hundreds of people go every day for shopping , located behind the municipality building. Both lo-cations represent vibrant and busy areas, where we expect to observe high-level events of interest such as music concerts, entertainment shows or even protests. The data collection started since the 11 th October 2013 in both locations.
Table 1 provides a summary of the sensor data collection and the metadata produced by processing the output from the microphones and the camera in each location. For pro-ducing the audio metadata, a supervised classifier using feed forward multilayer perceptron network and low-level audio features, such as those described in [6], was developed for each of the following 6 audio classes described in Table 1:  X  X rowd X ,  X  X raffic,  X  X usic X ,  X  X pplause X ,  X  X peaker X , and  X  X iren X . For video metadata, the video was processed for crowd anal-ysis where we calculate the crowd density, in desired areas, by estimating the foreground components of the video. In addition to the acoustic and visual sensors, we collected par-allel social media activity in the city. In particular, using the Twitter Public Streaming API 1 , we obtained tweets related to each location (as identified by their geo-locations).
In this section, we describe our approach for obtaining event annotations on the recordings collected from the two locations. Recall from Section 2, that our ranking units (the documents) are tuples of time and location. Each tuple rep-resents a segment of recordings at a location. The length of the segment, the sampling rate to obtain the tuples, can be predefined and we follow [1] in setting the sampling rate to 15 minutes. Coarser-or finer-grained sampling rate can be investigated in future work for different types of events e.g. emergency events may require a finer-grained sampling.
For annotation, we consider a period of 2 weeks starting from 19 October 2013, around a week after the start of the data collection (11 October 2013) to allow the estimation of background features . Since it is expensive to examine all recordings and annotate them with events, we employ a pooling approach [4], as commonly used in IR campaigns, such as TREC.

For pooling, we identify candidate segments of videos where high-level events may have occurred by applying the change component of the event retrieval framework in [1]. In par-ticular, the change component of this framework identifies segments where sensor metadata observations change un-usually in a location, e.g. unusual change in crowd density. We use 4 different types of sensor metadata observations to generate the pool (a subset of those listed in Table 1): (i) the median values of the video crowd density, (ii) the me-dian values of the crowd audio classification score, (iii) the median values of the music audio classification score, and (iv) the total number of tweets posted. As a result, we ob-tain a total of 155 candidate segments. The video recording software produced videos with lengths of either 30 minutes or 1 hour, and the total number of video recordings that correspond to the 155 segments are 69 videos.

The generated candidate segments of videos were then annotated by two groups of human annotators, English and Spanish annotators, who were asked to examine the videos, Figure 1: A snapshot from the annotation interface. describe events that they observe by typing in terms, and rate their intensity on a 3-point scale (Low, Medium, and High) according to how likely they are to generate public interest . The intensity is akin to graded relevance used in traditional IR evaluation approaches [12]. We provided the annotators with a web-based interface, of which we show a snapshot in Figure 1.
 Statistics for the obtained annotations are summarised in Table 2. From the last row we observe that we obtain a total of 55 annotated videos, of which 21 were annotated by more than one annotator. The agreement between annota-tors is estimated by converting the intensity levels to binary decisions, using  X  X edium X  as a threshold. We observe that a reasonable agreement is achieved in all cases (lowest is 71%), which gives us confidence in the annotations obtained.
For the set of annotated pooled segments, we obtain terms describing events that were identified in these segments. For each annotated segment, we construct a virtual document that consists of all of the terms provided by the annotators. Since the pooled videos were annotated by both spanish and english annotators, these virtual documents are bilin-gual and contain English and/or Spanish terms. To cluster events into various topics, we propose to use topic mod-elling on the document collection of all constructed virtual documents of terms . We use the Latent Dirichlet Allocation (LDA) topic modelling implemented in the Mallet toolkit [8]. In Table 3, we list the top terms of 7 identified topics from the English annotations only. From the table, we can ob-serve that the identified topics are reasonable where we see some interesting associations of terms that describe typical high-level events taking place in the square and the market, e.g.  X  X emonstration X  and  X  X how X  in topic 4, and  X  X hildren X  and  X  X ntertainment X  in topic 6.
In this section, we discuss our supervised approach for event topic identification, where the aim is to identify the topic of a segment  X  l i , t j  X  using the sensor metadata obser-vations struct a labelled dataset of event topics from the annotated video pool collected in Section 4. The labelled data consists of segments (tuples of time and location) labelled with ei-ther an event topic or with the label  X  X o event X  indicating that no event of interest has occurred in the corresponding Table 3: Topics identified with topic modelling using the English annotations time and location. We labelled each annotated segment in the pool to the most probable topic according to the LDA topic modelling configured by setting the number of topics to 7. 2 Unlabelled segments or where the annotators did not identify any event are associated to the  X  X o event X  label. To illustrate the volume of the data and the distribution of la-bels, we detail in Table 4 the number of segments for each label when using topic modelling on all Spanish and English annotations and setting the number of topics to 7.

We consider the problem of identifying the topic of a pooled segment as a classification task. Using the con-structed labelled data, we train a binary classifier for each of the labels with features derived from various sensor meta-data streams. Our intuition is that such labelled data would allow us to learn the semantics of a combination of sensor metadata. In other words we aim to match sensor meta-data to topics defined using the annotations . For training the classifier, we investigate two main sets of features for the segment, observation features and background features . Table 5 summarises those features. The observation features are extracted from the sensor metadata observed in the lo-cation and time corresponding to the segment. The back-ground features aim to model past observations and cyclic patterns of activities that take place over time in the same location. The intuition is that some events are periodic and exhibit a long-term pattern, e.g. traffic jams at peak times resulting in a high traffic audio classification score, or en-tertainment shows taking place in the square at the same time on the weekends. Modelling cyclic patterns, i.e. daily and weekly cycles, from the sensor metadata observations would enable the supervised classifier to identify recurring background events or noise which are not of interest such as traffic jams. Similarly, it would help to identify recurring events of interest such as entertainment shows.

Using the labelled dataset of segments along with the fea-tures described in Table 5, we apply supervised machine learning to learn a binary classifier for each label. In partic-ular, we experiment with Random Forests [3] as a learning algorithm. 3 Next we conduct a number of experiments to evaluate the accuracy of our classifier and the effectiveness of the various devised features.
To evaluate our approach for identifying the topic of a candidate segment, we use the dataset of labelled segments
Table 5: Features devised for topic identification described in Section 5. We perform a 10-fold cross validation and report the average accuracy across all labels (a label for each topic and the label  X  X o event X ). In addition to using different instantiations of our classifier, we also compare our classifier to an alternative baseline. The  X  X ajority X  baseline assigns the most common label in the training data to the segments in the testing data. Table 6 summarises the results.
We observe from the table that all instantiations of our approach are markedly better than the majority baseline. In particular, when using only the observation features our approach achieves an F 1 accuracy of 0.686. We also observe that this performance further increases when using the back-ground features. Indeed the best performance is achieved when using all background features along with the observa-tion features ( F 1 = 0 . 766). This illustrates that modelling cyclic patterns by aggregating sensor metadata from previ-ous observations helps in better identifying whether a can-didate segment represents an event and in identifying the topic of an event.

Furthermore, we conduct an ablation study to identify which features are more effective for topic identification. We remove one of our 8 observation features when learning the classifier. We report the results in Table 7. For example, the row headed  X -(Audio crowd) X  means that we use all the ob-servation features apart from the audio crowd score. We ob-serve that removing any of the features results in a degrading of performance for accuracy and precision. This is an inter-esting observation and highlights the importance of having rich metadata describing the environment for identifying the topics high-level events. However, we also observe that the performance degrades most when removing the audio crowd score and the crowd density features . This suggests that the crowd level, as detected by the acoustic or visual sensors, is important to identify events and to distinguish their topic.
In this paper, we proposed an approach for combining sen-sor metadata streams to identify the topics of events happen-ing within a city. Our approach trains a classifier to identify topics of candidate segments of recordings. Our results are promising and show that combining features from a variety of sensors (acoustic, visual and social) and modelling cyclic patterns from past observations provides the best accuracy for event topic identification. These results pave the way to-wards more robust implementations of local event retrieval that harness both physical and social sensor streams. This work has been carried out in the scope of the EC co-funded project SMART (FP7-287583) and also in the inte-grated Multimedia City Data project funded by the Eco-nomic and Social Research Council. [1] M.-D. Albakour, C. Macdonald, and I. Ounis.
 [2] P. K. Atrey, M. Maddage, and M. S. Kankanhalli. [3] L. Breiman. Random forests. Machine learning , [4] C. Buckley and E. M. Voorhees. Retrieval evaluation [5] F. Chen and W. Wang. Activity recognition through [6] J. Dennis, Q. Yu, H. Tang, H. D. Tran, and H. Li. [7] A. Dufaux, L. Besacier, M. Ansorge, and F. Pellandini. [8] A. K. McCallum. Mallet: A machine learning for [9] P. Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, [10] S. Phan, T. D. Ngo, V. Lam, S. Tran, D.-D. Le, D. A. [11] G. Sterling. Study: 43 percent of total google search [12] E. M. Voorhees. Evaluation by highly relevant [13] M. Walther and M. Kaisser. Geo-spatial event
