 nessing an exponential growth of both the volume and the complexity of biological data. The Human Genome Project is providing the primary structure of the three bil-lion bases that constitute human DNA. And, consequently, we will be provided too with the primary structures of about 100,000 pr oteins. Therefore, we are entering the postgenomic era: After having focussed s o much effort on the accumulation of data, we have now to focus as much effort, and ev en more, on the analysis of these data. task because, not only, of its complexity an d its multiple numerous correlated factors, mechanisms. Classical approaches of biological data analysis are no longer efficient and produce only a very limited amount of information, compared with the numer-ous and complex biological mechanisms under study. Actually, these approaches use involved in the biological mechanisms. From here comes the necessity to use com-of biological macromolecules, i.e. DNA, RNA and proteins, and, on the other hand, genetic and biochemical mechanisms. Data mining is a response to these new trends. tions or rules, hidden in bodies of data. The extracted information will be used in the rules from biological macromolecules.

In this paper, we present a new data mining approach, called Disclass , based on vote strategies to do classifi cation of primary structures: Let f families that represent, respectively, n samples of n sets S structures. Let us consider now a new primary structure w the decision to assign the new primary structure w to one of the sets S is taken as follows: (i) During the first step, for each family f construct the ambiguously discriminant and minimal substrings (ADMS) associated obtained ADMS are considered too to be associated with the whole set S the classification process, the ADMS associated with the set S substrings (Elloumi 2001) of the new primary structure strategy , the voice weights of the different ADMS, constructed during the first step. the set to which we assign the new primary structure
In the third section, we give some definitions and present our algorithm of con-struction of ADMS. Then we show how t o compute voice weights according to different vote strategies.

In the fourth section, on one hand, we do a comparative study of the error rates associated with the classification a pproach IPR (Maddouri and Elloumi 2002), strategies, majority vote strategy (MVS), unanimity vote strategy (UVS), similarity error rates associated with different classification approaches.
In this paper, we focus on three biological problems: (i) Classification of toll-like receptors (TLR) macromolecules (ii) Discrimination between exon and intron regions in DNA macromolecules (iii) And identification of junction regions in DNA macromolecules molecules, found at www.invivogen.fr/TLR/TLR_genelist.php .The Pa s -in studying the role of TLR in immunology. The biologists asked us to help them in identifying the TLR families. The hierarchy of TLR families is shown in Fig. 1. The
TLR macromolecules are organized as plasmides pUNO and pDUO. We can notice the same parent family pUNO. So, it is hard for a data miner to distinguish between them. However, the families TLR-1/TLR-2 and Tollip are distant relatives. longing to two distant families: TLR-1/TLR-2 (45 primary structures) and Tollip (32 primary structures). set DB 3 is taken from the machine learning databases at the University of California belonging to two families: Exon (53 primary structures) and Intron (53 primary struc-tures). Each primary structure is made up by exactly 57 characters. This problem consists in identifying the junction regions in DNA primary structures.
Junction 3 X  (767 primary structures) and Junction 5 X  (768 primary structures).
Let f 1 , f 2 ,... , f n be families that represent, respectively, n samples of n sets S ... , S is assumed to belong to one of the n sets S 1 , S 2 ,... , the introduction, by using our data mining approach Disclass , the decision to assign the new primary structure w to one of the sets S 1 (i) During the first step, for each family f i ,1  X 
Let us begin by some definitions and notations.
Let A be a finite alphabet, a string is a concatenation of elements of A .The length position j ,1  X  i  X  j  X  n , is called a substring of w tively, to the alphabets A ADN ={ A , C , G , T } , A ARN {
A , C , D , E , F , G , H , I , K , L , M , N , P , Q , R , cost of a sequence of edit operations , i.e. change of cost delete of cost  X  , that change one string x into another string x : necessary to change x into x .
 x of w such that: 1  X  i  X  n . The substring x is ambiguously discriminant between f where  X  and  X  are two fixed thresholds. A discriminant substring x is minimum if and only if it contains no other discriminant substring.
Let f 1 , f 2 ,... , f n be families that represent, respectively, n samples of n sets S ... , S minimal substrings (ADMS) associated with S i ,1  X  i  X  n is made by using an adap-tation (Elloumi 1994) of the Karp, Miller and Rosenberg (KMR) algorithm (Karp et al. 1972): We concatenate, respectively, the strings of f we get the substrings of t , of equal lengths, that appear in the i in strings of f i , that verify inequalities (3). The filtering of a vector V the first character in t of an occurrence of an ADMS of length k . The positive com-inequalities (3). These substrings can generate longer ADMS. The filtering is made after each construction of a vector V k from a vector V minimum. The construction of the different ADMS of length k associated with the different families is done simultaneously.
 of the strings of n i = 1 f i and L is the length of the longest string.
Let us consider a set of primary structures belonging to two families: f
GCUGAU } and f 2 ={ GA , AGGUAA } . Figure 2 shows the ADMS obtained thanks to our algorithm (Elloumi 1994).
Let w be a new primary structure that is assumed to belong to one of the n sets
S ,
S 2 ,... , S n . As we have mentioned in the introduction, the ADMS associated with aset S i ,1  X  i  X  n , that are approximate substrings of the new primary structure will vote with weighted voices for this set. Let  X  i given to this set: 2000): where index ( max ( X  1 , X  2 ,... , X  n )) denotes the index i ,1  X  that has the maximum value.
  X  ( x ) . For this issue, we consider four vote strategies .

By using this strategy, an ADMS x associated with a set S approximate substring of the new primary structure w , will vote for S of weight equal to 1:
The particularity of this strategy, versus t he MVS, is that the new primary structure w is assigned to a set S i ,1  X  i  X  n , if and only if j = i .
By using this strategy, we compute a weight  X  i ( x it  X  , that measures a certain similarity that exists between an ADMS x and the new primary structure w :
In the case of our classificatio n problem, we define the function cause x is an approximate substring of w , we measure the similarity between x and w by computing the rate discriminant it is and the greater the weight  X  i ( x )is. structure w is an element of S i ,1  X  i  X  n , given that the ADMS x is an approximate substring of w :
Because x is, on one hand, an approximate substring of w , on the other hand, a sub-string of primary structures of the family f i ,1  X  i  X  n , we estimate the conditional probability P ( S i | x ) by computing the rate where f i , j is the subfamily of f i made up by the primary structures for which x is a substring. In this way, the more the substring x appears in primary structures of f then the more biologically discriminant it is and the greater the weight
The search of a substring x in a primary structure approximate string-matching algorithm (Elloumi 2001).

As we can notice, while presenting the SVS and the PVS, we have given, re-spectively, two different meanings to the expression  X  more biologically discriminant . X 
Indeed, we can see the comparison of two ADMS from two viewpoints: it is longer. This viewpoint matches the SVS. (ii) While, from the second one, an ADMS is more biologically discriminant if and only if it is more frequent, i.e., it appears in more primary structures. This view-point matches the PVS.
In this section, on one hand, we do a comparative study of the error rates associated with the classification approach IPR ( Maddouri and Elloumi 2002) coupled with different encoding methods. On the other hand, we compare the vote strategies MVS,
Disclass . Then we do a comparative study of the error rates associated with different classification approaches. and  X  used to construct ADMS on the error rate of the classification approach IPR (Maddouri and Elloumi 2002). We consider the case where the case where  X  = 33% and  X  = 66% and the case where  X  =
Figure 3 shows the effect of these paramet ers on the error rate of IPR when applied by using the cross-validation estimator (Weiss and Kulikowski 1991) with 5 folders and 30 runs. The cross-validation estimator is the standard estimator for classification approaches. As one can see, the bes t error rates were obtained when  X  = 0%. On the other hand, the results obtained when not very different from those obtained when  X  = 20% and
ADMS on the classification error rate of IPR. Encoding methods code 1 , code 2 and code 3 are encoding methods that are based on biochemical properties of biological macromolecules (Hirsh and Sternberg 1992; Craven and Shavlik 1994; Hirsh and
Nooredewer 1994; Opitz and Shavlik 1997; Fu 2001; Qicheng et al. 2002): Encoding encoding method code 3 is based on the presence of a pair of nucleotides. validation estimator with 10 folders and 50 runs.
 tained by using code 2 . The results obtained by using code ADMS are close to those obtained by using code 2 . On two data sets over the five used, code ADMS generates tion error rates associated, respectively, with code 2 and code ADMS is very small,  X  1%. So, we can conclude that these two encoding methods are the most interesting ones.
In this subsection, we compare the vote strategies MVS, UVS, SVS and PVS by values for the parameters  X  and  X  .Weset  X  = 0% and  X  = a strict discrimination,  X  = 33% and  X  = 66% (Fig. 6) and classification error rate by using the cross-validation estimator with 5 folders and 30 runs.
 to cost  X  = 2and  X  =  X  = 1. (ii) Generally, the MVS and SVS give classification error rates that are much lower
In this section, we compare different appro aches used to classify primary structures: (i) Two artificial neural networks -based approaches, KBANN (Towell 1991) and (ii) A decision tree -based approach ID3 (Quinlan 1983) (iii) A nearest neighbor approach (Salzberg et al. 1995) (iv) An ad hoc biological approach, i.e. O X  X eill X  X  approach (O X  X eill 1989; O X  X eill (v) A concept lattice -based approach, LEGAL-E (Fu 2001) (vi) A machine learning approach IPR (Maddouri 2000) (vii) And finally, our approach Disclass
To compute the classification error rates on DB 3 ,weusethe leaving one out (LOO) estimator (Weiss and Kulikowski 1991), which is a special case of the cross-validation estimator. To compute the LOO estimator, one operates as follows: One removes only fier to predict the class of the removed primary structure. This process is iterated in the average of the different classification error rates.
 approach, a nearest neighbor approach and ID3.
To compute the classification error rates on DB 4 , we follow the same procedure fol-lowed in earlier works (Towell et al. 1990). Indeed, Towell et al. divided the database counter-examples (5 )asa testing set . This makes up the data set DB set . This makes up the data set DB 4 (5 ).

As one can see in Table 2, our approach Disclass gives acceptable results. For the approach, LEGAL-E, IPR and ID3. For the construction of Junction 5 , Disclass approach.
In this paper, we have presented a new data mining approach, called Disclass , based on vote strategies to do classification of biological primary structures: Let f be families that represent, respectively, n samples of n sets S mary structures. Let us consider now a new primary structure decision to assign the new primary structure w to one of the sets S taken as follows: (i) During the first step, for each family f the ambiguously discriminant and minimal substrings (ADMS) associated with this family. Because the family f i ,1  X  i  X  n , is a sample of the set S
ADMS are considered also to be ADMS associated with the whole set S the classification process, the ADMS associated with the set S set S i . (ii) During the second step, we compute according to a vote strategy the voice weights of the different ADMS, constructed during the first step. (iii) Finally, during the last step, the set that has the maximum weight of voices is the set to which we assign the new primary structure w .
 primary structures. These strategies are majority vote strategy (MVS), unanimity vote strategy (UVS), similarity vote strategy (SVS) and probability vote strategy (PVS). obtained with the UVS. Generally, the MVS and SVS gave error classification rates Finally, in general, the lowest error classification rates were obtained with the PVS. learning approach, IPR (Maddouri 2000); a nearest neighbor approach (Salzberg et al. 1995) and O X  X eill X  X  approach (O X  X eill 1989; O X  X eill and Chiafari 1989). with the one of encoding methods based on patterns with gaps (Wang et al. 1994, 2001; Elloumi 1998) and those based on regular expressions ; weighted matrices ,e.g. PAM (Dayhoff et al. 1978) and BLOSUM (Henikoff and Henikoff 1992) and hidden Markov Models (HMM) (Krogh et al. 1994).

