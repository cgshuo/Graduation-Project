  X  = {O~,,,, .... 8.., X . ; e~, ..... e~,, }. (3) 
For the labeled documents, the generating component is given by labels y~, we do not need to refer to all mixture components [13]. 
As described earlier, we use the MAP estimation for learning a classifier, argmax o P(OlD). By making use of the Bayes' for-mula and Equation 7, we obtain the MAP estimation of  X , which is equivalent to the value of X  that maximizes the log-posteriori: logP( X  I D) = ~ log(P(y, = cy [ O)P(d, [y, = cfi X )) 
It is difficult to compute Equation 8 directly, because the second term contains a log of summations. Here we introduce the class indicator variables Z, where each z X  e Z is defined to be one or zero according as d~ does or does not come from thejth class. By using the class indicator variables Z, we can write the complete-data log-posteriofi in the form: where the log of the priori P( X ) is approximated by using 
Dirichlet distribution [13]. Since we do not know the exact values of Z, we instead work with their expectation. The algorithm finds a local maximum of the complete-data log-posteriori by iterating the following two steps: where the E-step is the current parameter estimations of probabil-istic labels for every documents calculated by Equation 6, and the 
M-step is the new MAP estimations for the parameters calculated by Equation 4 and 5. 
In this section, we present the parallel implementation of the EM algorithm for text classification. We employ the Single Program 
Multiple Data (SPMD) model in our parallelization. In this model, a single source program is written and each processor executes its personal copy of this program. We assume that we have P proces-sors, where each processor is assigned a unique rank between 0 and P-I and has an individual local memory. The processors communicate with each other by using MPI (Message-Passing Interface) library [6]. 
The EM algorithm starts by using the naive Bayes classifier to initialize the parameters. The E-and M-step are iterated until the change of logP,( X  [D;Z) is less than some predefined threshold. The E-step almost dominates the execution time on each iteration, 
Processor P X  builds the initial global parameters  X g from only the labeled documents Dr, and broadcasts them to all processors 
Processor Pr reads training documents based on its respon-sibility from a disk 
Iterate until convergence 3.1 E-step: Each processor Pr estimates the class of each 3.2 M-step: Each processor Pr re-estimates its own local 3.3 Sum up the local parameters  X t to obtain the new 
Figure 2. The outline of the parallel EM algorithm for text classification. since it estimates the class labels for all the training documents. 
Fortunately, we observe that this step is inherently data parallel, because if the parameters  X  are available for each processor, the same operation can be performed on different documents simulta-neously. We parallelize the loop by evenly distributing the docu-ments across processors. If we partition the N documents into P blocks, each processor handles roughly N/P documents. In other words, P, is given a responsibility for documents di, where i = (r)(N/P) + 1 ..... (r + 1)(N/P). 
Let  X t and  X g be the local and global parameters of the model, respectively. In the training process, the processor P X  first com-putes the global parameters  X g from only the labeled training documents. Then, the processor P X  distributes them to the avail-able processors by using HPI Beast. We assign this task to only processor P0, because the naive Bayes classifier can learn in constant time. Next, each processor P, uses the current global parameters  X s to label the unlabeled documents for its partition. 
Finally, each processor P, calculates its local parameters  X t and calls 
MPI Allreduce to sum up the local parameters  X t to obtain the new global parameters  X s. The algorithm uses these parameters as the current parameters in the next iteration. Since each processor has the same global parameters Og, it can inde-pendently decide when it should exit the loop. Figure 2 gives the outline of the parallel EM algorithm. 
The MPI Allreduco function is a global communication op-processes [6]. The parameters of our model in Equation 3 are the probability estimations consisting of word and document counts among different classes. As a result, the local parameters Ot achieve the global parameters  X g by simply using 
MPI Allreduce with the reduction operation MPI SUM as shown in Figure 3. Our algorithm design can avoid the network bottleneck, because there are only the parameters that exchange across processes. In the test process, we use the final global pa-rameters  X s to classify the test documents that are evenly parti-tioned for each processor as in the E-step. Pl Pe-t 
Figure 3. MPI Allreduce with the reduction operation lq[pT SUM. O(IVM(ND, /p)+VM(ND /P)+I(VM+M)T,~,o), (11) as the operating system. It was constructed by using Beowulf architecture [14]. The configuration of components can be found at http ://pirun. ku. ac. th. 
The 20 Newsgroups data set was used in our experiments [7][10][13]. It consists of 20000 articles divided evenly among 20 different UseNet discussion groups. We extracted all unique words from the entire documents. After removing stop-words, low and high frequency words, we obtained i 1350 unique words. We randomly selected 4000 (20%) of the collection as a test set. The first remaining documents were used to form a labeled training set, containing 6000 documents (30%) drawn at random. The last remaining documents were used as an unlabeled set consisting of 10000 documents (50%). Each set is represented by a document-word matrix. 
Normally, the document-word matrix is very large and sparse, having mostly zero entries. For example, we have 10000 docu-ments and extract 20000 unique words from all the documents. If we use 4 bytes for each element, the matrix requires 10000 x 20000 x 4 = 800 Mbytes of main memory. Although we exploit from the distributed memory, reading this matrix can increase disk access costs. Moreover, it also causes network congestion, since we work on cluster. In order to reduce the matrix size, we look at a compression method called Scalar ITPACK [5]. The idea is to store non-zero elements of the matrix with their rows and column indices. 
To measure the computational efficiency of our parallel algo-rithm, we examined the speedup (S). The speedup is the ratio of the execution time for learning and classifying a document collec-tion on a single processor to execution time for the same tasks on 
P processors. Thus, the speedup of the parallel EM algorithm can be approximated as follow: S = O(IVMNvn" + VMND" ) (12) 
O(IVM(ND, -/P) + VM(ND. ' / P) + I(VM + M)T,~ ) which increases linearly with P, if we have the large numbers of 
No.~. and ND~ . We measured the elapsed time (disk accesses included) from start to complete the task. The classification result of each parallel execution is equivalent to its sequential execution. 
Figure 4 shows the curves of execution time on the 20 News-groups data set. Figure 5 demonstrates the relative speedups. The parallel EM algorithm was run on configurations of up to 16 processors. We varied the number of unlabeled documents to observe the effects of different problem sizes on the performance. 
Three sets were used with the number of unlabeled documents 2500, 5000, and 10000. For each set, the algorithm performs 6, 7, and 8 iterations, respectively. The number of dimensions V was also varied. The unlabeled documents were fixed at 10000, and the number of dinaensions were varied at 6750 and 11350. Figure 6 shows the relative speedups. 
The time of the initial step using the naive Bayes classifier does not affect the performance, since the naive Bayes can learn in constant time. From our experiments, it takes less than 18 seconds 
Figure 4. The execution time of the parallel EM algorithm with 2500, 5000, and 10000 unlabeled documents on 20 
Newsgroups data set. even though it uses the largest size of labeled training documents for learning. The computational time of the algorithm is mostly dominated by the loop in the training process. As we analyzed in the previous section, the speedup curves increase linearly in some cases. For example, on the largest unlabeled set, it achieves the relative speedups of 1.97, 3.72, 7.16, and 12.16 on 2, 4, 8, and 16 processors, respectively. When it accesses to a smaller set of unlabeled documents, the speedup curves tend to drop from the linear curve. The algorithm achieves the relative speedups of 1.82, 3.55, 6.18, and 8.38 on 2, 4, 8, and 16 processors, respec-tively. The smallest unlabeled document sizes give the same trend. If we increase the number of processors further, the speedup curves tend to significantly drop from the linear curve. For a given problem instant, the relative speedups decrease as the number of processors is increased due to increased overheads. This is a normal situation when the problem size is fixed as the number of processors increases. However, it can be solved by scaling the problem size. For example, in Figure 5, the speedups for three sets on 4 processors improve from 3.23 to 3.72, on 8 processors improve from 5.17 to 7.16, and on 16 processors im-prove from 6.46 to 12.16. It can be seen that our parallel algo-rithm yields better performance for the larger data sets. To ensure that the EM algorithm works well with the unlabeled documents, we also re-examined the quality of classification. The number of labeled training documents was varied, and compared the accuracy with the naive Bayes classifier. The parallel EM algorithm accessed to 10000 unlabeled documents in learning process. The parallel execution produced the same classification results as sequential execution. In our experiments, five trials of selecting test/train/unlabeled splits at random were conducted, and each reported accuracy was interpreted as an average over the five 
Figure 5. The relative speedup curves of the parallel EM algorithm corresponding to Figure 4. trials. Figure 7 shows the results on accuracy. We observe that the 
EM algorithm significantly outperforms the naive Bayes classifier when the amount of the labeled documents is small. For example, the EM algorithm achieves 36% accuracy while the naive Bayes classifier reaches 21% accuracy at 20 labeled documents (or one document per category). With 100 labeled documents, the EM algorithm achieves 59% accuracy while the naive Bayes classifier reaches 35% accuracy. The two approaches begin to converge when the amount of the labeled documents is large. We can see that the EM algorithm constantly outperforms the naive Bayes on the 20 Newsgroups data set. However, work by [13] also shows that the EM hurts accuracy on some data sets. When the labeled documents are large, the accuracy curve drops slightly. The rea-son is that the data do not fit the assumptions of the generative model. This indicates that using the simple generative model is inadequate to produce accurate classification results. This prob-lem can be solved by using more complex statistical model. We believe that our parallelization strategy can adapt to that model by adding some parameters. 
In this paper, we presented the paraUelization of the EM algo-rithm in the area of text classification. Since the EM algorithm uses both the labeled and unlabeled documents for learning, the computational time of the algorithm also increases. Consequendy, the parallel processing was applied to the algorithm. We parallel-ized the EM algorithm by using the idea of data parallel computa-tion. We evaluated our parallel implementation on a large Linux 
PC cluster called PIRUN Cluster. The experimental results on the efficiency indicate that our parallel algorithm design has good speedup characteristics when the problem sizes are scaled up. scalable and efficient parallel classification algorithm for mining large datasets. In Proceedings of International algorithms for text categorization. In Third Annual Symposium on Document Analysis and Information 
Retrieval, pages 81-93, 1994. models for naive Bayes text classification. Papers from the 
AAAI Workshop, pages 41-48, 1998. extensions. Iohn Wiley &amp; Sons, 1997. 1997. classification from labeled and unlabeled documents using 
EM. Machine Learning, pages 103-134, 2000. Harnessing the power of parallelism in a Pile-of-PCs. 
Proceedings, IEEE Aerospace. of large doetnnent bases in a parallel environment. JASIS 48(10), pages 932-943, 1997. noncontiguous accesses in MPI/IO. Parallel Computing, pages 83-105, 2002. 
