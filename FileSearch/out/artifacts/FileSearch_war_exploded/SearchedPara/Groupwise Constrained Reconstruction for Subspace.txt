 Ruijiang Li  X  rjli@fudan.edu.cn Bin Li  X  bin.li-1@uts.edu.au Ke Zhang  X  k_zhang@fudan.edu.cn Cheng Jin  X  jc@fudan.edu.cn Xiangyang Xue  X  xyxue@fudan.edu.cn Subspace clustering aims to group the given samples into clusters according to the criterion that samples in the same cluster are drawn from the same linear sub-space. In the last decade, a number of subspace clus-tering methods have been proposed with successful ap-plications in the areas including motion segmentation (Kanatani, 2001; Vidal &amp; Hartley, 2004; Elhamifar &amp; Vidal, 2009), image clustering under different illumi-nations (Ho et al., 2003), etc. Generally speaking, ex-isting approaches to subspace clustering can be classi-fied into the following categories: matrix factorization based, algebraic based, statistically modelling, and re-construction based, among which the reconstruction based approach has been proved most effective and has drawn much attention recently (Elhamifar &amp; Vi-dal, 2009; Liu et al., 2010; Wang et al., 2011). In this paper, we focus on the reconstruction based approach. The objective of the reconstruction based subspace clustering is to approximate the dataset X  X  R D  X  N ( N is the number of samples and D denotes the sample dimensionality) with the reconstruction XW , where W  X  R N  X  N is the reconstruction matrix which can be further used to build the affinity matrix | W | + W &gt; for spectral clustering. The intuition behind the re-construction is to make the value of w ij small or even vanish if samples x i and x j are not in the same sub-space , such that the subspaces/clusters can be easily identified by the subsequent spectral clustering . All the existing reconstruction based methods come with proofs claiming that the desired W could be ob-tained under the subspace independence assumption , i.e., the underlying subspaces S 1 , S 2 ,  X  X  X  , S K are lin-early independent, or mathematically, Unfortunately, this assumption will be violated if there exist bases shared among the subspaces. For example, given three orthogonal bases, b 1 , b 2 , b 3 , and two sub-spaces, S 1 = b 1  X  b 2 and S 2 = b 3  X  b 2 ( b 2 is shared in S 1 and S 2 ), the l.h.s. of Eq.(1) is 3 , which is smaller than the r.h.s. being 4 . In real-world scenarios, the subspace independence assumption does not always hold. For example, in human face clustering, as the number of clusters (persons) increases, the r.h.s. of Eq.(1) will exceed the l.h.s., which is upper bounded by the dimensionality of  X  X uman faces X , so the sub-space independence assumption will be violated even-tually. Figure 1 illustrates this phenomenon based on the Extended Yale Database B (Georghiades et al., 2001). Once the subspace independence assumption is violated, there is no guarantee that the existing recon-struction based methods are able to obtain the desired W . In practice, we observe that the subspace indepen-dence assumption is critical to the success of the exist-ing reconstruction based methods. Once the subspace independence assumption is violated, the performance of these existing reconstruction based methods become far from decent, even though the dimensionality of the underlying subspaces is low (shown in Section 4.1.1). To tackle the subspace clustering problem, we pro-pose a Groupwise Constrained Reconstruction (GCR) model, with the advantage that GCR no longer relies on the subspace independence assumption. In GCR, the sample cluster indicators are introduced as latent variables, conditioned on which the Slab-and-Spike-like priors are used as groupwise constraints to sup-press the magnitude of certain entries in W . Thanks to these constraints, the requirement of the subspace independence assumption is no longer needed to ob-tain the desired W . Our method significantly differs from the existing methods in that, the reconstruction in GCR incorporates the information that  X  X he samples can be grouped into clusters X ; whereas in the existing methods, this information is ignored and the recon-struction depends solely on the data.
 Another advantage of GCR is that, the affinity matrix needed for spectral clustering can be built from the cluster indicators rather than W . In our model, the reconstruction matrix W can be analytically marginal-ized out. We first use Gibbs Sampler to collect sam-ples from the posterior of the cluster indicators, then use the collected samples to build the  X  X robabilistic affinity matrix X , which is finally input to the spectral clustering algorithm to obtain the final clustering re-sult. Compared with | W | + W &gt; , which is used as the affinity matrix in the existing methods, the proba-bilistic affinity matrix built from the cluster indicators is more sophisticated, because it is naturally positive, symmetric and of clear interpretation. The experimen-tal results on synthetic dataset, motion segmentation dataset and human face dataset show that GCR can outperform the state-of-the-art. In this section, we give a brief introduction to the pre-vious works on subspace clustering. 2.1. Non-Reconstruction Based Matrix factorization based methods Costeira &amp; Kanade (1998); Kanatani (2001) approximate the data matrix with the product of two matrices, one contain-ing the bases and the other containing the factors. The final clustering result is obtained by exploiting the fac-tor matrix. These methods are not robust to noise and outliers and will fail if the subspaces are dependent. The algebraic based General Principle Component Analysis (GPCA) (Vidal et al., 2005) fits the samples with a polynomial, with the gradient of a point or-thogonal to the subspace containing it. This approach makes fewer assumptions on the subspaces, and the success is guaranteed when certain conditions are met. The major problem of the algebraic based approach is that the computational complexity is high (exponen-tial to the number of subspaces and their dimensions), which restricts its application scenarios. In (Rao et al., 2010), Robust Algebraic Segmentation (RAS) is pro-posed to handle the data with outliers, but the com-plexity issue still remains.
 Statistical models assume that the samples in each subspace are drawn from a certain distribution such as Gaussian, and take different objectives to find the optimal clustering result. For example, Mixture of Probabilistic PCA (Tipping &amp; Bishop, 1999) uses the Expectation Maximization (EM) algorithm to find the maximum likelihood over all the samples, k -subspaces method (Ho et al., 2003) alternates between assign-ing the cluster to each sample and updating the sub-spaces, Random Sample Consensus (RANSAC) (Fis-chler &amp; Bolles, 1981) keeps looking for the samples in the same subspace until the number of samples in the subspace is sufficient, then continues searching another subspace after removing these samples. Agglomerative Lossy Compression (ALC) (Ma et al., 2007) searches the latent subspaces by minimizing an objective con-taining certain information criteria with an agglomer-ative strategy.
 2.2. Reconstruction Based Reconstruction based methods usually consist of the following two steps: 1) Find a reconstruction for all the samples, in the form that each sample is approxi-mated by the weighted sum of the other samples in the dataset. The optimization problem in Eq.(2) is solved to get the reconstruction weight matrix W . where the term l (  X  ) : R D  X  N 7 X  R measures the er-ror made by approximating x i with its reconstruction P regularization, and  X  is a tradeoff parameter. 2) Apply spectral clustering algorithm to get the final cluster-ing result from the reconstruction weights W . Usually, | W | + | W | &gt; is treated as the affinity matrix input to the spectral clustering methods.
 The methods of this class distinguish from each other in employing different regularization terms, i.e.,  X ( W ) in Eq.(2). In Sparse Subspace Clustering (SSC) (El-hamifar &amp; Vidal, 2009), the authors propose to use the l 1 norm k W k 1 to enforce the sparseness in W , in the hope that the sparse coding process could shrink w ji to zero if x i and x j are not in the same subspace. In Low-Rank Representation (LRR) (Liu et al., 2010), nuclear norm k W k  X  is used to encourage W to have a low rank structure 1 , and l 2 , 1 norm is used as the ` (  X  ) term in Eq.(2) to make the method more robust to outliers. In SSQP (Wang et al., 2011), the authors choose  X ( W ) = W &gt; W be non-negative. As a consequence, the optimization problem in Eq.(2) turns out to be a quadratic pro-gramming problem, for which the projected gradient descend method can be used to find a solution. Consider a clustering task in which we want to group N samples, denoted by X = [ x 1 , x 2 ,  X  X  X  , x N ]  X  R
D  X  N , into K clusters, where N is the number of sam-ples, D is the sample dimensionality, and x i  X  R D de-notes the i -th sample. Let z = [ z 1 ; z 2 ;  X  X  X  ; z N ] be the cluster indicator vector, where z i  X  X  1 , 2 ,  X  X  X  ,K } indi-cates that sample x i is drawn from the z i -th cluster. The goal of subspace clustering is to find the cluster indicators z , such that for each k  X  X  1 , 2 ,  X  X  X  ,K } , the samples in the k -th cluster, i.e., { x i | z i = k } N i =1 side in the same linear space. This objective is quite different from the objective of traditional clustering methods, in which the variance of inter-cluster samples are minimized, such as K-means; or the  X  X ifference X  of clusters are maximized, such as Discriminative Clus-tering (Ye et al., 2007). 3.1. Model Following the idea of the reconstruction based ap-proach to subspace clustering, the Groupwise Con-strained Reconstruction (GCR) model uses p ( X | W ) in Eq.(3) to quantify the reconstruction, where N (  X |  X ,  X ) denotes the Gaussian distribution with mean  X  and variance  X  , w ji is the element at the j -th row, i -th column of matrix W  X  R N  X  N ,  X  2 i &gt; 0 is a random variable measuring the reconstruction error for the i -th sample, and  X  = [  X  1 ;  X  2 ;  X  X  X  ;  X  N ]  X  We place an inverse Gamma prior on all the  X  i  X  X : where IG denotes the inverse Gamma distribution, and  X  &gt; 0 and  X  &gt; 0 are given hyperparameters. What makes the GCR model different is that, GCR explicitly requires every sample to be reconstructed mainly by the samples in the same cluster . In other words, the magnitudes of weights for the samples in different clusters should be small. Intuitively, W should be nearly block-wise diagonal if the samples are rearranged in a proper order (see Figure 2(b) for an illustration). To enforce such property of W , we treat the cluster indicators z as latent random vari-ables, and introduce a prior for W conditioned on z and  X  as follows, where  X  H &gt;  X  L  X  0 are hyperparameters and  X  L  X  small. This prior is quite similar to the Slab and Spike prior used for variable selection (George &amp; Mcculloch, 1997), with  X  H corresponding to the slab and  X  L cor-responding to the spike. As the effects of Eq.(5), to generate W given the latent cluster indicators, if x i and x j are not in the same cluster/subspace, w ji and w ij are restricted to be small or close to the mean value 0 of the corresponding Gaussian distribution; if x j and x i come from the same cluster/subspace, the values of w ji and w ij could be either small or big. We make W dependent on  X  as well, so that both  X  and W can be further marginalized out by combining Eqs.(3), (4) and (5), which will be discussed later.
 Furthermore, we introduce a discrete prior for the cluster indicators z conditioned on  X  = [  X  ;  X  2 ;  X  X  X  ;  X  K ]  X  R K , where Cate( z i |  X  ) =  X  z i notes the categorical distribution, and  X  k  X  [0 , 1] can be viewed as the prior knowledge about the proportion of samples in the k -th cluster. Since it is difficult to set  X  beforehand, we use a Dirichlet distribution as a prior for  X  , where Dir(  X  ) denotes the Dirichlet distribution, and 1 K = [1 , 1 ,  X  X  X  , 1]  X  R K . The hierarchical representation for GCR model is shown in Figure 2(a), and the full probability can be written as follows, Observing that W ,  X  and  X  in Eq.(6) can be marginal-ized out analytically, we can write down p ( z | X ) , de-noted as q ( z ) for short, as follows, where f 0 and f i comes from the first and the second brackets in Eq.(6), respectively; n k ( z ) is the number of samples in the k -th cluster;  X (  X  ) denotes the Gamma function; and I D  X  R D  X  D denotes the identity matrix. 3.2. Obtaining the Final Clustering Result We use the Gibbs Sampling algorithm (MacKay, 2003) to approximate the posterior distribution q ( z ) . In each epoch, for i  X  { 1 , 2 ,  X  X  X  ,N } , the Gibbs sam-pler iteratively updates z i to a sample drawn from i } . A direct implementation will lead to the time com-plexity of O ( N 2 D 3 ) for each epoch. Fortunately, the complexity can be reduced to O ( N 2 D + KD 2 ) using rank-1 update. At the end of each epoch, we collect the values of all the cluster indicators as a sample of z . Finally, we save the samples of z from the last M epochs, denoted as s 1 , s 2 ,  X  X  X  , s M , and discard the samples left. We can use the following two approaches to obtain the final clustering result.
 MAP approach. Use the last collected sample s M as an initialization, then maximize the posterior q ( z ) in Eq.(7) by alternating among z 1 ,z 2 ,  X  X  X  ,z N . The local maximum is directly used as the clustering result. Bayesian approach. With the collected samples, we first compute an affinity matrix G m  X  R N  X  N over the N samples, where then compute the  X  X robabilistic affinity matrix X  G = ing method to obtain the final clustering result. Here, G ij can be treated as an approximation to the posterior distribution p ( z i = z j | X ) . Compared with existing reconstruction based methods which use | W | + | W | &gt; as the affinity matrix input into the spec-tral clustering algorithm, our probabilistic affinity ma-trix G is more sophisticated since G ij can be clearly interpreted as the the possibility that sample i and j share the same cluster label. What is more, our affinity matrix G is naturally positive and symmetric, whereas | W | + | W | &gt; is somehow like an ad-hoc way to  X  X orce X  W to be an affinity matrix. 3.3. When K  X  +  X  From Eq.(8) we see that, to obtain the probabilistic affinity matrix G , it is not mandatory to set K to be the exact number of subspaces. In fact, the proba-bilistic affinity matrix can be obtained with any pos-itive integer K . Particularly, we are interested in the GCR model when K goes to positive infinity, in which case, the number of non-empty clusters remains a fi-nite number (at most N when each sample forms its own cluster). This strategy is in analogy with the In-finite Gaussian Mixture Model with Dirichlet Process (Rasmussen, 1999). For this reason, we refer to the GCR model with K  X  +  X  as GCR-DP. As the limit of GCR, the posterior of z for GCR-DP is where f i remains the same as in Eq.(7), and  X  K is the number of non-empty clusters 2 . The Gibbs sampling procedure is similar to that of the original GCR, and the difference is described as follows. Suppose for now there are K 0 non-empty clusters, to update z i , besides computing K 0 values for the non-empty clusters by plugging z i  X  X  1 , 2 ,,  X  X  X  ,  X  K } into the r.h.s. of Eq.(9), we need to compute an extra value for a new empty cluster by plugging  X  K  X  K 0 + 1 and z i = K 0 + 1 into Eq.(9). Then the Categorical sampler picks a cluster indicator for z i according to these K 0 + 1 values. If the indicator for the new cluster ( K 0 + 1 ) is picked, we create a new empty cluster and put the i -th sample into it. The variables for the empty clusters can be removed to save the computational resource.
 In the case of K  X   X  , Eq.(6) shows that there ex-ists a trade-off among the reconstruction quality, prior for the cluster indicators and p ( W | z ,  X  ) . p ( W | z ,  X  ) prefers more clusters, in which case more spikes in Eq.(5) could be introduced into the model, resulting in high p.d.f. of p ( W | z ,  X  ) . On the contrary, the Dirich-let process prior favors fewer number of clusters. In the premise of good reconstruction quality ( p ( X | W ,  X  ) is high), the competition between the Dirichlet process prior p ( z ) and p ( W | z ,  X  ) provides a way to circum-vent the trivial solutions to the model (all the samples in one cluster or each sample in it X  X  own cluster). Due to the allowance to create more clusters, the out-liers, which cannot be well reconstructed by the inliers, have the chance to  X  X tand alone X . As a result, the in-fluence of the outliers can be reduced. 3.4. Hyperparameters  X  : Throughout our experiment,  X  0 for the Dirichlet distribution is always set to 1 .  X  and  X  : From Eq.(4) we see that  X  and  X  control the reconstruction quality. According to the property of the inverse Gamma distribution, we have E (  X   X  1 i ) = 1 and Var(  X   X  1 i ) = 2  X  2  X  . Thus, it is reasonable to set  X  to a smaller number if the dataset are less noisy, and set  X  to a smaller number if the variance of the recon-struction quality for different samples is higher (e.g., the dataset has more outliers). In our experiments, these two parameters are tuned for different datasets.  X 
H and  X  L : According to Eq.(5),  X  2 i  X  H and  X  2 i  X  L di-rectly influence the magnitude of w ji . Since E (  X   X  1 i , we can use  X  X  H and  X  X  L to control the mag-nitude of w ji intuitively. After integrating out  X  , we can rewrite the prior for W as p ( W | z ) = Q notes the student t distribution with degree of freedom u , mean v and variance w . Therefore, it is natural to use the mean value of the t distribution to control the magnitude of W . In practice, we find that  X  X  H = 0 . 1 In this section, we compare our methods with the other three reconstruction based subspace clustering meth-ods: LRR (Liu et al., 2010), SSC (Elhamifar &amp; Vidal, 2009) and SSQP (Wang et al., 2011). In our eval-uation, the quality of clustering is measured by ac-curacy, which is computed as the maximum percent-age of match between the clustering result and the ground truth. For GCR, the MAP estimation is di-rectly used as the final clustering result; for GCR-DP, we first compute the probabilistic affinity matrix ac-cording to Eq.(8), then use NCut (Shi &amp; Malik, 2000) to get the final clustering result. For MCMC, we treat G trix, and the result of spectral clustering is used as the initialization 3 . This can be understood by switching the rule between sample ( N ) and dimension ( D ), in such a way that G 0 becomes the precision matrix over N samples, and G (0) ij measures the dependency be-tween the i -th and the j -th samples conditioned on the other samples. We set the number of epochs for the Gibbs sampler to 500 , and use the last 100 samples to construct the probabilistic affinity matrix. We find that under such settings, our methods runs faster than SSC and SSQP empirically. 4.1. Synthetic Datasets We use synthetic datasets to investigate how these re-construction based methods perform when the sub-space independence assumption mentioned in Section 1 is violated. The synthetic data containing K sub-spaces are generated as follows: 1) Generate a ma-trix B  X  R 2  X  50 , each column of which is drawn from a Gaussian distribution N (  X | 0 , I 2 ) . 2) For the k -th cluster containing n k samples, generate y 1  X  R n k , the elements of which are drawn independently from the uniform distribution defined on [  X  1 , 1] . After that, generate y 2 = tan 16 k 17 K y 1 (avoiding tan  X  2 ). Fi-nally, generate the n k samples in the k -th cluster as [ y 1 , y 2 ] B  X  R n K  X  50 . All the experiments here are re-peated for 5 times. 4.1.1. Violation of Subspace Independence For K = 2 , 3 ,  X  X  X  , 8 , we generate 7 datasets according to the steps listed above. For these synthetic datasets, the l.h.s. of Eq.(1) is 2, and the r.h.s. of Eq.(1) is K . Thus, the degree of the violation of the subspace in-dependence assumption increases as K increases. The results are reported in Figure 3(a).
 As we can see, LRR and SSC perform well when the subspace independence assumption holds ( K = 2 ) or is slightly violated ( K = 3 ). However, their perfor-mance decreases significantly as the violation degree increases, even though their parameters are tuned for different K . In contrast, GCR and GCR-DP are able to retain high performance even though the violation degree keeps increasing.
 In the case of K = 8 , we compare the affinity matrices produced by these reconstruction based methods, as shown in Figure 4. Obviously, the affinity matrix pro-duced by GCR-DP has stronger discrimination power on the clusters than those of the others. The affinity matrix produced by SSQP looks promising. However, a deep investigation shows that in the matrix the sum of many rows are zero, making the clustering perfor-mance less satisfactory. 4.1.2. Increasing Portion of Noisy Samples Consider the case when there exist samples deviating from the exact positions in the subspaces. Following the previous listed steps, we generate a dataset con-taining 2 subspaces, each of which contains 50 samples. We add Gaussian noises N (  X | 0 , 3) to 0% , 5% ,  X  X  X  , 40% of the samples, respectively. The results on the 9 datasets are reported in Figure 3(b).
 The results show that our methods and LRR are able to maintain high accuracy even though high portion of the samples deviate from their ideal position. The success of LRR is due to the l 2 , 1 norm used for the loss term in Eq.(2), while the success of GCR and GCR-DP may be due to the model in which each sample has its own parameter  X  i to measure the reconstruction error. SSC performs less better, and its performance remains acceptable when the noise level is low. 4.2. Hopkins 155 Dataset We evaluate our models on the Hopkins 155 motion dataset. This dataset consists of 155 sequences, each of which contains the coordinates of about 39  X  550 points tracked from 2 or 3 motions. The task is to group the points into clusters according to their mo-tions for each sequence. Since the coordinates of the points from a single motion lie in an affine subspace with the dimensionality at most 4 (Elhamifar &amp; Vidal, 2009), we project the coordinates in each sequence into 4 r dimensions with PCA, where r is the number of mo-tions in the sequence, then append 1 as the last dimen-sion of each sample. The results are reported in Table 1. This dataset contains a small number of latent sub-spaces, and the results of the compared methods have no significant difference. 4.3. MSRC Dataset In the MSRC dataset, 591 images are provided with manually labeled image segmentation results (each re-gion is given a label, and there are totally 23 labels). Following (Cheng et al., 2011), for each image, we group the superpixels, which are small patches in an over-segmented result, with subspace clustering meth-ods. The groundtruth (cluster label) for a superpixel is given as the label of region it belongs to. In our experiment, 100 superpixels are extracted for each image with the method described in (Mori et al., 2004), and each superpixel is represented with the RGB Color Histogram feature of dimensionality 768 . We discard all the superpixels with label  X  X ackground X , and then discard the images containing only one label. Finally, we get 459 images. For each image, the av-erage number of superpixels is 91 . 3 , and the number of clusters ranges from 2 to 6 . We use PCA to reduce the dimensionality to 20 in order to keep 95% energy. The results are show in Table 2.
 Clearly, our methods outperform the other three on this dataset. GCR also performs better than GCR-DP because it utilizes the information about the number of latent subspaces during the reconstruction step. 4.4. Human Face Dataset We also evaluate our method on the Extended Yale Database B (Georghiades et al., 2001). This database contains 2414 cropped frontal human face images from 38 subjects under different illuminations, and grouping these images can be treated as a subspace clustering problem, because it is shown in (Ho et al., 2003) that the images for a fixed face under different illuminations can be approximately modeled with low dimensional subspace. To evaluate the performance of all these methods, we form 7 tasks, each of which contains the images from randomly picked { 3 , 4 ,  X  X  X  , 9 } subjects, respectively. We resize the images to 42  X  48 , then use PCA to reduce the dimensionality of the raw features to 30 . We repeat the experiment for 5 times and show the results in Figure 5.
 The performance of GCR and GCR-DP are better than the other three methods. In particular, with the number of subspaces increasing, the difference between the l.h.s. and r.h.s. of Eq.(1) increases (see Figure 1). Consequently, the performance of LRR, SSC and SSQP, which rely on the subspace independence as-sumption to build the affinity matrix, degrades quickly. On the contrary, GCR and GCR-DP utilize the infor-mation that  X  X he samples can be grouped into sub-spaces X , thus they are less influenced by the violation of subspace independence assumption.
 We propose the Groupwise Constrained Reconstruc-tion (GCR) models for subspace clustering in this pa-per. Compared with other reconstruction based meth-ods, our models no longer rely on the subspace in-dependence assumption, which usually gets violated in the applications in which the number of subspaces keeps increasing. On the synthetic datasets, we show that existing reconstruction based methods suffer from the violation of the subspace independence assump-tion, while the affinity matrix produced by our model, which is built from the posterior of the latent cluster indicators, is more sophisticated and of stronger dis-crimination power on discovering the latent clusters. On the three real-world datasets, our methods show promising results.
 Besides the subspace clustering problem, the idea of groupwise constraints can be further applied to other problems involving graph construction. For example, in semi-supervised learning (SSL), the constraints can be modified such that a sample is only allowed to be re-constructed by its neighbors in the Euclidean space. In this way, the cluster assumption and manifold assump-tion, which are two fundamental SSL assumptions, can be neatly unified within our framework. For dimension reduction methods such as LLE (Roweis &amp; Saul, 2000), it is also interesting to design new models to use the posterior of the reconstruction matrix for embedding, such that the local and global structure of the data could be preserved simultaneously.
 Acknowledgements We thank the anonymous reviewers for valuable com-ments. This work was partially supported by 973 Pro-gram (2010CB327906), Shanghai Leading Academic Discipline Project (B114), Doctoral Fund of Ministry of Education of China (20100071120033), and Shang-hai Municipal R&amp;D Foundation (08dz1500109). Bin Li thanks UTS Early Career Researcher Grants.
