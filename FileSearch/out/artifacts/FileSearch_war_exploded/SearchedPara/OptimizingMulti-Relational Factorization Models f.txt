 Multi-matrix factorization models provide a scalable and ef-fective approach for multi-relational learning tasks such as link prediction, Linked Open Data (LOD) mining, recom-mender systems and social network analysis. Such models are learned by optimizing the sum of the losses on all rela-tions in the data. Early models address the problem where there is only one target relation for which predictions should be made. More recent models address the multi-target vari-ant of the problem and use the same set of parameters to make predictions for all target relations. In this paper, we argue that a model optimized for each target relation indi-vidually has better predictive performance than models op-timized for a compromise on the performance on all target relations. We introduce specific parameters for each tar-get but, instead of learning them independently from each other, we couple them through a set of shared auxiliary parameters, which has a regularizing effect on the target specific ones. Experiments on large Web datasets derived from DBpedia, Wikipedia and BlogCatalog show the per-formance improvement obtained by using target specific pa-rameters and that our approach outperforms competitive state-of-the-art methods while being able to scale gracefully to big data.

A lot of work has been devoted to analyzing and learning from data in a table format where instances are represented as a feature vector and have a label associated with them. Although such approaches have been quite sucessful, new models able to cope with richer structures in the data are needed. Most data available on the Web have a complex graph structure comprising different relations (e.g., edge types). Thus, mining multi-relational data with noise, par-tial inconsistencies, ambiguities, or duplicate entities, has gained relevance in the last years and found applications in a number of tasks such as link prediction [14], Resource De-scription Framework (RDF) mining [6], entity linking [18], recommender systems [10], and natural language process-ing [9]. However, new paradigms are still needed for statis-tical and computational inference based on multi-relational data.

Recently, multi-relational factorization models have shown to scale well while providing good predictive performance and are currently considered the state-of-the-art for Statis-tical Relational Learning (SRL) tasks [13, 19]. Factoriza-tion models for multi-relational data associate entities and relations with latent feature vectors and define predictions about new relationships through operations on these vec-tors (e.g., dot products). A number of factorization models define one single relation for which predictions should be made, called the target relation, while the other relations are used as side information ( auxiliary relations) [10, 12, 19, 22]. Consider for instance the scenario of online social networks (OSNs), such as Facebook, YouTube, or Flickr, which encourage users to create connections between them-selves or to interesting items (e.g., songs, videos, or news items). The social information (connection between users) can be exploited by recommender systems to provide better recommendations of items of interest (connections between users and items) [10].

In order to illustrate how multi-relational factorization models work, we introduce a running example used across the paper. Consider a social media website where users can follow other users (much like in Twitter), be friends with Figure 1: In this Multi-Relation and Multi-Target ex-other users (forming a social graph) and consume products, e.g., read news items. In this example there are two entity types, namely users U and news items N , and three rela-tions: (i) follows F := U U , (ii) the social relationship S := U U and (iii) the product consumption (e.g. reading of news items) C := U N , as depicted in Figure 1.
Historically, the first multi-relational learning factoriza-tion models were concerned with making predictions for a single target relation based on information provided by a set of other relations which we refer to as auxiliary . However, in domains with many potential target relations, a more inter-esting model class is required in order to make predictions for all targets, e.g., in the context of recommender systems, one is not only interested in recommending news items to a user but also recommending other users whom she might want to follow, or be friends with.

Another example of a task where this is important is the mining of Linked Open Data bases like DBPedia, for in-stance supporting probabilistic queries on such databases and providing estimates of facts that are neither explicitly stated in the knowledge base nor can be inferred from logical entailment [6, 14]. Optimizing the predictions for a number of relations can be seen as a prediction task with multiple target variables. State-of-the-art factorization models ap-proach the problem by sharing the parameters used for pre-dicting all target relations. Instances of such approaches are RESCAL [13, 14], MOF-SRP [9] and SME [3], which share entity specific parameters among all relations in the data. This way, the best solution for the optimization problem is a compromise of the performance on all relations. Although most of these models have been evaluated on multi-target settings, none of them have explicitly investigated the prob-lem of how to optimize each target relation individually in-stead of learning the optimal performance compromise on all relations.

When optimizing a model for one specific relation (so called target relation), neglecting the information on the other relations leads to suboptimal results. Thus state-of-the-art models downweight the contribution of the auxiliary relations to the overall loss function. Thus, the model ex-ploits all the relational information available but it is still op-timized for the target. When optimizing for multiple target relations, we propose to learn a set of single target models each one optimized for one relation while downweighting the other ones. We call this approach Decoupled Target Specific Features Multi-Target Factorization (DMF) .

One drawback of this approach is that the number of pa-rameters to be learned grows too fast with the number of relations. However, when learning a model with DMF, a number of parameters are used only for auxiliary relations and never for predicting the targets. By sharing such param-eters among the models for different targets, one can reduce significantly the amount of memory required by the model. This second approach we call Coupled Auxiliary and Tar-get Specific Features Multi-Target Factorization (CATSMF). This is the first work to specifically investigate how to op-timize multi-relational factorization models for each target relation individually. In summary, the main contributions of this work are: 1. We propose a new factorization approach that opti-2. We also show that coupling the models for different 3. We empirically show the advantage of having specific
Relational data comprise a set of R 2 N relations among a set of entities E . The data for a given relation r 2f 1 ; : : : ; R can be described as D r := f ( e r ; y r ) j e r 2 E r ^ y E r E n r is called the extension of relation r , n r denotes its arity, and y r is a value associated with each observation. In this paper we assume all the relations to be binary, i.e. n r = 2 .

Let X r := E r ; Y r := R be sets called predictor and tar-get spaces of relation r , respectively, for r = 1 ; : : : ; R . The training data for a relation r can be written as D train r X r Y r . Many times y r denotes the truth value of a given observation and can be encoded as y r 2f 0 ; 1 g . As an exam-ple, imagine a binary relation relating countries to their cap-itals. Possible observations could be ( Germany ; Berlin ; 1) and ( Germany ; Hamburg ; 0) .

Let Y r = f ^ y r : X r ! Y r g be the space of all possible prediction models considered and L r : P ( X r Y r ) Y r ! R + be a loss function, where P denotes the power set. Given the training data, the multi-relational multi-target prediction problem is to find R models ^ y r : X r !Y r s.t. for some test data D test r X r Y r ( r = 1 ; : : : ; R ) stemming from the same data generating process as the training data and not being used for learning the models ^ y r , the test error is minimal.

For regression and classification problems, losses L r usu-ally are defined as a sum of pointwise losses  X 
A number of multi-relational datasets consist of positive instances only, e.g., the tuples of entities E in a subset of the extension of the relation. This means that we only observe a subset of the tuples of the type ( x; y ) where y = 1 . In this case we are interested in solving a ranking task where prediction functions Y r = f ^ y r : X r ! R g deliver ranking scores and the losses L r usually are defined pairwise: with pair ranking score losses  X  r : R R ! R + 0 . Finally, problems with additional entity type information can be modeled by choosing X r := E (1) r E (2) r with E (1) r E r E being subsets of entities that can possibly be related through relation r as subjects and objects respec-tively. In this section we discuss related works on Multi-Target Factorization. We introduce the state-of-the-art methods in this field and position our model.

Early work on Statistical Relational Learning (SRL) aims at statistically modeling relational data [7]. SRL combines graphical models such as Bayesian and Markov networks, with knowledge representation formalisms such as first or-der logic for an accurate modeling of the relationships [17]. Another approach to SRL is multi-relational factorization models, which embed entities into a latent space and recon-struct the relations through operations on this space. These embeddings are shared across the relations.

In order to discuss existing work, we make use of the run-ning example of a social media website introduced in Sec-tion 1, where users can follow other users (much like in Twit-ter), be friends with other users (forming a social graph) and consume products, e.g., read news items.

Early factorization approaches for multi-relational learn-ing were concerned with making predictions for a single tar-get relation based on information provided by a set of other relations which we refer to as auxiliary . These approaches learn the model parameters by optimizing the sum over the losses on each relation. In this way, the minimization of the loss on the auxiliary relations acts as a regularization term for the parameters. The overall loss is then a weighted sum of losses and the parameters are learned by optimizing the following loss function: where r 2 R + 0 is a hyperparameter for the contribution of the reconstruction of r to the overall loss and is the regu-larization constant for the model parameters . As already observed in previous work [10, 19], if there is a single target relation t , better results are achieved by having a weighted sum over the losses. The idea behind this is that differ-ent relations contain useful information about the others. For instance knowing which other users a given user follows might give some indication of which kind of news she is in-terested in. Each weight r models how much each relation contributes for the prediction of the target one.
However, in real world scenarios one is often interested in a model that is optimized for making predictions for all the relations. In our social media scenario one is not only interested in recommending news items to a user but also recommending other users whom she might want to follow, or be friends with. The differences between various multi-target approaches lie in (i) their parametrization, (ii) the prediction function ^ y and (iii) the loss function L r for which each relation is optimized. Singh and Gordon provide a unified view of such approaches in a model class they call Collective Matrix Factorization (CMF) [19]. CMF uses the following prediction function: where  X  : E ! R k associates latent features with every en-tity x 2E , with k 2 N being the number of latent features. Approaches like the Coupled Matrix and Tensor Factoriza-tion (CMTF) [1] and MetaFac [11] extended such models to deal with higher arity relations (i.e. relations between more than two entities). For the purposes of this work, we restrict ourselves to relations of arity two, in which case the predic-tion model of CMTF reduces to the same as in Equation 3. MetaFac on its turn introduces a set of global features which for the arity two case can be described as 2 R k k , a diag-onal matrix which is the same across the predictions for all the relations. The MetaFac prediction function for relations of arity two can be given as:
Such models have the advantage of computational ease but can poorly handle relations with a signature clash, i.e., different relations between the same entity types like the friends and follows relation from our example. For instance such a model would predict that every user who follows Barack Obama is also a friend of his.

One way to cope with this issue is to associate feature matrices r 2 R k k with each relation: If the relation features r are diagonal matrices, this model is equivalent to a PARAFAC tensor decomposition [8]. The Semantic Matching Energy (SME) model [3] also uses this approach although with a slightly different prediction func-tion. This solves the signature clash issue but another lim-itation remains. One can easily see that, for the models from Equation 3, Equation 4 and Equation 5 (with diag-when dealing with asymmetric relations, i.e., relations where
In this case the model would predict that Shakira is in-terested in following every user that follows her, which is not necessarily true. Using full instead of diagonal matrices for r yields a model capable of dealing with this problem. This is the prediction model used by RESCAL [13]. The dis-advantage of this model is that it comes at the expense of computational cost, both from processing time and memory standpoints. Another model which we will refer to as Mul-tiple Order Factorization with Shared Relation Parameters (MOF-SRP) 1 [9] aims at reducing the memory requirements by defining relation features as outer products of feature vec-tors.

None of the aforementioned state-of-the-art approaches make any distinction between target and auxiliary relations, and all of them use the same parameters for predicting all the targets, so that the learned parameters are a compromise for the performance over all targets, but not for each specific one. In our work, however, we propose to combine the idea of shared parameters and learn individual entity embeddings for different target relations which in turn leads to better predictive performance.
In state-of-the-art methods, the parameters are learned in such a way that they are optimized for the best perfor-mance compromise over all relations and not for the best performance on each relation individually (cf. Section 3). To see how this is suboptimal for a general model class, we first present an approach that we call Decoupled Target Spe-cific Features Multi-Target Factorization or DMF as a step-ping stone and introduction to our core contribution in this paper, namely: the Coupled Auxiliary and Target Specific Features Multi-Target Factorization (CATSMF) model. Let  X  be the set of model parameters and y r ( ;  X  ) a predic-tion model for relation r parametrized with  X  . Also, let the set of parameters with the best prediction performance on relation r be denoted by  X  r . Such parameters are defined as: Now, suppose the data comprise two distinct target rela-tions, namely r and s . State-of-the-art models solve this problem as follows: Now one would expect that  X  r  X  =  X  s . However, by opti-mizing an objective function like in Equation 2 one is con-strained to solutions of the form  X  r =  X  s =  X  . By definition,
The authors refer to the model as a multiple order factor-ization [9]. and from which it follows that This means that using parameters optimized specifically for each target relation is, in the worst case, at least as good as having one common set of parameters optimized for all relations. Thus a more appropriate solution is to learn one model for each target relation, an approach that we call De-coupled Target Specific Features Multi-Target Factorization (DMF) :  X  r := argmin  X  s := argmin with 0 r;s 1 and 0 s;r 1 and predict using ^ y r;r ( ;  X  r ) for relation r and ^ y s;s ( ;  X  s ) for relation s .
More generally, let ^ y t;r denote the prediction function for a given relation r when another relation t is the target. The loss function of multi-target factorization models can be written as follows:
Predictions for unseen data points are done using ^ y t := ^ y t;t . The functions ^ y t;r for r  X  = t are called auxil-iary reconstructions of relation r for the target relation t . L r is the loss on relation r , as defined in Section 2, and is the importance of relation r when relation t is the target, such that t;t = 1 and 0 t;r 1 .

The DMF framework can be used with models with differ-ent prediction functions ^ y t;r . To illustrate this, let us have a look into how a model like the one from Equation 5 can be learned using this framework. DMF associates one latent feature vector  X  r ( x ) with each instance x for each relation r = 1 ; : : : ; R . Accordingly, different feature matrices are associated with each relation r = 1 ; : : : ; R , one per tar-get t = 1 ; : : : ; R . The prediction function in Equation 5 can be rewritten under the DMF framework as in Equation 7.
The DMF loss decomposes over t and each component can be optimized independently of each other; this is equivalent to R independent models, one for each target relation. An-other point worth noting is that the t;r relation weights are crucial for this model, e.g., setting all of them to 1 is the same as learning the same model R times (up to a random initialization).

To make this argument more clear, let us revisit the social media example. In that example there are two entity types, namely users U and news items N and three relations: fol-lows F := U U , the social relationship S := U U and the product consumption (reading of news items) C := U N . A state-of-the-art multi-factorization model like, for instance RESCAL, would define latent features for users  X  ( U ) , news items  X  ( N ) as well as for the relations F , S and C and learn them as in Equation 8 (regularization terms are omit-ted here to avoid clutter). (  X  ( U ) ;  X  ( N ) ; F ; S ; C ) :=
This way, the same user features  X  ( U ) are used for mak-ing predictions for all relations and thus we will refer to this strategy as complete sharing . Now, suppose one uses differ-ent latent features for different target relations and  X  F  X  ( U ) ,  X  C ( U ) denote the user features used for making pre-dictions for relations F , S and C respectively. Then, it would be possible to learn features such that while models that follow the complete sharing strategy and learn parameters like in Equation 8 are constrained to solu-tions of the form However, when learning the parameters for a given relation, it is important to exploit the information about the other relations. Thus, we can reformulate the multi-target factor-ization problem as a set of single target problems, one for each target relation. This way, the parameters for relation F acting as a target relation are learned as: (  X 
F ( U ) ; F ) :=
The same way, when relation S is the target, the model looks like (  X 
S ( U ) ; S ) :=
Analogously, the same is done for relation C . Since there are three relations, each user u 2 U and news item n 2 N is associated with three latent feature vectors, each corre-sponding to the case where each relation acts as target. This can be seen in Figure 2. There, one can see that when one relation acts as a target the other ones are useful for regu-larizing the parameters for predicting it.

Since the follows ( F ) relation is a relation between users and users, one does not need the feature vectors of news items  X  F ( n ) for predicting it. However, these parameters are useful when learning user features  X  F ( u ) since news item Figure 2: DMF parameters for the social media example. features are needed to regularize user features using the con-sumes ( C ) relation. Hence we dub such parameters auxiliary parameters . In Figure 2, predictive parameters are depicted in a darker blue color and the auxiliary ones in a lighter gray.
 One issue with DMF is that the number of parameters to be learned grows by a factor R of the number of relations in the dataset. When relation feature vectors are used, DMF has in total R 2 k + R jEj k parameters. This is of course un-desirable from the scalability point of view. Furthermore, the fact that individual models are completely decoupled from each other prevents that one benefits from the learn-ing process of the other. To tackle both issues, we pro-pose to couple the models by sharing the parameters used for auxiliary relations. We call this approach the Coupled Auxiliary and Target Specific Features Multi-target Factor-ization (CATSMF) and it represents our core contribution. The prediction model from Equation 5 written using the CATSMF approach is as follows: ^ y where E (1) r E and E (2) r E are the sets of entities that could possibly occur as the subjects and the objects of rela-tion r , respectively, as defined in Section 2. This means that entities occurring within the target relation t are associated with target specific features  X  t , while entities that do not occur within the target relation t are associated with aux-iliary features  X  0 (pooled over all target relations). Every relation r has two feature matrices: one when used as target r; 1 and another one when used as auxiliary relation r; 0
While DMF defines a full set of parameters for each rela-tion, CATSMF defines parameters needed to make the pre-dictions for each target relation, plus one full set of auxiliary ones. For example, if a given entity x does not occur in a for x and thus  X  t ( x ) is never used and can be dropped. For instance, if r is the relation father-of , E (1) r and E (2) correspond to the subset of persons, but not, say, locations, and if r  X  is the relation capital-of , E (1) r  X  corresponds to the subset of cities while E (2) r  X  corresponds to the subset of coun-tries, but not persons. This means, that for two given enti-ties, a person John and location Berlin ,  X  capital-of ( John ) and  X  father-of ( Berlin ) need not be computed. Besides leading to a lower number of parameters, taking into consideration entity Figure 3: CATSMF parameters for the social media ex-types can lead to better predictive performance as observed in Section 5 among the results of our experiments.
Figure 3 shows the CATSMF setup for the social media example. Note how the number of parameters is reduced in comparison to DMF by sharing auxiliary parameters. In Figure 2 one can see that there is a lot of redundancy in DMF regarding auxiliary parameters. There are two copies of auxiliary parameters for item features and two copies of each relation auxiliary features. What CATSMF does is es-sentially to define one set of auxiliary parameters and share them through the cases of different target relations.
CATSMF has two main advantages over DMF: (i) since the auxiliary parameters are shared across the models for different target relations, such models are coupled and can profit from each other. (ii) CATSMF allows for a lower num-ber of parameters. The number of latent features needed by CATSMF is 2 Rk + r [E (2) r to simplify the notation. The lower the bigger the savings in the number of parameters. Even in the worst case scenario, where there is no entity type in-formation available, i.e., if E r = E for all r = 1 ; : : : ; R , the number of parameters required by CATSMF is 2 Rk + R jEj k . This means that, while the relationship between the number of parameters and the amount of relations is quadratic for DMF, for CATSMF it is linear.

CATSMF is learned through stochastic gradient descent as shown in Algorithm 1. The algorithm starts by initial-izing the parameters, drawing them from a 0-mean normal distribution (lines 2 X 7). Then, a target relation t is uni-formly sampled and a stochastic gradient descent update is made in one observation of t (lines 9 X 10) according to Al-gorithm 2. Finally, another relation r is uniformly sampled and an update on this relation, acting as an auxiliary rela-tion for t , is performed (lines 11 X 12). We do this oversam-pling of target specific parameters to guarantee that they are more often updated than the auxiliary ones, which leads to faster empirical convergence.

The parameter update is described in Algorithm 2. The first step is to uniformly sample an observation ( x 1 ; x D r (line 2). The next step is to determine the parameters to If r = t , then it plays a target relation role and only the tar-get specific parameters regarding t , namely t; 1 ,  X  t ( x  X  ( x 2 ) are updated. In case r  X  = t , then r plays the role of an auxiliary relation. In this case the auxiliary features r; 0 used. If x 1 is among the entities related by t , i.e., x Algorithm 1 CATSMF 1: pro cedure LearnMultiTarget 2: 8 x 2E  X  0 ( x ) N (0 ; 2 ) 3: for r = 1 ; : : : ; R do 5: r; 0 ( x ) N (0 ; 2 I ) 6: r; 1 ( x ) N (0 ; 2 I ) 7: end for 8: repeat 9: t Uniform (1 ; R ) 10: (  X ; ) = UpdateModel ( t; t; D t ;  X ; ; ; t ) 11: r Uniform (1 ; R ) 12: (  X ; ) = UpdateModel ( t; r; D r ;  X ; ; ; t ) 13: until convergence 14: end procedure Algorithm 2 CATSMF Stochastic Gradient Descent Update 1: pro cedure UpdateModel 2: ( x 1 ; x 2 ; y ) Uniform ( D r ) 3: r  X  t ( x 1 2E t ) 5: r  X  t ( x 2 2E t ) 7: r  X  r ( t = r ) 9: return (  X ; ) 10: end procedure then  X  t ( x 1 ) is used, otherwise the auxiliary features  X  are in place. We proceed analogously for x 2 . Finally, the chosen parameters are updated with a stochastic gradient descent step (lines 4, 6 and 8).
 Often overlooked in the multi-relational factorization liter-ature are bias terms. We use target-specific and auxiliary bias terms. The prediction function is the following: ^ y The parameters in this prediction function should be opti-mized for the task at hand, which is to make predictions based on positive only observations. In [6], it is empirically shown that the BPR optimization criterion (BPR-Opt) [16] is suitable for this task.

Let ( x ) = 1 instance of a pairwise loss and can be defined for a general multi-relational learning task as follows:
In this section, CATSMF and DMF are compared against each other and against state-of-the-art competitors. More specifically, we examine the impact of using target specific parameters as well as of considering target and auxiliary roles for relations.

The evaluation assesses the behavior of CATSMF on prac-tical Web applications using three large Web datasets. The datasets used in the experiments are described next, fol-lowed by the metrics and evaluation protocol and the state-of-the-art baselines used in the experiments. We conclude the section presenting the detail of the results of our empir-ical study.
 In our experiments, we used three large Web datasets col-lected from DBpedia, Wikipedia and BlogCatalog. DBpedia is one of the central interlinking-hubs of the emerging Web of Data, 2 which makes it really attractive to evaluate multi-relational learning approaches. The Wikipedia-SVO dataset has one of the highest number of relations among published multi-relational datasets [9]. The BlogCatalog dataset [21] has been used in the literature to evaluate recommender sys-tems that exploit social network information [10, 21]. The three Web datasets are detailed as follows: The dataset is split into training, validation, and test set. First, 10% of the positive tuples are randomly selected and assigned to the test set. Then, we randomly sample 10% of the remaining ones to form the validation set. The re-maining triples are used for training. To reduce variability, 10-fold cross-validation was performed. The results reported are the average over the rounds considering 99% confidence intervals. For this evaluation, we follow a protocol based on [4] as described next.

For each relation r and entity x on the test set: 1. First, we sample from r x f ( x; x 2 ; y ) j ( x; x 2 2. Then, we compute the score for the j r x j negative triples 3. Finally, we measure the precision and recall at n = Parameter Setting. For each dataset, split and model, we tune the hyperparameters using the train and validation set through grid-search. Next, the models were retrained on both train and validation sets and evaluated on test parti-tion. The results reported correspond to the performance of the methods on the test set only. This process was per-formed for all the models in the evaluation, including the baselines. Regarding hyperparameter values, the number of latent features k was searched in the range f 10 ; 25 ; 50 for all baselines and variants of our approach. The val-ues for t;r , r and were searched in f 0 : 25 ; 0 : 5 ; 0 : 75 f 0 : 0001 ; 0 : 001 ; 0 : 01 g and f 0 : 0005 ; 0 : 005 ; 0 : 05 All the hyperparameters for the baselines were searched in the ranges suggested by their respective authors in their pa-pers.
 As far as the approach proposed here is concerned we want to make sure that any effects observed come from the usage of predictive and auxiliary features and not from a specific loss or how relation feature matrices look like. Thus, three variants of the same prediction model are evaluated. They are detailed as follows: ) Relation loss Relation ) BPR Diagonal Matrix Complete ( x +
Our approaches are also compared against the following state-of-the-art models: Table 1 presents a summary of the approaches evaluated. In the DBpedia and BlogCatalog datasets we set t;t = 1 and estimated both the t;r and the t values through grid search. On the Wikipedia-SVO dataset is infeasible to esti-mate each t;r value through grid search, given the number of relations in this dataset. Therefore, we set each t;r = a , for t  X  = r , where a is a hyperparameter estimated on val-idation data using grid search. We did the same for the regularization constants, i.e., setting all t = and opti-mizing . Figure 4 shows the precision-recall curves with the results of our evaluation.

When analyzing the results we observe that our approach excels in the three Web datasets used in the empirical eval-uation as seen in Figure 4. However, since the models evalu-ated use different parametrization, prediction, and loss func-tions, we need to explore in more detail which aspects of the models are responsible for the relative differences in per-formance. Therefore, to answer the question: what is the impact on prediction performance of using target specific parameters, while using the same prediction function and the same relation specific losses? the first important as-pect to observe is how the CATSMF-Diag and DMF-Diag approaches compare to the Shared-Diag approach. This is because they are essentially the same model where each individual relation is optimized for the same loss function and the only differences between them are whether they use target specific predictive parameters or not, and the corre-sponding strategy used to this end. When comparing those three approaches, one can clearly see that both DMF and CATSMF outperform the complete sharing approach.

The results show that using target specific parameters im-proves over the complete parameter sharing scenario while using both shared and target specific parameters gives an even stronger performance boost.

In the comparison against the state-of-the-art approaches, the differences in performance can be mostly explained by the usage of different loss functions for individual relations. The BPR loss deals better with the scenario where only positive observations are available. On the other hand, the pairwise interactions modeled by MOF-SRP seem to play an important role in the DBPedia dataset, which explains the good performance of this model there. The fact that mod-eling pairwise interactions leads to better predictive perfor-mance on RDF datasets has been observed before in [6]. It is important to note that RESCAL and MOF-SRP also can be used within the target specific parameter framework offered by CATSMF and DMF.
 CATSMF has clearly the best performance in the Blog-Catalog and Wikipedia-SVO datasets. 5
We believe that the poor performance of RESCAL on the BlogCatalog dataset is due to the optimization for the squared error since it has been observed that models opti-mized for the BPR loss perform much better on this partic-ular dataset [10].
 We observe that MOF-SRP is not as competitive on the BlogCatalog dataset as it is on the other ones. One possible explanation is that this model does not take into account entity type information. This means that when learning on the User-Blog relation, the MOF-SRP assumes that users are also potential items to be recommended. As reported by [9], negative examples are sampled when learning the model, but since it does not differentiate between users and blogs and there are 10,312 users and only 39 blogs, one can expect that approximately 99.6% of the sampled negative examples are trivial ones containing recommendations of users. One can see this when looking into the performance on the in-dividual relations. On the social User-User relation, MOF-SRP achieves 0.901 AUC 6 against 0.961 AUC of CATSMF. On the User-Blog relation however the AUC for MOF-SRP
In addition to the results reported here, we reproduced the same experiment on the Wikipedia-SVO dataset performed by [9], where the hit rate at the top 5% (referred in their paper as p@5) and 20% (referred to in the original paper as p@20) is measured. CATSMF achieves a p@5 of 0.74 and a p@20 of 0.95, while MOF-SRP is reported to achieve 0.75 and 0.95, respectively.
AUC: area under the ROC curve. [2] is 0.481 against 0.825 of CATSMF. These results suggest MOF-SRP was not able to learn accurately the information about entity types for this dataset.

We discuss here the impact of (i) using auxiliary relations with CATSMF and (ii) optimizing each t;r value individu-ally instead of setting t;r = a and optimizing a (i.e., use the same value for all parameters). In Figure 5, we see the per-formance of CATSMF-Diag-BPR model on the BlogCatalog dataset under these various settings. This dataset has two relations, the social relationship s and the interest of users in blogs i , thus leaving us with just two values to optimize: s;i and i;s , since i;i = s;s = 1 . Each curve denotes a different setting of each t;r , i.e., setting all of them to the same value or optimizing them individually. One can see that by using no auxiliary relations ( = 0 ) the model presents its worst performance whereas by optimizing each relation weight individually, it exhibits its best performance. Figure 5 also provides empirical evidence that, in datasets with a large number of relations, where setting each relation weight individually might be costly or even infeasible, set-ting them to the same value might be a good compromise for the tradeoff between predictive performance and cost of model selection. Figure 5: Performance of CATSMF on the BlogCatalog
Here we report the average runtime over 10 single thread runs on the DBpedia dataset on a Xeon E5620 2.40GHz CPU. The average duration is 754 seconds for CATSMF-Diag-BPR, 7039.5 seconds for RESCAL and 77406.25 sec-onds or, approximately, 21 hours for MOF-SRP on the DB-pedia dataset, which shows that CATSMF-Diag-BPR scales much better w.r.t. runtime while providing very competitive prediction performance. There are two main reasons that can explain the better runtime performance of CATSMF-Diag-BPR: (i) the fact that CATSMF-Diag-BPR uses a di-agonal matrix for relation features whereas RESCAL uses a full matrix and MOF-SRP a matrix represented as outer products of feature vectors and (ii) we learn CATSMF-Diag-BPR using the scalable stochastic gradient descent learning algorithm.

As discussed in Section 3, using a diagonal matrix as re-lation features may not be the best choice in terms of pre-diction performance. However, as shown in Figure 4 the target-specific strategy of CATSMF-Diag-BPR improves the results making it competitive against state-of-the-art models while still having much lower runtime.

DMF and CATSMF implementations are available on-line. 7 For RESCAL and MOF-SRP we used the implemen-tations provided by the authors.
In this work we argue and show empirically how multi-relational factorization models can benefit from using differ-ent parametrizations of prediction functions for individual target relations. We first introduce a naive set of decoupled models, one for each target relation called DMF, followed by a more memory-efficient variant with shared auxiliary pa-rameters: CATSMF, which has fewer parameters thus scales better. The novelty of DMF and CATSMF lies in the fact that they learn different sets of parameters for reconstruct-ing particular target relations. In contrast to the trivial DMF solution of learning one model per target, where all models are completely decoupled from each other, CATSMF defines parameters to be used when a given relation plays an
Co de available at http://ismll.de/catsmf auxiliary role, which are shared among all different models for the various targets.

Our experiments show that (i) CATSMF is able to scale to large datasets better than state-of-the-art models, while still achieving competitive predictive performance; (ii) CATSMF and DMF are always at least as good as the standard ap-proach of using the same set of parameters for all target rela-tions, but often outperforms it; (iii) CATSMF outperforms competitor models in the Linked Open Data mining, natural language processing, and recommender systems tasks.
We are currently exploring how to effectively estimate the relation weights t;r and the regularization constants from the data, considering that setting them through model selection might be infeasible even for a moderate number of relations. Two promising approaches are (i) based on adaptive regularization [15] and (ii) learning the model in a Bayesian framework and estimate the hyperparameters us-ing hierarchichal models similar to what Singh and Gordon propose [20]. As future work, we plan to investigate a more memory efficient CATSMF variant, e.g., by reducing the number of parameters to be learned. One possible alter-native to this end would be to introduce an  X  1 regularizer so as to remove some of the small parameters and trim the model even further. An additional direction for future work is the extension of our framework for streaming data scenar-ios, e.g., [5], where the model parameters have to be learned online without compromising ranking performance.

