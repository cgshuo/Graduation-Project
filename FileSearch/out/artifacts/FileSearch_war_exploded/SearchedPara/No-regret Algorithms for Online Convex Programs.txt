 In a sequence of trials we must pick hypotheses y answer is revealed as a convex loss function  X  loss is therefore L our loss would have been P t  X  1 losses, with positive regret meaning that we would have pref erred y to our actual plays: We assume that Y is a compact convex subset of R d that has at least two elements. In classical no-regret algorithms such as weighted majority, Y is a simplex: the corners of Y represent pure many more extreme points than dimensions, n  X  d . For example, Y could be a convex set like { y | Ay = b, y  X  0 } for some matrix A and vector b , or it could even be a sphere. The shape of Y captures the structure in our prediction problem. Each poin t in Y is a separate bedded in the common representation space R d . While we could run a standard no-regret algorithm such as weighted majority on a structured Y by giving it hypotheses corresponding to the extreme points c corresponding loss in runtime and generalization ability) .
 Our algorithms below are stated in terms of linear loss funct ions,  X  but convex, we can substitute the derivative at the current p rediction,  X  X  X  OCP problem dates back to Hannan in 1957 [2]. The name  X  X nline convex programming X  is due to Zinkevich [3], who gave a clever gradient-descent algori thm. A similar algorithm and a weaker  X  clever algorithm for OCP was presented by Kalai and Vempala [ 4].
 previous bounds for general OCP algorithms are sublinear in the number of experts, but logarithmic core result, Thm. 4 below, takes only half a dozen short equat ions to prove.
 consider choosing predictions from an essentially-arbitr ary decision space and receiving outcomes from an essentially-arbitrary outcome space. Together a de cision and an outcome determine how a marker R t  X  R d will move. Given a potential function G , they present algorithms which keep Cesa-Bianchi and Lugosi X  X  to online convex programming.
 be additive (that is,  X ( u ) = P online convex programming problems. The most restrictive r equirement is additivity; for example, when defining potentials for OCPs via Eq. (7) below, unless th e set  X  Y can be factored as  X  Y . . .  X   X  Y N the potentials are generally not expressible as f ( X ( u )) for additive  X  . During the preparation of this manuscript, we became aware o f the recent work of Shalev-Shwartz and Singer [9]. This work generalizes some of the theorems in [6] and provides a very simple and elegant proof technique for algorithms based on convex pote ntial functions. However, it does not consider the problem of defining appropriate potential func tions for the feasible regions of OCPs requirement for applying potential-based algorithms to OC Ps.
 Figure 1: A set Y = { y dark line) and its safe set S (light shaded region). Lagrangian Hedging algorithms maintain their state in a regret vector , s with the base case s contains information about our actual losses and the gradie nts of our loss functions: from s find our regret versus any y as follows. (This property justifies the name  X  X egret vector . X ) We can define a safe set , in which our regret is guaranteed to be nonpositive: The goal of the Lagrangian Hedging algorithm is to keep its re gret vector s to the cone of unnormalized hypotheses: form is shown in Fig. 2. At each step it chooses its play based o n the current regret vector s be small when s is in the safe set, and large when s is far from the safe set.
 For this Y , two possible potential functions are where  X  &gt; 0 is a learning rate and [ s ] weighted majority [13] algorithms, while the potential F Theorem B]. For more examples of useful potential functions , see [6].
 To ensure the LH algorithm chooses legal hypotheses y constant 0 is arbitrary; any other k would work as well) Theorem 1 The LH algorithm is well-defined: define S as in (2) and fix a finite convex potential function F . If F ( s )  X  0 for all s  X  X  , then the LH algorithm picks hypotheses y (Omitted proofs are given in [6].) We can also define a version of the LH algorithm with an ad-justable learning rate: replacing F ( s ) with F (  X s ) is equivalent to updating s Adjustable learning rates will help us obtain regret bounds for some classes of potentials. as the gradient form (Fig. 2), but on each step it computes F and  X  X  by solving an optimization problem involving W and the hypothesis set Y (Eq. (8) below).
 For example, two possible hedging functions are If potentials F majority and external-regret matching algorithms. For an e xample where the hedging function is The optimization form of the LH algorithm using hedging func tion W is defined to be equivalent to the gradient form using Here  X  Y is defined as in (3). 2 To implement the LH algorithm using the F of Eq. (7), we need an efficient way to compute  X  X  . As Thm. 2 below shows, there is always a  X  y which satisfies pseudocode as the gradient form (Fig. 2), but uses Eq. (8) wit h s = s the unit simplex in R d ,  X  Y is the positive orthant in R d . So, with W problem (8) will be equivalent to That is,  X  y is the projection of s onto R d this projection replaces the negative elements of s with zeros,  X  y = [ s ]  X  y back into (7) and using the fact that s [ s ] Theorem 2 Let W be convex, dom W  X   X  Y be nonempty, and W (  X  y )  X  0 for all  X  y . Suppose the W is equivalent to the gradient form of the LH algorithm with po tential function F . Our main theoretical results are regret bounds for the LH alg orithm. The bounds depend on the Eq. (1): if F is too curved then  X  X  will change too quickly and our hypothesis y a lot, while if F is too flat then we will not react quickly enough to changes in r egret. We will state our results for the gradient form of the LH algor ithm. For the optimization form, instead. Therefore, we never need to work with (or even be abl e to write down) the corresponding tuning learning rates. The choice of learning rate below and the resulting bound are the same as replace W (  X  y ) with W (  X  y/ X  ) .
 We will need upper and lower bounds on F . We will assume for all regret vectors s and increments  X  , and We will bound the size of Y by assuming that for all y in Y . Here, kk The size of our update to s the vector u . We have already bounded Y ; rather than bounding C and u separately, we will assume that there is a constant D so that for all possible values of c are valid plays, we need not randomize, so we can drop the expe ctation in (12) and below.) in t , they show that Lagrangian Hedging is a no-regret algorithm when we choose an appropriate potential F .
 Theorem 3 Suppose the potential function F is convex and satisfies Eqs. (4), (9) and (10). Suppose achieves expected regret versus any hypothesis y  X  X  .
 If p = 1 the above bound is O ( t ) . But, suppose that we know ahead of time the number of trials t we will see. Define G ( s ) = F (  X s ) , where Then the LH algorithm with potential G achieves regret for any hypothesis y  X  X  .
 The full proof of Thm. 3 appears in [6]; here, we sketch the pro of of one of the most important intermediate results. Thm. 4 shows that, if we can guarantee E ( s level sets of F are related to S , we will be able to show s Theorem 4 (Gradient descent) Let F ( s ) and f ( s ) satisfy Equation (9) with seminorm kk and constant C . Let x constant so that E ( k x P obvious that the base case holds for t = 1 .) Then: which is the desired result. 2 play a repeated matrix game. These two tasks are essentially equivalent, since they both use the probability simplex Y = { y | y  X  0 , P standard no-regret algorithms such as Freund and Schapire X  s Hedge [5], Littlestone and Warmuth X  X  special cases of the LH algorithm.
 A large variety of other online prediction problems can also be cast in our framework. These prob-and online balancing of a binary search tree [4]. More uses of online convex programming are dimensionality of the appropriate hypothesis set and subli near in the number of trials. the LH algorithm with the potential function W of one-card poker. (The hypothesis space for this learning p roblem is the set of sequence weight vectors , which is convex because one-card poker is an extensive-for m game [17].) In one-card poker, two players (called the gambler and the dealer ) each ante $1 and receive one $1). In contrast to the usual practice in poker we assume that the payoff vector c each hand; the partially-observable extension is beyond th e scope of this paper. One-card poker is a simple game; nonetheless it has many of th e elements of more complicated games, including incomplete information, chance events, a nd multiple stages. And, optimal play 45679 and 24679 are almost equally strong hands in a showdown (they are both 9-high), holding it into a straight.
 gambler is playing a fixed policy. The x -axis shows number of hands played; the y -axis shows indicated with a dotted line. The middle solid curve shows th e actual performance of the dealer (who is trying to minimize the payoff).
 a strategy y avg worst-case value of y avg knows y avg the progress of the gambler X  X  learning.
 never plays a minimax strategy, as shown by the fact that the u pper curve does not approach the value of the game. Instead, she plays to take advantage of the gambler X  X  weaknesses. In the left strategies for both players are minimax.
 result from the dealer and the gambler  X  X hasing X  each other a round a minimax strategy. One player will learn to exploit a weakness in the other, but in doing so w ill open up a weakness in her own minimax equilibrium. This cycling behavior is a common phen omenon when two learning players value of the game, but our regret bounds eliminate this possi bility. We have presented the Lagrangian Hedging algorithms, a fami ly of no-regret algorithms for OCP based on general potential functions. We have proved regret bounds for LH algorithms and demon-bounds for LH algorithms have low-order dependences on d , the number of dimensions in the hy-problems with complicated hypothesis sets; these problems would otherwise require an impractical amount of training data and computation time.
 Our work builds on previous work in online learning and onlin e convex programming. Our contribu-from a more general class of potential functions; and a new wa y of building good potential func-vector ( e.g. , as might happen in an extensive-form game such as poker).
 Acknowledgments Thanks to Amy Greenwald, Martin Zinkevich, and Sebastian Th run, as well as Yoav Shoham and his research group. This work was supporte d by NSF grant EF-0331657 and DARPA contracts F30602-01-C-0219, NBCH-1020014, and HR00 11-06-0023. The opinions and conclusions are the author X  X  and do not reflect those of the US government or its agencies. [2] James F. Hannan. Approximation to Bayes risk in repeated play. In M. Dresher, A. Tucker, and [3] Martin Zinkevich. Online convex programming and genera lized infinitesimal gradient ascent. [4] Adam Kalai and Santosh Vempala. Geometric algorithms fo r online optimization. Technical [5] Yoav Freund and Robert E. Schapire. A decision-theoreti c generalization of on-line learning [8] David Blackwell. An analogue of the minimax theorem for v ector payoffs. Pacific Journal of [11] Eiji Takimoto and Manfred Warmuth. Path kernels and mul tiplicative updates. In COLT , 2002. [12] R. Tyrell Rockafellar. Convex Analysis . Princeton University Press, New Jersey, 1970. [13] Nick Littlestone and Manfred Warmuth. The weighted maj ority algorithm. Technical Report [14] Sergiu Hart and Andreu Mas-Colell. A simple adaptive pr ocedure leading to correlated equi-[15] H. Brendan McMahan, Geoffrey J. Gordon, and Avrim Blum. Planning in the presence of cost
