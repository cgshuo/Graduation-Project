 Henk van den Heuvel  X  Dorota Iskra  X  Eric Sanders  X  Folkert de Vriend Abstract Spoken language resources (SLRs) are essential for both research and application development. In this article we clarify the concept of SLR validation. We define validation and how it differs from evaluation. Further, relevant principles of SLR validation are outlined. We argue that the best way to validate SLRs is to external and experienced institute. We address which tasks should be carried out by illustrating how validation can prove its value throughout the production phase in terms of pre-validation, full validation and pre-release validation.
 Keywords Quality Assessment Validation Evaluation Language resources Spoken language resources Abbreviations ASR Automatic speech recognition DTD Document type definition ELRA European language resources association IMDI ISLE meta data initiative LR Language resource OLAC Open language archives community POS Part of speech SLR Spoken language resource SPEX Speech processing expertise centre TTS Text to speech QQC Quick quality check QQC_DB QQC on database QQC_DF QQC on description form WLR Written language resource 1 Introduction quality assessment a key issue in LR production. Both terms Quality and Assessment need some definition in this context. Cieri ( 2006 ) argued that the quality of an LR cannot be expressed on a single dimension  X  X ood X  X ad X , but comprises multiple dimensions. We identify the principle dimensions of LR quality as:  X  Consistency (both internal and with documentation)  X  Suitability/usability for the need of the users  X  Reusability/extensibility of the data  X  Compliance with best practices  X  Completeness and clarity of the documentation  X  Validation by independent validator  X  Accessibility
In this context assessment is the process of collecting valid and reliable information about an LR, integrating it, and interpreting it to make a judgement or a quality of LRs over the last decade: evaluation and validation .

Evaluation of an LR implies testing it by employing the LR in an actual application (Dybkjaer et al. 2007 ). An evaluation does not only require data sets but that reason the result of the evaluation is dependent on (the quality of) both: data and engines, and one can evaluate either one or both. Evaluation commonly focuses on the quality of systems or system components, as in the NIST Spoken Language accessible before the evaluation; obviously the evaluation database is the same for every comparative test in the evaluation. Alternatively, LRs as such can be evaluated; to that end the performance of various LRs on the same system can be measured. For instance, the usefulness of a speech database with car recordings can be evaluated by showing that a speech recognition engine trained on this database recorded in the car environment.
Validation refers to the other approach to assess the quality of the LR. Validation of an LR is defined as a check of an LR against its specifications, augmented by a set of tolerance margins for deviations of these specifications (Van den Heuvel et al. and tolerance margins are the validation criteria for an LR. Output of a validation is results of the checks. Validation does not involve application testing to judge the quality of the data.

Evaluation and validation are both essential means of quality assessment. Training, (development), and test databases should be properly validated before the evaluation assessment measures that are independent of and complimentary to each other.
This paper deals with the validation of LRs, more specifically of spoken language between a mere collection of speech and an actual SLR is  X  X  X he fact that the latter is cannot strictly be called SLRs, even when these annotations clearly refer to spoken versions of the database entries, as is the case for phonemic transcriptions.
The relevance of validation of large SLRs emerged when the SpeechDat project produced in a European framework according to design and recording specifications similar to the American-English Macrophone corpus (Bernstein et al. 1994 ) and the Dutch Polyphone corpus (Den Os et al. 1995 ). The SpeechDat SLRs were, however, produced by a large consortium, the idea being that each consortium member would produce from one to three SLRs and obtain the SLRs produced by the other partners included in the consortium as the validation centre with the task of monitoring the Another objective of SpeechDat was that the SLRs would become available to third independent validation centre.

Since SpeechDat, SPEX has been involved as a validation centre in many Car (Moreno et al. 2000a ), SpeeCon (Iskra et al. 2002 ), and OrienTel (Iskra et al. 2004 ). The experience on SLR validation gained over the years has been reported at annotated speech corpora including lexicons for prompted speech recordings). Much of our expertise has been developed in close cooperation with ELRA and its validation committee.
 intend to present or analyze a survey of errors that we came across as a validation centre. Our main purpose is to convey that validation is an essential element in the quality assessment and quality assurance of LRs, and to pinpoint the relevant issues involved in LR validation, more particularly in SLR validation. 2 Validation basics and principles Basic aspects of SLR validation have been addressed previously in Van den Heuvel overview of SLR validation is also presented by Maegaard et al. ( 2005 ). Most of the issues presented in this section are so general that they apply to other LRs as well. 2.1 Objectives presents a systematic survey of the validation criteria and the degree to which they were met by the SLR. It can serve a variety of purposes: 1. Quality assurance: in this case the validation report attests that the SLR meets 2. Quality improvement: the validation report shows where the SLR can be 3. Quality assessment: the validation report can be added as an appendix to the 2.2 Strategies SLR validation can be performed in two fundamentally different ways: (a) Quality assessment issues and checks are addressed in the specification phase of the SLR. formulated, and during the recording process pre-validations on the data are carried procedure are defined (and carried out) afterwards. Furthermore, validation can be done either in house by the producer (internal validation) or by another organization (external validation). This is schematically shown in Table 1 .

Internal pre-production validation (1) in this table is in fact essential for proper during collection and processing of the data. Internal post-production validation (2) should be an obvious part of this procedure. These principles are employed by the Linguistic Data Consortium (LDC) (Cieri and Liberman 2000 ; Strassel et al. 2003 ). The LDC has an independent validation team as part of their organization (Cieri, carried out in a consortium. Combined with external post-production validation (4), this strategy was adopted by many European Union (EU) funded projects, where all producers performed internal quality checks, whilst SPEX served as an independent performing intermediate and final quality assessments. An overview of these shown in Table 1 were carried out. This two-dimensional view of the SLR ( 2004 ) for lexicons. 2.3 Approval authority producer. This is not the case when the producer is not the owner of the SLR (e.g. production is sub-contracted), or when the SLR is produced within a consortium of partners producing similar SLRs with the aim of mutual exchange, as in SpeechDat. patron/consortium. The tasks of the validation institute are then to check an SLR successful check.

The owner (resp. consortium) should decide upon the acceptability of an SLR; voting. In these cases, the process is to send a validation report to the producer for comments. Minor textual or formatting errors that can be easily corrected have to be repaired in the SLR and clarifications for larger discrepancies included in the final report. After voting the outcome is reported to all parties concerned. 2.4 Role of a validation institute Validation is just one element in the process of quality control of SLRs. Repairing imperfections is the next stage. It is important to distinguish between the validation and correction of an SLR. The two tasks should not be performed by one and the end, checking its own corrections. The appropriate procedure is that the producer correctness of the adjustments.
 contribute expertise towards defining and fine-tuning the specifications. It can also make clear from the start which of these specifications can be reliably checked by specifications. For example, if half of the recordings in an SLR of 2,000 speakers should come from male speakers, will the SLR still be acceptable if it contains 999 male speakers, or 975, or even fewer?
When the specifications have been agreed upon, the contribution of the validation controls throughout the production process is presented.
 possible to achieve a consensus. In cases where consensus cannot be achieved, the disputed part of the data, and go back to the producer with the new results. data submissions, and keeps all communication channels open for consultation and feedback on the results found. In practice, this means that:  X  The data set is immediately checked for readability and completeness in terms of  X  If possible, the producer should be allowed to resubmit defective files on the fly  X  The validation report is first reviewed by the producer before it is disclosed to 3 SLR validation: what and how This section contains more practical information about the contents of an SLR that elements to be validated in an SLR, which are successively addressed in the next subsections. 1. Documentation 2. Database format 3. Design and contents 4. Acoustical quality of the speech files 5. Annotation files 6. Pronunciation lexicon 7. Speaker and environment distributions 8. Orthographic transcriptions
For each of these items we will list a number of basic considerations and typical validation criteria. These criteria were developed during discussions in many SLR centre aimed to strike a balance between delivering high quality SLRs and safeguarding the feasibility of data collection in practice. A more detailed overview of validation criteria can be found in Schiel and Draxler ( 2003 ) and Van den Heuvel criteria as used in the SALA-II-project. One can use this list as an example list for the validation of an SLR. 3.1 Documentation accompany the SLR. The documentation should contain:  X  An account of the specifications of the SLR;  X  An account of how they were fulfilled;  X  Instructions on how to use the SLR.

For a user the documentation is of paramount importance to obtain a view of the documentation template. This has a number of advantages:  X  All relevant aspects to be documented are listed beforehand;  X  The documentation is a proper reflection of the specifications of the SLR;  X  All documentation files within a multi-SLR project have the same uniform The validation institute checks if all relevant aspects of an SLR (see the list in Sect. 3 above) are properly described in terms of the three C X  X : clarity, completeness and correctness.

The documentation is the fundamental source of information for a user. The SLR properly documented the gems of these treasures will remain hidden. Both for a user order to reverse-engineer the producer X  X  intentions. Therefore, the documentation is more than just a component of the SLR, it is the very key to it.
 Relevant validation criteria for the documentation: The documentation should contain a clear, correct and complete description of:  X  Owner and contact point.  X  Database layout and media.  X  Application potential for the SLR.  X  Directory structure and file names.  X  Recording equipment.  X  Design and contents of the recordings.  X  Coding and format of the speech files.  X  Contents and format of the annotation files and speech files.  X  Speaker demographic information.  X  Recording environments distinguished.  X  Transcription conventions and procedure.  X  Lexicon: format and transcription conventions included. 3.2 Database format format. This is especially relevant in order to enable automatic search. Relevant validation criteria for the format:  X  Directory structure is as documented.  X  File names are as documented.  X  Empty (i.e. zero-length) files are not included.  X  Each speech file is annotated (either in a corresponding annotation file or in a  X  Each annotation file is connected to an existing speech file and vice versa.  X  The format is a well-known standard or it is well documented.  X  The database is free of viruses.
 3.3 Corpus design and contents Design and content checks include quality measures at several levels. Validation of specified in the documentation are present in the SLR and in sufficient quantities.
For SLRs with prompted material, it is necessary to make sure that all data types application words, phonemes). The frequency of the tokens at prompt level can be production, fewer tokens will commonly be contained in the SLR. This may be due to skipped prompts, missed words in a recorded item, mispronounced or truncated phoneme, digit) at the prompt level can be accompanied by another criterion for the minimum number of tokens required at the transcription level. This number is partly dependent on whether or not the recordings are supervised. In unsupervised recordings such as telephone calls, the practical experience is that 80 X 85% of the upper bound can be reasonably achieved. In supervised recordings a speaker can be stopped to repeat a mispronounced prompt and the threshold can be set to a higher percentage (90 X 95%).

For SLRs with unprompted material other content specifications, and thus other towards type of broadcasts and topics, minimum hours of transcribed speech, permitted time period between the recordings. For human X  X uman dialogues the dialogues, important design parameters are the domain(s) and topic(s) under discussion, the dialogue strategy followed by the machine (system-driven, mixed-speakers (if any).
 Relevant validation criteria for the design and contents of (prompted) SLRs:  X  All mandatory corpus items according to the documentation are included.  X  Number of missing files per corpus item is less than XX%. 3.4 Acoustic quality of the speech files impression of human judgement. The practical estimate SPEX is currently using is a combination of the average clipping rate, Signal-to-Noise Ratio (SNR), and mean these parameters are selected for auditory inspection of signal quality. On the basis extreme long or short durations of files can indicate serious recording defects. Relevant validation criteria for the acoustic quality:  X  Empty speech files are not permitted.  X  Acoustic quality of the speech files is measured, based on:
Apart from the above measurements, the speech files can be checked for a minimum period of silence at the beginning and/or end of the file.
 However for broadcast news SLRs or SLRs with speeches, the acoustic measure-where background noise (e.g. music, commercials, applause) is present.
Whether or not  X  X ad X  recordings should be discarded from the database is a controversial issue. On the one hand, evidently corrupt signals should be deleted. On the other hand, as much speech signal as possible should be retained; it is  X  X lways good for something X , e.g. as test material. Obviously, if recordings are intended for than for SLRs intended for training Automatic Speech Recognition (ASR) engines. 3.5 Annotation files In most SLRs speech files come with accompanying annotation files containing the speaker properties, recording environment, and characteristics of file formats.
The formal part of annotation of meta-data is greatly pushed by standardization initiatives. Initiatives such as the International Standards for Language Engineering (ISLE) Meta Data Initiative (IMDI, http://www.mpi.nl/IMDI ; Wittenburg et al. 2006 ) and Open Language Archives Community (OLAC, Simons and Bird 2003 ) pave the way for further formal validations of annotation schemes.
 contain a label followed by the actual content information or transcription. The label without yielding erroneous information.
 Document Type Definition (DTD) or Schema file that enforces some of the formal Schema file that resides on the web server of the validation institute (De Vriend and Maltese 2004 ). For the actual validation report the check is finally also performed by the validation institute itself.
 Relevant validation criteria for the annotation/label files:  X  No illegal labels are used.  X  All label files contain legal values.  X  Labels do not contain empty values (unless intended so).  X  XML files are well formed and valid against DTD (if included). 3.6 Pronunciation lexicon checked both at a formal and at a content level. At the formal level the encoding and format of the lexicon is examined. At the content level the information contained in correctness. For content checks like these, it is common to employ native speakers, although near-native speakers could also accomplish the task very well. The main when one needs to reassure producers about the quality of the validations. organized as follows:  X  A selection of 1,000 entries are randomly extracted from the lexicon;  X  In case of pronunciation variants, only one variant of the phonetic transcriptions  X  The check is carried out by a phonetically trained person who is a native speaker  X  In case of multiple possible correct transcriptions, the transcription given by the  X  The given transcription is correct if it represents a possible pronunciation of the  X  Each transcription is rated on a 3-point scale: OK; error with respect to a single
Our experience has shown that the maximum allowed number of incorrect higher (and thus stricter) for TTS purposes than for ASR purposes.
For a maximum error percentage of 5%, the 95%-confidence interval for a sample of 1,000 transcriptions is 3.6 X 6.4%. This means that the lexicon is rejected when the number of errors exceeds 6.4%.

In many lexicons the phonetic transcriptions are accompanied by POS-tags. For lexicons developed in the LC-STAR project a similar procedure as shown above for phonetic transcriptions was used to check the POS tags (cf. Shammass and Van den Heuvel 2002 ).
 Relevant validation criteria for the pronunciation lexicon: Formal:  X  All phone symbols in a lexicon agree with the specified set.  X  All documented phone symbols are used.  X  All used phone symbols are documented.  X  All words found in the orthographic transcriptions are present in the lexicon.  X  All words in the lexicon have at least one phonetic transcription.

Content:  X  A maximum of XX% of the entries may contain one erroneous phone symbol in  X  A maximum of YY% of the entries may contain more than one erroneous phone 3.7 Speaker and environment distributions sample of the population of interest in terms of (typically) gender, age and dialectal tions. That is, one would not expect to have a TTS database recorded in a car driving on a highway.
 Relevant validation criteria for the speaker and environment distributions:  X  Distributions of speaker properties are in agreement with specification.  X  The recording environments are in agreement with the specifications. 3.8 Orthographic transcriptions Similar to the lexicon, the orthographic transcriptions can be checked at a formal orthographic encoding is correct and if all symbolic representations for non-speech orthographic transcriptions (including those of the non-speech events) are a correct representation of what is audible in the speech signal.

For the content check, the validation is split into two parts. There is a validation The transcription validation of the non-speech annotations is not necessarily done by a native speaker of the language, but by someone experienced in listening to non-transcribed or not. The transcriptions are checked by listening to the corresponding speech files and by correcting the transcriptions if necessary. As a general rule, the submitted transcriptions always have the benefit of the doubt; only overt errors are marked.

Typically, a sample of 2,000 utterances (about 2 h of speech) is selected. This confidence level is dependent on the size of the sample (not of the population, i.e. confidence interval for a sample of 2,000 transcriptions is 4 X 6%. This means that the orthographic transcriptions are rejected when the number of utterances containing errors exceeds 6%.

Two types of errors are distinguished: 1. Errors in the transcription of speech. 2. Errors in the transcription of non-speech events.

The procedure described above works adequately for SLRs that are item-based, such as the databases from the SpeechDat family. In such databases an item is one utterance, e.g. a number, a date, a name etc. Transcription errors should be counted per utterance since a transcription error directly affects the usability of the whole item. The total number of transcription errors is less interesting than the number of items that contain one or more transcription errors.
 Relevant validation criteria for the quality of the orthographic transcriptions: Formal:  X  A max of XX% of the speech files may miss an orthographic transcription  X  All transcriptions for non-speech events are described in the documentation. Content:  X  Maximum number of transcription errors.
For other types of databases, this procedure is less suited. For instance, broadcast news databases are not divided in equivalent items, but in segments with speech of a should be revised. A common measure for this is the WER (Word Error Rate), for which a maximum of 0.5% can be demanded for speech and 1.5% for non-speech, for most types of SLRs. This is the case in the TC-STAR project (Van den Heuvel et al. 2006 ). 3.9 Automatic, manual or both? automatic procedure provides a consistent level of precision that only a computer can offer. As a general rule the formal aspects of an SLR can be validated by scripts, and the content checks need human intervention.

Automatic checks are fast, consistent and can deal with large amounts of data (in where the checks focus on content, require expert knowledge, and are more aimed at empirical quality.

Since the production of a script is human labour and time-consuming, one should always consider if the automation of a check is time-effective. Writing and testing processed and/or if (many) more SLRs of the same type are expected for validation. interpreted and reported by means of human labour and intervention.

On the other hand, evident manual work can be facilitated by scripts preparing the material and by interfaces that make manual verification work fast, efficient and less error-prone. For instance, for checking the orthographic transcriptions, a tool that quickly navigates through the selected material with simple buttons to indicate (types of) errors can seriously reduce the work load of the validator.
For the quality checks that were dealt with in the previous subsection, Table 3 manually.
 4 Validation types and procedures The checks mentioned in the previous section are all carried out by the validation institute upon completion of SLR recordings, annotations and packaging. However, production process to ensure optimal quality. SPEX has developed a standard have been developed in other contexts; these will be addressed in Sect. 4.2 . 4.1 Standard protocol the current state of affairs and is open to further development and refinement. 4.1.1 Pre-validation Pre-validation of an SLR is carried out before the stage of extensive data collection serious data collection starts. Secondary objectives are:  X  To enable the producer to go through the whole stage of documenting and
At the pre-validation phase three components are assessed: prompt sheets, lexicon, and a mini database. The producer can deliver these components together as one package, or one-by-one, submitting a new component after the previous has been validated. Since pre-validation is diagnostic in nature, normally there are no iterations of repairs and new pre-validations.

Prompt sheet validation Before embarking on recording the speakers, the for each item. Since in practice not all intended material is recorded due to problems background noise, etc., the reading scripts contain the (theoretical) upper bounds of types and tokens of what is achievable in a database.

The validation of the prompt sheets comprises checks with regard to the presence of the corpus items, adherence of their design to the specifications as well as the number of repetitions at word or sentence level calculated for the complete checked if a fixed minimum number of tokens per phoneme can be collected, provided that a lexicon containing all the words and their phonetic transcriptions is delivered as well.
 minimum which is required in the end), measures can still be easily taken to repair the errors. SLR producers indicate that they highly appreciate this part of validation which allows them to spot and repair errors in an early design stage. The prompt underspecified and need further clarification.

Lexicon validation A formal check of the lexicon with regard to the format and phoneticians familiar with each language, the validation institute contracts this task describing the principles of the phonetic transcriptions employed by the producer. they have to check manually. They are instructed to give the provided pronunciation the benefit of the doubt and only to mark transcriptions that reflect an overtly wrong pronunciation. This is in order to prevent marking as errors differences which are due to different phonetic theories or different ideas about what the  X  X ost common X  or  X  X est X  pronunciation is.

Mini database validation Commonly, about 10 initial recordings are made in different environments and annotated. The data is formatted and packaged as if it were a completed SLR, including documentation, and submitted to the validation Further, the format, and the annotations are inspected, all with the aim of preventing errors during large-scale production. Since the documentation is included as well, the producers are forced to start documenting at an early stage. The advantages of this are clearly gained in the final production phase; the burden of documenting in that phase is greatly reduced to some final text editing and modifications of numeric tables. 4.1.2 Full validation mentioned in Sect. 3 are carried out.

The validation institute may have a queue of SLRs to be validated. Because SLRs are typically handled in the order received, the validation institute performs a Quick required files are included in the SLR and if they have the correct formal structure. If not, the producer is requested to submit updated versions of defective or missing files before actual validation takes place. Quick Checks allow the producer and the validation institute to work efficiently in parallel.

Since the validation of the (orthographic) transcriptions is restricted to a sample would use up the main part of the validation effort. For this reason, in SpeeCon and Quick Check, for which the producer instantly had to provide speech files. Note that updates of the transcriptions are not accepted at a later stage. This is to avoid new transcriptions being made for the subset of files selected for validation.
If substantial shortcomings are found during validation, rectification and a subsequent re-validation of an SLR may become necessary. This is decided by the owner or the consortium in charge of the SLR production. Since usually not all parts iterate until approval of the SLR is achieved. 4.1.3 Pre-release validation The validation of a complete database results in a report containing a list of errors which were found in the database. Some of them are irreparable and related to flaws processing. These errors can easily be repaired and the producers are willing to do tencies during repair. Therefore, a pre-release validation has been introduced so that purpose of this validation is to make sure that the reparable errors which were found introduced.

After full validation the documentation file is augmented with an additional section:  X  X  X odifications after validation X  X . It is checked if all changes agreed upon are included in this section and if they have been implemented in the submitted pre-release version. The validation software is run, so that all formal checks on the data are carried out once more.
 however minor, since these corrections can introduce new (and larger) errors. The distribution. 4.2 Other types and procedures As the European Language Resources Association X  X  validation unit for SLR, SPEX has worked together with ELRA X  X  Validation Committee (Van den Heuvel et al. 2003 ) to establish additional means for SLR quality control.
 The first instrument is the Quick Quality Check (QQC) (Van den Heuvel 2004 ). intended for SLRs that are already in ELRA X  X  catalogue and for all SLRs that are validation.
 The following principles have been adopted for the QQCs: A. The QQC mainly checks the database contents against minimal requirements. B. Generally, a QQC should take about 6 X 7 h work at maximum.
 C. For each SLR two QQC reports are produced: One for the provider and users
During the QQC_DB the SLR is checked for compliance with a set of minimal requirements and for correspondence with its own documentation. The QQC_DB report is intended for ELRA X  X  database users if the SLR is already in the catalogue and for the database providers if the database is new and not in the catalogue yet. Each QQC_DB report is sent to the SLR provider for comments. Based on these a new version of the QQC_DB report and/or of the SLR may result. With permission of the provider the QQC_DB report is made available through ELRA X  X  catalogue on the web.

Each database at ELRA is accompanied by one or two description forms: a general description form and/or a specific description form (depending on the type of resource). These description forms contain the basic information about a database SLR provider. The form is used to inform potential customers about the database. The information provided in the description form should be correct. The correctness of this information is also a minimum requirement for a database and checked at the QQC. The QQC_DF report contains a quality assessment of the correctness of the information in the description forms.

A second means of monitoring and improving SLR quality is a bug report service (Van den Heuvel et al. 2002 ). This service is implemented and maintained at ELRA X  X  website ( http://www.elra.info ). The idea is that errors in SLRs distributed by ELRA are reported by SLR users through this bug report service. An error list per SLR is maintained and attached to the SLR information in ELRA X  X  catalogue on their website. This document contains a formal list of verified errors (Formal Error List, FEL). Patches or new SLR versions can be made to correct errors.
The access to the FEL through the web is free and allows bug reporting users to see the status. Based on an update of the FEL the provider of that SLR is asked to ELRA produce the corrected part. SPEX checks that corrections are properly made implemented for SLR, and similar services are now under development for Written Language Resources (Fers X e and Monachini 2004 ).

Finally, validation manuals have been written both for SLRs and WLRs. They are available from ELRA X  X  URL. These documents describe validation guidelines, procedures and criteria that should be taken into consideration by providers of new LRs. The documents give an idea of how validation at ELRA takes place, and allow way, this also contributes to the improvement of SLR quality. 5 Concluding remarks In this article we have clarified the concept of SLR validation. We have addressed the concepts  X  X uality X  and  X  X ssessment X  and have elaborated on the different roles of  X  X alidation X  and  X  X valuation X  in quality assessment. Furthermore, we have presented basic principles in LR validation. We have pinpointed a number of relevant issues shown illustrating how validation can prove its value all along the production phase in terms of pre-validation, full validation and pre-release validation. Other relevant LR quality control instruments have been briefly presented, too.

From our experience as a validation centre in many (mainly European) projects we have learnt a number of valuable lessons:  X  External validation is an important quality safeguard.  X  If the validation institute is involved during the specification phase of an SLR, it  X  The validation institute can provide important input at strategic points along the  X  The validation institute needs to keep open communication channels to the SLR  X  Clear validation protocols help structuring the work and effective quality  X  A documentation template provided by the validation institute is to the benefit of  X  A relevant part of the work of the validation institute is to find a proper balance  X  The validation institute, as a rule, does not claim the approval authority for an Appendix: Validation criteria used in the SALA II project 1. Documentation  X  File DESIGN.DOC is present  X  Language of doc file: English  X  Contact person: name, address, affiliation  X  Description of number of CDs and contents per CD  X  The directory structure of the CDs  X  The format of the speech files (A-law, Mu-law, 8 bit, 8 kHz, uncompressed)  X  File nomenclature  X  Contents and format of the label files  X  Description of recording platform  X  Explanation of speaker recruitment  X  Prompting information  X  Description of all recorded items  X  Analysis of frequency of occurrence of the phones represented in the full  X  Transcription conventions  X  Lexicon information  X  Speaker demographics  X  Recording conditions:  X  Information on test (set) specification  X  The validation report made by SPEX (VALREP.DOC) is referred to 2. Database structure, contents and file names  X  Directory/subdirectory conventions  X  File naming conventions  X  NNNN in filenames is not in conflict with BLOCK and SES numbers in  X  Contents lowest level subdirectories should be of one call only  X  All text files should be in MS-DOS format (&lt;CR&gt;&lt;LF&gt;) at line ends  X  A README.TXT file should be in the root describing all (documentation) files  X  A file containing a shortened version of the volume name (11 chars max.) should  X  A copyright statement should be present in the file COPYRIGH.TXT (root)  X  Documentation should be in \&lt;database_name&gt;\DOC  X  Tables should be in \&lt;database_name&gt;\TABLE  X  Index files (optional) should be in \&lt;database_name&gt;\INDEX.
  X  Prompt sheet files (optional) should be in \&lt;database_name&gt;\PROMPT  X  All sessions indicated in the documentation SUMMARY.TXT are present on the  X  Empty (i.e. zero-length) files are not permitted  X  File match: For each label file there must be one speech file and vice versa  X  Part of the corpus is designed for training and a smaller part for testing:  X  The contents of the database as given in CONTENTS.LST should comprise:  X  The contents of the SUMMARY.TXT files should comprise:  X  Missing items per session  X  The database should be free of viruses 3. Items Check on mandatory corpus items  X  6 common application words (code A1-6)  X  2 isolated digits (code I1-2)  X  1 sequence of 10 isolated digits (code B1)  X  4 connected digits (code C1-4)  X  9 X 11 digit telephone number (C2)  X  16 digit credit card number (C3)  X  6 digit PIN code (C4)  X  * 30 digits per session are required  X  digits must appear numerically on the sheet, not as words  X  1 date (code D1)  X  1 date (code D2)  X  1 general or relative date (code D3)  X  1 application word phrase (code E1)  X  3 spelled words (code L1-3)  X  1 money amounts (code M1)  X  1 natural number (code N1)  X  6 directory assistance names (code O1-7)  X  2 yes/no questions (code Q1-2)  X  9 phonetically rich sentences (code S1-9)  X  1 time of day (code T1)  X  1 time phrase (code T2)  X  4 phonetically rich words (code W1-4)  X  each word may appear a max. of five times at prompt level  X  Any additional, optional material: Checks on presence of corpus files The following completeness checks are performed:
Structurally missing corpus items:  X  Which items are not recorded at all?
Incidentally missing files: a. files that are not there b. files with empty transcriptions in the LBO label field (effectively missing files) c. corrupted speech files d. files containing truncation and mispronunciation marks
SALA II has the following criteria for missing items:  X  A maximum of 5% of the files of each mandatory item (corpus code) may be  X  As missing files are counted: absent files, and files containing non-speech events  X  For the phon. rich sentences a maximum of 10% of the files may be effectively  X  There will be no further comparison of prompt and transcription text in order to 4. Sampled data files Coding  X  A-law or Mu-law, 8 bit, 8 kHz, no compression Sample distribution File length: We calculated the length of the files in seconds in order to trace spurious recordings if files were of extraordinary length.
 Duration distribution over calls/ directories: Length (s) #Occurrences Min X  X ax samples: divided by all samples in the file.
 clipping rate intervals.
 Clip distribution over calls/directories:
Clipping Occurrences Mean values: many files were found in a set of mean sample value intervals. This overview can be used to trace files with large DC-offsets.
 Mean distribution over calls/directories: Mean Occurrences Signal to Noise Ratio: We split each signal file into contiguous windows of 10 ms and computed the Mean Square (energy) in each window. The mean sample value over the complete file was subtracted from each individual sample value before MS was computed. 30% of the windows that contained the lowest energy were assumed to contain line noise. In this way the signal to noise ratio could be calculated for each file by dividing the mean energy over all windows by the mean energy of the 30% sample mentioned above. The result was multiplied by 10*log for scaling.
 SNR distribution over calls/directories:
SNR occurrences 5. Annotation file  X  Each line must be delimited by &lt;CR&gt;&lt;LF&gt;  X  No illegal SAM mnemonics used  X  There are no SAM mnemonics missing  X  All files must contain the same mnemonics. This holds as well for the optional  X  No illegal field values should appear  X  For spontaneous speech LBR should contain the specified identification 6. Lexicon  X  Check lexicon existence (LEXICON.TBL)  X  The entries should be alphabetically ordered  X  Used SAMPA symbols are provided in SAMPALEX.PS  X  In transcriptions only SAMPA symbols are allowed  X  All SAMPA phoneme symbols should be covered  X  Phoneme symbols must be separated by blanks  X  A line in the lexicon should have the following format  X  Each line is delimited by &lt;CR&gt;&lt;LF&gt;  X  All entries should have at least one phone transcription  X  Alternative transcriptions are optional.
  X  Orthographic entries are taken from the LBO-transcriptions from the label files.  X  Words appearing only with * or * or % should not appear in the lexicon  X  The lexicon should be complete  X  Optional information: stress, word/morphological/syllabic boundaries. 7. Speakers  X  Obligatory information in the (optional) SPEAKER.TBL:  X  Optional information:  X  Each speaker only calls once. There is a tolerance of 5% of the speakers who  X  Balance of sexes  X  Balance of dialect regions  X  Balance of ages 8. Recording conditions  X  Obligatory attributes of the (optional) REC_COND.TBL file should all be  X  Obligatory attributes of the SESSION.TBL should all be present and complete  X  The recordings are distributed as follows (check ENV):  X  Recordings from the fixed net are not included 9. Transcription Validation by software tools  X  Transliterations is case-sensitive unless specified otherwise.  X  Punctuation marks should not be used in the transliterations  X  Digits must appear in full orthographic form  X  In principle only the following symbols are allowed to indicate non-speech  X  Asterisks should be used to indicate mispronunciations  X  Double asterisks should be used for not understandable parts  X  Tildes should be used to indicate truncations  X  Percent signs should be used to indicate speech distortions due to transmission Validation by a native speaker of the language This validation was carried out by taking 1,000 short items and 1,000 long items.
The transcriptions in the label files for these samples were checked by listening to the corresponding speech files and correcting the transcription if necessary. In case of doubt nothing was corrected.

This check was performed by a native speaker of the language. The background noise markers were checked by a trained (non-native) validator.  X  The evaluation comprised the following guidelines: References
