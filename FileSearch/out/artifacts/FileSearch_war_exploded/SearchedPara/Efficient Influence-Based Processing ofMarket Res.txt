 The rapid growth of social web has contributed vast amounts of user preference data. Analyzing this data and its relationships with products could have several practical applications, such as person-alized advertising, market segmentation, product feature promo-tion etc. In this work we develop novel algorithms for efficiently processing two important classes of queries involving user prefer-ences, i.e. potential customers identification and product position-ing. With regards to the first problem, we formulate product attrac-tiveness based on the notion of reverse skyline queries. We then present a new algorithm, termed as RSA, that significantly reduces the I/O cost, as well as the computation cost, when compared to the state-of-the-art reverse skyline algorithm, while at the same time being able to quickly report the first results. Several real-world ap-plications require processing of a large number of queries, in order to identify the product characteristics that maximize the number of potential customers. Motivated by this problem, we also develop a batched extension of our RSA algorithm that significantly im-proves upon processing multiple queries individually, by grouping contiguous candidates, exploiting I/O commonalities and enabling shared processing. Our experimental study using both real and syn-thetic data sets demonstrates the superiority of our proposed algo-rithms for the studied classes of queries.
 H.2.8 [ Database Management ]: Database Applications X  Spatial databases and GIS reverse skylines, preferences, market research
Analyzing user data (e.g., query logs, purchases) has seen con-siderable attention due to its importance in providing insights re-garding users X  intentions and helping enterprises in the process of  X  decision making. Recently, the rapidly growing social web has been a source of vast amounts of data concerning user preferences in the form of ratings, shares, likes, etc. Previous efforts (e.g., in preference learning and recommender systems) mainly focus on helping users discover the most interesting, according to their pref-erences, among a pool of available products.

Highlighting the manufacturer X  X  perspective, the need for tools to analyze user preferences for improving business decisions has been well recognized. Preference analysis has various important applica-tions such as personalized advertising, market segmentation, prod-uct positioning etc. For example, a laptop manufacturer might be interested in finding those users that would be more interested in purchasing a laptop model. Thereby, manufacturers can benefit by targeting their advertising strategy to those users. Or they might search for laptop feature configurations that are the most popular among customers. Similarly, a mobile carrier operator that is about to launch a new set of phone plans may want to discover those plans that would collectively attract the largest number of subscribers.
In this work we develop novel algorithms for two classes of queries involving customer preferences, with practical applications in market research. In the first query type that we consider, we seek to identify customers that may find a product as attractive. We formulate this problem as a bichromatic reverse skyline query we present a new algorithm, termed as RSA, that outperforms the state-of-the-art algorithm BRS [25] in terms of both I/O and com-putational cost. Compared to BRS, our RSA algorithm is based on a different processing order, which allows for significant im-provements with respect to the performance , the scalability progressiveness of returned results when compared to BRS.
Real world applications usually require processing multiple queries efficiently. For example, assume that a mobile carrier operator maintains a database of existing phone plans, customer statistics (i.e., voice usage duration, number of text messages sent, data vol-ume consumed per month) and a list of new phone plans under con-sideration. We formulate this problem as a new query type, namely the k -Most Attractive Candidates (k-MAC) queries. Given a set of existing product specifications P , a set of customer preferences and a set of new candidate products Q , the k-MAC query returns the set of k candidate products from Q that jointly maximizes the total number of expected buyers, measured as the cardinality of the union of individual reverse skyline sets (i.e., influence sets
Recent works [12, 15] have independently studied similar prob-lems over  X  X bjective X  attributes, i.e. those that have a globally pre-ferred value, such as (zero) price, (infinite) battery life, etc. In such a scenario, the dominance relationships among customer prefer-ences, existing products and candidates can be extracted by execut-ing a single skyline query over the data set. Thereby, these works focus on providing greedy algorithms that determine the most prof-itable solution by combining customer sets. In this work we ge n-eralize their definition of user preferences, such that we can also handle  X  X ubjective X  attributes i.e., those not having a strict order for all users, e.g., as screen size, processor type, operating system etc. For example, for customer A that prefers a portable laptop, a 11" laptop would be more preferable than a 15" one. On the other hand, for a customer B searching for a desktop replacement laptop, the latter model would be more appropriate.

For such attributes, applying methods such as those proposed in [12, 15] requires having extracted the product dominance relation-ships for all users, since these relations are user-dependent. Thus, we have to execute a dynamic skyline query [14] for each customer, which is prohibitively expensive. Further, applying single point re-verse skyline approaches to solve a k-MAC query would require calculating the influence set for each candidate product individ-ually, which is also a very expensive task, especially when han-dling large data sets. We, thus, propose a batched extension of our RSA algorithm for the k-MAC problem that improves upon pro-cessing candidates sequentially by grouping contiguous candidates, exploiting I/O commonalities and enabling shared processing. Af-ter extracting the influence set of each candidate product, we also propose an algorithm that greedily calculates the final solution for the k-MAC problem by combining the influence sets of individual candidate products. In brief, the contributions of this paper are:  X  We present a novel progressive algorithm, termed as RSA, for  X  We develop a batched variant of our RSA algorithm that im- X  We perform an extensive experimental study using both syn-Consider two sets of points, denoted as P and C , in the same dimensional space. We will refer to each point p  X  P as a uct . Each product is a multi-dimensional point, with p i denot-ing the product X  X  attribute value A i . For example, assuming that products are notebooks, the dimensions 1 of p i may correspond to the notebook X  X  price, weight, screen size, etc. Further, each point c  X  C represents a customer X  X  preferred notebook specifications that she would be interested in; we will refer to each point c tomer . Clearly, customers are more interested to the products that are closer to their preferences. In order to capture the preferences of a customer c , we formally define the notion of dynamic dominance.
D EFINITION 1. (Dynamic Dominance) (from [5]): Let c  X  C, p , p  X   X  P. A product p dynamically dominates p  X  with respect to c, denoted as p  X  c p  X  , iff for each dimension | p i  X  c i there exists at least one dimension such that | p i  X  c i
Note that this definition can accommodate dimensions with uni-versally optimal values where smaller (larger) values are preferred by simply setting c i to the minimum (resp. maximum) value of dimension A i . For example, assuming that lighter notebooks are preferred, we can simply set for all customers c weight = 0.
D EFINITION 2. (Dynamic Skyline) (from [5]): The dynamic sky-line with respect to a customer c  X  C, denoted as SKY ( c ) , contains all products p  X  P that are not dynamically dominated with respect to c by any other p  X   X  P.

Consider a set of existing products P = { p 1 , p 2 , p 3 tomers C = { c 1 , c 2 , c 3 } . Figure 1(a) illustrates the dynamic skyline of c 1 that includes notebooks p 2 and p 4 in a sample scenario with 2 dimensions corresponding to the CPU speed and the screen size of a notebook. Points in the shaded areas are dynamically dominated by points belonging to the dynamic skyline of c 1 . Since we are in-terested in the absolute distance between products, a product might dominate other products that belong to different quadrants with re-spect to a customer. For example, p 1 and p 3 in the upper right quadrant are dynamically dominated by p 2 in the lower right quad-rant because p 2 has a CPU speed and a screen size that are both closer to c 1 than the corresponding characteristics of p Figures 1(b) and 1(c) illustrate the dynamic skylines of customers c and c 3 respectively. We now highlight the product X  X  perspective by introducing the definition of bichromatic reverse skylines D EFINITION 3. (Bichromatic Reverse Skyline) (from [11]): Let P be a set of products and C be a set of customers. The bichromatic reverse skyline of p, denoted as RSKY ( p ) contains all customers c  X  C such that p  X  SKY ( c ) .

Thus, the bichromatic reverse skyline of a product p contains all customers c that find p as  X  X ttractive X . Henceforth, we refer to the bichromatic reverse skyline of p as the influence set Figure 1(d) illustrates the influence sets of products p 1 p .

The cardinality of RSKY ( p ) is a useful metric of the product X  X  impact in the market. We refer to | RSKY ( p ) | as the influence score IS ( p ) . In our example, IS ( p 1 )= IS ( p 2 )= 2 and IS ( p Consider a new product q . The new product partitions the dimensional space into 2 D orthants  X  i , each identified by a number in the range [ 0 , 2 D  X  1 ] . Since all orthants are symmetric and we are interested in the absolute distance between products, we can map all products to  X  0 as illustrated in Figure 2(a). For simplicity, we hereafter concentrate on  X  0 with respect to a query point
For every dynamic skyline point p i , let m i ( q ) be the midpoint of the segment connecting a query point q with p i . In Figure 2(b) black points m 1 , m 2 and m 4 represent the midpoints of p with respect to q . Henceforth, in order to alleviate the compli-cation of maintaining both points and midpoint skylines, whenever we refer to a product p i we imply the corresponding m i ( q ) with re-spect to q . We also assume that each dynamic skyline point respect to q is mapped to its midpoint skyline m i ( q ) on the fly.
The influence region of a query point q , denoted as IR ( union of all areas not dynamically dominated with respect to the midpoint skylines of q . The area in  X  0 that is not shaded in Fig-ure 2(b) draws the influence region for q . Note that the midpoints themselves belong to the IR, since a tuple cannot dominate itself.
L EMMA 1. (from [11]) A customer c belongs to the influence set RSKY ( q ) of q iff c lies inside the influence region of q i.e., c  X  IR ( q )  X  c  X  RSKY ( q ) . (a) D ynamic Skyline of c 1 (c) D ynamic Skyline of c 3
Returning to the example of Figure 2(b), notice that only inside IR ( q ) . Therefore, RSKY ( q ) = { c 2 } .

Hereafter, we assume that all points (either products or customer preferences) are indexed using a multidimensional index (e.g., R-trees, kd-trees etc.); for our presentation we will consider R-trees. Figure 3(a) shows an example minimum bounding box (MBB) e Inside each MBB e , let min-corner e  X  ( q ) denote the point in e having the minimum distance from a query point q . The min-corner dominates the largest possible space. The points that reside in each of the d faces closest to q and are the farthest from the origin are denoted as minmax-corners . Each MBB contains D minmax-corners. Independently of how products within e are distributed, any point in e certainly dominates the area that the minmax-corners do, while at best it dominates the area that the min-corner does.
Given a set of MBBs, we can derive two sets: the set of all min-corners denoted as L and the set of all minmax-corners w.r.t. denoted as U . Figure 3(b) presents an example, assuming E { e p 1 , e p 2 , e p 3 , e p 4 } , where e p i denotes a product entry. In Fig-ures 3(b), 3(c) black and hollow circles represent the min-corners and minmax-corners respectively and rectangles represent midpoints.
Continuing the example of Figure 3(b), the grey area represents a lower bound of the actual influence region IR  X  ( q ) and it is defined as the space not dominated w.r.t. q by any min-corner l  X  L spectively, the grey area in Figure 3(c) represents an upper bound of the actual influence region IR + ( q ) , defined as the space not dom-inated w.r.t. q by any minmax-corner u  X  U . It follows [25]:
L EMMA 2. If an entry e c is dominated by any u  X  U, i.e. e completely outside IR + ( q ) , e c cannot contain any customer inside IR ( q ) . Hence, according to Lemma 1, e c can be pruned.
For example, e c 1 in Figure 3(d) can be pruned because it is com-pletely outside IR + ( q ) .
In the following we detail the state-of-the-art Bichromatic Re-verse Skyline (BRS) algorithm [25] that efficiently calculates the influence set of a single query point q . BRS aims at minimizing the I/O cost ( i ) by progressively refining the influence region of the influence set of q has been retrieved, ( ii ) by applying Lemma 2 to prune e c entries that do not contribute to RSKY ( q ) BRS uses two indexes, an R-tree T P on the set of products another T C on the set of customers C . Initially, the algorithm inserts all root entries of T P (resp. T C ) in a priority queue E sorted with the minimum Euclidean distance of each entry from BRS extracts a set L of all min-corners and a set U of all minmax-corners of e p  X  E P . Further, in order to reduce the number of sub-sequent dominance checks, BRS calculates the skylines of L U , denoted as SKY ( L ) and SKY ( U ) respectively.
 In each iteration BRS expands the entry in E P with the minimum Euclidean distance from q and updates the current L and U their skylines SKY ( L ) and SKY ( U ) . Then, all e c  X  E for dominance with SKY ( L ) and SKY ( U ) . If e c is not dominated by
SKY ( L ) (i.e. it intersects IR  X  ( q ) ), BRS expands e contain customers inside IR ( q ) . Returning to Figure 3(d), sects IR  X  ( q ) ; therefore e c 3 is expanded. In contrast, if a customer entry e c (such as e c 1 in Figure 3(d)) is dominated by SKY ( U ) e can be safely pruned according to Lemma 2. BRS terminates when E C becomes empty, i.e. the position of all customers either inside or outside IR ( q ) has been determined.
In this section, we detail the drawbacks of BRS and then present a more efficient reverse skyline algorithm, termed RSA. Complexity Analysis . Let p k , c k denote the sizes of the currently active entries in E P and E C , respectively, after the k of the BRS algorithm. The worst-case cardinality of p k and are | P | and | C | respectively. In each iteration, the BRS algorithm maintains both SKY ( L ) and SKY ( U ) , two sets with O ( | P | ) O ( D | P | ) entries respectively, where D is the dimensionality of the data set. BRS then checks for dominance each entry in E P E C with both SKY ( L ) and SKY ( U ) . Thus, each iteration entails O ( D | P |  X  ( | P | + | C | )) dominance checks, which require a total of O ( D 2 | P |  X  ( | P | + | C | )) comparisons, since each dominance check requires O ( D ) comparisons.

Clearly, the processing cost of BRS depends on the size of the intermediate upper and lower skyline sets. [2] shows that for uni-formly distributed data the size of the skyline set is  X  ( T hus, for larger data sets or higher dimensional data the processing cost of maintaining SKY ( L ) and SKY ( U ) becomes prohibitively expensive. Our experimental evaluation (Section 6) confirms that BRS is impractical for | P |  X  10 6 or D  X  4. Motivated by the above analysis, we introduce a more efficient and scalable reverse skyline algorithm, which eliminates the dependency on the SKY ( L ) SKY ( U ) sets, thus being able to handle high dimensional data, or, in general, data where the size of skyline points is large. Processing Order . BRS performs a synchronous traversal on the T and T C indexes, which are built on product and customer points, respectively, following a monotonic order based on the Euclidean distance of e p entries from q . This processing order ensures that the number of I/Os on T P is minimized. However, in terms of the total I/Os, BRS might perform some unnecessary I/Os. Figure 4(a) illus-trates one such scenario, where the nodes e p 2 and e c 1 yet expanded. 2 BRS would proceed by expanding e p 2 , revealing e 3 and e p 4 . Unfortunately, e c 1 is not affected by this refinement and it still has to be accessed. On the other hand, if we first expand e , this operation would reveal e c 2 and e c 3 , which can be pruned by p and p 5 respectively, eliminating the need to access e p 2 in this scenario the I/O access on e p 2 was redundant. In order to avoid such redundant I/Os, our RSA follows a visiting strategy that is primarily based on the tree level of customer entries, which, as confirmed in our experiments, results in fewer total I/Os. Progressiveness . BRS iteratively refines IR  X  ( q ) and reports the customer points that lie inside IR ( q ) . In order to retrieve the first reverse skyline point, several iterations of BRS may be required, which is undesirable for applications that require a quick response containing only a fraction of the output, or if the complete output is not useful (e.g., if it contains too many results). We, thus, seek to develop an algorithm that reports the first results faster than BRS.
We now present our Reverse Skyline Algorithm (RSA), which aims to address the shortcomings outlined above.
 Data Structures Used and Basic Intuition. The RSA algorithm:  X  Does not require the maintenance of the SKY ( L ) and SKY ( U ) sets and is, thus, less expensive in terms of processing cost.  X  Checks one customer entry per iteration following a visiting strategy based on the entry X  X  tree level (primary sort criterion) and Euclidean distance from q (secondary sort criterion).  X  Accesses a product entry only if it is absolutely necessary in order to determine if a customer point belongs to RSKY ( q ) RSA maintains the following data structures for its operation:  X  A priority queue E P on the set of product entries  X  A priority queue E C on the set of customer entries  X  A set SKY ( q ) with the currently found midpoint skylines
The two priority queues are sorted based on a dual sorting cri-terion: primarily, based on the tree level of the stored entries and, subsequently, using the Euclidean distance of each entry from Algorithm 1: R SA Thus, leaf entries are given higher priority and are processe d first, while the examination of non-leaf entries is postponed as much as possible. By first processing all leaf e c entries, the algorithm may reveal a midpoint skyline, which will be subsequently used to prune a non-leaf e c based on Lemma 2, thus avoiding an access on The same intuition holds for e p entries as well; an already found midpoint skyline can be used to prune dominated non-leaf product entries, since these entries will not contribute to the skyline. Fur-ther, whenever an e c entry is checked for dominance with all leaf e p entries will be examined. As long as no leaf inates e c , only then will RSA proceed to expand the nearest to non-leaf e p entry. This change in the visiting order of E the number of accesses on T P as well. For instance, in Figure 4(b) BRS would access e p 2 that has the minimum Euclidean distance from q . In contrast, RSA will use p 1 to determine that c belong to RSKY ( q ) , hence avoiding the access of e p 2 Algorithm Description. The RSA algorithm is presented in Al-gorithm 1. Initially, RSA inserts all root entries of T P in the priority queue E P (resp. E C ). Further, RSA maintains a set SKY ( q ) of the currently found midpoint skylines which are used for pruning based on Lemma 1. RSA proceeds in iterations. In each it-eration RSA extracts the entry in E C having the minimum key from q (Line 5) and checks the following pruning conditions: 1. If e c is dominated by any point that belongs to the currently found midpoint skylines SKY ( q ) , e c can be removed from on Lemma 1 (Lines 6-8). 2. Otherwise, if e c is a non-leaf entry (Line 9), e c is expanded and child nodes are inserted into E C (Line 10). 3. Else, for all e p entries in E P (Lines 12-22):  X  If e c is dominated by the midpoint of a leaf entry e p  X  Else if e c is dominated by the midpoint of the min-corner e
Finally, if e c has not been pruned by any of the above conditions (Line 23), then e c is a reverse skyline point and can be at that stage reported as a result (Line 24). The RSA algorithm terminates when E C becomes empty and then RSKY ( q ) is returned (Line 25). Example. We illustrate the execution of RSA using the running example depicted in Figure 5. At the beginning, E P ( q ) = { e e first iteration, RSA will examine e c 1 that has the minimum distance from q . Since it is a non-leaf entry, RSA will expand e c Now E C ( q ) = { c 2 , e c 4 , e c 3 } and RSA selects to examine c the current skyline is empty, c 2 is not dominated by any product entry; hence RSA will proceed by checking if c 2 is dominated by any product entry contained in E P ( q ) . c 2 is dominated by the min-corner of the first entry in E P ( q ) , i.e. e p 7 (Line 14). In order to determine if there actually exists a point inside e p 7 that dominates c w.r.t. q , RSA will expand e p 7 (Line 21), pushing its child nodes p 8 and p 9 inside E P ( q ) , thus E P ( q ) = { p 8 , p 9 ure 5(c)). Now, RSA discovers that c 2 is dominated by p 8 is marked as a skyline point (Line 17) and c 2 is discarded. In the next iteration, RSA selects to examine e c 4 , which has the minimum distance from q . e c 4 is not dominated by any currently found sky-line point and since it is a non-leaf entry, it is expanded and child nodes c 5 and c 6 are inserted into E C ( q ) (Line 10) (see Figure 5(d)). Now we have E C ( q ) = { c 5 , c 6 , e c 3 } . Next, RSA will examine c Since c 5 is not dominated by any product entry, c 5 is reported as a reverse skyline result (Line 24). Now RSA examines c 6 which is already dominated by a currently found skyline point, i.e. 6), hence it is discarded. Finally, e c 3 is examined. Similarly, dominated by p 8 and it is also pruned. Since E C ( q ) is now empty, RSA terminates and outputs c 5 as the final answer.
 Complexity and Progressiveness Analysis. RSA requires at most | C | iterations (one for each customer), although in fact several e entries will be pruned by SKY ( q ) (Line 6). Each iteration en-tails a dominance check with ( i ) the currently found midpoint sky-line SKY ( q ) , and ( ii ) all product entries currently in ing O ( | P | ) worst-case cardinality. Overall RSA requires dominance checks, or O ( D | P || C | ) comparisons.

With respect to progressiveness, recall that RSA will first ex-amine leaf customer entries that have the minimum Euclidean dis-tance from the query point q (based on the dual sorting scheme on E ). In other words, the very first iterations of RSA concern cus-tomer entries that are very close to q . Intuitively, the closer to customer is, the more likely that q will not be dominated by any product w.r.t. the examined customer. Hence, customers that will be examined in the first iterations tend to have a higher probabil-ity of belonging to RSKY ( q ) . Further, since the first entries to be examined are actual points (not MBBs), the first iterations will not involve e c expansions (Line 10), which are expensive in terms of processing cost. Thus, the first iterations will be faster than subse-quent ones. Overall, RSA typically reports the first results in just a few iterations. In contrast, recall that BRS requires several itera-tions in order to adequately refine the influence regions, such that the first reverse skylines have been determined. Our experimental study (Section 6) verifies the superiority of RSA in terms of pro-gressiveness compared to the BRS algorithm.
We now present the k -Most Attractive Candidates (k-MAC) query, which serves as a motivating example that demonstrates the need to develop a batch processing algorithm for computing several re-verse skyline queries. The k-MAC query is a slight generalization of the problems studied in [15, 12] for the case when the customer preferences also include subjective dimensions. We first present a motivating scenario, which highlights the usefulness of k-MAC queries. We then present the definition of the k-MAC query. Motivating Scenario. A laptop manufacturer wants to produce new notebooks, among a set of feasible alternative configurations proposed by the engineering department. The manufacturer needs to consider three sets: ( i ) the existing competitor products set of customers X  preferred specifications C , and ( iii ) a set of can-didate products Q . We will refer to each q  X  Q as a candidate goal of the manufacturer is to identify the specifications that are ex-pected to jointly attract the largest number of potential buyers. Note that this is different than simply selecting the k products that are the most attractive individually, since it does not make much sense to select products that seem attractive to the same set of customers. Problem Definition. We first define the joint influence set set of candidates Q . We then define the notion of the joint influence score and introduce the k-Most Attractive Candidates (k-MAC)
D EFINITION 4. (Joint Influence Set): Given a set of products P, a set of customers C and a set of candidates Q, the joint influence set of Q, denoted as RSKY ( Q ) , is defined as the union of individual influence sets of any q i  X  Q: RSKY ( Q ) = [ Following the above definition, the joint influence score for a set of candidates Q is equal to the size of the joint influence set of Q , | RSKY ( Q ) | . We now introduce the k-Most Attractive Can-didates (k-MAC) query as follows: D EFINITION 5. (k-Most Attractive Candidates (k-MAC) query): Given a set of products P, a set of customers C, a set of candidates Q and an integer k &gt; 1 , determine the subset Q  X   X  Q, such that | Q | = k and the joint influence score of Q  X  , IS ( Q  X  ) , is maximized.
Note that several candidates might be interesting for the same customer. Additionally, we emphasize that for evaluating a k-MAC query each candidate q  X  Q is considered separately from other can-didates and only with respect to existing products. In other words, intra-candidate dominance relations are not taken into account for k-MAC queries. This is consistent with a real-world setting where a manufacturer is interested to compare their product portfolio only with respect to the competition. We discuss how we resolve ties at the end of this section where we present a greedy algorithm that computes an approximate solution for the k-MAC problem.
Unlike recent works [15, 12] that targeted similar problems a s-suming only  X  X bjective X  attributes, i.e. those having a globally pre-ferred value (such as zero price, infinite battery life, etc.), k-MAC can handle cases where customer preferences are expressed over  X  X ubjective X  dimensions (e.g., screen size, processor type). This generalisation is possible because the attractiveness of each candi-date products is computed based on the size of their bichromatic influence set. Moreover, while our focus is on efficiently comput-ing the influence sets of multiple candidate products, the emphasis of [15, 12] is on the selection of the proper candidates after dominance relationships among products have been determined. A Greedy Algorithm. Unfortunately, processing k-MAC queries is non-trivial. This problem can be reduced to the more general maximum k-coverage problem. Thus, even if we consider the much simpler problem where all the influence sets of all candidates have been computed, an exhaustive search over all possible k -cardinality subsets of Q is NP-hard. Based on the complexity of computing the subset of k products, we now seek an efficient, greedy algo-rithm for this problem. Our solution is based on the generic covering algorithm provided in [8], developed for finding efficient approximate solutions to the maximum k-coverage problem.
L EMMA 3. (from [8]) k-stage covering algorithm returns an approximate solution to the maximum k-coverage problem that is guaranteed to be within a factor 1  X  1 / e from the optimal solution.
We now show how we can adapt the k -stage covering algorithm for the k-MAC problem. kGSA ( k -stage Greedy Selection Algo-rithm) takes as input a set of candidate products Q and their associ-ated influence sets and returns a set Q  X   X  Q , | Q  X  | = k that contains the candidates which formulate a ( 1  X  1 / e ) -approximate solution to the k-MAC query. kGSA proceeds in iterations, by adding one candidate into Q  X  during each iteration. All candidates are exam-ined at each iteration, and kGSA selects the one that, if added in Q Q of IS ( Q  X  ) , kGSA applies a second criterion; it selects the candidate with the minimum sum of distances from its respective reverse sky-lines (customers). The intuition is that a candidate product that is closer to a user X  X  preferences would more likely increase user sat-isfaction. kGSA terminates after k iterations and returns
Solving the k-MAC problem requires processing all candidates, in order to first determine their influence sets. The kGSA algorithm can then be used to solve the k-MAC problem.

A straightforward way to process multiple candidates would be to apply either BRS or RSA for each candidate individually. How-ever this approach is very inefficient in terms of I/O accesses, be-cause it requires accessing each entry e p ( e c ) several times, if ( e ) appears in the priority queues of more than one candidate. Our bRSA algorithm. We now introduce our bRSA algorithm, which aims at eliminating the drawbacks of the baseline approach by exploiting I/O commonalities and by offering shared processing among candidates. bRSA utilizes in its core the RSA algorithm that we presented in Section 3. Note that, apart from k-MAC queries, bRSA can be applied for any query type that requires joint process-ing of multiple reverse skyline queries. bRSA efficiently processes candidates in parallel, by grouping them in batches, in such a way that grouped candidates benefit by the processing of other group members. A primary goal of bRSA is to save duplicate I/O accesses, by using entries that have been ex-panded during an iteration of the RSA subcomponent for one group member, in order to prune entries that appear in the local priority queues of other group members as well. In particular, whenever an entry e x is expanded, all local priority queues in which pears are appropriately updated. Hence, each disk page is accessed only once per batch. Additionally, in order to further optimize the processing of group members, bRSA maintains a list of currently found product points, that will expectedly have large pruning po-tential for other group members, based on Lemma 1. We will refer to these points as vantage points and we will explain their use in the following where we discuss bRSA execution in detail.
Note that we cannot safely assume that all the necessary data structures that bRSA utilizes (local priority queues, skyline sets for each candidate, list of vantage points etc.) will actually fit in main memory. Based on the memory capabilities of the hardware and worst case estimates of the amount of customer and product entries in
E P and E C , let us assume that G candidates (where G  X  | Q | in main memory and can be simultaneously processed. Using worst case estimates does not have a severe impact in the performance of bRSA; in fact, as we demonstrate experimentally, it is better to keep G to fairly modest values (i.e., up to 10 candidates). Larger batch sizes may result in increasing processing cost for the maintenance of local priority queues and significantly more dominance checks which gradually eliminates the benefit from shared processing.
Candidates in proximity in the multidimensional space are more likely to benefit from shared processing. Hence, as a preprocess-ing step, bRSA partitions the candidate set into  X  X  Q | / G  X  based on a locality preserving hashing method, such as the Hilbert space filling curve. 3 Then, bRSA picks one candidate at a time in a round robin fashion (Line 5), and executes a single iteration of a modified version of the RSA algorithm for that candidate, termed Batch-RSA. Batch-RSA extends RSA to be efficiently used on a batch setting. We now present the differences of Batch-RSA com-pared to its single point counterpart. First, whenever an entry is expanded, all local priority queues in which e x appears are ap-propriately updated. Further, when a leaf product entry, say discovered (Line 12), the algorithm decides whether p i should be inserted to a buffer H P that contains vantage points, i.e. those that will be used for pruning by other candidates (Line 17). Intuitively, product points that reside closer to a candidate, will dominate the largest possible space and their pruning power will be maximized. Thus, we implemented H P as a priority queue on the minimum Eu-clidean distance, among the candidates inside the batch. If full, the most distant point in H P , is replaced with p i points (essentially their respective midpoints) are used addition-ally to skyline points when checking each customer entry for dom-inance (second condition in OR clause of Line 5), hence avoiding some of the subsequent I/Os.
All algorithms examined in our experiments were implemented in C++ and executed on a 2.0 GHz Intel Xeon CPU with 4 GB RAM running Debian Linux. The code for the BRS algorithm was thankfully provided to us by the authors of [25].
We used a publicly available generator [1] in order to construct different classes of synthetic data sets, based on the distribution of the attributes X  values; i.e., uniform (UN), anti-correlated (AC) and correlated (CO). Due to space limitations, in the following we plot the results primarily for uniform (UN) data sets. Experiments involving AC and CO data, as well as combinations among them (e.g., uniformly distributed products and anti-correlated customers) Algorithm 2: b RSA Function B atch-RSA generally follow similar trends. We also evaluated our algor ithms on two real world data sets. The NBA data set (NBA) consists of 17,265 5-dimensional points, representing the average values of a player X  X  annual performance with respect to the number of points scored, rebounds, assists, steals and blocks. The household data set (HOUSE), consists of 127,930 6-dimensional points, representing the percentage of an American family X  X  annual income spent on 6 types of expenditure: gas, electricity, water, heating, insurance, and property tax. In order to generate customer and candidate sets, we added Gaussian noise to actual points. For both synthetic and real data sets we normalized all attribute values to [0,10000] and for each data set, we built an R-tree with a page size equal to 4KB.
We compared the performance of our RSA and bRSA algorithms with the state-of-the-art BRS algorithm for evaluating both reverse skyline and k-MAC queries. For BRS and RSA, we measured the total CPU time and I/O operations required for processing workload of | Q | reverse skyline queries, and (ii) a k-MAC query given an input of | Q | candidate products. The bRSA algorithm ap-plies only for the k-MAC query. In particular, we measured:  X  The number of I/Os (separately on product and customer en- X  The time spent on CPU.  X  The total query processing time, consisting of the time spent
Recall that for the reverse skyline query type, both BRS and RSA process query points sequentially. For evaluating k-MAC queries, we modified both algorithms by adding (i) a preprocessing step that presorts candidates based on their Hilbert hash value, and step that greedily outputs the best candidates using our kGSA algo-rithm. In our experiments the measured processing time required for both steps was negligible compared to the time required for the algorithm execution. Further, it is important to emphasize that none of the algorithms is affected by the value of k , since they first have to determine the influence sets of all candidates, and then greedily select the optimal k -subset based on kGSA.

In each experiment we vary a single parameter while setting the remaining to their default values. The default values for both prod-uct and customer data set cardinalities were set to 100,000, the de-fault data dimensionality was set to 3, the default domain range of each attribute was [0, 10000], the default batch size was set to 10, and the default buffer size was set to 12.5% of the data set size. Sensitivity Analysis vs. Data Dimensionality . We first vary the dimensionality of the data sets from 2 to 5 and examine the perfor-mance of all algorithms. Figures 6(a)-6(b) show the results for the number of I/Os and the total processing time, respectively, in loga-rithmic scale. The corresponding numbers for the BRS and RSA al-gorithms are also presented, for clarity, in Figure 6(c). As expected (refer to the complexity analysis in Section 3), BRS becomes pro-hibitively expensive for data with more than 3 dimensions. In par-ticular, BRS requires 3.35 times more CPU time than RSA even in 2 dimensions, and is about 46 times slower in terms of CPU time, and 13.5 times slower in terms of total processing time in the 5-dimensional data set. Figure 6(k) shows an analogous behaviour of the algorithms in anti-correlated data. We also experimented with higher dimensionalities, e.g., for D = 6, BRS took  X  15 hours to finish, whereas RSA terminated in 20.2 minutes. However, we did not include these results in the plots due to space limitations. Our experiments with real data sets show that BRS is impractical for higher dimensions, which justifies our motivation for a more ef-ficient reverse skyline algorithm. It is important to notice that in higher dimensionalities the I/O cost of BRS is dominated by the CPU cost (note that Figures 6(a)-6(b) are in logarithmic scale). To understand why BRS escalates poorly with D recall that the sizes of
SKY ( L ) and SKY ( U ) , which are maintained by BRS, increase rapidly with the data dimensionality [2]. Finally, our bRSA algo-rithm achieves significant performance gains with respect to both BRS and RSA in all settings. Note that our remaining sensitivity analysis using synthetic data sets, utilizes a modest value ( which is a favorable setting for BRS. Obviously, the benefits of our algorithms over BRS were significantly more in higher dimensions. Sensitivity Analysis vs. Data Set Size . We then perform a sen-sitivity analysis with respect to the size of the product data set. Notice the different behavior of the two algorithms with respect to the type of I/Os (Figure 6(d)), due to the different visiting orders followed; generally BRS entails more accesses on the products in-dex, whereas RSA requires more customer I/Os. In terms of I/Os, RSA exhibits similar performance with BRS in the case where the product and customer data have the same size (100K both). How-ever, as the number of products increases, the strategy followed by RSA proves to be more efficient in terms of the total I/O ac-cesses required. Moreover, w.r.t. the processing cost (Figure 6(e)), RSA is significantly faster than BRS, and scales better as | larger. Again, the corresponding numbers for the BRS and RSA algorithms are also presented, for clarity, in Figure 6(f). Finally, our bRSA algorithm is the most efficient algorithm for the case of k-MAC queries remaining essentially unaffected by the size of the product data set. Figure 6(l) shows an analogous behaviour of the algorithms in anti-correlated data, with the times being slightly smaller, as shown by the scale of the y-axis. In Figures 6(g)-6(h) we then plot the I/O and CPU costs when varying the size of the customer data set. As illustrated, RSA and BRS require roughly the same number of I/Os for large numbers of customers. This is predictable since RSA processes one customer entry per iteration, i.e., the number of iterations required by RSA is O ( | C | ) in the case when | C | is much larger than | P | , the visiting strategy followed by BRS would be a more reasonable choice. However, even in this worst case scenario, RSA exhibits better overall per-formance than BRS algorithm, due to the significant lower process-ing cost (Figures 6(h)-6(i)). Again, our bRSA algorithm is notably faster than both single point algorithms.
 Sensitivity Analysis vs. Memory Size . For this experiment we compare the number of page accesses required by each algorithm with respect to the memory size allocated for caching. We varied cache (buffer) size from 50 pages (corresponding to 6.25% of the data set size) up to 200 pages (25% of available memory). We also experimented with two different cache replacement policies. For the first policy, namely P 1, we followed a LRU strategy. Addi-tionally, motivated by the intuition that entries with higher levels in the R-tree will be accessed more frequently, we also used a buffer that maintains pages in descending order of their tree level ( Figure 6(j) plots the number of I/Os required for different cache sizes and cache replacement policies. As depicted, regardless of the memory size and strategy used, both RSA and bRSA algorithms are more efficient in terms of disk accesses. Further, notice that LRU was slightly more efficient for the cache size that we used in our default scenario (12.5% of the data set size).
 Sensitivity Analysis vs. Batch Size . We then investigate the per-formance of our bRSA algorithm with respect to the batch size We set each batch to contain from 5 to 100 candidates and plotted the results in Figures 7(a)-7(b). As expected, larger batch sizes result to fewer total I/O operations, since more pruning can be shared among candidates, whereas the processing cost increases. Interestingly, when the batch size becomes larger than a thresh-old, the total processing cost (cost of disk accesses plus CPU time) increases, due to the growing cost of maintaining all local prior-ity queues and the significantly more dominance checks required thereof. As showcased by the experiments, keeping fairly small batch sizes (  X  10 candidates) maximizes the efficiency of bRSA. Progressiveness . Finally, we compare the progressiveness of RSA and BRS algorithms on a workload consisting of | Q | reverse skyline queries. x-axis represents the percentage of reverse skyline results found so far compared to the total influence score. y-axis plots the time required to report the corresponding percentage of results, both in absolute time (Figure 7(c)) and as a percentage of the total time spent (Figure 7(d)). Both figures demonstrate that RSA is notably more progressive than BRS, especially for reporting the first query results. In particular, RSA outputs the first 5% of the reverse skylines in 1/10th of the time needed by BRS, which can be particularly important for applications that require a quick response or when the complete output is not useful.
 Experiments with Real Data . Figures 8(a)-8(c) and 8(d)-8(f) re-port our experimental findings on the NBA and HOUSE data sets respectively. The results are in accordance with our experiments on synthetic data sets. Moreover, the performance gains achieved by RSA are higher in real data sets (especially vs. | C | ), partly due to the higher data dimensionality (5 and 6 dimensions respectively), which results to more points belonging to the influence set (on av-erage 196 points in HOUSE data set vs 11 points in UN data set).
Market research is a systematic, objective collection and anal-ysis of data about a particular target market by taking into ac-count factors such as products, competition and customers behav-ior. The work of [9] proposed formulating several economically incentivised applications (e.g., potential customers identification product feature promotion , product positioning ) as optimization problems taking a data mining perspective. In the context of database research, DADA [10] was the first of a series of works in the field proposing various queries by capitalizing on the dominance rela-tionships among products and customer preferences.

Several works [5, 11, 25, 18, 6, 3] focus on identifying the po-tential customers of a product. In order to provide an insight on the product against the competition, [13, 24, 23] address the problem of discovering and promoting the best product features. Another practical application is how to design new products, such that they will maximize the expected utility, a problem known as product po-sitioning [20, 19, 21, 12, 15]. The utility function may incorporate various factors such as the number of expected product buyers [19, 21, 12, 15], the actual profit (price minus production cost) [10, 20, 21, 15] or the number and features of other competitive products [10, 12]. Other works [20, 21] seek profitable packages formed by combining individual products, e.g., a flight and a hotel room, such that the profit gain of the package is maximized.

With regards to the number of expected customers, one problem is how to best model user preferences. One way is to assume that a weight vector capturing the importance of different product fea-tures (attributes) has been determined for each customer, through a preference learning process. Based on this assumption, each prod-uct is assigned a score by applying the weight vector known for the user. Then, the products that score higher are those that would be more attractive to the respective user. This is the approach taken in top-k queries [7]. [18] tackled the reverse problem of discovering the most attractive products by introducing reverse top-k queries.
However, the weight vector formulation is often too difficult to come up with in real life [17]. A more natural way to model prefer-ences is by allowing users to directly specify their preferred product attribute values. Taking this approach, both products and user pref-erences can be represented as points in a multidimensional space. In such a scenario, different notions of user satisfaction have been proposed. One option is to allow users to specify the worst accept-able value for each dimension [15]; all products having better val-ues from the specified ones are considered as satisfactory. An im-portant limitation of such a formulation is that it cannot be used for  X  X ubjective X  types of attributes. Further, there is no metric of how relevant each product is w.r.t. the actual user preferences. Thus, another option is to measure product attractiveness based on how close the product attribute values are to the user-preferred ones. In order to find the k most attractive products for a customer can issue a k NN ( c ) query [16] on the product data set. However, in several real applications it could be hard to find an appropriate dis-tance function, because different dimensions might have different weights which depend on the preferences of each user.

With the goal to overcome the limitations of top-k and k NN queries, skylines have been widely used for multi-criteria decision analysis and for preference queries. The skyline query returns the set of not dominated objects, corresponding to the Pareto optimal set, which will always include the top-1 result for any monotone preference function. [4] introduces skyline queries in databases, also presenting various external memory algorithms. In order to capture subjective attributes, the dynamic skyline [14] returns all products that are  X  X ttractive X  according to a user X  X  preferences.
Viewing the problem from a manufacturer X  X  perspective, [5] in-troduces the reverse skyline query, which returns all customers that would find a product as  X  X ttractive X , and proposes a branch-and-bound extension of the BBS algorithm [14] that reduces the search space. [11] improves upon [5] by providing tighter pruning rules based on midpoint skylines (Section 2.2) and presents algorithms for calculating reverse skylines on uncertain data. [25] proposes the BRS algorithm (Section 2.3), which exploits additional optimiza-tions for precise data. [6] considers non-metric attribute domains and proposes non-indexed algorithms to efficiently calculate the in-fluence set in that case, whereas [22] studies how to process reverse skylines energy-efficiently in a wireless sensor network.
In this work, we formulate customers identification by follow-ing a reverse skyline approach, where we consider both subjective dimensions and competition. We focus on providing a more effi-cient and progressive algorithm for single-point reverse skylines. Further, we extend our methods for multiple query points, with ap-plications to the k-MAC query. The methods proposed in [12, 15] cannot be applied onto our setting, because they assume the same product dominance relationships holding for all users and that they can be calculated by executing only one skyline query. However, this is not true when each user has his preferred attribute values.
In this work, we studied two classes of queries involving cus-tomer preferences with important applications in market research. We first proposed the RSA algorithm for reverse skyline query eval-uation. We then developed a batched extension of our RSA algo-rithm that significantly improves upon processing multiple queries individually, by grouping contiguous candidates, exploiting I/O com-monalities and enabling shared processing, and applied this batched extension to solve the k-MAC query. Our experimental study on both real and synthetic data sets demonstrates that ( i ) RSA is sig-nificantly more efficient, scalable and progressive than the BRS al-gorithm for the reverse skyline problem, and ( ii ) that our proposed batched algorithm is the best choice for the k-MAC query. Acknowledgments This research has been co-financed by the Eu-ropean Union (European Social Fund -ESF) and Greek national funds through the Operational Program "Education and Lifelong Learning" of the National Strategic Reference Framework (NSRF) -Research Funding Program: Thales. Investing in knowledge soci-ety through the European Social Fund. The authors would also like to thank Prof. Dimitris Papadias and Prof. Timos Sellis for their guidance and support.
