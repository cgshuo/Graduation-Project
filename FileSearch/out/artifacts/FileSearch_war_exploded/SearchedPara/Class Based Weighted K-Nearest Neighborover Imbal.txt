 A data set is imbalanced, if its dependent variable is categorical and the num-ber of instances in one class is different from those in the other class. In many real world applications such as Web page search, scam sites detection, fraudu-lent calls detection etc, there is a highly skewed distribution of classes. Various classification techniques such as k NN [6], SVM [5], and Neural Networks[10] etc have been designed and used, but it has been observed that the algorithms do not perform as good on imbalanced datasets as on balanced datasets. Learning from imbalanced data sets has been identified as one of the 10 most challenging problems in data mining research [17]. In the literature of solving class imbalance problems, various solutions have been proposed. Such techniques broadly include two different approaches: (1) modifying existing methods or (2) application of a pre-processing stage.

In the recent past, a lot of research centered at nearest neighbor methodology has been done. Although k NN is computationally expensive, it is very simple to understand, accurate, requires only a few parameters to be tuned and is robust with regard to the search space. Also k NN classifier can be updated at a very little cost as new training instan ces with known classes are presented. A strong point of k NN is that, for all data distributions, its probability of error is bounded above by twice the Bayes probability of error [16]. However one of the major drawbacks of k NN is that, it uses only local prior probabilities to predict instance labels, and hence does not take into account, class distribution around the neighborhood of query instance. This results in undesirable performance on imbalanced data sets. The performance of k NN algorithm over imbalanced datasets can be improved, if it uses information about local class distribution while classifying instances.

Fig. 1 shows an artificial two-class imbalance problem, where the majority class  X  X  X  is represented by circles and t he minority class  X  X  X  by triangles. The query instance is represented by cross. As can be seen from the figure, the query instance would have been classified as the majority class  X  X  X  by a regular k NN algorithm with k value equal to 7. But if the algorithm had taken into account the imbalance class distribution around the neighborhood of the query instances (say in the region represented by dotte d square), it would have classified the query instance as belonging to minority class  X  X  X , which is the desired class.
In this paper, we propose a modified K-Nearest Neighbor algorithm to solve the imbalanced dataset classification problem. More specifically the contribu-tions of this paper are as follows: 1. First we present a mathematical model of K-Nearest Neighbor algorithm 2. To solve the above problem, we propose a Weighted K -Nearest Neighbor 3. A thorough experimental study of the proposed approach over several real The organization of rest of the paper is as follows. In section 2, we throw light on related, and recent, work in the liter ature. Section 3 deals with problem formulation and mathematical model of k NN. We explain the modified algorithm in Section 4. In Section 5, experimental r esults are presented together with a thorough comparison with the state-of-the-art algorithms. Finally, in Section 6, conclusions are drawn. In the literature of solving class imbalance problems, various solutions have been proposed; such techniques broadly include two different approaches, mod-ifying methods or the application of a pre-processing stage. The pre-processing approach focuses on balancing the data, which may be done either by reduc-ing the set of examples (undersampling) or replicate minority class examples (oversampling) [8]. One of such earliest and classic work is the SMOTE method [3] which increases the number of minor c lass instances by creating synthetic samples. This work is also based on the nearest neighbor analogy. The minor-ity class is over sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors. A recent modifi cation to SMOTE proposes that, using different weight degrees on the synthetic samples (so-called safe-level-SMOTE [2]) produces better accuracy than SMOTE. Alternative approaches that mod-ify existing methods focus on extending or modifying the existing classification algorithms so that they can be more effective in dealing with imbalanced data. HDDT [4] and CCPDT [15] are examples of such methods, which are modifica-tion of decision tree algorithms.

One of the oldest, accurate and simplest method for pattern classification and regression is K -Nearest-Neighbor ( k NN) [6]. k NN algorithms have been identified as one of the top ten most influential data mining algorithms [19] for their ability of producing simple but powerful classifiers. It has been studied at length over the past few decades and is widely applied in many fields. The k NN rule classifies each unlabeled example by the majority label of its k -nearest neighbors in the training dataset. Despite its simplicity, the k NN rule often yields competitive results. A recent work o n prototype reduction, called Weighted Distance Nearest Neighbor (WDNN) [11] is based on retaining the informative instances and learning their weights for classification. The algorithm assigns a non negative weight to each training instance tuple at the training phase. Only the training instances with positive weight are retained (as the prototypes) in the test phase. Although the WDNN algorithm is well formulated and shows encouraging performance, in pr actice it can only work with K =1.Amore recent approach WD k NN [20] tries to reduce the time complexity of WDNN and extend it to work for values of K greater than 1. Chawla and Liu in one of their recent work [14] presented a novel K -Nearest Neighbors weighting strategy for handling the problem of class imbalance. They proposed CCW (class confidence weights) that uses the probability of attribute values given class labels to weight prototypes in k NN. While the regular k NN directly uses the probabilities of class labels in the neighborhood of the query instance, they used conditional probabilities of classes. They have also shown how to calculate CCW weights using mixture modeling and Bayesian networks. The method performed more accurately than the existing state-of-art algorithms.
KaiYan Feng and others [7] defined a new neighborhood relationship known as passive nearest neighbors. For two points A and B belonging to class L ,point B is the local passive k th -order nearest neighbor of A , only and only if A is the k th nearest neighbor of B among all data of class L . For each query point, its k actual nearest neighbor and k passive nearest neighbors are first calculated and basedonit,aoverallscoreiscalculatedf or each class. The class score determines the likelihood that the query points belong to that class.

In another recent work [12], Evan and oth ers proposes to use geometric struc-ture of data to mitigate the effects of cla ss imbalance. The method even works, when the level of imbalance changes in the training data, such as online stream-ing data. For each query point, a k dimensional vector is calculated for each of the classes present in the data. The vector consist of distances of the query point to it X  X  k nearest neighbors in that class. Based on this vector probability that the query point belongs to a particular c lass is calculated. However the approach is not studied in depth.

Yang Song and others proposes [18] two different versions of k NNbasedonthe idea of informativeness. According to them, a point is treated to be informative, if it is close to the query point and far away from the points with different class labels. One of the proposed versions LI-KNN takes two parameters k and I ,It first find the k nearest neighbor of the query point and then among them it find the I most informative points. Based on the class label of the informative points, class label is assigned to the query point. They also showed that the value of k and I have very less effect on the final result. The other version GI-KNN works on the assumption that some points are more informative then others. It tries to find global informative points and then assigns a weight to each of the points in training data based on their informativ eness. It then uses weighted euclidean metric to calculate distances.

In another recent work [13], a k Exempl ar-based Nearest Neighbor (kENN) classifier was proposed which is more sensitive to the minority class. The main idea is to first identify the exemplar minority class instances in the training data and then generalize them to Gaussian balls as concept for the minority class. The approach is based on extending the decision boundary for the minority class. In this section, we presen t a mathematical model for k NN algorithm along with the notation used to model the dataset. We also show that k NN only makes use of local prior probabilities for classification.

The problem of classification is to estimate the value of the class variable based on the values of one or more independent variables (known as feature variables).Wemodelthetupleas { x , y } where x is an ordered set of attribute values and y is the class variable to be predicted. There are d attributes overall corresponding to a d -dimensional space.

Formally, the problem has the following inputs:  X  Asetof n tuples called the training dataset, D = { ( x 1 , y 1 ), ( x 2 , y 2 ), ...,  X  A query tuple x t .
 The output is an estimated value of the class variable for the given query x t , mathematically it can be expressed as: Where parameters are the arguments that the function f () takes. These are generally set by the user or are learned by some method. 3.1 Mathematical Model of k NN For a given query instance x t , k NN algorithm works as follows: Where y t is the predicted class f or the query instance x t and m is the number of classes present in the data. Also Eq. (2) can also be written as and we know that Where p ( c j ) ( x of x t . Hence Eq. 5 turns out to be It is clear from Eq. 7, that k NN algorithm uses only prior probabilities to calcu-late the class of the query instance. It ignores the class distribution around the neighborhood of query point. In this section, we will explain in detail our proposed algorithm. To tune the existing k NN algorithm , we introduce a weighting factor for each class. Our algorithm can be formally expressed as follows, for a given query instance x t : Where W [ c,x t ] denotes the weighting factor for the class c , while classifying query instance x t . For Weighting factor equal to 1 for all the classes, our algo-rithm reduces to the existing k NN classifier. This weighting factor is introduced to take into account, class distribution around the query instance. The proposed algorithm is sensitive to the value of weighting factor.

Now we discuss on how to learn the value of weighting factor for each of the classes. Fig. 2 illustrates an imbalance dataset, in which data points are present in clusters with each cluster having exact ly one major class. In this case, regular k NN algorithm would fail to classify the minority class instances present at the boundary of the cluster region (for example query instance 1).

To design the weights, we considered both query dependent and query inde-pendent weighting factor. If our learned weighting factors have a constant value for each of the class through out the dataset i.e. they do not depend on the query instance, and favors the minority class then, our algorithm would have classified the minority class instances present at the boundary of the clusters correctly, but have not classified points like instance 2 correctly. Having only class dependent weighting factor values would not capture the data distribution around the neighborhood of the query instance.
 Our weighting factor value W [ c,x t ] can be denoted as : where where c is the class to which x i is classified by existing k NN classifier. if c equals to y i then getcoef ( x i ) turns out to be 1.

Hence for a query point the weighting factor of a class is calculated based on how the k/m nearest neighbors of query point belonging to that class are classified by the existing k NN classifier. If a instance is classified correctly getcoef will return 1 for it, else it will return the value by which the class prior probability should be multiplied, so that it is classified correctly. The basic intuition about the above formulae is simple:  X  X f instances of a specific class are poorly classified in a particular region, then that class i s likely to be a minority class in that region and should be given a higher weight. X  4.1 Properties of Weighting Factor 1. The weighting factor for each of the class is calculated based on how the Algorithm 1. Pseudo code 2. Value of weighting factor is bounded between 0.5 to 1, proof of which is : 4.2 Complexity Analysis The proposed algorithm needs to search for the k nearest neighbors of the query point (global nearest neighbors, line 5 of Algorithm 1.), same as the regular k NN algorithm. Apart from finding the global nearest neighbors, it also need to calculate the weighting factor for each of the class. Following calculations are involved in the calculation of weighting factors : 1. For each of the class, find k/m nearest neighbors of query point among that 2. For each of the points obtained above calculate getcoef function, which needs 5.1 Performance Model In this section, we demonstrate our experimental settings. The experiments were conducted on a wide variety of datasets obtained from UCI data repository [1] and Weka Datasets [9]. A short description of all the datasets is provided in Table 1. These datasets have been selected as they typically have a low minority class percentage and hence are imbalance. We have evaluated our algorithm against the existing state of art approaches. All the results have been obtained using 10-fold cross validation technique, except for SMOTE. For SMOTE each of the dataset is first randomized and then divided into training and testing data. SMOTE sampling is applied on training data to oversample the minority class and then regular k NN is used to classify instances present in testing data using the sampled training data.

As it is known that the use of overall accuracy is not an appropriate evalu-ation measure for imbalanced datasets , because of the dominating effect of the majority class, we have used F-Score as the evaluation metric. F-Score considers both the precision and the recall of th e test to compute the score. We compared our performance against the following approaches: Regular K Nearest Neigh-bors ( k NN) 1 , Exemplar k NN ( k ENN) 2 ,SMOTE,HDDT 3 , C4.5 , CCPDT 4 , NaiveBayes (Naive). For all k NN based approaches (including SMOTE) F-Score obtained at maximum accuracy is mentioned. 5.2 Results and Discussion Table 2. compares the result of our modified algorithm with existing state of the art algorithm. The number in the parenth esis indicates the rank of the respec-tive algorithm. Also the top two algorithms are highlighted in bold. It can be seen that our approach produces consistent ly accurate classifier and outperforms other algorithms in most of the datasets. Also our proposed algorithm always outperform regular k NN on all the datasets, this confirms that the modified k NN algorithms takes into account the nature of the data to classify it. However for Ionosphere dataset decision tree based algorithms perform better than other state of the art algorithms.

Fig. 3 compares performance of our algorithm with k NN in terms of overall accuracy and accuracy to classify minority class, as the value of k varies for Hungarian dataset. It becomes clear from the figure that our algorithm based classifier are more sensitive to classify minority class and are still highly ac-curate. Also classifier learned from our approach are more accurate for larger values of k , this is evident from the fact that, for high value of k large region around the neighborhood of query point is considered to determine the local class distribution.
 In this paper, we have proposed a modified version of K -Nearest Neighbor al-gorithm so that it takes into account, class distribution around neighborhood of query instance during classification. In our modified algorithm, a weight is calcu-lated for each class based on how its instances are classified by existing K -Nearest Neighbor classifier around the query instance and then a weighted k NN is applied. We have also evaluated our approach against the existing standard algorithms. Our work is focused on tuning the K -Nearest Neighbor for imbalance data, so that its performance on imbalance data i s enhanced. As shown in the experimen-tal section, our approaches more than often, outperforms existing state of the art approaches on a wide variety of datasets. Also our modified algorithm perform as good as the existing K -Nearest Neighbor classifier on balance data.

