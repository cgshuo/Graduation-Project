 Attributed graphs are increasingly more common in many appli-cation domains such as chemistry, biology and text processing. A central issue in graph mining is how to collect informative subgraph patterns for a given learning task. We propose an iterative mining method based on partial least squares regression (PLS). To apply PLS to graph data, a sparse version of PLS is developed first and then it is combined with a weighted pattern mining algorithm. The mining algorithm is iteratively called with different weight vectors, creating one latent component per one mining call. Our method, graph PLS, is efficient and easy to implement, because the weight vector is updated with elementary matrix calculations. In exper-iments, our graph PLS algorithm showed competitive prediction accuracies in many chemical datasets and its efficiency was signif-icantly superior to graph boosting (gBoost) and the naive method based on frequent graph mining.
 I.5.2 [Pattern Recognition]: Des ign Methodology X  X  eature evalu-ation and selection; H.2.8 [Database Management]: Database Ap-plications X  X ata mining Algorithms, Experimentation, Performance Partialleastsquaresregression, graphmining, graphboosting, chemoin-formatics
As data mining and machine learning techniques continue to evolve and improve, the role of structure in the data becomes more and more important. Much of the real world data is represented not as vectors, but as graphs including sequences and trees, for ex-ample, biological sequences, semi-structured texts such as HTML and XML, chemical compounds, RNA secondary structures, and so Figure 1: Feature space based on subgraph patterns. The feature vector consists of binary pattern indicators. forth. Like ordinary vectorial data, there are two kinds of learning tasks; unsupervised [30, 31] and supervised [22]. Among super-vised learning tasks, graph regression and classification would be of wide interest. In graph regression, an attributed graph is given as an input, and a real-valued output variable is predicted. In classifi-cation, the output variable is binary.

In learning from graph data, one can rely on the similarity mea-sures derived from graph alignment [28] or graph kernels [10, 23, 6, 16]. However, one drawback is that the features used in learn-Another approach is based on graph mining, where a set of small graph is represented as a binary vector of pattern indicators (Fig-ure 1). Graph mining is especially popular in chemoinformatics, where the task is to classify chemical compounds [11, 7]. When all possible subgraphs are used, the dimensionality of the feature space is too large for usual statistical methods. Therefore, feature collection is a central issue in graph mining algorithms [30, 1, 35].
To summarize the feature collection methods proposed so far, let us classify them into two categories: mine-at-once and iterative mining. In the first category, the whole feature space is built by one mining run before the subsequent machine learning algorithm is started. A naive approach is to use a frequent substructure mining algorithm such as AGM [9], gSpan [36] or Gaston [20] to collect frequently appearing patterns. This approach was employed by [7] and [11], where a linear support vector machine is used for classifi-cation. A more advanced approach is to mine informative patterns with high correlation to the output variable [19, 1]. However, salient patterns depend on the optimal parameters of the subsequent learn-informative for any learning algorithm [12].

Among iterative mining methods, substructure boosting meth-ods [15, 22, 26] have been successfully applied to many different domains such as images [22], videos [21], chemical compounds [26] and biological mutation sets [27]. The boosting algorithm calls a pattern mining algorithm repeatedly to incrementally form a feature space. In the first iteration, the patterns with high correlation with the target variable are collected. In subsequent iterations, the algo-rithm updates the example weights such that more emphasis is put on mispredicted examples. It is reported that it creates less useless features compared to the mine-at-once methods [22]. In the very first paper by Kudo et al. [15], AdaBoost was used for updating the example weights. However, AdaBoost is not efficient in graph mining, because it takes too many iterations to finish. Thus recent papers use mathematical programming-based approaches such as linear programming boosting (LPBoost) [3, 24] and quadratic pro-gramming boosting (QPBoost) [26]. Furthermore, to reduce the number of iterations, several patterns are collected at the same time in one iteration by multiple pricing [22]. Nevertheless, substructure boostingcanstill beimprovedintermof efficiency, becausethecom-putation time for mathematical programming is substantially large. In itemset boosting [27], it is reported that the computational time for mathematical programming is much larger than that needed for mining. In particular, when solving a regression problem, one has to use a quadratic program that is computationally more demanding than a linear program.

We propose a new iterative mining method based on partial least squares regression (PLS) [33, 25, 34]. PLS is an iterative algo-rithm that extracts latent features iteratively from a high dimen-sional space. An attractive point of PLS is that it depends only on elementary matrix calculations (i.e., addition and multiplications). Therefore, it is more efficient than other methods depending on mathematical programming or eigen-decomposition. In gBoost, the transition from vectorial to graph data is achieved by replacing the feature selection step by a pattern mining algorithm [22]. In PLS, it is not so simple, because conventional algorithms for PLS such as NIPALS [33] require the deflation of the whole feature matrix. The feature matrix consists of the feature vectors of all training exam-ples, and NIPALS substracts a dense matrix from the feature matrix in each iteration. It is possible only if the whole feature matrix is loaded to memory, which is not practical in graph mining.
In this paper, we develop a sparse version of non-deflation PLS such that each latent component depends on a limited number of subgraph patterns. Then, it is combined with a pattern mining algo-rithm to deal with graph data. We call our algorithm graph PLS or gPLS in short. gPLS collects informative patterns in a limited num-ber of iterations, as it avoids the discovery of identical patterns by means of orthogonality constraint s. Like gspan and gBoost, gPLS employs the DFS code tree [36] as the canonical search space of graph patterns. The criterion for pattern search is quite simple and substructure mining) can be reused in gPLS as well.

This paper is organized as follows. In Section 2, we introduce the PLS regression and present its non-deflation version. Section 3 explains how PLS is applied to graph data. In Section 4, extensive experiments for various chemical datasets are presented. Section 5 discusses other possibilities in developing graph regression algo-rithms. Finally, we conclude the paper in Section 6.
This section reviews the partial least squares regression (PLS) algorithm for vectorial data. We first explain the conventional NI-PALS algorithm and introduce a new non-deflation algorithm. The transition from vectorial to graph data will be discussed in the next section.

Let us assume n training examples ( x 1 , y 1 ),...,( x n x  X  d and y i  X  . The output y i is assumed to be centralized i y i sponds to x i . Also denote by y the vector of all training outputs. The regression function of PLS is linear, but the following special form, where w i are weight vectors that reduce the dimensionality of x , satisfying the followi ng orthogonality condition: We need to determine two kind of parameters w i and  X  i . Basically, w i are learned first, and the coefficients  X  i are obtained by least squares regression without any regularization, Due to the orthogonality conditions, t his problem is easily solved as The weight vectors are determined by the following greedy algo-rithm. The first vector is obtained by maximizing the covariance between the mapped feature X w 1 and the output variable y , subject to w X X w = 1. This problem is solved analytically as where  X  is the normalization factor For the i -th weight vector, the same optimization problem is solved with additional constrai nts to keep or thogona lity, subject to Theoptimal solutionof this problemcannot beobtainedanalytically. Since the regression of  X  is done without any regularization, it is important to choose the number of weight vectors appropriately. model selection criteria such as AIC and BIC [25].
Letusdefinethe i -thlatent component as t i = X w i .TheNIPALS algorithm [33] solves the optimization problem (6) in an indirect way, namely the optimal latent components t i are obtained first and the corresponding w i is obtained later. Let us define T i matrix of latent components obtained so far, and define a projection matrix as The second equality is due to the orthogonal conditions (2). Then, a deflated design matrix  X  X is defined as Now we solve the following problem based on the deflated matrix, appropriate terminology for v i , but here we call it the i -th pre-weight vector, because it is used to create the  X  X eal X  weight vector w . Based on v i , the optimal latent component is obtained as Finally, we have to recover the optimal weight vector w i the following equation [8], Assuming the linear independence of rows of X , the equation is solved as which corresponds to the optimal solution of (6).

The NIPALS algorithm consists of only elementary matrix com-putations and therefore is more efficient than solving (6) as an con-strained quadratic program. The algorithm is summarized in Algo-rithm 1. Due to the following relationship, the deflated matrix is updated rather than recomputed in each iter-ation. However, for our purpose, the crucial drawback is that the sparseness of X is lost by deflation.
 Algorithm 1 The NIPALS algorithm. 1: Initial:  X  X 1 = X 2: for i = 1 ,..., m do 3: v i =  X  X i y / X  . Pre-weight vector 4: t i =  X  X i v i Latent components 5:  X  X i + 1 =  X  X i  X  t i t i  X  X i Deflation 6: end for 7: Conversion of v i to w i for all i as (9)
We now present an alternative derivation of PLS that avoids the deflation step and that is based on the connection of PLS the the Lanczos method and that uses recursive fitting of residuals [5, 13].
Substituting the definition of the projection matrix to the pre-weight vector (8), we obtain The NIPALS algorithm first computes the deflated matrix X T  X  1 T i  X  1 ) and then multiplies it with y . However, an obvious alternative way is to compute the residual vector and then multiply it with X . Following this idea, the NIPALS algorithm can be modified to a non-deflation version (Algorithm 2).
In graph mining, it is useful to have sparse weight vectors that only a limited number of patterns are used for prediction. To this aim, we modify the algorithm further by introducing sparseness to the pre-weight vectors v i as follows: Due to the linear relationship between v i and w i , it is understood that w i becomes sparse as well. The sparse weight vectors satisfy the orthogonality cond itions (2). There are two alternative ways to determine the threshold :1)Sort | v ij | in the descending order, take the top-k elements, and set all the other elements to zero. 2) Set to a fixed threshold. In the latter case, the number of non-zero elements in v i may vary. In the experiments presented in this paper, we took the former top-k approach to avoid unbalanced weight vectors and to make efficiency comparisons easier.

It is worthwhile to notice that the residual of regression up to the i  X  1-th features, the pre-weight vector v is obtained as the direction that maximizes the covariance with residues. This observation highlights the re-semblance of PLS and boosting algorithms [3]. In boosting, ex-ample weights are iteratively altered such that the examples with high residues are weighted more. In this formulation of PLS, it is clearer that the residue vector plays a role similar to that of boost-ing X  X  example weights. The connection between PLS and boosting is discussed in [17].
 Algorithm 2 Non-deflation Sparse PLS algorithm. 1: for i = 1 ,..., m do 2: r i = ( I  X  T i  X  1 T i  X  1 ) y Residue 3: v i = X r i / X  . Pre-weight vector 4: v ij = 0, if | v ij | X  , j = 1 ,..., d Sparsify 5: w i = v i  X  i  X  1 j = 1 (w j X X v i )w j Weight vector 6: t i = X w i Latent components 7: end for
In this section, we discuss how to apply the non-deflation PLS algorithm to graph data. Here we deal with undirected, labeled and connected graphs. To be more precise, we define the graph and its subgraph as follows:
Definition 1 (Labeled connected graph). Alabeledgraphisrep-E  X  V  X  V is a set of edges, L is a set of labels, and lV  X  is a mapping that assigns labels to the vertices and edges. A labeled connected graph is a labeled graph such that there is a path between any pair of vertices.

Definition 2 (Subgraph). Let G = ( V , E , L , l ) and G (
V , E , L , l ) be labeled connected graphs. G is a subgraph of G Figure 2: Schematic figure of the tr ee-shaped search space of graph patterns (i.e., the DFS code tree). To find the optimal pattern effi-ciently, the tree is systematically expanded by rightmost extensions. ( G  X  G ) if the following conditions are satisfied: (1) V (2) E  X  E ,(3) L  X  L ,(4)  X  v  X  V , l (v ) = l (v ) and (5)  X  e  X  E , l ( e ) = l ( e ) .If G is a subgraph of G ,then G is a supergraph of G .

Our training set is represented as ( G 1 , y 1 ),...,( G n G i is a graph and y i  X  is a target value. Let p be a subgraph subgraphs included in at least one graph. Then, the whole feature vector of each graph G i is encoded as a | P | -dimensional vector x This feature space has already been illustrated in Figure 1. Since | P | is a huge number, we cannot keep the whole design matrix. So we need to set X as the empty matrix first, and grow the matrix as the iterations proceed. In each iteration, we obtain the set of patterns p whose pre-weight | v ip | is above the threshold, which can be written as Then, the design matrix is expanded to include newly introduced patterns. The pseudocode of gPLS is described in Algorithm 3. Most numerical computations are carried over from Algorithm 2 except that the residue vector is updated.

The pattern search problem (12) is exactly the same as the one solved in gBoost [22]. So we can reuse the same method to enu-merate P i . More specifically, it can be done by gspan function in the gBoost MATLAB toolbox 1 . However, we explain the pattern search algorithm briefly for the completeness of this paper.
Our search strategy is a branch-and-bound algorithm that requires a canonical search space in which a whole set of patterns are enu-merated without duplication. As the search space, we adopt the DFS code tree [36]. The basic idea of the DFS code tree is to orga-nize patterns as a tree, where a child node has a supergraph of the pattern in its parent node. (Figure 2). A pattern is represented as a text string called the DFS (depth first search) code. The patterns are enumerated by generating the tree from the root to leaves using a recursive algorithm. To avoid duplications, node generation is systematically done by rightmost extensions. Algorithm 4 shows the pseudo code for the recursive algorithm.

For efficient search, it is important to minimize the size of the search space. To this aim, tree pr uning is crucially important [18, http://www.kyb.mpg.de/bs/people/nowozin/gboost/ Algorithm 3 gPLS 1: r 1 = y , X = X  2: for i = 1 ,..., m do 3: P i ={ p | n j = 1 r ij x jp  X  } Pattern search 4: X P i : design matrix restricted to P i 5: X  X  X  X  X P i 6: v i = X r i / X  Pre-weight vector 7: w i = v i  X  i  X  1 j = 1 (w j X X v i )w j Weight vector 8: t i = X w i Latent component 9: r i + 1 = r i  X  ( y t i ) t i Update residues 10: end for Algorithm 4 Pattern search algorithm 1: procedure Pattern Search 2: P  X  X  X  3: for p  X  DFS codes with single nodes do 4: project( p ) 5: end for 6: return P 7: end procedure 8: function project( p ) 9: if p is not a minimum DFS code then 10: return 11: end if 12: if pruning condition (13) holds then 13: return 14: end if 15: if p satisfies the condition (12) then 16: P  X  P  X  X  p } 17: end if 18: for p  X  rightmost extensions of p do 19: project( p ) 20: end for 21: end function 15]. Let us define the gain function as s ( p ) = n j = 1 Suppose the search tree is generated up to the pattern p .Ifitis guaranteed that the gain of any supergraph p is not larger than , we can avoid the generation of downstream nodes without losing the optimal pattern. Our pruning condition is described as follows.
Theorem 1. Define  X  y i = sgn ( r i ) . For any pattern p such that p  X  p , s ( p )&lt; ,if where Other conditions such as the maximum size of pattern (maxpat) and the minimum support (minsup) can be used in combination with the pruning condition (13). Figure 3: Regression accuracy (left) and computational time (right) against maximum pattern size (maxpat) in the EDKB dataset. Figure 4: Classification accuracy (left) and computational time (right) against maximum pattern size (maxpat) in the CPDB dataset.
In this section, we evaluate our method using four publicly avail-able chemical datasets: EDKB 2 ,CPDB 3 ,CAS 4 and AIDS 5 . Links to these datasets can be found in ChemDB [2]. Table 1 shows the summary of the datasets. Among them, the AIDS dataset [14, 4] is by far the largest both in the number of examples and the graph size. EDKB is a regression dataset, but the others are classification datasets. In gPLS, we solved classification problems by regress-ing the target values + 1 ,  X  1. In gBoost, we employed the gBoost MATLAB toolbox for classification datasets so that the experimen-tal results are easily reproducible. Since the toolbox does not offer regression solvers, we implemented a graph boosting regression al-gorithmbasedonquadraticprogramming. SeeAppendixfor details.
We set minimum support parameter (minsup) to 2 for relatively small datasets (EDKB, CPDB and AIDS1), and to 10% of the number of positives for large datasets (CAS, AIDS2 and AIDS3). Throughout the experiments maximum pattern size (maxpat) is set to 10. We used AMD Opteron 2.2GHz system with at most 8GB memory for all experiments.
GPLS is compared with gBoost in five fold cross validation exper-iments. In gPLS, there are two parameters to tune, namely the num-ber of iterations m and the number of obtained patterns per search k . For each dataset, we exhaustively tried all combinations from m ={ 5 , 10 , 15 , 20 , 25 , 30 , 35 } and k ={ 5 , 10 , 15 In the following, we always report the best test accuracy among all settings. Notice that, for AIDS datasets, the parameter values are http://edkb.fda.gof/databasedoor.html http://potency.berkeley.edu/cpdb.html http://www.chemoinformatics.org/datasets/bursi/ http://dtp.nci.nih.gov/index.html changed as m ={ 10 , 20 , 30 , 40 , 50 } , k ={ 10 , 20 , cope with large-scale data. In gB oost, the regularization param-C ={ 10 , 50 , 100 , 150 , 200 , 1000 } for regression. The number of patterns to add per iteration is set to 50 for CAS and AIDS, and 10 for the other datasets. The accuracy is measured by Q 2 for regres-sion and by the area under the ROC curve (AUC) for classification. The Q 2 score is defined as which is close to 1 when the regression function fits good, and is close to 0 when it does not. The interpretation is similar to that for the Pearson correlation coefficient.
 The results of gPLS and gBoost are compared in Table 2. For EDKB and CPDB datasets, we performed more detailed experi-ments with different settings of maximum pattern size (Figure 3 and 4). In terms of accuracy, it is difficult to decide which method is better. GPLS was better in EDKB, CPDB and AIDS1 but gBoost was better in CAS. However, in terms of computational time, gPLS is clearly superior. In the table, we distinguish the computational time for pattern search (mining time, MT) and the numerical com-putations (numerical time, NT). The numerical time of gBoost was significantly larger than that of gPLS in all datasets, showing that gPLS X  X  computational simplicity contributes to reduce the actual computational load. For large datasets (AIDS2 and AIDS3), gBoost did not finish in a reasonable amount of time.

Figure 5 shows the patterns selected by gPLS from the EDKB dataset. It is often observed that similar patterns are extracted to-gether in the same component. This property makes PLS stable, because the regression function is less affected by small changes in graph data.
The main idea of iterative mining is to gain efficiency by means of adaptive example weights. We evaluated how large the efficiency gain is by comparing gPLS and a naive method that enumerates all patterns first and apply PLS afterwards. Table 3 summarizes the results for different maximum pa ttern sizes (maxpat). In the naive method, the number of patterns grow exponentially, hence the com-putational time for PLS grows rapidly as well. GPLS successfully keeps computational time small in all occasions.
So far, we have mainly focused on gPLS and gBoost. They are similar in that graph patterns are iteratively collected based on the weighted mining criterion. Obviously, they are not the only ones belonging to this family of algorithms. Other boosting methods and PLS-like methods could be applied to graph data in the same fashion.

However, it is important to recognize that there are two distinct classes in boosting algorithms, sequential update and totally correc-tive update. Both classes are based on linear classification function, In sequential update algorithms including AdaBoost and its vari-ants (GradientBoost, MadaBoost etc.), a new feature x i + in one iteration and the corresponding weight w i + 1 is determined. However, previously fixed weights are never updated again. In these methods, numerical computation per iteration is very simple, but it (  X / C  X  )* P MT NT AUC/Q 2 X  ITR takes many iterations to converge. In graph mining, each iteration involves pattern search, so sequential update algorithms are not ef-ficient after all. On the other hand, totally corrective methods, such as gPLS, gBoost and TotalBoost [32], update all weights whenever new features are introduced. It requires more complicated numeri-cal computation but the number of iterations can be by far smaller. substantially helps to reduce the number of iterations further. Graph LARS [29] could be considered as a totally corrective method as it updates all weights. However, since it is based on regularization path tracking, it is not clear how to collect more than one pattern by a mining call.
We presented a novel graph regression method based on partial least squares regression. Experiments showed that gPLS has better efficiency than gBoost. However, gPLS cannot completely replace tle modification in mathematical programming formulation, gBoost can solve various machine learning problems, such as one-class SVM, ranking and the positive/negative unbalanced classification problem. In this paper, we used graph data only, but gPLS can be applied to subclasses of graphs such as trees, sequences and item-sets, simply by replacing graph mining with an appropriate mining algorithm.
 The authors would like to thank Pierre Mah X  for data preparation, Ichigaku Takigawa for figure preparation, and Sebastian Nowozin for preparation of MATLAB toolbox and proof reading. [1] B. Bringmann, A. Zimmermann, L. D. Raedt, and S. Nijssen. [2] J. Chen, S. J. Swamidass, Y. Dou, J. Bruand, and P. Baldi. [3] A. Demiriz, K. Bennet, and J. Shawe-Taylor. Linear [4] M. Deshpande, M. Kuramochi, N. Wale, and G. Karypis. [5] L. Eld X n. Partial least squares vs. lanczos bidiagonalization i: [6] H. Fr X hrich, J. Wegner, F. Sieker, and Z. Zell. Kernel [7] C. Helma, T. Cramer, S. Kramer, and L. Raedt. Data mining [8] A. H X skuldsson. PLS Regression Methods. Journal of [9] A. Inokuchi. Mining generalized substructures from a set of [10] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized [11] J. Kazius, S. Nijssen, J. Kok, and T. B. A. Ijzerman. [12] R. Kohavi and G. H. John. Wrappers for feature subset [13] N. Kr X mer and M. Braun. Kernelizing partial least squares, [14] S. Kramer, L. Raedt, and C. Helma. Molecular feature [15] T. Kudo, E. Maeda, and Y. Matsumoto. An application of [16] P. Mah X , L. Ralaivola, V. Stoven, and J.-P. Vert. The [17] M. Momma and K. Bennett. Constructing orthogonal latent [18] S. Morishita. Computing optimal hypotheses efficiently for [19] S. Morishita and J. Sese. Traversing itemset lattices with [20] S. Nijssen and J. Kok. A quickstart in frequent structure [21] S. Nowozin, G. Bakir, and K. Tsuda. Discriminative [22] S. Nowozin, K. Tsuda, T. Uno, T. Kudo, and G. Bakir. [23] L. Ralaivola, S. Swamidass, H. Saigo, and P. Baldi. Graph [24] G. R X tsch, A. Demiriz, and K. Bennett. Sparse regression [25] R. Rosipal and N. Kr X mer. Overview and recent advances in [26] H. Saigo, T. Kadowaki, and K. Tsuda. A linear programming [27] H. Saigo, T. Uno, and K. Tsuda. Mining complex genotypic [28] A. Sanfeliu and K. Fu. A distance measure between [29] K. Tsuda. Entire regularization paths for graph data. In [30] K. Tsuda and T. Kudo. Clustering graphs by weighted [31] K. Tsuda and K. Kurihara. Graph mining with variational [32] M. Warmuth, J. Liao, and G. R X tsch. Totally corrective [33] H. Wold. Path models with latent variables: The NIPALS [34] S. Wold, M. Sj X st X m, and L. Erikkson. PLS-regression: a [35] X. Yan, H. Cheng, J. Han, and P. S. Yu. Mining significant [36] X. Yan and J. Han. gSpan: graph-based substructure pattern Here we briefly describe the gBoost regression algorithm. Boost-ing methods construct a linear combination of weak hypotheses to come up with a better prediction. In our case, a weak hypothesis corresponding to each subgraph pattern p is described as The regression function is formulated as where  X , b are weight parameters to be learned. The learning prob-lem is written as where C is the regularization parameter to be adjusted. Using the L -norm regularizer (the first term), sparsity is enforced to the pa-rameters.

The problem is rewritten as the following quadratic program. program has | P | variables and 2 n constraints. Directly solving this primal problem is hard due to the large number of variables in Thus, we consider the dual problem: and (16), respectively. Once the dual problem is solved, the primal solution  X  and b are recovered from the Lagrange multipliers of the dual problem. Though the dual problem has too many constraints, it can be efficiently solved by an iterative procedure called the column generation algorithm [3]. First of all, an initial solution of tained from the problem with no cons traints (19). In each iteration, one finds the most violated constraint based on the current value of  X  , and add the found constraint to the quadratic program. In our case, a constraint corresponds to a subgraph pattern, so we need to solve the following search problem, where  X  =  X  +  X   X   X  . This search problem coincides with that of gPLS (12), and can be solved using the same algorithm (Algorithm 4). In each iteration of the algorith m, a dual quadratic program with a limited number of constraints is solved and the obtained solution will be used in the next search. The iteration will be continued until the dual parameter  X  converges.
