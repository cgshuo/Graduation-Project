 The Farwell-Donchin speller [4], also known as the  X  X 300 spe ller, X  is a Brain-Computer Interface which enables users to spell words provided that they can see sufficiently well. This BCI determines the intent of the user by recording and classifying his elect roencephalogram (EEG) in response to controlled stimulus presentations. Figure 1 shows a genera l P300 speller scheme. The stimuli are intensifications of a number of letters which are organized i n a grid and displayed on a screen. In a standard setup, the rows and columns of the grid flash in a rand om order. The intensification of the row or column containing the letter that the user wants to com municate is a target in a stimulus se-quence and induces a different brain response than the inten sification of the other rows and columns (the non-targets). In particular, targets and non-targets are expected to elicit certain event-related potential (ERP) components, such as the so-called P300, to d ifferent extents. By classifying the epochs (i.e. the EEG segments following each stimulus event ) into targets and non-targets, the target row and column can be predicted, resulting in the identificat ion of the letter of interest. The classification process in the speller can be considered a noisy communication channel where the sequence of EEG epochs is a modulated version of a bit stri ng denoting the user X  X  desired letter. Figure 1: Schematic of the visual speller system, illustrat ing the relationship between the spatial pattern of flashes and one possible codebook for letter trans mission (flash rows then columns). These bit strings or codewords form the rows of a binary codeb ook C , a matrix in which a 1 at did not. The standard row-column code, in which exactly one r ow or exactly one column flashes at any one time, will be denoted RC . It is illustrated in figure 1.
 signal-to-noise ratio of the ERPs hampers accurate classifi cation of the epochs, so the output bit string may differ from the transmitted bit string (decoding error). Also, the transmitted string may differ from the corresponding row in the codebook due to modu lation error, for example if the user lost his attention and missed a stimulus event. Coding theor y tells us that we can detect and correct transmission and decoding errors by adding redundancy to th e transmitted bit string. The Hamming distance d is the number of bit positions that differ between two rows in a codebook. The minimum Hamming distance d min of all pairs of codewords is related to the error correcting a bilities of the code by e = ( d min  X  1) / 2 , where e is the maximum number of errors that a code can guarantee to correct [9]. In general, we find the mean Hamming distance wit hin a given codebook to be a rough predictor of that codebook X  X  performance.
 In the standard approach, redundancy is added by repeating t he flashing of all rows and columns R times. This leads to d = 4 R between two letters not in the same row or column and d min = 2 R between two letters in the same row or column. The RC code is a poor code in terms of minimum Hamming distance: to encode 36 different letters in 12 bits, d min = 4 is possible, and the achievable d min increases supra-linearly with the total code length L (for example, d min = 10 is possible in L = 24 bits, the time taken for R = 2 repeats of the RC code).
 However, the codes with a larger d min are characterized by an increased weight compared to the RC code, i.e. the number of 1 X  X  per bitstring is larger. As targe t stimulus events occur more frequently overall, the expected target-to-target interval (TTI) dec reases. One cannot approach codebook op-timization, therefore, without asking what effect this mig ht have on the signals we are trying to measure and classify, namely the ERPs in response to the stim ulus events.
 The speller was originally derived from an  X  X ddball X  paradi gm, in which subjects are presented with a repetitive sequence of events, some of which are targets re quiring a different response from the (more frequent) non-targets. The targets are expected to ev oke a larger P300 than the non-targets. It was generally accepted that the amplitude of the target P3 00 decreases when the percentage of targets increases [3, 11]. However, more recently, it was su ggested that the observed tendency of the P300 amplitude (as measured by averaging over many targe ts) to decrease with increased target probability may in fact be attributed to greater prevalence of shorter target-to-target intervals (TTI) [6] rather than an overall effect of target frequency per se. In a different type of paradigm using only targets, it was shown that at TTIs smaller than about 1 second , the P300 amplitude is significantly decreased due to refractory effects [15]. Typical stimulus onset asynchronies (SOAs) in the oddball paradigm are in the order of seconds since the P300 component shows up somewhere between 200 and 800 msec[12]. In spellers, small SOAs of about 100 msec ar e often used [8, 13] in order to achieve high information transfer rates. Consequently, on e can expect a significant ERP overlap into the epoch following a target epoch, and since row flashes are often randomly mixed in with column flashes, different targets may experience very diffe rent TTIs. For a 6  X  6 grid, the TTI ranges from 1  X  SOA to 20  X  SOA, so targets may suffer to varying degrees from any refrac tory and overlap effects.
 In order to quantify the detrimental effects of short TTI we e xamined data from the two subjects in dataset IIa+b from the BCI Competition III[2]. Following th e classification procedures described in section 3.3, we estimated classification performance on the individual epochs of both data sets by 10-fold cross-validation within each subject X  X  data set. Bina ry (target versus non-target) classification results were separated according to the time since the previ ous target (TPT) X  X or the targets this distance measure is equivalent to the TTI. The left panel of fi g 4 shows the average classification error as a function of TPT (averaged across both subjects X  X ot h subjects show the same qualitative effect). Evidently, the target epochs with a TPT &lt; 0 . 5 sec display a classification accuracy that approximates chance performance. Consequently, the targe t epochs with TPT &lt; 0 . 5 sec, constituting about 20% of all target epochs in a RC code, do not appear to be useful for transmission [10]. Clearly, there is a potential conflict between information-theoretic factors, which favour increasing the minimum Hamming distance and hence the overall proporti on of target stimuli, and the detri-mental psychophysiological effects of doing so.
 In [7] we explored this trade-off to see whether an optimal co mpromise could be found. We initially built a generative model of the BCI system, using the competi tion data illustrated in figure 4, and then used this model to guide the generation and selection of speller code books. The results were not unequivocally successful: though we were able to show ef fects of both TTIs and of the Hamming distances in our codebooks, our optimized codebook perform ed no better than the row-column code for the standard flash stimulus. However, our series of exper iments involved another kind of stim-ulus, and the effect of our codebook manipulation was found t o interact with the kind of stimulus used.
 The purpose of the current paper is two-fold: 2.1 Probabilistic Approach to Classification and Decoding We assume an N -letter alphabet  X  and an N -letter by L -bit codebook C . The basic demodulation and decoding procedure consists of finding the letter  X  T among the possible letters t  X   X  showing the largest probability Pr ( t | X ) of being the target letter T , given C and the measured brain signals X = [ x 1 , . . . , x L ] , i.e., where the second equality follows from Bayes X  rule. A simple approach to decoding is to treat the individual binary epochs, with binary labels c = ( C factor Pr ( X | t ) into per-epoch probabilities Pr ( x where the second equality again follows from Bayes X  rule.
 This form of Bayesian decoding [5] forms the basis for our dec oding scheme. We train a probabilistic estimate Pr ( C that a particular letter t corresponds to the user-selected codeword. Note that for de coding purposes the terms Pr ( X ) and Pr ( x Q j Pr ( C tj ) easy to show that during decoding this term cancels out the ef fect of the binary prior, which may therefore be set arbitrarily without affecting the decisio ns made by our decoder. The simplest thing t . 2.1.1 Codebook Optimization We used a simple model of subjects X  responses in each epoch in order to estimate the probability of making a prediction error with the above decoding method. We used it to compute the codebook loss , which is the sum of error probabilities, weighted by the pro bability of transmission of each letter. This loss function was then minimized in order to obt ain an optimized codebook. Note that this approach is not a direct attempt to tackle the t endency for the performance of the binary target-vs-nontarget classifier to deteriorate when TTI is short (although this would surely be a promising alternative strategy). Instead, we take a  X  X orm al X  classifier, as susceptible to short-TTI effects as classifiers in any other study, but try to estimate the negative impact of such effects, and then find the best trade-off between avoiding short TTIs on th e one hand, and having large Hamming distances on the other hand.
 Since our optimization did not result in a decisive gain in pe rformance, we do not wish to emphasize the details of the optimization methods here. However, for f urther details see the supplementary material, or our tech report [7]. For the purposes of the curr ent paper it is the properties of the resulting codebooks that are important, rather than the pre cise criterion according to which they are considered theoretically optimal. The codebooks themselv es are described in section 3.1 and given in full in the supplementary material. We implemented a Farwell/Donchin-style speller, using a 6  X  6 grid of alphanumeric characters, presented via an LCD monitor on a desk in a quiet office. Subjec ts each performed a single 3-hour session during which their EEG signals were measured using a QuickAmp system (BrainProducts GmbH) in combination with an Electro-Cap. The equipment was set up to measure 58 channels of EEG, one horizontal EOG at the left eye, one bipolar vertical EOG signal, and a synchronization signal from a light sensor attached to the display, all sampl ed at 250 Hz. We present results from 6 healthy subjects in their 20s and 30s (5 male, 1 female).
 Two factors were compared in a fully within-subject design: codebook and stimulus. These are described in the next two subsections. 3.1 Codebook Comparison In total, we explored 5 different stimulus codes: Table 1: Summary statistics for the 24-bit versions of the 5 c odebooks used. E(# 11 ) means the average number of consecutive target letters per codeword, and Pr ( 1 ) the proportion of targets. L is our estimated probability of an error, according to the mo del (see supplementary material or [7]). 3.2 Stimulus Comparison Two stimulus conditions were compared. In both conditions, stimulus events were repeated with a stimulus onset asynchrony (SOA) of 167 msec, which as close as our hardware could come to recreating the 175-msec SOA of competition III dataset II.
 Flashes : grey letters presented on a black background were flashed in a conventional manner, being intensified to white for 33 msec (two video frames). An exampl e is illustrated in the inset of the left panel of figure 2.
 Flips : each letter was superimposed on a small grey rectangle whos e initial orientation was either horizontal or vertical (randomly determined for each lette r). Instead of the letter flashing, the rect-angle flipped its orientation instantaneously by 90  X  . An example is illustrated in the inset of the right panel of figure 2. Our previous experiments had led us to conclude that many subjects perform significantly better with this stimulus, and find it more plea sant, than the flash. As we shall see, our results from this stimulus condition support this finding, a nd indicate a potentially useful interaction between stimulus type and codebook design. 3.3 Experimental Procedure The experiment was divided into blocks, each block containi ng 20 trials with short (2 X 4 second) rest pauses between trials. Each trial began with a red box wh ich indicated to the subject which letter (randomly chosen on each trial) they should attend to  X  X his cue came on for a second, and was removed 1 second before the start of the stimulus sequence. S ubjects were instructed to count the stimulus events at the target location, and not to blink, mov e or swallow during the sequence. The sequence consisted of L = 72 stimulus events, their spatio-temporal arrangement being determined by one of the five code conditions. The 12-bit RC codes were repeated six times in order to make the length up to L = 72 (re-randomizing the row and column order on each repetition ) and the 24-bit optimized codes were repeated three times (reassigning the codewords between repetitions to ensure maximal gap between targets at the end of one repetition and t he beginning of the next) likewise to ensure a total code length of 72 bits.
 Each of the 5 code conditions occurred 4 times per block, the o rder of their occurrence being ran-domized. For a given block, the stimulus condition was held c onstant, but the stimulus type was alternated between blocks. In total, each subject performe d 16 blocks. Thus, in each of the 10 stimulus  X  code conditions, there were a total of 32 letter presentatio ns or 2304 stimulus events. 3.3.1 Online Verification Subjects did not receive feedback at the end of each trial. Ho wever, at the end of the experiment, we gave the subject the opportunity to perform free-spellin g in order to validate the system X  X  perfor-mance: we asked each subject whether they would prefer to spe ll with flips or flashes, and loaded a classifier trained on all data from their preferred stimulu s type into the system. Using the 72-bit codebooks, all subjects were able to spell 5-15 letters with online performance ranging from 90 to 100%. Our data analysis below is restricted to leave-one-le tter-out offline performance, excluding the free-spelled letters. 3.4 Data Analysis The 60-channel data, sampled at 250 Hz, were band-pass filter ed between 0.1 and 8 Hz using a FIR filter. The data were then cut into 600-msec (150-sample) epochs time-locked to the stimulus events, and these were downsampled to 25 Hz. The data were the n whitened in 60-dimensional sensor space (by applying a symmetric spatial filtering matr ix equal to the matrix-square-root of the data covariance matrix, computed across all training trial s and time-samples). Finally a linear LR classifier was applied [1, pp82-85]. The classifier X  X  regula rization hyperparameter C was found by 10-fold cross-validation within the training set..
 given code condition, each of the 32 letters was considered i n turn, and a probabilistic prediction was made of its binary epoch labels using the above procedure trained only on epochs from the other 31 letters. These probabilities were combined using the dec oding scheme described in section 2.1 and a prediction was made of the transmitted letter. We varie d the number of consecutive epochs of the test letter that the decoder was allowed to use, from the m inimum (12 or 24) up to the maximum 72. For each epoch of the left-out letter, we also recorded wh ether the binary classifier correctly classified the epoch as a target or non-target. Estimates of 36-class letter prediction performance are sh own in figures 2 (averaged across subjects, as a function of codeword length) and 3 (for each individual s ubject, presenting only the results for 24-bit codewords). The performance of the binary classi fier on individual epochs is shown in figure 4. Figure 2: Offline (leave-one-letter-out) 36-class predict ion performance as a function of codeword length (i.e. the number of consecutive epochs of the left-ou t letter that were used to make a predic-tion). Performance values (and standard-error bar heights ) are averaged across the 6 subjects. Our results indicated the following effects: Figure 3: Offline (leave-one-letter-out) 36-class predict ion performance when decoding codewords of length 24, for each of the subjects in each of the code condi tions. Figure 4: Illustration of effect of TPT on epoch classificati on performance, (left) in the data from competition III dataset II; (middle) in our experiments, av eraged across all subjects and code condi-tions for blocks in which the flash stimulus was used; (right) in our experiments, averaged across the same subjects and code conditions, but for blocks in which th e flip stimulus was used. The rightmost column of each plot shows average classification accuracy ac ross all epochs (remember that short TTIs are relatively uncommon overall, and therefore downwe ighted in the average). In summary, we have obtained empirical support for the idea t hat TTI (finding #1), Hamming dis-tance (finding #4) and stimulus type (finding #3) can all be man ipulated to improve performance. However, our initial attempt to find an optimal solution by ba lancing these effects was not successful (finding #2). In the flash stimulus condition, the row-column codes performed better than expected, matching the performance of our optimized code. In the flip st imulus condition, TTI effects were greatly reduced, making either D8 opt or D10 suitable despite the short TTIs of the latter. It seems very likely that the unexpectedly high performance of RC sep and RC mix can be at least partly explained by the idea that they have particular spatial properties that enhance their performance beyond what Hamming distances and TTIs alone would predict. This hypothesis is corroborated by finding #5. Models of such spatial effects should clearly be t aken into account in future optimization approaches.
 correcting codes, D8 opt or D10 : this consistently outperforms the traditional row-colum n flash design and shows that error-correcting code design has an importan t role to play in BCI speller develop-ment.
 As a final note, one should remember that a language model can b e used to improve performance in speller systems. In this case, the codebook optimization pr oblem becomes more complicated than the simplified setting we examined, because the prior Pr ( t ) in (2) is no longer flat. The nature of the best codes, according to our optimization criterion, mi ght change considerably: for example, a small subset of codewords, representing the most probable l etters, might be chosen to be particularly sparse and/or to have a particularly large Hamming distance between them and between the rest of the codebook, while within the rest of the codebook these two criteria might be considered relatively unimportant. Ideally, the language model would be adaptive (for example, supplying a predictive prior for each letter based on the previous three) which migh t mean that the codewords should be reassigned optimally after each letter. However, such cons iderations must remain beyond the scope of our study until we can either overcome the TTI-independen t performance differences between codes (perhaps, as our results suggest, by careful stimulus design), or until we can model the source of these differences well enough to account for them in our op timization criterion.

