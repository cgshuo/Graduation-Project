 The competitive business climate and the complexity of IT environments dictate efficient and cost-effective service de-livery and support of IT services. These are largely achieved by automating routine maintenance procedures, including problem detection, determination and resolution. System monitoring provides an effective and reliable means for prob-lem detection. Coupled with automated ticket creation, it ensures that a degradation of the vital signs, defined by ac-ceptable thresholds or monitoring conditions, is flagged as a problem candidate and sent to supporting personnel as an incident ticket. This paper describes an integrated frame-work for minimizing false positive tickets and maximizing the monitoring coverage for system faults.

In particular, the integrated framework defines monitor-ing conditions and the optimal corresponding delay times based on an off-line analysis of historical alerts and incident tickets. Potential monitoring conditions are built on a set of predictive rules which are automatically generated by a rule-based learning algorithm with coverage, confidence and rule complexity criteria. These conditions and delay times are propagated as configurations into run-time monitoring systems. Moreover, a part of misconfigured monitoring con-ditions can be corrected according to false negative tickets that are discovered by another text classification algorithm in this framework. This paper also provides implementation details of a program product that uses this framework and shows some illustrative examples of successful results. H.2.8 [ Database applications ]: Data mining Ticket analysis, System monitoring
IT service providers are facing an increasingly intense com-petitive landscape and growing industry requirements. In their quest to maximize customer satisfaction, Service Provider-s seek to employ business intelligence solutions, which pro-vide deep analysis, orchestration of business processes and capabilities for optimizing the level of service and cost. IT Infrastructure Library (ITIL) addresses monitoring as a con-tinual cycle of monitoring, reporting and subsequent action that provides measurement and control of services [4].
Modern forms of distributed computing (e.g., cloud) pro-vided some standardization of the initial configuration of the hardware and software. In order to enable most enterprise level applications, however, an individual infrastructure for the given application must be created and maintained on behalf of each outsourcing customer. This requirement cre-ates great variability in the services provided by IT support teams. The aforementioned issues largely contribute to the fact that routine maintenance of the information systems remains semi-automated and manually performed. System monitoring is an automated reactive system that provides an effective and reliable means of ensuring that degradation of the vital signs, defined by acceptable thresholds or monitor-ing conditions (situations), is flagged as a problem candidate (monitoring event) and sent to the service delivery teams as an incident ticket.

Defining monitoring conditions (situations) requires the knowledge of a particular system and its relationships with other hardware and software systems. It is a known practice to define conservative conditions in nature, thus erring on the side of caution. This practice leads to a large number of tickets that require no action (false positives). Continu-ous updating of modern IT infrastructures also leads to a number of system faults that are not captured by system monitoring (false negatives). To minimize the number of false positives and false negatives, this paper presents an integrated framework for optimizing automatic monitoring systems in large and dynamic IT infrastructures. It utilizes learning based approaches on historical monitoring events and incident tickets to help system administrators improve monitoring condition definitions. This framework is imple-mented as an event and ticket analysis system , which be-comes part of the IBM IT service management platform. The system is deployed and maintained at several IBM ser-vice centers. The end-users are the system administrators who are working with IBM Tivoli monitoring [2]. The ana-ly zed results of the system have also been deployed at several IBM customer accounts. A customer account stands for an enterprise IT infrastructure which is often constructed by over one thousand machines. The monitoring quality was improved significantly after deployment for several months.
This paper describes an integrated framework for optimiz-ing automatic monitoring systems in large and dynamic IT infrastructures. The innovations implemented through this framework include:
The rest of the paper is organized as follows: Section 2 provides a description of the problem settings and introduces the main notations used in the paper. Section 3 presents our developed framework and system for minimizing false posi-tive and false negative alerts. In Section 4, we present our empirical studies on IBM Tivoli monitoring systems with real IT incident tickets. Section 5 summarizes the related work for text classification and system event mining for sys-tem management. sSection 6 offers our paper X  X  conclusion. Fi gure 1: Problem Detection, Determination and Resolution
The typical workflow of problem detection, determination and resolution for the IT service provider is prescribed by the ITIL specification [4], and illustrated in Figure 1. De-tection is usually provided by monitoring software running on the servers of an account, which computes metrics for the hardware and software performance at regular interval-s. The metrics are then compared to acceptable thresholds, known as monitoring situations , and any violation results in an alert. If the alert persists beyond a certain delay spec-ified in the situation, the monitor emits an event. Events coming from an account X  X  entire IT environment are con-solidated in an enterprise console. The console uses rule-, case-or knowledge-based engines to analyze the monitoring events and decide whether to open a service ticket in the In-cident, Problem, Change (IPC) system. Additional tickets are created upon customer request. The information accu-mulated in the ticket is used by the System Administrators (SAs) for problem determination and resolution. As part of the service contracts between the customer and the service provider, the SLA (Service Level Agreement) specifies the maximum resolution times for various categories of tickets.
Performing a detailed analysis of IT system usage is time-consuming, so SAs often rely on default monitoring situa-tions. Furthermore, IT system usage is likely to change over time. This often results in a large number of alerts and tickets, which can be categorized in Table 1.
 Table 1: De nitions for Alert, Event and Ticket
Wh ether a ticket is real or false is determined by the res-olution message entered in the ticket tracking database by the system administrator it was assigned to. It is not rare to observe entire categories of alerts, such as CPU or pag-ing utilization alerts, that are almost exclusively false posi-tives. When reading the resolution messages one by one, it can be simple to find an explanation: Anti-virus processes cause prolonged CPU spikes at regular intervals; databases may reserve large amount of disk space in advance, mak-ing the monitors believe the system is running out of stor-age. With only slightly more effort, one can also fine-tune the thresholds of certain numerical monitored metrics, such as the metrics involved in paging utilization measurement. There are rarely enough human resources, however, to cor-rect the monitoring situations one system at a time, and we need an algorithm capable of discovering these usage-specific rules. There has been a great deal of effort spent on develop-ing the monitoring conditions (situations) that can identify potentially unsafe functioning of the system [14] [24]. It is understandably difficult, however, to recognize and quantify influential factors in the malfunctioning of a complex sys-tem. Therefore classical monitoring tends to rely on period-ical probing of a system for conditions that could potential-ly contribute to the system X  X  misbehavior. Upon detection of the predefined conditions, the monitoring systems trig-real? (2) If an alert is identified as false, what waiting time should be applied before ticket creation?
In our approach, the predictor is implemented by a rule-based classifier based on the historical tickets and events. The ground truth of the events is obtained from the asso-ciated tickets. Each historical ticket has one column that suggests this ticket is real or false. This column is manually filled by the system administrators and stored in the ticket-ing system. There are two reasons for choosing a rule-based predictor. First, each monitoring situation is equivalent to a quantitative rule. The predictor can be directly implement-ed in the existing monitoring system. Other sophisticated classification algorithms, such as support vector machine and neural network , may have a higher precision in predicting. Their classifiers, however, are very difficult to implement as monitoring situations in real systems. Second, a rule-based predictor is easily verifiable by the end users. Other compli-cated classification models represented by linear/non-linear equations or neural networks are very hard for end users to verify. If the analyzed results could not be verified by the system administrators, they would not be utilized in real production servers.
 Predictive Rule The alert predictor roughly assigns a label to each alert,  X  X alse X  X r X  X eal. X  X t is built on a set of predictive rules that are automatically generated by a rule-based learning algorithm [27] based on historical events and alert tickets. Example 1 shows a predictive rule, where  X  PROC C PU TI ME  X  is the CPU usage of a process. Here  X  PROC N AME  X  is the name of the process.

Example 1. if PROC CP U TI ME &gt; 50% and PROC N AME
A predictive rule consists of a rule condition and an alert label. A rule condition is a conjunction of literals , where each literal is composed of an event attribute, a relational opera-tor and a constant value. In Example 1, X  PROC CP U TI ME &gt; 50%  X  and  X  PROC N AME = `Rtvscan'  X  are two literals , where  X  PROC CPU TI ME  X  and  X  PROC NA ME  X  are event attributes,  X  &gt;  X  and  X = X  are relational operators, and  X 50% X  and  X  Rtvscan  X  are constant values. If an alert event satisfies a rule condition, we call this alert covered by this rule. Since we only need predictive rules for false alerts, the alert label in our case is always  X  X alse. X  Predictive Rule Generation The rule-based learning algorithm [27] first creates all liter-als by scanning historical events. Then, it applies a breadth-first search for enumerating all literals in finding predictive rules, i.e., those rules having predictive power. This algo-rithm has two criteria to quantify the minimum predictive power: the minimum confidence minconf and the minimum support minsup [27]. In our case, minconf is the minimum ratio of the numbers of the covered false alerts and all alerts covered by the rule, and minsup is the minimum ratio of the number of alerts covered by the rule and the total number of alerts. The two criteria govern the performance of our method, defined as the total number of removed false alerts. To achieve the best performance, we loop through the values of minconf and minsup and compute their performances. Predictive Rule Selection
Although the predictive rule learning algorithm can learn many rules from the historical events with tickets, we on-ly select those with strong predictive power. In our solu-tion, Laplace accuracy [36] [20] [17] is used for estimating the predictive power of a rule. According to the SLA, real tickets must be acknowledged and resolved within a certain time. The maximum allowed delay time is specified by a user-oriented parameter delay max for each rule. In the cal-culation of Laplace accuracy, those false alerts are treated as real alerts if their durations are greater than delay max delay max is given by the system administrators according to the severity of system incidents and the SLA.

Another issue is rule redundancy. For example, let us consider the two predictive rules: Clearly, if an alert satisfies Rule Y, then it must satisfy Rule X as well. In other words, Rule Y is more specific than Rule X. If Rule Y has a lower accuracy than Rule X, then Rule Y is redundant given Rule X (but Rule X is not redundant given Rule Y). In our solution, we perform redundant rule pruning to discard the more specific rules with lower accu-racies. The detailed algorithm is described in [31]. Calculating Waiting Time for Each Rule Waiting time is the duration by which tickets should be post-poned if their corresponding alerts are classified as false. It is not unique for all monitoring situations. Since an alert can be covered by multiple predictive rules, we set up dif-ferent waiting times for each of them. The waiting time can be transformed into two parameters in monitoring systems, the length of the polling interval with the minimum polling count [3]. For example, the situation described in Example 1 predicts false alerts about CPU utilization of  X  X tvscan. X  We can also find another predictive rule as follows: The job of  X  X erl X , however, is different from that of  X  X tvscan. X  Their durations are not the same, and the waiting time will differ accordingly. In order to remove as many false alerts as possible , we set the waiting time of a selected rule as the lo ngest duration of the transient alerts covered by it. For a selected predictive rule p , its waiting time is where F p = f e j e 2F ; isCovered ( p; e ) =  X  true  X  g , and the set of transient events. Clearly, for any rule p 2P , wait has a upper bound, wait p delay max . Therefore, no ticket can be postponed for more than delay max .
False negative alerts are the missing alerts that are not captured by the monitoring system due to some miscon-figuration. Real-world IT infrastructures are often over-monitored. False negative alerts are much fewer than false positive alerts. Since the number of false negative alerts is quite small, we only focus on the methodologies for discov-ering them with their corresponding monitoring situations. The system administrators can easily correct the misconfigu-ration by referring the results. The false negative tickets are recorded by the system administrators in the manual tick-ets. Each manual ticket consists of several textual messages that describe the detailed problem. In addition to system fault issues, manual tickets also track many other customer requests such as asking for resetting database passwords, in-stalling a new web server and so on. The customer request is the majority of the manual tickets. In our system the work for false negative alerts is to find out those monitor-ing related tickets among all manual tickets. This problem is formed as a binary text classification problem. Given an incident ticket, our method classifies it into  X 1 X  or  X 0 X , where  X 1 X  indicates this ticket is a false negative ticket, otherwise it is not. For each monitoring situation, we build a binary text classifier.

There are two challenges for building the classification model. First, the manual ticket data is highly imbalanced since most of the manual tickets are customer requests and only very few are false negative tickets. Figure 6 shows var-ious system situation issues in two manual ticket sets. This manual ticket set is collected from a large customer account in IBM IT service centers. The first month has 9854 man-ual tickets and the second month has 10109 manual tickets overall. As shown in this figure, only about 1% manual tick-ets are false negatives. Second, labeled data is very limited. Most system administrators are only working on some parts of incident tickets. Only a few experts can label all tickets. Selective Ticket Labeling It is time-consuming for human experts to scan all manual tickets and label their classes for training. In our approach, we only select a small proportion of tickets for labeling. A naive method is randomly selecting a subset of the manual tickets as the training data. However, the selection is crucial to the highly imbalanced data. Since the monitoring related tickets are very rare, the randomly selected training data would probably not contain any monitoring related ticket. As a result, the classification model cannot be trained well. On the other hand, we do not know which ticket is related to monitoring or not before we obtain the tickets X  class labels. To solve this problem, we utilize domain words in system management for the training ticket selection. The domain words are some proper nouns or verbs that indicate the scope of the system issues. For example, everyone uses  X  X B2 X  to indicate the concept of IBM DB2 database. If a ticket is about the DB2 issue, it must contain the word X  X B2 X .  X  X B2 X  is a domain word. There are not many variabilities for the concepts described by the domain words. Therefore, those domain words is helpful to reduce the ticket candidates for labeling. Table 2 lists examples of the domain words with their corresponding situations. The domain words can be obtained from the experts or related documents.

I n the training ticket selection, we first compute the rele-vance score of each manual ticket and ranks all the tickets based on the score, and then select the top k tickets in the ranked list, where k is a predefined parameter. Given a ticket T , the relevance score is computed as follows: where w ( T ) is the word set of ticket T , l is the number of predefined situations, M i is the given domain word set for the i -th situation, i = 1 ; :::; l . Intuitively, the score is the largest number of the common words between the ticket and the domain words.

In the evaluation section of this paper, we consider a base-line method that only uses the domain words to identify sit-uation tickets by simple word-matching. In dual supervision learning[26], the domain words are seen as the labeled fea-tures , which can also be used in active learning for selecting unlabeled data instances. But in our application, we have only the positive features but no negative features, and the data is highly imbalanced. Therefore, the uncertainty-based approach and the density-based approach in active learning are not appropriate for our system.
 Classi cation Model Building The situation ticket is identified by applying a SVM clas-sification model [28] on the ticket texts. For training this model, we have two types of input data: 1) the selectively labeled tickets, and 2) the domain words. To utilize the do-main words, we treat each domain word as a pseudo-ticket and put all pseudo-tickets into the training ticket set. To deal with the imbalanced data, the minority class tickets are over-sampled until the number of positive tickets is equal to the number of the negative tickets [9]. Figure 7 shows the flow chart for building the SVM classification model. This section presents empirical studies for our system. The system and the analysis results have been deployed for several customer accounts of IBM IT service. The empiri-cal studies have two types of evaluation. The first type of evaluation is on the collected historical data to validate the performance of the algorithms. The second on is on the pro-duction severs of IBM customers to validate the effectiveness on real IT infrastructures.
Our system is developed by Java 1.6. This testing ma-chine is Windows XP with Intel Core 2 Duo CPU 2.4GHz and 3GB of RAM. Experimental monitoring events and tick-et s are collected from production servers of the IBM Tivoli Monitoring system [2], summarized in Table 3. The data set of each account covers a period of 3 months. jDj is the num-ber of events that generated tickets in the ticketing systems. N non is the number of false events in all ticketed events. # Attributes is the total number of attributes of all events. # Situations is the number of monitoring situations. # N-odes is the number of monitored servers. In addition to the auto-generated tickets, we also collect manual tickets from two months. The first month has 9584 manual tickets. The second month has 10109 manual tickets. Performance Measure There are two performance measures: To achieve a better performance, a system should have a larger F P with a smaller F D . We split each data set into the training part and the testing part.  X  X esting Data Ratio X  is the fraction of the testing part in the data set, and the rest is the training part. For example,  X  X esting Data Ratio=0.9 X  means that 90% of the data is used for testing and 10% is used for training. All F P and F D are only evaluated for the testing part.
 Overall Performance Based on the experience of the system administrators, we set delay max = 360 minutes for all monitoring situation-s. Figure 8 presents the experimental results. Our method eliminates more than 75% of the false alerts and only post-pones less than 3% of the real tickets.
 Comparing with Revalidate Since most alert detection methods cannot guarantee no false negatives, we only compare our method with the idea mentioned in [8], Revalidate , which revalidates the status of events and postpones all tickets. Revalidate has only one parameter, the postponement time, which is the maximum allowed delay time delay max . Figure 8c compares the re-spective performance of our method and Revalidate , where each point corresponds to a different test data ratio. While Revalidate is clearly better in terms of elimination of false alerts, it postpones all real tickets, the postponement vol-ume being 1000 to 10000 times larger than our method. Predictive Rules Tables 4 lists several discovered predictive rules for false alerts, where wait p is the delay time for a rule, F P p is the number of false alerts eliminated by a rule in the testing data, and F D p is the number of real tickets postponed by a rule in the testing data.
The effectiveness is evaluated by the accuracy of the sit-uation discovery. The accuracy is measured by precision, recall and F1score, which are the standard accuracy metrics in classification problems [25]. We use one month X  X  tickets as the training data, and the other month X  X  tickets as the testing data. We first test the accuracy of the word-match method. The words are predefined in Table 6.

Figure 9 shows the tested F1 scores [28] of four monitor-ing situations about file system space issue, disk space issue, service availability and router/swith issues. Our method is denoted as  X  X elective X , the second baseline method is denot-ed as  X  X andom X . The  X  X andom X  method randomly selects a subset of manual tickets as the training data for build-ing the SVM model. The domain words for our  X  X elective X  method are shown in Table 6. As shown by those figures, the  X  X andom X  method can only achieve the same accuracy of our method when the number of training tickets is large (above 5000). This is because the real situation tickets are in the minority of the training data set. The training tickets in  X  X andom X  cannot capture real situation tickets unless the training data is large. If the training data is large, however, labeling would be time-consuming for humans.
Our developed system and analytic results have been de-ployed into several customer accounts of IBM IT services. We track the changes on those customer accounts after the deployment. Figure 10 shows the deployed results of one account in three months. Account1 is the account that pro-vides the historical data in the previous evaluation of this paper. This customer account is a large financial company in the United States. Its production servers are used main-ly to support financial investments. The deployment of our work is a step-by-step approach. In the first month, the deployment is only on a small group of testing and devel-(c ) Service not available Fig ure 10: Ticket Volume Changes on Account1
Fi gure 11: Event Volume Changes on Account2 opment machines. Then it spreads to a wide area of its IT environment. Hence, in Figure 10, the effect of our work gradually appears in three months. Although this compa-ny X  X  IT infrastructure changes every day, compared to the changes of real tickets, the reduction for the false tickets is still obvious. The total number of false positive tickets has been reduced by 21%.

Figure 11 shows the evaluation results for another cus-tomer account of IBM IT services. They compare the num-ber of false alerts before deployment and after deployment. Before the deployment, this account has many inappropri-ate CPU and networking monitoring situations, which pro-duce a large number of false alerts every day. By adjusting those monitoring situations according to our analysis report-s, more than 30% of the false alerts are eliminated.
Table 5 shows a sample list of discovered false negative tickets with their monitoring situations on Account1. For privacy issues, the administrators X  names and the server names are replaced by X  X xx X . Most of the false negative tick-ets are caused by some new servers or new databases that are not added into the configuration of monitoring system-s. When the new servers and new databases incur system faults or issues, only the database administrators or storage administrators discover them and create the manual tickets. The false negative tickets are quite few in real production servers, so there is no obvious impact on the volume change after the deployment.
This section reviews prior research studies related to our work. System monitoring has become a significant research area of the IT industry in the past few years. Commercial products such as IBM Tivoli [2], HP OpenView [1] and S-plunk [5] provide system monitoring. Numerous studies [15] [6] [19] [34] [10] [23] focus on monitoring that is critical for a distributed network. A number of studies focused on the analysis of historical events with the goal of improving the understanding of system behaviors. A significant amount of work was done on analysis of system log files and monitoring events. Another area of interest is the identification of ac-tionable patterns of events and misses, or false negatives, by the monitoring system. False negatives are indications of a problem in the monitoring software configuration, wherein a faulty state of the system does not cause monitoring alerts.
Net work monitoring is used to check the  X  X ealth X  of com-munications by inspecting data transmission flow, sniffing data packets, analyzing bandwidth, etc. [15] [6] [19] [34] [10] [23]. It is able to detect node failures, network intrusions, or other abnormal situations in the distributed system. The main difference between the network monitoring and frame-work we consider is the monitored target, which can be any component or subsystem of the system, hardware (such as CPU, hard disk) or software (such as a database engine, or web server). Only the system administrators, who are work-ing the monitored server, can determine whether an alert is real or false. This is why we incorporate ticket resolutions, which record how system administrators resolve those alerts using our solution.

A significant amount of work in data mining has been done to identify actionable patterns of events. See example, [13], [21], [16], [32]. Different types of patterns, such as (partial-ly) periodic patterns, event bursts, and mutually dependent patterns were introduced to describe system management events. Efficient algorithms were developed to find and in-terpret such patterns. Our work is based on the part of an event processing workflow that takes into account the hu-man processing of the tickets. This allowed us to identify non-actionable patterns and misses of the monitoring system configuration with significant precision. In the event pro-cessing workflow, false positive events are transformed into false positive tickets. Identification of false positive events makes it possible to significantly reduce the number of false positive tickets. The translation of the actionable patterns to enterprise software rules is considered in [11] and [22]. We implemented our findings as a component prototype for En-terprise Console. The framework obtains information about user preferences and SLA, mines events and suggests moni-toring conditions as well as duration parameters. A new set of rules is advised for mined false negatives.

Dealing with false negatives, or misses of the system, usu-ally includes the consideration of additional source of data. In our case, this additional source is ticketing data. As a source of information, it is difficult data to process, because there are no supporting standards or structure, and ticket-ing records are usually byproducts of the SA work, which are mainly incomplete and unfinished. An additional diffi-culty is that false negatives are rare and unbalanced due to the fact that historically tested and tuned configurations of the monitoring systems are used. Methods of dealing with unbalanced data was considered for example in [9].
To process the ticketing and logged data, specialized parser-s were created to parse and transform applications and infor-mation system operation logs. Usually, logs and tickets are semi-structured, containing both structured (e.g., log entry prefixes and timestamp) and unstructured text (e.g., excep-tion, error or warning descriptions, and display of applica-tion state). The parsers transform them into relational or extensible (XML like semi-structured) formats and can oper-ate in an offline (like [29], [30], [7], [35] ) or online, streaming regime (like [12] ). In our work, parsers are used to translate monitoring events and ticketing data into attribute-value pairs convenient for further analysis. Unlike existing work, however, we also include the analysis of ticket resolution descriptions for identifying real tickets where a non-trivial amount of work has been done. Such information was used to tag monitoring events as real alerts or false alerts.
Parameter tuning in log pattern mining is studied in [15], [6]. Usually mining parameters describe how strongly ele-ments of the pattern are interconnected or correlated (e.g., confidence), and what percentage of the data stream should be covered (e.g., support). Tuning up parameters is a del-icate process. Parameters that we tune are the percentage of false alerts covered and the number of events covered.
Discovering time-related patterns from system logs is con-sidered in [18], [33]. In our study, the duration time of a pattern depends on a couple of factors such as actual delay time and acceptable SLA thresholds. While the distribution of recognized non-actionable patterns depends only on his-torical data, we take the delay tolerance of a customer as additional input.
This paper presents an integrated framework to optimize the automatic system monitoring in large IT infrastructures. By combing the system event data and ticket data from IT service centers, this framework reduces the number of false positive (non-actionable) alerts and the number of false negative (missing) alerts for the automatic monitoring sys-tem. It minimizes the cost of providing effective and reliable mea ns for problem detection. The integrated framework has been implemented as a system in the IBM IT service management platform and deployed in several IBM service centers. This system is used periodically to refine and ad-just monitoring situations after a system has gone through a change, thus helping to enhance the overall reliability in IT service management.
