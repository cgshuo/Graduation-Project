 We introduce the posterior probab ilistic clustering (PPC), which provides a rigorous posterior probability interpretation for Non-negative Matrix Factorization (NMF) and removes the uncertainty in clustering assignment. Furthermore, PPC is closely related to probabilistic latent semantic indexing (PLSI).
 Intelligence ]: Learning Algorithms, Experimentation, Measurement, Performance, Theory Sparse, Posterior Probabilistic Clustering, NMF
Non-negative Matrix Factorization (NMF) [4] has been success-fully applied to document clustering recently [5, 1]. However, in the standard NMF clustering, cluster assignment is rather ad hoc . In addition, matrix factors lack clear interpretations.
In this work, we introduce the pos terior probabilistic clustering (PPC), which has 3 benefits: (1) It provides a rigorous posterior probability interpretation for both matrix factors F, G in the factor-ization of input X : X FG T . (2) It removes the uncertainty in clustering assignment. (3) Furthermore, when we perform simul-taneous word and document clustering, the new model has a very close relation to probabilistic latent semantic indexing (PLSI) [3]: in PLSI, F, G are class conditional probabilities; in PPC, F, G are class posterior probabilities.

Suppose we have n documents and m words (terms). Let X = ( X ij ) be the word-to-document matrix: X ij = X ( w i ,d j ) is the frequency of word w i in document d j . Standard NMF optimization is where X has size m  X  n , F has size m  X  K , G has size n  X  the solution ( F  X  ,G  X  ) is computed, standard approach is to assign d to the cluster C k where i.e., the largest element of j -th row of G .

There is a fundamental problem with this approach. First, the solution to NMF is not unique. For an arbitrary positive diagonal matrix D = diag ( d 1 ,  X  X  X  ,d k ) ,wehave assignment is modified to A different choice of D leads to different cluster assignment. An ad hoc solution is to choose D such that columns of F have unit length in L 2 norm. In this work, we present a principled way to resolve this problem. This is based on posterior probability interpretation of G . In fact, we can see from Eq.(3) that (roughly speaking) is the posterior probability that d j belongs to different clusters. Thus we wish to choose D such that This requirement has no solution, because there are n constraints and K variables, but K is much less n . Therefore, in standard NMF, there is no way to enforce posterior probability normalization.
In our approach, we enforce the posterior probability normaliza-tion directly. The posterior probabilistic clustering is to optimize Using Lagrangian multipliers to enforce the constraints, we derive the following updating rules to solve this problem The correctness and convergence can be proved rigorously. In the updating process, the constraints should be enforced periodically.
We generalize PPC to simultaneous word and document clus-tering. We use F as the posterior probability for word clustering, and the posterior probability normalization is K k =1 F ik simultaneous PPC (SPPC) problem becomes We derived the updating algorithm as follows. Let F = FS , G = GS T , the updating algorithm is We initialize F, G to the K-means clustering results on words F and on documents G 0 .where F 0 ,G 0 are cluster indicators. We set F = F 0 +0 . 2 and G = G 0 +0 . 2 .
In PLSI we view the word-document matrix X as the joint proba-bility of word and documents. [We re-scale the term frequency X by X ij  X  X ij /T w , where T w = ij X ij . With this, ij 1 . ] The joint occurrence probability is p ( w i ,d j )= X decompose it as product of cla ss-conditional p robab ilities: Let F ik = P ( word i | class k ) ,S kk = P ( class k ) , G ) , PLSI optimization problem is: Therefore, our SPPC is quite similar to PLSI, except SPPC has a different normalization K k =1 G jk =1 , K k =1 F ik =1 . In other words, SPPC treats G jk ,F ik as posterior probabilities; PLSI treats G jk ,F ik as class-conditional probabilities.

Note that in PLSI, the sum of probabilities of a document be-long to different classes, K k =1 G jk = P ( doc j | class tuitively for clustering, we would like the total probability adds up to 1. This deficiency is removed in SPPC. We give a simple example to illustrate the PPC and SPPC results. The data matrix is given bellow. From inspection, first 3 columns belong to one cluster and the last 4 columns belong to another. For rows, first 3 rows belong to one cluster and the last 2 row belong to another. The resulting F, G recover the clustering correctly.
We compare the clustering performance of each method on 5 We use accuracy as the performance measure. The experimental results are shown in Table 1. We see that SPPC performs slightly better than NMF and PLSI.
 Table 1: Clustering Results. Shown are the accuracy results for different methods.
