 1. Introduction
Complex systems are required to be dependable in use and one important aspect of a system X  X  dependability is availability.
Availability is intrinsically uncertain and is typically defined and measured as the probability of the system being available for use at a given point in time. A system might be unavailable if it has failed and is awaiting repair or the system has failed and is undergoing repair before re-entering service. Over a given time period, a system might therefore be available or unavailable depending not only on the system X  X  reliability but also on how well the support organisation might affect the rate of repair and the duration of such repairs (renewals). Additionally systems may also undergo preventative maintenance usually on a scheduled basis, and we might extend our analysis to consider the modes of failure, the subsystem failure rates, maintenance regimes and different methods of logistical support. Maintenance (renewal time) and reliability (failure time) are stochastic variables and it therefore makes sense to model these using appropriate statistical inference techniques. We could then predict future behaviour and make decisions about the acceptability of the availability one might expect to experience in a given system. For an overview of availability theory, concepts and models see Stapelberg (2009) .

We have used Bayesian networks (BNs) in a range of real-world applications of system dependability assessment; see for example Neil et al. (2001, 2003, 2008) . In such applications, it is inevitable that there will be a mixture of discrete and continuous nodes (the resulting BNs are called hybrid (HBNs)). The traditional approach to handling (non-Gaussian) continuous nodes is static: you have to discretize the continuous domains using some pre-defined range and intervals. However, this approach is unaccep-table for critical systems, where there is a demand for reasonable accuracy. To overcome this problem, we have developed a new and powerful approximate algorithm for performing inference in
HBNs. We use a process of dynamic discretization of the domain of all continuous variables in the HBN. The approach to discretizing the domain is influenced by the work of Kozlov and
Koller (1997) using an entropy error as the basis for approxima-tion. We differ from their approach by integrating an iterative approximation scheme within existing BN software architectures, such as in junction tree (JT) propagation ( Jensen et al., 1990 ).
Thus, rather than support separate data structures and a new propagation algorithm, we use the data structures commonly used in JT algorithms.

The power and flexibility of the approach are demonstrated here by applying it to estimate the availability of repairable systems represented by a series of models each designed to model distinct stages in the renewal process: logistics delays, repairs and scheduled maintenance. Traditionally, modelling these events has relied on Monte Carlo simulation, involving many repeated simulation runs. In contrast to the simulation approach, we show how our HBN algorithms can be used to represent repair and support processes and the durations involved, under any assumptions for renewal time distributions (lognormal, exponen-tial, etc.). The modelling has been made possible using the commercial general-purpose Bayesian network software tool
AgenaRisk, Agena Ltd. (2010) . 2. Bayesian networks
BNs have been widely used to represent full probability models in a compact and intuitive way. In the BN framework, the independence structure in a joint distribution is characterised by a directed acyclic graph, with nodes representing random variables (which can be discrete or continuous, and may or may not be observable), and directed arcs representing causal or influential relationship between variables ( Pearl, 1993 ). The conditional independence assertions about the variables, represented by the lack of arcs, reduce significantly the complexity of inference and allow the underlying joint probability distribution to be decom-posed as a product of local conditional probability distributions (CPDs) associated with each node and its respective parents. If the variables are discrete, the CPDs can be represented as node probability tables (NPTs), which list the probability that the child node takes on each of its different values for each combination of values of its parents. Since a BN encodes all relevant qualitative and quantitative information contained in a full probability model, it is an excellent tool for many types of probabilistic inference, where we need to compute the posterior probability distribution of some variables of interest (unknown parameters and unobserved data) conditioned on some other variables that have been observed.
A range of robust and efficient propagation algorithms has been developed for exact inference on Bayesian networks with discrete variables ( Pearl, 1988; Shenoy and Shafer, 1990; Jensen et al., 1990 ). The common feature of these algorithms is that the exact computation of posterior marginal distributions is per-formed through a series of local computations over a secondary structure, a tree of clusters, enabling calculation of the marginal without computing the joint distribution. See also Huang and Darwiche (1996) .

The present generation of BN software tools attempt to model continuous nodes by numerical approximations using static discretization as implemented in a number of software tools (e.g., Hugin, 2005; Netica, 2005 ). Although discretization allows approximate inference in a hybrid BN without limitations on relationships among continuous and discrete variables, current software implementations require users to define a uniform discretization of the states of any numeric node (whether it is continuous or discrete) as a sequence of pre-defined intervals, which remain static throughout all subsequent stages of Bayesian inference regardless of any new conditioning evidence. The more intervals you define, the more accuracy you can achieve, but at a heavy cost of computational complexity. This is made worse by the fact that you do not necessarily know in advance, where the posterior marginal distribution will lie on the continuum for all nodes and which ranges require the finer intervals. It follows that where a model contains numerical nodes having a potentially large range, results run the risk of being only crude approximations.
Alternatives to discretization have been suggested by Moral et al. (2001) and Cobb and Shenoy (2006) , who describe potential approximations using mixtures of truncated exponential (MTE) distributions, and by Murphy (1999) who uses variational methods. There have also been some attempts for approximate inference on hybrid BNs using Markov Chain Monte Carlo (MCMC) approaches ( Shacter and Peot, 1989 ). However, constructing dependent samples that mixed well (i.e., that moves rapidly throughout the support of the target posterior distribution) remains a complex task. 3. Dynamic discretization 3.1. Inference and discretization
In this paper, inference is carried out using a standard BN propagation algorithm ( Lauritzen and Speigelhalter, 1988; Jensen et al., 1990 ). Unfortunately, for hybrid BNs containing mixtures of discrete and continuous nodes with non-Gaussian distributions, exact inference becomes computationally intractable. The tradi-tional approach to handling (non-Gaussian) continuous nodes is static discretization. This requires the user to define a uniform discretization of the domains of all continuous nodes, using some pre-defined range and intervals. The discretization remains static throughout all subsequent stages of exact inference performed on the resulting discrete BN. The more intervals you define, the more accuracy you can achieve, but at a heavy cost of computational complexity. The level of accuracy of this approach is also constrained by the feasibility of identifying the high-density regions for each variable in the model, and this needs to be done in advance of any inference taking place. This is cumbersome, error prone and, where a model contains numerical nodes having a potentially large range, results are necessarily only crude approximations.

Let X be a continuous random node in the BN. The range of X is denoted by O X , and the probability density function (PDF) of X , with support O X , is denoted by f X . The idea of discretization is to approximate f X as follows: partition O X into a set of intervals C X  X  { w j }, and define a locally constant function ~ f X on the partitioning intervals.

Discretization operates in much the same way when X takes integer values, but in this paper we will focus on the case where X is continuous. As Kozlov and Koller (1997) , we use an upper bound of the Kullback X  X eibler (KL) metric between two density functions f and g : D  X  f J g  X  X  as an estimate of the relative entropy error induced by the discretized function. Under the KL metric, the optimal value for the discretized function ~ f is given by the mean of the function f in each of the intervals of the discretized domain. The main task reduces then to finding an optimal discretization set C X
Our approach to dynamic discretization searches O X for the most accurate specification of the high-density regions given the model and the evidence, calculating a sequence of discretization intervals in O X iteratively. At each stage in the iterative process, a candidate discretization, C X , is tested to determine whether the relative entropy error of the resulting discretized probability density below a given threshold, defined according to the tradeoff between the acceptable degree of precision and computation time.
By dynamically discretizing the model, we achieve more accuracy in the regions that matter and incur less storage space over static discretizations. Moreover we can adjust the discretiza-tion any time in response to new evidence to achieve greater accuracy. A detailed description of the dynamic discretization algorithm is given in Neil et al. (2007) . In an outline, dynamic discretization follows these steps: 1. Convert the BN to a junction tree (JT) and choose an initial discretization for all continuous variables. 2. Calculate the NPT of each node given the current discretization.
 3. Enter evidence, as data into nodes that have been observed, and perform global propagation on the junction tree, using standard JT algorithms. 4. Query the BN to get posterior marginal distributions for each node, compute the approximate relative entropy error and check if it satisfies the convergence criteria. 5. If not, create a new discretization for the node by splitting those intervals with the highest entropy error. 6. Repeat the process by recalculating the NPTs and propagating the BN, and then querying to get the marginal distributions, and then split intervals with the highest entropy error. 7. Continue to iterate until the model converges to an acceptable level of accuracy.

Nodes with continuous functions can be declared as a piecewise set of constant values and/or functions, and these are supported during dynamic discretization. The relevant subdo-mains are inserted for each constant/function at the start of the discretization process and then the algorithm initiated. These subdomains are maintained, unless a constant or function is assigned zero probability during an evidence propagation, in which case the relevant subdomain is removed from the node X  X  domain. Only those subdomains with non-zero probability are then represented in the posterior marginal distribution for a node at the end of the process. At its most basic, it means the algorithm can perform arithmetic calculations: although seemingly trivial, no other BN algorithm can perform this task.

More detail on how the algorithm can be used to solve reliability problems can be found in Neil et al. (2008, 2010) . Neil et al. (2010) demonstrate how the algorithm outperforms analytical, Monte
Carlo and other BN based algorithms for solving dynamic fault trees, whilst in Neil et al. (2007) the algorithm is compared with Markov
Chain Monte Carlo (MCMC) and other approaches. 3.2. Estimating the CPD using deterministic functions
In general, performing approximate inference on models that have conditionally deterministic variables with continuous parents, which are not normally distributed, represents a major challenge for most BN algorithms. Here, we describe an approximate solution which has the advantage of being general purpose, in that it provides approximate solutions for nonlinear deterministic functions and any parametric or non-parametric distributional form for the parent nodes.

A simple method for approximating the local conditional probability density f t ( t 9 pa ( t )) commonly used under the static discretization approach proceeds by first sampling values from each parent interval in O pa ( t ) for all parents of t and calculating the result t  X  g ( pa ( t )), then counting the frequencies with which the results fall within the static bins pre-defined for t and finally normalising the NPT.

Although simple, the above procedure is flawed. On the one hand, there is no guarantee that every interval in O t will contain a probability mass if the parents X  node values are under-sampled.
Any subsequent inference in the BN then will return an inconsistency when it encounters either a valid observation in a zero mass interval in t or attempts inference involving t . The only way to counter this under static discretization is to generate a large number of samples from each parent interval in O pa ( t ) not only might be expensive, but also, as the number of parent nodes increase, and the states in O t and O pa ( t ) increase, the number of cells in the approximating NPT increases exponen-tially. To avoid these issues, the conditional density function f ( t 9 pa ( t )) is approximated by dynamically discretizing the range of all the continuous random variables and approximating f ( t 9 pa ( t )) at each discretization step using a weighted uniform density function. The use of this deterministic function is entirely novel in this context and provides a means of estimating the time-to-failure distribution for all conditional constructs, including variables representing dynamic gates, no matter the form of the time-to-failure distributions of the input components.
Without loss of generality, we describe how the approach works in the case where there are just two parents: 1. Consider a system S with two basic components A and B .We can define the time-to-failure of the system t S as a determin-istic function of the time-to-failure of the basic components,  X  g ( t A , t B ). 2. Suppose the variables t A and t B have discretizations, C C
B over their respective domains O t A and O t B . For each pair of intervals in the respective sets C A and C B , such as interval ( a 1 , a 2 )in C A and interval ( b 1 , b 2 )in C B , the approach computes the minimum l and the maximum u for each of the set of values g ( a 1 , b 1 ), g ( a 1 , b 2 ), g ( a 3. If I is the set of all such pairs of intervals, then we get a set of intervals ( l i , u i ) for each i A I , and this generates a uniform probability density mass, U ( l i , u i ), over the range of t that C S consists of the intervals o 1 , y , o n , then the approx-imating NPT for the conditional density function of the node t ^ f  X  t 9 pa  X  t  X  X  , is defined as a weighted uniform distribution by p
U ( t ; l i , u i ) corresponding to the interval o k ,i.e., p
By iteratively updating the partitioning intervals using the dynamic discretization scheme (i.e., recalculating the NPT approx-imations over the current discretized domains, propagating the discrete BN to compute the approximate marginal posterior probability density function of each node and then splitting/ merging intervals according to the relative entropy criteria until the model converges), the approach is able to obtain accurate approximations for the marginal probability density functions of conditionally deterministic variables defined by any nonlinear deterministic function and general distributions. In particular, we are not limited to the exponential or Gaussian families. 4. Modelling renewal distributions using HBNs
In this section, we use the dynamic discretization approach to estimate the distribution of the renewal times of a continuous use repairable system. Bayesian modelling offers a suitable framework for assessing the repair durations of such systems, allowing us to integrate information from multiple sources at different levels of granularity, as well as expert opinion. Here, we are interested in modelling repair times, logistics delays and scheduled maintenance.
The model considered here is based on the following assumptions. 4.1. Modelling repair times using mixtures of lognormal repair time distributions
The inherent availability of a system ( Elsayed, 1996 ) is partly determined by the corrective repair time of the system. For complex systems, there may be a variety of repair locations or  X  X  X ines, X  X  each of which may specialise in particular classes of repair, where the repairs take a variable time. Examples of repair lines might be users, first line support, second line support, and manufacturer X  X  first and second lines support (these are often numbered as 0, 1, 2, 3, 4). Each of these lines of repair can be characterised by a repair time distribution; in this case we will assume a lognormal distribution (often with parameters defined by user supplied median, m , and 95th percentile repair times, a ).
The lognormal parameters can also be derived from historical data or the parameters can be learned using the HBN model as might be done when learning repair rates as described in Marquez et al. (2010) , where a hierarchical model was used to learn posterior marginal distributions of parameters from child nodes, instan-tiated with repair time data.

If we represent the repair lines as the states of a discrete labelled variable, L , with L  X  {0, 1, 2, 3, 4}, then formally the marginal distribution of the repair time, t , is given by the mixture of the repair time distributions for each repair line, t : f  X  t  X  X  five lines of repair and median and 95th percentile estimates for each. The HBN used to model this problem is shown in Fig. 1 .In
Fig. 1, the lines of repair node have prior probability distribution
The marginal probability distribution for repair time shown in Fig. 1 is therefore an average of the repair time distributions given in Table 1 weighted by the prior probabilities for each line of repair.
The mean repair time of the system as calculated from the HBN is 3.74 h and the 95th percentile is 12.19 h. 4.2. Modelling logistics delay times using mixtures of normal logistics delay time distributions
Operational availability of a system is dependent on the logistics delay time as well as the repair time. Here, we wish to model the logistics delay time for a system that has a logistics chain represented by a number of discrete delay events such as administration, resourcing, sparing and recovery, and which might themselves have different duration distributions depending on the line of repair involved.

Formally, we represent the total logistics delay time, T , as the sum of the delay times, T i , for the delay events i  X  1,2, T  X  P n i  X  1 T i . The duration of each delay event will depend on the line of repair and the conditions under which they are performed. For example, recovery may only be required in a small number of cases, and will only be enacted as repair line 1. Likewise the delay time for spares will be a function of whether they are actually required or whether they are at hand.

The conditioning event set will represent as a series of Boolean with the repair line variable, L , to index each delay time node and assign a delay time distribution to that node, such that f  X  T i 9 S j  X  Yes , L  X  l  X  X  TNormal  X  m jl , s 2 jl , 0 , 1 X  f  X  T i 9 S j  X  No , L  X  l  X  X  0
Here again, the parameters of the right truncated Gaussian normal distribution can be derived from historical data or expert knowledge. The switch variables, S j , are set with prob-abilities to indicate the proportion of repair events that involve each delay stage. So if a discrete stage does not take place, the delay time is zero and a normal distribution otherwise (and this can be implemented using the support offered for piecewise functions and constants in the dynamic discretization algorithm).

The marginal distributions for the logistics delay times are therefore f  X  T j  X  X 
Fig. 2 shows the HBN for an example logistics delay problem, with Boolean variables S 1  X  Administration performed?, S Resources needed?, S 3  X  Spares required?, S 4  X  Spares available?, and S 5  X  Recovery required?. Each of these influences the delay times, T 1  X  delay time for administration, T 2  X  delay time for resources, T 3  X  delay time for spares, and T 4  X  delay time for recovery.

Table 2 shows an example with three active lines of repair and mean and s.d. parameter values for each delay event. Also, the prior probabilities for the switch variables are P  X  S 1  X  Yes  X  X  0 : 9 P  X  S 2  X  Yes  X  X  0 : 7 P  X  S 3  X  Yes  X  X  0 : 5 P  X  S 4  X  Yes  X  X  0 : 8 P  X  S 5  X  Yes  X  X  0 : 1 The prior probabilities for each line of repair are shown in Table 2 . For simplicity, three repair lines are not used.
The resulting distribution for the total delay time, for this example, is shown in Fig. 3 . The mean total delay time for the a system as calculated from the HBN is E ( R )  X  7.17 h and the 95th percentile is 15.32 h.

It is straightforward to see how we could extend the modelling approach used here to sc heduled maintenance activities and to more complex supply processes containing cascading sets of conditional events. For example, we could model scheduled maintenance durations using fixed duration events with probabilities proportional to the schedule intervals. Likewis e complex cascading events could be modelled by families of parent no des representing the conditional logic governing the conditions under which repair is undertaken. 5. Bayesian model for system reliability
Let us now consider a system comprising n subsystems k  X  1,2, y , n , but we have not as yet been able to directly test the reliability of each one. Instead, we can estimate the reliability of each subsystem from historical data gathered on an arbitrary number of similar subsystems drawn from the same family as the subsystem of interest. Thus, the unseen subsystem is considered to be exchangeable with the historical subsystems.

In order to assess the reliability of similar subsystems, we assume that a series of reliability tests have been conducted under the same operational settings or that directly compar-able field data are available. The data resulting from these tests consist in the time-to-failure observed at the point of failure (assuming no censoring). Here, we make the assumption that the time-to-failure (TTF) distribution, t k , for a component in the system is Weibull (the TTF is not constant over time i.e., wear out): f  X  t 9 l , c  X  X  where l is the shape parameter and c is the scale parameter.
However, because reliability data can be sparse and heavily censored, additional information in the form of expert judgement plays an important role in the definition of statistical reliability models. Here, we choose illustrative distributions for the Weibull shape and scale parameters, but in practice these might be elicited by experts. For example, here we use l Triangular  X  1 : 5 , 2  X  c Uniform  X  10 E 6 , 200 , 000  X 
Therearetwopointstomakeabouttheexampledistributionswe have used for the prior parameters. Since no joint conjugate prior is available when l , c are both assumed unknown, their prior distributions are specified independently. In our HBN modelling approach, there is no restriction on the prior distributions for the parameters of the model. Secondly, the prior distributions are based on past experience, and in this particular case, asking experts to use a triangular or uniform distribution is relatively easier compared to using other more complex distributions.

The HBN model used to calculate the TTF for a subsystem, t k  X  1,2, y , n , is shown in Fig. 4 . This is an example of a parameter learning BN where the known, observed, TTFs for individual subsystems are represented by nodes and the unknown parameters are learned from this data. The TTF for a future, unknown, subsystem is then represented by the predictive posterior marginal distribution.

For a system S comprising n subsystems k  X  1,2, y , n and arranged in series (OR) configuration, the TTF, t S , is simply  X  min k f t k g and if configured in parallel t S  X  max k calculate the system level TTF from more complex gates such as PAND see Marquez et al. (2010) . Once we have the TTF distribution for the system, we can then easily derive the mean time between failures (MTBF) statistic. 6. Availability
Availability is defined as the long run probability of the system being available for use at any point in time ( Billinton and Allan, 1992 ). This is expressed as a point estimate and calculated from the mean delay and reliability point estimates: Inherent availability:
A I  X  MTTF MTTF  X  MRT where MRT is the mean repair time and MTTF is the mean time-to-failure.
 Operational availability: A where MDT is the mean logistical delay time.

Given that the summary statistics are constants, we need to calculate the values for A I and A O in the HBN, using the support for constants in the dynamic discretization algorithm. 7. Scheduled maintenance
For the purpose of failure prevention, scheduled maintenance is typically carried out at fixed intervals or when fixed exposure targets have been reached. For many applications, it is convenient to model scheduled maintenance frequency and duration times as deterministic constants.

Given this deterministic assumption, we can characterise the total availability, related to scheduled maintenance, A S scheduled maintenance event, as the product of the availability induced by each scheduled maintenance event, A i , i  X  1,2, A  X 
We can therefore adjust the system level inherent and operational availabilities, to account for scheduled maintenance, and obtain revised values, A u I and A u O , where  X  A I A S and A u O  X  A O A S
We can then calculate these within the HBN using the same algorithm as before. 8. Application support
Practical modelling of repairable systems using HBNs requires a modelling environment to design the model structure, encode design assumptions and specify model dependencies. The model described here was developed using AgenaRisk Enterprise Edition (EE), Agena Ltd. (2010). This allows the user to declare the system structure and specify how the application should automatically generate the corresponding HBN model from this structure.
Behind the scenes, AgenaRisk EE applies a relational object model to encode the allowable abstract system hierarchy and specify how this is linked to the required HBN models. The relational model is then encoded in XML and connectivity for data import/ export is supported via a database interface or via a CSV file format interface for connecting to Excel. 9. Conclusions
We have shown how we can use hybrid Bayesian networks (HBNs) to model renewable systems involving repairs and logistical delays. Particular focus has been given to modelling mixtures of lognormally distributed repair distributions, and this has been extended into the logistical delays case, involving the summation of delay times from many discrete events with normally distributed delay times.
 We show how to assess the reliability of subsystems using a
Bayesian model coupled with component aging assumption and integrating data with an expert opinion. These subsystem TTF distributions could then be aggregated to provide a system level estimate using appropriate logical gates that reflect system structure. The combined scheme of dynamic discretization and robust propagation algorithms on HBNs can be used to obtain accurate results, offering a viable alternative to Monte Carlo simulation approaches, implemented within an easy-to-use and user friendly environment. Finally, we show the simple step of calculating the operational and inherent availability of the system from the various delay and failure distributions, and also demonstrated how point values for availabilities, with and without scheduled maintenance, can be obtained.
 References
