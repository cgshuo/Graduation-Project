 Email classification is still a mostly manual task. Conse-quently, most Web mail users never define a single folder. Recently however, automatic classification offering the same categories to all users has started to appear in some Web mail clients, such as AOL or Gmail. We adopt this approach, rather than previous (unsuccessful) personalized approaches because of the change in the nature of consumer email traffic, which is now dominated by (non-spam) machine-generated email. We propose here a novel approach for (1) automati-cally distinguishing between personal and machine-generated email and (2) classifying messages into latent categories, without requiring users to have defined any folder. We re-port how we have discovered that a set of 6  X  X atent X  cate-gories (one for human-and the others for machine-generated messages) can explain a significant portion of email traf-fic. We describe in details the steps involved in building a Web-scale email categorization system, from the collection of ground-truth labels, the selection of features to the train-ing of models. Experimental evaluation was performed on more than 500 billion messages received during a period of six months by users of Yahoo mail service, who elected to be part of such research studies. Our system achieved pre-cision and recall rates close to 90% and the latent categories we discovered were shown to cover 70% of both email traffic and email search queries. We believe that these results pave the way for a change of approach in the Web mail industry, and could support the invention of new large-scale email discovery paradigms that had not been possible before. H.4.3 [ Information Systems Applications ]: Communi-cations Applications X  Electronic Email Information Systems; Algorithms; Experimentation Email Classification; Machine-generated Email; LDA
Email classification has been a user X  X  pain point since the early days of email systems. In the last decade, most at-tempts at automating this task have consisted in mimick-ing each individual user X  X  personal classification habits [12]. This highly personalized approach exhibits two main weak-nesses: it relies on small datasets as each individual inbox is analyzed independently, and it requires from users to have defined meaningful folders in the first place. We believe that relying on user-defined folders is the reason for which such methods were never widely adopted. It was shown in [13] that 70% of users do not create even a single folder. Further-more, we have verified on a large dataset, generated from Yahoo mail,that only 10% of users are  X  X ctive classifiers X , the latter being defined as users who moved more than 10 messages into folders over a period of 6 months.

This fact has not remained unnoticed. A few of commer-cial Web services have changed their approach towards email classification by offering  X  X lasses X  (implemented as labels, or categories for instance) that are common to all users. Thus, AOL mail, with its  X  X ulk Senders X  category, or Gmail, with its inbox tabs 1 , offer a few  X  X ommon X  classes. On the other end of the range, Koren et al. in [13], have analyzed pop-ular folders, in order to identify not just a few but several thousand possible common classes or  X  X ags X . While the wide range of tags is attractive, one weakness of this approach is the high variance in their level of abstraction. Highly sub-jective tags such as mom co-exist with more objective ones like recipes , or sender-based tags such as amazon overlap with related ones like shopping . In addition, they achieve recall and precision rates of only 80% for a coverage of 72%. This work explores a middle ground direction, where we pro-pose, like in [13], to derive high level-classes from mail data, and especially folders data provided by the minority of users who do define folders. However, similarly to AOL or Gmail, we give preference to fewer, consistent classes, for a more user-friendly experience, and offer these classes to all users, including those who did not define any folder.

First, we propose to exploit the fact that today X  X  com-mercial Web mail traffic is dominated by machine-generated messages, originating from mass senders, such as social net-works, e-commerce sites, travel operators etc. In their exper-iments, Ailon et al. stated that machine-generated mail rep-resent more than 60% of Yahoo mail traffic, [1]. We argue
These tabs include  X  X romotions X ,  X  X ocial X ,  X  X pdates X  and  X  X orums X , see https://support.google.com/mail/ here that, given the difference in syntax and semantics be-tween human-and machine-generated messages, one should first distinguish between machine and human-generated mes-sages before attempting any finer classification. Once fo-cusing on machine-generated messages, with huge numbers (millions in some cases) of users receiving the same type of machine-generated messages, data will be much less sparse, and classification techniques should become more precise. In order to validate our intuition, we estimated the actual volume of machine-generated messages on a very large Ya-hoo mail dataset. This dataset, built for the purpose of this work, covers 6 months of email traffic and more than 500 bil-lion messages. For privacy reasons, it includes messages be-longing exclusively to users who voluntarily opted-in for such studies and message bodies were not inspected by humans. We conducted experiments on this dataset (as detailed later on) and verified that non-spam machine-generated messages actually represent 90% of the entire dataset, which encour-aged us to proceed.

In the same dataset, we considered the subset of folders data that include messages classified by users into folders as well as folders labels, generated by more than 40 million users. Instead of a priori fixing high level classes, or deriv-ing them from popular folders labels, as done by the previ-ously mentioned efforts, we attempt here another approach. Namely, we propose to discover these categories by apply-ing an LDA (Latent Dirichlet Allocation) approach [4], on the folder dataset. We explain how  X  X atent X  categories can be identified in Section 2. More specifically, we detail how one human and five machine-generated categories can be in-ferred directly from the data. We introduce in Section 3 our email classification model and its associated features, such as content, sender, behavioral, and temporal behavioral fea-tures. We also justify the different aggregation levels we considered, namely message-, sender-and domain-level ag-gregation. An additional challenge is to obtain a sufficiently large training set. Due to privacy issues, the cost of edito-rial work, and the skewness of the email data, we use various automated methods. For the machine latent categories, our main data source is the folder data. Using our LDA analy-sis we obtain for a large number of messages a clear linkage to our latent categories. For the human category, folder data is not effective, and we use instead various heuristics leveraging sender information and extrapolated data via co-training inspired techniques. We detail how we generated the training data in Section 4. Section 5 describes our clas-sification mechanism and Section 6 presents our results. In section 7 we provide some additional statistics to demon-strate the impact of our classification system. Note that we exclusively consider consumer Web mail as opposed to corporate email, whose traffic is drastically different in size and nature. Consequently, we did not compare our results to the Enron dataset. Related work is discussed as relevant throughout the paper.

The key contributions of our work are the following: (1) we provide new insights on the importance of distinguish-ing between human-and machine-generated email, (2) we give to the best of our knowledge, the first large scale data-driven validation of the intuition that a few consistent au-tomatically discovered categories cover most of today X  X  Web email traffic, and (3) we describe an end-to-end Web mail categorization solution, including detailed models, learning mechanisms, evaluation methods, and results on real traffic.
Following [16], we define email categorization as the task of  X  X ssigning a Boolean value to each pair ( d j , c i )  X  D  X  C , where D is a domain of documents X  (here mail messages)  X  X nd C is a set of predefined categories. X  Previous research work on personalized email classification used one C set per user and populated it by reusing the user X  X  existing fold-ers, see [15, 7, 2, 10]. The approach we advocate for here is to adopt one single C for all users. Still C can be de-fined in many different manners: categories can relate to message importance (see Gmail Priority Inbox 2 ), number of recipients (see AOL Bulk Senders ), senders (see popular folders such as Amazon, Facebook ), or types (see Gmail Labs Smart Labels 2 Social, Promotion, Updates, Forums, Travel, Finance ) for instance. In addition, C should be derived from data (rather than predefined) and should fulfill the following requirements. 1. The number of categories should be small so as not to 2. Categories must be easily interpretable and at the same 3. Categories should cover a significant amount of email
We first examined all messages that have the potential to be classified by retrieving the most  X  X opular X  folders in terms of users X  X   X  X oldering X  actions. We ignored system fold-ers (e.g.,  X  X rash X ,  X  X pam X ) and folders created by third-party services such as OtherInbox 3 . We also removed small folders that count less than 1 , 000 messages, since they would not cover common needs. We remained with 100 , 000 human-generated folders. For illustration, we list here the top fold-ers thus identified: ebay, accounts, personal, school, saved mail, jobs, amazon, taxes, recipes, business, college, bills, house ,facebook, paypal, save, food, linkedin, pictures, sur-veys, travel, jobs, misc. etc.

We then associated with each of these 100 , 000 folders an artificial document obtained by concatenating across all rel-evant inboxes, all messages classified into this folder. We applied LDA [4], to these  X  X ocument folders X . LDA is com-monly used in natural language processing in order to dis-cover a set of latent topics that generated a given set of doc-uments. In our context, we hoped that latent topics would map into  X  X atent categories X  , which would then define our set C for further classification. To this effect, we trained an online LDA model [9]. Recall that LDA receives as input a set of documents (folders), each associated with a bag of words (words of messages put in the folder), and a number of topics K . It outputs for each topic the top words associated with it and the topic mixture of each document. We iter-ated over several values of number of topics, from K = 50 to K = 5. To choose the right K , we looked at coverage, in terms of portion of traffic covered by folders that were dominantly ( &gt; 50%) associated with topics. For a value of K and a specific topic, the coverage of the topic is defined as the amount of traffic covered by folders that are associated with said topic by at least 50% by the LDA output. The total coverage associated with a value K is the sum of cov-erages associated with the topics. Our objective was to find a value of K that would ensure that each individual topic as well as the overall set of topics achieve significant coverage. We observed that larger values of K produced topics that were narrow, e.g. Knitting, Cooking, Astrology with small coverage. The traffic coverage at K = 50 was 81%, and went down to 74% at K = 25, to about 70% at K = 6. At K = 6 the coverages of the topics were sufficiently large (see Table 9), while at larger values of K narrow topics emerged.
We further examined the topics obtained for K = 6, as this value exposed a good balance between total and individ-ual coverage, by inspecting their associated vector of words and weights. As evidenced in the five rightmost columns of Table 1, the words associated with each of these fives topics clearly shared a common underlying concept, respectively career, shopping, travel, financial and social . One topic re-mained that was a bit puzzling at first, as it contained many highly frequent (stop) words and no key concept seemed to emerge from them. After some quick examination of the messages (simply looking at senders) associated with this topic, we discovered that most were human-generated, as opposed to the five other topics that are mostly machine generated. The word associated with this first topic are quite frequent in personal communications, consequently we named this last topic human .

Given these 6 labeled topics, we further verified their dis-tribution over a sample of popular folders, as shown in Ta-ble 2. Some mappings between folders and latent topics were almost perfect, like the folder  X  X inancials X  being associated with the latent topic financial with a 96% weight, or  X  X otels X  with the topics travel at 74%, with still a flavor of financial at 15%. More interestingly, a highly subjective folder like  X  X om and dad X  had no clear winner. While it did reflect the human topic at 31%, it also (and ironically so) had financial at 55%! Based on this analysis, we finalized our decision of choosing as C { human, career, shopping, travel, financial } , as it meets our 3 requirements of small size, same abstraction level and coverage.
One key challenge in any classification task is data model-ing and fixing the level of granularity of data points i.e., the objects to classify. One option is to consider each individual message as a single data point, associated with various fea-tures extracted from the message header and body. A second approach consists in aggregating messages at higher levels, for instance at the SMTP address level [11], (referred to as  X  X ender X ), or at the mail domain level (easily extracted from the SMTP address by truncating its prefix up to the charac-ter  X  X  X .) This latter approach not only helps with sparseness issues but more importantly allows for much more efficient processing, without requiring expensive body analysis and feature extraction. This is critical at the scale of Web mail, when huge numbers of messages need to be classified at de-livery time. We explain how we use a combination of both approaches in Section 5. Feature extraction and different aggregation levels are described in the rest of this Section.
We follow the general categorization framework where nu-merical features are extracted from each object (in our case, message, sender or domain) later to be fed to a learning mechanism that given labelled examples, e.g. messages with manually assigned categories, will be able to categorize un-labeled objects. In this section we describe the types of features and how they are obtained.

A data point consists of content features, address features, and behavioral features which include a special subcategory of temporal behavior features.

Content features include features derived from the mes-sage subject and body. We extract words from the sub-ject line and message body, as well as the subject character length, body character length and the number of urls oc-curring in the body. To form sender features, we use the total counts of observed words across all messages from that sender, as well as the average, the minimum and the max-imum subject and body lengths and url counts. In large databases, such as the one we are dealing with, the most frequent words can easily occur millions of times (e.g.  X  X he X ,  X  X  X ,  X  X n X ). We eliminate the top 400 words found across most senders, as well as the usual stop words 4 for several common languages. Finally, we filtered out the low frequency words that occur in less than 100 different senders.

Address features include features extracted from the sender email address, including the subdomain, e.g. (.edu, .gov, etc.), subname (e.g. billing, noreply). To extract the subdomains we split the domain part of the address (after the  X  X  X  character) at the delimiter  X . X  location, and use the resulting words as features. We do the same for the subnames, but on the name portion of the email (before the  X  X  X  sign). We also extract  X  X ommercial X  keyword from senders. To this effect, we use a list of more than 500 key-words, typically used as bid words in ad targeting, such as itinerary, flight, ticket, order, confirmation, billing, payslip, payment, transaction, stocktrade, career, shopping, travel, etc., and look for a match in the email address. To counter the imbalance between rare and frequent words, we remove the 800 most frequent sender substrings. This avoids poor generalization when using biased training data. For exam-ple, very common domain names, such as  X  X ahoo.com X , or  X  X mail.com X , could get picked as the most informative fea-tures in a model trained to detects human senders. This rule clearly does not generalize well outside the training dataset. In addition to removing the most frequent substrings, we remove the substrings that appear in less than 100 senders. We also map very rare or random strings into a canoni-cal form, for instance 5tsdfocfyf66c@bookstore.com , is mapped into a single feature  X  X andom string X .

Behavioral features include features extracted from the sender X  X  and recipient X  X  actions over a given message. They can be split into outbound , inbound and action features. The outbound features for the n -th sender cover the sender X  X  out-going activity, such as weekly and monthly volumes of sent messages, histogram of the number of recipients in their mes-sages, volumes of messages sent as a reply (indicated by  X  X e: X  in the subject line), volumes of messages sent as forward (with FW: in the subject line). The inbound features for the n -th sender cover the sender X  X  incoming activity, such as volume of the messages received by the sender, as regular, reply or forward messages. The action features for the n -th sender cover the activity of the sender X  X  email recipients, such as how many times the messages from the n -th sender were read, deleted, replied to, forwarded, ended up in spam or any other folder. To create these features, raw counts are converted to percentages of total inbound volume (e.g., percentage of n -th sender messages moved to trash). The names of the folders to which a given sender messages have been moved, are especially interesting: they can be seen as a human label of a specific sender. Therefore, for the n -th sender, we also use the names of folders (with the counts of moves to that folder ) as folder features. We remove the folders that contain less than 100 different senders, as well as the 50 folders with the highest number of senders, including system folders ( X  X rash X ) and third-party services folders.
Temporal behavior features form a subcategory of the behavioral features. They reflect the frequency of specific actions over a given period of time. For instance, we record whether a sender sends more than X messages in an hour. These features are represented as a histogram, where X takes as values: 10, 60, 80, 100, 120. We refer to them as the burst features.

Many of the features described above are counts of actions or words. To handle large counts, we normalize the data at an instance level by using log(1+ x ( k )) instead of the original k -th feature value x ( k ). Features that represent percentages of total traffic (e.g. moves to trash, spam, etc.) are left in the 0 to 1 range. We consider three levels of aggregation, by message, sender Table 3: Generalization loss in domain-level categorization and domain. To choose the right level of aggregation for our system, we consider what we lose and what we gain by ag-gregating to the next level, starting from the level of the email message. We consider several dimensions: generaliza-tion loss, data size and classification scalability.
Generalization loss: Aggregating messages offers some risks, if by aggregating we put in the same class messages that should have been mapped to different categories. One typical mistake that occurs when aggregating by sender oc-curs when a given sender changes its behavior. It happens for instance when a sender (such as a recruiter, finance bro-ker or a travel agent) sends machine generated messages to clients in 90% of the cases, and personal messages otherwise. We refer to this issue as the machine-human mix . In some rare cases, we observed another type of loss we call the mul-tiple businesses loss , where a sender is a company that has more than one type of business and sends all messages from the same address. Although many companies conduct mul-tiple types of business, they typically use different addresses for each type, hence the multiple businesses loss is quite low. Domain-level aggregation, as the most aggressive type, suf-fers the most loss; in particular the multiple businesses loss is quite common there. Table 3 shows some examples of generalization loss due to aggregating at a domain level.
Data size: Data size is a key factor in deciding the right aggregation level as learning and categorizing at this scale is most challenging. Table 4 shows the number of data points in our dataset for the various aggregation levels. As ex-pected sender-level aggregation drastically reduces the data size, going from more than one trillion messages to about 330 million senders after aggressive filtering. Domain-level ag-gregation brings an additional decrease, bringing us to about 75 million domains, a significant drop but not as drastic as the one brought by sender-level aggregation.

Classification scalability: The highest gain in scala-bility is between sender and message aggregation. Indeed, extracting features from each message and running a classi-fier upon delivery might be extremely costly. On the other hand, mapping each message into its sender presents huge performance advantages. The classification process can be conducted offline on all the previously seen senders, and an incoming message can be assigned the category associated with its sender via a simple (yet very large) table lookup.
Consequently, we propose here a two-stage approach. The majority of the email traffic should be classified by sender for performance optimization. However, whenever the classifier returns a low confidence score for a given message sender, the more costly message-based classification should be applied. This approach is detailed in Section 5.
One key challenge in Web mail classification is to generate a labeled training dataset given the inherent size of the data as well as privacy issues. We consider here 3 types of labeling techniques: manual, heuristic-based and automatic, which we discuss below. We used as labels our 6 latent categories, as well as the human and machine labels to differentiate between human-and machine-generated messages.
Manual labeling consists of having human editors assign labels to specific examples. This method has some clear weaknesses. Given the data sensitivity, even with users that agreed having their mail examined, only a limited number of responsible editors can be involved. Then it becomes more difficult to have multiple editors looking at the same data, which is a must given the nature of the task. Indeed, look-ing at a confusion matrix, we observed many inconsistencies among editors, for instance between Shopping and Finance or Travel and Finance . We nevertheless generated such a dataset, denoted as D man , by giving to 5 paid editors a pool of about 18 , 000 messages originating from about 14 , 000 senders. This pool contained multiple messages per sender, with some overlap between editors X  assignments. Close to 400 inconsistent labels were generated, which were resolved via additional editor intervention.
Heuristic labeling consists of applying heuristic rules de-rived from world knowledge. Such labeling achieves high precision but is limited in coverage. We used this type of labeling mostly for differentiating between human and machine senders. One key challenge here lies in the fact that SMTP domain information is not sufficient. Indeed, large Web mail services are not only used by humans. Non-spam machine-generated messages can originate from do-mains such as gmail.com , hotmail.com , or yahoo.com , even for small distribution volumes. Conversely, domains such as amazon.com or ibm.com, intel.com or hp.com can be associated with both machine and personal commu-nications of corporate employees. We focused on corporate domains as they represent the main source of errors between human and machine labels, with the following simple heuris-tics. In order to identify corporate machine senders, we spot-ted reserved words such as mailer-daemon or no-reply , or repeating occurrences of words such as unsubscribe in message headers. We also used signals such as spam votes. In order to identify human senders, we looked for pat-terns such as &lt;first name&gt;.&lt;lastname&gt; and other var-Algorithm 1 Folder-based voting for assigning labels to senders ious regular expressions. We also applied a semi-supervised approach similar to co-training [5], in order to further in-crease our heuristic dataset, remove false positives, and most importantly obtain a less skewed sample. In a nutshell, the idea is to split the features into two independent sets, train a classifier on one set, then use the labels with high confi-dence to train a classifier on the second set, then possibly iterate on all features with all of the labels. Eventually, a heuristic dataset denoted D h  X  m was generated, which con-sists of 60 , 000 human senders and 80 , 000 machine senders. Most of the machine senders were further assigned machine latent categories as labels, using the techniques outlined be-low. The remaining ones were labeled as Other in the final dataset used for training of the production model.
For automatic labeling we apply two types of techniques, folder-based majority and LDA voting.

Folder-based majority voting (see Algorithm 1) lever-ages user-generated folders to obtain ground truth labels for senders. There are hundreds of folder names that relate to  X  X hopping X ,  X  X inance X , or  X  X ravel X , etc. Since there are much fewer folders than there are senders, we manually labeled close to 600 folders that carry the meaning of our latent categories, and let the folders vote for senders label. A sub-set of the labeled folders is shown in Table 5. Note that we did not include the Human category since we hardy ob-served any folder that would be perfectly aligned with it. Even our previous example of mom and dad folder in Ta-ble 2 includes machine-generated messages as evidenced by its strong Finance topic. We also saw multiple examples of human folders, related to family and friends, that contained messages from known vendors or travel companies. The la-beling procedure is detailed in Algorithm 1, with threshold parameters chosen via grid search.

For each sender, we identify the folders with labels that contain messages originating from this sender. Then, the folders vote for the labels. If there is enough confidence for the winning label, as per threshold, the sender is labeled accordingly. Several examples of the labeled senders with individual folder votes are given in Table 7. After applying this procedure to the entire dataset of senders, we end up with more than 81 , 000 labeled senders that form D the process of merging D f with D man and D h  X  m , there were approximately 5 , 000 matches (same label assigned) and 700 conflicts (different label assigned). Senders that were labeled as machine in D h  X  m and were assigned a machine latent category in D f or D man , were not counted as conflicts. The conflicts were resolved by human intervention. We denote the resulting merged dataset by D v 1 .

Folder-based LDA voting is the second technique we used in order to scale-up the majority vote technique. It is needed mostly because the majority of folders cannot be labeled, either because the folder name is not descriptive enough or it contains a mix of messages. As one of the results of LDA training, we know the topic distributions of 100 , 000 folders counting more than 1 , 000 messages, as illustrated in Table 2. We used these topics for  X  X oft voting X : instead of assigning 1 vote, each folder assigns partial votes to the senders of the messages it contains, using topic weights. We then re-normalized the votes to sum up to 1 in order to create a labeled set from the senders with a topic score above 80%. This process produced 42 , 000 labeled senders that already existed in D v 1 as well as 56 , 000 additional new senders. The labels of senders that were already in D v 1 mostly agreed with the existing ones, with only 121 conflicts, not counting the machine labels that received a subcategory label. The final, merged dataset D v 2 contains 210 , 000 senders. Table 6 shows the counts of human and machine labels in D v 2 .
The classification mechanism put in place has two major elements. The first is the online mechanism, designed in a cascading manner mainly to account for scalability. The online system leverages both message-level and sender-level classifiers in order to categorize incoming messages, while taking into consideration the strict requirements of a real time Web mail system. The system has three stages: (1) lightweight classification, (2) sender-based classification, and (3) heavy-weight message-based classification. The second component is the offline component where (1) we periodi-cally classify the known email senders into the 6 categories and (2) train a multi-label message classifier. The online system diagram is given in Figure 1. We describe its com-ponents in detail below.

Online lightweight classification: The initial classifi-cation is at a message-level, consisting of hard-coded rules designed to quickly classify a significant portion of email traffic. Easy cases include messages from the top 100 senders that cover a significant percentage of the total traffic and are category consistent. Also, as a heuristic decision, we categorized all reply/forward messages as human. By tak-ing precedence over sender-level, message-level classification
Table 7: Examples of folder voting (majority and LDA) helps avoid generalization errors due to mixed human-machine senders. As for performance, the process described requires very few resources and covers 32% of the email traffic.
Online sender-based classification: The second phase in our cascade classification process involves looking for the sender in a lookup table containing senders with known cat-egories. This table is created in our offline component, de-scribed below. A sender that does not appear in said table is absent due to one of the two reasons; (1) it was either never, or hardly seen in the past meaning that the sender features are too sparse and noisy to be useful, or (2) the offline classification process did not have an answer with a sufficient confidence. The second type of missing senders are those with the most chance of being mixed, meaning that the messages originating from them should not all be as-signed the same category. The amount of traffic that is not covered by this phase is roughly 8%.

Online Heavy-weight classification: Email messages whose sender did not appear in the classified sender table are sent to a heavy-weight message based classifier. As only 8% of the traffic end up in this last phase we can afford slightly heavier computations than we would have afforded had we employed the classifier on all incoming email. Here, we use all relevant feature, pertaining to the message body, subject line and sender name.

Offline creation of classified senders table: We use the training set D v 2 described in Section 4 in order to train a logistic regression model. The details of the implementation for both training and classification are detailed in Section 6. For each category we train a separate model in a one-vs-all manner, meaning that the senders assigned any different la-bel are considered negative examples of equal weights. The classification process is run performed periodically to ac-count for new senders, and modified behavior patterns (the features of the senders are refreshed at each run) of existing senders. We output for each sender a category and confi-dence score; if the confidence is sufficiently high then the sender enters the table. The process consists of the follow-ing: All 6 classifiers (one for each category) are run on all of the senders in our data base. To determine the category of a sender we choose, from the classifiers returning a pos-itive answer, the one with the highest confidence, and set this score as the final confidence. If all of the classifiers returned a negative answer, the chosen category is  X  X ther X , meaning machine generated that does not fit our predefined categories; the category confidence is set as the lowest con-fidence score of the individual classifiers.

Offline training the message-level classifier: This task is not done periodically but is done once based on the labeled data described in Section 4. The training process is quite similar to the sender classification in the sense that a logistic regression model is trained for each category in a one-vs-all model. The major difference is the training set, which is of course different as it contains messages rather than senders. To obtain it, from each sender in D choose 5 random email messages, and assign them the label of their sender. The features associated with the messages do not include any data on the sender. The other difference when compared to the sender-based classifier is that here we do not allow a non-decision, as this component is used in the final level of the online cascade classifier. Hence, there is no issue of having sufficiently large confidence. Due to space constraints, we do not elaborate on this process further but note that it is quite similar to the sender classification process and has relatively low impact (only 8% of the traffic is run through this process).
For the production system we trained 5 sender-based clas-sifiers for machine latent categories: Shopping, Financial, Travel, Career and Social , and 1 sender-based machine vs. human classifier. This section covers the main experimental results of our email categorization study, demonstrating how well the category classifiers generalize on a holdout dataset.
We start by discussing the data distribution required for testing. The most intuitive sampling is uniform sampling, in which each incoming message is assigned the same weight and the system is evaluated accordingly. In the machine-generated latent categories, our experiments did not con-sider this distribution as it would rate our system way too favorably: even an average system becomes indistinguish-able from an excellent one due to the large volume of mes-sages sent by top senders, as demonstrated in Figure 5.
Specifically, a small number of senders are responsible for the majority of the email traffic, hence a system that avoids classifying the low-weight senders might achieve a good score, but still incur a bad user experience. One ex-ample for this phenomenon is in the social category. Here, Facebook contains only a handful of (canonized) senders
We define a canonized sender as a regular expression over senders, such that all senders matching it are essentially the same. In the context of Facebook, an example would be update+. * @facebookmail.com yet is in charge of a huge portion of the email traffic in the social category; as a matter of fact, setting a classifier for social that only outputs  X  X rue X  for a message from Face-book would reach reasonable results when measured w.r.t the uniform distribution over email messages; clearly such a system would be horrible in terms of user X  X  experience. The distribution we use for testing is thus the uniform distribu-tion over email senders, where canonized email senders e.g. . * noreply@xyz.com are viewed as a single sender. Due to skewed data and privacy issues, we did not rely on a manu-ally labeled data set but rather on the data set D v 2 (ignoring the senders marked as human) described in Section 4. Al-though this set does not contain an actual uniform sample of senders, it covers senders whose categories are interesting to users, as their associated messages were manually assigned to folders. We mention that the machine sub-categories in the labeled set where chosen solely based on the folder data. For this reason, we excluded the folder features from the dataset. This was required to avoid over-fitting.

For the human-vs-machine classifier the training was per-formed on the set D v 2 ; however we used a different test set. The main reason is that unlike machine-labeled senders, human-labeled senders are not based on folder data but rather on features that we do use in our classification pro-cess. For this reason, testing on D v 2 would provide unfair re-sults. Instead, we sampled more than 600 senders uniformly from the pool of senders, then from each sender we sampled 1 email message thus creating a test set of more than 600l messages. The samples were made from the email repository after excluding easy-to-verify machine traffic such as senders whose outgoing traffic is larger than 100 , 000 messages per month, or senders that were never replied to despite the fact that they sent over 1 , 000 messages. This initial exclusion was necessary as human traffic is rather small compared to machine meaning that in order to get sufficiently many hu-man examples without exclusion we would have required a large amount of manual labor. We verified that a negligible amount of these excluded messages were written by a human by manually observing that out of 200 uniformly drawn ex-cluded examples none were human generated.

For the machine sub-category we randomly split D v 2 into a training set, used to train the models, and a test set, used to evaluate the models. Since a single domain may have multiple senders, they were all put in either the training or the test set. This way, we can test whether the model truly generalizes well outside of the domains it observed in training. The labeled dataset was split into 65% for training and 35% for testing purposes. The procedure was repeated 5 times. The average numbers for the 5 splits were reported.
In our experiments, as well as in the production system, we leveraged the MapReduce paradigm [8], implemented in the Hadoop 6 open source platform. All of our data process-ing, including aggregation of messages per sender, feature extraction, training and scoring, were done under Hadoop. The models were trained in a single MapReduce job, where the mapper reads D v 2 , and prints every data point 6 times, once for each problem at hand, playing a role of either a pos-itive or a negative example in the corresponding problems. The reducer trains a separate binary classification model: for each problem j  X  { Shopping, Financial, Travel, Career, Social, Human } . We used a logistic regression model, where the feature vector x is parameterized using a weight vec-tor w , with one weight per modeled feature. Specifically, we utilized the highly scalable Vowpal Wabbit 7 implemen-tation of logistic regression [14], which was found to work well in conjunction with MapReduce. In these experiments, the categorization was treated as a multi-class  X  X ne-vs.-all X  problem. Therefore, an additional step of merging the pre-dictions is required, where the final prediction is made based on the class with the highest probability.

In Figure 2, we show the averaged results achieved on the test set when classifiers were trained using all feature types except folders. The figure shows the Receiver Operat-ing Characteristic (ROC) curve that trades-off True Positive Rate (TPR) and False Positive Rate (FPR) as well as the Precision Recall (PR) curve that trades-off Precision and Recall. It can be observed that the models achieved desir-able performance, with Precision and Recall numbers both in the 0 . 9 range in most cases. Overall, the performance sug-gests that even with a very low FPR threshold, e.g. 0 . 5%, set for production purposes, large portions of true positives can be covered, i.e. , more than 85% in most cases.
To evaluate the influence of the feature types, table 8 shows the Area Under the ROC curve (AUC) results for each class, when our model was trained on subsets of features: (1) content features (email body, subject, etc.), (2) address features (subname, subdomain, commercial keywords) and (3) behavioral features (number of messages, sent, received, etc.) without folders. It should be noted that temporal be-havior features (burst features) were treated as behavioral features in this experiment. Even though there is an evi-dent drop when compared to using all feature types, content and address feature subsets achieved competent AUC per-formance that around 0 . 9. As expected, behavioral features showed good performance in distinguishing between human and machine senders, but could not on their own discrimi-nate between different machine categories.

The second portion of Table 8 shows the performance when some features were removed. Specifically, we were in-terested in the performance drop when burst features, and body words features were removed.

Figure 3 gives some insights on the human vs. machine classification. Figure 3a shows the histogram of the read ra-tio R rr for human and machine senders, where R rr is defined
A clear difference in the distribution can be observed. Fur-thermore, Figures 3b and 3c show to what extent certain binary features are observed in the human examples versus the machine examples. It can be observed that when the word unsubscribe is present in the message body, it is much more likely to be associated with a machine sender. Also, when common first names, such as  X  X ichael X ,  X  X usan X , etc., are present in the email address, they are much more likely to be associated with a human sender. Finally, Fig-(c) Common name occurrence ure 3d shows the presence of  X  X urst 100 X  feature, i.e. indica-tor of whether or not the sender sent more than 100 messages per hour on some occasion. It can be observed that the fea-ture is present in more than half machine senders and in very few human senders. Other interesting facts that were not depicted in the figures are that: 1) 34 . 56% of machine senders send messages with average subject character count higher than 30, while only 5 . 88% of human senders do the same; 2) 77 . 55% of machine senders send messages with av-erage body character count higher than 300, while only 1% of human senders do the same; and 3) 41 . 09% of machine senders send messages with more than 3 urls in the body on average, while only 2 . 24% of human senders do the same.
To get more insight into the latent category models, we generated feature clouds for j  X  { Shopping, Financial, Travel, Career } as illustrated in Figure 4. Features of higher  X  X mportance X  are printed using larger fonts. In Logistic Re-gression, large positive feature weights w are associated with a higher likelihood of user belonging to class j . Therefore, to calculate the feature importance scores with respect to class j we use the following procedure. First, we isolate only the features that have a positive weight in j -th class model w j . Next, we calculate the score for each of those number of examples, N + is the total number of class j ex-amples, N k is the number of examples that have feature k and N + k is the number of class j examples that have feature k . Finally, we generate a word cloud using the 1 , 000 fea-tures with the highest score. By examining the figures we can conclude that the main concepts of each class are very well captured using indicative body and subject words, as well as the address substrings.
In this section, we discuss the significance and potential impact of our mail categorization system. We estimated the coverage of latent categories in two different manners. First, we measured overall email traffic coverage by estimating the percentage of email messages that are mapped into one of our categories. Then, we measured how these categories cover mail search by manually categorizing a sample of mail search queries. The results are given in Table 9.

For email traffic coverage, we ran our classification sys-tem over real email traffic and counted the number of mes-sages classified in each category. The percentages listed in the email traffic column in Table 9 represent the number of messages classified as topic X divided by the total number of messages. For the search query coverage, a team of profes-sional editors manually labeled 2,500 common queries. To ensure privacy, we chose only queries that were used by sev-eral users, and in particular avoided queries that are unique to a single user. The percentages listed in the search query column in Table 9 represent the number of queries classified as topic X divided by the total number of queries. Table 10 gives a description of the typical queries for each category.
Table 9 clearly illustrates the fact that coverage by email traffic and coverage by search query are very different in nature. One notable difference is the human category that attracts (somewhat unsurprisingly, given our previous com-ments on the dominance of machine-generated traffic) a dis-proportionate amount of searches as compared to its rela-tively small email volume. The finance category is similar although the ratio between the coverages is not as large as in the human category. An opposite example is that of social networks. Despite their huge coverage of the email traffic, the coverage in terms of search queries is much lower. In-deed when considering the type of messages sent by social sites, these are mostly updates and summaries of the re-cent events, hence are typically not messages that will be read more than once, thus will not be searched for. Over-all, the coverage of the latent categories both in terms of email traffic and search queries is larger than 70%. These figures confirm the fact that latent categories could be used not only for browsing but probably even more for searching, and thus answer the needs of the two traditional discovery paradigms [6] since the early days of the Web.
We presented here a Web-scale categorization approach that combines offline learning and online classification com-ponents. One of our key contributions is the identification of categories common to all users. We discovered latent cat-egories by conducting a large-scale analysis of user folder data, which highligted the distinction between human and machine-generated email. Our categories cover more than 70% of both email traffic and email search queries. Our classification mechanism achieved precision and recall rates close to 90%. These results are achieved via an extremely scalable online system that assigns a category to incoming messages delivered to any user, including those who never defined a folder. We believe that this study shows a great deal of promises for this domain, as it demonstrates that email classification can be applied in production systems of the scale of Web mail.

Discussing how categories should be exposed to users, if at all, to users is out of the scope of this work but is clearly a critical challenge. Following the traditional Web discov-ery paradigms, they could be surfaced by explicit categories and search facets (for browsing) or behind the scenes, as an additional search signal. We hope this work will encour-age other researchers to build upon these categories in order to explore new email discovery paradigms. On the back-end side, which remains our focus, we plan to investigate whether sub-types within latent categories could be discov-ered, e.g., Travel promotions under Travel . We also intend to explore the  X  X ender cold-start X  issue, which occurs when a new sender appears, and its associated messages are too rare for us to conduct appropriate learning. As new senders keep appearing, we will need to devise appropriate methods to handle new senders in order to maintain quality. We are grateful to Andrei Broder for inspiring this work. Yehuda Koren and Roman Sandler spent a huge number of hours manipulating thousands of folders. We owe a great deal to Edo Liberty, who discovered the power of machine-generated email, and Nemanja Djuric who conducted LDA experiments during his internship at Yahoo. Finally, this work would not be possible without the constant support of the Mail engineering and product teams at Yahoo. [1] Nir Ailon, Zohar S. Karnin, Edo Liberty, and Yoelle [2] Inge Alberts and Dominic Forest. Email pragmatics [3] Olle B  X  alter. Keystroke level analysis of email message [4] David M Blei, Andrew Y Ng, and Michael I Jordan. [5] Avrim Blum and Tom Mitchell. Combining labeled [6] C. M. Bowman, P. B. Danzig, U. Manber, and M. F. [7] Jake D Brutlag and Christopher Meek. Challenges of [8] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: [9] Matthew Hoffman, Francis R Bach, and David M Blei. [10] Svetlana Kiritchenko and Stan Matwin. Email [11] J. Klensin. Simple mail transfer protocol, rfc 2821, [12] Bryan Klimt and Yiming Yang. The enron corpus: A [13] Yehuda Koren, Edo Liberty, Yoelle Maarek, and [14] John Langford, Lihong Li, and Tong Zhang. Sparse [15] Jefferson Provost. Na X ve-bayes vs. rule-learning in [16] Fabrizio Sebastiani. Machine learning in automated
