 How do we find a natural clustering of a real world point set, which contains an unknown number of clusters with different shapes, and which may be contaminated by noise? Most clustering algorithms were designed with certain as-sumptions (Gaussianity), they often require the user to give input parameters, and they are sensitive to noise. In this pa-per, we propose a robust framework for determining a nat-ural clustering of a given data set, based on the minimum description length (MDL) principle. The proposed frame-work, Robust Information-theoretic Clustering (RIC) , is or-thogonal to any known clustering algorithm: given a pre-liminary clustering, RIC purifies these clusters from noise, and adjusts the clusterings such that it simultaneously de-termines the most natural amount and shape (subspace) of the clusters. Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering. In fact, RIC is even able to purify and improve an initial coarse clustering, even if we start with very simple methods such as grid-based space partitioning. Moreover, RIC scales well with the data set size. Extensive experiments on synthetic and real world data sets validate the proposed RIC frame-work.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms, Performance Clustering, Noise-robustness, Parameter-free Data Mining, Data Summarization Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. Figure 1: A fictitious dataset, (a) with a good clus-tering of one Gaussian cluster, one sub-space clus-ter, and noise; and (b) a bad clustering.
The problem of clustering has attracted a huge volume of attention for several decades, with multiple books [ 11, 20], surveys [ 13] and papers (X-means [ 16], G-means [ 10], CLARANS [ 15], CURE [ 9], CLIQUE [ 2], BIRCH [ 22], DB-SCAN [ 7], to name a few). Recent interest in clustering has been on finding clusters that have non-Gaussian correlations in subspaces of the attributes, e. g. [ 5, 19, 1]. Finding corre-lation clusters has diverse applications ranging from spatial databases to bio-informatics. The hard part of clustering is to decide what is a good group of clusters, and which data points to label as outliers and thus ignore from clustering. For example, in Figure 1, we show a fictitious set of points in 2-d. Figure 1(a) shows a grouping of points that most hu-mans would agree is  X  X ood X : a Gaussian-like cluster at the left, a line-like cluster at the right, and a few noise points ( X  X utliers X ) scattered throughout. However, typical cluster-ing algorithms, like K-means may produce a clustering like the one in Figure 1(b): a bad number of clusters (five, in this example), with Gaussian-like shapes, fooled by a few outliers. There are two questions we try to answer in this work: Q1: goodness How can we quantify the  X  X oodness X  of a Q2: efficiency How can we write an algorithm that will The overview and contributions, of this paper, are exactly the answers to the above two questions: For the first, we propose to envision the problem of clustering as a com-pression problem and use information-theoretic arguments. The grouping of Figure 1(a) is  X  X ood X , because it can suc-cinctly describe the given dataset, with few exceptions: The points of the left cluster can be described by their (short) distances from the cluster center; the points on the right line-like cluster can be described by just one coordinate (the location on the line), instead of two; the remaining outliers each need two coordinates, with near-random (and thus un-compressible) values. Our proposal is to measure the good-ness of a grouping as the Volume after Compression (VAC): that is, record the bytes to describe the number of clus-ters k ; the bytes to record their type (Gaussian, line-like, or something else, from a fixed vocabulary of distributions); the bytes to describe the parameters of each distribution (e.g., mean, variance, covariance, slope, intercept) and then the location of each point, compressed according to the dis-tribution it belongs to.
 Notice that the VAC criterion does not specify how to find a good grouping; it can only say which of two groupings is better. This brings us to the next contribution of this pa-per: We propose to start from a sub-optimal grouping (e.g., using K-means, with some arbitrary k ). Then, we propose to use two novel algorithms: We continue fitting and merging, until our VAC criterion reaches a plateau. The sketch of our algorithm above has a gradient descent flavor. Notice that we can use any and all of the known optimization methods, like simulated anneal-ing, genetic algorithms, and everything else that we want: our goal is to optimize our VAC criterion, within the user-acceptable time frame. We propose the gradient-descent ver-sion, because we believe it strikes a good balance between speed of computation and cluster quality.
The proposed method, RIC, answers both questions that we stated earlier: For cluster quality, it uses the information-theoretic VAC criterion; for searching, it uses the two new algorithms (Robust Fit, and Cluster Merge). The resulting method has the following advantages: 1. It is fully automatic, i.e. no difficult or sensitive para-2. It returns a natural partitioning of the data set, thanks 3. It can detect clusters beyond Gaussians: clusters in 4. It can assign model distribution functions such as uni-5. It is robust against noise. Our Robust Fitting (RF) 6. It is space and time efficient, and thus scalable to large To the best of our knowledge, no other clustering method meets all of the above properties. The rest of the paper is or-ganized as follows: Section 2 gives a brief survey of the large previous work. Section 3 describes our proposed framework and algorithms. Section 4 illustrates our algorithms on real and synthetic data and Section 5 concludes our paper.
As mentioned, clustering has attracted a huge volume of interest over the past several decades. Recently, there are several papers focusing on scalable clustering algorithms, e. g. CLARANS [ 15], CURE [ 9], CLIQUE [ 2], BIRCH [ 22], DBSCAN [ 7] and OPTICS [ 3]. There are also algorithms that try to use no user-defined parameters, like X-means [16] and G-means [ 10]. However, they all suffer from one or more of the following drawbacks: they focus on spherical or Gaussian clusters, and/or they are sensitive to outliers, and/or they need user-defined thresholds and parameters. Gaussian clusters: Most algorithms are geared towards Gaussian, or plain spherical clusters: For example, the well known K-means algorithm, BIRCH [ 22] (which is suitable for spherical clusters), X-means [ 16] and G-means [ 10]. These algorithms tend to be sensitive to outliers, because they try to optimize the log-likelihood of a Gaussian, which is equiv-alent to the Euclidean (or Mahalanobis) distance -either way, an outlier has high impact on the clustering. Non-Gaussian clusters: Density based clustering meth-ods, such as DBSCAN and OPTICS can detect clusters of arbitrary shape and data distribution and are robust against noise. For DBSCAN the user has to select a density threshold, and also for OPTICS to derive clusters from the reachability-plot. K-harmonic means [ 21] avoids the prob-lem of outliers, but still needs k . Spectral clustering al-gorithms [ 14] perform K-means or similar algorithms after decomposing the n  X  n gram matrix of the data (typically us-ing PCA). Clusters of arbitrary shape in the original space correspond to Gaussian clusters in the transformed space. Here also k needs to be selected by the user. Recent in-terest in clustering has been on finding clusters that have non-Gaussian correlations in subspaces of the attributes [ 5, 19, 1]. Finding correlation clusters has diverse applications ranging from spatial databases to bio-informatics. Parameter-free methods: A disproportionately small num-ber of papers has focused on the subtle, but important prob-lem of choosing k , the number clusters to shoot for. Such methods include the above mentioned X-means [ 16] and G-means [ 10], which try to balance the (Gaussian) likelihood error with the model complexity. Both X-means and G-means are extensions of the K-means algorithm, which can only find Gaussian clusters and cannot handle correlation clusters and outliers. Instead, they will force correlation clusters into un-natural, Gaussian-like clusters. In our opinion, the most intuitive criterion is based on infor-mation theory and compression. There is a family of closely related ideas, such as the Information Bottleneck Method [18], which is used by Slonim and Tishby for clustering terms and documents [ 17]. Based on information theory they de-rive a suitable distance function for co-clustering, but the number of clusters still needs to be specified in advance by the user.
 There are numerous information theoretic criterions for model selection, such as the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), and Minimum Description Language (MDL) [ 8]. Among them, MDL is the inspiration behind our VAC criterion, because MDL also envisions the size of total, lossless compression as a mea-sure of goodness. The idea behind AIC, BIC and MDL is to penalize model complexity, in addition to deviations from the cluster centers. However, MDL is a general frame-work, and it does not specify which distributions to shoot for (Gaussian, uniform, or Laplacian), nor how to search for a good fit. In fact, all four methods (BIC, G-means, X-means and RIC) are near-identical for the specific setting of noise-free mixture of Gaussians. The difference is that our RIC can also handle noise, as well as additional data distributions (uniform, etc.).
 PCA: Principal Component Analysis (PCA) is a powerful method for dimensionality reduction, and is optimal under the Euclidean norm. PCA assumes a Gaussian data distrib-ution and identifies the best hyper-plane to project the data onto, so that the Euclidean projection error is minimized. That is, PCA finds global correlation structures of a data set [ 12]. Recent work have extended PCA to identify local correlation structures that are linear [ 5] or nonlinear [ 19], however, some method-specific parameters such as neigh-borhood size or the dimensionality of microclusters, are still required. It is desirable to have a method that is efficient and robust to outliers, minimizing the need of pre-specified parameters.
The quality of a clustering usually depends on noise in the data set, wrong algorithm parameters (e.g., number of clusters), or limitations on the method used (e.g., unable to detect correlation clusters), and resulting in a un-natural partition of the data set. Given an intial clustering of a data set, how do we systematically adjust the clustering, over-come the influence of noise, recognize correlation patterns for cluster formation, and to eventually obtain a natural clustering? In this section, we introduce our proposed framework, RIC, for refining a clustering and discovering a most natural clus-tering of a data set. In particular, we propose a novel cri-terion, VAC, for determining the goodness of a cluster, and propose algorithms for: The proposed algorithms and the criterion VAC are de-scribed in details in the following subsections. Table 1 gives a list of symbols used in this paper.

The idea is to invent a compression scheme, and to de-clare as winner the method that minimizes the compression cost, including everything : the encoding for the number of clusters k , the encoding for the shape of each cluster (e.g., mean and covariance, if it is a Gaussian cluster), the encod-ings for the cluster-id and the (relative) coordinates of the data points.
 We assume that all coordinates are integers, since we have finite precision, anyway. That is, we assume that our data points are on a d -dimensional grid. The resolution of the grid can be chosen arbitrarily.
 The description of the method consists of the following parts: (a) how to encode integers (b) how to encode the points, once we determine that they belong in a given cluster. The idea is best illustrated with an example. Suppose we have the dataset of Figure 1. Suppose that the available distributions in our RIC framework are two: Gaussian, and uniform (within a Minimum Bounding Rectangle). Once we decide to assign a point to a cluster, we can store it more economically, by storing its offset from the center of the cluster, and using Huffman-like coding, since we know the distribution of points around the center of the cluster. Self-delimiting encoding of integers. The idea is that small integers will require fewer bytes: we use the Elias codes, or self-delimiting codes [ 6], where integer i is repre-sented using O (log i ) bits. As Table 2 shows, we can encode the length of the integer in unary (using log i zeros), and then the actual value, using log i more bits. Notice that the first bit of the value part is always  X 1 X , which helps us decode a string of integers, without ambiguity. The system can be easily extended to handle negative numbers, as well as zero itself. Encoding of points. Associated with each cluster C is the following information: Rotatedness R (either false or a orthonormal rotation matrix to decorrelate the cluster), and for each attribute (regardless if rotated or not) the type T (Gaussian, Laplacian, uniform) and parameters of the data distribution. Once we decide that point P belongs to cluster C , we can encode the point coordinates succinctly, exploit-ing the fact that it belongs to the known distribution. If p is the value of the probability density function for attribute P i then we need O (log 1 /p ) bits to encode it. For a white Gaussian distribution, this is proportional to the Euclidean distance; for an arbitrary Gaussian distribution, this is pro-portional to the Mahalanobis distance. For a uniform dis-tribution in, say, the Minimum Bounding Rectangle (MBR) ( lb i , ub i , with 0  X  i&lt;d and lb for lower bound, ub for up-per bound, respectively), the encoding will be proportional to the area of the MBR.
 The objective of this section is to develop a coding scheme for the points x of a cluster C which represents the points in a maximally compact way if the points belong to the cluster subspace and to the characteristic distribution functions of the cluster. Later, we will inversely define that probability density function which gives the highest compression rate to be the right choice. For this section, we assume that all at-tributes of the points of the cluster have been decorrelated by PCA, and that a distribution function along with the corresponding parameters has already been selected for each attribute. For the example in Figure 2 we have a Laplacian distribution for the x -coordinate and a Gaussian distribu-tion for the y -coordinate. Both distributions are assumed with  X  =3 . 5and  X  = 1. We need to assign code pat-terns to the coordinate values such that coordinate values with a high probability (such as 3 &lt;x&lt; 4) are assigned short patterns, and coordinate values with a low probability (such as y = 12 to give a more extreme example) are as-signed longer patterns. Provided that a coordinate is really distributed according to the assumed distribution function, Huffman codes optimize the overall compression of the data set. Huffman codes associate to each coordinate x i abit string of length l =log 2 (1 /P ( x i )) where P ( x i ) is the prob-ability of the (discretized) coordinate value. Let us fix this in the following definition:
Definition 1 (VAC of a point x ). Let x  X  R d be a point of a cluster C and of probability density functions which are associated to C . Each pdf i ( x i ) is selected from a set of predefined probabil-ity density functions with the corresponding parameters, i.e.  X  ,lb i ,ub i ,a i  X  R , X  i ,b i  X  R + .Let  X  be the grid constant (distance between grid cells). The VAC i (volume after com-pression) of coordinate i of point x corresponds to The VAC (volume after compression) of point x corresponds to In Figure 2 this is shown for the marked example point: The x -coordinate (between 2 and 3) has a probability of 19%. Thus, Huffman compression needs a total of log 2 (1 / 0 . 19) = 2 . 3 bits. The y -coordinate of this point is in a range of lower probability (5%) and needs a longer bit string (4.3 bits). In addition to 6.6 bits for the coordinates, the Huffman-coded cluster-Id is stored for each point with log 2 ( n/ | C | Naturally, the question arises to what extent this coding depends on the choice of the grid resolution. The absolute value of the code length clearly depends on the grid reso-lution. It can easily be shown that the code length of each coordinate is increased by 1 bit if the number of grid cells per dimension is doubled (  X  is divided by 2). This is in-dependent of the applied probability distribution function, number of clusters, etc. Since we only compare the VAC of different cluster structures, distribution functions, sub-spaces, etc, and leave the grid resolution at a constant level, high enough to distinguish the different points from each other, the overall result of our algorithm is not sensitive to the grid resolution.
 Next, we address the question, which set of probability den-sity functions has to be associated to a given cluster C .Our optimization goal is data compression, so we should, for each coordinate, select that pdf (and corresponding parameter setting) which minimizes the overall VAC of the cluster. It is well known that for a fixed type of pdf (e.g. Gaussian) the optimal parameter setting can correspond to the sta-tistics (e.g. mean, variance, boundaries) of the data set. Therefore, if the Gaussian pdf is selected for an attribute i , we use the mean and variance of the i -th coordinate of the points as parameters of the pdf . Likewise, for the Laplacian distribution, we apply a i =  X  i and b i =  X  i / uniform distribution, we apply the lower and upper limit of the range of the coordinate values. For the selection of the type of probability density function, we explicitly minimize the VAC of the cluster, i.e.:
Definition 2 (characteristic Let C be a cluster with points x  X  C .
 Let stat =(  X  i , X  i ,lb i ,ub i ,... ) be the statistics of the data required in the set of allowed probability density functions PDF .Then, For the x -coordinate of the example in Figure 2 that means the following: First required statistics, i.e. mean (3.5), vari-ance (1.0), and lower and upper limit (1.4, 6.2) of the data set is determined. Then, VAC x is determined for all allowed pdf Lapl (3 . 5 , 0 . 7) . The function yielding the lowest VAX selected. Then, the same is done for VAC y . Throughout the paper we focuss on three widespread distributions of high practical relevance: Gaussian, Laplacian and uniform. Definition 2 can easily be extended to other pdf -functions. Finally, we define when to use a decorrelation matrix. A decorrelation matrix is needed whenever a cluster is a cor-relation cluster, i.e. if one (or more) attribute value of the points of the cluster depends on the value of one (or more) other attribute. The decorrelation matrix can be gained from principal component analysis (PCA) of the d  X  d covari-ance matrix  X  of the points of the cluster and corresponds to the transpose of the orthonormal matrix V T gained from PCA diagonalization V  X  V T =  X . We give more details on estimating the covariance matrix in a noise robust way in Section 3.2. Decorrelating data can greatly reduce the VAC of the cluster because, instead of having two attributes with a high variance (which incurs high coding cost for any model pdf ) and a high correlation, we obtain two new vari-ables without any correlation, one having variance close to zero (VAC of almost 0 bit). Intuitively, we want to use a decorrelation matrix if (and only if) the VAC improvement is considerable. To obtain a fully automatic method without user-defined limits we use decorrelation iff the VAC savings at least compensate the effort of storing the decorrelation matrix:
Definition 3 (Decorrelation of a cluster). Let C be a cluster of points x (in the original coordinate system),  X  be a covariance matrix associated to C and V the decorrela-tion matrix obtained by PCA diagonalization of  X  .Let Y be the set of decorrelated points, i.e. for each y  X  Y : y = V Let  X   X  pdf ( y ) that of the decorrelated set Y . The decorrelation of C is dec ( C )= The information which of the two cases is true, is coded by 1 bit. The matrix V is coded using d  X  d floating values using f bits. The identity matrix needs no coding (0 bits): VAC ( dec ( C )) = The following definition puts these things together.
Definition 4 (Cluster Model). The cluster model of a cluster C is composed from the decorrelation dec ( C ) and the characteristic point x  X  C . The Volume After Compression of the cluster VAC ( C ) corresponds to
We consider the combination of the cluster X  X  subspace and the characteristic probability distribution as the clus-ter model . A data point in a (tentative) cluster could be either a core point or an outlier , where core points are de-fined as points in the cluster X  X  subspace which follow the characteristic probability distribution of the cluster model, while the outliers are points that do not follow the distrib-ution specified by the cluster model. We will also call the outliers noise (points) .
 Having outliers is one reason that prevents conventional clustering methods from finding the right cluster model (us-ing e.g. PCA). If the cluster model is known, filtering out-liers is relatively easy  X  just remove the points which fit the worst according to the cluster model. Likewise, determining the model when clusters are already purified from outliers is equally simple. What makes the problem difficult and in-teresting is that we have to filter outliers without knowledge of the cluster model and vice versa.
 Partitioning clustering algorithms such as those based on K-means or K-medoids typically produce clusters that are mixed with noise and core points. The quality of these clus-ters is hurt by the existence of noise, which lead to a biased estimation of the cluster model.
 We propose an algorithm for purifying a cluster that, after the processing, noise points are separated from their origi-nal cluster and form a cluster of their own. We start with
Figure 3: Conventional and robust estimation. a short overview of our purification method before going into the details. The procedure starts with getting as input a set of clusters C = { C 1 ,...,C k } by an arbitrary clustering method. Each cluster C i is purified one by one: First, the algorithm estimates an orthonormal matrix called decorre-lation matrix ( V ) to define the subspace of cluster C i decorrelation matrix defines a similarity measure (an ellip-soid) which can be used to determine the boundary that separates the core points and outliers. Our procedure will pick the boundary which corresponds to the lowest overall VAC value of all points in cluster C i . The noise points are then removed from the cluster and stored in a new cluster. Next, we elaborate on the steps for purifying a cluster of points.
The decorrelation matrix of a cluster C i contains the vec-tors that define (span) the space in which points in cluster C i reside. By diagonalizing the covariance matrix  X  of these points using PCA ( X  = V  X  V T ), we obtain an orthonormal Eigenvector matrix V , which we defined as the decorrelation matrix . The matrices V and  X  have the following proper-ties: the decorrelation matrix V spans the space of points in C , and all Eigenvalues in the diagonal matrix  X  are pos-itive. To measure the distance between two points x and y , taking into account the structure of the cluster, we use the Mahalanobis distance defined by  X  and V :
Given a cluster of points C with center  X  , the conventional way to estimate the covariance matrix  X  is by computing a matrix  X  C from points x  X  C by the following averaging: where ( x  X   X  )  X  ( x  X   X  ) T is the outer vector product of the centered data. In other words, the ( i, j )-entry of the ma-trix  X  C ,( X  C ) i,j , is the covariance between the i -th and j -th attributes, which is the product of the attribute values ( x i  X   X  i )  X  ( x j  X   X  j ), averaged over all data points x  X 
C is a d  X  d matrix where d is the dimension of the data space.
 The two main problems of this computation when confronted with clusters containing outliers are that (1) the centering step is very sensitive to outliers, i.e. outliers may heav-ily move the determined center away from the center of the core points, and (2) the covariances are heavily affected from wrongly centered data and from the outliers as well. Even a small number of outliers may thus completely change the complete decorrelation matrix. This effect can be seen in Figure 3 where the center has been wrongly estimated using the conventional estimation. In addition, the ellipsoid which shows the estimated  X  X ata spread X  corresponding to the co-variance matrix has a completely wrong direction which is not followed by the core points of the clusters.
 To improve the robustness of the estimation, we apply an averaging technique which is much more outlier robust than the arithmetic means: The coordinate-wise median. To cen-ter the data, we determine the median of each attribute in-dependently. The result is a data set where the origin is close to the center of the core points of the cluster (  X  rather than the center of all points (  X  ).
 A similar approach is applied for the covariance matrix: Here, each entry of the robust covariance matrix ( X  R ) i,j x of the cluster. The matrix  X  R reflects more faithfully the covariances of the core points, compared to the covariance matrix obtained by the arithmetic means.
 The arithmetic-mean covariance matrix  X  C has the diag-onal dominance property, where the each diagonal element  X  i,i is greater than the sum of the other elements of the row  X   X  ,i . The direct consequence is that all Eigenvalues in the corresponding diagonal matrix  X  are positive, which is essential for the definition of d  X  ( x, y ).
 However, the robust covariance matrix  X  R might not have the diagonal dominance property. If  X  R is not diagonally dominant, we can safely add a matrix  X   X  I to it without affecting the decorrelation matrix. The value  X  can be cho-sen as the maximum difference of all column sums and the corresponding diagonal element (plus some small value, say 10%): It can easily be seen that adding the matrix  X I does only af-fect the Eigenvalues and not the Eigenvectors: If  X  = V  X  V then  X  +  X I = V  X  V T +  X I . Since V is orthonormal,  X I can also be written as V X IV T , and due to the distributive lawwehave X +  X I = V ( X  +  X I ) V T , i.e. each Eigenvalue is increased by  X  and matrix V is unaffected by this operation. Using our robust estimation technique, the center in Fig-ure 3 is correctly positioned and the ellipsoid which repre-sents the covariance matrix follows the distribution of the core points. The safe decorrelation matrix V (cf. Figure 4) which has been generated from the safely estimated covari-ance matrix is composed from Eigenvectors which indicate the directions of maximum variance of the core of the clus-ter. When transforming the data by multiplication of V T we remove the correlations of the attributes. Note that we do not decide about a projection into a lower dimensional space at this stage, i.e. no information loss.
The first step of purifying a cluster of points is to iden-tify the proper decorrelation matrix. We generate several estimates (called candidates ) of the covariance matrix, us-ing various estimation methods, and pick the one with the best overall VAC value. In our experiments, the candidates include the matrix  X  C from the conventional method us-ing arithmetic average, matrix  X  R from the robust method described above. We also determine a conventional and a robust candidate, matrices  X  C, 50 and  X  R, 50 respectively, by considering only a certain percentage (e.g. 50%) of points in the cluster being closest to the robustly estimated center  X  . In addition, we always have the identity matrix I as one candidate decorrelation matrix. Among these matrices, our algorithm selects the matrix giving the best (lowest) over-all VAC. For our example in Figure 3, the diagram at the right shows that the lowest VAC value of 1480 is reached for robust estimation in contrast to 1600 for conventional estimation.
 The next step is to detect noise points in the cluster. By now, we have computed the robust center  X  R , and chosen a candidate covariance matrix, which we call  X   X  (the corre-sponding decorrelation matrix is V  X  ). The goal is to parti-tion the set of points in cluster C into two new sets: C core (for the core points) and C out (outliers). First, our method orders the points of C according to the Mahalanobis distance defined by the candidate covariance matrix  X   X  . Initially, we define all points to be outliers ( C out = C, C core = {} we iteratively remove points x from C out (according to Ma-halonobis sort order starting with the point closest to the center) and insert them into C core , and compute the coding costs before and after moving the point x .
 At each iteration, the point x being moved from C out to C core , is first projected to the space defined by the selected candidate decorrelation matrix V  X  . Then, the coding cost of the new configuration ( C core  X  X  x } ,C out  X  X  x } ) is de-termined as the cost where each of the coordinates is mod-eled using that distribution function which gives least coding costs. Outlier points are always coded using uniform distri-bution. So each of these configurations corresponds to one given radius of the ellipsoid partitioning the set into core and noise objects. The partition which had the least overall cost in this algorithm is finally returned (cf. Figure 3 where at the minimum (1480) we have 24 objects in the core set and 6 objects in the noise set). The diagram in Figure 3 depicts the VAC value (Y-axis) of the different configura-tion ( C core ,C out ) at each iteration (X-axis). Figure 3 shows two VAC-value curves, one for the conventional candidate decorrelation matrix ( V C ) and the other for the robust es-timation ( V R ). At the beginning, all points are regarded as noise points, yielding a VAC value of approximately 1800 for both candidate matrices. As more and more points are moved from C out to the set of core points C core ,theVAC value improves (decreases). For the robust decorrelation ma-trix ( V R ), the VAC value reaches the minimum of 1480 when there are 24 core points. After this, the VAC value increases again to approximately 1800.
Our RIC framework is designed to refine the result of any clustering algorithm (e.g., K-means). Due to imperfection of the clusters given by an algorithm, our cluster purifying algorithm may lead to redundant clusters containing noise objects that fit well to other neighboring noise clusters. In this section we describe our proposed cluster merging proce-(c) RIC result.
 (c) RIC result. dure in more detail, to correct the wrong cluster assignments caused by the original clustering algorithm.
 For example, the K-means clustering algorithm tends to partition data incorrectly, when the true clusters are non-compact. These clusters are often split up into several parts by K-means. A typical, inappropriate partitioning is shown in Figure 6(a) . Our algorithm corrects the wrong partitions by merging clusters that share common characteristics, takes into account the subspace orientation and data distribution. We use the proposed VAC value to evaluate how well two clusters fit together. The idea is to check whether the merg-ing of a pair of clusters could decrease the corresponding VAC values. Mathematically, let VAC ( C )betheVACvalue for a cluster C . We also define savedCost ( C i ,C j ) of a clus-ter pair ( C i ,C j )as savedCost ( C i ,C j )= VAC ( C i )+ VAC ( C j )  X  VAC ( C If savedCost ( C i ,C j ) &gt; 0, then we consider the cluster pair ( C i ,C j ) a potential pair for merging.
 Our proposed merging process is an iterative procedure. At each iteration, our algorithm merges the two clusters which have the maximum savedCost ( ., . ) value, resulting in a greedy search toward a clustering that has the mini-mum overall cost. To avoid this greedy algorithm from get-ting stuck in a local minimum, we do not stop immediately, even when there is no saving of savedCost ( ., . ) value can be achieved by merging pairs of clusters. That is, we do not stop when savedCost ( ., . )  X  0. Instead, the algorithm con-tinues for another t iterations, continuous to merge cluster pairs ( C i ,C j )withthemaximum savedCost ( C i ,C j )value, even though now the savedCost ( C i ,C j ) value is negative, and merging C i and C j will increase the VAC value of the overall data set. Whenever a new minimum is reached the counter is reset to zero. Pseudocode for the RIC algorithm is given in Figure 5.
Especially widespread K-means and K-medoid clustering methods often fail to separate clusters from noise and, there-fore produce results where the actual clusters are contam-inated by noise points. Figure 6(a) shows the result of K-means with k = 8 on a synthetic 2-d data set consisting of 4751 data objects. Two of the resulting clusters con-tain many noise objects, among them the one dimensional correlation cluster. In Figure 6(b) the result of the cluster purifying algorithm is depicted. Five of the eight initial clus-ters have been split up into clusters containing noise objects and clusters with core points. Three of the initial clusters contain only noise objects. No objects need to be filtered out, so these partitions remain unchanged. The purifying algorithm reduces the overall VAC from 78,956 to 78,222. As a building block we provide fully automatic noise fil-tering and outlier detection. Our approach is model based, supports subspace and correlation clusters and various data distributions. It provides a natural cut-off point for the property of being an outlier based on the coding cost. After the initial clusters have been purified our algorithm merges together clusters with common characteristics, such as common subspace orientation or data distribution. In the example depicted in Figure 6(a) the cluster in the center has been split up into three parts by K-means. This inappro-priate partitioning is corrected by the cluster merging algo-rithm (cf. Figure 6(c) ). Also the noise clusters generated by the previously applied cluster purifying algorithm are now merged. The resulting clustering in our example consists of four clusters. The cluster merging algorithm drastically reduces the VAC-score by removing redundant clusters. As a particular value-added over conventional clustering, RIC provides information on the data distribution of the coordinates. In our example, the x -coordinate of the correla-tion cluster (top left in Figure 6(d) ) is uniformly distributed, the y -coordinate Gaussian. Both coordinates of the top right cluster follow a Gaussian distribution. Both coordinates of the bottom left cluster are Laplacian and both coordinates of the bottom right cluster (representing the noise objects) are uniformly distributed.
 We demonstrate the performance of the cluster filtering and merging algorithm on a 3-d synthetic data containing 7500 data objects (cf. Figure 7). This data set consists of one plane (2-d correlation cluster, 2000 objects) and 3 lines (1-d correlation clusters, two with 2000 objects each, one with 1000 objects) and 500 noise objects. Note that one of the lines is embedded in the plane. Figure 7(a) shows the clus-tering result of K-means with k = 20. The correlation clus-ters are split up in several parts and the noise objects are distributed among all clusters. This initial clustering ob-tains a VAC-score of 202,078. After applying the cluster purifying and merging algorithm, we obtain a much better clustering result with VAC 153,393. 98.6% of the noise ob-jects are correctly assigned to the noise cluster. The plane is 94.6% pure and the lines, even the one embedded in the plane, are from 99.5% to 100% pure.
 The DBSCAN algorithm ( M inP ts =4 , =0 . 1) correctly detects the lines but fails to separate the plane from the noise objects, and creates many small clusters in dense ar-eas of the plane (cf. Figure 7(b) ). There are 34 initial clus-ters in total. This result has a VAC-score of 195,276. After the purifying and merging algorithm we obtain a VAC of 155,412 and a very similar result as depicted in 7(c) .This demonstrates that the RIC framework can be applied with various partitioning clustering methods. Since the data set has been artificially generated, we can determine the VAC for the ideal clustering (exactly corresponding to the gener-ated clusters): The VAC of the ideal clustering (151 637) is almost reached by RIC after K-means as well as RIC after DBSCAN.
 The gready fashion optimization process is efficient. We implemented the RIC algorithm in Java. Runtimes for the synthetic data sets are 147 s for the 2-d data set and 567 s for the 3-d data set on a PC with 3 GHz CPU and 1 GB RAM.
We evaluate the RIC framework using a high dimensional metabolic data set. This 14-dimensional data set (643 in-stances) was produced by modern screening methodologies
RIC+K-means 1 0 275
K-means k =2 1 0 222
RIC+spectral 1 2 282 spectral k =2 1 2 224 and represents 306 cases of PKU, a metabolic disorder, and 337 objects from a healthy control group. As initial cluster-ings, we used spectral clustering (with d = 12 dimensions), and K-means; in both cases we used k = 6 initial clusters. To evaluate class purity of the clusterings, we report IMP, the count of  X  impurities  X , defined as the count of minority points in each cluster. The initial clusterings have an IMP of 31 and a VAC of 77,822 for K-means and an IMP of 26 and a VAC of 78,184 for spectral clustering, respectively. Table 3 shows the the same quantities and the clustering results after we apply RIC. Notice that in all cases, RIC achieved every-thing we wanted: (a) it found the correct number of clus-ters, (b) it achieved better compression (lower VAC score, as expected). For comparison, we also show the results of K-means and spectral clustering, after setting k =2 (which gives an unfair advantage to them over RIC). Even so, notice that RIC achieves both lower VAC score, as well as better impurity count IMP. Using k = 2, both, K-means and spec-tral clustering assign many instances of class PKU to the cluster of the control group.
The data we considered here are image blocks in retinal images from the UCSB BioImage ( http://bioimage.ucsb. edu/ ) database. The blocks are taken from 219 images of retina under 9 different conditions (healthy, diseased, etc.). Each image is of size 512-by-768. We take non-overlapped pixel blocks (which are called tiles ) of size 64-by-64 from each image, and collect in 96 tiles per image, or 21,024 tiles in total. Each tile is represented as a vector of 7 features, Figure 8: (a): Visualizing the distribution of the 7-dimensional retinal image tiles. Each subfigure shows the distribution of two dimensions. The data set contains non-Gaussian clusters. (b): The 13 clusters found by RIC. Figures look best in color. Figure 9: Example clusters on retinal image tiles found by RIC. Figure 10: The white boxes in the two retinal images indicate example tiles in selected clusters. Left (a): Tiles at position A of cluster of Figure 9(a) Right (b): Tiles at position B of cluster of Figure 9(d) . Best viewed in color. exactly as suggested in [ 4]. Figure 8(a) visualizes the distri-bution of the image tiles. The distribution is viewed from all possible pairs of dimensions  X  the ( i, j )-subfigure plots the i -th dimension versus the j -th dimension. The histograms at the diagonal subfigures depict the distribution of values in each dimension. The retina image tiles clearly have a non-Gaussian distribution with correlation clusters. Some views show strong correlation patterns, for example, the view of the first and 5-th dimensions (the subfigure at the first row and the fifth column). In the following discussion, we will focus on the view of the first and 5-th dimensions, and show that our RIC framework is able to find the non-Gaussian, correlation clusters in this data set.
 Moreover, most of the coordinates in the detected clusters clearly show a supergaussian distribution, which is reported as Laplacian by RIC. Let us note that our framework is extensible and can incorporate every data distribution that has a pdf . Figure 8(b) shows the RIC clustering result on the retinal tiles, where points of a cluster are plotted with an unique symbol. In total, RIC produces 13 clusters for this data set. We plot each cluster separately in different figures, for better visualization of the clustering result. Some plots of individual clusters are shown in Figure 9(a)-(f). It can be easily seen that the proposed RIC method successfully finds the correlation clusters in this data set, and, unlike other methods like K-means, it will neither over-cluster nor under-cluster the data set.
 The question is: is there any biological meaning to the clus-ters derived by RIC? The answer is  X  X es X : Tiles from cluster (A) (see Figure 9(a) ) are shown in Figure 10(a), and tend to correspond to the so-called  X  X   X  uller cells X . Similarly, tiles from cluster (B) (see Figure 9(d) ) are shown in Figure 10(b), and tend to correspond to the so-called  X  X od photorecep-tors X .
 Specifically, Figure 10 shows the layers of cells of a cat X  X  retina. The red and green colors in the image indicate the distribution of two proteins ( X  X od opsin X  and  X  X FAP X ). In Figure 10(a), the white boxes highlight two tiles at position A of the cluster shown in Figure 9(a) . The image shows the situation of a layer-detached retina being treated with oxy-gen exposition. The tiles highlighted are  X  X   X  uller cells X , with protein GFAP propagated from the inner layer of the retina. In Figure 10(b), the white boxes highlight two tiles at posi-tion B of the cluster shown at Figure 9(d) . The image shows the case of a retina which has suffered layer detachment for 3 months. The tiles highlighted are the  X  X od photorecep-tors X , with the protein rod opsin redistributed into the cell bodies, which are typical for detached retinas.
 The point is that our clustering method, without any do-main knowledge, manages to derive groups of tiles that do have biological meaning (M  X  uller cells and rod photorecep-tors, respectively).
The contributions of this work are the answers to the two questions we posed in the introduction, organized in our RIC framework.
We show that our RIC framework is very flexible, with several desirable properties that previous clustering algo-rithms don X  X  have: More importantly, the RIC framework does not compete with existing (or future) clustering methods: in fact, it can benefit from them! If a clustering algorithm is good, our RIC framework will use its grouping as a starting point, it will try to improve on it (through the  X  X obust Fit X  and  X  X luster Merge X  algorithms), and, it will either improve it, or declare it as the winner. In short, the RIC framework can not lose -at worst, it will tie! We also presented experiments on real and synthetic data, where we showed that our RIC framework and algorithms give intuitive results, while typical clustering algorithms fail.
This material is based upon work supported by the Na-tional Science Foundation under Grants No. IIS-0209107, SENSOR-0329549, EF-0331657, IIS-0326322, IIS-0534205. This work is also supported in part by the Pennsylvania In-frastructure Technology Alliance (PITA), a partnership of Carnegie Mellon, Lehigh University and the Commonwealth of Pennsylvania X  X  Department of Community and Economic Development (DCED). Additional funding was provided by donations from Intel, NTT and Hewlett-Packard. Any opin-ions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not neces-sarily reflect the views of the National Science Foundation, or other funding parties. [1] C. C. Aggarwal and P. S. Yu. Finding generalized [2] R. Agrawal, J. Gehrke, D. Gunopulos, and [3] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and [4] A. Bhattacharya, V. Ljosa, J.-Y. Pan, M. R. Verardo, [5] C. B  X  ohm, K. Kailing, P. Kr  X  oger, and A. Zimek. [6] D. Chakrabarti, S. Papadimitriou, D. S. Modha, and [7] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [8] P. Gr  X  unwald. A tutorial introduction to the minimum [9] S. Guha, R. Rastogi, and K. Shim. CURE: An [10] G. Hamerly and C. Elkan. Learning the k in k-means. [11] J. A. Hartigan. Clustering Algorithms . John Wiley &amp; [12] I. Jolliffe. Principal Component Analysis . Springer [13] F. Murtagh. A survey of recent advances in [14] A. Ng, M. Jordan, and Y. Weiss. On spectral [15] R. T. Ng and J. Han. Efficient and effective clustering [16] D. Pelleg and A. Moore. X-means: Extending [17] N. Slonim and N. Tishby. Document clustering using [18] N. Tishby, F. C. Pereira, and W. Bialek. The [19] A. K. Tung, X. Xu, and B. C. Ooi. CURLER: Finding [20] C. Van-Rijsbergen. Information Retrieval .
 [21] B. Zhang, M. Hsu, and U. Dayal. K-harmonic means -[22] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH:
