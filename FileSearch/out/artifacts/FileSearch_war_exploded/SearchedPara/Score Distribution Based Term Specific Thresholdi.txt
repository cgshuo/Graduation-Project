 The availability of vast multimedia archives calls for solutions to efficiently search this data. Multi-media content also enables interesting applications which utilize multiple modalities, such as speech and video. Spoken term detection (STD) is a sub-field of speech retrieval, which locates occurrences of a query in a spoken archive. In this work, STD is used as a tool to segment and retrieve the signs in news videos for the hearing impaired based on speech information. After the location of the query is extracted with STD, the sign video correspond-ing to that time interval is displayed to the user. In addition to being used as a sign language dic-tionary this approach can also be used to automat-ically create annotated sign databases that can be utilized for training sign recognizers (Aran et al., 2008). For these applications the precision of the system is more important than its recall.

The classical STD approach consists of convert-ing the speech to word transcripts using large vocab-ulary continuous speech recognition (LVCSR) tools and extending classical information retrieval tech-niques to word transcripts. However, retrieval per-formance is highly dependent on the recognition er-rors. In this context, lattice indexing provides a means of reducing the effect of recognition errors by incorporating alternative transcriptions in a prob-abilistic framework. A system using lattices can also return the posterior probability of a query as a de-tection score. Various operating points can be ob-tained by comparing the detection scores to a thresh-old. In addition to using a global detection thresh-old, choosing term specific thresholds that optimize the STD evaluation metric known as Term-Weighted Value (TWV) was recently proposed (Miller et al., 2007). A similar approach which trains a neural net-work mapping various features to the target classes was used in (Vergyri et al., 2007).
 The rest of the paper is organized as follows. In Section 2 we explain the methods used for spo-ken term detection. These include the indexing and search framework based on WFSTs and the detec-tion framework based on posterior score distribu-tions. In Section 3 we describe our experimental setup and present the results. Finally, in Section 4 we summarize our contributions and discuss possi-ble future directions. The STD system used in this study consists of four stages. In the first stage, an LVCSR system is used to generate lattices from speech. In the second stage the lattices are indexed for efficient retrieval. When a query is presented to the system a set of candidates ranked by posterior probabilities are obtained from the index. In the final stage, the posterior probabil-ities are compared to a threshold to decide which candidates should be returned. 2.1 Indexing and Retrieval using Finite-State General indexation of weighted automata (Allauzen et al., 2004) provides an efficient means of index-ing for STD (Parlak and Sarac  X lar, 2008; Can et al., 2009), where retrieval is based on the posterior prob-ability of a term in a given time interval. In this work, the weighted automata to be indexed are the preprocessed lattice outputs of the ASR system. The input labels are phones, the output labels are quan-tized time-intervals and the weights are normalized negative log probabilities. The index is represented as a WFST where each substring (factor) leads to a successful path over the input labels whenever that particular substring was observed. Output labels of these paths carry the time interval information fol-lowed by the utterance IDs. The path weights give the probability of each factor occurring in the spe-cific time interval of that utterance. The index is op-timized by WFST determinization and minimization so that the search complexity is linear in the sum of the query length and the number of times the query appears in the index. 2.2 Decision Mechanism Once a list of candidates ranked with respect to their posterior probabilities are determined using the in-dex, the candidates exceeding a threshold are re-turned by the system. The threshold is computed to minimize the Bayes risk. In this framework, we need to specify a cost function, prior probabilities and likelihood functions for each class. We choose the cost of a miss to be 1 and the cost of a false alarm to be a free parameter,  X  . The prior probabilities and the likelihood functions are estimated from the pos-terior scores of the candidate results for each query.
The likelihood functions are found by fitting para-metric models to the score distributions (Manmatha et al., 2001). In this study, the score distributions are modeled by exponential distributions. When the system returns a score, we do not know whether it belongs to the correct or incorrect group, so we use a mixture of two exponential distributions to model the posterior scores returned by the system. The exponential mixture model (EMM) parameters are determined via unsupervised estimation using the Expectation-Maximization (EM) algorithm. Fig-ure 1 shows the normalized histogram of posterior scores and the EM estimate given by our method for an example query.
If we denote the posterior score of each candidate by x , incorrect class by c 0 and correct class by c 1 , we have where the incorrect class likelihood p ( parameters  X  0 , X  1 ,P ( c 0 ) ,P ( c 1 ) are estimated using the EM algorithm given the scores x i for i = 1 ,...,N . Each iteration consists of first computing P ( c j | x i ) = P ( c j ) p ( x i | c j ) /p j = 1 , 2 and then updating After the mixture parameters are estimated, we as-sume that each mixture represents a class and mix-ture weights correspond to class priors. Then, the Minimum Bayes Risk (MBR) detection threshold for x is given as:  X  1 + log(  X  0 / X  1 ) + log( P ( c 0 ) /P ( c 1 )) + log  X  3.1 Data and Application Turkish Radio and Television Channel 2 (TRT2) broadcasts a news program for the hearing impaired which contains speech as well as signs. We have collected 11 hours (total speech time) of test ma-terial from this broadcast and performed our ex-periments on this data with a total of 10229 sin-gle word queries extracted from the reference tran-scriptions. We used IBM Attila speech recognition toolkit (Soltau et al., 2007) at the back-end of our system to produce recognition lattices. The ASR system is trained on 100 hours of speech and tran-scription data collected from various TV and radio broadcasts including TRT2 hearing impaired news, and a general text corpus of size 100 million words.
Our application uses the speech modality to re-trieve the signs corresponding to a text query. Re-trieved results are displayed as video demonstrations to support the learning of sign language. Since the application acts like an interactive dictionary of sign language, primary concern is to return correct results no matter how few they are. Thus high precision is appreciated much more than high recall rates. 3.2 Evaluation Measures In our experiments, we use precision and recall as the primary evaluation metrics. For a set of queries q ,k = 1 ,...,Q , Precision = where: R ( q A ( q C ( q
We obtain a precision/recall curve by changing the free parameter associated with each thresholding method to simulate different decision cost settings. Right end of these curves fall into the high precision region which is the main concern in our application.
For the case of global thresholding (GT), the same threshold  X  is used for all queries. TWV based term specific thresholding (TWV-TST) (Miller et al., 2007) aims to maximize the TWV metric introduced during NIST 2006 STD Evaluations (NIST, 2006).
TWV = 1  X  P where T is the total duration of the speech archive and  X  is a weight assigned to false alarms that is proportional to the prior probability of occurence of a specific term and its cost-value ratio. This method sets individual thresholds for each query term con-sidering per query expected counts and the tuning parameter  X  . In the proposed method  X  plays the same role as  X  and allows us to control the decision threshold for different cost settings. 3.3 Results
Figure 2 compares GT, TWV-TST, and the pro-posed method that utilizes score distributions to de-rive an optimal decision threshold. For GT and TWT-TST, last precision/recall point in the figure corresponds to the limit threshold value which is 1 . 0 . Both the TWV-TST and the proposed method out-perform GT over the entire region of interest. While TWV-TST provides better performance around the knees of the curves, proposed method achieves higher maximum precision values which coincides with the primary objective of our application.
Figure 2 also provides a curve of what happens when the correct class labels are used to estimate the parameters of the exponential mixture model in a supervised manner instead of using EM. This curve provides an upper bound on the performance of the proposed method. In this paper, we proposed a TST scheme for STD which works almost as good as TWV-TST. Extrapo-lating from the cheating experiment, we believe that the proposed method has potential for outperform-ing the TWV-TST over the entire region of interest given better initial estimates for the correct and in-correct classes.

A special remark goes to the performance in the high precision region where our method clearly out-performs the rest. While GT and TWV-TST meth-ods are bounded around 96.5% precision value, our method reaches at higher precision figures. For GT, this behavior is due to the inability to set differ-ent thresholds for different queries. For TWT-TST, in the high precision region where  X  is large, the threshold is very close to 1 . 0 value no matter what the expected count of the query term is, thus it es-sentially acts like a global threshold.

Our current implementation of the proposed method does not make use of training data to es-timate the initial parameters for the EM algorithm. Instead, it relies on some loose assumptions about the initial parameters of the likelihood functions and uses uninformative prior distributions. The signifi-cant difference between the upper bound and the ac-tual performance of the proposed method indicates that the current implementation can be improved by better initial estimates.

Our assumption about the parametric form of the likelihood function may not be valid at all times. Maximizing the likelihood with mismatched mod-els degrades the performance even when initial parameters are close to the optimal values. In the future, other parametric forms can be utilized to bet-ter model the posterior score distributions.
Maximum likelihood estimation with insufficient data is prone to overtraining. This is a common sit-uation with the STD task at hand. With the current data, three or less results are returned for half of the queries. Bayesian methods can be used to introduce priors on the model parameters in order to make the estimation more robust.
 This study was supported in part by Bo  X  gazic  X i Uni-versity Research Fund (BAP) under the project num-ber 05HA202, T  X  UB  X  ITAK under the project number 105E102 and Turkish State Planning Organization (DPT) under the project number DPT2007K120610.
