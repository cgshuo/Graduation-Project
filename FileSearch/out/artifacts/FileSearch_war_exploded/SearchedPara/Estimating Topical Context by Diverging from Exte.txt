 Improving query understanding is crucial for providing the user with information that suits her needs. To this end, the retrieval system must be able to deal with several sources of knowledge from which it could infer a topical context. The use of external sources of information for improving docu-ment retrieval has been extensively studied. Improvements with either structured or large sets of data have been re-ported. However, in these studies resources are often used separately and rarely combined together. We experiment in this paper a method that discounts documents based on their weighted divergence from a set of external resources. We present an evaluation of the combination of four re-sources on two standard TREC test collections. Our pro-posed method significantly outperforms a state-of-the-art Mixture of Relevance Models on one test collection, while no significant differences are detected on the other one. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Relevance feedback External resources, language models, topical context
When searching for specific information in a document col-lection, users submit a query to the retrieval system. The query is a representation or an interpretation of an under-lying information need, and may not be accurate depending on the background knowledge of the user. Automatically retrieving documents that are relevant to this initial infor-mation need may thus be challenging without additional in-formation about the topical context of the query. One com-mon approach to tackle this problem is to extract evidences from query-related documents [8, 16]. The basic idea is to expand the query with words or multi-word terms extracted from feedback documents. This feedback set is composed of documents that are relevant or pseudo-relevant to the ini-tial query, and that are likely to carry important pieces of information. Words that convey the most information or that are the most relevant to the initial query are then used to reformulate the query. They can come from the target collection or from external sources and several sources can be combined [1, 3]. These words usually are synonyms or related concepts, and allow to infer the topical context of the user search. Documents are then ranked based, among others, on their similarity to the estimated topical context.
We explore the opposite direction and choose to carry ex-periments with a method that discounts documents scores based on their divergences from pseudo-relevant subsets of external resources. We allow the method to take several resources into account and to weight the divergences in or-der to provide a comprehensive interpretation of the topical context. More, our method equally considers sequences of 1, 2 or 3 words and chooses which terms best describe the topical context without any supervision.

The use of external data sets had been extensively stud-ied in the pseudo-relevance feedback setting, and proved to be effective at improving search performance when choos-ing proper data. However studies mainly concentrated on demonstrating how the use of a single resource could improve performance. Data sources like Wikipedia [10, 15], Word-Net [11, 15], news corpora or even the web itself [1, 3] were used separately for enhancing search performances. Com-bining several source of information was nonetheless studied in [1]. However the authors used web anchor and heading texts, which are very small units that are less likely to carry a complete context. They also used the entire Wikipedia but they did not report results of its contribution in the information sources combination. Diaz and Metzler [3] in-vestigated the use of larger and more general external re-sources than those used in [1]. They present a Mixture of Relevance Models (MoRM) that estimates the query model using a news corpus and two web corpora as external sources, and achieves state-of-the-art retrieval performance. To our knowledge, this last approach is the closest one from the method we experiment in this paper.
In this work, we use a language modeling approach to information retrieval. Our goal is to accurately model the topical context of a query by using external resources. We use the Kullback-Leibler divergence to measure the infor-mation gain (or drift) between a given resource R and a document D . Formally, the KL divergence between two lan-guage models  X  R and  X  D is written as: KL (  X  R ||  X  D )= where t is a term belonging to vocabulary V . The first part is the resource entropy and does not affect ranking of doc-uments, which allows us to simplify the KL divergence and to obtain equation (1). In order to capture the topical con-text from the resource, we estimate the  X  R model through pseudo-relevance feedback. Given a ranked list R Q obtained by retrieving the top N documents of R using query likeli-hood, the feedback query model is estimated by: P ( t |  X   X  R )  X  The right-hand expression of this estimation is actually equiv-alent to computing the entropy of the term t in the pseudo-relevant subset R Q . One advantage of doing so it that t not be necessarily a single term, like in traditional relevance models approaches [3, 9], or a fixed-length term [12]. When forming the V set, we slide a window over the entire textual content of R Q and consider all sequences of 1, 2 or 3 words.
Following equation (1), we compute the information di-vergence between a resource R and a document D as: The final score of a document D with respect to a given user query Q is determined by the linear combination of query word matches (standard retrieval) and the weighted divergence from general resources. It is formally written as: s ( Q, D )=  X  log P ( Q |  X  D )  X  (1  X   X  ) where S is a set of resources, P ( Q |  X  D ) is standard query likelihood with Dirichlet smoothing and  X  R represents the weight given to resource R . We use here the information divergence to reduce the score of a document: the greater thedivergence,thelowerthescoreofthedocumentwillbe. Hence the combination of several resources intuitively acts as a generalization of the topical context, and increasing the number of resources will eventually improve the topi-cal representation of the user information need. While we chose to use traditional query likelihood for practical and re-producibility reasons, it could entirely be substituted with other state-of-the-art retrieval models (e.g. MRF-IR [12], BM25 [13]...).
We performed our evaluation using two main TREC 1 col-lections which represent two different search contexts. The first one is the WT10g web collection and consists of 1,692,096 http://trec.nist.gov web pages, as well as the associated TREC topics (451-550) and judgments. The second data set is the Robust04 collec-tion, which is composed of news articles coming from var-ious newspapers. It was used in the TREC 2004 Robust track and is composed of standard corpora: FT (Financial Times), FR (Federal Register 94), LA (Los Angeles Times) and FBIS (i.e. TREC disks 4 and 5, minus the Congres-sional Record). The test set contains 250 topics (301-450, 601-700) and relevance judgements of the Robust 2004 track. Along with the test collections, we used a set of external re-sources from which divergences are computed. This set is composed of four general resources: Wikipedia as an ency-clopedic source, the New York Times and GigaWord cor-pora as sources of news data and the category B of the ClueWeb09 2 collection as a web source. The English Gi-gaWord LDC corpus consists of 4,111,240 news-wire articles collected from four distinct international sources including the New York Times [4]. The New York Times LDC corpus contains 1,855,658 news articles published between 1987 and 2007 [14]. The Wikipedia collection is a recent dump from May 2012 of the online encyclopedia that contains 3,691,092 documents 3 . We removed the spammed documents from the category B of the ClueWeb09 according to a standard list of spams for this collection 4 . We followed authors recommen-dations [2] and set the  X  X pamminess X  threshold parameter to 70. The resulting corpus contains 29,038,220 web pages.
Indexing and retrieval were performed using Indri 5 .The two test collections and the four external resources were in-dexed with the exact same parameters. We use the standard INQUERY english stoplist along with the Krovetz stemmer. We employ a Dirichlet smoothing and set the  X  parame-ter to 1 , 500. Documents are ranked using equation (2). We compare the performance of the approach presented in Section 2 (DfRes) with that of three baselines: Query Like-lihood (QL), Relevance Models (RM3) [9] and Mixture of Relevance Models (MoRM) [3]. In the results reported in Table 1, the MoRM and DfRes approaches both perform feedback using all external resources as well as the target collection, while RM3 only performs feedback using the tar-get collection. QL uses no additional information.
RM3, MoRM and DfRes depend on three free-parameters:  X  which controls the weight given to the original query, k which is the number of terms and N which is the number of feedback documents from which terms are extracted. We performed leave-one-query-out cross-validation to find the best parameter setting for  X  and averaged the performance for all queries. Previous research by He and Ounis [5] showed that doing PRF with the top 10 pseudo-relevant feedback documents was as effective as doing PRF with only relevant documents present in the top 10, and that there are no sta-tistical differences. Following these findings, we set N =10 and also k = 20, which was found to be a good PRF set-ting. DfRes depends on an additional parameter  X  R which controls the weight given to each resource. We also per-form leave-one-query-out cross-validation to learn the best setting for each resource. Although the results in Table 1 correspond to this parameter setting, we explore in the fol-lowing section the influence of the N and k parameters. In http://boston.lti.cs.cmu.edu/clueweb09/ http://dumps.wikimedia.org/enwiki/20110722/ http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/ http://www.lemurproject.org/ the following section, when discussing results obtained using single sources of expansion with DfRes, we use the notation DfRes-r where r  X  (Web,Wiki,NYT,Gigaword).
The main observation we can draw from the ad hoc re-trieval results presented in Table 1 is that using a combina-tion of external information sources performs always better than only using the target collection. The numbers we re-port vary from those presented in [3], however we could not replicate the exact same experiments since the authors do not detail indexing parameters. DfRes significantly outper-forms RM3 on both collections, which confirms that state that combining external resources improves retrieval.
We see from Figure 1 that DfRes-Gigaword is ineffective on the WT10g collection, which is not in line with the re-sults reported in [3] where the Gigaword was found to be an interesting source of expansion. Another remarkable re-sult is the ineffectiveness of the WT10g collection as a single source of expansion. However we see from Table 2 that the learned weight  X  R of this resource is very low (= 0.101), which significantly reduces its influence compared to other best performing resources (such as NYT or Web). Table 2:  X  R weights learned for resources on the two collections. We averaged weights over all queries. Results are more coherent on the Robust collection. DfRes-NYT and DfRes-Gigaword achieve very good results, while the combination of all resources consistently achieves the best results. The very high weights learned for these re-sources hence reflect these good performances. As previ-ously noted, the Robust collection is composed of news ar-ticles coming from several newspapers (not including the NYT). In this specific setting, it seems that the nature of the good-performing resources is correlated with the nature of the target collection. We observed that NYT and Gi-gaword articles, which are focused contributions produced by professional writers, are smaller on average (in unique words) than Wikipedia or Web documents.

We explored the influence of the number of feedback doc-uments used for the approximation of each resource. We omit the plots of retrieval performances for the sake of space, and also because they are not noteworthy. Performances in-deed remain almost constant for all resources as N varies. Changes in MAP are about  X  2% from N =1to N =20 depending on the resource. However we also explored the influence of the number of terms used to estimate each re-source X  X  model. While we could expect that increasing the number of terms would improve the granularity of the model and maybe capture more contextual evidences, we see from Figure 2 that using 100 terms is not really different than us-ing 20 terms. We even see that using only 5 terms achieves the best results for DfRes on the WT10g collection.
Overall, these results show support for the principles of polyrepresentation [6] and intentional redundancy [7] which state that combining cognitively and structurally different representations of information needs and documents will in-crease the likelihood of finding relevant documents. Since we use several resources of very different natures ranging from news articles to web pages, DfRes takes advantage of this variety to improve the estimation of the topical con-text. Moreover, the most effective values of  X  tend to be low, which means that DfRes is more effective than the ini-tial query. We even see on Figure 1 that only relying on the divergence from resources (i.e. setting  X  =0)achievesbet-ter results than only relying on the user query (i.e. setting  X  = 1). More, setting  X  = 0 for DfRes also outperforms MoRM (significantly on the Robust collection). This sug-gests that DfRes is actually better as estimating the topical context of the information need than the user keyword query.
We also observe from Figure 1 and 2 that the NYT is the resource that provides the best estimation of the topical con-text for the two collections, despite being the smallest one. This may be due to the fact that articles are well-written by professionals and contain lots of synonyms to avoid repeti-tion. Likewise, the failure of Wikipedia may be due to the encyclopedic segmentation of articles. Since each Wikipedia article covers a specific concept, it is likely that only concept-related articles compose the pseudo-relevant set, which may limit a larger estimation of the topical context. One of the originality of the DfRes is that it can automatically take into account n -grams without any supervision (such as setting the size of the grams prior to retrieval). In practice, there is on average 1.19 words per term, but most of the time arti-cles like  X  X he X  are added to words that already were selected (i.e.  X  X he nativity scene X , where  X  X ativity X  and  X  X cene X  were used before as single words).
Accurately estimating the topical context of a query is a challenging issue. We experimented a method that dis-counts documents based on their average divergence from a set of external resources. Results showed that, while rein-forcing previous research, this method performs at least as good as a state-of-the-art resource combination approach, and sometimes achieves significantly higher results. Per-formances achieved by the NYT as a single resource are very promising and need further exploration, as well as the counter-performance of Wikipedia. More specifically, using nominal groups or sub-sentences that rely on the good qual-ityofNYTarticlescouldbeinterestingandinlinewith ongoing research in the Natural Language Processing field. Figure 1: Retrieval performance (in MAP) as a function of the resource language model. Legend is the same as in Figure 1.
This work was supported by the French Agency for Sci-entific Research (Agence Nationale de la Recherche) under CAAS project (ANR 2010 CORD 001 02).
