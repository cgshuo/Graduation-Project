 together [7].
 prototypes can best be combined to approximate d .
 Many useful applications arise from factored models: our conclusions in Section 5.
 sented as a column vector.
 the M  X  N term-document matrix such that column j represents document d the number of times term t term t L with P , a normalized version of L in which P sum to 1.
 Factoring: Let A represent a matrix to be factored (usually T or T augmented with some other matrix) into K factors. Factoring de-composes A into two matrices U and V (each of rank K ) such that A  X  U V . 1 In the geometric interpretation, columns of U contains the K prototypes, while columns of V indicate what mixture of pro-totypes best approximates the columns in the original matrix. The definition of what constitutes a  X  X est approximation X  leads to the many different factoring algorithms in use today. Latent Seman-tic Analysis [1] minimizes the sum squared reconstruction error of A , PLSA [2] maximizes the log-likelihood that a generative model using U as prototypes would produce the observed A , and Non-Negative Matrix Factorization [3] adds constraints that all compo-nents of U and V must be greater than or equal to zero. main concern is how A , the document matrix to be factored, is generated. 1.1 Factoring Text and Link Corpora K of the corpus documents. document belongs to the corresponding community. Additionally, U community j accords to document d 1.2 Factoring Text and Links Together matrix: When factored, the resulting U matrix can be seen as having two components, representing the two distinct types of infor-mation in [ T ; L ] . Column i of U term distribution of factor i , while the corresponding column of
U L indicates the distribution of documents that typically link to documents represented by that factor.
 In practice, L should be scaled by some factor  X  to control the relative importance of the two types of information, but empirical evidence [7] suggests that performance is somewhat insensitive to its exact value. For clarity, we omit reference to  X  in the equations below. workstation.
 inlinking documents, rather than by their explicit identities. 2.1 Attribute Factoring Each document d in the corpus, signifying the presence (or absence) of a link from d information from link matrix L , possibly in combination with the term matrix T . is just Colloquially, we can look at this representation as saying that a doc-ument has  X  X ome distribution of terms X  ( T ) and is linked to by doc-uments that have  X  X ome other term distribution X  ( T  X  L ). By substituting the aggregated attributes of the inlinks for their identities, we can reduce the size of the representation down from ( M + N )  X  N to a much more manageable 2 M  X  N . What is surpris-ing is that, on the domains tested, this more compact representation actually improves factoring performance. 2.2 Attribute Factoring Experiments We tested Attribute Factoring on two publicly-available corpora of interlinked text documents.
 The Cora dataset [10] consists of abstracts and references of of approximately 34,000 com-puter science research papers; of these we used the approximately 2000 papers categorized into the seven subfields of machine learning. The WebKB dataset [11] consists of approximately 6000 web pages from computer science depart-ments, classified by school and category (stu-dent, course, faculty, etc.).
 For both datasets, we factored the content-only, naive joint, and AF joint representations using PLSA [2]. We varied K , the number of com-puted factors from 2 to 16, and performed 10 factoring runs for each value of K tested. The factored models were evaluated by clustering each document to its dominant factor and mea-suring cluster precision: the fraction of docu-ments in a cluster sharing the majority label.
 abstracting the link information with Attribute Factoring improves it even more. Attribute Factoring reduces the number of attributes from N + M to 2 M , allowing existing factoring techniques to scale to web-sized corpora. This reduction in number of attributes however, comes at a cost. Since the identity of the document itself is replaced by its attributes, it is possible for unscrupu-lous authors (spammers) to  X  X ose X  as a legitimate page with high PageRank.
 Consider the example shown in Figure 5, showing two sub-graphs present in the web. On the right is a legitimate page like the Yahoo! homepage, linked to by many pages, and link-ing to page RYL (Real Yahoo Link). A link from the Ya-hoo! homepage to RYL imparts a lot of authority and hence is highly desired by spammers. Failing that, a spammer might try to create a counterfeit copy of the Yahoo! homepage, boost its PageRank by means of a  X  X ink farm X , and create a link from it to his page FYL (Fake Yahoo Link).
 look a lot like the Yahoo! homepage. 3.1 Recursive Attribute Factoring propagate this inference on to later pages.
 The AF representation introduced in the previous section can be easily fooled. It makes inferences about a document based on explicit attributes propagated from the documents linking to it, but this inference only propagates one level. For example it lets us infer that the fake Yahoo! homepage was counterfeit, but provides no way to propagate this inference on to later pages. This suggests that we need to propagating not only explicit attributes of a document (its component terms), but its inferred attributes as well.
 A ready source of inferred attributes comes from the factoring process itself. Recall that when factoring T  X  U  X  V , if we interpret the columns of U as factors or prototypes, then each column of V can be interpreted as the inferred factor member-ships of its corresponding document. Therefore, we can prop-agate the inferred attributes of inlinking documents by aggre-gating the columns of V they correspond to (Figure 6). Nu-merically, this replaces T (the explicit document attributes) in the bottom half of the left matrix with V (the inferred document attributes): convergence guarantees. The  X  X nferred X  attributes ( I P Algorithm 1 Recursive Attribute Factoring 1: Initialize I A 0 with random entries. 2: while Not Converged do 3: Factor A t = T 4: Update I A t +1 = V  X  P . 5: end while 3.2 Recursive Attribute Factoring Experiments information.
 better than with either alone (top lines in Figures 7(a) and (b)). with a matrix (call it I augmented matrix: The traditional joint model set I Attribute Factoring I M -dimensional inferred vector d 0 with inferred attributes for each document i.e. i th column of I are summarized in Table 1.
 Table 1: Variations on attribute weighting for Attribute Factoring. ( P ing documents by setting I I
A = [ T  X  L ; T  X  L  X  L ] level to the model ( I 1, P needs to be used instead of L to achieve convergence). smoothed version of the recursive equation, can be written as to +  X   X  V  X  P  X  U equation further simplifies to +  X   X  V  X  P  X  u  X  V .
 terms of T added, the intuition is that V and the inferred attributes I prototypes U improve the quality of the resulting factors.
 real (and as yet untried) criterion for success.
