 Hui Xiong  X  Michael Steinbach  X  Arifin Ruslim  X  Vipin Kumar Abstract This paper describes a new approach for clustering X  X attern preserving clustering X  X hich produces more easily interpretable and usable clusters. This approach data X  X atterns that may be key for the analysis and description of the data X  X hese patterns are often split among different clusters by current clustering approaches. This is, perhaps, not surprising, since clustering algorithms have no built-in knowledge of these patterns and may often have goals that are in conflict with preserving patterns, e.g., minimize the distance of points to their nearest cluster centroids. In this paper, our focus is to characterize (1) the benefits of pattern preserving clustering and (2) the most effective way of performing pattern preserving clustering. To that end, we propose and evaluate two clustering algorithms, HIerar-chical Clustering with pAttern Preservation (HICAP) and bisecting K-means Clustering with pAttern Preservation (K-CAP). Experimental results on document data show that HICAP can produce overlapping clusters that preserve useful patterns, but has relatively worse cluster-ing performance than bisecting K-means with respect to the clustering evaluation criterion of entropy. By contrast, in terms of entropy, K-CAP can perform substantially better than the bisecting K-means algorithm when data sets contain clusters of widely different sizes X  X  common situation in the real-world. Most importantly, we also illustrate how patterns, if preserved, can aid cluster interpretation.
 Keywords Pattern preserving clustering  X  K-means clustering  X  Hyperclique pattern  X  Hierarchical clustering 1 Introduction Clustering and association analysis are important techniques for analyzing data. Cluster analysis [ 11 ] provides insight into the data by dividing the objects into groups (clusters) of objects, such that objects in a cluster are more similar to each other than to objects in other clusters. Association analysis [ 1 ], on the other hand, provides insight into the data by finding a large number of patterns X  X requent patterns and other patterns derived from them X  X n the data set. Frequent patterns identify sets of items (attributes) by finding attributes that occur together within a sufficiently large set of transactions. Thus, noting that clustering and association analysis can be performed either on objects or attributes, and restricting our discussion to binary transaction data, clustering and association analysis are both concerned with finding groups of strongly related objects or attributes, although at different levels. Association of attributes or objects, while cluster analysis finds strongly related objects or attributes on a global level, i.e., by using all of the attributes or objects to compute similarity values.
Recently, we have defined a new pattern for association analysis X  X he hyperclique pattern [ 32 , 33 ] X  X hat demonstrates a particularly strong connection between the overall similarity of a set of attributes (or objects) and the itemset (local pattern) in which they are involved. property : the attributes (objects) in a hyperclique pattern have a guaranteed level of global pairwise similarity to one another as measured by the cosine measure (uncentered Pearson X  X  correlation coefficient) 1 [ 26 ]. Since clustering depends on similarity, it seems reasonable that the hyperclique pattern should have some connection to clustering. To this end, we posed the following question: What happens to hyperclique patterns when data is clustered by standard clustering techniques, e.g., how are they distributed among clusters?
We found that hyperclique patterns are mostly destroyed by standard clustering tech-niques, i.e., standard clustering schemes do not preserve the hyperclique patterns, but rather, understand why this is not desirable, consider a set of hyperclique patterns for documents. The high-affinity property of hyperclique patterns requires that these documents must be similar to one another; the stronger the hyperclique, the more similar the documents. Thus, for strong patterns, it would seem desirable (from a clustering viewpoint) that documents in the same pattern end up in the same cluster in many or most cases. As mentioned, however, this is not what happens for traditional clustering algorithms. This is not surprising since traditional clustering algorithms have no built-in knowledge of these patterns and may often have goals that are in conflict with preserving patterns, e.g., minimize the distance of points from their closest cluster centroid.

More generally, the breaking of these patterns is also undesirable from an application point of view. Specifically, in many application domains, there are fundamental patterns that dominate the description and analysis of data within that area, e.g., in text mining, collections of words that form a topic, and in biological sciences, a set of proteins that form a functional module [ 30 ]. If these patterns are not respected, then the value of a data analysis is greatly diminished for end users. If our interest is in patterns, such as hyperclique patterns, then we need a clustering approach that preserves these patterns, i.e., puts the objects or attributes of these patterns in the same cluster. Otherwise, the resulting clusters will be harder to understand since they must be interpreted solely in terms of objects instead of well-understood patterns. To address the above challenges, we propose a pattern preserving clustering approach. Our goal is to characterize pattern preserving clustering with respect to the following two issues: 1. The benefits of pattern preserving clustering. 2. The most effective way of performing pattern preserving clustering.

One benefit of pattern preserving clustering is to provide a better cluster interpretation than traditional clustering approaches by considering the patterns found in clusters. To achieve this benefit, it is necessary to determine the most effective approach for pattern preserving clustering. One important aspect of this is the choice of which association pattern to use. frequent patterns (frequent itemsets) for pattern preserving clustering. First, the hyperclique pattern possesses the high-affinity property . As a result, hyperclique patterns tend to include objects that are from the same class (cluster), and thus, are well suited for pattern preserving have low pairwise similarity. Second, hyperclique patterns have much better coverage of the targeted objects than frequent patterns since hyperclique patterns can be identified at low levels of support. Third, the computational cost of finding hyperclique patterns is significantly lower than that of frequent patterns [ 32 ]. Finally, there are fewer hyperclique patterns than frequent patterns, and thus, they are more manageable during the clustering process.
For instance, due to the high affiliation within hyperclique patterns, pattern preserving clustering naturally lends itself to various applications in search engines. For instance, instead of a long ranked list of keyword queries, it can be better to return clustered search results by topic. This can be accomplished by showing only the documents in the hyperclique patterns, which are more compact and representative of those topics. For a query session, instead of returning the documents where all the query words just co-occur, we can return the documents from hyperclique patterns that connect the queries and embody their common topics. In our preliminary work [ 31 ], we introduced the Hierarchical Clustering with Pattern Preservation (HICAP) algorithm, which is a pattern preserving clustering technique that utilizes hyperclique patterns to create the initial clusters, and then performs a Group Average agglomerative hierarchical clustering (also known as UPGMA [ 11 , 14 ]). To the best of our knowledge, HICAP is the first approach that is based on the idea of preserving patterns. While HICAP can produce overlapping clusters X  X on-overlapping clustering schemes tend to break patterns X  X nd has better interpretations for clustering results, it is computationally more expensive than K-means. More importantly, the cluster quality of HICAP with respect to entropy is worse than bisecting K-means with respect to entropy [ 31 ]. As we know, bisecting k-means, which is a variant of traditional K-means algorithm whose performance for clustering document data is among the best [ 24 , 35 ].
 In this paper, we present a bisecting K-means Clustering with pAttern Preservation (K-CAP) algorithm that exploits key properties of bisecting K-means and the hyperclique pattern. K-CAP provides the following two important benefits. 1. Improved clustering quality For data with widely differing cluster sizes, which is the 2. Pattern preservation Strong patterns are preserved during the clustering process; that is,
The reason that K-CAP can produce better clustering quality than bisecting K-means for data sets with widely differing cluster sizes is explained in more detail later, but we briefly summarize the approach here. Before clustering, the group of objects in a hyperclique pattern is replaced by their centroid, thus reducing the size of the data set. Larger clusters are reduced more than smaller clusters and the distribution of objects among clusters becomes more uniform. Clustering is then performed using bisecting K-means, and the K-CAP algorithm assigns all the objects in a hyperclique pattern to the cluster containing their corresponding centroid to produce the final clustering results.
 Outline Section 2 provides a background in clustering and also describes related work. Section 3 introduces the hyperclique pattern, while Sect. 4 presents the details of the HICAP and K-CAP algorithms. Experimental results are given in Sect. 5 . Section 6 provides a brief conclusion and an indication of our plans for future work. 2 Clustering background and related work Cluster analysis [ 4 , 5 , 10 , 12 , 36 ] has been the focus of considerable work, both within data mining and in other fields such as statistics, machine learning, and pattern recognition. Several recent surveys may be found in [ 4 , 12 ], while more discussions of clustering are provided by the following books [ 2 , 11 , 14 ]. The discussion in this section is, of necessity, quite limited.
While there are innumerable clustering algorithms, almost all of them can be classified as being either partitional , i.e., producing an un-nested set of clusters that partitions the objects in a data set into disjoint groups [ 7 ], or hierarchical , i.e., producing a nested sequence of partitions, with a single, all-inclusive cluster at the top and singleton clusters of individual points at the bottom [ 15 ]. While this standard description of hierarchical versus partitional clustering assumes that each object belongs to a single cluster (a single cluster within one level, for hierarchical clustering), this requirement can be relaxed to allow clusters to overlap. Thus, in this paper we will describe clustering algorithms as hierarchical or partitional and as overlapping or non-overlapping. 2
Perhaps the best known and most widely used partitional clustering technique is K-means [ 17 ], which aims to cluster a dataset into K clusters X  K specified by the user X  X o as to min-imize the sum of the squared distances of points from their closest cluster centroid (a cluster centroid is the mean of the points in the cluster). K-means is simple and computationally efficient, and a modification of it, bisecting K-means [ 24 ], can also be used for hierarchical clustering. Indeed, K-means is one of the best ways for generating a partitional or hierarchical clustering of documents [ 24 , 34 ]. We use K-means, as implemented by CLUTO [ 13 ], as one of the methods to compare with our approach.

Traditional hierarchical clustering approaches [ 11 ] build a hierarchical clustering in an agglomerative manner by starting with individual points or objects as clusters, and then successively combining the two most similar clusters, where the similarity of two clusters can be defined in different ways and is what distinguishes one agglomerative hierarchical technique from another. These techniques have been used with good success for clustering documents and other types of data. In particular, the agglomerative clustering technique known as Group Average or UPGMA [ 11 , 14 ], which defines cluster similarity in terms of the average pairwise similarity between the points in the two clusters, is widely used because it is more robust than many other agglomerative clustering approaches. Furthermore, a recent study found UPGMA to be the best of the traditional agglomerative clustering techniques for clustering text [ 34 ].

As far as we know, there are no other clustering methods based on the idea of preserving patterns. However, we mention three other types of clustering approaches that share some similarity with what we are doing here: constrained clustering, co-clustering, and frequent itemset based clustering. Constrained clustering [ 28 ] is based on the idea of using standard clustering approaches, but restricting the clustering process. Our approach can be viewed as constraining certain objects to stay together during the clustering process. However, our constraints are automatically enforced by putting objects in hypercliques together, before the clustering process begins, and thus, the general framework for constrained clustering is not necessary for our approach. Also, co-clustering [ 18 ] finds an optimal partitioning by co-clustering of both rows and columns of a data matrix. In a document clustering setting, co-clustering of both words and documents can provide more understandable clustering results. Viewed in this light, co-clustering is relevant to our pattern preserving clustering approach. However, our approach starts from a set of strong local patterns (hypercliques). Co-clustering schemes have to examine data in a global level and have no built in knowledge of these strong local patterns.

Our pattern preserving clustering technique is based on an association pattern, the hyper-clique pattern, but there have been other clustering approaches that have used frequent patterns or other patterns derived from them [ 3 , 6 , 29 , 20 ]. Specifically, Wang et al. [ 29 ] proposed a clustering approach based on the intuition that intra-cluster members should share many fre-quent items, while inter-cluster members should have little overlap in terms of frequent items. Beil et al. [ 3 ] proposed Hierarchical Frequent Term-based clustering (HFTC) for clustering documents. This technique uses a greedy approach to pick a set of frequent terms that has minimum overlap in terms of their document coverage. To improve HFTC, Fung et al. [ 6 ] proposed the Frequent Itemset-based Hierarchical Clustering (FIHC) method for document clustering. FIHC finds frequent terms and uses the documents covered by these frequent terms to create the initial clusters. Then, hierarchical clustering is performed using an intercluster similarity measure defined in terms of frequent patterns. However, these techniques are not designed for pattern preserving clustering. Finally, the hypergraph clustering approach in [ 9 ] creates a hypergraph based on frequent itemsets and association rules, and then uses a hyper-graph partitioning technique for finding clusters. Although hypergraph clustering inspired the clustering approach in the original hyperclique paper, this approach is not towards pattern preserving. 3 Basic concepts of association patterns The hyperclique pattern was the inspiration for pattern preserving clustering, and thus, the pattern that we use to explore this idea. In this section, we describe the concept of hyperclique patterns [ 32 ], after first introducing the concepts on which it is based: the frequent itemset and the association rule [ 1 ]. 3.1 Frequent itemsets and association rules We quickly review some standard definitions related to association rule mining, which is an important technique for mining market basket data [ 1 ].

Let I ={ i 1 , i 2 ,..., i m } be a set of items. Let T be a set of transactions, where each transaction t is a set of items such that t  X  I . An itemset is a set of items X  X  I .The support of X , supp ( X ) , is the fraction of transactions containing X . If the support of X is above a user-specified minimum, i.e., supp ( X )&gt; minsup, then we say that X is a frequent itemset .
An association rule captures the fact that the presence of one set of items may imply the presence of another set of items, and is of the form X  X  Y ,where X  X  I , Y  X  I ,and X  X  Y =  X  .The confidence of the rule X  X  Y is written as conf ( X  X  Y ) and is defined as conf ( X  X  Y ) = supp ( X  X  Y )/ supp ( X ) , where supp ( X  X  Y ) is the support of the rule. For example, suppose 70% of all transactions contain bread and milk, while 50% of the transactions contain bread, milk, and cookies. Then, the support of the rule { bread, milk } X  { cookies } is 50% and its confidence is 50% / 70% = 71%. 3.2 Hyperclique patterns A hyperclique pattern [ 32 ] is a new type of association pattern that contains items that are highly affiliated with each other. 3 By high affiliation, we mean that the presence of an item in a transaction strongly implies the presence of every other item that belongs to the same hyperclique pattern. The h-confidence measure [ 32 ] is specifically designed to measure the strength of this association.
 Definition 1 The h-confidence of an itemset P ={ i 1 , i 2 ,..., i m } , denoted as hconf ( P ) , i ,..., i m  X  1 }} , where conf follows from the conventional definition of association rule con-fidence as given above.

For instance, consider an itemset P ={ A , B , C } . Assume that supp ( { A } ) = 0 . 1, support of an itemset. Then Hence, hconf ( P ) = min { conf { B  X  A , C } , conf { A  X  B , C } , conf { C  X  A , B }} =0.6. Definition 2 Given a transaction database and the set of all items I ={ I 1 , I 2 ,..., I n } ,an itemset P is a hyperclique pattern if and only if 1. P  X  I and | P | &gt; 0. 2. hconf ( P )  X  h c ,where h c is a user-specified minimum h-confidence threshold.
Ta b l e 1 shows some hyperclique patterns identified from words of the LA1 dataset, which is part of the TREC-5 collection [ 27 ] and includes articles from various news categories such as  X  X inancial X ,  X  X oreign X ,  X  X etro X ,  X  X ports X , and  X  X ntertainment X . One hyperclique pattern in that table is { mikhai , gorbachev }, who is the ex-president of the former Soviet Union. Certainly, thepresenceof mikhai in one document strongly implies the presence of gorbachev in the same document and vice-versa.
 Definition 3 A hyperclique pattern is a maximal hyperclique pattern if no superset of this hyperclique pattern is also a hyperclique pattern.

In this paper, we use maximal hyperclique patterns as the patterns that we wish to preserve during the clustering process. Therefore, the study scope of this paper is on binary data. 3.3 Properties of the h-confidence measure The h-confidence measure has three important properties, namely the anti-monotone property, the cross-support property, and the strong affinity property. Detailed descriptions of these three properties were provided in our earlier paper [ 32 ]. Here, we provide only the following brief summaries.

The anti-monotone property guarantees that if an itemset { i 1 ,..., i m } has an h-confidence value greater or equal to h c , then every subset of size m  X  1 also has an h-confidence value greater or equal to h c . This property is analogous to the anti-monotone property of the support measure in association-rule mining [ 1 ] and allows us to use h-confidence-based pruning to speed the search for hyperclique patterns in the same way that support-based pruning is used to speed the search for frequent itemsets.

The cross-support property provides an upper bound for the h-confidence of itemsets that contain items from different levels of support. The computation of this upper bound is much cheaper than the computation of the exact h-confidence, since it only relies on the support values of individual items in the itemset. Using this property, we can design a partition-based approach that allows us to efficiently prune patterns involving items with different support levels.

The strong affinity property guarantees that if a hyperclique pattern has an h-confidence value above the minimum h-confidence threshold, h c , then every pair of items within the hyperclique pattern must have a cosine similarity [ 22 ] greater than or equal to h c . As a result, threshold.

As demonstrated in our previous paper [ 32 ], the anti-monotone and cross-support prop-erties allow the design of an efficient hyperclique mining algorithm that has much better performance than frequent itemset mining algorithms, particularly at low levels of support. Also, the number of hyperclique patterns is significantly less than the number of frequent itemsets. 4 Algorithm descriptions In this section, we first discuss why hyperclique patterns are better than frequent itemsets for pattern preserving clustering. Then, we present the details of the HIerarchical Clustering with the pAttern Preservation (HICAP) algorithm and the bisecting K-means Clustering with pAttern Preservation (K-CAP) algorithm. 4.1 Association pattern selection Here, we discuss the choice of an association pattern for pattern preserving clustering. Specif-ically, we present the results of an experiment that illustrates why the hyperclique pattern is a good pattern to use for pattern preserving clustering, but frequent itemsets are not. In this experiment, we employed entropy, a commonly used measure of purity. A formal definition of entropy is given below.

Entropy To compute the entropy of a set of clusters, we first calculate the class distribution of the objects in each cluster, i.e., for each cluster j we compute p ij , the probability that amemberofcluster j belongs to class i . Given this class distribution, the entropy, E j ,of cluster j is calculated using the standard entropy formula as follows: where the sum is taken over all classes and the log is log base 2. The total entropy for a set of clusters is computed as the weighted sum of the entropies of each cluster where n j is the size of cluster j , m is the number of clusters, and n is the total number of data points.

Figure 1 shows, for the LA1 dataset (a document data set containing news articles from the Los Angeles Times), the entropy of the discovered hyperclique patterns for different minimum h-confidence and support thresholds. Note that when the minimum h-confidence threshold is zero, we actually have frequent itemsets instead of hyperclique patterns. Figure 1 shows that the entropy of hyperclique patterns decreases dramatically as the minimum h-confidence threshold increases. For instance, when the h-confidence threshold is higher than 0.25, the entropy of hyperclique patterns is less than 0.1 for all the given minimum support thresholds. This indicates that hyperclique patterns are very pure patterns for certain h-confidence thresholds. In other words, a hyperclique pattern includes objects that are nat-urally from the same class category. In contrast, the entropy of frequent patterns is high, close to 1, for all the given minimum support thresholds. This means that frequent patterns include objects from different classes. Thus, with respect to purity as measured by entropy, the hyperclique pattern is a better candidate than frequent itemsets for pattern-preserving clustering.

Another trend that we can observe in Fig. 1 is that, as the minimum support threshold decreases, the entropy of hyperclique patterns from the LA1 dataset trends downward. This indicates that high-affinity patterns can appear at very low levels of support. As mentioned, frequent itemset mining algorithms have difficulties at identifying frequent itemsets at low levels of support. In contrast, the hyperclique pattern mining algorithm has much better computational performance at low levels of support [ 32 ]. 4.2 HICAP: HIerarchical Clustering with pAttern Preservation HICAP is based on the Group Average agglomerative hierarchical clustering technique, which is also known as UPGMA [ 11 ]. However, unlike the traditional version of UPGMA, which starts from clusters consisting of individual objects or attributes, HICAP uses hyperclique patterns to define the initial clusters, i.e., the objects or attributes of each hyperclique pattern become an initial cluster.

Figure 2 shows the pseudocode of the HICAP algorithm. This algorithm consists of two phases. In phase I, HICAP finds maximal hyperclique patterns, which are the patterns we want to preserve in the HICAP algorithm. We use only maximal hyperclique patterns since any non-maximal hyperclique will, during the clustering process, tend to be absorbed by its corresponding maximal hyperclique pattern and will, therefore, not affect the clustering process significantly. Indeed, the use of non-maximal hypercliques would add complexity without providing any compensating benefits.
 In phase II, HICAP conducts hierarchical clustering and outputs the clustering results. We highlight several important points. First, since hyperclique patterns can be overlapping, some of the resulting clusters will be overlapping. Second, identified maximal hyperclique patterns typically cover (contain) only 10 X 20% of all objects, and thus, HICAP also includes each uncovered object as a separate initial cluster, i.e., the hierarchial clustering starts with maximal hyperclique patterns and uncovered objects. Finally, the similarity between clus-ters is calculated using the average of the pairwise similarities between objects, where the similarity between objects is computed using the cosine measure. 4.3 K-CAP: bisecting K-means Clustering with pAttern Preservation In this subsection, we describe the details of the bisecting K-means Clustering with pAttern Preservation (K-CAP) algorithm. First, Fig. 3 shows the pseudocode for K-CAP.

K-CAP consists of two phases. In the first phase, K-CAP computes the disjoint maximal hyperclique patterns. There are several implementation details. First, we use only maximal hyperclique patterns that contain at least three objects. A hyperclique pattern with three or more objects is less likely to be spurious, because, by definition, the cosine similarity of each pair of objects in the pattern must be greater than the minimum h-confidence threshold. When finding disjoint hyperclique patterns, the K-CAP algorithm gives higher priority to the hyperclique patterns containing more objects. For maximal hyperclique patterns that contain the same number of objects, the K-CAP algorithm gives higher priority to the patterns with higher h-confidence values. In case of ties, an arbitrary pattern is selected by the algorithm. The resulting hyperclique patterns are disjoint sets of highly correlated objects and these patterns are the input for the second phase of K-CAP.

In the second phase, the K-CAP algorithm removes the objects in each disjoint maximal hyperclique pattern from the data set and inserts the centroid vector of all objects in the pattern. Because multiple object vectors of the hyperclique are replaced by their centroid, the size of the data set is reduced. The modified data is clustered using bisecting K-means. Once the clustering results are produced by bisecting K-means, K-CAP assigns all the objects in a maximal hyperclique pattern to the cluster containing their corresponding centroid.
Note that non-overlapping maximal hyperclique patterns are used in order to preserve independent strong concepts. Also, the K-CAP algorithm uses maximal hyperclique pat-terns because non-maximal hyperclique patterns tend to be absorbed by their corresponding maximal hyperclique patterns during the clustering phase. Furthermore, fewer hyperclique patterns result in less computation. Finally, the CLUTO implementation of bisecting K-means [ 13 ] has been used for the K-CAP algorithm.

Computation analysis K-CAP is a very efficient pattern based clustering algorithm. First, the computation of hyperclique patterns is much cheaper than that of frequent patterns, specially at low levels of support [ 32 ]. Second, the number of data objects for bisecting K-means clustering can be significantly reduced when the objects in each pattern are replaced by their centroid. The computation cost is reduced accordingly. Because of this and the efficiency with which hyperclique patterns can be found, K-CAP retains the computational efficiency of bisecting K-means [ 24 ]. 5 Experimental evaluation In this section, we present an experimental evaluation of the HICAP and K-CAP algorithms. After a brief description of our document data sets, we first illustrate the poor behavior of traditional clustering approaches in terms of pattern preservation, and show how hyperclique patterns can be used to interpret the clustering results produced by pattern preserving clus-tering. In addition, we evaluate the clustering performance of HICAP, K-CAP, UPGMA, and K-means with respect to the entropy. Finally, we show how K-CAP can reduce the skewness of the class distribution.
 Experimental data sets For our experiments, we used various real-world data sets that are widely used in document clustering research. Some characteristics of these data sets are shown in Table 2 . The data sets RE0 and RE1 are from the Reuters-21578 text categorization test collection Distribution 1.0 [ 16 ]. The WEST5 data set came from the Thompson Publishing Group and was derived from legal documents. The data set WAP is from the WebACE project Yahoo!. The OH8 data set was derived from the TREC-5 collection [ 27 ]; the LA1 dataset is part of the TREC-5 collection [ 27 ] and contains news articles from the Los Angeles Times. Datasets TR12 and TR32 were derived from the TREC-5 [ 27 ], TREC-6 [ 27 ], and TREC-7 [ 27 ] collections. The HITECH data set contains documents about computers, electronics, health, medical, research, and technology. Finally, the FBIS data set is from the Foreign Broadcast Information Service data of the TREC-5 collection [ 27 ]. For all data sets, we used a stop-list to remove common words, and the words were stemmed using Porter X  X  suffix-stripping algorithm [ 21 ].
 Evaluation To evaluate the quality of the clusters produced by the different clustering tech-niques, we employed entropy, which was introduced in Sect. 4 . Entropy is an  X  X xternal X  criterion; i.e., it uses external information X  X lass labels in this case. Specifically, entropy measures the purity of the clusters with respect to the given class labels. Thus, if all clusters consist of objects with only a single class label, the entropy is 0. However, as the class labels of objects in a cluster become more varied, the entropy increases. 5.1 Preserving patterns By design, HICAP and KCAP preserve all hyperclique patterns throughout the clustering process. However, as we show in this experiment, traditional clustering algorithms X  X PGMA and bisecting K-means X  X end to break hyperclique patterns. Figure 4 shows, for different number of clusters, the ratio of hyperclique patterns being split by the UPGMA and bisecting K-means algorithms. For every data set, the minimum number of clusters is specified as the original number of classes in that data set. In the figure, we observe that the ratio of patterns being split for both algorithms increases as the number of clusters increases. Furthermore, even when the number of clusters equals the number of classes, UPGMA and bisecting K-means still break patterns. Finally, bisecting K-means breaks more patterns than UPGMA, because its preference for relatively uniform cluster sizes tends to break long hyperclique patterns. 5.2 Interpretation of clusters using hyperclique patterns In this experiment, we provide two types of evidence to illustrate the usefulness of patterns for interpreting clustering results: specific examples and an analysis of the clusters on one level of the cluster hierarchy. For the first example, we picked two clusters at random from the hierarchical clustering generated by HICAP, and then looked at the hyperclique patterns that they contained to see if the nature of these hypercliques, which include only a fraction of the documents or words in the cluster, are useful for understanding the nature of the cluster. As we show below, this was indeed the case.

Cluster contains hypercliques of the same class Figure 5 shows a cluster randomly selected from the HICAP clustering results on the RE0 data set. One cluster with document IDs is presented. On further analysis, we found that two hyperclique patterns are in this cluster, and, as shown in the figure, both hyperclique patterns belong to the  X  X oney X  category. Since HICAP is based on the Group Average agglomerative clustering approach, it is natural to expect that other documents in the given cluster should have a significant level of similarity to the documents in two hyperclique patterns. In other words, if these two hyperclique patterns act as a  X  X ernel, X  the documents merged into this cluster are likely to have the same class label as the documents in these two hyperclique patterns. As a result, we might expect that a large population of documents in this cluster would have the class,  X  X oney. X  To verify this, we show the class labels of the cluster objects in Table 3 . As suggested by the two hyperclique patterns, all of the documents, except two, belong to the class,  X  X oney. X 
Cluster contains hypercliques of different classes Figure 6 shows another cluster randomly picked from the HICAP clustering of the WAP data set. This cluster contains two hyperclique patterns with documents from two different categories:  X  X ilm X  and  X  X elevision. X  As a result, we would expect that this cluster should be a hybrid cluster with documents mainly from two categories:  X  X ilm X  and  X  X elevision. X  Table 4 shows the class labels of the cluster objects in this cluster. Once again, the interpretation based on hyperclique patterns matches the classes found in the cluster.

Analyzing clusters on one level of the cluster hierarchy To further validate the hypothesis that the nature of the hyperclique patterns contained in a cluster tells us something about the nature of the cluster, we decided to look at the clusters on one level of the cluster hierarchy. We first identified the class of each of the hyperclique patterns X  X here were 115 of these patterns in the WAP data set, which together covered 265 out of 1560 documents. Finding the class of each hyperclique was an easy task since the hypercliques almost always consisted of objects of a single class, and if not, were predominantly of one class. Then, we found which of the 128 clusters contained hypercliques X  X here were 11 such clusters, which covered 808 of the 1560 documents (the skewed distribution of cluster sizes is a result of the skewed distribution of class sizes). We further analyzed each cluster with respect to the classes of documents that it contained and whether the classes of the documents in the cluster matched the classes of the hypercliques in the cluster. The results of this analysis are contained in Ta b l e 5 ( X  X No X  is cluster number,  X  X ize X  is the number of objects in the cluster,  X # unmatch X  is the number of objects in the cluster that do not match a class of the hypercliques in the cluster,  X # hyperclique X  is the number of hypercliques in the cluster, and  X  X lasses of hypercliques X  is the classes of the hypercliques).

The results confirm the observations suggested by the previous two examples. If the hypercliques in a cluster are of one class, then the objects in that cluster are predominantly of the same class. On the other hand, if the hypercliques in a cluster are of mixed classes, then the objects in the cluster are also of mixed class, although they tend to be very heavily composed of the classes of the hypercliques. The worst case in the table is cluster 3, which ( a ) (b) (c) ( a ) (b) (c) ( d ) ( a ) (b) (c) ( d ) has hyperclique patterns from three classes, and has 59 out of 169 documents that are not of these three classes. The documents in the other clusters almost always match the labels of the corresponding hyperclique patterns. 5.3 The clustering evaluation of HICAP using entropy Figure 7 shows the entropy values of the clustering results from HICAP, UPGMA, and bisecting K-means at different user-specified numbers of clusters. Bisecting K-means yields significantly better entropy values than HICAP and UPGMA for all three data sets. This is due to the fact that the entropy measure favors clustering algorithms, such as bisecting K-means, that produce clusters that have relatively uniform cluster size. Also, for all three clustering algorithms, entropy values tend to decrease as the number of clusters increases. The reason for this is that, when the number of clusters is increased, the resulting clusters tend to be more pure. Thus, the difference in entropy among the three algorithms decreases as we increase the number of clusters.

Another observation from Fig. 7 is that HICAP performs slightly better than UPGMA in most cases for the given data sets. However, the performance difference between HICAP and UPGMA is tiny. This is not surprising since UPGMA starts from individual objects, while HICAP starts from hyperclique patterns (and the uncovered objects). 5.4 The clustering effect of K-CAP Here, we compare clustering results of K-CAP and bisecting K-means on data sets with a skewed class distribution. Figures 8 , 9 , 10 ,and 11 show the entropy values of K-CAP ( a ) (b) (c) ( d ) and the CLUTO implementation of bisecting K-means at different minimum h-confidence settings for the data sets RE0, RE1, WAP, and OH8. All these data sets have very skewed class distributions as shown in Figs. 8 a, 9 a, 10 a, and 11 a. For RE0, Fig. 8 b, c, and d shows the entropy values at different support thresholds. For all cases, the average entropy values achieved by K-CAP at different h-confidence thresholds is lower than the entropy value of CLUTO. In other words, the K-CAP algorithm can improve the clustering quality of bisecting K-means. Similar trends can also be observed for RE1, WAP, and OH8, as shown in Figs. 9 , 10 ,and 11 , respectively.

To show the effect of skewness of class distribution on the clustering results of K-CAP, we present an additional experiment in which we take equal sized samples of each of the OH8 classes. Figure 12 shows the clustering results for this case. As shown in the figure, for the data set with uniform class distribution, the clustering quality of K-CAP is only comparable to that of K-means. However, if we sample the classes so that the class distribution becomes even more skewed than it was originally, the K-CAP algorithm tends to do a much better job than bisecting K-means, as illustrated in Figs. 13 and 14 . Note that all sample data sets have equal numbers of documents and there are multiple iterations for each test case. 5.5 Performance comparison of different clustering algorithms In the previous subsection, we have showed that K-CAP can have better clustering quality than bisecting K-means on data sets with a skewed class distribution. In the literature, there are several clustering methods that are generally believed to perform well on data sets with a skewed class distribution, such as the unweighted pair-group average algorithm (UPGMA) [ 11 ], biased agglomerative UPGMA (BAgglo-UPGMA)[ 35 ], and DBSCAN [ 23 ]. Here, we ( a ) (b) (c) ( d ) (c) ( d ) ( a ) (b) (c) ( d ) compare the performance of bisecting K-means, UPGMA, DBSCAN, and BAgglo-UPGMA on data sets with a skewed class distribution.

Ta b l e 6 shows the entropy values of these algorithms on various different data sets for different parameter settings. In the table, we can observe that DBSCAN generally had poor clustering quality in most cases, regardless of whether noise objects were removed or not. This is not surprising since DBSCAN is a density-based clustering algorithm and does not perform well for data sets with high dimensionality. Although UPGMA has better performance than DBSCAN, it is still worse than BAgglo-UPGMA and Bisecting K-means. The reason is that UPGMA is a hierarchical clustering method that is sensitive to the choice of objects for initial clustering creation. If two objects are erroneously put in the same cluster at a lower level of hierarchy, this error cannot be corrected at a higher level. Finally, BAgglo-UPGMA is a hybrid approach that first applies K-means to form small groups and then uses hierarchical clustering to obtain the final set of clusters. This approach can alleviate problems due to erroneous merges during the initial stages and tends to produce a better quality clusters in terms of entropy. Nonetheless, in most cases, the clustering quality of BAgglo-UPGMA is still worse than bisecting K-means.

From the above, we know that bisecting K-means performs best among all these clustering algorithm, even when data sets have skewed class distributions. 5.6 Pattern preservation for reducing skewness of the class distribution The previous experiments reveal that a pattern preserving approach to clustering based on the hyperclique pattern can improve K-means clustering quality for data sets with a skewed class distribution. In this section, we provide an explanation for why K-CAP improves clustering quality for these cases. Specifically, larger classes tend to have a greater reduction in size than smaller classes when the objects in a hyperclique are replaced by the centroid of the hyperclique. Since it is well-known that K-means has trouble when clusters are of widely varying sizes, we hypothesize that this reduction in the skewness of the class distribution results in an improvement in K-means performance.

To illustrate the reduction of skewness of the class distribution, we performed two exper-iments. Figures 15 and 16 show the change of class distributions with the increase in the number of hyperclique patterns for the RE0 and OH8 data sets, respectively. To better show the skewness of class distribution, we have added a linear regression line to each of the figures. For both data sets, the slope of this line decreases. In other words, the skewness of the class distribution is reduced as the number of hyperclique patterns increases. For instance, when there are 85 hyperclique patterns for the OH8 data set, the slope of class distribution becomes  X  6 . 879. The original distribution had a slope of  X  11 . 267. 6 Conclusions In this paper, we have introduced a new goal for clustering algorithms, namely, the preserva-tion of patterns, such as hyperclique patterns, that capture strong connections between groups of objects. Without such an explicit goal, clustering algorithms tend to find clusters that split the objects or attributes in these patterns between different clusters. However, keeping these patterns together has the potential to greatly aid cluster interpretation.

To that end, we presented two pattern preserving clustering algorithms: HIerarchical Clus-tering with pAttern Preservation (HICAP) and bisecting K-means Clustering with pAttern Preservation (K-CAP). HICAP is based on the Group Average (UPGMA) agglomerative clustering technique and uses maximal hyperclique patterns to define the initial clusters. In contrast, K-CAP exploits the best properties of the hyperclique pattern and bisecting K-means. As demonstrated by our experimental results, HICAP can produce overlapping clusters (thus, not splitting patterns), and has a better interpretation on the clustering results. However, the cluster quality of HICAP is worse than bisecting K-means with respect to entropy [ 31 ] and HICAP is computationally expensive. In contrast, our experimental results showed that K-CAP, which also preserves hyperclique patterns, produces better quality clus-tering results than bisecting K-means for data with widely differing clusters sizes, and retains the computational efficiency of bisecting K-means.
There are several potential directions for future research. We plan to further quantify the performance gains of K-CAP with respect to bisecting K-means by performing analysis for data with different types of class distributions. We also plan to further investigate why K-CAP reduces the skewness of the class distributions. Specifically, why are larger classes reduced by a larger factor than smaller classes? Finally, we propose to extend our methodology to handle data sets with continuous variables by using the continuous hyperclique approach we developed in [ 25 ].
 References Author Biographies
