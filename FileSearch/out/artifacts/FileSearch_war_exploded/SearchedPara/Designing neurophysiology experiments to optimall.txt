 A long standing problem in neuroscience has been collecting enough data to robustly estimate the response function of a neuron. One approach to this problem is to sequentially optimize a series of experiments as data is collected [1, 2, 3, 4, 5, 6]. To make optimizing the design tractable, we typically need to assume our knowledge has some nice mathematical representation. This restriction often makes it difficult to include the types of prior beliefs held by neurophysiologists; for example that the receptive field has some parametric form such as a Gabor function [7]. Here we consider Figure 1: A schematic illustrating how we use the manifold to improve stimulus design. Our method begins with a Gaussian approximation of the posterior on the full model space after t trials, p ( ~ next step involves constructing the tangent space approximation of the manifold M on which ~  X  is be-lieved to lie, as illustrated in the middle plot; M is indicated in blue. The MAP estimate (blue dot) is projected onto the manifold to obtain ~ X  M ,t (green dot). We then compute the tangent space (dashed red line) by taking the derivative of the manifold at ~ X  M ,t . The tangent space is the space spanned by vectors in the direction parallel to M at ~ X  M ,t . By definition, in the neighborhood of ~ X  M ,t , moving along the manifold is roughly equivalent to moving along the tangent space. Thus, the tangent space which are probable under p ( ~  X  | ~ X  t , C t ) and close to the manifold. the problem of incorporating this strong prior knowledge into an existing algorithm for optimizing neurophysiology experiments [8].
 We start by assuming that a neuron can be modeled as a generalized linear model (GLM). Our prior knowledge defines a subset of all GLMs in which we expect to find the best model of the neuron. We represent this class as a sub-manifold in the parameter space of the GLM. We use the manifold to design an experiment which will provide the largest reduction in our uncertainty about the unknown parameters. To make the computations tractable we approximate the manifold using the tangent space evaluated at the maximum a posteriori (MAP) estimate of the parameters projected onto the manifold. Despite this rather crude approximation of the geometry of the manifold, our simulations show that this method can significantly improve the informativeness of our experiments. Furthermore, these methods work robustly even if the best model does not happen to lie directly on the manifold. We begin by summarizing the three key elements of an existing algorithm for optimizing neuro-physiology experiments. A more thorough discussion is available in [8]. We model the neuron X  X  response function as a mapping between the neuron X  X  input at time t , ~s t , and its response, r t . We define the input rather generally as a vector which may consist of terms corresponding to a stimulus, is typically a non-negative integer corresponding to the number of spikes observed in a small time window. Since neural responses are typically noisy, we represent the response function as a con-the input for which observing the response will provide the most information about the parameters ~  X  defining the conditional response function. approximated by a generalized linear model [9, 10]. The likelihood of the response depends on the firing rate,  X  t , which is a function of the input, where f () is some nonlinear function which is assumed known 1 . To identify the response function, we need to estimate the coefficients of the linear projection, ~  X  . One important property of the GLM is that we can easily derive sufficient conditions to ensure the log-likelihood is concave [11]. The second key component of the algorithm is that we may reasonably approximate the posterior on ~  X  as Gaussian. This approximation is justified by the log-concavity of the likelihood function and asymptotic normality of the posterior distribution given sufficient data [12]. As a result, we can re-Here ( ~ X  t , C t ) denote the mean and covariance matrix of our Gaussian approximation: ~ X  t is set to the MAP estimate of ~  X  , and C t to the inverse Hessian of the log-posterior at ~ X  t . The final component is an efficient method for picking the optimal input on the next trial, ~s t +1 . Since the purpose of an experiment is to identify the best model, we optimize the design by max-mutual information measures how much we expect observing the response to ~s t +1 will reduce our uncertainty about ~  X  . We pick the optimal input by maximizing the mutual information with respect to ~s t +1 ; as discussed in [8], this step, along with the updating of the posterior mean and covariance ( ~ X  t , C t ) , may be computed efficiently enough for real-time implementation in many cases. 2.1 Optimizing experiments to reduce uncertainty along parameter sub-manifolds.
 For the computation of the mutual information to be tractable, the space of candidate models,  X  , must have some convenient form so that we can derive a suitable expression for the mutual infor-mation. Intuitively, to select the optimal design, we need to consider how much information an experiment provides about each possible model. Evaluating the mutual information entails an in-tegral over model space,  X  . The problem with incorporating prior knowledge is that if we restrict the model to some complicated subset of model space we will no longer be able to efficiently inte-grate over the set of candidate models. We address this problem by showing how local geometric approximations to the parameter sub-manifold can be used to guide optimal sampling while still maintaining a flexible, tractable representation of the posterior distribution on the full model space. In many experiments, neurophysiologists expect a-priori that the receptive field of a neuron will have some low-dimensional parametric structure; e.g the receptive field might be well-approximated by a Gabor function [13], or by a difference of Gaussians [14], or by a low rank spatiotemporal matrix [15, 13]. We can think of this structure as defining a sub-manifold, M , of the full model space,  X  , The vector, ~  X  , essentially enumerates the points on the manifold and  X () is a function which maps these points into  X  space. A natural example is the case where we wish to enforce the constraint that ~  X  has some parametric form, e.g. a Gabor function. The basic idea is that we want to run experiments which can identify exactly where on the manifold the optimal model lies. Since M can have some arbitrary nonlinear shape, computing the informativeness of a stimulus using just the models on the manifold is not easy. Furthermore, if we completely restrict our attention to models in M then we ignore the possibility that our prior knowledge is incorrect. Hence, we do not force the posterior distribution of ~  X  to only have support on the manifold. Rather, we maintain a Gaussian approximation of the posterior on the full space,  X  . However, when optimizing our stimuli we combine our posterior with our knowledge of M in order to do a better job of maximizing the informativeness of each experiment. weighted by the posterior probability on each model. We integrate over model space because the informativeness of an experiment clearly depends on what we already know (i.e. the likelihood we assign to each model given the data and our prior knowledge). Furthermore, the informativeness of an experiment will depend on the outcome. Hence, we use what we know about the neuron to make predictions about the experimental outcome. Unfortunately, since the manifold in general has some arbitrary nonlinear shape we cannot easily compute integrals over the manifold. Furthermore, we do not want to continue to restrict ourselves to models on the manifold if the data indicates our prior knowledge is wrong.
 We can solve both problems by making use of the tangent space of the manifold, as illustrated in Figure 1 [16]. The tangent space is a linear space which provides a local approximation of the manifold. Since the tangent space is a linear subspace of  X  , integrating over ~  X  in the tangent space is much easier than integrating over all ~  X  on the manifold; in fact, the methods introduced in [8] may be applied directly to this case. The tangent space is a local linear approximation evaluated at a particular point, ~ X  M ,t , on the manifold. For ~ X  M ,t we use the projection of ~ X  t onto the manifold (i.e., ~ X  M ,t is the closest point in M to ~ X  t ). Depending on the manifold, computing ~ X  M ,t can be nontrivial; the examples considered in this paper, however, all have tractable numerical solutions to this problem.
 The challenge is representing the set of models close to ~ X  M ,t in a way that makes integrating over the models tractable. To find models on the manifold close to ~ X  M ,t we want to perturb the parameters ~  X  about the values corresponding to ~ X  M ,t . Since  X  is in general nonlinear, there is no simple expression for the combination of all such perturbations. However, we can easily approximate the set of ~  X  resulting from these perturbations by taking linear combinations of the partial derivatives of  X  with respect to ~  X  . The partial derivative is the direction in  X  in which ~  X  moves if we perturb one of the manifold X  X  parameters. Thus, the subspace formed by linear combinations of the partial derivatives approximates the set of models on the manifold close to ~ X  M ,t . This subspace is the tangent space, where orth is an orthonormal basis for the column space of its argument. Here T x M denotes the tangent space at the point x . The columns of B denote the direction in which ~  X  changes if we perturb one of the manifold X  X  parameters. (In general, the directions corresponding to changes in different parameters are not independent; to avoid this redundancy we compute a set of basis vectors for the space spanned by the partial derivatives.) We now use our Gaussian posterior on the full parameter space to compute the posterior likelihood of the models in the tangent space. Since the tangent space is a subspace of  X  , restricting our Gaussian approximation of the posterior. Mathematically, we are conditioning on ~  X   X  T ~ X  M ,t M . The result is a Gaussian distribution on the tangent space whose parameters may be obtained using the standard Gaussian conditioning formula: where N denotes a normal distribution with the specified parameters. Now, rather than optimizing down as much as possible (a very difficult task in general), we pick the stimulus which best reduces rectly using the methods presented in [8]. Finally, to handle the possibility that ~  X  /  X  X  , every so sures that asymptotically we do not ignore directions orthogonal to the manifold; i.e., that we do not Figure 2: MAP estimates of a STRF obtained using three designs: the new info. max. tangent space design described in the text; an i.i.d. design; and an info. max. design which did not use the assumption that ~  X  corresponds to a low rank STRF. In each case, stimuli were chosen under the spherical power contraint, || ~s t || 2 = c . The true STRF (fit to real zebrafinch auditory responses and then used to simulate the observed data) is shown in the last column. (For convenience we rescaled the coefficients to be between -4 and 4). We see that using the tangent space to optimize the design leads to much faster convergence to the true parameters; in addition, either infomax design significantly outperforms the iid design here. In this case the true STRF did not in fact lie on the manifold M (chosen to be the set of rank-2 matrices here); thus, these results also show that our knowledge of M does not need to be exact in order to improve the experimental design. get stuck obsessively sampling along the incorrect manifold. As a result,  X  t will always converge asymptotically to the true parameters, even when  X  6 X  M .
 To summarize, our method proceeds as follows: 3.1 Low rank models To test our methods in a realistic, high-dimensional setting, we simulated a typical auditory neu-rophysiology [17, 15, 18] experiment. Here, the objective is to to identify the spectro-temporal receptive field (STRF) of the neuron. The input and receptive field of the neuron are usually repre-sented in the frequency domain because the cochlea is known to perform a frequency decomposition of sound. The STRF,  X  (  X ,  X  ) , is a 2-d filter which relates the firing rate at time t to the amount of energy at frequency  X  and time t  X   X  in the stimulus. To incorporate this spectrotemporal model in the standard GLM setting, we simply vectorize the matrix  X  (  X ,  X  ) .
 Estimating the STRF can be quite difficult due to its high dimensionality. Several researchers, however, have shown that low-rank assumptions can be used to produce accurate approximations of the receptive field while significantly reducing the number of unknown parameters [19, 13, 15, 20]. A low rank assumption is a more general version of the space-time separable assumption that is often used when studying visual receptive fields [21]. Mathematically, a low-rank assumption means that the matrix corresponding to the STRF can be written as a sum of rank one matrices, where M at indicates the matrix formed by reshaping the vector ~  X  to form the STRF. U and V are low-rank matrices with orthonormal columns. The columns of U and V are the principal components of the column and row spaces of  X  respectively, and encode the spectral and temporal properties of the STRF, respectively.
 We simulated an auditory experiment using an STRF fitted to the actual response of a neuron in the Mesencephalicus lateralis pars dorsalis (MLd) of an adult male zebra finch [18]. To reduce the di-mensionality we sub-sampled the STRF in the frequency domain and shortened it in the time domain to yield a 20  X  21 STRF. We generated synthetic data by sampling a Poisson process whose instan-taneous firing rate was set to the output of a GLM with exponential nonlinearity and ~  X  proportional to the true measured zebra finch STRF.
 For the manifold we used the set of ~  X  corresponding to rank-2 matrices. For the STRF we used, the rank-2 assumption turns out to be rather accurate. We also considered manifolds of rank-1 and rank-5 matrices (data not shown), but rank-2 did slightly better. The manifold of rank r matrices is convenient because we can easily project any ~  X  onto M by reshaping ~  X  as a matrix and then computing its singular-value-decomposition (SVD). ~ X  M ,t is the matrix formed by the first r singular vectors of ~ X  t . To compute the tangent space, Eqn. 3, we compute the derivative of ~  X  with respect to each component of the matrices U and V . Using these derivatives we can linearly approximate the effect on  X  of perturbing the parameters of its principal components.
 In Figure 3.1 we compare the effectiveness of different experimental designs by plotting the MAP estimate ~ X  t on several trials. The results clearly show that using the tangent space to design the experiments leads to much faster convergence to the true parameters. Furthermore, using the as-sumption that the STRF is rank-2 is beneficial even though the true STRF here is not in fact rank-2. 3.2 Real birdsong data We also tested our method by using it to reshuffle the data collected during an actual experiment to find an ordering which provided a faster decrease in the error of the fitted model. During the experiments, we recorded the responses of MLd neurons when the songs of other birds and ripple noise were presented to the bird (again, as previously described in [18]). We compared a design which randomly shuffled the trials to a design which used our info. max. algorithm to select the order in which the trials are processed. We then evaluated the fitted model by computing the expected log-likelihood of the spike trains, P  X  E ~  X  | ~ X  when inputs in a test set are played to the bird.
 To constrain the models we assume the STRF is low-rank and that its principal components are smooth. The smoothing prior means that if we take the Fourier transform of the principal compo-nents, the Fourier coefficients of high frequencies should be zero with high probability. In other words, each principal component (the columns of U and V ) should be a linear combination of sinu-soidal functions with low frequencies. In this case we can write the STRF as Each column of F and T is a sine or cosine function representing one of the basis functions of the principal spectral (columns of F ) or temporal (columns of T ) components of the STRF. Each column of  X  and  X  determines how we form one of the principal components by combining sine and cosine functions.  X  is a diagonal matrix which specifies the projection of  X  onto each principal Figure 3: Plots comparing the performance of an info. max. design, an info. max. design which uses the tangent space, and a shuffled design. The manifold was the set of rank 2 matrices. The plot shows the expected log-likelihood (prediction accuracy) of the spike trains in response to a birdsong in the test set. Using a rank 2 manifold to constrain the model produces slightly better fits of the data. component. The unknown parameters in this case are the matrices  X  ,  X  , and  X  . The sinusoidal functions corresponding to the columns of F and T should have frequencies { 0 , . . . , f o,f m f } and period corresponds to the dimensions of the STRF. m f and m t are the largest integers such that f o,f m f and f o,t m t are less than the Nyquist frequency. Now to enforce a smoothing prior we can simply restrict the columns of F and T to sinusoids with low frequencies. To project  X  onto the manifold we simply need to compute  X ,  X  and  X  by evaluating the SVD of F T  X  T .
 The results, Figure 3, show that both info. max. designs significantly outperform the randomly shuffled design. Furthermore, incorporating the low-rank assumption using the tangent space im-proves the info. max. design, albeit only slightly; the estimated STRF X  X  are shown in Figure 4. It is worth noting that in an actual online experiment, we would expect a larger improvement with the info. max. design, since during the experiment we would be free to pick any input. Thus, the different designs could choose radically different stimulus sets; in contrast, when re-analyzing the data offline, all we can do is reshuffle the trials, but the stimulus sets remain the same in the info. max. and iid settings here. We have provided a method for incorporating detailed prior information in existing algorithms for the information-theoretic optimal design of neurophysiology experiments. These methods use re-alistic assumptions about the neuron X  X  response function and choose significantly more informative stimuli, leading to faster convergence to the true response function using fewer experimental trials. We expect that the inclusion of this strong prior information will help experimentalists contend with the high dimensionality of neural response functions. We thank Vincent Vu and Bin Yu for helpful conversations. JL is supported by the Computa-tional Science Graduate Fellowship Program administered by the DOE under contract DE-FG02-97ER25308 and by the NSF IGERT Program in Hybrid Neural Microsystems at Georgia Tech via grant number DGE-0333411. LP is supported by an NSF CAREER award and a Gatsby Initiative in Brain Circuitry Pilot Grant. Figure 4: The STRFs estimated using the bird song data. We plot ~ X  t for trials in the interval over which the expected log-likelihood of the different designs differed the most in Fig. 3. The info. max. designs converge slightly faster than the shuffled design. In these results, we smoothed the STRF by only using frequencies less than or equal to 10 f o,f and 2 f o,t .
 [1] P. Foldiak, Neurocomputing 38 X 40 , 1217 (2001). [2] R. C. deCharms, et al. , Science 280 , 1439 (1998). [3] T. Gollisch, et al. , Journal of Neuroscience 22 , 10434 (2002). [4] F. Edin, et al. , Journal of Computational Neuroscience 17 , 47 (2004). [5] C. Machens, et al. , Neuron 47 , 447 (2005). [6] K. N. O X  X onnor, et al. , Journal of Neurophysiology 94 , 4051 (2005). [7] D. L. Ringach, J Neurophysiol 88 , 455 (2002). [8] J. Lewi, et al. , Neural Computation 21 (2009). [9] E. Simoncelli, et al. , The Cognitive Neurosciences , M. Gazzaniga, ed. (MIT Press, 2004). [10] L. Paninski, et al. , Computational Neuroscience: Theoretical Insights into Brain Function [11] L. Paninski, Network: Computation in Neural Systems 15 , 243 (2004). [12] L. Paninski, Neural Computation 17 , 1480 (2005). [13] A. Qiu, et al. , J Neurophysiol 90 , 456 (2003). [14] C. Enroth-Cugell, et al. , Journal of Physiology 187 , 517 (1966). [15] J. F. Linden, et al. , Journal of Neurophysiology 90 , 2660 (2003). [16] J. M. Lee, Introduction to Smooth Manifolds (Springer, 2000). [17] F. E. Theunissen, et al. , Journal of Neuroscience 20 , 2315 (2000). [18] S. M. Woolley, et al. , The Journal of Neuroscience 26 , 2499 (2006). [19] D. A. Depireux, et al. , Journal of Neurophysiology 85 , 1220 (2001). [20] M. B. Ahrens, et al. , Network 19 , 35 (2008). [21] G. C. DeAngelis, et al. , J Neurophysiol 69 , 1091 (1993).
