 We present a novel framework for answering complex questions that relies on question decomposition. Complex questions are de-composed by a procedure that operates on a Markov chain, by following a random walk on a bipartite graph of relations estab-lished between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that mani-fest these relations. Decomposed questions discovered during this random walk are then submitted to a state-of-the-art Question An-swering (Q/A) system in order to r etrieve a set of passages that can later be merged into a comprehensive answer by a Multi-Document Summarization (MDS) system. In our evaluations, we show that access to the decompositions generated using this method can sig-nificantly enhance the relevance and comprehensiveness of summary-length answers to complex questions.
 H.3.m [ INFORMATION STORAGE AND RETRIEVAL ]: Mis-cellaneous; I.2.7 [ ARTIFICIAL INTELLIGENCE ]: Natural Lan-guage Processing Algorithms, Performance, Experimentation Question Answering, Summarization
Complex questions cannot be answered using the same tech-niques that apply to  X  X actoid X  questions. Complex questions re-fer to relations between entities or events; they refer to complex processes and model scenarios that involve deep knowledge of the topic under investigation. For example, a question like Q are the key activities in the rese arch and development phase of cre-ating new drugs?  X  looks for information on two distinct phases of creating drugs. Typically, relevant information for these kinds Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. of questions can be found in multip le documents and needs to be fused together into a final answer. In the Document Understand-ing Conferences (DUC), the answer to complex questions like Q is considered to be a multi-sentence multiple document summary (MDS) that meets the information need of the question. We intro-duce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions (of the complex question); (b) factoid question answering (Q/A) techniques (to pro-cess decomposed questions); and (c) multi-document summariza-tion techniques (to fuse together the answers provided for each de-composed question). Central to this process is a question decom-position model that enables the selection of the textual information aggregated in the final answer.

We present a novel question decomposition procedure that op-erates on a Markov chain model inspired from the Markov chains used for expanding language models introduced in [9]. We propose that question decomposition depe nds on the successi ve recognition (and exploitation) of the relations that exist between words and con-cepts extracted from topic-relevant sentences. (For the purposes of this paper, we will define a relation as any semantic property that can exists between two or more entities or events in texts.) For ex-ample, if a topic-relation r 1 between  X  X evelop X  and  X  X rugs X  is rec-ognized in question Q 0 , we assume that this sentence (and all other sentences containing this particular relation) will contain relevant information that can be used to decompose of Q 0 .Furthermore,we expect that sentences containing topic-relevant relations will also contain other relevant relations that should be leveraged in question decomposition. For example, if r 1 is identified in the sentence s  X  The challenge for Glaxo was to develop a drug that was pleasant to swallow.  X , we expect that a new relation r 2 between the concept COMPANY ( X  X laxo X ) and  X  X evelop X  should be extracted and used to identify still other sentences that could potentially provide rele-vant information. As new relations are discovered, we expect that sentences containing the most relevant relations (or combinations of relations) can be used to generate questions that can represent possible decompositions of the original complex question. For ex-ample, given r 1 and r 2 in s 1 , a question like  X  What companies develop new drugs?  X  can be created which could be used to obtain a set of answers which could represent a partial response to Q Relevant answers to each newly-decomposed question can be used to discover more relevant relations, that in turn, prompt still more question decompositions. This process ends when either no new relations are discovered, or the r andom walk is stabilizing within a threshold.

We evaluate question decompositions in three ways. First, we compare them against decompositions produced by humans. Sec-ond, we conduct several evaluati ons of the quality of the MDS an-swers they enable. Third, we use every sentence from the MDS answer and generate questions with the same procedure employed when creating question decompositions from relevant sentences. The questions that have answers in the summaries are evaluated against questions generated by human linguists. They are also used for measuring the similarity to the decomposed questions. Our studies indicate that these comparisons correlate with the relevance of the answers. We claim that this is an important finding since current MDS evaluation methods typically rely on (a) human pro-duced answers, or (b) human judgments. The automatic scoring of the MDS answers based on comparisons of decomposed ques-tions allows a framework in which researchers can test multiple Q/A techniques or multiple MDS techniques that best operate for finding answers.

The remainder of the paper is organized as follows. Section 2 presents the framework we have designed for processing complex questions. Section 3 details the question decomposition procedure. Section 4 describes the random walk models employed for decom-posing questions. Section 5 details the evaluation results while Sec-tion 6 summarizes the conclusions.
In this section, we outline three methods for producing answers to complex questions from based on the output of a question de-composition system. By decomposing a complex question into a set of simpler subquestions that each represent a different dimen-sion of the complex question X  X  information need, we expect to be able to identify answers that are both informative and responsive.
In this paper, we introduce a new technique for question decom-position that uses a random walk in order to generate possible de-compositions of a complex question. Figure 1 illustrates the system described in this paper.

Figure 1 includes two types of question decomposition modules: a syntactic question decomposition module and a random walk-based question decomposition module. With syntactic question de-composition, overtly-mentioned subquestions are extracted from a complex question by separating conjoined phrases and recognizing embedded questions. While syntactic decomposition is an impor-tant part of any question decomposition algorithm, we will not be discussing techniques for this type of decomposition in this paper. For more information on syntactic decomposition, see [8].
After complex questions are decomposed syntactically, as illus-trated in Figure 1, keywords are extracted from each sub-question and are expanded with keyword alternations. The keywords are ex-panded by (1) identifying the semantic class to which they belong, and (2) using other terms from the lexicons associated with such semantic classes. To identify the semantic class, the keyword is matched against the lexicon of the class. The keyword alternations are selected from the first 20 scored words from the lexicon. The semantic classes are acquired off-line with a co-training method reported in [18].
Figure 2 illustrates the keyword alternations resulting for the keywords  X  X esearch X  and  X  X rug X  that were extracted from the sub-question  X  What are the key activities i n the research phase of cre-ating new drugs?  X .

Additionally, we use two different models of topic signatures to identify (a) the most representative relations for the topic referred by the complex question and evidence by the document collec-tion. The first topic signature ( TS 1 )wehaveimplementedwas reported in [10]. TS 1 is defined by a set of terms t i , where each term is highly correlated with the topic with an associated weight w : TS 1 = { topic ( t 1 ,w 1 ) , ( t 2 ,w 2 ) ,..., ( t n ,w lection of the terms for TS 1 as well as the assignment of the as-sociation weights is determined by the use of the likelihood ra-tio. The second topic signature ( TS 2 ) was introduced in [3]. It takes into account the fact that topics are not characterized only by terms, there are also relations between topic concepts that need to be identified. If only nouns and verbs from TS 1 are selected as topic concepts, the topic signature TS 2 is defined as TS { topic, ( r 1 ,w 1 ) , ( r 2 ,w 2 ) ,... ( r n ,w n ) } ,where r lation between two topic concepts. The procedure of generating TS 2 was detailed in [3], and it identifies two forms of relations: (a) syntax-based relations, and (b) salience-based context relations. The arguments of these relations may be (1) nouns or nominal-izations; (2) named entity type s that a Named Entity Recognizer (NER) identifies; and (3) verbs.
When topic signatures are available, each sentence from the doc-ument collection receives a score based on (a) the presence of a term from TS 1 ; (b) the presence of a relation from TS 2 presence of any of the keywords extracted from the sub-question or their alternations. The sentence scores determine a ranking of the sentences from the collection for each sub-question. Finally, the answer is produced by selecting for each decomposition only the corresponding highest ranked sentences. Redundancy is eliminated by checking that each new added sentence does not contain any predicate-argument relation that was already present in a previously selected sentence. Predicate-argument relations are discovered by processing sentences with a semantic parser trained on PropBank [16]. Additionally, each predicate and argument is mapped into ev-ery WordNet synonym to enable paraphrase identification. In this way Answer Summary 1 from Figure 1 is produced.

In addition to the method described above, complex questions can also be decomposed by another method that is described in Sections 3 and 4. Due to this, in our framework we can produce two additional answers as summaries. The second form of question decomposition discovers relations relevant to the complex question and sentences in which they are present. For each such sentence, one or multiple questions are generated, representing additional question decompositions. When these decompositions are ignored and only the sentences are considered, the topic signatures can be used to score them and to produce a second answer as summary (Answer Summary 2 illustrated in Figure 1).

When complex questions are decomposed using random walks, subquestions are submitte d a state-of-the-art question-answering (Q/A) system (described in [5]), which returns sets of ranked rele-vant answers for each such decomposition. All these answers are considered separate documents, which are used to produce a multi-document summary as the third answer (MDS) (Answer Summary 3 illustrated in Figure 1). The MDS system that was used has been reported in [7]. Furthermore, for each sentence in the third answer, we generate one or several questions with the same technique that is used for decomposing questions with random walks. Since ques-tions produced from the complex question, and questions produced from the answer are available, we argue that the answer is relevant if the two sets of questions are very similar. Question comparison is produced by a battery of four question similarity measures, pre-viously reported in [4]. In Section 5 of this paper we detail the similarity measures we used in the experiments. The selection of only the most similar questions improves the quality of the answer. Instead of submitting all questions generated by the random walks, only the most similar questions are processed again by the Q/A sys-tem, thus closing a fee dback loop. Using a hill-climbing technique, if the aggregate similarity of the new set of questions derived from the new answer is improved significantly, the feedback loop starts again. The aggregate similarity is also described in Section 5 of this paper.

The feedback loop ends either when new improvements are not obtained, or when the number of loops is larger than a threshold, in our case L T =7 . With this framework , we were able to study the effects of different forms of question decompositions on the quality of the answers.
In order to process complex questions like Q 0 : X  What are the key activities in the research and development phase of creating new drugs?  X , current Q/A systems need to decompose the ques-tion in a series of simpler questions, that can be tackled by the factoid-based techniques that have emerged from the TREC Q/A evaluations. Table 1 illustrates some of the questions that represent decompositions of Q 0 and can be generated automatically by the technique we present in this section.

The main feature of the decomposed question is related to the ability to easily detect their expected answer type (EAT), which represents the semantic class to which their answers should be-long. For example, the EAT of Q 1 0 is O RGANIZATION ,theEAT of Q 2 0 is D ISEASE , whereas the EAT of Q 3 0 is D URATION main assumption is that the question decomposition model should be based on several types of relations between words or concepts used in (a) the complex question, (b) in sentences that contain rele-vant information for the complex question, or (c) in other question decompositions that have been produced before for the same com-plex question.

In order to produce question decompositions, we follow four steps. In the first step we process the complex question for de-riving the relations that are meaningful. In the second step we gen-erate questions based on the relations selected. In the third step we enhance the meaningful set of relations with relations discov-ered when generating a question decomposition and then we select a new relation based on the latest decomposition. In the fourth step, we loop back to step 2 unless the probability to continue is not above a certain threshold. The detailed operations in each step are:
STEP 1: The complex question is lexically, syntactically and se-mantically analyzed with the goal of identifying the relationships between words that may lead to the generations of simpler ques-tions. The three forms of knowledge are marked up in each of the phases of the analysis: 1.a. ( lexical ) The determination of the part-of-speech of each word, generated by the Brill tagger [1]. 1.b. ( syntactic ) A full parse of the question is generated by the probabilistic parser reported in [2]. The result of the parse renders information about the syntactic c onstituents of the question and about their relations. For example, for the complex question Q derive the following constituents: VP 1 : { are } ; VP 2 : NP 1 : { the key activities } ; NP 2 : { the research } ; NP phase } ; NP 4 : { NP 2 and NP 3 } ; NP 5 : { new drugs } of NP 5 } ; PP 2 : { NP 1 in PP 1 } 2 . 1.c. ( lexical ) For each base NP (e.g. NP 1 , NP 2 , NP 3 determine whether the head is a nominalization of some verb, by accessing the WordNet database [12]. For example, the noun  X  de-velopment  X  is a morphological derivation of the verb  X  develop  X . The NPs having heads which are nominalizations are not consid-ered in Step 1.d. 1.d. ( lexical/semantic ) The generality of the heads of each NP is assessed in one of the two categories: abstract ,or concrete .The assessment is based on a large answer type taxonomy that was de-veloped for the TREC evaluations of Q/A systems. The taxonomy, which was described in [17] comprises 440 synsets from WordNet (and their hyponyms) and 150 semantic classes of names that are recognized by the Named Entity R ecognizers we have available. If any of the heads of an NP is found in the answer taxonomy, it is assigned the attribute concrete , otherwise it is labeled abstract .For the question Q 0 , only the head of NP 5 is categorized as concrete . NP 1 is labeled abstract . The question processing techniques ap-
NP stands for noun phrase, VP for verb phrase, and PP for prepo-sitional phrase plied for factoid Q/A identify NP 1 as being the constituent that indicates the EAT for the question. Since no EAT can be estab-lished for Q 0 , it is considered a complex question. 1.e. ( syntactic ) Relations between concrete NPs and other con-stituents are sought. The syntactic re lationship from the constituent PP 1 indicates a prepositional attachment relation between NP and NP 4 , which is a coordination between NP 2 and NP 3 .The syntactic decomposition of the coordination entails two relations between the verbs related to NP 2 and NP 3 and NP 5 . The output of Step 1 for Q 0 is: RELATIONS: { R 1 = [develop  X  new drugs]; R 2 = [research  X  new drugs] }
STEP 2: For a relation discovered at Step 1 we generate ques-tions that involve that relation. In order to generate questions au-tomatically, we employ a method that was first reported in [4]. In order to generate the question, we first find a sentence that consti-tutes an answer for that question. This is done by the following sub-steps: 2.a. Query Formulation . In order to find sentences in which el-ements from the RELATIONS list are discovered, we formulated two kinds of queries: (a) queries involving the lexical arguments of the relation, e.g. [ X  X evelop X  AND  X  X rug X  X  as well as (b) queries that involved semantic extensions. Four forms of extensions were considered: (1) extensions based on the semantic class of names that represent the nominal category (e.g. names of drugs), (2) extensions based on verbs which are semantically related to the verb in the WordNet database (e.g. develop(v)  X  X em.relation ate(v); develop(v)  X  X em.relation  X  produce(v)); (3) extensions that allow the nominal to be anaphoric, therefore replaced by a pronoun, e.g. [develop  X  it]; and (4) extensions that allow the nominaliza-tions, as well as the verbal conjuncts, to be considered. 2.b. Sentence Retrieval . We built an index based on the processing of relations in the text collection 3 . A sentence is added to the in-verted list of a relation r i when it may be composed with another relation r j in the same sentence and (a) relations r i and r one of the conditions listed in Table 2, or the predicates of relations r and r j may be composed with the predicate composition pro-cedure described as a special case in 2.c; and (b) the argument of the relation r j can be mapped in one of the EAT categories of the Q/A system. Examples of such sentences are illustrated in Table 3. Sentence S 1 is retrieved because it contains relation r i  X  drugs] and also a relation r j = [develop  X  Glaxo] that shares the same predicate ( X  X evelop X ) and  X  X laxo X  is mapped into the EAT = COMPANY. Similarly, sentences S 2 , S 3 ,and S 4 are retrieved because they contain three different expansions of r i and new rela-tions that are compatible with it. 2.c. Question Generation . Every sentence retrieved at 2.b. contains additional relations, besides those that were expressed by the query. Among those relations, some share arguments with the queried re-lations, some do not. The first group of relations may serve to point to EATs that the decomposed questions should refer to. For ex-ample, in sentence S 1 , the new relation [Glaxo  X  develop] can be generalized into [ORGANIZATION  X  develop] in which ORGA-NIZATION can be selected as the EAT of the question that shall be generated. Our named entity recognizer (NER) is able to dis-tinguish between different types of organizations, tagging  X  X laxo X 
We process the text collection and discover all syntactic and salient relations when we build the topic signature TS 2 described in Section 2. from sentence S 1 as COMPANY, and  X  X edical School X  from S as UNIVERSITY. When the EAT is established, the question stem that is associated with it is known (e.g.  X  X hat companies X ) and it substitutes the name from the sentence, to generate the question Q 0 from Table 1, in which relation R 1 is fully specified with all the argument adjuncts it had in the complex question Q 0 .Sentence S 1 generates the question Q 1 0 , whereas sentence S 4 generates the question  X  What universities develop drugs?  X . Sentence S trated in Table 3 enables the generation of Q 2 0 , whereas sentence S 6 is used for generating Q 3 0 . Starting from relation R TIONS, three new relations are discovered: R 1 1 =[[ R 1 =develop X  drug]  X  COMPANY], R 2 1 =[[ R 1 = develop  X  drug]  X  DISEASE], and R 3 1 =[[ R 1 = develop  X  drug]  X  DURATION]. Each of these new relations enable the generation of the decomposed questions listed in Table 1.
 Table 3: Sentences retrieved for relation R 1 : [develop  X  drug].
Special Cases . The properties between relations r i and r are used in the index cover three more cases that need to be ad-dressed by question generation. They are: 2.c. Argument Specialization . In order to inquire about the at-tributes of arguments, three forms of questions are generated: (i) questions that inquire about inst ances of en tities that are referred by the argument of a relation in which the semantic class of the argument is the EAT of the question, and the question becomes a list question; (ii) questions that specialize the argument of the rela-tion by using a modifier which becomes the EAT of the question; and (iii) questions that inquire about the characteristics of the ar-guments by using the question stem  X  X hat types X . An example of the first form of questions is Q 6 0 : X  What new drugs have been developed?  X , generated from the sentence:  X  Zinnat is a new drug which was developed because other drugs in its class needed to be injected and were therefore of little use outside the hospital environ-ment.  X . An example of the second form of questions is Q 7 many medicines are launched per year?  X  , in which the EAT is NUMBER, it modifies the argument  X  X edicines X  in sentence  X  The number of medicines launched during the early 1980s averaged about 60 per year.  X .
 2.d. Predicate Specialization . There are three ways of specializing the predicates from the relations: (i) by selecting the EAT of the question as a MANNER, and associating the question stem  X  X ow X ; (ii) by using adjuncts of the predicates in the question to produce ei-ther a specialized MANNER EAT or a YES/NO question; and (iii) by considering that the predicates represent complex events that have structure, and thus this structure can be inquired by using spe-cial constructs of the form  X  X hat steps are included in X , or  X  X hat types of activities are included in X . The first form of predicate spe-cialization is the most productive one, and it can be generated based on the recognition of MANNER relations that was reported in [6]. Examples of questions that were generated for predicate specializa-tion are listed in Table 4. 2.e. Predicate Composition . Some questions need to capture rela-tions between predicates. Such relations may be determined by the discovery of (a) causal relations, as it was reported in [4]; (b) tem-poral relations; or (c) because the predicates share an argument. Table 5 illustrates such questions, their type of relations, and the sentences that enabled them. In our implementation, we have used a set of cue phrases and causal verbs to detect causal relations be-tween predicates. For the temporal relations, we relied on the tem-poral signals annotated in TimeBank (e.g.  X  X efore X ,  X  X fter X ,  X  X ur-ing X ).
 Table 5: Questions Based on Relations Between Predicates.
STEP 3: The selection of a new relation is performed after newly discovered relations are added to the RELATIONS list. 3.a. Relations that specialize arguments or predicates are not added to RELATIONS, but all the other three types of relations r pended. For example, the relations R 1 1 =[[ R 1 = develop  X  drug]  X  COMPANY], R 2 1 =[[ R 1 = develop  X  drug]  X  DISEASE], and R =[[ R 1 = develop  X  drug]  X  DURATION] are added. 3.b. A new relation is selected to maximize the probability esti-mation that it will lead to another question decomposition of the complex question. The probability estimations are detailed in Sec-tion 4.

STEP 4: The decision to continue or stop the process of gener-ating question decompositions depends on our formalization of the process. We have formalized the process of generating question decompositions which lead to the discovery of new meaningful re-lations as a random walk on a bipartite graph of questions and re-lations. For a given relation, a sentence that contains the relation is selected. That sentence is considered to be the answer to a question decomposition, which is generated by identifying a new relation, which in turn, when selected will lead to a new question decom-position. Thus the random walk continues with a probability  X  , generating a new decomposition and selecting a new relation, or it stops with a probability (1  X   X  ) . Section 4 describes the formalisms that allow us to estimate the pr obability that the r andom walk ends after k steps, corresponding to k loops of the Steps 2 and 3 of this procedure.
In this section, we describe how we employ two different types of random walks to decompose complex questions for question-answering and/or multi-document summarization applications. We begin by describing how a random walk can be used to populate a network with potential decompositions of a complex question. Later, in Section 4.2, we show how another random walk can then be used to select a set of generated decomposed questions that best represents the information need of the complex question.
The question decomposition procedure detailed in Section 3 can be cast as a Markov chain (MC). A MC over a set of states S is specified by an initial distribution p 0 ( S ) over S , and a set of state transition probabilities p ( S t | S t  X  1 ) . In the case of question decom-position, the initial state is represented by one relation r from the list RELATIONS ( time =0 ), which is the set of relations generated when processing the complex question. The probability of the initial state is set as 1 n ,where n = | RELATIONS( time After selecting a relation r i at step i , the index is consulted to find sentences where r i and other relations r j having the proper-ties listed in Table 2 are present. If the argument of relation r be categorized in the EAT hierarchy as an expected answer type e , then it can enable the generation of a question decomposition QD i +1 with EAT = e j . The probability that a question decompo-sition QD i +1 , with EAT = e j , is generated from a relation r given by p ( QD i +1 | r i )= p ( e j | r i ) . The new relation r in the RELATIONS list. If the index of r i had only one sentence and only one relation r j = r i could be found in that sentence, then r is removed from the list RELATIONS.

A new relation r i +1 is selected from RELATIONS based on the probability p ( r i +1 | QD i +1 ) . Since question QD i +1 based on the EAT discovered with the help of relation r j to the EAT e j , we can evaluate p ( r i +1 | QD i +1 )= p this way we have defined the transition probabilities of the Markov Chain (MC) illustrated in Figure 3. The MC alternates from se-lecting relations from RELATIONS and generating a new question decomposition. In this way, the decomposition process is  X  X urfing X  the set of relations meaningful for the complex question, and also the decomposed questions that are generated based on these rela-tions. After each step there is some chance that the question de-composition process will stop. The process continues the random walk with probability  X  , generating a new set of question decom-positions. With probability (1  X   X  ) , the walk stops after step k (after producing the question decomposition QD k +1 ).
 Figure 3: The Markov chain alternates between relations and question decompositions.

Since our goal is to estimate the probability that the MC stops af-ter k steps, we produce a matrix formulation of the problem which is similar to the formulation reported in [9]. This formulation is described in Section 4.1. We also want to test our hypothesis that the decomposed questions are relevant for the complex question. Since these question decompositions have been generated by re-lations that we have discovered in the text to be associated with relations originating in the complex question, we want to test if our assumption that they are valid decompositions can be quantified by a measure of relatedness to the complex question. For this purpose, in Section 4.2 we define a mixture model which generates a dif-ferent random walk that evaluates the relevance of the decomposed questions.
The operation of the random walk can be cleanly described by using a matrix notation. Let N be the size of the index. The num-ber N corresponds to the relations that we have discovered in the text, having the properties that for every relation r i there is also a relation r j sharing with r i the properties from Table 2, and r the argument mapped in one of the semantic categories of the EAT classes. Let M be the number of EAT classes. We consider A to be a N  X  M stochastic matrix with entries a ij = p ( r i | e the probability that the relation r i will be composed in a sentence with a relation r j that has an argument of semantic type e will become the EAT of the question decomposition that is gener-ated. Similarly, a stochastic matrix B of dimensions M  X  N can be defined, in which the elements b ij = p ( e i | r j ) represent the proba-bility that a sentence that contains the relation r j can be the answer to a question with the EAT = e i . Then, the N  X  N stochastic matrix C is defined as C = A  X  B . The probability that the MC stops after k steps is given by (1  X   X  )  X  k C k r,e , if the last relation it discovered is r and the last question decompositions it has generated had the EAT = e .

To estimate p ( e i | r j ) we consider where p ( e i ) is the prior distribution of the semantic type e corpus, and p ( r j | e i ) is given by the maximum likelihood estimate of the relation distribution in the text collection. Let J number of instances of the relation r j composed with a relation r in the same sentence such that the argument of r i has the semantic
Recently, [15] introduced a random walk model for finding an-swers to complex questions. This model is based on the idea that answers can be found by scoring each sentence against a complex question and selecting only the first top-ranked sentences. The sen-tence rank is produced by a mixture model that combines an ap-proximation of a sentence X  X  relevance to a question with similarity measures that can be used to select answer sentences that are not similar to one another. Using the same idea, we devised a similar mixture model for measuring the relevance of a question decom-position qd i to the complex question cq . The relevance measure is defined as: relevance ( qd i ,cq )= d sim a
The similarities sim a and sim b are selected from the four simi-larity measures defined in Section 5. If k is the number of question decompositions that we consider, a stochastic matrix A of dimen-sions k  X  k is considered such that a ij =  X   X  relevance ( In order for matrix a to be stochastic, 1 /
P dimension k  X  k is defined such that b ij =  X  j  X  sim b ( where  X  j =1 / R for all question decompositions is defined by R =[ dA +(1  X  d )
B ] i  X  R . The square matrix E =[ dA +(1  X  d ) B ] defines a MC where each element e ij from E specifies the transition probability from state i to state j in the corresponding Markov Chain. The rel-evance vector R is the stationary distribution of the Markov chain. With probability d , a transition is made from the current question decomposition qd i to new question decompositions that are similar to the complex question cq . With a probability (1  X  d ) ,atransi-tion is made to question decompositions that are similar to the last question generated, qd i . We have used several values for d in our experiments.
Our experiments have targeted (1) the evaluation of the decom-posed questions; (2) the evaluation of the three forms of answers produced by the framework illustrated in Figure 1; and (3) the eval-uation of the impact of the decomposed questions on the quality of answer summaries.
 Evaluation of Decomposed Questions
The evaluation of the decomposed questions was performed in two ways. First, the decomposed questions were evaluated against decompositions created by humans. Second, question decomposi-tions were evaluated against questions generated from the answer summaries. The second evaluation was also compared against an evaluation involving only human-generated questions, both from the complex question and from the answer summaries. The evalua-tion was performed against 8 complex questions that were asked as part of the DUC 2005 question-directed summarization task. The questions correspond to the topics listed in Table 6.

Four human annotators performed manual question decomposi-tion based solely on the complex questions themselves. Annotators were asked to decompose each complex question into the set of subquestions they felt needed to be answered in order to assemble a satisfactory answer to the question. (For ease of reference, we will refer to this set of question decompositions as QD human
In order to evaluate the quality of the automatic question de-compositions produced by our system, we generated three different types of question decompositions for a total of 8 complex ques-tions that were asked as part of the 2005 DUC question-directed summarization task. First, we had 4 human annotators perform manual question decomposition based solely on the complex ques-tions themselves. Annotators were asked to decompose each com-plex question into the set of subquestions they felt needed to be an-swered in order to assemble a satisfactory answer to the question. (For ease of reference, we will refer to this set of question decom-positions as QD human .) The subquestions generated by the anno-tators were then compiled into a  X  X yramid X  structure similar to the ones proposed in (Nenkova and Passonneau, 2004). In order to cre-ate pyramids, humans first identified subquestions that sought the same information (or were reasonable paraphrases of each other) and then assigned each unique question a score equal to the num-ber of times it appeared in the question decompositions produced by all annotators. Second, we u tilized our random walk model to generate a set of question decompositions ( QD auto 1 ) for each com-plex questions. Third, as shown in Figure 1, the subquestions in QD auto 1 were used to generate multi-document summaries which were used to automatically generate a fourth set of question de-compositions ( QD auto 2 ). As with QD human , the subquestions generated for QD auto 1 and QD auto 2 were combined into pyramid structures by human annotators.

Each of these three sets of question decompositions were then compared against a set of  X  X old standard X  decompositions created by another team of 4 human annotators from from the 4  X  X odel summaries X  prepared by NIST annotators as  X  X old standard X  an-swers to the 8 complex questions. Each of the three question de-compositions described above (i.e. QD human , QD auto 1 ,and QD auto 2 ) were then scored against the corresponding  X  X odel X  ques-tion decomposition pyramid using the technique outlined in [14]. Table 6 illustrates the Pyramid coverage for QD auto 1 , QD and QD human It is to be noted that although the QD human tured 45% of the questions contained in the  X  X odel X  pyramids, the high average Pyramid score (0.5000) suggests that human question decompositions typically included questions that corresponded to the most vital information identified by the authors of the  X  X odel X  summaries.

Another important observation is that the coverage and the Pyra-mid score of QD auto 2 are almost 80% of the same measures ob-tained for QD human , whereas the Pyramid score of the question decompositions QD auto 1 is only 45% of the Pyramid score and coverage obtained for QD human . In fact, these scores vary based on the number of feedback loops allowed for the Answer Summary 3 from Figure 1. Figure 5 illustrates the average average Pyra-mid scores that were obtained at each step of the feedback loop for all eight questions, both for QD auto 1 ,and QD auto 2 . The Figure
Table 6: Pyramid Coverage of Question Decompositions. shows that the Pyramid scores improve for QD auto 1 . The improve-ment for QD auto 2 is less dramatic. This means that the compari-son and selection of new question decompositions at each feedback loop determines better questions and better answers.
 Table 7: Responsiveness Score for the Human Summaries and Answer Summaries 1, 2 and 3.

Four different similarity metrics are responsible for the compar-isons. They are listed in Figure 4. Pairs of these similarity metrics were also used for defining the relevance of question decomposi-tions to each complex question. The aggregate similarity between q  X  QD auto 1 and QD auto 2 is defined as A  X  sim ( q i ,QD tant role in the selections of questions from QD auto 1 for the next loop. In our experiments, we observed that if we take only similar-ity 3 we obtain the best results.
 Figure 5: Pyramid scores at each step of the feedback loop. Evaluation of Answers.

Answers were evaluated by the  X  X esponsiveness score X  designed by the NIST assessors. The score provides a coarse ranking of the summaries for each topic, according to the amount of informa-tion in the summary that helps to satisfy the information need ex-pressed in the topic statement. Four linguist assigned these scores for all three forms of answer summaries. Table 7 illustrates the re-sponsiveness scores for Answer Summary 1, Answer Summary 2, Answer Summary 3, from Figure 1 and the human generated sum-maries. The responsiveness score is measured on a scale from 1 to 5 and it quantifies how well does a summary answer the complex question. A score of 1 is the least responsive to the question. A score of 5 means that the summary answers completely the ques-tion.
 Evaluation of the Impact of the Decomposed Questions on Answer Summaries.

We were also interested to evaluate the impact the question de-compositions would have when we select different values for the parameter  X  which stops the Markov chain. Figure 6 illustrates the average Responsiveness score obtained when  X  =0 . 85 ,  X  =0  X  =0 . 5 ,  X  =0 . 3 and  X  =0 . 15 . Since the question decom-positions determine two different answers, as it was illustrated in Figure 1, we have measured the responsiveness for both of them and illustrate the results in Figure 6.
In a separate effort, we evaluated the impact of only the ques-tion decompositions that were considered relevant to the complex question by the random walk presented in Section 4.2. Since that random walk depends on the parameter d , we have tested the ques-tion coverage for d =0 . 85 , d =0 . 6 , d =0 . 5 , d =0 . d =0 . 15 . Figure 7 illustrates the aver age Responsiveness obtained in this case. Since only Answer Summary 3 is obtained by con-sidering the relevance of question decompositions to the complex question, unlike Figure 6, in Figure 7 we illustrate results only for Answer Summary 3. The best results are obtained for d =0 . This result supports our intuition that the question decompositions should not be necessarily very different, but they must be relevant to the complex question. The difference from the responsiveness of human-generated summaries indicates that relevance takes into account more sophisticated information than the one contained in questions.
We have presented a new framework for question decomposition that allows several forms of answers to be returned for complex questions. Two forms of random walks were used. The first ran-dom walk was used for surfing the space of relations relevant to the complex question, in order to generate question decompositions. The second random walk was used for measuring the relevance of the question decompositions to the complex question.

The evaluations have shown that the question decompositions lead to more relevant and complete answers. The results have also shown that the coverage of automatically generated question de-compositions, when compared with the questions generated from the answer summary are better indicators of answer quality than the relevance score to the complex question. The evaluations have also indicated the question coverage for automatic methods is 85% of the coverage of questions produced by humans.

In this paper we have also described a Q/A architecture which allows feedback loops for impro ving the quality of answers through the coverage of question decompositions.
This material is based upon work funded in whole or in part by the U.S. Government and any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the U.S. Government. [1] E. Brill. Transformation-Based Error-Driven Learning and [2] M. Collins. Head-Driven Statistical Models for Natural [3] S. Harabagiu. Incremental Topic Representations. In [4] S. Harabagiu, A. Hickl, J. Lehmann, and D. Moldovan. [5] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, [7] F. Lacatusu, A. Hickl, P. Aarseth, and L. Taylor.
 [8] F. Lacatusu, A. Hickl, and S. Harabagiu. Impact of Question [9] J. Lafferty and C. Zhai. Document language models, query [10] C.-Y. Lin and E. Hovy. The automated acquisition of topic [11] S. Lytinen and N. Tomuro. The Use of Question Types to [12] G. A. Miller. WordNet: a lexical database for English. [13] S. Narayanan and S. Harabagiu. Question Answering based [14] A. Nenkova and R. Passonneau. Evaluating Content [15] J. Otterbacher, G. Erkan, and D. Radev. Using random walks [16] M. Palmer, D. Gildea, and P. Kingsbury. The Proposition [17] M. Pasca and S. Harabagiu. High Performance [18] M. Thelen and E. Riloff. A Bootstrapping Method for
