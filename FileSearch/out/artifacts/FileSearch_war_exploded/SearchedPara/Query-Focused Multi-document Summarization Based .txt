 Query-oriented relevance, information richness and novel-ty are important requirements in query-focused summariza-tion, which, to a considerable extent, determine the summa-ry quality. Previous work either rarely took into account all above demands simultaneously or dealt with part of them in the dynamic process of choosing sentences to generate a summary. In this paper, we propose a novel approach that integrates all these requirements skillfully by treating them as sentence features, making that the finally gener-ated summary could fully reflect the combinational effect of these properties. Experimental results on the DUC2005 and DUC2006 datasets demonstrate the effectiveness of our approach.
 H.3.1 [ Information Store and Retrieval ]: Content Anal-ysis and Indexing X  Abstracting methods ; I.2.7 [ Arti cial Intelligence ]: Natural Language Processing X  Text analy-sis Algorithms, Experimentation, Performance query-focused summarization, query-biased sentence feature, topical vector space model, LDA
For query-focused multi-document summarization, a sum-marizer incorporates user-declared queries and generates sum-maries that not only deliver the majority of information con-tent from a set of documents but also bias to the queries. A good query-biased summary is generally supposed to meet the following typical demands: query-biased relevance, bi-ased information richness and biased novelty. Query-biased relevance requires that the sentences in the summary must overlap with the query in terms of topical content. Query-biased information richness denotes the information degree of the sentences with respect to both the sentence collection and the query. Query-biased information novelty is used to measure the content uniqueness of a sentence based on its capabilities in differentiating itself from other sentences as well as responding to the demands of the query.

Most existing work treats query-focused multi-document summarization as a sentence ranking problem at first, then exploit some strategies to deal with redundancy removal, coverage and balance during the sentence selection, which is a dynamic and greedy procedure, such as [4, 7]. In this paper, we treat some governing requirements as features of sentences. Hence we extract representative query-oriented sentence features entirely to form a feature space at the ini-tial stage, accordingly, Gaussian Mixture Model (GMM) is utilized to cluster sentences over the feature space so as to reduce the size of the target cluster (i.e., the cluster con-taining the most query-biased salient sentences). This also makes the subsequent ranking of sentences more robust be-cause it is less sensitive to outliers such as noise sentences. The rationale behind most existing clustering-based summa-rization methods is that different clusters represent differen-t aspects of the documents, which should all be covered if possible [4]. Differently, we only consider one most eligible cluster as our target cluster in this study because the feature space we have defined in initial phase makes the sentences in the target cluster be the most suitable choices to con-struct a summary. The only thing left to do is to sort the sentences within the cluster. Extensive experiments on the DUC2005-2006 benchmark data sets demonstrate that our method outperforms some representative baseline approach-es in all specified evaluation measures.
We briefly introduce some related summarization meth-ods, especially those explicitly taking various requirements into consideration.

Query-oriented sentence relevance in [6] was acquired by opting for the trade-off of the sentence X  X  initial relevance to the query and its similarities to other sentences in the document cluster. Li et al. [3] treated summarization as a supervised sentence ranking process, where coverage, bal-ance and novelty properties were incorporated. Whereas, it focused on generic summarization rather than query-biased situation. Wan et al. [7] gave explicit definitions of biased information richness and novelty, then, they proposed to co mpute biased information richness using manifold-ranking process and a greedy algorithm similar to [8] was applied to keep low information redundancy in the summaries. The method proposed in [4] is closely related to our work for it considered novelty, coverage and balance wholly. However, a common characteristic of existing methods in acquiring novelty or balance property lies in the assumption that we have already selected the first k -1 sentences s 1 ,  X  X  X  , s a summary, which makes the novelty or balance acquiring must be conducted during the dynamic and greedy process of choosing sentences to generate summary.
In order to relieve the inconvenience cast by the inher-ent information shortage of sentences on the calculation of sentence relatedness, we come up with a new model named TVSM through exploiting Latent Dirichlet Allocation (L-DA) [1] to modify Vector Space Model (VSM), which has been commonly utilized to represent texts.

In our model, given a sentence a =( w 1 , w 2 , w 3 , w 4 w , w 7 , w 8 , w 9 , w 4 ), using the posterior probabilities, we annotate each word in a with a topic assignment. Suppose this yields an ordered sequence of word-topic pairs: ( w 1 z ), ( w 2 , z 2 ), ( w 3 , z 3 ), ( w 4 , z 4 ), ( w 5 , z z discriminative terms are mapped to five topics. Accordingly, we could represent a over topic space and the topic weights f are derived as follows: f z 2 a = P ( z 2 | w 2 ) + P ( z 2 | w 7 ) + P ( z 2 | w In general, we collect those words assigned topic z i in sen-tence s as a set s z i (1  X  i  X  K ) and use symbol C ( w, s ) to denote the number of occurrences of word w in s . The formula for calculating the topic impacts on s is defined as follows: where Constant  X  in Equation 1 is set to e 4 empirically. We give those topics not used to annotate s a small impact so as to smooth the vectors. Provided with topic vector representa-tions of two sentences, we could make use of cosine measure or other strategies to determine their similarity. By using topic space, which is much smaller and denser than the o-riginal term vector space, we gain a reduced representation where similarity between short text can be more reliably es-timated. Furthermore, the latent topic space reduces the impact of noise terms. In experiments, we use the Gibb-sLDA++ 1 , a C/C++ implementation of LDA using Gibbs Sampling.
Gib bsLDA++: http://gibbslda.sourceforge.net
Query-oriented summarization not only requires the fea-tures to deliver the salient cluster content, demonstrate con-siderable consistency with the query motivation, it also prefer-s a low information redundancy in the generated summary.
Here, we make use of different types of information avail-able in the DUC2005 and DUC2006 multi-document sum-marization datasets: Each of these elements is represented as a vector over latent topic space presented in Section 3. The vector representa-tion of a document could be extracted easily from the output files of GibbsLDA++ and it is actually a multinomial distri-bution. The topic vector of a document cluster is obtained by averaging the distribution vectors of all the documents in the cluster. We do not conduct sentence segmentation if the query consists of more than one question, instead we treat it as a single, long sentence. Given the resulting representa-tion of above elements as vectors, we calculate the following four sentence features: Given two sentences b =( b z 1 , b z 2 , b z 3 ,  X  X  X  , b z c =( c z 1 , c z 2 , c z 3 ,  X  X  X  , c z i ,  X  X  X  , c z K ), b weight of topic z i in b and c , respectively. we choose to compute their similarity on the basis of Jensen-Shannon (JS) divergence. It is a symmetrized and smoothed version of the Kullback-Leibler (KL) divergence, calculated as the KL divergence of b , c with respect to the average of the two input distributions. The JS divergence is then defined as: where m = b + c 2 . To use the JS divergence as a relevance measure, we scale it to [0, 1] and invert by subtracting from 1, hence:
Given a sentence collection and a query q , the BIR of sentence s is used to indicate the information degree of the sentence s with regard to both the sentence set and q , i.e., the richness of information contained in the sentence s biased towards q .
Thi s feature score for each sentence is obtained via a vari-ant version of the manifold-ranking process proposed in [7]. Points { s 0 , s 1 ,  X  X  X  , s n } denote the query statement ( s all the sentences in the document collection ( { s i | 1  X  in a manifold space. The ranking function is denoted by sentences had blank prior knowledge so their initial scores were all set to zero. Whereas in this study, it is rational to treat the relevance between a sentence and the query dis-cussed in above section as prior knowledge of the sentence. Since s 0 denotes the query description, the initial score vec-tor of these sentences is y =[ y 0 , y 1 , ..., y n ], where y y = Rel( s i , s 0 ) (1  X  i  X  n). The manifold ranking can be performed iteratively using the following equation: where S is the symmetrically normalized similarity/relevance 0.6, and k indicates the k th iteration. Obviously, modified initial scores will exert a greater influence to sentence scores than the settings in [7] at each step of the iteration process. After convergence, let f  X  i denotes the limit of the sequence { f i ( t ) } , then the BIR of sentence s i is:
A desired summary should have low information redun-dancy. In order to devise a feature to differentiate those sentences that have a degree of similarity, we perform the modified MMR algorithm in [7] on the basis of feature BIR discussed Section 4.2. Lots of work (e.g., [7, 2]) used some strategies to impose penalty to the remaining sentences once a new sentence has been added to the summary. However, that treatment easily neglects the underlying positive effect-s on summary quality brought probably by other sentence features. In our approach, we just treat the MMR result as a sentence feature named biased information novelty . Hence, a proportion of existing methods are approximately equiva-lent to our current section. Our algorithm for BIN acquiring goes as follows: 1. Initiate two sets A =  X  , B = { s i | i=1,2,  X  X  X  , n } 2. Sort the sentences in B by their current BIN scores in 3. Suppose s i is the highest ranked sentence, i.e., the first 4. Go to step 2 and iterate until | B | =0.
Six representative sentence features have been extract-ed in Section 4. We use GMM to cluster sentences into four clusters over their feature space. The main motivation for the clustering is to reduce the size of the target cluster and make the subsequent sentence ranking more robust. It should be noted that the cluster number is flexible. Where-as, too many clusters probably results that the sentences in the target cluster are too few to meet the summary length. Therefore, we set it to 4 empirically. The following subsec-tion describes GMM in general and how we used it in our setting.
Gaussian Mixture Model (GMM) is a simple linear super-position of Gaussian components for the purpose of provid-ing a richer class of density models than the single Gaus-sian. Clustering based on GMM is probabilistic in nature and aims at maximizing the likelihood function with regard to the parameters (comprising the means and covariances of the components and the mixing coefficients). Consider n data points S = { s 1 , s 2 ,  X  X  X  , s n } in d-dimensional space, the probability density of s i can be defined as follows: where k is the component number,  X  z is the prior probability of the z th Gaussian component. N ( s i ;  X  z ,  X  z ) is defined as: Under the assumption that the data points are independent and identically distributed (i.i.d), the log of the likelihood function is given by: An elegant and powerful method for finding maximum like-lihood solutions for models with latent variables is Expec-tation Maximization algorithm (EM). EM is an iterative al-gorithm in which each iteration contains an E-step and a M-step. In the E-step, we compute the probability of the z th Gaussian component given the data point s i using the current parameter values: In the M-step, we re-estimate the parameters using the cur-rent responsibilities, as follows: where The EM algorithm runs iteratively until the log likelihood reaches (approximate) convergence. And we use K-means to determine the initial model parameters:  X  ,  X  and prior probabilities of the components  X  z (1  X  z  X  k ). In general, GMM performs better than classical hard clustering algo-rithms such as K-means as it is less sensitive to outliers. In experiments, we simply run GMM until the increment of log likelihood is less than 0.000001 and pick the one with largest log likelihood as the best estimate of the underlying clusters.
The above GMM algorithm gives probabilistic assignments of sentences belonging to a given cluster p ( z | s ,  X  ,  X  ,  X ). For each cluster, we pick all the sentences with this probability to be greater than 0.9. This is done as we want to determine the true representative sentences per cluster. Using these sentences, we compute the new average features per cluster and pick the cluster with the larger features (or best of all) as our target cluster. In order to sort sentences within the target cluster, we propose the following ranking algorithm.
We assume features to be Gaussian distributed (which is true in most case, though with a bit of skew in some cases). For any given feature s f i , we compute the  X  f and  X  f based on the sentences in the target cluster. The Gauss rank R G of a given sentence s i is then defined as follows: where N ( x ;  X  f ,  X  f ) is the univariate Gaussian distribution with model parameters as  X  f and  X  f , and  X  f is the weight of feature s f i . The inner integral in this equation computes the Gaussian cumulative distribution at s f i . Gaussian CDF (cumulative distribution function ) is a monotonically in-creasing function well suited to our ranking problem as we prefer a higher value over low value for each feature. Alter-nately, if a low value is preferred for some features, then s could be replaced by  X  s f i in the above formula.
This algorithm helps in devising a total ordering under  X  over all the sentences in the target cluster. Accordingly, we observe that weight normalization doesn X  X  change the ordering of sentences. Hence, the only constraint we put on these feature weights is { X  f : 0  X   X  f  X  1 } .
We evaluate our proposed approach for query-focused multi-document summarization on the main tasks of DUC2005 2 and DUC2006 3 . Each task has a gold standard dataset h ttp://www-nlpir.nist.gov/projects/duc/duc2005/tasks.html http://www-nlpir.nist.gov/projects/duc/duc2006/tasks.html consisting of document sets and reference summaries. Doc-uments are pre-processed by segmenting sentences and s-plitting words. Stop words are removed and the remaining words are stemmed using Porter stemmer 4 . Table 1 gives a short summary of the two datasets.

I n experiments, we use the ROUGE [5] (version 1.5.5) toolkit 5 for evaluation, which is officially adopted by DUC for evaluating automatic generated summaries. It measures summary quality by counting the overlapping units between system-generated summaries and human-written reference summaries. We report three common ROUGE scores in this paper, namely ROUGE-1, ROUGE-2 and ROUGE-SU4 which base on Uni-gram match, Bi-gram match, and uni-gram plus skip-bigram match with maximum skip distance of 4, respectively.
We compare our method G TV SM JS with the following algorithms. (1)Rel: a method considering only the sentence relevance towards the query and choosing the most relevant sentences to produce summary until length limit is reached. (2)Rel MM R: similar to (1) except that we utilize MMR algorithm to reduce redundancy. (3)Coverage: a baseline clustering-based method. It clusters sentences and selects the most relevant sentences from different clusters. (4)Man-ifold: ranking the sentences according to the manifold rank-ing scores. (5)Random: a baseline producing summary by random sentence selection. (6)top three systems with the highest ROUGE scores that participated in the DUC2005 (S4, S15, S17) and the DUC2006 (S12, S23, S24) for com-parison, respectively. It should be noted that JS divergence based on TVSM is applied to some baseline systems above if they need measure sentence similarity so as to enhance the comparability and persuasiveness of our experiments.
Tables 2 and 3 show the experimental results. From those statistics, following conclusions can be drawn: (1)Method Rel has relatively poor performance. It is merely better than Random on all three evaluation metrics, which suggests that considering only query-biased relevance is not enough. Sim-ilarly, only taking coverage property into account does not benefit the summary quality either. (2) Rel M M R outper-forms Rel . Clearly, combining novelty with biased relevance indeed boosts the summary quality. (3) Rel M M R is in-ferior to M anif old . Recall that Wan et al. [7] integrated manifold-ranking process and MMR algorithm to implement their system. Hence biased information richness is a more ef-fective indicator than query-oriented relevance in respect of reflecting the potential value of sentences. (4)G TV SM JS ou tperforms the comparative systems on all evaluation met-rics. It confirms our judgment that various query-aware sen-h ttp://tartarus.org/ , a  X nmartin/PorterStemmer/ http://www.isi.edu/licensed-sw/see/rouge/ G TV SM JS 0.3 9008 0.07991 0.13527
G TV SM JS 0.4 2367 0.10105 0.16425 te nce features could help to explore potential sentences from multiple aspects to meet the user X  X  information needs.
LDA is a crucial tool in devising TVSM. In order to inves-tigate how the topic number K exerts influence to system performance, K is varied from 20 to 170.

Figure 1 shows the influence of K over ROUGE-1 on two data sets. On the whole, the evaluation scores rise slowly with the increase of K , and reach their respective maximum around K = 110, then gradually go down when K contin-ues to become larger. Despite some fluctuations, they don X  X  matter to the overall conclusion. The observed performance variations can be explained by the way latent topic models map words to topics: Given a fixed number of latent topics to ll , the algorithm may split up a single topic into two dis-tinct ones, or vice versa merge different topics, which in turn affects similarity scores and the feature extraction. Another point worth mentioning is that even with K = 110 latent topics, the dimensionality of TVSM is much lower than that of VSM.
In order to estimate the weight parameters in Equation 16, for each weight vector  X  , we run our summarization al-gorithm and measure the ROUGE-1 value. This result is henceforth called the score of the weight vector  X  . To iden-tify the weights that maximize the score, we use stochas-tic hill climbing along with simulated annealing in the unit hypercube that encloses all possible weight vectors (recall that weights are bounded by [0, 1]). We skip the details of the algorithm and just report an approximate global op-timal weight vector adopted in our experiments: ( r t,s :0.05, r d,s :0.15, r c,s :0.1, r q,s :0.2, BIR:0.15, BIN:0.35), which is a feature:weight sequence. It suggests that the most dominant features in our algorithm are the biased information novelty BIN and sentence-query relevance r q,s . On the other hand, the sentence-title relevance r t,s does not have a significant influence on quality of the resulting summaries.
This research is financially supported by NSFC of China (Grant No. 60933004 and 61103027) and HGJ (Grant No. 2011ZX01042-001-001). [1] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [2] X. Cai and W. Li. A context-sensitive manifold ranking [3] L. Li, K. Zhou, G. Xue, H. Zha, and Y. Yu. Enhancing [4] X. Li, Y. Shen, L. Du, and C. Xiong. Exploiting [5] C. Lin. Rouge: A package for automatic evaluation of [6] J. Otterbacher, G. Erkan, and D. Radev. Using random [7] X. Wan, J. Yang, and J. Xiao. Manifold-ranking based [8] B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen,
