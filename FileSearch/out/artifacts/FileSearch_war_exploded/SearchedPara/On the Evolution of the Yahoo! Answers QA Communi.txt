 While question answering communities have been gaining popularity for several years, we wonder if the increased popularity actually improves or degrades the user experience. In addition, automatic QA systems, which utilize different sources such as search engines and social media, are emerging rapidly. QA communities have already created abundant resources of millions of questions and hundreds of millions of answers. The question whether they will continue to se rve as an effective source of information for web search and question answering is of vital importance. In this poster, we inve stigate the temporal evolution of a popular QA community  X  X ahoo! Answers, with respect to its effectiveness in answering three basic types of questions: factoid, opinion and complex questions. Our experiments show that Yahoo! Answers keeps growing rapidly, while its overall quality as an information source for factoid question-answering degrades. However, instead of answering f actoid questions, it might be more effective to answer opinion and complex questions. Categories and Subject Descriptors: H.3.4 Systems and Software: Question-answering (fact retrieval) systems ; H.3.3 Information Search and Retrieval: Search process, Selection process. H.3.5 Online Information Services: Web-based services General Terms: Measurement, Experimentation, Human Factors Keywords: Social media, Question answering Recently, community question answering (CQA) emerged as a popular alternative to finding information online. It has attracted millions of users who post millions of questions and hundreds of millions of answers, producing a huge knowledge repository of all kinds of topics, so many potential applications can be possibly made on top of it. For example, automa tic question answering systems, which try to find the information to questions directly instead of giving a list of related documents, might use CQA repositories as a useful information source. However, it is not clear what information needs these CQA portals serve, and how these communities are evolving. In this poster we st udy a popular CQA portal  X  X ahoo! Answers X  as our test case to quantify relative effectiveness for different types of queries such as factoid question, opinion queries, and complex question. As another motivation for our exploration, we examine how Yahoo! Answers has evolved recently and try to identify some trends in its development. We analyze the temporal changes of the Yahoo! Answers contents during the years of 2006 and 2007, so that we may get an idea of how it has progressed and to quantify potential uses of the services. Also we can gain a deeper understanding of community question answering phenomenon as it evolves over time. For this, we initially study CQA performance for simple factoid questions, and evaluate MRR: We use three variants of the standard MRR metric (Mean Reciprocal Rank) to examine the effectiveness of Yahoo! Answers for answering questions: z Max: the maximum MRR for each of the retrieved question z Strict: Computed by evaluating the answers for each question z RR: Round-robin evaluation of answers from each retrieved KL-Divergence: We also want to measure the similarity of the CQA content to known classified que ries (e.g., factoid questions) to better characterize the content and its evolution. Specifically, we use KL-Divergence, computed as: The lower the KL-Divergence value, the more similar are two distributions P and Q . In our setting we compare the language models for, say, TREC factoid questions ( P ), and Yahoo! Answers questions ( Q ). We use a simple word unigram language model with estimated likelihood smoothing (  X  = 0.2). We report the MRR, and the KL-divergence for factoid questions in Figure 1 (a), for varying time periods. Retrieved Yahoo! questions ( retrieved Y! Q in the figure) are the questions returned from Yahoo! Answers that were triggered by TREC queries (keywords) while Matched Yahoo questions ( matched Y! Q in the figure) refer to those that have at least one correct answer that matched against TREC patterns. The KL-Divergence values were calculated the between the sets retrieved Y! Q and matched Y! Q, and the language model build over all TREC Factoi d questions. We also report the MRR values for the same time periods. 
As we can see, for the first three time periods, KL-Divergence initially increases, but drops dur ing the final period. The MRR values appear to slightly decrease. This suggests that the questions posted on Yahoo! Answers increasingly diverge from the typical 
TREC factoid questions, and the CQA content is becoming less valuable as a source of factoid information. 
