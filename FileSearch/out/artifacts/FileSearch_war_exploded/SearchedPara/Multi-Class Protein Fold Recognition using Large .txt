 Inductive Logic Programming (ILP) systems have been suc-cessfully applied to solve complex problems in bioinformatics by viewing them as binary classification tasks. It remains an open question how an accurate solution to a multi-class problem can be obtained by using a logic based learning method. In this paper we present a novel logic based ap-proach to solve complex and challenging multi-class classifi-cation problems by focusing on a key task, namely protein fold recognition. Our technique is based on the use of large margin methods in conjunction with the kernels constructed from first order rules induced by an ILP system. The pro-posed approach learns a multi-class classifier by using a di-vide and conquer reduction strategy that splits multi-classes into binary groups and solves each individual problem re-cursively hence generating an underlying decision list struc-ture. The method is applied to assigning protein domains to folds. Experimental evaluation of the method demonstrates the efficacy of the proposed approach to solving multi-class classification problems in bioinformatics. The underlying aim of a multi-class approach is to learn a highly accurate function that categorizes examples into pre-defined classes. Effective multi-class techniques are crucial to solving the challenging and complex problems in bioin-formatics such as multi-class protein fold recognition. The two areas of machine learning, namely Inductive Logic Programming (ILP) and Kernel based methods (KMs) are well known for their distinguishing features: ILP techniques are characterized by their use of background knowledge and expressive language formalism whereas strong mathematical foundations and high generalization ability are remarkable characteristics of KMs. Recently some useful techniques (Support Vector Inductive Logic Programming (SVILP) [8], kFOIL [6] and RUMBLE [10]) have been designed by ex-ploiting the characteristics of KMs and ILP to solving binary classification problems and performing real-valued predic-tions. In this paper we study multi-class classification in the combined ILP and kernel based learning scenario by ex-tending SVILP for bioinformatics tasks.
 SVILP solves binary classification problems in a multi-stage learning process. In the first stage, a set of first order Horn clauses (rules) are obtained from an ILP system. In the next stages similarity between examples is computed by the use of a novel kernel function that captures semantic and struc-tural commonalities between the examples. The computed relational and logic based kernel is used in conjunction with a large margin learning algorithm to induce a binary clas-sifier. In this way, SVILP performs classification task by training a large margin first order classifier.
 In order to solve multi-class problems we propose a simple but accurate approach. The method is designed by reduc-ing multi-class classification task to binary problems. How-ever our approach is different from the existing reduction techniques as it learns hidden structure and characteristics from data and hence improves the performance of the classi-fier. The proposed method is based on a divide and conquer strategy and it discriminates different classes by using an un-derlying structure based on decision lists. The multi-class problem is reduced by recursively breaking it down into bi-nary problems where each binary task is solved by invoking an SVILP machine. At each node of the decision list the al-gorithm induces a classifier and updates the training set by removing the examples of the class chosen at the previous node. A label is assigned to a new example by traversing the list.
 The recognition of proteins having similar structure is a challenging and complex task in bioinformatics. It has key importance in studying protein structure and function and can provide answers to biological problems. In fold recogni-tion, labels are assigned to proteins from a set of predefined annotations (labels, folds). In this way protein fold recog-nition can be viewed as the multi-class classification task. The aim of a protein fold classification system is to assign proteins to one of many folds with high accuracy. Machine learning methods have been applied to investigate the prob-lem. The studies reported in [11; 3]) applied Support Vector Machines (SVMs) to solving multi-class protein fold classi-fication problem. Chen and Kurgan [1] and Shen and Chou [12] studied ensemble methods to assign proteins to 27 folds from SCOP [9]. In this paper we present a novel logic based approach to solving protein fold recognition problem. We also compare the proposed approach with standard multi-class logic based method and multi-class SVMs. The ex-perimental results demonstrate the efficacy of the proposed technique in assigning protein folds.
 Figure 2: Relationally encoded features of protein domain.  X  X 1all  X . ILP systems have been successfully applied to binary classi-fication tasks in bioinformatics. There are few ILP systems that can perform multi-class classification tasks [5]. The standard multi-class logic based method, described below, is biased towards the majority class. The method is based on learning theories H j (first order horn clauses) for each class j . The obtained theories for r classes are merged into a multi-theory H. For each class the number of correctly clas-sified training examples are recorded. A class is assigned to a new example if the example satisfies the conditions of the rules. In the case that an example is predicted to have multiple classes, then the class with the maximum number of predicted training examples is assigned to the example. If an example fails to satisfy the conditions of all the rules in H, a default class (majority class) is assigned to it. The method is termed as multi-class ILP (MC ILP). Support Vector Inductive Logic Programming [8] is a new machine learning technique that integrates Inductive Logic Programming and Support Vector Machines. SVILP learn-ing can be viewed as a multi-stage induction process. The four stages that comprise SVILP learning are described as follows.
 In the first stage a set of rules H is obtained from an ILP system that takes relationally encoded examples (positive, negative) and background knowledge as input. This stage maps the examples into a logic based relational space. A first order rule, h  X  X  , can be viewed as a boolean function of the form, h : D  X  X  0 , 1 } .
 In the next stage a subset H  X  X  is selected using an infor-mation theoretic measure, namely compression, described below. The subset of rules, H , is selected by thresholding Figure 4: Relational encoded features of protein domain  X  X 2hbg  X . the compression value. This stage maps the examples into another lower dimensional space containing the information relevant to the task at hand. The compression value of a rule ps is the number of positive examples correctly deducible from the rule, ng is the number of negative examples that satisfy the conditions of the rules, c is the length of the rule and PT is the total number of positive examples.
 In the third stage a kernel function is defined on the se-lected set of rules that can be weighted/unweighted. The kernel is based on the idea of comparing two examples by means of structural and relational features they contain; the more features in common the more similar they are. The function is given by the inner product between the mapped examples where the mapping  X  is implied by the set of rules H . The mapping  X  for an example d is given by, 1 .  X  : d  X  where h 1 , . . . , h t are rules and  X  is the weight assigned to each rule h i . The construction embeds the data into a fea-ture space, where dimensionality of the space is the same as the cardinality of the set of rules. In this way, an ex-ample is viewed as a column vector where each entry of the vector is indexed by a specific rule. The kernel for ex-amples d i and d j is given by, k ( d i , d j ) =  X   X  ( d P ner product between two mapped examples is a sum over all the common hypothesized rules. Given that  X  maps the data into feature space spanned by ILP rules, we can construct Gaussian RBF kernels, k RBF ( d i , d j ) = exp where k (  X  ( d i )  X   X  ( d j ) k = In the final stage learning is performed by using an SVM in conjunction with ILP kernel. SVILP is flexible to construct any kernel in the space spanned by the rules. However, in the present work we used k RBF . 0 specifies column vector We now show how ILP kernel measures similarity in logic and relational space by considering a pair of protein do-mains,  X 2hbg  X  and  X 1alla  X . The domains belong to  X  struc-tural class and  X  X lobin-like X  fold (SCOP classification scheme). Figures 1, 2, 3 and 4 show the two domains and their rela-tionally encoded features. Here predicates  X  X en X ,  X  X b alpha X , and  X  X b beta X  denote the length of the polypeptide chain, number of  X  -helices and  X  strands respectively. The other predicates represent the relationship between the secondary structure elements and their properties (hydrophobicity, the hydrophobic moment, the length of proline and etc.). Figure 5 shows a set of induced rules together with their En-glish conversion. A rule classifies an example positive (1) if it fulfils the conditions of the rule while an example that fails to satisfy the conditions is classified negative (0). The set of equally weighted rules maps the two examples as follows:  X  ( d 1 alla ) =  X  ( d 1) =  X  ( d 2) = equally weighted, each entry of the vector is multiplied by 1. The kernel values between the examples are as follows: We now propose a novel logic based method to solving multi-class classification problems like protein fold recognition. We apply inductive learning in which a learning algorithm is provided with a set of examples, D , of the form D = { ( d 1 , c 1 ) , ( d 2 , c 2) , . . . , ( d n , c n ) } where d ples and c i  X  { 1 , 2 , . . . , r } are classes (labels). The goal of the classification algorithm is to generate a function f : d  X  { 1 , 2 , . . . , r } that assigns a new example d to the class with low error probability.
 In order to solve multi-class problems we apply powerful but simple divide and conquer strategy. The complex multi-class classification task is divided into binary problems and each problem is solved recursively. The method constructs a decision list as shown in figure 6. Here each non-leaf node has two children. Classes are represented by non leaf nodes where edges are labeled by the binary classifier X  X  output. We term the divide and conquer technique as decision list based SVILP (DL SVILP). The method is shown as Algorithm 1. The technique reduces multi-class classification problem to r  X  1 binary problems, where r is the total number of classes. Figure 6: A decision list, learned by the large margin first order rule learner, for multi-class protein fold recognition. The algorithm can be viewed as comprising r  X  1 iterations. In each iteration a class is selected as the positive class and the remaining classes are reduced to the negative class. The binary problem is solved by using a large margin first order rule learner. The training set is updated by removing the examples of the chosen class. In this way the root node contains all the classes whereas the node at depth r  X  1 contains two classes. The size of the training set used at depth r  X  1 is (much) smaller than the size of the training set for the root node. DL SVILP assigns a class j to a new example d as follows: 1. Begin at the root node 2. Apply the classifier associated with the node to exam-3. Travel down the edge labeled by the classifier X  X  output 4. If the edge is labeled positive output the class asso-Algorithm 1 Support Vector Inductive Logic Programming (DL SVILP) for multi-class classification Input: A set of training examples { ( d 1 , c 1 ) , ( d 2 , c 2 ) , . . . , ( d n , c n ) } , where d c i  X  { 1 , 2 , . . . , r } and a vector index that represents learned structure of the list. for j = 1 to r  X  1 do end for return f i for i = 1 , . . . , r  X  1 We now describe how the underlying structure of the list is constructed. The method is dynamic and adaptive to learn-ing process. At each node the selection of the positive class is made in such a way so as the classifier can have high gen-eralization ability. The method is presented as Algorithm 2. For each class j a binary class problem is formulated by assigning label  X 1 X  to examples of the class j and  X -1 X  to ex-amples of remaining classes. The classifier, induced from the dataset, is evaluated on a validation set and its performance is measured and stored in a list. The process of inducing the classifiers and recording their performances in a list is repeated for all the r classes. Finally the list is sorted and this ranked list defines the underlying structure. In order to measure the performance of the underlying binary classifier we define the expression given by, Here P denotes the number of positive example, and N rep-resent number of negative examples. Similarly, the number of positive examples that are misclassified are represented by P  X  , where N + shows the number of negative examples that are classified positive. W P and W N are the weights assigned to P  X  , and N + respectively. The weights are assigned to give equal importance to all the classes in a dataset that is characterized by uneven class distribution. We select the weights by using a heuristic and set W P to N P where W N set to 1. We conducted experiments to evaluate the performance of the proposed method to solving multi-class protein fold recog-nition problem.
 We used accuracy as evaluation measures. Let P j denote the number of examples belonging to class j , P = represent total number of examples belonging to k classes, and TP j denote the number of correctly classified examples belonging to class j . The accuracy for each class j is given by Algorithm 2 Learning underlying structure for DL SVILP Input: Training set, d 1 , d 2 , . . . , d n , validation set, d 0 1 , d 0 2 , . . . , d 0 s , r classes and a large margin first or-der rule learner (for example SVILP) for j = 1 to r do end for /* Sort list S 0 in ascending order and reorder list index accordingly */
S = sort ( S 0 ) index = reorder ( index 0 ) return index and S We solved protein fold classification problem by applying the proposed method to the dataset presented in [13]. In or-der to compare the performance of SVILP based multi-class classification scheme with non-SVILP based methods we used multi-class SVM (MC SVM) and MC ILP. MC SVM was trained by using SVM light [4] where the method was presented in [2]. For MC SVM, we represented protein do-mains by using non-relational features namely, total num-ber of residues,  X  -helices and  X  -strands. Previous research demonstrated the effectiveness of these features for protein fold classification task. For MC ILP and SVILP based tech-niques we used relational fold discriminatory features de-scribed in [13]. These features are polypeptide chain length, number of  X  -helices and  X  -strands, adjacent secondary struc-ture elements, properties of the secondary structure such as the hydrophobicity, the hydrophobic moment, the length of Table 2: 5-fold cross-validated over all accuracy (OA)  X  standard deviation for protein fold dataset for MC ILP, DL SVILP and MC SVM. We also report cross-validated accuracy  X  standard deviation for 20 folds.
 proline (number of proline residues) and the length of the loop. In order to construct underlying binary SVILP clas-sifiers we used CProgol5 (PROGOL) [7] and SVM light . For MC ILP theories for were obtained by using CProgol5. The dataset comprises 381 protein domains. They belong to 20 folds of SCOP that have been categorized into 4 struc-tural classes, namely  X  ,  X  ,  X / X  and  X  +  X  . Table 1 shows the class distribution for 20 protein folds. The indices 1 to 20 shown in Tables 2 and 1 represent SCOP folds DNA 3-helical, EF hand-like, Globin-like, 4-Helical cytokines, Lambda repressor, Ig beta-sandwich, Tryp ser proteases, OB-fold, SH3-like barrel, Lipocalins,  X / X  (TIM)-barrel, Rossmann-fold, P-loop, Periplasmic II,  X / X  -Hydrolases, Ferredoxin-like, Zincin-like, SH2-like,  X  -Grasp, and Interleukin respec-tively. The dataset is characterized by uneven class distri-bution as shown in table 1.
 We randomly divided the dataset into 5 equal-sized folds and applied the experimental methodology as follows. At each cross-validation round 3-folds were used for training the classifiers where the remaining two folds were used as validation set and test set. The free parameter of SVM MC (C, width of the Gaussian kernel) and SVILP DL (C, width of the Gaussian kernel) were tuned by using the validation set.
 Table 2 lists the cross-validated accuracy for each protein fold for multi-class classification methods. Overall accuracy over 20 folds is also given. Table 2 shows that the accuracy values of DL SVILP are significantly higher than the other methods and it outperforms MC SVM and MC ILP. In this paper we presented a novel logic based multi-class classification method, DL SVILP. It produced an accurate solution to a complex bioinformatics problem, namely multi-class protein fold recognition. Experimental results showed that DL SVILP captured structural and relational similar-ities between proteins. It accurately assigned protein do-mains to folds and outperformed all the other methods in the study.
 The research presented in the paper is supported by Biotech-nology and Biological Sciences Research Council (BBSRC) grant with reference number BB/E000940/1. [1] K. Chen and L. Kurgan. PFRES: Protein fold clas-[2] K. Crammer and Y. Singer. On the algorithmic imple-[3] C. H. Ding and I. Dubchak. Multi-class protein fold [4] T. Joachims. Making large X  X cale SVM learning practi-[5] W. V. Laer, L. de Raedt, and S. Dzeroski. On multi-[6] N. Landwehr, A. Passerini, L. Raedt, and P. Frasconi. [7] S. Muggleton. Inverse entailment and progol. New Gen-[8] S. Muggleton, H. Lodhi, A. Amini, and M. J. E. Stern-[9] A. G. Murzin, S. E. Brenner, T. Hubbard, and [10] U. Ruckert and S. Kramer. Margin-base first-order rule [11] M. Shamim, M. Anwaruddin, and H. A. N. J. Nagara-[12] H. B. Shen and C. K. Chou. Ensemble classifier for [13] M. Turcotte, S. Muggleton, and J. E. Sternberg. Auto-
