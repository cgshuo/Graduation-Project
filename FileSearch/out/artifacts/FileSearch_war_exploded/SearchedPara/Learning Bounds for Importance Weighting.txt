 applications, there may be data regarding users who clicked on some advertisement link but little data available is drawn from a source domain different from t he target domain. These issues of biased sampling or adaptation have been long recognized and studied in the statistics literature. [11, 29, 16, 8, 25, 6] or domain adaptation [3, 7, 19, 10, 17] in the recent machine learning and natural language processing literature.
 Acommontechniqueusedinseveralofthesepublicationsfor correcting the bias or discrepancy is which training points are drawn. A favorable property of thi sdefinition,whichisnothardtoverify, is that it leads to unbiased estimates of the generalization error [8].
 was originally motivated by the observation that, while thi scorrectivetechniqueseemsnatural,in  X 
Q /  X  P .Theresultsindicatemeanvaluesoftheerrorover40runs  X  one standard deviation. at (0 , 0) and (2 , 0) but with standard deviation  X  Q .Thehypothesisclassisthatofhyperplanes In Section 4, we discuss other examples where importance wei ghting does not succeed. of boosting by [9] who suggest that importance weighting mus tbeusedwithcareandhighlightthe need for convergence bounds and learning guarantees for thi stechnique.
 We study the theoretical properties of importance weightin g. We show using standard generaliza-tion bounds that importance weighting can succeed when the w eights are bounded. However, this condition often does not hold in practice. We also show that, remarkably, convergence guarantees can be given even for unbounded weights under the weak assump tion that the second moment of the these bounds to guarantees for other possible reweightings .Theseresultssuggestminimizingabias-algorithm based on these ideas and report the results of expe riments demonstrating its benefits. Throughout this paper, we consider the case where the weight function w is known. When it is Neumann X  X  rejection sampling technique [28] can then be use d. We note however that it requires w to be bounded by some constant M ,whichisoftennotguaranteedandisthesimplestcaseofour eralization bounds for importance weighting in the bounded case. We also present a general lower deals with the more frequent case of unbounded w .Standardgeneralizationboundsdonotapply functions under the assumption that the second moment is bou nded (see Appendix) and use them to derive learning guarantees for importance weighting in thi smoregeneralsetting.InSection5,we sults. We also discuss why the commonly used remedy of trunca ting or capping importance weights the properties of an alternative reweighting also commonly used which is based on normalized im-portance weights, and discuss its relationship with the (un normalized) weights w . drawn. We also denote by H the hypothesis set used by the learning algorithm and by f : X  X  Y the target labeling function. 2.1 R  X  enyi divergences Our analysis makes use of the notion of R  X enyi divergence, an information theoretical measure of the difference between two distributions directly relevan ttothestudyofimportanceweighting.For  X   X  0 ,theR  X enyidivergence D  X  ( P $ Q ) between distributions P and Q is defined by [23]  X  =1 ,itcoincideswiththerelativeentropy.Wedenoteby d  X  ( P $ Q ) the exponential in base 2 of the R  X enyi divergence D  X  ( P $ Q ) : 2.2 Importance weights ing, the expectations are taken with respect to Q .
 Lemma 1. The following identities hold for the expectation, second m oment, and variance of w : Proof. The first equality is immediate. The second moment of w can be expressed as follows in terms of the R  X enyi divergence: Thus, the variance of w is given by  X  2 ( w )=E Q [ w 2 ]  X  E Q [ w ] 2 = d 2 ( P $ Q )  X  1 . about the target function f .Notethattheunnormalizedimportanceweightingofthelos sisunbiased: The following lemma gives a bound on the second moment.
 Lemma 2. Fo r a l l  X  &gt; 0 and x  X  X ,thesecondmomentoftheimportanceweightedlosscanbe bounded as follows: Fo r  X  =1 ,thisbecomes R ( h ) 2  X  E x  X  Q [ w 2 ( x ) L 2 h ( x )]  X  d 2 ( P $ Q ) . Proof .Thesecondmomentcanbeboundedasfollows:
E Proposition 1 (single hypothesis) . Fix h  X  H .Forany  X  &gt; 0 ,withprobabilityatleast 1  X   X  , The upper bound M ,thoughfinite,canbequitelarge. Thefollowingtheorempro vides a more favorable bound as a function of the ratio M/m when any of the moments of w , d  X  +1 ( P $ Q ) , function of  X  [23, 2], in particular: Theorem 1 (single hypothesis) . Fix h  X  H .Then,forany  X   X  1 ,forany  X  &gt; 0 ,withprobabilityat least 1  X   X  ,thefollowingboundholdsfortheimportanceweightingmet hod: Fo r  X  =1 after further simplification, this gives R ( h )  X  &amp; R w ( h )+ 2 M log 1  X  3 m + Thus, by Bernstein X  X  inequality [4], it follows that: holds for the importance weighting method: Using the sub-additivity of hypothesis set and for  X  =1 ,theapplicationoftheunionboundyieldsthefollowingres ult. probability at least 1  X   X  ,thefollowingboundholdsfortheimportanceweightingmet hod: instead of | H | or a related measure based on samples of size m [20].
 divergence of the second order in the convergence of importa nce weighting in the bounded case. Proposition 2 (Lower bound) . Assume that M&lt;  X  and  X  2 ( w ) /M 2  X  1 /m .Assumethat H contains a hypothesis h 0 such that L h c =2 / 41 2 ,suchthat Proof. Let  X  H =sup h  X  H  X  ( wL h ) .Ifforall x  X  X , L h  X  ( w )=  X  2 H .Theresultthenfollowsageneraltheorem,Theorem9proven in the Appendix. natural cases, as illustrated by the following examples. 4.1 Examples Assume that P and Q both follow a Gaussian distribution with the standard devia tions  X  P and  X  Q and with means  X  and  X  &amp; : importance weights are unbounded, d  X  ( P $ Q )=sup x P ( x ) Q ( x ) =+  X  ,andtheboundofTheorem1 is not informative. The R  X enyi divergence of the second orde risgivenby: That is, for  X  Q &gt; property of the R  X enyi divergence, a similar situation hold sfortheproductandsumsofsuchGaussian distributions. Hence, in the rightmost example of Figure 1, the importance weights are unbounded, but their second moment is bounded. In the next section we pro vide learning guarantees even for do not hold, and, as illustrated in Figure 1, learning is sign ificantly more difficult. This example of Gaussians can further illustrate what can go wrong in importance weighting. As-sume that  X  =  X  &amp; =0 ,  X  Q =1 and  X  P =10 .Onecouldhaveexpectedthistobeaneasycasefor importance weighting since sampling from Q provides useful information about P .Theproblem is, however, that a sample from Q will contain a very small number of points far from the mean asampleofsize m and  X  Q =1 ,theexpectedvalueofanextremepointis nate all other weights and necessarily have a huge influence o ntheselectionofahypothesisbythe learning algorithm.
 Another related example is when  X  Q =  X  P =1 and  X  &amp; =0 .Let  X  , 0 depend on the sample size m .If  X  is large enough compared to log( m ) ,then,withhighprobability,alltheweightswillbe be negligible (in fact both an event and its complement). If w enormalizetheweights,theissue is overcome, but then, with high probability, the maximum we ight dominates the sum of all other weights, reverting the situation back to that of the previou sexample. 4.2 Importance weighting learning bounds -unbounded case As in these examples, in practice, the importance weights ar etypicallynotbounded.However,we shall show that, remarkably, under the weak assumption that the second moment of the weights w , d 2 ( P $ Q ) ,isbounded,generalizationboundscanbegivenforthiscas easwell. Thefollow-Theorem 3. Let H be a hypothesis set such that Pdim( { L h ( x ): h  X  H } )= p&lt;  X  .Assumethat d following holds: d ( P $ Q ) (Lemma 2). Thus, by Corollary 1, we can write exists h  X  H such that Since by assumption w ( x i ) &gt; 0 for all i  X  [1 ,k ] ,thisimpliesthat The convergence rate of the bound is slightly weaker ( O ( m  X  3 / 8 ) )thanintheboundedcase the bound and thus in the convergence of importance weightin gintheunboundedcase. u&gt; 0 .Let &amp; R u ( h )= 1 m Theorem 4. Let H be a hypothesis set such that Pdim( { L h ( x ): h  X  H } )= p&lt;  X  .Assumethat the following holds: | R ( h )  X  &amp; R u ( h ) |  X  learning with equal weights for all examples (Unweighted), Importance weighting, using Quantiles to parameterize the function u ,andCappingthelargestweights.
 Proof. Since R ( h )=E[ w ( x ) L h ( x )] ,wecanwrite and thus 2 p =Pdim( { L h ( x ): h  X  H } ) by a proof similar to that of Theorem 3.
 The theorem suggests that other functions u than w can be used to reweight the cost of an error on each training point by minimizing the upper bound, which i satrade-offbetweenthebiasterm | E
Q [( w ( x )  X  u ( x )) L h ( x )] | and the second moment max leads to the following optimization problem: where U is a family of possible weight functions out of which u is selected.
 Here, we consider a family of functions U parameterized by the quantiles q of the weight function average of w over that quantile. For small values of  X  ,thebiastermdominates,andveryfine-grained quantiles minimize the bound of equation (11). For large val ues of  X  the variance term dominates and the bound is minimized by using just one quantile, corres ponding to an even weighting of the training examples. Hence by varying  X  from small to large values, the algorithm interpolates between standard importance weighting with just one exampl eperquantile,andunweightedlearning where all examples are given the same weight. Figure 2 also sh ows the results of experiments for optimal q is determined by 10-fold cross-validation. We see that a mor erapidconvergencecanbe obtained by using these weights compared to the standard imp ortance weights w . Another natural family of functions is that of thresholded v ersions of the importance weights { u by choosing an arbitrary value  X  .Theadvantageofthisfamilyisthat,bydefinition,theweig hts are in performance is observed until the largest 1% of the weight sarecapped,inwhichcaseweonly weights reflect the true w and are not an artifact of estimation uncertainties. weights. Thus, while in the unnormalized case the unweighte dempiricalerrorisreplacedby in the normalized case it is replaced by with W = advantage of the normalized weights is that they are by defini tion bounded by one. However, the similar to those we pointed out in the Section 4 affect the nor malized weights as well. Here, we maintain the assumption that the second moment of th eimportanceweightsisbounded and analyze the relationship between normalized and unnorm alized weights. We show that, under this assumption, normalized and unnormalized weights are i nfactveryclose,withhighprobability. Observe that for any i  X  [1 ,m ] , Thus, since w ( x i ) W  X  1 ,wecanwrite E the following inequality holds which implies the same upper bound on We presented a series of theoretical results for importance weighting both in the bounded weights case and in the more general unbounded case under the assumpt ion that the second moment of the depend on the R  X enyi divergence of the distributions P and Q .Accuratelyestimatingthatquantity samples. Finally, our novel unbounded loss learning bounds are of independent interest and could be useful in a variety of other contexts.
 [1] M. Anthony and J. Shawe-Taylor. A result of Vapnik with ap plications. Discrete Applied [2] C. Arndt. Information Measures: Information and its Description in S cience and Engineering . [3] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Ana lysis of representations for domain [5] A. Beygelzimer, S. Dasgupta, and J. Langford. Importanc eweightedactivelearning.In ICML , [6] S. Bickel, M. Br  X uckner, and T. Scheffer. Discriminativ elearningfordifferingtrainingandtest [7] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wor tman. Learning bounds for domain [8] C. Cortes, M. Mohri, M. Riley, and A. Rostamizadeh. Sampl eselectionbiascorrectiontheory. [9] S. Dasgupta and P. M. Long. Boosting with diverse base cla ssifiers. In COLT ,2003. [11] M. Dud  X  X k, R. E. Schapire, and S. J. Phillips. Correctin gsampleselectionbiasinmaximum [12] R. M. Dudley. A course on empirical processes. Lecture Notes in Math. ,1097:2 X 142,1984. [14] C. Elkan. The foundations of cost-sensitive learning. In IJCAI ,pages973 X 978,2001. [15] D. Haussler. Decision theoretic generalizations of th ePACmodelforneuralnetandother [16] J. Huang, A. J. Smola, A. Gretton, K. M. Borgwardt, and B. Sch  X olkopf. Correcting sample [17] J. Jiang and C. Zhai. Instance Weighting for Domain Adap tation in NLP. In ACL ,2007. [18] J. S. Liu. Monte Carlo strategies in scientific computing .Springer,2001. [19] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adapt ation: Learning bounds and algo-[20] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In [21] D. Pollard. Convergence of Stochastic Processess .Springer,NewYork,1984. [22] D. Pollard. Asymptotics via empirical processes. Statistical Science ,4(4):341 X 366,1989. [23] A. R  X enyi. On measures of information and entropy. In Proceedings of the 4th Berkeley Sym-[24] H. Shimodaira. Improving predictive inference under c ovariate shift by weighting the log-[25] M. Sugiyama, S. Nakajima, H. Kashima, P. von B  X unau, and M. Kawanabe. Direct importance [26] V. N. Vapnik. Statistical Learning Theory .JohnWiley&amp;Sons,1998. [27] V. N. Vapnik. Estimation of Dependences Based on Empirical Data, 2nd ed. Springer, 2006. [28] J. von Neumann. Various techniques used in connection w ith random digits. Monte Carlo [29] B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive le arning by cost-proportionate example
