 With a rapid expansion of online social networks (OSNs), millions of users are tweeting and sharing their personal status daily without being aware of where that information eventually travels to. Likewise, with a huge magnitude of data available on OSNs, it poses a substantial challenge to track how a piece of information leaks to specific targets. In this paper, we study the problem of smartly sharing infor-mation to control the propagation of sensitive information in OSNs.
 In particular, we formulate and investigate the Maximum Circle of Trust problem of which we seek to construct a circle of trust on the fly so that OSN users can safely share their information knowing that it will not be propagated to their unwanted targets (whom they are not willing to share with). Since most of messages in OSNs are propagated within 2 to 5 hops, we first investigate this problem under 2-hop infor-mation propagation by showing the hardness of obtaining an optimal solution, along with an algorithm with proven performance guarantee. In a general case where information can be propagated more than two hops, the problem is #P-hard i.e. the problem cannot be solved in a polynomial time. Thus we propose a novel greedy algorithm, hybridizing the handy but costly sampling method with a novel cut-based estimation. The quality of the hybrid algorithm is compa-rable to that of the sampling method while taking only a tiny fraction of the time. We have validated the effective-ness of our solutions in many real-world traces. Such an extensive experiment also highlights several important ob-servations on information leakage which help to sharpen the security of OSNs in the future.
 F.2.2 [ Analysis of Algorithms and Problem Complex-ity ]: Nonnumerical Algorithms and Problems Algorithms, Performance, Theory Social networks, circle of trust, algorithms, complexity
Recent rapid development of Online Social Networks (OSNs) has revolutionized the way of human interaction and dras-tically changed the landscape of communications and infor-mation sharing in the cyberspace nowadays [9, 25]. One of the most important characteristics of OSNs is the  X  X ord-of-mouth X  exchanges, in which information can be propa-gated from friends to friends of friends and eventually widely spread over the network. By leveraging this power, many organizations and companies have been using OSNs as an effective medium to increase their visibility and advertise their products [1]. Likewise, millions of OSN users are shar-ing their personal status daily with a hope to keep many of their friends, near or far, updated. Unfortunately, the fast information propagation is a double-edged sword, that said, it not only quickly propagates information to our friends but at the same time, it also spreads the information to many unwanted targets whom we do not want to share with.
Let us consider the following simple example which high-lights a basic need for any organizations or companies who use OSNs. Suppose that Bob wants to share with his friends some of his personal pictures and stories in Facebook, yet he is reluctant to let Chuck know about them. Being careful, Bob just shares to the list of his friends in Facebook where Chuck is not in that group with a belief that Chuck cannot see those pictures. Unfortunately, Alice, who is a friend of both Bob and Chuck, replied to the post, and thus Chuck will see the message from Alice and learn about Bob X  X  shar-ing. Assume that Bob is extra careful by using the Custom Privacy function provided by Facebook to hides his sharing from Chuck. Unfortunately, this function only tracks and hide the message based on the message-ID, not on its prop-agation. Therefore, when Bob X  X  friend Alice posts a new message mentioning Bob X  X  pictures and stories, this new message cannot be hidden from Chuck anymore since its ID is no longer the same as the original message from Bob. Consequently, Chuck will still learn about Bob X  X  pictures and stories. Thus it raises a practical question: Is there any mechanism for Bob to share his pictures and stories to as many friends as possible without reaching to Chuck?
What Bob really needs is that right before he is ready to share his stories, he should have an opportunity to con-struct on the fly a subset of his friends to share these stories with so that the probability of Chuck knowing them is very small (for example see Fig. 1). We refer to this subset as a circle of trust. Of course, we cannot filter out most of his friends because one of the main purposes of posting message on OSNs is to share the information with as many friends as possible. Therefore, we formulate a new optimization prob-lem, called Maximum Circle of Trust (MCT), to construct a circle of trust for Bob so that once Bob posts a message to this CT, the probability of such friends in this CT spread-ing the message to unwanted targets is bounded under some input thresholds and the expected visibility of the message is maximized. Here the message is visible to a friend of Bob if the message appears on the wall of the Bob X  X  friend.
The realization of this function in OSNs, unfortunately, requires us to study many fundamental problems in the large-scale networks. With a huge magnitude of OSN users and data available on OSNs, it poses a substantial challenge to understand how information reach to specific targets. In addition, it requires a significant work in data sampling to verify whether a message can be leaked from one user to another one in the network. Unfortunately, the time com-plexity of the sampling method is high, therefore, it does not suitable for an on-the-fly construction.

In this paper, we develop solutions to the MCT problem and address the above two fundamental problems. More specifically, our contributions are summarized as follows: The rest of this paper is organized as follows. In Section 2, we introduce a SML propagation model and the formal defi-nition of the MCT problem. Section 3 includes the complex-ity results and our randomized algorithm that comes with a performance guarantee for the 2-MCT problem. For general MCT problem, the IGC approach are provided in Section 4. The experimental evaluation is illustrated in Section 5 and related work is presented in Section 6. Finally, Section 7 provides some concluding remarks.
In order to solve the MCT problem, we first introduce in this section a novel information leakage model, namely Sharing-Mentioning Leakage (SML) model. The model is based on the well-known probabilistic model Independent Cascade (IC) [11, 16, 17, 7].

The social network is modeled as a directed graph G = ( V,E ), where vertices in V represent users and edges in E represent social links among the users. In the IC model [16], each edge ( u,v ) is associated with a real number w [0 , 1] that indicates how likely user u would influence user v . When u becomes active, it has a single chance to activate v with the probability of success w uv .

In the SML model, we consider two different informa-tion propagation (leakage) mechanisms, namely sharing and mentioning as follows. Correspondingly, each edge ( u,v ) is associated with two probabilities: the sharing probability a uv and the mentioning (a) Google+ (b) Facebook probability p uv that indicate how likely u will make the in-formation visible to v via sharing and/or mentioning. Note that while the sharing propagation can easily be blocked at will, e.g. by using privacy options in Facebook or build-ing manual sharing circles in Google+, it is much harder to block the message propagation via mentioning as there are many different ways to convey a same message.

We now describe the leakage propagation process in G which is illustrated in Fig. 2. Assume that a source user s  X  V shares a message m to a subset C = { v 1 ,v 2 ,...,v of his/her friends. In the first round, a user v  X  C will learn about the message m with the sharing probability a sv i.e. there is no propagation via mentioning in the first round. In the second round, if v learn about m in the first round, then each friend w of v can learn about message m from v via sharing with probability a vw or via mentioning with proba-bility p vw . The propagation continues up to  X  rounds, where  X  &gt; 0 is a predefined integral constant in the model. Our SML model assumes the independence propagation between different links as in the IC model.

Leakage Path. A leakage path ( s = v 0 ,v 1 ,...,v l = t ) is a path between the source s and an unwanted target t , in which ( v i ,v i +1 )  X  E and v i +1 learns about the message m from v i for all i = 0 , 1 ,...,l  X  1. If v i +1 learns about the message via sharing, then ( v i ,v i +1 ) is called a sharing edge, otherwise ( v i ,v i +1 ) is a mentioning edge. The first edge on a leakage path is always a sharing edge. In addition, at least one edge on the path must be a mentioning edge as we shall discuss next.

Sharing Options in OSNs. We present the privacy settings that help users control who see the message in OSNs and the constraint on the leakage paths in those networks.
Facebook . Facebook provides options to share a post to a specific group of friends. Furthermore, it provides the  X  X ide this from X  feature which hides posts from certain users (see Fig. 3b). Regardless of how the friends of the source and the unwanted targets reshare the message, Facebook keeps track of the original message and make it invisible from the unwanted targets. In other words, the unwanted targets cannot learn about the message through a path with all sharing edges. Thus every leakage path must have at least one mentioning edge.

Google+ . This social networks allows users to share the information to a circle of friends. Each circle is a predefined group of friends which can be overlapping and hierarchical. However, the  X  X ide this from X  option in Facebook is not avail-able. Instead, Google+ provides the  X  X ock this post X  option which prohibits resharing the message (see Fig. 3b). If the message is locked, mentioning is the only way to propagate the information further. Hence, the second edge in every leakage path in Google+ must be a mentioning edge.

Twitter . Twitter does not allow to hide messages (tweets) from some users. There are only two options to hide the tweets: either deleting the tweet permanently or hide all of the tweets at once by setting the profile to private. However, if Twitter allows users to share tweets to specific followers in future, then the circle of trust concept can be applied to give Twitter X  X  users more control over sharing their tweets.
Leakage Propagation model . We now provide some intuitive explanation and justification of our model. The SML model is defined based on the Independent Cascade (IC) model for influence propagation [16]. However, the SML model is different: 1) the propagation happens in two different channels: sharing and mentioning 2) the IC model is for influence propagation starting from a seed set, while our model is for the message originated from a single user. Note that other diffusion models e.g. the Linear threshold model [16, 17] can also be used to characterize the informa-tion propagation in the network.

For our study of the circle of trust concept, we consider that the social network with sharing and mentioning proba-bilities is given. A number of researches provide methods in extracting the social network and influence probabilities [13, 3, 23, 12]. Those methods can be applied to extract the shar-ing and mentioning probabilities. In addition, the EdgeRank values which indicate how likely two users interact with each other in Facebook [8] can be used to infer those probabilities. However learning those sharing-mentioning probabilities as well as the combination of the sharing-mentioning mecha-nisms with other information propagation models are not the focus of this work. Instead, this paper focuses on find-ing solutions for the MCT problem. The solutions for MCT, proposed in this paper, are relatively independent with the leakage propagation model. That is they can be easily mod-ified to work with other leakage propagation models.
In an OSN, when a user s posts some message m , his aim is usually to share it with as many friends as possible, i.e., to maximize the visibility of his message to his friends while preventing it from reaching to some unwanted targets . Considering the spread of information within  X  hops from the source, we study the following  X  -Hop-Propagation Max-imum Circle Of Trust (  X  -MCT) problem, which constructs a circle of trust, a subset of the source X  X  friends. The goal is to maximize the expected visibility of the message and to restrict the leakage probability of each unwanted target to a certain degree .

Note that in our model, the source user s will neither share nor mention the message to his unwanted targets in case the unwanted targets are the source X  X  friends.
We study the special case when only vertices within two-hop from the source are considered. We believe this spe-cial case covers most of real-world scenarios for two reasons. First, the source must be aware of the unwanted targets, thus, the unwanted targets are often within a close neigh-borhood of the source. Second, acquiring more than two-hop neighbors of the source might be computationally expensive to implement in distributed applications in OSNs.

The 2-MCT problem can be shown to be NP-hard by a reduction from the subset sum problem. Thus, we are left with two choices: designing heuristics which can have arbi-trary bad worst-case performance or designing approxima-tion algorithms which can guarantee the produced solutions are within a certain factor from the optimal. Formally, a  X  -approximation algorithm for a maximization (minimization) problem always returns solutions that are at most  X  times smaller (larger) than an optimal solution.

In this section, we first formulate the Integer Linear Pro-gramming (ILP) for MCT problem. Then, we present an approximation algorithm based on rounding the fractional solution of a relaxation of the ILP. We end the section with the proof on the hardness of approximation that shows the tightness of our obtained approximation factor. We define an indicator variable x u for each friend u  X  N ( s ) of s where x u = 1 if u is included to CT, and x u otherwise. Our objective is to maximize the expected visi-bility of the message, i.e. the expected number of s  X  friends that the message is visible to. Thus, it can be written as the sum of sharing probabilities of s  X  X  friends except unwanted
For an unwanted target t j , the message will be leaked to t iff a neighbor u of s is informed with probability a su and u further leaks to t j via mentioning with probability p ut Therefore, all leakage paths of length two will consist of one sharing edge following by one mentioning edge, which is true for both Facebook and Google+. Therefore, the constraint w.r.t. each unwanted target t j can be written as Rearrange and take the logarithm of both sides, we obtain the following Integer Linear Programming (ILP): where w uj =  X  log(1  X  a su p ut j ) and c j =  X  log(1  X   X 
We first solve the LP relaxation of (1) obtained by replac-ing the integral condition on x u with the condition x [0 , 1] ,  X  u . Let x be an optimal fractional solution of the LP relaxation and OPT LP = P a su x u be its objective. Our randomized rounding algorithm, as shown in Algorithm 1, consists of two phases. In the first phase, we independently select vertex u into a CT C with probability x u 4 k for each u  X  N ( s ) \ T . In the second phase, for each violated con-straint j where P u  X  C w uj x u &gt; c j , we remove v with the minimum w vj , i.e. set x v = 0, and repeat until the con-straint is satisfied. Finally, we return the remaining vertices in the CT denoted by C 0 .
 We can verify that C 0 is valid CT with probability one. In the rest, we bound the ratio between the visibility of our constructed CT and that of an optimal CT. Since the expected visibility of C 0 will be
E We present the key lemma for the approximation factor. Lemma 1. For any vertex u  X  N ( s ) \ T , the probability Pr [ u  X  C 0 | u  X  C ]  X  1 2 .

Proof. Let R uj be the event that vertex u is removed from C 0 . Consider the set C 0 at the time u is removed. Since the vertices are removed in non-decreasing order of w uj , all the remaining vertices u 0  X  C 0 have w u 0 j Let v  X  C 0 be the one with the maximum w vj .

Consider two possible cases w vj &gt; c j / 2 and w vj  X  c w vj &gt; c j / 2. Then, Pr [ R uj ] is no larger than the probability of having a vertex v  X  C 0 with w vj &gt; c j / 2. Denote that probability by  X  j . We have If w vj  X  c j / 2, then  X  u  X  C , w uj  X  c j / 2. Since constraint j the following well-known Markov X  X  inequality,
Markov X  X  inequality [2]. If X is any random variable and a &gt; 0 , then Pr( | X | X  a )  X  E ( | X | ) a , we can upper-bound this probability by 1 2 k . Thus, in both cases, Pr [ R uj ]  X  1 2 k . Then by the union bound, we obtain This yields the lemma.
Theorem 1. There is an O ( k ) randomized approxima-tion algorithm for 2-MCT.

Proof. From Lemma 1 and Eq. 2, the expected visibility of C 0 will be at least Since OPT LP is an upper-bound on the optimal solution of MCT, Algorithm 1 is an 8 k approximation algorithm.
In our experiments, we repeat the rounding algorithm (Al-gorithm 1) log 4 k times with  X  = 1 , 2 ,..., 2 log 4 k ; then the best solution is selected.
To investigate the tightness of the above theoretical bound for 2-MCT, we prove its inapproximability which illustrates that the theoretical guarantee is asymptotic tight in terms of the number of the unwanted targets k .

Theorem 2. For any &gt; 0 , 2-MCT is hard to be approx-imated within a factor Proof. We show a gap-preserving reduction [4] from the Max-Clique problem, finding a clique with the largest num-ber of vertices, to 2-MCT. The hardness of approximation of 2-MCT is then derived from the following inapproxima-bility results of Max-Clique by Hastad [14]: for any &gt; 0, there is no polynomial time algorithm that approximates Max-Clique within a factor We begin with the gap-preserving reduction. Given an in-stance G = ( V,E ) of Max-Clique with n = | V | vertices v ,...,v n , we construct an instance G 0 = ( V 0 ,E 0 ) of the 2-MCT problem. The vertices set V 0 includes a source node s , n vertices v 0 1 ,...,v 0 n , and a set T of k = n 2  X  X  E | unwanted targets (vertices). The set of edges E 0 includes n edges from s to all vertices v 0 1 ,...,v 0 n , and edges from v 0 1 constructed as follows. Associate each pair ( v i ,v j ) /  X  E with a vertex t ij  X  T and include both edges ( v 0 i ,t ij ) and ( v into E 0 . Furthermore, all edges have sharing probabilities one and mentioning probabilities 1 / 2. All unwanted targets t ij have the same leakage threshold  X  = 1 / 2.

The key property of the above construction is that for a pair ( v i ,v j ) /  X  E , the leakage probability at t ij than or equal to the leakage threshold 1 / 2, iff either v v (or none of them) is selected into the CT of G 0 . Thus we can verify that there is a one-to-one mapping between the cliques in G to the CT of the same sizes in G 0 .

We use the fact k &lt; n 2 to prove by contradiction that unless P = NP , 2-MCT is hard to approximate within a factor k 1 / 4  X  . Assume that we have a polynomial time al-gorithm to approximate the 2-MCT problem within a fac-tor k 1 / 4  X  . Then for each instance G = ( V,E ) of Max-Clique, we can use that approximation algorithm to find a solution of 2-MCT in G 0 = ( V 0 ,E 0 ) with visibility at least ity of an optimal solution of the 2-MCT instance. The so-lution of the 2-MCT instance induces a clique of the same size in G . Since k &lt; n 2 and the size of an optimal solu-tion of Max-Clique is equal to that of 2-MCT, the induced clique has size at most 1 /n 1 / 2  X  / 2 time the size of a maxi-mum clique. We can set 0 = / 2 and obtain a contradiction to the Hastad X  X  results [14]. Similarly, we can show that 2-MCT is hard to approximate within a factor k 1 / 2  X  for any &gt; 0, unless NP = ZPP . The  X  -MCT problem becomes much harder when  X   X  3.
 In fact, we prove that the problem is # P -complete [22] when  X   X  3 denying the existence of efficient algorithms to solve the problem. To solve the problem in different settings, we provide a family of greedy algorithms for  X  -MCT.

For simplicity, we assume the sharing options of Facebook are in effect. That is all leakage paths must have at least one mentioning edge. The other social networks X  sharing models can be tackled in a similar way.
 We propose an effective algorithm to construct CT, namely Iterative Greedy Construction (IGC). In fact, the algorithm is a meta-algorithm, which can be combined with different leakage estimation methods. First, we introduce this meta-algorithm IGC. Then, we present the combinations of a sam-pling algorithm, a Non-sampling algorithm to estimate leak-age with IGC and their hybrid in the sequential subsections.
The IGC algorithm, as shown in Algorithm 2, iteratively adds one of the source X  X  neighbors into the circle of trust until no more nodes can be added without causing the leak-age probabilities to exceed the thresholds. Specifically, in each iteration, we first update the set of candidate neigh-bors L , those whose addition to CT still guarantees that the leakage levels at each unwanted targets t j does not exceed the threshold  X  j . Denote by ` j ( C ) the probability that the message will be leaked to t j with respect to CT C . Then, v is a candidate if ` j ( C + { v } )  X   X  j  X  j = 1 ..k .
We use a greedy function f ( v ) to evaluate the fitness of user v . The function is defined as follows t after adding v to the CT. The numerator of f ( v ) is selected to be a sv so that the algorithm will favor close friends of s , whose adding increase the visibility.

In addition, the proposed greedy function takes into the account the following quantities By maximizing f ( v ) the algorithm will prefer user v with small denominator. Thus, the algorithm will select the users that have higher remaining leakage tolerances but lower fu-ture potential leakage.

Note that in IGC, we assume the existence of a proce-dure to estimate leakage  X  j ( C ), which can be invoked up to O ( S 2 n | T | ), where S n = | N ( s ) \ T | . Thus, the procedure to estimate leakage is the critical component of IGC. It de-cides both the solution quality and the running time of the meta-algorithm. Due to the #P-hardness of the estimating leakage, exact solutions require exponential time. Hence, we have to resort to approximation methods to estimate ` j ( C ). The most natural method to estimate ` j ( C ) is Monte Carlo Sampling , in which the accuracy depends on the num-ber of sampling times. The method stimulates the random process following the SML propagation model. Each edge ( u,v )  X  E will be generated independently and assigned la-bel  X  X haring X  with probability a uv or generated and assigned label  X  X entioning X  with probability p uv . The estimation of ` ( C ) is calculated as the ratio between the number of time the message reaches t j via a leakage path to the number of sampling times n S , which is chosen as 20,000 in our experi-ments. Recall that in a leakage path, there must be at least one mentioning edge; in addition the path must have length at most  X  . Thus after the sampled graph is generated, we use a modification of the breadth-first search algorithm to find such leakage paths from the source to each unwanted target.
Note that in the sampling process, each edge is either sharing or mentioning edge. This implies that each user in the network forwards the message by either sharing or mentioning but not both. Alternatively, we can allow both sharing and mentioning edges between a pair of users. In our experiments, we follow the first approach.

To speed up the estimation, we can estimate the leakage at all targets at the same time using a single (modified) bread-first search. Then the IGC algorithm, in the worst case, runs in time O ( n S S 2 n ( m + n )).
Sampling methods are rather expensive for large-scale net-works, thus, alternative leakage estimation methods are de-sired to handle the estimate the leakage ` j ( C ). Here, we pro-vide a fast method to calculate a non-trivial upper-bounds on the leakage ` j ( C ) at unwanted target t j . The upper bound is based on identifying minimal pseudo-cutset, as de-fined later, between the source and the unwanted target. In the rest of the section, we assume that a trusted circle C is already given without further mention; and each edge ( u,v ) is associated with a combined sharing-mentioning propaga-tion probability q ( u,v ).

We first define some necessary notations. Let t  X  T be an arbitrary unwanted target that we wish to estimate the leakage level  X  ( t ), defined as the probability of having a path between s and t . For each edge ( u,v ), define event X u,v if the information can propagate through the edge ( u,v ) and X u,v = 0 otherwise i.e. Pr [ X u,v = 1] = q ( u,v ). We define a cutset  X  S,T  X  as  X  S,T  X  = { ( u,v )  X  E | u  X  S,v  X  T } . For a subset of edges A  X  E , let A  X  be the event that all the edges of A do not let the information propagate through i.e. Pr [ A  X  ] =  X  e  X  A (1  X  q e ) . Then, the exact leakage  X  ( t ) can be computed as follow where C s,t is the set of all cutsets  X  S,T  X  satisfied s  X  S and t  X  T . However, computing this exact formulation is in-tractable. We first relax the formulation by replacing C with a collection of cutsets B  X  X  s,t . This yields the follow-ing upper-bound on the leakage  X  ( t ) Note that when B contains only disjoint cutsets, the up-per bound can be efficiently computed in polynomial time. Thus, in our non-sampling estimation method, we seek a maximal collections of disjoint cutsets. The selection of good disjoint cutsets that gives close estimation is governed by two factors. First, we prefer small cutsets; at the same time, we prefer a collection of many cutsets. Note that the number of cutsets is bounded by the distance between the source and the unwanted target. Since if P is a s,t path of minimum length l , every cutset must contains at least one edge in P . Thus, we have the following theorem.

Theorem 3. The maximum number of disjoint cutsets that separate s and t cuts equals the minimum length of a path between s and t .

Instead of using cutsets, we propose the use of pseudo-cutsets , which are the set of edges whose removal makes the distance from the source s to destination t at least  X  + 1 hops. The reason is that disrupting all paths of length at most  X  is sufficient to prohibit the message m to propagate to t . Our algorithm to find the pseudo-cutsets and compute the upper-bound is given in Algorithm 3.

The algorithm first applies Breadth-First Search algorithm to construct layers of vertices. Layer L i consists of vertices at distance i from the source in term of hops. The cutset C i is constructed by including all edges ( u,v ) with u  X  L and v  X  L i +1 but only if d s ( u ) + d t ( v )  X   X   X  1 i.e. there is a path of length at most  X  going through the edge ( u,v ). The upper-bound can be computed efficiently, since it only requires visiting nodes within  X  -hop from the source instead of doing so n S times as in the sampling method.
Although the Disjoint Cutset Upper-bound is not as ac-curate as the sampling method, provided a large number of sampling times, it is the only scalable method to estimate the leakage. Moreover, it is also rather effective in compar-ing the neighbors to find out the ones causing the leaks to the unwanted targets. We name the use of the disjoint cutset upper-bound to estimate the leakage in the IGC algorithm as the Non-sampling Method .
Since the Non-sampling method can overestimate the leak-age at the unwanted targets, it often stops early even more neighbors can still be added to CT without causing exces-sive leakage risks. Thus, the solution of the IGC algorithm using the upper-bound via disjoint cutsets can be further fine tuned.
 Our hybrid method combines the Sampling method and Non-sampling method. The hybrid method consists of two phases. In the first phase, the Non-sampling method is used to quickly construct a CT. In the second phase, the fol-lowing simple heuristics is used together with the sampling algorithm to add more neighbors to the circle: 1. Sort the neighbors that are not included in the circle 2. In that order, include each neighbor into CT and check The hybrid method achieves both the speed of the Non-sampling method and the advantage of the Sampling method, the high solution quality. Since, it can involve at most S calls to the sampling estimation algorithm, the running time of the hybrid method is very competitive for large networks.
We observe in our experiments that often only one to three neighbors are added using this heuristic. Thus, the first phase using the Non-sampling method is rather effec-tive in term of detecting trusted friends. Overall, this hybrid approach turns out to be excellent performance in our ex-periments and gives a great trade-off between the time and the solution quality.
In this section, we perform the experiments on a collection of large OSNs to measure the efficiency of our proposed algo-rithms and apply those algorithms to provide better insights into the information leakage process in social networks. We begin with describing the experimental setup. Then we an-alyze the solution quality and performance of the proposed algorithms. Finally, we seek the answers to the questions around how  X  X elebrities X  and their friends can be in control of their shared information. Dataset. We perform experiments on three real-world OSNs, including Facebook, Twitter and Foursquare. The statistics of the used networks can be found in Table 1. The Facebook dataset is obtained thanks to authors in [24]. The Twitter dataset is extracted from the Twitter network pro-vided by Cha et al. [5] by applying the unbiased sampling method in [10]. Foursquare dataset is our crawled dataset in which a set entrepreneurs and investors is selected as the seeding. After that, we use Foursquare API to get the users and their connections within two hop from the seeds.
Due to the lack of ground truth, we independently assign uniform random numbers between 0 and 1 to sharing proba-Dataset Nodes Edges Density Source Facebook 63,731 905,565 4.46% Ref [24] Foursquare 44,832 1,664,402 8.28% Our data Twitter 88,484 2,364,322 3.02% Sampling in [5] bility a uv and mention probability p uv of each edge. Unless otherwise mentioned, we select the source randomly, then pick up the unwanted targets randomly among the nodes that are within  X  hops from the source.

We implemented and compared the following algorithms 1. Sampling : IGC algorithm using Monte-Carlo Sampling 2. Non-Sampling : IGC algorithm using Disjoint Cut-sets 3. Hybrid : IGC-Hybrid algorithm in Section 4.2; 4. Removal-Greedy , in which users are removed one by 5. Randomized-Rounding : The approximation random-6. Optimal : The optimal solution found by solving the In the sampling subroutine, we stimulate the random process 20,000 times. The optimal solutions and the fractional LP solutions in the randomized algorithm are obtained by using CPLEX optimization package. All algorithms are performed on a cluster of 20 PCs with CPU AMD Opteron 2.00Ghz and 32GB RAM.

Measurements: We measure the following quantities: (1) Visibility(%) : The expected visibility measured as the (2) CT Size(%) : The fraction of the source X  X  friends in-(3) Running Time : The running time in seconds.
We measure the visibility(%) and running time when the leakage threshold ranges from 0.05 to 0.4. For each leakage threshold, we randomly choose 100 sets of random sources and five unwanted targets and average the results. The re-sults for  X  = 2 and  X  = 3 are shown in Figs. 4 and 5, respectively. As shown in Figs. 4, all algorithms Removal-Greedy, Sampling, Hybrid, Randomized Rounding produce optimal or close-to-optimal results in term of visibility. The hy-brid algorithm performs slightly worse than the others, how-ever, the gap is usually negligibly small. As expected, non-sampling method shows the worst results among all. The Visibility obtained with Non-sampling methods are 5% to 10% smaller than the others X . The reason is that when the Non-sampling method examines if any nodes can be added to the CT, it is often pessimistic about the current leakage situation and stop early. Clearly, the non-sampling method should not be used as a standalone method to construct CT, but rather as a method to construct an early version of CT. Similar observations for  X  = 3 can be seen in Fig. 5.
Moderately surprising, not too many friends need to be blocked to prevent information leakage. Even when the leak-age threshold is 5%, the visibility still ranges from 70% to 90%. In addition, the visibility increases quickly when the threshold increases.

We notice that when comparing the cases  X  = 2 and  X  = 3, the visibility in Foursquare decreases, while those in Face-book and Twitter show the reverse trend. Moreover, in case  X  = 3 the visibility in Fourquare does not approach 100% when the leakage threshold increases. The main reason for smaller visibility in Foursquare is the closer distance be-tween sources and targets in Foursquare. This is an artifact caused by the way we crawled the Foursquare network. As we only query the users within two-hops from the seeding in Foursquare, the randomly generated targets in Foursquare are often 2-hops away from the source, regardless of  X  . In contrast, there is a high probability that the distance in hop from the source and the target in Facebook and Twitter equal exactly the value of  X  . This observation, more or less, supports the hypothesis that the closer the source and the unwanted target, the smaller the CT.
The running time of the studied algorithms are shown in the last rows of Figs. 4 and 5. All methods are adequately fast when  X  = 2. When  X  = 3, the Non-sampling and Hybrid method are substantially faster than the other methods. Be-cause the Hybrid avoids most of the calls to the sampling at the early selection steps; it is up to a hundred times faster than the Removal-Greedy, and the Sampling methods.

In comparison to the Sampling method, the running time of Removal-Greedy is excessively high for small leakage thresh-old, when the algorithm takes many rounds to remove nodes until obtaining the CT. In contrast, Sampling method (as well as Hybrid and Non-sampling method) has the running time slightly increased when the leakage threshold increases, which leads to larger CTs. Since we often desire smaller leak-age threshold, in practical the algorithms that construct CT by adding nodes may have higher performance than those that find the CT in a removing nodes fashion.

Clearly, for  X  = 2 the optimal solution by solving IP is the best selection. When  X   X  3, the Hybrid is the method of choice due to both its outstanding solution quality and fast running time, less than 0.1 second. Note that the ability to construct CT quickly is critical, if one wishes to implement the algorithm on social network platforms. For larger net-works and  X  , the only applicable method is, however, the Non-sampling method.
CT might not be useful if it excludes most of the  X  X lose friends X , or strong ties, of the source. Thus we perform ex-periments to see the distribution of the ties X  strength, in-ferred via the sharing probabilities, of users inside (and out-side) CT. In each network, we randomly select 40 source users. For each source user, 100 sets of 5 unwanted targets are randomly selected. Then for each set of unwanted target, we solve IP to find the optimal solution for  X  = 2 and the leakage thresholds 0 . 15. The distribution of ties X  strength (sharing probabilities) are shown in Fig. 6.

As shown in Fig. 6, in all the three networks, the num-ber of strong ties inside CT is still greater than the number of strong ties outside CT, even at the highest level of ties X  strength. For examples, in Foursquare and Twitter, two third of the strong ties remain in CT. We observe, at the higher levels of ties X  strength, there is a tendency of more strong ties being excluded from CT. Nevertheless, the pro-duced CTs are acceptable as they include the majority of the strong ties.
We provide several observations obtained by solving IP (  X  = 2) and using the Hybrid method (  X  = 3) that give the best performance in general.

Can Celebrities Keep Their Secrets? We investigate the question whether or not  X  X elebrities X , users with a large number of friends or followers, can only share their informa-tion with a small number of their friends to avoid informa-tion leakage. Specifically, we are interested in the relation between the degree of the source and their CT sizes. Intu-itively, when a user has more friends, there is a high chance his friends will forward the information further. Thus, to avoid information leakage, the CT of  X  X elebrities X  possibly includes only a small fraction of their friends to CT.
To verify the hypothesis, we select the source users with different degrees, increasing by 1% of the maximum degree in each step. For each source, we choose ten random sets of five unwanted targets and compute the average size of CT by solving the IP. The results are shown in Fig. 7.
In contrast of our hypothesis, their is no sharp decrease in the CT size. Indeed, for the source with the highest de-gree, the CTs contain more than 95% of the neighbors. This suggests that by carefully constructing CT, smartly sharing information to avoid leakage is possible for  X  X elebrities X .
Can We Trust Our Popular Friends? There is an incentive not to share our sensitive information to a popu-lar person (celebrity) who will easily leak the information further. In other words, the friends of a celebrity are un-likely to trust him. To investigate the effect of the  X  X elebrity level X , we select in each network 10 users with different de-gree, incrementing by 10% of the maximum degree. For each  X  X elebrity X , we select each of his neighbor to be the source and compute the percentage that the  X  X elebrity X  is included in his friends X  CTs.

As illustrated in the Fig. 9, the  X  X rust level X  of the friends for a user decreases quickly when the degree of the user increases, i.e. when the user becomes more popular. How-ever, even for the user with the highest degree, there are still half of his friends trust him as in the cases of Facebook and Foursquare. For Twitter, only 20% of the friends trust the considered user. This can be explained by the fact that Twitter is a directed network with low reciprocity (A follows B, but B does not follow A) [18]. Thus, the celebrities are not even considered to be friends to be included in the CTs. Roughly speaking,  X  X elebrities X  in social networks such as Facebook are relatively  X  X rusted X , while the same conclusion does not hold for the  X  X ocial media X  network Twitter [18].
The Unwanted Friend of Friends. Another interest-ing question is the impact of the relation between source user and the unwanted targets to the CT. We have observed in the Foursquare network  X  = 3 that the closer the distance between sources and unwanted targets, the smaller the CT. Here, we study the relation between the number of common friends and the CT size. Is it necessary to block all these common neighbors?
To address these questions, we then plot the average Vis-ibility and CT Size with respect to the number of common friends on Fig. 8. As can be seen, the CT Size decreases sharply from 50% to 10% with the increase of the number of common friends. However, the visibility reduces at a much lower rate. Regardless of the number of common neighbors, the visibility is still nearly 60%.

Therefore, the case when the unwanted targets are close to the source, is not as bad as it appears. The growing gap between the Visibility and CT size reflects that we still can effectively select high visibility neighbors into the CT despite the shrinking of the CT.
Our work is the first attempt to address the smartly shar-ing information in OSNs without leaking them to unwanted targets, thus there is not many related work. The most rel-evant works are the works of selective sharing by Shen et al. [22] and Kairam et al. [15]. Shen et al. [22] address the selective sharing problem as an optimization problem and provide various hardness and polynomial-time approxima-tion schemes for the problem. Kairam et al. [15] analyze how users organize and select their audiences and derive in-teresting observations on how users share across life facets, and ties of vary strength.
 There is set of papers studied on the privacy issues in OSNs [19, 20, 21]. Lam et al. [19] showed that, in cur-rent OSNs, no matter how much efforts a user puts to pro-tect his personal information, it cannot be prevented from being revealed by some malicious users by examining their  X  X ublic X  interactions with friends. Later on, for the sake of such unintentional information spreading, Ngoc et al. [20] then presented a new metric to quantify the privacy. Noting the potential risks by disclosing information to OSN compa-nies, Nigusse et al. [21] proposed an information flow model , which made the existing privacy techniques more practical. However, these studies only focus on the users X  personal pro-file, i.e. name, address, etc., not on the information sharing and posting. In addition, they neglected the information leakage led by multi-hops diffusion.
In this paper, we study a problem of how smartly shar-ing our information in OSNs without leaking them to un-wanted targets. We formulate this problem as the optimiza-tion problem, namely maximizing a circle of trust (MCT), of which we construct a circle of trust to maximize the ex-pected visible friends such that the probability of informa-tion leakage is reduced to some degree. We have proven the inapproximability and provided a randomized algorithm with theoretical guarantees for the 2-MCT problem. When the information can be propagated more than 2 hops, we provided an iterative greedy algorithms based on a novel disjoint-cut estimation, instead of using sampling method. Extensive experiments in real-world traces show that our solutions can construct a large size of CT within a short amount of time.
 There are many future directions emerged from our work. First, the knowledge about how people form  X  X ircles X  of friends and share information can be incorporated to identify the proper input parameters for the SML models. Second, the MCT problem can be studied under different propagation models in literature. Furthermore, we believe controlling the flow of the shared information is an important issue which can inspire many other problems.
