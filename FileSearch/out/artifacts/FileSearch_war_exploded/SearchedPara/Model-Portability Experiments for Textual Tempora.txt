 The recognition of time expressions such as April 2011 , mid -September and early next week is a cr u-cial first step for applications like question answe r-ing that must be able to handle temporally anchored queries. This need has i n spired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference name d entity task (Grishman and Sundheim, 1996) , the Automatic Content Extraction t ime normaliz a tion task ( http://fofoca.mitre .org/tern.html ) and the TempEval 2010 time expression task (Verhagen et al., 2010) . Many researchers co m-peted in these tas ks, applying both rule -based and machine -learning approaches ( Mani and Wilson, 2000 ; N e gri and Marseglia, 2004 ; Hacioglu et al., 2005; Ahn et al., 2007; Po veda et al., 2007 ; S tr X tgen and Gertz 2010; Llorens et al., 2010) , and achieving F1 measures as high as 0.86 for reco g-nizing te mporal expre s sions.

Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We ther e-fore take a state -of -the -art time recognizer and ev a-luate it both on TempEval 2010 and on two new test sets drawn from Reuters and W i kipedia.
At the same time, we are interested in helping the model recognize more types of time expre s-sions than are available explicitly in the newswire training data. We therefore introduce a semi -supervised approach for expanding the training data, where we take words from temporal expre s-sions in the data, substitute these words with likely synonyms, and add the gene rated examples to the training set. We select synonyms both via Wor d-Net, and via predictions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009) . We then evaluate the semi -supervised mo d-el on the TempEval, Reuters and Wikipedia test sets and observe how well the model has e x panded its temporal vocabulary. Semi -supervised approaches have been a pplied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification ( Su r-deanu et al., 2006).

The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi -supervised approach to time expression re c-ognition. They begin by selecting 100 time expre s-sions as seeds, selecting only expressions that are almost always annotated as times in the tr aining half of the Automatic Co n tent Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding t o-kens, parts -of -speech, synta c tic chunks etc.) and t hen search for new seeds given their patterns. The patterns resulting from this iterative process achieve F1 scores of up to 0.604 on the test half of the Automatic Content Extraction co r pus.

Our approach is quite different from that of (P o-veda et al., 200 9)  X  we use our training corpus for learning a supervised model rather than for s e-lecting high precision seeds, we generate add i-tional training examples using synonyms rather than bootstrapping based on patterns, and we evaluate on Reuters and Wikipedia da ta that differ from the domain on which our model was trained. The proposed method implements a supervised machine learning approach that classifies each chunk -phrase candidate top -down starting at the parse tree root provided by the OpenNLP parser. Time expressions are identified as phrasal chunks with spans derived from the parse as described in (Kolomiyets and Moens, 2010). 3.1 Basic TempEval Model We implemented a l o gistic regression model with the following fe a tures for each phrase -candidate:  X  The h ead word of the phrase  X  The part -of -speech tag of the head word  X  All tokens and part -of -speech tags in the  X  The word -shape representation of the head  X  The condensed word -shape representation for  X  The c oncatenated string of the syntactic types  X  The depth in the parse tree 3.2 Lexical Resources for Bootstrapping Sparsity of annotated corpora is the biggest cha l-lenge for any supervised machine learning tec h-nique and especially for porting the trained models onto other domains . To overcome this problem we hypothesize that knowledge of semantic ally simi lar words , like temporal triggers, could be found by associa t ing words that do not occur in the training set to similar words that d o occur in the training set. Furthermore, we would like to learn these sim i larities automatically to be independent of knowl edge sources that might not be available for all languages or domains. The first option is to use the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009)  X  a la n guage model that learns from an unlabeled corpus how to pr o-vide a weighted set of synonyms for words in co n-text. The LWLM model is trained on the Reuters news article co r pus of 80 million words. 
WordNet ( M iller, 1995 ) is a nother r e source for synonyms widely used in research and applications of natural la n guage processing. Synonyms from WordNet seem to be very useful for bootstrapping as they provide replacement words to a specific word in a particular sense . F or each synset in WordNet there is a collection of other  X  X ister X  sy n-sets, called coordinate terms, which are topolog i-cally located under the same hypernym. 3.3 Bootstrapping Strategies Having a list of synonyms for each token in the sentence, we can repla ce one of the original tokens by its synonym while still mostly preserving the sentence sema n tics. We choose to replace just the headword, under the assumption that since temp o-ral trigger words usually occur at the headword p o sition, adding alternative syn onyms for the headword should allow our model to learn temp o-ral triggers that did not a p pear in the training data. 
We designed the following bootstrapping strat e-gies for generating new te m poral expressions:  X  LWLM : the phrasal head is replaced by one of  X  WordNe t 1 st Sense : Synonyms and coord i nate  X  WordNet P seudo -Lesk : The synset for the  X  LWLM+WordNet : The intersection of the In this way for every annotated time expressi on we generate n new examples ( n  X  [1,10]) and use them for training bootstrapped classification mo d els. The tested model is trained on the official Te m-pEval 2010 training data with 53450 tokens and 2117 annotated TIMEX3 tokens. For testi ng the portability of the model to other domains we ann o-tated two small target domain document colle c-tions with TIMEX3 tags. The first corpus is 12 Reuters news articles from the Reuters corpus (Lewis et al., 2004), containing 2960 total tokens and 240 ann o tated TIMEX3 tokens ( int er -annotator agre e ment 0.909 F1 -score). The second corpus is the Wikip e dia article for Barak Obama ( http://en.wikipedia.org/wiki/Obama ) , containing 7029 total tokens and 512 annotated TIMEX3 t o-kens (inter -annotator agreement 0.901 F1 -score).
The basic TempEval model is evaluated on the source domain ( TempEval 2010 evaluation set  X  9599 tokens in total and 269 TIMEX3 a n notated tokens) and target domain data (Reuters and Wikipedia) using the TempEval 2 010 evaluation metrics. Since por ting the model onto other d o-mains usually causes a performance drop, our e x-periments are focused on improving the results by employing different bootstrapping stra t egies 1 . The recognition performance of the model is r e-ported in Table 1 (column  X  X asic TempEval Mo d-el X ) for the source and the target domains. The basic Te m pEval model itself achieves F1 -score of 0.834 on the official TempEval 2010 evaluation corpus and has a potential rank 8 among 15 pa r-ticipated systems. The to p seven TempEval -2 sy s-tems achieved F1 -score between 0.83 and 0.86. However, this model does not port well to the Reuters corpus (0.773 vs. 0.834 F1 -score). For the Wikipedia -based corpus, the basic TempEval mo d-el actually performs a little better than on the source domain (0.859 vs. 0.834 F1 -score).

Four bootstrapping strategies were proposed and evaluated. Table 1 shows the maximum F1 score achieved by each of these strategies, along with the number of generated synonyms (between one and ten) at which this maximum was achieved. None of the bootstrapped models outperformed the basic Temp Eval model on the TempEval 2010 evalu a-tion data, and the WordNet 1 st Sense strategy and the WordNet Pseudo -Lesk strategy never outpe r-formed the ba sic TempEval model on any corpus.
However, for the Reuters and Wikipedia co r-pora, the LWLM and LWLM+Wor dNet bootstra p-ping strategies outperformed the basic TempEval model. The LWLM strategy gives a large boost to model performance on the Reuters corpus from 0.773 up to 0.826 (a 23.3% error reduction) when using the first 5 synonyms. This puts performance on Reuters near performance on the Temp Eval domain from which the model was trained (0.834). This suggests that the (Reuters -trained) LWLM is finding exactly the right kinds of syn o nyms: those that were not originally present in the Te m pEval data but are pr esent in the Reuters test data. On the Wikipedia corpus, the LWLM bootstrapping stra t-egy results in a moderate boost, from 0.859 up to 0.874 (a 10.6% error reduction) when using the first three syn o nyms. Figure 1 shows that using more synonyms with this strategy drops perfor m-ance on the Wikipedia co r pus back down to the level of the basic TempEval model.

The LWLM+WordNet strategy gives a mo d erate boost on the Reuters corpus from 0.773 up to 0.796 (a 10.1% error reduction) when four sy n onyms are used. Figure 2 shows that using six or more syn o-nyms dr ops this performance back to just above the basic TempEval model. On the W i kipedia corpus, the LWLM+WordNet strategy results in a mode r-ate boost, from 0.859 up to 0.877 (a 12.8% error r e duction), with five synonyms. Using additional synonyms results in a s mall decline in perfor m-ance, though even with ten synonyms, the pe r-formance is better than the basic TempEval model.
In general, the LWLM strategy gives the best performance, while the LWLM+WordNet strategy is less sensitive to the exact number of syn o nyms used when expanding the training data.
We were curious why synonym -based boo t-strapping did not improve performance on the source -domain TempEval 2010 data. An error analysis suggested that some time expressions might have been left unannotated by the human annotators. Two of the a u thors re -annotated the TempEval evaluation data, finding inter -annotator agre e ment of 0.912 F1 -score with each other, but only 0.868 and 0.887 F1 -score with the Temp Eval annotators, primarily due to unann otated time e x-pressions such as 23 -year , a few days and third -quarter .

Using this re -annotated TempEval 2010 data 2 , we re -evaluated the proposed bootstrapping tec h-niques. Figure 3 and Figure 4 compare per for m-ance on the original TempEval data to performance on the re -annotated version. We now see the same trends for the TempEval data as were observed for the Reuters and Wikipedia corpora: using a small number of synonyms from the LWLM to generate new train ing examples leads to performance gains. The LWLM bootstrapping model using the first synonym achieves 0.861 F1 score, a 22.8% error reduction over the baseline of 0.820 F1 score. We have presented model -portability experiments on time expression recognition with a number of bootstrapping strategies. These bootstrapping stra t-egies generate additional training examples by substituting temporal expression words with pote n-tial syn o nyms from two sources: WordNet and the Latent Word La n guage Model (LWLM).

Bootstra p ping with LWLM synonyms provides a large boost for Reuters data and TempEval data and a decent boost for Wikipedia data when the top few synonyms ar e used. Additional synonyms do not help, probably because they are too newswire -specific: both the contexts from the TempEval training data and the syn o nyms from the Reuters -trained LWLM come from newswire text, so the lower synonyms are probably more doma in -specific.
 Intersecting the synonyms generated by the LWLM and by WordNet moderates the LWLM, making the boo t strapping strategy less sensitive to the exact number of syn o nyms used. However, while the inte r sected model performs as well as the LWLM model o n Wikipedia, the gains over the non -bootstrapped model on Reuters and Te m p Eval data are smaller.

Overall, our results show that when porting time expression recognition models to other domains, a pe r formance drop can be avoided by synonym -based bootstra p p ing. Future work will focus on using synonym -based expansion in the contexts (not just the time expressions headwords), and on incorporating co n textual information and syntactic transform a tions.
 This work has been funded by the Flemish go v-e rnment as a part of the project AMASS++ (A d-vanced Multimedia Alignment and Structured Summarization) (Grant: IWT -SBO -060051).

