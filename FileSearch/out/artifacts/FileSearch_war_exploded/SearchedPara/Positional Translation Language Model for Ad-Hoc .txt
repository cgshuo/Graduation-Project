 Language modeling (LM) for Information Retrieval (IR) has been a promising area of [12][20]. However, language models suffer from one problem: term independence assumption which is common for all retrieval models. based on the use of the proximity features. These features capture the degree to which term position and term proximity under language model framework, Lv and Zhai [10] proposed a positional language model (PLM). In PLM, a language model for each term position in a document is defined, and document is scored based on the scores of its PLMs. 
The second one considers the use of semantic relationships between words. In or-der to reduce the semantic gap between documents and queries, statistical translation models (TLM) have been proposed for information retrieval to capture semantic word models can avoid exact matching of words between documents and queries. 
The previous studies have proven that term proximity features and semantic rela-tionships between words are both useful information to improve retrieval performance translation language model to explicitly incorporate these two types of information in a united way. In the first step, we present a proximity-based method to estimate word-word translation probabilities. Then, we define a translation document model for each position of a document and use these document models to score the document. 
The main contribution of this paper is as follows  X  First, we propose a new proxim-ity-based method, in which the proximity of co-occurrences is taking into account, to estimate word-word translation probabilitie s. Second, we propose a positional transla-tion language model (PTLM) to explicitly incorporate term proximity features and semantic relationships between words in a unified way. Finally, extensive experi-ments on standard TREC collections have been conducted to evaluate the proposed model. Experimental results on standard TREC collections show that PTLM achieves guage model, and translation language models. 2.1 Basic Language Modelling Approach The basic idea of language models is to vi ew each document to have its own language mentations were proposed [20]. The general ranking formula is defined as follows: where means equivalence for the purpose of ranking documents,  X  (  X , X  ) is the count of word w in query  X  , and  X  is the vocabulary set. The challenging part is to maximum likelihood estimator. However, this method is suffering from the data sparseness problem. To address this problem, some effective smoothing approaches, which combine the document model with the background collection model, have been proposed. One commonly used method is Dirichlet Prior smoothing methods [18], which is defined as follows: 2.2 Positional Language Model framework, Lv and Zhai [10] proposed a positional language model (PLM). In PLM guage model of this virtual document can be estimated as follow scores. Different strategies were used: Best Position Strategy, Multi-Position Strategy, Multi- X  Strategy. 2.3 Statistical Translation Language Model To incorporate the semantic relationship between terms under language model framework, Berger and Lafferty proposed translation language modelling approach to document model X : lated into its semantically related words with non-zero probability, which allows us to score a document by counting the matches between a query word and semantically related words in document. The key part for translation language model is estimating translation probabilities. generating synthetic query. This method is inefficient and does not have good cover-age of query words. In order to overcome these limitations, Karimzadehgan and Zhai [7] proposed an effective estimation method based on mutual information. Recently, language model should satisfy, and proposed a new estimation method which is shown to be able to better satisfy the constraints. This new estimation method, namely conditional context analysis, is described in formula 5. In this section, we will describe the PTLM in detail. In the first part, a proximity-based document. Finally, these positional document models are used to score the document. 3.1 Estimating Translation Probability In the conditional context analysis method pr oposed in [8], the probability of translat-ing word  X  into word  X  can be estimated as follows: the vocabulary. 
In this method, any co-occurrence within the document is treated in the same way, ment may cover several different topics and thus contain much irrelevant information. relevant. Therefore, we introduce a new concept, namely proximity-based word co-occurrence frequency ( pcf ) to model the proximity feature of co-occurrences. cases. The Gaussian-based pcf can be calculated as follows:  X  in document  X  . 
In this paper, three commonly used distance measures are adopted to explain how to calculate distance score in the three distance measures. 
Minimum pair distance: It is defined as the minimum distance between any oc-calculated from the position vectors. 
Average pair distance: It is defined as the average distance between w and u for all position combinations in  X  . In the example, the distances from the first occurrence of  X  (in position 1) to all occurrences of  X  are: {1 and 5}. This is computed for the next occurrence of  X  (in position 5) and so on. ) X , X , X ( X  X  X  X  for the example is (((2-1) + (6-1)) + ((5-2) + (6-5)) + ((9-2) + (9-6)))/(2  X  3) = 20/6 = 3.33. 
Average minimum pair distance: It is defined as the average of the shortest dis-tance between each occurrence of the least frequently occurring word and any occur-) X , X , X ( X  X  X  X  = ((2 X 1)+(6 X 5))/2 =1. 
Then, the probability of translating word u into word w can be estimated as follows: where  X  is a smoothing parameter in order to account for unseen words in the context of  X  . Here  X  is set equals to the smallest of all  X  X  X  values in collection. bilities as follows: when  X  = 1, the query likelihood model are gained. 3.2 Estimating Translation Document Model The state-of-art translation language models use an entire document as a unit to esti-document may cover several different topics. Intuitionally, the words referring to the same topic may occur close to each other. Positional language model has been proven under language model framework [10]. In this section, we will introduce a positional translation language model to naturally incorporate two types of information, includ-ing term proximity features and semantic relationships between terms, under language model framework in a united way. 
The key idea of our method is to extend the translation language model from doc-ument level to positional level via the positional language model. The proposed model can capture the topic of the document at the position by giving more weight on words model at each position can be estimated based on all the propagated counts of all the discounted counts. Previous studies have shown that translation language model works better with Dirichlet prior smoothing [7][8]. Therefore, in the rest of the paper, we further focus guage model for position  X  in document  X  can be defined as follows: estimated by formula 8 and 9;  X  (  X  |  X , X  ) is the positional document model at position i of document D , and can be estimated as follows: kernel function: 3.3 Ranking Document In the section 3.2, we have obtained a translation language model for each position in a document. Intuitively, we can imagine that the PTLMs give us multiple representa-[19] to score each PTLM as follows: ument D . In this paper, we compute the final score of document D using the best posi-and can be defined as follows: 4.1 Data Set We used six standard TREC data sets in our study. They represent different sizes and Each document is processed in a standard way for indexing. Words are stemmed (using porter-stemmer), and stop words are removed. In the experiments, we only use title of the queries because semantic word matching is necessary for such short queries. 
In each experiment, we use the KL-divergence model using Dirichlet prior smooth-then use the PTLM to re-rank them. The top-ranked 1000 documents are used for comparison with other models. In order to evaluate our model and compare it to other models we use the MAP measure, which is widely accepted measure for evaluating effectiveness of ranked retrieval systems. The methods used for the experiments are:  X 
QL: baseline, query likelihood model with Dirichlet prior smoothing [18].  X 
KL: baseline, KL-divergence model with Dirichlet prior smoothing [19].  X 
TM-MI: translation language model with mutual information [7].  X 
TM-CCON: translation language model with conditional context analysis [8].  X 
PLM: positional language model with the best position strategy [10].  X  PTLM-1: PTLM with minimum pair distance.  X 
PTLM-2: PTLM with average pair distance.  X 
PTLM-3: PTLM with average minimum pair distance. 4.2 Comparing with Existing Retrieval Models As we can see from all the PTLM models used in our experiments, there are several PTLMs and PLM by a 5-fold cross-validatio n on each collection. For the two base-for each collection. The results of TM-MI, TM-CCON are directly from [8]. 
Table 2 shows the results for these models with Dirichlet prior smoothing. Com-paring the rows in the table indicates that the PTLM models achieve significant im-provements over the state-of-the-art models, including positional language model and two types of information can be used in combination to improve retrieval perfor-mance. Comparing the three variants of PT LM, PTLM-3 is more effective and robust than PTLM-2 and PTLM-1. It also indicates that average-minimum-pair distance measure can capture the proximity feature of co-occurrences better than the other two the differences between the PTLM models and the start-of-art models are statistically significant. 4.3 Parameter Sensitivity Study An important issue that may affect the robustness of the PTLM models is the sensitiv- X  measure. the value of  X  affects the performance of all PTLM models extensively. The experi-values for these PTLM models are not the same. For example, on the TREC7 collec-tion, optimal  X  value for PTLM-1 is 80, and the corresponding value for PTLM-2 is 150. Thus, the optimal values of  X  depend on the proximity measures and the collec-tions. Figure 1 plots the evaluation metrics MAP obtained by the three PTLM models with  X  values ranging from 10 to 1000 on TREC7. 
The experimental results also show that the influence of  X  is collection-based. For one collection, the best  X  values for all PTLM models are the same. Specifically, the best  X  values are 0.7, 0.8, and 0.5 for TREC7, DOE, and WSJ, respectively. 
In order to see how the propagation scope parameter  X  (in Equation 12) affects the ments of 25. Overall, we see that a relatively large often brings the best performance. It also seems that the performance of the PTLM models stabilizes after  X  reach 175. performance of the PTLM models, we also change the settings of the smoothing pa-rameters for them. The results indicate that the optimal smoothing parameters are the same (equals to 500) for all the three PTLM models on all collections. models are based on the term independence hypothesis. Given common knowledge about language, such an assumption might seem unrealistic. To go beyond the term independency assumption in information retrieval, two main directions were investigated. 
The first one considers the use of the proximity features that capture the degree to which search terms appear close to each other in a document. For example, it looks at the minimum span of the query terms appearing in the document. Term proximity, as these papers, various methods have been proposed to integrate proximity information into different retrieval models. Keen [9] firstly attempted to import term proximity in proposed an integration of term proximity scoring into Okapi BM25 and obtain im-provements on several collections. Tao et al. [14] systematically studied five proximi-ty measures and compared their performance in various retrieval models. Zhao et al. guage model under the language modelling framework. Lv and Zhai [10] integrated guage model for each position within a document. Zhao et al. [17] introduce a pseudo Miao et al. [11] has attempted to incorporate proximity information into the Rocchio X  X  model. 
The second one considers the use of semantic relationships between words. Under this way, relevant words are used to enrich document or query representation. Many studies have tried to bridge the vocabulary gap between documents and queries both based on term co-occurrences [1, 6, 13] and hand-crafted thesaurus [15]. Some other works have considered to combine both appro aches [4]. Berger and Lafferty [2] firstly proposed a translation language model to corporate semantic relationship between words under the language modeling framework. To train translation models, they used translation model is based on document titles [5]. Recent works have relied on docu-ment-based word co-occurrences to es timate the translation model [7][8]. Term proximity features and semantic relationships between words have proven to be unified way. In the first step, a new proximity-based method is presented to estimate the translation model. Three proximity measures are then adopted for calculating the distance score of two words within a document. The corresponding models based on these measures, PTLM-1, PTLM-2 and PTLM-3, are evaluated on six standard TREC collections. Our experiment results indicate that the PTLM models are more effective guage models. Comparing the three variants of PTLM, PTLM-3 is more effective than the other two. PTLM to re-rank the top 2000 documents from initial search results. However, such a strategy does not fully take advantage of the capacity of PTLM to potentially retrieve retrieval performance. Acknowledgments. This work was partially supported by the National Science Foundation of China under grants No. 60803160 and No. 61300144, the Key Projects was partially supported by NSF of Hubei Prov. under grant number 2013CFB334, and the State Key Lab of Software Engineering Open Foundation of Wuhan University under grant number SKLSE2012-09-07. 
