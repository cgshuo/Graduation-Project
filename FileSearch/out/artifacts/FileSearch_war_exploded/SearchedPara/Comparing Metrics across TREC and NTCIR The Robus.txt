 Test collections are growing larger, and relevance data con-structed through pooling are suspected of becoming more and more incomplete and biased. Several studies have used evaluation metrics specifically designed to handle this prob-lem, but most of them have only examined the metrics under incomplete but unbiased conditions, using random samples of the original relevance data. This paper examines nine metrics in a more realistic setting, by reducing the number of pooled systems. Even though previous work has shown that metrics based on a condensed list, obtained by remov-ing all unjudged documents from the original ranked list, are effective for handling very incomplete but unbiased rel-evance data, we show that these results do not hold in the presence of system bias. In our experiments using TREC and NTCIR data, we first show that condensed-list metrics overestimate new systems while traditional metrics underes-timate them, and that the overestimation tends to be larger than the underestimation. We then show that, when rel-evance data is heavily biased towards a single team or a few teams, the condensed-list versions of Average Precision (AP), Q-measure (Q) and normalised Discounted Cumula-tive Gain (nDCG), which we call AP ,Q and nDCG ,are not necessarily superior to th e original metrics in terms of discriminative power, i.e., the overall ability to detect pair-wise statistical significance. Nevertheless, even under sys-tem bias, AP and Q are generally more discriminative than bpref and the condensed-list version of Rank-Biased Preci-sion (RBP), which we call RBP .
 Copyright 2008 ACM 978-1-59593-991-3/08/10 ... $ 5.00.
Test collections are growing larger, and relevance data constructed through pooling are suspected of becoming more and more incomplete and biased [6, 9]. Relevance data are incomplete if there exist some relevant documents among the unjudged documents in the test collection. Furthermore, in-complete relevance data are biased if they represent some limited aspects of the complete set of relevant documents. For example, if the number of pooled systems is small, the resultant test collection may overestimate these systems and underestimate systems that did not contribute to the pool, since these new systems are likely to retrieve relevant docu-ments that are outside the set of known relevant documents. We will refer to this phenomenon as system bias .Biasmay also be caused by shallow pools : If only documents at the very top of submitted ranked lists are judged, the resultant relevance data may contain relevant documents that are very easy to retrieve, but not those that are difficult to retrieve. For example, Buckley et al. [7] report that the TREC 2005 HARD/Robust test collection is biased towards documents that contain topic title words due to shallow pools. We will refer to this phenomenon as pool depth bias .

The objective of this paper is to examine the robustness of information retrieval (IR) effectiveness metrics in the pres-ence of system bias, with an emphasis on metrics that can handle graded relevance. Several researchers have proposed evaluation metrics specifically for handling the incomplete-ness of relevance data, but most of them have only exam-ined the metrics under incomplete but unbiased conditions, using random samples of the original relevance data [1, 4, 6, 20, 22, 30]. While random sampling may mimic a situation where the number of judged documents is extremely small compared to the entire document collection, it does not ad-dress the problems due to system bias and pool depth bias. Therefore, this paper examines metrics in a more realistic setting, by reducing the number of pooled systems. A simi-lar study that examined the effect of pool depth bias on IR metrics has recently been reported elsewhere [23].
The main contributions of this paper are as follows. First, we examine as many as nine metrics for handling system bias in test collections. The metrics examined are: Average Precision (AP) [15], Q-measure (Q) [18], normalised Dis-counted Cumulative Gain (nDCG) [12], Rank-Biased Pre-cision (RBP) [14], binary preference (bpref) [6], AP ,Q , nDCG and RBP . The latter four metrics are AP, Q, nDCG andRBPappliedtoa condensed list [20, 22], obtained by removing all unjudged documents from the original ranked list. Thus, just like bpref, these four metrics assume that retrieved unjudged documents are nonexistent , while tra-ditional metrics assume that the unjudged documents are nonrelevant . Even though previous work [20, 22] have shown that condensed-list metrics are effective for handling very in-complete but unbiased relevance data, we show that they are not necessarily superior to traditional metrics in the pres-ence of system bias. This discrepancy suggests that the results reported in previous studies that used random sam-pling should be interpreted with caution. Second, our exten-sive experiments cover two independent evaluation efforts, TREC and NTCIR [13], and utilise their graded relevance data. This is in contrast to most existing studies that are limited to TREC data and binary-relevance metrics [1, 6, 30]. Since our results are consistent across all of our data sets, we believe that our findings are general. Our main findings are: 1. Condensed-list metrics overestimate systems that did 2. When runs from a single team or a few teams is used 3. Nevertheless, even in the presence of system bias, AP The first observation above substantially generalises a find-ing by B  X  uttcher et al. [9], who analysed a TREC Terabyte data set and observed that  X  X here AP underestimates the performance of a [new] syste m, bpref overestimates it. X 
Section 2 discusses previous work, and Section 3 formally defines the nine metrics considered in this study. Section 4 describes the graded-relevance data and runs from TREC and NTCIR which we use for comparing the metrics. Sec-tion 5 reports on our leave-one-team-out experiments for examining how our metrics handle new systems. Section 6 reports on our take-just-one-team and take-just-three-teams experiments for examining the robustness of our metrics to heavy system bias. Finally, Section 7 concludes this paper.
A decade ago, Zobel [31] examined the effect of pool depth and that of leaving out one run for forming the TREC relevance data. As TREC test collections at that time, i.e., TRECs 3-5, were based on binary relevance, he used binary-relevance metrics such as 11-point average precision. Subsequently, TREC adopted his leave-one-out methodol-ogy for validating their test collections, but chose to leave out one participating team at a time since each team usu-ally contributes multiple runs to a pool [7, 26]. The present study also includes leave-one-team -out experiments as well as  X  X ake-just-one-team X  experiments which relies on runs from a single team to form the relevance data. Sander-son and Joho [24] have examined a  X  X ake-just-one-run  X  X p-proach, but they considered AP only, using data from TRECs 5-8. The present study compares nine metrics, and our anal-ysis covers recent TREC and NTCIR data.

Aslam and Yilmaz [2], B  X  uttcher et al. [9] and Carterette [10] have proposed methods for  X  X xpanding X  the original incom-plete relevance data. None of these studies used graded rel-evance. In contrast, the focus of the present study is on the choice of metrics given a set of relevance assessments which may or may not be incomplete and biased.

Most existing studies that compared metrics for evalua-tion with incomplete data used the aforementioned random sampling [1, 4, 6, 20, 22, 30]. For example, Yilmaz and Aslam [30] used this approach to evaluate their proposed metrics, including Induced AP which is exactly what we call AP ,and Inferred AP which aims to estimate the true value of AP. An exception is the aforementioned work by B  X  uttcher et al. [9] which included leave-one-team-out experiments to address the system bias issue. Their experiments covered a condensed-list version of precision at document cut-off 20 and RankEff [1]. However, precision is an unreliable met-ric [5, 18, 29], and RankEff is in fact as unreliable as bpref by definition, as we shall clarify in Section 3.3.
We do not consider the aforementioned Inferred AP in our present study, because (1) While the goal of Yilmaz and Aslam [30] was to estimate the true AP values, ours is not: We prefer to explore different metrics, especially those that can handle graded relevance; (2) Inferred AP requires knowledge of the pooled but unjudged documents, which may limit its applicability; (3) According to Bompada et al. [4], Inferred AP is not as robust as the original nDCG for eval-uation with incomplete relevance data.

Among the studies that used random sampling, Sakai [20] compared condensed-list metrics such as AP ,Q ,nDCG and bpref along with traditional metrics, using data sets from NTCIR. Sakai and Kando [22] repeated the experi-ments using graded-relevance data from TREC and NTCIR, and added RBP to their candidate metrics; they did not examine RBP . The study showed that, under very incom-plete but unbiased conditions, AP ,Q ,nDCG are superior to AP, Q, nDCG, bpref and RBP. In contrast, the present study shows that AP ,Q ,nDCG are not necessarily supe-rior to AP, Q, nDCG in the presence of system bias.
Traditional metrics assume that retrieved unjudged docu-ments are nonrelevant, while condensed-list metrics assume that retrieved unjudged documents are nonexistent. As a third approach, Baillie, Azzopardi and Ruthven [3] have proposed to quantify the uncertainty in system comparisons by reporting the proportion of unjudged documents within ranked lists, along with metric values such as AP.
This section formally defines the nine metrics examined in this paper. Let R denote the number of judged relevant documents. For any given ranked list of documents, let isrel ( r )be1if the document at rank r is relevant and 0 otherwise. Let count ( r )= given by P ( r )= count ( r ) /r . Hence AP is defined as:
Let L denote a relevance level, and let gain ( L )denote the gain value for retrieving a judged L -relevant document. Without loss of generality, we follow the NTCIR tradition and let L X  X  S, A, B } [13]. As for the TREC graded rel-evance data, we treat  X  X ighly relevant X  documents as S-relevant and  X  X elevant X  documents as B-relevant. We chose not to treat the latter as A-relevant as it is known that bi-nary relevance data created at TREC contain a considerable amount of partially or marginally relevant documents [16, 25]. Moreover, we let gain ( S )=3, gain ( A )=2and gain ( B ) = 1 hereafter as Q and nDCG are robust to the choice of gain values [18].

Let g ( r )= gain ( L ) if the document at rank r is L -relevant and g ( r ) = 0 otherwise, i.e., if the document at rank r is either judged nonrelevant or unjudged. The cumulative gain at rank r is given by cg ( r )= ideal ranked list of documents, which satisfies g ( r ) &gt; 0for 1  X  r  X  R and g ( r )  X  g ( r  X  1) for r&gt; 1. For NTCIR, listing up all S-, A-and B-relevant documents in this order produces an ideal ranked output. Let cg I ( r )denotethe cumulative gain of the ideal list. Q is defined as: where  X  is a parameter for reflecting the persistence of the user [19]. Clearly,  X  =0reducesQtoAP;welet  X  =1 throughout this paper.
 For a given logarithm base a ,letthe discounted gain at for r  X  a . Similarly, let dg I ( r ) denote the discounted gain for an ideal ranked list. nDCG at document cut-off l is defined as: Throughout this paper, we let l = 1000 as it is known that small document cut-offs hurt the stability of nDCG [18]. This original definition of nDCG is  X  X uggy X  in that a rele-vantdocumentretrievedatrank1andoneretrievedatrank a receive the same credit. We adhere to the original nDCG but let a = 2 to alleviate the effect of the bug 1 . Let H denote the highest relevance level across all topics. In all of our experiments, H = S .Let p be the persistence parameter that represents the fixed probability that the user moves from a document at rank r to rank ( r +1). RBP is defined as: Moffat and Zobel [14] explored p =0 . 5 , 0 . 8 , 0 . 95, and Sakai and Kando [22] showed that p =0 . 95 is the best choice among these three values in terms of system ranking stability and discriminative power. Hence we use p =0 . 95 thoughout this paper. RBP is different from the other metrics consid-ered in this paper in that it totally disregards recall. Sakai and Kando [22] have pointed out some weaknesses of this metric, including the fact that it does not average well and that it has low discriminative power.
The Microsoft version of nDCG uses dg ( r )= g ( r ) / log 1) for all r [8], but this cancels out a , depriving nDCG of a persistence parameter.
Sakai [20] showed that a family of metrics, which are ex-isting metrics applied to a condensed list of documents ob-tained by removing all unjudged documents from the origi-nal list, are simpler and better solutions than bpref. Bpref itself can be expressed as a metric based on a condensed list. Let r denote the rank of a judged document in a condensed list, whose rank in the original list was r (  X  r ). Let N de-note the number of judged nonrelevant documents. For any topic such that R  X  N , bpref reduces to bpref R : In fact, R  X  N holds for every topic used in our experiments, and therefore bprefisalwaysbpref R . Whereas, for any topic such that R  X  N , bpref reduces to bpref N : Note that bpref N does not require a minimum operator [20].
The only essential difference between bpref and AP ap-plied to a condensed list, which we call AP , is that bpref lacks the top-heaviness property of AP [20, 22]. Note that, from Eq. 1, AP can be expressed as: Thus, for each retrieved relevant document, AP uses r while bpref uses a very large constant (either R or N )for scaling r  X  count ( r ) i.e., the number of judged nonrelevant documents ranked above the relevant one at rank r . Scaling by a large constant is not good: For example, consider a con-densed list that has a judged nonrelevant document at rank 1 and a relevant one at rank 2. For this relevant document, the  X  X isplacement penalty X  is r  X  count ( r )=2  X  1=1andand P ( r )=1 / 2. Thus, the existence of the judged nonrelevant document at rank 1 weighs heavily. In contrast, this nonrel-evant document has very little impact on bpref, because the same misplacement penalty is divided by R ,or N which is generally even larger than R . (See Table 1 in Section 4.) In addition to discussing these inherent properties of AP and bpref, Sakai [20] demonstrated experimentally that AP is in fact superior to bpref in terms of system ranking stabil-ity and discriminative power, given incomplete but unbiased relevance data.

Condensed-list versions of Q, nDCG and RBP will be de-noted by Q ,nDCG and RBP . Thus this paper considers four metrics (AP, Q, nDCG and RBP) plus five condensed-list metrics (AP ,Q ,nDCG , RBP and bpref). Among these, AP, AP and bpref cannot handle graded relevance.
Ahlgren and Gr  X  onqvist [1] have claimed that, when the relevance data is reduced at random, a binary-relevance met-ric called RankEff provides more stable system ranking than AP and bpref-10 [6], which is a version of bpref. Let d be a judged relevant document, and let I ( d )denotethenum-ber of judged nonrelevant documents ranked lower than d .  X  RankEff is defined as: However, let us rewrite RankEff using a condensed list. Re-call that the number of judged nonrelevant documents ranked above a relevant one at rank r is given by r  X  count ( r ). Hence the number of judged nonrelevant documents ranked below r , including those not retrieved at all, is given by N  X  ( r  X  count ( r )). Whereas, for each relevant document not retrieved, I ( d ) = 0 by definition. That is, the summa-tion in Eq. 9 is essentially over retrieved relevant documents rather than all judged relevant documents. Hence, It is clear that RankEff is none other than bpref N (Eq. 7) which has been known as  X  X pref allnonrel X  in trec eval, the evaluation software distributed by TREC.

As discussed earlier, both theory and experiments have shown that bpref N (RankEff) is not a desirable metric, in that it is very insensitive to change in the top ranked doc-uments [20]. Ahlgren and Gr  X  onqvist [1] themselves report that the metric correlates poorly with AP: In their study, a system ranked at number 42 by AP was ranked at number 7byRankEff.
Table 1 provides some statistics of the TREC and NTCIR data we used for evaluating the nine metrics in the presence of system bias. The  X  X REC03 X  and  X  X REC04 X  data are from the TREC 2003 and 2004 robust track [27, 28], and the  X  X TCIR-6J X  (Japanese) and  X  X TCIR-6C X  (Chinese) data are from the NTCIR-6 CLIR task [13]. The NTCIR-6J and NTCIR-6C data contain a few teams that did not con-tribute any monolingual runs, which we have excluded from our analysis. Hence we considered only ten teams for both NTCIR-6J and NTCIR-6C.

Consider a particular topic. Let t denote a participating team, and let D t denote the set of documents contributed to the pool by this team. For TREC03, for example, D t is the union of the top 125 documents of each run submit-ted by t .Thesetof unique contributions by t is defined as U t = D t  X  X  X  t = t D t 2 . Similarly, let D rel t ( note the set of judged relevant documents obtained from t .Thesetof unique relevant documents from t is defined as U teams that we used, along with some statistics on U rel t example, Table 2(c) shows that a team called NICT con-tributed as many as 229.7 unique relevant documents per topic on average, and this was achieved by submitting 20 runs (including cross-lingual runs). For one topic, this team contributed 721 unique relevant documents.

Let J denote the complete set of judged documents for a topic. Section 5 reports on our leave-one-team-out exper-iments which replace J with J  X  U t for each t .Thatis, unique contributions from t are removed from the original
For the NTCIR data, { t } includes the teams that did not contribute any monolingual runs. Whereas, t represents a team that contributed at least one monolingual run. relevance data, so that t can be treated as a  X  X ew X  team. In Section 6, we go to the other extreme and replace J with D . That is, runs from a single team is used for forming the relevance data. In these  X  X ake-just-one-team X  experiments, the teams labelled with a  X   X   X  X nTable2failedtocontribute a relevant document (i.e., D rel t =  X  ) for at least one topic, and were therefore excluded from our analysis. In addition, we chose three teams from each data set to conduct  X  X ake-just-three-teams X  experiments, by replacing J with  X  t  X  T where T is the set of chosen teams. As indicated by  X   X   X  X  X  in Table 2, we chose three  X  X rdinary X  teams: Ones with the smallest number of unique relevant documents.

To examine the effect of system bias in terms of system ranking stability (as measured by Kendall X  X  rank correla-tion [17, 26] between two rankings based on two different relevance data sets) and discriminative power, we randomly selected one monolingual run per team for each data set. For example, for the NTCIR-6J data, we randomly selected ten monolingual runs, each representing a team.
As explained earlier, we formed leave-one-team-out rele-vance data J  X  U t for each team t to compare the robustness of our metrics to runs that did not contribute to the pool. Then, for each t , we randomly selected one monolingual run from t and evaluated this run using J  X  U t .Recallthat all runs submitted by t have been left out to form J  X  U t .
Table 3 shows, for each t from NTCIR-6J, how a selected monolingual run from t is affected when the original rele-vance data J is replaced by J  X  U t . For example, when a run from  X  X RKLY X  is evaluated using nDCG with this team X  s leave-one-team-out relevance data, the run X  X  abso-lute score goes down by .0016, and its rank among the 10 selected runs goes down from rank 6 to rank 8. In contrast, arunfrom X  X UM X  X oesupfromrank6torank5accord-ingtonDCG and this team X  X  leave-one-team-out relevance data. It can be observed that, according to condensed-list metrics, i.e., AP ,Q ,nDCG ,RBP and bpref, the scores and the ranks tend to go up with the use of each leave-one-team-out relevance data, while, according to traditional metrics, i.e., AP, Q, nDCG and RBP, the scores and the ranks tend to go down. Moreover, the  X  X verage absolute performance change X  row of Table 3 shows that the average score changes are higher for the condensed-list metrics than for the traditional ones. For example, for AP, this is com-puted as ( . 0021 + . 0002 + . 0005 + . 0000 + . 0013 + . 0050 + . 0011 + . 0000 + . 0003 + . 0003) / 10 = . 0011. Whereas, the av-erage for AP is . 0062. The trends are similar for TREC03, TREC04 and NTCIR-6C, but the tables are omitted here due to lack of space. Hence, our first observation is that condensed-list metrics overestimate new systems while tra-ditional metrics underestimate them, and that the overesti-mation tends to be larger than the underestimation .Anew run contains many unjudged documents. Therefore, con-densing its ranked list may move up the ranks of retrieved relevant documents dramatically. This is why condensed-list metrics, including bpref, overestimate new systems.
Our main criterion for comparing metrics is Sakai X  X  dis-criminative power [17, 21]. Let C be the set of all pairs of runs that are being considered. For a given significance level  X  ,let C  X  (  X  C ) be the set of pairs of runs with a statistically significant performance difference in terms of a given met-ric according to a two-sided, paired bootstrap hypothesis test . Then discriminative power is defined as C  X  /C : It means how often a metric manages to detect a statistically significant difference for a fixed probability of Type I Error. Although C /C can also be defined using a significance test other than the bootstrap test, one of the advantages of Sakai X  X  method is that it can also estimate the minimum performance dif-ference required to achieve statistical significance.
Suppose that C  X  was obtained using a given metric and the original relevance data. Now, let C  X  denote the set of pairs of runs with a statistically significant difference in terms of the same metric but with a different relevance data set. Assuming that the results based on the original rele-vance data are the ground truth, we can quantify the dis-crepancy between C  X  and C  X  by reporting the number of misses | C  X   X  C  X  | and that of false alarms | C  X   X  C  X  pada et al. [4] have also examined misses and false alarms for comparing bpref, nDCG and Inferred AP, but they used random sampling and did not consider system bias.
Table 4 summarises the results of our discriminative power experiments using  X  =0 . 05 with the leave-one-team-out rel-evance data. For example, Table 4(a) shows that Q man-ages to detect a statistically significant difference for 80 run pairs out of 120 (66.7%) using the original relevance data, and this is the highest discriminative power achieved across all metrics for TREC03, as indicated in bold. Moreover, given the 50 topics of TREC03, the performance difference required to achieve a significance level of  X  =0 . 05 is around 0.07 in Q. On the other hand, Table 4(a) also shows the corresponding results averaged over the 16 leave-one-team-out relevance data. For example, it can be observed that, by replacing the original relevance data of TREC03 with a leave-one-team-out relevance data, the discriminative power of Q goes up from 64.2% to 64.9%, but this is due to false alarms, which occur 0.81 times on average.

The results with the original relevance data in Table 4 con-firm those by Sakai and Kando [22], in that AP, Q, nDCG and their condensed-list versions are more discriminative than bpref and RBP. In addition, they are also more dis-criminative than RBP , which has been examined for the first time. It can also be observed that our discriminative power results using leave-one-team-out relevance data are similar to the ones using the original relevance data. This is because the only difference between the two relevance data sets is U t , the unique contributions from one team. (NTCIR-6J). A  X  +  X  indicates that a run is overestimated; a  X  row represents a different leave-one-team-out relevance data set. discriminative power is indicated in bold. The leave-one-team-out experiments replaced J with J  X  U . We now discuss a more extreme case of system bias, by replacing J with D t , the contributions from a single team. As we have explained in Section 4, we also form relevance data using contributions from three teams with the smallest number of unique relevant documents.

Table 5 summarises our take-just-one-team and take-just-three-teams results for NTCIR-6J in a way similar to Ta-ble 3. Thus, for each team t , the table shows how a partic-ular monolingual run from t (which is the same as the one we used for the leave-one-team-out experiments) is affected when the original relevance data J is replaced by D t .For example, when a run from  X  X RKLY X  is evaluated using Q with this team X  X  contributions only, the run goes down from rank 4 to rank 8. In contrast, when the same run is evalu-ated using Q with this team X  X  contributions only, it goes up from rank 7 to rank 4. As we have explained in Section 4, two teams with a  X   X   X  in Table 2 are excluded here. It can be observed that, if a single team t is used for forming the relevance data, the run score for t goes up for all metrics (ex-cept for RBP and RBP ); however, while traditional metrics overestimate the rank of a run from t , condensed-list metrics understimate it. Condensed-list metrics underestimate the rank of a run from t because all the other runs from t ( = t ) are substantially overestimated : These other runs are  X  X ew X  to the take-just-one-team relevance data of t , and we have already observed in Section 5 that condensed-list metrics overestimate new runs. As for RBP and RBP ,replacing J with D t does not substantially affect the run score for t , because this merely turns some relevant documents below the pool depth within that run, i.e., those that belong to J  X  D t , into nonrelevant documents. The stability of scores for RBP and RBP reflects the fact that they totally disre-gard recall, and not necessarily that they are superior: Note that the ranks according to RBP and RBP are altered just like the other metrics. The trends are similar for TREC03, TREC04 and NTCIR-6C, but only the TREC03 results are shown in Table 6 due to lack of space.

Table 7 compares, for each data set and metric, the rank-ing of the aforementioned selected runs based on the original relevance data and that based on a take-just-one-team or a take-just-three-teams relevance data. The similarity be-tween two rankings is quantified using Kendall X  X  rank cor-relation, which would be 1 if the two rankings are identical and  X  1 if the two rankings are the exact inverse of each other. The rank correlation values for the take-just-one-team relevance data have been averaged across teams. It can be observed that the correlation values are generally very high. That is, it is possible to replace the original rele-vance data with one that is based on a single team (or three teams) and still maintain a similar system ranking. As men-tioned in Section 2, this generalises a finding by Sanderson and Joho [24] who considered only AP and binary-relevance TREC data. However, obtaining a system ranking that is similar to the full relevance data is not sufficient for sound evaluation: We later show that strong system bias can in-troduce much noise in statistical significance tests.
For the two NTCIR data sets, the take-just-one-team rank-ings with AP ,Q ,nDCG appear to be more consistent with the original rankings than those with AP, Q and nDCG. However, we refrain from making a claim based on these re-sults because the NTCIR rankings contain only eight teams (See Table 2) and the trend is not clear for the two TREC data sets.

Table 8 summarises the results of our discriminative power experiments using  X  =0 . 05 with the take-just-one-team and take-just-three-teams relevance data, in a way similar to Ta-ble 4. The  X  X riginal relevance data X  rows have been copied from Table 4. For example, Table 8(a) shows that replacing the original relevance data with a take-just-three-teams rel-evance data superficially raises the discriminative power of Q from 66.7% to 68.3%, but this is due to four false alarms with two misses. False alarms in particular are not welcome in retrieval experiments: the take-just-three-teams relevance data declared that the four run pairs are significantly differ-ent, even though they are not significantly different accord-ing to the original relevance data. Figure 1 visualises the discriminative power values of Table 8.

According to Table 8, take-just-one-team relevance data generally yield more misses and false alarms than take-three-teams relevance data. Hence we observe that, even though take-just-one-team relevance data may produce a system ranking that is very similar to that produced by the origi-nal relevance data, pooling runs from several teams is better than pooling runs from a single team for obtaining reliable conclusions based on statistical significance tests. The focus of this study, however, is on the comparison of different met-rics under the same condition, and not on how many and what kind of teams are required to obtain reliable conclu-sions.

Table 8 also shows that AP, Q and nDCG are generally more discrimi native than AP ,Q and nDCG , respectively, even with take-just-one-team or take-just-three-teams rele-vance data. For example, for TREC03, the discriminative power of Q averaged over 12 take-just-one-team relevance data is 67.6% while the corresponding value for Q is only 59.0%, even though the number of misses and that of false alarms are more or less comparable. Thus, condensed-list metrics are not necessarily superior to traditional metrics when the relevance data is heavily biased towards one team or a few teams . On the other hand, even with take-just-three-teams and take-just-one-team relevance data, it can be observed that AP and Q are generally more discrimi-native than bpref and RBP . The nDCG results are not as strong: their take-one-team discriminative power values are comparable to those of bpref for the TREC04 and the NTCIR-6C data, as shown in Table 8(b) and (d).
Several recent studies [1, 4, 6, 20, 22, 30] discussed the effect of incomplete relevance data in retrieval evaluation using random samples of the original relevance data. Hence they discussed neither system bias nor pool depth bias. How-ever, in reality, relevance data formed through pooling are never a random sample of the full relevance data.
In light of the above observation, this paper examined the effect of system bias on IR metrics. Even though previous studies [20, 22] showed that AP ,Q and nDCG are effective for handling very incomplete but unbiased data, we showed that they are not necessarily effective in the presence of sys-tem bias. Using data from both TREC and NTCIR, we first showed that condensed-list metrics overestimate new systems while traditional metrics underestimate them, and that the overestimation tends to be larger than the under-estimation. We then showed that, when relevance data is 6J). A  X  +  X  indicates that a run is overestimated; a  X   X   X  indicates that it is underestimated. Rank changes heavily biased towards a single team or a few teams, AP , Q and nDCG are not necessarily superior to AP, Q and nDCG in terms of discriminative power. Moreover, a sep-arate study has shown that AP ,Q and nDCG are not advantageous in the presence of pool depth bias either [23]. Hence previous studies that used random sampling should be interpreted with caution. Nevertheless, AP and Q are generally more discriminative than bpref and RBP in the presence of system bias or pool depth bias.

Traditional metrics assume that retrieved unjudged docu-ments are nonrelevant, while condensed-list metrics, includ-ing bpref, assume that they are nonexistent. In essence, the present study showed that the latter assumption is no bet-ter than the former when the number of pooled systems is small. In our future work, we would like to couple efficient and reliable test construction methods (e.g. [11]) with re-liable graded-relevance metrics. We also plan to establish quantitative criteria for choosing good evaluation metrics: Although we believe that discriminative power is one im-portant criterion, there are p robably other aspects that need to be examined, including the ability to predict the system ranking according to an intuitive metric such as precision-at-ten, given a set of new topics [29]. [1] Ahlgren, P. and Gr  X  onqvist, L.: Evaluation of Retrieval [2] Aslam, J. A. and Yilmaz, E.: Inferring Document [3] Baillie, M., Azzopardi, L. and Ruthven, I.: Evaluating the highest discriminative power is indicated in bold. [4] Bompada, T. et al. : On the Robustness of Relevance [5] Buckley, C. and Voorhees, E. M.: Evaluating [6] Buckley, C. and Voorhees, E. M.: Retrieval Evaluation [7] Buckley, C. et al. : Bias and the Limits of Pooling for [8] Burges, C. et al. : Learning to Rank using Gradient [9] B  X  uttcher et al. : Reliable Information Retrieval [10] Carterette, B.: Robust Test Collections for Retrieval [11] Carterette, B. et al. : Evaluation Over Thousands of [12] J  X  arvelin, K. and Kek  X  al  X  ainen, J.: Cumulated [13] Kando, N.: Overview of the Sixth NTCIR Workshop, [14] Moffat, A. and Zobel, J.: Rank-Biased Precision for [15] Robertson, S.: A New Interpretation of Average [16] Sakai, T. and Sparck Jones, K.: Generic Summaries [17] Sakai, T.: Evaluating Evaluation Metrics based on the [18] Sakai, T.: On the Reliability of Information Retrieval [19] Sakai, T.: On Penalising Late Arrival of Relevant [20] Sakai, T.: Alternatives to Bpref, ACM SIGIR 2007 [21] Sakai, T.: Evaluating Information Retrieval Metrics [22] Sakai, T. and Kando, N.: On Information Retrieval [23] Sakai, T.: Comparing Metrics across TREC and [24] Sanderson, M. and Joho, H.: Forming Test Collections [25] Sormunen, E.: Liberal Relevance Criteria of TREC -[26] Voorhees, E. M.: The Philosophy of Information [27] Voorhees, E. M.: Overview of the TREC 2003 Robust [28] Voorhees, E. M.: Overview of the TREC 2004 Robust [29] Webber, W., Moffat, A., Zobel, J. and Sakai, T.: [30] Yilmaz, E. and Aslam, J. A.: Estimating Average [31] Zobel, J.: How Reliable are the Results of Large-Scale

Test collections are growing larger, and relevance data constructed through pooling are suspected of becoming more and more incomplete and biased [1]. Relevance data are incomplete if there exist some relevant documents among the unjudged documents in the test collection. Furthermore, incomplete relevance data are biased if they represent some limited aspects of the complete set of relevant documents. For example, if the number of pooled systems is small, the resultant test collection may overestimate these systems and underestimate systems that did not contribute to the pool. We will refer to this phenomenon as system bias .Biasmay also be caused by shallow pools : If only documents at the very top of submitted ranked lists are judged, the resultant relevance data may contain relevant documents that are very easy to retrieve, but not those that are difficult to retrieve. We will refer to this phenomenon as pool depth bias .
The objective of this paper is to examine the robustness of retrieval effectiveness metrics in the presence of pool depth bias, with an emphasis on those that can handle graded rel-evance. Several researchers have proposed evaluation met-rics specifically for handling the incompleteness of relevance data, but most of them have only examined the metrics un-der incomplete but unbiased conditions, using random sub-samples of the original relevance data (e.g. [1, 6, 7]). While random subsampling may mimic a situation where the num-ber of judged documents is extremely small compared to the entire document collection, it does not address the problems due to system bias and pool depth bias. Therefore, this pa-per examines metrics in more re alistic settings, by reducing the pool depth. We have also examined the effect of system bias, but will report on the results in a separate paper [5]. The metrics we examine are: Average Precision (AP), Q-measure (Q) [4, 6], nDCG [2], Rank-Biased Precision (RBP) [3], their condensed-list versions AP ,Q ,nDCG , hurt the system ranking, but the differences across metrics are not clear from this table.

What is probably more important than the system rank-ing is how significance test results are affected by shallow pools. Table 3 compares the metrics in terms of discrimina-tive power , or the overall ability to detect pairwise statistical significance [4]. The NTCIR results are omitted due to lack of space. For example, using the original relevance data of TREC03, Q is the most discriminative among the nine metrics, as it manages to detect a statistically significant difference at  X  =0 . 05 for 80 out of 120 (= 16  X  15 / 2) run pairs. Furthermore, Q maintains relatively high discrimi-native power even with shallow pools. It can be observed that, as the pool depth is reduced, the discriminative power of metrics go down, and the number of misses and false alarms increase: A miss is a significant difference detected by the original relevance data but not by the reduced rel-evance data; a false alarm is one detected by the reduced relevance data but not by the original relevance data. Table 3 shows that, in terms of discriminative power, AP, Q and nDCG are actually superior to AP ,Q and nDCG for handling shallow pools. Nevertheless, it also shows that AP and Q are superior to bpref, RBP and RBP .The trends are consistent across TREC and NTCIR.
Several recent studies (e.g. [1, 6, 7]) discussed the effect of incomplete relevance data in retrieval evaluation using random subsamples of the original relevance data. Hence they discussed neither system bias nor pool depth bias.
This paper examined the effect of pool depth bias. Even though Sakai and Kando [6] showed that AP ,Q and nDCG are effective for handling very incomplete but unbiased data, we showed that they are not superior to AP, Q and nDCG for handling shallow pools. Nevertheless, AP and Q are generally more discriminative than bpref, RBP and RBP
