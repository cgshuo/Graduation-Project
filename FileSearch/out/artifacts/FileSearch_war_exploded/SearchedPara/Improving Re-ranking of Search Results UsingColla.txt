 In general, interactions with current day web search engines could be character-ized as  X  X ne size fits all X . This means that all queries, posed by different users are treated similarly as simple keywords where the aim is to retrieve web pages matching the keyword. As a result, though the user has a focused information need, due to the excess information on the WWW, the amount of results re-turned for a particular keyword search is enormous. This places burden on the user to scan and navigate the retrieved material to find the web pages satisfying his actual information need. For example, two different users may use exactly the same query  X  X ava X  to search for different pieces of information - X  X ava island in Indonesia X  or  X  X ava programming language X . Existing IR systems would return a similar set of results for both these users. Incorporating the user X  X  interests and focus into the search process is quite essential for disambiguating the query and providing personalized search results.

One way to disambiguate the words in a query is to associate a categorical tag with the query. For example, if the category  X  X oftware X  or the category  X  X ravel X  is associated with the query  X  X ava X , then the user X  X  intention becomes clear. By utilizing the selected categories as a context for the query, a search engine is likely to return documents that are more suitable to the user. Current search engines such as Google or Yahoo! have hierarchies of categories to help users to specify his/her categories manually to the query. Unfortunately, such extra effort can not be expected from the user in a web search scenario. Instead it is preferred to automatically obtain a set of categories for a user query directly by a search engine. However, categories returned from a typical search engine are still independent of a particular user and many of the returned document results could belong to categories that may not reflect the intention of the searcher. This demands further personalization of the search results.

In this paper, we propose a two phase strategy to personalize search results over the WWW. We first learn a user profile based on his relevance feedback and use it effectively in a re-ranking phase to provide personalized search results. Since it is difficult to assume that the users will provide the relevant documents ([1], [2], [3] etc) explicitly, we make use of the implicit feedback given by the users which are captured in search engine interactions as  X  X uery logs X  or  X  X lick through data X . In the rest of the paper, we use the terms click through data, query log data and implicit feedback interchangeably. Such data consists of the queries, clicked documents and the identity of the user say ip address and is invaluable for research in search personalization.

Liu et al [4] successfully built user profiles for re-ranking by incorporating query categories in the learning process. We follow an approach similar to them. We first infer the category of a query using existing search engines and open directory project (ODP). We use the queries, their respective categories and the corresponding clicked documents in the learning of a user profile for the user. The user profile is represented as a matrix containing the pairs (term, category) and their corresponding weights. Machine Learning algorithms are used to automatically learn these term weights in the matrix. Each element represents how important the term is when the user is searching for a query of the given category. Re-ranking of the search results based on the user profile thus built, has shown improvement in performance. Though category helps to disambiguate the query, it adds another extra dimension to the user profile. This typically brings in sparsity in the user profile, which was observed in our case. Sparsity refers to the missing weights of certain words in the user profile.
In this paper, we present an effective re-ranking solution that compensates for the sparsity in a user profile, by collaborative filtering algorithms. A great deal of information overlap exists in web searches among users ([5], [6], [7] [8], etc). This overlap is seen due to users with similar information needs, posing similar queries. To our knowledge, this vast and rich source of information overlap hasn X  X  much been properly exploited for the WWW. Collaborative filtering algorithms work exceptionally well in a community like environment with significant overlap of information needs and interests. The novelty of our re-ranking algorithm lies in addressing the sparsity in the user profile by exploiting the information overlap, using collaborative filtering. Our approach shows an improvement in the overall results when compared to a re-ranking performed based on just the user profile. The rest of the paper is organized as follows. Section 2 discusses the Related Work, Section 3 discusses the proposed approach of learning user profiles, the re-ranking strategy, and addressing the sparsity in the user profiles. Section 4 describes the experimental setup and evaluation. Section 5 described our con-clusions and future work. The related work related to the approach proposed in this paper, can broadly be classified as work done in personalized search and work done in, collaborative filtering applied to search. 2.1 Personalized Search There has been a growing literature available with regard to personalization of search results. In this section, we briefly overview some of the available literature. Page et al [9] proposed personalized PageRank as a modification to the global PageRank algorithm. However, the com putation of personalized PageRank in the paper is not addressed beyond the original algorithm. Haveliwala [10] used personalized PageRank scores to enable topic sensitive web searches. However, no experiments based on a user X  X  context such as browsing patterns, bookmarks and so on were reported. Pretschner [11] used ontology to model a users in-terests, which are studied from users browsed web pages. Speretta and Gauch [12] used users search history to construct user profiles. Liu et. al [4] performed personalized web search by mapping a query to a set of categories using a user profile and a general profile learned from the user X  X  search history and a category hierarchy respectively. Shen et. al [13] proposed a decision theoretic framework for implicit user modeling for personali zed search. They consider the short term context in modeling a user. Radlinski and Joachims [[14], [15]] learn a ranking function using Support Vector Machines and using it to improving search results. 2.2 Collaborative Filtering and Search Chidlovski et al [16] describes the architecture of a system performing collabora-tive re-ranking of search results. The user and community profiles are built from the documents marked as relevant by the user or community respectively. These profiles essentially contain the terms and their appropriate weights. Re-ranking of the search results is done using the term wights using adapted cosine func-tion. The search process and the ranking of relevant documents are accomplished within the context of a particular user or community point of view. However the paper does not discuss much about the experimental details. Sugiyama et.al [17] performed personalization by adapting to users interests without any effort from users. Further, they modified the traditional memory based collaborative filtering algorithm to suit to the web search scenario and used it to improve the search results. They constructed a user-term weights matrix analogous to user-item matrix in memory based collaborative filtering algorithms and then applied traditional collaborative filtering predictive algorithms to predict a term weight in each user profile. Lin et. al [18] presented an approach to perform per-sonalized web search based on PLSA, Probabilistic Latent Semantic Analysis, a technique which stems from linear algebra. They extracted a co-occurrence triple containing the users, queries, and web pages by mining the web-logs of the users and modeled the latent semantic relationship between them using PLSA. Armin Hust [19] performed query expansion by using previous search queries by one or more users and their relevant documents. This query expansion method reconstructs the query as a linear combination of existing old queries. The terms of the relevant documents of these existing old queries are used for query expan-sion. However, the approach does not take the user into account. In ([6], [5], [20], [7], [20], [21]) a novel approach to web search -Collaborative Web search was introduced. It combined techniques for exploiting knowledge of the query-space with ideas from social networking to develop a Web search platform capable of adapting to the needs of communities of users. In brief, the queries submitted and the results selected by a community of users are recorded and reused in order to influence the results of future searches for similar queries. Results that have been reliably selected for similar queries in the past are promoted. Rohini and Vamshi [22] proposed an approach for re-ranking of search results in a digital library scenario. The user profiles were constructed from the documents marked as relevant or irrelevant. Re-ranking of the results is done using the user profile and profile of others users in the community. They assumed and assigned a set of static communities for each user which the user has selected while registering with the system. Also, the user also selects the community before posing the query and the re-ranking is done based on the community selected.

Several other works ([23], [24], [8], [25], [19] etc) have made use of past queries mined from the query logs to help the current searcher. The proposed approach to search result personalization consists of two phases. The first is a learning phase and the second is a retrieval/re-ranking phase. We use  X  X lick through data X  from a real world search engine, www.alltheweb.com, to build and test our proposed approach. In this section we discuss in detail our ap-proach of learning user profiles and re-ranking search results for personalization. 3.1 Mapping Query to a Category User profiles are learned on implicit feedback data, annotated with the category of the query posed. However, the click through data used here, does not consist of an associated category for the query. We therefore enhance the click through data by assigning category information to all the queries using the ODP, Open Di-rectory Project (http://dmoz.org). The DMOZ Open Directory Project (ODP) is the largest, most comprehensive human-edited web page catalog currently available. It covers 4 million sites filed into more than 590,000 categories (16 wide-spread top-categories, such as Arts, Computers, News, Sports, etc.) Cur-rently, there are more than 65,000 volunteering editors maintaining it. ODP X  X  data structure is organized as a tree, where the categories are internal nodes and pages are leaf nodes.

In our work, we consider only the top most ODP categories in the hierarchy to classify the query into categories. The category information of the query can be obtained by posing the query to one or more of the directory services available (directory.google.com, http://dmoz.org). which returns the related categories to the query. Otherwise, based on the categories of the top 10 documents in the search results, the most common document category is chosen and selected as the query category. Other effective solutions for query categorization exist, like training a text classifier on the documents contained in the ODP data. Such a classifier could be used to categorize the clicked documents in the click-through data. Improvements in performance of query categorization are always possible, and will enhance our proposed approach. However, for simplicity we currently focus on the former approach for query categorization that depends on direct ODP lookup. 3.2 Learning User Profiles We use machine learning algorithms for learning the user profiles from the im-plicit feedback provided by the user. The input to the learning algorithms is a user X  X  implicit relevance feedback, gathered from the click through data, along with the query and the associated category. Learning the user profile involves learning the weights of certain features extracted from the implicit feedback. The effectiveness of the user profile depends to a large extent on the representation of features. As mentioned earlier, we consider the features (term, category) to ef-fectively represent the context through the category. The weights of the features represent the importance of the term for the respective category.

We considered SVM for learning the weights of the features for its success in various text applications [26], [27]. An SVM is trained using our proposed features and at the end of the training phase, the weights of the features are learned which constitutes our user profile. The procedure of learning weights is similar to Radlinski and Joachims [14]. SVM light [28] has been used for training the SVM. 3.3 Re-ranking Re-ranking of the results is done by first retrieving a set of documents matching the query using a search engine. Then the top documents returned by the search engine are reranked using the user profile in the following manner. At first the test query category is inferred similar to the learning phase as discussed in section above (Mapping query to a category). Then for each word in the document, the weight of the pair (term, category) is obtained from the user profile.
Let c be the identified category of the query, t beawordinthedocument D j and w t,D j be its weight in the document D j (typically the term frequency TF or TFIDF etc). t c represent the pair (term, category). UP a,t c represents the weight of t c in the user profile of the user a . CP a,t c represents the predicted weight of t c using collaborative filtering. Then the document ranks are computed as weighted combination of its term frequency TF in the document and the weight obtained from his user profile as shown in Equation (1).
 Re-ranking is done by sorting the documents in decreased order of their rank. Based on experimentation, we set the value of  X  to be 0 . 7. 3.4 Addressing Sparsity in the Use r Profile to Improve Re-ranking Usage of a (term, category) pairs helps to disambiguate the query and act as good contextual information in building a user profile. However, this typically brings in sparsity in the user profile, due to an added dimension to the user pro-file -category of the query. Sparsity refers to missing weights of certain words in the user profile. We address the sparsity in the userprofile using collaborative filtering to improve the re-ranking of the documents. Certain weights of the pairs (term, category) not occurring the user X  X  profile are predicted using the adapted version of the collaborative filtering which we present below. In the following subsections, we first briefly review the pure collaborative filtering algorithms, especially neighborhood-based algorit hms, and then describe the adapted col-laborative filtering algorithms to address the sparsity in the userprofiles and then present how we make predictions of the pairs (term, category).
 Overview of the Pure Collaborative Filtering Algorithm Collaborative filtering is one of the most successful recommendation algorithms. They have been popular for recommending news [[29], [30]], audio CDs, movies, music [31], research papers etc. Recomme ndations are typically computed using the feedback taken from all the users in the community represented in a user-item matrix. The entries in the user-item matrix are the ratings given by the respective users for the respective items. Collaborative filtering can broadly be seen as the problem of predicting missing values in a user-item ratings matrix. Figure 1 shows a simplified example of a user-item ratings matrix. In the neighborhood-based algorithm [32], a subset of users is first chosen based on their similarity to the active user, and a weighted combination of their rating is then used to produce predictions for the active user. The algorithm can be summarized in the following steps: 1. Weight all users with respect to similarity to the active user. This similarity between users is measured as the Pearson correlation coefficient between their rating vectors. 2. Select n users that have the highest similarity with the active user. These users form the neighborhood. 3. Compute a prediction from a weighted combination of the neighbors ratings.

In step 1, S a,u , which denotes similarity between users a and u ,and is com-puted using the Pearson correlation coefficient as shown in Equation (3) where r a,i is the rating given to item i by user a ,and r a is the mean rating given by user a ,and I is the total number of items. In step 2, i.e., neighborhood-based methods, a subset of appropriate users is chosen based on their similarity to the active user computed in the above step, and a weighted aggregate of their ratings is used to generate predictions for the active user in the next step 3. In step 3, predictions are computed as the weighted average of deviations from the neighbors mean as shown in Equation (4) Adapted Collaborative Filtering Algorithm In the pure collaborative filtering algorithms described above, we considered a user-item ratings matrix. Similarly, we now consider user-(term,category) matrix (see Figure 2). Each row in the matrix represents the entries in the user profile of the respective user. By representing the user profile in this fashion, collaborative filtering algorithms can directly be applied.

The prediction of the (term,category) weights are computed by first identify-ing a set of similar users ( ie users who has similar (term,category) weights as measured using Equation (5). Then using these users, the predictions are com-puted analogous to pure collaborative filtering as shown in Equation (6) Then the re-ranking of the document is done as described in Equation (1). where p ( a, t c ) is the predicted computed for term t in query Category c and is equal to CP a,t c . 4.1 Data and Experimental Setup Query log data used in the experiments consist of the query, the clicked URLs for the query and the user identifier (ip addresses) and the time of click of the document. Such information though invaluable for research on information retrieval, is not released by major search engines. Recently, Alltheweb.com 1 has made available its search logs for research purposes. The data was collected from queries mainly submitted by European users on 6 February 2001. The data set contains approximately a million queries submitted by over 200,000 users and 977,891 unique click URLs. Further information on the data can be found in [33].
We use the query log data released by Alltheweb.com to perform our exper-iments and to evaluate the proposed approach We first divide the query logs into a large chunk of training clickthrough data, used for learning user profiles and a smaller chunk for testing and evaluating the approach. A direct evalua-tion experiment of our proposed re-ranking algorithm can not be performed on the present day X  X  search indices of Alltheweb.com or any other search engine for that matter. Document repositories on the WWW have been changing drasti-cally and undergo restructuring. Hence evaluation results can not be based on user profiles learnt from the query logs used in the current experiment. There-fore we first obtain all the documents corresponding to the queries in the testing data by crawling the click URLs and storing them as a repository. We were only successful in retrieving about 40% of the actual click URLs due to broken links and restructing of the WWW. These retrieved documents constitute the docu-ment repository used in current test experiments. With the volume of query log data we are working with, this repository could be considered as an analog to the WWW that corresponds to the query logs in discussion. For the purposes of these experiments, we name this repository as the mini-WWW, consisting of about 35,000 documents. We also pick queries from the query log data and pose it to Google to fetch and download the top 100 documents. These documents are added to the mini-WWW. This prevents any kind of bias that may have been introduced in the construction of mini-WWW from click URLs in the query log data. With availability of every day query log data we expect the proposed approaches to scale and be useful in the WWW scenario. We used Lucene 2 , an open source search engine for indexing this mini-WWW repository. All the evaluations reported below are obtained by performing our experiments through Lucene X  X  search engine. 4.2 Evaluation The testing data extracted from the clickthrough data is now used for evaluat-ing the performance of our re-ranking approach. The test data consists of 5,000 queries posed by 780 users, with an average repetition of 15.9% in the queries. Repeated experiments have been conducted by using subsets of this training data. Each query from the testing data set is posed to the search engine for the mini-WWW and results obtained are cross-validated with references to the actual clicked documents in the testing data. We follow an evaluation approach similar to the one followed in [5] . We compare three methods of re-ranking. Firstly we consider the ranking provided by search engine, in this case Lucene X  X  default ranking. The second approach tested is the ranking based on only the user profile. The third is the proposed approach for addressing the data sparsity problem using collaborative filtering. We refer to them as  X  X nranked X ,  X  X nly user profile X  and  X  X ollaborative X  respectively. The evaluation metrics used for compar-ison are minimum accuracy and precision @ N, N=5, 10 and 20. We could not evaluate the standard collaborative filtering measures like MAE etc because we assume boolean relevant judgments as opposed to the former which use ratings typically ranging from 0-5.
 Minimum Accuracy. Minimum accuracy has been used in [5] in evaluation of their approach. It measures the ability of a search engine to return at least a single relevant result in returned results. We compare the top 30 results returned by our ranking approaches in calculating the minimum accuracy. The percentage of the queries for which at least one relevant result is returned is computed. The results are presented in Figure 3. Precision. We used precision at N (p@N) defined as the number of relevant documents at a given cut-off rank at N. It is a widely used metrics for evaluating approaches performing re-ranking of results. The reported value is averaged over all users and queries. We compared the three approaches mentioned above. The precision values for the three approaches  X  X nranked X  (UP),  X  X nly user profile X  (PP) and  X  X ollaborative X  (CP). The results are shown in the Table 1. As it can be seen from the table, our approach showed an improvement over other approaches. In this paper, we proposed a two phase strategy to personalize search results over the WWW. We first learn a user profile from his  X  X lickthrough data X , collected from a real world search engine. This user profile is then used in a re-ranking phase to personalize the search results. We also used query and its cat-egory information to in learning the user profile. Category information helps to disambiguate the query and focus on the information need. However, in the sce-nario of WWW search, it adds another extra dimension to the user profile, typ-ically bringing in sparsity in the user profile. We propose an effective re-ranking strategy that compensates for the sparsity in a user X  X  profile, using collabora-tive filtering algorithms. We evaluate our approach using standard information retrieval metrics, to show an improvement i n performance over earlier re-ranking strategies based on only user profile.
 We would like to thank Dr. Vasudeva Varma, for dicussions on collaborative filtering and re ranking of search results.

