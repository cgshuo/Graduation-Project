 The automated targeting of online display ads at scale re-quires the simultaneous evaluation of a single prospect against many independent models. When deciding which ad to show to a user, one must calculate likelihood-to-convert scores for that user across all potential advertisers in the system. For modern machine-learning-based targeting, as conducted by Media6Degrees (m6d), this can mean scoring against thou-sands of models in a large, sparse feature space. Dimension-ality reduction within this space is useful, as it decreases scoring time and model storage requirements. To meet this need, we develop a novel algorithm for scalable supervised dimensionality reduction across hundreds of simultaneous classification tasks. The algorithm performs hierarchical clustering in the space of model parameters from histori-cal models in order to collapse related features into a single dimension. This allows us to implicitly incorporate feature and label data across all tasks without operating directly in a massive space. We present experimental results showing that for this task our algorithm outperforms other popular dimensionality-reduction algorithms across a wide variety of ad campaigns, as well as production results that showcase its performance in practice.
 I.5.4 [ Computing Methodologies ]: Pattern Recognition X  Applications supervised dimensionality reduction, clustering
Online display advertising, (broadly speaking, the show-ing of banner ads on websites) is a large and growing in-dustry, with consumer brands expected to spend over $34 billion on display ads in 2013 [22]. The problem of targeted display advertising involves determining where, when, and to whom to show a particular display ad on the Internet.
Display advertising decomposes roughly into two distinct tasks: retargeting , which involves advertising to people who have had some prior interaction with the brand (e.g., visiting a brand X  X  website) and prospecting , which seeks to acquire new customers by advertising to people without an observed prior affiliation with the brand.

At m6d, we operate a large-scale, machine-learning-based display advertising system that delivers tens of millions of display impressions for hundreds of different advertisers ev-ery day. Our goal is primarily prospecting, meaning that we want to identify, for each brand, a set of previously un-affiliated individuals who are most likely to respond to the brand X  X  advertising. A response in our case is not a click but rather a  X  X ost-view conversion, X  where a user takes a brand action (see below) within a given time period after seeing the ad. In practice, this means that we score hundreds of millions of Internet users as either  X  X ood X  or  X  X ad X  prospects for each of our client brands.

The central component of our targeting technology is a massive-scale machine learning system [12, 16, 17] that si-multaneously operates and autonomously updates thousands of classification models in parallel. Our primary predictive modeling learns directly from anonymized browsing history of browsers that allow third-party cookies. An individual data point, for the purposes of our system, is a browser cookie . Whenever someone visits our website or interacts with one of our data partners, we get the opportunity to set a cookie on his or her browser. We use these same cookies to maintain a partial history of the URLs on which we interact with the browser, either through our own systems or those of our data partners. Throughout the paper, we use the words browser , cookie , or user to refer to an individual instance of our cookie.

For the purposes of model-building, this history becomes a massive, sparse binary feature space of hashed URLs. A user gets 1 for all URLs in the cookie and 0 for everything else. Currently, we track over 100 million unique URLs in the system, any of which could be used for modeling. For most of our models, the class label during training is simply a binary indicator of whether or not the user has ever taken some sort of brand action . This action is typically visiting the brand X  X  home page, downloading target material, or making a purchase from the brand X  X  site.

Our high-dimensional models work quite well (as judged by our internal measurements and by clients X  comparisons with other targeters [13]), but there are situations where l ower-dimensional models are preferable, such as: 1. Rare Events : For some of our models, positive-class 2. Cold Start : We typically cannot observe brand ac-3. Overhead : High-dimensional models are expensive to
The goal of this work is to identify a good  X  X eneric X  lower-dimensional representation of our high-dimensional feature space that will be used across many different campaigns for predictive modeling. Keep in mind that with insufficient la-bels, supervised dimensionality reduction and feature selec-tion is unlikely to succeed. Additionally, it would imply that for each campaign we have to maintain a different feature space which burdens the system even further. The exist-ing literature provides a wide variety of unsupervised tech-niques for dimensionality reduction in large data sets that would be suitable. However, in our particular situation, we have additional information that can be brought to bear: a large library of historical models built on the same, massive feature set. These models encode information about each URL X  X  impact on conversion for hundreds of different cam-paigns. It seems natural to use as much of this information as possible.

To this end, we have developed a multi-task hierarchical clustering scheme for dimensionality reduction. The algo-rithm identifies closely related dimensions by finding clus-ters in the space of model parameters from all of the mod-els in our system. These clusters produce a much-lower-dimensional space in which similar dimensions combine URLs with similar predictive information (strength and direction of affiliation with the class label) across all of our models. As it turns out, models built in this lower-dimensional space are nearly as informative as models built on the original high-dimensional data. Furthermore, we show that our approach outperforms several popular alternatives..

The remainder of the paper is organized as follows: Sec-tion 2 gives a brief overview of dimensionality reduction techniques and the particular techniques that we consider in this paper. Section 3 describes our supervised multi-task dimensionality reduction technique in detail. Section 4 ex-perimentally compares our technique with several popular existing techniques, and shows its performance within the m6d production system, and Section 5 presents conclusions and some observations on the nature of dimensionality re-duction in large production data.
The goal of dimensionality reduction is to take a high-dimensional feature vector x = ( x 1 . . . x n ) and project it into a k -dimensional space, with k &lt;&lt; n , such that the resulting lower-dimensional vector x  X  captures the vast majority of the relevant information in x . Prior research has produced a number of very popular techniques for dimensionality re-duction (e.g. [6, 8, 9, 21]), and we cannot consider all of them. Instead, we focus in particular on techniques that are well-suited to our problem domain.

We prefer techniques that are feasible in large, sparse bi-nary data. This rules out, for example, PCA [21] and ICA [8], which require rescaling the columns to have zero mean. Such mean-centering destroys the sparsity of the data, dra-matically increasing the space and time required.
Additionally, we prefer techniques that will produce a sin-gle low-dimensional feature space across multiple classifica-tion tasks, since it is prohibitively expensive to maintain thousands of projection matrices for thousands of models. All unsupervised dimensionality reduction techniques triv-ially satisfy this requirement, but it rules out many existing supervised approaches [6, 9].

Subject to these constraints, we have experimented with a number of different dimensionality-reduction approaches, which we outline briefly here: Feature Hashing : Perhaps the simplest way to reduce a massive binary feature space is via feature hashing [20]. Fea-ture hashing transforms a bag of words into a bag of hashed IDs. Given a set of tokens and a hash function h (), we ap-ply the hash function to each of the tokens and the new feature space is simply the set of hashed values. Dimension-ality reduction results from hash collisions. For example, if a cookie contains { m6d.com , nytimes.com , nyu.edu } , and we have h ( X  m6d.com  X ) = 6, h ( X  nyu.edu  X ) = 6 and h ( X  nytimes. com  X ) = 8 then, in the new space, the cookie has values for features 6 and 8. Hash functions are typically 32-bit or 64-bit, and to project into an arbitrary k -dimensional feature space, we compute h (  X  ) mod k .

The method of [20] also employs a second one-bit hash, which assigns a sign (1 or -1) to each URL. The new feature space is integer-valued rather than binary, and example i  X  X  value for feature j in the new feature space is given by: where the summation runs over all the URLs u in example i for which h ( u ) = j .
 Contextual Categories : The web has a number of sources, both proprietary and free, that categorize specific web pages by their content. These categories serve as content-based groupings that can be used to reduce the dimensionality of the data. For the purposes of this paper, we have obtained two sources of category data: a freely available web ontol-ogy called the Open Directory Project (ODP) 1 , as well as a proprietary categorization from a commercial data provider. With category data, our original feature space of URLs be-comes a feature space of categories. A given browser gets 1 for any category in which he has visited at least one website and 0 for all other categories.
 Singular Value Decomposition : The reduced-rank sin-gular value decomposition (SVD) approximates an m  X  n data matrix X by a rank-k matrix X  X  , defined as the prod-uct of three matrices: w ww.dmoz.org where U a nd V are m  X  k and n  X  k orthonormal matrices of singular vectors respectively and  X  is a k  X  k diagonal matrix of singular values .

SVD is theoretically attractive because it produces the best (in the least-squares sense) reduced-rank approximation to the original data, and recent work has led to fast, block-wise algorithms for low-rank SVD X  X 7] that can operate in a distributed fashion on large data sets like ours.
The matrix U X  forms the reduced-rank data set used for classification. A new data set X 2 in the original n -dimensional feature space can be converted to its low-rank representation by:
SVD is unique among techniques that we discuss in that it produces a dense output matrix rather than a sparse one. The implications for storage and computability make this approach less appealing to us, but the predictive perfor-mance observed using SVD in recent data mining compe-titions [10] on sparse data makes it an attractive technique for comparison.

Supervised Clustering : Each of the prior techniques will work for our particular problem domain, since they can reduce dimensionality in large, sparse binary data. However, as unsupervised techniques, they cannot incorporate one of the richest sources of information that we have: the model-ing experience on hundreds of campaigns over the years. We develop in the next section a multi-task clustering approach that utilizes this knowledge by using models X  parameters as the features describing the URLs.
More so than in data mining environments where an objec-tive performance metric can be optimized, the effectiveness of clustering is driven by deliberate setup choices. The core of clustering is the distance metric and with it the repre-sentation of the entities of interest. We are trying to group together URLs such that using only the groups retains the maximal amount of predictive performance. Initially, there are a number of  X  X atural X  ways of thinking about the simi-larity between websites: m6d has another, much more relevant set of information about URLs in our system: how they affected the conversion-Task 1 1.3 0.9 1.1 0.1 -0.2 0 -2.3 -3.2 Task 2 0.4 0.5 0.2 -6.4 -5.3 -5.9 -5.7 -6.1 Task 3 1.9 2.1 1.7 -0.1 0.2 0.3 1.3 1.5 Task 4 -1.0 -1.2 -1.4 5.1 5.4 4.7 0.3 0.2 Table 1: Example representation where each feature r epresents the coefficient of a linear model that was estimated for a given task. We include here four hypothetical tasks. The curious reader should be able to identify 3 distinct groups of features (the answer is in the text). likelihood scores modeled across many past campaigns. It is likely that future campaigns share similarities with those of the past in terms of industry and product types. So in-tuitively, URLs that consistently affect the predictions in a similar way can be grouped together safely without losing predictive information. Consider Table 1 as an example. It shows the parameters of a linear model for 8 different fea-tures on 4 different tasks and there are 3 intrinsic feature groups.
Having decided on the representation X  X sing the param-eters from previous modeling tasks as features, the question of selecting a good distance measure is less difficult than for a typical clustering task. We are looking to group features with similar impact on the predictions across the different tasks. While it is less important with our binary indicator features, we would prefer a metric that is somewhat indiffer-ent to the scaling of the features and mostly directional. In a linear model if two features were nearly identical except that one is a multiple of the other, the parameters are likely to be similar subject to the scalar difference (the parameter scales counter-proportional to the scaling of the feature). As a re-sult we used correlation similarity (specifically 1-correlation as the distance function). Looking at the example in Table 1, the tree groups that become apparent using correlation sim-ilarity are features 1-3, 4-6 and 7-8.
There are a number of well-known clustering methods in-cluding k-means, hierarchical clustering, and (also applica-ble in this case) co-clustering. To maintain flexibility in the following step of cluster assignment we choose hierarchical clustering. Figure 1 shows an example result on a small subset of 70 URLs.

The clustering for the production system was done for the 15,000 URLs with the highest visitation rates during a one-week training period. We use URLs with the highest visita-tion rates because these are the URLs whose coefficients will have the least variance across all campaigns. The choice to limit the clustering to 15,000 URLs is mostly due to mem-ory constraints in R. If we ever want to increase the number of URLs covered, a simple approach would be to assign any new URL to the cluster with which it is most correlated. So it is not strictly necessary to include all 100 million URLs in the actual clustering algorithm. There is some danger in including rarer web sites into the clustering step as their pa-rameters are likely to suffer more from variance during the prior model estimation step. Having a majority of entities w ith basically random features would likely reduce the ef-fectiveness of the approach. We use the parameters for 100 campaigns with most positive examples that were running at the time. The parameters are estimated using a variant of naive Bayes that calculates a likelihood ratio [14]. In pro-duction, a large number of our models are built as logistic regressions using SGD [3] for estimation. For clustering, we deliberately use naive Bayes models as they maintain each individual feature X  X  signal more distinctly. For example, if there were a set of n URLs with nearly identical relation-ships to the targets, the logistic models would adjust the in-dividual parameters accordingly (ideally but not necessarily reducing each by a factor of 1 /n , depending on the regu-larization), whereas the class-conditional independence as-sumptions lead naive Bayes to overestimate the parameters. While this may be detrimental to predictive performance, it is beneficial for identifying similar URLs. Naive Bayes also tends to produce stable parameter estimates, since each indi-vidual parameter estimate unaffected by the others. The hi-erarchical clustering was performed in R using 1-correlation as the distance metric and the hclust package as shown in Appendix A. The small dendrogram in figure 1 shows a nice even breakup of the space. Upon investigation we find many meaningful groupings. This confirms our initial intu-ition that using the parameters from campaign models pro-vides a promising representation that does not suffer from size-induced artifacts so commonly observed in clustering.
Intuitively, the grouping should reveal both contextual similarity and co-occurrence. One example is information substitutes X  X ome people read the financial section of the NY Times and others the financial section of the Wall Street Journal. Only a few read both. The clustering identifies them as substitutes and by grouping them we improve the sparsity and reduce variance. But while context could cause content to cluster together, it does not have to be the case. The connection is  X  X iscovered X  by the predictive modeling and may work in a way we do not understand. Our notion of context may be different from the indication of consumer interest. In this sense, seeing www.toysrus.com and www. birthdayexpress.com together indicates that the consumer is probably in the market for children X  X  gifts. While the two sites might have co-occurrence of people shopping around (co-visitation), the intuitive  X  X onsuming intent X  relationship between these sites can be observed by the clustering even without co-visitation.
The most common way to assign entities to hierarchical clusters is to cut the tree at a suitable height. However, this may not lead to the best set of features as some of the clus-ters will have many fewer (and less commonly visited) URLs than others. A good feature needs to have useful coverage. Thus, rather than a static cutoff on the dendrogram, we de-signed a recursive cut function that traverses the cluster tree and cuts a branch once the URLs below reach a minimum coverage (number of browsers visiting them). The code is provided in Appendix A where the input data format has the number of browsers for each site in the first column. In essence, the recursive procedure attempts to balance the sparsity and the similarity of the elements in the final clus-ters. We assign a goal coverage of 1% resulting in a total of 4,318 final clusters, where each URL is assigned to exactly one cluster.
Let us quickly clarify how we convert the original features (indicators of URL visitation) into the new feature space. The two obvious options are: the count of number of origi-nal URLs in each cluster (the sum of the original indicator features), or the creation of another simple indicator when the user visits at least one URL in the cluster. For our production system we have chosen the latter as it is consis-tent with the representation used by our other models. We have never found increased predictive performance when re-placing the indicators with counts or other more complex expressions. Ultimately, keeping it simple also adds to the robustness of the system implementation [17] and avoids po-tential artifacts from the heavy-tailed Poisson distributions. Furthermore, some of our models use L1 regularization, so keeping the features on a similar scale is beneficial.
One more technical note is important. Clusters are con-structed as a group of websites that consistently get similar model parameter estimates across a wide range of tasks. By grouping them as one feature, we force all of these web-sites  X  X nto X  the same parameter and that may induce some bias in the individual estimates. On the other hand, the re-duced dimensionality reduces the variance of the estimates for the models built on the new clusters. Choosing to use di-mensionality reduction always displays a preference for bias over variance. It is an empirical question as to whether proposed clustering approach is more effective at limiting this bias compared to other approaches. We have not ex-amined the bias/variance decomposition in detail X  X argely because ours are ranking tasks, for which the decomposition i s non-trivial. We do believe that leveraging the learning from  X  X imilar X  tasks might indeed be beneficial, because we minimize the bias by using other models to tell us which websites can  X  X afely X  be put together without reducing the predictive information content.
Our clustering solution relates to a number of existing techniques and strategies in machine learning.
In order to demonstrate the utility of our proposed clus-ter method for dimensionality reduction, we report three dis-tinct evaluation settings. The first is a set of fully controlled experiments where we compare our clustering approach to the set of comparable techniques outlined in Section 2  X  X n vitro X  under identical settings (training data, model estima-tion, and campaign mix). The second scenario compares our production models (models estimated using our produc-tion system on potentially different date ranges) rather than the controlled experiments. We are trying to answer the question how much better the high-dimensional production models perform. We compare models estimated on the clus-ter and the original high-dimensional feature space  X  X n vitro X  in a controlled sandbox (identical test set) used by m6d to track the quality of our models internally. Finally we look at the real campaign performance when we show ads to the audience selected by the model in the m6d production en-vironment for a small set of campaigns that were running both cluster and high-dimensional models  X  X n vivo X .
In order to evaluate dimensionality-reduction algorithms, we need to compare them with respect to the performance of a particular classification algorithm. For the purposes of this paper, we use one of our production algorithms: a lo-gistic regression trained by stochastic gradient descent [3]. Given the speed and workload constraints in our production system, more complex classifiers like random forests are gen-erally not applicable, so we do not discuss alternatives here. Experimental Setup: We chose a set of 28 campaigns that are reasonably rep-resentative of our overall advertiser mix and built separate low-dimensional models for each campaign. We use real data from seven days in January 2013 for training and the follow-ing day X  X  data for evaluation.

For each campaign, we transformed both the training set and the evaluation set into the different feature spaces de-scribed in Section 2. For both contextual categories and feature hashing, the transformation is trivial: for the cate-gory data it is simply a lookup table, and for feature hash-ing it is provided by the hash function ( hash() in Python, in this case). Recall from Section 3 that we only use the 15,000 most popular URLs in our system to train the cluster model. For a fair comparison, we reduce the exact same 15,000-dimensional feature space for all techniques. The transfor-mation into the cluster space was described in Section 3.4.
For SVD, the transformation is derived from data, and we learn the transformation from a subset of our negative-class training data. Since we use the same negative set across all campaigns, this results in a single transformation matrix. Despite using the efficient stochastic SVD algorithm, sub-sampling was necessary in order to run it within our memory constraints.

There are exactly 4,318 dimensions in the low-dimensional feature space produced by the clustering (and running in production), so for comparison, we tried to get as close to this number as possible with the other techniques. Since it is trivial to hash into any number of buckets, we use exactly 4,318 features for the hash models. With Open Directory, our existing dimensions map into 5,594 distinct groups, and this is what we use. For the commercial category data, the number is 1,183.

SVD is considerably more difficult. The algorithm of [7] and its Hadoop implementation are relatively fast, but it operates in blocks to control memory consumption. As a result, computation time can be an issue. More constrain-ing, however, is the production of the training and test sets, where each combination of user and URL joins to k dif-ferent factor values in order to produce the product XV . Even after downsampling the training set to reduce compu-tation time, it proved prohibitively expensive to expand the feature set beyond 1,000 dimensions, as several different at-tempts to scale up to 2,000 ran our machines out of memory. Whenever we present SVD results here, we use 1,000 dimen-
T his was run on a cluster of 30 machines, each with 8 cores sions since that is the strongest performance we have. We briefly explore the performance of lower-dimensional projec-tions later in the paper.

Figure 2 compares the performance of our logistic classifier on clusters against each of the other reduced feature spaces. We report two performance metrics here: area under the ROC curve (AUC) and lift at a threshold of 5%. This is simply the number of positive examples in the top 5% of model scores divided by the number that would be expected from a random classifier (i.e., 5% of all positives). We often report this number internally because it reflects a typical campaign setup, where given a fixed budget that can reach only a small proportion of the targetable population, m6d is expected to return as many conversions as possible. All of our performance estimates are averaged over 100 bootstrap estimates in order to reduce the variance in the results.
Each plot compares the performance of our algorithm against the performance of a single competitor across all 28 cam-paigns as measured by either lift or AUC. Values above the identity line indicate that our algorithm performed best and values below the line mean that the challenger performed best. Several conclusions are immediately evident from the figure. First, our clusters outperform all of the other meth-ods most of the time. Furthermore, when they do lose, it is generally by a small margin.

The clusters seem to be stronger, relative to other meth-ods, under lift than under AUC. This is perfectly fine for our purposes, since performance on the highest-scoring browsers is most important for our business.

In order to quantify the performance differences presented visually in Figure 2, Table 2 summarizes the average perfor-and 16 GB of memory. We distributed the computation a cross dozens of machines (30,000 examples per input file; 65 reduce tasks) and limited each instance to 2 GB of memory. mance and average relative performance, as well as statis-tical interpretations of the performance differences. More specifically, average relative performance is the average ra-tio between our clustering and a competing method. We report a difference in performance as a  X  X in X  if our clusters perform significantly better than a competitor with 95% con-fidence using means and variances estimated from the boot-strap samples. Similarly, we report a loss if they perform significantly worse.

The supervised clusters outperform all competitors, in both lift and AUC, over our set of test campaigns. It has more statistically significant wins, fewer losses, and higher average performance. The most significant improvement is relative to feature hashing, where we perform 42% better on average. 3 Even if we remove the most extreme outlier, the number drops only to 35%.
 SVD performs admirably in a much smaller feature space. At 1,000 dimensions, SVD had the best relative performance in terms of both lift and AUC and clearly outperformed the commercial category feature space, which is its closest competitor in terms of dimensionality. One possible expla-nation is that we were not able to obtain contextual data for all 15,000 of our original URLs from either our commer-cial or free category sources, whereas SVD can make use of any information that we feed into it. This is a fundamental limitation of any third-party data source for dimensionality reduction: the power of the low-rank feature space is limited by the reach of the source data.

Even with this limitation, the human-curated contextual categories from Open Directory perform very well, just 18% behind our clusters on average. The Open Directory cate-
R elative differences in lift are more meaningful to us than relative differences in AUC. A 42% increase in lift means 42% more conversions at the same targeting budget. puted using 95% confidence intervals from bootstrap estimates. Figure 3: SVD-based model performance as a func-t ion of dimensionality in terms of average Lift over 28 campaigns. gories were statistically indistinguishable from our algorithm over 1/3 the time, but significantly outperformed our algo-rithm only once. It is worth mentioning that the OpenDi-rectory feature space is the largest in our study, which may play a part in its strong performance.

To the best of our knowledge [10], SVD is typically used to compress data into a much smaller dimensionality than we have attempted here, but we found these extremely low-dimensional models to be tremendously ineffective in our domain. Figure 3 plots the average performance of the SVD models against the average performance of cluster models as we scale up the dimensionality of the SVD. SVD model per-formance improves considerably from 125 features to 1,000 features (about 30% on average) and in lower dimensions compares very unfavorably against our cluster method, shown for reference as a black line in the figure.
We currently build  X  900 cluster -based models in produc-tion, serving hundreds of thousands of display ads per day. Like all of our other models [17] they are built automatically, with absolutely no human curation. While humans do ulti-mately decide what proportion of each campaign X  X  budget to allocate to a particular model, cluster models exist, and we score users against them, for the majority of our campaigns.
One of the quality control tools in the m6d system [17] is a  X  X andbox X  test dataset of random browsers for which we track brand actions and evaluate our models. This happens independently of whether or not we showed an ad for the campaign. We have established previously [17] that perfor-mance in this sandbox correlates with actual campaign per-formance. The goal of this metric is to isolate the model per-formance from the circumstances (such as bid price, scale, or audience) of the campaign and allows for more rigorous testing and quality control of the automated modeling.
Figure 4(a) summarizes the performance of all our cur-rently active cluster models, compared against the corre-sponding high-dimensional models, regardless of whether they are currently being used for targeting. We report the median lift at 1% over the 120 different models, since median model performance is our preferred internal summary met-ric of system performance. Cluster performance tracks fairly consistently at 15%-20% below the performance of our high-dimensional model. The average performance difference over the entire time window is 17.4% and the median is 19%. In summary, our low-dimensional cluster models achieve a sig-nificant fraction of our main model X  X  performance, but the full model has typically a distinct advantage.
Figure 4(b) shows median targeting performance, over all active campaigns, of both the cluster model and our high-dimensional production model over time. The metric here is post-impression conversion rate relative to random target-ing. In order to provide  X  X ontrol group X  baselines for com-parison, we continually serve a relatively small number of untargeted (random) impressions for each of our campaigns. The y-axis in Figure 4(b) is simply the (median) conversion rate of the targeted population divided by the conversion rate of the untargeted population. The plot runs from Oc-tober 2011 to February 2013, and the big increase in cluster performance corresponds to a redefinition of the underlying clusters. The main difference between Figures 4(a) and 4(b) is that since the latter shows actual targeting performance, it only includes models that actually delivered impressions in the wild. As a result, models that are allocated zero targeting budget, presumably due to poor performance, are excluded.

While the previous  X  X n vitro X  results depend only on the quality of the model, the  X  X n vivo X  performance of relative post-impression conversion rates depends on many other fac-tors, including the price that we bid for a particular cam-paign, the level of competition for good browsers, and the scale of the campaign. It is interesting to observe that while our high-dimensional model clearly outperforms the clus-ter model on average  X  X n vitro X , the difference diminishes considerably after taking into account our human budget-allocation decisions in the time period after the most recent re-clustering was done. 4 Here we see evidence of the cam-paign managers X  ability to  X  X herry pick X  only the best models
T he clustering itself runs infrequently in production. based on their historical performance, and in particular to use cluster models only when they perform well.
We present an algorithm for supervised, multi-task dimen-sionality reduction using hierarchical clustering and demon-strate its effectiveness in both laboratory and production environments. In particular, we show that classification models built in the low-dimensional space derived by our algorithm perform nearly as well, in production, as models trained on a massive feature space of about 30 million URLs. Additionally, we show that our algorithm outperforms other reasonable alternatives for classification in high-dimensional sparse binary data. In the course of this study, we have ex-perimented with a variety of dimensionality reduction tech-niques in a wide variety of classification scenarios. In the process, we have noticed a number of implementation issues that may be of interest to practitioners.
 Feature hashing is extremely fast and very easy to imple-ment, but not really suitable for dimensionality reduction in our case. Most applications of feature hashing (e.g., [1]) hash into a much larger feature space than we have used here. It is possible that our 15,000-dimensional feature space used for clustering is not sparse enough for the collisions induced by hashing to be harmless.
 Singular Value Decomposition performs reasonably well, even though we never reached 4,000 dimensions. It is inter-esting to note that while prior studies [10] have reported success reducing to much smaller spaces, we see consider-able benefit in going all the way to 1,000 dimensions. However, it became clear over the course of the study that SVD would be very difficult to implement in our production system. Transforming our training data into the learned feature space requires a massive database join that takes hours even when distributed across dozens of machines. Fur-thermore, the resulting training data files (the input to our learning algorithm) are huge compared to what we usually encounter (1,000 nonzeros per example, compared to 20 or 30 in a typical sparse model). This dramatically increased the strain on the system in terms of the time it takes to train models, the disk space used to store input files, and the I/O demands that we place on the system. Scoring new browsers against existing models requires this same expen-sive transformation, which severely limits the ability to keep scores up-to-date.

One of the key features of our targeting system [17] is that we continuously and autonomously rebuild thousands of classification models in order to keep pace with changes in people X  X  browsing behavior. Furthermore, we are constantly re-scoring browsers in order to incorporate the newest infor-mation from the browser X  X  cookies. As such, an approach that dramatically increases training or scoring time is unde-sirable. Our results may provide further support the contin-ued investigation of SVD for situations where model-building is infrequent, the number of models to build is small, and near-real-time scoring is not required.
 Category Data is unique to our domain (learning from web histories), but is not totally unreasonably to include, because there are a lot of similar applications where people learn from web histories or web traversal patterns. Our re-sults indicate that human-curated category data have some promise as a proxy for the individual URLs themselves, but such data will always be limited by the completeness of the associated database. Furthermore, the effective incorpora-tion of such data is not necessarily trivial. For example, the site that we know as health.yahoo.net is encoded in OpenDirectory as health.yahoo.com . By contrast, auto-mated techniques like hashing, SVD, and our own cluster algorithm utilize data we already have.

In summary, the designed cluster approach shows better predictive performance compared to any of the (feasible) al-ternative dimensionality reduction approaches. The reduc-tion in predictive performance in many campaigns is limited compared to using the full models with two to three orders of magnitude more parameters. Due to their lower dimen-sionality, cluster-based models are notably more efficient to build and maintain and serve in our system. Further, the clusters themselves can provide additional insights to the brands (an interesting direction for future research). [1] D. Agarwal, R. Agrawal, R. Khanna, and N. Kota. [2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex [3] L. Bottou. Large-scale machine learning with [4] L. Breiman. Stacked regressions. Machine learning , [5] R. Caruana. Multitask learning: A knowledge-based [6] X. Geng, D. Zhan, and Z. Zhou. Supervised nonlinear [7] N. Halko, P. Martinsson, and J. Tropp. Finding [8] A. Hyvarinen. Fast and robust fixed-point algorithms [9] G. Karypis and E. Han. Fast supervised [10] Y. Koren, R. Bell, and C. Volinsky. Matrix [11] S. Pan and Q. Yang. A survey on transfer learning. [12] C. Perlich, B. Dalessandro, R. Hook, O. Stitelman, [13] C. Perlich, B. Dalessandro, O. Stitelman, T. Raeder, [14] C. Perlich and F. Provost. Distribution-based [15] C. Perlich and S. Rosset. Identifying bundles of [16] F. Provost, B. Dalessandro, R. Hook, X. Zhang, and [17] T. Raeder, B. Dalessandro, O. Stitelman, C. Perlich, [18] R. Raina, A. Ng, and D. Koller. Constructing [19] G. Valentini and F. Masulli. Ensembles of learning [20] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, [21] S. Wold, K. Esbensen, and P. Geladi. Principal [22] ZenithOptimedia. Global ad expenditure to return to size=dat[,1] dat[is.na(dat)]=0; co=cor(t(dat[,2:ncol(dat)]),use="pairwise.complete.obs") co[is.na(co)]=0; cl=hclust(as.dist(2-co)) mergesize=1:(nrow(dat)-1) mergesize[]=0 for (i in 1:(nrow(dat)-1)) { } extract=function(ind,clusterid) { } clu=cbind(cl$merge,mergesize) res=dat[,1:2] res[,2]=0 i=nrow(dat)-1 n=0; while(i&gt;0) { }
