
In this paper, we propose a new text clustering algorithm, named Clustering based on Frequent Word Sequences (CFWS). A word sequence is frequent if it occurs in more than cer-tain percentage of the documents in the text database. In the past, the vector space model was commonly used for information retrieval, but it treats documents as bags of words, ignoring the sequentia l pattern of word occurrences in the documents. However, the meaning of natural lan-guages strongly depends on the word sequences, and the frequent word sequences can provide compact and valuable information about the text database. Bisecting k-means and FIHC algorithms are evaluated on the performance of text clustering, and are compared with the proposed CFWS al-gorithm. It has been shown that CFWS has much better performance.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval] Clustering General Terms: Algorithms, Performance Keywords: Text documents, clustering, frequent word se-quences, Web search
The text database which consists of documents is usually very large, and it is very popular to use search engines to get information from the text database. In order to increase the precision of the retrieval result, many methods have been proposed. One approach is clustering the retrieval result before showing it to the user. By clustering the text docu-ments, the documents sharing the same topic are grouped together. The user can select the one that interests him/her most among the clusters returned. This method makes the search engine more efficient and accurate. Text clustering is known as an unsupervised and automatic grouping of text documents into clusters, so that documents within a cluster have a high similarity between them, but they are dissimi-lar to documents in other clusters [1]. Let X  X  look at closely the special requirements for the clustering of text retrieval result. First, the document model better preserves the se-quential relationship between words in the document since the accurate meaning of a sentence has close relationship with it. Second, associating a meaningful label to each final cluster is essential. Third, overlapping between document clusters should be allowed because a document can cover several topics. Fourth, the high dimensionality of text doc-uments should be reduced. Last, the algorithm should find the number of clusters by itself.

Our new text clustering algorithm, named CFWS, is de-signed to meet the above special requirements for text clus-tering. The key features of the CFWS algorithm are: it treats the text document as a sequence of words, instead of a bag of words, and whether documents share frequent word sequences or not is used as the measurement of their closeness. In our algorithm, each document is reduced to a compact document by keeping only the frequent words. In the compact document, we keep the sequential occurrence of words untouched to explore the frequent word sequences. By building a Generalized Suffix Tree (GST) [5] for all the compact documents, the frequent word sequences and the documents sharing them are found. Then, frequent word sequences are used to create clusters and summarize their content for the user. We found our CFWS algorithm is more accurate than other clustering algorithms.
An ordered sequence of two or more words is called a word sequence. A word sequence S is represented as &lt; w 1 ,w 2 ,... &gt; . A frequent word sequence is denoted by FS . necessarily following w 1 immediately in a text document. There could be words between them as long as w 2 is after document d supports this word sequence if these four words word sequence S is an FS when there are at least a certain user-specified number of documents supporting S . Multiple occurrences of a sequence in the same document is counted as one. The term phrase is defined in [6] as an ordered sequence of one or more words, and no gaps are allowed be-tween words. Thus, our definition of frequent word sequence is more adaptable to the variations of human languages.
Finding the frequent word sequences has two steps: find-ing frequent 2-word sets first, then finding frequent word sequences of all lengths by using the generalized suffix tree (GST) data structure. We use an association rule miner to find the frequent 2-word sets that satisfy the minimum sup-port. All the words in frequent 2-word sets are put into a set, and we remove all the words in the documents that are not in this set. After the removal, the resulting documents are called compact documents . Then, to find frequent word sequences of all lengths, each compact document is inserted into a GST, one by one. After building the GST, we tra-verse it by depth-first. By checking the support count and the length of the word sequence associated with each node, we can get the information of all the frequent word sequences of this database. As the number of unique words and the size of the compact database are much smaller than those of the original database, our algorithm is clearly more efficient than the Suffix Tree Clustering (STC) algorithm [6], which clusters text documents by constructing the suffix tree of all the sentences of the documents in the collection.
A cluster candidate is a set of text documents support-ing the same frequent word sequence. In the GST, only the nodes representing frequent word sequences can produce the cluster candidates. The frequent word sequence associated with each clustering candidate describes what the cluster candidate is about. However, sometimes we do not need fine clusters, instead we may be interested in a more gen-eral topic. In that case, we can merge the related cluster candidates. Instead of using a distance function to measure the closeness between two cluster candidates, we use the k -mismatch concept of sequential patterns. Given a pattern p ,atext t , and a fixed number k that is independent of the lengths of p and t ,a k -mismatch of p is a | p | -substring of t that matches ( | p | X  k )charactersof p . That is, it matches p with k mismatches. In our case, we are checking the mis-matches between the frequent word sequences found. As k value increases, the topic covered by each cluster of the final clustering would be more general.

After we merge several cluster candidates into clusters, we may find some clusters have too much overlapping between their document id sets. The overlapping of two clusters, C and C j , can be measured as O ( C i ,C j )= Ids i  X  Ids j Ids i and Ids j are the sets of document ids in C i and C respectively. If O ( C i ,C j ) is larger than the specified over-lapping threshold value  X  , these two clusters are combined into one cluster. Obviously, the range of  X  is [0 , 1]: when  X  = 0, these two clusters are disjoint; and when  X  =1,these two clusters have the same set of documents, which does not mean these two clusters are identical because this set of documents may cover two different topics. In the end, we collect those documents that are not in any cluster because they do not contain any frequent word. These documents form a cluster by themselves, and their topic could be de-fined as  X  X ther issues X .
In this section, we evaluate the performance of our text clustering algorithm in terms of the accuracy of clustering by using the F-measure. We implemented our algorithm in C++ on a SuSE Linux PC with a Celeron 500 MHz proces-sor and 384 MB memory.

For the performance evaluation, two types of pre-classified data sets were used. We chose Reuters-21578 Distribution 1.0 to test our algorithm X  X  performance on typical text docu-ments, and the data sets are denoted by Re1, Re2, and Re3. Another type of data sets, denoted by Se1, Se2, and Se3, are the retrieval results of a simulated search engine (Lemur Toolkit [3]) working on the English newswire corpus of the HARD track of TREC 2004 [2].

For a comparison with our CFWS, we also executed bi-secting k-means [4] and FIHC [1] on the same data sets. Table 1 shows the characteristics of the data sets and the F-measures of all three algorithms. For the bisecting k-means algorithm, we specified the desired number of clusters to be same as the number of classes in each data set. Both FIHC and CFWS do not take the number of clusters as an input parameter, and we specified the same minimum support level for them, in the range of 5 X 15%, for a fair comparison. Based on the F-measures, it is clear that our CFWS algorithm consistently outperforms other two algo-rithms on the sets of typical text documents as well as on the sets of search query results. For CFWS, the overlapping threshold value  X  for the merging of clusters was 0.5, and the F-measure was not sensitive to  X  if it was higher than 0.5. The F-measure represents the clustering accuracy, and our CFWS algorithm has better F-measures because it uses a better model for text documents.

Table 1: F-measures of the clustering algorithms
In this paper, we proposed a new text document clus-tering algorithm, named CFWS. Unlike the traditional vec-tor space model, our model utilizes the frequent word se-quences to reduce the high dimensionality of the documents and to measure the closeness between documents. Moreover, CFWS provides overlapped clusters and self-explanatory la-bels of the clusters. Our experimental results show that CFWS performs better than other clustering algorithms in terms of accuracy.
