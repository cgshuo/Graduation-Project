 Recommender systems can help people nd the interesting and valuable infor-mation from books, articles, webpages, movies, music and so on. Collaborative ltering (CF) lters for information or patterns using techniques involving col-laboration among multiple agents, viewpoints, data sources, etc[1].
 it's seriously dependent on human ratings, performs decreasingly when data are sparse, and cannot recommend for new-coming users and items. The similarity measure in CF relies on ones with many common ratings and therefore it will become unreliable when data don't have enough common ratings, which causes performance degradation even failure. In reality, people usually have lots of data such as users' demographic data and behavior data, which can potentially be helpful for recommendations. This inspires us to consider integrating these non-rating data into CF framework to alleviate the diculty of CF in sparse-data situation while conventional CF cannot use these data.
 recommendations need to satisfy only one constraint, a wide range of recom-mendations are t, while if they need to satisfy many constraints, only some real ones could be t. More strict conditions can bring more accurate and in-tense recommendations, somehow because they could cost more consideration and time of human and hence show more values. Here, we take the data fea-tures of users and items as constraints of collaborate computation, which gives a strong representation of users' interests in items. It could be thought that users with more common relevant features have stronger common interests. This view is also consistent when it's applied to conventional CF. For conventional CF, if two users/items have more common ratings of user-items, their ratings to target items/users will be more similar. Simply, we generalize the rating-based similarity to multi-feature-based similarity.
 construct feature vectors for every user-item and the vectors are consisted of relevant features including ratings and other features instead of only ratings, then compute similarity between user-item vectors to obtain neighbors of the target user-item, and nally compute the ratings of the target users on the items. We use a prediction function to reweight the features in the vectors for improvement of the method. Moreover, we also employ a Bayesian view to construct a similarity comparison form between user-items. The results of the experiments demonstrate our methods perform much better than conventional CF.
 the related work about CF, especially CF for sparse data. In section 3, we present our methods in details. In section 4, we give the experiments and the related analyses. In the last section, we conclude our work. There have been a large number of studies on CF[3]. CF algorithms have per-formed well in a broad domain as recommendation technology[2]. GroupLens was a pioneering in collaborative ltering [4][5]. The GroupLens team initially implemented a neighborhood-based CF system for rating Usenet articles. They used a 1-5 integer rating scale and computed distance using Pearson correlations. Shardanand and Maes [6] designed a collaborative ltering system for music and experimented with a number of measures of distance between users, including Pearson correlation, constrained Pearson correlation, and vector cosine. They compared four di erent recommendation algorithms based on the Mean Abso-lute Error of predictions. All of their neighborhood-based algorithms require linear time of the number of users. Delgado [7] took an agent-based approach to CF, developing several algorithms that combined ratings data with other sources of information such as the geographic location of the user, and weighted majority voting was used to combine recommendations from di erent sources.
 CF[9]. Speci cally, when new users or items have come into the CF system, it's dicult to nd their similar neighbors because it lacks enough information, called cold start problem[3]. Herlocker et al. [10] studied small intersection sets for user-item matrix by reducing the weight of users with few common item-s. Chee et al. [11] constructed cliques of users who are approximately similar and extended users' rating history with the clique's averages. Breese et al. [12] assumed a neutral or somewhat negative preference for the unobserved ratings and then computed the users' similarity on the resulting ratings data. Principle Component Analysis (PCA)[13], and Latent Semantic Indexing (LSI) [14] were also proposed to address the sparsity problem by removing unrepresentative or insigni cant users or items through reducing the dimensionality of the user-item matrix.
 feature content of data, such as users' pro les, item descriptions and users' rating history, and nding regularities of data[3]. Content-based techniques have the limitation that users are recommended items similar to those that they already rated [15]. Many hybrid recommenders incorporating CF and content-based fea-tures are also proposed for avoiding limitations of either[3]. Content-boosted CF algorithm [16] uses external content information to produce predictions on non-rating data, and then provides personalized suggestions through CF. 3.1 CF and Features It is known that the fundamental assumption of conventional CF is that if users X and Y rate n items similarly, and hence they will rate on other items similarly. CF uses the ratings of these similar users to evaluate the target ratings. The basic formula is as follows: where u t denotes the target user, u i denotes any other user, r tt  X  and r it  X  de-note the rating of user u t and user u i on the target item t  X  , respectively. For the similarity computation sim ( u i ; u t ), conventional CF usually depends only on ratings. The similarity computation can also be explained in a conditional probabilistic prospect. In details, it is as follows: where n is the number of items, and r i 1 ; :::; r in represents all the ratings of probability with which u i has the rating of r tt  X  . This value is usually empirically computed by the similarity between users in practice since it is assumed that g () similarity function sim f unc (). From a wide view, P r ( r it  X  = r tt  X  | ::: ) can be computed integrating other signi cant features which are helpful for predicting r  X  . Or in other word, the function g () can be trained by all kinds of features involved in recommendation, not limiting to only ratings. Thus, sim f unc () can also be computed by other features besides rating.
 are correlated to the rating. That also inspires us to consider replacing ratings similarity with a combination of features and ratings. On one hand, the feature combination can work when the rating are missing, on the other hand, it can improve the representation for users' interests.
 creases as the number of their common related features increases. Let us give an example. When many users rate the same lm. some rate the lm high for its actor, some rate high for its director, and some rate high for its plot. All rate similar values for di erent reasons, so they may rate di erently on other lms. On the contrary, if people rate many lms similarly for the same reasons, they will rate another lm similarly more probably. This would be much more useful in sparse data situation where we can use feature information as the reason for choice instead of only ratings. 3.2 Vector-based Similarity A simple way is to compute the similarity of two vectors directly which consist of features and ratings. We choose the cosine similarity, which can support our assumption well, i.e., more similar features bring stronger relationship between users. In details, we can choose some plausible or relevant features, insert ratings and them into the vectors of user-item, and compute the similarity between the vectors. Its signi cance lies that it can compute the similarity between users using the features, even without the rating, in which situation we treat the rating value as 0. The formula is presented as follows: where f i  X  k and r i  X  denote a column vector which includes the k th features and the ratings for the user u i on all items, respectively. f t  X  k and r t  X  are used in a the user u i , and matrix mat ( f t  X  1 ; :::; f t  X  m ; r t  X  ) is used similarly. CF by t risk , and the risk of vector-based similarity CF by v risk . If all features have nothing to do with the rating and users' interests, v risk will be much bigger than t risk . If all features are equivalent to rating, v risk will be equal to t risk . If these features can help enhance the similarity of interests of users compared with only-rating, v risk will be smaller than t risk . If all features are equivalent to the real rating and the rating values are all missing, v risk will be reasonably small and t risk will be unable to compute. Moreover, if we think these features are constraints of interest similarity computation, we obtain smaller neighbor sets of target users than conventional CF, which will be easier to compute. and items to reduce the variance of similar users and interests. While compared with content-based recommenders, it can use CF technology to help predict the user's interests better. 3.3 Reweighting the Features In fact, the weight of each element in feature-rating vectors is di erent, and it is supposed to be set reasonable values for better recommendation performance. If we take the rating values as predicted results of a function involving the features, we can use the coecients of the function as the weight values. Here, linear functions should be a prior choice for their simplicity and directness for weight settings. For a classi cation problem, we can choose logistic regression to determine the weights of features and ratings, taking the probability of predicted ratings and the coecients of the feature variables as their own weights. For a regression problem, the option can be a linear function in which the coecients of the features are used as their weights directly. 3.4 A Bayesian View Then, we provide a Bayesian view to illustrate a new CF. The similarity between two users is represented as follows: where r it  X  denotes the rating for the user i on the target item t  X  . The rst equation above is obtained due to (2) by integrating the features f i  X  and f t  X  . r similarity between users only using the features with the same rating r it  X  . In another word, it's assumed that the features with ratings not equalling to r it  X  have no attribution to the similarity computation, because the features are not so closely related to r it  X  .
 between the target user and other users on each class, i.e., each rating label. Firstly, for each class, it selects the users whose rating on the target item belongs to the class. Then, it assigns 0 to the feature vectors of all selected users' items on which the rating has a di erent class label from the rating on the target item, and computes the similarity between these users and the target user. Finally, it uses the similarity and the rating of non-target users on the target item to compute the rating value of the target user on the target item. The similarity computation formula is further represented in a cosine similarity form: The formula can be computed easily. We performed two experiments to evaluate our methods. One experiment is for predicting repeat buyers, and the other experiment is for predicting movie ratings. 4.1 Predicting Repeat Buyers The data of this experiment was from Tmall IJCAI-15 Competition Dataset https://tianchi.shuju.aliyun.com/ ) which contained anonymous users' shop-ping logs in the past 6 months before and on the "Double 11" day, and the label information indicating whether they were repeat buyers. We preprocessed the source dataset simply, and obtained a dataset including 10000 users and 4993 items, which has sparse ratings with a sparse rate 0.158%. The dataset has user features including age and gene, item features including categories and brand-s, and user-item features including time-stamps, user behaviors(such like click, add-to-chart, purchase and add-to-favorite). The rating values are enumerated type 0, 1, where 1 means repeat buyer, and 0 means non-repeat buyer. sion CF (LRCF), vector CF (VCF), weighted vector CF (WVCF) and Bayesian CF (BCF) perform on the dataset. LRCF uses logistic regression to predict the missing ratings and then perform conventional CF. VCF denotes the vector based CF whose every element has a default weight 1.0. WVCF is another kind of VCF that we compute and set weight of every element of the vectors by logis-tic regression. BCF is the Bayesian multi-feature CF described in the previous sector. For this task, we use precision rate and recall rate as the test index, and perform a 10 repetitions to obtain the results.
 because the labels of the dataset are too sparse to calculate the similarity between users. All multi-feature CF perform well and obtain better results than LR. Speci cally, WVCF and BCF are the best methods on both precision and recall test on this dataset. WVCF behaves better than VCF because it sets a t weight to each feature. BCF obtained the best performance in all the methods, and it's also robust, suggested by the fact that the feature weights of BCF are all not man-made and equals to 1.0. 4.2 Predicting Movie Ratings In this experiment, we used a Yahoo! dataset[17] to test the methods. Our dataset has 1910 users and 2978 items, with a sparse rate less than 0.013%. The dataset includes movie user ratings of movies, user features including birthyear and gen-der, and item features including MPAA rating, release date, list of genres,list of directors, list of actors, average critic rating, the number of critic ratings, list of anonymized review owners, global non-personalized popularity and so on. To be noted, this dataset has a di erence with the dataset of last experiment that it does not have features related to user and item simultaneously. We select some features from the dataset for training a model. We compare CF only using rat-ings (i.e. conventional CF, called CF), CF only using features(FCF), CF using features and ratings(FRCF) and Bayesian CF(BCF), in order to testify whether multi-feature CF is better than CF. The results are showed in Table 2, and MAE and RMSE are used for error measurement.
 BCF achieves the best performance on this dataset. At the same time, FRCF with t weight values behaves better than CF and FCF.
 performance of FRCF, which is shown in Figure 1. We rst set a basic weight to each feature of FRCF according to the coecients in the regression models, and then use a series of values to multiply the basic weight. The results shows that the performance can be a ected much by the feature weights. In this paper, we present a new approach to use data features and ratings to-gether for CF. We set the weights of the features in CF by prediction models like logistic regression, and we also propose a Bayesian way to compute the similarity between users. Then, we perform two experiments to verify that our methods can behave better than conventional CF and other methods in the case of sparse data. In the future, we'll explore CF with complex features which can exist in many situations. The authors gratefully acknowledge the generous support from the Doctoral Fund of Shandong Jianzhu University (XNBS1527).

