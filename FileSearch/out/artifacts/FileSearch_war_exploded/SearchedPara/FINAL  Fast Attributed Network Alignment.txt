 H.2.8 [ Database Management ]: Database Applications X  Data Mining Attributed network alignment; Alignment consistency; On-query alignment
Multiple networks naturally appear in many high-impact application domains, ranging from computer vision, bioin-formatics, web mining, chemistry to social network analy-sis. More often than not, network alignment (i.e., to find node correspondence across different networks) is virtually the very first step for any data mining task in these appli-cations. For example, by linking users from different social network sites, we could recommend the products from one site (e.g., eBay ) to the users from another site (e.g., Face-book ) [29]. In bioinformatics, integrating different tissue-specific protein-protein interaction (PPI) networks has led to a significant improvement for candidate gene prioritiza-tion [16].
 Despite the extensive research on network alignment (see Section 6 for a review), most, if not all, of those work focus on inferring the node correspondence solely based on the topology . For instance, IsoRank [21] propagates pairwise topology similarities in the product graph. NetAlign uti-lizes max-product belief propagation based on the network topology [2]. BigAlign and UniAlign [11] aim to infer the soft alignment based on the assumption that the adjacency matrix of one network is a noisy permutation of another network. A fundamental assumption behind these existing methods is the topology consistency . That is, the same node has a consistent connectivity structure across different net-works (e.g., connecting to the same or similar set of the neighbors). However, such an assumption could be easily violated in some applications. For example, a user might be very active on one social network site (e.g., Facebook ), but behaves more quietly on another site (e.g., LinkedIn ) [11]; the same gene might exhibit dramatically different behav-iors across different tissue-specific PPI network [16]. In these cases, the topology-based methods could lead to sub-optimal or even misleading alignment results.

At the same time, many real networks often have rich accompanying attributes on the nodes and/or edges (e.g., demographic information of the users, the communication types between different users), which might provide a com-plementary solution to address the node topology consis-tency assumption violation. Nonetheless, it remains a daunt-ing task to align such attributed networks. To be spe-cific, the following questions have largely remained open. First ( Q1. Formulation ), it is not clear how to assimilate node/edge attribute information into the topology-based net-work alignment formulation. For instance, many topology-based alignment approaches can often be formulated from the optimization perspective, yet it is unknown what its attributed counterpart is. Second ( Q2. Algorithms ), the optimization problem behind the topology-based network alignment is often non-convex or even NP-hard. Introduc-ing attributes into the alignment process could only further complicate the corresponding optimization problem. How can we develop an effective solver for the attributed network alignment, with a similar or comparable time complexity to its topology-only counterpart? Third ( Q3. Scalable Compu-tation ), how can we scale up the attributed network align-ment process by taking advantage of some intrinsic proper-ties (e.g., low-rank) of real networks? For some applications (e.g., cross-network search), we might be interested in find-ing similar nodes across different networks (e.g., to find sim-ilar users on LinkedIn for a given user on Facebook ). How can we further speed up the computation for such an on-query attributed network alignment process, without solving the full alignment problem?
In this paper, we address the attributed network align-ment problem, aiming to answer all these questions. The main contributions of this paper are as follows: 1. Formulation. We formulate the attributed network 2. Algorithms and Analysis. We propose a family 3. Computations. We further develop (1) an approxi-4. Evaluations. We perform extensive experiments to
The rest of the paper is organized as follows. Section 2 defines the attributed network alignment problem and the on-query attributed network alignment problem. Section 3 presents the proposed optimization formulation of FINAL and its solutions. Section 4 proposes two speed-up meth-ods for approximate full alignment and on-query alignment. Section 5 presents the experimental results. Related work and conclusion are given in Section 6 and Section 7.
Table 1 summarizes the main symbols and notations used throughout the paper. We use bold uppercase letters for matrices (e.g., A ), bold lowercase letters for vectors (e.g., s ), and lowercase letters (e.g.,  X  ) for scalars. For matrix indexing, we use a convention similar to Matlab X  X  syntax as follows. We use A ( i,j ) to denote the entry at the intersec-tion of the i -th row and j -th column of matrix A , A ( i, :) to denote the i -th row of A and A (: ,j ) to denote the j -th column of A . We denote the transpose of a matrix by the superscript T (e.g., A T is the transpose of A ). We use on top to denote the symmetric normalization of a matrix (e.g., e A = D  X  1 / 2 AD  X  1 / 2 , where D is the degree matrix of A ). The vectorization of a matrix (in the column order) is denoted by vec( . ), and the resulting vector is denoted by the corresponding bold lowercase letter (e.g., a = vec( A )).
We represent an attributed network by a triplet: G = { A , N , E } , where (1) A is the adjacency matrix, and (2) N and E are the node attribute matrix and the edge attribute matrix, respectively 1 . The attribute of node-a corresponds to the value of N ( a,a ), and E ( a,b ) describes the edge at-tribute of the edge between node-a and node-b . For a given node attribute value k , we define N k as a diagonal matrix with the same size as N , where N k ( a,a ) = 1 if node-a has the attribute value k and N k ( a,a ) = 0 otherwise. For a given edge attribute value l , we define E l as a matrix of the same size with E , where E l ( a,b ) = 1 if edge ( a,b ) has the attribute value l and E l ( a,b ) = 0 otherwise.

Figure 1 presents an illustrative example. We can see from Figure 1(a), the set of nodes (2, 3, 4 and 5) from the first network share the exact same topology with another set of nodes (2 0 , 3 0 , 4 0 and 5 0 ). The topology alone would be inadequate to differentiate these two sets. On the other hand, we can see that (1) 2, 2 0 , 5 and 5 0 share the same node attribute value; (2) 3, 3 0 , 4 and 4 0 share the same node attribute value; and (3) the two edges incident to 3 share the same edge attribute value with those incident to 4 These node/edge attributes could provide vital information to establish the accurate node-level alignment (i.e., 2 aligns to 5 0 , 5 aligns to 2 0 , 3 aligns to 4 0 and 4 aligns to 3 is exactly what this paper aims to address. Formally, the attributed network alignment problem is defined as follows. Problem 1. Attributed Network Alignment .

Given: (1) two attributed networks G 1 = { A 1 , N 1 , E
In this paper, we use  X  X raph X  and  X  X etwork X  interchangeably, and  X (categorical) attribute X  and  X  X abel X  interchangeably. the desired alignment output (denoted by the red dashed lines). and G 2 = { A 2 , N 2 , E 2 } with n 1 and n 2 nodes respectively, (2 -optional) a prior alignment preference H .
 Output: the n 2  X  n 1 alignment/similarity matrix S , where S ( x,a ) represents the alignment/similarity between node-a in G 1 and node-x in G 2 .

In the above definition, we have an optional input, to en-code the prior knowledge of pairwise alignment preference H , which is an n 2  X  n 1 matrix. An entry in H reflects our prior knowledge of the likelihood to align two corre-sponding nodes across the two input networks. When such prior knowledge is absent, we set all entries of H equal, i.e., a uniform distribution. Without loss of generality, we as-sume that A 1 and A 2 share a comparable size, i.e., O ( n also help simplify the complexity analysis in the next two sections.

Notice that the alignment matrix S in Problem 1 is es-sentially a cross-network node similarity matrix. In some applications, we might be interested in finding a small num-ber of similar nodes in one network w.r.t a query node from the other network. For instance, we might want to find the top-10 most similar LinkedIn users for a given Face-book user. We could first solve Problem 1 and then return the corresponding row or column in the alignment matrix S , which might be computationally too costly as well as unnec-essary. Having this in mind, we further define the on-query attributed network alignment problem as follows:
Problem 2. On-Query Attributed Network Align-ment.

Given: (1) two attributed networks G 1 = { A 1 , N 1 , E and G 2 = { A 2 , N 2 , E 2 } , (2 -optional) a prior alignment preference H , (3) a query node-a in G 1 .

Output: an n 2  X  1 vector s a measuring similarities be-tween the query node-a in G 1 and all the nodes in G ciently.
In this section, we present our solutions for Problem 1. We start by formulating Problem 1 as a regularized optimization problem, and then propose effective algorithms to solve it, followed by some theoretic analysis.
The key idea behind our proposed formulation lies in the alignment consistency principle, which basically says that the alignments between two pairs of nodes across the two input networks should be consistent if these two pairs of nodes themselves are  X  X imilar/consistent X  with each other. Let us elaborate this using the following example. In Fig-ure 2, we are given two pairs of nodes: (1) node-a in G node-x in G 2 ; and (2) node-b in G 1 and node-y in G 2 alignment consistency principle, we require the alignment between a and x , and that between b and y to be consis-conditions hold, including
C1 Topology Consistency . a and b are close neighbors in G
C2 Node Attribute Consistency . a and x share the same
C3 Edge Attribute Consistency . Edge ( a,b ) and ( x,y )
The intuition behind the alignment consistency principle is as follows. If we already know that node-a is aligned to node-x (i.e., large S ( x,a )), then their close neighbors (e.g., b and y ) with same node attribute value should have a high chance to be aligned with each other (i.e., large S ( y,b )), where we say that b and y are close neighbors of a and y respectively if they are connected by the same edge at-tribute value, with large edge weights (i.e., large A and A 2 ( x,y )). This naturally leads to the following objec-tive function which we wish to minimize in terms of the alignment matrix S : J ( S ) = X where (1) a,b = 1 ,...,n 1 , and x,y = 1 ,...,n 2 ; (2) 1 (  X  ) is the indicator function, which takes 1 if the condition inside the parenthesis is true and zero otherwise; and (3) f (  X  ) is a node-pair normalization function that is defined as The function f ( x,a ) measures how many (weighted) neighbor-pairs a and x have that (1) share the same node attribute value between themselves (e.g., b and y ), and (2) connect to a and x via the same edge attribute value, respectively.
Notice that the indicator function 1 (  X  ) reflects whether the two input nodes/edges share the same attribute value 2 we factorize it as follows Substitute Eq. (3) into Eq. (1) and Eq. (2), we have and for nodes with the same attribute value, i.e., N 1 ( a,a ) = f ( x,a ) = X
Next, we present an equivalent matrix form of J 1 , which is more convenient for the following algorithm description and the theoretic proof. By vectorizing the alignment matrix S (i.e., s = vec( S )), and with the notation of element-wise product and Kronecker product, Eq. (1) can be re-written as where v = n 2 ( a  X  1) + x , w = n 2 ( b  X  1) + y , N = P N 2 , E = P L l =1 E l 1  X  E l 2 and W = N [ E ( A 1  X  A 2 )] N . f W = D  X  1 2 WD  X  1 2 is the symmetric normalized matrix of W . The diagonal degree matrix D of W is defined as Note that some diagonal elements in D could be zero (e.g., D ( v,v ) = 0). For such elements, we define the correspond-ing D ( v,v )  X  1 / 2 , 0.

Putting everything together, our proposed optimization problem can be stated as follows. where k X k F denotes the Frobenius norm, and  X  is the reg-ularization parameter. Notice that compared with J 1 have an additional regularization term k s  X  h k 2 F to reflect the prior alignment preference, where h = vec( H ). When no such prior information is given, we set h as a uniform column vector. From the optimization perspective, this ad-ditional regularization term would also help prevent a trivial solution of J 1 with a zero alignment matrix S .
The objective function in Eq. (7) is essentially a quadratic function w.r.t. s . We seek to find its fixed-point solution by setting its derivative to be zero
We remark that by replacing the indicator function 1 (  X  ) by an attribute value similarity function, the proposed formu-lation can be naturally generalized to handle the numerical attributes on nodes and/or edges.
 which leads to the following equation We could directly develop an iterative algorithm based on Eq. (8). However, such an iterative procedure involves the Kronecker product between A 1 and A 2 whose time com-plexity is O ( m 2 ).

In order to develop a more efficient algorithm, thanks to a key Kronecker product property (i.e., vec( ABC ) = ( C T A )vec( B )), we re-write Eq. (8) as follows where Q is an n 2  X  n 1 matrix reshaped by q = ND  X  1 2 s in column order, i.e., Q = mat( q ,n 2 ,n 1 ). We can show that Eq. (8) and Eq. (9) are equivalent with each other (detailed proofs are omitted due to space). The advantage of Eq. (9) is that it avoids the expensive matrix Kronecker product, which leads to a more efficient iterative algorithm FINAL-NE (summarized in Algorithm 1).
 Algorithm 1 FINAL-NE : Attributed Network Alignment. 1: Construct degree matrix D and node attribute matrix N ; 2: Initiate the alignment s = h = vec( H ), and t = 1; 7: for l = 1  X  L do 9: end for 11: Set t  X  t + 1; 12: end while
Our proposed FINAL-NE algorithm assumes that the in-put networks have both node and edge attributes. It is worth pointing out that it also works when the node and/or the edge attribute information is missing.

First, when only node attributes are available, we can set all entries in the edge attribute matrix E to 1 where an edge indeed exists. The intuition is that we treat all the edges in the networks to share a common edge attribute value. In this case, the fixed-point solution in Eq. (8) becomes notes the degree matrix of W N . Similar to Eq. (9), we can use the vectorization operator to accelerate the computa-tion. We refer to this variant as FINAL-N , and omit the detailed algorithm description due to space.

Second, when only the edge attributes are available, we treat all nodes to share one common node attribute value by setting N to be an identity matrix. In this case, the fixed-point solution in Eq. (8) becomes we omit the detailed algorithm description due to space, and refer to this variant as FINAL-E .

Finally, if neither the node attributes nor the edge at-tributes are available, Eq. (8) degenerates to where D U = D 1  X  D 2 , D 1 and D 2 are the degree matrix of A 1 and A 2 respectively. This variant is referred to as FINAL-P .
In this subsection, we first analyze the convergency , the optimality and the complexity of our FINAL algorithms. Due to the space limit, we only present the results for the most general case (i.e., FINAL-NE ). Then we analyze the relationships between FINAL and several classic graph min-ing problems.
 We start with Theorem 1, which says the proposed FINAL-NE algorithm converges to the global optimal solution of Eq. (7).
 Theorem 1. Convergency and Optimality of FINAL-NE . Algorithm 1 converges to the closed-form global mini-mal solution of J ( s ) : s = (1  X   X  )( I  X   X  f W )  X  1 h .
Proof. To prove the convergency of Eq. (8), we first show the eigenvalues of  X  f W are in (  X  1 , 1). Since similar to the stochastic matrix WD  X  1 = D 1 2 f WD  X  1 2 eigenvalues of f W are within [  X  1 , 1]. Since 0 &lt;  X  &lt; 1, the eigenvalues of  X  f W are in (  X  1 , 1).

We denote the alignment vector s in the t -th iteration as s ( t ) . We have that
Since the eigenvalues of  X  f W are in (  X  1 , 1), we have that these together, we have that
Next, we prove that the above result is indeed the global minimal solution of the objective function defined in Eq. (7). We prove this by showing that J ( s ) in Eq. (7) is convex. To see this, we have that the Hessian matrix of Eq. (7) is O
J = 2( I  X   X  f W ). By the Weyl X  X  inequality theorem [4], all eigenvalues of 2( I  X   X  f W ) are greater than 0. In other words, we have that O 2 J is positive definite. Therefore, the objective function defined in Eq. (7) is convex, and its fixed-point solution by Algorithm 1 corresponds to its global minimal solution, which completes the proof.

The time and space complexity of Algorithm 1 are summa-rized in Lemma 1. Notice that such a complexity is compa-rable to the complexity of topology-alone network alignment methods, such as IsoRank [21]. In the next section, we will propose an even faster algorithm.

Lemma 1. Complexity of FINAL-NE . The time com-plexity of Algorithm 1 is O ( Lmnt max + LK 2 n 2 ) , and its space complexity is O ( n 2 ) . Here, n and m are the orders of the number of nodes and edges of the input networks, re-spectively; K,L denote the number of unique node and edge attributes respectively, and t max is the maximum iteration number.
 Proof. Omitted for space.
 Finally, we analyze the relationships between the proposed FINAL algorithms and several classic graph mining prob-lems. Due to the space limit, we omit the detailed proofs and summarize the major findings as follows.

An important (single) network mining task is the node proximity, i.e., to measure the proximity/similarity between two nodes on the same network. By ignoring the node/edge attributes and setting A 1 = A 2 , our FINAL algorithms, up to a scaling operation D 1 / 2 , degenerate to SimRank [12] -a prevalent choice for node proximity. Our FINAL algorithms are also closely related to another popular node proximity method, random walk with restart [24]. That is, Eq. (8) can be viewed as random walk with restart on the attributed Kronecker graph with h being the starting vector. Note that neither the standard SimRank nor random walk with restart considers the node or edge attribute information.

The alignment result s by our FINAL algorithms is closely related to the random walk based graph kernel [25]. To be specific, if k ( G 1 , G 2 ) is the random walk graph kernel between the two input graphs and p is the stopping vector, we can show that k ( G 1 , G 2 ) = p T s . This intuitively makes sense, as we can view the graph kernel/similarity as the weighted aggregation (by the stopping vector p ) over the pairwise cross-network node similarities (encoded by the alignment vector s ). We also remark that in the original random walk graph kernel [25], it mainly focuses on the edge attribute information.

If we ignore all the node and edge attribute information, our FINAL -P algorithm is equivalent to IsoRank [21] by scaling the alignment result and alignment preference by D 1 / 2 . We would like to point out that such a scaling opera-tion is important to ensure the convergence of the iterative procedure. Recall that the key idea behind our optimiza-tion formulation is the alignment consistency . When the attribute information is absent, the alignment consistency principle is closely related to the concept of  X  X quares X  be-hind NetAlign algorithm [2]. Like most, if not all of the, existing network alignment algorithms, the node or the edge attribute information is ignored in IsoRank and NetAlign .
We remark that these findings are important in the follow-ing two aspects. First, they help establish a quantitative re-lationship between several, seemingly unrelated graph min-ing problems, which might in turn help better understand these existing graph mining problems. Second, these find-ings also have an important algorithmic implication. Take SimRank as an example, it was originally designed for plain graphs (i.e., without attributes), and was formulated from random walk perspective and it is not clear what the algo-rithm tries to optimize. By setting G 1 = G 2 and ignoring the attribute information, our objective function in Eq. (7) provides a natural way to interpret SimRank from an opti-mization perspective. By setting G 1 = G 2 alone (i.e., keeping the attribute information), our FINAL algorithms can be di-rectly used to measure node proximity on an attributed net-work. Finally, our upcoming FINAL On-Query algorithm also naturally provides an efficient way (i.e., with a linear time complexity) for on-query SimRank with or without at-tribute information (i.e., finding the similarity between a given query node and all the remaining nodes in the same network).
In this section, we address the computational issue. To be specific, we will focus on two scenarios. First, to solve Prob-lem 1, our proposed FINAL algorithms in Section 3 have a time complexity of O ( mn ), where we have dropped the lower order terms. We will propose an effective approximate algo-rithm that reduces the time complexity to O ( n 2 ). Second, for Problem 2, solving the full alignment problem not only still requires O ( n 2 ) time, but also is unnecessary, as we es-sentially only need a column or a row from the alignment matrix S . To address this issue, we will propose an effective algorithm for Problem 2 with a linear time complexity. For presentation clarity, we restrict ourselves to the case where there is only node attribute information, although our pro-posed strategies can be naturally applied to the more general case where we have both node and edge attributes. According to Theorem 1, the alignment vector s in FINAL-N converges to its closed-form solution as follows.
The key idea to speed up FINAL-N is to efficiently ap-proximate such a closed-form solution. To be specific, we first approximate the two adjacency matrices by top-r eigen-value decomposition: A 1 = U 1  X  1 U T 1 and A 2 = U 2  X  2 Then the rank-r approximation of W N can be defined as follows
Substitute Eq. (14) into Eq. (13), we can approximate the alignment vector s as where U = U 1  X  U 2 , and  X  is an r 2  X  r 2 matrix computed by Sherman-Morrison Lemma [18]:  X  = [(  X  1  X   X  2 )  X  1
Based on Eq. (15), our proposed FINAL-N+ algorithm is summarized in Algorithm 2. The time complexity of FINAL-N+ is summarized in Lemma 2. Notice that we often have r n , m n 2 and K n . Therefore, com-pared with FINAL-N , FINAL-N+ is much more efficient in time complexity.
 Lemma 2. Time Complexity of FINAL-N+ . FINAL-N+ takes O ( n 2 r 4 + Kn 2 ) in time ,where n is the order of the number of nodes, r is the rank of eigenvalue decomposition and K is the number of node attributes.
 Proof. Omitted for space.
 Algorithm 2 FINAL-N+ : Low-Rank Approximation of FINAL-N .
 1: Construct degree matrix D N and node attribute matrix N ; 2: Construct alignment preference vector h = vec( H ); 6: Compute s by Eq. (15) ;
In Problem 2, we want to find an n 2  X  1 vector s a which measures the similarities between the query node-a in G 1 all the n 2 nodes in G 2 (i.e., cross-network similarity search). It is easy to see that s a is essentially the a -th column of the alignment matrix S , or equivalently a certain portion of the alignment vector s , i.e., where v = ( a  X  1) n 2 + 1 and w = an 2 .

However, if we call FINAL-N or FINAL-N+ to find S (or s ) and then return the ranking vector s a , it would take at least O ( n 2 ) time. Next, we propose an approximate algo-rithm ( FINAL On-Query ) which directly finds the ranking vector s a in linear time, without solving the full alignment matrix S .

We first relax the degree matrix D N to its upper-bound  X  D
N = D 1  X  D 2 . There are two reasons for taking such a relaxation. First, it would take O ( n 2 ) time to compute the D
N matrix directly. On the other hand,  X  D N can be indi-rectly expressed by the Kronecker product between D 1 and D 2 , each of which only takes O ( m ) time. Second, since is an upper-bound of the D N matrix, such a relaxation will not affect the convergence of FINAL-N . By this relaxation, the fixed-point solution in Eq. (10) can be approximated as where  X  D N = D 1  X  D 2 .

By a similar procedure in FINAL-N+ , the low-rank ap-proximate solution for s is where  X   X  = [(  X  1  X   X  2 )  X  1  X   X  U T N  X  D  X  1 N U ]  X  1
Since both  X  D N and N are diagonal matrices, the ranking vector for node-a is
Notice that Eq. (18) still needs O ( n 2 ) time due to the last two terms. We reduce the time cost for computing g = U decomposition (SVD) on H , i.e., H = P p i =1  X  i u i v T by the vectorization operator, we have that We can see that the time cost for Eq. (19) is reduced to O ( pKrn ), which is linear w.r.t the number of nodes n .
We reduce the time cost for computing  X   X  by reformulating as follows, whose time complexity is O ( Knr 2 + Kr 4 + r
Putting everything together, the ranking vector of node-a now becomes
Based on Eq. (21), our proposed FINAL On-Query algo-rithm is summarized in Algorithm 3. The time complexity of
FINAL On-Query is summarized in Lemma 3. Notice that we often have r,p n , m H m n 2 and K n .
 FINAL On-Query has a linear time complexity w.r.t the size of the input network, which is much more scalable than both FINAL-N and FINAL-N+ .
 Algorithm 3 FINAL On-Query : Approximate On-Query Algorithm for Node Attributed Networks.
 1: Compute degree matrices D 1 and D 2 ; 6: Compute g by Eq. (19) ; 7: Compute  X   X  by Eq. (20) ; 8: Compute s a by Eq. (21) .
 Lemma 3. Time complexity of FINAL On-Query.
 The time complexity of FINAL On-Query is O ( r 6 + mr + nr 2 + m H p + np 2 + Knr 2 + Kr 4 + pKnr ) . where n,m are the orders of the number of nodes and edges respectively, r,p is the rank of eigenvalue decomposition and SVD, respectively, K is the number of node attributes and m H is the number of non-zero elements in H .

Proof. Omitted for brevity.
In this section, we present the experimental results and analysis of our proposed algorithms FINAL . The experi-ments are designed to evaluate the following aspects: Datasets. We evaluate our proposed algorithms on six real-world attributed networks.
Based on these datasets, we construct the following five alignment scenarios for evaluations.
 S1. DBLP vs. DBLP. We extract a subnetwork with 9,143 users/nodes from the original dataset, together with their publications in each conference. We randomly permute this subnetwork with noisy edge weights and treat it as the sec-ond network. We choose the most active conference of a given author as the node attribute, i.e., the conference with the most publications. We construct the prior alignment preference H based on the node degree similarity. For this scenario, the prior alignment matrix H alone leads to a very poor alignment result, with only 0 . 6% one-to-one alignment accuracy.
 S2. Douban Online vs. Douban Offline. We construct an alignment scenario for Douban dataset in the same way as [31]. We construct the offline network according to users X  co-occurrence in social gatherings. We treat people as (1)  X  X ontacts X  of each other if they participate in the same of-fline events more than ten times but less than twenty times, and (2)  X  X riends X  if they co-participate in more than twenty social gatherings. The constructed offline network has 1,118 users and we extract a subnetwork with 3,906 nodes from the provided online network that contains all these offline users. We treat the location of a user as the node attribute, and  X  X ontacts X / X  X riends X  as the edge attribute. We use the degree similarity to construct the prior alignment prefer-ence H . The prior alignment matrix H alone leads to 7 . 07% one-to-one alignment accuracy.
 S3. Flickr vs. Lastfm. We have the partial ground-truth alignment for these two datasets [30]. We extract the sub-networks from them that contain the given ground-truth nodes. The two subnetworks have 12,974 nodes and 15,436 nodes, respectively. We consider the gender of a user as node attribute. For those users with the missing informa-tion of gender, we treat them as  X  X nknown X . Same as [30], we sort nodes by their pagerank scores and label 1% high-est nodes as  X  X pinion leaders X , the next 10% nodes as  X  X iddle class X  and remaining nodes as  X  X rdinary users X . Edges are at-tributed by the level of people they connect to (e.g., leader with leader). We use the username similarity as the prior alignment preference by the Jaro-Winkler distance [6]. The username similarity alone can correctly align 61 . 50% users. S4. Flickr-Myspace. Same as S3 , we have the partial ground-truth alignment for these two datasets. We extract two subnetworks that contain these ground-truth nodes. The subnetwork of Flickr has 6,714 nodes and the subnetwork of Myspace has 10,733 nodes. We use the same way as S3 for node attributes, edge attributes and the prior align-ment preference. The username similarity alone can cor-rectly align 61 . 80% users.
 S5. ArnetMiner-ArnetMiner. We use the same method as S1 to construct the alignment scenario as well as the prior alignment preference. This scenario contains the largest net-works, and therefore is used for efficiency evaluations. Comparison Methods. For the proposed FINAL algo-rithms, we test the following variants, including (1) FINAL-NE with both node and edge attributes; (2) FINAL-N with node attributes only; (3) FINAL-E with edge attributes only; (4) FINAL-N+ , a low-rank based approximate al-gorithm of FINAL-N . We compare them with the follow-ing existing network alignment algorithms including (1) Iso-Rank [21], (2) NetAlign [2], (3) UniAlign [11] and (4) Klau X  X  Algorithm [2, 9].
 Machines and Repeatability. All experiments are per-formed on a Windows machine with four 3.6GHz Intel Cores and 32G RAM. The algorithms are programmed with MAT-LAB using a single thread. We intend to release the source code as well as all the non-proprietary datasets after the paper is published.
We first evaluate the impact of the permutation noise on the alignment accuracy. We use a heuristic greedy match-ing algorithm [10] as a post-processing step on the similarity matrix to obtain the one-to-one alignments between the two input networks, and then compute the alignment accuracy with respect to the ground-truth. The results are summa-rized in Figure 3. We have the following observations. First, all of our proposed methods outperform the three exist-ing alignment methods. Specifically, FINAL-NE achieves a 20%-30% improvement in terms of the alignment accu-racy over the existing methods (i.e., IsoRank , NetAlign and UniAlign ). Second, FINAL-N and FINAL-E both outper-form the existing methods, yet are not as good as FINAL-NE , suggesting that node attributes and edge attributes might be complementary in terms of improving the align-ment accuracy. Third, the alignment accuracy of FINAL-N+ is very close to its exact counterpart FINAL-N (i.e., with a 95% accuracy compared with FINAL-N ). Fourth, by jointly considering the attributes and the topology of net-works, our methods are more resilient to the permutation noise. Finally, for the two networks whose topologies are dramatically different from each other (e.g., Douban online-offline networks), the accuracy gap between FINAL-N+ and Figure 3: (Higher is better.) Alignment accuracy vs. the noise level in networks. ( t max = 30, r = 5).
 Figure 4: (Higher is better.) Alignment accuracy vs. the noise level in H . (  X  = 0 . 3, t max = 30, r = 5). the existing methods is even bigger (Figure 3(b)). This is be-cause in this case, the topology information alone ( IsoRank , NetAlign and Klau ) could actually mislead the alignment process.

Second, we evaluate the impact of the noise in the prior alignment preference (i.e., H ) on the alignment results, which is summarized in Figure 4. As expected, a higher noise in H has more negative impacts on the alignment accuracy for most of the methods. Nonetheless, our FINAL algorithms still consistently outperform all other four existing methods across different noise levels. Figure 5: Balance between the accuracy and the speed. t max = 30, r = 5.
 Quality-Speed Trade-off. We first evaluate how different methods balance the alignment accuracy and the running time for the full network alignment problem (i.e., Prob-lem 1). The results are summarized in Figure 5. As we can see, the running time of our proposed exact methods (e.g., FINAL-N , FINAL-E ) is only slightly higher than its topology-alone counterpart (i.e., IsoRank ), and in the meanwhile, they all achieve a 10%-20% accuracy improve-ment. FINAL-N+ and UniAlign are the fastest, yet the proposed FINAL-N+ produces a much higher alignment
Figure 6: Alignment accu-racy vs. running time for on-query alignment.
 accuracy. NetAlign takes the longest running time as it in-volves a time-consuming, greedy/max-weight matching pro-cess during each iteration. We do not show the balance of Klau X  X  Algorithm because the running time is usually several hours which is not comparable with other methods. Overall, FINAL-N+ achieves the best trade-off between the align-ment accuracy and the running time.

Second, we evaluate the quality-speed trade-off for on-query alignment problem (i.e., Problem 2). Here, we treat the top-10 ranking results by FINAL-N as the ground-truth, and compare the average ranking accuracy of 500 random nodes with two proposed approximate algorithms ( FINAL-N+ and FINAL On-Query ). The results are summarized in Figure 6. We can see, that (1) FINAL-N+ preserves a 95% ranking accuracy, with a more than 10  X  speedup over FINAL-N , (2) FINAL On-Query preserves an about 90% ranking accuracy, and it is 100  X  faster than the exact FINAL-N .
 Scalability. We first evaluate the scalability of FINAL-N+ , which is summarized in Figure 7. We can see that the running time is quadratic w.r.t the number of nodes of the input networks, which is consistent with the time complexity results in Lemma 2. Second, we evaluate the scalability of FINAL On-Query , for both its pre-compute phase and online-query phase. As we can see from Figure 8, the running time is linear w.r.t the number of nodes in both stages, which is consistent with Lemma 3. In addition, the actual online-query time on the entire ArnetMiner data set (with r = 10) is less than 1 second, suggesting that the proposed FINAL On-Query method might be well suitable for the real-time query response.

The network alignment has attracted lots of research in-terests with extensive literatures. It appears in numerous domains, ranging from database schema matching [15], bioin-formatics [21, 13, 8], chemistry [22], computer vision [7, 20], to data mining [11, 2].
 A classic alignment approach can be attributed to Iso-Rank algorithm [21], which is in turn inspired by PageRank [17]. The original IsoRank algorithm propagates the pair-wise node similarity in the Kronecker product graph. Sev-eral approximate algorithms have been proposed to speed up its computation. Kollias et al. [10] propose an efficient method based on uncoupling and decomposition. SpaIso-Rank modifies IsoRank to maximize the number of  X  X quares X  for sparse networks [2]. In addition, IsoRankN [13] extends the original IsoRank algorithm and uses a similar approach as PageRank-Nibble [1] to align multiple networks.

Bayati et al. [3] propose a maximum weight matching al-gorithm for graph alignment using the max-product belief propagation [26]. Bradde et al. [5] propose another dis-tributed message-passing algorithm based on belief prop-agation for protein-protein interaction network alignment. Recently, NetAlign [2] is proposed by formulating the net-work alignment problem as an integer quadratic program-ing problem to maximize the number of  X  X quares X . A near-optimal solution is obtained by finding the maximum a pos-teriori (MAP) assignment using message-passing approach. BigAlign formulates the bipartite network alignment prob-lem and uses the alternating projected gradient descent to solve it [11]. Zhang et al. solve the multiple anonymized network alignment in two steps, i.e., unsupervised anchor link inference and transitive multi-network matching [28].
A related problem is to identify users from multiple so-cial networks (i.e., the cross-site user identification prob-lem). Zafarani et al. identify users by modeling user be-havior patterns based on human limitations, exogenous and endogenous factors [27]. Tan et al. [23] propose a subspace learning method, which models user relationship by a hy-pergraph. Liu et al. propose a method to identify same users by behavior modeling, structure consistency model-ing and learning by multi-objective optimization [14]. COS-NET [30] considers both local the global consistency and uses an energy-based model to find connections among mul-tiple heterogeneous networks.
In this paper, we study the attributed network alignment problem, including the full alignment version as well as its on-query variant. We propose an optimization-based for-mulation based on the alignment consistency principle. We propose a family of effective and efficient algorithms to solve the attributed network alignment problem. In detail, we first propose exact algorithms ( FINAL ) which are proved to converge to the global optima, with a comparable com-plexity with their topology-alone counterparts. We then propose (1) an approximate alignment algorithm ( FINAL-N+ ) to further reduce the time complexity; and (2) an ef-fective alignment algorithm ( FINAL On-Query ) to solve the on-query network alignment problem with a linear time complexity. We conduct extensive empirical evaluations on real networks, which demonstrate that (1) by assimilating the attribute information during the alignment process, the proposed FINAL algorithms significantly improve the align-ment accuracy by up to 30% over the existing methods; (2) the proposed approximate alignment algorithm ( FINAL-N+ ) achieves a good balance between the running time and the alignment accuracy; and (3) the proposed on-query alignment algorithm ( FINAL On-Query ) (a) preserves an around 90%+ ranking accuracy, (b) scales linearly w.r.t the size of the input networks, and (c) responds in near real time. Future work includes generalizing FINAL algorithms to handle dynamic networks. This work is partially supported by the National Science Foundation under Grant No. IIS1017415, by DTRA under the grant number HDTRA1-16-0017, by Army Research Of-fice under the contract number W911NF-16-1-0168, by Na-tional Institutes of Health under the grant number R01LM 011986, Region II University Transportation Center under the project number 49997-33 25 and a Baidu gift. We would like to sincerely thank Dr. Jie Tang and Dr. Yutao Zhang for their generosity to share the datasets, and anonymous reviewers for their insightful and constructive comments. [1] R. Andersen, F. Chung, and K. Lang. Local graph [2] M. Bayati, M. Gerritsen, D. F. Gleich, A. Saberi, and [3] M. Bayati, D. Shah, and M. Sharma. Maximum [4] R. Bhatia. Linear algebra to quantum cohomology: [5] S. Bradde, A. Braunstein, H. Mahmoudi, F. Tria, [6] W. Cohen, P. Ravikumar, and S. Fienberg. A [7] D. Conte, P. Foggia, C. Sansone, and M. Vento. [8] S. Hashemifar and J. Xu. Hubalign: an accurate and [9] G. W. Klau. A new graph-based method for pairwise [10] G. Kollias, S. Mohammadi, and A. Grama. Network [11] D. Koutra, H. Tong, and D. Lubensky. Big-align: Fast [12] C. Li, J. Han, G. He, X. Jin, Y. Sun, Y. Yu, and [13] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger. [14] S. Liu, S. Wang, F. Zhu, J. Zhang, and R. Krishnan. [15] S. Melnik, H. Garcia-Molina, and E. Rahm. Similarity [16] J. Ni, H. Tong, W. Fan, and X. Zhang. Inside the [17] L. Page, S. Brin, R. Motwani, and T. Winograd. The [18] W. W. Piegorsch and G. Casella. Erratum: inverting a [19] A. Prado, M. Plantevit, C. Robardet, and J.-F. [20] C. Schellewald and C. Schn  X  orr. Probabilistic subgraph [21] R. Singh, J. Xu, and B. Berger. Global alignment of [22] A. Smalter, J. Huan, and G. Lushington. Gpm: A [23] S. Tan, Z. Guan, D. Cai, X. Qin, J. Bu, and C. Chen. [24] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast random [25] S. V. N. Vishwanathan, N. N. Schraudolph, [26] J. S. Yedidia, W. T. Freeman, and Y. Weiss.
 [27] R. Zafarani and H. Liu. Connecting users across social [28] J. Zhang and S. Y. Philip. Multiple anonymized social [29] Y. Zhang. Browser-oriented universal cross-site [30] Y. Zhang, J. Tang, Z. Yang, J. Pei, and P. S. Yu. [31] E. Zhong, W. Fan, J. Wang, L. Xiao, and Y. Li.
