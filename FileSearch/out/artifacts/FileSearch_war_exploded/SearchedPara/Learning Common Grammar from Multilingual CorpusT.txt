 Languages share certain common proper-ties (Pinker, 1994). For example, the word order in most European languages is subject-verb-object (SVO), and some words with similar forms are used with similar meanings in different languages. The reasons for these common properties can be attributed to: 1) a common ancestor language, 2) borrowing from nearby languages, and 3) the innate abilities of humans (Chomsky, 1965).
We assume hidden commonalities in syntax across languages, and try to extract a common grammar from non-parallel multilingual corpora. For this purpose, we propose a generative model for multilingual grammars that is learned in an unsupervised fashion. There are some computa-tional models for capturing commonalities at the phoneme and word level (Oakes, 2000; Bouchard-C  X  ot  X  e et al., 2008), but, as far as we know, no at-tempt has been made to extract commonalities in syntax level from non-parallel and non-annotated multilingual corpora.

In our scenario, we use probabilistic context-free grammars (PCFGs) as our monolingual gram-mar model. We assume that a PCFG for each language is generated from a general model that are common across languages, and each sentence in multilingual corpora is generated from the lan-guage dependent PCFG. The inference of the gen-eral model as well as the multilingual PCFGs can be performed by using a variational method for efficiency. Our approach is based on a Bayesian multitask learning framework (Yu et al., 2005; Daum  X  e III, 2009). Hierarchical Bayesian model-ing provides a natural way of obtaining a joint reg-ularization for individual models by assuming that the model parameters are drawn from a common prior distribution (Yu et al., 2005). The unsupervised grammar induction task has been extensively studied (Carroll and Charniak, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Klein and Manning, 2004; Liang et al., 2007). Recently, models have been pro-posed that outperform PCFG in the grammar in-duction task (Klein and Manning, 2002; Klein and Manning, 2004). We used PCFG as a first step for capturing commonalities in syntax across lan-guages because of its simplicity. The proposed framework can be used for probabilistic grammar models other than PCFG.

Grammar induction using bilingual parallel cor-pora has been studied mainly in machine transla-tion research (Wu, 1997; Melamed, 2003; Eisner, 2003; Chiang, 2005; Blunsom et al., 2009; Sny-der et al., 2009). These methods require sentence-aligned parallel data, which can be costly to obtain and difficult to scale to many languages. On the other hand, our model does not require sentences to be aligned. Moreover, since the complexity of our model increases linearly with the number of languages, our model is easily applicable to cor-pora of more than two languages, as we will show in the experiments. To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. In con-trast, our method does not require any such prior information. 3.1 Model Let X = { X l } l  X  L be a non-parallel and non-annotated multilingual corpus, where X l is a set of sentences in language l , and L is a set of lan-guages. The task is to learn multilingual PCFGs G = { G l } l  X  L and a common grammar that gen-erates these PCFGs. Here, G l = ( K , W l ,  X  l ) represents a PCFG of language l , where K is a set of nonterminals, W l is a set of terminals, and  X  l is a set of rule probabilities. Note that a set of nonterminals K is shared among languages, but a set of terminals W l and rule probabilities  X  l are specific to the language. For simplicity, we consider Chomsky normal form grammars, which have two types of rules: emissions rewrite a non-terminal as a terminal A  X  w , and binary pro-ductions rewrite a nonterminal as two nontermi-nals A  X  BC , where A,B,C  X  K and w  X  W l . The rule probabilities for each nonterminal A of PCFG G l in language l consist of: 1)  X  sent probabilities of choosing the emission rule and the binary production rule, respectively, 2)  X  sents the probability of nonterminal production A  X  BC , and 3)  X  lA = {  X  lAw } w  X  W  X  lAw represents the probability of terminal emis-sion A  X  w . Note that  X  lA 0 +  X  lA 1 = 1 ,  X  lAt  X  0 ,  X  and  X  lAw  X  0 . In the proposed model, multino-mial parameters  X  lA and  X  lA are generated from Dirichlet distributions that are common across lan-guages:  X  lA  X  Dir(  X   X  A ) and  X  lA  X  Dir(  X   X  A ) , since we assume that languages share a common syntax structure.  X   X  A and  X   X  A represent the param-eters of a common grammar. We use the Dirichlet prior because it is the conjugate prior for the multi-nomial distribution. In summary, the proposed model assumes the following generative process for a multilingual corpus, where L ( i ) and R ( i ) represent the left and right children of node i . Figure 1 shows a graphi-cal model representation of the proposed model, where the shaded and unshaded nodes indicate ob-served and latent variables, respectively. 3.2 Inference The inference of the proposed model can be ef-ficiently computed using a variational Bayesian method. We extend the variational method to the monolingual PCFG learning of Kurihara and Sato (2004) for multilingual corpora. The goal is to estimate posterior p ( Z ,  X  ,  X  | X ) , where Z is a set of parse trees,  X  = {  X  l } l  X  L is a set of language dependent parameters,  X  l = {  X  is a set of common parameters. In the variational method, posterior p ( Z ,  X  ,  X  | X ) is approximated by a tractable variational distribution q ( Z ,  X  ,  X  ) . We use the following variational distribution, q ( Z ,  X  ,  X  ) = where we assume that hyperparameters q (  X   X  A ) and infer them by point estimation instead of distribu-tion estimation. We find an approximate posterior distribution that minimizes the Kullback-Leibler divergence from the true posterior. The variational distribution of the parse tree of the d th sentence in language l is obtained as follows, q ( z ld )  X  where C ( r ; z ,l,d ) is the count of rule r that oc-curs in the d th sentence of language l with parse tree z . The multinomial weights are calculated as follows, The variational Dirichlet parameters for q (  X  lA ) = Dir(  X   X  lA ) , q (  X  lA ) = Dir(  X   X  lA ) , and q (  X  lA Dir(  X   X  lA ) , are obtained as follows,  X  where C ( A,t ; z ,l,d ) is the count of rule type t that is selected in nonterminal A in the d th sen-tence of language l with parse tree z .

The common rule type parameter  X   X  At that min-imizes the KL divergence between the true pos-terior and the approximate posterior can be ob-tained by using the fixed-point iteration method described in (Minka, 2000). The update rule is as follows,  X  where L is the number of languages, and  X ( x ) = common production parameter  X   X  ABC can be up-dated as follows, where J ABC =  X (
Since factored variational distributions depend on each other, an optimal approximated posterior can be obtained by updating parameters by (2) -(10) alternatively until convergence. The updat-ing of language dependent distributions by (2) -(8) is also described in (Kurihara and Sato, 2004; Liang et al., 2007) while the updating of common grammar parameters by (9) and (10) is new. The inference can be carried out efficiently using the inside-outside algorithm based on dynamic pro-gramming (Lari and Young, 1990).

After the inference, the probability of a com-mon grammar rule A  X  BC is calculated by  X   X  the mean values of  X  l 0 and  X  lABC , respectively. We evaluated our method by employing the Eu-roParl corpus (Koehn, 2005). The corpus con-sists of the proceedings of the European Parlia-ment in eleven western European languages: Dan-ish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Italian (it), Dutch (nl), Portuguese (pt), and Swedish (sv), and it contains roughly 1,500,000 sentences in each language. We set the number of nonterminals at |
K | = 20 , and omitted sentences with more than ten words for tractability. We randomly sampled 100,000 sentences for each language, and ana-lyzed them using our method. It should be noted that our random samples are not sentence-aligned.
Figure 2 shows the most probable terminals of emission for each language and nonterminal with a high probability of selecting the emission rule. Figure 2: Probable terminals of emission for each language and nonterminal. 0  X  16 11 (R  X  S . ) 0.11 16  X  7 6 (S  X  SBJ VP) 0.06 6  X  2 12 (VP  X  V NP) 0.04 12  X  13 5 (NP  X  DT N) 0.19 15  X  17 19 (NP  X  NP N) 0.07 17  X  5 9 (NP  X  N PR) 0.07 15  X  13 5 (NP  X  DT N) 0.06 Figure 3: Examples of inferred common gram-mar rules in eleven languages, and their proba-bilities. Hand-provided annotations have the fol-lowing meanings, R: root, S: sentence, NP: noun phrase, VP: verb phrase, and others appear in Fig-ure 2.
 We named nonterminals by using grammatical cat-egories after the inference. We can see that words in the same grammatical category clustered across languages as well as within a language. Fig-ure 3 shows examples of inferred common gram-mar rules with high probabilities. Grammar rules that seem to be common to European languages have been extracted. We have proposed a Bayesian hierarchical PCFG model for capturing commonalities at the syntax level for non-parallel multilingual corpora. Al-though our results have been encouraging, a num-ber of directions remain in which we must extend our approach. First, we need to evaluate our model quantitatively using corpora with a greater diver-sity of languages. Measurement examples include the perplexity, and machine translation score. Sec-ond, we need to improve our model. For ex-ample, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (John-son et al., 2007), and use probabilistic grammar models other than PCFGs. In our model, all the multilingual grammars are generated from a gen-eral model. We can extend it hierarchically using the coalescent (Kingman, 1982). That model may help to infer an evolutionary tree of languages in terms of grammatical structure without the etymo-logical information that is generally used (Gray and Atkinson, 2003). Finally, the proposed ap-proach may help to indicate the presence of a uni-versal grammar (Chomsky, 1965), or to find it.
