 Hangzai Luo  X  Jianping Fan  X  Xiaodong Lin  X  Aoying Zhou  X  Elisa Bertino Abstract This paper proposes a novel approach for privacy-preserving distributed model-based classifier training. Our approach is an important step towards supporting customizable privacy modeling and protection. It consists of three major steps. First, each data site indepen-dently learns a weak concept model (i.e., local classifier) for a given data pattern or concept by using its own training samples. An adaptive EM algorithm is proposed to select the model structure and estimate the model parameters simultaneously. The second step deals with combined classifier training by integrating the weak concept models that are shared from multiple data sites. To reduce the data transmission costs and the potential privacy breaches, only the weak concept models are sent to the central site and synthetic samples are directly generated from these shared weak concept models at the central site. Both the shared weak concept models and the synthetic samples are then incorporated to learn a reliable and com-plete global concept model . A computational approach is developed to automatically achieve a good trade off between the privacy disclosure risk, the sharing benefit and the data utility. The third step deals with validating the combined classifier by distributing the global concept model to all these data sites in the collaboration network while at the same time limiting the potential privacy breaches. Our approach has been validated through extensive experiments carried out on four UCI machine learning data sets and two image data sets.
 Keywords Privacy-preserving classifier training  X  Synthetic samples  X  Adaptive EM algorithm 1 Introduction The rapid growth of the Internet has enabled many relevant knowledge-based collaborative applications. Through the Internet, large volumes of data can be easily acquired, combined and analyzed to extract relevant knowledge. Organizations, in a large number of domains, are today interested in the possibility of combining their data records so that more reliable and complete knowledge can be extracted accurately. Such an unprecedented opportunity is however hindered by privacy and confidentiality concerns [ 1  X  8 ]. Organizations are usually interested in the possibility of collaborating in order to derive complete knowledge which is of interest to them; however, in most cases, organizations cannot make directly available their data records to other parties for confidentiality reasons, for example when the data records disclose important business information of the organization, or for privacy reasons, for example when the data records refer to individuals.

Such concerns have motivated intense research in the area of privacy-preserving data mining techniques [ 9  X  32 ]. Most existing techniques for privacy-preserving data mining can be categorized into three broad approaches: (a) The first approach is based on the use of geometric or statistical data transformation [ 27 ], and multiple imputation [ 28  X  30 ] has widely been used for data privacy protection. Recently, geometric rotation transformation has been proposed that transforms the confiden-tial data records onto another geometric space [ 25 ]. One basic requirement for geometric and statistical data transformation is that the particular geometric or statistical properties for the original data records should be preserved effectively and efficiently. For data perturbation, the original data records are altered by using additional randomized noise before delivering them to the central site or to the data miner [ 9  X  16 ]. Obviously, perturbation can protect indivi-dual data records and data mining algorithms are still able to recover aggregate information or to build data mining models from the perturbed data records. However, data perturba-tion may result in information loss as well as in privacy breaches due to the disclosure of large amounts of perturbed data records [ 33  X  35 ]. Since this transformation-based approach requires to share large amounts of the transformed data records (i.e., data records after per-forming geometric or statistical transformation), it may be too expensive for distributed data mining. (b) The second approach is based on the use of secure multi-party computation (SMC) [ 17  X  24 ]. This approach assumes that the local sites storing the data records cooperate to learn the global data mining results without revealing their original data records. The SMC-based approach can provide perfect data privacy protection, but it is very expensive , since it requires the exchange of large amounts of encrypted data records. (c) The third approach is based on distributed computation for each query [ 36 ]. When a participant needs to compute a query, it first asks each participant compute a local solution for the query, then compute the global solution via an iterative protocol. This approach has two problems. Firstly, the query is disclosed to all participant, thus there is a potential privacy information breaking via queries. Secondly, even though the high cost training step is removed, each query must be processed by all participants, results in significant increasing of the overall cost. 1.1 Our approach In this paper, we have proposed a novel approach for enabling privacy-preserving model-based classifier training that can overcome the drawbacks of the previous approaches. Our approach deals specifically with the problem of the distributed training of a model-based classifier when the data records are stored by multiple data sites, each of which requires its local data records to be maintained private. In addition, the statistics on the available training samples at each individual data site may not be representative of the principal statistical properties of the given data pattern or concept. In its essence, our approach consists of three major phases (see Fig. 1 ). In the first phase, each data site independently builds its own weak concept model (i.e., local classifier) for the given data pattern or concept, by using some local training samples. In the second phase, each site sends only its weak concept model to the central site. No local training samples are sent to the central site, thus greatly reducing both the data transmission costs and the potential privacy breaches. At the central site, synthetic samples are directly generated from the collected weak concept models by using Markov Chain Monte Carlo sampling technique [ 9 , 10 ]. Because the synthetic samples have the same statistical properties as the original data records, they can be used to replace the original data records for training the combined classifier (i.e., global concept model). Finally, the third phase validates the combined classifier by distributing the global concept model to all these data sites in the collaboration network. In order to limit the privacy breaches, we have also developed a computational approach to achieving a good trade off between the sharing benefit, the privacy disclosure risk, and the data utility.

The paper is organized as follows. Section 2 gives a brief overview of the related works and the comparison with our proposed approach; Sect. 3 introduces our framework for cus-tomizable privacy modeling to achieve more flexible privacy protection. In order to develop solid foundations for privacy-preserving distributed classifier training, Sect. 4 and Sect. 5 introduce our adaptive EM algorithm for model-based classifier training and classifier com-bination. Section 6 presents our distributed approach for privacy-preserving model-based classifier training. Section 7 presents our experimental results for algorithm evaluation. Sec-tion 8 concludes the paper. 2 Related works and comparison Many techniques have been proposed for privacy-preserving data mining in the past, but the limitation of pages does not allow us to survey all these interesting works. Instead we try to emphasize some of these works that are most relevant to our proposed work. The references below are to be taken as examples of these related works, not as the complete list of these works in the relevant research areas.

By using the perturbed data records for classifier training, Agrawal et al. used the randomi-zed data distortion technique to learn decision trees [ 11 ]. A novel technique is also proposed by applying an expectation maximization (EM) algorithm on the perturbed data records to learn a model-based classifier [ 12 ]. Recently, Evfimievski et al. have also developed an approach to estimating the incurred privacy breaches when data perturbation is used [ 13 , 14 ]. Ma et al. have incorporated the perturbed data records to enable privacy-preserving lear-ning of Bayesian Network structure and parameters [ 16 ]. Multiplicative data perturbation is also developed for more effective privacy protection [ 37 , 38 ]. There are two conflict requests for data perturbation: (a) strong noises are expected to protect the data privacy effectively; (b) strong noises may induce higher information loss. Thus data perturbation may result in information loss as well as in privacy breaches [ 33  X  35 ]. For distributed classifier training, it often requires to transmit large amounts of perturbed data records for reliable classifier training and thus the data transmission cost may be very expensive.

The SMC problem was firstly introduced by [ 17 ]. Lindell et al. then applied such tech-nique to the problem of privacy-preserving learning of the decision trees [ 18 ]. Du et al. have proposed several new SMC protocols for privacy-sensitive statistical data analysis [ 20 , 21 ]. Vaidya et al. have developed several SMC methods for privacy-preserving data mining appli-cations [ 22 , 23 ]. Recently, Wright et al. have incorporated SMC approach to enabling privacy-preserving learning of Bayesian Network structure and parameters [ 24 ]. As we have already mentioned before, such SMC-based approaches are very inefficient and are thus not adequate when large data sets are involved.

Recently, data transformation has been used for data privacy protection. By using geome-tric rotation transformation, Chen et al. have proposed a novel approach to enabling privacy-preserving training of rotation-invariant classifiers [ 25 ]. Without releasing the confidential original data records for classifier training, automatic synthetic data generation may provide an attractive solution to data privacy protection [ 27  X  30 ]. By incorporating synthetic samples for data privacy protection, Merugu et al. have proposed a new approach to achieving privacy-preserving ensemble classifier training via model averaging [ 31 ]. However, there are three basic assumptions of most existing techniques for ensemble classifier training, such as voting [ 39 ], meta-learning [ 32 ], stacked generalization [ 39 ]: (a) the training samples at each data site are representative to reliably learn the local concept model with an acceptable accuracy rate in a certain input space; (b) all these local concept models are learned from the same training data set or the resampled versions of the same training date set and have significant variations in their overall performance (i.e., error diversity); (c) a large-scale validation data set is available for generating the meta-level training data set or voting the final decision.
Unfortunately, all these three basic assumptions may not hold when dealing with privacy-preserving distributed classifier training on heterogeneous data sets. Thus, there is an urgent need to develop new approach for privacy-preserving distributed model-based classifier trai-ning. It is also important to note that the critical information is actually task-specific and different subjects have different privacy preferences, and thus customizable privacy modeling and protection are strongly needed [ 25 , 33 ]. Based on these observations, we have proposed a customizable approach to enabling privacy-preserving distributed model-based classifier training.

Our proposed approach has several major differences and advantages with respect to the existing approaches: (a) Our approach does not require to share large amounts of perturbed data records or encrypted data records for distributed classifier training, and thus the data transmission costs are reduced significantly. (b) Our approach only requires the weak concept models to be shared with the central site for combined classifier training; the potential privacy breaches are thus reduced drastically. (c) Our approach can support customizable privacy modeling, and thus more flexible privacy protection is effectively achieved.
 The major differences between our approach and the technique proposed by [ 31 ]are:(1) Our approach integrates both the weak concept models and the synthetic samples to enable more accurate learning of the global concept model . (2) An adaptive EM algorithm is proposed for learning both the weak concept models (i.e., local classifiers) and the global concept model (i.e., combined classifier) accurately by updating the mixture components automatically according to the real class distribution of the training samples. (3) A computational approach is developed to achieve a good trade off between the sharing benefit, the privacy disclosure risk, and the data utility automatically. Thus our approach can learn more reliable and complete global concept model (i.e., combined classifier) and results in higher classification accuracy. In addition, the privacy disclosure risk at the data record level is effectively controlled by performing customizable privacy. 3 Customizable privacy modeling All existing techniques for privacy-preserving data mining provide the same level of privacy for all subjects without catering to their personalized needs [ 25 , 33 ]. Thus they may offer insufficient privacy protection for some subjects while applying excessive privacy control to other subjects. According to Alan Westin,  X  X rivacy is the claim of individuals, groups, or institutes to determine for themselves when, how and to what extend information is commu-nicated to others X  [ 1 ]. Such definition of privacy emphasizes the fact that different subjects may have different privacy preferences and they may want to share different information with different data seekers. We thus need techniques able to support customizable privacy modeling , so that each data holder (i.e., one certain data site in the collaboration network) can specify its individual privacy and the degree of privacy protection under a given colla-boration task and environment. In addition, a computational model is needed to quantify the customizable privacy precisely.

As shown in Fig. 2 , customizable privacy largely depends on six inter-related factors [ 40 ]: (1) data seeker and data holder (i.e., two data sites in the collaboration network) because the privacy concerns depend on the data sites involved in the collaboration; (2) data application task because the critical information is actually task-specific and heavily depends on the underlying application environment; (3) trust between the data holder and the data seeker; (4) benefit of data sharing; (5) privacy disclosure risk ; and (6) data holder X  X  individual perception/judgment about privacy of the data being shared because privacy may often have different meanings for different subjects. In collaborative environments, data privacy is also related to a balance between the data holder X  X  concerns for privacy disclosure privacy as confidentiality or unavailability of individual pieces of information are inadequate in collaborative environments. Motivated by this observation, we propose to integrate all these inter-related factors to model customizable privacy in collaborative environments. In this paper, the data seeker is named as the central site and the data holder is one certain data site in the collaboration network.

To achieve customizable privacy modeling, we need to address two key problems: (a) How to take into account the individual data holder X  X  privacy concerns? (b) How to quantify the privacy disclosure risk, the data utility, and the sharing benefit, and achieve a good balance among them? To address both these problems, we propose a computational approach to customizable privacy modeling for the specific task of privacy-preserving distributed model-based classifier training. By incorporating synthetic data generation for privacy protection, our approach is able to automatically achieve an acceptable trade off between the data utility, the sharing benefit, and the privacy disclosure risk.

Compared with the existing approaches for privacy modeling, our approach has the follo-wing advantages: (a) it can quantify the privacy under different contexts; (b) it can adapt to the benefit/risk optimization between the data seeker and the data holder and thus it can sup-port more flexible privacy protection; (c) it can take the personal perception/judgment of the privacy into account and thus it is customizable, and (d) it is very suited for the collaborative environments. 4 Model-based classifier training The original data records c j ={ X l , C j ( X l ) | l = 1 ,..., N } that are available at each data site are labeled for classifier training: positive samples that are relevant to the given data pattern or concept C j and negative samples that are irrelevant to the given data pattern or concept C j . Each labeled training sample is a pair ( X l , C j ( X l )) that consists of a set of attributes X l and the semantic label C j ( X l ) for the corresponding training sample.
To exploit the contextual relationships between the given data pattern or concept C j and the available training samples, we use the finite mixture model to approximate the underlying class distribution for the training samples that are relevant to C j [ 41 ]:
In the above expression, P ( X | C j ,  X  i ) is the i th mixture component to interpret one rele-vant class of the training samples that are used to characterize the statistical properties of the given data pattern or concept C j . c j ={  X  j ,  X  c j ,  X  c j } is the parameter tuple that includes the model structure, model parameters and weights, where:  X  j is the model structure (i.e., optimal number of mixture components),  X  c j ={  X  i = ( X  i ,  X  i ) | i = 1 ,..., X  j } is the set of the model parameters (mean  X  i and covariance  X  i )forthese  X  j mixture components,  X  Finally, X is the m -dimensional attributes that are used for characterizing the relevant trai-ning samples.

Maximum likelihood (ML) criterion can be used to determine the underlying model para-meters in Eq. ( 1 ), but it prefers complex models with more free parameters [ 41 ]; thus a penalty term is added to determine the optimal model structure. The optimal parameter set of model structure, weights, and model parameters c j = (  X  j ,  X  c j ,  X  c j ) for the given data pattern or concept C j is then determined by: where: L ( C j , c j ) = X  X  X  the complex models [ 41 ], and the MDL term is not dependent on any prior assumption of data distribution, N is the total number of training samples, and m is the dimensions of the attributes for the training samples X i  X  c j .
 The estimation of maximum likelihood described in Eq. ( 2 ) is usually obtained by using the EM algorithm with a pre-defined model structure  X  j [ 41  X  43 ]. However, different data patterns or concepts should have different model structures because they may relate to different numbers and types of data classes.

Without organizing the distribution of mixture components according to the underlying class distribution of training samples, a mismatch may arise when there are too many mixture components in one sample area and too few in another. In order to effectively address such mismatch problem, we propose an adaptive EM algorithm to achieve more accurate model selection and parameter estimation simultaneously. Our adaptive EM algorithm performs automatic merging, splitting, and elimination to re-organize the distribution of the mixture components and modify the optimal number of the mixture components according to the real class distribution of the available training samples.

To exploit the most suitable data classes for accurately interpreting the principal statistical properties of the given data pattern or concept C j , our adaptive EM algorithm starts from alargevalueof  X  j and takes the major steps shown in Algorithm 1. With the given  X  j , k -means clustering technique (i.e., k =  X  j ) is used to select the robust initial values for the model parameters (i.e., means and covariance for each cluster). To avoid the local minimum problem, each cluster starts from multiple centers randomly.

To determine the underlying optimal model structure, we use two criteria to perform automatic splitting, merging, and elimination of mixture components: (a) fitness between one specific mixture component and the local distribution of the relevant training samples; (b) overlapping between the mixture components from the same concept model or different concept models.

Our adaptive EM algorithm uses symmetric Jensen-Shannon (JS) divergence (i.e., intra-concept JS divergence) JS ( C j ,  X  l ,  X  k ) to measure the divergence between two mixture com-ponents P ( X | C j ,  X  l ) and P ( X | C j ,  X  k ) from the same concept model P ( X , C j , c j ) . where H ( P (  X  )) = X  P (  X  ) log P (  X  ) is the well-known Shannon entropy,  X  1 and  X  2 are weights defining the relative importance of the two mixture components. In our current experiments, we set  X  1 = N l N
If the intra-concept JS divergence J S ( C j ,  X  l ,  X  k ) is small, these two mixture components are strongly overlapped and may overpopulate the real distribution of the relevant training samples (i.e., mismatch with the relevant training samples); thus they are merged into a single to measure the divergence between the merged mixture component P ( X | C j ,  X  lk ) and the local density of the training samples P ( X , C j ,  X ) . The local sample density P ( X , C j ,  X ) is modified as the empirical distribution weighted by the posterior probability. Our adaptive EM algorithm tests  X  j ( X  j  X  1 ) 2 pairs of mixture components that could be merged and the pair with the minimum value of the local JS divergence is selected as the best candidate for merging .
Two types of mixture components may be split : (a) The elongated mixture components which underpopulate the relevant training samples (i.e., characterized by the local JS diver-gence); (b) The tailed mixture components which overlap with the mixture components that are used to approximate the class distributions of the negative samples for interpreting other data patterns or concepts (i.e., characterized by the inter-concept JS divergence ). To select the mixture component for splitting, two criteria are combined: (1) The local JS divergence JS ( C j , S i ,  X  i ) to characterize the divergence between the i th mixture component P ( X | C j ,  X  ) and the local density of the training samples P ( X , C j ,  X ) ; (2) The inter-concept JS diver-gence JS ( C j , C h ,  X  i ,  X  m ) to characterize the overlapping between the mixture components P ( X | C j ,  X  i ) and P ( X | C h ,  X  m ) from different data patterns or concepts C j and C h .
If one specific mixture component is only supported by few training samples, it may be removed from the concept model. To determine the unrepresentative mixture component for elimination , our adaptive EM algorithm uses the local JS divergence JS ( C j ,  X  i ) to charac-terize the representation of the mixture component P ( X | C j ,  X  i ) for the relevant training samples. The mixture component with the maximum value of the local JS divergence is selected as the candidate for elimination. Eliminating the unrepresentative mixture compo-nents located at the boundaries of the concept models is able to maximize the margins among the concept models for different data patterns or concepts and result in higher classification accuracy.

To jointly optimize the operations of merging, splitting and elimination, their probabilities are defined as: where  X  is a normalized factor and it is determined by: The acceptance probability to prevent poor results of the merging, splitting, and elimination operation is defined by: where L ( C j , 1 ) and L ( C j , 2 ) are the objective functions for the models 1 and 2 (i.e., before and after performing the merging, splitting or elimination operation) as described in Eq. ( 2 ),  X  is a constant that is determined experimentally.  X  is set as  X  = 9 . 8 and it is uniform for all these data sets that are used in our current experiments.

By optimizing these three operations jointly, our adaptive EM algorithm achieves the following advantages : (a) It does not require a careful initialization of the model structure and model parameters. By starting from a reasonably large number of mixture components, our adaptive EM algorithm is able to automatically select the optimal model structure to capture the essential structure of the data classes by performing automatic merging, splitting and elimination of mixture components. Thus, it is able to achieve a better approximation of the real class distribution for the given data pattern or concept by running the local search from many different starting points. (b) By integrating the negative samples to maximize the margins among the model-based classifiers for different data patterns or concepts, it is able to improve the prediction power and generalization ability of the model-based classifiers by achieving discriminative learning of finite mixture models. (c) It is able to address the mismatching problem effectively by re-organizing the distribution of mixture components and modifying the optimal number of mixture components according to the underlying class distribution of the available training samples. (d) By performing automatic merging, splitting, and elimination of mixture components, it is able to support more effective solution for distributed model-based classifier training (see Sect. 5 ). 5 Distributed model-based classifier training For a given data pattern or concept C j , each data site in the collaboration network can learn a weak concept model (i.e., local classifier) independently by using its own training samples (see Fig. 1 ). Our proposed technique for model-based classifier training as described in Sect. 4 is first used to select the optimal model structures and estimate the accurate model parameters for these M weak concept models. Because the training samples distributed at these M data sites may be heterogeneous and incomplete, it is impossible for any one of these M data sites to learn a global concept model individually for accurately interpreting the given data pattern or concept C j .

To achieve distributed classifier training, large amounts of training samples with diverse statistical properties should be collected from these M data sites for combined classifier training at the central site. However, sending large amounts of training samples to the central site is too expensive and may also result in privacy breaches. To reduce the data transmission costs and the potential privacy breaches, each data site shares only its weak concept model P (
X , C j , c j ) and the value N i for the total number of training samples that are used to learn the corresponding weak concept model.
To enable distributed model-based classifier training, synthetic samples are directly gene-rated from the shared weak concept models at the central site by using Markov Chain Monte Carlo sampling technique [ 9 , 10 ]. The number of synthetic samples which are generated from the i th weak concept model is controlled by using the shared value N i . We refer to these trai-ning samples that are directly generated from M weak concept models as synthetic samples because they are not obtained from the original data records. The synthetic samples have the same statistical properties as the original data records because both of them originate from the same mixture density function (i.e., same weak concept model), and thus such synthetic samples can be used to replace the original data records for combined classifier training [ 27  X  30 ]. In addition, the synthetic samples are also sufficiently different from the original data records and thus they are able to protect the privacy of the original data records. Thus, generating the synthetic samples from these M weak concept models at the central site can significantly reduce the privacy breaches and drastically reduce the data transmission costs.
Because the training samples distributed at these M data sites are heterogeneous and incomplete, each of these M weak concept models is only able to characterize different useful aspects (i.e., different statistical properties at different input spaces) of the given data pattern or concept C j . Thus, the individual prediction outputs of these M weak concept models are too uncertain to be useful for learning or boosting a reliable combined classifier, and most existing techniques for classifier combination may fail as pointed out in [ 39 ]. In addition, the training samples that are distributed at these M data sites may be redundant; thus some of their mixture components may overlap, that is, common data classes may appear in multiple weak concept models because multiple data sites may have some common data records, for examples, different supermarkets may have common customers. For some interesting data classes (i.e., some interesting statistical properties for the given data pattern or concept C ), each data site may not have enough training samples to learn the corresponding mixture components accurately. Thus, integrating the weak concept models shared from these M data sites may improve the estimation of such interesting data classes (i.e., mixture components) and achieve more accurate interpretation of the given data pattern or concept C j .
Instead of using their unreliable prediction outputs for combined classifier training, we directly integrate these M weak concept models to achieve a better approximation of the principal statistical properties for interpreting the given data pattern or concept C j effectively. In addition, each data site also shares the total number of training samples (i.e., N i )thatare used to learn the corresponding weak concept model, and N i is further used to determine the relative weights for the mixture components from the i th data site in the global concept model.

Based on these observations, our approach for combined classifier training is organized according to the following steps: (a) The weak concept models for all these M data sites are combined to obtain a  X  X seudo-complete X  global concept model for interpreting the given data pattern or concept C j accurately and completely. (b) The synthetic samples are auto-matically generated by using these shared weak concept models and the number of synthetic samples generated from the i th weak concept model is controlled by the value of N i (i.e., the total number of training samples that are used to learn the i th weak concept model). These synthetic samples generated from different weak concept models (i.e., combined synthetic samples) are integrated for combined classifier training. (c) Based on the available weak concept models shared by these M data sites, our adaptive EM algorithm is used to select the optimal model structure and estimate the accurate model parameters simultaneously by per-forming automatic merging, splitting, and elimination of mixture components. The mixture components with less prediction power on the combined synthetic samples are eliminated from the global concept model. The overlapped mixture components are merged as a single mixture component. The elongated mixture components that underpopulate the combined synthetic samples are split into multiple representative mixture components.

By integrating all these M weak concept models shared from M data sites, the global concept model for interpreting the given data pattern or concept C j is defined as: where  X  j is the total number of mixture components for the j th weak concept model from the j th data site, and N j is the total number of training samples that are available at the j th data site for learning the j th weak concept model. By integrating the normalization factors model parameters and weights,  X  = M j = 1  X  j is the total number of the mixture components shared from these M data sites,  X  j is the total number of mixture components for the j th weak concept model,  X  l = N j M
If one mixture component, P ( X | C j ,  X  m ) ,is eliminated , the global concept model for accurately interpreting the given data pattern or concept C j is then refined as:
If two mixture components P ( X | C j ,  X  m ) and P ( X | C j ,  X  l ) are merged as a single mixture component P ( X | C j ,  X  ml ) , the relevant model parameters are updated as follows: where  X  ml ={  X  ml , X  ml } ,  X  ml and  X  ml are the mean and covariance for the merged mixture component P ( X | C j ,  X  ml ) . Thus, the global concept model for accurately interpreting the given data pattern or concept C j is refined as: If one mixture component, P ( X | C j ,  X  h ) ,is split into two new mixture components, P (
X | C j ,  X  r ) and P ( X | C j ,  X  t ) , the relevant model parameters are updated as follows: where  X  is a pre-defined m -dimensional perturbation vector. Thus, the global concept model for accurately interpreting the given data pattern or concept C j is refined as:
One of the approach to compute  X  is to use the principal axis of  X  h , because it has the largest deviation. However, the principal axis may not be the suitable split direction as addressed in [ 44 ]. Therefore, to find a better split direction, we compute the projection pursuit index value proposed in [ 45 ] along all eigenvectors e v j of  X  h , then the direction with the smallest index value is used to compute  X  :  X  =  X  j e v j . Where e v j is the eigenvector with the smallest index value, and  X  j is the relevant eigenvalue.

By updating the mixture components automatically according to the real class distribution of the combined synthetic samples, our algorithm for combined classifier training is expected to derive a more reliable and complete global concept model that is able to interpret the principal statistical properties of the given data pattern or concept C j effectively.
To validate the combined classifier, the global concept model is distributed to all these data sites in the collaboration network. Obviously, distributing the global concept model may also induce the privacy breaches, and a model privacy protection technique is needed to enable privacy-preserving distributed model-based classifier training. 6 Privacy-preserving distributed classifier training By treating these M data sites as M horizontally partitioned data sources with potential data overlapping, we propose a distributed approach to privacy-preserving model-based classifier training that automatically achieves a good trade off between data utility, sharing benefit, and risk of privacy breaches. 6.1 Model quality To support more effective training of model-based classifier, it is very important to develop new frameworks for evaluating the quality of the global concept model. The quality of the global concept model D (  X  c j ,  X  c j ) is defined as: where P ( X , C j ,  X  c j ) is the global concept model that is learned by using our approach, P (
X , C j ,  X  c j ) is the mean model [ 31 ], D (  X  ,  X  ) is the JS divergence. The mean model is defined as: where P ( X , C j , c j ) is the weak concept model that is shared from the j th data site, N j is the total number of training samples that are used to learn the j th weak concept model P (
X , C j , c j ) . One can observe that the relative importance (i.e., weights) for different data classes in the mean model is simply determined by the sizes of the training samples at the corresponding data sites. On the other hand, the relative importance (i.e., weights) for different data classes in our global concept model is automatically determined by our adaptive EM algorithm according to the real class distribution of the training samples (i.e., the shared mixture components are updated automatically according to the real class distribution of the combined synthetic samples). Thus our approach is able to learn more reliable and complete global concept model and results in higher prediction power.

To enable privacy-preserving distributed classifier training, it is also very important to enhance the individual data site X  X  ability to control other data site X  X  usage of its weak concept model because sharing the weak concept models (i.e., data mining results) may also result in privacy breaches [ 8 ]. Thus, a good trade off between the privacy disclosure risk and the quality of the global concept model (i.e., the accuracy of the global concept model for interpreting the principal statistical properties of the given data pattern or concept) can be achieved by controlling the number of mixture components to be shared with the central site. To obtain the maximum privacy, a strategy would be to share only part of the mixture components with the central site; however, this strategy decreases the quality of the global concept model and the performance of the combined classifier may also decrease. To enhance the quality of the global concept model and improve the classifier X  X  performance, a strategy would be to share all the mixture components, which however results in a higher risk of privacy breaches. Thus, there is the need to achieve a good balance between the privacy disclosure risk and the quality of the global concept model. 6.2 Automatic benefit/risk optimization In our proposed approach for distributed model-based classifier training, there are two steps that may induce privacy breaches: (a) sharing the weak concept models with the central site for combined classifier training; (b) distributing the global concept model to all these individual data sites in the collaboration network for classifier validation. To enable privacy-preserving distributed model-based classifier training, new techniques are needed to automatically esti-mate and control the potential privacy breaches, when the weak concept model and global concept model are shared among the data sites in the collaboration network. 6.2.1 Benefit/risk optimization for combined classifier training To enable customizable benefit/risk optimization for sharing the weak concept model in the combined classifier training procedure, we define two data sets for the j th data site in the collaboration network: (1) for the original data records that are available at j th data site; and (2) for the synthetic samples that can be generated from the weak concept model for the j th data site, when the weak concept model for the j th data site is shared with the central site.
To customize and quantify the privacy disclosure risk (, ) for the j th data site, we have developed a computational approach to quantifying the following four different types of privacy disclosure risk: (1) re-identification disclosure risk  X  r (, ) : the risk of disclosing the one-to-one relationship between the synthetic samples (i.e., generated from the j th weak concept model) and the original data records; (2) linkage disclosure risk  X (, ) :therisk of re-identifying the values of the original data records by linking the synthetic samples with other public-available data sets or the data records that are available at other collaboration data sites; (3) confidentiality-interval inference risk  X (, ) : the risk of disclosing the tight the risk of disclosing confidential data statistics.

The re-identification disclosure risk  X  r (, ) is defined as: where N and N s are the total numbers of the original data records and the synthetic samples,  X  r is the privacy disclosure risk for re-identification of the i th original data record X i by using the j th synthetic sample Z j .

The confidentiality-interval inference risk  X (, ) is defined as: where | X i  X  Z j | is the approximation accuracy by using the j th synthetic sample Z j to approximate the i th original data record X i , and the optimal value for such approximation accuracy is set as 0.001 and thus  X  c is set to 1,000 in our current experiments to avoid division by zero.

The linkage disclosure risk  X (, ) is defined as: where  X  c is set to 1,000 in our current experiments to avoid division by zero, (, ) is the information loss incurred by using the synthetic samples to approximate the original data records.
The statistical inference risk  X  s ( S ,  X  S ) is defined as: where r c () and r c ( ) are the confidential data statistics that can be extracted from the original data set and the synthetic sample set .FromEqs.( 20 )and 21 ), one can observe that the weak concept model with higher information loss will decrease the linkage disclosure risk because its synthetic samples have larger differences with the relevant original data records.

The the privacy disclosure risk (, ) is defined as: where  X  1 ,  X  2 ,  X  3 ,and  X  4 are the weighting factors denoting the relative importance between  X  (, ) ,  X (, ) ,  X (, ) ,and  X  s (, ) . To enable customizable privacy modeling, each data site can select different values for these weighting factors according to its indivi-dual concerns of various types of the privacy disclosure risk. Because the re-identification disclosure is more critical than others, we set  X  1 = 0 . 4,  X  2 =  X  3 =  X  4 = 0 . 2 in our current experiments.

Obviously, the weak concept model with higher quality may result in higher utility of its synthetic samples and lower information loss, thus the utility (, ) of the synthetic sample set is defined as the inversion of information loss [ 46 , 47 ]. By incorporating the original data set and the synthetic sample set for classifier training, the performance of the combined classifier may be different because of information loss. Thus such performance difference is used to quantify the sharing benefit  X (, ) : where | L ( )  X  L ( ) | 2 is the performance difference when incorporating the original data set and the synthetic sample set for classifier training, E (  X  ) represents the expectation value for | L ( )  X  L ( ) | 2 [ 46 , 47 ]. L ( ) is defined as the classification accuracy resulting from incorporating the synthetic sample set for classifier training, L () is the classification accuracy resulting when incorporating the original data set for classifier training.
It is important to note that the quality of the synthetic sample set largely depends on the number of mixture components to be shared with the central site, that is, if more mixture components are shared, then more principal statistical properties of the original data records can be characterized precisely. On the other hand, the privacy disclosure risk (, ) , the sharing benefit  X (, ) , and the data utility (, ) also depend on the quality of the synthetic sample set . Thus the privacy disclosure risk (, ) , the sharing benefit  X (, ) , and the data utility (, ) for the j th data site explicitly depends on the number of mixture components to be shared with the central site.

For the j th data site in the collaboration network, it is very important to determine the optimal number N optimal of the mixture components to be shared with the central site; N optimal is automatically determined by achieving a good trade off between the privacy disclosure risk (, ) , the sharing benefit  X (, ) , and the data utility (, ) : subject to : where 0 &lt; X   X  1 is a weighting factor and  X  is the upper bound of the privacy disclosure risk accepted by the j th data site when the weak concept model is shared.

By determining the optimal number of mixture components to be shared with the central site, our approach is able to perform a privacy-preserving distributed model-based classifier training while limiting the potential privacy disclosure risk effectively. It is important to note that our approach is able to achieve customizable privacy modeling and protection, and each data site in the collaboration network has full control on the optimal number of mixture components to be shared for combined classifier training. Thus each data site can have full control on its privacy disclosure risk by achieving a good trade off between the data utility, the sharing benefit, and the privacy disclosure risk. 6.2.2 Privacy-preserving classifier validation When the global concept model (i.e., combined classifier) is available, it is distributed to all the data sites in the collaboration network for classifier validation. Each data site can thus obtain the mixture components that are used to characterize the principal statistical properties for other data sites in the collaboration network. These mixture components in the global concept model can be categorized into two groups: (a) the common mixture components that also exist in the weak concept model for the j th data site; (b) the new mixture components that do not exist in the weak concept model for the j th data site. Because these shared mixture components are used to characterize the principal statistical properties for other data sites in the collaboration network, the dishonest data sites may incorporate the global concept model to infer other data site X  X  private data.

By knowing the common mixture components from other M  X  1 data sites, the dishonest data site may incorporate its own data records to infer the private information of the other M  X  1 sites via linkage analysis, i.e., identifying the privacy-sensitive data records from the synthetic samples that can be generated from the common mixture components. Even if the dishonest data sites does not know the exact correspondences between these common mixture components and the relevant data sites (i.e., the common mixture components may be from one of M  X  1 data sites or all of them), supporting such linkage analysis may still leak some private information. To avoid the misusing of the common mixture components, model perturbation is used by adding additional noise to the global concept model before it is distributed to the individual data sites. In our current implementations, Gaussian functions are used to represent the mixture components. The mixture component is interpreted by: where  X  i ={  X  i ,  X  i } is the mean and covariance for the mixture component P ( X | C j , X  i ) .By adding the additional multivariate noise function with zero mean and covariance d  X  i (shown in Fig. 3 ), the perturbed mixture component is then defined as: where  X  i ={  X  i ,  X  i } is the mean and covariance for the perturbed mixture component P (
X | C j ,  X  i ) ,  X  i = ( 1 + d ) X  i , the constant d is the parameter to adjust the extent of the additional multivariate noise. By adding the additional multivariate noise on the mixture components, the synthetic samples generated from the corresponding mixture components are randomized, and thus our model perturbation technique is able to preserve accurate lin-kage analysis with acceptable effectiveness. In addition, adding the multivariate noises on the model level (i.e., on the level of mixture components) is much easy and effective than perturbing the data records directly.

Model perturbation may provide privacy protection as well as result in the loss of model X  X  approximation accuracy (i.e., model quality). Thus the loss of model quality is defined as: where D (  X  ,  X  ) is the JS divergence, P ( X , C j ,  X  c j ) is the original global concept model and P ( X , C j ,  X  c j ) is the perturbed global concept model, and they are defined as: By determining the relationship between the loss of model quality D (  X  c j ,  X  c j ) and the strength d of the additional multivariate noises, it is able for us to trace the effectiveness of our model perturbation technique for privacy protection.

Because each data site has a reasonable number of original data records, the dishonest data sites can first use their own data records to learn a prediction model for inferring other data site X  X  private information. However, each data site may not have enough data records to learn such prediction model with acceptable prediction power, and thus the dishonest data site may not obtain reliable inference of other data site X  X  privacy by only using its own data records. Distributing the new mixture components (i.e., principal statistical properties for other data sites in the collaboration network) may result in a new challenge for privacy protection. By treating the prediction model that is learned by only using its own data records as the weak concept model, the dishonest data site can generate large amounts of synthetic samples from the shared new mixture components to improve its prediction model (i.e., incorporating unlabeled samples (synthetic samples) for model-based classifier training [ 40 , 48 , 49 ]). Because the dishonest data sites may not know the exact correspondences between the new mixture components and the relevant data sites, they can treat these synthetic samples generated from the shared new mixture components as the unlabeled samples.

It is widely accepted that the unlabeled samples (i.e., synthetic samples generated from the shared new mixture components) can improve the classifier training significantly [ 40 , 48 , 49 ], thus distributing the new mixture components (i.e., global concept model) may potentially induce the probabilistic privacy breaches. In this paper, we argue that this widely-accepted conclusion is not true for at least two cases: (a) the dishonest data sites may not have repre-sentative data records that can be used to initiate the learning of such privacy predictor, i.e., they do not have the suitable data records that can be used to interpret the principal statistical properties of other data sites and predict their private data; (b) the dishonest data sites may have a limited number of such data records that can be used to learn the privacy predictor, but its prediction is not reliable. When one of these two cases arises, the dishonest data sites cannot incorporate these new mixture components to infer other data site X  X  privacy confi-dently. Our experimental results have also proved that our arguments are correct, and thus our approach can protect the privacy of the global concept model effectively. 7 Algorithm evaluation Our experiments have been conducted on four real-world data sets from the UCI machine learning web site [ 50 ] and two image data sets: (1) Letter image recognition data, we call it  X  X etter X ; (2) Pen-based handwritten digits, we call it  X  X en X ; (3) Landsat multi-spectral scanner image data, we call it  X  X atimage X ; (4) SPAM E-mail database, we call it  X  X pam X ; (5) Blood regions in our medical education videos, we call it  X  X lood X ; (6) Face regions in our medical education videos, we call it  X  X ace X . The test environments for these data sets are given in Ta b l e 1 .

Our experimental algorithm evaluation focuses on: (a) evaluating the performance diffe-rences between our adaptive EM algorithm and other existing techniques for model-based classifier training; (b) comparing the performance differences between our approach and other existing approaches for classifier combination; (c) evaluating the effectiveness of our privacy protection model when different sizes of unlabeled samples (i.e., synthetic samples generated from the shared global concept model) are used for inferring private data; (d) eva-luating the performance of our approach for data privacy protection; (e) comparing the data transmission costs of our approach against other existing approaches for privacy-preserving distributed classifier training.

The benchmark metric for the classifier evaluation includes precision  X  and recall .They are defined as: where  X  is the set of true positive samples that are related to the given data pattern or concept and are classified correctly,  X  is the set of true negative samples that are irrelevant to the given data pattern or concept and are classified incorrectly, and  X  is the set of false positive samples that are related to the given data pattern or concept but are misclassified.
In our adaptive EM algorithm, multiple operations, such as merging, splitting, and eli-mination, have been integrated to re-organize the distribution of mixture components, select the optimal model structure and construct more flexible decision boundaries among different data patterns or concepts according to their real class distributions. Thus our adaptive EM algorithm is expected to have better performance than the traditional EM algorithm and its recent variants [ 42 , 43 ].

In order to assess the actual benefits of our adaptive EM algorithm for classifier training, we have evaluated the performance differences between our adaptive EM algorithm and other existing techniques for model-based classifier training. As shown in Figs. 4 , 5 and 6 , we have tested the performance differences of the model-based classifiers by using different techniques for model selection and parameter estimation: the traditional EM algorithm by starting with a small value of model structure  X  (i.e., using only splitting for model selection); the traditional EM algorithm by starting with a large value of model structure  X  (i.e., using only merging for model selection); the SMEM algorithm by integrating deterministic annealing and regularization MDL for model selection (i.e., using both merging and splitting for model selection, SM )[ 43 ]; our adaptive EM algorithm by integrating negative samples and new criteria for model selection and parameter estimation (i.e., combining splitting, merging, and elimination, SM + Neg . ). From these experimental results, one can observe that our adaptive EM algorithm improves the classifier X  X  performance significantly. The reasons are: (a) The negative samples are incorporated to maximize the margins among the concept models for different data patterns or concepts; (b) The optimal model structure and model parameters are simultaneously estimated by using a single algorithm; (c) The optimal number of mixture components and the distribution of mixture components are automatically adapted to the real class distribution of training samples.

To evaluate our approach for combined classifier training, we have also compared multiple approaches for classifier combination, e.g., our approach, stacked generalization (i.e., using the prediction outputs of these weak concept models for classifier combination) [ 39 ], model averaging used by [ 31 ]. The comparison results of their average performances on multiple data sets are given in Table 2 . One can observe that using our adaptive EM algorithm for combined classifier training can obtain higher classification accuracy. By performing auto-matic merging, splitting and elimination of the mixture components shared from different data sites, our approach is able to learn more reliable and complete global concept model (i.e., combined classifier) and thus results in higher classification accuracy.

From the results shown in Figs. 4 , 5 , 6 and Table 2 , one can conclude that our algorithms have provided a solid foundation to enable more effective solution to privacy-preserving distributed model-based classifier training (i.e., our algorithms are able to learn more accurate weak concept models and global concept model).

When the global concept model is distributed for classifier validation, the dishonest data sites may generate large amounts of synthetic samples from the shared global concept model to infer other site X  X  private data. Because the global concept model is able to characterize synthetic samples can be used to infer other site X  X  private data. The dishonest data sites may not know the exact correspondences between the shared mixture components (i.e., mixture components in the global concept model) and the relevant data sites; they can just treat these synthetic samples as the unlabeled samples to improve their predictors for private data inference [ 40 ]. To construct the predictor for private data inference, the dishonest data sites can first use their own data records (i.e., labeled samples) to learn a weak concept model for the predictor, and then the synthetic samples generated from the shared global concept model are incorporated to improve the predictor X  X  performance. In our experiments, we have obtained such performance improvement by incorporating different number of synthetic samples for predictor training, e.g., with different size ratios  X  = N u N (synthetic samples) N u and the labeled samples (original data records) N L . The prediction performance improvements for different data sets are given in Figs. 7 , 8 and 9 .
From these experimental results, one can find that incorporating the synthetic samples (unlabeled samples) for privacy inference can improve the predictor X  X  performance with dishonest data site that other data site in the collaboration network also have the similar data samples that share the similar statistical properties with the labeled samples (i.e., original data records which the dishonest data site has). The dishonest data site can further infer that how many data sites in the collaboration network have such data records according to the values of the weights for the corresponding mixture components in the shared global concept model, i.e., larger weight values mean that more data sites have such data records and smaller weight values mean that fewer data sites have such data records. Obviously, the dishonest data site does not know which data site has such data records.
Ideally, it is possible for the dishonest data sites to generate unlimited numbers of synthetic samples (i.e., unlabeled samples) from the shared global concept model to infer other site X  X  private data. However, each data site in the collaboration network does not have enough data records that are able to interpret the principal statistical properties of other data sites (i.e., this is also the major reason for them to collaborate for extracting more reliable and complete knowledge), and thus it cannot learn a reliable predictor with acceptable prediction accuracy rate. When only a limited number of such data records are available for predictor training, we have also obtained a significant decrease in the predictor X  X  performance when large amounts of synthetic samples are incorporated for predictor training, because large amounts of synthetic samples (i.e., unlabeled samples) may dominate the statistical properties and mislead the predictor. Thus, this empirical observation (i.e., decrease of prediction accuracy) has provided very convincing evidence for the efficiency of our proposed solution for protecting the privacy of the global concept model. It is very hard for the dishonest data sites to incorporate large amounts of synthetic samples to obtain reliable prediction results when the dishonest data sites have only a limited number of such data records to initiate the learning of the privacy predictor.

To evaluate our approach to privacy-preserving distributed classifier training, we parti-tioned each test data set into three groups with different sizes and performed model-based classifier training on these three groups independently. Each group has full control on the balance between the privacy disclosure risk, the sharing benefit and the data utility. In addi-tion, all these three issues, such as the privacy disclosure risk, the sharing benefit, and the data utility, largely depends on the number of mixture components to be shared with the central site. Based on this understanding, we have obtained the empirical relationship bet-ween the privacy disclosure risk and the number of mixture components to be shared. As shown in Fig. 10 , sharing more mixture components may induce higher risk of privacy breaches. On the other hand, we have also obtained that sharing more mixture components may enhance both the data utility and the sharing benefit significantly as shown in Fig. 11 . To address such conflict, we have proposed a computational approach to achieving a good balance between the privacy disclosure risk, the data utility, and the sharing benefit. One major advantage of our distributed approach for privacy-preserving model-based classifier training is that it is able to control the privacy disclosure risk effectively by sharing an opti-mal number of mixture components, and such optimal number of mixture components can be customizable for each data site in the collaboration network and can also be determined automatically.

When the global concept model is distributed for classifier validation, the dishonest data sites may be able to identify the common mixture components accurately and generate large amounts of synthetic samples from these common mixture components to infer other site X  X  private data. Based on this understanding, we have also obtained the empirical relationships between the privacy disclosure risk and the numbers of synthetic samples that can be generated from the shared common mixture components. As shown in Fig. 12 , one can observe that the privacy disclosure risk sequentially decreases with the number of synthetic samples to be generated from the shared common mixture components. When more synthetic samples are generated, it becomes more difficult to identify the one-to-one relationships between the original data records and the synthetic samples. However, the privacy disclosure risk consists of four individual parts: re-identification risk, linkage disclosure risk, confidentiality-interval inference risk, and statistical inference risk. Generating more synthetic samples may also increase the adverse ability to predict the value intervals of the original data records (i.e., confidentiality-interval inference risk), induce higher linkage disclosure risk, and result in higher risk for statistical inference, and thus we also have a slight increase in the privacy disclosure risk when more synthetic samples are generated from the shared common mixture components.

As shown in Fig. 13 , the dishonest data site may incorporate the shared common mixture components to re-identify the original common data records in other data sites. Because the common mixture components are perturbed before they are shared (i.e., model perturbation), it is very hard for the dishonest data site to achieve accurate and reliable linkage analysis as shown in Fig. 13 . Thus our model perturbation technique can prevent such linkage analy-sis effectively. Obviously, its effectiveness largely depends on the strength of the additional multivariate noises. To trace the effectiveness of adding additional multivariate noises (i.e., model perturbation) for model privacy protection, we have also obtained the empirical rela-tionship between the loss of model quality D (  X  c j ,  X  c j ) and the noise strength d . As shown in Figs. 14 and 15 , one can observe that strong additional noises may protect the model privacy effectively as well as result in significant loss of model quality. By tracing the joint impacts of the additional multivariate noises on the privacy protection and the loss of model quality, it is able for us to determine an optimal strength value d of the additional noises for model privacy protection.

We have also compared the differences on the data transmission costs between our approach and the perturbation approach [ 12 ] and the SMC approach [ 24 ]. The data transmis-sion cost is defined as the percentage between the number of shared samples and the total number of samples that are needed for achieving accurate classifier training. As shown in Fig. 16 , one can conclude that our proposed approach can reduce the data transmission costs significantly because only the weak concept models are needed to be shared for combined classifier training. 8 Conclusions and future works With our capacity of supporting customizable privacy modeling and protection, a novel approach is proposed for privacy-preserving distributed model-based classifier training. By sharing only the weak concept models and generating the synthetic samples at the central site, our proposed approach is able to reduce both the data transmission costs and the potential privacy breaches significantly. A computational approach is proposed to achieve a good automatically determining the optimal number of mixture components to be shared. Our experimental results on four UCI machine learning data sets and two image data sets are very convincing and show that our approach is highly effective.

Our future works will focus on: (a) Incorporating benefit/risk negotiation in our dis-tributed approach for privacy-preserving model-based classifier training, so that each data site can share different numbers of mixture components with other data sites or the same the privacy disclosure risk when the global concept model or the weak concept models are shared.
 References Author Biographies
