 The problem of extracting a minimal number of data points from a large dataset, in order to generate a support vector machine (SVM) classifier, is formulated as a concave min-imization problem and solved by a finite number of linear programs. This minimal set of data points, which is the smallest number of support vectors that completely char-acterize a separating plane classifier, is considerably smaller than that required by a standard 1-norm support vector ma-chine with or without feature selection. The proposed ap-proach also incorporates a feature selection procedure that results in a minimal number of input features used by the classifier. Tenfold cross validation gives as good or better test results using the proposed minimal support vector ma-chine (MSVM) classifier based on the smaller set of data points compared to a standard 1-norm support vector ma-chine classifier. The reduction in data points used by an MSVM classifier over those used by a 1-norm SVM classifier averaged 66% on seven public datasets and was as high as 81%. This makes MSVM a useful incremental classification tool which maintains only a small fraction of a large dataset before merging and processing it with new incoming data. support vector machines, data classification, data selection, concave minimization, linear programming 1. INTRODUCTION Support vector machines [20, 8, 3, 7, 14] are powerful tools for data classification. Classification is achieved by a lin-ear or nonlinear separating surface in the input space of the dataset. The separating surface depends only on a subset of the original data. This subset of data, which is all that is needed to generate the separating surface, constitutes the set of support vectors. In this paper we give a method for selecting as small a set of support vectors as possible which completely determines a separating plane classifier. We term such a set of support vectors minimal , and the corresponding classifier, a minimal support vector machine . Such a classi-fier turns out to have improved testing set accuracy over one chosen by a standard support vector machine. Mathe-matically, support vectors are data points corresponding to constraints with positive multipliers in a constrained opti-mization formulation of a support vector machine. Compu-tationally, the problem of determining a minimal set of sup-port vectors does not appear to have been addressed before as proposed in this paper. This is an important problem in applications such as fraud detection where the dataset may contain millions of data points. To make support vec-tor machines viable for such applications, it is important to identify a minimal set of support vectors, often an order of magnitude smaller than the original dataset, which de-termines the separating surface and allows the removal of redundant data. This dependence on a small subset of a given dataset, which often leads to an improved classifier, can be utilized in an incremental approach such as chunk-ing [3, 15] where a small fraction of the data is maintained before merging and processing it with new incoming data. For the sake of simplicity and getting basic ideas across we shall confine ourselves here to linear separating surfaces. We briefly summarize the contents of the paper now. In Section 2 we introduce the linear support vector machine as a separating plane classifier midway between and parallel to two bounding planes, with maximum margin (distance) between them. See Figures 1 and 2. The bounding planes attempt to place the two classes of a given dataset on op-posite sides. The separating plane is obtained by solving a quadratic program (1) or a linear program (8), depend-ing on the norm used in measuring the margin between the bounding planes. In order to incorporate a concave suppres-sion term in the objective function that eliminates as many redundant data points as possible and still maintain concav-ity of the objective function for computational purposes, we utilize the linear programming formulation (8) and combine it with a step function in (9) to eliminate as many misclassi-fied points as possible. This translates into a minimal set of support vectors that determine the separating plane. The Successive Linearization Algorithm (SLA) 3.1 obtains a very effective local solution to (9) by solving 4 to 7 linear pro-grams. This leads to a classifier with as good or improved generalization and which depends on a substantially smaller number of data points when compared to other classifiers, as shown by the numerical tests of Section 4 on seven pub-lic datasets. These results indicate a reduction of support permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 vectors, i.e. data points that define the separating surface, as high as 81% and a corresp onding test set correctness in-crease of 5.6%.
 We now describ e our notation and give some bac kground material. All vectors will be column vectors unless trans-posed to a row vector by a prime 0 . For a vector x in the n -dimensional real space R n , | x | will denote a vector in R absolute values of the comp onen ts of x . For a vector x  X  R n , x  X  denotes the vector in R n with comp onen ts ( x  X  ) i = 1 if x i &gt; 0 and 0 otherwise (i.e. x  X  is the result of applying the step function comp onen t-wise to x ). The base of the natural logarithm will be denoted by  X  , and for a vector y  X  R m ,  X   X  y will denote a vector in R m with comp onen ts  X   X  y i , i = 1 , . . . , m . For x  X  R n and 1  X  p &lt;  X  , the p -norm and the  X  -norm are defined as follo ws: The notation A  X  R m  X  n will signify a real m  X  n matrix. For suc h a matrix A 0 will denote the transp ose of A , and A i will denote the i -th row of A . A column vector of ones in a real space of arbitrary dimension will be denoted by e . Thus, for the column vectors e and y in R m , the scalar pro duct e 0 y denotes the sum a real space of arbitrary dimension will be denoted by 0. A separating plane, with resp ect to two given point sets A and B in R n , is a plane that attempts to separate R n into two halfspaces suc h that eac h open halfspace con tains points mostly of A or B . A real valued function f ( x ) on R n is conca ve ( X  X oun tain-lik e X ) if linear interp olation between two function values nev er overestimates the function. 2. THE LINEAR SUPPOR T VECTOR MA CHINE We consider the problem, depicted in Figures 1 and 2, of classifying m points in the n -dimensional real space R n , rep-resen ted by the m  X  n matrix A , according to mem bership of eac h point A i in the class A + or A  X  as specified by a given m  X  m diagonal matrix D with plus ones or min us ones along its diagonal. For this problem the standard supp ort vector mac hine with a linear kernel [20, 8] is given by the follo wing quadratic program with parameter  X  &gt; 0: Written in individual comp onen t notation, and taking into accoun t that D is a diagonal matrix of  X  1, this problem becomes: Here, w is the normal to the bounding planes: and  X  determines their location relativ e to the origin. See Figure 1. When the two classes are strictly linearly separa-ble, that is when the error variable y = 0 in (1)-(2), as in the case of Figure 1, the plane x 0 w =  X  + 1 bounds the class A + points, while the plane x 0 w =  X   X  1 bounds the class A  X  points as follo ws: The linear separating surface is the plane: midw ay between the bounding planes (3). The quadratic term in (1), whic h is twice the recipro cal of the square of the 2-norm distance 2 k w k of (3) (see Figure 1), maximizes this distance, often called the  X  X argin X . Maximizing the margin enhances the gener-alization capabilit y of a supp ort vector mac hine [20, 8]. If the classes are linearly inseparable then the two planes bound the two classes with a  X  X oft margin X  (i.e. bound ap-pro ximately with some error) determined by the nonnegativ e error variable y , that is: The 1-norm of the error variable y is minimized parametricly with weigh t  X  in (1) resulting in an appro ximate separation as depicted in Figure 2, for example. Points of A + that lie in the halfspace { x | x 0 w  X   X  + 1 } (i.e. on the plane x w =  X  + 1 and on the wrong side of the plane) as well as points of A  X  that lie in the halfspace { x | x 0 w  X   X   X  1 } are called supp ort vectors . Figure 1: The Linearly Separable Case: The bound-ing planes of equation (3) with margin 2 k w k plane of equation (5) separating A + , the points rep-resen ted by rows of A with D ii = +1 , from A  X  , the points represen ted by rows of A with D ii =  X  1 . PSfrag replacemen ts Figure 2: The Linearly Inseparable Case: The ap-pro ximately bounding planes of equation (3) with a soft (i.e. with some error) margin 2 k w k plane of equation (5) appro ximately separating A + from A  X  .
 Supp ort vectors, whic h constitute the complemen t of the strictly separated points by the bounding planes (3), com-pletely determine the separating surface. Minimizing the num ber of suc h exceptional points can lead to a minim um length description mo del [17, p. 66],[1] that dep ends on much few er data points. Computational results indicate that suc h lean mo dels generalize as well or better than mo dels that dep end on man y more data points. We give in the next section of the pap er an algorithm that minimizes the num-ber of supp ort vectors that determine the separating plane as well as the num ber of input space features. 3. MSVM: A MINIMAL SUPPOR T VECTOR MA-In order to mak e use of a faster linear programming based approac h, instead of the standard quadratic programming form ulation (1), we reform ulate (1) by replacing the 2-norm by a 1-norm as follo ws [14, 2]: This SVM k X k 1 reform ulation in effect maximizes the margin, the distance between the two bounding planes of Figures 1 and 2, using a differen t norm, the  X  -norm, and results with a margin in terms of the 1-norm, 2 k w k The mathematical program (7) is easily con verted to a linear program as follo ws: where, at a solution, v is the absolute value | w | of w . This fact follo ws from the constrain ts v  X  w  X   X  v whic h imply otherwise the objectiv e function can be strictly decreased without changing any variable except v . We will mo dify this linear program so as to generate an SVM with as few supp ort vectors as possible by adding an error term e 0 y the objectiv e function, where  X  denotes the step function as defined in the Introduction. The term e 0 y  X  suppresses mis-classified points and results in our minimal supp ort vector mac hine MSVM: Note that when the error vector y is zero all the points have been strictly separated by the plane x 0 w =  X  . Thus, the separation error term in the objectiv e function of (9) results in: where  X  m is the num ber of positiv e comp onen ts of y equiv alen tly the num ber of misclassified points by the bound-ing planes x 0 w =  X   X  1. This num ber is directly related to the num ber of supp ort vectors as sho wn below follo w-ing equation (13). The positiv e parameter  X  , chosen by a tuning set, multiplies the term e 0 y  X  whic h eliminates pos-itiv e comp onen ts of the error variable y . The justification for eliminating comp onen ts of the error vector y , other than the intuitiv e idea of having a separating surface with as few misclassified points as possible, is as follo ws. If we define nonnegativ e multipliers u  X  R m asso ciated with the first set of constrain ts of the linear program (8), and multipliers ( r, s )  X  R n + n for the second set of constrain ts of (8), then the dual linear program [9] asso ciated with the linear SVM form ulation (8) is the follo wing: Equalit y of the primal objectiv e function of (8) and the dual objectiv e function of (11) imply the (equalit y) complemen-tarit y conditions of the follo wing Karush-Kuhn-T ucker op-timalit y conditions [10, p. 94] for (8): These optimalit y conditions lead to the follo wing implica-tions for i = 1 , . . . , m : Thus, a positiv e y i implies a positiv e multiplier u i =  X  &gt; 0 and a corresp onding supp ort vector A i that violates (4). Consequen tly eliminating positiv e comp onen ts of y tends to minimize the num ber of multipliers at the upp er bound  X  as well as data points A i that violate (4), that is, points that lie on the wrong sides of the bounding planes (3). Minimizing e y  X  works remark ably well computationally in eliminating positiv e comp onen ts of the multiplier u and consequen tly the num ber of misclassified points.
 Even though the discon tinuity of the step function term e in (9) can be handled directly by an algorithm suc h as that of [12, Algorithm 1 SLA], we prefer to appro ximate it here by a smo oth conca ve exp onen tial on the nonnegativ e real line [11] as was done in the feature selection approac h of [2]. For y  X  0, the appro ximation of the step vector y  X  of (9) by is: where  X  is the base of natural logarithms, leads to the fol-lowing smo oth reform ulation of problem (9), the smo oth MSVM: Note that: It can be sho wn [4, Theorem 2.1] that, for a finite value of the parameter  X  (app earing in the conca ve exp onen tial), the smo oth problem (15) generates an exact solution of the non-smo oth problem (9). We note that this problem is the min-imization of a conca ve objectiv e function over a polyhedral set. Even though it is difficult to find a global solution to this problem, a fast successiv e linear appro ximation (SLA) algorithm [5, Algorithm 2.1] terminates finitely (usually in 4 to 7 steps) at a stationary point whic h satisfies the min-imum principle necessary optimalit y condition for problem (15) [5, Theorem 2.2] and leads to a locally minimal num ber of supp ort vectors, that is, a minimal num ber of data points A i with positiv e multipliers u i that completely determine the separating surface.

Algorithm 3.1. Successiv e Linearization Algorithm (SLA) for (15). Cho ose  X ,  X ,  X  &gt; 0 . Start with some ( w iter ate by solving the line ar program: Stop when: Comment: The parameter  X  was set to 5. The parameters  X  and  X  wer e chosen with the help of a tuning set surr ogate for a testing set to simultane ously minimize the numb er of supp ort vectors, numb er of input space featur es and tuning set error.
 We turn our atten tion now to numerical implemen tation and testing. 4. NUMERICAL IMPLEMENT ATION AND COM-Before applying Algorithm 3.1, whic h typically consists of solving 4 to 7 linear programs, the dimensionalit y of w  X  R n was reduced by solving the 1-norm SVM (8), as a single linear program, with weigh t  X   X  [0 . 01 , 0 . 1] and discarding comp onen ts of w less than 10  X  8 in magnitude. The reason for this dimensionalit y reduction, describ ed more fully in [2], is the presence of the term k w k 1 in (7), whic h suppresses comp onen ts of w .
 The remaining comp onen ts of w with the corresp onding val-ues of  X , y and v were used as the initial starting point ( w 0 ,  X  0 , y 0 , v 0 ) in Algorithm 3.1. After the termination of Algorithm 3.1, only supp ort vectors were kept, that is A whic h the multiplier u i &gt; 10  X  8 . This small set of supp ort vectors generated the same stationary point for problem (15) as that generated by the entire dataset. Suc h stationary points, whic h satisfy necessary optimalit y conditions, are typically good candidates to being a global solution to opti-mization problems of the type considered here.
 The smo oth minimal supp ort vector mac hine (MSVM) (15) whic h generates a linear separating surface (5) by using a minim um num ber of data points was compared with the 1-norm supp ort vector mac hine SVM k X k 1 (7) as well as the the 1-norm supp ort vector mac hine with feature selection FSV [2] whic h is problem (7) with the added feature-suppression term  X e 0 | w |  X  in the objectiv e function and smo othed to: This smo othing, similar to that of (15)-(16) except that it is applied here to | w | instead of y , leads to a selection of  X  n ( &lt; n ) input space features. The three classifiers MSVM (15), SVM k  X  k 1 (8) and FSV [2, Eqn. (8)] were tested on sev en datasets, the first five of whic h, WPBC, Iono-sphere, Clev eland Heart, Pima Indians, and BUP A Liv er are from the Irvine Mac hine Learning Rep ository [18], while the Galaxy Dim dataset is from [19], and the Census dataset is a version of the US Census Bureau  X  X dult X  dataset, whic h is publicly available from Silicon Graphics X  website [6]. For WPBC(60), 110 breast cancer patien ts were classified into those who had a recurrence of the disease within 60 mon ths and those who had not. For the Census dataset, ten fea-tures were used to predict whether the income of a person was greater or equal to the mean income or below it. Our computational results are summarized in Table 1 for the three classifiers. We mak e the follo wing observ ations based on numerical results: 1. For all test problems MSVM had the least num ber of 2. For the Ionosphere problem, the reduction in the num-3. Tenfold testing set correctness of MSVM was as good 4. Computing times were higher for MSVM than those 5. CONCLUSION AND FUTURE WORK We have prop osed a minimal supp ort vector mac hine that extracts a minim um num ber of points from a given dataset in order to define a separating surface that classifies the dataset into two categories, based on this minimal subset of the data only . This minimalit y prop erty whic h is in the spirit of Occam X  X  Razor [1], not only is useful in classifying very large datasets by using only a fraction of the data, but also main tains or impro ves generalization over other classifiers that use a considerably higher num ber of data points in order to determine the separating surface. Elimination of a large portion of a dataset mak es MSVM suitable as an incremen tal algorithm that main tains only a small portion of a large dataset before merging and pro cessing it with new incoming data. Since MSVM requires the solution of a few linear programs to determine a separating surface, this mak es it easier than a standard supp ort vector mac hine that uses a quadratic programming form ulation [20, 8]. Our future work includes the application of MSVM to mas-sive datasets using chunking approac hes [3, 15] that break a linear program into smaller ones, as well as extension to nonlinear separating surfaces generated by generalized non-linear supp ort vector mac hines [14] where the dep endence on the training data size becomes more critical. The poten-tial of MSVM as an incremen tal algorithm will be utilized in all these applications to solv e massiv e data classification problems.
 Ackno wledgemen ts The researc h describ ed in this Data Mining Institute Rep ort 00-02, February 2000, was supp orted by National Science Foundation Gran ts CCR-9729842 and CD A-9623632, by Air Force Office of Scien tific Researc h Gran t F49620-00-1-0085 and by the Microsoft Corp oration. and with feature selection (FSV).
 [1] A. Blumer, A. Ehrenfeuc ht, D. Haussler, and M. K. [2] P. S. Bradley and O. L. Mangasarian. Feature [3] P. S. Bradley and O. L. Mangasarian. Massiv e data [4] P. S. Bradley , O. L. Mangasarian, and J. B. Rosen. [5] P. S. Bradley , O. L. Mangasarian, and W. N. Street. [6] US Census Bureau. Adult dataset. Publicly available [7] C. J. C. Burges. A tutorial on supp ort vector [8] V. Cherk assky and F. Mulier. Learning from Data -[9] G. B. Dan tzig. Line ar Programming and Extensions . [10] O. L. Mangasarian. Nonline ar Programming . SIAM, [11] O. L. Mangasarian. Mac hine learning via polyhedral [12] O. L. Mangasarian. Solution of general linear [13] O. L. Mangasarian. Arbitrary-norm separating plane. [14] O. L. Mangasarian. Generalized supp ort vector [15] O. L. Mangasarian and David R. Musican t. Data [16] MA TLAB. User X  X  Guide . The MathW orks, Inc., [17] T. M. Mitc hell. Machine Learning . McGra w-Hill, [18] P. M. Murph y and D. W. Aha. UCI rep ository of [19] S. Odew ahn, E. Sto ckwell, R. Pennington, [20] V. N. Vapnik. The Natur e of Statistic al Learning
