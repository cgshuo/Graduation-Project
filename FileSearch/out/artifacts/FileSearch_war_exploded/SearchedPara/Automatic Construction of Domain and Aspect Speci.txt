 Automatically analyzing the opinions expressed in customer reviews is of high relevance in many application scenarios, e.g., market research, trend analysis, or reputation man-agement. A great share of current sentiment analysis ap-proaches makes use of special purpose lexicons that provide information about the polarity (e.g., positive or negative) of individual words and phrases. One major challenge is that the actual sentiment polarity of a specific expression is of-ten context dependent (e.g.,  X  X ong + battery life X  vs.  X  X ong flash recycle time X ). However, the vast majority of exist-ing approaches focuses on creating general purpose lexicons. Especially in the context of mining customer review data, the use of such lexicons is rather suboptimal as they fail to adequately reflect the domain specific lexical usage. We propose a novel method that allows to automatically adapt and extend existing lexicons to a specific product domain. We follow a corpus-based approach and exploit the fact that many customer reviews exhibit some form of semi-structure. The method is fully automatic and thus scales well across different product domains. Our experiments show that the extracted lexicons are highly accurate and significantly im-prove the performance in a sentiment classification scenario. I.2 [ Natural Language Processing ]: Text analysis sentiment analysis; sentiment lexicon; review mining
Nowadays, if we want to book a hotel, buy a new smart-phone, or search for a nice restaurant, we commonly con-sult online customer reviews prior to making a purchase decision 1 . As people increasingly tend to share their opin-ions and experiences online, vast amounts of customer feed-back data are conveniently available. For us as consumers, but also for vendors and market researches, this genuine feedback represents an extremely valuable source of infor-mation (e.g. for trend analysis, product quality manage-ment, reputation management, or monitoring marketing ac-tivities). However, for popular products, there often exist several thousand reviews and reading even a small share of the (often contradictory) information is by itself a dif-ficult cognitive challenge. The data is typically unstruc-tured and thus automatic text mining systems are needed to support the discovery and interpretation of relevant in-formation. Consequently, in recent years there has been a tremendous amount of work on automatically analyzing the sentiment expressed in natural language text [30]. The field of research associated with this problem setting is generally denoted as sentiment analysis or opinion mining . In general, sentiment analysis addresses the problem of automatically identifying and analyzing subjective information in natural language texts. The goal is to determine the author X  X  opin-ion about a specific target, or more abstract, about a specific topic.

Applied to customer review data, the task is typically de-noted as review mining [20]. Most previous works consider the task of classifying review documents by sentiment, e.g.,  X  X ositive X  vs.  X  X egative X  vs.  X  X eutral X  [31, 37, 10]. They cast the task as an instance of a text categorization problem and experiment with different machine learning algorithms and varying feature representations. However, for many appli-cation scenarios this document level classification of reviews does not provide the required level of detail. Most review-ers express both, positive and negative sentiments in a sin-gle review. For instance, a hotel reviewer may praise the friendly staff, but criticize that the bed was too soft. The goal of aspect-oriented review mining [11, 33, 10, 41, 5] is to analyze the reviewers X  sentiment w.r.t. individual product aspects. Given a collection of customer reviews for some specific product or product type, the two main tasks are (1) to automatically identify all relevant aspects the review-ers have commented on and (2) to categorize the individual comments according to their sentiment polarity (commonly positive vs. negative).

In this paper, we set focus on the second subtask, i.e., de-tecting sentiment expressions and their polarity. The major
Studies report that online product research has become an integral part of the consumers X   X  X urchase experience X  [17, 21]. share of current review mining approaches involves the use of special purpose lexicons for this task  X  so called sentiment lexicons . Such resources encode information about the sen-timent polarity of individual words or phrases, i.e., an entry is associated with categories like positive, negative, or neu-tral polarity. During text analysis this information (often in combination with further linguistic clues) is then used to determine lexical indicators for evaluative content. For in-stance, an occurrence of the verb  X  X ike X  generally indicates a positive or affirmative sentiment.

When constructing a sentiment lexicon, one major chal-lenge is that the actual sentiment polarity of an expression is often dependent on the context. In fact, the polarity of a specific term may differ from domain to domain (e.g., movies:  X  X redictable  X  plot X  vs. automobiles:  X  X redictable steering X ) or may even differ within a single application do-main, depending on the concrete target (e.g., digital cam-eras:  X  X ong + battery life X  vs.  X  X ong  X  flash recycle time X ). Although this phenomenon is known and discussed in the literature [14, 26, 2, 12, 22, 34, 9], the vast majority of ap-proaches focuses on creating general purpose lexicons. How-ever, especially in the context of customer review mining, the use of such lexicons is rather suboptimal as they fail to adequately reflect the domain-specific lexical usage. To this end, we propose a novel method that allows to automatically adapt and extend existing lexicons to a specific product do-main. We follow a corpus-based approach and exploit the fact that many customer reviews exhibit some form of semi-structure. In particular, we make use of the structural clues inherently provided by the pros and cons summaries that are part of many customer reviews. Our approach is fully automatic and thus scales well across different application domains. As the approach directly utilizes the target corpus it can also compensate a potential concept drift in the target domain. Take note that pros and cons summaries are such a common concept w.r.t. product reviews, so that relying on their existence is not a strong assumption.

The main contributions of this work are:  X  We propose and study a corpus-based method that au-tomatically identifies domain and aspect specific sentiment words in customer review datasets.  X  We experiment with different approaches to incorporating the extracted knowledge into existing general purpose sen-timent lexicons.  X  We compare the utility of our extracted lexicons to other state-of-the-art sentiment lexicons (manually and automat-ically constructed). We can show that our methods allow for significant improvements compared to the baseline ap-proaches. The remainder of this paper is organized as fol-lows: Section 2 provides an overview of the most relevant related work. In Section 3, we define some basic terminology and concepts. Section 4 introduces our approach in detail. Section 5 describes our experiments and reports results. We summarize and conclude our findings in Section 6.
Sentiment lexicons are either constructed manually by do-main experts or they are generated by an automatic pro-cess. Manually compiled lexicons are for example the Har-vard General Inquirer 2 , the MPQA subjectivity lexicon 3 http://www.wjh.harvard.edu/~inquirer/Home.html http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ the opinion lexicon by Liu et al. 4 . Most automatic ap-proaches are either dictionary-based or corpus-based . Approaches in both classes have in common that they typi-cally involve a seed set of pre-labeled examples. By testing the semantic similarity between unknown words and labeled seed words the sentiment status of new entries is derived (often in an iterative process).
Dictionary-based approaches typically rely on a thesaurus or on lexical databases such as WordNet [29]. A common assumption is that semantic relations such as synonymy transfer the sentiment polarity of related words [24, 20, 16]. For example, the adjective  X  X ovely X  transfers positive polarity to its synonyms  X  X dmirable X ,  X  X dorable X ,  X  X miable X ,  X  X retty X , and transfers negative polarity to its antonyms  X  X w-ful X ,  X  X nlovely X ,  X  X gly X . Albeit being transitive, the strength of the relations weakens with the distance (in fact there exist synonym paths from  X  X ood X  to  X  X ad X  of length 3 in Word-Net [16]). Appropriate measures need to be devised to ac-count for the path length [35, 5]. A further dictionary-based approach is to infer the semantic relatedness of entries by calculating similarity by means of the glosses [36, 3]. An obvious disadvantage of dictionary-based approaches is that they are typically unable to identify domain dependent sen-timent words.
Also with corpus-based approaches the basic idea is to calculate a measure of semantic relatedness between words and to use this to derive the sentiment status of new words or phrases. Examining these approaches more closely, we identify mainly four different ways to infer relatedness: co-occurrence statistics , distributional context similarity , lin-guistic clues , and structural clues .

Turney and Littman [38] extend the general idea of co-occurrence by hypothesizing that also the  X  X emantic orien-tation of a word tends to correspond to the semantic orien-tation of its neighbors X . Using the Web as a corpus, they apply measures of association, such as pointwise mutual in-formation (PMI), to derive a correlation statistic of an un-seen word with a set of positive and negative seed words. Besides inferring relatedness of two terms directly by their co-occurrence, a common approach is to define similarity in-directly by means of the words X  context. Following Firth X  X  Contextual Theory of Meaning , which states that  X  X  word is characterized by the company it keeps X  [15], it is assumed that words with a similar context also exhibit a similar sen-timent polarity. Corpus-based approaches that exploit this idea are for instance [39, 4].

Besides relying on purely statistical analysis, other studies propose to consider linguistic clues to determine the senti-ment polarity of words. For instance, Hatzivassiloglou et al. [18] point out the observation that conjunctions (e.g.,  X  X nd X  or  X  X ut X )  X  X mpose constraints on the semantic orientation of their arguments X . For instance, the phrase  X  X he hotel staff was helpful and courteous X  sounds natural, but the phrase  X  X he hotel staff was helpful and impolite X  does not. The con-junction rule is for example exploited by [33] or [14]. [25] extend the basic (intra-sentential) rule by also looking at inter-sentential conjunctions. The utility of structural clues http://www.cs.uic.edu/~liub/FBS/ sentiment-analysis.html#lexicon for polarity detection is for instance examined in [23]. They exploit the structure of HTML documents to extract pos-itively and negatively connoted sentences from tables and listings that address advantages and disadvantages. Using a natural language parser, they extract adjective phrases from these sentences and apply statistic measures to deter-mine whether a phrase predominantly occurs in positive or negative sentences.
More recently, researchers examine methods to adapt ex-isting sentiment lexicons. Typically, the purpose of adapta-tion is either to augment a general purpose sentiment lexicon to better fit a specific domain or to extend a monolingual lexicon to cover multiple languages . Domain adaptation is for instance examined by [9] who propose an approach based on linear programming, [12] who introduce an information theoretic framework, or [34] who expand existing lexicons by means of linguistic patterns. Language adaptation is for example considered by [28]. In addition to purely dictionary or corpus-based approaches, some researchers study the util-ity of hybrid approaches, which combine indicators from the different sources. For instance, [19] propose to use semantic relations in WordNet to create an initial sentiment lexicon, which is then refined by incorporating statistical informa-tion gathered from the Web. Both information sources are combined by means of an error minimization algorithm. Lu et al. [27] combine four types of signals that indicate the sentiment polarity of words: Information is gathered from a general purpose sentiment lexicon, from a thesaurus, from linguistic clues, and from structural clues in domain specific documents. The different signals are combined in an opti-mization framework that is based on a linear programming approach. The work most closely related to this one, is our previous approach presented in [7]. In fact, we extended the previous approach. The major differences are the applica-tion of more robust extraction heuristics (incl. neutraliza-tion detection), the use of an aspect taxonomy (which groups similar aspects), and a different setup for statistical hypoth-esis tests. Experiments are also much more convincing, as the evaluation corpora are larger and we consider multiple state-of-the-art baseline methods for comparison.
This paper addresses the task of constructing aspect-aware sentiment lexicons for the use in aspect-oriented review min-ing systems. For this purpose, we assume that a set of rel-evant product aspects is provided in form of a hierarchical knowledge base, which we denote as product aspect taxon-omy . Such a knowledge base may be created manually by domain experts [6] or automatically by means of ontology learning methods [8].

Definition (Product Aspect Taxonomy) Let P be a spe-cific type of product (e.g., digital cameras or hotels). Then a product type taxonomy T is defined by a tree with P as the root and subordinate product aspects A i as inner or leaf nodes. Each node A i is represented by a unique text string (e.g.,  X  X attery life X  or  X  X harging time X ) and is related to its parent through a semantic relation, such as  X  X art-of X ,  X  X eature-of X ,  X  X ype-of X , or  X  X ynonym-of X . Inner nodes A group their children to a higher level concept and define a canonical name for this concept. For example, the aspects  X  X olor reproduction X ,  X  X mage noise X , or  X  X harpness X  are all grouped into the concept  X  X icture quality X .

The motivation for such a model is that it allows to aggre-gate sentiments on each level of the hierarchy. It therefore helps to structure and summarize the reviewers X  sentiments in a comprehensive form and a review mining tool may vi-sualize the hierarchy to support the users.

Definition (Aspect-Aware Sentiment Lexicon) Let A be a product aspect string and E be a sentiment expression string. Then an aspect-aware sentiment lexicon L is a dic-tionary that maps tuples of type ( A , E ) to sentiment polar-ity values p  X  R . Polarity values p &lt; 0 indicate negative sentiment polarity of a tuple, whereas p &gt; 0 indicates posi-tive polarity. Aspect independent sentiment expressions are mapped by allowing tuples of the form (  X  , E ), where  X * X  rep-resents the wild card character.

For example, an aspect-aware lexicon for the domain of digital cameras may contain mappings such as ( X  X attery life X ,  X  X ong X )  X  +1 . 8, ( X  X hutter lag X ,  X  X ong X )  X   X  2 . 4, or ( X * X ,  X  X orrible X )  X  X  X  5 . 1.
The general idea of our approach is to exploit the infor-mation contained in pros and cons summaries , which are often attached to customer reviews. A reasonable as-sumption is that authors choose positive expressions when describing a product aspect in the pros, whereas negative expressions are used in the cons. Input to our method is a collection of customer reviews C with associated pros and cons lists ( C + and C  X  ), a product aspect taxonomy T , and a general purpose sentiment lexicon L . Output is a domain specific aspect-aware sentiment lexicon L  X  . Figure 1 illus-trates the basic ideas of our approach. First (1.), we utilize the taxonomy to identify all mentions of product aspects A within C + and C  X  . In addition, we apply some heuristics to find potential sentiment expressions E that are linked to the aspects. This procedure generates a huge number of tuples ( A,E ). In a next step (2.), we utilize the semantic relations encoded in T to group tuples that refer to similar aspects. Each such group is associated with a single  X  X anonical tuple X  as representative. For each group, we acquire the occurrence counts (3.) in C + and C  X  and apply statistical means (4.) to decide whether a group predominantly stems from the pros or from the cons. A group that significantly more often occurs in pros than in cons is considered as having positive semantic orientation and vice versa for groups originating from the cons. Step (5.) tries to increase the coverage of the final lexicon by means of a thesaurus. The following sections discuss the individual steps in more detail.
We detect mentions of product aspects in the pros/cons summaries by lookups in the product aspect taxonomy. In particular, we apply the Aho-Corasick string matching algo-rithm [1] and guarantee non-overlapping matches by select-ing only the left-most, longest-matching lexicon entry. Both, group tuples the strings in the aspect taxonomy and the input strings from the summaries are lemmatized.

To build tuples of the form ( A,E ), the next goal is to find sentiment expression candidates that are related to the iden-tified product aspects. As pros/cons summaries are often short and mostly do not consist of complete, grammatically correct sentences 5 , we use pattern-based extraction instead of relying on the results of a natural language parser.
First, we consider all commas and similar enumeration markers (e.g.,  X * X ,  X  X ) X ,  X 1. X ) as boundaries of so-called ex-traction units . We restrict valid co-occurrences of product aspects and sentiment expression candidates to these units  X  that is, a valid relation between an aspect and an expres-sion must not span such a boundary. The basic idea with extraction units is to compensate for the fact that we can-not consider the real dependencies that a natural language parse would provide. For the actual extraction, we define high-precision patterns based on the part-of-speech tags as-sociated with an extraction unit. For instance, consider an extraction unit that reads  X  X he/DT zoom/NN buttons/NN are/VB quite/RB small/JJ X . We are interested in extract-ing the tuple ( X  X oom buttons X ,  X  X mall X ). To generalize from this particular example, we can define a pattern such as DT A 1 VB RB JJ 1 , where A 1 masks an identified product aspect (here  X  X oom buttons X ) and the subscripts denote the parts that should be extracted as a tuple. We further generalize patterns by making parts optional or including valid alter-natives. We only consider adjectives as potential sen-
Authors often simply enumerate the advantages and disad-vantages in comma-separated or similarly structured lists. timent expressions . Adjectives typically account for the major share of sentiment expressions (see also Table 1).
In total, we identified 9 high-precision patterns for the extraction of aspect/sentiment expression tuples from pros and cons. Our basic procedure for gathering the patterns was to collect the part-of-speech sequences for all extraction units, measure the frequency of each distinct sequence, and then to manually analyze the 100 most frequent ones.
The previous discussion tacitly disregarded the fact that the labels  X  X ros X  and  X  X ons X  indicate the contextual polar-ity of expressions, whereas we are interested in extracting the aspect specific prior polarity of words. For example, excerpts from a cons-labeled text may read  X  X isplay not very large X  or  X  X attery life not long enough X . Clearly, we would make an error when associating the tuples ( X  X isplay X ,  X  X arge X ) or ( X  X attery life X ,  X  X ong X ) with negative polarity. We would have neglected the sentiment shifter  X  X ot X . Another cons-labeled example may read  X  X ould like to have a large display X . Here, sentiment is shifted by a neutralizer that would render an extraction ( X  X isplay X ,  X  X arge X ) erroneous. To interpret contextual polarity correctly, we thus need to cope with sentiment shifters. Whereas other types of shifters exist [32], only negation and neutralization influence the sen-timent polarity. Again, we make use of heuristics. The large number of extractions helps to compensate potential errors in shifter detection  X  it does not count whether an individ-ual extraction is correct or not, it is important that the vast majority is correctly handled.
We use simple dictionary-based approaches. For negation detection we look for tokens such as  X  X ot X ,  X  X  X  X  X ,  X  X ever X ,  X  X l-though X , etc. and  X  X lip X  the polarities (change the direction) of affected sentiment expression candidates. The scope of negation is restricted to the extraction unit the indicating token occurs in. For detection of neutralization we use in-dicators such as  X  X ould X ,  X  X ould X ,  X  X hould X ,  X  X ight X ,  X  X ish X ,  X  X ope X , etc. and simply discard the associated extraction unit(s). In contrast to negation, we allow that the scope of neutralization may span the boundaries defined by the ex-traction units. For example, we would discard the complete sequence  X  X ish it had a larger display, stronger flash, oth-erwise perfect camera X , instead of discarding only the first extraction unit  X  X ish it had a larger display X .
The next step is to count the extracted tuples so that we can subsequently assess their polarity value by statisti-cal means. To prevent problems with data sparsity and to eventually increase the coverage of the generated sentiment lexicon, we group semantically similar aspects and subse-quently calculate with the aggregated counts. For grouping, we exploit the semantic relations  X  X ype-of X  and  X  X ynonym-of X , which are defined within the aspect taxonomy. We pos-tulate that the aspect specific prior polarity of an adjective is consistent with regard to product aspects that are related to each other via these relations. For example, the positive prior polarity of  X  X arge X  in the context of the aspect  X  X creen X  is also valid for the (near) synonyms  X  X isplay X ,  X  X onitor X , or  X  X ideo screen X , as well as for the derived types  X  X ouch screen X ,  X  X CD screen X , or  X  X wivel screen X . We are aware that this assumption is not always true and counter exam-ples can be brought in. However, although this may cause that we construct some false entries, it is very unlikely that these mistakes lead to errors when applying the lexicon for polarity detection.

Our counts represent document frequencies, where we re-gard each pros or cons text as a single document (a sin-gle pros/cons document may consist of multiple extraction units). We use the following notation: Let C + ( ws ) be the document frequency of a word sequence ws in a corpus of pros documents, and let C  X  ( ws ) be defined analogously. Further, the variables C + ( ws 1 ,ws 2 ) and C  X  ( ws 1 ,ws fer to the document frequency of co-occurrences ( ws 1 ,ws in the corresponding corpora. For all counts we will make use of aggregated group counts and statistically analyze the group as a whole.
We need to examine whether a given tuple ( A,E ) exhibits a strong correlation either with the pros or with the cons cor-pus. In other words, we need to exclude the possibility that co-occurrence in one of the corpora is only by coincidence or due to the imperfect extraction heuristics. We make use of statistical hypothesis testing. In particular, we determine the strength of correlations by means of the likelihood-ratio test (LRT) [13]. It compares the likelihood of two different hypotheses H 0 and H 1 by computing the ratio  X  = L ( H 0 the maximum likelihood L ( H 0 ) for the null hypothesis and the maximum likelihood L ( H 1 ) for the alternative hypoth-esis. The value  X  expresses how much more likely it is to make a certain observation under the assumption H 0 than under the assumption H 1 .

To implement the test, we consider the probabilities where E is a sentiment expression candidate and A is a product aspect. Then p 1 denotes the probability in a cor-pus of pros lists that we observe E , given that the product aspect A occurs in the same extraction unit, and p 2 refers to the same probability, but in a corpus of cons lists. Our null hypothesis H 0 is p 1 = p = p 2 , i.e., we assume that, in-dependent of whether we consider the pros or cons corpus, the strength of association between the sentiment expression and product aspect is the same. The alternative hypothesis H 1 is p 1 6 = p 2 . We estimate p , p 1 , and p 2 as
Assuming that each occurrence of a tuple is a Bernoulli event, the occurrence counts follow a binomial distribution. With b ( p,k,n ) = n k p k (1  X  p ) n  X  k as the standard notation for the probability mass function of the binomial distribu-tion 6 , the log-likelihood ratio computes as:
Based on the fact that the value  X  2 log  X  is asymptotically  X  -distributed [13], we can evaluate the test at a chosen confidence level by looking up the corresponding threshold in a statistical table. If the test rejects H 0 , we postulate that the tuple ( A,E ) has positive prior polarity if p 1 and otherwise has negative polarity. We set the threshold to 3.84, which corresponds to a 95% confidence level.
We hypothesize that we can expand the previously ac-quired knowledge by means of a thesaurus. For instance, knowing that  X  X arge X  has positive polarity in the context of the aspect  X  X creen X , we can conclude that also  X  X ig X ,  X  X reat X , or  X  X uge X  exhibit a positive connotation regarding this as-pect, whereas antonyms such as  X  X ittle X ,  X  X mall X , or  X  X iny X  are negatively connoted. For this purpose, we use WordNet [29] and apply a label propagation algorithm [42] in a similar fashion as described in [5].
Label propagation is a graph-based semi-supervised learn-ing method. Starting with a seed set of labeled nodes, the label information is iteratively propagated along the edges of the graph to previously unlabeled nodes. In the context which computes the probability of exactly k successes in n trials given a success probability of p of sentiment lexicon construction, the core idea is to inter-pret WordNet as a graph, where the lexicon entries represent the nodes and the semantic relations (e.g. synonymy) form the edges. Using a seed set of words with known sentiment polarity, the algorithm allows to determine the polarity of lexicon entries with previously unknown polarity. The out-put of the algorithm is a set of tuples ( e,v ), where e is a lexicon entry and v  X  R is the associated polarity value. Positive values represent positive polarity and vice versa for negative values. Larger absolute values indicate more ex-treme polarity (see [5] for more details).
We run the label propagation algorithm for each group of similar product aspects that we previously extracted. Let G be such a group. Then we collect all adjectives S G that have been found to exhibit a significant sentiment polar-ity in the context of G . Let S + G be the subset of posi-tively connoted adjectives and let S  X  G be the subset of ex-pressions with negative polarity. Furthermore, let Pos and Neg be two seed sets of general purpose sentiment words with known positive and negative polarity (e.g. Pos = {  X  X ood X ,  X  X eautiful X ,  X  X ice X , . . . } and Neg = {  X  X ad X ,  X  X gly X ,  X  X asty X , . . . } ). We experiment with two dif-ferent expansion strategies: Strategy A : We built combined seed sets Pos  X  = Pos  X  S + and Neg  X  = Neg  X  S  X  G and run the label propagation al-gorithm as described in [5]. Let L  X  G be the output of the algorithm. The intuition for including the general purpose seeds is to provide the algorithm with as much labeled data as possible. However, the strategy mixes aspect specific and aspect independent sentiment words, but we are only in-terested in finding further aspect specific expressions. For instance, we want to identify an entry ( X  X creen X ,  X  X uge X , +), but not an entry ( X  X creen X ,  X  X xcellent X , +). Obviously, all words in S + G \ Pos and S  X  G \ Neg belong to the desired cate-gory. Out of the remaining sentiment expressions in L  X  G select the l = 20 expressions with highest absolute polarity value. Then we lookup their polarity in the lexicon that is to be extended and compare with the polarity we have found through aspect specific expansion. We only include a new entry if the signs of the polarities differ. That may be the case if the aspect specific polarity flips the original polarity or the original polarity value was zero.

Strategy B : We only consider S + G \ Pos and S  X  G \ Neg as positive and negative seed sets and run the label propagation algorithm as before. With this strategy we can assume that each word that reaches a sufficiently high polarity score is dependent on G and thus represents a valid extension to the existing lexicon. Again, we select only the k sentiment expressions with the highest absolute polarity value.
We may further utilize the extracted aspect-sentiment tu-ples to collect a set of domain specific, but aspect indepen-dent sentiment expressions. For instance, we may observe tuples such as ( X  X oom X ,  X  X omey X ) or ( X  X oom X ,  X  X ildewy X ) or misspellings such as ( X  X oom X ,  X  X onfortable X ), ( X  X oom X ,  X  X pa-tious X ), or ( X  X oom X ,  X  X osy X ). These types of sentiment ex-pressions have in common that they are not contained in general purpose sentiment lexicons. Furthermore, their oc-currence frequency is rather low, so that we cannot assign a polarity value with sufficient statistical confidence. statistic hotel camera sentiment expressions 1402 1196 avg. expressions per review 9.35 7.97 avg. tokens per expression. 1.23 1.32 unique sentiment expressions 488 473 expressions with multiple targets 3.50% 3.68% aspect specific expressions 20.54% 19.23% adjectival sent. expressions 67.26% 54.01% Table 1: Basic descriptive statistics of the evaluation corpora.

Thus, in addition to assessing tuples, we also evaluate the polarity of sentiment expression candidates independently from any associated aspect. In particular, we aggregate the counts for each distinct sentiment expression by summing up the counts for each tuple the expression occurs in. Again, we design a log-likelihood ratio test that compares the occur-rence probabilities in the pros versus the cons corpus. The null hypothesis is that the expression is equally distributed in both corpora and the alternative hypothesis is that it is more likely to occur in either the pros or in the cons cor-pus. For this test we require a confidence level of 99% (as threshold value), which corresponds to a log-likelihood ra-tio of 6.63. We further trim the procedure towards higher precision by considering only the top-k positive and top-k negative expressions (ordered by absolute score). In our ex-periments we set k to 750.
We evaluate our approaches on two datasets from very different domains, namely hotel reviews and digital camera reviews. For each of the two domains, we manually annotate a set of 150 review documents. The annotated documents serve as reference corpora for our experiments. Our simple annotation schema comprises the two annotation types  X  X s-pect X  and  X  X entiment expression X . The former type marks a span of text that refers to a relevant product aspect, while the latter marks a span that expresses sentiment towards an aspect. The types are existentially dependent and form a many-to-many relationship. In effect, we assume that a sentiment expression always refers to one or more aspects and we do not consider aspects that are not targeted by any sentiment expression. For each sentiment expression, the annotator determines the polarity (positive or negative) and notes whether the polarity is aspect specific or not. The polarity value is determined independent of any associated sentiment shifter. For instance, in the sentence  X  X t does not have a long battery life. X , we would mark  X  X ong X  as aspect specific sentiment expression with positive polarity, despite the fact that the overall (contextual) polarity is negative. Table 1 summarizes the basic statistics of our corpora with regard to sentiment expressions.

Furthermore, we need to provide adequate datasets of pros and cons summaries to evaluate our approach: For the hotel domain, we crawl summaries from the website Price-line.com. From this crawl, we randomly sample 150,000 pros and 150,000 cons summaries. Regarding the digital camera domain, similar samples of 100,000 summaries (each) are sampled from a crawl of the websites Epinions.com, Buzzil-lions.com, and Reevoo.com.
The goal of our approach is to extend a general purpose sentiment lexicon with domain and aspect specific senti-ment expressions. As baseline approaches, we consider four different general purpose lexicons: The lexicon denoted as  X  X PQA X  is an excerpt of the MPQA Subjectivity Lexicon [40]. We only consider  X  X trongly subjective X  entries with pos-itive or negative polarity and further remove some stemmed forms, which results in a lexicon of 4,422 entries  X  X iu X  sentiment lexicon 8 is a dictionary that has been cre-ated specifically for the task of customer review mining. It consists of roughly 6,800 entries, explicitly containing many misspellings and colloquial expressions that are frequently used in user generated content. The lexicon has been created with the help of automatic methods (synonym/antonym ex-pansion), but was manually revised and extended over a period of many years. Besides these manually labeled lexi-cons, we consider the dictionary-based lexicon construction method described in [5]. This label propagation method re-lies on sets of positive seeds, negative seeds, and stop words. For this purpose, we manually compiled a list of 120 words (54 positive, 66 negative) with unquestionable prior polar-ity (e.g.,  X  X ood X ,  X  X ad X ,  X  X ike X ,  X  X ove X ,  X  X ate X ). The list of stop words comprises 638 entries. All words are tagged with part-of-speech information to guarantee a shallow word sense disambiguation. The seed words are not optimized for any of the two target domains (hotel or digital camera reviews). We adapt the original approach from [5] by con-sidering additional WordNet relations, namely  X  X imilar-to X ,  X  X erivationally related X , and  X  X ee also X . As a fourth base-line, we consider using the set of polar seed words alone, i.e., without further expansion by means of label propaga-tion. The configuration is denoted as  X  X eed X  in the following. Configurations prefixed with  X  X G X  indicate that label prop-agation with the Blair-Goldensohn et al. method [5] is used. Naturally, we also compare to our previous method as pre-sented in [7]. Configurations suffixed with  X  X E10 X  indicate the application of this approach.
Tables 2 and 3 show some exemplary aspect specific senti-ment expressions that our approach extracted from the two different datasets. We present ten examples with positive and ten examples with negative polarity for each domain. The examples are ordered by the confidence value as com-puted by our statistic test. We can observe that the method successfully determines the correct aspect specific polarity of words that are per se not polar. For instance, adjectives such as  X  X hin X ,  X  X umid X ,  X  X ndustrial X , or  X  X idden X  would not be marked polar in a general purpose sentiment lexicon. But in the context of hotel reviews the combinations  X  X hin sheet X ,  X  X umid room X ,  X  X ndustrial location X , or  X  X idden parking fee X  are correctly identified as expressing a negative opinion. We can also see that many words with ambiguous polarity are correctly classified. For instance, the word  X  X ow X  in the con-text of the aspect  X  X esolution X  has a negative connotation, the original lexicon consisted of roughly 8,700 entries, but led to inferior results in comparison
See http://www.cs.uic.edu/~liub/FBS/ sentiment-analysis.html for some further explanations. Table 2: Hotel dataset: exemplary aspect specific sentiment expressions. Table 3: Camera dataset: exemplary aspect specific sentiment expressions. whereas it expresses a positive opinion in the phrase  X  X ow image noise X .
This section reports the results of an intrinsic evaluation of our approaches. We manually inspect the correctness of each lexicon entry, which allows us to reason about the precision of the construction process (i.e., about the ratio of correct to incorrect entries). Regarding incorrect entries, we differenti-ate between two types of errors: 1) The construction process correctly identifies a polar term, but assigns the wrong polar-ity , e.g., ( X  X attery life X ,  X  X hort X )  X  +3 . 7 . 2) The construc-tion process generates an entry that is not polar at all, e.g., (*,  X  X nalog X )  X  +1 . 3 or ( X  X attery life X ,  X  X old X )  X  X  X  2 . 5 .
Table 4 reports the results for the approach of identifying domain specific polarity words. Manual inspection reveals that wrong polarity assignment occurs very rarely and the major source of error is the inclusion of non polar words. Looking more closely at the latter error type, we find that they predominantly stem from mistakes of the part-of-speech tagger 9 or from missing entries in the product aspect taxon-omy. Such missing entries may lead to mistakes if a modi-fier, which would actually be part of an aspect, is erroneously identified as a sentiment expression. In general, we conclude that our heuristics to derive domain specific sentiment ex-pressions are sufficiently accurate with precision values of around 88% (hotel) and 82% (camera). Typically, such au-tomatically extracted lexicons are manually revised. The high precision values help to reduce the effort entailed with such a manual revision.

Table 5 presents the results for constructing a lexicon of aspect specific polarity words. We observe very high preci-sion values of 94.5% (hotel) and 92.7% (camera). The pre-cision for extracting negative tuples is significantly higher with nearly perfect precision values. We assume that our extraction patterns exhibit a higher precision on the cons summaries than on the pros summaries. Confusing posi-tive with negative polarity is very unlikely to occur. Again, the major source of error is the inclusion of non polar en-tries. Further mistake analysis reveals, that most of these errors are either due to senseless combinations, such as ( X  X v X ,  X  X lean X ) or ( X  X oom size X ,  X  X lean X ), or due to misinterpreted modifiers, such as ( X  X v X ,  X  X latscreen X ) or ( X  X arking X ,  X  X nder-the tagger was not explicitly trained on pros/cons sum-maries Table 4: Accuracy of extracting domain specific sen-timent expressions. Table 5: Accuracy of extracting aspect specific sen-timent expressions. ground X ). Whereas the former errors are unlikely to affect precision in a real application, the latter may in fact decrease the accuracy.
Whereas the previous section gave insight into the quality of the lexicon construction process, we are now interested in whether our approaches actually help in a sentiment detec-tion task. For this purpose, we compare the classification performance of the baseline lexicons with and without addi-tion of the domain and aspect specific entries. In particular, we evaluate as follows:
To avoid unwanted influences, we assume that aspect de-tection works with perfect accuracy. Given a product aspect A from the annotated evaluation corpus, we try to match sentiment expressions E in the immediate context of A (re-stricting the context to the same extraction unit as A and limiting it to a window of size  X  window = 5 around A ). If a phrase E in the context matches a lexicon entry, we look up the corresponding polarity value. In case of an aspect-aware lexicon, we look up the tuples ( A,E ) or (  X  ,E ). In case of a general purpose lexicon, we ignore the aspect A . If multiple matches are found, we evaluate the correctness of each individual match. A match is a true positive if it ex-actly matches the span of a sentiment expression annotation in the evaluation corpus and also agrees with it in terms of the polarity value. We evaluate the approaches in terms of precision, recall, and F-measure.

We extend the baseline lexicons by simply adding the do-main and/or target specific sentiment expression. In case of a conflict, we overwrite an existing lexicon entry. For the lexicon created with the Blair-Goldensohn method, we also examine an alternative approach. Instead of adding domain specific entries, we consider them as additional seeds for the label propagation step. We indicate this approach with the notation  X  X G-d X . The other approach is labeled  X  X G+d X . Adding aspect specific expressions is indicated with the no-tation  X  &lt; lexicon-name &gt; +a X .

Tables 6 and 7 show the results of extrinsic evaluation for both datasets. We use the four original (non-extended) lex-icons  X  X eed X ,  X  X PQA X ,  X  X G X , and  X  X iu X  as references. Num-bers in brackets refer to the differences calculated in com-parison to the respective reference lexicon. When comparing the baseline approaches, we can observe that the Liu and Blair-Goldensohn lexicons perform best, with F-measures Table 6: Hotel dataset: sentiment classification per-formance with different lexicons. between 71% and 75%. The MPQA lexicon exhibits F-measures which are about 20 percentage points lower, which is mostly due to its low recall. The reason is mainly that the Liu lexicon as well as our seed word list are tuned towards the domain of customer reviews. Interestingly, even the very small seed word list performs as good or even better than the MPQA lexicon (we therefore did not further consider the MPQA lexicon). In general, the precision with the baseline lexicons is quite high, while the recall is much lower. Table 7: Camera dataset: sentiment classification performance with different lexicons.

Adding the domain and aspect specific expressions sig-nificantly improves the classification performance. Regard-ing the Liu baseline, we observe increases in F-measure of up to 9.4 (hotel) and 6.5 (camera) percentage points (con-figuration Liu+d+a). For the Blair-Goldensohn baseline, the numbers are 5.7 (hotel) and 7.8 (camera) percentage points. The results further show that both types of entries (domain and aspect specific) lead to improvements and that the types complement each other. The major reason for the increased F-measure is a significantly higher recall. Observe that all three extended baseline lexicons exhibit a similar performance. Even the much smaller extended seed lexicon performs only slightly worse than the other extended lexi-cons. This shows that our extraction methods successfully find a great share of the sentiment expressions that are rel-evant for a particular domain. Also when comparing to the Table 8: Results with different expansion strategies. method described in [7] (suffix  X  X E10 X ), we find that our approach consistently achieves significantly better results.
Regarding the BG-approach, we experimented with two different strategies of integrating the domain specific entries. Comparing the results for the two strategies (configurations BG-d vs. BG+d and BG-d+a vs. BG+d+a), we do not find significant differences in classification performance. When using the domain specific words as additional seeds (BG-d and BG-d+a), the resulting lexicons are much larger than with the other strategy and we exhibit higher recall values (at the cost of reduced precision). Considering the marginal differences in F-measure and the reduced lexicon sizes, we conclude that simply adding the domain specific entries is the better strategy.
Table 8 shows our results with the two different strategies of expanding the set of aspect specific expressions. Here, we report a negative result; our hypothesis that expansion via label propagation in WordNet would improve the lexicon coverage could not be affirmed. Neither of the two strategies leads to significantly better results. Although the lexicon size increases by around ten percent, the effect on classifica-tion performance is marginal with differences of 0.1%. More closely inspecting the really target-specific sentiment expres-sions in our corpora shows that most of them describe (phys-ically) quantifiable properties such as size, length, weight, speed, temperature, or brightness. We further find that the lexical variability to describe such properties is rather low. For instance, to describe the size of an entity the vast major-ity of reviewers refers to only six adjectives ( X  X mall X ,  X  X ittle X ,  X  X arge X ,  X  X ig X ,  X  X iny X , and  X  X uge X ). However, nearly all of these types of adjectives are already included within our ex-tracted target-specific lexicon. Thus, further expanding it does not lead to better results.
In this paper we examined the problem of automatically constructing domain and aspect aware sentiment lexicons from semi-structured customer reviews. In particular, we proposed methods to exploit the information contained in pros and cons summaries of reviews. We could demonstrate that the approach successfully learns new domain and as-pect specific sentiment words. Manual inspection showed that the construction process is highly accurate and is capa-ble of finding a huge number of relevant sentiment expres-sions. Further quantitative evaluation with several general purpose sentiment lexicons demonstrated that the addition of the learned domain and aspect specific words significantly improves the performance in a sentiment classification sce-nario. Expansion of the aspect specific words via a thesaurus did not further improve the results. A main advantage of our approach is that it is fully automatic and thus scales well across domains. [1] A. V. Aho and M. J. Corasick. Efficient string [2] A. Andreevskaia and S. Bergler. When specialists and [3] S. Baccianella, A. Esuli, and F. Sebastiani.
 [4] F. Baron and G. Hirst. Collocations as cues to [5] S. Blair-Goldensohn, K. Hannan, R. McDonald, [6] K. Bloom, N. Garg, and S. Argamon. Extracting [7] J. Bross and H. Ehrig. Generating a context-aware [8] X. Cheng and F. Xu. Fine-grained opinion topic and [9] Y. Choi and C. Cardie. Adapting a polarity lexicon [10] K. Dave, S. Lawrence, and D. M. Pennock. Mining the [11] X. Ding, B. Liu, and P. S. Yu. A holistic lexicon-based [12] W. Du, S. Tan, X. Cheng, and X. Yun. Adapting [13] T. Dunning. Accurate methods for the statistics of [14] A. Fahrni and M. Klenner. Old wine or warm beer: [15] J. R. Firth. A synopsis of linguistic theory 1930-55. [16] N. Godbole, M. Srinivasaiah, and S. Skiena.
 [17] U. Gretzel and K. H. Yoo. Use and impact of online [18] V. Hatzivassiloglou and K. R. McKeown. Predicting [19] L. Hoang, J.-T. Lee, Y.-I. Song, and H.-C. Rim. [20] M. Hu and B. Liu. Mining and summarizing customer [21] J. Jansen. Online product research, Sept. 2010. [22] V. Jijkoun, M. de Rijke, and W. Weerkamp.
 [23] N. Kaji and M. Kitsuregawa. Building lexicon for [24] J. Kamps, M. Marx, R. J. Mokken, and M. De Rijke. [25] H. Kanayama and T. Nasukawa. Fully automatic [26] M. Klenner, A. Fahrni, and S. Petrakis. PolArt: A [27] Y. Lu, M. Castellanos, U. Dayal, and C. Zhai. [28] R. Mihalcea, C. Banea, and J. Wiebe. Learning [29] G. A. Miller. WordNet: A lexical database for English. [30] B. Pang and L. Lee. Opinion Mining and Sentiment [31] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: [32] L. Polanyi and A. Zaenen. Contextual valence shifters. [33] A.-M. Popescu and O. Etzioni. Extracting product [34] G. Qiu, B. Liu, J. Bu, and C. Chen. Expanding [35] D. Rao and D. Ravichandran. Semi-supervised [36] H. Takamura, T. Inui, and M. Okumura. Extracting [37] P. D. Turney. Thumbs up or thumbs down?: Semantic [38] P. D. Turney and M. L. Littman. Measuring praise [39] L. Velikovich, S. Blair-Goldensohn, K. Hannan, and [40] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing [41] J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. [42] X. Zhu and Z. Ghahramani. Learning from labeled
