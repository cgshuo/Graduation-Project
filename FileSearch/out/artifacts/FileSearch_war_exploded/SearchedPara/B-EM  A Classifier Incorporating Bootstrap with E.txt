 This paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy. This is significant for many applications such as image clas-sification where obtaining classification labels is expensive, while large unlabeled examples are easily available. We in-vestigate an Expectation Maximization (EM) algorithm for learning from labeled and unlabeled data. The reason why unlabeled data boosts learning accuracy is because it pro-vides the information about the joint probability distribu-tion. A theoretical argument shows that the more unlabeled examples are combined in learning, the more accurate the result. We then introduce B-EM algorithm, based on the combination of EM with bootstrap method, to exploit the large unlabeled data while avoiding prohibitive I/O cost. Experimental results over both synthetic and real data sets show that the proposed approach has a satisfactory perfor-H.4.m [Information Systems]: Miscellaneous Expectation Maximization, Classification, Supervised and Unsupervised learning, Bootstrap Method 
Classification has been identified as an important problem in data mining field. There has been focus on algorithms [17, The intuition there is that by building classifier over large training data sets, we will be able to improve the accuracy of the classification model. One key difficulty with these current algorithms is that the assumption requires a large, permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ... $5.00. 
However, in many modern classification applications such 
Consider the problem of training an image classifier to au-
In this paper, we investigate an algorithm that learns The remainder of the paper is structured as follows. In unsupervised learning. In Section 3, we formally introduce how to extend EM over labeled and unlabeled training data. We present our new algorithm B-EM which is applicable to large training data set. In Section 4, we present experimen-tal results and performance evaluation over both synthetic and real data sets. We present the conclusion and address the future work in Section 5. 
The EM algorithm [7] is a general technique for finding maximum likelihood estimates for parametric models when the data are not fully observed. EM has been well studied for unsupervised learning and has been shown to be supe-rior to other alternatives for statistical modeling purposes [6]. The well-known AutoClass project[6] investigates the combination of the EM algorithm with naive bayes classifier where they emphasize how to discover clusters for unsuper-vised learning over unlabeled data. Bradley et al. present a scalable clustering framework where they apply EM al-gorithm to the data summary instead of the original data [2]. The algorithm presented effectively scales to very large databases as it requires at most one scan of the database. However, it is unknown how to apply the above algorithm for supervised learning. 
Ghahramani et al. present a framework [9] based on max-imum likelihood density estimation where EM is applica-ble both for supervised and unsupervised learning problems. For example, by estimating the joint density of the input and class label using a mixture model, the classification problems can be thought learning a mapping from an input space into a set of discrete class labels. 
The theoretical work [3, 4] shows use of unlabeled data can improve parameter estimates of mixture model. The results can be highlighted as following 1) unlabeled data does not improve the classifiaton results in the absence of labeled data; 2) the classification error approaches the bayes optimal solution at an exponential rate in the number of labeled examples given if infinite amounts of unlabeled data are available; 3) the labeled data can be exponentially more valuable than unlabeled data in reducing the probability of classification error; 4) the additional unlabeled samples should always improve the performance. 
Nigam et al. introduce an algorithm for learning from labeled and unlabeled text, based on the combinations of EM with a naive bayes classifer and show that the accuracy of learned text classifiers be improved by augumenting a small number of labeled training documents with a large pool of unlabeled documents [14]. 
We are given n records in a training set S = SIUS u. Each d measurements made on the data from d attributes, rep-from m classes C = {cl,... ,cm}. The record si E S l comes with the known class label yi E C, and for the rest of the records, in subset S ~', the class label yi is unknown. 1 x~ contains the relevant information required for measuring the similarity between the data. 
Now the learning task is, given S = S z US t', how to build a classifer which can predict yi based on xi for new data si E S t, where S t is test data sets . Note the traditional classifcation approach is to build the classifer only based on labeled training data SL 
In this paper, we will apply a mixture model for charac-terizing the nature of the data and classifiers. The mixture model follows two commonly used assumptions about the data: 1) the data are produced by a mixture model; 2) each record only belongs to one class and there is a one-to-one cor-respondence between the components in the mixture model and classes. 
The mixture model, as shown in Equation 1 has two parts: the first part gives the interclass mixture probability P(cs) that an example sl is a member of class c5, independently of anything else we may know of the data; the second part by a class distribution (component), giving the probability of observing the instance attribute values xi, conditional on the assumption that instance si belongs in class c 5. 
The interclass pdf is a Bernoulli distribution characterized by the class number m and the probabilities of each class. As the distribution of each class is unknown, the multi-dimensional Gaussian distribution is usually assumed for it provides a good approximation for unknown distributions. In this paper, we assume all attributes xi are continuous variables 2 . 
Equation 2 shows a d-dimensional Gaussian distribution for class c5, where j = 1,... ,m, #5 is d-dimensional mean vector and E5 is d  X  d covariance matrix, the superscript T indicates transpose , I~jl is the determinant of ~j and ~-1 is its matrix inverse. 
In this setting, each record, si, is created by first selecting a component according to priors P(cj), then, second, having the mixture component generate the record according to its own distribution P(xilcj, 0j) with parameters 0j 3. 
Under this framework, classification problems can be solved by estimating joint density of the known attributes and class label using a mixture model and computing a maximum like-lihood estimate of 0, i.e., finding the parameterization that is most likely given our S. EM is a widely used iterative technique which can concurrently generate probabilistically-assigned labels for the unlabeled data, and a more probable model with smaller parameter variance that predicts these same probabilistic labels. 
When traditional classifier approach such as naive bayesian, decision trees etc. is given a small set of labeled training 2Discrete or categorical data can be modeled as generated by a mixture of multinomial densities and similar derivations for the learning algorithm can be applied. 3Under the assumption of Gaussian distribution, 0j includes pj, ~j. data, classification accuracy suffers. This section shows, by augmenting this small set with a large set of unlabeled data and combining the two sets with EM, we can improve the parameter estimates and hence classification accuracy. 
Consider the probability of all the labeled and unlabeled work. The probability of the whole data is simply the prod-uct over all the data shown as, here we assume one record is independent with the others. 
The likelihood of an unlabeled record can be characterized as the sum of total probability over all mixture components. 
For the labeled record, we are given the label yi do not need to sum over all class components as shown, 
When combining both labeled and unlabeled records, the probability of the whole training data set is shown as Equa-tion 3. 
By the maximum likelihood principle, the best model of the data has parameters that maximize P(OIS ). Equation 4 shows the log likelihood of the parameters given the data set. 
Note the first part is a constant for P(S) is a constant and maximum likelihood estimation assumes that P(O) constant. 
However, the second part of this equation has a log of sums, it is not easily maximized numerically. Intuitively, it is unclear which component of the mixture model gen-erated a given record and thus which parameters to adjust to fit the feature value of that record. When all the class labels in S t are given, we could express this complete log likelihood of the parameters without a log of sums [9]. We introduce the binary indicator z~ i where zij = 1 iif yi = cj else zij = 0 in Equation 5. By introducing a hidden vari-able z that indicate which record was generated by which component, then the maximization problem decouples into a set of simple maximizations. 
Since z is unknown, the log(P(O}S, z)) can not be utilized directly. The EM algorithm can be used to find a local maximum likelihood parameter by an iterative procedure through the following two steps. 
The parameter 0 generated by EM that locally maximizes the probability of all the data (both the labeled and unla-beled) will be used to label the test data with the largest posterior probability. In this paper, we assume that the size of labeled data set S z is small, hence, it can be easily fitted in memory. While memory, computing a mixture model over large databases via standard EM would not be acceptable as hundreds of it-erations or more may be required during iterative EM refine-ment step. One straightforward approach is to sample un-labeled records as many as the memory can hold. As shown from theoretical work [3, 4], the more additional unlabeled data, the more accuracy we achieve. Although guaranteed to converge, a general bound on the number of unlabeled data required for a given training data set is not available. As shown in experiment results, the training data set gener-ated by class distributions with larger variances needs more unlabeled data to converge. In this paper, we combine the EM with bootstrap methods [5] to classify with large sizes of training set. 
The resulting B-EM algorithm is very straightforward and can be outlined at a high level as follows: 1. Build an initial classifier by estimating the parameters of model from the labeled data only. 2. Repeat M times  X  Obtain a radom bootstrap sample from unlabeled  X  Repeat until the parameters 0 z do not change 3. compute 0* = ~i=1~ M 0 z. 4. Apply the 0* to probabilistically label all the unlabeled data. ^ Ideally, we would like to say that /9" is very close to the 0 computed over all records in training set. The bootstrap principle [5] shows the two estimates converges the same when bootstrap steps M is sufficiently large (e.g., 100). We can see the B-EM needs at most two scans of data while achieving almost the same accuracy. When the train-ing data set is totally unordered, we get bootstrap samples by randomly fetching from disk block. In this case, we only need one scan (the step 4 which labels all unlabeled data). 
When the training data set is not totally unordered, we need one more read scan to generate M bootstrap samples and write each samples to disk. In this case, the total number of scans is bounded by two. It is also worth pointing out B-EM also saves CPU cost. The explaination has two parts. First, for B-EM, the number of unlabeled data invloved in 
EM iterative steps is less than standard EM. Second, EM empirically tends to need fewer iterations with less data. In this paper, we also combine EM with naive bayesian approach which assumes the values of the attributes are con-ditionaly independent of one another when the number of dimensions is too large. The explaination has two parts. 
First, in estimating P(ej]x~, 0) as shown in Equation 6, we need compute, ~-t, the inverse of covariance matrix. When the values of the attributes axe not conditionaly indepen-dent, the covariance matrix Ez is singular which causes the estimation overflow. Second, we can reduce computation cost in evaluating P(x[cj, 0j) involved in EM. Under the naive bayesian assumption, the class distribu-tion shows as, The probabilities P(x k [cj) can be estimated from the train-ing sample, where each attribute A k is assumed to have a normal distribution with mean #~t and standard variance ing samples of class c a. The experiments are conducted on a Dell PowerEdge 4400, with two processors and 1G bytes of RAM. to 5M. As can be observed, the B-EM method scales well the standard EM gets to be impraticial for large sizes (more that B-EM is running faster than the full in-memory EM algorithm for data sets that fit in memory: 100k, 200k and section. Note the classification accuracy of B-EM and EM for this experiment is almost the same (the difference is less than 0.5%). 
EM over the Corel images collection [15]. The 68,040 images are cataloged into broad categories. Four sets of features, color histogram, color histogram layout, color moments, and exploit the huge volume of unlabeled data while avoding I/O cost. to improve the accuracy of classifiers when 1) the probability distribution that generates the underlying data can be de-~ 25 unlabeled images scribed as a mixture distribution and 2) there is a one-to-one correspondence between the components and class labels. 
However, the complexity of real-world applications will not be completely captured by statistical models and the real-world data is not totally consistent with the assumptions of the model. For example, we assume one Gaussian distribu-tion over feature space for mamal in our experiment with 
Coral data set. This assumption is clearly not true. Our fu-ture work will investigate how to build mixture models over concept hierarchy. Another interesting direction for future work with unla-beled data is how to build an incremental learning algorithm for stream data [1] where the unlabeled data is infinitely available. The incremental algorithm may expliot the un-labeled test data received in the testing phase to improve performance on the later test data. The authors would like to thank Michael Ortega-Binderberger for his help in providing the labels of Corel images. [1] S. Babu and J. Widom. Continuous queries over data streams. SIGMOD Record, 30(3), Sept 2001. [2] P. S. Bradley, U. M. Fayyad, and C. A. Reina. Scaling clustering algorithms to large databases. In pages 9-15, August 1998. [3] V. Castelli and T. Cover. On the exponential value of labeled samples. Pattern Recognition Letters, 6:105-111, 1995. [41 V. Castelli and T. Cover. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameters. IEEE Transaction on 
Information Theory, 42(6), 1996. [5] B. Chapmann and R. Tibshirani. An Introduction to 
Probability. Chapman and Hall, 1993. [6] P. Cheeseman and J. Stutz. Bayesian classification (autoclass): Theory and results. Advances in 
Knowledge Discovery and Data Mining, 1996. [7] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em algorithm. 
Journal of the Royal Statistical Society, 39(1):1-38, [8] J. Gehrke, V. Ganti, R. Ramakrishnan, and W.-Y. Loh. 13oat-optimistic decision tree construction. In 
Proceedings of the SIGMOD Conference, pages 169-180, 1999. [9] Z. Ghaharmani and M. Jordan. Supervised learning from incomplete data via an em approach. Advances in Neural Information Processing Systems 6, 1994. [10] D. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning. Morgan 
Kanfmann, 1999. [11] M. James. Classification Algorithms. Wiley, 1985. [12] R. Lippmann. An introduction to computing with neural nets. IEEE ASSP Magazine, 4(22), 1987. [13] M. Mehta, R. Agrawal, and J. Risanen. Sliq: A fast scalable classifier for data mining. In Proceedings of 
Database Technology, March 1996. [14] K. Nigam, A. Mccallum, S. Thrun, and T. Mitchel. 
Text classification from labeled and unlabeled documents. Machine Learning, 39(2/3):103-134, 2000. [15] M. Ortega-Binderberger. Cord image features. [16] J. Quinlan. Induction of decision trees. Machine 
Learning, 1:81-106, 1986. [17] J. Sharer, R. Agrawal, and M. Mehta. Sprint: A scalable parallel classifier for data mining. In 
Proceedings of the 2Pnd VLDB Conference, pages 544-555, 1996. [18] 13. Shanhshanhani and D. Landgrebe. The effect of unlabeled samples in reducing the small sample size problem and mitigating the hughes phenomenon. 
Sensing, 32(5):1087-1095, 1994. 
