 Abnormal event detection is one of the most important problems in analyzing video sequences. Consider the situation that you are facing with a long video sequence, possibly thousands of hours in duration, but all you want is to find critical events among those video sequences. Anomaly detection, in the context of activity recognition, is the process whereby a baseline of normal behavior is established, with deviation from this norm triggering an alert.

Detecting unusual activities is difficult since unusual events by definition rarely occur, they may be unexpected as Zhang et al.[3] suggests, but at the same time they are relevant for the task. This difficulty becomes more significant during training due to the fact that we are in short of such labeled sequences of unusual activities. Traditional framework for abnormal event detection is mainly addressed by state-space models and differential equation models, such as works in [2][6][7][29].
To address the complexity of time-series data, parametric approaches such as state-space models have to address the problem of model selection. In this context, a methodology for defining an appropriate set of states constitutes a key issue, among those available methods, state decomposition has typically been performed in an adhoc manner [30], this method essentially is a trial-and-error process to choose the number of states. In some particular settings, such as tracking-based abnormal event detection problem, we always can calculate many latent useful features after the tracking procedure, and always wish to select a number of critical features from those features [27], if we choose the Bayesian classifier, we have to decide the number of states for every state-space models[14]; this is impossible if the total number of features are large. Thus, in a word, previous state-space models all suffer the problem of deciding an appropriate number of features, and a non-optimal number of features decided would affect the overall performance a lot.
 Here we apply the Hierarchical Dirichlet Process Hidden Markov model (HDP-HMM) [17], which by construction has an infinite number of states and can decide the suitable, or optimal, number of states automatically. Thanks to the effec-tiveness of HDP-HMM, we can propose a new feature selection algorithm which calculates the confidence vector by an ensemble of HDP-HMM-based weak clas-sifiers. This ensemble of weak classifiers forms a general model, whose threshold is trained mainly to reduce the false positive rate. After we detected abnormal events from this general model, we filter out the misclassified normal activities later on, in order to reduce the false negative rate. Our experimental results show the effectiveness of this framework.

In the remaining of this paper, we will first review relevant background in abnor-mal event detection in Section 2. In Section 3 we describe our proposed three-phrase framework to tackle the problem of abnormal event detection in detail. In Section 4 we present the experimental results to show the effectiveness of our algorithm. Finally we conclude the paper and provide possible directions for future work. There are many important previous work done in the problem of abnormal ac-tivity recognition. However, due to a lack of space, we can only briefly review some related works using state-based models. Another important point in tack-ling in this problem is the feature selection step, which we will also review some important works.

Most of the previous work proposed in the topic of event detection [2][5][7][14] were centered on the recognition of predefined events in particular situations[13]. Now we consider the problem of abnormal event detection, and according to [7], the approaches for abnormal event detection can be divided into: similarity-based approaches and state model-based approaches. In model-based approaches, HMMs[2][6][7], and other graphical models[5] are most widely adopted.
In this paper, we only care about the trajectories of the objects, which is the results of object movement tracking [12]. Based on the tracking result we can, al-most always, get a better result than by just using the original image sequences, because more useful and meaningful features can be computed from these tra-jectories, for example, semantic primitives can be included [29]. To address the problem of feature selection, which by definition is calculated from the object moving trajectories, [6] proposed a hybrid discriminative / generative approach, where the useful features were extracted by a variation of the AdaBoost algo-rithm proposed in [27]. At the same time [14] suggested several useful methods for feature selection, including Relief algorithm[15][16]. In this section, we present our three-phase framework for abnormal event de-tection from video surveillance. Intuitively, tracking-based abnormal detection techniques typically compare a profile of all normal activity sequences to objects X  moving coordinates, we should calculate candidate features from those coordi-nates first, then use feature selection techniques to choose critical features to train the model. Any deviation from the model is flagged as a potential alarm.
Here we provide an overview of our proposed framework. First we will use every single calculated features to train an HDP-HMM based classifier using Beam Sampler in a sequential manner, by deciding a suitable model for every feature automatically.

In the second phase, we apply a new AdaBoost-like algorithm to train an en-semble of these weak classifier and get a confidence vector for later classification use, we propose a new algorithm because classical algorithms such as Relief[15] and its variants[16], and ensemble learning framework such as AdaBoost[27] can-not be applied directly to our setting, as explained later in this section. These weak classifiers can identify normal activities with a higher likelihood and assume that everything else is abnormal with a lower likelihood, and when choosing the threshold for the general model, we always intend to reduce the false positive rate, but this somehow involved with increasing the false negative rate. There-fore, abnormal event detection has the potential to detect even the unknown events that have rarely happened[7].
 Based on this intuition, our proposed algorithm goes to a third phase process. We adapt the already well-estimated normal event model to a particular unusual event model using detected outlier if low likelihood observed[3], i.e, we start from the usual event model, and move towards an unusual event model in some con-strained way, this process is proved to be able to effectively reduce false negative rate. In Section 3.1, we quickly review the Hierarchical Dirichlet Process and Gibbs Sampling methods, and introduce the sequential beam sampling meth-ods. In Section 3.2, we briefly introduce how we calculate features from input coordinate data and how we build an ensemble of weak classifiers. Finally in Section 3.3, we describe how we build suitable model adaptation techniques. 3.1 HDP-HMM and Beam Sampling Method Hierarchical Dirichlet Process Hidden Markov Model. Consider groups j, J denotes the total number of groups thought to be produced by related, yet unique, generative processes. Each group of data is modeled via a mixture model[17]. A Dirichlet Process(DP) representation may be used separately for each of the data group. In an HDP manner, the base distribution of each of the DPs are drawn from a DP, which is discrete with probability 1, so each of the DPs can share the statistical strength, for instance, this encourages appropriate sharing of information between the data sets. An HDP formulation can decide the right number of states for the hidden Markov model(HMM); from its posterior density function on the appropriate number of mixture components, to some extent, the number of states in HMM can go to infinite if necessary. Besides, it learns the appropriate degree of sharing of data across data sets through the sharing of mixture components.

The HDP could be built as follows (Due to space constraint, we will omit the detailed explanation of HDP in this paper, interested readers could refer to [17] for details):
G 0 (  X  )= G j (  X  )=
Where GEM (  X  ) stands for the stick-breaking process [17] as follows: To better illustrate the construction of HDP-HMM, we introduce another equiv-alent representation of the generative model using indicator random variables  X   X  GEM (  X  )  X  j  X  DP (  X ,  X  ) z ji  X  Mult (  X  j )  X  k  X  H (  X  ) y ji  X  F (  X z ji ) Identifying each G ( k ) as describing both the transition probabilities  X  kk from state k to k and the emission distributions parametrized by  X  k , we can now formally define the HDP-HMM as follows: The Beam Sampler. The beam sampler[18] is a successful combination of slice sampling and dynamic programming, it does not marginalize out neither  X  nor  X  . Specifically, the beam sampler iteratively samples the auxiliary variables u , the trajectory s , the transition probabilities  X  , the shared DP parameters  X  and the hyperparameters conditioned on all other variables. We describe the Beam Sampler as follows:
Sampling u : u t  X  Uniform(0 , X  s
Sampling s : Due to the fact that only the trajectories s with  X  s all t will have non-zero probability given u , and there are only finite number of such trajectories due to the nature of stick-breaking process, as a consequence, we can apply dynamic programming to do the sampling job as classical HMM.[18] From (9), we can apply the classic dynamic programming[25] to sample the whole trajectory s .

Sampling  X  , m jk is as follows[17]:
Sampling m jk : where s ( n, m ) are unsigned Stirling numbers of the first kind.

The conditional distribution of the observation y t given an assignment s t = k and given all other observations y  X  , having marginalized out  X  k , is derived as follows: p ( y t | y \ t ,s t = k, s \ t , X  )  X 
Towards each  X  k we notice that[17] they are independent of others condi-tional on s , y and their prior distribution H . One of the greatest merits of Beam Sampler is that even if the base distribution H is not conjugate to the data dis-tribution F , an effective and simple Metropolis-Hastings could be developed[18]. 3.2 Feature Calculation and Selection In this section, we mainly introduce the feature calculation and selection method we adopted in our abnormal event detection framework.
 Feature Calculation. Our abnormal event detection framework is based on the results of object tracking, so the first step of our method is to calculate useful features from the input coordinate sequences. There are many methods to calculate useful features, we adopt the method proposed in [14]. In [14], fea-tures are derived from the target global position provided by a tracker, and they can be divided into three groups:(1) instantaneous measurements, (2) aver-age speed/velocity based features and (3) second order moments/energy related indicators.
 Feature Selection. A variation of AdaBoost algorithm for classifier learning is provided by [27] for feature selection, AdaBoost is widely adopted in practice[6], but it X  X  mainly designed for the discriminant classifiers. Therefore it X  X  not suit-able for our generative HDP-HMM based approach. At the same time, Relief algorithm [15] and its variations [16] are widely accepted in feature selection ap-plication, However, due to the fact that the majority of our training data always come from only one class, so neither can we apply Relief algorithm directly. For our HDP-HMM based generative model, we proposed a new AdaBoost-like algo-rithm as below, which combines merits from those two famous algorithms listed above, namely, our algorithm favors the feature that distinguishes the data best, and this algorithm belongs to an ensemble learning framework. Here we describe our algorithm in the following steps: 3. Normalize the votes into weights: u i,j  X  v i,j V 4. for t=1:T, T is the number of iterations 5. Do the same procedure for the abnormal sequences 6. then we get two vectors for both the abnormal and normal training se-7. The final classifier is: Result ( i )= J j =1 H ( j )  X  Likelihood ( J ). 8. Sequence(i) is identified as abnormal if Result ( i ) &lt; T hreshold . In our framework, we always choose a threshold with lower false positive rate, to make sure most of the outliers / abnormal activities can be detected and sent to the third phase. 3.3 Model Adaptation The abnormal event models are derived from a general normal model in an unsupervised manner, and the benefit of such an unsupervised manner is that this framework can address the unbalanced label problem due to the scarcity of training data and the difficulty in pre-defining abnormal events, more specifically, after the second step we may get a high false negative rate, so it X  X  necessary for us to apply a third phase, that is, to adapt models for the abnormal events, and use these abnormal classifiers to reduce the false negative rate. Besides, due to the lack of negative training data, we can not directly build models for abnormal events. However, we can use adaptation techniques to get them during the test time or even in the future use, that is, we can dynamically build the model for the abnormal event after the training phrase. Here we briefly introduce the algorithm X  X  framework[3][7] first. The steps are listed as below:
Prerequisites: A well defined general HDP-HMM with Gaussian observation density trained by all normal training sequences.
 Step 0 : Using the first outlier detected from the former phase -which is consid-Step 1 : Slice the test sequence into fixed length segments, calculate the likeli-Step 2 : If the maximum likelihood is larger than the threshold, we consider this Step 3 : Using adaptation method to adapt the general model to a new abnor-Step 4 : Goto Step 11 if new outlier comes.

In this procedure, we provide the outlier a second chance to be recognized as a normal events, in this way, we try to identify those normal events that are misclassified due to their unexpectedness or scarcity in training data. Thanks to the effectiveness of beam sampler again, we can do the adaptation effectively without other special design. Suppose now we have the new parameters for the HDP-HMM  X  , here the parameters that are adapted are the HDP parameter  X ,  X  0 , X ,K and the  X ,  X  for the HMM.

Here a latent problem for the adaptation is that, we can no longer use traditional adaptation algorithms, such as Maximum Likelihood Linear Regres-sion(MLLR) and Maximum a posterior(MAP), or Kernel Nonlinear Regres-sion(KNLR) because the number of states K will change due to the adaptation, leading to the different dimensions between old and new variables, we adopted a heuristic method to deal with the problem, as will be discussed in the experi-mental part, and we will discuss it later in the final section. In this section, we first describe our dataset and performance measures, then we describe the baseline systems we use to evaluate our results. At last we illustrate the effectiveness of the proposed algorithm framework using the dataset. 4.1 Dataset Description We have concentrated on low-level, short term events, especially human activi-ties, as those studied in the context of the CAVIAR project [28]. We will explore the availability of a large dataset of video sequences and manually classified ac-tivities, in a total of about 16,000 images with ground truth data. The activities (classes) considered can be detected from a relatively short video sequence (a few seconds) and are described in Table 1, we train the general model from a portion of the set of normal sequences, and test the model for the rest of normal sequences and the other sets of abnormal sequences, all of the data is available at [28].
 4.2 Performance Measures One of the most challenging problems involved with abnormal detection is the different misclassification costs corresponding to different types of error[19]. Ac-cording to [7], cost-sensitive learning are useful for learning problems where the classes are extremely unbalanced. And in most of the time it X  X  more important to recognize the negative class. We usually apply cost-sensitive learning under such situation, for example, Receiver Operating Characteristic (ROC) curves are recommended instead of accuracy [20]. In [21], the authors show that class-imbalance often affects the performance of cost-sensitive classifiers: when the misclassification costs are not seriously unequal, cost-sensitive classifiers gener-ally favor natural class distribution although it might be imbalanced; when mis-classification costs are seriously unequal, a balanced class distribution is more favorable.

As we mentioned above, the problem of abnormal event detection can be recognized as a two-class classification problem, where the positive class(normal events) takes dominating majority in the whole training data, and so the tradi-tional accuracy-based performance measure is no longer suitable for this setting. Here we evaluate the performance of the abnormal event detection algorithm framework in terms of two error rates: detection rate and false alarm rate. The detection rate is the same as true-positive rate while the false alarm rate is the same as false positive rate. 4.3 Results on CAVIAR Sequences In order to evaluate the performance of our proposed algorithm, experiments were carried out on a real data set from [28]. For comparison, two other ap-proaches were used as baselines. The first one is composed by HDP-HMM + AL, the second one is composed by HDP-HMM LITE + Adaptation, and our proposed algorithm is referred to as HDP-HMM + AL + Adaptation, here AL is referred to our proposed AdaBoost-like feature selection algorithm, and LITE is referred the feature selection algorithm proposed in[14], which is proven to be effective to the problem of activity recognition also based on data set[28].
We randomly select a 50% subset of normal event sequences(50 sequences) for training, these sequences are of 13 KB size in average. The other 50 normal traces and all the 20 abnormal traces were randomly mixed together for testing. Fig. 1 shows the ROC curve with respect to the detection rate and the false alarm rate.
We can see from Figure 1 that, HDP-HMM + AL gives the poorest detection result because it achieves a high detection rate at the cost of incurring a high false alarm rate. From Figure 1, it is also noteworthy to mention that our AL method for feature selection performs much better than the LITE algorithm, which will achieve a perfect detection rate of 1 at the cost of 0.4 false alarm rate, while LITE algorithm requires a 0.6 False Alarm Rate when the detection rate is 1.
Furthermore, after we adopted the Adaptation technique, both HDP-HMM + LITE + Adaptation and HDP-HMM + AL + Adaptation received better results. This is because the adapted abnormal activity models contribute to distinguishing normal and abnormal activities, which can significantly reduce the false alarm rate, in this sense the formerly misclassified normal events get a second chance to be differentiated from the abnormal events. The essential advantage of AL, compared to LITE is that it not only gains the information of how the feature contributes to classification. In this paper, we propose a novel approach for abnormal event detection. To deal with the scarcity of training data for abnormal events, we proposed a three-phrase abnormality detection algorithm. In the first phrase, we train a set of weak classifiers based on HDP-HMM, in particular, we have chosen beam sam-pling for inference, which combines slice sampling with dynamic programming. In the second phase, we developed an ensemble learning algorithm to filter out abnormal events, and the suspicious traces are then passed on to a collection of abnormal events models adapted from the general model for further detection. The major advantage of our approach is that HDP-HMM can naturally capture the underlying features and the hierarchical detection phase can achieve a bet-ter tradeoff between detection rate and false alarm rate. We demonstrate the effectiveness of our approach using real data collected by CAVIAR PROJECT.
One limitation of our approach is the adaptation technique we adopted in this framework. We have chosen HDP-HMM because it does not select a fixed number of states, and a full posterior density function is inferred for all model parameters, this introduces one problem: since the number of parameters may change after the adaptation, as a result, we can not employ classic adaptation techniques such as MLLR[6]. Our solution is just based on the sequential nature of beam sampler but not consider the mismatch between an initial model and the adaptation data. Therefore we need to be able to develop an adaptation technique to address this problem. Another latent problem involved with the adaptation step is that there is a risk of generating a large number of abnormal models when abnormal activities suddenly become the norm, as suggested in [7], we need to detect when abnormal activities turns normal and vice versa. We would like to acknowledge support for this project from the National Sci-ence Foundation of China(NSFC grant No.60775046 and No.60721002) and the National Grand Fundamental Research 973 Program of China (grant No.2009CB320700).

