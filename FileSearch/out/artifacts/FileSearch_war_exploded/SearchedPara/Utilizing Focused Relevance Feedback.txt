 We present a novel study of ad hoc retrieval methods utiliz-ing document-level relevance feedback and/or focused rele-vance feedback ; namely, passages marked as (non-)relevant. The first method uses a novel mixture model that integrates relevant and non-relevant information at the language model level. The second method fuses retrieval scores produced by using relevant and non-relevant information separately. Em-pirical exploration attests to the merits of our methods, and sheds light on the effectiveness of using and integrating rel-evance feedback for textual units of varying granularities.
Most previous work on using relevance feedback for ad hoc (query-based) document retrieval has focused on utiliz-ing feedback provided at the document level. Utilizing in-formation induced from relevant documents can significantly improve retrieval effectiveness [12, 13]. The effective utiliza-tion of non-relevant documents, on the other hand, has been demonstrated mainly for very difficult queries [16, 7].
Relevant documents can also contain non-relevant infor-mation. Thus, utilizing focused relevance feedback , that is, feedback for passages in relevant documents, can be of merit. For example, using information induced from relevant pas-sages can improve retrieval effectiveness [14].

We present a study of methods that utilize positive and/or negative relevance feedback for documents and/or for pas-sages. Our first method uses a novel mixture model that integrates, at the language model level, information induced from relevant and non-relevant units (documents and/or pas-sages). Our second method fuses retrieval scores attained by using, separately, relevant and non-relevant units.
Empirical evaluation sheds light on the effectiveness of using information induced from relevant and non-relevant units of different granularities and their integration. For example, the best performance of our methods was attained using information induced from relevant passages and non-relevant documents.

Our main novel contribution is the development of meth-ods that (i) use relevance feedback at both document and passage levels, and/or (ii) utilize non-relevant passages.
Methods using information induced from both relevant and non-relevant documents emphasize terms that appear in the former and downplay the importance of those appear-ing in the latter (e.g., [6, 12, 13, 16]). A similar principle, although implemented using different techniques, is applied in our methods that utilize also (non-)relevant passages.
Findings about the merits of using information induced from non-relevant documents have largely been inconclusive [6, 5, 13]. Notable exceptions are work on addressing very difficult queries [16, 7] and on the document routing task [15]. In contrast to our work, information induced from (non-)relevant passages in relevant documents was not used.
Shen and Zhai [14] showed the merits of using relevant (but not non-relevant) passages in the mixture model [17]. We extend the mixture model by using both relevant and non-relevant feedback units (documents and/or passages). Automatically identifying (non-)effective passages in (non-)relevant documents and using these for retrieval has shown no merit [11, 9]. In contrast, our methods utilize true rele-vance feedback for passages and documents.

A mixture model, different than ours, was used to induce a query model from pseudo relevant documents using non-relevant documents [10]. In contrast to our work, relevant documents and passage-level feedback were not used.
Let D init denote an initial list of documents retrieved from corpus C in response to query q by some retrieval method. Suppose that relevance feedback is provided for documents in D F (  X  D init ); specifically, R d and N R d are the sets of relevant and non-relevant documents in D F , respectively.
Relevant documents can also contain non relevant infor-mation. Accordingly, we further assume that focused rele-vance feedback is provided for relevant documents. Namely, non-overlapping variable length passages of documents in R d are marked as relevant to q ; unmarked passages are con-sidered non-relevant. We concatenate all relevant and non-relevant passages in each relevant document into a single relevant pseudo passage and a single non-relevant pseudo passage , respectively; the order of concatenation has no ef-fect since we use unigram language models that assume term independence. R p an d N R p are the sets of pseudo relevant and non-relevant passages in documents in R d , henceforth simply referred to as relevant and non-relevant passages, re-spectively. Herein, R refers to the set of either relevant doc-uments R d or relevant passages R p ; N R is the set of non-relevant documents ( N R d ) or non-relevant passages ( N R
Below we present two retrieval methods, based on uni-gram language models, that utilize the relevance feedback. The maximum likelihood estimate (MLE) of term w with re-c ( w, x ) is the count of w in text x . We use p Dir x ( w ) to denote the probability assigned to term w by a Dirichlet smoothed unigram language model induced from text x [17]. The sim-ilarity between two language models, p y (  X  ) and p z (  X  ), is mea-sured using cross entropy, where higher values correspond to decreased similarity:
The goal of our first method is to X  X istill X  X he aspects most relevant to the information need from the feedback. For example, the premise of the mixture model [17] is that terms in relevant documents are generated either by a relevance topic language model or by the corpus language model. We generalize the mixture model to utilize both relevant and non-relevant feedback units (documents and/or passages).
We assume that terms in units in R (i.e., all relevant doc-uments or all relevant passages) are generated by a mixture of (i) the relevance topic model, p rel (  X  ), which we want to estimate; (ii) the corpus language model, p MLE C (  X  ), which is assumed to represent a general non-relevant document; and, (iii) a query-specific irrelevance topic model, p MLE NR (  X  ), in-duced from the non-relevant units ( N R ). Following work on using negative relevance feedback, if w is a query term we set p MLE NR ( w ) def = 0 and re-normalize the probabilities [16].
We estimate p rel (  X  ) by using the EM algorithm to maxi-mize the log likelihood of units in R :
X  X  1 and  X  2 are free parameters. As is common in work on query language models [17, 1], we interpolate p rel (  X  ) with the original query model:  X  q is a free parameter. We then rank the documents in the corpus using  X  CE ( p distill (  X  ) || p Dir d (  X  )).
We instantiate Equation 2 using R (  X  X  R d , R p } ) and N R (  X  X  N R d , N R p } ). The resultant four models attained from Equation 3 are denoted Distill(R,NR) .
The second retrieval model is based on the principle that documents similar to the relevant units and dissimilar from the non-relevant units should be rewarded. Specifically, we apply a two-step approach inspired by work on using only the query and non-relevant documents [16]. First, a rele-vance topic model, p r (  X  ), is induced from the relevant units in R (  X  X  R d , R p } ) using some approach . Then, the docu-ment corpus is ranked using  X  CE ( p r (  X  ) || p Dir d (  X  )). Second, the top n documents are re-ranked by the similarity of their language models with p r (  X  ) and dissimilarity from the lan-guage models induced from non-relevant units in N R . For-mally, documents d are ranked in descending order of the following score-based fusion:  X   X CE ( p r (  X  ) || p Dir d (  X  )) + (1  X   X  ) min  X  is a free parameter. As in the distillation model, for query term w and non-relevant unit x (  X  N R ): p MLE x ( w ) def the probabilities are re-normalized 1 .

Various methods can be used to induce p r (  X  ) from a set of relevant units, R (  X  X  R d , R p } ). We use the standard mixture model [17] which is a special case of our distilla-tion model from Equation 3 when setting  X  1 = 0 in Equa-tion 2. 2 Equation 4 is then instantiated using a choice of N R (  X  X  N R d , N R p } ). The four resultant score-based fu-sion methods are denoted SF(R,NR) .
For experiments we used the INEX corpus 3 which con-tains 2,666,190 Wikipedia articles. We used the 120 queries from the ad hoc tracks of 2009 and 2010 for which binary document-level and (arbitrary-length) passage-level relevance judgments are available; unmarked text in relevant docu-ments is considered non-relevant [2]. The average number of relevant documents per query is 86. The average per-centage of relevant text in a relevant document is 41 . 5%; i.e., most text in relevant documents does not pertain to the query. We re-visit this important point below.

Krovetz stemming was applied to documents and queries and stopwords on the INQUERY list were removed. Indri 5.3 (http://www.lemurproject.org/indri) was used for ex-periments. The initial ranking from which D init is derived is induced using standard language-model-based retrieval [17]: document d is scored by  X  CE ( p MLE q (  X  ) || p Dir d (  X  )) (see Equa-tion 1). The Dirichlet smoothing parameter in document language models,  X  , was set to 1000 in all methods [18].
The document feedback set, D F , contains 2 k documents: the k highest ranked relevant documents ( R d ) and the k highest ranked non-relevant documents ( N R d ) in D init { 1 , 2 , . . . , 5 } ; each value entails an experimental setting. The goal was to ameliorate across-query effects that are due to varying numbers of (non-)relevant documents at top ranks.
Mean average precision at cutoff 1000 (MAP) serves as the retrieval evaluation measure. Two evaluation paradigms were employed: standard (regular) and residual collec-tion. In the residual paradigm [3], all documents in D F
E quation 4 is conceptually reminiscent of the MultiNeg method from [16] that utilizes the query and non-relevant documents for re-ranking. Experiments  X  numbers are omitted due to space considerations  X  reveal the following. The approach in Equation 4 yields better performance in our setting when using min rather than average. The approach is also superior to using a single model, p MLE NR (  X  ), induced from the non-relevant units; cf., the SingleNeg method [16].
We found that using relevance model #3 (RM3) [8, 1] re-sults in similar conclusions to those we present below. Ac-tual results are omitted due to space considerations. http://www.mpi-inf.mpg.de/departments/ databases-and-information-systems/software/inex were removed from result lists and relevance judgments files; M AP is measured on result lists of 1000 documents. Sta-tistically significant performance differences are determined using the paired two-tailed t-test with p &lt; 0 . 05.
Free-parameter values are set using leave-one-out cross-validation performed over queries per experimental setting; MAP is the optimization measure. The value ranges are as follows:  X  q (Equation 3) is in { 0 . 2 , 0 . 5 , 0 . 8 } ;  X  (Equation 2) are in { 0 , 0 . 1 , 0 . 5 , 0 . 9 } ;  X  1 +  X  tion 4) is in { 0 , 0 . 2 , . . . , 1 } ; the number of documents re-ranked in the score-based fusion method, n , is set to 1000. As is common [17, 1], language models induced using rele-vance feedback are clipped to  X  (  X  X  10 , 25 , 50 } ) terms. As noted, the standard mixture model [17], henceforth MM, is a special instance of our distillation model when setting  X  1 = 0 in Equation 2 (i.e., non-relevant units are not used) and applying Equation 3. MM is also used to in-duce the relevance topic model, p r (  X  ), used in Equation 4. Hence, we use MM( R d ) which utilizes the relevant docu-ments ( R d ) and MM( R p ) (also used in [14]) which utilizes relevant passages ( R p ) as reference comparisons. The EM algorithm used in the mixture and distillation models con-verged in 13-14 iterations.
Figure 1 depicts the performance results. We see that the more feedback documents are used (i.e., higher k ), the more effective the retrieval. Specifically, for the residual evaluation paradigm (Figures 1(c) and 1(d)), where MAP values decrease with increasing k due to removing the given relevant documents from all rankings [4], the relative per-formance improvements of the feedback-based methods with respect to the initial ranking increase as a function of k . Relevant units. Figure 1 shows that in all cases, using rel-evant passages ( R = R p ) yields better performance than using relevant documents ( R = R d ): compare a solid curve with white markers ( R = R p ) to a dotted curve with gray markers of the same type ( R = R d ). This finding can be at-tributed to the fact that relevant documents contain much non-relevant information as mentioned in Section 4. comparison to MM( R ), which does not utilize non-relevant units, and regardless of the choice of R , using also non-relevant documents in our distillation model improves re-trieval effectiveness in a vast majority of cases. For the standard evaluation, the distillation method yields improve-ments in 9 out of 10 cases (Distill( R d , N R d ) vs. MM( R and Distill( R p , N R d ) vs. MM( R p ) over 5 values of k ); in 7 cases the improvements are statistically significant. For the residual evaluation, effectiveness is improved in 7 out of 10 cases with 4 improvements being statistically significant.
The effectiveness of using N R = N R p depends on the relevant units used. Distill( R d , N R p ) outperforms MM( R in all cases (often statistically significantly) for both eval-uation paradigms. Moreover, Distill( R d , N R p ) outperforms Distill( R d , N R d ) in all cases for the residual evaluation, al-though few improvements are statistically significant. The merits of using non-relevant passages ( N R = N R p ) to dis-till a relevance topic model from relevant documents can be attributed to the fact that relevant documents contain much non-query-pertaining text (see Section 4). However, using the relevant passages alone, MM( R p ), is more effective than using relevant documents and non-relevant passages, Distill( R d , N R p ), and is as effective as using non-relevant passages in addition to relevant passages, Distill( R p , N R That is, using non-relevant passages to distill a relevance topic model from either relevant documents or relevant pas-sages has no merit over using only the relevant passages.
Distill( R p , N R d ) is the most effective distillation model in a vast majority of cases; most improvements over other distillation models and the mixture models are statistically significant for both evaluation paradigms. Thus, in contrast to non-relevant passages in relevant documents, non-relevant documents can be effectively used to distill a relevance topic model from relevant passages 4 .
 score-based fusion model (Equation 4) is presented in Fig-ures 1(b) and 1(d). The curves of SF( R , N R p ) (almost) coin-cide with the curves of MM( R ) regardless of the choice of R . This means that using the similarity of a document to non-relevant passages has little merit. In contrast, SF( R , N R outperforms MM( R ), regardless of the choice of R . The improvements are statistically significant for all 10 cases (5 values of k  X  2 choices of R ) for the standard evaluation, and in 4 out of 10 cases for the residual evaluation.
Overall, the most effective score-based fusion model for both evaluation paradigms is SF( R p , N R d ). The improve-ments it posts over the other methods (specifically, MM) are statistically significant in a vast majority of the cases
Our distillation and score-based fusion methods use rel-evance feedback for documents and passages in different ways. The distillation model utilizes both relevant and non-relevant units in a mixture model to rank the entire corpus. The score-based fusion model re-ranks a list retrieved using information induced only from relevant units by using, in addition, dissimilarities with non-relevant units.
Despite these differences, the conclusions regarding the merits of using the different types of (non-)relevant units are similar in most cases. That is, using relevant passages is superior to using relevant documents regardless of the non-relevant units used. Yet, using non-relevant documents in addition to relevant passages is of much merit and results in the best performance for both methods. A noticeable differ-ence between the two methods is the effectiveness of using non-relevant passages in addition to relevant documents in the distillation model. No such merits were observed for the score-based fusion method.
 Acknowledgments. We thank the reviewers for their com-ments. This paper is based upon work supported in part by the German Research Foundation (DFG) via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1), the Israel Science Foundation under grant no. 433/12, and the Technion-Microsoft Electronic Commerce Research Center.
E xtending the distillation model to use both N R p and N R showed merit over using each alone when relevant documents are used but not when relevant passages are used. We found that a score-based fusion method that extends Equation 4 by using both non-relevant documents and non-relevant passages does not statistically significantly outper-form SF( R d , N R d ) and SF( R p , N R d ).
