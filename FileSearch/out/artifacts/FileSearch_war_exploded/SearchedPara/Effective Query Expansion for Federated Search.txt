 While query expansion techniques have been shown to im-prove retrieval performance in a centralized setting, they have not been well studied in a federated setting. In this paper, we consider how query expansion may be adapted to federated environments and propose several new methods: where focused expansions are used in a selective fashion to produce specific queries for each source (or a set of sources). On a number of different testbeds, we show that focused query expansion can significantly outperform the previously proposed global expansion method, and X  X ontrary to earlier work X  X how that query expansion can improve performance over standard federated retrieval.

These findings motivate further research examining the different methods for query expansion, and other forms of system and user interaction, in order to continue improving the performance of interactive federated search systems. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; H.3.4 [ Information Storage and Retrieval ]: Distributed Systems; H.3.7 [ Information Storage and Retrieval ]: Digital Libraries X  hidden web Design, Experimentation distributed information retrieval, query expansion
Federated Information Retrieval (FIR) systems are re-quired to provide effective retrieval performance, in dis-tributed, and generally uncooperative, environments. Past research has largely focused on the necessary building blocks required for effective federated retrieval [3], such as represen-tation [4, 20], selection [9, 21], and merging [22]. This has resulted in the development of FIR systems which deliver performance on par with centralized systems [27].
This success has opened up a significantly more challeng-ing area of federated retrieval research: exploring inter-action within the federated search process. Already, sys-tem and user interactions such as pseudo-relevance feed-back [13], query expansion/reformulation [12, 26], implicit feedback [25], etc have been shown to provide significant in-creases to performance in centralized search systems. Now that the underlying machinery of FIR has matured, the next logical progression is to determine whether such per-formance enhancing techniques can also be employed to im-prove the effectiveness of FIR systems. The key challenge is determining how such techniques can be adapted to fit the distributed environment in order to realize potential in-creases. The focus of this paper is on the application and development of automatic query expansion (QE) techniques for FIR. Given the FIR environment, we propose a number of novel techniques for QE which treat constituent servers differently and dispatch focused, or server-specific, queries. This contrasts the only other proposed method [15] of query expansion for FIR which performs expansion in a globalized,  X  X ne query fits all X , fashion. By creating query expansions for each local server, a focused query can be generated which better matches the content within the server, potentially avoiding topic drift or vocabulary mismatch. Our exper-iments on several testbeds, with a variety of training pa-rameters and state-of-the-art FIR components, show that local and focused QE can outperform existing techniques and (contrary to earlier findings [15]) can improve FIR per-formance over a no-expansion baseline.

The remainder of this paper is as follows: the following section provides the necessary background describing the current state of the art in Federated Information Retrieval. In section 3, we describe our proposed methods for query expansion. We conduct a comprehensive and thorough ex-perimental study in sections 4 and 5 to assess the impact on retrieval accuracy: we consider a number of different scenar-ios to determine whether the proposed QE techniques can improve retrieval effectiveness over several competitive base-lines. Finally, we conclude with a discussion and summary of our main findings, before wrapping up with directions for future work. retrieval (FIR) X  X lso called distributed information retrieval (DIR) X  X rovides a single interface to any number of inde-pendent servers (also referred to as collections, sources, or search engines). An FIR tool, also referred to as a broker , will characterize the available servers (representation); ac-cept a user X  X  query; decide which server to use (selection); translate the user X  X  query to some appropriate syntax for each selected server; and collate the results from servers, typically into a single ranked list (merging).

Brokers rely on local representations of each server, which capture for example term occurrence; collection size; or sub-jects covered. Systems such as STARTS [7] and SDLIP [16] assume that servers are cooperative and make accurate rep-resentations available. However, in the general case, servers will be uncooperative and only provide a conventional query interface. In these cases, a broker must generate its own rep-resentation using a technique such as query-based sampling [4] or focused probing [8]. The former, the most commonly-used technique, submits a series of probe queries to a server and downloads a copy of documents from each result set: the union of these documents constitutes a representation set , and stands in for the server X  X  contents.

Sampled documents have been used to estimate server characteristics X  X or example, the capture history estimator used in our experiments [19] uses the overlap between con-secutive samples to estimate the size of a server X  X  holdings X  and so to inform selection (the CRCS algorithm we use as-sumes that the subject matter covered by a sample is rep-resentative of the whole collection [18]). In algorithms out-lined in Section 3 we suggest also using sampled documents to inform query expansion.
 signed to improve search effectiveness by providing a richer description of the information need. One or more additional terms are chosen and used to augment the user X  X  original query: since user queries are typically very short, this can mitigate the vocabulary mismatch between the query and documents, and improve the retrieval effectiveness.
Expansion terms can be selected from external sources such as query logs [24], or dictionaries [6], or can be gener-ated from a feedback process [5, 13, 17]. In the former cat-egory, the expansion terms are selected from pre-generated sources based on query reformulations, synonyms, etc. In the latter category however the terms are often generated on the fly based on feedback documents. These can be a hand-picked sample of relevant documents ( explicit feedback ) [17]. Alternatively, the top-ranked documents returned by the retrieval system for a query can be considered as pseudo-relevant for implicit feedback . The latter form of feedback is obviously far more scalable, but potentially noisier. The implicit form of relevance feedback is not only limited to pseudo-relevant documents but can be also collected from other sources such as clicks [11], or search trails [2].
Term weights in the expanded query can be calculated in a variety of ways. In the work of Ogilvie and Callan [15], which we follow, term weights are estimated using the rele-vance model (RM) [13]. This is based on language modeling and assumes the query Q and the top-ranked documents are sampled from the same model  X  R  X  X his is the relevance model.  X  R is considered as a black box that determines the likelihood of each term given the query as follows: where C denotes the document collection, V repre-sents the vocabulary used and F represents the set of (pseudo-)relevant documents returned for query Q . We as-sume that the document prior P (  X  d ) is uniform in our exper-iments. The most likely k terms according to the relevance model generate the expansion candidates.

While many methods for expansion exist, their applica-tion in FIR is largely unexplored.Ogilvie and Callan have proposed a global approach to query expansion for FIR [15]. This approach uses a central index of all the documents sam-pled from all the servers, which a user X  X  query is matched against. The top-ranked passages/documents are returned, and the terms in these documents are used to expand the user X  X  original query. The expanded query is then submit-ted to each of the selected servers. However, this approach represents only one possible way in which to perform QE in a distributed environment. We distinguish between the possibilities in the following section.

Ogilvie and Callan X  X  study reveals that the global method does not perform significantly better than the unexpanded FIR baseline suggesting that QE is not particularly useful in FIR. However, this study was only performed on one testbed, with one set of topics, and so it is not possible to generalize these findings. The experimental study con-ducted here is performed on four testbeds with two sets of topics in order to thoroughly evaluate existing and proposed expansion methods.
The different approaches in which query expansion can be applied to FIR vary over four main factors: (i) data for expansion, (ii) level of aggregation, and (iii) query special-ization. The fourth factor is the point of application, for re-selection of servers or for retrieval of documents. In this work, we only consider expansion for document retrieval. Below is a brief outline of the other three factors that could be varied: 1. Data for Expansion : Query expansion can be ap-2. Level of Aggregation : the documents (or passages) 3. Query Specialization : The expanded query could
Given these different factors, we can say that the previ-ous work on QE in FIR [15] investigated a global method; which sends general queries based on a global sample of doc-uments. It should be noted that there is some dependence between these factors, and that more sophisticated methods may use both local and global information, or decide to send a mixture of general and focused queries. These conceptual factors are used to identify the main characteristics of the methods. In this section, we propose several novel alterna-tives, which use local and cluster aggregation of data, and focused queries. Thus the aim of this paper is to address the following questions:
The benefits of using the local information obtained from a source is that a focused query can be generated which is tailored to the content in the source. The idea is that this will avoid vocabulary mismatch problems and topic drift. However, the disadvantage is that there is less data to use in order to select query terms for expansion. An operational question, then, is, what is the trade-off between the level of aggregation and effectiveness?
Below we present each of the proposed methods, along with the previously proposed global method. A summary of the methods is shown in Table 1.
 sampled documents from all servers together on the bro-ker. The top-ranked results returned for each query from this index are then used to expand the query. The selected servers receive the expanded query for document retrieval. Note, that all the selected servers receive exactly the same query. We refer to this method as global expansion, and use it as the competitive baseline. The weight of the expansion terms using the global model is estimated using: where G represents the set of sampled documents from all servers, and P G ( t, Q ) denotes the probability of relevance computed by the relevance model as shown in Equation (1). In this approach, the same query is sent to each of the se-lected servers ( C ) X  X .e. there is no query specialization X  X nd thus the method is global and general. Specialization could be applied to the global method, but we do not propose such an extension here.
 sion strategy performs server-specific query expansion. An expanded query is formulated for each server using the doc-uments sampled from that server. The idea here is to ex-pand the queries with representative terms that are specific to each server. The drawback may be that the samples contain fewer documents from which to find any reason-ably good pseudo-relevant documents, compared to using the global information. Since this may affect the quality of the query expansion, in our experiments we investigate how the size of the samples affects retrieval performance. The Local query expansion method can be formalized as follows. For a server C , with a set of sampled documents S C , the expansion weights are estimated using: Note that the expansion terms and weights vary for each server producing a focused query.
 suffer from noise, when the sample of documents is small, we consider employing a local but general approach: where each collection provides a set of possible query expansion terms, then from these suggestions a subset is selected to form the expanded query. This single expanded query is then issued to each of the selected servers. We employ a simple vot-ing scheme to select the best expansion terms, which uses CombMNZ [14] to pick the best k expansion candidates. We refer to this approach as the Fuse QE method. The terms are sorted according to their CombMNZ scores (calculated as below), and the top k are selected for query expansion. Here  X  t , is the number of servers that recommend term t for expansion, and is used to boost the weights of terms that are suggested by more servers. S C denotes the set of sampled documents from server C .
 Cluster. The Cluster method is motivated by the two key points discussed for earlier methods; (a) increasing the size of the corpus used for query expansion improves the quality of feedback documents, and (b) using the sample documents from all collections for query expansion, may generate ex-pansion terms that are not suitable for selected servers. In fact, it is possible that the expansion terms do not exist in a selected server. The two contradicting points above moti-vate a compromise between the local and global approaches: where the information used to perform expansion is based on clusters of sampled documents. Using clusters should in-crease the amount of data which is used to perform query expansion, while still trying to ensure that the terms selected in expansion are related to the collections.

The Cluster method can be summarized in three steps: (1) cluster the sample of documents into n buckets (here, we employ kmeans [10]), where n should be small enough to ensure the quality of feedback terms, and large enough to generate different expansion terms from each cluster, (2) assign servers to clusters according to their number of sam-pled documents in each cluster. A server C is assigned to a cluster  X  , if cluster  X  has more sampled documents from C than any other cluster; and (3) for each selected server C , expand the query according to the corresponding cluster for C . Here  X  refers to the cluster that server c is assigned to (query independent). The expansion terms generated by this model are cluster-specific . That is, all the servers in the same clus-ter share the same expansion candidates.
The aim of this paper is to evaluate the different strategies for query expansion in the context of FIR. In this section, we detail the experiments undertaken in order to perform a comprehensive and thorough evaluation of the competing strategies. These laboratory based experiments are designed to reflect the real world application of such technology and provide the best indication of performance in a simulated environment. The FIR system used in these experiments is described, along with the configuration parameters for query expansion and respective baselines. Then, we describe the set of testbeds used to perform the evaluation.
 our experiments is comprised of the state of the art in rep-resentation, selection and merging algorithms. To repre-sent each server, query-based sampling [4] was used to gen-erate the server representation sets (i.e. the set of sam-pled documents used to represent the collection). Each sampling query is selected with uniform probability for the downloaded documents. The top four documents returned by sampling queries are added to the server representation set. Three representations were used during the course of these experiments, where we considered the impact of sam-ple size on retrieval effectiveness examining representations with 300, and 2900 documents.

CRCS with an exponential decay function [18] was em-ployed for collection selection, whilst for results merging SSL [22] was employed. CRCS uses server size statistics to rank servers. Such information may not be avaible in practice. We used the capture-history method [19], using 140 probe queries, to estimate the size of servers. CRCS, SSL, and capture-history are all among state of the art tech-niques.

Avrahami et al. [1] showed that in real-world applications of federated search, selecting three to five servers usually produces the best trade-off between search effectiveness and efficiency. Therefore, in our experiments we report results for server selection cutoff value of three. Note that we found similar trends when a cutoff value of five was used; they are excluded here for brevity.

All servers use the KL divergence retrieval model, while query expansion was performed using the relevance model. gence as our document ranking method for all the experi-ments in this paper. The relevance score of a document d for query Q is computed according to the divergence between www.lemurproject.org the language models of query  X  Q , and document  X  d :
S ( Q, d ) =  X  KL (  X  Q ||  X  d ) =  X  X where t denotes a term in the corpus vocabulary V . Dirichlet smoothing is used to avoid the zero probability. To gener-ate the expansion candidates and their weights we use the relevance model [13], with the default parameters. the parameter space over the following parameters was em-ployed, where the best settings were used on the testing test. Given the relevance model [13], the number of feedback doc-uments was set according to  X  = { 1 , 10 , 50 } , the number of expansion terms  X  = { 1 , 10 , 50 } , and the combination weight  X  = { 0 . 5 , 0 . 7 , 0 . 9 } (i.e the weight specifying the importance of original query words). For the cluster method we empir-ically set the number of clusters to n = 4 and leave further investigation of the best value for future work. be used in order to compare and contrast the differences between methods: each gold standard baseline (or oracle) assumes complete information under a centralized setting. The first oracle baseline (BONE) is without query expan-sion and the second oracle baseline (BOQE) is with query expansion. Our third baseline is obtained by performing federated retrieval without query expansion (BSNE). These baselines provide the reference points, while we also include Ogilvie and Callan X  X  global model as a fourth baseline. This baseline is the only QE method currently proposed for FIR and thus represents the state of the art.
 et al. [4, 21, 22, 23] were used in this study. These testbeds provide four different distributed environments to thoroughly evaluate the performance of FIR systems. Table 2: The effectiveness of query expansion meth-ods on different testbeds for TREC topics 101 X  150. Expansion parameters are tuned by training on TREC topics 51 X 100. CRCS is used to select three servers per query.
 relevant nonrel.
 trec123 represent.
 100 and 101 X 150 to evaluate the performance of each ap-proach. A training-testing methodology was employed, where each topic set was used for training the expansion parameters, while the other was used for testing, and vice versa. The performances reported are those obtained from the test sets. During training the parameters are tuned for maximizing the P@5 value when three servers are selected (CO=3). Optimizing for P@10 and other server selection cutoff values showed very similar results.

We use Student X  X  t -test to measure the significance of dif-ferences between the methods, particularly compared to the no expansion baseline (BSNE). The notations  X  and  X  re-spectively indicate significant differences at p &lt; 0 . 05 and p &lt; 0 . 01. Italicized text represents performance significantly worse than the BSNE baseline. We focus mainly on P@5, as this is the metric we use to tune the expansion parameters. of different expansion methods compared to the baselines for TREC topics 101 X 150. The first notable observation is that the previously proposed global method is outper-formed by the baseline without expansion (BSNE) on P@5 for three of four testbeds (relevant, representative, nonrel-evant). This is consistent with the findings reported by Ogilvie and Callan [15]. Similar observations can be made for the Local and Fuse models. Local, Fuse and Global, do slightly better on P@10 overall. The cluster method does slightly better than BSNE on the trec123, slightly worse on the relevant testbed and equally good on the representa-tive testbed. On the non-relevant testbed, Fuse performs significantly worse than the baseline and other expansion methods, and the advantage of Local over the BSNE base-line is negligible. The global method has a lower P@5 than BSNE, but produces significantly better results on P@10. The cluster method again produces the best result by out-performing other QE methods across all testbeds (given the trained parameters from TREC Topics 51 X 100). The differ-ences between the Cluster method and all the other methods are statistically significant for P@10 ( p &lt; 0 . 05).
Table 4 provides an overall comparison between meth-ods across all testbeds for TREC topics 101 X 150 (trained on Topics 51 X 100). The Local and Fuse models perform slightly worse than BSNE. The P@5 loss is statistically sig-nificant for the fuse model, which we expect is due to dif-ferences in quality between servers: servers which have few on-topic documents or which are otherwise poor candidates still contribute votes to select expansion terms. This might be mitigated by using a selection algorithm to rank servers ahead of expansion, and by disregarding those which seem poor or off-topic. The global method produces lower P@5 values compared to BSNE, while significantly outperforms it in terms of P@10. The cluster model shows the best per-formance among all expansion models, and outperforms all the alternatives significantly ( p &lt; 0 . 05) for P@5.
We also repeated the experiments, by tuning the expan-sion parameters on TREC topics 101 X 150 and testing them on topics 51 X 100. The results an be found in Table 3, under the  X 300 documents X  column (also see Table 4 for an overall summary). Except for one case in the representative testbed where Local performed poorer than BSNE, no statistically significant difference was detected among methods. While all query expansion methods produce slightly better results than BSNE overall, none of the differences were statistically significant. In summary, the cluster and global expansion method tend to outperform the other methods.

Though, it should be noted that none of the tested meth-ods produced operationally significant improvements over the no expansion baseline. However, we argue that our thor-ough analysis on four testbeds provides sufficient evidence that query expansion can be applied effectively in federated search, even with small summaries.
 samples have shown to improve FIR performance, and we therefore considered the effect of larger samples on our ex-pansion methods. Work by Ogilive and Callan [15] used samples of varying size, from 300 to 2900 documents per server, and we repeat this analysis here.

Figure 1 depicts the changes to retrieval performance us-ing different sample sizes: Table 3 summarizes results when we increased our sample size to 2900 documents, using the same training/testing split as earlier. To make the results comparable across different experiments, we use the original 300 samples for server selection and result merging. There-fore, any changes in search effectiveness is solely due to the quality of query expansion candidates. It can be seen from the table that increasing the sample size does not always improve. Here,  X  and  X  represent statistically significant dif-ferences between experiments with small and large samples. Contrary to earlier work [15], we find this is particularly the case for the global method which performs poorly on two of the testbeds despite more data to estimate query expan-sions from. However, the performance of the local and clus-ter methods generally improved with larger samples. This is intuitive, because larger samples provide richer sources of text for pseudo-relevance feedback and are more likely to be representative of servers X  holdings. For the local methods this is more important because this is the only information used for expansion.
 for selection. We have so far been using CRCS for selection as it has been shown to be one of the best-performing al-gorithms [18]; a further set of experiments considered the impact of using CORI [3], another popular algorithm for selection. By doing so, different servers will be selected, and it will be interesting to see whether this will improve or degrade the query expansion methods.
 Figure 2 illustrates P@5 for each selection algorithm. What is immediately striking is that using the CORI selec-tion algorithm results in a significant loss in performance: this was the case across almost any combination of param-eters, testbeds, and expansion methods.
 Table 4: The average performance of query expan-sion methods across different testbeds for TREC topics 101 X 150 and TREC topics 51-100.

However, using CORI the local query expansion methods did perform as well as, if not better than the cluster and global methods. For example, see Figure 2 for the results on the nonrelevant testbed.
Although query expansion techniques have been well-studied in the case of centralized IR, they have been largely ignored in federated IR research. As shown in Table 1, we have considered several means by which a FIR system could make use of query expansion: choosing expansion terms based on each collection separately (local expansion) and sending individual expanded queries to each collection (fo-cused querying) using sampled documents.

We performed a comprehensive study on query expansion in FIR, testing the four algorithms (three novel) across four testbeds and employing a train-test methodology with two TREC topic sets. The results suggest that QE techniques can significantly improve FIR performance over the baseline, even with relatively small document samples from which to draw additional terms, although some level of aggregation is useful. This is an important finding as previous work [15] suggested that no improvement could be obtained over a baseline without query expansion (using small server sum-maries). However, this work provides evidence to suggest that query expansion can lead to improvements in perfor-mance in the FIR environment.

Generally, the cluster method performed consistently well across the different testbeds, which significantly outper-formed the baseline without expansion, and the global method on TREC topics 101 X 150. On TREC topics 51 X 100 there was no significant difference between the global and cluster methods. The use of some local evidence (through clustering) shows that different aggregations can generate better expansions. While the local method was sensitive to the amount of data available to expansion, the focused queries performed better than the fuse method, which com-bined the local knowledge to form general queries. De-spite the poor results obtained by these methods, the lo-cal method could be more useful when relevance feedback is provided, as this would remove the problems with using a small sample of documents, but provide specific and fo-cused queries for each server. This remains a direction for future work. On the other hand, the Fuse method suffers because all local servers were treated equally in the process of forming a general query and this lead to noisy, and poorly performing expansions. By restricting the set of servers that form the general query to the best servers, improvements could also be achieved.

The taxonomy of factors for applying QE in FIR suggests further scope for experimentation: for example, with meth-ods which use returned (not sampled) documents as sources of terms. Further investigation of clustered expansion, pos-sibly in conjunction with ideas from the fuse or local meth-ods, may also be worthwhile. More generally, other query modification and interaction techniques from conventional retrieval could also be investigated in the context of FIR en-vironments, where the interaction can be used to drive and develop more effective methods for interactive FIR. We would like to thank Dr. Mark Baillie for his input and assistance in developing this work. the data range. Other testbeds showed similar trends.
