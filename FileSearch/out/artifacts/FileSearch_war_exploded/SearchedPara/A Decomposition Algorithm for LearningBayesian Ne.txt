 Bayesian networks (BN) [1,2] are widely used to represent probabilistic relation-ship among random variables. They have been successfully applied in many do-mains such as medical diagnosis, gene data analysis, and hardware troubleshooting. Over the last decade, much progress has been made regarding structural learning in Bayesian networks including both the score-based and the constraint-based learn-ing methods [3,4,5,6]. The score-based method tries to optimize a scoring function by means of a search strategy. Since finding the optimal Bayesian networks was shown to be an NP-complete problem [7], one has to resort to some heuristic search strategy. The other is constraint-based and infers structures through conditional independency tests. The constraint-based is generally faster than the score-based method and gives a trustworthy result provided there are sufficient data. We focus on the constraint-based learning methods in this paper. Currently, with the mass-throughput data in biomedical informatics, data analysis demands more powerful learning algorithms that could handle data sets having thousands of variables but with limited sample sizes. Most conventional learning methods run into the com-putational and statistical problems in such a domain. They either can X  X  complete the learning process, or produce a poor structure even when the learning is done. Hence, in this paper, we propose a decomposition algorithm for learning a large Bayesian network from a small amount of data.

The decomposition learning algorithm adopts the divide-and-conquer strat-egy and contains several procedures to co mplete the learning task. We discover a set of clusters from a dependency graph built directly from the data. Each cluster is expected to represent a local d omain structure. We establish connec-tion between clusters and learn each cluste r separately. The lea rned clusters are joined together to compose the complete targeted network.

The novel algorithm reduces the computational complexity since it learns clusters instead of the complete network directly. In addition, the algorithm learns clusters in a separated way so that the structural error(due to conditional independency tests) that occurs in the learning of clusters does not influence the global structure learning. Hence, the algorithm avoids the cascading effect of incorrect statistical tes t results in the structural learning. We experiment the proposed algorithm on several benchmark Bayesian networks and compare with other typical constraint-based learning algorithms. The empirical results show that the decomposition algorithm achieves good performance regarding both structure learning a ccuracy and run times.

This paper is organized as follows. In Section 2, we discuss some related works on structural learning. In Section 3, we present the decomposition learning algorithm by illustrating embeded procedures. Then, we show comparison results in Section 4. Finally, in Section 5, we conclude the paper with some hint on future work. The divide-and-conquer strategy has served as the technique of many learn-ing algorithms that aim at recovering a large Bayesian network structure from data [8,9,10]. The foundation of this strategy is to identify some appropriate components in a large model. For example, in the approach of learning module networks [9], a module is defined as a set of variables that have similar behavior. All variables in a module share both the same parents and the same conditional probability distribution. It seems that the module formulation is quite strict. However, the formulation is well consistent with some domain concepts such as genes in a cell, stocks in a stock sector, and so on. The sparse candidate al-gorithm [8] recovers Bayesian networks by specifying the maximum number of parents of variables in the learning proce ss, which significantly reduces the learn-ing complexity. Both the module learning and the sparse candidate approaches orient score-based learning.

The most relevant work is the block learning algorithm [10] that already shows the ability of learning a large Bayesian network from limited data. Similarly, the block learning algorithm recovers struct ures through procedures of identifying blocks and combining the learned blocks. However, the block identification pro-cedure is incomplete since the block is composed of nodes that have at most two-length distance from block centers. The searching largely depends on the topology of a dependency graph and probably leads to disconnected blocks in some domains. In addition, as shown in the previous work, a large amount of run times are spent in learning overlapping st ructures. However, the extra procedure does not have much benefit to the final (heuristic) structural combination. Our new algorithm improves the block learning algorithm by designing more robust and appropriate procedures.
 A constraint-based approach learns a network structure by using some statistical hypothesis tests to detect dependency or (conditional) independency among vari-ables or attributes in a data set. The resu lts of several tests are combined by the constraint-based approach in order to construct a Bayesian network structure. The test results might be incorrect esp ecially when insufficient data are pro-vided. Since various test results might depend on each other in some unknown manner, the error of the induced structure is not under control and spread in a global way. In addition, a large number of variables lead to an increasing or-der of conditional independency tests, which makes the learning intractable. To circumvent these shortcomings, we propose the decomposition algorithm that enhances the learning ability of conventional learning techniques by specifying a modular framework.

We briefly describe the decomposition learning algorithm in Fig. 1. The algo-rithm receives the data set D of size l 1 and the parameter  X  used to control the cluster expansion (line 4). We firstly construct the dependency graph M directly from the data through the procedure of building a dependency graph ( BDG )(line 2). The first procedure also produces two sets of edge weights, W M and W G ,forthe dependency graph M and the complete graph G respectively. Then, we partition M into several disjoint clusters using the procedure of star discovery ( SD )(line 3). The procedure is highly motivated by current research work on complex net-work [11,12] and is rather reliable to generate consistent clusters. The disjoint clus-ters reveal some local components that s hall be connected in the domain. Hence, we expand the clusters into a set of overlapping clusters by discovering a high correla-tion between inter-cluster memberships. The procedure of cluster expansion ( CE ) uses the parameter to control the proportion of overlapping variables in the truly correlated clusters (line 4). We proceed to learn a Bayesian knot for each overlap-ping cluster separately by structuring the relation of cluster variables. The pro-cedure of learning Baysian knots ( LBK ) may utilize any of available structural learning algorithms (line 5). Finally, we use structural rules to combine the learned Bayesian knots and recover the complete Bayesian network structure B in which a set of nodes V B are connected with directed edges E B . 3.1 Build a Dependency Graph Bayesian network structures exhibit the dependency among variables in a data set. A strong dependency always gathers the variables into one local component. In other words, the tightly linked variables are potential nodes that will be enclosed in the same cluster. We expect to build a representative dependency graph from which some sound clusters could be discovered. The graph must be able to characterize a strong dependency of domain variables through their connectivity. We select the maximum spanning tree [13] as the dependency graph M since the tree is the smalle st connected graph that optimally approximates the joint distribution of domain variables. The dependency graph M =( V M ,E M ) is a tree in which each edge, e i,j  X  E M , connects a pair of nodes v i and v j ( v i , v j  X  V M ) and the edge has the weight w i,j ( w i,j  X  W M )measuredbythe mutual information MI ( v i ,v j ). We show the procedure of building a dependency graph ( BDG )inFig.2.

We compute MI ( v i ,v j ) for all pairs of variables (for l -size samples) and con-struct the complete graph G (line 1). We use the hash table that shows efficient computation. The complexity of this task is in the order of O ( n 2 ). We slightly modify the Kruskal X  X  algorithm to build the tree M (the original Kruskal X  X  al-gorithm [14] finds the minimum spanning tree by sorting weights decreasingly instead of increasingly) (lines 3-6). We use an union-finder data structure and a sorted list for adding arcs into M . The complexity is in the order of O ( n log n ). 3.2 Discover Local Components The output M is a minimal description of dependency among the variables. We opt for this dependency graph because it represents the most significant interactions in a topology that could be clustered (recall that variable clustering in complex graphs is an NP-hard problem). Many clustering methods [15,16] have appeared and shown competitive results in some domains. However, most of them aim for different optimization problems. Moreover, they can X  X  generate consistent clusters due to random selection of initi al cluster modes. We are interested in offering a robust algorithm that clusters a set of truly dependent variables by examining a graph topology together with edge weights.
 We aim to find a set of clusters C (each cluster C i contains a set of vertices V
C i in which one vertex v j is called as cluster center node o j ) that maximize the function in Eq. 1. In other words, we want to maximize the sum of depenency weights (between cluster variables v i and cluster nodes o j ) over multiple clusters. where o j is the center node v j in the cluster C i and w i,j  X  W G . We use some sound graph operations to maximize Eq. 1 and show the Star Discovery ( SD ) procedure in Fig. 3. The idea is motivated by current research results on complex networks and evolves from the spanning star in the scale-free networks [17]. The research characterizes domain patterns in terms of connectiv-ity of nodes, densities of clusters of nodes, and so on. It indicates that nodes of strong relations are always close and reside in a neigboring position. It suggests some hidden, but natural, domain patterns could be discovered by investigating the constructed graph topology.

We start by building a set of stars S = { S 1 ,  X  X  X  ,S n | S i =( V S i ,E S i ) } (lines 2-7). Each star S i is not a single node, but a connected sub-graph in the dependency graph. We initialize each node v i as the star center node o i (line 2). The center node o i , together with its adjacent nodes Adj ( v i ) and leaf nodes Leaf ( v j )next to the adjacent nodes v j ( v j  X  Adj ( v i )), composes the initial n stars (lines 3-6). edges in S i (line 7). Then, we find a set of clusters C from the set of stars S the cluster (line 9). When the star S i becomes a cluster it will be removed from the set S together with the stars S j that have the center node o j residing in the selected star S i (line 10). Afterwards, we select the star of the s econd largest weight as a new cluster. Hence, we get a set of k clusters in an iterative way without having to specify the cluster number k in the initialization. The SD complexity is dominated by the building of stars and takes O ( n 3 )operations searching for all adjacent and leaf nodes.

The SD procedure maximizes Eq. 1 through finding clusters that contain nodes close to cluster centers in the dependency graph. We notice the SD proce-dure avoids random initialization of clusters since it builds clusters by selecting the star that has the largest weight among all remained stars. Consequently, we do not need to specify the cluster number k and get consistent clusters upon one data set. This is significantly different from other clustering methods that need to assume a number of initial clusters at random. 3.3 Cluster Expansion A cluster contains a set of most correlated variables that may compose a local component in the domain. Since the SD procedure may result in disjoint clus-ters we may lose some local correlations that link variables in separated clusters. In addition, we need to recover the complete network structure by joining lo-cal cluster structures. The interdependency of clusters will provide foundation in the combination phase. Hence, we pro ceed to expand disjoint clusters into overlapping clusters by discovering cluster interdependency.

We present the Cluster Expansion ( CE ) procedure in Fig. 4. The basic idea is to expand clusters by including outlier variables that have most strong de-pendency with cluster memberships. The procedure uses two phases, cluster expansion (lines 1-6) and region expansion (lines 7-14), to generate a set of over-lapping clusters. In the first phase, we use the parameter to control the number of overlapping variables for possibly expanded clusters (line 3). For each clus-ter C i ,weidentify | C i | X  (the ceil function  X  ) numbers of outlier variables v j that have the most strong dependency with cluster variables v i by measur-ing their weights (line 4), and include these outlier varliables into the targeted cluster (line 6). The complexity of this phase is governed by the searching of relevant variables in k disjoint clusters and is in the order of O ( ksn )where s is the maximal cardinality of any given cluster C i .
In the first phase, clusters are expanded through absorbing a limited num-ber of outlier variables that have strong dependency with cluster memberships. Consequently, some isolated regions that contain a set of connected clusters may appear. For example, through the CE procedure, four disjoint clusters ( C 1 , C 2 , C ,and C 4 ) may result in two isolated regions, ( OC 1  X  OC 2 )and( OC 3  X  OC 4 ). The cluster C 1 locates | C 1 | X  most relevant variables all of which reside in the cluster C 2 ,andthecluster C 2 finds all the most relevant variables in the cluster C 1 ;sodotheclusters C 3 and C 4 . We need to remedy the cluster expansion phase to ensure the cluster reachability (direct or undirect) if it is necessary.

In the same vein as cluster expansion phase, the second phase expands isolated regions by including (region) outlier variables that have the most dependency with region variables. We compose the region R by connecting the clusters (from the first phase) that have already shared some overlapping variables (lines 9-10). Then, we detect possible iso lated regions (line 11). If such regions exist we need to connect them by adding the mos t relevant outlier variables v j into the targeted cluster (lines 12-13). We also get the byproduct of a set of overlapping nodes OV i (line 13). The complexity of the second phase is dominated by the searching of relevant nodes in possibly isolated regions and is in the order of O ( nm )where m is the maximum number of variables within one region. 3.4 Recover Bayesian Network Structures The CE procedure expands the disjoint clusters so that each cluster is con-nected to at least one of other cluster s. We proceed to learn a set of Bayesian Knots ( BK ) by structuring relations of variables in clusters. We describe the learning Bayesian knots ( LBK ) procedure in Fig. 5. The procedure receives the input of the data set D and a set of overlapping clusters OC (line 1). We apply any of available structual learning algorithms to construct a Bayesian knot ( BK ) that is a directed acyclic graph (line 3). Each BK i contains a set of nodes V BK i domain. The procedure complexity relies on the selected learning algorithm (line 3). For example, if the PC algorithm is used the complexity is in the order of O ( kr q ) for learning k clusters where q is the maximum number of parents for anodeand r is the largest cluster size. In ge neral, a cluster contains a small subset of domain variables ( r n ). The complexity is relatively low comparing with the order of O ( n q ) for learning the complete network directly. The final procedure is to complete the learning task by joining the learned Bayesian knots that share common variables. We show the procedure of combin-ing Bayesian knots in Fig. 6. The procedure takes some rules to address conflict-ing structural problems and to avoid glo bal directed cycles in the network. The conflict occurs when the direction of arcs c onnecting overlapping nodes differs in linked knots.
 We start the Bayesian network B with a complete undirected graph (line 1). Then, we remove edges from the complete graph that do not exist in any of the learned Bayesian knots (line 2). All the remained edges must be directed in at least one of the Bayesian knots. We direct those edges that have already been oriented in at most one Bayesian knot (line 4). Subsequently, we use three rules to orient the rest undirected edges since the edges are directed differently in over-lapping Bayesian knots (lines 5-8). The first rule is to avoid new v-structures (line 6). In most constraint-based learning methods, directions of edges participating in v-structures are uncovered using independency tests, rather than through structural rules afterward s. The second rule avoids directed cycles by forcing the arc direction (line 7). Finally, if both rules can X  X  be applied we orient edges randomly following directions in one Bayesian knot (line 8). The combination procedure aims for the arc orientation using structural rules instead of expensive independency tests. We demonstrate the empirical perform ance of the decomposition learning al-gorithm on several benchmarks : ALARM (37 nodes), Hailfinder (56 nodes), HeparII (70 nodes), Pathfinder (109 nodes), and Andes (223 nodes). We also compare the performance with two typical constraint-based learning methods. One is the basic learning method of the PC algorithm [3] and the other is three phase dependency analysis ( TPDA ) [18] algorithm that is the winner of 2001 KDD cup. In addition, we compare with the block learning algorithm. We gener-ate several data sets (ranging from small to large sample sizes) and compute the Euclidean distance (of the sensitivity and specificity from the perfect score 1) [19] between the learned structures and the be nchmarks. The Euclidean distance is defined in Eq. 2. where the sensitivity of the algorithm is the ratio of correctly identified edges (undirected arcs) over the total number of edges in the real network while the specificity is the ratio of edges correctly identified as not belonging in the graph over the true number of edges not present in the real network.

In most cases, we show that the decomposition learning algorithm outperforms other learning algorithms and achieves lower distance values. In particular, the new algorithm keeps a good quality struc ture even when the data set is reduced. Furthermore, we obtain computational savings from using the decomposition algorithm as indicated by the low run times. We show the performance of the decomposition learning algorithm in Fig. 7 3 . Each data point is the average of 10 runs for different data sets of same size 4 . Both the decomposition and the block learning algorithms that are equipped with learning engines have better performance than the PC and TPDA learning algorithms regarding the distance measure. This remains true for a range of data sets. For small domains, such as Alarm and Hailfinder networks, both the decom-position learning algorithm and the block learning algorithm exhibit similar per-formance of low distance. However, the decomposition algorithm has significantly better results on the rest three large netwo rks, especially for small data sets. We report only the performance of the BL and DL learning algorithms on the three larger networks since both the PC and TPDA algorithms fail.

We also observe from Fig. 7 that the decomposition learning algorithm retains a good quality of learned structures when the sample size is noticeably reduced. In addition, the decomposition algorithms have a lower variance than the block learning algorithms. This is due to the DL method has a reliable clustering method SD comparing with the incomplete block identification in BL .
Finally, the run times in Fig. 8 are indicative of the computational savings incurred by using the decomposition learning algorithm. The decomposition al-gorithm achieves more savings than the block learning algorithm since the latter needs an expensive procedure of learning overlapping structures. Using the de-composition algorithm we were able to learn the three large domains of HeparII, Pathfinder, and Andes, while both the PC and TPDA algorithms run out of memory. We expect similar results of good performance without intensive com-putation in real applications. The decomposition learning algorithm is able to learn a large Bayesian network structure and shows good performance even when insufficient data are provided. It significantly improves the block learning algorithm on the aspects of robust clustering methods and well-defined combination rules. The modular design pro-vides a way to exploit state-of-the-art of both Bayesian network learning and at-tribute clustering techniques. In addition, the decomposition learning algorithm offers useful intermediate clusters or Bay esian knots that repr esent local domain structures and may attract interest into f urther study. Several issues relevant to the decomposition learning algorithm deserves further study. We are currently investigating an adaptive cluster expansion.

