 We are interested in retrieving information from speech data like broadcast news, telephone conversations and roundtable meetings. Today, most systems use large vocabulary contin-uous speech recognition tools to produce word transcripts; the transcripts are indexed and query terms are retrieved from the index. However, query terms that are not part of the recognizer X  X  vocabulary cannot be retrieved, and the recall of the search is affected. In addition to the output word transcript, advanced systems provide also phonetic transcripts, against which query terms can be matched pho-netically. Such phonetic transcripts suffer from lower accu-racy and cannot be an alternative to word transcripts.
We present a vocabulary independent system that can han-dle arbitrary queries, exploiting the information provided by having both word transcripts and phonetic transcripts. A speech recognizer generates word confusion networks and phonetic lattices. The transcripts are indexed for query pro-cessing and ranking purpose. The value of the proposed method is demonstrated by the relative high performance of our system, which received the highest overall ranking for US English speech data in the recent NIST Spoken Term Detection evaluation [1].
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms Speech retrieval, spoken term detection, out-of-vocabulary
The rapidly increasing amount of spoken data calls for solutions to index and search this data.
 Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
The classical approach consists of converting the speech to word transcripts using a large vocabulary continuous speech recognition (LVCSR) tool. In the past decade, most of the research efforts on spoken data retrieval have focused on ex-tending classical IR techniques to word transcripts. Some of these works have been done in the framework of the NIST TREC Spoken Document Retrieval tracks and are described by Garofolo et al. [12]. These tracks focused on retrieval from a corpus of broadcast news stories spoken by profes-sionals. One of the conclusions of those tracks was that the effectiveness of retrieval mostly depends on the accu-racy of the transcripts. While the accuracy of automatic speech recognition (ASR) systems depends on the scenario and environment, state-of-the-art systems achieved better than 90% accuracy in transcription of such data. In 2000, Garofolo et al. concluded that  X  X poken document retrieval is a solved problem X  [12].

However, a significant drawback of such approaches is that search on queries containing out-of-vocabulary (OOV) terms will not return any results. OOV terms are missing words from the ASR system vocabulary and are replaced in the output transcript by alternatives that are probable, given the recognition acoustic model and the language model. It has been experimentally observed that over 10% of user queries can contain OOV terms [16], as queries often re-late to named entities that typically have a poor coverage in the ASR vocabulary. The effects of OOV query terms in spoken data retrieval are discussed by Woodland et al. [28]. In many applications the OOV rate may get worse over time unless the recognizer X  X  vocabulary is periodically updated.
Another approach consists of converting the speech to phonetic transcripts and representing the query as a se-quence of phones. The retrieval is based on searching the sequence of phones representing the query in the phonetic transcripts. The main drawback of this approach is the in-herent high error rate of the transcripts. Therefore, such approach cannot be an alternative to word transcripts, es-pecially for in-vocabulary (IV) query terms that are part of the vocabulary of the ASR system.

A solution would be to combine the two different ap-proaches presented above: we index both word transcripts and phonetic transcripts; during query processing, the in-formation is retrieved from the word index for IV terms and from the phonetic index for OOV terms. We would like to be able to process also hybrid queries, i.e, queries that in-clude both IV and OOV terms. Consequently, we need to merge pieces of information retrieved from word index and phonetic index. Proximity information on the occurrences of the query terms is required for phrase search and for proximity-based ranking. In classical IR, the index stores for each occurrence of a term, its offset. Therefore, we cannot merge posting lists retrieved by phonetic index with those retrieved by word index since the offset of the occurrences retrieved from the two different indices are not comparable. The only element of comparison between phonetic and word transcripts are the timestamps. No previous work combin-ing word and phonetic approach has been done on phrase search. We present a novel scheme for information retrieval that consists of storing, during the indexing process, for each unit of indexing (phone or word) its timestamp. We search queries by merging the information retrieved from the two different indices, word index and phonetic index, according to the timestamps of the query terms. We analyze the re-trieval effectiveness of this approach on the NIST Spoken Term Detection 2006 evaluation data [1].

The paper is organized as follows. We describe the audio processing in Section 2. The indexing and retrieval methods are presented in section 3. Experimental setup and results are given in Section 4. In Section 5, we give an overview of related work. Finally, we conclude in Section 6.
We use an ASR system for transcribing speech data. It works in speaker-independent mode. For best recognition results, a speaker-independent acoustic model and a lan-guage model are trained in advance on data with similar characteristics.

Typically, ASR generates lattices that can be considered as directed acyclic graphs. Each vertex in a lattice is associ-ated with a timestamp and each edge ( u, v ) is labeled with a word or phone hypothesis and its prior probability , which is the probability of the signal delimited by the timestamps of the vertices u and v , given the hypothesis. The 1-best path transcript is obtained from the lattice using dynamic programming techniques.

Mangu et al. [18] and Hakkani-Tur et al. [13] propose a compact representation of a word lattice called word confu-sion network (WCN). Each edge ( u, v ) is labeled with a word hypothesis and its posterior probability , i.e. , the probability of the word given the signal. One of the main advantages of WCN is that it also provides an alignment for all of the words in the lattice. As explained in [13], the three main steps for building a WCN from a word lattice are as follows: 1. Compute the posterior probabilities for all edges in the 2. Extract a path from the word lattice (which can be 3. Traverse the word lattice, and align all the transitions The 1-best path of a WCN is obtained from the path con-taining the best hypotheses. As stated in [18], although WCNs are more compact than word lattices, in general the 1-best path obtained from WCN has a better word accuracy than the 1-best path obtained from the corresponding word lattice.
 Typical structures of a lattice and a WCN are given in Figure 1.
 Figure 1: Typical structures of a lattice and a WCN.
The main problem with retrieving information from spo-ken data is the low accuracy of the transcription particu-larly on terms of interest such as named entities and con-tent words. Generally, the accuracy of a word transcript is characterized by its word error rate (WER). There are three kinds of errors that can occur in a transcript: sub-stitution of a term that is part of the speech by another term, deletion of a spoken term that is part of the speech and insertion of a term that is not part of the speech.
Substitutions and deletions reflect the fact that an occur-rence of a term in the speech signal is not recognized. These misses reduce the recall of the search. Substitutions and in-sertions reflect the fact that a term which is not part of the speech signal appears in the transcript. These misses reduce the precision of the search.

Search recall can be enhanced by expanding the transcript with extra words. These words can be taken from the other alternatives provided by the WCN; these alternatives may have been spoken but were not the top choice of the ASR. Such an expansion tends to correct the substitutions and the deletions and consequently, might improve recall but will probably reduce precision. Using an appropriate rank-ing model, we can avoid the decrease in precision. Mamou et al. have presented in [17] the enhancement in the recall and the MAP by searching on WCN instead of considering only the 1-best path word transcript in the context of spoken doc-ument retrieval. We have adapted this model of IV search to term detection. In word transcripts, OOV terms are deleted or substituted. Therefore, the usage of phonetic transcripts is more desirable. However, due to their low accuracy, we have preferred to use only the 1-best path extracted from the phonetic lattices. We will show that the usage of phonetic transcripts tends to improve the recall without affecting the precision too much, using an appropriate ranking.
As stated in the STD 2006 evaluation plan [2], the task consists in finding all the exact matches of a specific query in a given corpus of speech data. A query is a phrase con-taining several words. The queries are text and not speech. Note that this task is different from the more classical task of spoken document retrieval. Manual transcripts of the speech are not provided but are used by the evaluators to find true occurrences. By definition, true occurrences of a query are found automatically by searching the manual transcripts us-ing the following rule: the gap between adjacent words in a query must be less than 0.5 seconds in the corresponding speech. For evaluating the results, each system output oc-currence is judged as correct or not according to whether it is  X  X lose X  in time to a true occurrence of the query retrieved from manual transcripts; it is judged as correct if the mid-point of the system output occurrence is less than or equal to 0.5 seconds from the time span of a true occurrence of the query.
We have used the same indexing process for WCN and phonetic transcripts. Each occurrence of a unit of indexing (word or phone) u in a transcript D is indexed with the following information:
In addition, for WCN indexing, we store
Note that since the task is to find exact matches of the phrase queries, we have not filtered stopwords and the cor-pus is not stemmed before indexing.
In the following, we present our approach for accomplish-ing the STD task using the indices described above. The terms are extracted from the query. The vocabulary of the ASR system building word transcripts is given. Terms that are part of this vocabulary are IV terms; the other terms are OOV. For an IV query term, the posting list is extracted from the word index. For an OOV query term, the term is converted to a sequence of phones using a joint maximum entropy N-gram model [10]. For example, the term prosody is converted to the sequence of phones (p, r, aa, z, ih, d, iy) . The posting list of each phone is extracted from the phonetic index.

The next step consists of merging the different posting lists according to the timestamp of the occurrences in order to create results matching the query. First, we check that the words and phones appear in the right order according to their begin times. Second, we check that the gap in time be-tween adjacent words and phones is  X  X easonable X . Conform-ing to the requirements of the STD evaluation, the distance in time between two adjacent query terms must be less than 0.5 seconds. For OOV search, we check that the distance in time between two adjacent phones of a query term is less that 0.2 seconds; this value has been determined empirically. In such a way, we can reduce the effect of insertion errors since we allow insertions between the adjacent words and phones. Our query processing does not allow substitutions and deletions.

Example: Let us consider the phrase query prosody research . The term prosody is OOV and the term research is IV. The term prosody is converted to the sequence of phones (p, r, aa, z, ih, d, iy) . The posting list of each phone is extracted from the phonetic index. We merge the posting lists of the phones such that the sequence of phones appears in the right order and the gap in time between the less than 0.2 seconds. We obtain occurrences of the term prosody . The posting list of research is extracted from the word index and we merge it with the occurrences found for prosody such that they appear in the right order and the distance in time between prosody and research is less than 0.5 seconds.

Note that our indexing model allows to search for different types of queries: 1. queries containing only IV terms using the word index. 2. queries containing only OOV terms using the phonetic 3. keyword queries containing both IV and OOV terms 4. phrase queries containing both IV and OOV terms; for The STD evaluation has focused on the fourth query type. It is the hardest task since we need to combine posting lists retrieved from phonetic and word indices.
Since IV terms and OOV terms are retrieved from two dif-ferent indices, we propose two different functions for scoring an occurrence of a term; afterward, an aggregate score is as-signed to the query based on the scores of the query terms. Because the task is term detection, we do not use a docu-ment frequency criterion for ranking the occurrences.
Let us consider a query Q = ( k 0 , ..., k n ), associated with a boosting vector B = ( B 1 , ..., B j ). This vector associates a boosting factor to each rank of the different hypotheses; the boosting factors are normalized between 0 and 1. If the rank r is larger than j , we assume B r = 0.
For IV term ranking, we extend the work of Mamou et al. [17] on spoken document retrieval to term detection. We use the information provided by the word index. We define the score score ( k, t, D ) of a keyword k occurring at a time t in the transcript D , by the following formula: Note that 0  X  score ( k, t, D )  X  1.

For OOV term ranking, we use the information provided by the phonetic index. We give a higher rank to occurrences of OOV terms that contain phones close (in time) to each other. We define a scoring function that is related to the average gap in time between the different phones. Let us consider a keyword k converted to the sequence of phones ( p 0 , ..., p k l ). We define the normalized score score ( k, t of a keyword k = ( p k 0 , ..., p k l ), where each p k i t with a duration of d k i in the transcript D , by the following formula: Note that according to what we have ex-plained in Sec-tion 3.3, we have  X  1  X  i  X  l, 0 &lt; t k i  X  ( t k i  X  1 0 &lt; score ( k, t k 0 , D )  X  1. The duration of the keyword oc-currence is t k l  X  t k 0 + d k l .

Example: let us consider the sequence (p, r, aa, z, ih, d, iy) and two different occurrences of the sequence. For each phone, we give the begin time and the duration in second.
 Occurrence 1: ( p , 0.25, 0.01), ( r , 0.36, 0.01), ( aa , 0.37, 0.01), Occurrence 2: ( p , 0.45, 0.01), ( r , 0.46, 0.01), ( aa , 0.47, 0.01), According to our formula, the score of the first occurrence is 0.83 and the score of the second occurrence is 1. In the first occurrence, there are probably some insertion or silence between the phone p and r , and between the phone d and iy . The silence can be due to the fact that the phones belongs to two different words ans therefore, it is not an occurrence of the term prosody .
The score of an occurrence of a query Q at time t 0 in the document D is determined by the multiplication of the score of each keyword k i , where each k i occurs at time t i with a duration d i in the transcript D : Note that according to what we have ex-plained in Sec-tion 3.3, we have  X  1  X  i  X  n, 0 &lt; t i  X  ( t i  X  1 + d
Our goal is to estimate for each found occurrence how likely the query appears. It is different from classical IR that aims to rank the results and not to score them. Since the probability to have a false alarm is inversely proportional to the length of the phrase query, we have boosted the score of queries by a  X  n exponent, that is related to the number of keywords in the phrase. We have determined empirically the value of  X  n = 1 /n .

The begin time of the query occurrence is determined by the begin time t 0 of the first query term and the duration of the query occurrence by t n  X  t 0 + d n .
Our corpus consists of the evaluation set provided by NIST for the STD 2006 evaluation [1]. It includes three differ-ent source types in US English: three hours of broadcast news (BNEWS), three hours of conversational telephony speech (CTS) and two hours of conference room meetings (CONFMTG). As shown in Section 4.2, these different col-lections have different accuracies. CTS and CONFMTG are spontaneous speech. For the experiments, we have processed the query set provided by NIST that includes 1100 queries. Each query is a phrase containing between one to five terms, common and rare terms, terms that are in the manual tran-scripts and those that are not. Testing and determination of empirical values have been achieved on another set of speech data and queries, the development set , also provided by NIST.

We have used the IBM research prototype ASR system, described in [26], for transcribing speech data. We have produced WCNs for the three different source types. 1-best phonetic transcripts were generated only for BNEWS and CTS, since CONFMTG phonetic transcripts have too low accuracy. We have adapted Juru [7], a full-text search li-brary written in Java, to index the transcripts and to store the timestamps of the words and phones; search results have been retrieved as described in Section 3.

For each found occurrence of the given query, our system outputs: the location of the term in the audio recording (begin time and duration), the score indicating how likely is the occurrence of query, (as defined in Section 3.4) and a hard (binary) decision as to whether the detection is cor-rect. We measure precision and recall by comparing the results obtained over the automatic transcripts (only the re-sults having true hard decision) to the results obtained over the reference manual transcripts. Our aim is to evaluate the ability of the suggested retrieval approach to handle tran-scribed speech data. Thus, the closer the automatic results to the manual results is, the better the search effectiveness over the automatic transcripts will be. The results returned from the manual transcription for a given query are consid-ered relevant and are expected to be retrieved with highest scores. This approach for measuring search effectiveness us-ing manual data as a reference is very common in speech retrieval research [25, 22, 8, 9, 17].

Beside the recall and the precision, we use the evaluation measures defined by NIST for the 2006 STD evaluation [2]: the Actual Term-Weighted Value (ATWV) and the Maxi-mum Term-Weighted Value (MTWV). The term-weighted value (TWV) is computed by first computing the miss and false alarm probabilities for each query separately, then us-ing these and an (arbitrarily chosen) prior probability to compute query-specific values, and finally averaging these query-specific values over all queries q to produce an overall system value:
T W V (  X  ) = 1  X  average q { P miss ( q,  X  ) +  X   X  P F A where  X  = C V ( P r  X  1 q  X  1).  X  is the detection threshold. For the evaluation, the cost/value ratio, C/V , has been deter-mined to 0.1 and the prior probability of a query P r q to 10  X  4 . Therefore,  X  = 999 . 9.

Miss and false alarm probabilities for a given query q are functions of  X  : different source types. where:
ATWV is the  X  X ctual term-weighted value X ; it is the de-tection value attained by the system as a result of the system output and the binary decision output for each putative oc-currence. It ranges from  X  X  X  to +1. MTWV is the  X  X ax-imum term-weighted value X  over the range of all possible values of  X  . It ranges from 0 to +1.

We have also provided the detection error tradeoff (DET) curve [19] of miss probability ( P miss ) vs. false alarm prob-ability ( P F A ).

We have used the STDEval tool to extract the relevant results from the manual transcripts and to compute ATWV, MTWV and the DET curve.

We have determined empirically the following values for the boosting vector defined in Section 3.4: B i = 1 i .
We use the word error rate (WER) in order to characterize the accuracy of the transcripts. WER is defined as follows: where N is the total number of words in the corpus, and S , I , and D are the total number of substitution, insertion, and deletion errors, respectively. The substitution error rate (SUBR) is defined by Deletion error rate (DELR) and insertion error rate (INSR) are defined in a similar manner.

Table 1 gives the WER and the distribution of the error types over 1-best path transcripts extracted from WCNs. The WER of the 1-best path phonetic transcripts is approx-imately two times worse than the WER of word transcripts. That is the reason why we have not retrieved from phonetic transcripts on CONFMTG speech data.
We have determined empirically a detection threshold  X  per source type and the hard decision of the occurrences having a score less than  X  is set to false ; false occurrences returned by the system are not considered as retrieved and therefore, are not used for computing ATWV, precision and recall.
 The value of the threshold  X  per source type is reported in Table 2. It is correlated to the accuracy of the transcripts. Basically, setting a threshold aims to eliminate from the retrieved occurrences, false alarms without adding misses. The higher the WER is, the higher the  X  threshold should be.
 Table 2: Values of the  X  threshold per source type.
We report in Table 3 the processing resource profile. Con-cerning the index size, note that our index is compressed using IR index compression techniques. The indexing time includes both audio processing (generation of word and pho-netic transcripts) and building of the searchable indices. Table 3: Processing resource profile. (HS: Hours of Speech. HP: Processing Hours. sec.P: Processing seconds) We compare our approach ( WCN phonetic ) presented in Section 4.1 with another approach ( 1-best-WCN phonetic ). The only difference between these two approaches is that, in 1-best-WCN phonetic , we index only the 1-best path ex-tracted from the WCN instead of indexing all the WCN. WCN phonetic was our primary system for the evaluation and 1-best-WCN phonetic was one of our contrastive sys-tems. Average precision and recall, MTWV and ATWV on the 1100 queries are given in Table 4. We provide also the DET curve for WCN phonetic approach in Figure 2. The point that maximizes the TWV, the MTWV, is specified on each curve. Note that retrieval performance has been eval-uated separately for each source type since the accuracy of the speech differs per source type as shown in Section 4.2.
As expected, we can see that MTWV and ATWV decrease in higher WER. The retrieval performance is improved when
Figure 2: DET curve for WCN phonetic approach. using WCNs relatively to 1-best path. It is due to the fact that miss probability is improved by indexing all the hy-potheses provided by the WCNs. This observation confirms the results shown by Mamou et al. [17] in the context of spo-ken document retrieval. The ATWV that we have obtained is close to the MTWV; we have combined our ranking model with appropriate threshold  X  to eliminate results with lower score. Therefore, the effect of false alarms added by WCNs is reduced.

WCN phonetic approach was used in the recent NIST STD evaluation and received the highest overall ranking among eleven participants. For comparison, the system that ranked at the third place, obtained an ATWV of 0.8238 for BNEWS, 0.6652 for CTS and 0.1103 for CONFMTG.
We have analysed the retrieval performance according to the average duration of the occurrences in the manual tran-scripts. The query set was divided into three different quan-tiles according to the duration; we have reported in Table 5 ATWV and MTWV according to the duration. We can see that we performed better on longer queries. One of the rea-sons is the fact that the ASR system is more accurate on long words. Hence, it was justified to boost the score of the results with the exponent  X  n , as explained in Section 3.4.3, according to the length of the query. Table 5: ATWV, MTWV according to the duration of the query occurrences per source type.
We have randomly chosen three sets of queries from the query sets provided by NIST: 50 queries containing only IV terms; 50 queries containing only OOV terms; and 50 hybrid queries containing both IV and OOV terms. The following experiment has been achieved on the BNEWS collection and IV and OOV terms has been determined according to the vocabulary of BNEWS ASR system.

We would like to compare three different approaches of retrieval: using only word index; using only phonetic index; combining word and phonetic indices. Table 6 summarizes the retrieval performance according to each approach and to each type of queries. Using a word-based approach for dealing with OOV and hybrid queries affects drastically the performance of the retrieval; precision and recall are null. Using a phone-based approach for dealing with IV queries affects also the performance of the retrieval relatively to the word-based approach.

As expected, the approach combining word and phonetic indices presented in Section 3 leads to the same retrieval performance as the word approach for IV queries and to the same retrieval performance as the phonetic approach for OOV queries. This approach always outperforms the others and it justifies the fact that we need to combine word and phonetic search.
In the past decade, the research efforts on spoken data retrieval have focused on extending classical IR techniques to spoken documents. Some of these works have been done in the context of the TREC Spoken Document Retrieval evaluations and are described by Garofolo et al. [12]. An LVCSR system is used to transcribe the speech into 1-best path word transcripts. The transcripts are indexed as clean text: for each occurrence, its document, its word offset and additional information are stored in the index. A generic IR system over the text is used for word spotting and search as described by Brown et al. [6] and James [14]. This strat-egy works well for transcripts like broadcast news collections that have a low WER (in the range of 15%-30%) and are redundant by nature (the same piece of information is spo-ken several times in different manners). Moreover, the algo-rithms have been mostly tested over long queries stated in plain English and retrieval for such queries is more robust against speech recognition errors.

An alternative approach consists of using word lattices in order to improve the effectiveness of SDR. Singhal et al. [24, 25] propose to add some terms to the transcript in order to alleviate the retrieval failures due to ASR errors. From an IR perspective, a classical way to bring new terms is document expansion using a similar corpus. Their approach consists in using word lattices in order to determine which words returned by a document expansion algorithm should be added to the original transcript. The necessity to use a document expansion algorithm was justified by the fact that the word lattices they worked with, lack information about word probabilities.

Chelba and Acero in [8, 9] propose a more compact word lattice, the position specific posterior lattice (PSPL). This data structure is similar to WCN and leads to a more com-pact index. The offset of the terms in the speech documents is also stored in the index. However, the evaluation frame-work is carried out on lectures that are relatively planned, in contrast to conversational speech. Their ranking model is based on the term confidence level but does not take into consideration the rank of the term among the other hypothe-ses. Mamou et al. [17] propose a model for spoken document retrieval using WCNs in order to improve the recall and the MAP of the search. However, in the above works, the prob-lem of queries containing OOV terms is not addressed.
Popular approaches to deal with OOV queries are based on sub-words transcripts, where the sub-words are typically phones, syllables or word fragments (sequences of phones) [11, 20, 23]. The classical approach consists of using pho-netic transcripts. The transcripts are indexed in the same manner as words in using classical text retrieval techniques; during query processing, the query is represented as a se-quence of phones. The retrieval is based on searching the string of phones representing the query in the phonetic tran-script. To account for the high recognition error rates, some other systems use richer transcripts like phonetic lattices. They are attractive as they accommodate high error rate conditions as well as allow for OOV queries to be used [15, 3, 20, 23, 21, 27]. However, phonetic lattices contain many edges that overlap in time with the same phonetic label, and are difficult to index. Moreover, beside the improvement in the recall of the search, the precision is affected since pho-netic lattices are often inaccurate. Consequently, phonetic approaches should be used only for OOV search; for search-ing queries containing also IV terms, this technique affects the performance of the retrieval in comparison to the word based approach.

Saraclar and Sproat in [22] show improvement in word spotting accuracy for both IV and OOV queries, using pho-netic and word lattices, where a confidence measure of a word or a phone can be derived. They propose three dif-ferent retrieval strategies: search both the word and the phonetic indices and unify the two different sets of results; search the word index for IV queries, search the phonetic in-dex for OOV queries; search the word index and if no result is returned, search the phonetic index. However, no strategy is proposed to deal with phrase queries containing both IV and OOV terms. Amir et al. in [5, 4] propose to merge a word approach with a phonetic approach in the context of video retrieval. However, the phonetic transcript is obtained from a text to phonetic conversion of the 1-best path of the word transcript and is not based on a phonetic decoding of the speech data.

An important issue to be considered when looking at the state-of-the-art in retrieval of spoken data, is the lack of a common test set and appropriate query terms. This paper uses such a task and the STD evaluation is a good summary of the performance of different approaches on the same test conditions.
This work studies how vocabulary independent spoken term detection can be performed efficiently over different data sources. Previously, phonetic-based and word-based approaches have been used for IR on speech data. The for-mer suffers from low accuracy and the latter from limited vocabulary of the recognition system. In this paper, we have presented a vocabulary independent model of indexing and search that combines both the approaches. The system can deal with all kinds of queries although the phrases that need to combine for the retrieval, information extracted from two different indices, a word index and a phonetic index. The scoring of OOV terms is based on the proximity (in time) be-tween the different phones. The scoring of IV terms is based on information provided by the WCNs. We have shown an improvement in the retrieval performance when using all the WCN and not only the 1-best path and when using phonetic index for search of OOV query terms. This approach always outperforms the other approaches using only word index or phonetic index.

As a future work, we will compare our model for OOV search on phonetic transcripts with a retrieval model based on the edit distance. Jonathan Mamou is grateful to David Carmel and Ron Hoory for helpful and interesting discussions.
 [1] NIST Spoken Term Detection 2006 Evaluation [2] NIST Spoken Term Detection (STD) 2006 Evaluation [3] C. Allauzen, M. Mohri, and M. Saraclar. General [4] A. Amir, M. Berg, and H. Permuter. Mutual relevance [5] A. Amir, A. Efrat, and S. Srinivasan. Advances in [6] M. Brown, J. Foote, G. Jones, K. Jones, and S. Young. [7] D. Carmel, E. Amitay, M. Herscovici, Y. S. Maarek, [8] C. Chelba and A. Acero. Indexing uncertainty for [9] C. Chelba and A. Acero. Position specific posterior [10] S. Chen. Conditional and joint models for [11] M. Clements, S. Robertson, and M. Miller. Phonetic [12] J. Garofolo, G. Auzanne, and E. Voorhees. The TREC [13] D. Hakkani-Tur and G. Riccardi. A general algorithm [14] D. James. The application of classical information [15] D. A. James. A system for unrestricted topic retrieval [16] B. Logan, P. Moreno, J. V. Thong, and E. Whittaker. [17] J. Mamou, D. Carmel, and R. Hoory. Spoken [18] L. Mangu, E. Brill, and A. Stolcke. Finding consensus [19] A. Martin, G. Doddington, T. Kamm, M. Ordowski, [20] K. Ng and V. W. Zue. Subword-based approaches for [21] Y. Peng and F. Seide. Fast two-stage [22] M. Saraclar and R. Sproat. Lattice-based search for [23] F. Seide, P. Yu, C. Ma, and E. Chang.
 [24] A. Singhal, J. Choi, D. Hindle, D. Lewis, and [25] A. Singhal and F. Pereira. Document expansion for [26] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, [27] K. Thambiratnam and S. Sridharan. Dynamic match [28] P. C. Woodland, S. E. Johnson, P. Jourlin, and K. S.
