 The performance of Learning to Rank algorithms strongly depend on the number of labelled queries in the training set, while the cost incurred in annotating a large number of queries with relevance judgements is prohibitively high. As a result, constructing such a training dataset involves se-lecting a set of candidate queries for labelling. In this work, we investigate query selection strategies for learning to rank aimed at actively selecting unlabelled queries to be labelled so as to minimize the data annotation cost. In particular, we characterize query selection based on two aspects of in-formativeness and representativeness and propose two novel query selection strategies (i) Permutation Probability based query selection and (ii) Topic Model based query selection which capture the two aspects, respectively. We further ar-gue that an ideal query selection strategy should take into account both these aspects and as our final contribution, we present a submodular objective that couples both these as-pects while selecting query subsets. We evaluate the quality of the proposed strategies on three real world learning to rank datasets and show that the proposed query selection methods results in significant performance gains compared to the existing state-of-the-art approaches.
 H.3.3 [ Information Storage And Retrieval ]: Informa-tion Search and Retrieval X  Learning to Rank Learning to Rank, Query Selection, Active Learning, Sub-modularity
Most modern search technologies are based on machine learning algorithms that learn to rank documents given a query, an approach that is commonly referred to as  X  X earning to rank X . Learning to Rank algorithms aim to learn ranking functions that achieve good ranking objectives on test data. c  X  Such learning methods require labelled data for training. As is the case with many supervised learning algorithms, the performance of Learning to Rank algorithms are often highly correlated with the amount of labelled training data available[1 ][17 ][7 ].

Constructing such labelled training data for learning-to-rank tasks incurs prohibitive costs since it requires selecting candidate queries, extracting features from query-document pairs and annotating documents in terms of their relevance to these queries (annotations are used as labels for train-ing). The major bottleneck in constructing learning-to-rank collections is annotating documents with query specific rele-vance grades. It is essential therefore, both for the efficiency of the construction methodology and for the efficiency of the training algorithm, that only a small subset of queries be selected. The query selection, though, should be done in a way that does not harm the effectiveness of learning.
Active Learning algorithms help reduce the annotation costs by selecting a subset of informative instances to be la-belled. Unlike traditional algorithms, active learning strate-gies for ranking algorithms are more complex because of the inherent query-document pair structure embodied in rank-ing datasets, non-smooth cost functions, etc., hence these cannot be applied directly in ranking setting.

Existing approaches for active learning for ranking have focused on selecting documents [ 1], selecting queries [ 17 ] or balancing number of queries with depth of documents judged using random query selection [27 ].

In this work, we focus on selecting subset of queries to be labelled so as to minimize the data annotation cost. Prior work on selecting queries made use of expected loss opti-mization [17 ] to estimate which queries should be selected but their approach is limited to rankers that predict abso-lute graded relevance which is not the case with modern Learning to Rank algorithms since many of them induce a ranking and not absolute labels [ 4]. Apart from the learning to rank setting, query selection has also received significant attention for evaluation setting [13 ] wherein the goal was to find a subset of queries that most closely approximates the system evaluation results that would be obtained if instead documents for the full set of queries was judged instead. However, it was shown by Aslam et a.l [1 ] that learning to rank and evaluation of retrieval systems are quite different from each other and that datasets constructed for evaluat-ing quality of retrieval systems are not necessarily good for training and vice versa. Therefore, query selection strategies that are directly devised for learning to rank purposes are needed.
Intuitively, an optimal subset of queries constructed for learning to rank should have two characteristics: (i) in-formativeness , which measures the ability of an instance (query) in reducing the uncertainty of a statistical model (ranking model) and (ii) representativeness , which measures if an instance (query) well represents the possible input pat-terns of unlabelled data (unlabelled queries) [ 22]. Most ex-isting active learning for ranking algorithms solely focus on the informativeness aspect of queries without considering the representativeness aspect which can lead to possible se-lection of noisy queries, not quite representative of the whole population of queries; thus, significantly limiting the perfor-mance of query selection.

In this work, we focus on query selection strategies for learning to rank and propose novel query selection algo-rithms aimed at finding an optimal subset of queries to be labelled. Since problems associated with subset selection are generally NP-Hard or NP-Complete[12 ], we approximate the solution by an iterative query selection process so as to min-imize the data annotation cost without severely degrading the performance of the ranking model.

We describe two paradigms of query selection strategies based on the aspects of informativeness and representative-ness described above and propose novel query selection tech-niques: Permutation Probability based query selection and query selection based on topic models which capture these two aspects, respectively. We further present a new algo-rithm based on defining a submodular objective that com-bines the powers of the two paradigms. Submodular func-tions have the characteristic of diminishing returns [19 ], which is an important attribute of any query-subset selection tech-nique since the value-addition from individual queries should ideally decrease as more and more queries are selected. Thus, not only are submodular functions natural for query subset selection, they can also be optimized efficiently and scalably such that the result has mathematical performance guaran-tees.

We show that our proposed algorithms result in signifi-cant improvements compared to state-of-the-art query selec-tion algorithms thereby helping in reducing data annotation costs. Active Learning for Labelling Cost Reduction: A number of active learning strategies have been proposed for the traditional supervised learning setting, a common one being uncertainty sampling which selects the unlabelled example about which the model is most uncertain how to label. Some of the others adopt the idea of reducing the generalization error and select the unlabelled example that has the highest effect on the test error, i.e. points in the maximally uncertain and highly dense regions of the under-lying data distribution[9 ]. A comprehensive active learning survey can be found in [22 ].

Reducing judgment effort for learning to rank has received significant amount of attention from the research commu-nity. Learning to rank methods are quite different than approaches used for classification as they require optimiz-ing nonsmooth cost functions such as NDCG and AP [ 24]. Moreover, owing to the unique query-document structure which inherent to the learning to rank setting, it is not straightforward to extend the models devised for traditional supervised learning settings to ranking problems. In recent years, active learning has been actively extended to rank learning and can be classified into two classes of approaches: document level and query level active learning.
 Document Selection for Learning to Rank: Based on uncertainty sampling, Yu et al [28] selected the most ambiguous document pairs, in which two documents received close scores predicted by the current model, as in-formative examples. Donmez et al. [8 ] chose those document pairs, which if labelled could change the current model pa-rameters significantly. Silva et al [23 ] proposed a novel doc-ument level active sampling algorithm based on association rules, which does not rely on any initial training seed. Query Selection for Learning to Rank: For query level active learning, Yilmaz et al. [27 ] empir-ically showed that having more queries but shallow docu-ments performed better than having less queries but deep documents. They balance number of queries with depth of documents judged using random query selection. Cai et al. [5] propose the use of Query-By-Committee (QBC) based method to select queries for ranking adaptation but omit the evaluation of the query selection part and focussed on the ranking adaptation results instead. Long et al. [17] intro-duced an expected loss optimization (ELO) framework for ranking, where the selection of query and documents were integrated in a principled 2 staged active learning frame-work and most informative queries selected by optimizing the expected DCG loss but the proposed approach is lim-ited to rankers that predict absolute graded relevance and hence not generalizable to all rankers. Authors in [2 ] adapt ELO to work with any ranker by introducing a calibration phase where a classification model is trained over in the val-idation data. Moreover, they show that estimating expected loss in DCG is more robust than NDCG even when the final performance measure is NDCG.

Thus, QBC attempts to capture the informativeness as-pect of queries by selecting queries which minimize the dis-agreement among a committee of rankers while the Expected loss optimization based approach formulates informativeness in terms of expected DCG loss; both these approaches fail to capture the representativeness aspect of queries which we show outperforms both these approaches.
 Submodular Maximization: Submodularity is a property of set functions with deep the-oretical and practical consequences. Submodular maximiza-tion generalizes to many well-known problems, e.g., maxi-mum weighted matching, max coverage, and finds numerous applications in machine learning and social networks. In In-formation Retrieval, submodular objectives have been ma-jorly employed for diversified retrieval[29 ] &amp; learning from implicit feedback[ 21 ]. A seminal result of Nemhauser et al. [19] states that a simple greedy algorithm, based on a sub-modular objective, produces solutions competitive with the optimal (intractable) solution. In fact, if assuming nothing but submodularity, no efficient algorithm produces better solutions in general [10 ].
Our aim is to actively select the optimal subset of unla-belled queries for obtaining relevance judgements so as to re-duce data annotation costs. Intuitively, the selected queries should have two major properties: informativeness &amp; repre-sentativeness. We describe both these properties below and provide intuitions motivating each.
Informativeness measures the ability of an instance in re-ducing the uncertainty of a statistical model[ 22 ]. Ideally, the selected queries should be maximally informative to the ranking model. In learning to rank setting, Informativeness based query selection focusses on greedily selecting queries which are most informative to the current version of the ranking model.

Different notions of informativeness can be encapsulated by different techniques depending on how query-level infor-mativeness is quantified. Two possible measures of captur-ing a query X  X  informativeness include: (i) Uncertainty based informativeness &amp; (ii) Disagreement based informativeness.
Uncertainty based informativeness quantifies the query-level information as the uncertainty associated with the opti-mal document ranking order for that query. Query selection strategies focusing on uncertainty reduction would greedily select the query instance about which the current ranking model is most uncertain about, thereby trying to reduce the overall uncertainty associated with the ranking model.
Disagreement based informativeness, on the other hand, quantifies the query-level informativeness as the disagree-ment in this query X  X  document rankings among a committee of ranking models. The key idea here is that the maxi-mally informative query is one about whose document rank-ings, the committee of ranking models maximally disagree; hence obtaining relevance labels for such a query would pro-vide the maximum information. Among the existing ap-proaches for query selection for ranking models, the Query-by-Committee [ 5] attempts to capture the Informativeness aspect of queries based on a disagreement measure.
Representativeness measures if an instance well represents the overall input patterns of unlabelled data [22 ]. Web search queries can span a multitude of topics and informa-tion needs, with even a small dataset containing a broad set of queries ranging from simple navigational queries to very specific domain-dependent queries. In learning to rank set-tings, this implies that selected queries should have strong correlation with the remaining queries, as without this cor-relation there is no generalizability and predictive capability. Different notions of representativeness can be defined cover-ing different characteristics of individual queries. Improving the representativeness of the selected query subset improves the coverage aspect of the query collection -the more repre-sentative selected queries are, the more they cover the entire query collection.
Selecting queries solely based on their informativeness as-pects could possibly lead to selection of noisy queries. In line with the Meta-Search Hypothesis [14 ][15 ], rankers tend to agree on relevant documents and disagree about nonrele-vant docs. Hence, the queries that a ranker is unsure about or there is big disagreements across rankers are likely to be the ones that contain a lot of nonrelevant documents. Such noisy, outlier queries which majorly have non-relevant docu-ments would lead to maximal disagreement and uncertainty among ranking models, and thus would be wrongly labelled maximally informative. Also, the set of informative queries might not necessarily represent the set of all possible queries, which lead to less coverage of the unlabelled query set.
On the other hand, selecting queries based on representa-tiveness aspects could lead to the selection of a query that is very similar to the a query already in the labelled set and hence, does not provide much information to the ranking model. Despite being representative, such queries possibly offer redundant information to the ranking models. Ideally, a query selection algorithm should take into account both these aspects while selecting queries. Existing work has ma-jorly looked into selecting queries by considering informa-tiveness based on disagreement among rankers (Query-by-Committee) or informativeness in terms of expected DCG loss (Expected loss optimization). Both these approaches fail to capture the representativeness aspect of queries. In addition to a novel informativeness approach based on un-certainty reduction, we present a representativeness based approach and finally couple both these aspects for query selection via a joint submodular objective which jointly in-corporates informativeness &amp; representativeness.
As our first contribution, we present a novel informative-ness based query selection scheme (  X  4) based on permuta-tion probabilities of document rankings which tries to reduce uncertainty among rankers. While no existing query selec-tion scheme for learning to rank incorporates the represen-tativeness aspect of queries, we propose a LDA topic model based query selection scheme (  X  5) which captures the rep-resentative aspect of queries while constructing the query subset. An ideal query subset would have both informa-tive &amp; representative queries. As our third contribution, we combine the two paradigms of representativeness &amp; informa-tiveness by proposing a coupled model based on submodular functions(  X  6).
Our first novel query selection scheme is aimed at cap-turing the informative-aspect of queries. We maintain a committee of ranking models C = {  X  1 , X  2 ,..., X  C } which are trained on a randomly selected subset from the current la-belled set, and thus contain different aspects of the training data depending on the queries in their subset. It is to be noted that these ranking models could be generated using any learning to rank algorithm. Given the set of currently labelled query instances, our goal is to pick the next query ( q  X  ) from the set of unlabelled queries by selecting the max-imally informative query instance. The query-level infor-mativeness is defined in terms of the uncertainty associated with the optimal document ranking orders among the | C | ranking models. We follow a similar approach as outlined by Cai et al. [ 5] to maintain a committee of rankers. However, unlike Query-By-Committee [5 ] which encapsulates infor-mativeness via ranker disagreements, our approach presents an alternate view of informativeness based on uncertainty reduction wherein a ranking model X  X  uncertainty for the query X  X  document ranking order is defined based on the con-cept of permutation probabilities.

More specifically, each committee ranking model is al-lowed to score the documents associated with each query fol-lowing which a permutation probability is calculated on the ranking obtained on sorting these document scores. Thus, each query gets a permutation probability score by each committee member. The most informative query is consid-ered to be the query instance which minimizes the maximum permutation probability of document scores given by each ranking model committee member.

We postulate that a query which has the minimum per-mutation probability score from among the maximum scores assigned between the different ranking models is maximally informative in the sense that even the best ranker among the committee is highly uncertain about its document rankings and hence this query obtained the least permutation prob-ability score among the set of unlabelled candidate queries. We select a query for which the probability with respect to the most certain (maximum permutation probability) model is minimal, i.e., a query for which even the most certain com-mittee member has minimum confidence.
 To define permutation probabilities, we make use of the Plackett-Luce model [20 ]. The Plackett-Luce (P-L) model is a distribution over rankings of items (documents) which is described in terms of the associated ordering of these items (documents). We define P (  X  |  X  ) as the probability of obtain-ing the ranking order (  X  ) based on the score (  X  k ) assigned to each document ( k ) by the ranking model learnt thus far. For each query, we rank the documents based on the scores assigned by model learnt so far and calculate the probabil-ity of the ranking order obtained (  X  ) using the permutation probability defined as follows: where each ranking  X  has an associated ordering of docu-ment scores  X  = (  X  1 ,  X  X  X  , X  K ) and an ordering is defined as a permutation the K document indices with  X   X  i repre-senting the score assigned to document i (at rank  X  the ranking model. We make use of a committee of ranking models and select the maximally informative query based on a greedy min-max algorithm described next.
Building a Min-Max PL Probability based selection sys-tem involves two components: (i) building a committee of ranking models that are well diversified and compatible with the currently labelled data and (ii) computing permutation probabilities by each committee member for each query in the unlabelled set of queries &amp; selecting maximally informa-tive query as per the min-max score.
 Committee Construction: Following the work of [5], we use query-by-bagging approach to construct the members. Given the set of currently la-belled instances, bagging generates C partitions of sub sam-ples by sampling uniformly with replacement, and then the committee can be constructed by training each of its mem-bers on one portion of the sub-sample partitions. We ran-domly initialize the initial set of labelled queries with a small base set of queries and their labelled documents. We sample with replacement for C times in the set of labelled queries and train a ranking model on each subset of queries. Such a sampling procedure allows us to create various dif-ferent training datasets that each represent a subset of the data possibly having very different characteristics than each other. These C models represents our C committee mem-bers. We set the size of each subset to be 50 % of the current labelled subset size at each step. The maximally informative query q  X  is selected for annotation which obtains the lowest min-max score, the calculation of which is described below. Calculating min-max score: For each query q in the candidate set of unlabelled queries, the C committee members return C ranked lists. Following the construction of | C | ranking models, for each ranking model per query, we sort the documents based on the scores given by the ranking model and compute the permutation probability of obtaining this ranking order.
 Thus, each query has | C | permutation probability scores. In order to minimize the overall uncertainty associated with the ranking models, we select the maximally informative query q  X  , i.e., the query that has the minimum value of the permutation probability assigned by its most certain com-mittee member, i.e., the committee member that has the highest permutation probability score associated with the query X  X  document ranking order. Thus, where each ranking  X  c q has an associated ordering  X  = (  X  1 ,  X  X  X  , X  c K ) and an ordering is defined as a permutation the K document indices with  X  c  X  k representing the score assigned to document k by the ranking model c .
A major drawback associated with pure-Informativeness based models is that often they tend to select outlier queries. As is confirmed by the Meta-Search Hypothesis [14][15 ], rankers tend to agree on relevant documents but disagree on non-relevant documents. In such a scenario, an outlier query which majorly has non-relevant documents would lead to maximal disagreement and uncertainty in the ranking model, and thus will be wrongly labelled maximally infor-mative. This motivates the need for considering the repre-sentativeness aspect of queries.

The information-seeking behaviour of users tend to vary based on the search task at hand [ 25 ] which suggests that the importance of feature weights for queries belonging to different tasks or topics are likely to be very different. The relative importance of different features are likely to be very different for different tasks. For example, queries belonging to a topic such as news would warrant high authority web-sites to be ranked higher (i.e., larger weight on the pagerank score) while queries belonging to (say) educational informa-tional content would prefer the documents better matched with their query terms be ranked higher (i.e., larger weight on the relevance features such as BM25). To capture these diverse variations in the feature weights, the training set should ideally be composed of representative queries from different tasks. This makes it necessary that the labelled set of queries have representative queries spanning the entire array of different topics. We propose a Latent Dirichlet Al-location (LDA) [3 ] topic model based query selection scheme which tries to capture this insight by selecting representa-tive queries which are most topically similar to the set of unlabelled queries.

Based on this intuition, we conjecture that representative queries would be those that are most similar to the set of unlabelled queries in terms of their topical distribution. To capture the heterogeneity among all queries in the search logs, we make use of the concept of latent topics. We learn these latent topics from the collection of queries and repre-sent each query as a probability distribution over these latent topics. We train an LDA model, a generative model which posits that each document (query in our case) is a mixture of a small number of topics and that each word X  X  (query term X  X ) creation is attributable to one of the document X  X  (query X  X ) topics. Each query is represented as a feature vector corre-sponding to its distribution over the LDA topics. To find representative queries, we select the query with the maxi-mum average similarity from among the unlabelled set of queries, i.e., where | D u | represents the number of queries in the unla-belled set D u ; T q represents the query q  X  X  feature vector in the LDA topic space and sim ( T q ,T q i ) can be any similarity score between queries; we use the cosine similarity between the topic-space representations of queries q and q i .
The approaches discussed so far have looked at either the informativeness of queries and selected queries which are most informative in terms of their ability reduce the uncer-tainty of the ranking model or they have focussed on repre-sentativeness of queries and selected representative queries spanning the entire array of different topics. As we dis-cussed earlier in subsection 3.3, optimizing for only one of the two criteria for query selection could significantly limit the performance of query selection by selecting suboptimal query subsets. In this section we present a way of combin-ing the two objectives by means of submodular functions and propose a submodular objective which jointly captures the notions of representativeness and informativeness.
Submodular functions are discrete functions that model laws of diminishing returns and can be defined as follows:[ 19]: Given a finite set of objects (samples) Q = { q 1 ,...,q n a function f : 2 S  X  &lt; + that returns a real value for any subset S  X  Q , f is submodular if given S  X  S 0 , and q /  X  S That is, the incremental  X  X alue X  of q decreases when the set in which q is considered grows from S to S 0 . A function is monotone submodular if  X  S  X  S 0 , f ( S )  X  f ( S 0 ). Powerful guarantees exist for such subtypes of monotone submodular function maximization. Though NP-hard, the problem of maximizing a monotone submodular function subject to a cardinality constraint can be approximately solved by a sim-ple greedy algorithm [ 19] with a worst-case approximation factor (1  X  e  X  1 ). This is also the best solution obtainable in polynomial time unless P=NP [10 ].
Submodularity is a natural model for query subset selec-tion in Learning to Rank setting. Indeed, an important char-acteristic of any query-subset selection technique would be to decrease the value-addition of a query q  X  Q based on how much of that query has in common with the subset of queries already selected ( S ). The value f ( q | S ) of a query in the context of previously selected subset of queries S further diminishes as the subset grows S 0  X  S . In our setting, each q  X  Q is a distinct query, Q corresponds to the entire col-lection of queries and S corresponds to the subset of queries already selected from Q .

Mathematically, the query subset selection problem can be formulated as selecting the subset of queries S which maximizes the value of f ( S ) where f ( S ) captures both the representativeness aspect as well as the informativeness as-pects of queries. We next describe in detail the construction of such a monotone submodular function and later present a greedy algorithm to approximately solve the problem of query subset selection.
We model the quality of the query subset in terms of both the representativeness &amp; informativeness. To capture both these traits, we model the quality of the query subset as: where  X ( S ) captures the representativeness aspect of the query subset ( S ) with respect to the entire query set Q while  X ( S ) rewards selecting informative queries. The parameter  X  controls the trade-off between the importance of represen-tativeness &amp; informativeness while selecting queries. A sin-gle weighting scheme would not be suitable for all problems since depending on the constituent queries, size of the overall dataset and the size of the subset that needs to be selected, different weighting schemes would produce different results. The function F ( S ) will be monotone submodular if each of  X ( S ) and  X ( S ) are individually monotone submodular. We defer an in-depth analysis of the trade-off between represen-tativeness &amp; informativeness aspects to subsection 8.1 and next describe the details of both these functions. 6.3.1 Representativeness:  X ( S )
 X ( S ) can be interpreted either as a set function that mea-sures the similarity of query subset S to the overall query set Q , or as a function representing some form of  X  X epresenta-tion X  of Q by S . Most naturally,  X ( S ) should be monotone, as representativeness improves with a larger subset.  X ( S ) should also be submodular: consider adding a new query to two query subsets, one a subset of the other. Intuitively, the increment when adding a new query to the small subset should be larger than the increment when adding it to the larger subset, as the information carried by the new query might have already been covered by those queries that are in the larger subset but not in the smaller subset. Indeed, this is the property of diminishing returns.

We employ the same functional form of  X ( S ) as was adopted by Lin et al. [16]. Specifically, a saturated coverage function is defined as follows: where C q ( S ) is a set based function defined as C q ( S ) : 2 &lt; and 0  X   X   X  1 is a threshold co-efficient. Intuitively, C ( S ) measures how topically similar S is to query q or how much of the query q is covered by the subset S . Building on top of the earlier proposed LDA topic model based query selection, we define the coverage function C q ( S ) in terms of the topical coverage of queries. More specifically, where w q,q 0  X  0 measures the topical similarity between queries q and q 0 . Since C q ( S ) measures how topically similar S is to query q , summing C q ( S )  X  q  X  Q would measure how similar the current subset S is to the overall set of queries Q . It is important to note that C q ( Q ) is just the largest value C ( S ) can ever obtain because Q is the set of all the queries we have and it maximally represents all the information we have. We call a query q saturated by the subset of queries S when min { C q ( S ) , X C q ( Q ) } =  X C q ( Q ). When q is saturated in this way, any new query cannot further improve the cover-age even if it is very similar to the query q . Thus, this gives other queries which are not yet saturated a higher chance of being better covered and hence the resulting subset tends to better cover the entire set of queries Q . 6.3.2 Informativeness:  X ( S )
The  X ( S ) function described above intuitively captures the notion of coverage or representativeness by selecting sub-set of queries S which are topically most representative of the entire set of queries Q . While representativeness is an important trait, we also wish to capture the informativeness aspect of queries and select queries which are most infor-mative to the current version of the ranking model. We formulate the functional form of  X ( S ) based on top of the earlier proposed ways of encapsulating query-level informa-tiveness in terms of either ranker disagreements or model uncertainity, or both. As a precursor, it is worth mention-ing that to define the function  X ( S ) we make use of LDA topic model which gives us k-topics and we associate each query to one of these k-topics. Formally, we define the  X ( S ) function as follows: where P i ,i = 1 ,...,K is the topical-partition of the set of queries Q into K-topics and  X  q captures the informativeness carried by the query q based on the current ranking model. The function  X ( S ) rewards topical-diversity along with valu-ing informativeness since there is usually more benefit to se-lecting a query from a topic not yet having one of its query already chosen. As soon as a query is selected from a topic, other queries from the same topic start having diminishing gain owing to the square root function ( It is easy to show that  X ( S ) is submodular by the compo-sition rule. The square root is non-decreasing concave func-tion. Inside each square root lies a modular function with non-negative weights (and thus is monotone). Applying the square root to such a monotone submodular function yields a submodular function, and summing them all together re-tains submodularity.

The informativeness of a query  X  q can be defined based on the metrics proposed earlier. To incorporate the infor-mativeness aspects of queries, we experiment with various different formulations of the singleton-query rewards ( X  include the following:: Based on empirical analysis, we find that the disagreement based reward functions perform better than the rest of the formulations across all datasets, so we skip the performance comparisons among these.
Having defined the individual functions based on the dif-ferent paradigms, we formulate the overall query subset se-lection problem as the selection of the subset S of queries which maximizes the following function: Modelling the query selection problem in such an objective provides many advantages. Firstly, the submodular formula-tion provides a natural way of coupling the different aspects of query selection. Secondly, the above formulation can be optimized efficiently and scalably given the monotone sub-modular form of the function F ( S ). Assuming we wish to select a subset of N queries from the total unlabelled set of Q queries, the problem reduces to solving the following optimization problem: While solving this problem exactly is NP-complete [ 10 ], tech-niques like ILP [ 18 ] can be used but scaling it to bigger datasets becomes prohibitive. Since the function F ( S ) is submodular, it can be shown that a simple greedy algorithm will have a worst-case guarantee of f ( S  X  )  X  (1  X  1 e ) F ( S 0 . 63 F ( S opt ) where S opt is the optimal and S  X  is the greedy solution [ 10 ]. This constant factor guarantee has practical importance. First, a constant factor guarantee stays the same as N grows, so the relative worst-case quality of the solution is the same for small and for big problem instances. Second, the worst-case result is achieved only by very con-trived and unrealistic function instances -the typical case is almost always much better. The greedy solution works by starting with an empty set and repeatedly augmenting the set as until we select the N number of queries in the subset we intended.

Overall, we select query subsets based on the aforemen-tioned formulations; we next describe in detail the experi-mental evaluation performed to compare the performances of the three proposed approaches against state-of-the-art baselines.
We evaluate the proposed query selection strategies on web search ranking and show that the proposed techniques can result in good performance with much fewer labelled queries. We next describe our experimental settings along with the baselines, dataset and evaluation metrics used.
We compare the performance of the proposed query selec-tion strategies against existing state-of-the-art approaches. The compared approaches include:
We use three commonlyused real-world learning to rank datasets: (i) MQ2007; (ii) MQ2008 from LETOR 4.0 which uses query sets from Million Query track of TREC 2007, TREC 2008 and (iii) the OHSUMED test collection, a sub-set of the MEDLINE database, which is a bibliographic database of important, peer-reviewed medical literature main-tained by the National Library of Medicine. It is worth mentioning that the proposed approaches make use of query term information which is not available in many other rank-ing datasets, hence we restrict our evaluation to these three datasets having query term information. There are  X  1700 queries in MQ2007,  X  800 queries in MQ2008 and  X  100 queries in the OHSUMED dataset. The MQ2007 &amp; MQ2008 datasets are of notable size and query selection indeed makes sense in the such datasets; the OHSUMED dataset, on the other hand, has too few queries to select from which isn X  X  ideal for a query selection scenario. Nevertheless, we compare performances across all datasets.

We adopt a 5-fold cross validation scheme with each fold divided into three parts, one each for training, validation and testing in the ratio 3:1:1. Each query-document pair is represented using 46 features [45 in case of the OHSUMED dataset) along with the relevance score from among { 0,1,2 } . The test set is used to evaluate the different query selection strategies while active learning is performed on queries from the training set.
We start with a base set of 40 labelled queries randomly sampled from the entire query set; the rest of the queries form the candidate set. We make use of C = 4 committee members (where applicable) each of which is constructed based on the procedure described earlier ( subsection 4.1 ). To learn the initial ranking models for each of the commit-tee members, we randomly select a sample of 20 queries from the base set of 40 queries and build a ranking model based % Queries SF LDA PL ELO QBC RDM SF LDA PL ELO QBC RDM on these queries as training data. We first focus on Lamb-daMART [11 ], (a state-of-the-art learning to rank algorithm that was the winner of the Yahoo! Learning to Rank chal-lenge [4 ]) to build ranking models used in the initial part of our experiments. We later show ( subsection 8.2 ) that the queries selected by this method could also be used by other Learning to Rank algorithms.

The entire experiment is repeated multiple times over the 5 folds on each dataset. We perform batch mode Active Learning for queries by selecting a batch of top 10 queries from the candidate set of queries based on the query se-lection criterion at each round and iteratively add them to our base set. Queries having no relevant documents were ignored while calculating the different metrics. Based on empirical estimation, the threshold parameter in equation 6 was initialized as  X  = 0 . 8. For our initial results, we evaluate the performance of the proposed query selection strategies based on their NDCG@10 values in the test set. We later analyse the generalizability of our approach on a different metric (MAP).
We compare the NDCG@10 performance of the test set against the number of queries in training set (base queries plus actively selected) in Figure 1 for the different datasets and compare the performance of the proposed query selec-tion schemes against the QBC, ELO and Random baselines (statistically significant results are highlighted in the respec-tive tables). For all the methods, the NDCG@10 values tends to increase with the number of iterations which is in line with the intuition that the quality of the ranking model is positively correlated with the number of examples in the training set.

While min-max PL based query selection stems from the same class of approaches (informativeness based) like the two baselines ELO &amp; QBC, it performs better than both these baselines in most cases; this is in line with our ini-tial claim of capturing informative queries from an alternate view of informativeness based on uncertainty reduction. We observe that LDA Topic Model based query selection per-forms better than existing baselines as well as the PL model which suggests that the quality of the queries selected by this scheme is better than those selected by other strate-gies which are mostly based on the informativeness aspect. Perhaps selecting queries based on the informativeness re-sults in some noisy outlier queries getting selected, a case which LDA topic model based query selection avoids by se-lecting representative queries. The minor fluctuations and occasional dip in the NDCG values on adding more queries Figure 2: Tradeoff analysis between Informativeness &amp; Rep-resentativeness for the MQ2007 datasets. The  X  coefficient in equation 5 controls the relative importance of the two aspects. to the labelled set could be explained by the fact that some queries are indeed noisy and selecting such queries induces noise in the ranking models, which results in a slightly worse ranker performance.

Finally, we observe from the results that the submod-ular objective (SF) outperforms the baselines as well as (in most cases) our own proposed purely informativeness &amp; purely representativeness based query selection schemes across the different datasets. While purely informativeness based methods tend to select noisy queries, purely repre-sentativeness based methods might possibly select queries which are representative but add redundant information. Hence, selecting queries based on the coupled aspects selects queries which are not only representative of other unselected queries, but are also informative to the ranking model.
Our main motivation behind introducing the submodular objective was to couple the notions of informativeness and representativeness in a joint coherent manner. Indeed, an ideal subset of queries would be a fine blend of queries which convey the maximal amount of information to the ranking model while at the same time, be characteristic of the unse-lected set of queries. In Figure 2, we present a example anal-ysis on one of the datasets of the relative importance of the two aspects and how they contribute to the overall ranking performance. As can be seen in the figure, a relative weight-% Queries SF LDA PL ELO QBC RDM SF LDA PL ELO QBC RDM ing scheme of  X  = 0 . 3 (which weighs representativeness-vs-informativeness in 3:7 proportions) works best for query selection. This highlights that while representativeness is important, selecting informative queries from the different topics indeed helps. Also, it must be noted that the in-formativeness term in Equation 9 not only contains contri-butions from query X  X  singleton informativeness reward, but also has contributions from the topical segregation of queries into partitions. Overall, we chose the coefficient  X  = 0 . 3 to weigh the contributions from the two aspects while reporting results. It is to be noted that domain knowledge about the dataset in consideration can be used to vary  X  accordingly, depending on the desired proportion of representativeness &amp; informativeness.

For a milder sized dataset (MQ2008), putting more weight on informativeness helps initially while the relative contri-butions tend toequal out once a certain threshold of queries have been selected. Overall, the general weighting factor or  X  = 0 . 3 works well consistently across different datasets.
For initial results shown before, the query selection method uses LambdaMART as the learning to rank algorithms opti-mized for the NDCG metric. Since the labelled learning to rank dataset generated as a result of the query selection pro-cess could potentially be used in any future ranking systems, the selected queries should ideally be usable by any learn-ing to rank algorithm, optimized for any metric. We analyze such generalization performance in these sets of experiments. While the initial set of results presented above were NDCG values based on LambdaMART ranking algorithm optimiz-ing for NDCG metric, we divert from our original setting and present results on a different ranker: AdaRank [ 26 ] in table 1. Similar results for the OHSUMED dataset can be seen in Fig 3. Additionally, we demonstrate the performance of the proposed query selection strategies on a different met-ric (MAP) and report results in Table 2. Overall, we can see that the proposed query selection methodologies con-sistently perform better than the baselines across different ranking algorithms and metrics.
We next analyse the reduction in labelling cost achieved as compared to the case where the entire set of unlabelled queries were labelled. The performance of the ranking func-tion trained with the whole labelled data set is referred to as the optimal performance. When the performance of the ac-tive learning model obtained with the proposed algorithms Table 3: The performance in terms of the Labelling Cost Reduction (LCR) and the Saturated Size (SS) for the various compared approaches.
 is comparable to the optimal performance, we call the size of training data as the saturated size (SS). Table 3 highlights the approximate labelling cost reduction (LCR) results ob-tained via the proposed query selection techniques. The %-ages were calculated based on the average number of queries in the training set. The corresponding values were calcu-lated using the LambdaMART implementation with NDCG metric. Experimental evaluation shows the proposed query selection algorithms indeed require less number of queries to be labelled than baseline methods to achieve compara-ble ranking performance. It is worth mentioning that at some point, adding more queries to the labelled training set doesn X  X  help improve ranking performance, as can be seen by the results of the RDM algorithm in the table: with about 720 labelled queries out of 1015 queries, the algorithm is able to demonstrate comparable ranking performance.
We formulated approaches to the query selection prob-lem into two classes: informativeness based and represen-tativeness based strategies and proposed two novel query selection strategies, one from each class respectively: per-mutation probability based and LDA Topic Model based query selection. Additionally, we argued that an ideal query selection scheme should incorporate insights from both the aspects and presented a principled way of coupling informa-tion from the two aspects. Based on rigorous experiments we demonstrated the efficacy of the proposed query selec-tion schemes. A possible line of future work could look at enriching the representativeness aspect by adding document level information to the topic model. [1] J. A. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and [2] M. Bilgic and P. N. Bennett. Active query selection [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] C. J. Burges, K. M. Svore, P. N. Bennett, [5] P. Cai, W. Gao, A. Zhou, and K.-F. Wong. Relevant [6] O. Chapelle and Y. Chang. Yahoo! learning to rank [7] O. Chapelle, Y. Chang, and T.-Y. Liu. Future [8] P. Donmez and J. G. Carbonell. Optimizing estimated [9] P. Donmez, J. G. Carbonell, and P. N. Bennett. Dual [10] U. Feige. A threshold of ln n for approximating set [11] Y. Ganjisaffar, R. Caruana, and C. V. Lopes. Bagging [12] Y. Hamo and S. Markovitch. The compset algorithm [13] M. Hosseini, I. J. Cox, N. Milic-Frayling, M. Shokouhi, [14] J. H. Lee. Combining multiple evidence from different [15] J. H. Lee. Analyses of multiple evidence combination. [16] H. Lin and J. Bilmes. Multi-document summarization [17] B. Long, O. Chapelle, Y. Zhang, Y. Chang, Z. Zheng, [18] G. L. Nemhauser and L. A. Wolsey. Integer and [19] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [20] R. L. Plackett. The analysis of permutations. Applied [21] K. Raman, P. Shivaswamy, and T. Joachims. Online [22] B. Settles. Active learning literature survey. University [23] R. Silva, M. A. Gon  X calves, and A. Veloso. Rule-based [24] M. Taylor, J. Guiver, S. Robertson, and T. Minka. [25] R. W. White and D. Kelly. A study on the effects of [26] J. Xu and H. Li. Adarank: a boosting algorithm for [27] E. Yilmaz and S. Robertson. Deep versus shallow [28] H. Yu. Svm selective sampling for ranking with [29] Y. Yue and C. Guestrin. Linear submodular bandits
