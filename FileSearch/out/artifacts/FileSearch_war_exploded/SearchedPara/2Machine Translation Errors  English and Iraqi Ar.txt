 S. CONDON, D. PARVAZ, J. ABERDEEN, C. DORAN, A. FREEMAN, and M. AWAD, Methods for evaluating machine translation (MT) output have proliferated rapidly: 39 automated measures were submitted to the NIST 2008 Metrics for Machine Trans-lation Challenge [Przybocki et al. 2008]. A limitation of most measures of transla-tion quality, including human judgments, is that they are not diagnostic, and there have been few attempts to analyze MT errors so that researchers can identify the types of errors that are made. Analyses of the errors produced by machine transla-tion (MT) systems have the potential to focus research aimed at improving translation performance, but translation error annotation is problematic because there are many ways to translate a single expression from one language to another.
 Presented with the opportunity to analyze translation errors from four English-Iraqi Arabic speech translation systems, a team of linguists at The MITRE Corporation developed a methodology that incorporates techniques from the Trans-lation Error Rate (TER) family of measures [Snover et al. 2006] in order to facilitate identification and annotation of errors. In the methods used to compute the Human Translation Error Rate (HTER) score, human editors create a reference translation of a source input that is as close as possible to the system X  X  machine translation of that input, while preserving the content of the source. We adopted this process so that the differences between system hypotheses and reference translations are more likely to reflect problems in the MT rather than variation among possible translations.
Previous research on MT errors has not addressed the problem that identifying errors depends on what is considered correct. In one comprehensive study, a hierarchi-cal taxonomy of errors created for use in refining rules of transfer-based MT systems [Llitj  X  os et al. 2005] was extended to annotate errors in translations from Spanish to English, English to Spanish, and Chinese to English [Vilar et al. 2006]. Three types of inputs from audio data were compared: clean text transcripts, verbatim transcripts, and the output of automated speech recognition (ASR).

The error annotation was performed by human annotators using reference trans-lations and a tool that highlights the differences between the MT and a reference translation. Aside from explaining that the use of reference translations  X  X ust be done with care X  [Vilar et al. 2006, p. 697], there is no description of the annotation process, but the results provide a detailed catalogue of the types of errors produced by the Rheinisch-Westf  X  alische Technische Hochschule Aachen (RWTH) statistical MT system [Matusov et al. 2006].

By combining part-of-speech tagging with Word Error Rate (WER) and Position In-dependent Word Error Rate (PER) measures [Tillmann et al. 1997], automated error analyses of the same RWTH translations were produced in another study [Popovi  X  c and Ney 2007]. The results of the automated analyses are comparable to the results of the human analyses and provide error frequencies for a variety of syntactic and morphosyntactic classes without the time and expense required by human annota-tors. Popovi and Ney do not specify whether they used the clean text transcripts or the verbatim ones, but Vilar et al. note that the latter include some ungrammati-cal constructions which present an additional source of errors. It remains to be seen whether conventional parsers and morphological analyzers can provide accurate au-tomated analyses of highly disfluent speech data such as the verbatim transcripts we analyzed.

Section 2 describes the methods we developed to analyze errors in MT output and the data we analyzed. Section 3 discusses the experience of leveraging HTER in the an-notation process and presents the frequencies of annotated errors. Section 4 presents the proportions of error types that were annotated for translations from Arabic to Eng-lish, and Section 5 does the same for translations from English to Arabic. Section 6 summarizes the proportions of errors for both languages and compares them. Our approach to error annotation has been to adopt the methods that are used to com-pute the HTER score for the Defense Advanced Research Projects Agency X  X  (DARPA) Global Autonomous Language Exploitation (GALE) program. The guidelines for post-editing were similar to the GALE guidelines [NIST 2007]. For HTER, The National Institute of Standards for Technology (NIST) developed a post-editing tool to facili-tate this process [NIST 2008]. The human editor modifies the MT output so that it preserves the meaning of the source input, making as few changes in the MT output as possible. The NIST tool allows the editor to view the TER score for the machine translation compared to the editor X  X  reference translation so that the editor can ex-periment with alternative wordings and see which is scored closest to the machine translation. In addition, the tool displays reference translations produced for each source transcription by independent human translators. For the corpus we analyzed, human post-editors were able to view four reference translations of each source input. Because the NIST tool was created only for translations to English, we developed a similar tool that can handle translations to Arabic, too.

After producing the customized reference translations, annotators used the TER scorer output to classify each error. The TER scorer computes the number of deletions, insertions, substitutions, and shifts of words that are required to modify the machine translation so that it matches the reference translation. The tool that computes TER aligns the machine translation and the reference translation and annotates each error by associating an abbreviation I , D , or S with the word that is inserted, deleted, or substituted respectively. For shifts, the  X  X  X  symbol appears in the position that the shifted word has moved from.

Figure 1 illustrates the output that annotators see after the TER scorer is used to align the post-edited reference translation (second column) with the MT output (third column) and to annotate the differences between the two (fourth column). In Figure 1, is is annotated as an insertion (I), and the fact that the MT output does not include was is annotated as a deletion (D). Annotators realign the TER alignment in cases such as Figure 1 when the TER alignment did not capture the relations between the MT and the reference in a satisfactory way. Usually these changes were made to align words in the same syntactic categories. In Figure 1, TER failed to align is and was because the forms do not match, even though they are different tenses of the same passive auxiliary. The human annotator re-annotated the TER annotations as if is had been moved to align with was : the @ annotation that represents a shift is entered adjacent to is and the S annotation representing the substitution of was by is is entered adjacent to was .

The errors identified by the TER notations were used to classify the errors into the following common grammatical classes: pronoun, noun, verb, and other (prepositions, adjectives, conjunctions, and adverbs). The result is an annotation that combines the TER classifications with the word class annotations. For example, ivp was used to annotate an inserted verb or predicate element such as a modal or auxiliary.
Two additional syntactic categories were adopted in an attempt to limit the annotations associated with each error to a single category. One category, labeled Pronoun-Verb (or Pro-Verb ) in this article, was used to annotate errors involving a single element consisting of a pronoun and verb. In translations to English, these were insertions or substitutions of contractions such as I m . The second category, la-beled Subject Person , was used exclusively to annotate Arabic verbs when the subject person inflection did not agree with the subject. Table I lists the major annotation categories with brief descriptions and extensions.

Errors that changed the polarity or speech act of an input were noted, as were words that were wrongly transliterated (or replaced by question marks). The latter were annotated as Untranslated . If one of these errors was annotated, other errors were not. Thus, in Figure 1, the annotation for a speech act error takes priority over the word order annotation.

Not all differences between machine and human translation noted by TER were annotated as errors. Minor inflectional differences such as number, gender and even tense were not annotated as errors if the annotator judged that the speaker X  X  intent could be inferred adequately from the translation and its context. In this respect, the annotations resemble the METEOR measure [Banerjee and Lavie 2005], which em-ploys stemming before matching reference and machine translations. Similarly, other differences were annotated as null if the annotator judged that these errors would be ignored by interlocutors who were tolerant of machine translation and were able to make sense of the machine outputs in the dialogue context.

Other differences annotated as null were deleted or inserted articles (the, a), deleted and (if the absence did not affect the meaning), repetitions, disfluencies, fillers, and synonyms. Subject inflection on Arabic verbs that did not agree with the subject was usually annotated as null if the error was in number or gender, unless the annotator believed that the result would be ambiguous or confusing for the listener. In Figure 1, the error in tense inflection was annotated as null because the error did not seem significant.

A set of annotation guidelines was compiled so that annotators could complete the annotation process as consistently as possible. Three native English speakers annotated the translations from Arabic to English, and three Arabic speakers anno-tated the translations from English to Arabic. For the latter, two non-native Arabic linguists each annotated half of the data, and a third native Arabic speaker reviewed the annotations. Differences were reconciled by the relevant annotator pairs.
The translations selected for the study were from an evaluation of four speech trans-lation systems that were developed for the DARPA Spoken Language Communication and Translation System for Tactical Use (TRANSTAC) program [Weiss et al. 2008]. All of the systems were trained on data collected for the TRANSTAC program using prompts and scenarios for role-playing that elicited language in military domains such as checkpoints, infrastructure surveys, military training, civil affairs, and joint mili-tary operations. Audio recordings of responses to the prompts and dialogues elicited by the scenarios were transcribed and translated [Condon et al. 2008, Weiss et al. 2008].
The four TRANSTAC speech translation systems all employed phrase-based statis-tical MT engines. However, one system X  X  English-to-Arabic translation was rule-based with a statistical fall-back, while another system included processes to associate Eng-lish inputs with canonical inputs that were pre-translated. Another system combined phrase-based with multiple graph-and formal syntax-based statistical MT, using a confusion network and voting algorithm. All the systems were installed on laptop devices with 2.5 to 3.5 GB of RAM available.

In addition to live evaluations, TRANSTAC evaluations include evaluations based on recorded dialogues, which allows all systems to be tested on identical inputs [Condon et al. 2008]. For the latter, systems are also tested using transcriptions of the audio inputs in order to evaluate the translation capability without speech recog-nition errors. We used outputs from those text translations for the error analyses, and we chose the same subset of translations for which NIST obtained human judgments [Sanders et al. 2008].

Translations were analyzed from an evaluation conducted in June 2008. There were 109 source utterances translated from English to Iraqi Arabic and 96 from Iraqi Arabic to English. The translations from four machine translation systems were processed and annotated so that 384 Iraqi Arabic to English translations and 436 English to Iraqi Arabic translations were annotated. We incorporated TER and HTER into the annotation process order to make it maxi-mally systematic and comparable across annotators. First, human post-editing of ref-erence translations addressed the problem that there is no single correct translation of a single input. By producing reference translations that were maximally similar to system outputs, differences between the system outputs and reference translations were minimized. In this way, we expected to limit annotators X  identification of what was wrong, by limiting identification of what was correct. Second, using TER to align and annotate the differences between each system output and the corresponding post-edited reference translation eliminated human errors and variation from the tedious and time-consuming task of identifying differences between the system output and the reference translation.

Many limitations of the TER alignment were overcome by allowing annotators to realign TER alignments and by incorporating the null annotation. For example, cases in which a single error seems to cause several differences in the aligned text, annota-tors are able to annotate the difference that best reflects the error and annotate the collateral differences as null. Human annotators are also able to use the null annota-tion when differences are due to synonymous paraphrases. However, the freedom to realign TER alignments and annotate errors as null introduced variation among the annotators and made it difficult to achieve high levels of annotator agreement.
Table II provides the total numbers of errors for each system and translation di-rection. The frequency of errors identified in the TER alignments of system outputs and post-edited reference translation is presented as the  X  X ER Errors, X  while the fre-quency of errors that were not annotated as null is presented as  X  X on-null Errors X  (see Section 2 for the description of non-null errors). The average proportion of errors per utterance, computed by dividing the non-null errors by the total utterances is labeled  X  X verage Per Utterance. X  Finally, the average proportion of errors per word,  X  X verage Per Word X  is computed by dividing the non-null errors by the total number of words in the test sets.

Table II shows that only about half of the differences between English-to-Arabic sys-tem outputs and reference translations were annotated as significant (non-null) errors. Higher proportions, ranging from 57% to 67% of the TER errors in Arabic-to-English translations were annotated as null. We speculate that these directional asymmetries may be due to inconsequential orthographic variation in Arabic as well as inflectional differences that annotators judged to be insignificant. There were fewer errors per input in the English to Arabic translations compared to the Arabic to English transla-tions, but the English inputs averaged 1.4 more words per input. The errors per word suggest that there is not a clear trend with respect to the translation direction. Figure 2 presents the proportion of non-null errors produced by each system for each TER error type in translations from Iraqi Arabic to English. Deletions and substi-tutions are more frequent than insertions and shifts, which are labeled word order . (The TER scorer labels errors as deletions if a word that occurs in the reference trans-lation does not align with a word in the MT system translation.) The systems vary more in the proportions of deletions and insertions than they do in the proportions of substitutions and word order errors.

Figure 3 presents the proportion of non-null errors for each word class that was annotated in the translations from Iraqi Arabic to English. The proportions of noun errors are consistently low, while the proportions of pronoun and verb errors are much higher among the four systems. Errors annotated as Pronoun-Verb are primarily inser-tions of contractions. The proportion of errors annotated as  X  X ther X  is relatively high because it includes errors involving all other word classes such as adjectives, adverbs, and prepositions. A significant result is the high proportions of errors involving pronouns compared to other nouns. The pronoun annotation includes many types of pronouns in addition to the familiar personal pronouns such as I, they, and you . Indefinite pronouns (anyone, something), interrogative and relative pronouns (what, who, which), demonstratives (this, those), and quantifiers (more, few, each) are all included when they function as pronouns. However, pronouns are a relatively small, closed class of words in every language, whereas other nouns constitute a large, constantly growing class of words. Dialogue tends to include many pronouns, and there are about the same number of nouns (17%) as pronouns (about 19%) in the English reference translations. Yet the proportions of errors involving pronouns are two or more times higher than the pro-portions of errors involving other nouns for every system.

Although a variety of pronouns are annotated in the pronoun class, a majority of the pronoun errors involve personal pronouns. Many of these errors appear to result from a clash between the linguistic properties of Arabic and those of English. Several differences create problems for pronouns. First, Arabic is a language in which sub-ject pronouns are not usually expressed. Like Spanish and Italian, Arabic has rich inflection on the verb that agrees with the subject in person, number, and gender. Consequently, the inflection on the verb can be adequate to indicate which pronom-inal subject the speaker intends, and in all three languages, the subject pronoun is usually produced only for emphasis. The example in (1) demonstrates a pronoun er-ror in which the machine translation has failed to include a subject pronoun because the Arabic did not.  X  X T X  indicates the system output, and  X  X ef X  indicates the human post-edited translation. (1) MT: was bitten by a scorpion Pronoun deletions ranged from 28% to 62% of the pronoun errors produced by the four machine translation systems. The kind of error illustrated in (1) can be seri-ous if the intended subject is not clear from the context, but at least the listener is aware that there is a potential ambiguity in the reference due to a missing word. In contrast, substituting an incorrect pronoun can be a serious problem that leads to misunderstandings.

Although Iraqi Arabic has rich subject agreement inflection, verb forms do not al-ways distinguish unambiguously between pronominal subjects. For example, in (2), the subject of the verb  X  X nderstand X  can be either  X  X  X  or  X  X ou. X  (Arabic words are transliterated using the Buckwalter system. 1 (2) ftahamit In the case of utterances such as (2), the only way to determine the referent of the verb X  X  subject is to use the context. Human interlocutors are very good at resolving this kind of ambiguity from the discourse context, but the task has proven to be quite challenging for language processing systems. The example in (3) demonstrates a pro-noun substitution in machine translation that is likely to have been caused by the ambiguity illustrated in (3). (3) MT: you see his symptoms
Another difference between English and Arabic can cause incorrect pronouns to oc-cur in machine translations. Like Romance languages, Arabic has the property of gen-der associated with nouns. Every noun is either masculine or feminine, and there are no pronouns like it . Consequently, in order to select the correct English pronoun form, it is necessary to know the referent of the pronoun, which as we observed above, is very difficult to do automatically. Example (4) provides some examples of this problem. (4) MT: are taking care of it god willing and hopefully it will get better a little bit Both translations in (4) occur in hospital scenarios, and the third person pronouns refer to a patient. Like the examples in (4), most pronoun gender errors involve use of the neutral gender instead of the masculine or feminine. We found only the error in (5) to exemplify the reverse. (5) MT: he civilian house consists of three rooms This asymmetry is probably due to the fact that errors such as (5) do not typically prevent comprehension of the speaker X  X  intended meaning so that they are annotated as null.

Of the pronoun errors generated by the machine translation systems, 25% X 47% were pronoun substitutions, but not all of them were substitutions of one pronoun by another. Some were substitutions of pronouns by other word classes, which occurs when TER aligns the machine translation and the reference translation. Most of these might be better classified as deletions.

Inserted pronouns are not as frequent as deleted or substituted pronouns. There are two likely sources of inserted pronouns. First, because it is often necessary for the translation to insert a pronoun subject when the Arabic source does not have one, there is a possibility that systems will mistakenly insert the pronoun subject when the source already has another subject expressed. An example is presented in (6). (6) MT: at the same time those people they store them in this complex Translations such as (6) are typically annotated as null because the presence of the extra pronoun does not affect the meaning that is communicated. Second, Arabic and many other languages often require pronouns where English permits a gap. The most common examples occur as resumptive pronouns in relative clauses, as in (7). (7) MT: it is about three kilometers from the point the checkpoint that he ran away More common than inserted pronouns such as (6), pronoun errors such as (7) also detract from the fluency of the machine translation, though they are often annotated as null because they are unlikely to lead to serious miscommunication. The average frequency of verb errors across the four systems (29%) is about the same as the average frequency of pronoun errors (30%). Verb deletions occurred most fre-quently, ranging from 39% to 51% of the verb errors for the four systems. A signif-icant number of the verb errors involve forms of be , which serves several functions in English. The progressive auxiliary be is used to express progressive aspect, as il-lustrated in (8a), while the passive auxiliary be is used to form the passive voice, as illustrated in (8b). The verb be , called the copula , is used to express identity (8c) and attribution (8d). (8) a. They are eating at the restaurant. The copula is unique because it does not contribute meaning beyond a generic equiva-lence or attribution plus whatever tense or other verbal inflection it might carry. As a consequence, in Arabic and other languages such as Russian, the copula is omitted in the present tense. In (9), there is no Arabic word in the source utterance corresponding to is in the reference translation. (9) MT: no sir all the family in the house The translation in (5) is another example in which the system fails to insert a present tense copula into the English translation of an Arabic source that did not include one.
If the MT system failed to produce the progressive auxiliary, the annotation was usually null because the  X  X ng form of the verb is sufficient to communicate the pro-gressive aspect. In spite of this, the proportion of verb errors involving a form of be is quite high. Figure 4 presents the proportions of verb errors in which a form of be was deleted, inserted, substituted for another verb, or substituted by another verb.
Like pronoun errors, errors involving forms of be cannot be attributed to forms and structures that occur infrequently. Instead, they reflect linguistic differences that are challenging for translation systems. Together, these errors constitute a large propor-tion of the errors that each system produced, as illustrated in Figure 5. The results of the polarity annotation provide an estimate of the frequency of poten-tially catastrophic errors in which a negative is translated as positive or vice versa. The examples in (10) illustrate the types of errors that were annotated as polarity errors.
 (10) MT: I don X  X  have at the moment thirty soldier trained and they have light The frequency of polarity errors that were found in the machine translations is very low. In fact, the errors in (10) are the only clear polarity errors among the four sys-tems X  Iraqi Arabic to English translations. Another five translations annotated as polarity errors occurred with the same expression translated as  X  X o fear X  or  X  X o be scared/afraid, X  as in (11). (11) MT: because the situation is getting worse and i am scared for the kids not to go
A closer look at utterances such as (11) suggests that the transcription which was used for input to the translation systems might be incorrect. The input for (11) is reproduced in Arabic script in (12). After listening to the audio recording, a native speaker confirmed that the sequence represented by the Arabic negative particle laa (  X  X  ) ) in (12), is actually pronounced with a short vowel: la (  X  X  ). We consulted the arabic-l e-mail list and reference texts about this structure and found a  X  X  translated as  X  X est X  in a dictionary of Egyptian Arabic [Badawi and Hinds 1986, p. 774], which is said to occur especially after the verb  X  X  X   X  X o fear. X  Informants on the arabic-l list viewed the form as analogous to  X  X  X  X  [laHsan]  X  X est X  in Egyptian Arabic.

The state of affairs represented in the clause introduced by  X  X  ) in (12) is unrealized, and other grammars associate  X  X  with the unrealized modality of the apodosis for a contrary to fact protasis (e.g., if I had known, I would have gone ) in Modern Standard Arabic [Haywood and Nahmad 1965], Levantine Arabic [McCarus 1979], and Yemeni Arabic [Watson 1993]. Consequently, the scholarly evidence supports the acoustic ev-idence that the form is not the negative particle  X  X  ) , but a modal particle  X  X  . (13) is another source input from our data originally transcribed with  X  X  ) but corrected here with  X  X  .
If we assume the inputs were incorrect and exclude the errors they caused, the proportion of non-null errors annotated as polarity errors drops from 0.009 to 0.003. 2
The presence of very few negative polarity errors raises the question of how many instances of negation occur in the data. The reference translations may differ for each system, but taking those for system A as an example, there were 10 references consist-ing of an initial no followed by a clause containing not or the contraction n t .Another three instances of no occurred in the expression no problem , and there were eight other occurrences of not or the contraction n t . Figure 6 presents the proportion of non-null errors produced by each system for each TER error type in translations from English to Iraqi Arabic. Substitutions are more frequent than all of the other TER error types, and the proportions of substitutions were similar for all systems. Most systems have similar proportions of insertions and word order errors, too.

Figure 7 presents the proportion of non-null errors for each word class that was annotated in the translations from English to Iraqi Arabic. The proportions of errors in the  X  X ther X  class are much higher than the proportions in pronoun, noun, or verb classes. The single error type annotated as Subject Person occurs nearly as frequently as all other verb errors for most systems. The exception is System D, which employed rule-based MT for English to Iraqi Arabic translations. The much lower proportion of Subject Person errors for System D in Figure 7 suggests that rule-based approaches produce correct agreement inflection more reliably than statistical approaches. In addition to Subject Person errors, many of the errors annotated as Substituted Pro-noun and Pronoun-Verb errors also involve subject inflection. Example (14) provides an example of the type of error that was annotated as Subject Person . (14) Ref:  X  X  X  X  X   X  X  X  X  X  X   X  X   X  X  X  X  X   X  X  X  X  X  X  X  X  In (14), the third person plural subject  X  X y marines X  should occur with third-person plural subject inflection on the verb, as in the reference translation, but in the system output, it occurs with first person singular inflection. The MT would be translated as  X  X y marines I will search the house, X  which could be interpreted several ways. If these Subject Person errors are treated as verb errors, the proportion of verb errors would nearly double for all systems, except System D.

Another type of error involving subject inflection on the verb is annotated as Sub-stituted Pronoun. As explained in Section 3.2, pronominal subjects are not usually expressed in Arabic because the subject inflection on the verb is adequate to identify a pronominal subject. Consequently, if the inflection is wrong, then the pronominal subject is wrong, as in (15). (15) Ref:  X  X  X  X  X   X  X  X  X   X  X  X  X  X  X  X   X  X  X  X   X  X  X  X   X  X  X  X  In (15) the speaker is referring to himself in first person singular ( X  X  might need... X ), but the matrix verb in the system output is inflected for a second person masculine or third person feminine subject ( X  X ou/she might need... X  Although the subject inflection on the verb is not a pronoun, it determines the interpretation of the unexpressed pronoun subject. Consequently, we annotate it as a pronoun error.

Occasionally both the verb and its subject inflection are wrong, as in (16). (16) Ref:  X  X  X   X  X  X   X  X   X  X  X   X  X  X  X   X  X   X  X  X  X  In (16) the first person plural subject (we) of the source contrasts with the third person singular subject inflection (he) in the system output, and the verb is also wrong. The Pronoun-Verb annotation is used to annotate this type of error. This category of error is not included in Figure 7 because it is so rare: less than 1% of non-null errors. Errors annotated as insertion, deletion, and substitution of  X  X ther X  word classes re-flect a variety of problems. Some translations are so garbled that errors are difficult to analyze in more detail. Many involve closed class function terms such as confusion between interrogative and relative pronouns or among the relative pronoun, comple-mentizer, and demonstrative uses of that . Example (17) illustrates the kind of error that is associated with the Deleted Other Class annotation. (17) Ref:  X  X  X  X  X   X  X  X  X  X   X  X  X  X   X  X   X  X  X  X  X   X  X  X  X  X  X   X  X  X  X  X   X  X   X  X  X  X  The complementizer so should be translated by a complementizer like  X  X  X  X  X   X  X o that. X 
The most frequent type of error in the  X  X ther X  class annotations can be generalized as errors involving multiword dependencies. Phrasal verbs and prepositions governed by verbs are two sources of this type of error. An example is (18). (18) Ref:  X  X  X  X  X   X  X  X  X  X   X  X  X  X   X  X  X  X  The phrasal verb go through is translated as if it were the verb go plus a locative prepositional phrase like through the city , and the MT system even supplies a noun object for the preposition. Table III presents the proportions of errors that were annotated in each TER error type for each word class, and Table IV presents the proportions of errors that were not annotated using TER error types and word classes. Together, Tables III and IV present 100% of the non-null annotated errors. The proportions for both translation directions are side-by-side so that they can be compared. The primary difference be-tween the two translation directions is in nouns and pronouns: Iraqi Arabic-to-English translations have higher proportions of pronoun errors, while English-to-Iraqi Arabic translations have higher proportions of noun errors. The proportion of verb errors in English-to-Iraqi Arabic translations would be nearly as high as the proportion in Iraqi Arabic-to-English translations if we added the 0.09 of Subject Person errors and 0.007 of Pronoun-Verb errors. This would also increase the total proportion of English-to-Iraqi Arabic translation errors in Table III to 0.829, which is nearly identical to the 0.831 total for Iraqi Arabic-to-English translation errors.

Subject Person and Pronoun-Verb errors are included in Table IV, which presents the proportions of errors that were not annotated according to TER error type and word class. Table IV shows that the proportion of word order errors in translations to Iraqi Arabic is slightly higher than the proportion in translations to English, even though Arabic word order is considered to be freer than English word order. Many of the word order errors in both directions are caused by the fact that most noun modifiers in Arabic follow the noun, whereas many noun modifiers in English precede the noun. In (19) the noun modifier  X  X  X  X  X  X  X  X   X  X he electricity X  follows the noun it modifies in Arabic, whereas it precedes the noun in English. (19) Ref:  X  X  X  X  X  X   X  X  X  X  X  X  X  X   X  X  X  X   X  X  X  X   X  X  X  X  X   X  X  X  X  (19) illustrates the Arabic construct or idafa structure in which a noun phrase follows another noun phrase that it modifies, and the order of nouns in the idafa is often incorrectly reversed, especially when translating English compounds and noun-noun modifier structures such as power station or birth weight .

Two results stand out in Tables III and IV. First, in translations from Iraqi Arabic to English, the proportion of verb deletions is relatively high. It is the highest propor-tion of deletions and the highest proportion of errors for Arabic to English translations in Table III. This result documents anecdotal observations of unexpectedly high lev-els of verb deletion for Arabic to English machine translation. Less expected are the low proportions of polarity errors. For English to Iraqi Arabic translations, the low proportions likely reflect the fact that there were only approximately five negative in-puts. For Iraqi Arabic to English, there were approximately 20 instances of negation, but more than half of these were initiated by a negative particle translated as  X  X o, X  which may have increased the likelihood that a negative utterance would follow in the systems X  models.

Some of the errors that we have documented can be observed when Arabic speakers learn English and vice versa. For example Noor [1996] cites nine studies mentioning the tendency of Arabic speakers to produce resumptive pronouns that do not occur in English and 13 studies describing errors due to deletion of the copula. It would be an interesting avenue for further investigation to review the literature on second language errors and compare them to the errors in our data. Of course, most of the studies focus on Modern Standard Arabic (MSA), which differs from dialects like Iraqi Arabic. Some recent work has demonstrated that colloquial dialects do influence lan-guage learning, though contrastive studies of English and Arabic typically do not take this fact into account [Al-Ajlouny 2007]. Nevertheless, many of the errors presented here do seem to result from grammatical features that are shared by both MSA and Iraqi varieties.

Another direction for further research is to analyze the differences in error types among the four systems and to relate these differences to the different MT strategies that each system used. This effort would motivate more refined annotation, especially among the errors that were folded into the  X  X ther X  set of word classes.

We are currently annotating translations for the same content, but produced from speech inputs, instead of the transcriptions that were used for this study. This will allow us to assess the effects of speech recognition errors on the types of translation errors that the systems produce. Also, we selected the corpus because NIST obtained human judgments of the translations from speech inputs, and our goal is to analyze the influence of the error types on the NIST adequacy scores [Sanders et al. 2008]. One of the DARPA teams has already conducted a small study along these lines [Stallard et al. 2008], and we plan to use our more systematic annotations and the larger corpus from all four teams, treating each error type as a variable that affects the human scores. A significant result of this study has been documentation of errors involving basic, high-frequency forms such as pronouns and the copula. The problems occur not for lack of examples, but because of linguistic differences that are difficult to resolve with statistical approaches to machine translation. These differences include (1) head and modifier order, particularly for nouns, (2) subject inflection on verbs in Iraqi Arabic, (3) unexpressed pronoun subjects in Iraqi Arabic, (4) gender inflection on pronouns, and (5) unexpressed copula in Iraqi Arabic. Of course, the fact that these linguistic differences are problematic does not mean that statistical approaches are inherently unable to handle them.

Many of these errors share another property which is known to challenge MT: they require inserting linguistic material into the translation that was not present in the source. Or they require the reverse: refraining from inserting linguistic material into the translation to correspond to items in the source. This failure to achieve one-to-one correspondence between the translation and the source also tends to be a feature of multi-word expressions, which are known to be sources of difficulty for MT and other language processing operations.
 Some errors involve more than structural differences between the two languages. The pronoun errors in particular often require knowledge of the discourse context to resolve. Iraqi Arabic speakers communicate effectively even though the past forms of verbs do not distinguish between first and second person subjects because the context of the interaction makes it clear whether the speaker is talking about  X  X  X  or  X  X ou. X  Similarly, knowing whether to translate an Arabic he or she as it requires knowledge of the referent of the pronoun. These problems will continue to challenge machine translation systems until they are resolved.

Finally, this research has provided some estimates of the relative proportions in which the error types impact machine translation. These estimates can be used to guide efforts to improve translation quality.

