 High dimensional directional data is becoming increasingly imp ortan t in con temp orary applications suc h as analysis of text and gene-expression data. A natural mo del for multi-variate directional data is pro vided by the von Mises-Fisher (vMF) distribution on the unit hyp ersphere that is anal-ogous to the multi-v ariate Gaussian distribution in R d . In this pap er, we prop ose mo deling complex directional data as a mixture of vMF distributions. We deriv e and analyze two varian ts of the Exp ectation Maximization (EM) framew ork for estimating the parameters of this mixture. We also pro-pose two clustering algorithms corresp onding to these vari-ants. An interesting asp ect of our metho dology is that the spherical kmeans algorithm (kmeans with cosine similarit y) can be sho wn to be a special case of both our algorithms. Th us, mo deling text data by vMF distributions lends theo-retical validit y to the use of cosine similarit y whic h has been widely used by the information retriev al comm unit y. As part of exp erimen tal validation, we presen t results on mo deling high-dimensional text and gene-expression data as a mix-ture of vMF distributions. The results indicate that our approac h yields sup erior clusterings esp ecially for dicult clustering tasks in high-dimensional spaces.
 H.3.3 [ Information Storage and Retriev al ]: Informa-tion Searc h and Retriev al; I.5.3 [ Pattern Recognition ]: Clustering Clustering, directional data, mixtures, von Mises-Fisher, EM Clustering or segmen tation of data is a fundamen tal data analysis step that has been widely studied across various dis-ciplines [19]. Ho wever, sev eral large datasets that are being acquired from scien ti c domains, as well as the world wide web, have a variet y of complex characteristics that sev erely challenge traditional metho ds for clustering [15]. This ar-ticle is concerned with the clustering of high-dimensional DC, USA. Cop yright 2003 ACM 1 X 58113 X 737 X 0/03/0008... $ 5.00. directional data that is becoming increasingly common in sev eral application domains.
 One can broadly categorize clustering approac hes into gener-ativ e (parametric) [29, 18] and discriminativ e (non-parametric) [17, 27] ones. The performance of an approac h (and of a spe-ci c metho d within that approac h) is quite data dep enden t; there is no clustering metho d that works the best across all typ es of data. Generativ e mo dels, however, often pro-vide better insigh t into the nature of the clusters. From an application poin t of view, a lot of domain kno wledge can be incorp orated into generativ e mo dels so that clustering of data unco vers speci c desirable patterns that one is look-ing for. Clustering algorithms using the generativ e mo del framew ork, often involv e an appropriate application of the Exp ectation Maximization (EM) algorithm [8, 5] on a prop-erly chosen statistical generativ e mo del for the data under consideration. For vector data, there are well studied clus-tering algorithms for popular generativ e mo dels suc h as a mixture of Gaussians, whose e ect is analogous to the use of Euclidean or Mahalanobis typ e distances from the dis-criminativ e persp ectiv e. There are sev eral application domains where clustering based on minimizing Euclidean distortions yields poor results [30]. For example, empirical studies in information retriev al ap-plications sho w that cosine similarity is a far more e ectiv e measure of similarit y for analyzing and clustering text docu-men ts. Suc h domains require the use of dir ectional data [22] | data that deals only with the directions of unit vectors. Th us, there is a need for generativ e mo dels that are more ap-propriate for the analysis and clustering of directional data. In this article, we prop ose a generativ e mixture mo del for directional data on the unit hyp ersphere and deriv e two clus-tering algorithms using this mixture mo del. We sho w the connection between the prop osed algorithms and a class of existing algorithms for clustering high-dimensional, direc-tional data and presen t detailed exp erimen tal comparisons among them.
 As already men tio ed, one imp ortan t domain where direc-tional data is encoun tered is text analysis, and text clus-tering in particular. It has been exp erimen tally sho wn that in order to remo ve the bias arising due to the length of a documen t, it often helps to normalize the data vectors [11]. Further, the spkmeans algorithm [11], that performs kmeans using cosine similarit y instead of Euclidean distortion, has been found to empirically outp erform sev eral other schemes. There are quite a few other domains suc h as bioinformat-ics [13], collab orativ e ltering [26] etc., in whic h directional data is encoun tered. A similarit y measure that has been found to be useful in these domains is the Pearson corre-lation coecien t. Giv en x ; y 2 R d , the Pearson pro duct momen t correlation between them is giv en by ( x ; y ) = p Th us, the Pearson correlation is exactly the cosine similarit y between ~ x and ~ y Hence, analysis and clustering of data us-ing Pearson correlations is essen tially a clustering problem for directional data.
 The application domains describ ed above share another char-acteristic, namely that the ob jects to be clustered reside in a very high dimensional space, with d sometimes in the thou-sands or more. Clustering of suc h high-dimensional data has been of great interest lately [9], with most of the prop osed metho ds follo wing a densit y-based heuristic or a discrimi-natory approac h [15, 1, 16]. Perhaps the most notew orth y generativ e approac h for clustering high-dimensional data is to use mixtures of Gaussians [7]. Certain other works suc h as [24] use a generativ e mo del from the exp onen tial fam-ily and have been explicitly dev elop ed for mo deling text. The spkmeans algorithm for clustering normalized data was prop osed in [11], while the connection between a genera-tiv e mo del involving vMF distributions and the spkmeans algorithm was rst observ ed in [3]. An online comp etitiv e learning scheme using vMF distributions for minimizing a KL-div ergence based distortion was prop osed in [28]. Ho w-ever, these approac hes do not cluster directional data using explicit probabilistic generativ e mo dels. In this work, the directional nature of the data is used explicitly resulting in signi can tly better clustering performances over certain standard tec hniques, (suc h as kmeans with cosine similar-ity) esp ecially for dicult clustering tasks: when clusters overlap, when cluster sizes are skew ed, and when cluster sizes are small relativ e to the dimensionalit y of the data. Interestingly , as we shall see later on, one of our prop osed approac hes has an implicit sim ulated annealing typ e beha v-ior that seems to alleviate some of the problems asso ciated with high-dimensionalit y.
 Before pro ceeding further, we giv e a brief outline of the pa-per. We rst presen t the vMF distribution on the sphere in section 2. In section 3, we intro duce the generativ e mo del for a mixture of vMF distributions and analyze the max-imum likeliho od parameter estimate of the mixture mo del from a giv en dataset using the EM framew ork. Based on the analysis of section 3, two clustering algorithms using soft and hard-assignmen ts resp ectiv ely are prop osed in sec-tion 4. We presen t detailed exp erimen tal results on the pro-posed algorithms in section 5. Some interesting asp ects of the prop osed algorithms are discussed in section 6. Section 7 presen ts concluding remarks and directions for future work. A word about the notation: bold faced variables, e.g., x ; , etc., represen t vectors; kk denotes the L 2 norm; sets are represen ted by calligraphic upp er-case letters, e.g., X , Z , etc.; R denotes the set of reals, S d 1 denotes the ( d 1)-dimensional sphere em bedded in R d . Probabilit y densit y functions are denoted by lower case alphab ets, e.g., f ( ), p ( ), etc.; probabilit y of a set of events is denoted by P . If a random variable z is distributed as p ( ), exp ectations of functions of z are denoted by E p [ ]. A d -dimensional unit random vector x (i.e., k x k = 1) is said to have d -variate von Mises-Fisher (vMF) distribution if its probabilit y densit y function is giv en by: where k k = 1, 0. Th us, x ; 2 S d 1 . The normalizing constan t c d ( ) is giv en by: where I r ( ) represen ts the mo di ed Bessel function of the rst kind of order r , see [22] and [12] for more details on vMF distributions. The densit y f ( x j ; ) is parameterized by the mean direction, , and the concen tration parameter, , so-called because it characterizes how strongly the unit vectors dra wn according to f ( x j ; ) are concen trated about the mean direction . Larger values of imply stronger concen tration about the mean direction. In particular when = 0, f ( x j ; ) reduces to the uniform densit y on S d 1 and as !1 , f ( x j ; ) tends to a poin t densit y. The von Mises-Fisher distribution is natural for directional data and has prop erties analogous to those of the multi-variate Gaussian distribution for multi-v ariate data in R d For example, the maxim um entrop y densit y on S d 1 sub ject to the constrain t that E [ x ] is xed is a vMF densit y (see [25] and [21] for details). In this section, we intro duce a mixture of k vMF (mo vMF) distributions as a generativ e mo del for directional data, and then deriv e the mixture-densit y parameter estimation up-date equations from a giv en data set using the exp ectation maximization (EM) framew ork. The probabilit y densit y function of the mo vMF generativ e mo del is giv en by where = f 1 ; ; k ; 1 ; ; k g with h 0, and f h ( x j h ) is a single vMF distribution with pa-rameters h = ( h ; h ). In order to sample a poin t from the generativ e mo del persp ectiv e, the h -th vMF is chosen at random with probabilit y h , and then a poin t is sam-pled from S d 1 follo wing f h ( x j h ). Let X = f x 1 ; ; x be a data set generated by sampling indep enden tly follo wing this generativ e mo del. Let Z = f z 1 ; ; z n g be the corre-sponding set of the so-called hidden random variables suc h that z i = h when x i has been generated follo wing f h ( x j Then, with the kno wledge of the values of the hidden vari-ables, the log-lik eliho od of the observ ed data is giv en by from whic h maxim um likeliho od parameter estimates can be obtained. Ho wever, the values of the hidden variables are not kno wn and so (4) is really a random variable dep enden t on the distribution of Z , and will be called the complete data log-likeliho od . No w, for a giv en ( X ; ), it is possible to es-timate the most likely conditional distribution of Zj ( X ; ), and this forms the E-step of the EM framew ork. The exact details of how this estimation is done will be deferred for the momen t. In fact we will discuss two ways of estimating the hidden variable distributions that lead to signi can tly di eren t algorithms. For now, we will assume that the dis-all the data poin ts. Supp ose the posterior distribution, p ( h j x i ; ) ; 8 h;i , of the hidden variables Zj ( X ; ) is kno wn. Unless otherwise spec-i ed, henceforth all exp ectations will be tak en over the dis-tribution of the (set of) random variable(s) Zj ( X ; ). No w, exp ectation of the complete data log-lik eliho od, giv en by (4), over the giv en distribution p is
E p [ln P ( X ; Zj )] = In the parameter estimation or M-step, is re-estimated suc h that the above expression is maximized. Note that in order to maximize this expression, we can maximize the term con taining h and the term con taining h separately since they have no functional dep endencies (note that p ( h j x is xed).
 To nd the expression for h , we intro duce the Lagrangian multiplier with the constrain t that ing partial deriv ativ es of the Lagrangian ob jectiv e function w.r.t. eac h h and solving, we get Next we concen trate on the terms con taining h = ( h ; h under the set of constrain ts T h h = 1 ; h 0 ; 8 h . Then, us-ing Lagrange multipliers 1 ; ; k corresp onding to these constrain ts 1 , the Lagrangian is giv en by Taking partial deriv ativ es of (7) with resp ect to f h ; and setting them to zero, for h = 1 ; 2 ;:::;k we get: analysis using the KKT conditions is skipp ed for brevit y. h 0 is accoun ted for by taking positiv e square roots. The nal estimates come out to be the same [2]. not possible and one can use numerical tec hniques to solv e for ^ h . A reasonable appro ximation to the solution is ob-tained by setting where r h = A d (^ h ) [2]. From the standard setting of the EM algorithm [8, 5], (6), (8), and (9) giv e the up date equations for the parameters in-volv ed. Giv en this set of parameters, in this section we out-line two schemes for up dating the distributions of Zj ( X ; ) so that the likeliho od of the data is maximized.
 The rst up date scheme exactly follo ws the soft-assignmen t scheme for learning mixture mo dels using EM. From the standard EM framew ork, the distribution of the hidden vari-ables [23] is giv en by: Our second up date scheme is based on the widely used hard-assignmen t heuristic for unsup ervised learning. In this case, the distribution of the hidden variables is giv en by Though soft-assignmen ts are theoretically very well moti-vated [5, 23], hard-assignmen ts have not receiv ed much the-oretical atten tion (though some discussion can be found in [20]). In the rest of this section, we formally study the connection between soft and hard-assignmen ts in the EM framew ork. Note that this analysis is applicable to general mixture mo del learning in the EM framew ork.
 At rst, follo wing the argumen ts in [23], we intro duce the function F (~ p; ) giv en by where ~ p is some distribution of Zj ( X ; ). From a maxim um-likeliho od persp ectiv e, the primary ob jectiv e function that we want to maximize to get the correct mixture mo del is the inc omplete data log-likeliho od ln P ( Xj ). No w, one in-teresting prop ert y of the function F is that it lower bounds ln P ( Xj ) over choices of ~ p . Hence, it mak es sense to try to maximize F with resp ect to ~ p . For a giv en , the ~ p that maximizes F ( ; ) is giv en by (11) [23]. The corresp onding optimal value of the function is giv en by
F ( p; ) = E p [ln P ( X ; Zj )] + H ( p ) the incomplete data log-lik eliho od. Further, one can easily see that for a giv en ~ p , the that optimizes F (~ p; ) is the the same as the one that optimizes the exp ectation of the complete data log-lik eliho od in (5). Hence, the M-step is equiv alen t to optimizing F with resp ect to for a giv en ~ p . It follo ws [5, 23], that the incomplete data log-lik eliho od, ln p ( Xj ), is non-decreasing at eac h iteration of the alter-nate maximization of the function F in terms of and ~ p , or equiv alen tly the parameter and the distribution up dates giv en by (6), (8), (9), and (11). Iteration over this set of up dates forms the basis of our algorithm soft-movMF to be discussed in section 4.
 In the hard-assignmen t case, e ectiv ely eac h of the hidden random variables has a distribution that has probabilit y 1 for one of the mixture comp onen ts, and 0 for all the oth-ers. We shall denote this class of distributions by H . The imp ortan t question is: is there a way to optimally pic k a dis-tribution from H and then perform a regular M-step, and guaran tee, as before, that the incomplete log-lik eliho od of the data is non-decreasing at eac h iteration of the up date? Unfortunately , this is not possible in general. Ho wever, we sho w that it is possible to reasonably lower bound the in-complete log-lik eliho od of the data using exp ectations over q 2 H as in (12). The distribution q 2 H is optimal in the sense that it giv es the tigh test lower bound to the in-complete log-lik eliho od among all distributions in H . The lower bound is reasonable in the sense that the exp ecta-tion over q is itself lower bounded by (5), the exp ectation of the complete log-lik eliho od over the distribution p giv en by (11). Then, an iterativ e up date scheme analogous to regular EM guaran tees that this tigh t lower bound on the incomplete log-lik eliho od is non-decreasing at eac h iteration of the up date. The parameter estimation, or, M-step re-mains practically unc hanged, with p replaced by q in the up date equations (6), (8), and (9).
 Since p giv en by (11) optimizes F for a giv en , the func-tional value F ( ; ) is smaller for any other choice of ~ p . In particular, if ~ p = q as in (12), we have No w, all distributions in H have the prop ert y that their entrop y is 0. In particular, H ( q ) = 0. Then, from the de nition of the function F , we have So, the exp ectation over q actually lower bounds the likeli-hood of the data. Using the de nitions of p and q , one can No w, adding the incomplete data log-lik eliho od ln P ( Xj ) to both sides of this inequalit y, we have Com bining (16) and (17), we get E p [ln P ( X ; Zj )] E q [ln P ( X ; Zj )] ln P ( Xj ) : (18) Th us, the exp ectation over q lies in between the incomplete data likeliho od and the exp ectation of the complete data likeliho od over p and hence is a reasonable lower bound to the incomplete data likeliho od value.
 Finally , we sho w that our choice of the distribution q is opti-mal in the sense that the exp ectation over q giv es the tigh test lower bound among all distributions in H . Let ~ q be any other distribution in the subset so that ~ q ( h i j x i ; ) = 1. Let h argmax Then,
E ~ q [ ln P ( X ; Zj )] Hence, the choice of q as in (12) is optimal. This analysis forms the basis of our algorithm hard-movMF to be discussed in section 4. In this section, we prop ose two algorithms for clustering directional data based on the dev elopmen t of the previous section. The two algorithms are based on soft and hard-assignmen t schemes and are resp ectiv ely called soft-movMF and hard-movMF . The soft-movMF algorithm, presen ted in Algorithm 1 estimates the parameters of the mixture mo del exactly follo wing the deriv ations in section 3 using EM. Hence, it assigns soft (or probabilistic) lab els to eac h poin t that are giv en by the posterior probabilities of the comp o-nen ts of the mixture conditioned on the poin t. On termina-tion, the algorithm giv es the parameters = f h ; h ; h g of the k vMF distributions that mo del the data set X , as well as the soft-clustering , i.e., the posterior probabili-ties p ( h j x i ; ) ; 8 h;i . Appropriate con vergence criteria de-termine when the algorithm should terminate.
 Algorithm 1 soft-movMF Input: Set X of data poin ts on S d 1 Output: A soft clustering of X over a mixture of k vMF distributions
Initialize all h ; h ; h ; h = 1 ; ;k rep eat until conver genc e The hard-movMF algorithm estimates the parameters of the mixture mo del by a hard assignmen t, based on a deriv ed posterior distribution giv en by (12). The algorithm is ob-tained from Algorithm 1 by replacing all the posteriors p by the hardened posteriors q as in (12). Th us, after the hard assignmen ts in every iteration, eac h poin t belongs to a single cluster. As before, the up dates of the comp onen t parameters are done using the posteriors of the comp onen ts giv en the poin ts. The only di erence in this case is that the posterior probabilities can tak e values only in f 0 ; 1 g . On termination, the algorithm giv es the parameters = f h ; h ; h g k h =1 of the k vMFs that mo del the data set X under the hard assignmen t setup, and the har d-clustering , i.e., a disjoin t k -partitioning of X based on the conditional posteriors q on the comp onen t vMF distributions. We brie y revisit the spkmeans algorithm [11], that has been sho wn to perform well on real life text clustering tasks [11, 3], in the ligh t of the dev elopmen ts of sections 3 and 4. First, we presen t the spkmeans pro cedure in Algorithm 2. The main observ ation is that the spkmeans algorithm can be looked up on as a special case of the soft-movMF as well as the hard-movMF algorithms under certain restrictiv e as-sumptions on the generativ e mo del. To be more precise, say we assume that the generativ e mo del of the mixture of vMFs is suc h that the priors of all the comp onen ts are the same, i.e., h = 1 =k; 8 h . In order to get spkmeans as a reduction from soft-movMF , we further assume that all the comp onen ts have (equal) in nite concen tration parameters, i.e., h = ! 1 ; 8 h . With these assumptions, the E-step reduces to assigning a poin t to its nearest cluster where near-ness is computed as a cosine similarit y between the poin t and the cluster represen tativ e. Th us, a poin t x i will be assigned to cluster h = argmax and p ( h j x i ; ) ! 0 ; 8 h 6 = h .
 To sho w that spkmeans can also be seen as a special case of the hard-movMF algorithm, in addition to assuming the priors of the comp onen ts to be equal, we further assume that the concen tration parameters of all the comp onen ts are equal, i.e., h = ; 8 h . Then, hard-movMF reduces to spkmeans . With these assumptions on the mo del, the es-timation of the common concen tration parameter becomes unessen tial since the hard assignmen t will dep end only on the value of the cosine similarit y x T i h . Giv en a set of values for f h g k h =1 , de ne X h = f x : x 2X ;h = argmax h 0 It is easy to see that fX h g k h =1 forms a disjoin t k -partitioning of X . For a giv en set of values for f h ; h g k h =1 , we can rewrite the hard-movMF algorithm using a similar notation of set partitions, X h = f x : x 2X ;h = argmax h 0 h 0 x T h 0 In addition to the above three algorithms, we rep ort ex-perimen tal results on another algorithm fskmeans [3] that belongs to the same class in the sense that, like spkmeans , it can be deriv ed from the mixture of vMF mo dels with some restrictiv e assumptions. In fskmeans , the cen troids of the mixture comp onen ts are estimated as in hard-movMF . The concen tration of a comp onen t is set to be inversely prop or-tional to the num ber of poin ts in the cluster corresp onding to that comp onen t in order to sim ulate a frequency sensitiv e Algorithm 2 spkmeans Input: Set X of data poin ts on S d 1 Output: A disjoin t k -partitioning fX h g k h =1 of X
Initialize h ; h = 1 ; ;k rep eat until conver genc e comp etitiv e learning that implicitly prev ents the formation of null clusters, a well-kno wn problem in regular kmeans [4]. In this section we brie y describ e the datasets and exp er-imen tal metho dology used. Then, we discuss the perfor-mance of the four algorithms under consideration on the various datasets. We presen t results on two standard text datasets: 20 News-groups 2 , and Yaho o News 3 , and a dataset of Yeast gene-expression lev els.
 The 20 Newsgr oups dataset is a collection of 19997 docu-men ts from 20 di eren t Usenet newsgroups with (appro xi-mately) equal num ber of documen ts from eac h group. We presen t results on sev eral subsets of this dataset. The full dataset will be called news20 . A smaller version small-news20 was created with 100 randomly chosen documen ts from eac h of the 20 newsgroups. Various subsets of these two datasets were studied to understand the impact of dataset prop erties on the relativ e performance of the algorithms. news-diff3 is a subset consisting of 3 very di eren t newsgroups (alt.atheism, rec.sp ort.baseball, sci.space) with 1000 documen ts per clus-ter, and small-news-diff3 consists of the same 3 news-groups with 100 documen ts per cluster. news-sim3 is a sub-set consisting of 3 very similar newsgroups (comp.graphics, comp.os.ms-windo ws, comp.windo ws.x) with 1000 documen ts per cluster, and small-news-sim3 consists of the same 3 newsgroups with 100 documen ts per cluster.
 The Yaho o News (K-series) dataset is a collection of 2340 Yaho o news articles from 20 di eren t categories. We used the full yahoo dataset for exp erimen tation. The underlying clusters in this dataset are highly skew ed in terms of the num ber of documen ts per clusters, with sizes ranging from 9 to 494.
 The Rosetta Inpharmatics Yeast gene-expr ession dataset [14] is a collection of gene-expression vectors constructed using http://www.ai.mit.edu/p eople/jrennie/20 newsgroups ftp://ftp.cs.umn.edu/users/b oley/PDDPdata/ DNA microarra y exp erimen ts on the Yeast genes. The origi-nal dataset consists of 300 exp erimen ts measuring expression of sev eral yeast genes. We used a subset of 996 genes (with kno wn phylogenetic pro les) for our exp erimen ts. Performance of the four algorithms discussed in section4 on all the text datasets have been analyzed using mutual infor-mation (MI) between the cluster and class lab els. The MI giv es the amoun t of statistical similarit y between the clus-ter and class lab els [6]. If X is a random variable for the cluster assignmen ts and Y is a random variable for the pre-existing lab els on the same data, then their MI is giv en by on the join t distribution of ( X;Y ) estimated from a particu-lar clustering of the dataset under consideration. All results rep orted are averaged over 10 runs and all algorithms were started with the same random initialization. Since the the standard deviations of MI were reasonably small for all al-gorithms, to reduce clutter, error bars have not been sho wn in the plots. Also note that for the gene-expression datasets, since class lab els are una vailable, the average cosine similar-ity between eac h data-p oin t and its cluster represen tativ e is used to evaluate the performance of the algorithms. In this section, we brie y discuss the performance of the al-gorithms on the 20 Newsgroups datasets. All the algorithms performed similarly for the news20 dataset achieving a MI of appro ximately 1.5 at the correct num ber of clusters. Exp er-imen ts with arti cial directional data rev ealed the empirical fact that if the clusters have almost equal priors, have su-cien tly large num ber of data poin ts per cluster, and sev eral of them are reasonably separated the algorithms perform similarly , since the clustering problem is rather simple. On the other hand, if one or more of these conditions are vio-lated, they giv e quite di eren t clusterings according to their resp ectiv e biases. Lesion exp erimen ts with the various sub-sets of news20 are performed to study these biases in detail. For example, even though the small-news20 dataset is just a sampled version of the news20 dataset, signi can t perfor-mance di erences are observ ed as sho wn in Figure 1. The num ber of documen ts per cluster being small, spkmeans and fskmeans do not perform that well, even for the true num ber of clusters, since for a small num ber of poin ts, they become more susceptible to getting trapp ed in bad local minima of their resp ectiv e ob jectiv e functions. On the other hand, soft-movMF performs signi can tly better than all the other algorithms over the entire range, while hard-movMF giv es satisfactory MI values till the true num ber of clusters after whic h it falls sharply . Next, the e ect of small num ber of poin ts per cluster in high-dimensions is studied more closely using the two vari-ants of the news20-diff3 dataset. The basic news20-diff3 dataset is an easy dataset to cluster since it has equal priors, reasonably separate clusters and sucien t num ber of docu-men ts per cluster. Ho wever, as we shall shortly see, when the smaller version, small-news20-diff , of this simple dataset is used, the performance of the algorithms change by signif-ican t amoun ts. The results on news20-diff3 are sho wn in Figure 2. The performance of all the algorithms are compa-rable, though the vMF based algorithms giv e higher values of MI consisten tly over the entire range of the num ber of clusters we exp erimen ted with. Further, at the correct num-ber of clusters k = 3, the vMF-based algorithms perform signi can tly better. We demonstrate this more clearly by presen ting confusion matrices generated by spkmeans and soft-movMF for k = 3 in Table 1. Among the two vMF based algorithms, soft-movMF performs consisten tly better than hard-movMF over the entire range.
Figure 3: Clustering results on small-news20-diff3 Table 2: Confusion matrix for small-news20-diff3 Results on the small-news20-diff3 dataset are presen ted in Figure 3. We note that the vMF-based algorithms perform signi can tly better than fskmeans and spkmeans , whic h sho w a rather poor performance since the num ber of data poin ts per cluster is relativ ely small compared to the dimension-alit y of the data. In fact, for suc h high-dimensional data with sparse represen tation, suc h as text documen ts, a few poin ts chosen at random have a high probabilit y of being orthogonal to eac h other, so that a kmeans-based iterativ e up date scheme ma y get stuc k at a local minim um at the very poin t it is initialized (see [10]). As a result, kmeans-based algorithms su er in high-dimensions, esp ecially when the num ber of data poin ts available is small. This explains the performance of fskmeans and spkmeans . Among the vMF-based algorithms, soft-movMF performs signi can tly better than hard-movMF over the entire range. This can be attributed to a very interesting implicit sim ulated annealing typ e beha vior that soft-movMF demonstrates. This beha v-ior will be discussed in some detail in section 6. Typical confusion matrices generated by spkmeans and soft-movMF for the correct num ber of clusters are presen ted in Table 2. The next two datasets help us study the e ect of high-dimensional overlapping clusters on the performance of the algorithms. Results on the news20-sim3 data are presen ted in Figure 4. Though eac h of the clusters has sucien t num ber of poin ts, this is a rather dicult dataset to clus-ter because the 3 newsgroups are on very similar topics. Since there are enough poin ts to work with, fskmeans and spkmeans attain reasonable performance all along, but su er a little due to the cluster overlaps. Again, the vMF-based algorithms consisten tly perform better than the kmeans-based algorithms. Among the two vMF-based algorithms, soft-movMF seems to achiev e higher values of MI, though the di erences are not alw ays signi can t. Represen tativ e confusion matrices generated by spkmeans and soft-movMF are presen ted in Table 3. Figure 5: Clustering results on small-news20-sim3 The small-news20-sim3 dataset is perhaps the most di-cult dataset to cluster among the news20 subsets that we
Table 4: Confusion matrix for small-news20-sim3 consider. It has overlapping clusters with a relativ ely small num ber of poin ts per cluster. As exp ected, fskmeans and spkmeans do poorly on this dataset. The vMF-based algo-rithms perform signi can tly better as before. Again, soft-movMF achiev es much higher values of MI over the entire range of cluster values over whic h exp erimen ts were performed. Typ-ical confusion matrices are presen ted in Table 4. The Yaho o News dataset is a dicult dataset for cluster-ing in the sense that in addition to having some amoun t of overlap in the clusters and insucien t poin ts per cluster, the clusters are highly skew ed in terms of the num ber of poin ts per cluster. We presen t results for the di eren t algorithms in Figure 6. Ov er the entire range, soft-movMF consisten tly performs better than the other algorithms. Ev en at the cor-rect num ber of clusters k = 20, it performs signi can tly bet-ter than the other algorithms. fskmeans and spkmeans have a very similar beha vior till a mo derate num ber of clusters. For higher num bers of clusters, spkmeans generates empt y clusters because of whic h its MI does not increase as fast as fskmeans that has an explicit mec hanism for prev enting null clusters. The performance of hard-movMF is interest-ing; its MI values are comparable and at times worse than those of fskmeans and spkmeans . Though it seems to per-form marginally better at the correct num ber of clusters, the impro vemen t is not statistically signi can t. As seen earlier, hard-moVMF performed signi can tly better than the kmeans-based algorithms on the other datasets that we discussed. In fact, similar beha vior is observ ed on all other datasets we exp erimen ted with [2]. The skewness of yahoo in terms of cluster sizes can be a possible reason for its bad performance Figure 7: Clustering results on Yeast gene-expression data on this dataset, but exp erimen ts on other skew ed datasets [2] do not explain this beha vior. Results on the Yeast gene-expression dataset are presen ted in Figure 7. As can be clearly seen, for a rather di er-ent clustering domain with a di eren t performance measure, the vMF-based algorithms perform signi can tly better than the kmeans-based algorithms. Among the vMF-based algo-rithms, the performance of hard-movMF is marginally better than soft-movMF , but the di erences are not signi can t. It is interesting to note that the vMF-based algorithms, while trying to maximize rather di eren t ob jectiv e functions, ac-tually perform signi can tly better than spkmeans in terms of the average cosine similarit y, even though the latter is the ob jectiv e function that spkmeans explicitly tries to maxi-mize. The mixture of vMF distributions giv es a parametric mo del-based generalization of the widely used cosine similarit y measure. As discussed in section 4.1, the spherical kmeans algorithm that uses cosine similarit y arises as a special case of the EM on mixture of vMFs when, among other things, the concen tration of all the distributions is held constan t. From a di eren t persp ectiv e, we argue that if a particular dataset has been sampled follo wing a vMF distribution with a giv en , say = 1, the natur al similarit y between any pair of data poin ts in that set is giv en by the cosine similarit y. More precisely , if data-p oin ts are represen ted in a feature-space with orthonormal basis, the Fisher-Information ma-trix is iden tity, and hence, the Fisher kernel similarit y [18] is giv en by
K ( x i ; x j ) = ( r ln f ( x i j )) T ( r ln f ( x j j )) (see (1)) whic h is exactly the cosine similarit y. In other words, when-ever cosine similarit y is used in a particular application (not necessarily clustering), the implicit assumption one mak es is that the data has been dra wn from a vMF distribution. In terms of performance, the magnitude of impro vemen t sho wn by the soft-movMF algorithm for the dicult cluster-ing tasks was surprising, esp ecially since for low-dimensional non-directional data, the impro vemen ts using a soft-assignmen t based kmeans over the standard hard-assignmen t based ver-sions are often times quite minimal. In particular, we were curious regarding a couple of issues: (i) wh y is soft-movMF performing substan tially better than hard-movMF , even though the nal probabilit y values obtained by soft-movMF are ac-tually very close to 0 and 1; and (ii) wh y is soft-movMF , whic h needs to estimate more parameters, doing better even when there are insucien t num ber of poin ts relativ e to the dimensionalit y of the space, e.g., all the small-datasets. It turns out that both these issues can be understo od by taking a closer look at how the soft-mo vMF con verges. In all our exp erimen ts, we initialized to 10, and initial cen-troids to small random perturbations of the global cen troid. Hence, for soft-movMF , the initial posterior mem bership dis-tributions of the data poin ts are almost uniform and the entrop y H ( p ) of the hidden random variables is very high. The change of this entrop y over iterations for the news20 subsets is presen ted in Figure 8. The beha vior is similar for all the other datasets. Unlik e kmeans-based algorithms where most of the relo cation happ ens in the rst two or three iterations with only minor adjustmen ts later on, in soft-movMF the data poin ts are noncommittal in the rst few iterations, and the entrop y remains very high (the max-imum possible entrop y can be log 2 3 = 1 : 585). The cluster patterns are disco vered only after sev eral iterations, and the entrop y drops drastically within a small num ber of iterations after that. When the algorithm con verges, the entrop y is practically zero and all poin ts are e ectiv ely hard-assigned to their resp ectiv e clusters. Note that this beha vior is strik-ingly similar to sim ulated annealing approac hes where can be considered as the inverse of the temp erature parameter. The drastic drop in entrop y after a few iterations is the typ-ical critical temp erature beha vior observ ed in annealing. As text data has only non-negativ e features values, all the data poin ts lie in the rst orthan t of the d -dimensional hy-persphere and hence, is naturally very concen trated. The gene-expression data, though spread all over the hyp ersphere seemed to have some high concen tration regions. In either case, the nal values on con vergence are very high, re ect-ing the concen tration in the data, and implying a low nal temp erature from the annealing persp ectiv e. Then, initial-izing to a low value, or equiv alen tly a high temp erature, is a good idea because in that case when soft-movMF is run-ning, the values will keep on increasing over successiv e iterations to get to its nal large values, giving the e ect of a decreasing temp erature in the pro cess, without any ex-plicit sim ulated annealing strategy . This explains wh y the soft-movMF algorithm performs so well for dicult cluster-ing tasks in high-dimensions. The hard-movMF algorithm, instead of using the more general vMF mo del, su ers be-cause of hard-assignmen ts from the very beginning. The  X  fskmeans and spkmeans do not do well for dicult datasets due to their hard assignmen t scheme as well as their signif-ican tly less mo deling capabilities. Figure 8: Variation of En trop y of hidden variables with num ber of Iterations In this pap er we have prop osed two algorithms for clustering directional data. The algorithms are essen tially exp ectation maximization schemes applied to an appropriate generativ e mo del, namely a mixture of von Mises-Fisher distributions. We sho w that the spkmeans algorithm [11], that has been sho wn to work well for text clustering, is a special case of both the prop osed schemes. Another high-dimensional clus-tering algorithm, fskmeans [3], is also a special case of one of the prop osed algorithms. All the algorithms are compre-hensiv ely evaluated using sev eral real-life high dimensional datasets with varying degrees of complexit y. From the ex-perimen tal results, it seems that certain high-dimensional datasets, e.g., text, gene-expression etc., after L 2 normal-ization have prop erties that matc h well with the mo deling assumptions of the vMF mixture mo del. Hence, the pro-posed algorithms form powerful clustering tec hniques for suc h datasets.
 The vMF distribution that we considered in the prop osed tec hniques, is one of the simplest parametric distributions for directional data. More general mo dels, e.g., the Fisher-Bingham distribution, have added expressiv e power and ma y be useful under certain circumstances. Ho wever, the param-eter estimation problem is signi can tly more dicult for suc h mo dels. Also, one needs substan tially more data to get reliable estimates of the parameters. Hence these more complex mo dels ma y not be viable for most problems. Nev-ertheless, the tradeo between mo del complexit y (in terms of the num ber of parameters), and sample complexit y needs to be studied in more detail in the con text of directional data. We could also adapt a local searc h strategy suc h as the one in [10], for incremen tal EM to yield better local minima for both hard and soft-assignmen ts.
 part by NSF gran t ECS-9900353 and NSF CAREER Aw ard No. ACI-0093404 and Texas Adv anced Researc h Program gran t 003658-0431-2001. [1] C. Aggarw al. Re-designing distance functions and [2] A. Banerjee, I. S. Dhillon, J. Ghosh, and S. Sra. [3] A. Banerjee and J. Ghosh. Frequency sensitiv e [4] P. S. Bradley , K. P. Bennett, and A. Demiriz. [5] M. Collins. The EM algorithm. In ful llmen t of [6] T. M. Co ver and J. A. Thomas. Elements of [7] S. Dasgupta. Learning mixtures of Gaussians. In [8] A. P. Dempster, N. M. Laird, and D. B. Rubin.
 [9] I. Dhillon and J. Kogan, editors. Pr oc. Workshop on [10] I. S. Dhillon, Y. Guan, and J. Kogan. Iterativ e [11] I. S. Dhillon and D. S. Mo dha. Concept [12] I. S. Dhillon and S. Sra. Mo deling data using [13] M. B. Eisen, P. T. Sp ellman, P. O. Bro wn, and [14] T. R. H. et. al. Functional disco very via a comp endium [15] J. Ghosh. Scalable clustering metho ds for data [16] G. K. Gupta and J. Ghosh. Detecting seasonal and [17] P. Indyk. A sublinear-time appro ximation scheme for [18] T. Jaakk ola and D. Haussler. Exploiting generativ e [19] A. K. Jain and R. C. Dub es. Algorithms for Clustering [20] M. Kearns, Y. Mansour, and A. Ng. An [21] K. V. Mardia. Statistic al Distributions in Sciencti c [22] K. V. Mardia and P. Jupp. Dir ectional Statistics . John [23] R. M. Neal and G. E. Hin ton. A view of the EM [24] K. Nigam, A. K. Mccallum, S. Thrun, and [25] C. R. Rao. Line ar Statistic al Infer enc e and its [26] B. M. Sarw ar, G. Karypis, J. A. Konstan, and [27] B. Sch X olk opf and A. Smola. Learning with Kernels . [28] J. Sinkk onen and S. Kaski. Clustering based on [29] P. Sm yth. Clustering sequences with hidden Mark ov [30] A. Strehl, J. Ghosh, and R. Mo oney . Impact of
