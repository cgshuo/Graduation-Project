 Marco Kuhlmann * Uppsala University dependencies. In this article we present a formalism for non-projective dependency grammar formalism is a close correspondence between the non-projectivity of the dependency trees admitted by a grammar on the one hand, and the parsing complexity of the grammar on the time. An evaluation on five dependency treebanks shows that these grammars have a good coverage of empirical data. 1. Introduction
Syntactic representations based on word-to-word dependencies have a long-standing and Word Grammar (Hudson 2007). In recent years they have also been used for a wide range of practical applications, such as information extraction, machine translation, and accurate and efficient dependency parsers for a wide range of languages (Buchholz and Marsi 2006; Nivre et al. 2007).
 formal point of view. In particular, there are relatively few results that bridge between dependency syntax and other traditions, such as phrase structure or categorial syntax.
This makes it hard to gauge the similarities and differences between the paradigms, and hampers the exchange of linguistic resources and computational methods. An overarching goal of this article is to bring dependency grammar closer to the mainland of formal study.
 (1965), who studied a formalism that we will refer to as Hays X  X aifman grammar, and proved it to be weakly equivalent to context-free phrase structure grammar. Although this result is of fundamental importance from a theoretical point of view, its practical usefulness is limited. In particular, Hays X  X aifman grammar is restricted to projective dependency structures, which is similar to the familiar restriction to contiguous con-language. One classic example of this is the phenomenon of cross X  X erial dependencies in Dutch. In this language, the nominal arguments of verbs that also select an infinitival complement occur in the same order as the verbs themselves: (i) dat Jan 1 Piet 2 Marie 3 zag 1 helpen 2 lezen 3 (Dutch)
In German, the order of the nominal arguments instead inverts the verb order: (ii) dass Jan 1 Piet 2 Marie 3 lesen 3 helfen 2 sah 1 (German)
Figure 1 shows dependency trees for the two examples. gives rise to a projective structure, where the verb X  X rgument dependencies are nested within each other, whereas the Dutch linearization induces a non-projective structure with crossing edges. To account for such structures we need to turn to formalisms more expressive than Hays X  X aifman grammars.
 based on linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988). This framework was introduced to facilitate the comparison of various 356 grammar formalisms, including standard context-free grammar, tree-adjoining gram-mar (Joshi and Schabes 1997), and combinatory categorial grammar (Steedman and
Baldridge 2011). It also comprises, among others, multiple context-free grammars (Seki grammars (Boullier 2004).
 ground to our work; in particular, we introduce our terminology and notation for linear context-free rewriting systems. An LCFRS generates a set of terms (formal expressions) which are interpreted as derivation trees of objects from some domain. Each term also has a secondary interpretation under which it denotes a tuple of strings, representing the string yield of the derived object. In Section 3 we introduce the central notion of a lexicalized linear context-free rewriting system, which is an LCFRS in which each rule of the grammar is associated with an overt lexical item, representing a syntactic head rise to an additional interpretation under which each term denotes a dependency tree on its yield. With this interpretation, lexicalized LCFRSs can be used as dependency grammars.
 banks. This works in much the same way as the extraction of context-free grammars from phrase structure treebanks (cf. Charniak 1996), except that the derivation trees of dependency trees are not immediately accessible in the treebank. We therefore present dency tree; from this derivation tree, the rules of the grammar can be extracted in a straightforward way. The algorithm was originally published by Kuhlmann and Satta (2009). It produces a restricted type of lexicalized LCFRS that we call  X  X anonical. X  In
Section 5 we provide a declarative characterization of this class of grammars, and show it induces the same set of dependency trees.
 the polynomial depends on two grammar-specific measures called fan-out and rank. hard problem. It is important therefore to keep the fan-out and the rank of a grammar the development of techniques that optimize parsing complexity in various scenarios and Crescenzi et al. 2011).

Section 7 we present the structural correspondent of the fan-out of a lexicalized LCFRS, a measure called block-degree (or gap-degree) (Holan et al. 1998). Although there is linguistic analysis, we provide evidence from several dependency treebanks showing
In Section 8 we study a second constraint on non-projectivity called well-nestedness (Bodirsky, Kuhlmann, and M  X  ohl 2005), and show that its presence facilitates tractable parsing. This comes at the cost of a small loss in coverage on treebank data. Bounded dependency grammars that can be parsed in polynomial time.
 2. Technical Background We assume basic familiarity with linear context-free rewriting systems (see, e.g., Vijay-notation that we use in this article.
 production rules, and S  X  N is a distinguished start symbol. Rules take the form where f is a function symbol and the A i are nonterminals. Rules are used for rewriting yield, more specifically a tuple of strings. For this, every function symbol f comes with a yield function that specifies how to compute the yield of a term f ( t yields of its subterms t i . Yield functions are defined by equations occurrence of each variable. For a yield function f defined by an equation of this form, we say that f is of type k 1  X  X  X  k m  X  k 0 , denoted by f : k fan-out  X  ( A )  X  1, and it is required that for every rule (1),
The rank and the fan-out of an LCFRS are the maximal rank and fan-out of its yield functions.
 Example 1
Figure 2 shows an example of an LCFRS for the language { a 358 illustrated in the right column of Figure 2. In this notation, to save some subscripts, we use the following shorthands for variables: x and x 1 for x y and y 1 for x 2,1 ; y 2 for x 2,2 ; y 3 for x 2,3 . 3. Lexicalized LCFRSs as Dependency Grammars
Recall the following examples for verb X  X rgument dependencies in German and Dutch from Section 1: (iii) dass Jan 1 Piet 2 Marie 3 lesen 3 helfen 2 sah 1 (German) (iv) dat Jan 1 Piet 2 Marie 3 zag 1 helpen 2 lezen 3 (Dutch) Figure 3 shows the production rules of two linear context-free rewriting systems (one for
German, one for Dutch) that generate these examples. The grammars are lexicalized in yield functions can be read as dependency rules. For example, the rules can be read as stating that the verb to see requires two dependents, one noun (N) and strings: Each parent X  X hild relation in the term represents a dependency between the associated lexical items (cf. Rambow and Joshi 1997). Thus every lexicalized LCFRS can be reinterpreted as a dependency grammar. To illustrate the idea, Figure 4 shows (the tree representations of) two terms generated by the grammars G the dependency trees induced by them. Note that these are the same trees that we gave for (iii) and (iv) in Figure 1.
 operations on dependency trees.
 3.1 Dependency Trees
By a dependency tree, we mean a pair ( w , D ), where w is a tuple of strings, and D is a tree-shaped graph whose nodes correspond to the occurrences of symbols in w ,and whose edges represent dependency relations between these occurrences. We identify that component. We can then formally define a dependency graph foratupleof strings as a directed graph G = ( V , E ) where
We u s e u and v as variables for nodes, and denote edges ( u , v )as u 1  X  j  X | w | .
 Example 2
Figure 5 shows examples of dependency trees. In pictures of such structures we use dashed boxes to group nodes that correspond to occurrences from the same tuple 360 component; however, we usually omit the box when there is only one component. Writing D i as D i = ( V i , E i ) we have: relations between their nodes. In particular, for a node u ,thesetof descendants of u , which we denote by u , is the set of nodes that can be reached from u by following a precedes the node v when reading the yield from left to right. Formally, precedence is the lexicographical order on occurrences: 3.2 Operations on Dependency Trees lexicalized yield function f we associate an operation f on dependency trees as follows.
Let w 1 , ... , w m , w be tuples of strings such that occurrence u in an input tuple w i corresponds to exactly one occurrence in the output hasanedge  X  u  X   X  v whenever there is an edge u  X  v in some D not correspond to any occurrence in some w i ; this is the occurrence of the anchor of f .
Let D be the dependency tree for w that is obtained by adding to the graph G all edges of the form r  X   X  r i , where r i is the root node of D i of the anchor becomes the root node of D , and the root nodes of the input dependency trees D i become its dependents. We then define Example 3 We consider a concrete application of an operation on dependency trees, illustrated in
Figure 6. In this example we have and the dependency trees D 1 , D 2 are defined as We show that f (( w 1 , D 1 ), ( w 2 , D 2 )) = ( w , D ), where D = ( V , E )with
The correspondences between the occurrences u in the input tuples and the occur-rences  X  u in the output tuple are as follows: dependency graph G = ( V , E )for w , where
The occurrence r of the anchor b of f in w is (1, 2); the nodes of G that correspond to the root nodes of D 1 and D 2 are  X  r 1 = (1, 1) and  X  r obtained by adding the edges r  X   X  r 1 and r  X   X  r 2 to G . 4. Extraction of Dependency Grammars context-free grammars from phrase structure treebanks (Charniak 1996).
 for the extraction of range concatenation grammars from discontinuous constituent structures, due to Maier and S X gaard (2008). To simplify our presentation we restrict our attention to treebanks containing simple dependency trees. 362 a term t over yield functions that induces ( w , D ). Then we collect a set of production rules, one rule for each node of the construction trees. As an example, consider Fig-ure 7, which shows a dependency tree with one of its construction trees. (The analysis nodes.
Rules like these can serve as the starting point for practical systems for data-driven, non-projective dependency parsing (Maier and Kallmeyer 2010).
 lem that we focus on in this section is how to obtain these trees in the first place. Our procedure for computing construction trees is based on the concept of  X  X locks. X  4.1 Blocks
Let D be a dependency tree. A segment of D is a contiguous, non-empty sequence precedence order. For a node u of D ,a block of u is a longest segment consisting of in its component, or is preceded by a node that is not a descendant of u . A symmetric property holds for the right endpoint.
 Example 4
Consider the node 2 of the dependency tree in Figure 7. The descendants of 2 fall into two blocks, marked by the dashed boxes: 1 2 and 5 6 7. precedes the left endpoint of v . 4.2 Computing Canonical Construction Trees of u , in the order of their precedence, and let w 1 , ... , w the children of u . We may view blocks as strings of nodes. Taking this view, we compute the (unique) yield function g with the property that of u , the variables in the template of g represent the blocks of these children, and the components of the template represent the blocks of u .Toobtain f , we take the template of g and replace the occurrence of u with the corresponding lexical item. Example 5
Node 2 of the dependency tree shown in Figure 7 has two children, 1 and 5. We have w = 12,567 w 1 = 1 w 2 = 567 g = x 2, y f = x hearing , y specific choice of this order. In the following we assume that children are ordered from left to right based on the position of their leftmost descendants. 4.3 Computing the Blocks of a Dependency Tree
The algorithmically most interesting part of our extraction procedure is the computation of the yield function g . The template of g is uniquely determined by the left-to-right sequence of the endpoints of the blocks of u and its children. An efficient algorithm that can be used to compute these sequences is given in Table 1. 4.3.1 Description. We start at a virtual root node  X  (line 1) which serves as the parent of the real root node. For each node next in the precedence order of D , we follow the shortest path from the current node current to next . To determine this path, we compute the lowest common ancestor lca of the two nodes (lines 4 X 5), using a set of markings current (including the virtual root node  X  ) are marked; therefore, we find lca by going upwards from next to the first node that is marked. To restore the loop invariant, we then unmark all nodes on the path from current to lca (lines 6 X 9). Each time we move down from a node to one of its children (line 12), we record the information that next is the left endpoint of a block of current . Symmetrically, each time we move up from a node to its parent (lines 8 and 17), we record the information that next endpoint of a block of current .The while loop in lines 15 X 18 takes us from the last node of the dependency tree back to the node  X  . 364
Because each iteration of loop 2 and loop 4 determines the right endpoint of a block, we stack in loop 1 is popped again in loop 3; therefore, n 1
Note that this runtime is asymptotically optimal for the task we are considering. 5. Canonical Grammars declarative characterization of these grammars, and show that every lexicalized LCFRS is equivalent to a canonical one. 5.1 Definition of Canonical Grammars
We are interested in a syntactic characterization of the yield functions that can occur yield function components in the template of f represent the blocks of a node u , and the variables in the template represent the blocks of the children of u . For a variable x argument index and j the component index of the variable.
 Property 1
For all 1  X  i 1 , i 2  X  m ,if i 1 &lt; i 2 then x i to right based on the position of their leftmost descendants. A variable with argument function that does not have Property 1 is x 2,1 x 1,1 , which defines a kind of  X  X everse concatenation operation. X  Property 2
For all 1  X  i  X  m and 1  X  j 1 , j 2  X  k i ,if j 1 &lt; j 2
This property reflects that, in our extraction procedure, the variable x based on their precedence. An example of a yield function that violates the property is x 1,2 x 1,1 , which defines a kind of swapping operation . In the literature on LCFRSs (Michaelis 2001; Kracht 2003), ordered (Villemonte de la Clergerie 2002; Kallmeyer 2010), and non-permuting (Kanazawa 2009).
 Property 3 No component  X  h is the empty string.
 has been discussed for multiple context-free grammars (Seki et al. 1991, Property N3 in Lemma 2.2) and range concatenation grammars (Boullier 1998, Section 5.1). For our extracted grammars it holds because each component  X  h represents a block, and blocks are always non-empty.
 Property 4
No component  X  h contains a substring of the form x i , j
This property, which does not seem to have been discussed in the literature before, is a reflection of the facts that variables with the same argument index represent blocks of the same child node, and that these blocks are longest segments of descendants. all of its yield functions are canonical.
 Lemma 1
A lexicalized LCFRS is canonical if and only if it can be extracted from a dependency treebank using the technique presented in Section 4.
 Proof
We have already argued for the  X  X nly if X  part of the claim. To prove the  X  X f X  part, it 366 a dependency tree such that the construction tree extracted for this dependency tree contains f . This is an easy exercise.
 extracted from constituency treebanks using the technique by Maier and S X gaard (2008). 5.2 Equivalence Between General and Canonical Grammars
Two lexicalized LCFRSs are called strongly equivalent if they induce the same set of dependency trees. We show the following equivalence result: Lemma 2 For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized LCFRS G such that G is canonical.
 Proof grammars: Michaelis (2001, Section 2.4) provides a construction that transforms a mul-tiple context-free grammar into a weakly equivalent multiple context-free grammar in sponding construction for Property 3. Whereas both constructions are only quoted to grammar is a lexicalized LCFRS, they also preserve the set of induced dependency trees.
To complete the proof of Lemma 2, we show that every lexicalized LCFRS can be cast into normal forms that satisfy Property 1 and Property 4. It is not hard then to combine the four constructions into a single one that simultaneously establishes all properties of canonical yield functions.
 Lemma 3 For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized LCFRS G such that G only contains yield functions which satisfy Property 1. Proof arguments of yield functions. To establish it, we determine, for every yield function f , a permutation  X  that renames the argument indices of the variables occurring in the template of f in such a way that the template meets Property 1. This renaming gives rise to a modified yield function f  X  . We then replace every rule A modified rule A  X  f  X  ( A  X  (1) , ... , A  X  ( m ) ).
 Lemma 4 For every lexicalized LCFRS G one can construct a strongly equivalent lexicalized LCFRS G such that G only contains yield functions which satisfy Property 4. Proof
The idea behind our construction of the grammar G is perhaps best illustrated by an example. Imagine that the grammar G generates the term t shown in Figure 8a. The yield contains the offending substring x 2 x 3 .Wesetup G in such a way that instead of t it generates the term t shown in Figure 8b in which f 1 is replaced with the yield function f = x 1 cx 2 .Toobtain f 1 from f 1 we reduce the offending substring x variable x 2 . In order to ensure that t and t induce the same dependency tree (shown in
Figure 8c), we then adapt the function f 2 = x 1 b , y , x
Dual to the reduction, we replace the two-component sequence y , x with the single component yx 2 ; in this way we get f 2 = x recursive algorithm to compute the rules of the grammar G . Such an algorithm is given in Table 2. For every rule A  X  f ( A 1 , ... , A m )of G we construct new rules where g and the g i are yield functions encoding adaptation operations. As an example, the adaptation of the function f 2 in the term t may be encoded into the adaptor function x , x 2 x 3 . The function f 2 can then be written as the composition of this function and f
The yield function f and the adaptor functions g i are computed based on the template of the g -adapted yield function f , that is, the composed function g the adapted function g  X  f by  X  .An i-block of  X  is a maximal, non-empty substring of some component of  X  that consists of variables with argument index i . To compute the template of g i we read the i -blocks of  X  from left to right and rename the variables by changing their argument indices from i to 1. To compute the template of f we take the 368 template  X  and replace the j th i -block with the variable x and component indices j .
 pairs also constitute the nonterminals of the new grammar G . The fan-out of a non-terminal is the fan-out of g . The agenda is initialized with the pair ( S , x ) where x the algorithm terminates, one may observe that the fan-out of every nonterminal ( A , g ) added to the agenda is upper-bounded by the fan-out of A . Hence, there are only finitely many pairs ( A , g ) that may occur in the chart, and a finite number of iterations of the while -loop.
 be careful about the order in which the individual constructions (for Properties 1 X 4) are combined. One order that works is 6. Parsing and Recognition
Lexicalized linear context-free rewriting systems are able to account for arbitrarily non-projective dependency trees. This expressiveness comes with a price: In this section we the class of grammars. 6.1 Parsing Algorithm
To ground our discussion of parsing complexity, we present a simple bottom X  X p parsing algorithm for LCFRSs, specified as a grammatical deduction system (Shieber, Schabes, and Pereira 1995). Several similar algorithms have been described in the literature (Seki et al. 1991; Bertsch and Nederhof 2001; Kallmeyer 2010). We assume that we are given a grammar G = ( N ,  X  , P , S )andastring w = a 1  X  X  X  a n  X 
Item form. The items of the deduction system take the form where A  X  N with  X  ( A ) = k , and the remaining components are indices identifying the left and right endpoints of pairwise non-overlapping substrings of w . More formally, 0  X  l interpretation of an item of this form is that A derives a term t specified substrings of w ,thatis, from the start symbol S and yields the full string w .
Inference rules. The inference rules of the deduction system are defined based on the rules in P . Each production rule is converted into a set of inference rules of the form
Each such rule is subject to the following constraints. Let 1 and 1  X  j  X  k i . We write  X  ( l , v ) = r to assert that r = l + of w between indices l and r .
 inference rule can be combined into the substrings corresponding to the conclusion by means of the yield function f .
 using standard dynamic programming techniques. This parser will compute a packed string w . Such a packed representation is often called a shared forest (Lang 1994). In combination with appropriate semirings, the shared forest is useful for many tasks in syntactic analysis and machine learning (Goodman 1999; Li and Eisner 2009). 6.2 Parsing Complexity
We are interested in an upper bound on the runtime of the tabular parser that we have just presented. We can see that the parser runs in time O ( the size of some suitable representation of the grammar G ,and c denotes the maximal to this value as the parsing complexity of f (cf. Gildea 2010). Then to show an upper bound on c it suffices to show an upper bound on the parsing complexities of the yield functions that the parser has to handle. An obvious such upper bound is
Here we imagine that we could choose each endpoint in Equation (3) independently of all the others. By virtue of the constraints, however, some of the endpoints cannot be chosen freely; in particular, some of the substrings may be adjacent. In general, to show 370 an upper bound c ( f )  X  b we specify a strategy for choosing b endpoints, and then argue that, given the constraints, these choices determine the remaining endpoints. Lemma 5
For a yield function f : k 1  X  X  X  k m  X  k we have Proof
We adopt the following strategy for choosing endpoints: For 1 value of l h . Then, for 1  X  i  X  m and 1  X  j  X  k i , choose the value of r endpoint l i , j will be shared either with the left endpoint l constraint c2), or with some right endpoint r i , j (by constraint c4). 6.3 Universal Recognition
The runtime of our parsing algorithm for LCFRSs is exponential in both the rank and the fan-out of the input grammar. One may wonder whether there are parsing algorithms that can be substantially faster. We now show that the answer to this question is likely to be negative even if we restrict ourselves to canonical lexicalized LCFRSs. To this end we study the universal recognition problem for this class of grammars.
 whether G yields w . A straightforward algorithm for solving this problem is to first compute the shared forest for G and w , and to return  X  X es X  if and only if the shared forests can be decided in linear time and space with respect to the size of the forest.
Therefore, the computational complexity of universal recognition is upper-bounded by the complexity of constructing the shared forest. Conversely, parsing cannot be faster than universal recognition.
 canonical lexicalized LCFRSs is NP-complete unless we restrict ourselves to a class of grammars where both the fan-out and the rank of the yield functions are bounded by constants. Lemma 6, which shows that the universal recognition problem of lexicalized
LCFRSs is in NP, distinguishes lexicalized LCFRSs from general LCFRSs, for which the universal recognition problem is known to be PSPACE-complete (Kaji et al. 1992). The crucial difference between general and lexicalized LCFRSs is the fact that in the latter, the size of the generated terms is bounded by the length of the input string. Lemma 7 stronger versions of the corresponding results for general LCFRSs presented by Satta (1992), and are proved using similar reductions. They show that the hardness results hold under significant restrictions of the formalism: to lexicalized form and to canonical yield functions. Note that, whereas in Section 5.2 we have shown that every lexicalized
LCFRS is equivalent to a canonical one, the normal form transformation increases the size of the original grammar by a factor that is at least exponential in the fan-out. Lemma 6
The universal recognition problem of lexicalized LCFRSs is in NP. Proof
Let G be a lexicalized LCFRS, and let w be a string. To test whether G yields w ,we guess a term t  X  T ( G ) and check whether t yields w .Let string representation of t . Since the yield functions of G are lexicalized, that we have
Using a simple tabular algorithm, we can verify in time O (
O ( | w || G | ). Thus we have a nondeterministic polynomial-time decider for the universal recognition problem.
 to be NP-complete. An instance of 3SAT is a Boolean formula  X  in conjunctive normal form where each clause contains exactly three literals, which may be either variables or negated variables. We write m for the number of distinct variables that occur in  X  ,and n for the number of clauses. In the proofs the index i will always range over values from 1 to m , and the index j will range over values from 1 to n .
 is severely restricted, however, and each of our grammars can be transformed into a proper lexicalized LCFRS without affecting the correctness or polynomial size of the reductions.
 Lemma 7
The universal recognition problem for canonical lexicalized LCFRSs with unbounded fan-out and rank 1 is NP-hard.
 Proof
To prove this claim, we provide a polynomial-time reduction of 3SAT. The basic idea is to use the derivations of the grammar to guess truth assignments for the variables, and to use the feature of unbounded fan-out to ensure that the truth assignment satisfies all clauses.
 string w as follows. Let M denote the m  X  n matrix with entries M entries in the same row share the same variable, and entries in the same column share the same clause. We set up G in such a way that each of its derivations simulates a row-wise iteration over M . Before visiting a new row, the derivation chooses a truth value for the corresponding variable, and sticks to that choice until the end of the row. The string w takes the form
This string is built up during the iteration over M in a column-wise fashion, where each column corresponds to one component of a tuple with fan-out n . More specifically, for each entry ( v i , c j ), the derivation generates one of two strings, denoted by  X  372
The string  X  i , j is generated only if v i can be used to satisfy c we will be able to construct a derivation of G that yields w .
 consider the grammar fragment in Figure 9. Each of the rules specifies one possible step of the iteration for the pair ( v i , c j ) under the truth assignment v hand side F i , j (not shown here) specify possible steps under the assignment v Lemma 8
The universal recognition problem for canonical lexicalized LCFRSs with unbounded rank and fan-out 2 is NP-hard.
 Proof
We provide another polynomial-time reduction of 3SAT to a grammar G and a string w , again based on the matrix M mentioned in the previous proof. Also as in the previous reduction, we set up the grammar G to simulate a row-wise iteration over M .Themajor derivation, but during mn rather short fan-out 2 subderivations. The string w is substrings of w . The right component of the tuple consists of one the two strings  X  and  X   X  i , j mentioned previously. As before, the string  X  used to satisfy c j under the hypothesized truth assignment. The left component consists of one of two strings, denoted by  X  i , j and  X   X  i , j
These strings are generated to represent the truth assignments v respectively. By this construction, each substring w , i can be derived in exactly one of two ways, ensuring a consistent truth assignment for all subderivations that are linked to the same variable v i .
 the start symbol S ; this rule sets up the general topology of w .Let I be the m where the argument index i is taken from a row-wise reading of the matrix I ;inthis case, the argument indices in x will simply go up from 1 to mn .Nowdefine x sequence of variables of the form x h ,2 , where h is taken from a column-wise reading of the matrix I . Then S can be expanded with the rule
Note that there is one nonterminal V i , j for each variable X  X lause pair ( v terminals can be rewritten using the following rules:
The remaining rules rewrite the nonterminals T i , j and F
It is not hard to see that both G and w can be constructed in polynomial time. 7. Block-Degree
To obtain efficient parsing, we would like to have grammars with as low a fan-out as possible. Therefore it is interesting to know how low we can go without losing too much coverage. In lexicalized LCFRSs extracted from dependency treebanks, the fan-out of a grammar has a structural correspondence in the maximal number of blocks per subtree, a measure known as  X  X lock-degree. X  In this section we formally define block-degree, and evaluate grammar coverage under different bounds on this measure. 7.1 Definition of Block-Degree
Recall the concept of  X  X locks X  that was defined in Section 4.2. The block-degree of a node u of a dependency tree D is the number of distinct blocks of u . The block-degree of D is the maximal block-degree of its nodes. 2 Example 6
Figure 10 shows two non-projective dependency trees. For D descendants of 2 fall into two blocks, marked by the dashed boxes. Because this is the maximal number of blocks per node in D 1 , the block-degree of D verify that the block-degree of the dependency tree D 2 is 3. 374 projective dependency tree, a subtree may span over several, discontinuous substrings. 7.2 Computing the Block-Degrees
Using a straightforward extension of the algorithm in Table 1, the block-degrees of all number of blocks. To compute the block-degree of D , we simply take the maximum over the degrees of each node. We can also adapt this procedure to test whether D is more than one block. The runtime of this test is linear in the number of nodes of D . 7.3 Block-Degree in Extracted Grammars correspondence between the blocks of a node u and the components of the template the treebank translates into a bound on the fan-out of the extracted grammar. This has consequences for the generative capacity of the grammars: As Seki et al. (1991) show, generated by the class of LCFRSs with fan-out k  X  1.
 grammars (Properties 3 and 4), and does not hold for non-canonical lexicalized LCFRSs.
 Example 7
The following term induces a two-node dependency tree with block-degree 1, but in this term violate both Property 3 and Property 4. 7.4 Coverage on Dependency Treebanks
In order to assess the consequences of different bounds on the fan-out, we now evaluate the block-degree of dependency trees in real-world data. Specifically, we look into five dependency treebanks used in the 2006 CoNLL shared task on dependency parsing (Buchholz and Marsi 2006): the Prague Arabic Dependency Treebank (Haji  X  c et al. 2004), the Prague Dependency Treebank of Czech (B  X  ohmov  X  a et al. 2003), the Danish Depen-2006), and the Metu-Sabanc X  treebank of Turkish (Oflazer et al. 2003). The full data used in the CoNLL shared task also included treebanks that were produced by conversion findings. Here, we consider only genuine dependency treebanks. More specifically, our similar results on other data sets, see Kuhlmann and Nivre (2006), Havelka (2007), and Maier and Lichte (2011).
 extracted from that treebank, as well as the number of corresponding dependency trees.
We then list the number of rules that we lose if we restrict ourselves to rules with fan-out = 1, or rules with fan-out  X  2, as well as the number of dependency trees that we from different trees, or from different nodes in the same tree.
 in the treebanks. This loss is quite substantial. If we instead put the bound at fan-out  X  2, then rule loss is reduced by between 94.16% (Turkish) and 99.76% (Arabic), and tree loss is reduced by between 94.31% (Turkish) and 99.39% (Arabic). This outcome
Czech. Here we find that, if we are ready to accept a loss of as little as 0.02% of the rules extracted from the Prague Dependency Treebank, and up to 0.5% of the trees, then such an upper bound can be set at a block-degree as low as 2. 8. Well-Nestedness grammars. In this section we study  X  X ell-nestedness, X  another restriction on the non-projectivity of dependency trees, and show how enforcing this constraint allows us to restrict our attention to the class of LCFRSs with rank 2.
 376 8.1 Definition of Well-Nestedness
Let D be a dependency tree, and let u and v be nodes of D . The descendants of u and v overlap , denoted by u v , if there exist nodes u such that
A dependency tree D is called well-nested if for all pairs of nodes u , v of D
In other words, u and v may overlap only if u is an ancestor of v ,or v is an ancestor of u . If this implication does not hold, then D is called ill-nested . Example 8
Figure 11 shows three non-projective dependency trees. Both D
D 1 does not contain any overlapping sets of descendants at all. In D and 2 overlap, it is also the case that 1  X  2 . In contrast, D Lemma 9 u , u 2 of u and v 1 , v 2 of v such that Proof Let D be a dependency tree. Suppose that D contains a configuration of the form (4).
This configuration witnesses that the sets u and v overlap. Because u , v are siblings, u  X  v =  X  . Therefore we conclude that D is ill-nested. Conversely now, suppose that D is ill-nested. In this case, there exist two nodes u and v such that
Here, we may assume u and v to be siblings: otherwise, we may replace either u or v with its parent node, and property (  X  ) will continue to hold. Because u v , there exist descendants u l , u r  X  u and v l , v r  X  v such that
Without loss of generality, assume that we have the first case. The nodes u to different blocks of u ,say u 1 and u 2 ; and the nodes v of v ,say v 1 and v 2 . Then it is not hard to verify Equation (4).
 every node has exactly one block, so configuration (4) is impossible. For every k &gt; 1, there are both well-nested and ill-nested dependency trees with block-degree k . 8.2 Testing for Well-Nestedness
Based on Lemma 9, testing whether a dependency tree D is well-nested can be done in time linear in the number of blocks in D using a simple subsequence test as follows. We run the algorithm given in Table 1, maintaining a stack s [ u ] for every node u .Thefirst time we make a down step to u ,wepush u to the stack for the parent of u ; every other stack becomes empty. In the latter case, we terminate the computation and report that D empty, we report that D is well-nested.
 when making a down step to some child v of p . In this case, the node v must have been popped from s [ p ] when making a down step to some other child u of p , and that child must have already been on the stack before the first down step to v . This witnesses the existence of a configuration of the form in Equation (4). 8.3 Well-Nestedness in Extracted Grammars Just like block-degree, well-nestedness can be characterized in terms of yield functions.
Recall the notation x &lt; f y from Section 5.1. A yield function is ill-nested if there are argument indices 1  X  i 1 , i indices 1  X  j 1 , j 1  X  k i
Otherwise, we say that f is well-nested . As an immediate consequence of Lemma 9, a restriction to well-nested dependency trees translates into a restriction to well-nested
Kanazawa (2009) calls  X  X ell-nested multiple context-free grammars. X  378 have a number of interesting properties that set them apart from general LCFRSs; in particular, they have a standard pumping lemma (Kanazawa 2009). The yield languages generated by well-nested multiple context-free grammars form a proper subhierarchy within the languages generated by general LCFRSs (Kanazawa and Salvati 2010). Per-haps the most prominent subclass of well-nested LCFRSs is the class of tree-adjoining grammars (Joshi and Schabes 1997).

For non-canonical grammars, syntactic well-nestedness alone does not imply structural well-nestedness, nor the other way around. 8.4 Coverage on Dependency Treebanks
To estimate the coverage of well-nested grammars, we extend the evaluation presented in Section 7.4. Table 4 shows how many rules and trees in the five dependency treebanks we lose if we restrict ourselves to well-nested yield functions with fan-out requirement, rule loss is still reduced by between 92.65% (Turkish) and 99.51% (Arabic) when compared to the fan-out = 1 baseline. 8.5 Binarization of Well-Nested Grammars Our main interest in well-nestedness comes from the following: Lemma 10
The universal recognition problem for well-nested lexicalized LCFRS with fan-out k and unbounded rank can be decided in time nested lexicalized LCFRSs. In the context of LCFRSs, a binarization is a procedure for algorithm, is essential for achieving efficient recognition algorithms, in particular the usual cubic-time algorithms for context-free grammars. Note that our binarization only preserves weak equivalence; in effect, it reduces the universal recognition problem for well-nested lexicalized LCFRSs to the corresponding problem for well-nested LCFRSs with rank 2. Many interesting semiring computations on the original grammar can be nested dependency trees has been presented by G  X  omez-Rodr  X   X guez, Carroll, and Weir (2011).
 by G  X  omez-Rodr  X   X guez, Kuhlmann, and Satta (2010). They show that every well-nested
LCFRS can be transformed (at the cost of a linear size increase) into a weakly equivalent functions of one of two types:
A concatenation function takes a k 1 -tuple and a k 2 -tuple and returns the ( k tuple that is obtained by concatenating the two arguments. The simplest concatenation function is the standard concatenation operation xy . We will write conc : k to a concatenation function of the type given in Equation (6). By counting endpoints, we see that the parsing complexity of concatenation functions is
A wrapping function takes a k 1 -tuple (for some k 1  X  2) and a k ( k + k 2  X  2)-tuple that is obtained by  X  X rapping X  the first argument around the second argument, filling some gap in the former. The simplest function of this type is x which wraps a 2-tuple around a 1-tuple. We write wrap : k function of the type given in Equation (7). The parsing complexity is
The constants of the binarized grammar have the form  X  ,  X  ,  X  ,and a , where a is the anchor of some yield function of the original grammar. parsing complexity of the binarized grammar. Because the binarization preserves the catenation functions conc : k 1 k 2 we have k 1 + k 2  X  achieved by wrapping operations. This gives the bound stated in Lemma 10. 380 8.5.2 Binarization. We now turn to the actual binarization. Consider a rule decompose this rule into up to three rules as follows. We match the template of f against one of three cases, shown schematically in Figure 12. In each case we select a concatenation or wrapping function f (shown in functions f 1 and f 2 , respectively. In Figure 12, f 1 is drawn shaded, and f shaded. 4 The split of f partitions the variables that occur in the template, in the sense that if for some argument index 1  X  i  X  m , either f argument index i , then it contains all such variables. The two sequences by collecting the nonterminal A i if the variables with argument index i belong to the do not create rules for f 1 and f 2 if they are identity functions.
 Example 9 We illustrate the binarization by showing how to transform the rule variable x 3 . We therefore split the template into two smaller parts x y , y 2 , y 3 . The function y 1 , y 2 , y 3 is an identity. We therefore create two rules:
Note that the index j for the wrapping function was chosen to be j = 2 because there were more component boundaries between x 2 and x 3 than between x template x 1 ax 2 , x 3 requires further decomposition according to Case 3. This time, the two smaller parts are the identity function x 1 , x 2 , x create the following rules:
At this point, the transformation ends. 8.5.3 Correctness. We need to show that the fan-out of the binarized grammar does not exceed the fan-out of the original grammar. We reason as follows. Starting from some three functions by h , h 1 , h 2 , respectively. We have
From Equation (8) it is clear that in Case 1 and Case 2, both h that h 2  X  h . However, h 1 is upper-bounded by h only if h be greater than h . As an example, consider the decomposition of x the wrapping function x 1 , x 2 (fan-out 2) and the constant a (fan-out 1). But because in Case 3 the index j is chosen to maximize the number of component boundaries between the variables x i , j and x i , j + 1 , the assumption h components of f 1 contains at least one variable with argument index i  X  X f there were 382 a component without such a variable, then the two variables that surrounded that com-ponent would have given rise to a different choice of j . Hence we deduce that h 9. Conclusion
In this article, we have presented a formalism for non-projective dependency grammar based on linear context-free rewriting systems, along with a technique for extracting grammars from dependency treebanks. We have shown that parsing with the full class of these grammars is intractable. Therefore, we have investigated two constraints on the non-projectivity of dependency trees, block-degree and well-nestedness. Jointly, these two constraints define a class of  X  X ildly X  non-projective dependency grammars that can be parsed in polynomial time.
 to the structural properties of the dependency structures that it induces. Although we have used this relation to identify a class of dependency grammars that can be parsed in polynomial time, it also provides us with a new perspective on the question about the descriptive adequacy of a grammar formalism. This question has traditionally been
Huybregts 1984; Shieber 1985). A notion of generative capacity based on dependency trees makes a useful addition to this discussion, in particular when comparing formalisms for which no common concept of strong generative capacity exists. As an example for a result in this direction, see Koller and Kuhlmann (2009). compositional mapping on the derivations. While we would claim that compositionality
In particular, our interpretation of derivations may not always be in line with how the grammar producing these derivations is actually used . One formalism for which such a mismatch between derivation trees and dependency trees has been pointed out is tree-adjoining grammar (Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998). Resolving this mismatch provides an interesting line of future work.
 degree and well-nestedness. Each of our dependency grammars is restricted to a finite block-degree. As a consequence of this restriction, our dependency grammars are not expressive enough to capture linguistic phenomena that require unlimited degrees of non-projectivity, such as the  X  X crambling X  in German subordinate clauses (Becker, on the block-degree of dependency trees, perhaps for some performance-based reason, is open. Likewise, it is not clear whether well-nestedness is a  X  X atural X  constraint on dependency analyses (Chen-Main and Joshi 2010; Maier and Lichte 2011).
 retical nature, some of them have found their way into practical systems. In particular, the extraction technique from Section 4 is used by the data-driven dependency parser of Maier and Kallmeyer (2010).
 Acknowledgments 384 386
