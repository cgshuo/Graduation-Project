 A.1. Privacy Guarantee We restate a version of the privacy theorem by (Gupta et al., 2011) in the context of this paper.
 Theorem 9 (Theorem 4.1 from Gupta et al. (2011)) . Let T be the total number of queries and B be the number of updates allowed in Algorithm 1, let 0 = maximum change in the output of a query (using w  X  ) bitrarily modified. Let ( , X  ) be the privacy parameters and  X  be the failure probability in Algorithm 1. Under We now provide privacy proof of our PINP algorithm (Algorithm 1).
 Proof of Theorem 2. The proof proceeds in two stages. In the first stage, we show that prediction func-tion is relatively insensitive to change in the dataset. where z  X  X and G , G 0 are two datasets differing in optimal solution to regularized ERM (2) when the un-derlying datasets are G and G 0 , respectively. In the sec-ond stage, we invoke Theorem 9 with sensitive bound W.l.o.g. we can assume that the datasets G and G 0 ( x for (2) (with dataset G and G 0 respectively) and strong convexity of the ERM (2): 1 n  X  Hence, 1 n
X  X  Adding the above two equations and using Lipschitz continuity of ` : Finally, using Cauchy-Schwarz inequality and the above inequality, we have, With this bound in hand, we invoke Theorem 9 (The-orem 4.1 by (Gupta et al., 2011)) to complete the proof.
 A.2. Utility Guarantee In the following we restate a version of Theorem 5.2 from (Gupta et al., 2011) in the context of this paper. Setting the parameters as in Theorem 3 gives us the desired utility guarantee.
 Theorem 10 (Theorem 5.2 from Gupta et al. (2011)) . Let T be the total number of queries and B be the number of updates allowed in Algorithm 1, let 0 = maximum change in the output of a query (using w  X  ) bitrarily modified. Let ( , X  ) be the privacy parameters and  X  be the failure probability in Algorithm 1. As long as the variable counter in Algorithm 1 is less than B , following is true. |  X  v t  X  X   X  ( z t ) , w  X   X  X  = O B.1. Privacy Guarantee of Test Data Proof of Theorem 4. From (3), we know that for any two training data sets G and G 0 differing in exactly one entry, the following is true: Therefore by Cauchy-Schwarz inequality, for any z  X  X we have Theorem now follows by using the above given bound with the following composition theorem.
 Theorem 11 (Composition Theorem from (Dwork et al., 2010)) . Let 0 , X  0 &gt; 0 . The class of -differentially private mechanisms satisfy ( 0 , X  0 differential privacy under k -fold adaptive composition for: B.2. Utility Guarantee of Test Data Proof of Theorem 5. Let, Since  X  w = arg min the following holds: X .
 Let b =  X  b 1 ,  X  X  X  ,b T  X  . Using Cauchy-Schwarz inequal-ity and the fact that k v k 1  X  Since  X  is the scaling parameter for the Laplace dis-tribution from which each b t are drawn, therefore by the tail property of Laplace distribution it follows that w.p.  X  1  X   X  , X Plugging in the value of  X  = O LR  X  2 have
X g ( w ; z t ) is a convex cost functions in w . Now, using Theorem 1 from (Shalev-Shwartz et al., 2009) (stated below) we obtain the following:.
 Therefore, using (4) and (5), we get (w.p.  X  1  X   X  ): E z  X  X  [ g (  X  w ; z )]  X  where C 1 ,C 2 &gt; 0 are global constants.
 Theorem now follows by setting T as mentioned in the theorem along with using Lipschitz property of ` . Theorem 12 (Theorem 1 from (Shalev-Shwartz et al., has L 2 -norm of at most R  X  , and let f : R  X X  X  R its first parameter. Then for any P over the domain following is true with probability at least 1  X   X  . B.3. Generalization Bound for Test Data Theorem 13 (Error Bound over Test Distribution) . Let P be a fixed test distribution and let Z = { z 1 ,  X  X  X  , z T } be sampled uniformly from P . If T = w.p. 1  X   X  , Proof sketch of Theorem 7. For a given dataset G , let G and G 0 differing in exactly one entry (see Theorem 2 from Section 5), it directly follows that | f ( G )  X  f ( G . Hence, it follows that each iteration of Line 3 in Algorithm 3 is 0 / 2-differentially private. Now from the analysis of Theorem 2 (from Section 5), it follows that Algorithm 3 is ( , X  )-differentially private. Proof of Theorem 8. Intuition: The proof of this theorem goes via the following key insight: if we can make almost every round of Algorithm 3 an update round, then the iterates w t will become representative of w  X  as time t progresses. This can be formalized via a simple potential argument. (See (Gupta et al., 2011) for the exact formalization.) The way we ensure that each iteration is an update round is by finding a z (via exponential mechanism) such that it can distinguish value of  X   X  ( z ) ,  X  w  X  w  X   X  is greater than  X  4 ). Main Proof: We apply exponential mechanism to a  X  is as given in the Theorem. That is, we divide the entire space into (overlapping) L 2 balls of radius  X  and S is the collection of centers of all such balls. Also, it is known that | S | = 4  X  d .
 Now, using the exponential distribution specified in Step 3 of Algorithm 3, we get:
Pr[ z s.t. | X   X  ( z ) , w t  X  w  X   X  X  X  OPT  X   X   X  ]  X | S | e Now, let OPT  X  be the maximum value of OPT  X  = max z  X  X  | X   X  ( z ) , w t  X  w  X   X  X  . Also, k z  X  z k 2  X  2  X  where z  X  = arg max z  X  S | X   X  ( z ) , w t  X  w is the optimal over S . Hence, using Lipschitz continu-ity of the mapping  X  , we obtain a sample z w.p. at least 1  X   X  s.t.: | X   X  ( z ) , w t  X  w  X   X  X  X  OPT  X   X  Hence, selecting  X  = dR  X  Now, using Theorem 7.3 of (Gupta et al., 2011) (see Theorem 14), we get, | X   X  ( z ) , w t  X  w  X   X  X  X   X  for all z  X  R d and k z k 2  X  1. Hence, minimizing over  X  , we get | X   X  ( z ) , w t  X  w  X   X  X  for all z  X  R d and k z k 2  X  1. The theorem now follows using Lipschitz continuity of the loss function ` and using the bound k w  X  k 2  X  2 LR  X  / X  .
 Theorem 14 (Modified Theorem 7.3 from (Gupta Algorithm 3 outputs a z (with k z k 2  X  1 ) at each step t  X  { 1 ,  X  X  X  ,B } such that with prob-ability at least 1  X   X  (over all the B -steps), | X  w  X   X  w t , X  ( z )  X  X  = max the B -steps), | X   X  ( z ) , w t  X  w  X   X  X   X   X  , where  X  =
