 Named Entity Recognition (NER) is generally understood as the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations, and locations [Nadeau and Sekine 2007]. Proposed solutions to NER fall into three categories: (1) rule based [Krupka and Hausman 1998]; (2) machine learning based [Finkel and Manning 2009; Singh et al. 2010]; and (3) hybrid methods [Jansche and Abney 2002]. With the availability of annotated corpora, such as ACE05, Enron [Minkov et al. 2005], and CoNLL03 [Tjong Kim Sang and De Meulder 2003], the data-driven methods now become the dominating methods.

However, current NER mainly focuses on formal text such as news articles [Mccal-lum and Li 2003; Etzioni et al. 2005]. Exceptions include studies on informal text such as emails, blogs, and clinical notes [Wang 2009]. Because of the domain mismatch, current systems trained on nontweets perform poorly on tweets, a new genre of text, which are short, mostly informal, often ungrammatical and noise prone. For example, the average F1 of the Stanford NER [Finkel et al. 2005], which is trained on the CoNLL03 shared task dataset and achieves state-of-the-art performance on that task, drops from 90.8% [Ratinov and Roth 2009] to 45.8% on tweets from out dataset (described in Section 5.1).

Therefore, it is necessary to build a dedicated NER for tweets, which requires a lot of annotated tweets or rules given the diversified written styles of tweets. However, manually creating them is tedious and prohibitively unaffordable. Proposed solutions to alleviate this issue include: (1) domain adaption, which aims to reuse the knowledge of the source domain in a target domain. Two recent examples are Wu et al. [2009], which uses data that is informative about the target domain and also easy to be labeled to bridge the two domains, and Chiticariu et al. [2010], which introduces a high-level rule language, called NERL, to build the general and domain-specific NER systems; and (2) semisupervised learning, which aims to use the abundant unlabeled data to compensate for the lack of annotated data. Suzuki and Isozaki [2008] is one such example.
Another challenge is the limited information in a single tweet, which is a result of two factors: (1) the tweet X  X  informal nature, making conventional features such as Part-Of-Speech (POS) and capitalization not reliable. The performance of current NLP tools drops sharply on tweets. For example, OpenNLP 1 , the state-of-the-art POS tagger, gets only an accuracy of 74.0% on our test dataset (as apposed to an accuracy of more than 95.0% on the Penn Treebank corpus); and (2) the tweet X  X  short nature, leading to the excessive abbreviations in tweets, and the availability of very limited context informa-tion. Tackling this challenge, ideally, requires adapting related NLP tools to fit tweets, or normalizing tweets to accommodate existing tools, both of which are hard tasks.
We propose a novel NER system to address these challenges. Firstly, a linear discrim-inative tweet normalization model is used to correct ill-formed words (e.g.,  X  X ooood X ) as a preprocessing step. Secondly, a K-Nearest Neighbors (KNN)-based classifier is adopted to conduct word-level classification, leveraging the similar and recently la-beled tweets. Following the two-stage prediction aggregation methods [Krishnan and Manning 2006], such prelabeled results, together with other conventional features used by the state-of-the-art NER systems, are fed into a linear Conditional Random Fields (CRF) [Lafferty et al. 2001] model, which conducts fine-grained tweet-level NER. Fur-thermore, the KNN and CRF model are repeatedly retrained with an incrementally augmented training set, into which the tweets labeled with confidence are added. Indeed, it is the combination of KNN and CRF under a semisupervised learning frame-work that differentiates ours from existing work. Finally, following Ratinov and Roth [2009], 30 gazetteers are used, which cover common names, countries, locations, tem-poral expressions, etc. These gazetteers represent general knowledge across domains. The underlying idea of our method is to combine global evidence from KNN and the gazetteers with local contextual information, and to use common knowledge and unla-beled tweets to make up for the lack of training data. 12,245 tweets are manually annotated as the gold-standard dataset. Experimen-tal results show that our method outperforms the baselines. It is also demonstrated that tweet normalization, integrating KNN classified results into the CRF model, and semisupervised learning considerably boost the performance.

Our contributions are summarized as follows. (1) We propose a novel method that conducts tweet normalization and combines a KNN (2) We evaluate our method on a human annotated dataset, and show that our method Our article is organized as follows. In the next section, we introduce related work. In Section 3, we formally define the task and present the challenges. In Section 4, we detail our method. In Section 5, we evaluate our method. Finally, Section 6 concludes our work. We briefly review four categories of related work: NER on tweets, NER on nontweets (e.g., news, biological medicine, and clinical notes), semisupervised learning for NER, and text normalization. Finin et al. [2010] use Amazon X  X  Mechanical Turk service 2 and CrowdFlower 3 to anno-tate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. In contrast, our work aims to build a system that can automatically identify named entities in tweets. To achieve this, a KNN classifier with a CRF model is combined to leverage cross-tweet information, and semisupervised learning is adopted to leverage unlabeled tweets. NER has been extensively studied on formal text, such as news, and various approaches have been proposed. For example, Krupka and Hausman [1998] use manual rules to extract entities of predefined types; Zhou and Ju [2002] adopt Hidden Markov Models (HMM) while Finkel et al. [2005] use CRF to train a sequential NE labeler, in which the BIO (meaning Beginning, the Inside, and the Outside of an entity, respectively) schema is applied. Other methods, such as classification based on Maximum Entropy models and sequential application of Perceptron or Winnow [Collins 2002a], are also practiced. The state-of-the-art system, for example, the Stanford NER, can achieve an F1 score of over 92.0% on its test set.

Biomedical NER represents another line of active research. Machine-learning-based systems are commonly used and outperform the rule-based systems. A state-of-the-art biomedical NER system [Yoshida and Tsujii 2007] uses lexical features, orthographic features, semantic features, and syntactic features, such as Part-Of-Speech (POS) and shallow parsing.
 There is a handful of work on other domains. For example, Wang [2009] introduces NER on clinical notes. A dataset is manually annotated and a linear CRF model is trained, which achieves an F-score of 81.48% on his test dataset; Downey et al. [2007] employ capitalization cues and n-gram statistics to locate names of a variety of classes in Web text; most recently, Chiticariu et al. [2010] design and implement a high-level language NERL that is tuned to simplify the process of building, understanding, and customizing complex rule-based named-entity annotators for different domains.
Ratinov and Roth [2009] systematically study the challenges in NER, compare several solutions, and report some interesting findings. For example, they show that a conditional model that does not consider interactions at the output level performs comparably to beam search or Viterbi, and that the BILOU (the Beginning, the Inside, and the Last tokens of multitoken chunks, the Outside of the text segments as well as Unit-length chunks) encoding scheme significantly outperforms the BIO schema (the Beginning, the Inside, and Outside of a chunk).

In contrast to the preceding work, our study focuses on NER for tweets, a new genre of texts, which are short, noise prone, and often ungrammatical. Semisupervised learning exploits both labeled and unlabeled data. It proves useful when labeled data is scarce and hard to construct while unlabeled data is abundant and easy to access.

Bootstrapping is a typical semisupervised learning method. It iteratively adds data that has been confidently labeled but is also informative to its training set, which is used to retrain its model. Jiang and Zhai [2007] propose a balanced bootstrapping algorithm and successfully apply it to NER. Their method is based on instance reweighting, which allows the small amount of the bootstrapped training sets to have an equal weight to the large source domain training set. Wu et al. [2009] propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human-labeled tweets as seeds.

Another representative of semisupervised learning is learning a robust representa-tion of the input from unlabeled data. Miller et al. [2004] use word clusters [Brown et al. 1992] learned from unlabeled text, resulting in a performance improvement of NER. Guo et al. [2009] introduce Latent Semantic Association (LSA) for NER. In our pilot study of NER for tweets, we adopt bag-of-words models to represent a word in tweet, to concentrate our efforts on combining global evidence with local information and semisupervised learning. We leave it to our future work to explore which is the best input representation for our task. Text normalization is the task of restoring a text string into its formal form. Domi-nating methods are based on the noisy channel model [Shannon 1948]. That is, let t denote a tweet, and t denote its possible corresponding correct form, then the expected language model, respectively. Existing methods mainly differ in how the error model is implemented. For example, Brill and Moore [2000] characterize the error model by computing the product of editing operation probabilities in word partition basis from t to t ; Cook and Stevenson [2009] introduce inference from different erroneous formation processes according to the sampled error distribution; Han and Baldwin [2011] propose a cascaded normalization method, which first identifies possible ill-formed words with a classifier and then use an error model that considers morphological and phonetic variations to generate candidates. Unlike the preceding studies, our proposed normal-ization method unifies the language model and the error model into a global linear discriminative model, and can integrate arbitrary features. We first introduce some background about tweets, then give a formal definition of the task, and finally revisit the challenges of this task. A tweet is a short text message containing no more than 140 characters in Twitter, the biggest microblog service. Here is an example of a tweet:  X  X ycraftingworld: #Win Microsoft Office 2010 Home and Student *2Winners* #Contest from @office and @mom-tobedby8 #Giveaway http://bit.ly/bCsLOr ends 11/14 X , where  X  X ycraftingworld X  is the name of the user who published this tweet. Words beginning with the  X # X  character, like  X #Win X ,  X #Contest X  and  X #Giveaway X , are hash tags, usually indicating the topics of the tweet; words starting with  X  X  X , like  X  X office X  and  X  X momtobedby8 X , represent user names, and  X  X ttp://bit.ly/bCsLOr X  is a shortened link.

Twitter users are interested in named entities, such as person names, organization names, and product names, as evidenced by the abundant named entities in tweets. According to our investigation on 12,245 randomly sampled tweets that are manually labeled, about 46.8% have at least one named entity. Figure 1 shows the portion of named entities of different types. Given a tweet as input, our task is to identify both the boundary and the type of each mention of entities. We focus on four types of entities in our study, that is, persons, organizations, products, and locations, which, according to our investigation as shown in Figure 1, account for 89.0% of all the named entities.

Here is an example to illustrate the task. The input is  X . . . Me without you is like an iphone without apps, Justin Bieber without his hair, Lady gaga without her telephone, it just wouldn. . .  X  The expected output is as follows:  X . . . Me without you is like an &lt; PRODUCT &gt; iphone &lt; /PRODUCT &gt; without apps, &lt; PERSON &gt; Justin Bieber &lt; /PERSON &gt; without his hair, &lt; PERSON &gt; Lady gaga &lt; /PERSON &gt; without her telephone, it just wouldn. . .  X , meaning that  X  X phone X  is a product, while  X  X ustin Bieber X  and  X  X ady gaga X  are persons. We further discuss the two main challenges faced by our task: limited information contained in a single tweet, and the lack of annotated tweets.

Limited Information in a Single Tweet . This challenge is rooted in the informal and short nature of tweets. Tackling this challenge, ideally, requires adapting related NLP tools to fit tweets, or normalizing tweets to accommodate existing tools, both of which are hard tasks. Our solution is to aggregate as much trustworthy information as possible. That is, a KNN classifier is used to collect global information, such as how the word is labeled in other similar tweets, together with prebuilt gazetteers that encode the common knowledge about NER. Furthermore, a normalization component is developed to make tweets more formal.

Few Training Tweets . A large volume of annotated data is needed to train an NER system for tweets, particularly given the huge noise in tweets and their diversified written styles. However, manually annotating such a corpus is tedious and unafford-able. In our work, we manually label a only small amount of tweets and use gazetteers and semisupervised learning to relieve this issue.
 Now we present our solution to the challenging task of NER for tweets. An overview of our method is first given, followed by a detailed discussion of its core components. An NER task is naturally divided into two subtasks, that is, entity boundary detection and entity type classification. Following the common practice [Finkel et al. 2005; Ratinov and Roth 2009], we adopt a sequential labeling approach to jointly resolve these subtasks, that is, for each word in the input tweet, a label is assigned to it, indicating both the boundary and entity type. Inspired by Ratinov and Roth [2009], we use the BILOU schema.

Algorithm 1 outlines our method, where: train s and train k represent two machine learning processes to get the CRF labeler and the KNN classifier, respectively; nor m denotes the normalization process called before NER to correct common ill-formed words. nor m is also called to normalize the initial training tweets; repr w converts a word in a tweet into a bag-of-words vector; the repr t function transforms a tweet into a feature matrix that is later fed into the CRF model; the knn function predicts the class of a word; the update function applies the predicted class by KNN to the inputted tweet; the cr f function conducts word-level NE labeling;  X  and  X  represent the minimum labeling confidence of KNN and CRF, respectively, which are experimentally set to 0.1 and 0.001; N (1,000 in our work) denotes the maximum number of new accumulated training data; o is the output stream, and labeled tweets are put into it 4 .
As illustrated in Algorithm 1, the new confidently labeled tweets are repeatedly added to the training set 5 , and both the KNN classifier and the CRF labeler are re-trained once the number of new accumulated training data goes above the threshold N . Algorithm 1 also demonstrates one striking characteristic of our method: A KNN classifier is applied to determine the label of the current word before the CRF model. The labels of the words that are confidently assigned by the KNN classifier are treated as visible variables for the CRF model. This also makes our method different from other two-stage methods [Krishnan and Manning 2006; Ratinov and Roth 2009], which ap-ply a baseline NER system and use the resulting predictions as features (rather than visible variables) in a second level of inference. Our model is hybrid in the sense that a KNN classifier and a CRF model are sequen-tially applied to the target tweet, so that the KNN classifier captures global coarse evidence while the CRF model utilizes fine-grained information encoded in a single tweet and in the gazetteers. Algorithm 2 outlines the training process of KNN, which records the labeled word vector for every type of label.

Algorithm 3 shows how the KNN classifier predicts the label of the word. In our work, K is fine-tuned on the development dataset, and is experimentally set to 20.
The KNN model is chosen for two reasons: (1) it can incorporate evidence from newly labeled tweets and retraining is fast; and (2) combining with a CRF model, which is good at encoding the subtle interactions between words and their labels, compensates for KNN X  X  incapability to capture fine-grained evidence involving multiple decision points.
The linear CRF model is used as the fine model, with the following considerations: (1) it is well-studied and has been successfully used in state-of-the-art NER systems [Finkel et al. 2005; Wang 2009]; (2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semisupervised learning framework.
 In our experiments, the CRF++ 6 toolkit is used to train a linear CRF model. And a Viterbi decoder that can incorporate partially observed labels is developed to implement the cr f function in Algorithm 1. Our normalization process adopts an unified linear discriminant model as defined in Eq. (1), where: t and t  X  denote the original and the corresponding normalized tweet, respectively; the GEN ( t ) function generates all the possible normalized tweets of t ; w is the feature weight vector; f t is the vector representation of t ; w  X  f t indicates the possibility of t being the normalized tweet of t . Advantages of our normalization model are that it allows various features (e.g., features derived from the language model or the error model or both) to be integrated into a single model and that the feature weights can be consistently and efficiently learned using simple statistical learning algorithms, for example, the voted Perceptron [Collins 2002b].
In our work, the GEN ( t ) function uses a predefined list of ill-formed/correct word pairs to generate all possible candidates 7 . Specifically, it replaces each ill-formed word of the input tweet t with each of its correct forms. For simplicity, only one feature, that is, the logarithms of the language model score, is considered (and thus weight training is not necessary). The language model is a trigram language model trained on LDC data 8 using SRILM [Stolcke 2002]. The average five-fold cross-validation F1 score is 60.5% on the dataset provided by Han and Baldwin [2011]. Given a word in a tweet, the KNN classifier considers a text window of size 5 with the word in the middle [Zhang and Johnson 2003], and extracts bag-of-word features from the window as features. For each word, our CRF model extracts similar features as Wang [2009] and Ratinov and Roth [2009], namely, orthographic features, lexical features, and gazetteer-related features. In our work, we use the gazetteers provided by Ratinov and Roth [2009], which consists of two parts: (1) four high-precision, low-recall lists extracted from the Web, which cover common names, countries, monetary units, temporal expressions, etc; and (2) sixteen gazetteers mined from Wikipedia, which collectively contain over 1.5 million entities. The details of these three kinds of features are listed next.

Orthographic features : whether the words are capitalized or upper case; whether they are alphanumeric or contain any slashes; word prefixes and suffixes.
Lexical features : current token; the left and right token; POS of current token; POS of the left and right token; the target and previous labels.

Gazetteer-related features : Category label of current token. The same BILOU label schema is used. For instance, suppose  X  X arack Obama X  is in gazetteers with  X  X ERSON X  as the category label, then for tweets like  X  ... loooov barack obama, ...  X ,  X  X  PERSON X  and  X  X  PERSON X  will be extracted as gazetteer-related features for  X  X arack X  and  X  X bama X , respectively 9 .

Two points are worth noting here. One is that before feature extraction, stop-words are removed for either the KNN-based classifier or the CRF-based sequential labeler. The stop-words used here are mainly from a set of frequently used words 10 . The other is that every link and every account name in each tweet are replaced by LINK and ACCOUNT, respectively. Hash tags are treated as common words. We now discuss several design considerations related to the performance of our method, that is, additional features, gazetteers, scalability, and alternative models.
Additional Features . Features related to chunking and parsing are not adopted in our final system, because they give only a slight performance improvement while a lot of computing resources are required to extract such features. The ineffectiveness of these features is linked to the noisy and informal nature of tweets. Word class [Brown et al. 1992] features are not used either, which prove to be unhelpful for our system. We are interested in exploring other tweet representations, which may fit our NER task, for example, the LSA models [Guo et al. 2009].

Gazetteers . In our work, gazetteers prove to be substantially useful, which is consis-tent with the observation of Ratinov and Roth [2009]. However, the gazetteers used in our work contain noise, which hurts the performance. Moreover, they are static, di-rectly from Ratinov and Roth [2009], thus with a relatively lower coverage, especially for person names and product names in tweets. We are developing tools to clean the gazetteers. In the future, we plan to feed the fresh entities correctly identified from tweets back into the gazetteers. The correctness of an entity can rely on its frequency or other evidence.

Scalability . Scalability is an important issue for a practical NER system since every day a huge number of tweets are generated. We take the following steps to ensure the scalability of our method: (1) index only top M most recently labeled tweets to efficiently compute the top K most similar tweets given a bag of words (line 1 in Algorithm 3). M is properly chosen so that the index can be loaded into memory; (2) increase N to reduce the CRF model retraining times (line 21 in Algorithm 1); and (3) copy our system to multiple servers so that tweets can be processed in parallel 11 .

Alternative Models . We have replaced KNN by other classifiers, such as those based on Maximum Entropy and Support Vector Machines, respectively. KNN consistently yields comparable performance, while enjoying a faster retraining speed. Similarly, to study the effectiveness of the CRF model, it is replaced by its alternations, such as the HMM labeler and a beam search plus a maximum-entropy-based classifier. In contrast to what is reported by Ratinov and Roth [2009], the CRF model gives remarkably better results than its competitors in our experiments. Note that all these evaluations are on the same training and testing datasets as described in Section 5.1. In this section, we evaluate our method on a manually annotated dataset and show that our system outperforms the baselines. The contributions of tweet normalization, the combination of KNN with CRF, and the semisupervised learning are studied, respectively. We use the Twigg SDK 12 to crawl tweets from April 20 th 2010 to April 25 th 2010, then drop non-English tweets and get about 11,371,389, from which 15,800 tweets are randomly sampled, and are then labeled by two independent annotators, so that the beginning and the end of each named entity are marked with &lt; TYPE &gt; and &lt; /TYPE &gt; , respectively. Here TYPE is PERSON, PRODUCT, ORGANIZATION, or LOCATION. 3555 tweets are dropped because of inconsistent annotation. Finally we get 12,245 tweets, forming the gold-standard dataset. Figure 1 shows the portion of named entities of different types. On average, a named entity has 1.2 words. The gold-standard dataset is randomly divided into three parts: 1,245 tweets for development, 5,000 tweets for training, and the remainder for testing. Note that to simulate the situation where our system is processing real-time tweets from a tweet stream, testing tweets are fed to our system one by one in chronological order. For every type of named entity, Precision (Pre.), Recall (Rec.), and F1 are used as the evaluation metrics. Precision is a measure of what percentage of the output labels are correct, and recall tells us to what percentage the labels in the gold-standard dataset are correctly labeled, while F1 is the harmonic mean of precision and recall, as defined in Eq. (2). For the overall performance, we use the average Precision, Recall, and F1, where the weight of each name entity type is proportional to the number of entities of that type. These metrics are widely used by existing NER systems for performance evaluation. In all the experiments, the p-value is computed using the sign test. Two systems are used as baselines: (1) a dictionary lookup system based on the gazetteers ( NER DIC ); and (2) a modified version of our system without KNN and semisupervised learning ( NER BA ). A customized POS tagger is developed to extract POS-related features, which combines the Stanford tagger [Toutanova et al. 2003], a simple rule-based tagger [Brill 1992] with the OpenNLP tagger using a simple voting strategy. Its final POS accuracy on our test dataset is 85.7%. The OpenNLP and the Stanford parser [Klein and Manning 2003] are used to extract other linguistic features. Note that the Stanford NER is not directly comparable to our method because it is not trained on tweets; and NER BA , which adopts similar features and uses the same linear CRF model, can be regarded as the customized version of the Stanford NER trained for tweets. It is also worth mentioning that the normalization process is disabled by default for our method and the two baselines to isolate its influence on the results. We study the function of normalization in Section 5.5. Table I shows the overall results for the baselines and ours ( NER CB ). Here our system is trained as described in Algorithm 1, combining a KNN classifier and a CRF labeler, with semisupervised learning enabled. As can be seen from Table I, on the whole, our method significantly outperforms (with p &lt; 0 . 001) the baselines. Tables II through V report the results on each entity type, indicating that our method consistently yields better results on all entity types. We now enable the normalization process discussed in Section 4.3 for our method to study its contribution. The updated system is denoted by NER CB  X  NORM .Asshown in Table VI, NER CB  X  NORM yields a remarkably higher precision (with p &lt; 0 . 002) and recall (with p &lt; 0 . 001). Table VII shows the performance of our method without combining the KNN classifier, denoted by NER CB  X  KNN . A drop in performance is observed then. We further check the confidently predicted labels of the KNN classifier, which account for about 22.2% of all predications, and find that its F1 is as high as 87.5% while the baseline system based on the CRF model achieves only an F1 of 75.4%. This largely explains why the KNN classifier helps the CRF labeler. The KNN classifier is replaced with its competitors, and only a slight difference in performance is observed. We do observe that retraining KNN is obviously faster. The CRF model is replaced by its alternatives to study its influence on the performance. As is opposite to the finding of Ratinov and Roth (2009), the CRF model gives remark-ably better results, that is, 2.1% higher in F1 than its best followers (with p &lt; 0 . 001). Table VIII shows the overall performance of the CRF labeler with various feature set combinations, where F o , F l ,and F g denote the orthographic features, the lexical fea-tures, and the gazetteers-related features, respectively. It can be seen from Table VIII that the lexical and gazetteer-related features are helpful. Other advanced features such as chunking are also explored but with no significant improvement. Table IX compares our method with its modified version without semisupervised learn-ing, suggesting that semisupervised learning considerably boosts the performance. To get more details about self-training, we evenly divide the test data into 10 parts and feed them into our method sequentially; we record the average F1 score on each part, as shown in Figure 2. It is worth mentioning that after a certain amount of retraining, the F1 may drop caused by the accumulated noise in the training set. One possible way to address this issue is to stop retraining when F1 goes above a threshold or to enhance the training set using manually verified samples periodically. We will further study this problem in the future. Errors made by our system on the test set fall into three categories. The first kind of error, accounting for 34.5% of all errors, is largely related to slang expressions and informal abbreviations. For example, our method identifies  X  X ali X , which actually means  X  X alifornia X , as a PERSON in the tweet  X  X  love Cali so much X . We are further developing our normalization component to handle such slang expressions and informal abbreviations.

The second kind of error, accounting for 35.2% of all errors, is mainly attributed to the errors in the gazetteers. For example, for this tweet  X  X ome to see jaxon someday X , our method mistakenly labels  X  X axon X  as a LOCATION, which actually denotes a PERSON. This error is understandable, since it fully matches a term of LOCATION type in gazetteers. Possible solutions to these errors include continually cleaning errors in gazetteers and aggregating additional external knowledge from other channels such as traditional news.

The last kind of error, which represents 30.3% of all errors, somehow links to the noise-prone nature of tweets. Consider this tweet  X  X esley snipes ws cought 4 nt payin tax coz ths celebz dnt take it cirus. X , in which  X  X esley snipes X  is not identified as a PERSON but simply ignored by our method, because this tweet is too noisy to provide effective features. Tweet normalization technology seems a possible solution to alleviate this kind of error. We propose a novel NER system for tweets, which conducts tweet normalization and combines a KNN classifier with a CRF labeler under a semisupervised learning frame-work. The tweet normalization step corrects common ill-formed words. The KNN clas-sifier collects global information across recently labeled tweets while the CRF labeler exploits information from a single tweet and from the gazetteers. A serials of experi-ments show the effectiveness of our method, and particularly, show the positive effects of tweet normalization, KNN, and semisupervised learning.

In the future, we plan to explore two directions to further improve the performance of our method through two directions. First, we hope to develop advanced tweet nor-malization technologies to further reduce noise in tweets. Second, we are interested in identifying and cleaning errors in the gazetteers and also integrating new entities mined from tweets into the gazetteers.

