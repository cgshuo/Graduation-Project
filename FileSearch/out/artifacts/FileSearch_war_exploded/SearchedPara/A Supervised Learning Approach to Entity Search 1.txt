 Guoping Hu 1 , Jingjing Liu 2 , Hang Li, Yunbo Cao, Jian-Yun Nie 3 , and Jianfeng Gao Despite the progress on document search, many specialized searches are still not well types of information. More precisely, the user types a search query and designates the times, places, organizations, or URLs) that are likely to be associated with the query. and URL search. 
Entity search will be particularly useful at enterprise. People at enterprise are often company may want to know the time of the next event concerning a product, the right person to contact for a problem, and so on. 
Traditional search approach does not support entity search. For example, for a keyword, including  X  X ate. X  As a consequence, documents containing the keywords However, the documents or passages retrieved do not necessarily contain the required information, i.e. the date of the exhibition in this example. 
There is an increasing interest in entity search in the research community. For [2, 6, 7, 13, 14, 15, 21 and 22]. There was also a task on expert search at TREC 2005. However, we observe that all the proposed methods relied on traditional IR techniques or simple features for ranking entities. For example, Craswell et al. [3] proposed using the co-occurrences of people and keywords in documents as evidence of association between them, and ranking people according to the strength of association. One may consider dealing with the problem with Question Answering (QA). adequately applied here. 
In this paper, we aim to develop an appropriate approach for entity search. Ranking variety of features in a linear combination model. Supervised learning is employed to train the model. 
Two specific types of search are considered as examples in this paper: time search and people search. For time search, experiments have been carried out on two Web and QA Tracks. For expert search, experiments have been performed with the TREC expert search data. The experimental results indicate our method performs significantly better than the baseline methods. describe our proposed approach to entity search. Section 5 gives our experimental results. We conclude in Section 6. 2.1 Traditional Search Conventional Information Retrieval aims to identify documents or passages that may be  X  X elevant X  to the query. The evidence used for the relevance judgment is the appearance of the query terms in the documents or passages. The query terms are models such as TF-IDF (e.g., [20]), BM25 (e.g., [19]), and Language Model (e.g., [17]). Usually these term weighting methods do not require labeled data for training. In that sense, the methods are unsupervised. 
There is also a new trend in IR recently that manages to employ supervised learning methods for training ranking functions. Herbrich et al. [9] cast the IR problem as ordinal regression. They proposed a method for training the ordinal discriminate training on a linear model for IR and they observed a significant improvement of accuracy on document retrieval by using the method. The success of the approach is mainly due to the following two factors: (1) The model provides the flexibility of incorporating an arbitrary number of features; (2) The model is trained in a supervised manner and thus has a better adaptation capability. 2.2 Entity Search Entity search tries to identify entities strongly associated with query terms. There were several studies on entity search. The most studied type of entity was people (or expert). 
Expert search was investigated in [2, 6, 7, 13, 14, 15, 21 and 22]. There was also a only exploited simple features or traditional IR techniques for ranking. For example, Craswell et al. [3] proposed using co-occurrences between people and query words as evidence to rank people. 
Many features may be useful for entity search, including new features that are not easily incorporate new features. In this paper, we propose employing supervised explored the same approach for entity search. 2.3 Factoid Question Answering answer to a question. Many QA systems were developed, including Webclopedia [10], NSIR [18], MultiText [4], MULDER [12], AskMSR [1], and the statistical QA system of IBM [11]. 
Factoid QA is the most studied subtask, which tries to answer questions about type of the answer) is identified. In question expansion, the synonymous expressions the content words in the question and its expansions. In answer ranking, the retrieved passages are ranked, and the potential answers within the passages are marked (in the from the top ranked passages. 
People may take QA as an appropriate means for entity search. However, the assumptions for QA may not hold for entity search. First, QA heavily relies on NLP techniques to analyze the question. The basic assumption is that the question is a well formulated question. For entity search, we have to deal with queries instead of complete questions. That means that NLP would not help entity search. Second, users methodologies used in QA. The problem of entity search such as expert search and time search can be described query (just like people usually do in search of documents) and designates a type, the strong relationship between the query and the entity (e.g., a person is an expert on the topic of the query). In practice, an association presented in a document can be bogus; however, we do not consider the problem in this paper. 
A single user interface can be considered for entity search. There are check boxes on the UI, and each represents one type of entity (as shown in Figure 1). The user can designate the types of information to search by checking the corresponding boxes. The user can then input queries in the search box. 
Figure 1 shows an example of expert search. When the user checks the box  X  X xpert search X  and inputs the query  X  X emantic web coordination, X  the system returns a list of with supporting documents. The documents can help the user to figure out whether the answers are correct. recognition. For example, in time search, rules are created to identify time expressions such as  X  X ov. 17, 2004. X  Here, we do not consider conducting anaphoric resolution, for example, identifying the date of  X  X esterday X  in a document. 
For each identified entity, a passage is constructed. A passages is a window of Around the center, features are extracted and used for the ranking of the passage with regard to the query. 
The approach used in our study, which makes it different from most existing ones, method is motivated by the following observations: 1) Features are usually extracted according to heuristics, and thus are different in nature. It is difficult to combine them in a traditional IR function like BM25. 2) Once new features are added, one has to re-tune the ranking function. This is difficult if the tuning is done manually. 3) process can be executed again, once new features are added. In this study, our goal is to develop a general method for entity search and thus we employ the supervised learning approach to perform the task. 4.1 Ranking Function Two ranking functions are defined: one for passage and the other for entity. Our entity determining the top N entities from these passages. 
Each passage retains a ranking score representing its association to the query. Let words). Then, ) , ( p q s is determined according to the following equation:  X  denotes the weight of w . We will explain the function ) , ( p w  X  in more details in the next subsection. We select the top K passages based on the ) , ( p q s scores. 
Once the K top passages are identified, a weighted voting is conducted to determine Specifically, the ranking score of an entity e with respect to the query q is calculated as follows: highest ) , ( e q s scores. 4.2 Learning the Weighting for a Keyword In conventional IR, the weight of a keyword is determined according to its frequency. mechanism should be adopted. For example, a keyword may be more important if it is Climbing algorithm as described later. 4.3 Feature Sets characteristics of keyword, the characteristics of passage, etc. We describe the features in this section. Time Search. For time search, we define 35 features and Table 1 shows some Some features take on numerical values, while some features are Boolean features. Expert Search. For expert search, we define 15 features and Table 2 shows some examples of them. Here, we denote the entity in a passage as center, and the keyword as word too. 4.4 Training The key problem in ranking is to make a correct utilization of the features, i.e. a correct setting of the weights ) ,..., ( training data. The training data contains a set of queries, a set of passages obtained as described above, and the relevance judgments provided by human judges. 
Different algorithms can be employed in training of the weights ) ,..., ( Hill-climbing algorithm is slightly modified so as to deal with over-tuning. This algorithm works as follows: (1) Initialize each weight c j as 0.0; (2) For j = 1 to m (3) Record the highest performance achieved in step (2) and the corresponding 5.1 Baseline Methods Our method is compared with two baseline methods: (1) BM25: In this method we calculate the BM25 score of the passage with respect (2) Distance: In this method we calculate the following distance score: 
The first baseline method is representative of the traditional IR approach. These baseline methods are chosen because they are commonly used in the previous studies. BM25 method. The second one is similar to a typical QA approach [18]. 
For all the two methods, the top K passages are identified according to their passage ranking functions. Then two voting methods are used to further rank entities: simple voting or weighted voting. Simple voting means voting entities by the number of supported passages, and weighted voting is similar to that in equation (2). 5.2 Evaluation Measures We use Mean Average Precision (MAP) and R-Precision [5] as the measures for evaluations of entity search. 5.3 Time Search Data Sets. Our experiments on time search were carried out on two data sets: 1) The first one was created from the query logs of a search engine on the intranet  X  X hen, X   X  X chedule, X   X  X ay, X  and  X  X ime X  were collected, and then the clue words were removed from the queries. The remaining parts were used as pseudo queries time search. This set contains 100 queries (referred to as MS hereafter). The documents are from the same intranet. 2) The other query set was created from the temporal questions in the TREC QA Track (i.e., questions with  X  X hen, X   X  X n what time, X  etc). Again, stop words and time clues were removed from the queries. This query set contains 226 queries (referred to as TREC hereafter). The documents are those used in TREC Web Track. 
For each query, each of the methods tested returned 100 answers. These answers were judged manually by 2 human evaluators. For 23% of the MS queries correct answer could not be found in the retrieved documents, and the number was 51% for TREC.
 Time Expression Identification Experiment. As our method depends on the quality expression identification. 300 documents we re randomly selected. A human annotator was asked to annotate all the time expressions within them. This enabled us to evaluate the time identification method. Table 3 shows the evaluation results. We see that in general our identification method obtains high accuracies. The lower precision programming codes in the data set (e.g.,  X  X oney 2002 X ). Time Expression Ranking Experiment. We conducted time search using our method and the two baselines. The top 100 answers with each method were used. We performed 4-fold cross validation , i.e. 3/4 of the queries were used in training and 1/4 for testing. Table 4 shows the time search results of the different methods in terms of MAP and R-Precision. 
We can observe that Distance performs better than BM25. This shows that the sufficient for entity search. We can also observe that weighted voting performs better than simple voting. However, the differences are not large. Comparing our method with the baseline methods, we can see that our learning method outperforms both baseline methods with quite large margins. One may notice that the overall accuracies in terms of MAP and R-Precision are not very high. An important reason is that 23% of the MS queries and 51% of the TREC queries do not have correct answers. aspects) in our learning based method, we used different subsets of the features in our ranking functions. The feature subsets 1, 2, 3 and 4 in Figure 2 correspond to the four feature aspects described in Section 4.3. From Figure 2, it can be seen that when more features are used the search results can be improved. This result validates our supervised learning method we propose is capable of combine different features so as to take advantage of each of them. 5.4 Expert Search Data Set. In our expert search experiments, we used the data set in the expert search from the public W3C [24] sites in June 2004, which contains 331,307 web pages, including specifications, email discussion, wiki pages and logs of source control. The ground truth on expert search was obtained from an existing database of W3C working groups. For each query (the name of the working group), a list of people (the group members) is associated as experts on the topic. There are in total 1,092 members in all the groups. (For details of the TREC data, see [25]). Personal Name Identification. Heuristic rules were used to identify personal names from the W3C document collection were randomly selected. All the personal names from these documents were manually checked by a human annotator. The evaluation recall are 100% and 90%, respectively. The process missed some names mainly Connolly X ). In general, our name identification process is satisfactory. Expert Ranking Experiment. In the official tests in TREC, two sets of queries were provided: a set of 10 training topics and another set of 50 topics for testing. However, data are combinations of several phrases. Therefore, we only used the 50 test queries in our experiment, and conducted 10-fold cross-validation in our evaluation. The results reported below are the averaged results over 10 trials. 
Table 5 shows the results on expert search for our method and the baseline the BM25 method and the Distance method: both baseline methods perform quite poorly. In comparison, our method based on supervised learning outperforms them significantly. This confirms again that our method is more suitable for entity search. 
In order to see the contributions of different features, we ran our method with search. Therefore, we have more evidence to say that it is advantageous to incorporate appropriately combine the uses of the features. Entity search such as expert search and time search is useful in many search scenarios, particularly in enterprise. Methods based on the use of co-occurrence have been proposed in the literature for expert search. This paper explores a new approach for entity search on the basis of supervised learning. Specially, it makes use of a linear features. 
We have applied our method to two of the entity searches  X  time search and expert search in this paper. Experimental results show that the proposed method performs significantly better than the baseline methods solely using co-occurrence or distance. 
The main contributions of this work are (1) a proposal of a supervised learning approach; and (2) identification of some useful features for time search and expert search. commonly required entity searches are developed effectively. Such a system would offer great facilities for people to find their required types of information. 
