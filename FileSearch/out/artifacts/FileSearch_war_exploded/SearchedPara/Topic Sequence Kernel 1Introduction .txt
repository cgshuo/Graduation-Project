 The Support Vector Machine (SVM) has been widely applied in [1-3] and it is well known for using kernel methods to handle non-separable data points by the hype-plane in the kernel space. The commonly used kernels are linear, polynomial, and RBF kernels. [4] proposed the string kernel which took the ordered subsequence of characters for document representation. This kernel considers the sequential order between characters in a document. However, measuring similarity between two sequences requires a lot of computational resources. To resolve this problem, [4] pro-posed a dynamic programming technique to promote computation efficiency. To fur-ther reduce the computational complexity, [5 ] used the words instead of characters as the sequences for document representation. 
In this paper, we extend the basic idea of string kernel to represent documents as sequences of topics instead of words or ch aracters. As topics are a summary of docu-ments, they can better capture the document semantics. One document might be about crude oil (0.6), ship (0.3) an d trade (0.1). Another document may be about trade (0.5), crude oil (0.3) and ship (0.2). Two documents both have three topics but with differ-ent topic proportions. For the first document, crude oil has the greatest proportion among the three topics, ship the second greatest proportion and trade is the least. In-tuitively, most important topic will be first expressed, and less important topic will be so doing, a document can be represented in a sequence of topics according to the topic pected to have not only similar topics, but also the topic sequences. Based on this assumption, we try to classify texts based on the kernel approach using the sequence of topics instead of words or characters. This could greatly reduce the computational complexity of string kernel as there is a small number of topics compared to the words in the whole document collection. 
The rest of this paper is organized as follows. Section 2 describes the related works of kernel methods and topic modeling approaches. Section 3 presents the approach of generating topic sequences using topic modeling technique and introduces the string kernel using the topic sequences. Section 4 gives the performance evaluation. Section 5 is the conclusion. Kernel functions are computational shortcuts that are able to represent linear patterns in high-dimensional space [6]. They are used to compute pairwise inner products between mapping examples in the feature space [4]. A kernel is valid only when it meets the Mercer X  X  conditions: symmetry and positive semi-definiteness [7]. 
Currently, there are various kinds of kernels used in SVM, including the polynomi-al kernel, radio basis function (RBF), and so on. Different from previous kernels that are dependent on the word frequencies, the string kernel takes into account the rela-tive positional information of characters in documents. It compares two documents by enumerating the substrings they contain: the more substring they share, the more simi-lar they are [4]. In this paper, we extend the basic idea of string kernel using the sequence of topics. Hence, topic modeling is vital to generate topic sequences. Methods to discover the semantic structure of a document collection using the probabilistic model include latent semantic indexing (LSI), probabilistic LSI, latent Direchlet allocation (LDA) [8-10]. Besides, two extensions to LDA has been proposed: the correlated topic model topics given the document collection, various approximate approaches have been used, including mean-field variational inference [8], expectation propagation [13] and Gibbs sampling [14] and collapsed variational inference [15]. 3.1 Topic Modeling and Sequence Generating Documents that have similar topics are expected to exhibit similar topic proportions. Important topics will have large proportions in a document. Suppose there are four documents: Doc1 , Doc2 , Doc3 and Doc4 , and three topics: trade , crude oil , and ship . The topic proportions in these four documents are: Obviously, Doc1 and Doc3 have similar topic proportions. They both have crude oil least important topic. Similarly, Doc2 and Doc4 have a sequential order of ship , crude oil and trade according to the topic proportions. In this sense, Doc1 is similar to Doc3 sequential topic order. Therefore, the topics in four documents can be re-arranged in a sequential order according to their proportions in each document. the LDA [10]. LDA is a generative model which is based on probabilistic sampling techniques investigating how words in documents are generated with the hidden va-documents are observable and topics are latent variables hidden in these documents. Its graphical representation is given in Fig. 1. 
In the Fig.1, each node denotes a random variable and the edge between nodes represents dependency relations between nodes. The double circles around the ran-dom variable denote an observable node (evidence node). The plate surrounding the number of topics, respectively.  X  and  X  are hyper-parameters on the mixture propor-document d and  X  k is multinomial word distributions for topic k .  X  d,n denotes a topic from which the n th word in document d is drawn and W d,n indicates the observable n th word in d . 
In LDA model, for a document d , a vector of topic distributions d  X  is drawn from a Dirchlet distribution; topic assignment for n th word  X  d,n follows from a multinomial distribution; and the n th word W d,n in document d is sampled from multinomial dis-tribution. To generate topic sequences, p(z|w) must be obtained for the hyper-Gibbs sampler is used. When p(z|w) is obtained, the topic distributions  X  for each document can be estimated. These topic distributions are used to generate topic sequences. 3.2 Topic Sequence Kernel The topic sequences are generated in Section 3.1. Then, the subsequences of the topic sequences are extracted as features in the topic sequence kernel model. 
Given  X  as a finite topic set, let S=z 1 z 2 ...z |S| be a sequence of topics for a document, z The span of S[I] , denoted by l(I), is the distance of the first topic and the last topic u = z 1 z 4 , then the index set, I=[1,3] such that u=S[1,3] , and the span of S[1,3] is 3-1+1 =3. The feature matching of u for a given topic sequence S, denoted b y  X  , is: where  X  is the decay factor, in the range of [0,1], that penalizes the longer span l(I) of subsequences. Based on topic sequence, any two documents, represented by their topic sequences S, and T , are compared through the topic subsequences as features. To control the feature space, the topic sequence kernel has a parameter n which de-notes the length of subsequences in the feature space. Then, the similarities are: where n  X  is the set of all topic subsequences of length n . S[I] and T[J] are the sub-sequences in S and T . l(I) and l(J) are the spans of the subsequences in S and T . 
In fact, each topic sequence has the unique topics. This means that the subse-quences will occur only once in a topic sequence. Therefore, the feature matching of u in the topic sequences S and T will be changed to, And the kernel function K n (S,T) will be changed to, In this sense, the computational cost will be reduced due to the cancellation of sum-mation procedure in each topic sequence. To avoid enumeration of all subsequences for similarity measurement, dynamic programming, similar to the method in [4] is used here for similarity calculation. Experiments are conducted on the Reuters-21578 dataset, from which we used Mod-ified Apte ( X  X odeApte X ) split. Due to the concern of computational complexity of the string kernel, [4] drew a subset of 470 documents with 380 documents for training and 90 documents for testing. [5] proposed to used word sequence kernel on the ten frequent categories. This word sequence kernel, however, is still resource demanding as they claimed. In this paper, the topic sequence kernel will not suffer from this prob-lem since the number of topics is much less than that of characters or words. 
In the following experiments, the values for the hyper-parameters  X  and  X  are 50/K testing documents are placed together to obt ain the posterior topi c distribution. Based on topic distributions in each document, a t opic sequence is created for the document. 
In terms of classifier, the LIBSVM 1 tool with the one-versus-one strategy is used and default parameters are kept. Since the training sets for the ten categories are unba-lanced in favor of negative examples, we weigh the relative importance of positive and negative examples by the ratio between negative and positive examples [5]. 
For evaluation, we used the precision ( p ), recall ( r ) and F-measure ( F ). In order to have a general overview of performance on the ten categories, the micro-averaged and macro-averaged performances are used. To control the effectiveness of the topic se-quence kernel, three parameters need to be tuned manually: length of a subsequence l and the decaying factor  X  and the number of topics K . Table 1 gives the overall best used to denote the topic sequence kernel. 
Table 1 shows that the best performance is achieved when the number of topics K is 10. And K=10 is the number of categories in this experimental dataset. This means that if the number of topics is known beforehand, the LDA model can well capture the topic structure in documents. The detailed classification results for each category are listed in Table 2. 
From the Table 2, we found that the earn category gives the best performance be-performance than the money-fx category. However, higher negative-to-positive ratio will naturally produce worse classification results. The corn and ship categories have factor  X  and the number of topics K . 4.1 Effectiveness of Varying the Number of Topics The number of topics is crucial to the topic sequence kernel. It determines the compu-values of the parameters the subsequence length l=2 and the decaying factor  X  =0.55 fixed and observed how the performance is influenced by the number of topics K from 5 to 20. 
Table 3 shows that when the topic number K is 10, the system gives the best per-formance, implying that latent topics in the document collection are well captured by K=10 and other configurations cannot detect the topic structures properly if the K is too large or too small. Moreover, K =10 is the number of categories of document col-lections. On the other hand, if the number of topics is known beforehand, the latent topics can be well modeled out of the document collections by LDA. It is worth noting the case of K=5 in which the micro-average score is high, but the macro-average score is rather low. This is because the corn , ship and interest catego-ries all get zero classification precision and recall values. However, the acq and earn categories get high precision and recall and these two categories have a large number of testing documents, thus contributing to the overall higher micro-average score. 4.2 Effectiveness of Varying the Subsequence Length In this set of experiments, the values of the number of topics K=10 and the decaying from 2 to 6. 
Table 4 shows that the TSK can be more effective for smaller subsequence as com-pared to larger subsequences since the smaller topic subsequences are able to capture the document semantics than the longer ones. Besides, the longer subsequences have a strict requirement over the matching unit of the two sequences. For example, the the two subsequence sets. If the subsequence length is set to 2, we will find an inter-average and macro-average scores when the subsequence length is set to 2. 4.3 Effectiveness of Varyin g the Decaying Factor The decaying factor  X  controls how many gaps are allowed in the matching subse-quences of the two sequences. If  X  =1, the gaps between the subsequences are not penalized. If 0 &lt;  X  &lt; 1, the larger the gaps are, the more penalty will be placed on the subsequence. For this set of experiments, we kept K=10 and l=2 fixed and studied the effects of varying the decaying factor  X  . 
In Table 5, the highest micro-precision is achieved at  X  = 0.1 while all other highest micro-average and macro-average scores are obtained at  X  =0.55. The higher values of  X  will place more weights to contiguous topic subsequences. In other words, this is the parameter that penalizes the topic subsequences with large interior gaps. 4.4 Computational Complexity To derive an effective computation of this kernel, [4] proposed to use the dynamic programming technique to reduce the complexity of computation to O(n||S||T|) . n is (words/characters/topics) in S and T . Therefore, the number of symbols in a sequence determines the efficiency of the string kernel. The problem is that the semantic struc-tures of documents cannot be discovered when the number of symbols is greatly reduced. Differently, the topics are a summary of document and can detect the semantic structure of a document. 
To further reduce the computational cost, [5] proposed to use the sequence of words instead of characters on the Reuters dataset. The average number of words per document is 141 before removing the stop words and this number drops to 77 after the stop words are removed. In our experiments, the average number of topics is 10 . [5] claimed that if the average sequence length is reduced by about 50% , the kernel com-putation time would be reduced by 75% . By comparison to 77 , the average sequence length is reduced by about 87% since the topic number of topics is 10 in this paper. Hence, the computational complexity would be greatly reduced. 4.5 Does the Topic Sequential Order Matter? In this section, we will investigate whether the sequential order of topics matter in text classification. We compared the topic sequence kernel with other kernels including linear, polynomial, RBF kernel and sigmoid kernel. For these kernels, we do not rear-range the topics in a sequential order. We simply use the topic proportions as the weight of each topic. Then each document is represented by a feature vector with topic proportions. Experiments are conducted by varying the number of topics ( K ) from 5 to 20. From the experimental results, we found that no matter what kind of kernels we used, the performance remains the same for each kernel if the number of topics remains unchanged. Therefore, we name other kernels as ALL plus the number of topics. ALL_5 , for example, indicates the linear, polynomial, RBF or sigmoid ker-nel with the number of topics being 5. Similarly, ALL_10 , ALL_15 and ALL_20 are either of these kernels with the number of topics being 10, 15 and 20, respectively. 
Table 6 shows that the topic sequence kernel ( TSK ) gives the best micro-average and macro-average F-scores when compared to other kernels. This testifies our hypo-topics. As the Table 6 shows, among the other kernels, ALL_10 gives the best micro-average and macro-average F-scores when the number of topics is 10. Similar to the string kernel and word sequence kernel, topic sequence kernel considers the sequential structure between the symbols (characters/words/topics). In this paper, reduce the computational time cost. Initially, the LDA algorithm is used to extract posterior topic distri butions in each document and generate the topic sequence based on these topic distributions. Our observations suggest that the optimal result is ob-stability might be damaged if a document belongs to more than one category. 
We focused on topic sequence kernels which are based on the topics. One advantage is that topics are summaries of documents and they can well capture the semantics of documents. The other advantage is that number of topics per document is much less than the number of words in a document. This can bring the string kernel into practical usage, since string kernel computation is rather resource demanding. Topic sequence kernel is an extension of string kernel. Our work contributes to the structural document representation using topics instead of words or characters and to the reduction of computational runtime cost. 
