
Recommender systems have achieved much commercial success and are becoming increasingly popular in a wide vari-ety of practical applications. For example, online stores such as Amazon, iTunes and Walmart.com provide customized recommendations for additional products or services based on a consumer X  X  history. Since it is widely believed that even minor improvements in recommender systems can boost the profitability of e-commerce companies, these systems have been studied intensively by researchers in industry and in academia.

An important limitation of existing studies is that they as-sume that the properties of items (i.e. products) are static. However, an e-commerce company could tailor some prop-erties of a product for a particular customer, and that could dramatically improve the effectiveness of a recommendation. In particular, we argue that price is a controllable property that the recommender system should incorporate. A con-sumer might like a recommended product, but may reject it because the price is too high, and the purchasing de-cision could be changed by a personalized promotion. Of course, outside of the literature on recommender systems, the crucial role of pricing is widely recognized. Researchers in marketing, for example, have shown the importance of personalized promotion for increasing sales volume [17].
In this paper, we introduce personalized promotion into e-commerce recommender systems. The objective is to im-prove the effectiveness of product recommendations through customizing product price on an individual basis.

To achieve this goal, we create a novel auction/lottery mechanism to elicit consumer willingness to pay (WTP) for relevant products in an e-commerce setting. Using an on-line shopping website with products from Amazon, we re-cruit experimental subjects via Mechanical Turk and use responses elicited for some chosen products to predict a in-dividual X  X  WTP on other products. The predicted WTP and the given production cost enable us to find profitable per-sonalized prices. The data indicate that the new approach achieves nearly 200% improvement in gross profit when com-pared with Amazon X  X  default pricing strategy.

Our contribution is three-fold. First, to the best of our knowledge, our work is the first attempt to add personal-ized promotion to e-commerce recommender systems, and we measure recommendation performance with metrics of direct interest to industry. Second, we introduce a novel WTP elicitation procedure that is adapted to e-commerce systems. Most previous WTP elicitation studies are con-ducted in settings that are very different from an e-commerce Lens dataset) are used. It X  X  not clear how the approaches proposed by these pilot studies can be applied in a real e-commerce system with a large and varied set of users and many products of different types, brands and price ranges.
There is an established literature in economics and mar-keting showing that personalized promotion is an important marketing tactic for increasing sales volume [17]. Especially important for our paper is prior research, also in economics and marketing, on estimating a consumer X  X  WTP for a par-ticular product. Roughly speaking, there are four empirical approaches: estimating WTP from transactions data, direct surveys, indirect surveys or laboratory auctions. Transac-tion data is incentive compatible in that it represents actual purchase decisions. However, the transaction price is only the lower bound on WTP, and equally relevant non-purchase decisions are missing from transactions data. Direct surveys (see, e.g., [31, 15]) estimate a consumer X  X  WTP by directly asking indicate their maximum acceptable price for a given product [1]. However, as pointed out by Breidert et al. [9], direct surveys are not incentive compatible  X  the respon-dent is not motivated to reveal his or her true WTP, and often is motivated to understate it substantially. Indirect surveys offer respondents a list of alternative products (of-ten hypothetical products with their properties varied inde-pendently) and ask them to either to rank them according to personal preference [23] as in conjoint analysis, or simply to choose the most favored alternative as in choice-based conjoint analysis (CBC)[22][2, 3, 4, 5, 13, 14]. Similar to direct surveys, the conjoint analysis and CBC are not in-centive compatible. To overcome this problem, Ding et.al in [13] proposed an ICBC -incentive aligned choice-based con-joint method, which applies the BDM mechanism described below to the WTP inferred from conjoint analysis. Dong et al. [14] proposed ranking revealed products based on the WTP inferred from conjoint analysis data. The authors ar-gue their proposed approach is incentive aligned as subjects will receive top ranked products.

The remaining empirical approach is WTP elicitation via laboratory auction. The famous Vickrey auction [33], re-quires each bidder to a submit sealed bid (not seen by other bidders); the bidder with highest bid wins the auction and pays a price equal to the second highest bid. This proce-dure is incentive compatible. The intuition is that a per-son X  X  own bids only determine whether or not she wins the auction but never affects the price that she pays, and there-fore she has no incentive to understate (or overstate) her WTP. Closely related to the Vickrey auction is the Becker-DeGroot-Marschak (BDM) mechanism to elicit WTP [8]. Under BDM, each bidder submits a bid to purchase a prod-uct. A sales price is randomly drawn from an interval which covers all plausible bids. If that sales price is lower than a participant X  X  bid, then she receives the product and pays the sales price. The BDM is theoretically incentive compatible for the same reason as the Vickrey auction; indeed (although it was invented independently) it is equivalent to a Vickrey auction with two bidders, one human and the other an au-tomaton who bids randomly. Our own approach exploits the fact that BDM is easier to operate in an e-commerce setting since, unlike the Vickrey auction, it does not require gather-ing all participants X  bids in order to determine the winner.
Despite considerable research on WTP elicitation [26] and existing pilot research on pricing strategies when recom-mending, we still face several challenges when adapting ex-Equation 3 shows that the participant X  X  expected payoff is a quadratic function of her decision variable y 0 whose unique global optimum is reached when y 0 = y . In other words, it is in the subject X  X  best interest to state her WTP truthfully.
Note for later reference that when the participant indeed sets y 0 = y , her expected payoff can be written as In our experiment we set R L = 0 and R U = p , where p is the known outside (undiscounted) market price for the product. In this case, where y = k  X  p , and we refer to k as the normalized bidding price.
 What if the subject wants to bid above R U or below R L ? In principle, there is no problem; in the first case she obtains the object for sure and reveals that her true WTP exceeds R
U , and in the second case she keeps the cash for sure and reveals that her true WTP is below R L .

A potential problem does arise when the number of prod-ucts N &gt; 1. Running a separate BDM elicitation for several products that are substitutes for each other will motivate participants to underbid, since the probability of winning at least one of several bids is higher than winning a single bid. We eliminate this problem by having each participant enter a bid for each product, but only drawing a random bid for one of the products chosen randomly. That is, we conduct a lottery over which of the products will actually be avail-able to the participant. The expected payoff (or 1 /N of the expected payoff) is still maximized by truthfully reporting WTP for each product.

An extension of this lottery approach enables us to econ-omize payments in the experiment. We can also conduct a lottery over which participants actually get the cash C and perhaps one of the products. Again, that attenuates the expected payoff but doesn X  X  change where it is maximized.
Our lottery/auction procedure gives us training data for predicting a consumer X  X  WTP on a range of products that might be recommended. Given the values of y elicited from consumer u for product i , we run the regression where b 0 is the global bias and b u is the user bias, x ui is a feature vector representing information about consumer u , product i and their relationships. The functional form of the regression function f depends on the machine learning algorithm used, such as linear regression or gradient boosted trees. We used linear regression (LR) in our experiments. Thus our model parameters are: ( b 0 ,b u , w  X  ) = arg min where in our implementation the loss function L is quadratic.
Given the model parameters learned from the training dataset, we use Equation 6 to predict WTP for any user u and item i pair. To capture the uncertainty in our estima-tion, we assume that the true WTP value follows a Gaussian
Subjects recruited from Amazon Mechanical Turk were paid a participation fee of $0 . 50 for around 10 minutes to complete the experiment. Figure 1 is an overview of the pro-cedure. First, the subject sees a set of recommended prod-ucts produced by a standard recommendation algorithm. This step would be straightforward for a typical e-commerce system, which can find relevant products based on subject X  X  past purchasing or click data. Since our custom website don X  X  have the recruited subject X  X  purchasing history, we just let each subject search and identify at least 5 products which he or she thinks are worth buying; these are referred to as best value products. Then the subject proceeds to next step to rank their best value products in decreasing order of interest. Next, the system recommends a list of products us-ing Amazon X  X   X  X onsumer who bought this also bought these X  recommendations.

Having completed the steps labelled 1.1-1.3 in Figure 1, the recruited subject then participates in N BDM lotteries. That is, she enters a bid on each of N  X  5 products that she chooses from those recommended in the previous step, knowing that at most one of the products will be randomly selected to go through the BDM procedure. Finally, one of the subjects recruited that day is randomly selected as the lottery winner, and one of her chosen N products is randomly selected. If her bid on that product exceeds the random buyout price r , then she receives the product and $100 minus r . Otherwise she gets $100 cash. We showed earlier that truth telling (bidding one X  X  actual WTP) is the unique optimal strategy in BDM. However, earlier empirical research has shown that people may have misconceptions about the game and may not use the opti-mal bidding strategy [11]. Guided by the earlier research, we provided detailed explanations about the game rule and encouraged our subjects to practice with a bidding simulator to see how the outcome changes with different bids. Then each subject was required to take a quiz to check whether the participant understands the optimal bidding strategy. Only the 79 subjects who passed the quiz (out of 130 who responded initially) were allowed to participate in the game.
To clean the data, records with normalized bidding prices either below 10% or more than two times higher than the Amazon sale price are filtered out, as are the bids of subjects who took less than 5 minutes to complete the experiment. After cleaning, we ended up with 339 product bids from 54 subjects. The bid price is normalized using the product X  X  Amazon sale price, as k in Equation 5.

To construct x ui , we used the 9 features described in Ta-ble 1. The features are based on information collected from each experiment step.
RMSE (Root Mean Square Error) is the most commonly used metric and is naturally adopted in our experiments. However, to understand the commercial value of personal-ized promotion based on the estimation of consumer X  X  WTP, we go beyond RMSE and introduce seller profit to measure the effectiveness of WTP prediction. Seller profit of a sin-gle product purchase is simply the purchase price minus the cost of producing the product. For the proposed WTP pre-diction method in Section 3.2, the profit is expected profit given pricing via Equation 9. Hence where t  X  ui is the optimal personalized promotion price from Equation 9, y ui is the consumer X  X  true WTP and c i is the unit production cost of product i .
We compared the proposed approach with Amazon sale price with 0, 10% and 20% discounts and ZeroR. The Ama-zon sale price methods serve as baselines. ZeroR algorithm is a simple learning based method that find the mean value of y on the training data and simply set the price at the learned mean value, which means a fixed discount rate. In our experiment, the normalized bidding price for product i by ZeroR is 0 . 72 (i.e. the learned discount rate is 0 . 28). Our proposed algorithm LR sets the price at the point where the expected profit is maximized (Equation 9). All algorithms are evaluated by 10 fold cross validation.

In our experiments, we don X  X  know the production cost of each product. Because retail businesses commonly set price using a fixed markup on cost, we assume that production cost is a certain percentage of the sale price. We varied the percentage from 0 to 50% with an incremental step of 10%. Table 2 reports the corresponding profit and conversion rate for each algorithm at different production costs level.
Table 2 shows that our proposed method (i.e. LR) signif-icantly outperforms Amazon X  X  pricing strategy and simple learning method (i.e. ZeroR). All improvements are sta-tistically significant (p &lt; 5%). The LR algorithm max-imizes expected profit as in Equation 9, using  X  = 0 . 22 based on the training dataset. Is it important to model the uncertainty of WTP and set the price by maximizing the expected profit? The answer is yes. We compared the proposed method with simply setting the price at the most likely WTP (i.e. b 0 + b i + f ( x ui ,w )), and we found the simple method is statistically significantly worse. The feature weights of normalized bidding price k in the LR model may be of interest. Table 1 indicates that:
List price : normalized WTP decreases slightly as prod-uct X  X  list price increases.

Number of reviews : normalized WTP decreases as the number of reviews increases. This is counter intuitive, as consumers presumably would tend to perceive goods with more reviews as more valuable. Of course, the expression is reduced form and for normalized (not direct) WTP, so future work may be able to find an explanation.

Discount : Products with larger Amazon discounts have higher normalized WTP. Like the list price impact, this ef-fect probably also works mainly through the normalization (again recall Equation 5) but it is much stronger.
Average rating : Consumers are willing to pay more for products rated higher.

User rank : Not surprisingly, a consumer is willing to pay more for products she ranks more highly.

Switch brand : A consumer switching to a new brand tends to have lower WTP. This seems closely related to the predict each consumer X  X  WTP on a wide range of other prod-ucts. We demonstrated the feasibility of the proposed ap-proach in an experiment with real world products from Ama-zon and subjects recruited from Amazon Mechanical Turks. The results suggest that personalized promotion leads to sig-nificantly higher profits for sellers compared to the baseline pricing.

Our work also shows the viability of doing e-commerce ex-periments via crowdsourcing; our Mechanical Turk subjects turned out to be quite useful. Besides recommender sys-tems, the proposed approach also has practical implication for managerial marketing.

The work just presented is only first step; much remains to be done. First and foremost, personalized promotion and recommendation should be considered jointly within a uni-fied framework, and not remain as separate problems. Sec-ond, we focused on seller X  X  profit as the evaluation metric. However, we believe it is crucial to include consumer sur-plus into the objective function and we see no conceptual obstacles in doing so. Finally, as alluded to in the caveats, it will be crucial to study the longer term effects on con-sumer satisfaction with personalized promotion. We hope that researchers in industry, with better access to normal e-commerce consumers, will be inspired to do so. Part of this work is sponsored by the National Science Foundation under grant CCF-1101741 and IIS-0953908. Any opinions, findings, conclusions or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsors. [1] J. Abrams. A new method for testing pricing [2] G. M. Allenby, N. Arora, and J. L. Ginter.
 [3] G. M. Allenby, N. Arora, and J. L. Ginter. On the [4] N. Arora, G. M. Allenby, and J. L. Ginter. A [21] Y. J. Lim and Y. W. Teh. Variational Bayesian [22] J. J. Louviere and G. Woodworth. Design and analysis [23] Y. Marbeau. What value pricing research today. [24] M. Massoud and M. Abo-Rizka. A conceptual model [25] P. McNamee, C. Piatko, and J. Mayfield. JHU/APL [26] K. M. Miller, R. Hofstetter, H. Krohmer, and Z. J. [27] A. Paterek. Improving regularized singular value [28] E. Rich. User modeling via stereotypes. Cognitive [29] S. Robertson and I. Soboroff. The TREC-10 filtering [30] M. Srikanth, X. Wu, and R. Srihari. UB at TREC 11: [31] R. G. Stout. Developing data to estimate [32] H. R. Varian and W. Norton. Microeconomic analysis , [33] W. Vickrey. Counterspeculation, auctions, and [34] J. Wang and Y. Zhang. Utilizing marginal net utility [35] L. Wu, X. Huang, J. Niu, Y. Xia, Z. Feng, and [36] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness [37] Y. Zhang. Using bayesian priors to combine classifiers
