 ORIGINAL PAPER Scott MacLean  X  George Labahn Abstract We present a new approach for parsing two-dimensional input using relational grammars and fuzzy sets. A fast, incremental parsing algorithm is developed, moti-vated by the two-dimensional structure of written mathemat-ics. The approach reports all identifiable parses of the input. The parses are represented as a fuzzy set, in which the mem-bership grade of a parse measures the similarity between it and the handwritten input. To identify and report parses efficiently, we adapt and apply existing techniques such as rectangular partitions and shared parse forests, and introduce new ideas such as relational classes and interchangeability. We also present a correction mechanism that allows users to navigate parse results and choose the correct interpretation in case of recognition errors or ambiguity. Such corrections are incorporatedintosubsequentincrementalrecognitionresults. Finally, we include two empirical evaluations of our recog-nizer. One uses a novel user-oriented correction count metric, while the other replicates the CROHME 2011 math recogni-tion contest. Both evaluations demonstrate the effectiveness of our proposed approach. 1 Introduction It is generally acknowledged that the primary methods by which people input mathematics on computers (e.g., L A T E Maple,Mathematica)areunintuitiveanderror-prone.Amore natural method, at least on pen-based devices, is to use hand-written input. However, automatic recognition of handwrit-ten mathematical expressions is a hard problem. Indeed, even after forty years of research, no existing recognition system is able to accurately recognize a wide range of mathematical notation [ 5 , 9 ].

There are many similarities between math notation and other natural languages [ 6 ]. In particular, notations are not formally defined and can be ambiguous. For example, with-out contextual information, it is impossible to tell whether the notation u ( x + y ) denotes a function application or a multipli-cation operation. Such semantic ambiguities, along with the syntactic ambiguities generally associated with handwriting recognition, make math notation a challenging recognition domain. These difficulties are compounded by the two-dimensional structures prevalent in handwritten math. Many well-known approaches for recognition and domain mod-eling (e.g., Markov models, grammars, dictionary lookup) cannot be directly applied to the more complicated structures found in math notation.

The present work describes the math recognition system used in MathBrush, our pen-based system for interactive mathematics [ 17 ]. Using MathBrush, a user writes mathe-matical expressions as if they were using pen and paper. After entry, expressions may be edited and manipulated using com-puter algebra system (CAS) operations that are invoked by pen-based interaction. The output of CAS operations may be captured and further edited with the pen.

MathBrush is designed for real-time interaction, recogniz-ing and reporting parse results as the user is writing. Its recog-nizer must therefore be fast enough to update parse results in real-time and flexible enough to support a mix of typeset and handwritten input. However, the ambiguity present in hand-written math makes it all but impossible to develop a recog-nition system with perfect or even near-perfect accuracy. To ensure that the system remains usable when recognition is imperfect, we believe that it is essential to capture multiple parses simultaneously, and, by so doing, to allow the user to choose between alternatives in case of recognition errors. At the same time, the system must remain highly responsive, so efficiency is vital.

Motivated by these design considerations, this paper presents two main contributions in regard to recogniz-ing handwritten math notation: fuzzy relational context-free grammars (fuzzy r-CFGs) and relational classes. Fuzzy r-CFGs specifically address recognition problems in struc-tured, ambiguous domains. The definition of a fuzzy r-CFG incorporates not only the structure of the recognition domain, but also the uncertainty inherent in recognizing that structure. The grammar, thus, provides a formal model of the recogni-tion process itself and is presented in detail in Sect. 3 .
Relational classes are a generalization and formalization of the syntactic symbol classes used by other authors. They represent families of similar-looking mathematical expres-sions and replace the typical assumption of independence between parse tree branches. Parses within a relational class are assumed to be interchangeable within a larger parse with-out affecting the parse confidence. This interchangeability reduces the number of expressions that must be considered when forming parse trees.

Together, these two ideas form the basis for a two-step parsing process that captures all recognizable parses of the input, while avoiding the exponential runtime typically associated with such an approach. In the first step, terminal symbols and potential subexpression structures are identi-fied efficiently using a fuzzy r-CFG and associated parsing algorithms. This step generates a shared parse forest that simultaneously represents every recognizable parse tree. In the second step, particular parse trees are extracted from the shared parse forest in decreasing order of recognition confidence. Each tree represents the syntactic layout of a math expression and is rewritten to represent the expres-sion X  X  semantics. Trees are generated on demand as they are requested by the user through the interface of MathBrush. We use a modest set of relational classes to improve recognition accuracy while maintaining fast tree extraction. Although there may be exponentially many trees, extraction of a single tree remains fast, so that the user experiences no delay.
A secondary contribution of this paper is the correction count metric for evaluating recognition accuracy (Sect. 6 ). The metric provides an implementation-independent way to compare the effectiveness of recognizers that permit users to make corrections to their output in case of errors. Unlike the expression-level correctness rate that typically serves as a lowest common denominator for recognizer comparison, the correction count metric measures  X  X ow correct X  a particular recognition result is without reference to the implementation details of any one recognizer.

Grammar-based approaches have been proposed for rec-ognizing mathematical notation for decades X  X he next sec-tion surveys past and current related research. Section 3 presents fuzzy r-CFGs in an abstract setting, while Sect. 4 describes our two-step parsing process in detail. Section 5 provides some details of the symbol and relation classifi-cation systems used in MathBrush. Section 6 evaluates our math recognition system empirically on a publicly available data set. Finally, Sect. 7 offers some concluding remarks and future research directions. 2 Related work There are a large number of existing grammar formal-isms addressing the problem of two-dimensional parsing; Marriott, Meyer, and Wittenberg [ 29 ] provide an extensive survey. Generally, each formalism strikes a balance between the severity of the geometric constraints it imposes on a parser X  X  input and the amount of time and space required to parse that input. Some formalisms, like general relational grammars and graph rewriting grammars, impose so little constraintthatitisintractabletoparsethelanguagestheygen-erate. Our work most closely resembles Tomita X  X  2D-CFGs [ 36 ] and the positional grammars developed by Costagliola and others [ 11 ].

Both of these formulations fit input elements into a rect-angular grid-based structure. In the 2D-CFG case, each grid cell must be filled by a terminal, and adjacent grid regions of the same height (width) may be merged into horizontal (vertical) concatenations by applying grammar productions. However, this formalism is not rich enough to support math-ematical notation. For example, consider fitting the symbols of the expression a b + 1 into a regular grid using one terminal per cell. The fraction requires three vertically adjacent cells, each of which has a neighbor to the right. But these three neighboring cells cannot be filled by the plus sign alone, and none are allowed to be empty. The expression can therefore not be represented by a 2D-CFG in a natural way.

The positional grammar formalism allows grid cells to be empty, avoiding the problem we just saw with 2D-CFGs. Positional grammar productions take the general form A  X  A r 1 A 2  X  X  X  r k  X  1 A k , where the A i are terminals and/or non-terminals, and the r i are relations describing spatial config-urations. Crucially, given a current grid cell in the input and a grammar relation, the next cell in which parsing contin-ues must be uniquely determined. Positional grammars, thus, require distinct language elements to be parsed by distinct paths through the input. Consider again the example a b + which one might naturally parse by following the path a  X   X   X  b  X + X  1, where the arrows indicate grammar relations. But then how would one parse a b + 1 ? After parsing the b ,the  X  relation can direct the grammar to move hori-zontally, as required for the second case, or horizontally and upward, as required for the first case. But it cannot choose one or the other depending on the circumstances. Again, the formalism is too weak for math notation.

Our work adopts aspects of both of these formalisms. We include multiple geometric relations similarly to positional grammars, but restrict the form of grammar productions, so that a single production may only use a single relation. This restriction facilitates efficient parsing via partitioning the input into horizontal and vertical concatenations X  X n aspect of Tomita X  X  work that we share (along with Chou, as discussed below). Unlike 2D-CFGs, we do not require concatenated regions to have the same size and do not fit ter-minal symbols into a grid. And we omit the requirement of positional grammars that each relation direct the parser to a unique successor position. Instead, we use fuzzy sets to main-tain multiple successor positions and associated confidence scores.

For math recognition in particular, grammar models are occasionally used through rule-based approaches, as in AlgoSketch [ 21 ] and its relatives, which take an approach similar to those of Zanibbi et al. [ 41 ] and Rutherford [ 33 ]. In Zanibbi X  X  DRACULAE system, the input is processed in three passes based on tree rewriting. The first pass identi-fies baselines, which it organizes in a tree structure in which each edge represent geometric relationships. The second pass rewrites geometric structures as syntactic structures. (E.g., a horizontal line above one subexpression and below another would be rewritten as a fraction). The third pass rewrites syntactic structures as semantic parse trees suitable for com-putation. (E.g., the fraction would be rewritten as a division operation.)
Grammars may also be used as a verification step to con-firm the validity of an expression recognized by some other means (e.g., [ 13 , 37 ]). Garain and Chaudhuri, for example, view an expression as a collection of horizontal baselines, which they call  X  X evels X . Each symbol is placed in a level as it is drawn. After the entire drawing is complete, it is segmented into horizontal and vertical  X  X tripes, X  which are merged based on grammar rules and the levels. The result-ing parse is validated against a grammar that generates T strings. If the parse is not valid, then alternatives are sought from the symbol recognizer.

Often, grammars are used as a flexible rule set to guide a recognition process based on traditional parsing techniques. Such use goes back as far as Anderson X  X  original graphi-cal rewriting system [ 2 ]. Already many of the features of modern grammar-based recognition systems are present in Anderson X  X  work. In his approach, each grammar produc-tion is equipped with predicates that determine how a set of input elements should be partitioned for parsing. The predicates include rules applying to the minimum, maxi-mum, and central x -and y -coordinates of expression and symbol bounding boxes, as well as threshold-based rules to ensure proper alignment of neighboring subexpressions. Productions not containing a terminal symbol on the RHS are restricted to contain only two non-terminals, as in Chomsky Normal Form. In such cases, Anderson includes a restriction similar to one later proposed by Liang et al. [ 22 ](whichwe will encounter in Sect. 4 ) that input elements should be par-titioned by drawing a straight line in the plane that splits the elements into two subsets and does not intersect any of them. Another early grammar-based approach was proposed by Belaid and Haton [ 4 ]. In it, a symbol is chosen by the system as a starting point, and the symbols around it are divided into  X  X ones X  depending on context and grammar productions. The zones are then processed recursively in a top-down parsing scheme directed by operator identities. If the union of zones does not cover the desired subset of the input (i.e., the whole input for a complete parse), then the grammar is searched for rules that derive the starting point operator, and parsing proceeds in a bottom-up fashion.

Our ordering assumption, described in Sect. 4 , which treats grammar productions as either vertical or horizontal concatenation, was first developed by Chou [ 10 ], who used a stochastic grammar to recognize synthetic binary images of math expressions. In Chou X  X  model, each grammar pro-duction is assumed to be equally likely, terminal symbols are assigned probabilities based on Hamming distance between the input image and a template, and geometric relation-ships between symbols and subexpressions are determined by fixed, non-probabilistic rules about symbol size and base-lines. Symbol ambiguity is permitted up to a predetermined probability threshold, and relation ambiguity is permitted only in the case of horizontal concatenation where base-lines differ by one pixel or less. The most likely parse tree is obtained by a variant of the Cocke-Younger-Kasami (CYK) CFG parsing algorithm.

Winker et al. [ 39 ] later proposed an approach that pro-vided for some ambiguity in geometric relationships, but not in terminal symbol identities. In their system, the space sur-rounding certain mathematical operators (e.g., fraction bar, summation sign, square root) is divided into a number of regions. Symbols lying in different regions may lead to dif-ferent parses, and each of these possible parses is maintained and reported to the user. However, each time an ambigu-ity is detected, the parse is duplicated and both possibilities explored independently. This may lead to an exponential run-time, as well as an exponential number of parse results.
Miller and Viola [ 30 ], expanding on earlier work of Hull [ 16 ], proposed an approach in which a graph is dynamically created, the edges of which represent the application of gram-mar rules. They weight the graph edges by probability esti-mates of the parse results and use A  X  to navigate to the most likely parse. The probability of a parse tree is the product of terminal symbol recognition probabilities and probabilities derived from geometric rules. Each production is assumed to be equally likely. Miller and Viola also introduced a convex hull constraint. Instead of parsing all 2 n subsets of the input, this constraint restricts the parser to only those subsets that are geometrically convex, leading to a significant speedup.
To allow for ambiguous parse results, Miller and Viola divided the set of terminal symbols into four syntactic clas-ses and allowed the parser to choose the class that best fits into the expression X  X  geometry. This does permit some ambiguity between symbols, but if the top-ranked candidate within a class is incorrect, or if the syntactic class eventually settled upon by the parser is incorrect, there is no way to choose an alternative. The geometric rules used in the approach are not described, making it difficult to assess how much ambiguity in an expression X  X  geometry they permit.

A CYK-based approach was developed by  X lvaro et al. [ 1 ] for parsing typeset mathematics using 2-D stochastic CFGs. In their approach, the grammar is written in Chomsky nor-mal form, and each production is associated with a geomet-ric relation. Probability distributions over the relations were defined manually, and distributions over terminal symbols were derived from neural network outputs. These values are incorporated into a CYK parsing algorithm that outputs the most likely overall parse.

The four approaches just cited use probabilistic grammars to recognize math expressions. The main characteristic of probabilistic grammars is that one can assign probabilities to individual productions, and hence to derivation sequences and language elements. Each of the approaches applies a uni-form distribution to grammar productions and treats terminal symbols and/or geometric relationships as probabilistic. In this context, the use of a uniform distribution over produc-tions can be interpreted as making no assumptions about the composition of mathematical expressions aside from their formal properties, which are encoded in the grammar rules themselves. This lack of assumptions means that, at any given parsing step, the parser is not influenced by the production distribution and considers all its options to be equally likely. It also provides a neutral starting point from which train-ing algorithms may learn more specific production probabil-ities. But even the uniform distribution cannot completely avoid bias during parsing, as stochastic CFGs are inherently biased toward parse trees of small height. Each derivation step reduces the probability of a parse, so, all else being equal, a more deeply nested tree will be considered to be less probable than a shallow tree. Approaches using stochastic grammars with uniform distributions and no training inherit this downside without benefiting from the ability to use a realistic distribution over productions.

The multiplicative model used by these approaches assumes independence of individual symbols and subexpres-sions. This assumption is invalid (though understandable for the sake of runtime efficiency) in many domains, includ-ing math recognition. Consider, for example, the expression ax 3 + bx 2 + c ? + d , and suppose the system is trying to determine the identity of the letter indicated by a question mark. Assuming independence, it may be more likely that the missing letter is classified as X or  X  than the x that a human reader would expect. Miller and Viola [ 30 ] also point out the necessity of incorporating contextual information. Indeed, they give a convincing quantitative argument as motivation for their division of terminal symbols into syntactic classes which we discussed earlier. But it is unclear how these classes are incorporated into their geometry rules and how they fit into their probabilistic model, which assumes independence of all terminals.

As alluded to above, independence assumptions are known to degrade parser performance in natural language applications. A common technique used to improve perfor-mance is to use the geometric mean of probabilities rather than their product [ 28 ,  X 12.1]. This leads to the same type of scoring function we propose in the next section for use with fuzzy r-CFGs. Our approach is inspired by probabilistic parsing techniques, but is not, strictly speaking, a valid prob-abilistic method. While some approaches exist for relaxing the independence assumption in natural language processing (e.g., that of Caraballo and Charniak [ 7 ]), they use n -gram models, which may only be applied to mathematics when large corpora of realistic expressions are available. One such corpus, owned by Microsoft, has been used by Shi, Li, and Soong [ 35 ] to develop a math recognizer based on hidden Markov models. In their method, an observed stroke sequence is used to compute likelihoods of stroke bound-aries, symbol identities, and relations between symbols. In contrast to the work cited earlier, probability distributions for symbol and relation bigrams are determined empirically from the corpus data. A symbol graph representing alterna-tive sequences of symbol identity assignment is created, and the optimal sequence is extracted and passed to a separate math structure analysis system. The approach of Shi et al. is sensitive to writing order X  X ymbols must be written with consecutive strokes, and symbols adjacent in time must also be adjacent (in some model-specific sense) in the expression being written. These temporal restrictions are insufficient for our purposes. For example, a user may draw two parentheses separatedbyemptyspaceandthenfill inthespacewithafrac-tion. We wish to recognize such inputs, in which consecutive strokes are not directly connected by geometric relationships.
Even so, such probabilistic models are promising and should be extended and generalized once large realistic, cor-pora of handwritten math expressions are generally available. In the meantime, it is reasonable to model the ambiguity in math recognition using fuzzy sets rather than probabil-ity distributions. Recent work by Zhang et al. [ 42 ] added support for fuzzy geometric relations to FFES, a math rec-ognition system using Zanibbi X  X  DRACULAE recognizer, so that it could report multiple interpretations of the user X  X  input. However, the number of interpretations can only be controlled by adjusting thresholds. The ambiguous interpre-tations are pursued independently and reported to the user simultaneously, so recognition time is proportional to the amount of ambiguity in the drawing.

Fitzgerald et al. [ 12 ] proposed an approach to math recog-nition which models geometric relations as fuzzy constraints on the productions of an attribute grammar. In case of strong ambiguity in the identity of a symbol or geometric relation, the input is parsed repeatedly, choosing different identities eachtimeinabest-firstexplorationofalternatives.Two-sided sigmoid functions of geometric features are used to represent the membership grades in fuzzy geometric relation. It is not clear if these membership grades combine into grades for entire parse trees, or if parses are simply reported as they arise from the best-first parsing process. On one hand, the system is efficient in the sense that only strong ambiguities cause it to investigate alternative parses, but on the other, such alternatives must be parsed independently and serially, reducing efficiency. What constitutes a strong ambiguity is hard-coded, so the correct parse may fail to be reported if the membership grade of a particular geometric relation falls below a threshold. Like the work of Fitzgerald et al, our method of tree extraction X  X resented in Sect. 4.3  X  X mploys a best-first search strategy. But, it can report as many trees as the user requests, making available all parse trees with non-zero recognition confidence.

Our work avoids many of the difficulties identified in this section. It permits ambiguity of terminal symbols as well as geometric relations. It does not limit the number of alternative parses and allows users to select between them, but it does not construct all of the (potentially exponentially many) interpretations of the user X  X  writing before producing output. By using fuzzy sets, we need not be constrained by probabilistic independence and can use context-sensitive scoring functions when combining subex-pressions into larger expressions. By generating interpreta-tions on demand as the user requests them, only as much work is done as is necessary to produce the correct interpre-tation. 3 Fuzzy relational context-free grammars Recognition may generally be seen as a process by which an observed, ambiguous, input is interpreted as a certain, structured expression. Fuzzy r-CFGs explicitly model this interpretation process as a fuzzy relation between concrete inputs and abstract expressions. The formalism, therefore, captures not only the idealized, abstract syntax of domain objects (as with a typical context-free grammar), but also the ambiguity inherent in the recognition process itself. In this section, we define fuzzy r-CFGs, give examples of their use, and describe fuzzy parsing in an abstract setting. 3.1 Review of fuzzy sets Recall that a fuzzy set  X  X is a pair ( X , X  ) , where X is some underlying set and  X  : X  X  [ 0 , 1 ] is a function giving the membership grade in  X  X of each element x  X  X .A fuzzy relation on X is a fuzzy set ( X  X  X , X ) . The notions of set union, intersection, Cartesian product, etc. can be similarly extended to fuzzy sets. For details, refer to Zadeh [ 40 ]. To denote the grade of membership of a in a fuzzy set  X  X , we will write  X  X ( a ) rather than referring explicitly to the name of the membership function, which is typically left unspecified. By x  X   X  X = ( X , X ) , we mean  X ( x )&gt; 0, and by |  X  X | we mean the number of elements having non-zero membership grade, rather than the sum of membership grades over x  X  X . 3.2 Fuzzy r-CFG intuition Before defining fuzzy r-CFGs formally, we motivate the def-inition through an example. Consider the top expression in Fig. 1 : reasonable interpretations include Ax + b , Ax + A x + b , A x tb , etc. There are three important points to note. The identity of each terminal symbol is ambiguous. (Gen-erally, even which strokes ought to be combined to form a distinct symbol may be ambiguous, as in the bottom pair of expressions in the figure. Does the first contain a binomial term or a fraction? Is the second km or lcm ?) The geomet-ric relationships between symbols and subexpressions deter-mine the mathematical semantics with which those symbols should be interpreted (e.g., Ax vs. A x ). And, given a partic-ular choice of symbol identities and spatial relationships, all remaining ambiguity is semantic. In cases of semantic ambi-guity (i.e., the same notation representing multiple distinct expressions, as in u ( x + y ) ), it is not possible for a syntactic parser to reliably obtain the correct parse.

The rules governing the assembly of mathematical expres-sions from their component parts are known with certainty. Uncertainty only arises because the identities of and rela-tionships between handwritten symbols are ambiguous. This ambiguity is a property of each particular drawing, not of the formal structure of math expressions. As such, it is not nec-essary to assign probabilities or fuzzy membership grades to each production in our grammar, though such an extension could certainly be made to incorporate prior domain knowl-edge. Instead, we model formally only the ambiguity arising from symbol and geometric relation classification processes.
Thisrepresentsasignificantdifferencebetweenourmodel and traditional fuzzy grammars. Typically, fuzzy grammars are a straightforward modification of probabilistic grammars in which each non-terminal is considered to be a fuzzy set, and its productions are each assigned a membership grade [ 20 ]. Such grammars generate languages in which each string has a particular grade of membership, just as each string in a stochastic language is derived with a particular probability.
In our approach, the language itself is not fuzzy. It consists of exactly those math expressions derivable from the gram-mar X  X  start symbol. Fuzziness represents the degree to which a particular drawing is compatible with a particular math expression in the grammar X  X  language. It is therefore not a property of an expression, but of the input, and it provides a way to measure the similarity between a handwritten input and one of its potential interpretations as a math expression. 3.3 Fuzzy r-CFG definition Fuzzy relational context-free grammars are formally defined as follows: Definition 1 A fuzzy relational context-free grammar G is a tuple ( , N , S , T , R , r , P ) , where:  X  is a set of terminal symbols;  X  N is a set of non-terminal symbols;  X  S  X  N is the start symbol ;  X  T is a set of observables;  X  R is a set of fuzzy relations on I , where I is the set of  X  r is a fuzzy relation on ( T , ) ; and  X  P is a set of productions, each of the form A 0 r  X 
The form of a fuzzy r-CFG is generally similar to that of a traditional context-free grammar. We point out the differ-ences below. 3.3.1 Observables The set T of observables represents the set of all possible concrete inputs. Formally, T must be closed under union and intersection. In practice, for online recognition problems, an observable t  X  T is a set of ink strokes, each tracing out a particular curve in the ( x , y ) plane. 3.3.2 Set of interpretations While typical context-free grammars deal with strings ,we call the objects derivable from fuzzy r-CFGs expressions . Any terminal symbol  X   X  is an expression. An expression e may also be formed by concatenating a number of expres-sions e 1 ,..., e k by a relation r  X  R . Such an r -concatena-counts the number of terminal symbols that appear in it. For example, the size of A x is | A x |= 2 and the size of A x + b is | ( A x )  X + X  b |= 4. In these expressions, the parentheses indicate subexpression grouping, not termi-nal symbols. The arrows indicate spatial relationships corre-sponding to general writing direction; they are described in detail below.

The representable set of G is the set E of all expressions derivable from the non-terminals in N using productions in P . It may be constructed inductively as follows: For each terminal  X   X  , E  X  = {  X  } .
 For each production p of the form A 0 r  X  A 1  X  X  X  A k , E For each non-terminal A , E and finally, E =
The set of interpretations is I = T  X  E . A pair in I corre-sponds to the interpretation, through recognition, of a partic-ular observable as a particular expression. We make I fuzzy by assigning membership grades to each interpretation. As described in the motivation section, the membership grade I (( t , e )) measures the degree to which the observable t is compatible with the expression e , as measured by the fuzzy relations in the grammar. Note that the (crisp) language gen-erated by G is E S , where S is the start symbol.

The language structure generated by a fuzzy r-CFG is sim-ilar tooneweusedinprevious workonsketchcorpus creation [ 24 ]. In that formulation, a grammar was used as a generative language model for producing random mathematical expres-sions. Context-sensitive probabilities were assigned to pro-duction rules, similarly to stochastic CFG applications. More importantly, recognition processes were not involved, so the relations acted only on expressions, not on interpretations. This distinction reflects the grammar X  X  purpose in each case: as a language model for the generation application and as a model of sketch interpretation for the recognition application that we address in this paper. 3.3.3 Relations The set R contains fuzzy relations that give structure to expressions by modeling the relationships between subex-pressions. These relations act on interpretations, allowing context-sensitive statements to be made about recognition in an otherwise context-free setting.

The necessity of context-sensitive evaluation was men-tioned in Sect. 2 and is further illustrated by Fig. 2 .The expression shown in the figure may be best interpreted as A or AP depending upon the case of the P symbol. Denote the A symbol by t 1 and the P symbol by t 2 .Let  X  R be a fuzzy relation for diagonal spatial relationships and  X  be similar for horizontal adjacency relationships. Then we expect that and  X  ( ( t
That is, the membership grade of a pair of interpretations in a spatial relation should vary, depending on the expressions (i.e., the context) involved. Given explicit membership func-tions, we could evaluate whether or not these expectations are met. 3.3.4 Terminal relation The relation r models the relationship between observables and terminal symbols. In practice, it may be derived from the output of a symbol recognizer. For example, if a subset t of the input observable was recognized as the letter b with, say, 60 % confidence, then one could take r (( t , b )) = 0 . 6. 3.3.5 Productions The productions in P are similar to context-free gram-mar productions in that the left-hand element derives the sequenceofright-handelements.Thefuzzyrelation r appear-ing above the  X  production symbol indicates a requirement that the relation r is satisfied by adjacent elements of the RHS. Formally, given a production A 0 r  X  A 1 A 2  X  X  X  A k if t i denotes an observable interpretable as an expression e i derivable from A i (i.e., e i then for k i = 1 t i to be interpretable as ( e 1 r  X  X  X  re (( t 3.4 Examples The following examples demonstrate how fuzzy r-CFG pro-ductions may be used to model the structure of mathematical writing. We use five binary spatial relations: ,  X  , ,  X  , . The arrows indicate a general writing direction, while denotes containment (as in notations like 1. Suppose that [ADD] and [EXPR] are non-terminals 2. The production [SUP]  X  [EXPR][EXPR] models 3. The following pair of productions models the syntax of 3.5 Semantic expression trees and textual output Fuzzy r-CFG productions model the two-dimensional syn-tax of mathematical expressions. To represent mathemati-cal semantics, each production is also associated with rules for generating textual output and math expression trees with semantic information. For example, consider again the pro-duction [ADD] a MathML string would be written in our grammar format as notation indicates that the string representation of the n th RHS element should be inserted at that point. A rule to generate a semantic expression tree would be written ADD (%1,%3) . This rule would generate a tree with the root node labeled with addition semantics ( X  ADD  X ) and two sub-trees. Similarly to the string case, the % n notation indi-cates that the tree representation of the n th RHS element should be used as a subtree. Hence, the first child tree cor-responds to the left-hand operand of the addition expres-sion, and the second child tree corresponds to the right-hand operand. 3.6 The fuzzy set of interpretations Because of ambiguity, there are usually several expres-sions that are reasonable interpretations of a particular input observable t  X  T (e.g., A p and AP are both rea-sonable interpretations of Fig. 2 ). We represent all of the interpretations of t as a fuzzy set I t of expressions. The membership function of I t gives the extent to which the structure of an expression matches the structure of t ,as measured by r and the other grammar relations. This set can be thought of as a  X  X lice X  of the fuzzy set I of inter-pretations mentioned above; i.e., I t = { e : ( t , e )  X  remains to specify the membership function I t ( e ) = I ( concretely.

For cleaner notation, assume that the grammar produc-tions are in a normal form such that each production is either of the form A 0  X   X  , where  X   X  is a terminal symbol, or of the form A 0 r  X  A 1  X  X  X  A k , where all of the A i are non-terminals. This normal form is easily realized by, for each  X   X  , introducing a new non-terminal X  X  , replacing all instances of  X  in existing productions by X  X  , and adding the production X  X   X   X  .

The set I t of interpretations of t is then constructed as fol-lows. For every terminal production p of the form A 0  X   X  define a fuzzy set I p t = {  X  } , where I p t ( X ) = r ( ( I ( X ) = 0for  X  =  X  .

For every production p of the form A 0 r  X  A 1  X  X  X  A k , define a fuzzy set (taking the union over all partitions of t ) I where I
There is room for experimentation when choosing the membership function of I p t rule in fuzzy systems is to take the minimum of all rele-vant membership grades. In the present context, this rule has the disadvantage that all parses sharing a low-scoring sym-bol or relation are assigned the same score. Zhang et al. [ 42 ] found that using multiplication when combining member-ship grades helped to prevent ties. We, therefore, compute the membership grade of an r -concatenation e = e 1 r  X  X  X  in I p t I
As an expression always contains exactly one more ter-minal than relation, this function assigns as an expression X  X  membership grade the geometric mean of the grades of all of its components. Geometric averaging preserves the tie-breaking properties of multiplication while normalizing for expression size.

Given all of the I p t , the fuzzy set of interpretations for a particular non-terminal A  X  N is I and the fuzzy set of interpretations of an observable t is I I , where S is the start symbol.

An expression e is in I t if t is interpretable as e and e is derivable from the start symbol S . The recognition prob-lem is therefore equivalent to the extraction of elements of I ( t being the user X  X  input) in decreasing order of membership grade. 4 Parsing fuzzy r-CFGs There are two particular properties of fuzzy r-CFGs that make parsing difficult. Like other relational grammars, the languages they generate are multi-dimensional. This pre-vents the straightforward application of common parsing techniques like Earley X  X  algorithm, which assume a simply ordered sequence of input tokens. Multi-dimensionality also complicates the task of deciding which subsets of the input may contain a valid parse. Furthermore, because our input is ambiguous, we require a parser to report all recognizable parse trees. Since there may be exponentially many trees, some effort is required to ensure a reasonable running time. This section presents two formal assumptions on the struc-ture of the relations of fuzzy r-CFGs and describes how to constructanefficientfuzzyr-CFGparserusingthoseassump-tions. Detailed algorithms are omitted but may be found in a technical report [ 27 ]. 4.1 Representing I t as a shared parse forest In Eq. 2 , each set I p t sions. It is therefore infeasible to identify or construct each expression individually before reporting parse results to the user. This problem is similar to that of parsing ambiguous languages, in which the same input may be represented by many parse trees. Indeed, the language of math expressions is ambiguous even in the absence of syntactic fuzziness due to the semantic ambiguities identified earlier. Parsing ambigu-ous languages is a well-studied problem; we adopt the shared parse forest approach of Lang [ 18 ], in which all recognizable parses are simultaneously represented by an AND-OR tree. For example, consider again the expression shown in Fig. 1 , along with the following toy grammar: [ST]  X  [ADD] | [TRM] [ADD] [TRM]  X  [MUL] | [SUP] | [CHR] [MUL] [SUP]  X  [CHR][ST] [CHR]  X  [VAR] | [NUM] [VAR]  X  a | b |  X  X  X  | z [NUM]  X  0 | 1 |  X  X  X  | 9
Figure 3 depicts a shared parse forest representing some possible interpretations of Fig. 1 . In the figure, the boxed arrows are AND nodes. Those arrows indicate the relation that links derived subexpressions. The ovals are OR nodes representing derivations of non-terminals on particular sub-sets of the input. The circles relate subsets of the input with terminal symbols from the grammar. Simple productions of the form [CHR]  X  [VAR] , for example, have been omit-ted for clarity. Any tree rooted at the [ST] node that has exactly one path to each input element is a valid parse tree. This shared parse forest captures, for example, the expres-sions Ax + b , AX + 6 , A x + 6 , A X tb , etc. If an expression is incomplete (e.g., ( x + y without the closing parenthesis), then no parse will exist for the correct interpretation. How-ever, other parses using different interpretations of the input may exist (e.g., lx + y or C x ty ).

Parsing a fuzzy r-CFG may be divided into two steps: forest construction, in which a shared parse forest is cre-ated that represents all recognizable parses of the input, and tree extraction, in which individual parse trees are extracted from the forest in decreasing order of membership grade. We describe each of these steps in turn. 4.2 Shared parse forest construction Because the symbols appearing in a two-dimensional math expression cannot be simply ordered, one would naively have to parse every subset of the input in order to obtain all possi-ble parses. Similarly, recall from Eq. 2 that I p t is constructed from a union taken over all partitions of t . It is intractable to take this union literally. To develop a practical parsing algo-rithm, we introduce constraints on partitions so as to limit how many must be considered. The constraints are based on the two-dimensional structure of mathematical notation. 4.2.1 The ordering assumption and rectangular sets Define two total orders on observables: &lt; x orders observ-ables by minimum x-coordinate from left to right and &lt; orders observables by minimum y-coordinate from top to bottom. We take the y axis to be oriented downward. Asso-ciate each relation r  X  R with one of these orders, denoted ord r . ord r is the dominant writing direction used for a par-ticular relation. For math recognition, we use ord  X = ord = ord = ord = &lt; x , and ord  X = &lt; y .

Informally, we assume that each geometric relation r  X  R is embedded in either &lt; x or &lt; y . Thus, we may treat any grammar production as generating either horizontal or verti-cal concatenations of subexpressions, making the partition-selection problem much simpler.

More formally, denote by min d t the element a  X  t such that a &lt; d b for all b  X  t aside from a and define max similarly.
 Assumption 1 ( Ordering )Let t 1 , t 2 be observables, and let e 1 , e 2 be representable expressions. We assume that r ( ( t
The ordering assumption says that, for a parse to exist on t  X  t of t 2 along the dominant writing direction of the expression being parsed. For example, in Fig. 1 , to parse A x + b in the obvious way requires that the A begins before the x , and the + begins after the x but before the b , when the symbols are considered from left to right (i.e., ordered by &lt; x ).
Similarly, we could formulate a production for fractions as [FRAC] would require that the bottom symbol of the numerator began before the fraction bar, and the fraction bar began before the top symbol of the denominator, when considered from top to bottom (i.e., ordered by &lt; y ).

Liang et al. [ 22 ] proposed rectangular hulls as a subset-selection constraint for two-dimensional parsing. A very sim-ilar constraint that we call rectangular sets is implied by the ordering assumption.
 Definition 2 ( Rectangular set/partition ) Call a subset t of t rectangular in t if it satisfies t = a  X  t : min Call a partition t 1  X  X  X  X  X  t k of a rectangular set t rectangular if every t i is rectangular in t .

From the definition of &lt; x and &lt; y ,aset t that is rectangu-lar in t must include all input elements in t whose left edge lies between the left-most left edge of an element in t and the right-most left edge and whose top edge lies between the top-and bottom-most top edges of elements of t .
 Proposition 1 Let t  X  T be an observable, and let p be a production of the form A 0 r  X  A 1  X  X  X  A k . Under the order-ing assumption, if ( e 1 r  X  X  X  re k )  X  I p t t  X  X  X  X  X  t Proof Let d = ord r , and choose any t i . We must show that t = a  X  t : min
It is clear that t i is a subset of the RHS, so suppose that there is some a  X  t in the RHS put into t j = t i by the partition of t .If j &lt; i , then ( t
By the assumption, max
But min d t j  X  d a  X  d max d t j since a  X  t j , and min d t i  X  d a  X  d max d t i since a is in the RHS, so min d t i  X  d a  X  d max d t j , a contradiction. A similar con-tradiction can be obtained in the case where j &gt; i .
Rectangular sets are the natural two-dimensional general-ization of contiguous substrings in one-dimensional string parsing. This definition could be generalized to arbitrary dimension, giving  X  X ypercube sets X  of input elements.
Following Liang et al, notice that any rectangular set u  X  can be constructed by choosing any four input elements in t and taking them to be represent the left, right, top, and bottom boundaries of the set. There are therefore O | t | 4 rectangular subsets of t . If we instead naively parsed every subset of the input, there would of course be 2 | t | subsets to process. The ordering assumption, thus, yields a substantial reduction in the number of subsets that must be considered for parsing.
Liang et al. define a rectangular hull of a set of input ele-ments to be their geometric bounding box, and they parse only those sets whose rectangular hulls have a null intersec-tion. I.e., no bounding box of a subset selected for parsing can intersect that of any other parsed set. This formulation causes problems for containment notations like square roots, as well as for somewhat crowded or messy handwriting styles, which ourrectangularsetformulationavoids.Forexample,consider the square root expression on the left of Fig. 4 . The rectan-gular hulls of the root symbol and its argument are shown as solid boxes and are the (union of) geometric bounding boxes of the symbols. Note that the rectangular hull of the square root symbol intersects (in fact contains) that of its contents. The argument cannot be separated from the operator into non-intersecting hulls.

When considering input subsets as rectangular sets, we do not use the natural geometric bounding box of the strokes, as Liang et al. do. Instead, we take only the minimal x -and y -coordinates of each stroke and consider the bounding box (or rectangular hull) of those points. The  X  X oundary X  of the root symbol in Fig. 4 is thus the single point at the top-left of its bounding box, and the boundary of the rectangular set representing the argument 2  X  is shown as a dotted box. By using minimal coordinates instead of complete bounding boxes, the rectangular set boundaries do not intersect.
Similarly, in the expression on the right of Fig. 4 , the rect-angular hull of opening parenthesis intersects the hull of the a symbol and that of the closing parenthesis intersects the hulls of both the 3 and the exponent 2. As before, the expression cannot be partitioned such that the required subsets X  hulls are non-intersecting. But the boundary of the rectangular set rep-resenting a  X  3 (indicated by a dotted box) extends only to the left edge of the 3 symbol. The boundaries of the parentheses and the exponent are, as for the root sign, single points at the top-left corner of their bounding boxes. This small change X  taking the left-and top-most coordinates of strokes as their representative points,  X  X paces out X  overlapping writing and facilitates non-intersecting partitions.

Figure 5 illustrates schematically the recursive rectangular partitioning of an expression, following the expected parse central dotted vertical line indicates a rectangular partition into the sum symbol (with limits) and the summand. In the summand, for example, the two horizontal dashed lines indi-cate a rectangular partition into the numerator, fraction bar, anddenominator, andthenumerator anddenominator arefur-ther partitioned into rectangular sets, each containing a single symbol. Note that resulting boxes in the figure are meant to emphasize the hierarchical structure of the partition. They do not indicate the geometric bounding boxes of the rectangular sets. 4.2.2 Parsing algorithm Using the restriction to rectangular partitions derived above, it is straightforward to adapt the CYK bottom-up parsing algorithm so that is generates shared parse forests for fuzzy r-CFGs. Detailed algorithms are given in a technical report [ 27 ].ExperimentswiththeCYKapproachshowedthat,while all rectangular sets must be enumerated and parsed, relatively few actually contribute to valid parse trees. A similar obser-vation was made by Grune and Jacobs [ 15 ] in the case of ambiguous languages. The algorithm X  X  runtime, while pre-dictable, is always the worst-case time.

Instead of CYK, we, therefore, adapted to fuzzy r-CFGs, a tabular variant of Unger X  X  method for CFG parsing [ 38 ]. In this approach, we assume that the grammar is in the normal form described in Sect. 3.6 . At a high level, the algorithm parses a production p on an input subset t as follows: 1. If p is a terminal production, A 0  X   X  , then check if 2. Otherwise, p is of the form A 0 r  X  A 1  X  X  X  A k . For every
One drawback of this algorithm is that its runtime is expo-nential in the size of the RHS of a grammar production, since Case 2 iterates over | t | k  X  1 partitions. This bound is obtained by sorting the input by &lt; ord r and choosing k  X  1 split points to induce a rectangular partition. In the worst case, then (i.e., when parses exist on every partition), our algorithm must consider every grammar production on every rectangular set, giving a complexity of O n 3 + k pk , where n is the num-ber of elements in the input observable, p is the number of grammar productions, and k is the number of RHS tokens in the largest production. The extra factor of k arises from writing up to k partition subsets into a parse table entry in Case 2 . Importantly, k can be controlled by the designer of a grammar as a tradeoff between RHS size and number of grammar productions. For example, if the grammar is writ-ten in Chomsky Normal Form (CNF), then the complexity is O n 5 p . Note that the general bound is asymptotically tight to the worst-case size of the parse forest, since there may be
O n 4 p table entries (counting each production as a dis-tinct entry), each of which may link to O n k  X  1 k other table entries.

Instead of writing our grammar in CNF, we allow arbitrary production lengths and use the following three optimizations to reduce the number of partitions that must be considered. The first two are typical when using Unger X  X  method [ 15 ]; the third is specific to fuzzy r-CFG parsing. 1. Terminal symbol milestones. The terminal symbol rela-2. Minimum non-terminal length. If there are no empty pro-3. Spatial relation test. Just because an input subset can 4.2.3 Incremental parsing The parsing algorithm above may be used incrementally withoutanysignificantchanges.Supposethatparsingiscom-plete for some observable t = { a 1 ,..., a n } . We must handle two cases: the addition of a new observable a n + 1 to t if the user draws a new stroke and the removal of an observable a from t if the user erases a stroke.

In the case where a new observable a n + 1 is added, every existing entry of the parse table remains valid. We may sim-ply invoke the parser on the new input { a 1 ,..., a n + 1 existingparseresultswillbereused.Notethat,althoughexist-ing parses remain valid, they will not necessarily be reused. For example, suppose the expression a 3 + b is changed to a 3 + 2 b by the insertion of a new stroke representing the num-ber 2. Then the parse of a 3 + b on the original three strokes is still a valid interpretation of those strokes. But those strokes no longer form a rectangular set with respect to the new set of input strokes, so they will not be considered as a group when the new input is parsed. However, the existing parses of a 3 and + remain individually valid, and they need only be combined with the new parse of 2 b to form the entire expression.

In the case where an existing observable a i is removed, the situation is slightly more complicated. Any parse table entry foraninputsubset t thatincludes a i becomesinvalidandmust be removed from the table. When the parser is invoked on the revised input set { a 1 ,..., a i  X  1 , a i + 1 ,..., ing parse results that do not include the stroke a i will be reused.

This approach to incremental parsing works particularly well when subsets of the input are represented by bit vectors. Each input element is represented by a bit, where the first stroke drawn corresponds to the lowest-order bit and the most recent stroke to the highest-order bit. If a bit is set, then the corresponding input element is included in the subset, otherwise it is not.

Using this representation, when the first stroke is drawn, the parser might create an entry ( 1 , A ) in the parse table. After a second stroke is drawn, the bit vectors representing subsets will contain two bits. But since the low-order bit cor-responds to the first stroke, accessing ( 01 , A ) is the same as accessing the entry ( 1 , A ) that was created when the input was just one stroke.

Incremental parsing is useful for two main reasons. It per-mits a parser to process in the background as the user is writing, helping to avoid long delays after the user has fin-ished writing the expression. It also allows for reporting of intermediate parse results so that the user can tell whether the parser is going off-track and make any necessary corrections to the output. The tree extraction technique described in the next subsection extracts only complete parse trees (and thus cannotreportpartialresults,like a + ?,inwhichtheright-hand operand has not yet been written). However, it is straightfor-ward to adapt a grammar so as to support partial expressions: one simply creates new non-terminals that derive prefixes or suffixes of complete expressions. 4.3 Parse tree extraction The links between entries in the parse table implicitly spec-ify a shared parse forest. It remains to extract individual parse trees from this forest in decreasing order of member-ship grade in the fuzzy set I t of interpretations of the user X  X  input t . To do so, we must explicitly evaluate the membership grades of particular parse trees and compare them to deter-mine which is highest, second highest, and so on. Given the lack of constraint on the grammar relations and the large number of parse trees in the forest, this may naively be a very time-consuming task.

Consider, for example, the problem of determining the highest-ranked parse tree in the forest shown in Fig. 3 .Call check whether  X  (( t 1 , A ), ( t 2 , x )) &gt; (( t 1 , to decide whether the first two symbols form a multiplica-tion or exponentiation. But those geometric relations may give different results for different expression choices, so we also need to check whether  X  (( t 1 , A ), ( t 2 , X )) &gt; (( t has a higher grade when t 2 is interpreted as x , and the other is higher when it is interpreted as X . Indeed, such behav-ior is desirable to ensure correct recognition of ambiguous cases as illustrated in Fig. 2 . We further need to combine those geometric relation grades with the relevant terminal relation grades, r (( t 1 , A )), r (( t 2 , x )), r (( t membership grades in I t 1  X  t 2 , and thereby determine the best-first ordering of subexpressions.

Because we have specified no constraints on the gram-mar relations, it is possible for two expressions e 1 , e very low membership grades in I t 1 and I t 2 , but for the pair membership in a relation r that e 1 re 2 is the highest-graded interpretation in I t 1  X  t 2 . To determine the highest-graded parse tree overall, one must examine every single parse tree rep-resented in the forest. In particular, at the parse forest node representing the set I p t A This quantity is exactly I p t total number I A 0 t of parses of A 0 on t . At a node higher up the parse forest referencing parses of A 0 on t , every tree in I t would need to be extracted and combined with sibling trees. The number of trees that must be examined by this naive approach thus increases combinatorially as parse tree depth increases. Such an approach is obviously not feasible if we wish to report parses to the user in real-time as (s)he writes. 4.3.1 Relational classes Recall from Sect. 3 that E is the set of all expressions generated by the productions of a given fuzzy r-CFG. We define several relational classes c 1 ,..., c m , and assign every expression in E to at least one class. Thus, E = c 1  X  X  X  X  X  but the c i need not form a partition of E .

These classes play a role in our system similar to that of syntactic or symbol classes in the approaches of Miller and Viola [ 30 ], Zanibbi et al. [ 41 ], and Rutherford [ 33 ]. We include five relational classes for archetypal symbol shapes, which we extend by a special class, box , which represents all multi-symbol expressions. Details are provided in the next section.

To facilitate efficient tree extraction, we assume that the grammar relations depend only on expressions X  relational classes, not on their precise structures. In the example above, one might expect that  X  (( t as x is a centered symbol and X is an ascender. One might also expect that  X  (( t
Thatis,theextenttowhichthefirsttwosymbolsarejudged to be horizontally adjacent with the plus sign does not depend on whether one interprets them as a multiplication or an expo-nentiation.

Our selection of relational classes captures the intuition that symbol identities have a greater effect on subexpres-sion layout than subexpression identities have on the layout of larger expressions. If such an intuition seems unreason-able, one is free to introduce relational classes corresponding to subexpression structures, so that, for example,  X  (( t t , e ), ( t Ax . One is not restricted, as in existing approaches, to syntac-tic classes that represent collections of individual symbols. The relational class approach, thus, provides control over the tradeoff between context sensitivity in grammar relations and execution time.

Formally, denote by C the set of relational classes and by cl ( e ) the set of classes to which an expression e belongs. Each grammar relation may be viewed as a union of class-specific relations: r = where r c 1 , c 2 ( ( t 1 , e 1 ), ( t 2 , e 2 ) ) = 0if c cl ( e 2 ) . Then, by the usual rules of fuzzy sets, r ( ( t
Using the formulation above, we constrain the grammar relations by the following assumption: Assumption 2 ( Interchangeability )Let t 1 , t 2 be observ-r  X  R be a relation. We assume for relational classes c that r whenever c 1  X  cl ( e 1 ), cl (  X  e 1 ) and c 2  X  cl ( e 2 Each production p , and hence each non-terminal symbol ses in the natural way, so that, if A can derive an expression e via production p , then cl ( e )  X  cl ( p )  X  cl ( A ) .
When constructing the parse table, each time a potential parse of some production p is identified on an input subset t , the relational classes associated with p are noted in the parse table. Thus, when extracting trees, the potential classes of expressions that might be obtained from any node of the shared parse forest are known.

This formulation is similar in some ways to the scoring rules developed by Rhee and Kim [ 32 ] to represent all parse trees with identical structure, but differing symbol identities by a single tree with indeterminate terminal symbols. Work-ing in a cost-minimization framework, they defined the cost of a spatial relationship between two indeterminate symbols as the minimum relationship cost between determinate sym-bols, taken over all recognized possibilities for the indeter-minate symbols X  identities. This definition is similar in form to our calculation of a relation membership grade as the max-imum class-specific membership grade, taken over relational classes. In our language, Rhee and Kim assign each terminal symbol to its own relational class. 4.3.2 Spatial relation test in parse forest construction Recall from Sect. 4.2.2 that, during parse forest construction, we test whether a partition of the input is feasible for pars-ing by evaluating spatial relations that approximate the fuzzy grammar relations. Now we may define these approximate relations more specifically, using relational classes.
A pair of observables ( t 1 , t 2 ) should have membership grade 1 in an approximate relation  X  r if there exist expressions e , e has non-zero membership in the corresponding grammar relation r . Using relational classes, this is the same as asking whether max
That is, do relational classes exist such that the interpre-tations will have non-zero membership in the grammar rela-tion, regardless of what the expressions are (since they are assumed to be interchangeable)? If so, we take  X  r ( t 1 , if not,  X  r ( t 1 , t 2 ) = 0.

Note that these  X  r relations are used only in the forest con-struction stage. In the tree extraction algorithm given in the next section, explicit expressions are constructed, so the true grammar relations are used based on each particular expres-sion X  X  relational classes. 4.3.3 Efficient tree extraction using relational classes Consider again the problem of determining the most highly-graded parse of an input t . The most highly-graded parse of a non-terminal A on t is just the most highly-graded parse, taken over all productions with LHS A . Similarly, the most highly-graded parse of a production p on t is the most highly-graded parse, taken over all partitions of t on which potential parses of p were found. These partitions are noted in the parse forest by our variant of Unger X  X  method.

To find the most highly-graded parse of p on a particular partition t 1  X  X  X  X  X  t k of t , suppose that p is of the form A A then the problem is trivial.) If any table entry ( t i , A only one relational class, then the most highly-graded parse must include the most highly-graded interpretation in I A because under the interchangeability assumption, choosing a lower-graded interpretation cannot increase the grade of membership in r , hence it must decrease the overall mem-bership grade in I p t . In such a case, we can simply recursively extract the most highly-graded parse of A i on t i and paste it into a parse tree.

More generally, if a table entry ( t i , A i ) contains several classes, then the most highly-ranked interpretation of each class must be considered. Thus, the membership grade in I must be evaluated for at most i | C i | interpretations, where C is the set of relational classes that were noted at table entry ( t , A a given table entry share relational classes (as they usually will for a reasonable selection of classes), significantly fewer interpretations need to be extracted and graded than in the naive approach to tree extraction discussed at the outset of this subsection.

Note that, because grammar relations are binary, two table some between i and j such that table entry ( t , A ) contains only one relational class. In such cases, the relational classes to each other, reducing the number of evaluations needed.
To find the most highly-graded parse of A i on t i that has relational class c , one need to only know which produc-tions of A i generate expressions of that class and recursively find the most highly-graded parse among those productions.
To extract the top-ranked parse of the entire input, there-fore, one visits every parse table cell in bottom-up order, determining the highest-ranked local parse at each one. Con-sidering different productions with the same LHS as different parse table cells, there are at most n 4 p cells, where n is the number of input elements and p is the number of grammar productions. The forest construction algorithm identifies at most n k  X  1 distinct rectangular partitions on which to parse p , where k is the number of right-hand tokens in the largest production. Using relational classes, i | C i | interpretations must be considered per partition, as described above. This is bounded naively by c k , where c =| C | is the total number of classes. Note that, since the approach proceeds bottom-up, the top-ranked interpretations from partition subsets are available in constant time, as they have already been com-puted and stored. Still, it takes time O ( k ) to extract them, combine them into a larger interpretation and compute its membership grade. Thus, obtaining the top-ranked parse tree takes worst-case time O n 3 + k pc k k . This is the same com-plexity as the forest construction algorithm with an additional factor of c k arising from the relational classes. Importantly, both c and k are determined by the grammar designer, allow-ing a tradeoff between flexibility and speed. If all expressions are considered to be in the same relational class (i.e., if rela-tional membership functions are context-insensitive), then c = 1, and this extra factor disappears.

The above process may be generalized to solve the prob-lem of finding the m th-most highly-graded parse, given that all of the m  X  1 more-highly-graded parses are known. To find the m th parse of a non-terminal A , we maintain a priority queue of known parses. When the ( m  X  1 ) st parse is extracted, it was obtained as, say, the j th parse from a particular pro-duction p . Thus, to find the m th parse, we extract the ( parse from p , add it to the queue, and pop the most highly-ranked parse out of the queue. A similar process may be used to extract the m th-most highly-graded parse from a particular production (selecting over partitions), and from a particular relational class (selecting over productions).

A slightly more complicated process is required to find the n th-most highly-graded parse of a production A 0 r  X  A r  X  X  X  rA k on a particular partition t 1  X   X  X  X   X  t k of t . Again, we maintain a priority queue of known parses. During extraction of the most highly-ranked parse, each interpretation whose membership grade in I t was evalu-ated is added to the queue, and only one X  X he most highly ranked X  X s removed. Inductively, then, the m  X  1st parse was an r -concatenation of the j i th parses of each A i on t of class c i ,for i = 1 ,..., k . Denote this parse by ( j the ranking of the subexpression at that position in the con-catenation. By the interchangeability assumption, the next most highly-ranked parse using the same selection of rela-tional classes must be one of ( j 1 + 1 , j 2 ,..., j k ), ( 1 , j 3 ,..., j k ),...,( j 1 ,..., j k  X  1 , j k + 1 ) . Therefore, the membership grades of all k of these expressions are eval-uated, and the expressions are added to the priority queue. The m th-most highly-ranked parse overall is simply popped off of the queue and its provenance noted, as in the previous case.

In this case, tree extraction is much faster than for the top-ranked tree. That case requires the top-ranked tree to be determined for each cell in the parse table, whereas, when finding the m th-most highly-ranked parse, only those table cells and relational classes explicitly used to form the ( m  X  1 ) st most highly-ranked parse must be considered. There are O ( np ) such table cells. ( O ( n ) for the number of nodes in the parse tree which have more than one child, and O ( p ) to account for trivial productions of the form A  X  B .) At the O ( n ) table cells arising from non-triv-ial productions, at most k new candidate interpretations are created and evaluated, each requiring the extraction of only one subinterpretation. Extraction, thus, has worst-case cost O  X  ( np + nk ) , where the so-called  X  X oft-O X  notation O  X  (  X  ) ignores the logarithmic factors arising from priority queue operations. 4.4 Handling user corrections As mentioned in the introduction, we wish to provide to users a simple correction mechanism so that they may select their desired interpretation in case of recognition errors or multiple ambiguous interpretations. Our recognizer facilitates such corrections by allowing locks to be set on any subset of the input.

Two types of locks are supported: 1. Expression locks , which fix the interpretation of a par-2. Semantic locks , which force interpretations of a partic-
Both of these lock types are useful in different circum-stances.Forexample,supposeauserdrawsthetopexpression in Fig. 1 , meaning the expression A X + b , but the first result returned by the recognizer is A x tb . In our system, the user can correct the result to an addition, A x + b . This applies a semantic lock to the entire input, requiring all derived expres-sions to be additions. The user could then correct the x to X , applying an expression lock to the corresponding ink subset. The graphical interface used in MathBrush for corrections is shown in Fig. 6 .

Consider extracting an expression tree from input t .If t is locked to an expression e by an expression lock, then we consider I t to contain only one element, e , with membership grade1.If t islockedtoanon-terminal A L byasemanticlock, then we take I A t to be empty for all non-terminals A = A except those for which a derivation sequence A  X   X  A L exists, in which case we take I A t = I A L t . 5 Computing the grammar relations The fuzzy r-CFG formulation requires the calculation of two types of fuzzy relations: the terminal symbol relation r and the geometric relations in R . This section outlines the symbol and relation classification subsystems from which our math recognizer obtains membership grades for those relations. 5.1 Terminal symbol relation The symbol classification subsystem receives strokes as input one at a time as they are drawn by the user. Strokes may be merged if they tend in the same direction and their ends are sufficiently close together. For example, in Fig. 7 , the square root sign and fraction bar are each drawn with two strokes that would be merged together.

The input strokes are maintained in a list. The first ele-ment of the list is the first stroke to be received as input. A stroke X  X  successor in the list is the stroke that is nearest to it that has not already appeared in the list. There is no reli-ance on temporal information, so writing or editing order has minimal impact on classification results. Special processing is invoked for strokes that appear to be dots: they are placed after the stroke with which a combination of horizontal and vertical distance measurements is minimized.

Next, groups of strokes that may correspond to distinct ter-minal symbols are identified. Two types of candidate groups are identified by extracting contiguous sublists from the list of strokes. First, proximity groups are extracted and scored based on a combination of stroke proximity and bounding box alignment. Then stacked groups are constructed by iden-tifying collections of proximity groups arranged in vertical stacks; they capture such symbols as  X  ,  X  ,  X  ,etc.
Model-based symbol recognition is performed on each candidate group in a two-step process. Feature-based match-ing is used first as a pruning step: features such as first and last point, width, height, and arc length are extracted from the candidate group. They are compared with corresponding features of model symbols, and the numerical differences are tallied in a weighted sum.

Those models with a sufficiently small feature difference are then compared with the candidate stroke group using a fast variant of elastic matching distance [ 26 ]. Based on com-parison of stroke-based features, the input strokes may be reordered and/or individually reversed so as to obtain the smallest match distance. The elastic matching distances are inverted so that small distances correspond to large scores and then weighted by the appropriate group score (proxim-ity group score for most symbols, stacked group score for stack-based symbols, as described above).

Many symbols share similar-looking strokes or subsym-bols. For example, the symbol E  X  X ontains X  the symbol F. Because symbols with more strokes are drawn with more variability, they typically are scored more poorly by our rec-ognizer. We, therefore, augment the scores of larger symbols by a weighted sum of the scores of similar-looking smaller symbols. The weighting coefficients are automatically deter-mined ahead of time by measuring symbol-to-symbol sim-ilarity among the models in the symbol database using the elastic matching algorithm.

Finally, the augmented scores are normalized so that the highest-scoring symbol has a score of one. These final scores give the membership grades for the fuzzy relation r . 5.2 Geometric relations The geometric relation membership functions are based on bounding box geometry. In our discussion in Sect. 4 of rect-angular sets, we used a definition of bounding boxes based with actual expression geometry, so we consider the natural definition of a bounding box. That is, a stroke X  X  bounding box is the smallest axis-aligned rectangle that completely covers the stroke.

The membership function for the containment relation only considers the amount of overlap between the bound-ing boxes of t 1 and t 2 . We define the overlap proportion , ol ( t divided by the area of the smaller box. The membership func-tion for the containment relation is ( ( t 1 , e 1 ), ( t ol ( t
The membership functions for the other geometric rela-tions incorporate the distance and angle between the bound-ing boxes of the observables. They are all of the form r ( ( t 1 , e 1 ), ( t 2 , e 2 ) ) =  X  ( ( t where  X  is a scoring function based on angle and d is a dis-tance-based penalty function.

The penalty function d ensures that observables satisfying the relations are within a reasonable distance of one another. To compute d , the size of each of t 1 and t 2 is calculated as the average of bounding box width and height. Next, a distance threshold t is obtained as half the average of the two sizes, clamped to the range [ 1 / 6 , 1 / 3 ] (measured in inches). If the distance between the bounding boxes is less than t , then d = 1. Otherwise, d decreases linearly to zero at = 3 t .
The points between which the angle is measured to com-pute  X  are selected based on the relation r and the relational class of the expression e 2 .The x coordinate is chosen cen-trally,butthechoiceof y coordinatevaries.Forthe  X  relation, it is chosen centrally. Table 1 summarizes how it is chosen for the other relations (  X  , , and ), as well as listing the relational classes used in our system. These choices, as well as the selection of threshold values below, were made through experimentation on a small dataset unrelated to those used in the evaluation section.

Given those measurement points, we measure the angle  X  between them relative to the x axis. The angle-based function  X  is triangular with three parameters  X  0 , X  1 , X  2 , as follows:  X ( X ) = Figure 8 indicates the functions X  behavior schematically. Table 2 lists the thresholds for each relation, in degrees. As mentioned above, these values were selected based on manual examination of a small dataset. Note that the y -axis increases downward. 6 Evaluation We evaluated the accuracy of our math recognizer exper-imentally using two publicly available databases of hand-drawn math expressions. For the first evaluation, we used an expression corpus developed by our research group at the University of Waterloo [ 24 , 25 ]. For the second, comparative evaluation, we obtained the data set and marking scripts used at the recent CROHME 2011 math recognition competition [ 31 ]. By using the CROHME data, we may compare our sys-tem against the four recognizers which participated in the competition, as well as a baseline recognizer developed by one of the competition organizers. 6.1 Evaluation on the Waterloo corpus Many of the roughly 4,500 legible expressions in our corpus include mathematical notations not currently supported by our parser (e.g., set notation, multi-symbol variable names). Our test set thus contained 3,672 expressions from 20 writ-ers, of which 53 expressions were common to all writers, and the remainder were randomly generated for each writer. Since this corpus was also intended to provide examples of individual symbols and the geometric relationships between symbols, it contains a large number of expressions with only one or two symbols. We used all of the expressions contain-ing three or fewer symbols as training data for our symbol recognizer, as described in the methodology section below. In all, our grammar contains 49 non-terminals and 104 termi-nal symbols organized into 176 productions. The grammar is given in its entirety as Appendix A . 6.1.1 Correction count metric Devising objective metrics for evaluating the real-world accuracy of a recognition system is difficult. Several authors have proposed accuracy metrics (e.g., [ 8 , 14 , 19 , 33 , 41 ]) based on implementation details specific to their particular recognizers. It is therefore difficult to directly compare their evaluation results to one another, or to apply their methodol-ogies to our evaluation.

Recently, though, some authors have proposed some rec-ognizer-independent evaluation schemes that rely only on the output of a recognizer and treat its implementation as a black box. Awal et al. [ 3 ] proposed an approach in which the accuracy of symbol segmentation, symbol recognition, relation, and expression recognition are reported separately. A positive feature of this approach is that it makes clear, in the case of incorrect expression recognition, which rec-ognizer subsystem failed, and a version of it was used for the CROHME competition (no relation accuracy measure-ments were used). Sain et al. [ 34 ], following the intuition of Garain and Chaudhuri [ 14 ] that errors far from the dominant baseline are less important than those on or near the main baseline, proposed a scheme called EMERS. In it, the edit distance between parse trees representing recognized and ground-truth expressions measures the accuracy of a recog-nition result. Edits are weighted so that those more deeply nested in the parse tree have less cost.

These approaches are both valuable in that they are easily implemented, permit objective comparisons between recog-nizers, and provide valuable feedback to recognizer develop-ers. But they both measure the amount by which a recognized expression deviates from its ground-truth and do not consider whether the ground-truth was achievable by the recognizer at all. If a recognizer consistently fails to recognize a particular structure or symbol, the deviation from ground-truth may be small, even though the recognizer is effectively useless for that recognition task.

Our recognizer was designed for the MathBrush pen-based mathematics system and is intended for real-time, interactive use by human writers. As such, we believe that a user-oriented accuracy model provides the best way to assess its performance. So as well as asking  X  X s the recognition cor-rect? X ,wewanttoasknot X  X owmanysymbolswerewrong? X  or  X  X ow many transformation steps give the correct answer X , but  X  X s the desired result attainable? X , and  X  X ow much effort must the user expend to get it? X  To a user, it is not necessarily the case that an error on the main baseline (say, recognizing a + b as atb ) is more incorrect than one on a secondary baseline (say, recognizing n 2 1  X   X  as n 2 l  X   X  ).
To answer these questions, we count the number of correc-tions that a user would need to make to a recognition result in order to obtain the correct parse. If an input is recognized correctly, then it requires zero corrections. Similarly, if it is recognized  X  X lmost correctly X , it requires fewer corrections than if the recognition is quite poor. This metric is gener-ally applicable to any recognition system, though it clearly is intended to be used with systems providing some correction or feedback mechanism. One could similarly navigate the recognition alternatives provided by Microsoft X  X  math rec-ognizer, for instance, count the number of required correc-tions and obtain comparable measurements. Our evaluation scheme, thus, provides an abstract way to compare the per-formance of recognition systems without direct reference to their implementation details.

Liu et al. [ 23 ] also devised a user-based correction cost model for evaluating a diagram recognition system. They measured the physical time required to correct the errors in recognized diagrams. This approach would also be useful for evaluating our system, but the size of the expression corpus makes it impractical to manually test and correct the recog-nition results.

Instead, we have automated the evaluation process. We developed a testing program that simulates a user interacting with the recognizer. The program passes ink representing a math expression to the recognizer. Upon receiving the rec-ognition results, the program compares them to the ground-truth associated with the ink. If the highest-ranked result is not correct, then the testing system makes corrections to the recognition results, as a user would, to obtain the cor-rect interpretation. That is, the system browses through lists of alternative interpretations for subexpressions or symbols, searching for corrections matching the symbols and struc-tures in the ground-truth. It returns the number of corrections required to obtain the correct expression.

Algorithm 1 outlines this process. Our recognizer uses a  X  X ontext X  to refer to any node in the shared parse forest. So, as the algorithm descends into an expression tree, it can request alternatives for any particular subexpression and having any particular semantics. For example, if an expression intended to be a + c was instead recognized as a + C , the algorithm would detect the correct top-level structure and the correct expression on the left side of the addition sign. On the right side, it would request alternatives  X  X n context X ; that is, using only the ink related to the c symbol and being feasible for the right side of an addition expression, as determined by the grammar.
 Algorithm 1 cc ( g , e ) : Count the number of corrections required to match recognition results to ground-truth.
The correction count produced by this algorithm is akin to a tree edit distance, except that it is constrained so that the edits must be obtainable through the recognizer X  X  output. They cannot be arbitrary terminal or subtree substitutions. 6.1.2 Methodology Although the correction count metric provides accuracy scores for both correct and incorrect recognition results, there are some types of recognition errors that it cannot account for. For example, if an expression is recognized correctly except for one symbol, for which the correct alternative is not avail-able, then the correction count will be  X  , even though the expression is  X  X early correct X .

To reduce the number of these types of results, we tested the recognizer in two scenarios. In the first, called default ,we divided the 3,672 corpus transcriptions into training and test-ing sets. The training set contained all 1,536 transcriptions having fewer than four symbols. The remaining 2,136 tran-scriptions formed the testing set. These transcriptions con-tained between four and 23 symbols each, with an average of seven. All of the symbols were extracted from the training set and used to augment our pre-existing symbol database. The pre-existing database contained samples of each symbol written by one to three writers and is unrelated to the evalu-ation data set. It was used to ensure baseline coverage over all symbol types.

The second scenario, called perfect , evaluated the quality of expression parsing in isolation from symbol recognition. In it, the same testing set of 2,136 transcriptions was used, but the terminal symbol relation was evaluated so as to exactly match ground truth. There were no alternatives for the parser to choose between, and no ambiguities in stroke grouping. This scenario, thus, represents a  X  X est possible world X  for the parser and gives an upper bound on its real-world perfor-mance.

Each transcription used for testing may be placed into one of the following four categories: 1. Correct : No corrections were required. The top-ranked 2. Attainable : The correct interpretation was obtained from 3. Incorrect : The correct interpretation was not obtained 4. Infeasible : The symbol recognizer failed to provide the
Note that the  X  X ncorrect X  and  X  X nfeasible X  categories both refer to incorrectly recognized transcriptions. Using two cat-egories allows us to distinguish between those transcriptions for which the correct expression structure was not identified at all ( X  X ncorrect X ) and those for which the correct structure may have been identified, but correct recognition was impos-sible because one or more symbols did not have the correct candidate available for the parser to choose ( X  X nfeasible X ). Our symbol classifier distinguishes between over 100 sym-bols, but is significantly less accurate than the relation clas-sifier. As such, it is useful to divide incorrectly recognized transcriptions into two categories, so as to know which of the two subsystems caused the failure.
 6.1.3 Accuracy evaluation Figure 9 summarizes the categorization of the testing set in both scenarios. Figure 10 shows the correction count in each scenario, averaged over correct and attainable transcriptions. The correction count is divided into terminal corrections (i.e., corrections that only change terminal symbol identity) and structural corrections (i.e., all others).

Overall, about 19 % of the transcriptions were recognized correctlyinthedefaultscenario,comparedwithabout83%in the perfect scenario. 67 % of the transcriptions were attain-able in the default scenario with about 1.2 corrections on average, while over 98 % were attainable in the perfect sce-nario, with about 0.2 corrections on average. If we consider only feasible transcriptions (those for which the correct sym-bol identities were detected, but not necessarily top-ranked, by the symbol classifier), then 27 % of the transcriptions were recognized correctly, and 98 % were attainable in the default scenario. (The figures for the perfect scenario are unchanged, as all transcriptions were feasible in that case.) This attain-ability rate is very close to the one attained in the perfect scenario, but the correct rate is still quite low, reflecting the difficulty of simultaneously resolving all symbol classifica-tion ambiguity perfectly.

When an expression was attainable, it rarely required more than a few corrections. The top graph in Fig. 11 shows how many expressions required various numbers of corrections in the default scenario. The bottom graph shows the same fre-quency measurement for the perfect scenario. This indicates that it would usually be fairly simple and quick for a user to manually correct recognition errors and obtain the correct results.

Aside from symbol classification errors, there were two main reasons why transcriptions were classified as incorrect. 1. Violation of the ordering assumption. Although the 2. Geometric relation failure. When symbol or subexpres-6.1.4 Performance evaluation The worst-case runtime estimates given in Sect. 4 are quite pessimistic. In actual use, the time required to parse an input and report results depends on many factors that are diffi-cult to quantify generally, including the number of distinct rectangular subsets of the input, the number of subset pairs which have non-zero membership grade in relations (and in how many relations they have such scores), the amount of ambiguity in the symbols used and in the placement of the strokes making them up, and so on. It is therefore interesting to investigate how parse table size varies with input size and to, thus, obtain empirical estimates of the parser X  X  behavior in realistic cases.

We first consider the number of rectangular subsets of the input actually used by the parser. (Recall that, in the worst case, there are n 4 = O n 4 distinct rectangular subsets.) The graphs in Fig. 14 show log X  X og plots of the number of subsets considered with respect to the number of strokes appearing in the input. The default scenario is shown on the top, and the perfect scenario on the bottom. Based on this data, the parser explored on the order of n 2 . 9 subsets, on average, in the default scenario, and n 2 . 2 subsets in the per-fect scenario. This second figure mirrors findings by Liang et al. [ 22 ] in their work on rectangular hulls.

The number of parse table cells depends primarily on the grammar (which is fixed throughout this evaluation), the number of rectangular subsets, and the number of pairs of subsets that are members of the fuzzy geometric relations. Figure 15 shows log X  X og plots of the number of parse table cells with respect to the number of input strokes. As before, the top graph corresponds to the default scenario, while the bottom graph corresponds to the perfect scenario. In the default scenario, the parse table contained on the order of n 1 cells on average, while it contained n 1 . 7 in the perfect scenario. It is interesting that the exponents are lower when counting parse cells than when counting subsets, indicating that not all of the rectangular subsets explored are involved in any parses. This is possible because the forest construction algorithm from Sect. 4 parses a production A 0 r  X  A 1  X  X  X  on a partition t 1  X  X  X  X  X  t k by first parsing A 1 on t 1 , then A on t 2 , and so on, until it reaches the end of the production, or a subparse fails. So some of the subsets t i may be explored without contributing to any useful parse results. It is possible that a heuristic more sophisticated than the rectangular set restriction would reduce the amount of such wasted effort. At the same time, the small size of the parse table relative to the number of subsets available for parsing shows that our parser does a good job of ignoring unproductive subsets since they are identified early in the parsing process.

Finally, we consider the number of links between cells in the parse table (represented by sets of arrows emanating from the AND nodes in Fig. 3 ). The number of alternative interpretations the tree extraction algorithm must consider is directly linked to the number of inter-cell parse table links. The top graph in Fig. 16 indicates how link count scales with input size in the default scenario, and the bottom graph shows the same measurements for the perfect scenario. As before, we use this data to estimate that, on average, the parse table contains on the order of n 2 . 4 links in the default scenario and n 1 . 7 in the perfect scenario. These figures indicate that the parse graph is typically quite sparsely linked. That is, the parser identifies only a small number of ways to parse a given production on a given input subset. Intuitively, this makes sense: to parse a production like [EXPR] + [EXPR] , one must have a symbol that looks like a plus sign, surrounded by expressions, all plausibly horizontally adjacent. For most written addition expressions, there will be few reasonable ways of breaking up the input to satisfy these constraints. That the parse table links reflect this fact illustrates the effi-cacy of the terminal symbol milestone and spatial relation test heuristics built into our parser. 6.2 Evaluation on the CROHME 2011 corpus A recent math recognition competition compared the accu-racy of five recognizers on the CROHME 2011 corpus [ 31 ]. To compare these five recognizers to our own, we acquired the corpus data and associated marking scripts and evaluated the MathBrush recognizer on the competition data.

The data are divided into two parts, each of which includes training and testing data. The first part includes a rela-tively small selection of mathematical notation, while the second includes a larger selection. For details, refer to the competition paper [ 31 ]. According to competition organiz-ers, the recognizers were typically not trained exclusively on the training data. We, therefore, trained our symbol recog-nizer on the same data as in the previous evaluation, aug-mented by all of the symbols appearing in the appropriate part of the CROHME training data. For recognition, we used the grammars provided in the competition documentation by converting them into the file format recognized by our parser.
The accuracy of our recognizer was measured using a perl script provided by the competition organizers. In this eval-uation, only the top-ranked parse was considered. There are four accuracy measurements. Stroke reco. indicates the per-centage of input strokes that were correctly recognized and placed in the parse tree. Symbol seg. indicates the percent-age of symbols for which the correct strokes were properly grouped together. Symbol reco. indicates the percentage of symbol recognized correctly, out of those correctly grouped. Finally, expression reco. indicates the percentage of expres-sions for which the top-ranked parse tree was exactly cor-rect (corresponding to a  X  X orrect X  result in our classification scheme for the previous evaluation).

TheresultsareshowninTable 3 ,whichalsoreproducesfor comparison purposes the updated competition results appear-ing on the CROHME website. 1 Note that the IRCCyN-IVC recognizer was developed by one of the competition orga-nizers and did not directly participate in the competition. Our MathBrush parser obtained higher expression recogni-tion rates than the competition participants, though they were still lower than those of the IRCCyN-IVC recognizer. How-ever, our symbol segmentation and, in particular, symbol rec-ognition rates were not as high as those of some competition participants. These lower-level subsystems would therefore be promising areas on which to focus in future work, so as to improve our overall recognition rates. 7 Conclusions and future work We have described a recognition system for handwritten mathematical notation, which reports all recognizable inter-pretations of users X  writing and allows users to repair incor-rect symbols or subexpressions, while maintaining a runtime that is appropriate for real-time use. To build this system, we introduced a new fuzzy r-CFG formalism designed for pars-ing ambiguous, non-linear input. This formalism captures both the structures and the ambiguities inherent in mathemat-ical writing in particular, and it explicitly models recognition processes like symbol and relation classification. We believe that this formalism is applicable to any structured domain exhibiting the types of syntactic ambiguities found in natural languages.

To facilitate efficient parsing, we introduced the order-ing assumption and demonstrated how it leads naturally to the rectangular hulls proposed by Liang et al. While this assumption theoretically allows most mathematical nota-tions to be adequately described, there are practical exam-ples of mathematical writing which, while easily readable by humans, cannot be parsed by our current algorithm because of the ordering assumption. We intend to investigate how this assumption may be relaxed to more flexibly model expres-sion geometry while still affording efficient parsing algo-rithms.

To report parse results in ranked order of confidence, we extended the idea of syntactic classes to one of relational classes. We plan to expand our selection of relational classes from representing symbol shapes to subexpression shapes, so that our relation membership functions can account for superscript and subscript geometry, for example, rather than treating all multi-symbol expressions as equivalent. How-ever, parse speed decreases significantly when the parser must choose between many classes. We, therefore, plan to investigate more targeted assumptions on the behavior of grammar relations which will allow a greater degree of con-text sensitivity without a significant performance penalty. The rule-based approach used in this paper does not scale well to large sets of relational classes, for which empirical approaches are better-suited. At present, we lack sufficient empirical data to condition the class-specific relation mem-bership functions, but the creation of new handwritten cor-pora should provide useful training data.

The fuzzy membership grade functions defined in Sect. 3 , while reasonable, cannot naturally account for some types of information that might be useful during recognition. (For example, subexpression co-occurence frequencies, the distri-bution of subexpressions within larger expressions, etc.) We plan to investigate the application of Bayesian probability theory and formal statistical methods to the two-dimensional parsing problem in a way that avoids dependence on writing order. As well as the examples mentioned above, there are several existing components of our system, such as the rela-tion classifier, for which it seems likely that appropriately conditioned statistical information would be useful. We are looking into efficient computational methods for combining variegated information (e.g., probabilistic, possibilistic, rule-based, etc.) during parsing.
 Finally, our algorithms must be made more scalable. While they do address the complexity issues arising in our parsing scheme, they take a fairly brute-force approach within the constraints allowed by the simplifying assump-tions. As such, there is a considerable decrease in parsing speed as the input becomes large. We plan to investigate ways in which per-strokes efficiency can be improved while keep-ing available all relevant recognition results.
 Appendix A: Grammar specification Belowaretheproductionsusedinourmathrecognitiongram-mar. Note that, although matrices are included in the gram-mar so that they may be part of mathematical expressions, the [MATRIX-INTERNAL] non-terminal causes the parser to invoke a specialized matrix recognition engine instead of parsing the corresponding rectangular set in the usual way. This special system is necessary to ensure that the matrices are well-formed, as it is impossible in a context-free language to specify in a general way that each row contains the same number of entries. The matrix analysis engine will be the subject of a future report. References
