 weights) of human- X  X  X iven features good representa:ons (or features) representa:on of increasing complexity/abstrac:on Output layer Hidden layers Input layer 
Clustering  X  Human languages, ideas, and  X  Recursion: the same  X  Deep architectures learn  X  Insufficient depth of  X  Mul:ple levels of latent To generalize locally (e.g., We cannot hope to learn from Classical solu:ons: algorithmic techniques ... What has changed? Sen:ment analysis of MSR MAVIS Speech System Changes in compu:ng technology favor deep learning 1. The Basics 2. Recursive Neural Networks 3. Applica:ons, Discussion, and Resources 1. The Basics 2. Recursive Neural Networks 3. Applica:ons, Discussion, and Resources 1. The Basics 2. Recursive Neural Networks 3. Applica:ons, Discussion, and Resources Neural networks come with their own terminological baggage (of  X  X eurons X ,  X  X c:va:on func:ons X , and  X  X eight decay X ) But if you understand how maxent /logis:c regression models work  X  Vector form:  X  Vector form:  X  Make two class: 
P ( c  X  Output of one class:  X  Or we can have x 1 be an always- X  X  X n input  X  Then: func:ons, then we get a vector of outputs [ Glorot and Bengio AISTATS 2010] other models you use  X  Neuron = logis:c regression or similar func:on  X  Input layer = input training/test vector  X  Bias unit = intercept term  X  Ac:va:on = response  X  Weight decay = regulariza:on / Bayesian prior [ Erhan et al., JMLR 2010] hotel , conference , walk AND hotel [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] = 0 informa:on, roughly as a class HMM very useful desparsifica:on of word data distribu:on over words as probabili:es or strengths Compared to a method like LSA:  X  These are the word features we want to learn  X  Also called a look- X  X  X p table  X  score( cat chills on a mat )  X  Then concatenate them to (5n) vector:  X  x =[ ]  X  How do we then compute score(x)?  X  We want all combina:ons of  X  Solu:on: Outer product:  X  Take deriva:ve of score with  X  Now, we cannot just take  X  Very deep once unfolded!  X  Inference  X  X  d iscrete choices  X  The inference opera:ons are flow graph nodes  X  Collobert &amp; Weston 2008: max- X  X  X ooling layer  X  Collobert &amp; Weston (2008)  X  Similar to the word vector  X  so@max ( maxent ) classifier  X  Training is again done via  X  Generalizing beBer to new  X  Deep architectures learn  X  Good representa:ons make  X  Collobert et al (2011) share  X  Major breakthrough in 2006:  X  Hypothesis: P(x) shares structure with P( y|x )
More abstract 
More abstract 
More abstract 
More abstract 
More abstract  X  Preven:ng them to learn the iden:ty everywhere:  X  Regulariza:on hypothesis:  X  Op:miza:on hypothesis:  X  Sparse/saturated units seem to help  X  Why?  X  How to train more towards that objecHve?  X  Just add a penalty on learned representa:on  X  Locally low- X  X  X imensional representa:on = local chart  X  Generalizing beBer to new  X  Deep architectures learn  X  Good representa:ons make  X  Collobert et al (2011) share 1. Mo:va:on 2. Recursive Neural Networks for Parsing 3. Theory: Backpropaga:on Through Structure 4. Recursive Autoencoders 6. Composi:onality Through Recursive Matrix- X  X  X ector Spaces 7. Rela:on classifica:on Inputs: two candidate children X  X  representa:ons Outputs: 2. Score of how plausible the new node would be.  X  Pollack (1990) : Recursive auto- X  X  X ssocia:ve memories  X  The score of a tree is computed by  X  Introduced by [ Goller et al. 1996]  X  Two differences resul:ng from the tree structure:  X  Split deriva:ves at each node  X  Sum deriva:ves of W from all nodes  X  Intui:on via example: 1. Mo:va:on 2. Recursive Neural Networks for Parsing 3. Theory: Backpropaga:on Through Structure 4. Recursive Autoencoders 6. Composi:onality Through Recursive Matrix- X  X  X ector Spaces 7. Rela:on classifica:on is more boring; manipula:ve and contrived boring than anything else.; a major waste ... generic loud, silly, stupid and pointless. ; dull, dumb and deriva:ve horror film.  X  Probability of being posi:ve of several n- X  X  X rams 
Average KL between gold and predicted label distribu:ons:  X  Experiments on Microso\ Research Paraphrase Corpus (Dolan et al. (2004)) 1. Mo:va:on 2. Recursive Neural Networks for Parsing 3. Theory: Backpropaga:on Through Structure 4. Recursive Autoencoders 6. Composi:onality Through Recursive Matrix- X  X  X ector Spaces 7. Rela:on classifica:on 1. Applica:ons 2. Resources (readings, code, ...) 3. Tricks of the trade 4. Discussion: Limita:ons, advantages, future direc:ons  X  Language Modeling  X  Part- X  X  X f- X  X  X peech Tagging  X  Chunking  X  Named En:ty Recogni:on  X  Seman:c Role Labeling  X  Sen:ment Analysis  X  Paraphrasing  X  Ques:on- X  X  X nswering  X  Word- X  X  X ense Disambigua:on  X  Bengio et al NIPS X 2000  X  [Minh &amp; Hinton 2007]  X  APNews perplexity  X  Predict next word given previous words  X  Applica:ons to Speech, Transla:on and Compression P( word|category,context ) only for category=category(word)  X  3 steps: below all other triplets. Train by stochas:c gradient descent : - X  X  Else , construct x  X  = ( lhs 3. If E(x 4. Constraint embedding vectors to norm 1  X  Senseval- X  X 3 results  X  XWN results 
XWN = eXtended WN  X  Tackles problems with polysemous words  X  State of the art  X  Visualiza:on of learned word vectors from  X  [Mohamed et al, 2011, IEEE Tr.ASLP ]  X  Phoneme classifica:on on TIMIT:  X  [ Glorot et al, ICML 2011]  X  Bag- X  X  X f- X  X  X ords input  X  Embeddings pre- X  X  X rained in  X  Disentangling effect  X  25 Amazon.com domains: toys, so\ware, video, books, music, beauty, ...  X 
Unsupervised pre- X  X  X raining of input space on all domains  X 
Supervised SVM on 1 domain, generalize out- X  X  of- X  X  X omain  X  Baseline: bag- X  X  X f- X  X  X ords + 
SVM  X  Deep Learning tutorials  X  Recursive Autoencoder class project:  X  Senna by ( Collobert et al 2011, JMLR)  X  Recurrent Neural Network Language Model with hyper- X  X  X arameters  X  fan- X  X  X ut, Collobert uses Uniform(- X  X 1,1). [Dauphin et al, ICML 2011]  X  Applied to bag- X  X  X f- X  X  X ords input for sen:ment analysis, with denoising auto- X  X  X ncoders  X  Always reconstruct the non- X  X  X eros in the input, and reconstruct as many randomly chosen zeros 
