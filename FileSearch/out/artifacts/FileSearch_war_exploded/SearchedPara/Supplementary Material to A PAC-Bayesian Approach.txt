 Pascal Germain pascal.germain@ift.ulaval.ca Amaury Habrard amaury.habrard@univ-st-etienne.fr Emilie Morvant emilie.morvant@lif.univ-mrs.fr Section 3 introduces other PAC-Bayesian bounds for and implementation details about PBDA (our pro-posed learning algorithm for PAC-Bayesian DA tasks). Lemma 1 (Markov X  X  inequality) . Let Z be a random variable and t  X  0 , then, Lemma 2 (Jensen X  X  inequality) . Let Z be an integra-If g (  X  ) is convex, then, If g (  X  ) is concave, then, tor of i.i.d. random variables, 0  X  X i  X  1 . Then,  X  We recall the Theorem 3 of the main paper. Theorem 3. For any distributions D S and D T over ( D S  X  D T ) m , for every  X  on H , we have, dis  X  ( D S ,D T )  X  Proof. Firstly, we propose to upper-bound, by its empirical counterpart, and some extra terms related to the Kullback-Leibler divergence between the posterior and the prior.  X   X  (  X  we obtain that KL(  X   X  k  X   X  ) = 2KL(  X  k  X  ), amples ( x s , x t )  X  D S  X  T = D S  X  D T by, L bution is defined as, with this loss is, and, It is easy to show that, and consider the non-negative random variable, We apply Markov X  X  inequality (Lemma 1 of this Supp. By taking the logarithm on each side of the previous into an expectation over  X   X  , we obtain that, ln  X  ln = ln X with a probability of success R (1) D (Eq. (11)), leads to,
E  X  E = E We can now upper bound Eq. (12) simply by, ln function F , ln " = ln E  X  E  X  m F ( E We then have, This, in turn, implies that, Now, by isolating R (1) D R and from the inequality 1  X  e  X  x  X  x , R ( D S  X  D T ) m , we have, L over the choice of S  X  T  X  ( D S  X  D T ) m , that d (1) =  X  d (2) , hence and, bound on d (2) gives a bound on dis  X  ( D S ,D T ). we have, | d (1) | + 1 or, which is equivalent, dis  X  ( D S ,D T )  X  and we are done. 3.1. PAC-Bayesian Bounds with the kl term Theorem 6 (Seeger (2002)) . For any domain P S over over H , we have, kl R S ( G  X  ) R P S ( G  X  )  X  Here is a  X  X eeger X  X  type X  PAC-Bayesian bound for our domain disagreement dis  X  .
 Theorem 7. For any distributions D S and D T over H , we have, first bound, by its empirical counterpart, and some extra terms related to the Kullback-Leibler will no need here to redo the all the proof to bound the union bound argument.
 by, L We apply Markov X  X  inequality (Lemma 1). For every choice of S  X  T  X  ( D S  X  T ) m , we have, By taking the logarithm on each side of the previous into an expectation over  X   X  , we then obtain that, ln  X  ln  X  ln (Lemma 4).
 tion kl, ln = ln E  X  E  X  m kl E This implies that, d we have, We claim that we also have, which, since to consider.
 coincide.
 ward calculations, one can show that, d Hence, from Equations (17) and (14), we have, as wanted.
 done.
 then obtain the following PAC-Bayesian DA-bound. Theorem 8. For any domains P S and P T (respec-choice of S  X  T  X  ( P S  X  D T ) m , we have, R D (with  X  :=  X  2 ) in Th. 4 of the main paper. 3.2. PAC-Bayesian Bounds when m 6 = m 0  X  X cAllester X  X  type X  of bound (Similar results can be achieved for  X  X atoni X  X  type X  or  X  X eeger X  X  type X ). First we recall the PAC-Bayesian bound proposed by and the risk.
 Theorem 9 (McAllester (2003)) . For any domain P S over X  X  Y , any set of hypothesis H , and any prior  X  over H , we have, R Now we can prove the following consistency bound for dis  X  ( D S ,D T ), when m 6 = m 0 .
 Theorem 10. For any marginal distributions D S and D
T over X , any set of hypothesis H , any prior dis-T  X  ( D T ) m 0 , for every  X  over H , we have, dis  X  ( D S ,D T )  X  dis  X  ( S,T )  X  Proof. Let us consider the non-negative random vari-able, We apply Markov X  X  inequality (Lemma 1). For every choice of S  X  ( D S ) m , we have, By taking the logarithm on each side of the previous tion  X  , we have,  X  ln Jensen X  X  inequality (Lemma 2). Then, for every  X   X  we have,  X  ln By the Equation (8), distribution  X  , we have,  X  2KL(  X  k  X  ) + E  X  ln Jensen inequality, posterior distribution  X  , we have, 2 m E Let us now bound, ln To do so, we have, and  X  2 . The Pinsker X  X  inequality, Maurer X  X  lemma (Lemma 4).
 posterior distribution  X  , we obtain,  X  E  X  E the following result.
 distribution  X  , and (22). This, together with the union bound that result because, | ( a 1  X  a 2 )  X  ( b 1  X  b 2 ) | X  c 0 1 + c 0 2 . Then we can obtain the following PAC-Bayesian DA-bound.
 Theorem 11. For any domains P S and P T (respec-tively with marginals D S and D T ) over X  X  Y , and and T  X  ( D T ) m 0 , for every  X  over H , we have, 10 (with  X  :=  X  2 ) in Th. 4 of the main paper. 4.1. Objective function and gradient ing the weight vector w minimizing, k w k 2 where, Erf being the Gauss error function, Figure 1 illustrates these three functions. The gradient of the Equation (23) is given by, w + C and s = sgn 4.2. Using a kernel function have, such as, where, as, 1 2
X + A
X where, s = sgn 4.3. Implementation details function using a Broyden-Fletcher-Goldfarb-Shanno method (BFGS) implemented in the scipy python li-URL: http://graal.ift.ulaval.ca/pbda/ 1 . 0 and 10 8 , both on a logarithm scale. CoRR , cs.LG/0411099, 2004.
 McAllester, D. PAC-Bayesian stochastic model selec-tion. Machine Learning , 51:5 X 21, 2003.
 gaussian processes. Journal of Machine Learning
Research , 3:233 X 269, 2002.
